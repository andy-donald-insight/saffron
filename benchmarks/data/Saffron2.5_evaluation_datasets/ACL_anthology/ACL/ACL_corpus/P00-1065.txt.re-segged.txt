Automatic Labeling of Semantic Roles
Daniel Gildea
University of California , Berkeley , and
International Computer Science Institute

Daniel Jurafsky
Department of Linguistics
University of Colorado , Boulder


We present a system for identifying the semantic relationships  , or semantic roles , lled by constituents of a sentence within a semantic frame  . 
Various lexical and syntactic features are derived from parse trees and used to derive statistical clas-siers from hand-annotated training data  . 
1 Introduction
Identifying the semantic roles lled by constituents of a sentence can provide a level of shallow semantic analysis useful in solving a number of natural language processing tasks  . 
Semantic roles represent the participants in an action or relationship captured by a semantic frame  . For example , the frame for one sense of the verb\crash " includes the roles 
Agent , Vehicle and To-Location.
This shallow semantic level of interpretation can be used for many purposes  . Current information extraction systems often use domain-specic frame-and-slot templates to extract facts about  , for example , nancial news or interesting political events . A shallow semantic level of representation is a more domain-independent  , robust level of representation . Identifying these roles , for example , could allow a system to determine that in the sentence\Therstone crashed " the subject is the vehicle  , but in the sentence \ Therstone crashed it " the subject is the agent  , which would help in information extraction in this domain  . Another application is in word sense disambiguation  , where the roles associated with a word can be cuestoits sense  . For example , Lapata and Brew ( 1999 ) and others have shown that the dierent syntactic sub-catgorization frames of a verb like \ serve " can be used to help disambiguate a particular instance of the word \ serve "  . Adding semantic role subcategorization information to this syntactic information could extend this idea to use richer semantic knowledge  . Semantic roles could also act as an important intermediate representation in statistical machine translation or automatic text summarization and in the emerging eld of Text Data Mining  ( TDM )   ( Hearst ,  1999) . Finally , incorporating semantic roles into probabilistic models of language should yield more accurate parsers and better language models for speech recognition  . 
This paper proposes an algorithm for automatic semantic analysis  , assigning a semantic role to constituents in a sentence  . 
Our approach to semantic analysis is to treat the problem of semantic role labeling like the similar problems of parsing  , part of speech tagging , and word sense disambiguation . We apply statistical techniques that have been successful for these tasks  , including probabilistic parsing and statistical classica-tion  . Our statistical algorithms are trained on a hand -labeled dataset : the FrameNet database  ( Baker et al ,  1998) . The FrameNet database denes a tagset of semantic roles called frame elements  , and includes roughly 50 , 0 00 sentences from the British National Corpus which have been hand-labeled with these frame elements  . The next section describes the set of frame elements/semantic roles used by our system  . In the rest of this paper we report on our current system  , as well as a number of preliminary experiments on extensions to the system  . 
2 Semantic Roles
Historically , two types of semantic roles have been studied : abstract roles such as Agent and Patient  , and roles specic to individual verbs such as Eater and Eatenfor \ eat "  . 
The FrameNet project proposes roles at an intermediate level  , that of the semantic frame . 
Frames are dened as schematic representations of situations involving various participants  , props , and other conceptual roles ( Fillmore ,  1976) . For example , the frame\conversation " , shown in Figure 1 , is invoked by the semantically related verbs \ argue "  , \ banter " , \ debate " , \ converse " , and \ gossip " as well as the nouns \ argument " , \ dispute " , \ discussion " and \ ti " . The roles dened for this frame , and shared by all its lexical entries , include Protagonist1 and Protagonist2 or simply Protagonists for the participants in the conversation  , as well as Medium , and Topic . Example sentences are shown in Table 1 . Dening semantic roles at the frame level avoids some of the di  culties of attempting to nda small set of universal  , abstract thematic roles , or case roles such as
Agent , Patient , etc(asin , among many others , ( Fillmore , 1968) ( Jackendo ,  1972)) . 
Abstract thematic roles can be thought of as being frame elements dened in abstract frames such as \ action " and \ motion " which are at the top of in inheritance hierarchy of semantic frames  ( Fillmore and Baker ,  2000) . 
The preliminary version of the FrameNet corpus used for our experiments contained  67 frames from 12 general semantic domains chosen for annotation . Examples of domains ( see Figure 1 ) include\motion " , \ cognition " and\communication " . Within these frames , examples of a total of 1462 distinct lexical predicates , or target words , were annotated : 927 verbs , 339 nouns , and 175 adjectives . There are a total of 49 , 013 annotated sentences , and 99 , 232 annotated frame elements ( which do not include the target words themselves )  . 
3 Related Work
Assignment of semantic roles is an important part of language understanding  , and has been attacked by many computational systems  . Traditional parsing and understanding systems , including implementations of unication-based grammars such as HPSG  ( Pollard and Sag ,  1994) , rely on hand-developed grammars which must anticipate each way in which semantic roles may be realized syntactically  . Writing such grammars is time-consuming , and typically such systems have limited coverage . 
Data-driven techniques have recently been applied to template-based semantic interpretation in limited domains by \ shallow " systems that avoid complex feature structures  , and often perform only shallow syntactic analysis  . For example , in the context of the Air Traveler Information System  ( ATIS ) for spoken dialogue , Miller et al ( 1996 ) computed the probability that a constituent such as \ Atlanta " lled a semantic slot such as Destination in a semantic frame for air travel  . In a datadriven approach to information extraction  , Rilo ( 1993 ) builds a dictionary of patterns forlling slots in aspe-cic domain such as terrorist attacks  , and Rilo and Schmelzenbach ( 1998 ) extend this technique to automatically derive entire case frames for words in the domain  . These last systems make use of a limited amount of handlabor to acceptor reject automatically generated hypotheses  . They show promise for a more sophisticated approach to generalize beyond the relatively small number of frames considered in the tasks  . More recently , a domain independent system has been trained on general function tags such as Manner and Temporal by Blaheta and Charniak  ( 2000 )  . 
4 Methodology
We divide the task of labeling frame elements into two subtasks : that of identifying the boundaries of the frame elements in the sentences  , and that of labeling each frame element , given its boundaries , with the correct role . Werst give results for a system which confer ? v debate ? vconverse?vgossip?vdispute?n discussion ? n t iff ? n 






Frame Elements : talk ? v
Domain : Communication Domain : Cognition
Frame : Questioning


Frame Elements : Speaker





Frame Elements : Speaker




Frame Elements :




Role dispute?nblame?vfault?nadmire?vadmiration ? n d is approve ? v blame ? n appreciate ? v 

Frame Elements :





Figure 1: Sample domains and frames from the FrameNet lexicon  . 
Frame Element Example ( in italics ) with target verb Example ( in italics ) with target noun Protagonist 1 Kim argued with Pat Kim had an argument with Pat Protagonist  2 Kim argued with Pat Kim had an argument with Pat Protagonists Kim and Patargued Kim and Pathad an argument Topic Kim and Patargued about politics Kim and Path a dan argument about politics Medium Kim and Patargued in French Kim and path a dan argument in French Table  1: Examples of semantic roles , or frame elements , for target words \ argue " and\argument " from the\conversation " frame labels roles using human-annotated boundaries  , returning to the question of automatically identifying the boundaries in Section  5  . 3 . 
4.1 Features Used in Assigning
Semantic Roles
The system is a statistical one , based on training a classier on a labeled training set  , and testing on an unlabeled test set . The system is trained by rst using the Collins parser  ( Collins , 1997) to parse the 36 , 995 training sentences , matching annotated frame elements to parse constituents  , and extracting various features from the string of words and the parse tree  . During testing , the parser is run on the test sentences and the same features extracted  . Probabilities for each possible semantic role r are then computed from the features  . The probability computation will be described in the next section  ; the features include : Phrase Type : This feature indicates the syntactic type of the phrase expressing the semantic roles : examples include noun phrase  ( NP )  , verb phrase ( VP ) , and clause ( S ) . Phrase types were derived automatically from parse trees generated by the parser  , as shown in Figure 2 . The parse constituent spanning each set of words annotated as a frame element was found  , and the constituent's nonterminal label was taken as the phrase type  . As an example of how this feature is useful , in communication frames , the Speaker is likely appeara a noun phrase , Topic as a prepositional phrase or noun phrase , and Medium as a prepostional phrase , as in : \ We talked about the proposal over the phone  . " When no parse constituent was found with boundaries matching those of a frame element during testing  , the largest constituent beginning at the frame element's left boundary and lying entirely within the element was used to calculate the features  . 
Grammatical Function : This feature attempts to indicate a constituent's syntactic relation to the rest of the sentence  , 











NP PP
PRPIN


Goal Source Theme Target

He heard the sound of liquids lurping in a metal container as approached him from behind Farrell Figure  2: A sample sentence with parser output ( above ) and FrameNet annotation ( below )  . 
Parse constituents corresponding to frame elements are highlighted  . 
for example as a subject or object of a verb . As with phrase type , this feature was read from parse trees returned by the parser  . After experimentation with various versions of this feature  , we restricted it to apply only to NPs , as it was found to have little eect on other phrase types  . Each NP's nearest S or VP ancestor was found in the parse tree  ; NPs with an Sancestor were given the grammatical function subject and those with a VP ancestor were labeled object  . In general , agenthood is closely correlated with sub -jecthood  . For example , in the sentence \ He drove the car over the cli " , the rstNP is more likely toll the Agent role than the second or third  . 
Position : This feature simply indicates whether the constituent to be labeled occurs before or after the predicate den-ing the semantic frame  . We expected this feature to be highly correlated with grammatical function  , since subjects will generally appear before a verb  , and objects after . Moreover , this feature may overcome the shortcomings of reading grammatical function from a con-stituent 's ancestors in the parse tree  , as well as errors in the parser output . 
Voice : The distinction between active and passive verbs plays an important role in the connection between semantic role and grammatical function  , since direct objects of active verbs correspond to subjects of passive verbs  . From the parser output , verbs were classied as active or passive by building a set of  10 passive-identifying patterns . Each of the patterns requires both a passive auxiliary  ( some form of \ to be " or \ to get " ) and a past participle . 
HeadWord : As previously noted , we expected lexical dependencies to be extremely important in labeling semantic roles  , as indicated by their importance in related tasks such as parsing  . Since the parser used assigns each constituent a head word as an integral part of the parsing model  , we were able to read the head words of the constituents from the parser output  . For example , in a communication frame , noun phrases headed by \ Bill " , \ brother " , or \ he " are more likely to be the Speaker , while those headed by \ proposal " , \ story " , or \ question " are more likely to be the Topic . 
For our experiments , we divided the
FrameNet corpus as follows : one-tenth of the annotated sentences for each target word were reserved as a test set  , and another one-tenth were set as ideas a tuning set for developing our system  . A few target words with fewer than ten examples were removed from the corpus  . In our corpus , the average number of sentences per target word is only  34  , and the number of sentences per frame is 732 both relatively small amounts of data on which to train frame element classiers  . 
Although we expect our features to interact in various ways  , the data are too sparse to calculate probabilities directly on the full set of features  . For this reason , we built our classier by combining probabilities from distributions conditioned on a variety of combinations of features  . 
An important caveat in using the FrameNet database is that sentences are not chosen for annotation at random  , and therefore are not necessarily statistically representative of the corpus as a whole  . Rather , examples are chosen to illustrate typical usage patterns for each word  . We intend to remedy this in future versions of this work by bootstrapping our statistics using unannotated text  . 
Table 2 shows the probability distributions used in the nal version of the system  . Coverage indicates the percentage of the test data for which the conditioning event had been seen in training data  . Accuracy is the proportion of covered test data for which the correct role is predicted  , and Performance , simply the product of coverage and accuracy , is the overall percentage of test data for which the correct role is predicted  . Accuracy is somewhat similar to the familiar metric of precision in that it is calculated over cases for which a decision is made  , and performance is similar to recall in that it is calculated over all true frame elements  . However , unlike a traditional precision/recall trade-o , these results have no threshold to adjust , and the task is a multiway classication rather than a binary decision  . The distributions calculated were simply the empirical distributions from the training data  . That is , occurrences of each role and each set of conditioning events were counted in a table  , and probabilities calculated by dividing the counts for each role by the total number of observations for each conditioning event  . For example , the distribution P(rjpt ; t ) was calculateds as follows:
P ( rjpt ; t ) = #( r ; pt ; t ) #( pt ; t)
Some sample probabilities calculated from the training are shown in Table  3  . 
5 Results
Results for dierent methods of combining the probability distributions described in the previous section are shown in Table  4  . The linear interpolation method simply averages the probabilities given by each of the distributions in Table  2: 
P ( rj  constituent ) =  where
Pi  i = 1 . The geometric mean , expressed in the log domain , is similar :
P ( rj constituent ) = expf  where Z is a normalizing constant ensuring that 
Pr
P ( rj  constituent ) = 1.
The results shown in Table 4 reect equal values of  for each distribution de ned for the relevant conditioning event  ( but excluding distributions for which the conditioning event was not seen in the training data  )  . 
Distribution Coverage Accuracy Performance
P(rjt ) 100% 40.9% 40.9%
P(rjpt ; t ) 92.5 60.1 55.6
P(rjpt;gf ; t ) 92.0 66.6 61.3
P ( rjpt ; position ; voice ) 98 . 8 57 . 1 56 . 4 P(rjpt ; position ; voice ; t ) 90 . 8 70 . 1 63 . 7
P(rjh ) 80.3 73.6 59.1
P(rjh;t ) 56.0 86.6 48.5
P(rjh;pt ; t ) 50.1 87.4 43.8
Table 2: Distributions Calculated for Semantic Role Identi cation : r indicates semantic role  , pt phrase type , g f grammatical function , hhead word , and t target word , or predicate . 
P ( rjpt ; gf ; t ) Count in training data
P ( r = Agtjpt = NP ; gf = Subj ; t = abduct ) = : 466P ( r = Thmjp t = NP ; gf = Subj ; t = abduct ) = : 547P ( r = Thmjp t = NP ; gf = Obj ; t = abduct ) = 19
P ( r = Agtjpt = PP ; t = abduct ) = : 331
P ( r = Thmjp t = PP ; t = abduct ) = : 331
P ( r = CoThmjp t = PP ; t = abduct ) = : 331
P ( r = Manrjpt = ADVP ; t = abduct ) = 11
Table 3: Sample probabilities for P ( rjpt ; g f ; t ) calculated from training data for the verb abduct . 
The variable gf is only dened for noun phrases . The roles dened for the removing frame in the motion domain are : Agent  , Theme , CoTheme(\ . . . had been abducted with him ") and

Other schemes for choosing values of  , including giving more weight to distributions for which more training data was available  , were found to have relatively little eect . We attribute this to the fact that the evaluation depends only the the ranking of the probabilities rather than their exact values  . 
P ( rh , t)P ( rpt , t)P ( rpt , position , voice ) P(rpt , position , voice , t)P ( rpt , g f , t )
P ( rt)P ( rh)
P ( rh , pt , t)
Figure 3: Lattice organization of the distributions from Table  2  , with more specic distributions towards the top . 
In the \ backo " combination method , a lattice was constructed over the distributions in Table  2 from more specic conditioning events to less specic  , as shown in Figure 3 . The less specic distributions were used only when no data was present for any more specic distribution  . As before , probabilities were combined with both linear interpolation and a geometric mean  . 
Combining Method Correct
Linear Interpolation 79.5%
Geometric Mean 79.6
Backo , linear interpolation 80.4
Backo , geometric mean 79.6
Baseline : Most common role 40.9
Table 4: Results on Development Set , 8148 observations
The nal system performed at 80 . 4% accuracy , which can be compared to the 40 . 9% achieved by always choosing the most probable role for each target word  , essentially chance performance on this task . Results for this system on test data , heldout during development of the system , are shown in Table

Backo Baseline
Development Set 80.4% 40.9%
TestSet 76.9 40.6%
Table 5: Results on Test Set , using backolinear interpolation system . The test set consists of 7900 observations . 

5.1 Discussion
It is interesting to note that looking at a con -stituent's position relative to the target word along with active/passive information performed as well as reading grammatical function o the parse tree  . A system using grammatical function , along with the head word , phrase type , and target word , but no passive information , scored 79 . 2% . A similar system using position rather than grammatical function scored  78  . 8% nearly identical performance . However , using head word , phrase type , and target word without either position or grammatical function yielded only  76  . 3% , indicating that while the two features accomplish a similar goal  , it is important to include some measure of the constituent's syntactic relationship to the target word  . Our nal system incorporated both features , giving a further , though not signicant , improvement . As a guideline for interpreting these results , with 8176 observations , the threshold for statistical signifance with p <   :05 is a 1  . 0% absolute dierence in performance . 
Use of the active/passive feature made a further improvement : our system using position but no grammatical function or passive information scored  78  . 8% ; adding passive information brought performance to  80  . 5% . 
Roughly 5% of the examples were identied as passive uses . 
Head words proved to be very accurate indicators of a constituent's semantic role when data was available for a given head word  , conrming the importance of lexicalization shown in various other tasks  . While the distribution P(rjh ; t ) can only be evaluated for 56 . 0% of the data , of those cases it gets 86 . 7% correct , without use of any of the syntactic features . 
5.2 Lexical Clustering
In order to address the sparse coverage of lexical head word statistics  , an experiment was carried out using an automatic clustering of head words of the type described in  ( Lin ,  1998) . A soft clustering of nouns was performed by applying the cooccurrence model of  ( Hofmann and Puzicha ,  1998 ) to a large corpus of observed direct object relationships between verbs and nouns  . The clustering was computed from an automatically parsed version of the British National Corpus  , using the parser of ( Carroll and Rooth ,  1998) . The experiment was performed using only frame elements with a noun as head word  . This allowed a smoothed estimate of P(rjh ; nt ; t ) to be computed as
Pc
P ( rjc ; nt ; t)P ( cj  h) , summing over the automatically derived clusters c to which a nominal head word h might belong  . 
This allows the use of head word statistics even when the head word h has not been seen in conjunction was the target word t in the training data  . While the unclustered nominal head word feature is correct for  87  . 6% of cases where data for P(rjh ; nt ; t ) is available , such data was available for only 43 . 7% of nominal head words . The clustered head word alone correctly class i ed  79  . 7% of the cases where the head word was in the vocabulary used for clustering  ;  97 . 9% of instances of nominal head words were in the vocabulary  . Adding clustering statistics for NP constituents into the full system increased overall performance from  80  . 4% to 81 . 2% . 
5.3 Automatic Identication of
Frame Element Boundaries
The experiments described above have used human annotated frame element boundaries here we address how well the frame elements can be found automatically  . Experiments were conducted using features similar to those described above to identify constituents in a sentence's parse tree that were likely to be frame elements  . The system was given the human-annotated target word and the frame as inputs  , whereas a full language understanding system would also identify which frames come into play in a sentence essentially the task of word sense disambiguation  . The main feature used was the path from the target word through the parse tree to the constituent in question  , represented as a string of parse tree nonterminals linked by symbols indicating upward or downward movement through the tree  , as shown in
Figure 4.

NP VP
VNP
Det N

Heate some target word frame element pan cakes Figure  4: In this example , the path from the frame element\He " to the target word\ate " can be represented as NP " S  #VP  #V  , with " indicating upward movement in the parse tree and  #downward movement  . 
The other features used were the identity of the target word and the identity of the constituent's head word  . The probability distributions calculated from the training data were P  ( fej path )  , P ( f  ejpath ; t) , and P(fejh ; t) , where fe indicates an event where the parse constituent in question is a frame element  , path the path through the parse tree from the target word to the parse constituent  , t the identity of the target word , and h the head word of the parse constituent . By varying the probability threshold at which a decision is made  , one can plot a precision/recall curve as shown in Figure  5  . P ( f  ejpath ; t ) performs relatively poorly due to fragmentation of the training data  ( recall only about 30 sentences are available for each target word )  . 
While the lexical statistic P ( fejh ; t ) alone is not useful as a classier , using it in linear interpolation with the path statistics improves results  . Note that this method can only identify frame elements that have a corresponding constituent in the automatically generated parse tree  . For this reason , it is interesting to calculate how many true frame elements overlap with the results of the system  , relaxing the criterion that the boundaries must match exactly  . Results for partial matching are shown in Table 6 . 
When the automatically identied constituents were fed through the role labeling system described above  ,  79 . 6% of the constituents which had been correctly identied in the rst stage were assigned the correct role in the second  , roughly equivalent to the performance when assigning roles to constituents identied by hand  . 
0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 recall precision

P ( fepath , t) . 75*P ( fepath ) + . 25* P(feh , t ) Figure 5: Precison/Recall plot for various methods of identifying frame elements  . Recall is calculated over only frame elements with matching parse constituents  . 
6 Conclusion
Our preliminary system is able to automatically label semantic roles with fairly high accuracy  , indicating promise for applications in various natural language tasks  . Lexical statistics computed on constituent head words were found to be the most important of the features used  . While lexical statistics are quite accurate on the data covered by observations in the training set  , the sparsity of the data when conditioned on lexical items meant that combining features was the key to high overall performance  . While the combined system was far more accurate than any feature Type of Overlap Identied Constituents Number 
Exactly Matching Boundaries 66% 5421
Identied constituent entirely within true frame element  8   663 True frame element entirely within identied constituent  7   599 
Partial overlap 026
No match to true frame element 13972
Table 6: Results on Identifying Frame Elements ( FEs )  , including partial matches . Results obtained using P ( fej path ) with threshold at . 5 . A total of 7681 constituents were identied as FEs , 8167 FEs were present in hand annotations , of which matching parse constituents were present for  7053   ( 86% )  . 
taken alone , the specic method of combination used was less important  . 
We plan to continue this work by integrating semantic role identication with parsing  , by bootstrapping the system on larger , and more representative , amounts of data , and by attempting to generalize from the set of predicates chosen by FrameNet for annotation to general text  . 

Collin F . Baker , Charles J . Fillmore , and John B . 
Lowe . 1998. The berkeley framenet project.
In Proceedings of the COLINGACL , Montreal,

Dan Blaheta and Eugene Charniak .  2000 . Assigning function tags to parsed text . In Proceedings of the 1st Annual Meeting of the North American Chapter of the ACL  ( NAACL )  , Seattle , Washington . 
Glenn Carroll and Mats Rooth .  1998 . Valence induction with a head-lexicalized pcfg . In Proceedings of the 3rd Conference on Empirical Methods in Natural Language Processing  ( EMNLP3 )  , Granada , Spain . 
Michael Collins .  1997 . Three generative , lexicalised models for statistical parsing . In Proceedings of the 35th Annual Meeting of the

Charles J . Fillmore and Collin F . Baker . 2000.
Framenet : Frame semantics meets the corpus.
In Linguistic Society of America , January.
Charles Fillmore .  1968 . The case for case . In Bach and Harms , editors , Universals in Linguistic Theory , pages 188 . Holt , Rinehart , and
Winston , New York.
Charles J . Fillmore .  1976 . Frame semantics and the nature of language . In Annals of the New York Academy of Sciences : Conference on the Origin and Development of Language and 
Speech , volume 280, pages 2032.
Marti Hearst . 1999. Untangling text data mining.
In Proceedings of the 37rd Annual Meeting of the ACL . 
Thomas Hofmann and Jan Puzicha .  1998 . Statistical models for cooccurrence data . Memo , Massachussetts Institute of Technology Arti- cial Intelligence Laboratory  , February . 
Ray Jackendo .  1972 . Semantic Interpretation in Generative Grammar . MIT Press , Cambridge,

Maria Lapata and Chris Brew .  1999 . Using subcategorization to resolve verb class ambiguity  . In Joint SIGDAT Conference on Empirical Methods in NLP and Very Large Corpora  , 

Dekang Lin .  1998 . Automatic retrieval and clustering of similar words  . In Proceedings of the
COLINGACL , Montreal , Canada.
Scott Miller , David Stallard , Robert Bobrow , and Richard Schwartz .  1996 . A fully statistical approach to natural language interfaces  . In Proceedings of the 34th Annual Meeting of the

Carl Pollard and Ivan A . Sag .  1994 . Head-Driven Phrase Structure Grammar . University of Chicago Press , Chicago . 
Ellen Rilo and Mark Schmelzenbach .  1998 . An empirical approach to conceptual case frame acquisition  . In Proceedings of the Sixth Workshop on Very Large Corpora  . 
Ellen Rilo .  1993 . Automatically constructing a dictionary for information extraction tasks  . 
In Proceedings of the Eleventh National Conference on Articial Intelligence  ( AAAI )  . 
