Efficient Linear Logic Meaning Assembly
Vine et Gupta
Caelum Research Corporation
NAS AAmes Research Center
Moffett Field CA 94035
vg up ta@p to lemy , arc . nasa.gov
John Lamping
Xerox PARC
3333 Coyote Hill Road
Palo Alto CA 94304 USA
iamping ? parc , xerox , tom
1 Introduction
The " glue " approach to semantic omposition in Lexical-Functional Grammar uses linear logic to assemble meanings from syntactic analyses  ( Dalrymple et al ,  1993) . It has been compu-rationally feasible in practice  ( Dalrympl et al . , 1997b ) . Yet deduction in linear logic is known to be intractable  . Even the propositional tensor fragment is NP complete  ( Kanovich ,  1992) . 
In this paper , we investigate what has made the glue approach computationally feasible and show how to exploit that to efficiently deduce underspecified representations  . 
In the next section , we identify a restricted pattern of use of linear logic in the glue analyses we are aware of  , including those in ( Crouch and Genabith , 1997; Dalrymple et al , 1996; Dalrymple et al ,  1995) . And we show why that fragment is computationally feasible  . In other words , while the glue approach could be used to express computationally intractable analyses  , actual analyses have adhered to a pattern of use of linear logic that is tractable  . 
The rest of the paper shows how this pattern of use can be exploited to efficiently capture all possible deductions  . We present a conservative x tension of linear logic that allows a reformulation of the semantic ontributions to better exploit this pattern  , almosturning them into Horn clauses . We present a deduction algorithm for this formulation that yields a compact description of the possible deductions  . And finally , we show how that description of deductions can be turned into a compact underspeci-fled description of the possible meanings  . 
Throughout hepaper we will use the illustrative sentence " every gray cat left "  . It has flH lctional structure ( 1 ) \[ PRED'LEAVE'1
PRED'CAT'f:SUBJ g:\[SPEC'EVERY'\[ MODS\[PRED'GRAY'\] and semantic ontributions leave:Vx  . ga ', -* x-- . of o ', -* leave(x ) cat : w . ( gaVAR ) --** ~( gaRESTR )-, ~ Cat (*) gray NP . \[ Vx . ( gaVAtt)---*x--o(g~RESTR)- , -* P(x)\]-o\[w . ( g ~ VAR ) ~, every : VH , R , S . 
\[ w . ( g ~ VAR ) -- ~ , -- o(g ~ RESTR)--~R(z)\]?\[Vx . g , , ' . . . * x~g- , -* S(x)\]--~H' , -* every ( R , S ) For our purposes , it is more convenient to follow ( Dalrympletal . , 1997a ) and separate the two parts of the semantic ontributions : use a lambda term to capture the meaning formulas  , and a type to capture the connections to the f -structure  . In this form , the contributions are leave : cat : gray : every  : 
Ax . leave(x ): g ,, -- of a . , ~ x . cat(x):(gaVAR)--o(gaRESTR )
AP . Ax . gray ( P ) ( x )  :  ( ( g ~ vAR ) -- o ( gaRESTR ) ) -- o ( g ~ V An )  ~  ( gaRESTR ) 
AR . AS . every ( R , S):
VH . ((( g ~' CAR)--o(gaRESTR )) ?( g . ~ H )) - - oH With this separation , the possible derivations are determined solely by the " types "  , the connections to the f-structure . The meaning is assembled by applying the lambda terms in accordance with a proof of a type for the sentence  . We give the formal system behind this approach , C in Figure 1 -- this is a different presentation of the system given in  ( Dalrymple for tensor , using pairing for meanings . For the types , the system merely consists of the Inear logic rules for the glue fragment  . 
We give the proof for our example in Figure 2 , where we have written the types only , and have omitted the trivial proofs at the top of the tree  . 
The meaning every ( gray ( cat ) , left ) may be assembled by putting the meanings back in according to the rules of C and r /- reduction  . 
M : A~-cM /: A where M--a , nM ~
F,P,Q,A~-cR
F,Q,P,AF-cRr , : A\[B/X\]-oR
F,M:VX.AF-cRrM:A\[Y/X\](rnew)
FtcM:VX . A
F~-cN:AA,M\[N/x\]:BF-cR
F , A , Ax . M : A--o B~-cRr , y : A be M\[y/x\]:BrF-cAx . M : A . - . oB(ynew )
F,M:A,N:BI--Rr,(M,N):A?B~-R
FF-M : AA~-N : B
F,A~-(M,N):A?B
Figure 1: The system C . M , N are meanings , and x , y are meaning variables . A , B are types , and X , Y are type variables . P , Q , R are formulas of the kind M : A . F , A are multisets of formulas . 
2 Ske leton re ferences and modi f ie r ref-e rences The terms that describe atomic types  , terms Ikega and ( g~vA1 Q , are semantic structure references , the type atoms that connect he semantic assembly to the syntax  . There is a pattern to how they occuring lue analyses  , which reflects their function in the semantics . 
Consider a particular type atom in the example , such as g ~ . It occurs once positively in the contribution of " every " and once negatively in the contribution of " leave "  . A sightly more compIcated example , the type ( gal~nSTR ) occurs once positively in the contribution of " cat "  , once negatively in the contribution of " every " , and once each positively and negatively in the contribution of " gray "  . 
The pattern is that every type atom occurs once positively in one contribution  , once negatively in one contribution , and once each positively and negatively in zero or more other contributions  . ( To make this genera Ization hold , we add a negative occurrence or " consumer " of f a  , the final meaning of the sentence . ) This pattern holds in all the glue analyses we know of  , with one exception that we will treat shortly . 
We call the independent occurrences the skeleton occurrences  , and the occurrences that occur paired in a contribution modifier occurrences  . 
The pattern reflects the functions of the lexical entries in LFG  . For the type that corresponds to a particular f -structure  , the idea is that , the entry corresponding to the head makes a positive skeleton contribution  , the entry that subcategorizes for the f-structure makes a negative skeleton contribution  , and modifiers on the f-structure make both positive and negative modifier contributions  . 
Here are the contributions for the example sentence again  , with the occurrences classified . 
Each occurrence is marked positive or negative , and the skeleton occurrences are underlined . 
leave : g_Ka--ofa + cat : ( gaVAtt ) - - - o ( gatt ESWtt ) + gray: ( ( ga VAn ) + -- o ( gaaESTR ) - ) - - - o ( ga VA n ) - - - o ( gaRESTR ) + every : VH . ((( ga VAR ) + -- o(gaRESTR . )-) ?( g_z . ~~ -- ~ g- )   ) --- oH + This pattern explains the empirical tractability of glue inference  . In the general case of multiplicative In ear logic  , there can be complex combinatorics in matching up positive and negative occurrences of literals  , which leads to NP-completeness ( Kanovich ,  1992) . But in the glue fragment , on the other hand , the only combinatorial question is the relative ordering of modi-tiers  . In the common case , each of those orderings is legal and gives rise to a different meaning  . So the combinatorics of inference tends to be proportional to the degree of semantic ambiguity  . The complexity per possible reading is thus roughly tnear in the size of the utterance  . 
But , this simple combinatoric structure suggests a better way to exploit the pattern  . 
Rather than have inference x plore all the com -binatorics of different modifier orders  , we can get a single underspecitied representation that captures all possible orders  , without having to cat ,   ( ( ga VAR ) - - - o ( gaRESTR ) ) -- o ( ga VAR ) - - - o ( gaRESTR )  ~  ( ga VAR ) - - - o ( gaRESTR ) gray , cat ~- ( ga VAR )  --~  ( gaRESTR ) leave F-ga--of ogray , cat , leave F((goVAR)--~(goRESTR )) ~( ga . -- of a ) fo ~- fagray , cat , leave , ((( ga VAR)--o(gaRESTR )) ?( go . - - - of o )) -- of o ~- fo every , gray , cat , leave F-fa Figure 2: Proof of " Every gray cat left " , omitting the lambda terms explore them . 
The idea is to do a preliminary deduction involving just the skeleton  , ignoring the modifier occurrences . This will be completely deterministic and linear in the total length of the formulas  . Once we have this skeletal deduction , we know that the sentence is wellformed and has a meaning  , since modifier occurrences essentially occur as instances of the identity axiom and do not contribute to the type of the sentence  . Then the system can determine the meaning terms , and describe how the modifiers can be attached to get the final meaning term  . 
That is the goal of the rest of the paper.
3 Conversion toward horn clauses
The first hurdle is that the distinction between skeleton and modifier applies to atomic types  , not to entire contributions . The contribution of " every " , for example , has skeleton contributions forgo , ( go VAR ) , and ( gaRESTR ) , but modifier contributions for H . Furthermore , the nested implication structure allows no nice way to disentangle the two kinds of occurrences  . When a deduction interacts with the skeletal go in the hypothetical it also brings in the modifier H  . 
If the problematic hypothetical could be converted to Horn clauses  , then we could get a better separation of the two types of occurrences  . 
We can approximate this by going to an indexed linear logic  , a conservative x tension of the system of Figure 1 , similar to Hepple's sys-tem ( Hepple ,  1996) . 
To handle nested implications , we introduce the type construct or AB , which indicates an A whose derivation made use of B  . This is similar to Hepple's use of indices , except that we indicate dependence on types , rather than on indices . This is sufficient in our application , since each such type has a unique positive skeletal occurrence  . 
We can eliminate problematic nested implications by translating them into this construct  , in accordance with the following rule : For a nested hypothetical ttoplevel that has a mix of skeleton and modifier types : 
M : ( A-oB ) - oC replace it with x : A , M : ( BA---oC ) where x is a new variable , and reduce complex dependency formulas as follows  :   1  . Replace AB---oC with ACB . 
2. Replace ( A--oB ) C with A--o BC.
The semantics of the new type constructors is captured by the additional proof rule : 
F , x : AF-M:B
F,x:A~-Ax.M:BA
The translation is sound with respect o this rule : Theorem  1 If F is a set of sentences in the unextended system of Figure  1  , A is a sentence in that system , and F ~ results from F by applying the above conversion rules  , then FF-A in the system of Figure 1 iff F'F-A in the extended system . 
The analysis of pronouns present a different problem  , which we discuss in section 5 . For all other glue analyses we know of , these conversions are sufficient to separate items that mix interaction and modification into statements of skeleton and Mispure modifier  . Furthermore, . h4 will be of the form A-oA , where A may be a formula , not just an atom . In other words , the type of the modifier will be an identity axiom  . 
The modifier will consume some meaning and produce a modified meaning of the same type  . 
In our example , the contribution of " every " , can be transformed by two applications of the nested hypothetical rule to every : AR  . AS . every ( R , S):
VH . ( gaRESTR ) ( gaVAR ) - - oHg q-oHx : ( go VAR )
Y : ga
Here , the last two sentences are pure skeleton , producing ( g ~ VAR ) and ga , respectively . 
The first is of the form S-oM , consuming ( gaRESTR ) , to produce a pure modifier . 
While the rule for nested hypotheticals could be generalized to eliminate all nested implications  , as Hepple does , that is not our goal , because that does remove the combinatorial combination of different modifier orders  . We use the rule only to segregate skeleton atoms from modifier atoms  . Since we want modifiers to end up looking like the identity axiom  , we leave them in the A-oA form , even if A contains further implications . For example , we would not apply the nested hypothetical rule to simplify the entry for gray any further  , since it is already in the form A---oA . 
Handling intensional verbs requires a more precise definition of skeleton and modifier  . The type part of an intensional verb contribution looks like  ( VF . (ha - o F ) -- oF ) -oga-of a ( Dalrympl et al . , 1996) . 
First , we have to deal with the small technical problem that the VF gets in the way of the nested hypothetical translation rule  . This is easily resolved by introducing as kolem constant  ,  5' , turning the type into ( ( h ~ - o 5' ) -- o5' ) -- og~--of ~ . Now , the nested hypothetical rule can be applied to yield  ( ho-oS ) and S5"h~---oga--of a . 
But now we have the interesting question of whether the occurrences of the skolem constant  , S , are skeleton or modifier . If we observe how 5' resources get produced and consumed in a deduction involving the intensional verb  , we find that ( ha--o5') produces an 5' , which may be modified by quantifiers , and then gets consumed by SS ha---oga-o f  ~  . Sounlike a modifier , which takes an existing resource from the environment and puts it back  , the intentional verb places the initial resource into the environment  , allows modifiers to act on it , and then takes it out . In other words , the intensional verb is acting like a combination of a skeleton producer and a skeleton consumer  . 
So just because an atom occurs twice in a contribution doesn't make the contribution a modifier  . It is a modifier if its atoms must interact with the outside  , rather than with each other . Roughly , paired modifier atoms function as f-of , rather than as f?f ? , as do the Satoms of intensional verbs . 
Stated precisely :
Definition 2 Assume two occurrences of the same type atom occur in a single contribution  . 
Convert the formula to a normal form consisting of just ?  ,  ~  , and J_on atoms by converting subformulas A-oB to the equivalent A ? : ~ B  , and then using DeMor gan's laws to push all J_'s down to atoms  . Now , if the occurrences of the same type atom occur with opposite polarity and the connective between the two subexpressions in which they occur is ~  , then the occurrences are modifiers . All other occurrences are skeleton . 
For the glue analyses we are aware of , this definition identifies exactly one positive and one negative skeleton occurrence of each type among all the contributions for a sentence  . 
4 Eff ic ient deduct ion of underspec i f ied representation In the converted form  , the skeleton deductions can be done independently of the modifier deductions  . Furthermore , the skeleton deductions are completely trivial , they require just a linear time algorithm : since each type occurs once positively and once negatively  , the algorithm just resolves the matching positive and negative skeleton occurrences  . The result is several deductions starting from the contributions  , that collectively use all of the contributions . One of the deductions produces a meaning for fa , for the whole f-structure . The others produce pure modifiers -- these are of the form A--oA  . For leave : cat : gray : every x : every2 : every a :
Ax . leave(x ): ga--ofc , ax . eat(x ): VAR ) R . STR ): VAR)--oRSTR )) VAR)--oRESTR ) AR . AS . every(R,S):vg . ( g~RnSTR )( g~'CAR)--ogga---oH zVAR )
Y : g ~
The following can now be proved using the extended system : gray ~- AP  . Ax . gray ( P ) ( x )  :  ( ( ga VAR ) -- o ( g ~ RESTR ) ) ---- O ( g ~ VAR ) -- o ( gaRESTR ) every 2 , cat , every 1~-AS . every(Ax . eat(x ), S : VH . Hga--o Heverya , leave F-leave ( y ) : fa Figure 3: Skeleton deductions for " Everygray cat left " . 
the example sentence , the results are shown in
Figure 3.
These skeleton deductions provide a compact representation of all possible complete proofs  . 
Complete proofs can be read off from the skeleton proofs by interpolating the deduced modi-tiers into the skeleton deduction  . One way to think about interpolating the modifiers is in terms of proofnets  . A modifier is interpolated by disconnecting the arcs of the proof net that connect he type or types it modifies  , and recon-necting them through the modifier . Quantifiers , which turn into modifiers of type VF . F---oF , can choose which type they modify . 
Not all interpolations of modifiers are legal . however . For example , a quantifier must out scope its noun phrase . The indices of the modifier record these limitations  . In the case of the modifier resulting from " every cat "  , VH . Hga---oH , it records that it must outscope " every cat " in the ga  . The indices determine a partial order of what modifiers must outscope other modifiers or skeleton terms  . 
In this particular example , there is no choice about where modifiers will act or what their relative order is  . In general , however , there will be choices , as in the sentence " someone likes every cat " , analyzed in Figure 4 . 
To summarize so far , the skeleton proofs provide a compact representation fall possible deductions  . Particular deductions are read off by interpolating modifiers into the proofs  , subject to the constraints . But we are usually more interested in all possible meanings than in all possible deductions  . Fortunately , we can extract a compact representation fall possible meanings from the skeleton proofs  . 
We do this by treating the meanings of the skeleton deductions as trees  , with their arcs annotated with the types that correspond to the types of values that flow along the arcs  . Just as modifiers were interpolated into the proofnet links  , now modifiers are interpolated into the links of the meaning trees  . Constraints on what modifiers must outscope become constraints on what tree nodes a modifier must dominate  . 
Returning to our original example , the skeleton deductions yield the following three trees : ! g 
RESTR)/~/-/Iga~tg a VAR)---o ?, ~ Z . \] gaRESTR ) leave ( go RESTR ) Igray catlga VAR ) -- o
Ig~(goVAR)~Itgo'RESTR ) yleave(y)aS . every(;~x . cat(x ), S)aP . ax . gray(P)(x)
Notice that higher order arguments are reflected as structured types  , like(g~VAR)----o(g~RESTR ) . These trees are a compact description of the possible meanings  , in this case the one possible meaning . We believe it will be possible to translate this representation i to a UDRS representation  ( Reyle ,  1993) , or other similar representations for ambiguous entences  . 
We can also use the trees directly as an underspecified representation  . To read out a particular meaning , we just interpolate modifiers into the arcs they modify  . Dependencies on a

SUBJ/:

The lexical entries after ' LIKE'h:\[pRro' so MroNE '\] 
PRED'eAT'\]g:SPEC~EVERY'conversion to indexed form : like : cat : someone l :  someone2 : every l: every2 : every a :
Ax . Ay.tike(x , y ) : ( ho?go)-o/o
Ax . cat(x):(goVAR)-o(gaRESTR)z:hv
AS . some ( person , S):VH . Hho)--oH
AR . AS . every(R,S):vg .   ( go RESTR ) (go VA1Q ) -- oHgo ) - - oHx : ( go VAR ) 

From these we can prove : someone 1 , every a , like ~- like(z , y ): fo someone 2F-AS . some ( person , S ): VH . Hho--o Hevery 2, cat , every 1 bAS . every(cat,S):VH . Hgo-oH Figure 4: Skeleton deductions for " Someone likes every cat " modifier's type indicate that a lambda abstraction is also needed  . So , when " every cat " modifies the sentence meaning , its antecedent , instantiated to fogo ) indicates that it lambda abstracts over the variable annotated with go and replaces the term annotated fo  . So the result is:
If o , every
RESTR .) AAx.Y.
(go RESTR ) \]\] fo cat leave ( go VAR ) !:/ o Similarly " gray " can modify this by splicing it into the line labeled  ( go VAR ) -- o ( go RESTR ) to yield ( aftery-reduction , and removing labels on the arcs ) . 
If o/vergray leave
I cat
This gets us the expected meaning every ( gray ( cat ) , leave ) . 
In some cases , the link called for by a higher order modifier is not directly present in the tree  , and we need to do A-abstraction to support it . Consider the sentence " John read Hamlet quickly  "  . We get the following two trees from the skeleton deductions : re!fdg/\ho 
John Hamletread ( John , Hamlet)
Igo--ofo quickly
Igo-ofo
AP . Ax . quickly ( P ) ( x)
There is no link labeled ga--of a to be modified  . The left tree however may be converted by A -abstraction to the following tree  , which has a required link . The @ symbol represents A application of the right subtree to the left  . 

Ax . John
I/oreadgj\hoxHamlet
Now quickly can be interpolated into the link labeled go--of o to get the desired meaning quickly  ( read ( Hamlet )  , John ) , after r /- reduction . The cases where A-abstraction is required can be detected by scanning the modifiers and noting whether the links to be modified are present in the skeleton trees  . If not , A-abstraction can introduce them into the un -introduction is unavoidable  , as the link will be present in any final meaning . 
5 Anaphora
As mentioned earlier , anaphoric pronouns present a different challenge to separating skeleton and modifier  . Their analysis yields types like f~--o ( f ~ ? g ~ ) where g~is skeleton and f ~ is modifier . We sketch how to separate them . 
We introduce another type constructor ( B ) A , informally indicating that A has not been fully used  , but is also used to get B . 
This lets us break a part an implication whose righthand side is a product in accordance with the following rule : For an implication that occurs at top level  , and has a product on the right hand side that mixes skeleton and modifier types : 
Ax . ( M , N ): A---o(B?C ) replace it with
Ax.M:(C)A-oB,N:C
The semantics of this constructor is captured by the two rules : 
M1: AI~...,M,~:An~-M:A
M1:(B ) A1, . . . , Mn : ( B)A,~t-M : ( B)A
F,M1:(B)A,M2:B~-N:C
F t , M ~: A , M~:B~-N':C where the primed terms are obtained by replacing free x's with what was applied to the Ax  . in the deduction of ( B ) A With these rules , we get the analogue of Theorem 1 for the conversion rule . In doing the skeleton deduction we don't worry about the  ( B ) A constructor , but we introduce constraints on modifier positioning that require that a hypothetical dependency can't be satisfied by a deduction that uses only part of the resource it requires  . 
6 Acknowledgements
We would like to thank Mary Dalrymple , John Fry , Stephan Kauffmann , and Hadar Shemtov for discussions of these ideas and for comments on this paper  . 

Richard Crouch and Josef van Genabith . 1997.
How to glue a donkey to an f-structure , or porting a dynamic meaning representation into LFG's linear logic based glue-language semantics  . Paper to be presented at the Second International Workshop on Computational Semantics  , Tilburg , The Netherlands , 
January 1997.
Mary Dalrymple , John Lamping , and Vijay
Saraswat .  1993 . LFG semantics via constraints . In Proceedings of the Sixth Meeting of the European ACL  , pages 97-105 , University of Utrecht . European Chapter of the Association for Computational Linguistics  . 
Mary Dalrymple , John Lamping , Fernando
C . N . Pereira , and Vijay Saraswat .  1995 . Linear logic for meaning assembly . In Proceedings of CLNLP , Edinburgh . 
Mary Dalrymple , John Lamping , Fernando
C . N . Pereira , and Vijay Saraswat .  1996 . In-tensional verbs withoutype-raising or lexical ambiguity  . In Jerry Seligman and DagWest-erst?hl , editors , Logic , Language and Computation , pages 167-182 . CSLI Publications,
Stanford University.
Mary Dalrymple , Vine et Gupta , John Lamp-ing , and Vijay Saraswat . 1997a . Relating resource-based semantics to categorial semantics  . In Proceedings of the Fifth Meeting on Mathematics of Language  ( MOL5 )  , Schloss
Dagstuhl , Saarbriicken , Germany.
Mary Dalrymple , John Lamping , Fernando
C.N . Pereira , and Vijay Saraswat . 1997b.
Quantifiers , anaphora , and intensionality.
Journal of Logic , Language , and Information ,  6(3):219-273 . 
Mark Hepple .  1996 . A compilation-chart method for linear categorical deduction  . In
Proceedings of COLING96, Copenhagen.
MaxI . Kanovich .  1992 . Horn programming in linear logic is NP-complete . In Seventh Annual IEEE Symposium on Logic in Computer Science  , pages 200-210 , Los Alamitos , California . IEEE Computer Society Press . 
Uwe Reyle .  1993 . Dealing with ambiguities by underspecification : Construction  , representation , and deduction . Journal of Semantics , 10:123-179 . 

