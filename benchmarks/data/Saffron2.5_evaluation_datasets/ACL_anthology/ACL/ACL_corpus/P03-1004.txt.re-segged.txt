Fast Methods for Kernel-based Text Analysis
Taku Kudo and Yuji Matsumoto
Graduate School of Information Science,
Nara Institute of Science and Technology


Kernel-based learning ( e . g . , Support Vector Machines ) has been successfully applied to many hard problems in Natural 
Language Processing ( NLP ) . In NLP , although feature combinations are crucial to improving performance  , they are heuristically selected . Kernel methods change this situation . The merit of the kernel methods is that effective feature combination is implicitly expanded without loss of generality and increasing the computational costs  . Kernel-based text analysis shows an excellent performance in terms in accuracy  ; however , these methods are usually too slow to apply to largescale text analysis  . In this paper , we extend a Basket Mining algorithm to convert a kernelbased classifier into a simple and fast linear classifier  . Experimental results on English Base NP Chunking , Japanese
Word Segmentation and Japanese Dependency Parsing show that our new classifiers are about  30 to 300 times faster than the standard kernelbased classifiers  . 
1 Introduction
Kernel methods ( e . g . , Support Vector Machines ( Vapnik ,  1995 ) ) attract a great deal of attention recently . In the field of Natural Language Processing , many successes have been reported . Examples include Part-of-Speech tagging ( Nakagawa et al , 2002) Text Chunking ( Kudo and Matsumoto ,  2001) , Named Entity Recognition ( Isozaki and Kazawa ,  2002) , and Japanese Dependency Parsing ( Kudo and Matsumoto , 2000; Kudo and Matsumoto ,  2002) . 
It is known in NLP that combination of features contributes to a significant improvement in accuracy  . 
For instance , in the task of dependency parsing , it would be hard to confirm a correct dependency relation with only a single set of features from either a head or its modifier  . Rather , dependency relations should be determined by at least information from both of two phrases  . In previous research , feature combination has been selected manually , and the performance significantly depended on these selections  . This is not the case with kernelbased methodology  . For instance , if we use a polynomial kernel , all feature combinations are implicitly expanded without loss of generality and increasing the computational costs  . Although the mapped feature space is quite large , the maximal margin strategy ( Vapnik ,  1995 ) of SVMs gives us a good generalization performance compared to the previous manual feature selection  . This is the main reason why kernelbased learning has delivered great results to the field of 

Kernel-based text analysis shows an excellent performance in terms in accuracy  ; however , its inefficiency in actual analysis limits practical application  . For example , an SVM-based NE-chunker runs at a rate of only 85 byte/sec , while previous rule-based system can process several kilobytes per second  ( Isozaki and Kazawa ,  2002) . Such slow execution time is inadequate for Information Retrieval  , Question Answering , or Text Mining , where fast analysis of large quantities of text is indispensable  . 
This paper presents two novel methods that make the kernelbased text analyzers substantially faster  . 
These methods are applicable not only to the NLP tasks but also to general machine learning tasks where training and test examples are represented in a binary vector  . 
More specifically , we focus on a Polynomial Kernel of degreed , which can attain feature combinations that are crucial to improving the performance of tasks in NLP  . Second , we introduce two fast classification algorithms for this kernel  . One is PKI ( Polynomial Kernel Inverted) , which is an extension of Inverted Index in Information Retrieval  . The other is PKE ( Polynomial Kernel Expanded ) , where all feature combinations are explicitly expanded  . By applying PKE , we can convert a kernelbased classifier into a simple and fast liner classifier  . In order to build PKE , we extend the PrefixSpan ( Pei et al ,  2001) , an efficient Basket Mining algorithm , to enumerate effective feature combinations from a set of support examples  . 
Experiments on English Base NP Chunking,
Japanese Word Segmentation and Japanese Dependency Parsing show that PKI and PKE perform respectively  2 to 13 times and 30 to 300 times faster than standard kernelbased systems , without a discernible change in accuracy . 
2 Kernel Method and Support Vector

Suppose we have a set of training data for a binary classification problem :  ( x1 , y1) ,   .   .   .   , ( xL , yL)xj ? < N , yj?+1 , ?1 , where x j is a feature vector of the jth training sample  , and yj is the class label associated with this training sample  . The decision function of SVMs is defined by y ( x ) = sgn ( ? j?SV yj?j ? ( xj )  ? ? ( x ) + b )   ,   ( 1 ) where: ( A ) ? is a non-liner mapping function from < N to < H  ( N?H )  . ( B ) ? j , b ? < , ? j ? 0 . 
The mapping function ? should be designed such that all training examples are linearly separable in < H space  . Since H is much larger than N , it requires heavy computation to evaluate the dot products ?  ( xi )  ? ? ( x ) in an explicit form . This problem can be overcome by noticing that both construction of optimal parameter ? i  ( we will omit the details of this construction here  ) and the calculation of the decision function only require the evaluation of dot products ?  ( xi )  ?? ( x )  . This is critical , since , in some cases , the dot products can be evaluated by a simple Kernel Function : K  ( x1 , x2) = ?( x1) ? ?( x2) . Substituting kernel function into (1) , we have the following decision function . 
y(x ) = sgn(?j?SV yj?jK(xj , x ) + b )   ( 2 ) One of the advantages of kernels is that they are not limited to vectorial object x  , but that they are applicable to any kind of object representation  , just given the dot products . 
3 Polynomial Kernel of degreed
For many tasks in NLP , the training and test examples are represented in binary vectors  ; or sets , since examples in NLP are usually represented in socalled Feature Structures  . Here , we focus on such cases 1 . 
Suppose a feature set F = 1, 2, .   .   . , N and training examples X j ( j = 1, 2, .   .   . , L ), all of which are subsets of F ( i . e . , Xj?F ) . In this case , X j can be regarded as a binary vector x j = ( x j 1 , xj2 ,   .   .   .   , xjN ) where x j i=1 if i ? X j , xji = 0 otherwise . The dot product of x1 and x2 is given by x1  ?  x2  =   X1  ?  X2  . 
Definition 1 Polynomial Kernel of degreed Given sets X and Y , corresponding to binary feature vectors x and y , Polynomial Kernel of degreed
Kd(X,Y ) is given by
Kd(x , y ) = Kd(X , Y ) = (1 + X ? Y ) d , (3) where d = 1 ,  2 ,  3 ,   .   .   . .
In this paper ,   ( 3 ) will be referred to as an implicit form of the Polynomial Kernel  . 
1In the Maximum Entropy model widely applied in NLP  , we usually suppose binary feature functions fi ( Xj )  ? 0 ,  1 . This formalization is exactly same as representing an example Xj in a set kfk  ( Xj )  = 1 . 
It is known in NLP that a combination of features , a subset of feature set F in general , contributes to overall accuracy . In previous research , feature combination has been selected manually . The use of a polynomial kernel allows such feature expansion without loss of generality or an increase in computational costs  , since the Polynomial Kernel of degreed implicitly maps the original feature space F into Fd space  . ( i . e . , ?: F?Fd ) . This property is critical and some reports say that  , in NLP , the polynomial kernel outperforms the simple linear kernel  ( Kudo and Matsumoto , 2000; Isozaki and Kazawa ,  2002) . 
Here , we will give an explicit form of the Polynomial Kernel to show the mapping function ?  ( ? )  . 
Lemma 1 Explicit form of Polynomial Kernel.
The Polynomial Kernel of degreed can be rewritten as 
Kd(X , Y ) = d?r=0 cd(r ) ? Pr(X ? Y) ,   ( 4 ) where ? Pr ( X ) is a set of all subsets of X with exactly r elements in it  , ? cd ( r ) = ? dl = r ( dl ) (? r m=0 ( ?1 ) r ? m ? ml ( rm ) )  . 
ProofSee Appendix A.
cd ( r ) will be referred as a subset weight of the Polynomial Kernel of degreed  . This function gives a prior weight to the subset s  , where s = r . 
Example 1 Quadratic and Cubic Kernel
Given sets X =  a , b , c , d and Y = a , b , d , e , the Quadratic Kernel K2(X , Y ) and the Cubic Kernel K3(X , Y ) can be calculated in an implicit form as : K2 ( X , Y ) = (1 + X ? Y ) 2 = (1 + 3) 2 = 16 , K3(X , Y ) = (1 + X ? Y ) 3= (1 + 3) 3= 64 . 
Using Lemma 1 , the subset weights of the Quadratic Kernel and the Cubic Kernel can be calculated as  c2  ( 0 )  = 1 , c2(1) = 3 , c2(2) = 2 and c3(0) = 1 , c3(1) = 7 , c3(2) = 12 , c3(3) = 6 . 
In addition , subsets Pr(X?Y ) ( r = 0 ,  1 ,  2 , 3) are given as follows : P0(X?Y ) = ? , P 1 ( X ? Y ) = a , b , d , P2(X?Y ) = a , b , a , d , b , d , P3(X?Y ) = a , b , d . 
K2(X , Y ) and K3(X , Y ) can similarly be calculated in an explicit form as : function PKI classify  ( X ) r = 0  #an array , initialized as 0 for each i ? X for each j ? h ( i ) rj = rj + 1 end end result = 0 for each j ? SV result = result + y j ? j ? ( 1 + rj ) dend return sgn ( result + b ) end
Figure 1: Pseudocode for PKI
K2(X , Y ) = 1 ? 1 + 3 ? 3 + 2 ?3 = 16,
K3(X,Y ) = 1?1 + 7 ?3 + 12 ?3 + 6?1 = 64.
4 Fast Classifiers for Polynomial Kernel In this section  , we introduce two fast classification algorithms for the Polynomial Kernel of degreed  . 
Before describing them , we give the baseline classifier ( PKB ) : y ( X ) = sgn ( ? j?SV yj?j ? ( 1 + Xj ? X ) d + b )   . (5) The complexity of PKB is O(X ? SV ) , since it takes O ( X ) to calculate ( 1 + Xj ? X ) d and there are a total of SV support examples . 
4.1 PKI ( Inverted Representation )
Given an item i ? F , if we know in advance the set of support examples which contain item i ? F  , we do not need to calculate Xj ? X for all support examples  . This is a naive extension of Inverted Indexing in Information Retrieval  . Figure 1 shows the pseudo code of the algorithm PKI . The function h ( i ) is a precompiled table and returns a set of support examples which contain item i  . 
The complexity of the PKI is O(X ? B+SV ) , where B is an average of h ( i ) over all item i ? F . 
The PKI can make the classification speed drastically faster when B is small  , in other words , when feature space is relatively sparse ( i . e . , B?SV ) . 
The feature space is often sparse in many tasks in NLP  , since lexical entries are used as features . 
The algorithm PKI does not change the final accuracy of the classification  . 
4 . 2 PKE ( Expanded Representation ) 4 . 2 . 1 Basic Idea of PKE Using Lemma 1 , we can represent the decision function ( 5 ) in an explicit form : y ( X ) = sgn ( ? j?SV yj?j ( d ? r=0 cd ( r ) ? Pr ( Xj ? X )   ) + b )   .  (6)
If we , in advance , calculate w ( s ) = ? j?SV yj ? jcd ( s ) I ( s ? P s ( Xj ) )  ( where I ( t ) is an indicator function 2 ) for all subsets s ? ? dr = 0 Pr ( F )  ,   ( 6 ) can be written as the following simple linear form : y  ( X ) = sgn ( ? s ? ? d ( X ) w ( s ) + b )   . (7) where ? d(X ) = ? dr = 0Pr(X ) . 
The classification algorithm given by ( 7 ) will be referred to as PKE . The complexity of PKE is O(?d(X )) = O(X d) , independent on the number of support examples SV  . 
4.2.2 Mining Approach to PKE
To apply the PKE , we first calculate ? d ( F ) degree of vectors w = ( w ( s1 )  , w(s2) ,   .   .   . , w(s ? d(F ))) . 
This calculation is trivial only when we use a Quadratic Kernel  , since we just project the original feature space F into F ? F space  , which is small enough to be calculated by a naive exhaustive method  . However , if we , for instance , use a polynomial kernel of degree 3 or higher , this calculation becomes not trivial , since the size of feature space exponentially increases  . Here we take the following strategy : 1 . Instead of using the original vector w , we use w ? , an approximation of w . 
2 . We apply the Subset Mining algorithm to calculate w ? efficiently  . 
2I(t ) returns 1 if tistrue , returns 0 otherwise.
Definition 2 w ? : An approximation of w
An approximation of w is given by w ? = ( w?(s1) , w?(s2) ,   .   .   .   , w?(s ? d(F ))) , where w ? ( s ) is set to 0 if w ( s ) is trivially close to 0 . ( i . e . , ? neg < w(s ) < ? pos (? neg < 0 , ? pos > 0) , where ? pos and ? neg are predefined thresholds ) . 
The algorithm PKE is an approximation of the PKB , and changes the final accuracy according to the selection of thresholds ? pos and ? neg  . The calculation of w ? is formulated as the following mining problem : Definition  3 Feature Combination Mining Given a set of support examples and subset weight cd  ( r )  , extract all subsets s and their weights w ( s ) if w ( s ) holds w ( s ) ? ? pos or w ( s ) ? ? neg . 
In this paper , we apply a Sub-Structure Mining algorithm to the feature combination mining problem  . Generally speaking , substructures mining algorithms efficiently extract frequent substructures  ( e . g . , subsets , subsequences , subtrees , or subgraphs ) from a large database ( set of transactions )  . 
In this context , frequent means that there are no less than ? transactions which contain a substructure  . 
The parameter ? is usually referred to as the Minimum Support  . Since we must enumerate all subsets of F , we can apply subset mining algorithm , in sometimes called as Basket Mining algorithm , to our task . 
There are many subset mining algorithms proposed , however , we will focus on the PrefixSpan algorithm , which is an efficient algorithm for sequential pattern mining  , originally proposed by ( Pei et al . , 2001) . The Prefix Span was originally designed to extract frequent subsequence  ( not subset ) patterns , however , it is a trivial difference since a set can be seen as a special case of sequences  ( i . e . , by sorting items in a set by lexicographic order  , the set becomes a sequence ) . The basic idea of the PrefixS-pan is to divide the database by frequent subpatterns  ( prefix ) and to grow the prefix-spanning pattern in a depth-first search fashion  . 
We now modify the Prefix Spantosuit to our feature combination mining  . 
? size constraint
We only enumerate up to subsets of sized.
when we plan to apply the Polynomial Kernel of degreed  . 
? Subset weight cd(r )
In the original Prefix Span , the frequency of each subset does not change by its size  . However , in our mining task , it changes ( i . e . , the frequency of subsets is weighted by cd(s)) . 
Here , we process the mining algorithm by assuming that each transaction  ( support example X j ) has its frequency Cd yj?j , where Cd = max(cd(1) , cd(2) ,   .   .   . , cd(d )) . The weight w ( s ) is calculated by w ( s )  = ? ( s ) ? cd ( s ) /Cd , where ?( s ) is a frequency of s , given by the original Prefix Span . 
? Positive/Negative support examples
We first divide the support examples into positive  ( yi > 0 ) and negative ( yi < 0 ) examples , and process mining independently . The result can be obtained by merging these two results  . 
? Minimum Supports ? pos , ? neg
In the original Prefix Span , minimum support is an integer . In our mining task , we can give a real number to minimum support , since each transaction ( support example X j ) has possibly non-integer frequency Cdyj?j . Minimum supports ? pos and ? neg control the rate of approximation  . For the sake of convenience , we just give one parameter ? , and calculate ? pos and ? neg as follows ? pos = ? ?   (  #of positive examples  #of support examples )   , ? neg = ??? (  #of negative examples  #of support examples )   . 
After the process of mining , a set of tuples ? = ? s , w(s ) ? is obtained , where s is a frequent subset and w ( s ) is its weight . We use a TRIE to efficiently store the set ? . The example of such TRIE compression is shown in Figure  2  . Although there are many implementations for TRIE , we use a Double-Array ( Aoe , 1989) in our task . The actual classification of PKE can be examined by traversing the TRIE for all subsets s ? ? d  ( X )  . 
5 Experiments
To demonstrate performances of PKI and PKE , we examined three NLP tasks : English Base NP Chunking  ( EBC )  , Japanese Word Segmentation ( JWS ) and                                     fffiflffiffiflflfl!# "$"&% '  ( # )  * '+ , '-+  . 




'-+ s
Japanese Dependency Parsing ( JDP) . A more detailed description of each task , training and test data , the system parameters , and feature sets are presented in the following subsections  . Table 1 summarizes the detail information of support examples  ( e . g . , size of SVs , size of feature set etc . ) . 
Our preliminary experiments show that a
Quadratic Kernel performs the best in EBC , and a Cubic Kernel performs the best in JWS and JDP  . 
The experiments using a Cubic Kernel are suitable to evaluate the effectiveness of the basket mining approach applied in the PKE  , since a Cubic Kernel projects the original feature space F into F  3 space , which is too large to be handled only using a naive exhaustive method  . 
All experiments were conducted under Linux using XEON  2  . 4 Ghz dual processors and 3 . 5G byte of main memory . All systems are implemented in C++ . 
5.1 English Base NP Chunking ( EBC)
Text Chunking is a fundamental task in NLP ? dividing sentences into nonoverlapping phrases  . Base NP chunking deals with a part of this task and recognizes the chunks that form noun phrases  . Here is an example sentence : [ He ] reckons [ the current account deficit ] will narrow to [ only $  1  . 8 billion ] . 
A Base NP chunk is represented as sequence of words between square brackets  . Base NP chunking task is usually formulated as a simple tagging task  , where we represent chunks with three types of tags : B : beginning of a chunk  . I : non-initial word . O : outside of the chunk . In our experiments , we used the same settings as ( Kudo and Matsumoto ,  2002) . 
We use a standard dataset ( Ramshaw and Marcus ,  1995 ) consisting of sections 1519 of the WSJ corpus as training and section 20 as testing . 
5.2 Japanese Word Segmentation ( JWS)
Since there are no explicit spaces between words in Japanese sentences  , we must first identify the word boundaries before analyzing deep structure of a sentence  . Japanese word segmentation is formalized as a simple classification task  . 
Let s = c1c2 ??? cm be a sequence of Japanese characters , t = t1t2 ??? tm be a sequence of Japanese character types  3 associated with each character , and yi?+1 , ?1 , ( i = (1 ,  2 ,   .   .   . , m ? 1)) be a boundary marker . If there is a boundary between ci and ci+1 , yi = 1 , otherwise yi = ?1 . The feature set of example x i is given by all characters as well as character types in some constant window  ( e . g . , 5):  ci?2 , ci?1 ,  ? ? ?  , ci+2 , ci+3 , ti?2 , ti?1 ,  ? ? ?  , ti+2 , ti+3 . 
Note that we distinguish the relative position of each character and character type  . We use the Kyoto University Corpus ( Kurohashi and Nagao ,  1997) ,  7 , 9 58 sentences in the articles on January 1st to January 7th are used as training data , and 1 , 2 46 sentences in the articles on January 9th are used as the test data . 
5.3 Japanese Dependency Parsing ( JDP)
The task of Japanese dependency parsing is to identify a correct dependency of each Bunsetsu  ( base phrase in Japanese )  . In previous research , we presented a state-of-the-art SVMs-based Japanese dependency parser  ( Kudo and Matsumoto ,  2002) . We combined SVMs into an efficient parsing algorithm  , Cascaded Chunking Model , which parses a sentence deterministically only by deciding whether the current chunk modifies the chunk on its immediate rate right hand side  . The input for this algorithm consists of a set of the linguistic features related to the head and modifier  ( e . g . , word , part-of-speech , and inflections ) , and the output from the algorithm is either of the value  +1   ( dependent ) or -1 ( independent )  . We use a standard dataset , which is the same corpus described in the Japanese Word Segmentation  . 
3Usually , in Japanese , word boundaries are highly constrained by character types  , such as hiragana and katakana ( both are phonetic characters in Japanese )  , Chinese characters , 
English alphabets and numbers.
5.4 Results
Tables 2 , 3 and 4 show the execution time , accu-racy 4 , and ? ( size of extracted subsets ) , by changing ? from 0 . 01 to 0 . 0005 . 
The PKI leads to about 2 to 12 times improvements over the PKB . In JDP , the improvement is significant . This is because B , the average of h(i ) over all items i ? F , is relatively small in JDP . The improvement significantly depends on the sparsity of the given support examples  . 
The improvements of the PKE are more significant than the PKI  . The running time of the PKE is 30 to 300 times faster than the PKB , when we set an appropriate ? , ( e . g . , ? = 0 . 005 for EBC and JWS , ? = 0 . 0005 for JDP ) . In these settings , we could preserve the final accuracies for test data  . 
5.5 Frequency-based Pruning
The PKE with a Cubic Kernel tends to make ? large ( e . g . , ? = 2 . 32 million for JWS , ? = 8 . 26 million for JDP ) . 
To reduce the size of ? , we examined simple frequency-based pruning experiments  . Our extension is to simply give a prior threshold  ?  ( = 1 ,  2 ,  3 ,  4  .   .   . ) , and erase all subsets which occur in less than ? support examples  . The calculation of frequency can be similarly conducted by the PrefixS-pan algorithm  . Tables 5 and 6 show the results of frequency-based pruning , when we fix ? = 0 . 005 for
JWS , and ? = 0.0005 for JDP.
In JDP , we can make the size of set ? about one third of the original size  . This reduction gives us not only a slight speed increase but an improvement of accuracy  ( 89 . 29%?89 . 34%) . Frequency-based pruning allows us to remove subsets that have large weight and small frequency  . Such subsets may be generated from errors or special outliers in the training examples  , which sometimes cause an overfitting in training . 
In JWS , the frequency-based pruning does not work well . Although we can reduce the size of ? by half , the accuracy is also reduced (97 . 94%?97 . 83%) . It implies that , in JWS , features even with frequency of one contribute to the final decision hyperplane  . 
4In EBC , accuracy is evaluated using Fmeasure , harmonic mean between precision and recall . 
Table 1: Details of Data Set
Data Set EBCJ WSJ DP  #of examples 135 , 692 265 , 413 110 , 355 SV  #of SVs 11 , 690 57 , 672 34 , 996  #of positive SVs 5 , 637 28 , 440 17 , 528  #of negative SVs 6 , 053 29 , 232 17 , 468 F ( size of feature ) 17 , 470 11 , 643 28 , 157
Avg . of Xj 11.90 11.73 17.63
B ( Avg . of h(i ))) 7 . 74 58 . 13 21 . 92 ( Note : In EBC , to handle K-class problems , we use a pairwise classification ; building K ? ( K?1 ) /2 classifiers considering all pairs of classes , and final class decision was given by majority voting  . The values in this column are averages over all pairwise classifiers  . )  6 Discussion There have been several studies for efficient classification of SVMs  . Isozaki et al propose an XQK ( eXp and the Quadratic Kernel ) which can make their Named Entity recognizer drastically fast  ( Isozaki and Kazawa ,  2002) . XQK can be subsumed into PKE . Both XQK and PKE share the basic idea ; all feature combinations are explicitly expanded and we convert the kernelbased classifier into a simple linear classifier  . 
The explicit difference between XQK and PKE is that XQK is designed only for Quadratic Kernel  . It implies that XQK can only deal with feature combination of size up to two  . On the other hand , PKE is more general and can also be applied not only to the Quadratic Kernel but also to the general-style of polynomial kernels  ( 1 + X ? Y ) d . In PKE , there are no theoretical constrains to limit the size of combinations  . 
In addition , Isozaki et aldid not mention how to expand the feature combinations  . They seem to use a naive exhaustive method to expand them  , which is not always scalable and efficient for extracting three or more feature combinations  . PKE takes a basket mining approach to enumerating effective feature combinations more efficiently than their exhaustive method  . 
7 Conclusion and Future Works
We focused on a Polynomial Kernel of degreed , which has been widely applied in many tasks in NLP 
Table 2: Results of EBC
PKE Time Speedup F1??(sec . /sent . ) Ratio (? 1000) 0 . 01 0 . 0016 105 . 2 93 . 79 518 0 . 005 0 . 0016 101 . 3 93 . 85 668 0 . 001 0 . 0017 97 . 7 93 . 84 858 0 . 0005 0 . 0017 96 . 8 93 . 84 889
PKI 0.020 8.39 3.84
PKB 0.164 1.09 3.84
Table 3: Results of JWS
PKE Time Speedup Acc . (%) ?? ( sec . /sent . ) Ratio (? 1000) 0 . 01 0 . 0024 358 . 2 97 . 93 1,228 0 . 005 0 . 0028 300 . 1 97 . 95 2,327 0 . 001 0 . 0034 242 . 6 97 . 94 4,392 0 . 0005 0 . 0035 238 . 8 97 . 94 4,820
PKI 0.4989 1.79 7.94
PKB 0.8535 1.09 7.94
Table 4: Results of JDP
PKE Time Speedup Acc . (%) ?? ( sec . /sent . ) Ratio (? 1000) 0 . 01 0 . 0042 66 . 8 88 . 91 73 0 . 005 0 . 0060 47 . 8 89 . 05 1,924 0 . 001 0 . 0086 33 . 3 89 . 26 6,686 0 . 0005 0 . 0090 31 . 8 89 . 29 8,262
PKI 0.0226 12.68 9.29
PKB 0.2848 1.08 9.29
Table 5: Frequency-based pruning ( JWS)
PKE time Speedup Acc . (%) ?? ( sec . /sent . ) Ratio(?1000) 10 . 0028 300 . 1 97 . 95 2,327 2 0 . 0025 337 . 3 97 . 83 954 3 0 . 0023 367 . 0 97 . 83 591
PKB 0.8535 1.09 7.94
Table 6: Frequency-based pruning ( JDP )
PKE time Speedup Acc . (%) ?? ( sec . /sent . ) Ratio(?1000) 10 . 0090 31 . 8 89 . 29 8,262 2 0 . 0072 39 . 3 89 . 34 2,450 3 0 . 0068 41 . 8 89 . 31 1,360
PKB 0 . 2848 1 . 0 89 . 2 9 and can attain feature combination that is crucial to improving the performance of tasks in NLP  . Then , we introduced two fast classification algorithms for this kernel  . One is PKI ( Polynomial Kernel Inverted) , which is an extension of Inverted Index . The other is PKE ( Polynomial Kernel Expanded ) , where all feature combinations are explicitly expanded  . 
The concept in PKE can also be applicable to kernels for discrete data structures  , such as String Kernel ( Lodhi et al , 2002) and Tree Kernel ( Kashima and Koyanagi , 2002; Collins and Duffy ,  2001) . 
For instance , Tree Kernel gives a dot product of an ordered -tree  , and maps the original ordered-tree onto its all subtree space  . To apply the PKE , we must efficiently enumerate the effective subtrees from a set of support examples  . We can similarly apply a subtree mining algorithm  ( Zaki , 2002) to this problem . 
Appendix A . : Lemma 1 and its proof cd ( r ) = d?l = r ( dl ) ( r ? m=0 ( ?1 ) r ? m ? ml ( rm ) )  . 

Let X , Y be subsets of F = 1, 2, .   .   . , N . In this case , X ? Y is same as the dot product of vector x , y , where x = x1 , x2 ,   .   .   . , xN , y = y1, y2, .   .   .   , yN(xj , yj ? 0 , 1) x j=1 if j ? X , x j = 0 otherwise . 
(1 +  X ? Y ) d = ( 1 + x ? y ) d can be expanded as follows ( 1 + x ? y ) d = d ? l = 0 ( dl ) ( N ? j=1 xjyj ) l = d ? l = 0 ( dl )  ? ? ( l ) where ? ( l ) = k1 + . . . + kN = l ? kn ? 0 l!k1! .   .   . kN !( x1 y1) k1 .   .   . ( xNyN)kN . 
Note that x k j j is binary ( i . e . , x kj j ? 0 ,  1) , the number of r-size subsets can be given by a coefficient of  ( x1 y1 x2 y2 .   .   . xryr ) . Thus , cd(r ) = d?l = r(dl ) ( k1 + . . . + kr = l ? kn?1, n = 1, 2, . . . , rl!k1! .   .   . kr ! ) = d?l = r ( dl ) ( rl ? ( r ( r ? 1 ) l + ( r ( r ? 2 ) l ? .   .   . 
) = d ? l=r ( dl ) ( r ? m=0 ( ?1 ) r ? m ? ml ( rm ) )  .  2

Junichi Aoe .  1989 . An efficient digital search algorithm by using a double-array structure  . IEEE Transactions on Software
Engineering , 15(9).
Michael Collins and Nigel Duffy .  2001 . Convolution kernels for natural language . In Advances in Neural Information Processing Systems  14  , Vol . 1 ( NIPS 2001), pages 625?632 . 
Hideki Isozaki and Hideto Kazawa .  2002 . Efficient support vector classifiers for named entity recognition  . In Proceedings of the COLING 2002, pages 390?396 . 
Hisashi Kashima and Teruo Koyanagi .  2002 . Svm kernels for semistructured data . In Proceedings of the ICML-2002, pages 291?298 . 
Taku Kudo and Yuji Matsumoto .  2000 . Japanese Dependency Structure Analysis based on Support Vector Machines  . In Proceedings of the EMNLP/VLC-2000, pages 18?25 . 
Taku Kudo and Yuji Matsumoto .  2001 . Chunking with support vector machines . In Proceedings of the the NAACL , pages 192?199 . 
Taku Kudo and Yuji Matsumoto .  2002 . Japanese dependency analyis is using cascaded chunking  . In Proceedings of the
CoNLL 2002, pages 63?69.
Sadao Kurohashi and Makoto Nagao .  1997 . Kyoto University text corpus project . In Proceedings of the ANLP 1997, pages 115?118 . 
Huma Lodhi , Craig Saunders , John ShaweTaylor , Nello Cristianini , and Chris Watkins .  2002 . Text classification using string kernels . Journal of Machine Learning Research , 2 . 
Tetsuji Nakagawa , Taku Kudo , and Yuji Matsumoto .  2002 . Revision learning and its application to part-of -speech tagging  . 
In Proceedings of the ACL 2002, pages 497?504.
Jian Pei , Jiawei Han , and et al 2001 . Prefix span : Mining sequential patterns by prefix -projected growth  . In Proc . of International Conference of Data Engineering , pages 215?224 . 
Lance A . Ramshaw and Mitchell P . Marcus .  1995 . Text chunking using transformationbased learning . In Proceedings of the VLC , pages 88?94 . 
Vladimir N . Vapnik .  1995 . The Nature of Statistical Learning
Theory . Springer.
Mohammed Zaki .  2002 . Efficiently mining frequent trees in a forest . In Proceedings of the 8th International Conference on Knowledge Discovery and Data Mining KDD  , pages 71?80 . 
