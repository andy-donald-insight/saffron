PART-OF-SPEECHTAGGINGUSING
A VARIABLE MEMORY MARK OVMODEL
Hinrich Schiitze
Center for the Study of
Language and Information
Stanford , CA 94305-4115
Internet : schuetze ~ csli.stanford.edu
Yoram Singer
Institute of Computer Science and
Center for Neural Computation
Hebrew University , Jerusalem 91904
Internet : singer@cs.huji.ac.il
Abstract
We present a new approach to disambiguating syntactically ambiguous words in context  , based on Variable Memory Markov ( VMM ) models . In contrast to fixed-length Markov models , which predict based on fixed-lengthistories , variable memory Markov models dynamically adapt their history length based on the training data  , and hence may use fewer parameters . In a test of a VMM based tagger on the Brown corpus  ,  95 . 81% of tokens are correctly classified . 

Many words in English have several parts of speech  ( POS )  . For example " book " is used as a noun in " She read a book  . " and as a verb in " She didn't book atrip . " Part-of-speech tagging is the problem of determining the syntactic part of speech of an occurrence of a word in context  . In any given English text , most tokens are syntactically ambiguou since most of the high-frequency English words have several parts of speech  . Therefore , a correct syntactic lassification of words in context is important for most syntactic and other higher-level processing of natural language text  . 
Two stochastic methods have been widely used for POS tagging : fixed order Markov models and Bidden Markov models  . Fixed order Markov models are used in ( Church , 1989) and ( Charniak et al ,  1993) . Since the order of the model is assumed to be fixed  , a short memory ( small order ) is typically used , since the number of possible combinations grows exponentially  . For example , assuming there are 184 differentags , as in the Brown corpus , there are 1843 = 6 , 229 , 504 different order 3 combinations of tags ( of course not all of these will actually occur , see ( Weischedel et al ,  1993)) . Because of the large number of parameters higher -order fixed length models are hard to estimate  . ( See ( Brill ,  1993 ) for a rule-based approach to incorporating higher -order information  . ) In a Hiddeniarkov Model ( HMM ) ( Jelinek , 1985; Kupiec ,  1992) , a different state is defined for each POS tag and the transition probabilities and the output probabilities are estimated using the EM  ( Dempster et al , 1977) algorithm , which guarantees convergence to . a local minimum ( Wu , 1983) . The advantage of an HMM is that it can be trained using untagged text  . On the other hand , the training procedure is time consuming , and a fixed model ( topology ) is assumed . Another disadvantage is due to the local convergence properties of the EM algorithm  . The solution obtained depends on the initial setting of the model's parameters  , and different solutions are obtained for different parameter initialization schemes  . This phenomenon discourages linguistic analysis based on the output of the model  . 
We present a new method based on variable memory Markov models  ( VMM )   ( Ron et al , 1993; Ron et al ,  1994) . The VMM is an approximation of an unlimited order Markov source  . It can incorporate both the static ( order 0 ) and dynamic ( higher-order ) information systematically , while keeping the ability to change the model due to future observations  . This approach is easy to implement , he learning algorithm and classification of new tags are computationally efficient  , and the results achieved , using simplified assumptions for the static tag probabilities  , are encouraging . 
VARIABLE MEMORY MARKOV

Markov models are a natural candidate for language modeling and temporal pattern recognition  , mostly due to their mathematical simplicity . However , it is obvious that finite memory Markov models cannot capture the recursive nature of language  , nor can they be trained effectively with long memories  . The notion of variable contez ~ length also appears naturally in the context of universal coding  ( Rissanen , 1978; Rissanen and Langdon ,  1981) . This information theoretic notion is now known to be closely related to efficient modeling  ( Rissanen ,  1988) . The natural measure that length , as measured by the statistical predictability via the KullbackLeibler  ( KL ) divergence . 
The VMM learning algorithm is based on minimizing the statistical prediction error of a Markov model  , measured by the instantaneous KL divergence of the following symbols  , the current statistical surprise of the model . The memory is extended precisely when such a surprise is significant  , until the overall statistical prediction of the stochastic model is sufficiently good  . For the sake of simplicity , a POS tag is termed a symbol and a sequence of tags is called a string  . We now briefly describe the algorithm for learning a variable memory Markov model  . See ( Ron et al , 1993; Ron et al . , 1994 ) for a more detailed description of the algorithm . 
We first introduce notational conventions and define some basic concepts  . Let \] E be a finite alphabet . Denote by \] ~* the set of all strings over \] E . A string s , over LTM of length n , is denoted by s = sls2 .   .   . sn . We denote by ? the empty string . The length of a string s is denoted by Isl and the size of an alphabet \]~ is denoted by  \[\]~1  . Let Prefix(s ) = SlS2 .   .   . Sn_l denote the longest prefix of a strings , and let Prefix * ( s ) denote the set of all prefixes of s , including the empty string . Similarly , Suffix(s ) = s2sz .   .   . s , and Suffix * ( s ) is the set of all suffixes of s . A set of strings is called a suffix ( prefix ) freeset if , VsES : SNSuffiz * ( s )  = $  ( SNP refiz * ( s )  = 0 )  . 
We call a probability measure P , over the strings in E * proper if P ( o ) = 1 , and for every strings , Y  ~ , erP(sa ) = P ( s ) . Hence , for every prefix freeset S ,  ~'~ , esP ( s ) < 1 , and specifically for every integer n > O , ~' ~ se ~ , P ( s ) = 1 . 
A prediction suffix tree T over \] E , is a tree of degree I~l . The edges of the tree are labeled by symbols from ~ E  , such that from every internal node there is at most one outgoing edge labeled by each symbol  . The nodes of the tree are labeled by pairs ( s , % ) where s is the string associated with the walk starting from that node and ending in the root of the tree  , and 7s : ~---*\[0 , 1\] is the output probability function of s satisfying  ) "\] ~ o ~7 s ( a )  = 1 . A . prediction suffix , tree . induces probabilities on arbitrarily long strings m the following manner  . The probability that T generates a string w = wtw2  .   .   . wninE ~, denoted by PT(w ), is IIn = l % . i- , ( Wi ) , where so = e , and for 1<i<n-1 , sJ  is the string labeling the deepest node reached by taking the walk corresponding to wl  .   .   . wistarting at the root of T . By definition , a prediction suffix tree induces a proper measure over E *  , and hence for every prefix free set of strings wX  ,  . . .   , wm , ~ = ~ PT(wi ) < 1 , and specifically for n > 1 , then ~ , E  ~ , PT(S ) = 1 . 
A Probabilistic Finite Automaton ( PFA ) A is a 5-tuple ( Q , E , r ,  7 ,  ~) , where Q is a finite set of n states , ~ is an alphabet of size k , v : QxE - - ~ Q is the transition function , 7: Q?E~\[0 , 1\] is the output probability function , and ~ r:Q~\[0 , 1\] is the probability distribution over the start states  . 
The functions 3' and r must satisfy the following requirements : for every qEQ  , ) -' ~ oe ~7( q , a ) = 1 , and ~ e ~ Orr(q ) = 1 . The probability that A generates a string s = s l  s2   .   .   . s . EEn 0n is PA ( s ) = ~- ~ q o E q l r ( q ) I-Ii = x7 ( qi1 , sl ) , where qi + l~_r(qi , si ) .   7" can be extended to be defined on QxE * as follows:  7"  ( q , sts2 .   .   . s t ) = 7" (7"( q , st .   .   . s t-x ) , s t ) = 7" (7"( q , Prefiz(s )) , s t ) . 
The distribution over the states , 7r , can be replaced by a single start state , denoted by e such that r (? , s ) = 7 r(q ) , where s is the label of the state q . Therefore , r(e ) = 1 and r(q ) = 0 if q#e . 
For POS tagging , we are interested in learning a subclass of finite state machines which have the following property  . Each state in a machine M belonging to this subclass is labeled by a string of length at most Lover E  , for some L_>O . The set of strings labeling the states is suffix free  . We require that for every two states qX , q2EQ and for every symbol aE ~ , if r(q1 , or ) = q2 and q t is labeled by a string s1 , then q2 is labeled by a string s ~ which is a suffix of s  1 ? or . Since the set of strings labeling the states is suffix free  , if there exists a string having this property then it is unique  . Thus , in order that r be well defined on a given set of string S  , not only must the set be suffix free , but it must also have the property , that for every strings in the set and every symbol a  , there exists a string which is a suffix of scr . For our convenience , from this point on , if q is a state in Q then q will also denote the string labeling that state  . 
A special case of these automata is the case in which Q includes all I~lL strings of length L  . 
These automatare known as Markov processes of order L  . We are interested in learning automata for which the number of states  , n , is much smaller than IEIL , which means that few states have long memory and most states have a short one  . We refer to these automatas variable memory Markov  ( VMM ) processes . In the case of Markov processes of order L , the identity of the states ( i . e . the identity of the strings labeling the states ) is known and learning such a process reduces to approximating the output probability function  . 
Given a sample consisting of mPOS tag sequences of lengths Ix  , 12 ,  . . . , l , ~ we would like to find a prediction suffix tree that will have the same statistical properties as the sample and thus can be used to predict the next outcome for se-c  ; uences generated by the same source . At each memory Markov process . The key idea is to iteratively build a prediction tree whose probability measure quals the empirical probability measure calculated from the sample  . 
We start with a tree consisting of a single node and add nodes which we have reason to believe should be in the tree  . A node as , must be added to the tree if it statistically differs from its parent nodes  . A natural measure to check the statistical difference is the relativentropy  ( also known as the KullbackLeibler ( KL ) divergence )   ( Kullback ,  1959) , between the conditional probabilities P ( . Is ) and P ( . las ) . Let X be an observation space and P1 ,   P2 be probability measures over X then the KL divergence between  P1 and
P1 xP2 is , DL(PIl IP ) = ? In our case , the KL divergence measures how much additional information is gained by using the suffix ~ rs for prediction instead of the shorter suffixs  . 
There are cases where the statistical difference is large yet the probability of observing the suffix as itself is so small that we can neglect those cases  . 
Hence we weigh the statistical error by the prior probability of observing as  . The statistical error measure in our case is,
Err(as , s ) = P ( crs ) Dg L(P ( . las)llP( . ls )) = P ( as)P ( a'l as ) log : ~ , 0 , ~ P ( asa ') log p(P/s ?; p'() Therefore , a node as is added to the tree if the statistical difference  ( defined by Err ( as , s ) ) between the node and its parrent s is larger than a predetermined accuracy e  . The tree is grown level by level , adding a son of a given leaf in the tree whenever the statistical error is large  . The problem is that the requirement that a node statistically differs from its parent node is a necessary condition for belonging to the tree  , but is not sufficient . The leaves of a prediction suffix tree must differ from their parents  ( or they are redundant ) but internal nodes might not have this property . Therefore , we must continue testing further potential descendants of the leaves in the tree up to depth L  . In order to avoid exponential grow in the number of strings tested  , we do not test strings which belong to branches which are reached with small probability  . The set of strings , tested at each step , is denoted by S , and can be viewed as a kind of frontier of the growing tree T  . 
USING AVMM FOR POS

We used a tagged corpus to train a VMM . The syntactic information , i . e . the probability of a spe-using maximum likelihood estimation from the individual word counts  . The states and the transition probabilities of the Markov model were determined by the learning algorithm and tag output probabilities were estimated from word counts  ( the static information present in the training corpus  )  . The whole structure , for two states , is depicted in Fig .  1 . Si and Si+lare strings of tags corresponding to states of the automaton  . P ( ti\[Si ) is the probability that tag ti will be output by state Si and P  ( ti+l\]Si + l ) is the probability that the next tag ti+l is the output of state Si + l  . 

V7
P ( TilSi)P ( Ti + IlSi+I)
Figure 1: The structure of the VMM based POS tagger . 
When tagging a sequence of words W l , , , we want to find the tag sequence t l , n that is most likely for W l , n . We can maximize the joint probability of wl , , and tl , n to find this sequence : 1
T ( W l , n ) = argmax t ,, P ( tl , nl Wl , n)
P ( t , . .,~,, . ) = argmax t ~ , . P ( w l , . ) = argmax t ~ , . P(tl, . , wl , . ) P ( tl , . , Wl , . ) can be expressed as a product of conditional probabilities as follows: 
P ( tl , ., Wl , .) =
P ( ts)P ( wlItl)P ( t~ltl,wl)e(w21 tl,2, wl) . . . P ( t . It 1, . _1, W l , . -1)P ( w . It 1, . , wl , . -1) = fiP ( tiltl , i-1 , wl , i-1)P(wiltl , i , Wl , /-1 )   i=1 With the simplifying assumption that the probability of a tag only depends on previous tags and that the probability of a word only depends on its tags  , we get : P ( tl , n , wl ,  . ) = fixP ( tiltl , i-1 ) P ( wilti ) i = 1 Given a variable memory Markov model M , P ( tilQ , i-1) is estimated by P ( tilSi-l , M ) where 1 Part of the following derivation is adapted from ( Charniak et al ,  1993) . 
Si = r(e , tx , i ) , since the dynamics of the sequence are represented by the transition probabilities of the corresponding automaton  . The tags t l , n for a sequence of words w t , n are therefore chosen according to the following equation using the Viterbi algorithm : t %  7-M  ( Wl , n ) -- argmax q . . H P ( tilSi-l'M ) P ( wilti ) i = 1 We estimate P ( wilti ) indirectly from P ( t i l w i ) using Bayes ' Theorem :
P ( wilti ) = P ( wi)P ( tilwi)

The terms P ( wi ) are constant for a given sequence wi and can therefore be omitted from the maximization  . We perform a maximum likelihood estimation for P ( ti ) by calculating the relative frequency of ti in the training corpus  . The estimation of the static parameters P ( t i l w i ) is described in the next section . 
We trained the variable memory Markov model on the Brown corpus  ( Francis and Kujera ,  1982) , with every tenth sentence removed ( a total of 1 , 022 , 462 tags ) . The four stylistic tag modifiers " FW " ( foreign word )  , " TL " ( title) , " NC " ( cited word) , and " HL " ( headline ) were ignored reducing the complete set of 471 tags to 184 different tags . 
The resulting automaton has 49 states : the null state ( e )  , 43 first order states ( one symbol long ) and 5 second order states ( two symbols long )  . This means that 184-43=141 states were not ( statistically ) different enough to be included as separate states in the automaton  . An analysis reveals two possible reasons . Frequent symbols such as " ABN "(" half " , " all " , " many " used as pre-quantifiers , e . g . in " many a younger man " ) and " DTI " ( determiners that can be singular or plural , " any " and " some " ) were not included because they occur in a variety of diverse contexts or often precede unambiguous words  . For example , when tagged as " ABN . . . . half " , " all " , and " many " tend to occur before the unambiguous determiners " a "  , " an " and " the " . 
Some rare tags were not included because they did not improve the optimization criterion  , minimum description length ( measured by the KL-divergence )  . For example , " HVZ *" ( " hasn't " ) is not a state although a following "- ed " form is always disambiguated as belonging to class " VBN "  ( past participle )  . But since this is a rare event , describing all " HVZ*VBN " sequences separately is cheaper than the added complexity of an automaton with state " HVZ *"  . We in fact lost some accuracy in tagging because of the optimization criterion : Several "- ed " forms after forms of " have " were mistagged as " VBD "  ( past tense )  . 
transition to one-symbol two-symbol state state
NNJJ : 0.45 ATJJ : 0.69
INJJ : 0.06 ATJJ : 0.004
INNN : 0.27 ATNN : 0.35
NN : 0.14 ATNN : 0.10






VBN : 0.08 AT VBN : 0.48
VBN : 0.35 ATVB N : 0.003
CC : 0.12 JJ CC : 0.04
CC : 0.09 JJ CC : 0.58
RB : 0.05 MDRB : 0.48
RB : 0.08 MDRB : 0.0009
Table 1: States for which the statistical prediction is significantly different when using a longer suffix for prediction  . Those states are identified automatically b the VMM learning algorithm  . A better prediction and classification of POS-tags is achieved by adding those states with only a small increase in the computation time  . 
The two-symbol states were " ATJJ "," AT
NN " , " ATVBN " , " JJCC " , and " MDRB " ( article adjective , article noun , article past participle , adjective conjunction , modal adverb ) . Table 1 lists two of the largest differences in transition probabilities for each state  . The varying transition probabilities are based on differences between the syntactic onstructions in which the two competing states occur  . For example , adjectives after articles ( " ATJJ " ) are almost always used attributively which makes a following preposition impossible and a following noun highly probable  , whereas a predicative use favors modifying prepositional phrases  . Similarly , an adverb preceded by a modal ( " MDRB " ) is followed by an infinitive ( " VB " ) half the time , whereas other adverbs occur less often in pre -infinitival position  . On the other hand , a past participle is virtually impossible after " MDRB " whereas adverbs that are not preceded by modals modify past participles quite often  . 
While it is known that Markov models of order 2 give a slight improvement over order1 models ( Charniak et al ,  1993) , the number of parameters in our model is much smaller than in a full  order2 Markov model ( 49"184=9016 vs .  184"184"184 --6,229,504) . 
ESTIMATION OF THESTATIC

We have to estimate the conditional probabilities P  ( ti\[wJ )  , the probability that a given word u fi will appear with tag ti  , in order to compute the static parameters P ( wjIt / ) used in the tagging equations described above . A first approximation would be p ( ti\[wj ) = C ( ti , wi)c(w ) where C(ti , wj ) is the number of times t i is tagged as w ~ in the training text and C  ( wJ ) is the number of times w / occurs in the training text  . However , some form of smoothing is necessary , since any new text will contain new words , for which C(wj ) is zero . Also , words that are rare will only occur with some of their possible parts of speech in the training text  . One solution to this problem is GoodTuring estimation : p  ( t i l w j ) _C ( t ' , wJ ) +1 c ( wJ ) + I where I is the number of tags , 184 in our case . 
It turns out that GoodTuring is not appropriate for our problem  . The reason is the distinction between closed -class and open-class words  . Some syntactic lasses like verbs and nouns are productive  , others like articles are not . As a consequence , the probability that a new word is an article is zero  , whereas it is high for verbs and nouns . We need a smoothing scheme that takes this fact into account  . 
Extending an idea in ( Charniak et al ,  1993) , we estimate the probability of tag conversion to find an adequate smoothing scheme  . Open and closed classes differ in that words often add a tag from an open class  , but rarely from a closed class . 
For example , a word that is first used as a noun will often be used as a verb subsequently  , but closed classe such as possessive pronouns ( " my " , " her " , " his " ) are rarely used with new syntactic categories after the first few thousand words of the Brown corpus  . We only have to take stock of these " tag conversions " to make informed predictions on new tags when confronted with unseen text  . Formally , let W\]''~ be the set of words that have been seen with ti  , but not with tk in the training text up to word wt  . Then we can estimate the probability that a word with tag ti will later be seen with tag t ~ as the proportion of words allowing tag ti but not tk that later add tk : 
P ~ m(i - -* k ) =
In ll < n < m^i ~ k , ~kwnEWI " OW , ,- t ^ t~=t~l iw~' . -kl This formula also applies to words we haven't seen so far  , if we regard such words as having occurred with a special tag " U " for " unseen "  . ( In this case , W ~'-' k is the set of words that haven't occurred up to l  . ) PI , n ( U---*k ) then estimates the probability that an unseen word has tag tk  . Table 2 shows the estimates of tag conversion we derived from our training text for  1  =  1022462-   100000  , m = 1022462 , where 1022462 is the number of words in the training text . To avoid sparse data problems we assumed zero probability for types of tag conversion with less than  100 instances in the training set . 
tag conversion
U--*NN
U  ~ JJ
U--~NNS
U--*NP
U ~ VBD
U ~ VBG
U--~VBN
U--~VB
U---, RB
U ~ VBZ
U--*NP $
VBD-~VBN
VBN - - * VBD
VB--*NN
NN ~ VB estimated probability 0 . 29 0 . 13 0 . 12 0 . 08 0 . 07 0 . 07 0 . 06 0 . 05 0 . 05 0 . 01 0 . 01 0 . 09 0 . 05 0 . 05 0 . 01
Table 2: Estimates for tag conversion
Our smoothing scheme is then the following heuristic modification of GoodTuring : 
C(ti , Wj)-k ~ k , ETiRim(k1--+i)g(tilwi ) = C(wi)+Ek , ETi , k2ETPam ( kz--"ks ) where Tj is the set of tags that w/has in the training set and T is the set of all tags  . This scheme has the following desirable properties : ? As with GoodTuring  , smoothing has a small effect on estimates that are based on large counts  . 
? The difference between closed-class and open -class words is respected : The probability for conversion to a closed class is zero and is not affected by smoothing  . 
? Prior knowledge about the probabilities of conversion to different tag classes is incorporated  . 
For example , an unseen word w i is five times as likely to be a noun than an adverb  . Our estimate for P ( ti \] wj ) is correspondingly five times higher for " NN " than for " RB "  . 
ANALYSISO FRESULTS
Our result on the test set of 114392 words ( the tenth of the Brown corpus not used for training  ) was 95 . 81% . Table 3 shows the 20 most frequent errors . 
Three typical examples for the most common error ( tagging nouns as adjectives ) are " Commu-nist " , " public " and " home run " in the following sentences  . 

VMM : correct :





VB "' CS'NP



QL\]1 JIVBNINIVB ? IINI?s I259102
VBI VBG 69   66 * the Cuban fiasco and the Communist military victories in Laos ? to increase public awareness of the movement ? the best home run hitter The words " public " and " communist " can be used as adjectives or nouns  . Since in the above sentences an adjective is syntactically more likely  , this was the tagging chosen by the VMM . The noun " home run " didn't occur in the training set  , therefore the priors for unknown words biased the tagging towards adjectives  , again because the position is more typical of an adjective than of a noun  . 
Two examples of the second most common error ( tagging past tense forms ( " VBD " ) as past participles ( " VBN " ) ) are " called " and " elected " in the following sentences : ? the party called for government operation of all utilities ? When I come back hereafter the November election you'll think  , you'remyman-elected . 
Most of the VBD/VBN errors were caused by words that have a higher prior for " VBN " so that in a situation in which both forms are possible according to local syntactic on text  , " VBN " is chosen . More global syntactic context is necessary to find the right tag " VBD " in the first sentence  . 
The second sentence is an example for one of the tagging mistakes in the Brown corpus  , " elected " is clearly used as a past participle , not as a past tense form . 
Comparison with other Results
Charniak et al's result of 95 . 97% ( Charniak et al , 1993) is slightly better than ours . This difference is probably due to the omission of rare tags that permit reliable prediction of the following tag  ( the case of " HVZ . " for " hasn't ") . 
Kupiec achieves up to 96 . 36% correctness ( Kupiec ,  1992) , without using a tagged corpus for training as we do  . But the results are not easily comparable with ours since a lexicon is used that lists only possible tags  . This can result in increasing the error rate when tags are listed in the lexicon that do not occur in the corpus  . But it can also decrease the error rate when errors due to bad tags for rare words are avoided by looking them up in the lexicon  . Our error rate on words that do not occur in the training text is  57%  , since only the general priors are used for these words in decoding  . This error rate could probably be reduced substantially by incorporating outside lexical information  . 
DISCUSSION
While the learning algorithm of a VMM is efficient and the resulting tagging algorithm is very simple  , the accuracy achieved is rather moderate . This is due to several reasons . As mentioned in the introductory sections , any finite memory Markov model cannot capture the recursive nature of naturalan-guage  . The VMM can accommodate longer statistical dependencies than a traditional full-order Markov model  , but due to its Markovian nature long-distance statistical correlations are neglected  . 
Therefore , a VMM based tagger can be used for pruning many of the tagging alternatives using its prediction probability  , but not as a complete tagging system . Furthermore , the VMM power can be better utilized in low level language processing tasks such as cleaning up corrupted text as demonstrated in  ( Ron et al ,  1993) . 
We currently investigate other stochastic models that can accommodate long distance statistical correlation  ( see ( Singer and Tishby , 1994) for preliminary results ) . However , there are theoretical clues that those models are much harder to learn  ( Kearns et al ,  1993) , including HMM based models ( Abe and Warmuth ,  1992) . 

Another drawback of the current tagging scheme is the independence assumption of the underlying tags and the observed words  , and the adhoc estimation of the static probabilities  . We are pursuing a systematic scheme to estimate those probabilities based on Bayesian statistics  , by assigning a discrete probability distribution , such as the Dirichlet distribution ( Berger ,  1985) , to each tag class . The a posteriori probability estimation of the individual words can be estimated from the word counts and the tag class priors  . Those priors can be modeled as a mixture of Dirichlet distributions  ( Antoniak ,  1974) , where each mixture component would correspond to a different a g class  . 
Currently we estimate the state transition probabilities from the conditional counts assuming a uniform prior  . The same technique can be used to estimate those parameters as well  . 

Part of this work was done while the second author was visiting the Department of Computer and Information Sciences  , University of California , Santa-Cruz , supported by NSF grant IRI-9123692 . 
We would like to thank Jan Pedersen and Naftali Tish by for helpful suggestions and discussions of this material  . Yoram Singer would like to thank the Charles Clore foundation for supporting this research  . We express our appreciation to faculty and students for the stimulating atmosphere at the  1993 Connectionist Models Summer School at which the idea for this paper took shape  . 
References
N . Abe and M . Warmuth , On the computational complexity of approximating distributions by probabilistic automata  , Machine Learning , 
Vol . 9, pp . 205-260, 1992.
C . Antoniak , Mixture of Dirichlet processes with applications to Bayesian nonparametric problems  , Annals of Statistics , Vol . 2, pp .  1152-174, 1974 . 
J . Berger , Statistical decision theory and Bayesian analysis  , New York : Springer-Verlag ,  1985 . 
E . Brill . Automatic grammar induction and parsing freetext : A transformation based approach  . In Proceedings of ACL31, pp .  259-265, 1993 . 
E . Charniak , Curtis Hendrickson , Neil Jacobson , and Mike Perkowitz , Equations for Part-of-Speech Tagging , Proceedings of the Eleventh National Conference on Artificial Intelligence  , pp .  784-789, 1993 . 
K . W . Church , A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text  , 
Proceedings of ICASSP , 1989.
A . Dempster , N . Laird , and D . Rubin , Maximum Likelihood estimation from Incomplete Datavia the EM algorithm  , J . Roy . Statist . Soc . ,
Vol . 39(B ), pp . 138, 1977.
W . N . Francis and F . Ku~era , Frequency Analysis of English Usage , Houghton Mifflin , Boston
MA , 1982.
F . Jelinek , Robust part-of-speech tagging using a hidden Markov model  , IBM Tech . Report , 1985 . 
M . Kearns , Y . Mansour , D . Ron , R . Rubinfeld , R . Schapire , L . Sellie , On the Learnability of Discrete Distributions , The 25th Annual ACM Symposium on Theory of Computing ,  1994 . 
S . Kullback , Information Theory and Statistics,
New York : Wiley , 1959.
J . Kupiec , Robust part-of-speech tagging using a hidden Markov model  , Computer Speech and
Language , Vol . 6, pp . 225-242, 1992.
L . R . Rabiner and B . H . Juang , An Introduction to Hidden Markov Models , IEEE ASSP Magazine , Vol . 3, No . 1, pp .  416, 1986 . 
J . Rissanen , Modeling by shortest data discription , Automatica , Vol . 14, pp .  465-471, 1978 . 
J . Rissanen , Stochastic omplexity and modeling , The Annals of Statistics , Vol . 14, No . 3, pp . 

J . Rissanen and G . G . Langdon , Universal modeling and coding , IEEE Trans . on Info . Theory,
IT-27, No.3, pp . 1223, 1981.
D . Ron , Y . Singer , and N . Tishby , The power of Amnesia , Advances in Neural Information
Processing Systems 6, 1993.
D . Ron , Y . Singer , and N . Tishby , Learning Probabilistic Automata with Variable Memory Length  , Proceedings of the 1994 Workshop on
Computational Learning Theory , 1994.
Y . Singer and N . Tishby , Inferring Probabilistic Acyclic Automata Using the Minimum Description Length Principle  , Proceedings of IEEE Intl . Symp . on Info . Theory , 1994 . 
R . Weischedel , M . Meteer , R . Schwartz , L.
Ramshaw , and : I . Palmucci . Coping with ambiguity and unknown words through probabilistic models  . Computational Linguistics , 19(2):359-382, 1993 . 
J . Wu , On the convergence properties of the EM algorithm  , Annals of Statistics , Vol . 11, pp . 


