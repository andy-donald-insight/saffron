Compilation of Weighted Finite State Transducers from 
Decision Trees
Richard Sproat
Bell Laboratories
700 Mountain Avenue
Murray Hill , NJ , USA
rws@bell-labs , tom
Michael Riley
AT&T Research
600 Mountain Avenue
Murray Hill , NJ , USA
riley@research , att.com
Abstract
We report on a method for compiling
decision trees into weighted finite-state
transducers . The key assumptions are
that the tree predictions specify how to
rewrite symbols from an input string,
and the decision at each tree node is
state able in terms of regular expressions
on the input string . Each leaf node
can then be treated as a separate rule
where the left and right contexts are
constructable from the decisions made
traversing the tree from the root to the leaf . These rules are compiled into transducers using the weighted rewrite-rule rule-compilation algorithm described in  ( Mohri and Sproat ,  1996) . 
1 Introduction
Much attention has been devoted recently to methods for inferring linguistic models from data  . 
One powerful inference method that has been used in various applications are decision trees  , and in particular classification and regression trees  ( Breiman et al ,  1984) . 
An increasing amount of attention has also been focussed on finite-state methods for implementing linguistic models  , in particular finite-state transducers and weighted finite-state transducers  ; see ( Kaplan and Kay , 1994; Pereira et al ,  1994 , inter alia ) . The reason for the renewed interest infinite -state mechanisms is clear  . Finite state machines provide a mathematically well-understood computational framework for representing a wide variety of information  , both in NLP and speech processing . Lexicons , phonological rules , Hidden Markov Models , and ( regular ) grammars are all representable as finite-state machines  , and finite-state operations such as union , intersection and composition mean that information from these various sources can be combined in useful is referred to the above-cited papers  ( among others ) for more extensive justification . 
This pape reports on a marriage of these two strands of research in the form of an algorithm for compiling the information in decision trees into weighted finite-state transducers  . 1 Given this algorithm , information if erred from data and represented in a tree can be used directly in a system that represents other information  , such as lexicons or grammars , in the form of finite-state machines . 
2 Quick Review of Tree-Based
Modeling
A general introduction to classification and regression trees  ( ' CART ' ) including the algorithm for growing trees from data can be found in  ( Breiman et al ,  1984) . Applications of tree-based modeling to problems in speech and NLP are discussed in  ( Riley , 1989; Riley , 1991; Wang and Hirschberg , 1992; Magerman ,  1995 , inter alia ) . In this section we presume that one has already trained a tree or set of trees  , and we merely remind the reader of the salient points in the interpretation f those trees  . 
Consider the tree depicted in Figure 1 , which was trained on the TIMIT database ( Fisher et al ,  1987) , and which models the phonetic realization of the English phoneme/aa /  ( / a / ) in various environments ( Riley ,  1991) . When this tree is used in predicting the allophonic form of a particular instance of/aa  /  , one starts at the root of the tree , and asks questions about the environment in which the/aa / is found  . Each nonleaf node n , dominates two daughter nodes conventionally labeled as  2n and 2n  +  1  ; the decision on whether to go left to 2n or right to 2n  +  1 depends on the answer to the question that is being asked at node n  . 
1The work reported here can thus be seen as complementary to recent reports on methods for directly inferring transducers from data  ( Oncina et al ,  1993;
Gildea and Jurafsky , 1995).
A concrete xample will serve to illustrate . Consider that we have/aa/in some environment . The first question that is asked concerns the number of segments  , including the/aa/itself , that occur to the left of the/aa/in the word in which/aa/oc-curs  .   ( See Table 1 for an explanation of the symbols used in Figure 1  . ) In this case , if the/aa/is initial--i . e . , lseg is 1 , one goes left ; if there is one or more segments to the left in the word  , goright . Let us assume that this/aa/is initial in the word  , in which case we go left . The next question concerns the consonantal ' place ' of articulation of the segmento the right of/an /  ; if it is alveolar go left ; otherwise , if it is of some other quality , or if the segment to the right of/aa/is not a consonant  , hengoright . Let us assume that the segment to the right is / z /  , which is alveolar , so we go left . This lands us atterminal node 4 . The tree in Figure 1 shows us that in the training data 119 out of 308 occurrences of/aa / in this environment were realized as\[ao \]  , or in other words that we can estimate the probability of/aa/being realized as\[ao\ ] in this environment as  . 385 . The full set of realizations at this node with estimated nonzero probabilities is as follows  ( see Table 2 for a relevant set of ARPABET-IPA correspondences  ) : phone probability-log prob . ( weight ) ao0 . 385 0  . 95 aa 0 . 289 1 . 24 q+aa 0 . 103 2 . 27 q+ao 0 . 096 2 . 34 ah 0 . 069 2 . 68 ax 0 . 058 2 . 8 4 An important point to bear in mind is that a decision tree in general is a complete description  , in the sense that for any new data point , there will be some leaf node that corresponds to it  . So for the tree in Figure 1 , each new novel instance of/aa/will be handled by  ( exactly ) one leaf node in the tree , depending upon the environment in which the/an /finds itself  . 
Another important point is that each decision tree considered here has the property that its prediction specify how to rewrite a symbol  ( in context ) in an input string . In particular , they specify a two-level mapping from a set of input symbols  ( phonemes ) to a set of output symbols ( allophones )  . 
3 Quick Review of Rule
Compilation
Work on finite-state phonology ( Johnson , 1972; Koskenniemi , 1983; Kaplan and Kay ,  1994 ) has shown that systems of rewrite rules of the familiar form ?--*?/  ) ~ p , where ? ,  ? , A and paretionally as finite-state transducers ( FSTs ) : note that ? represents the rule's input rule , ? the output , and ~ and p , respectively , the left and right contexts . 
Kaplan and Kay ( 1994 ) have presented a concrete algorithm for compiling systems of such rules into FSTs  . These methods can be extended slightly to include the compilation of probabilistic or weighted rules into weighted finite-state-transducers  ( WFST s--see ( Pereira et al ,  1994 ) ): Mohri and Sproat ( 1996 ) describe a rule-compilation algorithm which is more efficient hant he Kaplan-Kay algorithm  , and which has been extended to handle weighted rules  . For present purposes it is sufficient o observe that given this extended algorithm  , we can allow ? in the expression ?--~?/~ p , to represent a weighted regular expression . The compiled transducer corresponding to that rule will replace ? with ? with the appropriate weights in the context Ap  . 
4 The Tree Compi la t ion A lgor i thm The key requirements on the kind of decision trees that we can compile into WFSTs are  ( 1 ) the predictions at the leaf nodes specify how to rewrite a particular symbol in an input string  , and ( 2 ) the decisions at each node are state able as regular expressions over the input string  . Each leaf node represents a single rule . The regular expressions for each branch describe one aspect of the left context  ) ~ , right context p , or both . The left and right contexts for the rule consist of the intersections of the partial descriptions of these contexts defined for each branch traversed between the root and leaf node  . The input ? is predefined for the entire tree , whereas the output ? is defined as the union of the set of outputs  , along with their weights , that are associated with the leaf node . The weighted rule belonging to the leaf node can then be compiled into a transducer using the weighted-rule-compilation algorithm referenced in the preceding section  . The transducer for the entire tree can be derived by the intersection of the entire set of transducers associated with the leaf nodes  . Note that while regular relations are not generally closed under intersection  , the subset of same-length ( or more strictly speaking length-preserving ) relations is closed ; see below . 
To see how this works , let us return to the example in Figure 1 . To start with , we know that this tree models the phonetic realization of/aa /  , so we can immediately set ? to be a a for the whole tree  . Next , consider again the traversal of the tree from the root node to leaf node  4  . The first decision concerns the number of segments to the left of the/aa/in the word  , either none for the left0/34969/1281011 , many 2080/2080   415/439   14   15 Figure 1: Tree modeling the phonetic realization of/aa / . All phones are given in ARPABET . Table 2 gives ARPABET-IPA conversions for symbols relevant oth is example  . See Table 1 for an explanation of other symbols cpn cp-n place of articulation of consonant n segments to the right place of articulation of consonant n segments to the left values : alveolar  ; bilabial ; labiodental ; dental ; palatal ; velar ; pharyngeal ; n/a if is a vowel , or there is no such segment v pnvp-n place of articulation of voweln segments to the right place of articulation of voweln segments to the left values : central-mid-high  ; back-low ; back-mid-low ; back-high ; front-low ; front-mid-low ; front-mid-high ; front-high ; central-mid-low ; back-mid-highn/a if is a consonant , or there is no such segment I segnumber of preceding segments including the segment of interest within the word rseg number of following segments including the segment of interest within the word values :  1  ,  2 ,  3 , many str stress assigned to this vowel values : primary  , secondary , no ( zero ) stress n/a if there is no stress mark Table 1: Explanation of symbols in Figure 1  . 
217 aa ( \] aoaxah^q-baa 7 ( \] q + ao ? ~ Table 2: ARPABET-IPA conversion for symbols relevant for Figure  1  . 
branch , or one or more for the right branch . Assuming that we have a symbol are presenting a single segment  , the symbol  #representing a word boundary , and allowing for the possibility of intervening optional stress marks ~ which do not count as segments  , these two possibilities can be represented by the regular expressions for A in  ( a ) of Table 3 .   2 At this node there is no decision based on the righthand context  , so the right hand context is free . We can represent his by setting pat this node to be E *  , where E ( conventionally ) represents the entire alphabet : note that the alphabet is defined to be an alphabet of all ?:? correspondence pairs that were determined empirically to be possible  . 
The decision at the left daughter of the root node concerns whether or not the segment otheright is an alveolar  . Assuming we have defined classes of segments alv , blab , and so forth ( represented as unions of segments ) we can represent the regular expression for pas in  ( b ) of Table 3 . In this case it is A which is unrestricted , so we can set that at ~* . 
We can derive the ~ and p expressions for the rule at leaf node  4 by intersecting together the expressions for these contexts defined for each branch traversed on the way to the leaf  . For leaf node 4 , A=#Opt(')NE *= #Opt(') , and p = E*nOpt(')(alv ) = Opt(')(alv) . 3 The rule input ? has already been given as a a . The output ? is defined as the union of all of the possible expressions -- at the leaf node in question -- that a a could become  , with their associated weights ( negative log probabilities )  , which we represent here as subscripted floating -point numbers : ? =  a00  . 95 Uaal . 24 Oq+aa2 . 27 Oq-l-ao2 . 34 Uah 2 . 6s Uax 2 . s 4 Thus the entire weighted rule can be written as 2As far as possible , we use the notation of Kaplan and Kay (1994) . 
3 Strictly speaking , since the As and ps at each branch may define expressions of different lengths  , it is necessary to left-pade ach ) ~ with ~* , and right-pad each p with ~* . We gloss over this point here in order to make the regular expressions somewhat simpler to understandaa--~  ( aoo . 95Uaal . 24tdq+aa2 . 27Uq-bao2 . 34t . Jah2 . 6s Uax 2 . s4 ) /#Opt ( ' ) Opt ( ' ) (alv ) By a similar construction , the rule at node 6 , for example , would be represented as : aa --* ( aa0 . 40 Uaol . n ) /N ( Z * ( (cmh ) U ( bl ) U ( bml ) U ( b h ) ) ) r : Each node thus represents a rule which states that a mapping occurs between the input symbol ? and the weighted expression ? in the condition described by Ap  . Now , in cases where ? finds itself in a context hat is not subsumed by Ap  , the rule behaves exactly as a two-level surface coercion rule  ( Koskenniemi ,  1983 ) : it freely allows ? to correspond to any ? as specified by the alphabet of pairs  . These ?: ? correspondences are , however , constrained by other rules derived from the tree , as we shall see directly . 
The interpretation of the full tree is that it represents the conjunction of all such mappings : for rules  1  ,  2  .   .   . n , ? corresponds to ?1 given condition ~1__P l and ? corresponds to ? ~ given condition ~2   P2   .   .   . and ? corresponds to ?,, given condition ~ p ~ . But this conjunction is simply the intersection of the entire set of transducers defined for the leaves of the tree  . Observe now that the ?: ? correspondences that were leftfree by the rule of one leaf node  , are constrained by intersection with the other leaf nodes : since  , as noted above , the tree is a complete description , it follows that for any leaf node i , and for any context Ap not subsumed by hiP i , there is some leaf node j such that ) ~ jpj subsumes ~ p . 
Thus , the transducers compiled for the rules at nodes 4 and 6  , are intersected together , along with the rules for all the other leaf nodes  . Now , as noted above , and as discussed by Kaplan and Kay ( 1994 ) regular relations -- the algebrai counter-part of FSTs--are not in general closed under intersection  ; however , the subset of same-length regula relations is closed under intersection  , since they can be thought of as finite-state acceptors ex-  ( a ) left branch A =  #Opt ( ' ) p = E * right branch A ( b ) left branch A = E * p = Opt ( O ( alv )  =  (  #Opt ( ' ) a Opt ( ' ) ) U (  #Opt ( ' ) aOpt ( ' ) aOpt ( ' ) )U (  #Opt ( ' ) aOpt ( ' ) a Opt ( ' ) ( aOpt ( ' )  + )  =  ( Opt ( ' ) ~ ) + Opt ( ' ) right branch A = p = ( Opt ( ' ) ( blab ) U ( Opt ( ' ) ( labd ) U ( Opt ( ' ) ( den ) ) U ( Opt ( ' ) (pal )   ) U ( Opt ( ' ) (vel ) ) U ( Opt ( ' ) (pha ) ) U ( Opt ( ' ) (n/a ) ) Table 3: Regular-expression interpretation of the decisions involved in going from the root node to leaf node  4 in the tree in Figure 1  . Note that , as per convention , superscript '+' denotes one or more instances of an expression  . 
pressed over pairs of symbols .   4 This point can be extended somewhat o include relations that involve bounde deletions or insertions : this is precisely the interpretation necessary for systems of two-level rules  ( Koskenniemi ,  1983) , where a single transducer expressing the entire system may be constructed via intersection of the transducers expressing the individual rules  ( Kaplan and Kay ,  1994 , pages 367-376) . Indeed , our decision tree represents neither more nor less than a set of weighted two-level rules  . Each of the symbols in the expressions for A and pactually represent  ( sets of ) pairs of symbols : thus alp , for example , represents all lexical alveolars paired with all of their possible surface realizations  . And just as each tree represents a system of weighted two-level rules  , so a set of trees -- e . g . , where each tree deals with the realization of a particular phone -- represents a system of weighted two-level rules  , where each two-level rule is compiled from each of the individual trees  . 
We can summarize this discussion more formally as follows  . We presume a function Compile which given a rule returns the WFST computing that rule  . The WFST for a single leaf L is thus defined as follows  , where CT is the input symbol for the entire tree , eL  is the output expression defined at L ,   t95 represents the path traversed from the root node to L  , p is an individual branch on 4One can thus define intersection for transducers analogously with intersection for acceptors  . Given two machines Gz and G2 , with transition functions 51 and 52 , one can define the transition function of G ,  5 , as follows : for an input-output pair ( i , o ) , 5(( ql , q2) , ( i , o )) = ( q ~ , q ~) if and only if 5 z(ql , ( i , o )) = q ~ and 62 ( q2 , ( i , o )) = q ~ . 
219 that path , and Ap and pp are the expressions for
A and p defined at p:
Rule L = Compite(?-eL/NaPN ; P)
PEPLpEPL
The transducer for an entire tree T is defined as:
Rule T----D Rule L

Finally , the transducer for a forest F of trees is just :
Rule F = N Rule T
TEF5Empirical Verification of the

The algorithm just described has been empirically verified on the Resource Management  ( RM ) continuous peech recognition task ( Price et al ,  1988) . Following somewhat the discussion in ( Pereira et al , 1994; Pereira and Riley ,  1996) , we can represent he speech recognition task as the problem of finding the best path in the composition of a grammar  ( language model ) G , the transitive-closure of a dictionary D mapping be -" tween words and their phonemic representation  , a model of phone realization ( I ) , and a weighted lattice representing the acoustic observations A  . 

Best Path ( GoD * o ? oA ) (1)
The transducer ? fo = ~ e ~ Rule T can be constructed out of the r of  40 trees , one for each phoneme , trained on the TIMIT database . 
The size of the trees range from 1 to 23 leaf nodes , with a totM of 291 leaves for the entire forest . 
The model was tested on 300 sentences from the RM task containing 2560 word tokens , and approximately 10 , 500 phonemes . A version of the model of recognition given in expression  ( 1 )  , where q ~ is a transducer computed from the trees  , was compared with a version where the trees were used directly following a method described in  ( Ljolje and Riley ,  1992) . The phonetic realizations and their weights were identical for both methods  , thus verifying the correctness of the compilation algorithm described here  . 
The sizes of the compiled transducers can be quite large  ; in fact they were sufficiently large that instead of constructing ? b beforehand  , we intersected the 40 individual transducers with the lattice D*at runtime  . Table 4 gives sizes for the entire set of phone trees : tree sizes are listed in terms of number of rules  ( terminal nodes ) and raw size in bytes ; transducer sizes are listed in terms of number of states and arcs  . Note that the entire alphabet comprises 215 symbol pairs . Also given in Table 4 are the compilation times for the individual trees on a Silicon Graphics  R4400 machine running at 150 MHz with 1024 M bytes of memory . 
The times are somewhat slow for the larger trees , but still acceptable for offline compilation . 
While the sizes of the resulting transducers seem at first glance to be unfavorable  , it is important to bear in mind that size is not the only consideration i deciding upon a particular epre-sentation  . WFSTs posses several nice properties that are not shared by trees  , or handwritten rule sets for that matter . In particular , once compiled into a WFST , a tree can be used in the same way as a WFST derived from any other source  , such as a lexicon or a language model ; a compiled WFST can be used directly in a speech recognition model such as that of  ( Pereira and Riley ,  1996 ) or in a speech synthesis text-analysis model such as that of  ( Sproat ,  1996) . Use of a tree directly requires a special-purpose interpreter  , which is much less flexible . 
It should also be borne in mind that the size explosion evident in Table  4 also characterizes rules that are compiled from handbuilt rewrite rules  ( Kaplan and Kay , 1994; Mohri and Sproat ,  1996) . For example , the text-analysis ruleset for tem ( see ( Sproat , 1996; Mohri and Sproat ,  1996 ) ) contains ets of rules for the pronunciation of various orthographic symbols  . The rule set for < a > , for example , contains 25 ordered rewrite rules . 
Over an alphabet of 194 symbols , this compiles , using the algorithm of ( Mohri and Sproat ,  1996) , into a transducer containing 213 , 408 arcs and 1 , 927 states . This is 72% as many arcs and 48% as many states as the transducer for/ah/in Table  4  . The size explosion is not quite as great here , but the resulting transducer is still large compared to the original rule file  , which only requires 1428 bytes of storage . Again , the advantages of representing the rules as a transducer outweigh the problems of size  .   5   6 Future Applications We have presented a practical algorithm for converting decision trees inferred from data into weighted finite-state transducers that directly implemen the models implicit in the trees  , and we have empirically verified that the algorithm is correct  . 
Several interesting areas of application come to mind  . In addition to speech recognition , where we hope to apply the phonetic realization models described above to the much larger North American Business task  ( Paul and Baker ,  1992) , there are also applications to TTS where , for example , the decision trees for prosodic phrase-boundary prediction discussed in  ( Wang and Hirschberg ,  1992 ) can be compiled into transducers and used directly in the WFST-based model of text analysis used in the multilingual version of the Bell Lab -oratories TTS system  , described in ( Sproat ,  1995;
Sproat , 1996).
7 Acknowledgments
The authors wish to thank Fernando Pereira , Mehryar Mohri and two anonymous referees for useful comments  . 
References
Leo Breiman , Jerome H . Friedman , Richard A.
Olshen , and Charles J . Stone .  1984 . Clas-5 Having said this , we note that obviously one would like to decrease the size of the resulting transducers if that is possible  . We are currently investigating ways to avoid precompiling the transducers beforehand  , but rather to construct ' on the fly ' , only those portions of the transducers that are necessary for a particular intersection  . 
ARPABET phone  nodes size of tree ( bytes )  #arcs  #states time ( sec ) zh 1472 1510 . 3 jh 21 46 67 56 0 . 8 aw 2149 1,720 81f21 1966 960 . 9 ng 21 50 64 53 0 . 8oy2159 1,72081 uh 21266 453 0 . 9 p3 25 26 , 42690 4ay 32284 , 467 382 m 325 72 , 71 127 1ow 323 63 , 0 10   14   3 sh 3   230   694   8   1 v 3   230   685   8   1 b 4   354   3  , 978332 ch 435 33 , 010 254 th 437 31 , 351 112 dh 549 61 , 2906 3ey 5480 11 , 5 109627g 64273 72 , 339 3 , 00021k 65006 , 013859 aa 669318 , 441 10615 ah78554 0 , 135 273 110y 771 29 , 2454 312 ao 81 , 099 85 , 43984121eh896016 , 731 1671 3er8894 101 , 76582131 w8680118 , 154 1 , 1475 1hh 9968 17 , 459 160 10 1 9 947 320 , 266 3 , 15231 uw 91 , 318 44 , 86855228 z 91 , 045 1 , 987335s 101 , 060 175 , 901 2 , 03225 ae111 , 598 582 , 445 4 , 152231 iy 111 , 196 695 , 255 9 , 625 103d 121 , 414 36 , 06738938 n1 61 , 899 518 , 066 3 , 827 256 r1 61 , 901 131 , 90368069 ih 172 , 748 108 , 97066971t222 , 990 1 , 542 , 612 8 , 38 2628 ax2 34 , 281 295 , 954 3 , 9 66   77 Table 4: Sizes of transducers corresponding to each of the individual phone trees  . 
221 sification and Regression Trees . Wadsworth & Brooks , Pacific Grove CA . 
William Fisher , Victor Zue , D . Bernstein , and David Pallet .  1987 . An acoustic-phonetic database . Journal of the Acoustical Society of America ,  91 , December . Supplement 1 . 
Daniel Gildea and Daniel Jurafsky .  1995 . Automatic induction of finite state transducers for simple phonological rules  . In 33rd Annual Meeting of the Association for Computational Linguistics  , pages 915 , Morristown , NJ . Association for Computation M Linguistics . 
C . Douglas Johnson .  1972 . Formal Aspects of Phonological Description . Mouton , Mouton,
The Hague.
Ronald Kaplan and Martin Kay .  1994 . Regular models of phonological rule systems . Computational Linguistics , 20:331-378 . 
Kimmo Koskenniemi .  1983 . Two-Level Morphology : a General Computational Model for Word Form Recognition and Production  . 
Ph.D . thesis , University of Helsinki , Helsinki.
Andrej Ljolje and Michael D . Riley .  1992 . Optimal speech recognition using phone recognition and lexical access  . In Proceedings of ICSLP , pages 313-316 , Banff , Canada , October . 
David Magerman .  1995 . Statistical decision-tree models for parsing . In 33rd Annual Meeting of the Association for Computational Linguistics  , pages 276-283 , Morristown , NJ . Association for Computational Linguistics . 
Mehryar Mohri and Richard Sproat .  1996 . An efficient compiler for weighted rewrite rules . In 34rd Annual Meeting of the Association for Computational Linguistics  , Morristown , NJ . 
Association for Computational Linguistics.
Jos ~ Oncina , Pedro Garela , and Enrique Vidal.
1993 . Learning subsequential transducers for pattern recognition tasks  . IEEE Transactions on Pattern Analysis and Machine Intelligence  ,  15:448-458 . 
Douglas Paul and Janet Baker .  1992 . The design for the Wall Street Journal-based CSR corpus  . 
In Proceedings of International Conference on Spoken Language Processing  , Banff , Alberta . 

Fernando Pereir and Michael Riley .  1996 . Speech recognition by composition of weighted finite automata  . CMP-LG archive paper 960 3001 . 
Fernando Pereira , Michael Riley , and Richard Sproat .  1994 . Weighted rational transductions and their application to human language processing  . In ARPA Workshop on Human Language Technology , pages 249-254 . 

Advanced Research Projects Agency , March 811.
Patty Price , William Fisher , Jared Bernstein , and David Pallett .  1988 . The DARPA 1000-word Resource Management Database for continuous speech recognition  . In Proceedings of ICASSP 88 , volume 1 , pages 651-654 , New
York . ICASSP.
Michael Riley .  1989 . Some applications of tree-based modelling to speech and language  . In Proceedings of the Speech and Natural Language Workshop  , pages 339-352 , Cape Cod
MA , October . DARPA , Morgan Kaufmann.
Michael Riley .  1991 . A statistical model for generating pronunciation networks  . In Proceedings of the International Conference on Acoustics  , Speech , and Signal Processing , pages Sl1 . 1 . -
Sll . 4. ICASSP 91, October.
Richard Sproat .  1995 . A finite-state architecture for tokenization and grapheme-to-phoneme conversion for multilingual text analysis  . In Susan Armstrong and Evelyne Tzoukermann , editors , Proceedings of the EACL SIGDAT Workshop , pages 65-72 , Dublin , Ireland . Association for Computational Linguistics . 
Richard Sproat .  1996 . Multilingual text analysis for text-to-speech synthesis  . In Proceedings of the ECAI-96 Workshop on Extended Finite State Models of Language  , Budapest , Hungary . European Conference on Artificial

Michelle Wang and Julia Hirschberg .  1992 . Automatic classification of intonational phrase boundaries  . Computer Speech and Language , 6:175-196 . 
