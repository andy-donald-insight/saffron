ADP based Search Using Monotone
Alignments in Statistical Translation
C . Tillmann , S . Vogel , H . Ney , A . Zubiaga
Lehrstuhlf/Jr Informa , tik VI , RWTH Aachen
D-52056 Aachen , Germany
tillmann , ney?informatik , rwth-aachen , de
Abstract
In this paper , we describe a Dynamic Pro-
gramming ( DP ) based search algorithm
for statistical translation and present experimental results  . The statistical translation uses two sources of information : a translation model and a language model  . The language model used is a standard bigram model  . For the translation l node l , the alignment probabilities are made dependent on the differences in the alignment positions rather than on the absolute positions  . Thus , the approach amounts to a first-order Hidden Markov model  ( HMM ) as they are used successfully in speech recognition for the time alignment problem  . Under the assumption that the alignment is monotone with respect othe word order in both languages  , an efficient search strategy for translation can be formulated  . The details of the search algorithm are described  . Experiments on the Eu Trans corpus produced a word error rate of  5  . 1(/~ . .
1 Overview : The Statistical
Approach to Translation
The goal is the translation of a text given in some source language into a target language  . We are given oJ a source ( ' French ' ) string f l = f l . . . fj . . . f . l , which is to be translated into a target ( ' English ' ) string c ~= el . . . ei . . . el . Among all possible target strings , we will choose the one with the highest probability which is given by Bayes ' decision rule  ( Brown et al . .
1993):  , ~ = argmaxP , '( e\]~lfg ~) = argmaxP , '( ef ) . Pr ( . f/lef ) Pr ( e ) is the language model of the target language . 
whereas Pr ( j ' lale ) is the string translation model . 
The argmax operation denotes the search problem.
In this paper , we address ? the problem of introducing structures into the probabilistic dependencies in order to model the string translation probability Pr  ( f\]\[e ~ )  . 
? the search procedure , i . e . an algorithm to perform the argmax operation in an efficient way  . 
? transformation steps for both the source and the target languages in order to improve the translation process  . 
The transformations are very much dependent on the language pair and the specific translation task and are therefore discussed in the context of the task description  . We have to keep in mind that in the search procedure both the language and the translation model are applied after the text transformation steps  . However , to keep the notation simple we will not make this explicit distinction in the subsequent exposition  . The overall architecture of the statistical translation approach is summarized in Figure  1  . 
2 Aligmnent Models
A key issue in modeling the string translation probability Pr  ( f ( leI ) is the question of how we define the correspondence bt ween the words of the target sentence and the words of the source sentence  . In typical cases , we can assume a sort of pairwise dependence by considering all word pairs  ( fj , ei ) for a given sentence pair \[ f(;el\] . We further constrain this model by assigning each source word to exactly one target word  . Models describing these types of dependencies are referred to as align rnen  . t models ( Brown et al . , 1993), ( Daganeta \] .   .  1993) . ( Kay & R6scheisen , 1993) . ( Fung&Church . 1994), ( Vogel et al , 1996) . 
In this section , we introduce a monotoue HMM based alignment and an associated DP based search algorithm for translation  . Another approach to statistical machine translation using DP was presented in  ( Wu ,  1996) . The notational convention will be a , s follows . We use the symbol Pr ( . ) to denote general ? ~
Global Search : j ~ Lexicon Model maximize Pr(el) . pr(f ~ lell II Allgnment Model ov or j . pc(e ~)\ [ Language Model , \[; . . . .,! . . . , , on \] Figure I : Architecture of the translation approach based on Bayes decision rule  . 
probability distributions with ( nearly ) no specific as-snmptions . In contrast , for model-based probability distributions , we use the generic symbol p ( . ) . 
2.1 Alignment with HMM
When aligning the words in parallel texts ( for Indo-European language pairs like Spanish -English  , German-English , halian-German . . . .) , we typically observe a strong localization effect  . . Figure 2 illustrates this effect , for the language pair Spanish-to-English . In many cases , although not always , there is an even stronge restriction : the difference in the position index is smaller than  3 and the alignment . 
is essentially monotone . To be more precise , the sentences can be partitioned into a small number of segments  , within each of which the alignment is monotone with respect to word order in both lan-gaages  . 
To describe these word-by-word alignments , we introduce the mapping j--oj , which assigns a position j ( with source word . fj ) to the position i = aj ( with target word ei )  . The concept of these alignments is similar to the ones introduced by  ( Brown et al ,  1993) , but we will use another type of dependence in the probability distributions  . Looking at . such alignments produced by a human expert , it , is evident that the mathematical model should try to capture the strong dependence of a j on the preceding alignment  aj1  . Therefore the probability of alignment aj for position j should have a dependence on the previous alignment position Oj_l : 
P ((/ j\[(/j1)
A similar approach has been chosen by ( Dagan et al . , 1993) and ( Vogel et al .  1996) . Thus the problem formulation is similar t . o that of / , he time alignment problem in speech recognition , where the socalled Hidden Markov models have been successfully used for a long time  ( Jelinek .  1976) . Using the same basic principles , we can rewrite the probability by introducing the ' hidden " aligmnents a ~:= al  . . . aj . . . a a for a sentence pair \[ f ~; c/\]:
P , , ( s'lcI =
J ~ i ' j=1
To avoid any confnsion with the term ' hidden ' in comparison with speech recognition  , we observe that the model states as such ( representing words ) are not hidden but the actual alignments , i . e . the sequence of position index pairs ( j . i = aj ) . 
So fart here has been no basic restriction of the approach  . We now assume a first-order dependence on the alignments ajonly : Pr  ( fj , ajlf ~- l , a-1 . e ) = p(fj , (/ jlaj-l , e ) = p(ajlaj_l) . p(fj  lea ,  )  , where , in addition , we have assumed that the lexicon probability p ( fle ) depends only on a j and not . on aj_ 1 ? To reduce the number of alignment parameters , we assume that the HMM alignment probabilities p ( i \[ i' ) depend only on the jump width ( i - i ' )  . The monotony condition can than be formulated as : p  ( i \[ i' ) = O for i ? i ' + O . i '+ l,i'+2 . 
This monotony requirement limits the applicability of our approach  . However , by performing simple word reorderings , it . is possible to approach this requirement ( see Section 4 . 2) . Additional counter measures will be discussed later  . Figure 3 gives an illustration of the possible alignments for the monotone hidden Markov model  . To draw the analogy with speech recognition , we have to identify the states ( along the vertical axis ) with the position si of the target words ei and the time  ( along the horizon t . alaxis ) with the positions j of the source words J )  . 
2.2 Training
To train the alignment and the lexicon model , we use the maximum likelihood criterion in the so called maximum approximation  , i . e . the likelihood criterion covers only the most likely alignment rather than the set of all alignments : 

Pr( . f(leI ) = ~1-i\[P ( aJlaJ-l' . I ) " P ( fJ le ? . i)\]"i'j = i
J -'= max1-~\[p(ajla . o_~, I ) . p( . l ) leo , )\] j al j = l two o fo r o room o doub le o a o i so much how IoI  .   .   .   . L___L___L___L .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 
cvuhd pdd
Uanaoao '' I abbrsia eiiaante staociroom J  , othe J . oinJocold\[,o tooI . o is I . 
it J.o
J .....................
eIh hdf na a aerbcm ' i ea it so ai
Caidnnight a for tva and safeate lephone laJ with Jroom JaI booked I have we 


I .   .   .   .  - - - - ' - - - - - - - - -  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 
truhct cfytpuneen ao eaunsab n  1 jeeei ' armrte to vafes a codin a  ) oe a no 1 race ahvei
Si
Figure 2: Word aligmnents for Spanish-English sentence pairs  . 
291 o *"
Zr . ~?
L5 iv , <

IIII\[I123456
SOURCE POSITION
Figure 3: Illustration of alignments for the nlonotone

To find the optimal alignment , we use dynamic programming for which we have the following typical recursion formula : Q  ( i , j ) = p(fj\]ei)max\[p(i li') . Q(i ', j-1)1i'Here . Q(i . j ) is a sort of partial probability as in t . ime alignment for speech recognit . ion ( aelinek , 1976) . As a result , the training procedure amounts to a sequence of iter at  . ions , each of which consists of two steps : ? posilion alignm~TH : Given the model parame-t  . ers , det . ermine the most likely position align-n-lent . 
? parame * e-reslimalion : Given the position alignment  . i . e . going along the alignment paths for all sentence pairs  , perform maximum likelihood estimation of the model parameters  ; for model-free distributions , these estimates result in rel-a . tive fi'equencies . 
The IBM model 1 ( Brown et al ,  1993 ) is used to find an initial estimate of the translation probabilities  . 
3 Search Algor i thm for T rans la t ion For the translation operat  . ion , we use a bigram language model , which is given in terms of the con-dit . ional probability of observing word e i given the predecessor word e  . i-1: p ( ~ilei-: ) Using the conditional probability of the bigram language model  , we have the overall search criterion in the maxinmm approximation : max p  ( eile ; _:) lnaxl'I\[p(ajla ~- :) P ( fJ lea , )\] "  , ,' t i =: ~ i ~=: Here and in the following , we omit a special treatment of the start and end conditions like j =  1 or j = J in order to simplify the presentation and avoid confusing details  . Having the above criterion in mind , we try t . o associate the language model probabilities with the aligmnent sj~i-aj  . To this purpose , we exploit the monotony property of our alignment model which allows only transitions from aj-i tO aj if the difference  6 = oj-aj -1 is 0  , 1 , 2 . 
We define a modified probability p ~ ( el # ) for the language model depending on the alignment difference t ~  . We consider each of the three cases 5= 0 ,  1 , 2 separately : ? ~ = 0 ( horizontal transition = alignment repetition ) : This case corresponds to a target word with two or more aligned source words and therefore requires ~=  #so that there is no contribution fl ' om the language model:  1 for e = e '
P~=? ( ele ' ) = 0 foreee ' ? 6= 1 ( forward transition = regular alignment . ): This case is the regular one , and we can use directly the probability of the bigram language model : p ~ = :  ( ele ' ) = p ( ele ' )  ? ~ = 2  ( skip transition = nonaligned word ) : This case corresponds to skipping a word . i . e , there is a word in the target string with no aligned word in the source string  . We have to find the highest probability of placing a nonaligned word e_-between a predecessor word e ' and a successor word e  . Thus we optimize the following product , over the nonaligned word g : p ~ = ~ ( eJe ' ) = max b ~ ( elg )  . p ( gIe ' ) \] i This maximization is done beforehand and the result is stored in a table  . 
Using this modified probability p~(ele') , we can rewrite the overall search criterion : a T l-I  ) \] . 
The problem now is to find the unknown mapping : j  - -   ( aj , ca . , ) which defines a path through a network with a uniform trellis structure  . For this trellis , we can still use Figure 3 . However . in each position i along the ! nput : source str ing/l  .   .   . fj .   .   . fJ  initialization for each position j = 1, 2 . . . . . d in source sel'ltence do for each position i = 1 , 2 ,   . . . ,/ maz in target sentence do for each target word e do V Q  ( i , j , e ) = p(fjle)'ma ; xp(i\[i-6) . p~(e\[e') . Q(i-6 . j-1 , e ') 6 , etrace back:-find bestend hypothesis : maxQ ( i , J , e ) -recover optimal word sequence vertical axis . we have to allow all possible words e of the target vocabulary  . Due to the monotony of our alignnaent model and the bigral n language model  . we have only first-order type dependencies such that the local probabilities  ( or costs when using the negative logarithms of the probabilities  ) depend on-I . q on the arcs ( or transitions ) in the lattice . Each possible index triple ( i . j . e ) defines a grid point in the lattice , and we have the following set of possible transitions fi'om one grid point to another grid point : ~ fi  0  . 1 . 2: ( i -6 . j-l . e ') --( i , j , e ) Each of these transitions is assigned a local probability : p  ( ili-6 )  . p , , ( ele ') . p ( fjle ) Using this formulation of the search task , we can now use the method of dynamic programming ( DP ) to find the best path through the lattice . To this purpose , we introduce the auxiliary quantity : Q(i . j . e ): probability of the best . partial path which ends in the grid point ( i , j , e ) . 
Since we have only first-order dependencies in our model  , it is easy to see that the auxiliary quantity nmst satisfy the following DP recursion equation : 
Q(i.j.e ) = p(fjle).
maxp(ili-~) . maxp , , ( ele ') . Q(i-6, j-1, e') . 
To explicitly construct he unknown word sequence ~ . it is convenien to make use of socalled backpointers which store for each grid point  ( i . j , e ) the best predecessor gridpoint ( Ney et al .  1992) . 
The DP equation is evaluated recursively to find the best partial path to each grid point  ( i , j , e ) . The resuhing algorithm is depicted in Table 1 . The complexity of the algorithm is J . I , , , . , . ? E'-' . where E is the size of t . he target language vocabulary and I , , , , ~ . 
is then ~ aximum leng'h of the target sentence considered  . It is possible to reduce this CO ml ) utational complexity by using socalled pruning methods  ( Ney et al . 1992): due to space limitatiol ~ s , they are not discussed here . 
4 Experimental Results 4 . 1 The Task and the Corpus The search algorithln proposed in this paper was tested on a subtask of the "' Traveler Task "  ( Vidal ,  1997) . The general domain of the task comprises typical situations a visitor to a foreign country is faced with  . The chosen subtask corresponds to a scenario of the hulnan-to-human communication situations at the registration desk in a hotel  ( see Table 4 )  . 
The corpus was generated in a semiautomatic way . On the basis of examples from traveller booklets , aprol ) a bilistic gralm nar for different language pairs has been constructed from which a large corpus of sentence pairs was generated  . The vocabulary consisted of 692 Spanish and 518 English words ( in-eluding punctuatioll marks )  . For the experiments , at railfing corpus of 80 , 000 sentence pairs with 628 , 117 Spanish and 684 . 777 English words was used . In addition , a test corpus with 2 . 7 30 sentence pairs different froln the training sentence pairs was constructed  . This test corpus contained 28 . 642 Spanish a . nd24 . 927 English words . For the English sentences , we used a bigram language model whose perplexity on the test corpus varied between  4  . 7 for the original text . and 3 . 5 when all transformation steps as described below had been applied  . 
Table 2: Effect of the transformation steps on the vocabulary sizes in both languages  . 
Transformation Step Spanish English
Original ( with punctuation ) 692518 + C . ategorization 416   227 +' por_~avor ' 417 + V~'ol'dS plkt . ing 374 + Word Joining 237 +' Word Reordering The purpose of the text transformations is to make the two languages resenable ach other as closely as possible with respect  , to sentence lngth and word order . In addition , the size of both vocabularies i reduced by exploiting evident regularities  ; e . g . proper names and numbers are replaced by category markers  . We used different , preprocessing steps which were applied consecutively : ? Original Corpus : Punctuation marks are treated like regular words  . 
? Categorization : Some particular words or word groups are replaced by word categories  . 
Seven nonoverlapping categories are used : three categories for names  ( surnames , name and female names ) , two categories for numbers ( regular numbers and room numbers ) and two categories for date and time of day . 
?' D_'eatment of'pot:favor':The word'pot : favor ' is always moved to the end of the sentence and replaced by the one word token'pot_ favor '  . 
? Word Splitting : In Spanish , the personal pronouns ( in subject case and in object , case ) can be part of the inflected verb form . To counteract this phenomenon , we split the verb into a verb part and pronoun part  , such as ' darnos " --" dar_nos ' and " pien so "--'_y opienso '  . 
? Word Joining : Phrases in the English language such as " Would yogim ind doing  .   .   . ' and '1 would like you to do . . . " are difficult to handle by our alignment model . Therefore , we apply some word joining , such as ' would yo ~ tmi71d "--' wo~dd_yo ' , _mind " and ~ would like ' --" wotdd_like ' . 
? Word Reordering : This step is applied to the Spanish text to take into account  , cases like the position of the adjective in noun -adjective phrases and the position of object  , pronouns . 
E . g . " habit acid T~dobh'--'doble habitaci6~' . 
By this reordering , our assumption about the monotony of the alignment model is more often satisfied  . 
The effect of these transformation steps on the sizes of both vocabularies i shown in Table  2  . In addition to all preprocessing steps , we removed the punc-t . uation marks before translation and resubstituted t  . henaby rule into the target sentence . 
4.3 Translation Results
For each of the transformation steps described above  , all probability models were trained a new , i . e , the lexicon probabilities p(fle ) , the alignment probabilities p ( ili-6 ) and the bigram language probabilities p ( ele ' )  . To produce the translated sentence in normal anguage  , the transformation steps in the target language were inverted  . 
The translation results are summarized in Table 3 . As an aut . omatic and easy-to-use measure of the translation errors  , the Levenshtein distance between the automatic translation and the reference translation was calculated  . Errors are reported at the word level and at . the sentence level : ? word leveh insertions ( INS )  . deletions ( DEL ) , and totallmmber of word errors (\ VER ) . 
? sentence level : a sentence is counted as correct only if it is identical to the reference sentence  . 
Admittedly , this is not a perfect measure . In particular , the effect of word ordering is not taken into account appropriately  . Actually , the figures for sentence error rate are overly pessimistic  . Many sentences are acceptable and semantically correct rans-lations  ( see the example translations in Table 4 )  , Table 3: Word error rates ( INS/DEL , WER ) and sentence rror rates ( SER ) for different ransforma-tion steps . 
Transformation Step
Original Cor Pora + Categorization +' por2 favor ' + WordSplitting
Translation Errors\[~ . \] 423/11 . 2 21 . 2 85 . 5 2 . 5/? . 6 16 . 1 81 . 0 2 . 6/8 . 3 14 . 3 75 . 6 2 . 5/7 . 4 12 . 3 65 . 4 i . 3/4 . 9 44 . 6 + Word Joining 7 . 3 + Word Reordering 0 . 9/3 . 4 5 . 1 30 . 1 As can be seen in Table 3 . the translation errors can be reduced systen ~ at . ically by applying all transformation steps . The word error rate is reduced from 21 . 2, t . o5 . 12 ~: the sentence rror rate is reduced from 85 . 55 ~, to 30 . 1% . The two most in a-portan transformation steps are categorization and word joining  . What is striking , is the large fi'action of deletion errors . These deletion errors are often caused by the omission of word groups like ' formeplease " and " could you "  . Table 4 shows some example translations ( for the best translation results )  . It can be seen that the semantic meaning of the sentence in the source language may be preserved even if there are three word errors according t  . o our performance criterion . To study the dependence on the amount of training data  , we also performed a training w it . laonly 5   000 sentences out of the training corpus . For this training condition , the word error rate went uponly slightly , namely from 5 . 15 . ( for 80,000 training sentences ) to 5 . 3% ( for 5000 training sentences ) . 
To study the effect of the language model , we tested a zerogram , a unigram and a bigram language model using the standard set of  80   000 training sentences . The results are shown in Table 5 . Thet . ranslatiol~ . 
O : He he chola reserva de una habit a cidn con televisidny t  . el ~ fonoa hombre delse fior Morales . 
R : I have made a reservation for a room with TV and telephone for Mr  . Morales . 
A : I have made a reservation for a room with TV and telephone for Mr  . Morales . 
O : S fiban mela smale ta sami ha bit ac idn , pot favor . 
R : Sendup my suit cases to my room , please.
A : Sendup my suit cases to my room , please.
O : Potfavor , querra quanos diese lasl laves de la habit acidn . 
R : I would like you to give us the keys to the room  , please . 
A : I would like you to give us the keys to the room  , please . 
O : Potfavor , mepidemit axipara lahabit a cidntresve intidds ? R : Could you ask for nay taxi for room number three two two form e  . please ' ? A : Could you ask for my taxi for room number three two two  . please ? O : Porfavor , reservamos do shabitaciones dobles coneuar to de bafio  . 
R : We booked two double rooms with a bathroom.
A : We booked two double rooms with a bathroom , please . 
O : Quisier aquanos despert a ranma fianal as dosy cuarto  , pot favor . 
R : l would like you to wake us up to morrow at . a quarter past two . please . 
A : I want you to wake us up to morro wat a quarter past two  . please . 
O : Rep/semelacuent a delal ~ a bitacidn ochocientos veintiuno  . 
R : Could . you check the bill for room number eight two one form e  , please ' ? A : Check the bill for room lmmber eight two one  . 
WER decreases from 31 . 1 c / c for the zerogram model to 5 . 1% for the bigram model . 
The results presented here can be compared with the results obtained by the finite-state transducer approach described in  ( Vidal , 1996: Vidal ,  1997) , where the same training and test conditions were used  . However the only preprocessing step was categorization  . In that work . a WER of 7 . 1c ) ~ . was obtained as opposed to 5 . 1 (7 c presented in this paper . 
For smaller amounts of training data ( say 5000 sentence pairs )  , the DP based search seems to be evenlnore superior  . 
Table 5: Language model perplexity ( PP ) , word error rates ( INS/DEL . WER ) and sentence rror rates ( SER ) for different language models . 
Model Language PPINS/DEL Translation WER Errors \ [ SER\[%\] 
Zerogram 237.0 0.6/18.6 31.1 98.1
Unigram 74.4 0.9/1 2.4 20.4 94.8
Bigram 4 . 1 0 . 9/3 . 4 5 . 1 30 . 1 4 . 4 Effect of the Word Reorder ing In more general cases and applications  , there will a hvays be sentence pairs with word alignments for which the monotony constraint is \ ] lot satisfied  . However even then , then lonotouy constraint is satisfied locally for the lion's share of all word alignments in such sentences  . Therefore . we expect t . o extend the approach presented by the following methods : ? more systelnatic approaches to local and global word reorderiugs that try to produce the same word order in both languages  . 
? a multli-level approach that allows a small ( say 4 ) number of large forward and backward transitions . Within each level , the monotone alignment model can still be applied  , and only when moving from one level to the next , we have to handle the problem of different word orders  . 
To show the usefulness of global word reordering . we changed the word order of some sentences by hand  . Table 6 shows the effect of the global reordering for two sentences  . In the first example , we changed the order of two groups of consecutive words and placed an a  . d ditional copy of the Spanish word " euest , a '" into the source sentence . In the second example , the personal pronoun "' me " was placed at the end of the source sentence  . In both cases , we obtained a correct translation . 
5 Conclusion
In this paper , we have presented an HMM based approach to handling word alignlnents and an associated search algorithm for autonaatic translation  . The characteristic feature of this approach is to make the aligmnent probabilities explicitly dependent on the Mignment position of the previous word and t  . o assume a monotony constraint for the word order in both languages  . Duet . othism Oll Otony constraint . 
we are able to apply an efficient DP based search al-gorithln  . We have tested the model successfully on the Eu Transtraveller task  , a limited domain task with a vocabulary of 200 to 500 words . The result-translation , O '= original sentence reordered , A '= aut , omatic translation after reordering . 
O : Cu?n to cue staun a habit a cidn do ble para cinco noches in cluyen do servicio dehabitaciones ? R : How much does a double room including room service cost for fivenights ? A : How much does a double room including room service ? O':Cu ~ into cuest a unahabit a cidndoble in cluyen do servicio'de habitaciones cuesta paracinco noches ? A':How much does a double room hlcluding room service cost for fivenights ? O  :  . Expli'que_mel afactura de laha bitacidntres doscuatro  . 
R : Explain the bill for room number three two four form e  . 
A : Explain the bill for room number three two four  . 
O': Expliquela faclura de la habitaci6n tresdoscuatro . ane . 
A : Explaintile bill for rool n number three two four form e  . 
ing word error rate was only 5 . 1V ( . To mitigate the monotony constraint , we plan to reorder the words in the source sentences to produce the same word order in both languages  . 
Acklmwledgement
This work has been supported partly by t . he German Federal Ministry of Education . Science . Research and Technology under the contract number  01 IV 601 A ( Verbmobil ) and by the European Community under the ESPRIT project number  20268   ( Eu Trans )  . 

A . L . Berger . P . F . Brown . S . A . Della Pietra , V . J . 
Della Pietra .  ,\] . R . Gillett . J . D . Lafferty . R . L . 
Mercer . H . Printz . and L . Ures .  1994 . " The Call-dide System for Machine Translation " . In Proc . of ARPA Huma ~ La , guage Technology Workshop . 
pp . 152-157. Plainsboro . NJ . Morgan Kaufinann
Publishers . San Mateo . CA , March.
P . F . Brown , V . J . Della Pietra . S . A . Della Pietra , and R . L . Mercer .  1993 . "' The Mathematics of Statistical Machine Translation : Parameter Esti-mat  . ion " . Comp , fational Linguistics , Vol . 19, No . 
2.pp . 263-311.
I . Dagan . K . W . Church . and W . A . Gale .  1993 . 
"' Robust Bilingual Word Alignment for Machine Aided Translation "  . In Proc . of the Workshop on I . <ry Large Corpora . pp .  18 . Columbus , OH . 
P . Fung . and K . W . Church .  1994 . "' Kvec : A New Approach for Aligning Parallel Texts "  , In Proc . of lhe15th Ini . Conf . on (' ompulalim ~ al Linguistics , pp .  10 . ( . ) 6-1102, Kyoto . 
F . .lelinek .  1 . ( . t76 . "' Speech Recognition by Statistical Methods " . Proc . of lhe IEEE . Vol .  64 . pp .  532-556 . April . 
M . Kay . and M . R6s cheisen . 1993." Text-
Translation Alignlnent " . Comp ~ talional Lin . gu~s-lie . s . Vol .  19 . No .  2 . pp .  121-142 . 
H . Ney , D . Mergel , A . Noll , A . Paeseler .  1992 . " Da-t . a Driven Search Organization for Continuons Speech Recognition "  . IEEE Trans . on Signal Processing , Vol . SP-40 . No .  2 . pp .  272-281 . February . 
E . Vidal .  1996 . " Final report of Esprit Research Project .  20268  ( Eu Trans ) : Example-Based Understanding and Translation Systelns "  . Universidad Polit ~ cnica de Valencia , Instituto Tecnol 6 giode
Informgtica , October.
E . Vidal .  1997 . " FiniteState Speech-to-Speech Translation " . In Proc . of lhe Int . Co , , f . on Acous-fits , Speech and Signal Processing . Munich . April . 
S . Vogel , H . Ney , and C . Tillmmm .  1996 . " HMMBased Word Alignment in Statistical Translation "  . In Proc . of the 16~h Inf . Conf . on Computational Linguistics . pp .  836-841 . Copenhagen,

D . Wu .  1996 . "' A Polynomial-Time Algorithm for Statistical Machine Translation "  . In Proc . of the 34th Annual Conf . of the Associalio ~ for Comp ~ l-talional Linguistics  , pp .  152-158 . Santa Cruz , CA . 
Julle,
