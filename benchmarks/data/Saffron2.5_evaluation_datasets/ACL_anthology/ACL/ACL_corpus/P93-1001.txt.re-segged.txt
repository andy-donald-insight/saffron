Char_align : A Program for Aligning Parallel Texts
at the Character Level
Kenneth Ward Church
AT&T Bell Laboratories
600 Mountain Avenue
Murray Hill NJ , 07974-0636
kwc@research.att.com

There have been a number of recent papers on aligning parallel texts at the sentence lvel  , e . g . , Brown et al(1991) , Gale and Church ( to appear) , Isabelle (1992) , Kay and R/S senschein ( to appear) , Simard et al (1992) , Warwick-Armstrong and Russell (1990) . On clean inputs , such as the Canadian Hansards , these methods have been very successful ( at least 96% correct by sentence )  . Unfortunately , if the input is noisy ( due to OCR and/or unknown markup conventions )  , then these methods tend to break down because the noise can make it difficult to find paragraph boundaries  , let alne sentences . This paper describes a new program , charalign , that aligns texts at the characterlevel rather than at the sentence/paragraph level  , based on the cognate approach proposed by Simard et al  1  . Introduction Parallel texts have recently received considerable attention in machine translation  ( e . g . , Brown et al1990), bilingual lexicography ( e . g . , Klavans and Tzoukermann ,  1990) , and terminology research for human translators ( e . g . , Isabelle , 1992) . We have been most interested in the terminology application  . 
Translators find it extremely embarrassing when " store "  ( in the computer sense ) is translated as " grocery , " or when " magnetic fields " is translated as " magnetic meadows  . " Terminology errors of this kind are all too common because the translator is generally not as familiar with the subject domain as the author of the source text or the readers of the target text  . Parallel texts could be used to help translators overcome their lack of domain expertise by providing them with the ability to search previously translated documents for examples of potentially difficult expressions and see how they were translated in the past  . 
While pursuing this possibility with a commercial translation organization  , AT&T Language Line Services , we discovered that we needed to completely redesign our alignment programs in order to deal more effectively with texts supplied by AT&T Language Line's customers in whatever form a they happen to be available in  . All too often these texts are not available in electronic form  . And even if they are available in electronic form  , it may not be worth the effort to clean them up by hand  . 
2. Real Texts are Noisy
Most previous work depends on being able to identify paragraph and sentence boundaries with fairly high reliability  . We have found it so difficult to find paragraph boundaries in texts that have been OCRed that we have decided to aband on the paragraph/sen-tence approach  . Figurei , for example , shows some parallel text ( selected from the official record of the European Parliament  ) hat has been processed with the Xerox Scan Wor X OCR program  . The OCR output is remarkably good , but nevertheless , the paragraphs are more elusive than it might appear at first  . 
The first problem we encountered was the missing blank line between the second and third paragraphs in the French  ( Figure lb )  . Although this missing line might obscure the boundary between the two paragraphs  , one could imagine methods that could overcome missing blank lines  . 
A more serious problem is illustrated by two phrases highlighted in italics in Figure  1  , " Petitions Documents received . . . ," and its French equivalent , " Prtittons-Drprt de documents . . . ." When we first read the OCR output , we found these two expressions somewhat confusing  , and didn't understand why they ended up in such different places in the OCR output  . 
After inspecting the original hard copy , we realized that they were footnotes , and that their location in the OCR output depends on the location of the page breaks  . Page breaks are extremely complicated . Most alignment programs don't attempt to deal with issues such as footnotes  , headers , footers , tables , figures and other types of floating displays . 
One might believe that these layout problems could be avoided if only we could obtain the texts in electronic format  . Perhapso . Butironically , electronic formats are also problematic , though for different reasons . 
Figurela : An Example of OCRed English 4 . Agenda PRESIDENT . -Wenow come to the agenda for this week . 
SEAL (5) . -Mr President , I should like to protest most strongly against the fact that there is node bate on topical and urgent subjects on the agenda for this part -session  . I know that his decision was taken by the enlarged Bureau because this is an extraordinary meeting  . Nonetheless , how can we betaken seriously as a Parliament if we are going to consider only internal matters while the world goes on outside ? I would like to ask you to ask the enlarged Bureau to look at how we might have extrasittings in which urgencies would be included  . 
Having said that othe Chair and bearing in mind that there are nour gencies  , I should like to ask the Commission to make statements on two items  . First of all , what action is the Community taking to help the people of Nicaragua  , who have suffered a most enormous natural disaster which has left one-third of the population homeless ? Secondly  , would Commissioner Suth-erl and make a statement on the situation that has aft-sen in the United Kingdom  , where the British Government has subsidized A erospace to the tune of UKL  1 billion by selling them the Royal Ordnance factories at a knock down price and allowing them to asset-strip in order to get this kind of cash ?   ( Protests from the right ) Petitions Documents received-Texts of treaties forwarded by the Council : see minutes  . \[ italics add ed\]No 2-370/6 Debates of the European \ [ . . . \] PRESIDENT . -I think you have just raised about four urgencies in one them  . We cannot allow this . The enlarged Bureau made a decision . This decision came to this House and the House has confirm edit  . This is a special part-session . We have an enormous amount of work to do and I suggest we get on with it  . 
There are a large number of different markup languages  , conventions , implementations , platforms , etc . , many of which are obscure and some of which are proprietary  . In more than one instance , we have decided that the electronic format was more trouble than it was worth  , and have resorted to OCR . Even when we didend up using the electronic format  , much of the markup had to be treated as noise since we haven't been able to build interpreters to handle all of the world's markup languages  , or even a large percentage of them . 

Figurelb : An Example of OCRed French 4 . Ord redujour Le Pr 6sident . -Nouspassons maintenant hl'or-dredujour de cettese maine  . 
Seal(s ) . -( EN > Monsie urle Pr 6sident , jepro-teste 6nergiquement contrele fait quel ' or dm duj our decette session e  pr6voit pasde d6bat   d'actualit6 etd'urgence . Jesais quecette d6cision a 6t6 prise parle Bureau 61argi parce qu ' ils ' agit d ' une session extra ordinaire . N 6 an moins , comment pour rions-nous , entant que Parlement , & reprisau s6rieux sinous nenous occup on sque denospet its probl ~mes internesans nous soucier decuise passe dans lemonde ? Jevoussera is recon-naissant de bien vouloir demander au Bureau  61ar-gi devoircommentous pour rions avoir des s6ances   suppl6mentaims pour a borderles question surgentes . 
Celadit , et puis qu ' iln ' ya pas de probl ~ mesurgents , jevoudra is demander ~ tla Commission de faire des  d6clarations surdeux points . Premie re-merit : quelles actions la Communaut6 env is a ge-t-elle pour venire naide aup euple du Nicaragua  , Pd titton s-D dp St de documents Transmission par le Conseil detextes d ' accords:CE  . proc~s-verb ai . \[ italics add ed\]quivient desubirune immense catastrophenatu-rellelaissant sansabri letiers de la population ? Deuxi ~ mement : le commissaire Sutherland pour-rait-il faireune  d6claration ausujet delasituation cr66e au Royaume-Uniparla d6cision du gouver-nement britanniqued ' accorder ~ tla soci~t6 A erospace une subvention s'61evant hunmilliard delivressterling enluivendant les Royal Ordinance Factories ~ tun prix cade au et enluiper mettant debrader des  616ments d ' act if a find e r6unir des liquidit6s decetordre ? ( Protestations ~ tdroite > Le Pr 6sident . -Jepense quevous venez deparler dequatre urgences enuneseule  . Nousnepouvon sle permettre . Le Bureau 61 argia prisuned 6cision . Cette d6cision a 6t6 transmise ~ l'Assem-bl6e et l'Assembl6e l ' a ent6rin6e   . Lapr ~ sentep ~- riode desessionestune p6riode desessions p~-ciale . Nous avons beau coup de pains urla plan cheet je vous proposed ' a vancer  . 
3. Aligning at the Character Level
Because of the noise issues , we decided to look for an alternative to paragraph-based alignment methods  . 
The resulting program , charalign , works at the characterlevel using an approach inspired by the cognate method proposed in S imard et al  ( 1992 )  . 
Figures 2 show the results of char_align on a sample of Canadian Hansard data  , kindly provided by Simard et al along with al ignments as determined by their panel of  8 judges . Simard et al ( 1992 ) refer to this dataset as the " bard " dataset and their other dataset as the " easy " dataset  , so-named to reflect the fact that the former dataset was relatively more difficult than the latter for the class of alignment methods that they were evaluating  . Figure 2 plots f(x ) as a function of x , where x is a byte position in the English text and f  ( x ) is the corresponding byte position in the French text  , as determined by char_align . For comparison's sake , the plot also shows a straight line connecting the two endpoints of the file  . Note that f ( x ) follows the straight line fairly closely , though there are small but important residuals , which may be easier to see in
Figure 3.
Figure 3 plots the residuals from the straight line . The residuals can be computed as f(x)-cx , where c is the ratio of the lengths of the two files  ( 0 . 91) . The residuals usually have fairly small magnitudes  , rarely more than a few percent of the length of the file  . In Figure 3 , for example , residuals have magnitudes less than 2% of the length of the target file . 
If the residuals are large , or if they show a sharp discontinuity , then it is very likely that the two texts don't match up in some way  ( e . g . , a page/figure is missing or misplaced ) . We have used the residuals in this way to help translators catch potentially embarras-sing errors of this kind  . 
Figure 4 illustrates this use of the residuals for the European Parliamentary text presented in Figure  1  . 
Note that the residuals have relatively large magnitudes  , e . g . , 10% of the length of the file , ' compared with the 2% magnitudes in Figure 3 . 
Moreover , the residuals in Figure 4 have two very sharp discontinuities . The location of these sharp discontinuities is an important diagnostic clue for identifying the location of the problem  . In this case , the discontinuities were caused by the two trouble some footnotes discussed in section  2  . 
_m , +!
II 0   50000   150000   250000 x = Position in English File Figure 2: char_align output on the " Hard " Dataset
Itxo
Ax 0   50000   150000   250000 x = Position in English File
Figure 3: rotated version of Figure 2
II~m050010001500
X = Position in English
Figure 4: Residuals for text in Figure 1 ( large discontinuities correspond to footnotes ) o
IIxr ~ 0   50000   150000   250000 x = Position in English File Figure 5: Figure 3 with judges ' alignments " Hard " Dataset -200   -100   0   100 
Error ( in characters)
Figure 6: histogram of errors-200-1000 100
Error ( in characters)
Figure 5 shows the correct alignments , as determined by Simar detars panel of 8 judges ( sampled at sentence boundaries )  , superimposed over char_align's output . Char_align's results are so close to the judge's alignments that it is hard to see the differences between the two  . Char_align's errors may be easier to see in Figure  6  , which shows a histogram of charalign's errors .   ( Errors with an absolute value greater than 200 have been omitted ; less than 1% of the data fall into this category . ) The errors ( 2_+46 bytes ) are much smaller than the length of a sentence ( 129_+84 bytes )  . Half of the errors are less than 18 characters . 
In general , performance is slightly better on shorter files than on longer files because char_align doesn't use paragraph boundaries to break up long files into short chunks  . Figure 7 shows the errors for the " easy " dataset ( -1___57 bytes )  , which ironically , happens to be somewhat harder for char_align because the " easy " set is  2  . 75 times longer than the " hard " dataset . ( As in Figure 6 , errors with an absolute value greater than 200 have been omitted ; less than 1% of the data fall into this category . ) How does char_align work ? The program assumes that there will often be quite a number of words nearx that will be the same as  , or nearly the same as some word near f(x ) . This is especially true for historically related language pairs such as English and French  , which share quite a number of cognates , e . g . , government and gouvernement , though it also holds fairly well for almost any language pair that makes use of the Roman alphabet since there will usually be a fair number of proper nouns  ( e . g . , surnames , company names , place names ) and numbers ( e . g . , dates , times ) that will be nearly the same in the two texts . We have found that it can even work on some texts in English and Japanese such as the AWK manual  , because many of the technical terms ( e . g . , awk , BEGIN , END , get line , print , pring3 are the same in both texts . We have also found that it can work on electronic texts in the same markup language  , but different alphabets ( e . g . , English and Russian versions of 5ESS ? telephone switch manuals , formatted introff ) . 
Figures 8 and 9 below demonstrate the cognate property using a scatter plot technique which we call dotplots  ( Church and Helfman , to appear ) . The source text ( Nxbytes ) is concatenated to the target ext ( Nybytes ) to form a single input sequence of Nx+Ny bytes . A dot is placed in position i , j whenever the input token at position i is the same as the input token at position j  .   ( The origin is placed in the upper left corner for reasons that need not concern us here  . ) Various signal processing techniques are used to compress dotplots for large Nx+Ny  . The implementation of dotplots are discussed in more detail in section  7  . 
The dotplots in Figures 8 and 9 look very similar , with diagonalines superimposed over squares , though the features are somewhat sharper in Figure  8 because the input is much larger . Figure 8 shows a dotplot of 3 years of Canadian Hansards ( 37 million words ) in English and French , tokenized by words . Figure 9 shows a dotplot of a short article ( 25k bytes ) that appeared in a Christian Science magazine in both English and German  , tokenized into 4grams of characters . 
The diagonals and squares are commonly found in dotplots of parallel text  . The squares have a very simple explanation . The upper-left quadrant and the lower-right quadrant are darker than the other two quadrants because the source text and the target ext are more themselves than either is like the other  . This fact , of course , is not very surprising , and is not particularly useful for our purposes here  . However , the diagonal line running through the upper-right quadrant is very important  . This line indicates how the two texts should be aligned  . 
Figure 10 shows the upper-fight quadrant of Figure 9  , enhanced by standard signal processing techniques  ( e . g . , low pass filtering and thresholding ) . The diagonal line in Figure 10 is almost straight , but not quite . The minor deviations in this line are crucial for determining the alignment of the two texts  . Figures 11 and 12 make it easier to see these deviations by first rotating the image and increasing the vertical resolution by an order of magnitude  . The alignment program makes use of both of these transformation in order to track the alignment path with as much precision as possible  . 
"~!! . ~ . ~ . ,,  . . ? . ~ :~ . , . ~-: .  ", . .~,, ~,: . ;:  . :~: ?:  . ' : : : , (~ i . ;~ '  . ! ~ J': . ,:: . "- < :',',: . -;: . ~ :  . ~," '! ',: ? ~;":~"-"," '~:" :: . ii ! ~: . " . i:;,?~'Z'~; .  :; . : . ~ i  ~ . '-~ ::~ . i ~; . ' . !::' . : . ?" : ~ ,~< .  :  .   .  : ' ;  . < : i ~; . ~<:"~ ' :~-  . :'", P~I ~': ~:: i::,' . ; , Figure 8: A dotplot demonstrating the cognate property ( 37 million words of Canadian Hansards )  \ [~#~ . _l ' % ~ i ~ ' ~ l ~ . lgLl~li ~'/ ~ .   . ?  . ~ . ~ .  : "~ ' : ~*~ .  ' "  .   . ! . . ~ : " "~ t ,~: ' , ~"' . : "< : 7~"- L ? ~ . "~ . '5,' ' : ~  .  -  .   .   .  ?  .  ~  . r  ~ .   .   .   .   .   .   .   .  ;  .   .   .   .   .   .   .   .   .   .   .   .  ~ . ~, . . r .   .   .   .   .   .   .   . . ~," , . .,  .   .   . 
,~ .  ~ . ~: ~ miu ~ ~, ~: . . ~- '-:~ . :,~, ~ . w .  , ' ,  .  , :~ " ,~: ' : . ~ W  ~ . ' . .'= ~,  . ~  .  ~' . . . , r ~! . ~: L ; . ~ . :; : i : ~ i , ~, ll ~, N~l ~ l . ~ rgr ~ - - - - a ~ . _ ~ . 
Figure 9: A dotplot demonstrating the cognate property ( 25k bytes selected of Christian Science material ) \[\] lalhow r ~ -- lr ~ --~- o
I " I
I , *! ? o
I -
O@OI % xo _',"!
Figure 10: Upper-right quadrant of Figure 9 ( enhanced by signal processing )   . ? Figure 11: Rotated version of Figure 10 current best estimate of the position in the target file that corresponds to position x in the source file  . On subsequentierations , the bounds are reduced as the algorithm obtains tighter estimates on the dynamic range of the signal  . The memory that was saved by shrinking the bounds in this way can now be used to enhance the horizontal resolution  . We keep iterating in this fashion as long as it is possible to improve the resolution by tightening the bounds on the signal  . 
while ( making_progress )
Estimate_Bounds : Bn~n,Bm~x
Estimate_Resolution_Factor : r

Compute_Alignment_Path
Figure 13 shows the four iterations that were required for the Christian Science text  . For expository convenience , the last three iterations were enhanced with a low pass filter to make it easier to see the signal  . 
??~, gl . ?41"*',' i "", ? . ~,~ .   . .  . . "  .   .   .   . ~ , Figure 12: Figure 11 with 10x gain on vertical axis 5  . Bounds Estimation It is difficult to know in advance how much dynamic range to set aside for the vertical axis  . Setting the range too highwastes memory , and setting it too low causes the signal to be clipped  . We use an iterative solution to find the optimal range  . On the first iteration , we set the bounds on the search space , Brain and Brnax , very wide and see where the signal goes . 
The search will consider matching any by texin the source file with some byte in the target file between f  ( x ) -Bn , an and f(x ) + Bmax , where f(x ) is the ? ~ . -' a *, . , f t .  : ,4  .  ,  . , ~ e#p .  ? ?  .   . ~  .  ?  .  ,  . ~ .  -3 - _~  .   . 
? mr;-;':"- .  "  . I "'"," .  , ,  .  -  .  ~  .   .  - ;  .   .   .   .   .  ; ;  .   .  ? ,4  .  _  .  ~  .   .  - _  .   . ~-  .  - _ '  .  :  .   .   .   .   .  "  .   . 
?  .  4  . o ; -: ' ~ .  " -  . ' U ' ~ .   .   .   .  ~" - " ? " - ~ :*  .  - - : - -  . 4" -~ .   .  ?  .   . ~" ? - _ '2"  .   .   .   .   .   .   .  "  .   . 
Figure 13: Four iterations 6 . Resolution Factor Estimation We need to allocate an array to hold the dots  . Ideally , we would like to have enough memory so that no two points in the search space corresponded to the same cell in the array  . That is , we would like to allocate the dotplot array with a width of w = Nx + Ny and a height of h = B max + Bmin  . ( The array is stored in rotated coordinates . ) Unfortunately , this is generally not possible . Therefore , we compute a " resolution " factor , r , which indicates how much we have to compromise from this ideal ? The resolution factor  , r , which depends on the available . amount of memory M , indicates the resolution of the dotplot array in units of bytes per cell  . 
\]( Nx+Ny ) ( Bmax+Brain)r = M
The dotplot array is then allocated to have a width of 
Nx+NyBmax + Bminw = and a height of h-rr The dots are then computed  , followed by the path , which is used to compute tighter bounds , if possible . 
As can be seen in Figure 13 , this iteration has a tendency to start with a fairly square dotplot and generate ever wider and wider dotpiots  , until the signal extends to both the top and bottom of the dotplot  . 
In practice , the resolution places a lower bound on the error rate  . For example , the alignments of the " easy " and " hard " datasets mentioned above had resolutions of  45 and 84 bytes percell on the final iterations . It should not be surprising that the error rates are roughly comparable  , ?46 and __ . 57 bytes , respectively . Increasing the resolution would probably reduce the error rate  . This could be accomplished by adding memory ( M ) or by splitting the input into smaller chunks ( e . g . , parsing into paragraphs ) . 
7. Dotplot Calculation
In principle , the dotplot could be computed by simply iterating through all pairs of positions in the two input files  , x and y , and testing whether the 4gram of characters in text x starting at position i are the same as the  4gram of characters in text y starting at position j . 
float dotplot\[Nx\]\[Ny\] ; for ( i = 0 ; i < Nx ; i ++) for ( j = 0 ; j < Ny ; j ++) if ( chars4(x , i ) == chars4(y , dotplot\[i \]\[ j\]=i ; else do tplot\[i \]\[ j \] = 0 ; j )) In fact , the dotplot calculation is actually somewhat more complicated  . First , as suggested above , the dotplot is actually stored in rotated coordinates  , with a limited resolution , r , and b and limited between Bmin and Bmax . These heuristics are necessary for space considerations  . 
In addition , another set of heuristics are used to save time . The dots are weighted to adjust for the fact that some matches are much more interesting than others  . 
Matches are weighted inversely by the frequency of the token  . Thus , low frequency tokens ( e . g . , content words ) contribute more to the dotplot than high frequency tokens  ( e . g . , function words ) . This weighting improves the quality of the results  , but more importantly , it makes it possible to save time by ignoring the less important dots  ( e . g . , those 100) . This heuristic is extremely important , especially for large input files . See Church and Helfman ( to appear ) for more details and fragments of c code . 
8. Alignment Path Calculation
The final step is to find the best path of dots . A suboptimal heuristic search ( with forward pruning ) is used to find the path with the largest average weight  . That is , each candidate path is scored by the sum of the weights along the path  , divided by the length of the path , and the candidate path with the best score is returned  . Admittedly , this criterion may seem a bit ad hoc , but it seems to work well in practice . It has the desirable property that it favors paths with more matches over paths with fewer matches  . It also favors shorter paths over longer paths . It might be possible to justify the optimization criterion using a model where the weights are interpreted as variances  . 
9. Conclusion
The performance of charalign is encouraging . The error rates are often very small , usually well within the length of a sentence or the length of a concordance line  . The program is currently being used by translators to produce bilingual concordances for terminology research  . For this application , it is necessary that the alignment program accept noisy  ( realistic ) input , e . g . , raw OCR output , with little or no manual cleanup . It is also highly desirable that the program produce constructive diagnostics when confronted with texts that don't align very well because of various snafus such as missing and/or misplaced pages  . Charalign has succeeded in meeting many of these goals because it works at the characterlevel and does not depend on finding sentence and/or paragraph boundaries which are surprisingly elusive in realistic applications  . 

Brown , P . , J . Cocke , S . Della Pietra , V . Della Pietra , F . Jelinek , J . Lafferty , R . Mercer , and P . Rooss in ,   ( 1990 ) " A Statistical Approach to Machine Translation , " Computational Linguistics , vol . 16, pp . 

Brown , P . , Lai , J . , and Mercer , R . (1991) " Aligning Sentences in Parallel Corpora , " ACL-91 . 
Church , K . and Helfman , J .   ( to appear ) " Dotplot : A Program for Exploring Self -Similarity in Millions of Lines of Text and Code  , " The Journal of Computational and Graphical Statistics  , also presented at lnterface-92 . 
Gale , W . , and Church , K .   ( to appear ) "A Program for Aligning Sentences in Bilingual Corpora  , " Computational Linguistics , also presented at ACL-91 . 
Isabelle , P . (1992) " Bi-Textual Aids for Translators , " in Proceedings of the Eigth Annual Conference of the UWC entre for the New OED and Text Research  , available from the UWC entre for the New OED and Text Research  , University of Waterloo , Waterloo , 
Ontario , Canada.
Kay , M . and R/Ssenschein , M . ( to appear ) " Text-Translation Alignment , " Computational Linguistics . 
Klavans , J . , and Tzoukermann , E . , (1990) , " The BICORD System , " COLING 90 , pp 174-179 . 
Simard , M . , Foster , G . , and Isabelle , P .   ( 1992 ) " Using Cognates to Align Sentences in Bilingual Corpora  , " Fourth International Conference on Theoretical and Methodological Issues in Machine Translation  ( TMI-92 )  , Montreal , Canada . 
Warwick-Armstrong , S . and G . Russell ( 1990 ) " Bilingual Concordancing and Bilingual Lexicography  , " Euralex . 

