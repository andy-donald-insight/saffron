Machine Transliteration
Kevin Knight and Jonathan Graehl
Information Sciences Institute
University of Southern California
Marina del Rey , CA 90292
knight~isi , edu , graehl@isi , edu
Abstract
It is challenging to translate names and technical terms across languages with different alphabets and sound inventories  . These items are commonly transliterated , i . e . , replaced with approximate phonetic equivalents . 
For example , computer in English comes out as ~ i/l:::'=--~- -   ( konpyuutaa ) in Japanese . 
Translating such items from Japanese back to English is even more challenging  , and of practical interest , as transliterated items make up the bulk of text phrases not found in bilingual dictionaries  . We describe and evaluate a method for performing backwards transliterations by machine  . This method uses a generative model , incorporating several distinct stages in the transliteration process  . 
1 Introduction
Translators must deal with many problems , and one of the most frequent is translating proper names and technical terms  . For language pairs like Spanish/English , this presents no great challenge : a phrase like Antonio Gilusually gets translated as Antonio Gil  . However , the situation is more complicated for language pairs that employ very different alphabets and sound systems  , such as Japanese/English and Arabic/English . Phonetic translation across these pairs is called transliteration  . We will look at Japanese/English transliteration in this paper  . 
Japanese frequently imports vocabulary from other languages  , primarily ( but not exclusively ) from English . It has a special phonetic alphabet called katakana  , which is used primarily ( but not exclusively ) to write down foreign names and loan words . To write a word like golf bagin katakana , some compromises must be made . For example , Japanese has no distinct L and R sounds : the two English sounds collapse onto the same Japanese sound  . 
A similar compromise must be struck for English H and F  . Also , Japanese generally uses an alternating consonant -vowel structure  , making it impossible to pronounce LFB without intervening vowels  . Katakana writing is a syllabary rather than an alphabet--there is one symbol for ga  ( ~ I )  , another for gi (4e ) , another for gu(P') , etc . So the way to write gol\]baginkatakana is = ~' ~ 7  ~ ~ ,  ~ , roughly pronounced goruhu baggu . Here are a few more examples :
Angela Johnson
TvzJ~?"J ~ vYv(anjirajyonson )
New York Times ( nyuu yookutaimuzu ) ice cream
T4x ~, ~) -- z , ( aisu kuriimu)
Notice how the transliteration is more phonetic than orthographic  ; the letter h in Johnson does not produce any katakana  . Also , a dot-separator ( . ) is used to separate words , but not consistently . And transliteration is clearly an information -losing operation : a isukuriimuloses the distinction between ice cream and I scream  . 
Transliteration is not trivial to automate , but we will be concerned with an even more challenging problem--going from katakana back to English  , i . e . , back-transliteration . Automating back-transliteration has great practical importance in Japanese/English machine translation  . Katakana phrases are the largest source of text phrases that do not appear in bilingual dictionaries or training corpora  ( a . k . a . " not-found words ") . However , very little computational work has been done in this area  ; ( Yamron et al ,  1994 ) briefly mentions a pattern-matching approach , while ( Arbabi et al ,  1994 ) discuss a hybrid neural-net/expert-system approach to  ( forward ) transliteration . 
The information-losing aspect of transliteration makes it hard to invert  . Here are some problem instances , taken from actual newspaper articles : 1 I Texts used in ARPA Machine Translation evaluations  , November 1994 . 

T - - x ~ - - ( aasudee )  '9  ( roba atoshyo on renaado ) ?"~':~--:~" l .  - -  ) - ~ yI- ( masu ~ aazu  ~ oonamen~o ) English translations appear later in this paper . 
Here are a few observations about back -transliteration : ? Back transliteration is less for giving than transliteration  . There are many ways to write an English word like switch in katakana  , all equally valid , but we do not have this flexibility in the reverse direction  . For example , we cannot drop the t in switch , nor can we write arture when we mean archer . 
? Backtransliteration is harder than romanization , which is a ( frequently invertible ) transformation of a non-roman alphabet into roman letters  . There are several romanization schemes for katakana writing -- we have already been using one in our examples  . Katakana Writing follows Japanese sound patterns closely  , so katakana often doubles as a Japanese pronunciation guide  . However , as we shall see , there are many spelling variations that complicate the mapping between Japanese sounds and katakana writing  . 
? Finally , not all katakana phrases can be " sounded out " by back-transliteration  . Some phrases are shorthand , e . g . , r \] _ 7" ~ ( uaapuro ) should be translated as word processing . Others are onomatopoetic and difficult to translate  . 
These cases must be solved by techniques other than those described here  . 
The most desirable feature of an automatic back -transliterator is accuracy  . If possible , our techniques should also be : ? portable to new language pairs like Ara-bic/English with minimal effort  , possibly reusing resources . 
? robust against errors introduced by optical character recognition  . 
? relevant to speech recognition situations in which the speaker has a heavy foreign accent  . 
? able to take textual ( topical/syntactic ) on text into account , or at least be able to return a ranked list of possible English translations  . 
Like most problems in computational linguistics , this one requires full world knowledge for a 100% solution . Choosing between Katarina and Catalina ( both good guesses for ~'~~" ) - ) might even require detailed knowledge of geography and figure skating  . 
At that level , human translators find the problem quite difficult as well  . so we only aim to match or possibly exceed their performance  . 
2 A Modular Learning Approach
Bilingual glossaries contain many entries mapping katakana phrases onto English phrases  , e . g . : ( aircraft carrier --, ~ T~~7I .  ~ ~ ~3 7" ) . It is possible to automatically analyze such pairs to gain enough knowledge to accurately map new katakana phrases that come along  , and learning approach travels well to other languages pairs  . However , a naive approach to finding direct correspondences between English letters and katakana symbol suffers from a number of problems  . One can easily wind up with a system that proposes is krym as a back-transliteration of a isukuriimu  . Taking letter frequencies into account improves this to a more plausible-looking is clim  . 
Moving to real words may give is crime : the i corresponds to ai  , the scorresponds to su , etc . Unfortunately , the correct answer here is ice cream . After initial experiments along these lines , we decided to step back and build a generative model of the transliteration process  , which goes like this : 1 . An English phrase is written . 
2. A translator pronounces it in English.
3. The pronunciation is modified to fit the
Japanese sound inventory.
4. The sounds are converted into katakana.
5. Katakana is written.
This divides our problem into five subproblems.
Fortunately , there are techniques for coordinating solutions to such subproblems  , and for using generative models in the reverse direction  . These techniques rely on probabilities and Bayes ' Rule  . Suppose we build an English phrase generator that produces word sequences according to some probability distribution P  ( w )  . And suppose we build an English pronouncer that takes a word sequence and assigns it a set of pronunciations  , again probabilistically , according to some P ( plw ) . Given a pronunciation p , we may want to search for the word sequence w that maximizes P  ( wtp )  . Bayes " Rulelets use quivalently maximize P(w ) . P ( plw) . exactly the two distributions we have modeled . 
Extending this notion , we settled down to build five probability distributions :  1  . P ( w ) -- generates written English word sequences . 
2. P ( elw)--pronounces English word sequences.
3 . P ( jle ) -- converts English sounds into Japanese sounds . 
129 4 . P ( k\[j ) ~converts Japanese sounds to katakana writing . 
5 . P ( ok ) ~ introduces misspellings caused by optical character recognition  ( OCR )  . 
Given a katakana string o observed by OCR , we want to find the English word sequence w that maximizes the sum  , overall e , j , and k , of P ( w ) ? P ( e\[w ) . P ( jle ) " P ( kJj) . P ( olk ) Following ( Pereira et al , 1994; Pereira and Riley , I996) , we implement P ( w ) in a weighted finite-state a ceeptor ( WFSA ) and we implement the other distributions in weighted finite-state transducers  ( WFSTs )  . A WFSA is an state/transition diagram with weights and symbols on the transitions  , making some output sequences more likely than others  . A WFST is a WFSA with a pair of symbols on each transition  , one input , and one output . Inputs and outputs may include the empty symbol e  . Also following ( Pereira and Riley ,  1996) , we have implemented a general composition algorithm for constructing an integrated model P  ( zlz ) from models P ( ~IY ) and P ( ylz )  , treating WFSAs as WFSTs with identical inputs and outputs  . We use this to combine an observed katakana string with each of the models in turn  . The result is a large WFSA containing all possible English translations  . We use Dijkstra's shortest-path algorithm Dijkstra  , 1959) to extract the most probable one . 
The approach is modular . We can test each engine independently and be confident hat their results are combined correctly  . We do no pruning , so the final WFSA contains every solution , however unlikely . The only approximation is the Viterbi one , which searches for the best path through a WFSA instead of the best sequence  ( i . e . , the same sequence does not receive bonus points for appearing more than once  )  . 
3 Probabilistic Models
This section describes how we desigued and built each of our five models  . For consistency , we continue to print written English word sequences in italics  ( golf ball )  , English sound sequences in all capitals ( GAAL FBA0L )  . Japanese sound sequences in lower case ( goruhubooru ) and katakana sequences naturally ( =': t .  7  . ~- ~) . 
3.1 Word Sequences
The first model generate scored word sequences , the idea being that ice cream should score higher than ice creme  , which should score higher than nice kreem . We adopted a simple unigram scoring method that multiplies the scores of the known words and phrases in a sequence  . Our 262 , 0 00-entry frequency list draws its words and phrases from the Wall Street Journal corpus  , an online English name list , and an online gazeteer of place names . " A portion of the WFSA looks like this : los/0 . 000087 federal/O . O013 ~ angeles ~ ~ month 1 0 . 0 00992 An ideal word sequence model would look a bit different  . It would prefer exactly those strings which are actually grist for Japanese transliter a-tots  . For example , people rarely transliterate auxiliary verbs , but surnames are often transliterated . 
We have approximated such a model by removing high -frequency words like has  , an , are , am , were , their , and does , plus unlikely words corresponding to Japanese sound bites  , like coup and oh . 
We also built a separate word sequence model containing only English first and last names  . If we know ( from context ) that the transliterated phrase is a personal name  , this model is more precise . 
3.2 Words to English Sounds
The next WFST converts English word sequences into English sound sequences  . We use the English phoneme inventory from the online CMU Pronunciation Dictionary  , 3 minus the stress marks . This gives a total of 40 sounds , including 14 vowel sounds ( e . g . , AA , AE , UW ), 25 consonant sounds ( e . g . , K , 1tlt , It ) , plus our special symbol ( PAUSE ) . The dictionary has pronunciations for 110 , 000 words , and we organized a phoneme-tree based WFST from it : 
E:E : E
E:IH ?; : : K
Note that we insert an optional PAUSE between word pronunciations  . Due to memory limitations , we only used the 50 , 000 most frequent words . 
We originally thought to build a general letter-to -sound WFST  , on the theory that while wrong ( over generalized ) pronunciations might occasionally be generated , Japanese transliterators also mispro-nounce words  . However , our letter-to-sound WFST did not match the performance of Japanese  translit-2Available from the ACL Dat ~ Collection Initiative . 
3ht % p://~ww . speech , cs . cmu . edu/cgi-bin/cmudict . 
130 erators , and it turns out that mispronunciations are modeled adequately in the next stage of the cascade  . 
3.3 English Sounds to Japanese Sounds
Next , we map English sound sequences onto Japanese sound sequences  . This is an inherently information-losing process , as English R and L sounds collapse onto Japanese r  , the 14 English vowel sounds collapse onto the 5 Japanese vowel sounds , etc . We face two immediate problems : 1 . What is the target Japanese sound inventory ?2 . How can we build a WFST to perform the sequence mapping ? An obvious target inventory is the Japanese syllabary itself  , written down in katakana ( e . g . , ") or arom an equivalent ( e . g . , hi ) . With this approach , the English sound K corresponds to one of 2 ( ka )  , -' Y(ki ) , ~'( ku ) , ~( ke ) , or = ( ko ) , depending on its context . Unfortunately , because katakana is a syllabary , we would be unable to express an obvious and useful generalization  , amely that English gusually corresponds to Japanesek  , independent of context . Moreover , the correspondence of Japanese katakana writing to Japanese sound sequences in ot perfectly one -to-one  ( see next section )  , so an independent sound inventory is well -motivated in any case  . Our Japanese sound inventory includes 39 symbols : 5 vowel sounds , 33 consonant sounds ( including doubled consonants like kk )  , and one special symbol ( pause ) . An English sound sequence like ( PROWPAUSESAAKER ) might map onto a Japanese sound sequence like ( puropa uses a kkaa )  . Note that long Japanese vowel sounds are written with two symbols  ( aa ) instead of just one ( an )  . This scheme is attractive because Japanese sequences are almost always longer than English sequences  . 
Our WFST is learned automatically from 8 , 000 pairs of English/Japanese ound sequences , e . g . , (( sAAKER ) --* ( sakkaa)) . We were able to pro-duce ' these pairs by manipulating a small English-katakana glossary  . For each glossary entry , we converted English words into English sounds using the previous section's model  , and we converted katakana words into Japanese sounds using the next section's model  . We then applied the estimation-maximization ( EM ) algorithm ( Baum , 1972) to generate symbol-mapping probabilities , shown in Figure 1 . Our EM training goes like this : 1 . For each English/Japane sequence pair , compute all possible alignments between their elements  . In our case . an alignment is a drawing . that connects each English sound with one or more Japanese sounds  , such that all Japanese sounds are covered and no lines cross  . For example , there are two ways to align the pair (( L
OW ) <-> ( roo)):
LOW LOW l/\/\I roor oo 2 . For each pair , assign an equal weight to each of its alignments , such that those weights sum to 1 . In the case above , each alignment gets a weight of 0 . 5 . 
3 . For each of the 40 English sounds , count up instances of its different mappings , as observed in all alignments of all pairs . Each alignment contributes counts in proportion to its own weight  . 
4 . For each of the 40 English sounds , normalize the scores of the Japanese sequences it maps to  , so that the scores sum to 1 . These are the symbol-mapping probabilities shown in Figure  1  . 
5 . Recompute the alignment scores . Each alignment is scored with the product of the scores of the symbol mappings it contains  . 
6 . Normalize the alignment scores . Scores for each pair's alignments should sum to 1 . 
7 . Repeat 36 until the symbol-mapping probabilities converge . 
We then build a WFST directly from the symbol -mapping probabilities : 

AA : a/0024 ~ AA : o / 0,018 o<--o
Our WFST has 99 states and 283 arcs.
We have also built models that allow individual English sounds to be " swallowed "  ( i . e . , produce zero Japanese sounds ) . However , these models are expensive to compute ( many more alignments ) and lead to a vast number of hypotheses during WFST composition  . Furthermore , in disallowing " swallowing , " we were able to automatically remove hundreds of potentially harmful pairs from our training set  , e . g . , (( BAARBERSHAAP)--(baabaa)) . Because no alignments are possible , such pairs are skipped by the learning algorithm ; cases like these must be solved by dictionary lookup anyway  . Only two pairs failed to align when we wished they had--both involved turning English YUW into Japaneseu  , as in (( YUWKAHLEY LIY)~(ukurere)) . 
Note also that our model translates each English sound without regard to context  . We have built also context-based models , using decision trees receded as WFSTs . For example , at the end of a word , English T is likely to come out as ( = o ) rather than ( 1 ; ) . 
However , context-based models proved unnecessary o0 . 566 a 0 . 382 aa 0 . 024 oo 0 . 018
AEa 0.942 ya 0.046
AHa 0.486o 0.169e 0.134 i0. II Iu 0.076
AOo 0.671 oo 0.257 a 0.047
A Wau 0.830 aw 0.095 oo 0.027 ao 0.020 a 0.014
AYai 0.864i 0.073 a 0.018 aiy 0.018
Bb 0.802 bu 0.185
CH chy 0 . 277 ch 0 . 240 tchi 0 . 199 chi 0 . 159 tch 0 . 038 chyu 0 . 021 t chy 0 . 020
DHd 0 . 535 do 0 . 329 ddo 0 . 053 j 0 . 032 z 0 . 670 zu 0 . 125 j 0 . 125 az 0 . 080
EHe 0.901 a 0.069
ER aa 0 . 719 a 0 . 081 ar 0 . 063 er 0 . 042 or 0 . 029 e

JP ( Jle)ee 0 . 641 a 0 . 122 e 0 . 114 ei 0 . 080 ai 0 . 014
Fh 0.623 hu 0.331 hh 0.019 ahu 0.010
Gg 0.598 gu 0.304 ggu 0.059 gg 0.010
HHh 0.959 w 0.014
IHi 0.908e 0.071
IYii 0.573i 0.317 e 0.074 ee 0.016
JRj 0 . 329 jy 0 . 328 ji 0 . 129 jji 0 . 066 eji 0 . 057 z 0 . 032 g 0 . 018 jj 0 . 012 e0 . 012 k 0 . 528 ku 0 . 238 kku 0 . 150 kk 0 . 043 ki 0 . 015 ky 0 . 012
Lr 0.621 ru 0.362
Mm 0.653 mu 0.207n 0.123 nm 0.011
Nn 0.978
NGngu 0.743n 0.220 ng 0.023 ejP(jIe)
OWo 0.516 oo 0.456 ou 0.011
OYoi0 . 828 ooi 0 . 057 i 0 . 029 oiy 0 . 029 o 0 . 027 ooy 0 . 014 oo 0 . 014
Pp 0.649 pu 0.218 ppu 0.085 pp 0.045
PAUSE pause 1.000
Rr 0 . 661 a 0 . 170 o 0 . 076 ru 0 . 042 ur 0 . 016 ar 0 . 012 su 0 . 539 s 0 . 269 sh 0 . 109 u 0 . 028 ss 0 . 0148 Hshy 0 . 475 sh 0 . 175 sshyu 0 . 166 sshy 0 . 088 shi 0 . 029 ssh 0 . 027 shyu 0 . 015 t 0 . 463 to 0 . 305 t to 0 . 103 ch 0 . 043 tt 0 . 021 ts 0 . 020 tsu 0 . 011
THsu 0 . 418 s 0 . 303 sh 0 . 130 ch 0 . 038 t 0 . 029 ej PUle )
UHu 0.794 uu 0.098dd 0.034 a 0.030 o 0.026
UWuu 0.550u 0.302 yuu 0.109 yu 0.021
Vb 0.810 bu 0.150 w 0.015
Ww0 . 693 u 0 . 194 o 0 . 039 ? 0 . 027 a 0 . 015 e 0 . 012 y 0 . 652 i 0 . 220 yu 0 . 050 u 0 . 048 b 0 . 016 z 0 . 296 zu 0 . 283 j 0 . 107 su 0 . 103 u 0 . 073 a 0 . 036 o 0 . 018 s 0 . 015 n 0 . 013 i 0 . 011 sh 0 . 011
ZH jy 0 . 324 shi 0 . 270 ji 0 . 173 j 0 . 135 ajyu 0 . 027 shy 0 . 027 s 0 . 027 aji 0 . 016 Figure 1: English sounds ( in capitals ) with probabilistic mappings to Japanese sound sequences  ( in lower case )  , as learned by estimation-maximization . Only mappings with conditional probabi lities greater than  1% are shown , so tile figures may not sum to 1 . 
132 for back-transliteration .   4 They are more useful for English-to-Japanese forward transliteration  . 
3.4 Japanese sounds to Katakana
To map Japanese sound sequences like ( moo 1: a a ) onto katakana sequences like ( ~--$ t-- )   , we manually constructed two WFSTs . Composed together , they yield an integrated WFST with 53 states and 303 arcs . The first WFST simply merges long Japanese vowel sounds into new symbol saa  , ii , uu , e e , and oo . The second WFST maps Japanese sounds onto katakana symbols  . The basic idea is to consume a whole syllable worth of sounds before producing any katakana  , e . g . : :-: , 0 951 This fragment shows one kind of spelling variation in Japanese : long vowel sounds  ( oo ) are usually written with a long vowel mark ( ~-  ) but are sometimes written with repeated katakana ( ~ )   . 
We combined corpus analysis with guidelines from a Japanese textbook  ( Jorden and Chaplin ,  1976 ) to turn up many spelling variations and unusual katakana symbols : ? the sound sequence  ( j ? ) is usually written ~ , but occasionally ?: . 
?(gua ) is usually ~' T , but occasionally YT . 
?( woo ) is variously ~ z'--- , ~ r - , or with a special , old-style katakana for wo . 
?( ye ) may be = I = , d ~, or d ~.
?(wi ) is either #~" or ~4.
?( nye ) is a rare sound sequence , but is written - ~* when it occurs . 
? (1: yu ) is rarer than ( chyu ) , but is written ~-~- when it occurs . 
and so on.
Spelling variation is clearest in cases where an English word likes wi Ieh shows up transliterated variously  ( :~ ~" : ,  ?- ,  :~4 ~ ,  ?- , x  ~ ,  4 ~ , 4-) in different dictionaries . Treating these variations as an equivalence class enables us to learn general sound mappings even if our bilingual glossary adheres to a single narrow spelling convention  . We do not , however ,   4And harmfully restrictive in their unsmoothed incarnations  . 
generate all katakana sequences with this model ; for example , we do not output strings that begin with a subscripted vowel katakana  . So this model also serves to filter out some ill -formed katakana sequences  , possibly proposed by optical character recognition  . 
3.5 Katakanato OCR
Perhaps uncharitably , we can view optical character recognition ( OCR ) as a device that garbles perfectly good katakana sequences  . Typical confusions made by our commercial OCR system include ~ for ~-'  , ?- for -)' , T for 7 , and 7 for 7" . To generate pre-OCR text , we collected 19 , 500 characters worth of katakana words , stored them in a file , and printed them out . To generate post-OCR text , we OCR'd the print outs . We then ran the EMM gorithm to determine symbol -mapping  ( " garbling " ) probabilities . 
Here is part of that table : koP ( o\[k )  ~:" ~:" 0 . 492 ~" O . 434 0 . 042 7 0 . 011 ~" ~" 1 . 000  . , ~ z , 0 . 964 \ ] , 0 . 0 36 This model outputs a superset of the 81 katakana symbols , including spurious quote marks , alphabetic symbols , and the numeral 7 . 
4 Example
We can now use the models to do a sample back -transliteration  . We start with a katakana phrase as observed by OCR  . We then serially compose it with the models , in reverse order . Each intermediate stage is a WFSA that encodes many possibilities  . 
The final stage contains all back-transliterations suggested by the models  , and we finally extract hebest one . 
We start with them as utaa zu too namento problem from Section  1  . Our OCR observes : ~ x ~,--;~?1 . ---/- j:/1 . 
This string has two recognition errors : ~' ( ku ) for $ ( ta )  , and ?-( ch ?) for "3-( na ) . We turn the string into a chained 12-state/l l-arc WFSA and compose it with the P ( k\[o ) model . This yields a fatter 12-state/15-arc WFSA , which accepts the correct spelling at a lower probability  . Next comes the P ( jlk ) model , which produces a 28-state/31-arc WFSA whose highest-scoring sequence is : masut a a zu too chimen to Next comes P  ( elj )  , yielding a 62-state/241-arc
WFSA whose best sequence is:
MAEST AEAED HUHTAO AOCHIH MEHNTAO state/4601-arc WFSA whose best sequence ( out of myriads ) is : masters to neamenta we This English string is closest phonetically to the Japanese  , but we are willing to trade phonetic proximity for more sensical English  ; we restore this WFSA by composing it with P ( w ) and extract the best translation : masters tournament  ( Other Section 1 examples are translated correctly as earthday and roberts can leonard  . )  5 Experiments We have performed two largescale xperiments  , one using a full-language P ( w ) model , and one using a personal name language model . 
In the first experiment , we extracted 1449 unique katakana phrases from a corpus of 100 short news articles . Of these , 222 were missing from an online 100 , 000-entry bilingual dictionary . We back-transliterated these 222 phrases . Many of the translations are perfect : technical program  , sezscandal , omahabeach , new york times , ramon diaz . Others are close : t any a harding , nickel simps on , danger washington , world cap . Some miss the mark : nancy care again , plus occur , patriot miss real . While it is difficult to judge overall accuracy- -some of the phases are onomatopoetic  , and others are simply too hard even for good human translators -- its easier to identify system weaknesses  , and most of these lie in the P(w ) model . For example , nancy kerrigan should be preferred over nancy care again  . 
In a second experiment , we took katakana versions of the names of 100U . S . politicians , e . g . :- Jm :/ . 7'=--(jyon . buroo ), T ~/ ~ . 
~'0' I "( a . rhonsu . dama ~;'? o ), and "~'43'?~7, f:/(maiku . de ~ ain ) . We back-transliterated these by machine and asked four human subjects to do the same  . These subjects were native English speakers and news-aware : we gave them brief instructions  , examples , and hints . The results were as follows : correct ( e . g . , spencer abraham / spencer abraham ) phonetically equivalent , but misspelled ( e . g . , richard brian/richard bryan ) incorrect ( e . g . , olin hatch / omen hatch ) human machine 27% 64% 7 , %  12%   66%   24% There is room for improvement on both sides . Being English speakers , the human subjects were good at English name spelling and U  . S . politics , but not at Japanese phonetics . A native Japanese speaker might be expert at the latter but not the former  . 
People who are expert in all of these areas , however , are rare . 
On the automatic side . many errors can be corrected . A first-name/last-name odel would rank richard bryan more highly than richard brian  . A bigram model would prefer or renhatch overolin hatch  . 
Other errors are due to unigram training problems , or more rarely , incorrect or brittle phonetic models . 
For example , " Long " occurs much more often than " R . on " in newspaper text , and our word selection does not exclude phrases like " Long Island  . " So we get long wy den instead oft on wy den . Rare errors are due to incorrect or brittle phonetic models  . 
Still the machine's performance is impressive.
When word separators ( , ) are removed from the katakana phrases , rendering the task exceedingly difficult for people  , the machine's performance is unchanged . When we use OCR . 7% of katakana tokens are misrecognized , affecting 50% of test strings , but accuracy only drops from 64% to 52% . 
6 Discussion
We have presented a method for automatic back -transliteration which  , while far from perfect , is highly competitive . It also achieves the objectives outlined in Section  1  . It ports easily to new language pairs ; the P ( w ) and P ( e\[w ) models are entirely reusable , while other models are learned automatically . It is robust against OCR noise , in a rare example of high-level language processing being useful  ( necessary , even ) in improving low-level OCK . 
We plan to replace our shortest-path extraction algorithm with one of the recently developed k -shortest path algorithms  ( Eppstein ,  1994) . We will then return a ranked list of the kbest translations for subsequent contextual disambiguation  , either by machine or as part of an interactive man-machine system  . We also plan to explore probabilistic models for Arabic/English transliteration  . Simply identifying which Arabic words to transliterate is a difficult task in itself  ; and while Japanese tends to insert extra vowel sounds  , Arabic is usually written without any ( short ) vowels . Finally , it should also be possible to embed our phonetic shift model P  ( jle ) inside a speech recognizer , to help adjust for a heavy Japanese accent , although we have not experimented in this area . 
7 Acknowledgments
We would like to thank Alton Earl I ngram , Yolanda Gil , Bonnie Glover-Stalls , Richard Whitney , and Kenji Yamada for their helpful comments . We would

References
M . Arbabi , S . M . Fischthal , and V . C . Cheng and dE . Bart .  1994 . Algorithms for Arabic name transliteration . IBM J . Res . Develop . , 38(2) . 
L . E . Baum .  1972 . An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process  . In equalities , 3 . 
E . W . Dijkstra .  1959 . A note on two problems in connexion with graphs . Numerische Malhematik , 1 . 
David Eppstein .  1994 . Finding the k shortest paths . 
In Proc . 35th Syrup . Foundations of Computer
Science . IEEE.
E . H . Jorden and H . I . Chaplin .  1976 . Reading Japanese . Yale University Press , New Haven . 
F . Pereira and M . Riley .  1996 . Speech recognition by composition of weighted finite automata  . In preprint , cmp-lg/9603001 . 
F . Pereira , M . Riley , and R . Sproat .  1994 . Weighted rational transductions and their application to human language processing  . In Proe . ARPA Human
Language Technology Workshop.
J . Yamron , J . Cant , A . Demedts , T . Dietzel , and Y . Ito .  1994 . The automatic component of the LING STAT machine -aided translation system  . In Proc . ARPA Workshop on Human Language Technology . 

