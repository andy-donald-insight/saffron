Never Look Back : An Alternative to Centering
Michael Strube
IRCS-Institute for Research in Cognitive Science
University of Pennsylvania
3401 Walnut Street , Suite 400A
Philadelphia PA 19104
Strube@linc , c is . upenn , edu

I propose a model for determining the hearer's attentional state which depend solely on a list of salient discourse entities  ( S-list )  . The ordering among the elements of the S-list covers also the function of the backward-looking center in the centering model  . The ranking criteria for the S-list are based on the distinction between hearer-old and hearer-new discours entities and incorporate preferences for inter - and intrasentential anaphora  . The model is the basis for an algorithm which operates incrementally  , word by word . 
1 Introduction
I propose a model for determining the heater's attentional state in understanding discourse  . My proposal is inspired by the centering model ( Grosz et al ,  1983 ;  1995 ) and draws on the conclusions of Strube&Hahn's ( 1996 ) approach for the ranking of the forward-looking center list for German  . Their approach as been proven as the point of departure for a new model which is valid for English as well  . 
The use of the centering transitions in Brennan et al's  ( 1987 ) algorithm prevents it from being applied incrementally  ( cf . Kehler (1997)) . In my approach , I propose to replace the functions of the backward-looking center and the centering transitions by the order among the elements of the list of salient discours entities  ( S-list )  . The S-list ranking criteria define a preference for hearer-old overhearer-new discourse ntities  ( Prince ,  1981 ) generalizing Strube&Hahn's ( 1996 ) approach . Because of these ranking criteria , I can account for the difference in salience between definite NPs  ( mostly hearer-old ) and indefinite NPs ( mostly hearer-new )  . 
The S-list is not a local data structure associated with individual utterances  . The S-list rather describes the attentional state of the hearer at any given point in processing a discourse  . The S-list is generated incrementally , word by word , and used immediately . Therefore , the S-list integrates in the simplest manner preferences for inter-and intrasentential anaphora  , making further specifications for processing complex sentences unnecessary  . 
Section 2 describes the centering model as the relevant background formy proposal  . In Section 3 , I introduce my model , its only data structure , the S-list , and the accompanying algorithm . In Section 4 , I compare the results of my algorithm with the results of the centering algorithm  ( Brennan et al ,  1987 ) with and without specifications for complex sentences  ( Kameyama ,  1998) . 
2 A Look Back : Centering
The centering model describes the relation between the focus of attention  , the choices of referring expressions , and the perceived coherence of discourse . 
The model has been motivated with evidence from preferences for the antecedents of pronouns  ( Grosz et al ,  1983 ;  1995 ) and has been applied to pronoun resolution ( Brennan et al ( 1987 )  , inter alia , whose interpretation differs from the original model  )  . 
The centering model itself consists of two constructs  , the backward-looking center and the list of forward-looking centers  , and a few rules and constraints . Each utterance U i is assigned a list of forward -looking centers  , Cf(Ui ) , and a unique backward-looking center , Cb(Ui ) . A ranking imposed on the elements of the Cf reflects the assumption that the most highly ranked element of Cf  ( U i )   ( the preferred center Cp ( U i ) ) is most likely to be the Cb ( U i + l )  . The most highly ranked element of Cf ( U i ) that is realized in Ui+x ( i . e . , is associated with an expression that has a valid interpretation in the underlying semantic representation  ) is the Cb ( U i + l )  . Therefore , the ranking on the Cf plays a crucial role in the model  . Grosz et al ( 1995 ) and Brennan et al ( 1987 ) use grammatical relations to rank the Cf ( i . e . , subj- . < obj -< . . . ) but state that other factors might also play a role  . 

Cb(Ui ) =

Cb(Ui)y ?

For their centering algorithm , Brennan et al (1987 , henceforth BFP-algorithm ) extend the notion of centering transition relations  , which hold across adjacent utterances , to differentiate types of shift ( cf . Table 1 taken from Walker et al (1994)) . 
Cb(Ui ) = Cb(Ui-1)Cb(Ui)
OR no Cb(Ui-1)Cb(Vi-1)
CONTINUES MOOTH-SHIFT
RET AIN ROUGH-SHIFT
Table 1: Transition Types
Brennan et al ( 1987 ) modify the second of two rules on center movement and realization which were defined by Grosz et al  ( 1983 ;  1995 ) : Rule 1: If some element of Cf ( Ui-1 ) is realized as a pronoun in Ui , then so is Cb(Ui ) . 
Rule 2" Transition states are ordered . CONTINUE is preferred to RETAIN is preferred to SMO OTH- 
SHIFT is preferred to ROUGH-SHIFT.
The BFP-algorithm ( cf . Walker et al ( 1994 ) ) consists of three basic steps : 1 . GENERATE possible Cb-Cf combinations . 
2 . FILTER by constraints , e . g . , contra-indexing , sortal predicates , centering rules and constraints . 
3. RANK by transition orderings.
To illustrate this algorithm , we consider example (1) ( Brennan et al ,  1987 ) which has two different final utterances ( ld ) and ( ld  ~ )  . Utterance ( ld ) contains one pronoun , utterance ( ldt ) two pronouns . We look at the interpretation of ( ld ) and ( ldt )  . After step 2 , the algorithm has produced two readings for each variant which are rated by the corresponding transitions in step  3  . In ( ld ) , the pronoun " she " is resolved to " her " ( = Brennan ) because the CONTINUE transition is ranked higher than SMOOTH-SHIFT in the second reading  . In ( ld ~) , the pronoun " she " is resolved to " Friedman " because SMOOTH- 
SHIFT is preferred over ROUGH-SHIFT.
(1) a . Brennan drives an Alfa Romeo.
b . She drives too fast.
c . Friedman races her on weekends.
d . Shegoes to Laguna Seca.
d . ' She often beats her.
3 An Alternative to Centering 3 . 1 The Model The realization and the structure of my model departs significantly from the centering model : ? The model consists of one construct with one operation : the list of salient discoursentities  ( S-list ) with an insertion operation . 
? The S-list describes the attentional state of the hearer at any given point in processing a discourse  . 
? The S-list contains some ( not necessarily all ) discours entities which are realized in the current and the previous utterance  . 
? The elements of the S-list are ranked according to their information status  . The order among the elements provides directly the preference for the interpretation of anaphoric expressions  . 
In contras to the centering model , my model does not need a construct which looks back  ; it does not need transitions and transition ranking criteria  . Instead of using the Cb to account for local coherence  , in my model this is achieved by comparing the first element of the S-list with the preceding state  . 
3.2 S-List Ranking
Strube & Hahn ( 1996 ) rank the Cf according to the information status of discoursentities  . I here generalize these ranking criteria by redefining them in Prince's  ( 1981 ; 1992) terms . I distinguish between three different sets of expressions  , hearer-old discourse entities ( OLD ) , mediate discourse entities ( MED ) , and hearer-new discours entities ( NEW ) . 
These sets consist of the elements of Prince's familiarity scale  ( Prince ,  1981 , p . 245) . OLD consists of evoked ( E ) and unused ( U ) discours entities while NEW consists of brand -new  ( BN ) discourse entities . MED consists of inferrables ( I) , containing inferrables ( Ic ) and anchored brand-new ( BNA ) discoursentities . These discours entities are discourse-new but mediated by some hearer-oM discours entity  ( cf . Figure 1) . I do not assume any difference between the elements of each set with respect to their information status  . E . g . , evoked and unused iscourse ntities have the same information status because both belong to OLD  . 
For an operationalization fPrince's terms , I stipulate that evoke discoursentitites are coreferring expressions  ( pronominal and nominal anaphora , previously mentioned proper names , relative pronouns , appositives ) . Unused discours entities are Figure 1: S-list Ranking and Familiarity proper names and titles  . In texts , brand-new proper names are usually accompanied by a relative clause or an appositive which relates them to the hearer's knowledge  . The corresponding discoursentity is evoked only after this elaboration  . Whenever these linguistic devices are missing , proper names are treated as unused I . I restrict inferrables to the particular subset defined by Hahn et al  ( 1996 )  . Anchored brand-new discours entities require that the anchor is either evoked or unused  . 
I assume the following conventions for the ranking constraints on the elements of the S-list  . The 3-tuple ( x , utt x , posz ) denotes a discoursentity x which is evoked in utterance utt z at the text position pos z  . With respecto any two discourse n-tities(x , utt z , posz ) and ( y , utty , pOSy ) , utt z and utty specifying the current utterance Ui or the preceding utterance U/_  1  , I setup the following ordering constraints on elements in the S-list  ( Table 2 )  2  . 
For any state of the processor/hearer , the ordering of discours entities in the S-list that can be derived from the ordering constraints  ( 1 ) to ( 3 ) is denoted by the precedence rlation --< . 
(I ) If xEOLD and yEMED , then x-~y.
If xEOLD and yENEW , then x-<y.
lfxEMED and yENEW , then x-<V.
(2) If x , y EOLD , or x , vEMED , or x , yENEW , then if utt x >- utt ~ , then x-<y , if utt z = utt ~ and pos ~< pos ~ , then x-<y . 
Table 2: Ranking Constraints on the S-list Summarizing Table  2  , I state the following preference ranking for discours entities in Ui and Ui-l:hearer-oM discours entities in Ui  , hearer-old is-course entities in Ui-1 , mediate discours entities in Ui , mediate discours entities in Ui-1 , hearer-new discours entities in Ui , hearer-new discourse entities in Ui-1 . By making the distinction in ( 2 ) ~ For examples of brand-new proper names and their introduction cf  . , e . g . , the " obituaries " section of the New York Times . 
2The relations >- and = indicate that the utterance containing x follows  ( >- ) the utterance containing y or that x and y are elements of the same utterance  ( = )  . 
between discours entities in Ui and discoursent i -ties in  Ui-1  , I amable to deal with intrasentential anaphora . There is no need for further specifications for complex sentences  . A finer grained ordering is achieved by ranking discours entities within each of the sets according to their text position  . 
3.3 The Algorithm
Anaphora resolution is performed with a simple lookup in the S-list  3  . The elements of the S-list are tested in the given order until one test succeeds  . Just after an anaphoric expression is resolved , the S-list is updated . The algorithm processes a text from left to fight  ( the unit of processing is the word ) : 1 . If a referring expression is encountered , ( a ) if it is a pronoun , test the elements of the S-list in the given order until the test  suc-ceeds4  ; ( b ) update S-list ; the position of the referring expression under consideration is determined by the S-list-ranking criteria which are used as an insertion algorithm  . 
2 . If the analysis of utterance U5 is finished , remove all discourse entities from the S-list , which are not realized in U . 
The analysis for example (1) is given in Table 36 . 
I show only these steps which are of interest for the computation of the S-list and the pronoun resolution  . The preferences for pronouns ( in bold font ) are given by the S-list immediately above them . The pronoun " she " in ( lb ) is resolved to the first element of the S-list . When the pronoun " her " in ( lc ) is encountered , FRIEDMAN is the first element of the S-list since FRIEDMAN is unused and in the current utterance  . Because of binding restrictions , " her " cannot be resolved to FRIED MAN buttO the second element  , BRENNAN . In both ( ld ) and ( ld  ~ ) the pronoun " she " is resolved to FRIED MAN . 
3The S-list consists of referring expressions which are specified for text position  , agreement , sortal information , and information status . Coordinated NPs are collected in a set . The S-list does not contain predicative NPs , pleonastic "' it " , and any elements of direct speech enclosed in double quotes  . 
4The test for pronominal anaphora involves checking agreement criteria  , binding and sortal constraints . 
5 I here define that an utterance is a sentence.
61 n the following Tables , discours entities are represented by SMALL CAPS , while the corresponding surface expression appears on the right side of the colon  . Discoursentitites are annotated with their information status  . An " e " indicates an elliptical NP . 
1253 ( la ) Brerman drives an Alfa Romeo
S:\[BRENNANu : Brennan,
ALFAROMEOBN : Alf a Romeo\] ( lb ) She drives too fast . 
S :\[ BRENNANE : she\](1c ) Friedman
S:\[FRIED MANu : Friedman , BRENNANE : she\]races her on weekends . 
S:\[FRIED MANu : Friedman , BRENNANE : her\] ( ld ) Shedrives to Laguna Seca . 
S:\[FRIED MANE : she,
LAGUNASECAu : LagunaSeca\](ld')She
S:\[FRIED MANE : she , BRENNANE : her \] often beats her . 
S:\[FRIED MANE : she , BRENNANE : her\]
Table 3: Analysis for ( 1 )   ( 2a ) Brennandrives an Alfa Romeo
S:\[BRENNANu : Brennan,
ALFAROMEOBN : Alf a Romeo\] ( 2b ) She drives too fast . 
S:\[BRENNANE : she\] ( 2c ) A professional driver S:\[BRENNANE : she , DRIVERBN : Driver \] races her on weekends . 
S :\[ BRENNANE : her , DRIVERBN : Driver\] ( 2d ) Shedrives to Laguna Seca . 
S:\[BRENNANE : she,
LAGUNASECAu : LagunaSeca\](2d')She
S :\[ BRENNANE : she , DRIVERBN : Driver \] often beats her . 
S:\[BRENNANE : she , DRIVERE : her\]
Table 4: Analysis for (2)
The difference between my algorithm and the BFP -algorithm becomes clearer when the unused discoursentity " Friedman " is replaced by a brand-new discours entity  , e . g . , " a professional driver ''7 ( cf . example (2)) . In the BFP-algorithm , the ranking of the Cf-list depends on grammatical roles  . 
Hence , DRIVER is ranked higher than BRENNAN in the Cf ( 2c )  . In (2d ) , the pronoun " she " is resolved to BRENNAN because of the preference for CONTINUE over RET AIN  . In (2d ~) , "she " is resolved to DRIVER because SMOOTH -SHIFT is preferred over ROUGH-SHIFT  . In my algorithm , at the end of ( 2c ) the evoked phrase " her " is ranked higher than the brand-new phrase " a professional driver "  ( cf . Table 4) . In both ( 2d ) and ( 2d  ~ ) the pronoun " she " is resolved to BRENNAN . 
(2) a . Brennan drives an Alfa Romeo.
b . She drives too fast.
c . A professional driveraces her on weekends.
d . Shegoes to Laguna Seca.
d / She often beats her.
Example ( 3 )   8 illustrates how the preferences for intra-and intersentential anaphora interact with the information status of discourse entitites  ( Table 5 )  . 
Sentence (3a ) starts a new discourse segment . The phrase " a judge " is brand-new . " Mr . Curtis " is mentioned several times before in the text  , Hence , 7Iowe this variant Andrew Kehler . -This example can mis-direct readers because the phrase "' a professional driver " is assigned the " default " gender masculine  . Anyway , this example-like the original example-seems not to be felicitous English and has only illustrative character  . 
Sin : The New York Times . Dec . 7, 1997, p . A48 (" Shot in head , suspect goes free , then to college ") . 
the discoursentity CURTIS is evoked and ranked higher than the discourse entity JUDGE  . In the next step , the ellipsis refers to JUDGE which is evoked then  . The nouns " request " and " prosecutors " are brand-new  9  . The pronoun " he " and the possessive pronoun " his " are resolved to CURTIS  . 
" Condition " is brand-new but anchored by the possessive pronoun  . For ( 3b ) and ( 3c ) I show only the steps immediately before the pronouns are resolved  . In (3b ) both " Mr . Curtis " and " the judge " are evoked . However , " Mr . Curtis " is the leftmost evoked phrase in this sentence and therefore the most preferred antecedent for the pronoun " him "  . 
For my experiments I restricted the length of the S-list to five elements  . Therefore " prosecutors " in ( 3b ) is not contained in the S-list . The discourse entity SMIRGA is introduced in ( 3c )  . It becomes evoked after the appositive . Hence SM1RGA is the most preferred antecedent for the pronoun " he "  . 
(3) a . A judge ordered that Mr . Curt is be released , bute agreed with a request from prosecutors that hebe reexamined achyear to see if his condition has improved  . 
b . But authorities lost contact with Mr . Curtisafter the Connecticut Supreme Courtruled in  1990 that the judge hader red , and that prosecutors had no right to reexamine him  . 
c . John Smirga , the assistant state's attorney in charge of the original case  , said last week that he always had doubts abou the psychiatric reports that said Mr  . Curtis would never improve . 
9I restrict inferrables tothe cases pecified by Hahn et al  ( 1996 )  . Therefore " prose cutors " is brand-new ( cf . Prince ( 1992 ) for a discussion of the form of inferrables )  . 
1254 (3a ) A judge
S :\[ JUDGEBN : judge\]ordered that Mr . Curtis S:\[CURTISE : Mr . Curtis , JUDGEBN : judge\]be released , bute
S:\[CURTISE : Mr . Curtis , JUDGEE : e\]agreed with a request S :\[ CURTISE : Mr  . Curtis , JUDGEE : e , REQUESTBN : request \] from prosecutors S :\[ CURTISE : Mr  . Curtis , JUDGEE : e , REQUESTBN : request , PROSECUTORSBN : prose cutors \] that he S :\[ CURTISE : he  , JUDGEE : e , REQUESTBN : request , PROSECUTORSBN : prose cutors \] be reexamined ach year S:\[CURTISE : he  , JUDGEE : ~ , REQUESTBN : request , PROSECUTORSBN : prosecutors , YEARBN : year \] to see if his S:\[CURTISE : his  , JUDGEE : ~ , REQUESTBN : request , PROSECUTORSBN : prosecutors , YEARBN:year\]conditionS:\[CURTISE : his , JUDGEE : e , CONDITION BNA : condition , REQUESTBN : request , PROSECUTORSBN : prose c . \] has improved . 
S:\[CURTISE : his , JUDGEE : e , CONDITION BNA : condition , REQUESTBN : request , PROSECUTORSBN : prose c . \] (3b ) But authorities lost contact with Mr . Curtisafter the Connecticut Supreme Courtruled in  1990 that the judge hader red , and that prosecutors had no right S:\[CURTISE : his  , CSCOURT u:CS Court , JUDGEE : judge , CONDITION BNA : condition , AUTH . BN:auth . \] to reexamine him . 
S:\[CURTISE : him , CSCOURT u:CS Court , JUDGEE : judge , CONDITION BNA : condition , AUTH . BN:auth . \](3c ) John Smirga , the assistant state's attorney in charge of the original case  , said last week S:\[SMIRGAE : attorney , CASEE : case , CURTISE : him , CSCOURT u:CS Court , JUDGEE : judge \] that he had doubts abou the psychiatric reports that said Mr  . Curtis would never improve . 
S:\[SMIRGAE : he , CASEE : case , REPORTSE : reports , CURTISE : Mr . Curtis , DOUBTSBN : doubts\]
Table 5: Analysis for ( 3 ) 4 Some Empirical Dat : i In the first experiment , I compare my algorithm with the BFP-algorithm which was in a second experiment extended by the constraints for complex sentences as described by Kameyama  ( 1998 )  . 
Method . I use the following guidelines for the hand -simulated analysis  ( Walker ,  1989) . I do not assume any world knowledge as part of the anaphora resolution process  . Only agreement criteria , binding and sortal constraints are applied . I do not account for false positives and error chains  . Following Walker (1989) , a segment is defined as a paragraph unless its first sentence has a pronoun in subject position or a pronoun where none of the preceding sentence -internaloun phrases matches its syntactic features  . At the beginning of a segment , anaphora resolution is preferentially performed within the same utterance  . My algorithm starts with an empty
S-list at the beginning of a segment.
The basic unit for which the centering data structures are generated is the utterance U  . For the BFP-algorithm , I define U as a simple sentence , a complex sentence , or each full clause of a compound sentence . Kameyama's ( 1998 ) intrasentential centering operates at the clause level  . While tensed clauses are defined as utterances on their own  , untensed clauses are processed with the main clause  , so that the Cf-list of the main clause contains the elements of the untensed embedded clause  . 
Kamey amadistinguishes for tensed clauses further between sequential and hierarchical centering  . Except for reported speech ( embedded and inaccessible to the superordinate l vel  )  , non-report complements , and relative clauses ( both embedded but accessible to the superordinate lvel  ; less salient hant he higher levels ) , all other types of tensed clauses build a chain of utterances on the same level  . 
According to the preference for intersentential candidates in the centering model  , I define the following anaphora resolution strategy for the BFP-algorithm :  ( 1 ) Test elements of Ui-1 . (2) Test elements of Uileft-to-right . (3) Test elements of Cf(Ui-2), Cf(Ui-3) . . . . In my algorithm steps (1) and (2) fall together .   ( 3 ) is performed using previous states of the system . 
Results . The test set consisted of the beginnings of three short stories by Hemingway  ( 2785 words , 153 sentences ) and three articles from the New York Times ( 4546 words , 233 sentences ) . The re-suits of my experiments are given in Table  6  . The sive pronouns . The remainder of the Table shows the results for the BFP-algorithm  , for the BFP-algorithm extended by Kamey ama's intrasentential specifications  , and formy algorithm . The overall error rate of each approach is given in the rows marked with wrong  . The rows marked with wrong ( strat . ) give the numbers of errors directly produced by the algorithms ' strategy  , the rows marked with wrong ( ambig . ) the number of analyses with ambiguities generated by the BFP-algorithm  ( my approach does not generate ambiguities )  . The rows marked with wrong ( intra ) give the number of errors caused by ( missing ) specifications for intrasentential anaphora . Since my algorithm integrates the specifications for intrasentential naphora  , I count these errors as strategic errors . The rows marked with wrong ( chain ) give the numbers of errors contained in error chains  . The rows marked with wrong ( other ) give the numbers of the remaining errors ( consisting of pronouns with split antecedents , errors because of segment boundaries , and missing specifications for event anaphora ) . 
Hem . NYT
Pron . and Poss . Pron . 274302


My Algo.


Wrong ( strat.)
Wrong ( ambig.)
Wrong ( intra )
Wrong ( chain )
Wrong ( other)


Wrong ( strat.)
Wrong ( ambig.)
Wrong ( intra )
Wrong ( chain )
Wrong ( other)


Wrong ( strat.)
Wrong ( chain )
Wrong ( other )  189 231 85 71 14 2 9 15 17 13 29 32 16 9 3 0 17 8 17 27 29 15 15 7 21 12 22 9 14 6
Table 6: Evaluation Results
Interpretation . The results of my experiments showed not only that my algorithm performed better than the centering approaches but also revealed insight in the interaction between inter-and intrasentential preferences for anaphoric antecedents  . 
Kameyama'specifications reduce the complexity in that the Cf-lists in general are shorter after splitting up a sentence into clauses  . Therefore , the BFP-algorithm combined with her specifications has almost no strategic errors while the number of ambiguities remains constant  . But this benefit is achieved at the expense of more errors caused by the intrasentential specifications  . These errors occur in cases like example (3) , in which Kamey a ma's intrasentential strategy makes the correct antecedent less salient  , indicating that a clause-based approach is too finegrained and that the hierarchical syntactical structure as assumed by Kameyama does not have a great impact on anaphora resolution  . 
I noted , too , that the BFP-algorithm can generate ambiguous readings for Ui when the pronoun in Ui does not co-specify the Cb  ( Ui-1 )  . In cases , where the Cf ( Ui-1 ) contains more than one possible antecedent for the pronoun  , several ambiguous readings with the same transitions are generated  . 
An example l ?: There is no Cb ( 4a ) because no element of the preceding utterance is realized in  ( 4a )  . 
The pronoun " them " in ( 4b ) co-specifies " deer " but the BFP-algorithm generates two readings both of which are marked by a RETAIN transition  . 
(4) a . J impulled the burlaps acks off the deer b . and Lizlooked at them . 
In general , the strength of the centering model is that it is possible to use the Cb  ( Ui-t ) as the most preferred antecedent for a pronoun in U i  . In my model this effect is achieved by the preference for hearer-oldiscours entities  . Whenever this preference is misleading both approaches give wrong results  . Since the Cb is defined strictly local while hearer-ol discours entities are defined global  , my model produces less errors . In my model the preference is available immediately while the BFP-algorithm can use its preference not before the second utterance has been processed  . The more global definition of hearer-oldiscourse ntities leads also to shorter error chains  . -However , the test set is too small to draw final conclusions  , but at least for the texts analyzed the preference for hearer-old is-course entities is more appropriate than the preference given by the BFP-algorithm  . 
5 Comparison to Related Approaches
Kameyama's ( 1998 ) version of centering also omits the centering transitions  . But she uses the Cb and a ranking over simplified transitions preventing the incremental pplication of her model  . 
l ? In : Emest Heming way . Upin Michigan . ln . The Complete Short Stories of Ernest Heming way . New York : Charles
Scribner's Sons , 1987, p . 60.

The focus model ( Sidner , 1983; Suri & McCoy ,  1994 ) accounts for evoke discourse entities explicitly because it uses the discourse focus  , which is determined by a successful anaphora resolution  . Incremental processing is not a topic of these papers  . 
Even models which use salience measures for determining the antecedents of pronoun use the concept of evoked discourse entities  . Haji ~ ov ~ i et al ( 1992 ) assign the highest value to an evoked discourse entity  . Also Lappin & Leass (1994) , who give the subject of the current sentence the highest weight  , have an implicit notion of evokedness . 
The salience weight degrades from one sentence to another by a factor of two which implies that a repeatedly mentioned discourse entity gets a higher weight than a brand-new subject  . 
6 Conclusions
In this paper , I proposed a model for determining the hearer's attentional state which is based on the distinction between hearer-old and hearer-new discourse entities  . I showed that my model , though it omits the backward-looking center and the centering transitions  , does not lose any of the predictive power of the centering model with respect to anaphora resolution  . In contrast to the centering model , my model includes a treatment for intrasentential anaphora and is sufficiently well specified to be applied to real texts  . Its incremental character seems to be an answer to the question Kehler  ( 1997 ) recently raised . Furthermore , it neither has the problem of inconsistency Kehler mentioned with respect to the BFP -algorithm nor does it generate unnecessary ambiguities  . 
Future work will address whether the text position  , which is the weakest grammatical concept , is sufficient for the order of the elements of the S-list at the second layer of my ranking constraints  . I will also try to extend my model for the analysis of definite noun phrases for which it is necessary to integrate it into a more global model of discourse processing  . 
Acknowledgments : This work has been funded by a postdoctoral grant from DFG  ( Str 545/1-1 ) and is supported by a postdoctoral fellowship award from IRCS  . I would like to thank Nobo Ko-magata , Rashmi Prasad , and Matthew Stone who commented on earlier drafts of this paper  . I am grateful for valuable comments by Barbara Grosz  , Udo Hahn , Aravind Joshi , Lauri Karttunen , Andrew Kehler , Ellen Prince , and Bonnie Webber . 

Brennan , S . E . , M . W . Friedman & C . J . Pollard (1987) . A centering approach to pronouns . In Proc . of the 25 th Annual Meeting of the Association for Computational Linguistics  ; Stanford , Cal . , 69 July 1987, pp .  155-162 . 
Grosz , B . J . , A . K . Joshi & S . We instein (1983) . Providing a unified account of definite noun phrases in discourse  . 
In Proc . of the 21 stAnnual Meeting of the Association for Computational Linguistics  ; Cambridge , Mass . , 15-17 June 1983, pp .  44-50 . 
Grosz , B . J . , A . K . Joshi & S . Weinstein (1995) . Centering : A framework for modeling the local coherence of discourse  . Computational Linguistics , 21(2):203-225 . 
Hahn , U . , K . Markert & M . Strube (1996) . A conceptual reasoning approach to textual ellipsis  . In Proc . of the 12 th European Conference on Artificial h ~telligence  ( ECAI'96 )  ; Budapest , Hungary , 1216 August 1996 , pp .  572-576 . Chichester : John Wiley . 
Haji~ov ~ i , E . , V . Kubofi & P . Kubofi (1992) . Stock of shared knowledge : A tool for solving pronominal anaphora  . In Proc . of the 14th h~t . Conference on Computational Linguistics ; Nantes , France , 2328 August 1992 , Vol . 1, pp . 

Kameyama , M .  (1998) . Intrasentential centering : A case study . 
In M . Walker , A . Joshi & E . Prince ( Eds . ), Centering Theory in Discourse , pp .  89-112 . Oxford , U . K . : Oxford
Univ . Pr.
Kehler , A .  (1997) . Current theories of centering for pronoun interpretation : A critical evaluation  . Computational Linguistics , 23(3):467-475 . 
Lappin , S . & H . J . Leass (1994) . An algorithm for pronominal anaphora resolution . Computational Linguistics , 20(4):535-56I . 
Prince , E . E (1981) . Toward a taxonomy of givennew information . In E Cole ( Ed . ), Radical Pragmatics , pp .  223-255 . 
New York , N.Y .: Academic Press.
Prince , E . E (1992) . The ZPG letter : Subjects , definiteness , and information-status . In W . Mann & S . Thompson ( Eds . ), Discourse Description . Diverse Linguistic Analyses of a Fund-Raisbzg Text  , pp .  295-325 . Amsterdam : John Benjamins . 
Sidner , C . L .  (1983) . Focusing in the comprehension of definite anaphora  . In M . Brady & R . Berwick ( Eds . ), Con , pu-tational Models of Discourse , pp .  267-330 . Cambridge,
Mass .: MIT Press.
Strube , M . & U . Hahn (1996) . Functional centering . In Proc . of the 34 th Annual Meeting of the Association for Computational Linguistics  ; Santa Cruz , Cal . , 2328 June 1996, pp .  270-277 . 
Suri , L . Z . & K . EMcCoy (1994) . RAFT/RAPR and centering : A comparison and discussion of problems related to processing complex sentences  . Computational Linguistics , 20(2):301-317 . 
Walker , M . A .  (1989) . Evaluating discourse processing algorithms . In Proc . of the 27 th Annual Meeting of the Association for Computational Linguistics  ; Vancouver , B . C . ,
Canada , 26-29 June 1989, pp . 251-261.
Walker , M . A . , M . lida & S . Cote (1994) . Japanese discourse and the process of centering . Computational Linguistics , 20(2):193-233 . 

