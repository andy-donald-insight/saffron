Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics , pages 299?308,
Portland , Oregon , June 1924, 2011. c?2011 Association for Computational Linguistics
Word Maturity : Computational Modeling of Word Knowledge
Kirill Kireyev Thomas K Landauer Pearson Education , Knowledge Technologies Boulder , CO { kirill.kireyev , tom.landauer}@pearson.com


Abstract
While computational estimation of difficulty of words in the lexicon is useful in many educational and assessment applications , the concept of scalar word difficulty and current corpus-based methods for its estimation are inadequate . We propose a new paradigm called word meaning maturity which tracks the degree of knowledge of each word at different stages of language learning . We present a computational algorithm for estimating word maturity , based on modeling language acquisition with Latent Semantic Analysis . We demonstrate that the resulting metric not only correlates well with external indicators , but captures deeper semantic effects in language . 1 Motivation It is no surprise that through stages of language learning , different words are learned at different times and are known to different extents . For example , a common word like ? dog ? is familiar to even a first-grader , whereas a more advanced word like ? focal ? does not usually enter learners ? vocabulary until much later . Although individual rates of learning words may vary between high - and low-performing students , it has been observed that ? children [?] acquire word meanings in roughly the same sequence ? ( Biemiller , 2008). The aim of this work is to model the degree of knowledge of words at different learning stages . Such a metric would have extremely useful applications in personalized educational technologies , for the purposes of accurate assessment and personalized vocabulary instruction.
2 Rethinking Word Difficulty Previously , related work in education and psychometrics has been concerned with measuring word difficulty or classifying words into different difficulty categories . Examples of such approaches include creation of word lists for targeted vocabulary instruction at various grade levels that were compiled by educational experts , such as Nation (1993) or Biemiller (2008). Such word difficulty assignments are also implicitly present in some readability formulas that estimate difficulty of texts , such as Lexiles ( Stenner , 1996), which include a lexical difficulty component based on the frequency of occurrence of words in a representative corpus , on the assumption that word difficulty is inversely correlated to corpus frequency . Additionally , research in psycholinguistics has attempted to outline and measure psycholinguistic dimensions of words such as age-of-acquisition and familiarity , which aim to track when certain words become known and how familiar they appear to an average person . Importantly , all such word difficulty measures can be thought of as functions that assign a single scalar value to each word w : ? ?????? ? ? ? ? ? (1) There are several important limitations to such metrics , regardless of whether they are derived from corpus frequency , expert judgments or other measures . First , learning each word is a continual process , one that is interdependent with the rest of the vocabulary . Wolter (2001) writes : 3 Modeling Word Meaning Acquisition with Latent Semantic Analysis 3.1 Latent Semantic Analysis ( LSA ) An appealing choice for quantitatively modeling word meanings and their growth over time is Latent Semantic Analysis ( LSA ), an unsupervised method for representing word and document meaning in a multidimensional vector space . The LSA vector representation is derived in an unsupervised manner , based on occurrence patterns of words in a large corpus of natural language documents . A Singular Value Decomposition on the highdimensional matrix of word/document occurrence counts ( A ) in the corpus , followed by zeroing all but the largest r ele-ments1 of the diagonal matrix S , yields a lower-rank word vector matrix ( U ). The dimensionality reduction has the effect of smoothing out incidental cooccurrences and preserving significant semantic relationships between words . The resulting word vectors2 in U are positioned in such a way that semantically related words vectors point in similar directions or , equivalently , have higher cosine values between them . For more details , please refer to Landauer et al (2007) and others.
Figure 1. The SVD process in LSA illustrated . The original highdimensional word-by-document matrix A is decomposed into word ( U ) and document ( V ) matrices of lower dimensionality . In addition to merely measuring semantic relatedness , LSA has been shown to emulate the learning of word meanings from natural language ( as can be evidenced by a broad range of applications from synonym tests to automated essay grading ), at rates that resemble those of human learners ( Laundauer et al 1997). Landauer and Dumais (1997) have demonstrated empirically that LSA can emulate not only the rate of human language acquisition , but also more subtle phenomena , such as the effects of learning certain words on meaning of other words . LSA can model meaning with 1 Typically the first approx . 300 dimensions are retained 2 U ? is used to project word vectors into V-space
SVD x x
Document ? VectorsWord ? VectorsOriginal ? Matrix word ?1word ?2 word ? n doc ?1 doc ?2 doc ? m . ?. ?.
. ?. ?. r r r r A U S V ? parisons across two different spaces , even of the same dimensionality , are meaningless , due to a mismatch in their coordinate systems . Fortunately , we can employ a multivariate algebra technique known as Procrustes Alignment ( or Procrustes Analysis ) ( PA ) typically used to align two multivariate configurations of a corresponding set of points in two different geometric spaces . PA has been used in conjunction with LSA , for example , in cross-language information retrieval ( Littman , 1998). The basic idea behind PA is to derive a rotation matrix that allows one space to be rotated into the other . The rotation matrix is computed in such a way as to minimize the differences ( namely : sum of squared distances ) between corresponding points , which in the case of LSA can be common words or documents in the training set . For more details , the reader is advised to consult chapter 5 of ( Krzanowski , 2000) or similar literature on multivariate analysis . In summary , given two matrices containing coordinates of n corresponding points X and Y ( and assuming mean-centering and equal number of dimensions , as is the case in this work ), we would like to minimize the sum of squared distances between the points : ?? = ? ? ? ?? ????? ? ??? We try to find an orthogonal rotation matrix Q , which minimizes M2 by rotating Y relative to X . That matrix can be obtained by solving the equation : ?? = ???? (??? + ??? ? 2?????) It turns out that the solution to Q is given by VU ?, where U?V ? is the singular value decomposition of the matrix X?Y . In our situation , where there are two spaces , adult and intermediate , the alignment points are the corresponding document vectors corresponding to the documents that the training corpora of the two models have in common ( recall that the adult corpus is a superset of each of the intermediate corpora ). The result of the Procrustes Alignment of the two spaces is effectively a joint LSA space containing two distinct word vectors for each word ( e.g . ? dog1?, ? dog2?), corresponding to the vectors from each of the original spaces . After passages by difficulty , in order to mimic the way typical human learners encounter progressively more difficult materials at successive school grades . After creating the corpora , we : 1. Build LSA spaces on the adult and each of the intermediate corpora 2. Merge the intermediate space for level l with the adult space , using Procrustes Alignment . This results in a joint space with two sets of vectors : the versions from the intermediate space { vlw }, and adult space{vaw }. 3. Compute the cosine in the joint space between the two word vectors for the given word w ? ?, ? = ?? ?( ? ? , ? ?) (5) In the cases where a word w has not been encountered in a given intermediate space , or in the rare cases where the cosine value falls below 0, the word maturity value is set to 0. Hence , the range for the word maturity function falls in the closed interval [0.0, 1.0]. A higher cosine value means greater similarity in meaning between the reference and intermediate spaces , which implies a more mature meaning of word w at the level l , i.e . higher word meaning maturity . The scores between discrete levels are interpolated , resulting in a continuous word maturity curve for each word . Figure 1 below illustrates resulting word maturity curves for some of the words.
!" !#$" !#%" !#&" !#'" (" !" (" $" )" %" *" &" +" '" ," (!" ((" ($" ()" (%"-." !" #$%& '() #*(+ % ,-.-/% /01"234567"846/9204":0;9<" Figure 2. Word maturity curves for selected words . Consistent with intuition , simple words like ? dog ? approach their adult meaning rather quickly , while ? focal ? takes much longer to become known to any degree . An interesting example is ? turkey ?, which has a noticeable plateau in the middle . This can be explained by the fact that this word has two distinct senses . Closer analysis of the corpus and the semantic near-neighbor word vectors at each in - 4 Values between discrete levels are obtained using piecewise linear interpolation 5.2 Age-of-Acquisition Norms Age-of-Acquisition ( AoA ) is a psycholinguistic property of words originally reported by Carol & White (1973). Age of Acquisition approximates the age at which a word is first learned and has been proposed as a significant contributor to language and memory processes . With some exceptions , AoA norms are collected by subjective measures , typically by asking each of a large number of participants to estimate in years the age when they have learned the word . AoA estimates have been shown to be reliable and provide a valid estimate for the objective age at which a word is acquired ; see ( Davis , in press ) for references and discussion . In this experiment we compute Spearman correlations between time-to-maturity and two available collections of AoA norms : Gilhooly et al , (1980) norms5, and Bristol norms6 ( Stadthagen-Gonzalez et al , 2010). Measure Gilhooly ( n=1643) Bristol ( n=1402) (-) Frequency 0.59 0.59 Time-to-Maturity (?=0.45) 0.72 0.64 Table 2. Correlations with Age of Acquisition norms . 5.3 Instruction Word Lists In this experiment , we examine leveled lists of words , as created by Biemiller (2008) in the book entitled ? Words Worth Teaching : Closing the Vocabulary Gap ?. Based on results of multiple-choice word comprehension tests administered to students of different grades as well as expert judgments , the author derives several word difficulty lists for vocabulary instruction in schools , including : o Words known by most children in grade 2 o Words known by 4080% of children in grade 2 o Words known by 4080% of children in grade 6 o Words known by fewer than 40% of children in grade 6 One would expect the words in these four groups to increase in difficulty , in the order they are presented above.
5 http://www.psy.uwa.edu.au/mrcdatabase/uwa_mrc.htm 6 http://language.psy.bris.ac.uk/bristol_norms.html Measure Correlation Frequency ( avg . of unique words ) 0.60 Coleman-Liau 0.64 Time-to-maturity (?=0.45) ( avg . of unique non-stopwords ) 0.70 Table 4. Correlations of grade levels with different metrics . 6 Emphasis on Meaning In this section , we would like to highlight certain properties of the LSA-based word maturity metric , particularly aiming to illustrate the fact that the metric tracks acquisition of meaning from exposure to language and not merely more shallow effects , such as word frequency in the training corpus . 6.1 Maturity based on Frequency For a baseline that does not take meaning into account , let us construct a set of maturity-like curves based on frequency statistics alone . More specifically , we define the frequency-maturity for a particular word at a given level as the ratio of the number of occurrences at the intermediate corpus for that level ( l ) to the number of occurrences in the reference corpus ( a ): ? ?, ? = ?? _ ???? ?(?)?? _ ???? ?(?) Similarly to the original LSA-based word maturity metric , this ratio increases from 0 to 1 for each word as the amount of cumulative language exposure increases . The corpora used at each intermediate level are identical to the original word maturity model , but instead of creating LSA spaces we simply use the corpora to compute word frequency . The following figure shows the Spearman correlations between the external measures used for experiments in Section 5, and time-to-maturity computed based on the two maturity metrics : the new frequency-based maturity and the original LSA-based word maturity.
304 Figure 3. Correlations of word maturity computed using frequency ( as well as the original ) against external metrics described in Section 5. The results indicate that the original LSA-based word maturity correlates better with realworld data than a maturity metric simply based on frequency . 6.2 Homographs Another insight into the fact that the LSA-based word maturity metric tracks word meaning rather than mere frequency may be gained from analysis of words that are homographs : words that contain two or more unrelated meanings in the same written form , such as the word ? turkey ? illustrated in Section 4. ( This is related to but distinct from the merely polysemous words that have several related meanings ), Because of the conflation of several unrelated meanings into the same orthographic form , homographs implicitly contain more semantic content in a single word . Therefore , one would expect the meaning of homographs to mature more slowly than would be predicted by frequency alone : all things being equal , a learner has to learn the meanings for all of the senses of a homograph word before the word can be considered fully known . More specifically , one would expect the time-to-maturity of homographs to have greater values than words of similar frequency . To test this hypothesis , we obtained8 a list 174 common English homographs . For each of them , we compared their time-to-maturity to the average time-to-maturity of words that have the same (+/- 1%) corpus frequency . 8 http://en.wikipedia.org/wiki/List_of_English_homographs The results of a paired ttest confirms the hypothesis that the time-to-maturity of homographs is greater than other words of the same frequency , with the pvalue = 5.9e-6. This is consistent with the observation that homographs will take longer to learn and serves as evidence that LSA-based word maturity approximates effects related to meaning . 6.3 Size of the Reference Corpus Another area of investigation is the repercussions of the choice of the corpus for the reference ( adult ) model . The size ( and content ) of the corpus used to train the reference model is potentially important , since it affects the word maturity calculations , which are comparisons of the intermediate LSA spaces to the reference LSA space built on this corpus . It is interesting to investigate how the word maturity model would be affected if the adult corpus were made significantly more sophisticated . If the word maturity metric were simply based on word frequency ( including the frequency-based maturity baseline described in Section 6.1), one would expect the word maturity of the words at each level to decrease significantly if the reference model is made significantly larger , since each intermediate level will have encountered fewer words by comparison . Intuition about language learning , however , tells us that with enough language exposure a learner learns virtually all there is to know about any particular word ; after the word reaches its adult maturity , subsequent encounters of natural readings do little to further change the knowledge of that word . Therefore , if word maturity were tracking something similar to real word knowledge , one would expect the word maturity for most words to plateau over time , and subsequently not change significantly , no matter how sophisticated the reference model becomes . To evaluate this inquiry we created a reference corpus that is twice as large as before ( four times as large and of the same difficulty range as the corpus for the last intermediate level ), containing roughly 329,000 passages . We computed the word maturity model using this larger reference corpus , while keeping all the original intermediate corpora of the same size and content . The results show that the average word maturity of words at the last intermediate level (14) de-0.66 ? 0.56 ? 0.42 ? 0.68 ? 0.72 ? 0.64 ? 0.49 ? 0.71 ? 0.0 ? 0.2 ? 0.4 ? 0.6 ? 0.8 ? 1.0 ?
AoA ? ( Gilhooly ) ?
AoA ? ( Bristol ) ?
Word ? Lists ? Readings ? freq-??WM ?(?=0.15) ?
LSA-??WM ?(?=0.45) ? 1. All the passages were introduced at the first level ( l=1) intermediate corpus 2. All the passages were introduced at the last level ( l=14) intermediate corpus . This resulted in two new variants of word maturity models , which were computed in all the same ways as before , except that all of these 89 advanced passages were introduced either at the very first level or at the very last level . We then computed the word maturity at the levels they were introduced . The hypothesis consistent with a meaning-based maturity method would be that less learning ( i.e . lower word maturity ) of the relevant words will occur when passages are introduced prematurely ( at level 1). Table 5 shows the word maturities measured for each of those cases , at the level (1 or 14) when all of the passages have been introduced . Word Introduced at l=1 ( WM at l=1) Introduced at l=14 ( WM at l=14) chromosome 0.51 0.73 neutron 0.51 0.72 filibuster 0.58 0.85 Table 5. Word maturity of words resulting when all the relevant passages are introduced early vs late . Indeed , the results show lower word maturity values when advanced passages are introduced too early , and higher ones when the passages are introduced at a later stage , when the rest of the supporting vocabulary is known . 7 Conclusion We have introduced a new metric for estimating the degree of knowledge of words by learners at different levels . We have also proposed and evaluated an implementation of this metric using Latent Semantic Analysis . The implementation is based on unsupervised word meaning acquisition from natural text , from corpora that resemble in volume and complexity the reading materials a typical human learner might encounter . The metric correlates better than word frequency to a range of external measures , including vocabulary word lists , psycholinguistic norms and leveled texts . Furthermore , we have shown that the metric is based on word meaning ( to the extent that it can be approximated with LSA ), and not merely on shallow measures like word frequency.
306
Many interesting research questions still remain pertaining to the best way to select and partition the training corpora , align adult and intermediate LSA models , correlate the results with real school grade levels , as well as other free parameters in the model . Nevertheless , we have shown that LSA can be employed to usefully mimic model word knowledge . The models are currently used ( at Pearson Education ) to create state-of-the-art personalized vocabulary instruction and assessment tools.
307 References Andrew Biemiller (2008). Words Worth Teaching . Columbus , OH : SRA/McGraw-Hill . John B . Carrol and M . N . White (1973). Age of acquisition norms for 220 picturable nouns . Journal of Verbal Learning & Verbal Behavior , 12, 563-576. Meri Coleman and T.L . Liau (1975). A computer readability formula designed for machine scoring , Journal of Applied Psychology , Vol . 60, pp . 283-284. Ken J . Gilhooly and R . H . Logie (1980). Age of acquisition , imagery , concreteness , familiarity and ambiguity measures for 1944 words . Behaviour Research Methods & Instrumentation , 12, 395-427. Wojtek J . Krzanowski (2000) Principles of Multivariate Analysis : A User?s Perspective ( Oxford Statistical Science Series ). Oxford University Press , USA . Thomas K Landauer and Susan Dumais (1997). A solution to Plato's problem : The Latent Semantic Analysis Theory of the Acquisition , Induction , and Representation of Knowledge . Psychological Review , 104, pp 211-240. Thomas K Landauer (2002). On the Computation Basis of Learning and Cognition : Arguments from LSA . In N . Ross ( Ed .), The Psychology of Learning and Motivation , 41, 43-84. Thomas K Landauer , Danielle S . McNamara , Simon Dennis , and Walter Kintsch (2007). Handbook of Latent Semantic Analysis . Lawrence Erlbaum . Paul Nation (1993). Measuring readiness for simplified material : a test of the first 1,000 words of English . In Simplification : Theory and Application M . L . Tickoo ( ed .), RELC Anthology Series 31: 193-203. Hans Stadthagen-Gonzalez and C . J . Davis (2006). The Bristol Norms for Age of Acquisition , Imageability and Familiarity . Behavior Research Methods , 38, 598-605. A . Jackson Stenner (1996). Measuring Reading Comprehension with the Lexile Framework . Forth North American Conference on Adolescent/Adult Literacy . Brent Wolter (2001). Comparing the L1 and L2 Mental Lexicon . Studies in Second Language Acquisition . Cambridge University Press.
308
