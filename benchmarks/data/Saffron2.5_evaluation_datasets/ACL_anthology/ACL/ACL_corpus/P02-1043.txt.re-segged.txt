Generative Models for Statistical Parsing with Combinatory Categorial 

Julia Hockenmaier and Mark Steedman
Division of Informatics
University of Edinburgh
Edinburgh EH8 9LW , United Kingdom
fjulia , steedmang@cogsci.ed.ac.uk

This paper compares a number of generative probability models for a wide-coverage Combinatory Categorial Grammar  ( CCG ) parser . These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into 
CCG normal-form derivations . According to an evaluation of unlabeled word word dependencies  , our best model achieves a performance of 89 . 9% , comparable to the figures given by Collins ( 1999 ) for a linguistically less expressive grammar . In contrast to Gildea (2001) , we find a significant improvement from modeling word word dependencies  . 
1 Introduction
The currently best single-model statistical parser  ( Charniak ,  1999 ) achieves Parseval scores of over 89% on the Penn Treebank . However , the grammar underlying the Penn Treebank is very permissive  , and a parser can do well on the standard Parseval measures without committing itself on certain semantically significant decisions  , such as predicting null elements arising from deletion or movement  . 
The potential benefit of wide-coverage parsing with CCG lies in its more constrained grammar and its simple and semantically transparent capture of extraction and coordination  . 
We present a number of models over syntactic derivations of Combinatory Categorial Grammar  ( CCG , see Steedman (2000) and Clark et al (2002) , this conference , for introduction ) , estimated from and tested on a translation of the Penn Treebank to a corpus of CCG normal-form derivations  . CCG grammars are characterized by much larger category sets than standard Penn Treebank grammars  , distinguishing for example between many classes of verbs with different subcategorization frames  . As a result , the categorial lexicon extracted for this purpose from the training corpus has  1207 categories , compared with the 48 POS tags of the Penn Treebank . 
On the other hand , grammar rules in CCG are limited to a small number of simple unary and binary combinatory schemata such as function application and composition  . This results in a smaller and less overgenerating grammar than standard PCFGs  ( ca . 
3 , 0 00 rules when instantiated with the above categories in sections  02-21  , instead of >12 , 400 in the original Treebank representation ( Collins ,  1999)) . 
2 Evaluating a CCG parser
Since CCG produces unary and binary branching trees with a very finegrained category set  , CCGParseval scores cannot be compared with scores of standard Treebank parsers  . Therefore , we also evaluate performance using a dependency evaluation reported by Collins  ( 1999 )  , which counts word word dependencies as determined by local trees and their labels  . According to this metric , a local tree with parent node P , head daughter H and nonhead daughter S ( and position of S relative to P , i e . left or right , which is implicit in CCG categories ) defines a hP ; H ; Si dependency between the head word of S , wS , and the head word of H , wH . This measure is neutral with respect to the branching factor  . Furthermore , as noted by Hockenmaier (2001) , it does not penalize equivalent analyses of multiple modi-Computational Linguistics  ( ACL )  , Philadelphia , July 2002 , pp .  335-342 . 
Proceedings of the 40th Annual Meeting of the Association for Pierre Vinken  ; 61 years old ; will join the board as a nonexecutive director Nov  29 N = NN ; N = NN(S[adj]nNP ) nNP ;   ( S[dcl]nNP ) = ( S [ b]nNP )   ( ( S[b]nNP ) =PP ) = NPNP=NNPP=NPNP=NN=NN ( ( SnNP ) n ( SnNP ) )=N N > > > > >
NNNN(SnNP)n(SnNP ) >
NP NP NP NP < > >
NPS [ adj]nNP ( S[b]nNP ) = PPPP >
NP nNPS [ b]nNP < <
NPS [ b]nNP >
NPS [ dcl]nNP <

Figure 1: A CCG derivation in our corpus fiers . In the unlabeled case hi ( where it only matters whether word a is a dependent of word b  , not what the label of the local tree is which defines this dependency  )  , scores can be compared across grammars with different sets of labels and different kinds of trees  . In order to compare our performance with the parser of Clark et al  ( 2002 )  , we also evaluate our best model according to the dependency evaluation introduced for that parser  . For further discussion we refer the reader to Clark and Hockenmaier  ( 2002 )   . 
3 CCGbank ? a CCG treebank
CCGbank is a corpus of CCG normal-form derivations obtained by translating the Penn Treebank trees using an algorithm described by Hockenmaier and Steedman  ( 2002 )  . Almost all types of construction ? with the exception of gapping and UCP  ( ? Unlike Coordinate Phrases ? ) are covered by the translation procedure , which processes 98 . 3% of the sentences in the training corpus ( WSJ sections 02-21 ) and 98 . 5% of the sentences in the test corpus ( WSJ section 23 )  . The grammar contains a set of type-changing rules similar to the lexical rules described in Carpenter  ( 1992 )  . Figure 1 shows a derivation taken from CCGbank . Categories , such as (( S [ b]nNP ) = PP ) = NP , encode unsaturated subcat frames . The complement-adjunct distinction is made explicit  ; for instance as a nonexec-utive director is marked up as PP-CLR in the Treebank  , and hence treated as a PP-complement of join , whereas Nov .   29 is marked up as an NP-TMP and therefore analyzed as VP modifier  . The-CLR tag is not in fact a very reliable indicator of whether a constituent should be treated as a complement  , but the translation to CCG is automatic and must do the best it can with the information in the Treebank  . 
The verbal categories in CCGbank carry features distinguishing declarative verbs  ( and auxiliaries ) from past participles in past tense , past participles for passive , bare infinitives and ing-forms . 
There is a separate level for nouns and noun phrases  , but , like the nonterminal NP in the Penn Treebank , noun phrases do not carry any number agreement . 
The derivations in CCGbank are ? normal-form ? in the sense that analyses involving the combinatory rules of typeraising and composition are only used when syntactically necessary  . 
4 Generative models of CCG derivations
Expansion Head Cat Non Head Cat
P ( expj : : :) P ( Hj : : :) P ( Sj : : :)
Baseline PP ; expP ; exp ; H+ConjP ; conjPP ; exp ; conjPP ; exp ; H ; conjP+Gr and parentP ; GPP ; GP ; expP ; GP ; exp ; H + ? P#?L ; RPP ; exp#?L ; RPP ; exp ; H#?L ; RP
Table 1: The unlexicalized models
The models described here are all extensions of a very simple model which models derivations by a topdown tree-generating process  . This model was originally described in Hockenmaier  ( 2001 )  , where it was applied to a preliminary version of CCGbank  , and its definition is repeated here in the top row of Table  1  . Given a ( parent ) node with category P , choose the expansion exp of P , where exp can be leaf ( for lexical categories ) , unary ( for unary expansions such as typeraising )  , left ( for binary trees where the head daughter is left ) or right ( binary trees , head right ) . If P is a leaf node , generate its head word w . Otherwise , generate the category of its head daughter H . If P is binary branching , generate the category of its nonhead daughter S ( a complement or modifier of H )  . 
The model itself includes no prior knowledge specific to CCG other than that it only allows unary and binary branching trees  , and that the sets of nonterminals and terminals are not disjoint  ( hence the need to include leaf as a possible expansion  , which acts as a stop probability ) . 
All the experiments reported in this section were conducted using sections  02-21 of CCGbank as training corpus , and section 23 as test corpus . We replace all rare words in the training data with their POS tag  . For all experiments reported here and in section 5  , the frequency threshold was set to 5 . Like Collins (1999) , we assume that the test data is POS-tagged , and can therefore replace unknown words in the test data with their POS tag  , which is more appropriate for a formalism like CCG with a large set of lexical categories than one generic token for all unknown words  . 
The performance of the baseline model is shown in the top row of table  3  . For six out of the 2379 sentences in our test corpus we do not get a parse  . 1 The reason is that a lexicon consisting of the word-category pairs observed in the training corpus does not contain all the entries required to parse the test corpus  . We discuss a simple , but imperfect , solution to this problem in section 7 . 
5 Extending the baseline model
State-of-the-art statistical parsers use many other features  , or conditioning variables , such as head words , subcategorization frames , distance measures and grandparent nodes . We too can extend the baseline model described in the previous section by including more features  . Like the models of Goodman (1997) , the additional features in our model are generated probabilistically  , whereas in the parser of Collins ( 1997 ) distance measures are assumed to be a function of the already generated structure and are not generated explicitly  . 
In order to estimate the conditional probabilities of our model  , we recursively smooth empirical estimates e?i of specific conditional distributions with  ( possible smoothed ) estimates of less specific distributions e?i   1 , using linear interpolation : e?i = ? e?i + ( 1 ? ) e ?i 1 ? is a smoothing weight which depends on the particular distribution  . 2 When defining models , we will indicate a backoff level with a  #sign between conditioning variables  , eg . A ; B  #C  #D means that we interpolate ? P (: : : jA ; B ; C ; D ) with ? P (: : : jA ; B ; C ) , which is an interpolation of ? P (::: jA ; B ; C ) and ? P (::: jA ; B) . 
1We conjecture that the minor variations in coverage among the other models  ( except Grandparent ) are artefacts of the beam . 
2 We compute ? in the same way as Collins (1999) , p .  185 . 
5.1 Adding nonlexical information
The coordination feature We define a boolean feature  , conj , which is true for constituents which expand to coordinations on the head path  . 
S , + conj
S = NP , + conj
S = NP ,   conj


IBM(SnNP ) = NP buys
S = NP[c ], + conj conj but
S = NP[c ],   conj


Lotus ( SnNP ) = NP sells
NP shares
This feature is generated at the root of the sentence with P  ( conjjTOP )  . For binary expansions , conjH is generated with P ( conjHjH ; S ; conjP ) and conjS is generated with P ( conjSjS#P ; expP ; H ; conjP ) . Table 1 shows how conj is used as a conditioning variable  . This is intended to allow the model to capture the fact that  , for a sentence without extraction , a CCG derivation where the subject is type-raised and composed with the verb is much more likely in right node raising constructions like the above  . 
The impact of the grandparent feature
Johnson ( 1998 ) showed that a PCFG estimated from a version of the Penn Treebank in which the label of a node?s parent is attached to the node?s own label yields a substantial improvement  ( LP/LR : from 73 . 5%/69 . 7% to 80 . 0%/79 . 2%) . 
The inclusion of an additional grandparent feature gives Charniak  ( 1999 ) a slight improvement in the Maximum Entropy inspired model  , but a slight decrease in performance for an MLE model  . Table 3 ( Grandparent ) shows that a grammar transformation like Johnson ?s does yield an improvement  , but not as dramatic as in the Treebank-CFG case . 
At the same time coverage is reduced ( which might not be the case if this was an additional feature in the model rather than a change in the representation of the categories  )  . Both of these results are to be expected ? CCG categories encode more contextual information than Treebank labels  , in particular about parents and grandparents ; therefore the history feature might be expected to have less impact  . 
Moreover , since our category set is much larger , appending the parent node will lead to an even more finegrained partitioning of the data  , which then results in sparse data problems . 
Distance measures for CCG Our distance measures are related to those proposed by Goodman  ( 1997 )  , which are appropriate for binary trees ( unlike those of Collins ( 1997 ) ) . Every node has a left distance measure , ? L , measuring the distance from the head word to the left frontier of the constituent  . 
There is a similar right distance measure ? R . We implemented three different ways of measuring distance : ? Adjacency measures string adjacency  ( 0 , 1 or 2 and more intervening words ) ; ? Verb counts intervening verbs ( 0 or 1 and more )  ; and ? Pct counts intervening punctuation marks (0 ,  1 , 2 or 3 and more ) . 
These?s are generated by the model in the following manner : at the root of the sentence  , generate ? L with P (? LjTOP ) , and ? R with P (? RjTOP ; ? L) . 
Then , for each expansion , if it is a unary expansion , ? LH = ? LP and ? RH = ? RP with a probability of  1  . If it is a binary expansion , only the ? in the direction of the sister changes  , with a probability of P (? LHj ? LPH  #P ; S ) if exp = right , and analogously for exp = left . ? LS and ? RS are conditioned on S and the ? of H and P in the direction of S : P  ( ? LSjS#?RP ; ? RH ) and P (? RSjS ; ?LS#?RP ; ?RH) . 
They are then used as further conditioning variables for the other distributions as shown in table  1  . 
Table 3 also gives the Parseval and dependency scores obtained with each of these measures  . ? Pc t has the smallest effect . However , our model does not yet contain anything like the hard constraint on punctuation marks in Collins  ( 1999 )  . 
5.2 Adding lexical information
Gildea ( 2001 ) shows that removing the lexical dependencies in Model  1 of Collins ( 1997 )   ( that is , not conditioning on wh when generating ws ) decreases labeled precision and recall by only 0 . 5% . 
It can therefore be assumed that the main influence of lexical head features  ( words and preterminals ) in Collins ? Model 1 is on the structural probabilities . 
In CCG , by contrast , preterminals are lexical categories , encoding complete subcategorization information . They therefore encode more information about the expansion of a nonterminal than Treebank POS tags and thus are more constraining  . 
Generating a constituent?s lexical category cat its maximal projection  ( i e . either at the root of the tree , TOP , or when generating a nonhead daughter S ) , and using the lexical category as conditioning variable  ( LexCat ) increases performance of the baseline model as measured by hP  ; H ; Siby almost 3% . In this model , cS , the lexical category of S depends on the category S and on the local tree in which S is generated  . However , slightly worse performance is obtained for LexCat Dep  , a model which is identical to the original LexCat model  , except that cS is also conditioned on cH , the lexical category of the head node , which introduces a dependency between the lexical categories  . 
Since there is so much information in the lexical categories  , one might expect that this would reduce the effect of conditioning the expansion of a constituent on its head word w  . However , we did find a substantial effect . Generating the head word at the maximal projection  ( Head Word ) increases performance by a further 2% . Finally , conditioning wSonwH , hence including wordword dependencies , ( HWDep ) increases performance even more , by another 3 . 5%, or 8 . 3% overall . This is in stark contrast to Gildea?s findings for Collins ? Model  1  . 
We conjecture that the reason why CCG benefits more from word word dependencies than Collins ? Model  1 is that CCG allows a cleaner parametrization of these surface dependencies  . In Collins ? Model 1 , wS is conditioned not only on the local tree hP ; H ; Si , cH and wH , but also on the distance ? between the head and the modifier to be generated  . 
However , Model 1 does not incorporate the notion of subcategorization frames  . Instead , the distance measure was found to yield a good , if imperfect , approximation to subcategorization information . 
Using our notation , Collins ? Model 1 generates wS with the following probability :
PCollins1(wSjcS ; ? ; P ; H ; S ; cH ; wH ) = ?1?P ( wSjcS ; ? ; P ; H ; S ; cH ; wH ) + (1   ?1) ?2 ? P ( wSj cS ; ? ; P ; H ; S ; cH ) + ( 1 ?2 ) ? P ( wSj cS )  ? whereas the CCG dependency model generates wS as follows: 
PCCG dep(wS jcS ; P ; H ; S ; cH ; wH ) = ? ? P ( wSj cS ; P ; H ; S ; cH ; wH ) + (1  ?) ? P ( wSjcS ) Since our P , H , S and cH are CCG categories , and hence encode subcategorization information , the local tree always identifies a specific argument slot  . 
Therefore it is not necessary for us to include a distance measure in the dependency probabilities  . 
Expansion Head Cat Non Head Cat LexCat Head word P ( expj ::: ) P ( Hj ::: ) P ( Sj ::: ) P ( cSj ::: ) P ( cTOPj::: ) P ( wSj ::: ) P ( wTOPj : : ) LexCatP ; cPP ; exp ; cPP ; exp ; H#cPS#H ; exp ; PP = TOP ? ? LexCatDepP ; cPP ; exp ; cPP ; exp ; H#cPS#H ; exp ; P#c PP = TOP ? ? HeadWordP ; cP#w PP ; exp ; cP#w PP ; exp ; H  #cP  #w PS  #H ; exp ; PP=TOP cScPHW DepP ; cP#w PP ; exp ; cP#w PP ; exp ; H  #cP  #w PS  #H ; exp ; PP = TOP cS  #P ; H ; S ; wP c PHW Dep ? P ; cP#?L ; RP  #w PP ; exp ; cP#?L ; RP  #w PP ; exp ; H#?L ; RP  #cP  #w PS  #H ; exp ; PP = TOP cS  #P ; H ; S ; wP cPHW DepConjP ; cP ; conjP#w PP ; exp ; cP ; conjP#w PP ; exp ; H ; conjP#cP#wPS#H ; exp ; PP = TOP cS  #P ; H ; S ; wP cP
Table 2: The lexicalized models
Model No Parse LexCat LP LR BP BR h P ; H ; Sih SihiCMonhi  2 CD Baseline 687 . 7 72 . 8 72 . 4 78,3 77 . 9 75 . 7 81 . 1 84 . 3 23 . 0 51 . 1 Conj 987 . 8 73 . 8 73 . 9 79 . 3 79 . 3 76 . 7 82 . 0 85 . 1 24 . 3 53 . 2 Grandparent 91 88 . 8 77 . 1 77 . 6 82 . 4 82 . 9 79 . 9 84 . 7 87 . 9 30 . 9 63 . 8 ? Pct6 88 . 1 73 . 7 73 . 1 79 . 2 78 . 6 76 . 5 81 . 8 84 . 9 23 . 1 53 . 2 ? Verb 688 . 0 75 . 9 75 . 5 81 . 6 81 . 1 76 . 9 82 . 3 85 . 3 25 . 2 55 . 1 ? Adjacency 688 . 6 77 . 5 77 . 3 82 . 9 82 . 8 78 . 9 83 . 8 86 . 9 24 . 8 59 . 6 LexCat 988 . 5 75 . 8 76 . 0 81 . 3 81 . 5 78 . 6 83 . 7 86 . 8 27 . 4 57 . 8 LexCatDep9 88 . 5 75 . 7 75 . 9 81 . 2 81 . 4 78 . 4 83 . 5 86 . 6 26 . 3 57 . 9 Head Word 889 . 6 77 . 9 78 . 0 83 . 0 83 . 1 80 . 5 85 . 2 88 . 3 30 . 4 63 . 0 HWDep8 92 . 0 81 . 6 81 . 9 85 . 5 85 . 9 84 . 0 87 . 8 90 . 1 37 . 9 69 . 2 HWDep ? 8 90 . 9 81 . 4 81 . 6 86 . 1 86 . 3 83 . 0 87 . 0 89 . 8 35 . 7 68 . 7 HWDep Conj 991 . 8 80 . 7 81 . 2 84 . 8 85 . 3 83 . 6 87 . 5 89 . 9 36 . 5 68 . 6 HWDep(+tagger ) 791 . 7 81 . 4 81 . 8 85 . 6 85 . 9 83 . 6 87 . 5 89 . 9 38 . 1 69 . 1 Table 3: Performance of the models : LexCat indicates accuracy of the lexical categories  ; LP , LR , BP and BR ( the standard Parseval scores labeled/bracketed precision and recall  ) are not commensurate with other Treebank parsers . hP ; H ; Si , hSi , and hi are as defined in section 2 . CM on hi is the percentage of sentences with complete match on hi  , and 2 CD is the percentage of sentences with under 2 ? crossing dependencies ? as defined by hi . 
The hP ; H ; Si labeled dependencies we report are not directly comparable with Collins  ( 1999 )  , since CCG categories encode subcategorization frames  . 
For instance , if the direct object of a verb has been recognized as such  , but a PP has been mistaken as a complement ( whereas the gold standard says it is an adjunct )  , the fully labeled dependency evaluation hP ; H ; Si will not award a point . Therefore , we also include in Table 3 a more comparable evaluation hSi which only takes the correctness of the nonhead category into account  . The reported figures are also deflated by retaining verb features like tensed/untensed  . If this is done ( by stripping off all verb features )  , an improvement of 0 . 6% on the hP ; H ; Si score for our best model is obtained . 
5 . 3 Combining lexical and nonlexical information When incorporating the adjacency distance measure or the coordination feature into the dependency model  ( HWDep ? and HWDep Conj )  , overall performance is lower than with the dependency model alone  . We conjecture that this arises from data sparseness  . It cannot be concluded from these results alone that the lexical dependencies make structural information redundant or superfluous  . Instead , it is quite likely that we are facing an estimation problem similar to Charniak  ( 1999 )  , who reports that the inclusion of the grandparent feature worsens performance of an MLE model  , but improves performance if the individual distributions are modelled using Maximum Entropy  . This intuition is strengthened by the fact that , on casual inspection of the scores for individual sentences  , it is sometimes the case that the lexicalized models perform worse than the unlexicalized models  . 
5.4 The impact of tagging errors
All of the experiments described above use the POS tags as given by CCGbank  ( which are the Treebank tags , with some corrections necessary to acquire correct features on categories  )  . It is reasonable to assume that this input is of higher quality than can be produced by a POS tagger  . We therefore ran the dependency model on a test corpus tagged with the POS tagger of Ratnaparkhi  ( 1996 )  , which is trained on the original Penn Treebank ( see HWDep ( + tagger ) in Table 3 )  . Performance degrades slightly , which is to be expected , since our approach makes so much use of the POS tag information for unknown words  . However , a POS tagger trained on CCGbank might yield slightly better results  . 
5.5 Limitations of the current model
Unlike Clark et al (2002) , our parser does not always model the dependencies in the logical form  . 
For example , in the interpretation of a coordinate structure like ? buy and sell shares ?  , shares will head an object of both buy and sell . Similarly , in examples like ? buy the company that wins ? , the relative construction makes company depend upon both buy as object and wins as subject  . As is wellknown ( Abney ,  1997) , DAG-like dependencies cannot in general be modeled with a generative approach of the kind taken  here3  . 
5.6 Comparison with Clark et al (2002)
Clark et al ( 2002 ) presents another statistical CCG parser , which is based on a conditional ( rather than generative ) model of the derived dependency structure , including non-surface dependencies . The following table compares the two parsers according to the evaluation of surface and deep dependencies given in Clark et al  ( 2002 )  . We use Clark et al?s parser to generate these dependencies from the output of our parser  ( see
Clark and Hockenmaier (2002)) 4.
LPLRUPUR
Clark 81.9% 81.8% 89.1% 90.1%
Hockenmaier 83 . 7% 84 . 2% 90 . 5% 91 . 1%  6 Performance on specific constructions One of the advantages of CCG is that it provides a simple  , surface grammatical analysis of extraction and coordination  . We investigate whether our best 3It remains to be seen whether the more restricted reentrancies of CCG will ultimately support a generative model  . 
4Due to the smaller grammar and lexicon of Clark et al  , our parser can only be evaluated on slightly over  94% of the sentences in section 23  , whereas the figures for Clark et al ( 2002 ) are on 97% . 
model , HWDep , predicts the correct analyses , using the development section 00 . 
Coordination There are two instances of argument cluster coordination  ( constructions like cost $5 , 000 in July and $6 , 000 in August ) in the development corpus . Of these , HWDep recovers none correctly . This is a shortcoming in the model , rather than in CCG : the relatively high probability both of the NP modifier analysis of PPs like in July and of NP coordination is enough to misdirect the parser  . 
There are 203 instances of verb phrase coordination ( S [:] nNP , with [:] any verbal feature ) in the development corpus . On these , we obtain a labeled recall and precision of 67 . 0%/67 . 3% . Interestingly , on the 24 instances of right node raising ( coordination of ( S [:] nNP ) = NP )  , our parser achieves higher performance , with labeled recall and precision of 79 . 2% and 73 . 1% . Figure 2 gives an example of the output of our parser on such a sentence  . 
Extraction Long-range dependencies are not captured by the evaluation used here  . However , the accuracy for recovering lexical categories for words with ? extraction ? categories  , such as relative pronouns , gives some indication of how well the model detects the presence of such dependencies  . 
The most common category for subject relative pronouns  , ( NP nNP ) = ( S[dcl]nNP) , has been recovered with precision and recall of 97  . 1% (232 out of 239) and 94 . 3% (232/246) . 
Embedded subject extraction requires the special lexical category  ( ( S[dcl]nNP ) =NP ) = ( S[dcl]nNP ) for verbs like think . On this category , the model achieves a precision of 100% ( 5/5 ) and recall of 83 . 3% (5/6) . The case the parser misanalyzed is due to lexical coverage : the verb agree occurs in our lexicon  , but not with this category . 
The most common category for object relative pronouns  , ( NP nNP ) = ( S[dcl ] = NP) , has a recall of 76 . 2% (16 out of 21) and precision of 84 . 2% (16/19) . 
Free object relatives , NP = ( S[dcl ] = NP) , have a recall of 84 . 6% (11/13), and precision of 91 . 7% (11/12) . However , object extraction appears more frequently as a reduced relative  ( the man John saw )  , and there are no lexical categories indicating this extraction  . Reduced relative clauses are captured by a type -changing rule NP nNP ! S[dcl ] = NP  . This rule was applied 56 times in the gold standard , and 70

NP the suit

S[dcl]nNP ( S[dcl]nNP ) = NP seeks
NP a court order ( SnNP ) n(SnNP)
S[ng]nNP ( S[ng]nNP ) = PP ( ( S[ng]nNP ) =PP ) = NP preventing
NP the guild

PP = ( S[ng]nNP ) from
S[ng]nNP ( S[ng]nNP ) = NP ( S[ng]nNP ) = NP punishing ( S[ng]nNP ) = NP[c]conjor ( S[ng]nNP ) = NP ( S[ng]nNP ) = PP retaliating
PP = NP against

Mr : Trudeau
Figure 2: Right node raising output produced by our parser . Punishing and retaliating are unknown words . 
times by the parser , out of which 48 times it corresponded to a rule in the gold standard  ( or 34 times , if the exact bracketing of the S[dcl ] = NP is taken into account ? this lower figure is due to attachment decisions made elsewhere in the tree  )  . 
These figures are difficult to compare with standard Treebank parsers  . Despite the fact that the original Treebank does contain traces for movement  , none of the existing parsers try to generate these traces  ( with the exception of Collins ? Model 3 , for which he only gives an overall score of 96 . 3%/98 . 8% P/R for subject extraction and 81 . 4%/59 . 4% P/R for other cases ) . The only ? long range ? dependency for which Collins gives numbers is subject extraction hSBAR  , WHNP , SG , Ri , which has labeled precision and recall of 90 . 56% and 90 . 56% , whereas the CCG model achieves a labeled precision and recall of  94  . 3% and 96 . 5% on the most frequency subject extraction dependency h NP nNP  , ( NP nNP ) = ( S[dcl]nNP) , S[dcl]nNPi , which occurs 262 times in the gold standard and was produced 256 times by our parser . However , out of the 15 cases of this relation in the gold standard that our parser did not return  ,   8 were in fact analyzed as subject extraction of b are infinitival sh NP nNP  , ( NP nNP ) = ( S[b]nNP) , S[b]n NP i , yielding a combined recall of 97 . 3% . 
7 Lexical coverage
The most serious problem facing parsers like the present one with large category sets is not so much the standard problem of unseen words  , but rather the problem of words that have been seen  , but not with the necessary category . 
For standard Treebank parsers , the latter problem does not have much impact , if any , since the Penn Treebank tagset is fairly small , and the grammar underlying the Treebank is very permissive  . However , for CCG this is a serious problem : the first three rows in table  4 show a significant difference in performance for sentences with complete lexical coverage  ( ? No missing ? ) and sentences with missing lexical entries ( ? Missing ? )  . 
Using the POS tags in the corpus , we can estimate the lexical probabilities P ( w j c ) using a linear interpolation between the relative frequency estimates ? P  ( w j c ) and the following approximation : 5 ? P tags ( w j c ) = ? t2 tags ? P ( w j t ) ? P ( t j c ) We smooth the lexical probabilities as follows : ? P  ( w j c ) = ? ? P ( w j c ) + ( 1 ? ) ? P tags ( w j c ) Table 4 shows the performance of the baseline model with a frequency cutoff of  5 and 10 for rare words and with a smoothed and nonsmoothed lexi-con  . 6 This frequency cutoff plays an important role here-smoothing with a small cutoff yields worse performance than not smoothing  , whereas smoothing with a cutoff of 10 does not have a significant impact on performance  . Smoothing the lexicon in this way does make the parser more robust  , resulting incomplete coverage of the test set . However , it does not affect overall performance , nor does it alleviate the problem for sentences with missing lexical entries for seen words  . 
5 We compute ? in the same way as Collins (1999) , p .  185 . 
6Smoothing was only done for categories with a total frequency of  100 or more . 
Baseline , Cut off = 5 Baseline , Cut off = 10HWDep , Cut off = 10 ( Missing = 463 sentences )   ( Missing = 387 sentences )   ( Missing = 387 sentences ) Non-smoothed Smoothed Non-smoothed Smoothed Smoothed 
Parse failures 6?5 ? ? hP;H;Si,All 75 . 7 73 . 2 76 . 2 76 . 3 83 . 9 hP;H;Si , Missing 66 . 4 64 . 2 67 . 0 67 . 1 75 . 1h P ; H ; Si , Nomissing 78 . 5 75 . 9 78 . 5 78 . 6 86 . 6 Table 4: The impact of lexical coverage , using a different cutoff for rare words and smoothing  ( section 23 )   8 Conclusion and future work We have compared a number of generative probability models of CCG derivations  , and shown that our best model recovers 89 . 9% of wordword dependencies on section 23 of CCGbank . On section 00, it recovers 89 . 7% of wordword dependencies . These figures are surprisingly close to the figure of  90  . 9% reported by Collins (1999) on section 00 , given that , in order to allow a direct comparison , we have used the same interpolation technique and beam strategy as Collins  ( 1999 )  , which are very unlikely to be as well-tuned to our kind of grammar  . 
As is to be expected , a statistical model of a CCG extracted from the Treebank is less robust than a model with an overly permissive grammar such as Collins  ( 1999 )  . This problem seems to stem mainly from the incomplete coverage of the lexicon  . We have shown that smoothing can compensate for entirely unknown words  . However , this approach does not help on sentences which require previously unseen entries for known words  . We would expect a less naive approach such as applying morphological rules to the observed entries  , together with better smoothing techniques , to yield better results . 
We have also shown that a statistical model of CCG benefits from word word dependencies to a much greater extent than a less linguistically motivated model such as Collins ? Model  1  . This indicates to us that , although the task faced by a CCG parser might seem harder prima facie  , there are advantages to using a more linguistically adequate grammar  . 

Thanks to Stephen Clark , Miles Osborne and the ACL02 referees for comments . Various parts of the research were funded by EPSRC grants  GR/M96889 and GR/R02450 and an EPSRC studentship . 

Steven Abney .  1997 . Stochastic Attribute Value Grammars . 
Computational Linguistics , 23(4).
Bob Carpenter .  1992 . Categorial Grammars , Lexical Rules , and the English Predicative . In R . Levine , ed . , Formal Grammar : Theory and Implementation . OUP . 
Eugene Charniak .  1999 . A Maximum-Entropy-Inspired Parser . 
TRCS-99-12, Brown University.
David Chiang .  2000 . Statistical Parsing with an Automatically -Extracted Tree Adjoining Grammar  38th ACL , Hong Kong , pp .  456-463 . 
Stephen Clark and Julia Hockenmaier .  2002 . Evaluating a Wide-Coverage CCG Parser . LREC Beyond PARSEVAL workshop , Las Palmas , Spain . 
Stephen Clark , Julia Hockenmaier , and Mark Steedman . 
2002 . Building Deep Dependency Structures Using a Wide -Coverage CCG Parser  . 40th ACL , Philadelphia . 
Michael Collins .  1997 . Three Generative Lexicalized Models for Statistical Parsing  . 35th ACL , Madrid , pp .  16?23 . 
Michael Collins .  1999 . Head-Driven Statistical Models for Natural Language Parsing  . Ph . D . thesis , University of

Daniel Gildea .  2001 . Corpus Variation and Parser Performance . EMNLP , Pittsburgh , PA . 
Julia Hockenmaier .  2001 . Statistical Parsing for CCG with Simple Generative Models  . Student Workshop , 39th ACL/10th EACL , Toulouse , France , pp .  7?12 . 
Julia Hockenmaier and Mark Steedman 2002 . Acquiring Compact Lexicalized Grammars from a Cleaner Treebank  . Third
LREC , Las Palmas , Spain.
Joshua Goodman .  1997 . Probabilistic Feature Grammars . 
IWPT , Boston.
Mark Johnson .  1998 . PCFG Models of Linguistic Tree Representations . Computational Linguistics , 24(4) . 
Adwait Ratnaparkhi .  1996 . A Maximum Entropy Part-Of-Speech Tagger . EMNLP , Philadelphia , pp .  133?142 . 
Mark Steedman .  2000 . The Syntactic Process . The MIT Press,
Cambridge Mass.
