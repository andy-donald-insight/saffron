The Computational Lexical Semantics of Syntagmatic Relations 
Evelyne Viegas , Stephen Beale and Sergei Nirenburg
New Mexico State University
Computing Research Lab,
Las Cruces , NM 88003,

viegas , sb , sergei?crl , nmsu.edu
Abstract
In this paper , we address the issue of syntagmatic expressions from a computational lexical semantic perspective  . From a representational viewpoint , we argue for a hybrid approach combining linguistic and conceptual paradigms  , in order to account for the continuum we find in natural languages from free combining words to frozen expressions  . In particular , we focus on the place of lexical and semantic restricted cooccurrences  . From a processing viewpoint , we show how to generate/analyze syntagmatic expressions by using an efficient constraint -based processor  , wellfitted for a knowledge-driven approach . 
1 Introduction
You can take advantageo \] the chamber maid 1 is not a collocation one would like to generate in the context of a hotel to mean " use the services of  . " This is why collocations should constitute an important part in the design of Machine Translation or Multilingual 
Generation systems.
In this paper , we address the issue of syntagmatic expressions from a computational lexical semantic perspective  . From a representational viewpoint , we argue for a hybrid approach combining linguistic and conceptual paradigms  , in order to account for the continuum we find in natural languages from free combining words to frozen expressions  ( such as in idioms kick the ( proverbial ) bucket )  . In particular , we focus on the representation f restricted semantic and lexical cooccurrences  , such as heavy smoker and pro#ss or . . . students respectively , that we define later . From a processing viewpoint , we show how to generate/analyze syntagmatic expressions by using an efficient constraint -based processor  , wellfitted for a knowledge-driven approach . In the following , we first compare different approaches to collocations  . Second , we present our approach in terms of representation ad processing  . Finally , we show how to facilitate the acquisition of cooccurrences by using  1  ) the formalism of lexical rules ( LRs )  , 2) an 1Lederer , R .  1990 . Anguished English A Laurel Book , Dell

inheritance hierarchy of Lexical Semantic Functions  ( LSFs )  . 
2 Approaches to Syntagmatic
Relations
Syntagmatic relations , also known as collocations , are used differently by lexicographers , linguists and statisticians denoting almost similar but not identical classes of expressions  . 
The traditional approach to collocations has been lexicographic  . Here dictionaries provide information about what is unpredictable or idiosyncratic  . Benson ( 1989 ) synthesizes Hausmann's studies on collocations , calling expressions such as commit murder , compile a dictionary , inflict a wound , etc . " fixed combinations , recurrent combinations " or " collocations " . In Hausmann's terms ( 1979 ) a collocation is composed of two elements , abase ( " Basis " ) and a collocate ( " Kollokator " )  ; the base is semantically autonomous whereas the collocate cannot be semantically interpreted in isolation  . In other words , the set of lexical collocates which can combine with a given basis is not predictable and therefore collocations must be listed in dictionaries  . 
It is hard to say that there has been a real focus on collocations from a linguistic perspective  . The lexicon has been broadly sacrificed by both English-speaking schools and continental European schools  . 
The scientific agenda of the former has been largely dominated by syntactic issues until recently  , whereas the latter was more concerned with pragmatic aspects of natural anguages  . The focus has been on grammatical collocations uch as adapt to  , aimat , look \] or . Lakoff ( 1970 ) distinguishes a class of expressions which cannot undergo certain operations  , such as nominalization , causativization : the problem is hard ; * the hardness of the problem ; * the problem hardened . The restriction on the application of certain syntactic operations can help define collocations such as hard problem  , for example . Mel'~uk's treatment of collocations will be detailed below  . 
In recent years , there has been a resurgence of statistical approaches applied to the study of natural languages  . Sinclair ( 1991 ) states that '% word vestigation is called a collocate of it  .   .   .   . Collocation is the occurrence of two or more words within a short space of each other in a text "  . The problem is that with such a definition of collocations  , even when improved , zone identifies not only collocations but free -combining pairs frequently appearing together such as lawyer-client  ; doctor-hospital . 
However , nowadays , researchers seem to agree that combining statistic with symbolic approaches lead to quantifiable improvements  ( Klavans and Resnik ,  1996) . 
The Meaning Text Theory Approach The Meaning Text Theory  ( MTT ) is a generator-oriented lexical grammatical formalism  . Lexical knowledge is encoded in an entry of the Explanatory Combinatorial Dictionary  ( ECD )  , each entry being divided into three zones : the semantic zone  ( a semantic network representing the meaning of the entry in terms of more primitive words  )  , the syntactic zone ( the grammatical properties of the entry ) and the lexical combinatorics zone ( containing the values of the Lexical Functions ( LFs )  3 )  . LFs are central to the study of collocations : A lexical function F is a correspondence which associates a lexical item L  , called the keyword of F , with a set of lexical items
F ( L)-the value of F . ( Mel'6uk , 1988) 4
We focus here on syntagmatic LFs describing cooccurrence rlation such as pay attention  , legitimate complaint ; from a distance . 5 Heylen et al ( 1993 ) have worked out some cases which help license a starting point for assigning LFs  . 
They distinguish four types of syntagmatic LFs : ? evaluative qualifier 
Magn ( bleed ) = profusely ? distributional qualifier
Mult(sheep ) = flock ? co-occurrence
Loc-in(distance ) = at a distance ? verbal operator
Operl ( attention ) = pay
The MTT approach is very interesting as it provides a model of production well suited for generation with its different strata and also a lot of lexical-semantic information  . It seems nevertheless that all 2Church and Hanks ( 1989 )  , Smadja ( 1993 ) use statistics in their algorithms to extract collocations from texts  . 
3See ( Iordanskaja et al , 1991) and ( Ramos et al ,  1994 ) for their use of LFs in MTT and NLG respectively . 
4 ( Held , 1989) contrasts Hausman's base and collate to
Mel'tuk's keyword and LF values.
5There are about 60 LFs listed said to be universal ; the lexicograpproach of Mel'tuk and Zolkovsky has been applied among other languages to Russian  , French , German and English . 
the collocational information is listed in a static way  . 
We believe that one of the main drawbacks of the approach is the lack of any predictable calculi on the possible expressions which can collocate with each other semantically  . 
3 The Computational Lexical
Semantic Approach
In order to account for the continuum we find in natural languages  , we argue for a continuum perspective , spanning the range from free-combining words to idioms  , with semantic ollocations and idiosyncrasies in between as defined in  ( Viegas and Bouillon ,  1994 ) : ? free-combining words ( the girlate can dies ) * semantic collocations ( fast car ; longbook ) 6? idiosyncrasies ( large coke ; green jealousy ) ? idioms ( to kick the ( proverbial ) bucket ) Formally , we go from a purely compositional approach in " free-combining words " to a noncompositional approach in idioms  . Inbetween , a ( semi- ) compositional approach is still possible . ( Vie-gas and Bouillon ,  1994 ) showed that we can reduce the set of what are conventionally considered as idiosyncrasies by differentiating " true " idiosyncrasies  ( difficult to derive or calculate ) from expressions which have well-defined calculi , being compositional in nature , and that have been called semanticollo-cations . In this paper , we further distinguish their idiosyncrasies into : ? restricted semantic co-occurrence  , where the meaning of the cooccurrence is semi -compositional between the base and the collocate  ( strong coffee , pay attention , heavy smoker ,   . . . ) ? restricted lexical co-occurrence , where the meaning of the collocate is compositional but has a lexical idiosyncratic behavior  ( lecture . . . 
student ; rancid butter ; sour milk).
We provide below examples of restricted semantic cooccurrences in  ( 1 )  , and restricted lexical cooccurrences in (2) . 
Restricted semantic co-occurrence The semantics of the combination of the entries is semi-compositional  . In other words , there is an entry in " the lexicon for the base ,   ( the semantic ollocate is encoded inside the base )  , whereas we cannot directly refer to the sense of the semantic collocate in the lexicon  , as it is not part of its senses . We assign the cooccurrence a new semi -compositional sense  , 6See ( Pustejovsky ,  1995 ) for his account of such expressions using a coercion operator  . 
1 329 where the sense of the base is composed with a new sense for the collocate  . 
( la ) #O=\[key:rel: ( lb ) #0 =\[ key:rel:"smoker " , \[ syntagmatic : LSF Intensity \[ base : #0 , collocate : \[ key : " heavy " , gram:\[subCat : Attributive , freq:\[value:8\]\]\]\]\] . . . \]" attention " , \[ syntagmatic : LSF Oper\[base : #0 , collocate : \[ key : " pay " , gram:\[subCat : Support Verb , freq:\[value:5\]\]\]\]\] . . . \] In examples (1) , the LSFs ( LSF Intensity , LS-FO per ,   . . . ) are equivalent ( and some identical ) to the LFs provided in the ECD . The notion of LSF is the same as that of LFs . However , LSFs and LFs are different in two ways : i ) conceptually , LSFs are organized into an inheritance hierarchy ; ii ) formally , they are rules , and produce a new entry composed of two entries , the base with the collocate . 
As such , the new composed entry is ready for processing . These LSFs signal a compositional syntax and a semi-compositional semantics  . For instance , in(la ) , a heavy smoker is somebody who smokes a lot , and nota " fat " person . It has been shown that one cannot code in the lexicon all uses of heavy for heavy smoker  , heavy drinker ,   . . . . Therefore , we do not have in our lexicon for heavy a sense for " alot "  , or a sense for " strong " to be composed with wine  , etc . . . It is wellknown that such cooccurrences are lexically marked  ; if we allowed in our lexicons a proliferation of senses  , multiplying ambiguities in analysis and choices in generation  , then there would be no limit to what could be combined and we could end up generating * heavy coffee with the sense of " strong " for heavy  , in our lexicon . 
The left hand side of the rule LSF Intensity specifies an " Intensity-Attribute " applied to an event which accepts aspectual features of duration  . In ( la ), the event is smoke . The LSF Intensity also provides the syntax -semantic interface  , allowing for an Adj-Noun construction to be either predicative  ( the car is red ) or attributive ( the red car )  . We need therefore to restrict the cooccurrence to the Attributive use only  , as the predicative use is not allowed : ( the smoker is heavy ) has a literal meaning or figurative , but not collocational . 
In ( lb ) again , there is no sense in the dictionary for pay which would mean concentrate  . The rule LS-FO per makes the verb a verbal operator  . No further restriction is required . 
Restricted lexical co-occurrence The semantics of the combination of the entries is compositional  . In other words , there are entries in the lexicon for the base and the collocate  , with the same senses as in the cooccurrence . Therefore , we can directly refer to the senses of the cooccurring words  . 
What we are capturing here is a lexical idiosyncrasy or in other words  , we specify that we should prefer this particular combination of words  . This is useful for analysis , where it can help disambiguate a sense , and is most relevant for generation ; it can be viewed as a preference among the paradigmatic family of the cooccurrence  . 
(2a)#O=\[key:tel:"truth " , \[ syntagmatic : LSFSyn\[base : #0 , collocate : \[ key : " plain " , sense : adj 2 , Ir : \[ comp:no , superl:no\]\]\]\] . . . \](2b)#0=\[key:rel:"pupil " , \[ syntagmatic : LSFSyn\[base : #0 , collocate : \[ key : " teacher " , sense : n2 , freq:\[value:5\]\]\]\] . . . \](2c)#O=\[key:tel:"conference " , \[ syntagmatic : LSFSyn\[base : #0 , collocate : \[ key:"student " , sense : nl , freq:\[value:9\]\]\]\] . . . \] In examples (2) , the LSF Synproduces a new entry composed of two or more entries  . As such , the new entry is ready for processing . LSF Synsignals a compositional syntax and a compositional semantics  , and restricts the use of lexemes to be used in the composition  . We can directly refer to the sense of the collocate  , as it is part of the lexicon . 
In ( 2a ) the entry for truth specifies one cooccurrence ( plaintruth )  , where the sense of plain here is adj2 ( obvious ) , and not say adj 3 ( flat ) . The syntagmatic expression inherits all the zones of the entry for " plain "  , sense adj 2 , we only code here their regularities . For instance , " plain " can be used as " plainer .   .   .   . plainest " in its " plain " sense in its adj2 entry , but not as such within the lexical cooccurrence " * plainer truth "  , "* plainest truth " , we therefore must block it in the collocate , as expressed in ( comp:no , superhno ) . In other words , we will not generate " plainer/plainest truth " . Examples ( 2b ) and ( 2c ) illustrate complex entries as there is no direct grammatical dependency between the base and the collocate  . In (2b ) for instance , we prefer to associate teacher in the context of a pupil rather than any other element belonging to the paradigmatic family of teacher such as professor  , instructor . 
Formally , there is no difference between the two types of cooccurrences  . In both cases , we specify the base ( which is the word described in the en-occurrence in some corpus  , and the LSF which links the base with the collocate  . Using the formalism of typed feature structures , both cases are of type
Cooccurrence as defined below:
Cooccurrence=\[base : Entry , collocate : Entry , freq : Frequency \]; 3 . 1 Processing of Syntagrnatic Relations We utilize an efficient constraint-based control mechanism called Hunter-Gatherer  ( HG )   ( Beale ,  1997) . 
HG allows us to mark certain compositions as being dependent on each other and then forget about h + them  . Thus , once we have two lexicon entries bitter that we knowgo together  , HG will ensure that heavy they do . HG also gives preference to cooccurring big compositions  . In analysis , meaning representations constructed using cooccurrences are preferred over v + those that are not  , and , in generation , realizations oppose involving cooccurrences are preferred over equally oblige correct  , but non-cooccurring realizations , r The real work in processing is making sure that we have the correct two entries to put together  . In re-striated semantico-occurrences , the cooccurrence does not have the correct sense in the lexicon  . For example , when the phrase heavy smoker is encountered , the lexicon entry for heavy would not contain the correct sense  . (la ) could be used to create the correct entry . In ( la ) , the entry for smoker contains the key , or trigger , heavy . This signals the analyzer to produce another sense for heavy smoker  . This sense will contain the same syntactic information present in the " old " heavy  , except for any modifications listed in the " gram " section  ( see ( la ) ) . The semantics of the new sense comes directly from the LSF  . Generation works the same , except the trigger is different . The input to generation will be a SMOKE event along with an Intensity-Attribute  . 
(la ) , which would be used to realize the SMOKE event , would trigger LSF Intensify which has the Intensity-Attribute in the lefthand side  , thus confirming the production of heavy . 
Restricted lexical cooccurrences are easier in the v+N sense that the correct entry already exists in the lexicon  . The analyzer/generator simply needs to detect the cooccurrence and add the constrain that the N+N corresponding senses be used together  . In examples like (2b ) , there is no direct grammatical or semantic relationship between the words that cooccur  . Thus , the entire clause , sentence or even text may have to be searched for the cooccurrence  . In practice , we limit such searches to the sentence level . 
7The selection of cooccurrences i part of the lexical process  , in other words , if there are reasons not to choose a cooccurrence because of the presence of modifiers or because of stylistics reasons  , the generator will not generate the cooccurrence . 
3 . 2 Acquisition of Syntagmatic Relations The acquisition of syntagmatic relations is knowledge intensive as it requires human intervention  . In order to minimize this cost we rely on conceptual tools such as lexical rules  , on the LSF inheritance hierarchy . 
Lexical Rules in Acquisition The acquisition of restricted semantico-occurrences can be minimized by detecting rules between different classes of cooccurrences  ( modulo presence of derived forms in the lexicon with same or subsumed semantics  )  . Looking at the following example,
N <=> V+Adv resentment resent bitterly smoker smoke heavily eater eat * bigly hdv <=> Adv+Adj -ed strongly strongly opposed morally morally obliged we see that after having acquired with human intervention cooccurrences belonging to the A+N class  , we can use lexical rules to derive the V+Adv class and also Adv+Adj-ed class  . 
Lexical rules are a useful conceptual tool to extend a dictionary  . ( Viegas et al ,  1996 ) used derivational lexical rules to extend a Spanish lexicon  . We apply their approach to the production of restricted semantico-occurrences  . Note that eat bigly will be produced but then rejected  , as the form bigly does not exist in a dictionary . The rules over generate cooccurrences . This is a minor problem for analysis than for generation  . To use these derived restricted cooccurrences in generation  , the output of the lexical rule processor must be checked  . This can be done in different ways : dictionary check  , corpus check and ultimately human check . 
Other classes , such as the ones below can be extracted using lexicostatistical tools  , such as in ( Smadja ,  1993) , and then checked by a human . 
pay attention , meet an obligation , commit an offence ,   . . . 
dance marathon , marriage ceremony object of derision .   .   .   . 
LSFs and Inheritance We take advantage of 1 ) the semantics encoded in the lexemes , and 2) an inheritance hierarchy of LSFs . We illustrate briefly this notion of LSF inheritance hierarchy  . For instance , the lefthand side of LSF Change State specifies that it applies to foods  ( solid or liquid ) which are human processed , and produces the collocates rancid , rancio ( Spanish ) . Therefore it could apply to milk , butter , or wine . The rule would end up ( rancid wine ) which is fine in Spanish . We therefore need to further distinguish LSF Change State into LSF Change State Solid and LSF Change State Liquid  . 
This restricts the application of the rule to produce rancid butter  , by going down the hierarchy . This enables us to factor out information common to several entries  , and can be applied to both types of cooccurrences  . We only have to code in the cooccurrence information relevant to the combination  , the rest is inherited from its entry in the dictionary  . 
4 Conclusion
In this paper , we built on a continuum perspective , knowledge-based , spanning the range from free-combining words to idioms  . We further distinguished the notion of idiosyncrasies as defined in  ( Viegas and Bouillon ,  1994) , into restricted semantic co--occurrences and restricted lexical cooccurrences  . 
We showed that they were formally equivalent , hus facilitating the processing of strictly compositional and semi-compositional expressions  . Moreover , by considering the information in the lexicon as constraints  , the linguistic difference between compositionality and semi-compositionality becomes a virtual difference for Hunter -Gatherer  . We showed ways of minimizing the acquisition costs  , by 1 ) using lexical rules as a way of expanding cooccurrences  ,  2 ) taking advantage of the LSF inheritance hierarchy . 
The main advantage of our approach over the ECD approach is to use the semantics coded in the lexemes along with the language independent LSF inheritance hierarchy to propagater stricted semantic cooccurrences  . The work presented here is complete concerning representational aspects and processing aspects  ( analysis and generation ) : it has been tested on the translations of on-line unrestricted texts  . The largescale acquisition of restricted cooccurrences is in progress  . 
5 Acknowledgements
This work has been supported in part by DoDunder contract number  MDA-904-92-C-5189  . We would like to thank Pierrette Bouillon , L~o Wanner and R~miZajac for helpful discussions and the anonymous reviewers for their useful comments  . 

S . Beale .  1997 . HUNTER-GATHERER : Applying Constraint Satisfaction  , Branch-and-Bound and Solution Synthesis to Computational Semantics  . 
Ph.D . Diss ., Carnegie Mellon University.
M . Benson .  1989 . The Structure of the Colloca-tional Dictionary . In International Journal of Lexicography . 
K . W . Church and P . Hanks .  1989 . Word Association Norms , Mutual Information and Lexicography . In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics  . 
F . J . Hausmann .  1979 . Undictionnaire des collocation sest-il possible ? In Travaux de Linguistique et de Littdrature XVII  ,  1 . 
U . Heid .  1979 . D~crireles collocations : deux approches lexicographique set leur application dans un outil informatis d  . Internal Report , Stuttgart

D . Heylen .  1993 . Collocations and the Lexicalisa-tion of Semantic Information  . In Collocations , TR
ET-10/75, Taaltechnologie , Utrecht.
L . Iordanskaja , R . Kittredge and A . Polgu~re .  1991 . 
Lexical Selection and Paraphrase in a Meaning-text Generation Model  . In C . L . Paris , W . 
Swart out and W . Mann ( eds ), NLG in AI and
CL . Kluwer Academic Publishers.
J . Klavans and P . Resnik .  1996 . The Balancing Act , Combining Symbolic and Statistical Approaches to Language  . MIT Press , Cambridge Mass . , London

G . Lakoff .  1970 . Irregularities in Syntax . New York:
Holt , Rinehart and Winston , Inc.
I . Mel'~uk .  1988 . Paraphrase tlexique dans lath ~ orie Sens-Texte . In Bes & Fuchs ( ed ) Lexique 6 . 
S . Nirenburg and I . Nirenburg .  1988 . A Framework for Lexical Selection in NLG . In Proceedings of
COLING88.
J . Pustejovsky .  1995 . The Generative Lexicon . MIT

M . Ramos , A . Tutin and G . Lapalme .  1994 . Lexical Functions of Explanatory Combinatorial Dictionary for Lexicalization i Text Generation  . In P . 
St-Dizier & E . Viegas ( Ed ) Computational Lexical
Semantics : CUP.
J . Sinclair .  1991 . Corpus , Concordance , Collocations . Oxford University Press . 
F . Smadja .  1993 . Retrieving Collocations from Texts : Xtract . Computational Linguistics , 19(1) . 
E . Viegas and P . Bouillon .  1994 . Semantic Lexicons : the Cornerstone for Lexical Choice in Natural Language Generation  . In Proceedings of the 7th INLG , Kennebunk port . 
E . Viegas , B . Onyshkevych , V . Raskin and S . Nirenburg .  1996 . From Submito Submitted via Submission : on Lexical Rules in Large-scale Lexicon Acquisition  . In Proceedings of the 34th Annual Meeting of the Association for Computational Linguists  . 
L . Wanner .  1996 . Lexical Functions in Lexicography and Natural Language Processing  . John Benjamin
Publishing Company.

