Maximum Entropy Model Learning of the Translation Rules 
Kengo Sato and Masakazu Nakanishi
Department of Computer Science
Keio University
3-14-1 , Hiyoshi , Kohoku , Yokohama 223-8522 , Japan
email : satoken , czl@nak,ics.keio.ac.jp

This paper proposes a learning method of translation rules from parallel corpora  . This method applies the maximum entropy principle to a probabilistic model of translation rules  . First , we define feature functions which express statistical properties of this model  . Next , in order to optimize the model , the system iterates following steps : ( 1 ) selects a feature function which maximizes loglikelihood  , and ( 2 ) adds this function to the model incrementally . As computational cost associated with this model is too expensive  , we propose several methods to suppress the overhead in order to realize the system  . The result shows that it attained 69 . 54% recall rate . 
1 Introduction
A statistical natural anguage modeling can be viewed as estimating a combinational distribution X x Y -+  \[0  , 1\] using training data ( xl , yl > ,  . . . , < XT , YT > 6X . xY observed in corpora . For this topic , Baum (1972) proposed EM algorithm , which was basis of Forward-Backward algorithm for the hidden 
Markov model ( HMM ) and Inside-Outside algorithm ( Lafferty ,  1993 ) for the pr0 babilis-tic contextfree grammar ( PCFG )  . However , these methods have problems such as increasing optimization costs which is due to a lot of parameters  . Therefore , estimating a natural language model based on the maximum entropy  ( ME ) method ( Pietra et al , 1995; Berger et al , 1996) has been highlighted recently . 
On the other hand , dictionaries for multilingual natural anguage processing such as the machine translation has been made by human hand usually  . However , since this work requires a great deal of labor and it is difficult to keep description of dictionaries consistent  , here searches of automatical dictionaries making for machine translation  ( translation rules ) from corpora become active recently ( Kay and R Sschesen , 1993; Kaji and Aizono ,  1996) . 
In this paper , we notice that estimating a language model based on ME method is suitable for learning the translation rules  , and propose several methods to resolve problems in adapting ME method to learning the translation rules  . 
2 Problem Setting
If there exist(xl , Yl >, . . . ,  XT , YT )   6 X ? Y such that each x i is translated into Y i in the parallel corpora X  , Y , then its empirical probability distribution/5 obtained from observed training data is defined by : p  ( x , y)-c(x , y ) (1)
Ex , c(x , y ) where c(x , y ) is the number of times that x is translated into y in the training data  . 
However , since it is difficult to observe translating between words actually  , c(x , y ) is approximated with equation ( 2 ) for sentence aligned parallel corpora . 
<( x , y)c(x , y ) =
T (2) where X ~ is ith sentence in X . We denote that sentence X i is translated into sentence Y/in aligned parallel corpora  . And c~(x , y ) in the ith sentence . 
Our task is to learn the translation rules by estimating probability distribution p  ( yIx ) that xEX is translated into y EY from 15 ( x , y ) given above . 
3 Maximum Entropy Method 3 . 1 Feature Funct ion We define binary-valued indicator function f : X ? Y -+  0  , 1 which divide X x Y into two subsets . This is called feature function , which expresses statistical properties of a language model  . 
The expected value off with respected to iS(x , y ) is defined such as : p(f ) = p(x , y)f(x , y)(z)x , y Thus training data are summarized as the expected value of feature function f  . 
The expected value of a feature function f with respected top  ( ylx ) which we would like to estimate is defined such as : p  ( f ) = y~f i ( x ) p ( ylx ) f ( x , y ) (4) x , y where 15 ( x ) is the empirical probability distribution on X . Then , the model which we would like to estimate is under constraint to satisfy an equation such as : p  ( f ) = iS ( f )   ( 5 ) This is called the constraint equation . 
3.2 Maximum Entropy Principle
When there are feature functions fi(iE1,2, .   .   .   , n ) which are important to modeling processes , the distribution p we estimate should be included in a set of distributions defined such as : C = pE  7   9 IP ( f i )  =16 ( f i ) for iE 1 , 2 ,  . . . , n ( 6 ) where P is a set of all possible distributions on X ? Y  . 
For the distribution p , there is no assumption except equation (6) , so it is reasonable that the most uniform distribution is the most suitable for the training corpora  . 
The conditional entropy defined in equation ( 7 ) is used as the mathematical measure of the uniformity of a conditional probability p  ( ylx )  . 
H(p ) = - y ~( x ) p(ylx ) log p(ylx ) (7) x , y That is , the model p . which maximizes the entropy H should be selected from C  . 
p . -- argmaxH(p ) ( S ) pet
This heuristic is called the maximum entropy principle  . 
3.3 Parameter Estimation
In simple cases , we can find the solution to the equation ( 8 ) analytically . Unfortunately , there is no analytical solution in general cases , and we need a numerical algorithm to find the solution  . 
By applying the Lagrange multiplier to equation ( 7 )  , we can introduce the parametric form of p . 

Px(YIx)-Z >,( x ) exphifi(x,y ) (9)
Z , x(x ) = y ~ exp ( ~, ~ if i(x , y))
Y where each hi is the parameter for the feature f i  . P ~ is known as Gibbs distribution . 
Then , to solve p . EC in equation (8) is equivalent to solve h . that maximize the loglikelihood :=- ( x ) log zj , ( z ) + xi (10) h . = argmaxkV(h ) Such h . can be solved by one of the numerical algorithm called the Improved Itera-tire Scaling Algorithm  ( Berger et al ,  1996) . 
1 . Start with hi = 0 for all iE 1, 2, .   .   . , n2 . Do for each iE 1, 2, .   .   .   , n : ~- ~( x ) p(ylx )$ i(x , y ) exp(AA if #( x , y )) = P ( f i ) x ~ y (11) where f#(x , y ) = E i ~= tf~(x , y ) ( b ) Update the value of Ai according to:
Ai ~- A ~+ AAi 4 Maximum Entropy Model
Learning of the Translation

The art of modeling with the maximum entropy method is to define an informative set of computationally feasible feature functions  . In this section , we define two models of feature functions for learning the translation rules  . 
3 . Goto step 2 if not all the Ai have converged To solve AAi in the step  ( 2a )  , the Newton's method is applied to equation (11) . 
3.4 Feature Selection
In general cases , there exist a large collection ~" of candidate features  , and because of the limit of machine resources , we cannot expect to obtain all iS ( f ) estimated in real-life . However , the Maximum Entropy Principle does not explicitly state how to select those particular constraints  . We build a subset SC ~" incrementally by iterating to adjoin a feature fE ~" which maximizes loglikelihood of the model to S  . This algorithm is called the Basic Feature Selection  ( Berger et al ,  1996) . 
Model 1: Cooccurrence Information
The first model is defined with cooccurrence information between words appeared in the corpus X  . 
1(xeW(d , w )) (12) f w(x , y ) = 0 ( otherwise ) where W(d , w ) is a set of words which appeared within d words from wEX  ( in our experiments , d = 5) . fw(x , y ) expresses the information on w for predicting that x is translated into y  ( Figure 1 )  . 
.  .   .   .   . W .   .   .   .   .   .   .   .   .   .   .   . X .   .   .   .   .   .   .   .   .   . ~' X pred ~ cti ~ power ""/ translation role .   .   .   .   .   .   .   .   .   .   .   .   .   .   . y .   .   .   .   .   .   .   .   .   .   .   .   .   .   . ~Y 1 . Start with S = O Figure 1: cooccurance information . Do for each candidate feature fE ~':
Compute the model Psus using Improve
Iterative Scaling Algorithm and the gain in the loglikelihood from adding this feature Model  2: Morphological Information The second model is defined with morphological information such as part-of-speech  . 
3 . Check the termination condition 4 . Select the feature \] with maximal gain 5 . Adjoinf to S6 . Compute Ps using Improve Iterative Algorithm 7 . Go to Step 2 lo sxt lft , s(x , Y ) = 1 and POS ( y ) s0 ( otherwise )   ( 13 ) where POS ( x ) is a part-of-speech tag for x . 
ft , u(x , y ) expresses the information on part-of-speech t , s for predicting that x is translated into y ( Figure 2 )  . If part-of-speech tag- .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . predictive ~"/ x .   .   .   .   .   .   .   .   .   . ~'- Xpower_"l ~'~' J translation mle .   .   .   .   .   .   .   .   .   .   .   .   .   .   . y .   .   .   .   .   .   .   .   .   .   .   .   .   .   .  ,  . y
Figure 2: morphological information gers for each language work extremely accurate  , then these feature functions can be generated automatically  . 
5 Implementation
Computational cost associated with the model described above is too expensive to realize the system for learning the translation rules  . We propose several methods to suppress the overhead  . 
An estimated probability p ~ ( yIx ) for a pair of ( x , y ) EX xY which has not been observed as the sample data in the parallel corpora X  , Y should be kept lower . According to equation (9) , we can allow to let fi(x , y ) = 0 ( for all iE 1 ,   .   .   . , n ) for non-observed ( x , y) . Therefore , we will accept observed ( x , y ) only instead of all possible ( x , y ) in summationi equation (11) , so that p ~ ( ylx ) can be calculated much more efficiently . 
Suppose that a set of ( x , y ) such that each member activates a feature function f is defined by : D  ( f ) =  ( x , y)eX?rlf(x , y ) = 1  ( 14 ) Shirai et al ( 1996 ) showed that if D ( f i ) and D ( fj ) were exclusive to each other , that is D(f i ) f qD(f j ) = O , then Ai and Xj could be estimated independently . Therefore , we can split a set of candidate feature functions  . T " into several exclusive subsets , and calculate Px ( Yl X ) more efficiently by estimating on each subset independently  . 
6 Experiments and Results
As the training corpora , we used 6 , 0 57 pairs of sentences included in Kodansya JapaneseEnglish Dictionary  , a machine-readable dictionary made by the Electrotechnical Laboratory  . By applying morphological analysis for the corpora  , each word was transformed to the infinitive form . We excluded words which appeared below 3 times or over 1  , 000 times from the target of learning . Consequently , our target for the experiments included 1 , 375 English words and 1 , 195 Japanese words , and we prepared 1 , 375 feature functions for model 1 and 2 , 744 for model 2 ( 5 6 part-of-speech for English and 49 part-of-speech for Japanese )  . 
We tried to learn the translation rules from English to Japanese  . We had two experiments : one of model 1 as the set of feature functions , and one of model 1+2 . For each experiment ,   500 feature functions were selected according to the feature selection algorithm described in section  3  . 4 , and we calculated p(yIx ) in equation (9) , that is , the probability that English word x is translated into Japanese word y  . For each English word , all Japanese word we reordered by estimated probability p  ( yix )  , and we evaluated the recall rates by comparing the dictionary  . 
Table 1 shows the recall rates for each experiment . The numbers for 15(x , y ) are the
Ta)le1:rec1sty ) 44 . 55% model 1 41 . 58% model 1+2 58 . 29% dlrates-~3rd 53 . 47% 63 . 37% 69 . 54%, - ~10th 58 . 42% 76 . 24% 80 . 1 3% recall rates when the empirical probability defined by equation  ( 1 ) was used instead of the estimated probability . It is showed that the model 1  +  2 attains higher recall rates than the model 1 and ~ ( x , y ) . 
Figure 3 shows the loglikelihood for each model plotted by the number of feature functions in the feature selection algorithm  . Notice that the loglikelihood for the model 1+2 is always higher than the model 1  . 
Thus , the model 1  +  2 is more ' effective than the model 1 for learning the translation rules . 
However , the result shows that the recall-9 . 04  . 11 . 06*&08-Ik12-&14-9 . 14  . 9  . 1 6 IIIIIIII 50   100   1~0   290   2~   ~0   350   400  ~  500 
I henun ~ od ~ t ~ ll
Figure 3: loglikelihood rates of the '1st ' for all models are not favorable . We consider that it is the reason for this to assume word-to-word translation rules implicitly  . 
7 Conclusions
We have described an approach to learn the translation rules from parallel corpora based on the maximum entropy method  . As feature functions , we have defined two models , one with cooccurrence information and the other with morphological information  . 
As computational cost associated with this method is too expensive  , we have proposed several methods to suppress the overhead in order to realize the system  . We had experiments for each model of features , and the result showed the effectiveness of this method  , especially for the model of features with cooccurrence and morphological information  . 

We would like to thank the Electrotechni-cal Laboratory forgiving us the machine-readable dictionary which was used as the training data  . 

L . E . Baum .  1972 . An inequality and associated maximumization technique in statistical estimation of probabilistic functions of a markov process  . In equalities , 3:1-8 . 
Adam L . Berger , Stephen A . Della Pietra , and Vincent J . Della Pietra .  1996 . A maximum entropy approach to natural language processing  . Computational Linguistics , 22(1):39-71 . 
Hiroyuki Kaji and Toshiko Aizono . 1996.
Extracting word correspondences from bilingual corpora based on word cooccurrence information  . In Proceedings of the 16th International Conference on Computational Linguistics  , pages 2328 . 
M . Kay and M . RSs chesen .  1993 . Text translation alignment . Computational
Linguistics , 19(1):121-142.
J . D . Lafferty .  1993 . A derivation of the insideoutside algorithm from the EM algorithm  . IBM Research Report . IBMT . J . 
Watson Research Center.
Stephen Della Pietra , Vincent Della Pietra , and John Lafferty .  1995 . Inducing features of random fields . Technical Report CMU-CS-95-144 , Carnegie Mellon University , May . 
Adwait Ratnaparkhi .  1997 . A linear observed time statistical parser based on maximum entropy models  . In Proceedings of Second Conference On Empirical Methods in Natural Language Processing  . 
Jeffrey C . Reynar and Adwait Ratnaparkhi.
1997 . A maximum entropy approach to identifying sentence boundaries  . In Proceedings of the 5th Applied Natural Language Processing Conference . 
Ronald Rosenfeld .  1996 . A maximum entropy approach to adaptive statistical language modeling  . Computer , Speech and
Language , (10):187-228.
Kiyoaki Shirai , Kentaro Inui , Takenobu
Tokunaga , and Hozumi Tanaka . 1996.
A maximum entropy model for estimating lexical bigrams  ( in Japanese )  . In SIG Notes of the Information Processing Society of Japan  , number 96-NL-116 . 
Takehito Utsuro , Takashi Miyata , and Yuji
Matsumoto .  1997 . Maximum entropy model learning of subcategorizato in preference  . In Proceedings of the 5th Workshop on Very Large Corpora , pages 246-260 , August . 

