Automatic Retrieval and Clustering of Similar Words 
Dekang Lin
Department of Computer Science
University of Manitoba
Winnipeg , Manitoba , Canada R3T2N2
lindek@cs.umanitoba.ca

Bootstrapping semantics from text is one of the greatest challenges in natural language learning  . 
We first define a word similarity measure based on the distributional pattern of words  . The similarity measure allows us to construct a thesaurus using a parsed corpus  . We then present a new evaluation methodology for the automatically constructed thesaurus  . The evaluation results show that the the-saurns is significantly closer to WordNet han Roget 
Thesaurus is.
1 Introduction
The meaning of an unknown word can often be inferred from its context  . Consider the following ( slightly modified ) example in ( Nida ,  1975 , p . 167): (1) A bottle of tezgii in o is on the table . 
Everyone likestezgiiino.
Tezgii in o makes you drunk .
We make tezgii in o out of corn.
The contexts in which the word tezgii in o is used suggest hattez giiino may be a kind of alcoholic beverage made from corn mash  . 
Bootstrapping semantics from text is one of the greatest challenges in natural anguage learning  . It has been argued that similarity plays an important role in word acquisition  ( Gentner ,  1982) . Identifying similar words is an initial step in learning the definition of a word  . This paper presents a method for making this first step  . For example , given a corpus that includes the sentences in (1) , our goal is to be able to infer that tezgii in o is similar to " beer "  , " wine " , " vodka " , etc . 
In addition to the long term goal of bootstrapping semantics from text  , automatic identification of similar words has many immediate applications  . 
The most obvious one is thesaurus construction . An automatically created thesaurus offers many advantages over manually constructed thesauri  . Firstly , the terms can be corpus-or genre-specific . Manually constructed general-purpose dictionaries and thesaur include many usages that are very infrequent in a particular corpus or genre of documents  . 
For example , one of the 8 senses of " company " in WordNet 1 . 5 is a " visitor/visitant " , which is a hyponym of " person " . This usage of the word is practically never used in newspaper articles  . However , its existance may prevent a coreferencer cognizer to rule out the possiblity for personal pronouns to refer to " company "  . Secondly , certain word usages may be particular to a period of time  , which are unlikely to be captured by manually compiled lexicons  . For example , among 274 occurrences of the word " westerner " in a 45 million word San Jose Mercury corpus , 55% of them refer to hostages . If one needs to search hostage-related articles , " west-emer " may well be a good search term . 
Another application of automatically extracted similar words is to help solve the problem of data sparseness in statistical natural language processing  ( Dagan et al , 1994; Essen and Steinbiss ,  1992) . 
When the frequency of a word does not warrant reliable maximum likelihood estimation  , its probability can be computed as a weighted sum of the probabilities of words that are similar to it  . It was shown in ( Dagan et al ,  1997 ) that a similarity-based smoothing method achieved much bettere sults than backoff smoothing methods in word sense disambiguation  . 
The remainder of the paper is organized as follows  . The next section is concerned with similarities between words based on their distributional patterns  . The similarity measure can then be used to create a thesaurus  . In Section 3 , we evaluate the constructed thesauri by computing the similarity between their entries and entries in manually created thesauri  . Section 4 briefly discuss future work in clustering similar words  . Finally , Section 5 reviews related work and summarize our contributions  . 
7682 Word Similarity
Our similarity measure is based on a proposal in ( Lin ,  1997) , where the similarity between two objects is defined to be the amount of information contained in the commonality between the objects divided by the amount of information in the descriptions of the objects  . 
We use a broadcoverage parser ( Lin , 1993; Lin ,  1994 ) to extract dependency triples from the text corpus  . A dependency triple consists of two words and the grammatical relationship between them in the input sentence  . For example , the triples extracted from the sentence " I have a brown dog " are:  ( 2 )   ( have subjI )  , ( Isubj-of have ) , ( do gobj-of have ) , ( do gadj-mod brown ) , ( brown adj-mod-of dog ) , ( dog deta ) , ( a det-of dog ) We use the notation II w , r , w'll to denote the frequency count of the dependency triple  ( w , r , w ~) in the parsed corpus . When w , r , or w ~ is the wildcard (*) , the frequency counts of all the dependency triples that matches the rest of the pattern are summed up  . For example , Ilcook , obj ,   *11 is the total occurrences of cook-object relationships in the parsed corpus  , and I1 . , * ,   *11 is the total number of dependency triples extracted from the parsed corpus  . 
The description of a word w consists of the frequency counts of all the dependency triples that matches the pattern  ( w ,   .  ,  . ) . The commonality between two words consists of the dependency triples that appear in the descriptions of both words  . For example , (3) is the the description of the word " cell " . 
(3) Ilcell , subj-of , absorbll = l
Ilcell , subj-of , adapt\[l = l
Ilcell , subj-of , behavell = l\[I cell , pobj-of , in 11 = 159\[\[cell , pobj-of , insidell = 16
Ilcell , pobj-of , into ll = 30
Ilcell , nmod-of , abnormality ll = 3
Ilcell , nmod-of , anemial l = 8
Ilcell , nmod-of , architecturell = l\[\[cell , obj-of , attackl\[=6\[\[cell , obj-of , bludge on \[\[= l\[I cell , obj-of , call ll = l1
H cell , obj-of , come from l\[=3
Ilcell , obj-of , contain ll--4
Ilcell , obj-of , decoratell = 2...***
I\[cell,nmod , bacterial l = 3
Ilcell , nmod , blood vessel H = l
II cell , nmod , bod Yll = 2
Ilcell , nmod , bone marrow ll = 2
Ilcell , nmod , burial H = l
Ilcell , nmod , chamele onll = l
Assuming that the frequency counts of the dependency triples are independent of each other  , the information contained in the description of a word is the sum of the information contained in each individual frequency count  . 
To measure the information contained in the statement IIw  , r , w'H = c , we first measure the amount of information in the statement that a randomly selected dependency triple is  ( w , r , w ') when we do not know the value of II w , r , w'll . We then measure the amount of information in the same statement when we do know the value of II w  , r , w'II . The difference between these two amounts is taken to be the information contained in Hw  , r , w'\[l = c . 
An occurrence of a dependency triple ( w , r , w ' ) can be regarded as the cooccurrence of three events : 
A : a randomly selected word is w;
B : a randomly selecte dependency tpeisr;
C : a randomly selected word is w~.
When the value of Ilw , r , w ' ll is unknown , we assume that A and C are conditionally independent given B  . The probability of A , B and C cooccurring is estimated by
PMLE(B ) PMLE(A\[B)PMLE(C\[B) , where PMLE is the maximum likelihood estimation of a probability distribution and 
P . LE(B ) = II *,*,* ll'
P ., ~ E(AIB ) = II *, ~,* ll'
P , LE(CIB ) =
When the value of Hw , r , w ~ H is known , we can obtain PMLE(A , B , C ) directly : PMLE(A , B , C ) =\[\[ w , r , wll/\[\[* ,   , * HLetI(w , r , w  ~ ) denote the amount information contained in Hw , r , w ~\]\] = c . Its value can be corn-simHindte , ( Wl , W2) = ~ , ( r , w)eT(w , ) nT ( w2) min(I(w l , r , w ) , I(w2 , r , w ))\] T ( W l ) NT ( w2) I simcosine ( W l , W2 ) = x/IZ ( w~ ) l?lZ ( w2 ) l2xIT ( wl ) nZ ( w2 ) lsimDice ( Wl , W2) = iT(wl ) l + lT(w2)I simJacard(Wl , W2) = T ( wl ) OT ( w2) l
T ( wl ) + T ( w2) l-IT ( W l ) rlT(w2) l
Figure 1: Other Similarity Measures puted as follows:
I(w , r , w ' ) =_ Iog ( PMLE ( B ) PMLE ( A\]B ) PMLE ( CIB ) ) -- ( -- logPMLE ( A , B , C )) - logIIw , r , wfl?ll* , r , * ll--II w , r , * llxll * , r , w'll It is worth noting that I(w , r , w ' ) is equal to the mutual information between w and w '  ( Hindle ,  1990) . 
Let T ( w ) be the set of pairs ( r , w ' ) such that logI w'r'w'l r?ll*'r'*ll is positive . We define the sim-wlr ~* X*~r~w ! ilarity sim ( wl , w2 ) between two words wl and w2 as follows: ) "~ ( r , w)eT(w , ) NT(w ~) ( I(Wl , rw ) + I(w2 , r , w )) ~- , ( r , w)eT ( wl)I(W l , r , w)q-~(r , w)eT ( w2) I(w2 , r , w ) We parsed a 64-million-word corpus consisting of the Wall Street Journal ( 24 million words )  , San Jose Mercury ( 21 million words ) and APNewswire ( 19 million words )  . From the parsed corpus , we extracted 56 . 5 million dependency triples (8 . 7 million unique ) . In the parsed corpus , there are 5469 nouns , 2173 verbs , and 2632 adjectives/adverbs that occurred at least 100 times . We computed the pairwise similarity between all the nouns  , all the verbs and all the adjectives/adverbs , uing the above similarity measure . For each word , we created a thesaurus entry which contains the top N ! words that are most similar to it  .   2 The thesaurus entry for word w has the following format : w  ( pos ) : Wl ,  81 , W2 ,  82 ,   .  ? ?  , WN , 8N where pos is a part of speech , wi is a word , si = sim(w , wi ) and si's are ordered in descending ' We used N=200 in our experiments 2The resulting thesaurus iavailable at : http://www . cs . umanitoba . caflindek/sims . htm . 
order . For example , the top 10 words in the noun , verb , and adjectiventries for the word " brief " are shown below : brief  ( noun ) : affidavit 0 . 13, petition 0 . 05, memo-randum 0 . 05, motion 0 . 05, lawsuit 0 . 05, deposition 0 . 05, slight 0 . 05, prospectus 0 . 04, document 0 . 04 paper 0 . 04  . . . .
brief ( verb ): tell 0 . 09, urge 0 . 07, ask 0 . 07, meet 0 . 06, appoint 0 . 06, elect 0 . 05, name 0 . 05, empower 0 . 05, summon 0 . 05, over rule 0 . 04  . . . .
brief ( adjective ): lengthy 0 . 13, short 0 . 12, recent 0 . 09, prolonged 0 . 09, long 0 . 09, extended 0 . 09, day long 0 . 08, scheduled 0 . 08, stormy 0 . 07, planned 0 . 06  . . . .
Two words are a pair of respective nearest neighbors  ( RNNs ) if each is the other's most similar word . Our program found 543 pairs of RNN nouns ,   212 pairs of RNN verbs and 382 pairs of RNN adjectives/adverbs in the automatically created thesaurus  . Appendix A lists every 10th of the RNNs . 
The result looks very strong . Few pairs of RNNs in Appendix A have clearly better alternatives  . 
We also constructed several other thesauri using the same corpus  , but with the similarity measures in Figure 1 . The measure simHinate is the same as the similarity measure proposed in  ( Hindle ,  1990) , excep that it does not use dependency triples with negative mutual information  . The measures imHindle , , is the same as simHindle xcept hat all types of dependency relationships are used  , instead of just subject and object relationships . The measure simcosine , simdice and simdacard are versions of similarity measures commonly used in information retrieval  ( Frakes and Baeza-Yates ,  1992) . 
Unlike sim , simninale and simHinater , they only simwN(wl , w2 ) = max c~eS ( w~ ) Ac2eS ( w2 ) maxcesuper ( c~ ) nsuper ( c2 ) logP ( cl + logP ( c2 ) !21 R ( ~l ) nR ( w2 ) lsim Roget ( Wl , W2 ) = IR ( wx ) l+lR ( w2 ) l where S ( w ) is the set of senses of w in the WordNet , super ( c ) is the set of ( possibly indirect ) superclasses of concept c in the WordNet , R ( w ) is the set of words that belong to a same Roget category as w  . 
Figure 2: Word similarity measures based on WordNet and Roget make use of the unique dependency triples and ignore their frequency counts  . 
3 Evaluation
In this section , we present an evaluation of automatically constructed thesauri with two manually compiled thesauri  , namely , WordNet l . 5 ( Miller et al , 1990) and Roget Thesaurus . We first define two word similarity measures that are based on the structures of WordNet and Roget  ( Figure 2 )  . The similarity measure simwN is based on the proposal in  ( Lin ,  1997) . The similarity measures imRoget treats all the words in Roget as features  . A word w possesses the feature fiff and w belong to a same Roget category  . The similarity between two words is then defined as the cosine coefficient of the two feature vectors  . 
With simwN and simRoget , we transform WordNet and Roget into the same format as the automatically constructed thesauri in the previou section  . 
We now discuss how to measure the similarity between two thesaurus entries  . Suppose two thesaurus entries for the same word are as follows : ' tO :  '//31~   81~'//12~   82~  .   .   . ~I ) N~SN
Their similarity is defined as : (4) sis
For example ,   ( 5 ) is the entry for " brief ( noun ) " in our automatically generated thesaurus and ( 6 ) and ( 7 ) are corresponding entries in WordNet hesaurus and Roget thesaurus  . 
(5) brief ( noun ): affidavit 0 . 13, petition 0 . 05, memor and um 0 . 05, motion 0 . 05, lawsuit 0 . 05, deposition 0 . 05, slight 0 . 05, prospectus 0 . 04, document 0 . 04 paper 0 . 04 . 
(6) brief ( noun ): outline 0 . 96, instrument 0 . 84, summary 0 . 84, affidavit 0 . 80, deposition 0 . 80, law 0 . 77, survey 0 . 74, sketch 0 . 74, resume 0 . 74, argument 0 . 74 . 
(7) brief ( noun ): recital 0 . 77, saga 0 . 77, autobiography 0 . 77, anecdote 0 . 77, novel 0 . 77, novelist 0 . 77, tradition 0 . 70, historian 0 . 70, tale 0 . 64 . 
According to (4) , the similarity between (5) and (6) is 0 . 297 , whereas the similarities between ( 5 ) and ( 7 ) and between ( 6 ) and ( 7 ) are 0 . 
Our evaluation was conducted with 4294 nouns that occurred at least 100 times in the parsed corpus and are found in both WordNet l  . 5 and the Roget Thesaurus . Table 1 shows the average similarity between corresponding entries in different hesauri and the standard deviation of the average  , which is the standardeviation of the data items divided by the square root of the number of data items  . 
Since the differences among simcosine , simdice and simJa card are very small , we only included the results for simcosine in Table  1 for the sake of brevity . 
It can be seen that sire , Hindler and cosine are significantly more similar to WordNet than Roget is  , but are significantly less similar to Roget than WordNet is  . The differences between Hindle and Hindler clearly demonstrate that the use of other types of dependencies in addition to subject and object relationships ivery beneficial  . 
The performance of sim , Hindler and cosine are quite close . To determine whether or not the differences are statistically significant  , we computed their differences in similarities to WordNet and Roget thesaurus for each individual entry  . Table 2 shows the average and standardeviation of the average difference  . Since the 95% confidence inter-

Roget sim
Hindle ~ cosine
Hindle average 0 . 178397 0 . 212199 0 . 204179 0 . 199402 0 . 164716 ~ av ~0 . 001636 0 . 001484 0 . 001424 0 . 001352 0 . 001200
Roget average
WordNet 0.178397 sim 0.149045
Hindler 0.14663 cosine 0.135697
Hindle 0 . 115489 aav 80 . 001636 0 . 001429 0 . 001383 0 . 001275 0 . 0 01140 vals of all the differences in Table 2 are on the positive side , one can draw the statistical conclusion that sim is better than simnind le ~  , which is better than simcosine . 
Table 2: Distribution of Differences sim-Hindle ~ sim -cosine 
Hindler-cosine sim-Hindle ~ sim-cosine

WordNet average ff avg 0 . 008021 0 . 000428 0 . 012798 0 . 000386 0 . 004777 0 . 000561
Roget average trav 80 . 002415 0 . 000401 0 . 013349 0 . 000375 0 . 010933 0 . 0 00509   4 Future Work Reliable extraction of similar words from text corpus opens up many possibilities for future work  . For example , one can go a step further by constructing a tree structure among the most similar words so that different senses of a given word can be identified with different subtrees  . Letwl, .   .   .   , Wn be a list of words in descending order of their similarity to a given word w  . The similarity tree for w is created as follows : ? Initialize the similarity tree to consist of a single node w  . 
? For i = l , 2 .   .   .   .   . n , insert wi as a child of wj such that wj is the most similar one to wi among w  , Wl .   .   .   .   . wi1 . 
For example , Figure 3 shows the similarity tree for the top-40 most similar words to duty . The first number behind a word is the similarity of the word to its parent  . The second number is the similarity of the word to the root node of the tree  . 
duty responsibility 0.21 role 0.120. ii
I a c t i o n 0 . ii 0 . 21 0 . i0 change 0 . 24 0 . 08 l__ . rule 0 . 16 0 . 08 l__ . restriction 0 . 27 0 . 08
IIban 0.30 0.08
Il__.sanction 0.19 0.08
I schedule 0. Ii 0.07
I regulation 0 . 37 0 . 07 challenge 0 . 13 0 . 07 l__ . issue 0 . 13 0 . 07
I reason 0.14 0.07
I matter 0 . 28 0 . 07 measure 0 . 22 0 . 07' obligation 0 . 12 0 . 10 power 0 . 17 0 . 08
Ijurisdiction 0.13 0.08
Iright 0.12 0.07
I control 0.20 0.07
I ground 0 . 08 0 . 07 accountability 0 . 14 0 . 08 experience 0 . 12 0 . 07 post 0 . 14 0 . 14 job 0 . 17 0 . I0 l__work0 . 17 0 . i0
I training 0 . Ii 0 . 07 position 0 . 25 0 . 10 task 0 . 10 0 . 10
Ichore 0. ii 0.07 operation 0.10 0.10
I function 0.i0 0.08
Imission 0.12 0.07
IIpatrol 0.07 0.07
Istaff 0.i0 0.07 penalty 0.09 0.09
Ifee 0.17 0.08
Itariff 0.13 0.08
Itax 0.19 0.07 reservist 0.07 0.07
Figure 3: Similarity tree for " duty "
Inspection of sample output shows that this algorithm works well  . However , formal evaluation of its accuracy remains to be future work  . 
5 Related Work and Conclusion
There have been many approaches to automatic detection of similar words from text corpora  . Ours is 1992 ) in the use of dependency relationship as the word features  , based on which word similarities are computed . 
Evaluation of automatically generated lexical resources is a difficult problem  . In ( Hindle ,  1990) , a small set of sample results are presented . In ( Smadja ,  1993) , automatically extracted collocations are judged by a lexicographer  . In ( Dagan et al . , 1993) and ( Pereira et al ,  ! 993) , clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time  . In ( Alshawi and Carter ,  1994) , the collocations and their associated scores were evaluated indirectly by their use in parse tree selection  . The merits of different measures for association strength are judged by the differences they make in the precision and the recall of the parser outputs  . 
The main contribution of this paper is a new evaluation methodology for automatically constructed thesaurus  . While previous methods rely on indirect tasks or subjective judgments  , our method allows direct and objective comparison between automatically and manually constructed thesauri  . The results show that our automatically created thesaurus i si si si gnificantly closer to WordNet than Roget Thesaurusis  . Our experiments also surpasses previous experiments on automatic thesaurus construction i scale and  ( possibly ) accuracy . 

This research as also been partially supported by NSERC Research Grant  OGP121338 and by the Institute for Robotics and Intelligent Systems  . 

Hiyan Alshawi and David Carter .  1994 . Training and scaling preference functions for disambiguation  . 
Computational Linguistics ,  20(4):635-648 , December . 
Ido Dagan , Shaul Marcus , and Shaul Markovitch .  1993 . 
Contextual word similarity and estimation from sparse data  . In Proceedings of ACL93, pages 164-171,
Columbus , Ohio , June.
Ido Dagan , Fernando Pereira , and Lillian Lee .  1994 . 
Similarity-based estimation of word cooccurrence probabilities  . In Proceedings of the 32nd Annual Meeting of the ACL , pages 272-278 , Las Cruces , NM . 
Ido Dagan , Lillian Lee , and Fernando Pereira .  1997 . 
Similarity-based method for word sense disambiguation  . In Proceedings of the 35th Annual Meeting of the ACL , pages 5663 , Madrid , Spain . 
Ute Essen and Volker Steinbiss .  1992 . Cooccurrence smoothing for stochastic language modeling  . In Proceedings of lCASSP , volume 1, pages 161-164 . 
W . B . Frakes and R . Baeza-Yates , editors .  1992 . In . 
formation Retrieval , Data Structure and Algorithms . 
Prentice Hall.
D . Gentner .  1982 . Why nouns are learned before verbs : Linguistic relativity versus natural partitioning  . In S . A . Kuczaj , editor , Language development : Vol .  2 . 
Language , thought , and culture , pages 301-334 . Erlbaum , Hillsdale , NJ . 
Gregory Grefenstette .  1994 . Explorations in Automatic Thesaurus Discovery . Kluwer Academic Press,
Boston , MA.
Donald Hindle .  1990 . Noun classification from predicate-argument structures  . In Proceedings of ACL90 , pages 268-275 , Pittsburg , Pennsylvania , 

Dekang Lin .  1993 . Principle-based parsing without overgeneration . In Proceedings of ACL93 , pages 112-120 , Columbus , Ohio . 
Dekang Lin .  1994 . Principarm an efficient , broadcoverage , principle-based parser . In Proceedings of COLING94, pages 482-488 . Kyoto , Japan . 
Dekang Lin .  1997 . Using syntactic dependency as local context to resolve word sense ambiguity  . In Proceedings of ACL/EACL-97 , pages 6471 , Madrid , Spain , 

George A . Miller , Richard Beckwith , Christiane Fellbaum , Derek Gross , and Katherine J . Miller .  1990 . 
Introduction to WordNet : An online lexical database  . 
International Journal of Lexicography ,  3(4):235-244 . 
George A . Miller .  1990 . WordNet : An online lexical database . International Journal of Lexicography ,  3(4):235-312 . 
Eugene A . Nida .  1975 . Componential Analysis of Meaning . The Hague , Mouton . 
F . Pereira , N . Tishby , and L . Lee .  1993 . Distributional Clustering of English Words . In Proceedings of ACL-93 , pages 183-190 , Ohio State University , Columbus , 

Gerda Ruge .  1992 . Experiments on linguistically based term associations  . Information Processing & Management ,  28(3):317-332 . 
Frank Smadja .  1993 . Retrieving collocations from text : Xtract . Computational Linguistics , 19(1):143-178 . 

Appendix A : Respective Nearest Neighbors

Rank Respective Nearest Neighbors Similarity 1 earnings profit 0  . 5725 2511 plan proposal 0 . 4747521 employee worker 0 . 41393631 battle fight 0 . 38977641 airline carrier 0 . 37058951 share stock 0 . 35129461 rumor speculation 0 . 32726671 outlay spending 0 . 32053581 accident incident 0 . 31012191 facility plant 0 . 284845 101 charge count 0 . 278339111 babyinfant 0 . 268093121 actoractress 0 . 25 5098 131 chance likelihood 0 . 248942141 catastrophedisaster 0 . 241986151 fine penalty 0 . 237606161 legislature parliament 0 . 231528171 oilpetroleum 0 . 227 277 181 strength weakness 0 . 2180 27191 radiotelevision 0 . 215043201 coupesedan 0 . 209631211 turmoilupheaval 0 . 205841221 music song 0 . 202 102 231 bomb grenade 0 . 198707 241 gallery museum 0 . 194591251 leave 0 . 192483261 fuelg as oline 0 . 186045271 door window 0 . 181301281 emigration immigration 0 . 176331291 espionagetreason 0 . 1726 2301 per ilp itf all 0 . 1695873 11 surcharge surtax 0 . 166831321 ability credibility 0 . 163301331 pubtavern .  0 . 1588 15341 lmense permit 0 . 156963351 excerpt transcript 0 . 150941361 dictatorship reglme 0 . 148837371 lake river 0 . 145586381 discdisk 0 . 142733391 interpreter translator 0 . 138778401 bacteria organism 0 . 135539411 ballet symphony 0 . 131688421 silkwool 0 . 128999431 intent intention 0 . 125236441 waiter waitress 0 . 122373451 bloodurine 0 . 118063461 mosquitotick 0 . 115499471 fervorzeal 0 . 11 208748 1 equal equivalent 0 . 107159491 freezer efrigerator 0 . 103777501 humor wit 0 . 0991 1085 11 cushion pillow 0 . 0944567521 purse wallet 0 . 0914273531 learning listening 0 . 0859118541 clown cowboy 0 . 0714762

Rank Respective Nearest Neighbors Similarity 1 fall rise 0  . 67411311 injurekill 0 . 37825421 concern worry 0 . 34012231 convict sentence 0 . 28967841 limit restrict 0 . 27158851 narrow widen 0 . 25838561 attract draw 0 . 24233171 discourage encourage 0 . 23442581 hitstrike 0 . 2217191 disregard ignore 0 . 210 27101 over state understate 0 . 199197111 affirmreaffirm 0 . 182765121 inform notify 0 . 170477131 differ vary 0 . 16182 1141 scream yell 0 . 150168 151 laughsmile 0 . 14 295 1161 competecope 0 . 135869171 add whisk 0 . 129 205 181 blossommature 0 . 123351 191 smell taste 0 . 112418201 barkhowl 0 . 1015662 11 black white 0 . 0694954

Rank Respective Nearest Neighbors Similarity 1 high low 0  . 580 408 11 badgood 0 . 37674421 extremely very 0 . 35760631 deteriorating improving 0 . 33266441 alleged suspected 0 . 31716351 clerical salaried 0 . 30544861 often sometimes 0 . 28144471 bleakgloomy 0 . 27555781 adequate inadequate 0 . 26313691 affiliated merged 0 . 257666 101 stormy turbulent 0 . 252846 111 paramilitary uniformed 0 . 246638121 sharp steep 0 . 240788131 communistleftist 0 . 232518141 indoor outdoor 0 . 224183151 changed changing 0 . 219697161 defensive offensive 0 . 21 1062 171 sad tragic 0 . 206688181 enormously tremendously 0 . 199936191 defective faulty 0 . 193863201 concerned worried 0 . 1868992 11 dropped fell 0 . 184768221 bloody violent 0 . 183058231 favorite popular 0 . 179234241 permanently temporarily 0 . 174361251 confidential secret 0 . 17022261 privately publicly 0 . 165313271 operating sales 0 . 162894281 annually apiece 0 . 159883291 ~ gentlekind 0 . 154554301 losing winning 0 . 149447311 experimental test 0 . 146435321 designer dress 0 . 142552331 dormantin active 0 . 13700 2341 commercially domestically 0 . 13291835 l complimentary free 0 . 1281 17361 constantly continually 0 . 122342371 hardy resistant 0 . 112133381 any more anyway 0 . 103241
