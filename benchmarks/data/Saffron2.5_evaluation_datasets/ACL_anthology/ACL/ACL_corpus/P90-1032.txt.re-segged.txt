AUTOMATICALLY EXTRACTING AND REPRESENTING
COLLOCATION SFORLANG UAGEGENERATION *
Frank A . Smadjat

Kathleen R . McKeown
Department of Computer Science
Columbia University
New York , NY 10027

Collocational knowledge is necessary for language generation  . The problem is that collocations come in a large variety of forms  . They can involve two , three or more words , these words can be of different syntacticate -gories and they can be involved in more or less rigid ways  . This leads to two main difficulties : collocational knowledge has to be acquired and it must be represented flexibly so that it can be used for language generation  . 
We address both problems in this paper , focusing on the acquisition problem . We describe a program , Xtract , that automatically acquires a range of collocations from large textual corpora and we describe how they can be represented in a flexible lexicon using a unification based formalism  . 
1 INTRODUCTION
Language generation research on lexical choice has focused on syntactic and semantic on straints on word choice and word ordering  . Colloca ~ ional constraints , however , also play a role in how words can cooccur in the same sentence  . Often , the use of one word in a particular context of meaning will require the use of one or more other words in the same sentence  . While phrasal lexicons , in which lexical associations are pre-encoded ( e . g . , \[ Kukich 83\] , \[Jacobs85\] , \[Danlos87\]) , allow for the treatment of certain types of collocations  , they also have problems . Phrasal entries must be compiled by hand which is both expensive and incomplete  . Furthermore , phrasal entries tend to capture rather rigid , idiomatic expressions . In contrast , collocations vary tremendously in the number of words involved  , in the syntactic at-egories of the words , in the syntactic relations between the words , and in how rigidly the individual words are used together  . For example , in some cases , the words of a collocation must be adjacent , while in others they can be separated by a varying number of other words  . 
* The research reported in this paper was partially supported by DARPA grant  N00039-84-C-0165  , by NSF grant IRT-84-51438 and by ONR grant N00014-89-J-1782  . 
tMost of this work is also done in collaboration with Bell Communication Research  , 445 South Street , Morristown , NJ07960-1910 In this paper , we identify a range of collocations that are necessary for language generation  , including open compounds of two or more words , predicative relations ( e . g . , subjectverb ) , and phrasal templates representing more idiomatic expressions  . We then describe how X tract automatically acquires the full range of collocations using a twostage statistical analysis of large domain specific corpora  . Finally , we show how collocations can be efficiently represented in a flexible lexicon using a unification based formalism  . This is a word based lexicon that has been macro coded with collocational knowledge  . 
Unlike a purely phrasal lexicon , we thus retain the flexibility of word based lexicons which allows for collocations to be combined and merged in syntactically acceptable ways with other words or phrases of the sentence  . Unlike pure word based lexicons , we gain the ability to deal with a variety of phrasal entries  . Furthermore , while there has been work on the automatic retrieval of lexical information from text \ [ Garside  87\]  , \[Choueka88\] , \[Klavans88\] , \[ Amsler89\] , \[ Boguraev & Briscoe89\] , \[Church89\] , none of these systems retrieves the entire range of collocations that we identify and no real effort has been made to use this information for language generation \[ Bogura ev & 
Briscoe 89\].
In the following sections , we describe the range of collocations that we can handle  , the fully implemented acquisition method , results obtained , and the representation of collocations in Functional Unification Grammars  ( FUGs ) \[Kay 79\] . Our application domain is the domain of stock market reports and the corpus on which our expertise is based consists of more than  10 million words taken from the Associated Press newswire  . 
SINGLE WORDS TO WHOLE
PHRASES : WHATKINDOF
LEXICALUNITSARENEEDED ?
Collocational knowledge indicates which members of a set of roughly synonymous words cooccur with other words and how they combine syntactically  . These affinities cannot be predicted on the basis of semantic or syntactic rules  , but can be observed with some regularity in ? text\[Cruse  86\]  . We have found a range of collocations from word pairs to whole phrases  , and as we shall show , tion . 
3 THEACQUISITION METHOD :

Open Compounds . Open compounds involve unin-terrupted sequences of words such as " stock market  , "" foreignez change , "" New York Stock Ez-change , " " The Dow Jones average of $0 indust ~- als . " They can include nouns , adjectives , and closed class words and are similar to the type of collocations retrieved by \[ Choueka  88\] or \[ Amsler 89\]  . An open compound generally functions as a single constituent of a sentence  . More open compound examples are given in figure 1 . x Predicative Relations consist of two ( or several ) words repeatedly used together in a similar syntactic relation  . These lexical relations axe harder to identify since they often correspond to interrupted word sequences in the corpus  . They axe also the most flexible in their use . This class of collocations is related to Mel'~uk 's Lexical Functions\[Mel'~uk  81\]  , and Benson's L-type relations\[Benson86\] . Within this class , Xtract retrieve subject-verb , verb-object , noun-adjective , verb-adverb , verb-verb and verb-particle predicative relations  . Church\[Church 89\] also retrieves verb-particle associations . 
Such collocations require a representation that allows for a lexical function relating two or more words  . Examples of such collocations axe given in figure  2  .   2 Phrasal templates : consist of idiomatic phrases containing one  , several or no empty slots . They axe extremely rigid and long collocations . These almost complete phrases are quite representative of a given domain  . Due to their slightly idiosyncratic structure , we propose representing and generating them by simple template filling  . Although some of these could be generated using a word based lexicon  , in general , their usage gives an impression of fluency that cannot be equaled with compositional generation alone  . Xtract has retrieved several dozens of such templates from our stock market corpus  , in-eluding : " The NYSE's composite indez of all its listed common stocks rose * NUMBER * to * NUMBER * " " On the American Stock Ez change the market value in dez was up * NUMBER * at * NUMBER *"" The Dow Jones average of  30 industrials fell*NUMBER * points to * NUMBER *" " The closely watched indez had been down about * NUMBER * points in the first hour of trading "" The average finished the week with an et loss of * NUMBER * " I All the examples related to the stock market domain have been actually retrieved by Xtract  . 
2 In the examples , the " ~" sign , represents a gap of zero , one or several words . The "?*" sign means that the two words can be in any order  . 
In order to produce sentences containing collocations  , a language generation system must have knowledge about the possible collocations that occur in a given domain  . 
In previous language generation work\[Danlos 87\]  , \[Ior-danskaja88\] , \[Nirenburg88\] , collocations are identified and encoded by hand , sometimes using the help of lexicographers ( e . g . , Danlos ' \[ Daulos 87\] use of Gross'\[Gross 75\] work )  . This is an expensive and time-consuming process , and often incomplete . In this section , we describe how X tract can automatically produce the full range of collocations described above  . 
Xtract has two main components , a concordancing component , X concord , and a statistical component , X stat . Given one or several words , X concord locates all sentences in the corpus containing them  . X stat is the cooccurrence compiler . Given X concord's output , it makes statistical observations about these words and other words with which they appear  . Only statistically significant word pairs are retained  . In\[Smadja89 a\] , and\[Smadja88\] , we detail an earlier version of Xtract and its output  , and in\[Smadja 891 ) \] we compare our results both qualitatively and quantitatively to the lexicon used in \[ Kukich  83\]  . Xtract has also been used for information retrieval in \[ Maa arek & Smadja  89\]  . In the updated version of Xtract we describe here  , statistical significance is based on four parameters  , instead of just one , and a second stage of processing has been added that looks for combinations of word pairs produced in the first stage  , resulting in multiple word collocations . 
Stage one-In the first phase , X concord is called for a single open class word and its output is pipe Iined to X stat which then analyses the distribution of words in this sample  . The output of this first stage is a list of tuples  ( wx , w2 , distance , strength , spread , height , type ) , where ( wl , w2 ) is a lexical relation between two open-class words  ( Wx and w2 )  . Some results are given in Table 1 . " Type " represents the syntactic categories of wl and  w2  .  3 . "Distance " is the relative distance between the two words  , wl and w2(e . g . , a distance of 1 means w ~ occurs immediately after wx and a distance of-imeans it occurs immediately before it  )  . A different uple is produced for each statistically significant word pair and distance  . 
Thus , ff the same two words occur equally often separated by two different distances  , they will appear twice in the list . " Strength " ( also computed in the earlier version of Xtract ) indicates how strongly the two words are related ( see\[Smadja89 a\] )  . " Spread " is the distribution of the relative distance between the two words  ; thus , the larger the " spread " the more rigidly they are used in combination to one another  . " Height " combines the factors of " spread " 3In order to get part of speech information we use a stochastic word tagger developed at AT&TBell Laboratories by Ken Church \ [ Church  88\] stock president trade Table 1: Some binary lexical relations . 
word2 market vice deficit distance-Istrength 47 . 018 40 . 6496 30 . 3384 spread 28 . 5 29 . 7 28 . 4361 11457 . 1 vreravmcm ' am ; ,   ,   , Lo ? , ~ , c--i ~ fft ~ ,   ,   ,   , ~ l , i l l l l ( ; t?1I ~ . ' lgl~:l~iIg ~ llI,~lt: . .
composite blue totaled closing-112 . 3874 29 . 0682 3139 . 89 ind exchip-1-4-1-2-1-110 . 078 shares price stocks volume 20 . 7815 23 . 0465 27 . 354 16 . 8724 19 . 3312 13 . 5184 5 . 4 3739 listed take over take overstake over take overs 25  . 9415 23 . 8696 29 . 7 28 . 1071 29 . 3682 25 . 7917 totaled bidhostileo ~ er 2721 . 06 5376 . 87 4615 . 48 4583 . 57 4464 . 89 4580 . 39 3497 . 67 1084 . 05 Ill " i ~ . ~l '_ll - ~ , ' lI ~ , \[ lllJill'\[Ib'\]l ~$' l\[Type














NJiNN
INV
Table 2: Concordances for " average indus ~ rial " On Tuesday the Dow Jones industrial average rose  26  . 28 points to 2304 . 69 . 
The Dow . . . a sellings purt that sent the Dow
On Wednesday the Dow
The Dow
The Dow . . . Thursday with the Dow . . . swelling the Dow
The rise in the Dow
Jones industrial average
Jones industrial average
Jones industrial average
Jones industrial average
Jones industrial average
Jones industrial average
Jones industrial average
Jones industrial average went up 11 . 36 points today . 
downsharply in the first hour of trading.
showed some strength as ...
was down 17.33 points to 2,287.36 ...
had the biggest one day gain of its history ...
soaring a record 69.89 points to ...
by more than 475 points in the process ...
was the biggest since a 54.14 point jump on ...

The NYSEs composite index
The NYSEs composite index
The NYSEs composite index
The NYSEs composite index
The NYSEs composite index
The NYSEs composite index
The NYSEs composite index
The NYSEs composite index
The NYSEs composite index 3: Concordances for " composite indez " of all its listed common stocks fell  1  . 76 to 164 . 13 . 
of all its listed common stocks fell 0 . 98 to 164 . 91 . 
of all its listed common stocks fell 0 . 96 to 164 . 93 . 
of all its listed common stocks fell 0 . 91 to 164 . 98 . 
of all its listed common stocks rose 1 . 04 to 167 . 08 . 
of all its listed common stocks rose 0 . 76 of all its listed common stocks rose 0 . 50 to 166 . 54 . 
of all its listed common stocks rose 0 . 69 to 166 . 73 . 
of all its listed common stocks fell 0 . 33 to 17 0 . 63 . 
2 54 open compound open compound open compound open compound open compound open compound open compound open compound open compound open compound open compound open compound open compound open compound open compound qeading industrialized countries " " the Dow Jones average of  . 9 0 industriais " " bear/buil market " " the Dow Jones industrial average "" The NYSEs composite indez of all  it8 listed common stocks "" Advancing/winuing/losing /declluing issues " " The NASDAQ composite in dez for the over the countermarket "" stock market "" centralbank'qeveraged buy out "" the gross national product "' q ~ lue chip stocks "" White House spokesman Marlin Fitz to a ter "" take over speculation/strategist/target / threat/attempt "" take overbid/battle/defense/efforts / flght/law/proposal/rumor " Figure  1: Some examples of open compounds noun adjective noun adjective noun adjective subject verb subject verb subject verb verb adverb verb object verb object verb particle verb verb verb verb examples " heavy/HghtDtradlng/smoker/traffic " " hlgh/low~fertility/pressure/bounce "" large /small Dcrowd/retailer/client "" index ~ rose " stock ~\[ rose  , fell , closed , jumped , continued , declined , crashed ,   . . . \]"" advancers D\[outnumbered , outpaced , overwhelmed , outstripped\]""trade ? ~ actively , " mix ? ~ narrowly , " use ? ~ widely , "" watch ? ~ closely " ~ posted ~ gain ' ~ momentum D\[pickup  , build , carry over , gather , loose , gain \] " " take ~ from , "" raise ~ by , "" mixD with "" offer to \[ acquire , buy "\] " agreeto\[acquire , buy "\] Figure 2: Some examples of predicative collocations and " strength " resulting in a ranking of the two words for their " distances "  . Church\[Church 89\] produces results similar to those presented in the table using a different statistical method  . However , Church's method is mainly based on the computation of the " strength " attribute  , and it does not take into account " spread " and " height "  . As we shall see , these additional parameters are crucial for producing multiple word collocations and distinguishing between open compounds  ( words are adjacent ) and predicative relations ( words can be separated by varying distance )  . 
Stage two : In the second phase , Xtraet first uses the same components but in a different way  . It starts with the pairwise lexical relations produced in Stage one to produce multiple word collocations  , then classifies the collocations as one of three classes identified above  , end finally attempts to determine the syntactic relations between the words of the collocation  . To do this , Xtract studies the lexical relations in context , which is exactly what lexicographers do . For each entry of Table 1 , Xtract calls X concord on the two words wl and w ~ to produce the concordances  . Tables 2 and 3 show the concordances ( output of X concord ) for the input pairs : " average-industrial " end " indez-composite "  . 
X stat then compiles information on the words surrounding both wl and  w2 in the corpus . This stage allows us to filter out incorrect associations such as " blue  . stocks " or " advancing-market " and replace them with the appropriate ones  , " bluechip stocks , "" the broader market in the NYSE advancing is . 
sues . " This stage also produces phrasal templates such as those given in the previou section  . In short , stage two filters in a propriate results and combines word pairs to produce multiple word combinations  . 
To make the results directly usable for language generation we are currently investigating the use of a bottom-u parser in combination with stage two in order to classify the collocations according to syntactic criteria  . For example if the lexical relation involves a noun and a verb it determines if it is a subject verb or a verb-object collocation  . We plan to do this using a deterministic bottom upparser developed at Bell Communication Research \[ Abney  89\] to parse the concordances . The parser would analyse ach sentence of the concordances and the parse trees would then be passed to X stat  . 
Sample results of Stage two are shown in Figures 1 , 2 and 3 . Figure 3 shows phrasal templates and open compounds . X statnotices that the words " composite and " indez " are used very rigidly throughout the corpus  . They almost always appear in one of the two composite-indez composite-indez collocation " The NYSE's composite indez of all its listed common stocks fell*NUMBER * to*NUMBER *"" the NYSE 's composite in dez of all its listed common stocks rose * NUMBER * to * NUMBER *  . "\[" close-industrial "" Five minutes before the close the Dow Jones average of  30 industrials ~ asup/down*NUMBER*to/from * NUMBER * "" the Dow Jones industrial average  . "" average industrial "" advancing-market "" block-trading " " cable-television " " the broader market in the NYSE advancing issues "" Jack Baker head of block trading in Shearson Lehman Brothers Inc  . "" cable television " Figure 3: Example collocations output of stage two . 
sentences . The lexical relation composite-indez thus produces two phrasal templates  . For the lexical relation average-industrial X t ract produces an open compound collocation as illustrated in figure  3  . Stage two also confirms pairwise relations . Some examples are given in figure 2 . By examining the parsed concordances and extracting recurring patterns  , X stat produces all three types of collocations . 
4 HOWTORE PRESENT THEM
FORLANG UAGEGENERATION ?
Such a wide variety of lexical associations would bedif-ficnlt to use with any of the existing lexicon formalisms  . 
We need a flexible lexicon capable of using single word entries  , multiple word entries as well as phrasal templates and a mechanism that would be able to gracefully merge and combine them with other types of constraints  . 
The idea of a flexible lexicon is not novel in itself  . The lexical representation used in \[ Jacobs 85\] and later refined in \[ Desemer & Jabobs 87\] could also represent a wide range of expressions . However , in this language , collocational , syntactic and selectional constraints are mixed together into phrasal entries  . This makes the lexicon both diffic nlt to use and difficult to compile  . In the following we briefly show how FUGs can be successfully used as they offer a flexible declarative language as well as a powerful mechanism for sentence generation  . 
We have implemented a first version of Cook , a surface generator that uses a flexible lexicon for express-in ~ cooccurrence on straints  . Cook uses FUF\[Elhadad 90J , an extended implementation fPUGs , to uniformly represent the lexicon and the syntax as originally suggested by Halliday \ [ Halliday  66\]  . Generating a sentence is equivalent ounifying a semantic structure  ( Logical Form ) with the grammar . The grammar we use is divided into three zones , the " sentential , " the " lezical " and " the syntactic zone . " Each zone contains constraints pertaining to a given domain and the input logical form is unified in turn with the three zones  . As it is , fullback tracking across the three zones is allowed  . 
? The sentential zone contains the phrasal templates against which the logical form is unified first  . A sententiai entry is a whole sentence that should be used in a given context  . This context is specified by subparts of the logical form given as input  . When there is a match at this point , unification succeeds and generation is reduced to simple template filling  . 
? The lezical zone contains the information used to lexicalize the input  . It contains collocational information along with the semantic on text in which to use it  . This zone contains predicative and open compound collocations  . Its role is to trigger phrases or words in the presence of other words or phrases  . 
Figure 5 is a portion of the lexical grammar used in Cook . It illustrates the choice of the verb to be used when " advancers " is the subject  . ( See below for more detail ) . 
? The syniacgic zone contains the syntactic grammar  . 
It is used last as it is the part of the grammar ensuring the correctness of the produced sentences  . 
An example input logical form is given in Figure 4 . In this example , the logical form represent she fact that on the New Yorkstock exchange  , the advancing issues ( se-mantic representation rsere-R:c:winners ) were a head ( predicate c : lead ) of the losing ones ( sem-R : c : losers ) and that there were 3 times more winning issues than losing ones ratio )  . In addition , it also says that this ratio is of degree 2 . A degree of 1 is considered as as lim lead whereas a degree of 5 is a commanding margin . When unified with the grammar , this logical form produces the sentences given in Figure  6  . 
As an example of how Cook uses and merges cooccurrence information with other kind of knowledge consider Figure  5  . The figure is an edited portion of the lexical zone  . It only includes the parts that are relevant to the choice of the verb when " advancers " is the subject  . The lexand sem-R attribute specify the lexeme we are considering  ( " advancers " ) and its semantic representation ( c : winners )  . 
The semantic on text ( sere-context ) which points to the logical form and its features will then be used in order predicate-name = p: lead leaders =\[ sem-RL ratio trailers : c : winners\] 
J : 3 sem-R:c:losers\]:ratio----Idegree = 2 Figure 4: LF : An example logical form used by Cooko ,   , ??? ooolex="advancer"sam-R=c:~oinners sem-context=<logical-form > 
OO010eo,osem-context
SV-collocates=predicate-name=p:lead\] degree =  2 lex----"o . u  ~ nurn , ber "/ lex="lead"lex="finish " lex = " hold " lex="~eept'lex="have "  ,   , ? sem-context
SV-collocates=predicate-name:p:lead=degree :   4 lex : U ? verp ? ~ er " 1 lex="outstrip " lex:"hold " lex:"keel ' ? Figure  5: A portion of the lexical grammar showing the verbal collocates of " advancers "  . 
" Advancers outnumbered declining issues by a margin of  3   4o   1  . "" Advancers had as limle adover losing issues wi~ha margin of  3   4o   1  . "" Advancer skep ~ as limlead over decliners wi ~ hamargin of  3 ~ o 1" Figure 6: Example sentences that can be generated with the logical form LF figure we only included two alternatives  . Both are relative to the predicate p : lead but they axe used with different values of the degree attribute  . When the degree is 2 then the first alternative containing the verbs listed under SV-colloca ~ es  ( e . g . " outnumber ") will be selected . 
When the degree is 4 the second alternative containing the verbs listed under SV-collocal  ; es(e . g . " over-power ") will be selected . All the verbal collocates shown in this figure have actually been retrieved by Xtract at a preceding stage  . 
The unification of the logical form of Figure 4 with the lexical grammar and then with the syntactic grammar will ultimately produce the sentences shown in Figure  6 among others . In this example , the sentencial zone was not used since no phrasal template expresses its semantics  . The verbs selected are all listed under the SV -collocates of the first alternative in Figure  5  . 
We have been able to use Cook to generate several sentences in the domain of stock maxket reports using this method  . However , this is still ongoing reseaxch and the scope of the system is currently limited  . We are working on extending Cook's lexicon as well as on developing extensions that will allow flexible interaction among collocations  . 
5 CONCLUSION
In summary , we have shown in this paper that there axe many differentypes of collocations needed for language generation  . Collocations axe flexible and they can involve two  , three or more words invaxious ways . We have described a fully implemented program , Xtract , that automatically acquire such collocations from large textual corpora and we have shown how they can be represented in a flexible lexicon using FUF  . In FUF , cooccurrence constraints axe expressed uniformly with syntactic and semantic on straints  . The grammax's function is to satisfy these multiple constraints  . We are currently working on extending Cook as well as developing a full sized from X tract's output  . 

We would like to thank Kaxen Kukich and the Computer Systems Research Division at Bell Communication Research for their help on the acquisition part of this work  . 
References\[Abney89\]S . Abney , " Parsing by Chunks " in C . Tenny ~ ed . , The MIT Parsing Volume , 1989, to appeax . 
\[ Amsler89\]R . Amsler , " Research Towards the Development of a Lezical Knowledge Base for Natural Language Processing " Proceedings of the  1989 SIGIR Conference , Association for Computing Ma-\[Benson86\]M . Benson , E . Benson and R . Ilson , Lezi-cographic Description of English . John Benjamins Publishing Company , Philadelphia ,  1986 . 
\[ Boguraev & Briscoe89\]B . Boguraev & T . Briscoe , in Computational Lezicography for natural language processing  . B . Boguraev and T . Briscoeeditors . 
Longmans , NY 1989.
\[Choueka88\]Y . Choueka , Looking for Needles in a Haystack . In Proceedings of the RIAO , p : 609-623, 1988 . 
\[Church88\]K . Church , A Stochastic Par~s Program and Noun Phrase Parser for Unrestricted Tezt In Proceedings of the Second Conference on Applied Natural Language Processing  , Austin , Texas ,  1988 . 
\[Church89\]K . Church & K . Hanks , Word Association Norms , Mutual Information , and Lezicography . In Proceedings of the 27th meeting of the Association for Computational Linguistics  , Vancouver , 
B . C , 1989.
\[Cruse86\]D . A . Cruse , Lezical Semantics . Cambridge
University Press , 1986.
\[Danlos87\]L . Danlos , The linguistic Basis of Tezt Generation . Cambridge University Press , 1987 . 
\[Desemer & Jabobs87\]D . Desemer & P . Jacobs , FLUSH : A Flezible Lezicon Design . In proceedings of the 25th Annual Meeting of the ACL , Stanford
University , CA , 1987.
\[ Elhadad90\]M . Elhadad , Types in Functional Unification Grammars , Proceedings of the 28th meeting of the Association for Computational Linguistics  , 
Pittsburgh , PA , 1990.
\[Gaxside87\]R . Gaxside , G . Leech & G . Sampson , editors , The computational Analysis of English , a corpus based approach . Longmans , NY 1987 . 
\[Gross 75\]M . Gross , Mdthodesen Syntaze . Hermann,
Paxis , France , 1975.
\[ Halliday 66\]M . A . K . Halliday , Lezis as a Linguistic Level . In C . E . Bazell , J . C . Catford , M . A . K Halliday and R . H . Robins ( eds . ), In memory of J . R . 
Firth London : Longmans Linguistics \] la Libraxy ,  1966 , pp : 148-162 . 
\[Iordanskaja88\]L . Iordanskaja , R . Kittredge , A . 
Polguere , Lezical Selection and Paraphrase in a Meaning -Tezt Generation Model Presented at the fourth International Workshop on Language Generation  , Catalina Island , CA ,  1988 . 
\[Jacobs85\]P . Jacobs , PHRED : a generator for natural language interfaces  , Computational Linguistics , volume 114 , 1985\[Kay 79\] M . Kay , Functional Grammar , in Proceedings of the 5th Meeting of the Berkeley Linguistic Society , Berkeley Linguistic Society ,  1979 . 
\[Klavans88\]J . Klavans , " COMPLEX : a computational lezicon for natural anguage systems  . " In proceeding of the 12th International Conference on Corn-chinery . Cambridge , Ma , June 1989 . 
258 putational Linguistics , Budapest , Hungary ,  1988 . 
\[Kukich83\]K . Kukich , Knowledge-Based Report Generation : A Technique for Automatically Generating Natural Language Reports from Databases  . 
Proceedings of the 6th International ACM SIGIR
Conference , Washington , DC , 1983.
\[ Maarek & Smadja89\]Y . SMaarek & F . A . Smadja , Full Tezt Indezing Based on Lezical Relations , An Ap . 
plication : Software Libraries . Proceedings of the 12th International ACM SIGIR Conference , Cambridge , Ma , June 1989 . 
\[ Mel'~uk81\]I . A Mel'euk , Meaning-Tezt Models : a Recent Trend in Soviet Linguistics  . The annual review of anthropology , 1981 . 
\[Nirenburg88\]S . Nirenburg et al , Lezicon building in natural language processing . In program and abstracts of the 15 th International ALLC , Conference of the Association for Literary and Linguistic 
Computing , Jerusalem , Israel , 1988.
\[Smadja88\]F . A . Smadja , Lezical Cooccurrence : The Missing link . In program and abstracts of the 15 th International ALLC , Conference of the Association for Literary and Linguistic Computing  , Jerusalem , Israel ,  1988 . Also in the Journal for Literary and Linguistic computing  , Vol . 4, No . 3,1989, Oxford University Press . 
\[Smadja89 a\]F . A . Smadja , Microcoding the Lezicon for Language Generation , First International Workshop on Lexical Acquisition  , IJCAI'89 , Detroit , Mi , August 89 . Also in " Lezical Acquisition : Using online resources to build a lezicon "  , MIT press , 
Uri Zeruikeditor , to appear.
\[Smadja89b\]F . A . Smadja , On the Use of Flezible Collocations for Language Generation  . Columbia University , technical report , TR#CUCS-507-89 . 

