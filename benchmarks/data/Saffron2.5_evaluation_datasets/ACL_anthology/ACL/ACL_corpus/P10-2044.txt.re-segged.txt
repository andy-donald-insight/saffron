Proceedings of the ACL 2010 Conference Short Papers , pages 236?240,
Uppsala , Sweden , 1116 July 2010. c?2010 Association for Computational Linguistics
Optimizing Question Answering Accuracy by Maximizing LogLikelihood
Matthias H . Heie , Edward W . D . Whittaker and Sadaoki Furui
Department of Computer Science
Tokyo Institute of Technology
Tokyo 152-8552, Japan
{heie,edw,furui}@furui.cs.titech.ac.jp
Abstract
In this paper we demonstrate that there is a strong correlation between the Question Answering ( QA ) accuracy and the loglikelihood of the answer typing component of our statistical QA model . We exploit this observation in a clustering algorithm which optimizes QA accuracy by maximizing the loglikelihood of a set of question-and-answer pairs . Experimental results show that we achieve better QA accuracy using the resulting clusters than by using manually derived clusters.
1 Introduction
Question Answering ( QA ) distinguishes itself from other information retrieval tasks in that the system tries to return accurate answers to queries posed in natural language . Factoid QA limits itself to questions that can usually be answered with a few words . Typically factoid QA systems employ some form of question type analysis , so that a question such as What is the capital of Japan ? will be answered with a geographical term . While many QA systems use handcrafted rules for this task , such an approach is time-consuming and doesn?t generalize well to other languages . Machine learning methods have been proposed , such as question classification using support vector machines ( Zhang and Lee , 2003) and language modeling ( Merkel and Klakow , 2007). In these approaches , question categories are predefined and a classifier is trained on manually labeled data . This is an example of supervised learning . In this paper we present an unsupervised method , where we attempt to cluster question-and-answer ( qa ) pairs without any predefined question categories , hence no manually class-labeled questions are used.
We use a statistical QA framework , described in Section 2, where the system is trained with clusters of qa pairs . This framework was used in several TREC evaluations where it placed in the top 10 of participating systems ( Whittaker et al , 2006).
In Section 3 we show that answer accuracy is strongly correlated with the loglikelihood of the qa pairs computed by this statistical model . In Section 4 we propose an algorithm to cluster qa pairs by maximizing the loglikelihood of a disjoint set of qa pairs . In Section 5 we evaluate the QA accuracy by training the QA system with the resulting clusters.
2 QA system
In our QA framework we choose to model only the probability of an answer A given a question Q , and assume that the answer A depends on two sets of features : W = W ( Q ) and X = X(Q):
P ( A|Q ) = P ( A|W,X ), (1) where W represents a set of | W | features describing the question-type part of Q such as who , when , where , which , etc ., and X is a set of features which describes the ? information-bearing ? part of Q , i.e . what the question is actually about and what it refers to . For example , in the questions Where is Mount Fuji ? and How high is Mount Fuji ?, the question type features W differ , while the information-bearing features X are identical.
Finding the best answer A ? involves a search over all A for the one which maximizes the probability of the above model , i.e.:
A ? = arg max
A
P ( A|W,X ). (2)
Given the correct probability distribution , this will give us the optimal answer in a maximum likelihood sense . Using Bayes ? rule , assuming uniform P ( A ) and that W and X are independent of each other given A , in addition to ignoring P ( W,X ) since it is independent of A , enables us to rewrite Eq . (2) as
A
P ( A | X ) ? ?? ? retrieval model ? P ( W | A ) ? ?? ? filter model . (3) 2.1 Retrieval Model The retrieval model P ( A|X ) is essentially a language model which models the probability of an answer sequence A given a set of information-bearing features X = { x1, . . . , x|X |}. This set is constructed by extracting single-word features from Q that are not present in a stoplist of high-frequency words . The implementation of the retrieval model used for the experiments described in this paper , models the proximity of A to features in X . It is not examined further here ; see ( Whittaker et al , 2005) for more details.
2.2 Filter Model
The question-type feature set W = { w1, . . . , w|W |} is constructed by extracting n-tuples ( n = 1, 2, . . .) such as where , in what and when were from the input question Q . We limit ourselves to extracting single-word features . The 2522 most frequent words in a collection of example questions are considered in-vocabulary words ; all other words are out-of-vocabulary words , and substituted with ? UNK?.
Modeling the complex relationship between W and A directly is nontrivial . We therefore introduce an intermediate variable CE = { c1, . . . , c|CE |}, representing a set of classes of example qa pairs . In order to construct these classes , given a set E = { t1, . . . , t|E |} of example qa pairs , we define a mapping function f : E 7? CE which maps each example qa pair tj for j = 1 . . . | E | into a particular class f(tj ) = ce.
Thus each class ce may be defined as the union of all component qa features from each tj satisfying f(tj ) = ce . Hence each class ce constitutes a cluster of qa pairs . Finally , to facilitate modeling we say that W is conditionally independent of A given ce so that,
P ( W | A ) = | CE |? e=1
P ( W | ceW ) ? P ( ceA | A ), (4) where ceW and ceA refer to the subsets of question-type features and example answers for the class ce , respectively.
P ( W | ceW ) is implemented as trigram language models with backoff smoothing using absolute discounting ( Huang et al , 2001).
Due to data sparsity , our set of example qa pairs cannot be expected to cover all the possible answers to questions that may ever be asked.
We therefore employ answer class modeling rather than answer word modeling by expanding Eq . (4) as follows:
P ( W | A ) = | CE |? e=1
P ( W | ceW )? | KA |? a=1
P ( ceA | ka)P ( ka | A ), (5) where ka is a concrete class in the set of | KA | answer classes KA . These classes are generated using the Kneser-Ney clustering algorithm , commonly used for generating class definitions for class language models ( Kneser and Ney , 1993).
In this paper we restrict ourselves to single-word answers ; see ( Whittaker et al , 2005) for the modeling of multiword answers . We estimate
P ( ceA | kA ) as
P ( ceA | kA ) = f(kA , ceA ) | CE |? g=1 f(kA , c g
A ) , (6) where f(kA , ceA ) = ? ? i:i?ceA ?( i ? kA ) | ceA | , (7) and ?(?) is a discrete indicator function which equals 1 if its argument evaluates true and 0 if false.
P ( ka | A ) is estimated as
P ( ka | A ) = ? j:j?Ka ?( A ? j ) . (8) 3 The Relationship between Mean
Reciprocal Rank and LogLikelihood
We use Mean Reciprocal Rank ( MRR ) as our metric when evaluating the QA accuracy on a set of questions G = { g1...g|G|}:
MRR = ?| G | i=1 1/Ri | G | , (9) 0.16 0.17 0.18 0.19 0.2 0.21 0.22 0.23 -1.18 -1.16 -1.14 -1.12
M
R
R
LL ? = 0.86
Figure 1: MRR vs . LL ( average per qa pair ) for 100 random cluster configurations.
where Ri is the rank of the highest ranking correct candidate answer for gi.
Given a set D = ( d1...d|D |) of qa pairs disjoint from the qa pairs in CE , we can , using Eq . (5), calculate the loglikelihood as
LL = | D | ? d=1 logP ( Wd|Ad ) = | D | ? d=1 log | CE |? e=1
P ( Wd | ceW )? | KA |? a=1
P ( ceA | ka)P ( ka | Ad).
(10)
To examine the relationship between MRR and LL , we randomly generate configurations CE , with a fixed cluster size of 4, and plot the resulting MRR and LL , computed on the same data set D , as data points in a scatter plot , as seen in Figure 1. We find that LL and MRR are strongly correlated , with a correlation coefficient ? = 0.86.
This observation indicates that we should be able to improve the answer accuracy of the QA system by optimizing the LL of the filter model in isolation , similar to how , in automatic speech recognition , the LL of the language model can be optimized in isolation to improve the speech recognition accuracy ( Huang et al , 2001).
4 Clustering algorithm
Using the observation that LL is correlated with MRR on the same data set , we expect that optimizing LL on a development set ( LLdev ) will also improve MRR on an evaluation set ( MRReval).
Hence we propose the following greedy algorithm to maximize LLdev : init : c1 ? CE contains all training pairs | E | while improvement > threshold do best LLdev ? ?? for all j = 1...|E | do original cluster = f(tj)
Take tj out of f(tj ) for e = ?1, 1...|CE |, | CE |+ 1 do
Put tj in ce
Calculate LLdev if LLdev > best LLdev then best LLdev ? LLdev best cluster ? e best pair ? j end if
Take tj out of ce end for
Put tj back in original cluster end for
Take tbest pair out of f(tbest pair)
Put tbest pair into cbest cluster end while In this algorithm , c?1 indicates the set of training pairs outside the cluster configuration , thus every training pair will not necessarily be included in the final configuration . c|C|+1 refers to a new , empty cluster , hence this algorithm automatically finds the optimal number of clusters as well as the optimal configuration of them.
5 Experiments 5.1 Experimental Setup
For our data sets , we restrict ourselves to questions that start with who , when or where . Furthermore , we only use qa pairs which can be answered with a single word . As training data we use questions and answers from the Knowledge-Master collection1. Development/evaluation questions are the questions from TREC QA evaluations from TREC 2002 to TREC 2006, the answers to which are to be retrieved from the AQUAINT corpus . In total we have 2016 qa pairs for training and 568 questions for development/evaluation . We are able to retrieve the correct answer for 317 of the devel-opment/evaluation questions , thus the theoretical upper bound for our experiments is an answer accuracy of MRR = 0.558.
Accuracy is evaluated using 5-fold ( rotating ) crossvalidation , where in each fold the TREC QA data is partitioned into a development set of 1http://www.greatauk.com / manual -1.18 0.262 3 all-in-one -1.32 0.183 1 one-in-each -0.87 0.263 2016 automatic -0.24 0.281 4 Table 1: LLeval ( average per qa pair ) and MRReval ( over all heldout TREC years ), and number of clusters ( median of the cross-evaluation folds ) for the various configurations.
4 years ? data and an evaluation set of one year?s data . For each TREC question the top 50 documents from the AQUAINT corpus are retrieved using Lucene2. We use the QA system described in Section 2 for QA evaluation . Our evaluation metric is MRReval , and LLdev is our optimization criterion , as motivated in Section 3.
Our baseline system uses manual clusters.
These clusters are obtained by putting all who qa pairs in one cluster , all when pairs in a second and all where pairs in a third . We compare this baseline with using clusters resulting from the algorithm described in Section 4. We run this algorithm until there are no further improvements in LLdev . Two other cluster configurations are also investigated : all qa pairs in one cluster ( all-in-one ), and each qa pair in its own cluster ( one-in-each ). The all-in-one configuration is equivalent to not using the filter model , i.e . answer candidates are ranked solely by the retrieval model . The one-in-each configuration was shown to perform well in the TREC 2006 QA evaluation ( Whittaker et al , 2006), where it ranked 9th among 27 participants on the factoid
QA task.
5.2 Results
In Table 1, we see that the manual clusters ( baseline ) achieves an MRReval of 0.262, while the clusters resulting from the clustering algorithm give an MRReval of 0.281, which is a relative improvement of 7%. This improvement is statistically significant at the 0.01 level using the Wilcoxon signed-rank test . The one-in-each cluster configuration achieves an MRReval of 0.263, which is not a statistically significant improvement over the baseline . The all-in-one cluster configuration ( i.e . no filter model ) has the lowest accuracy , with an MRReval of 0.183.
2http://lucene.apache.org / -1.4 -1.2 -1 -0.8 -0.6 -0.4 -0.2 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 0.32
LL M
R
R # iterations
LLdevMRRdev ( a ) Development set , 4 year?s TREC.
-1.4 -1.2 -1 -0.8 -0.6 -0.4 -0.2 0.16 0.18 0.2 0.22 0.24 0.26 0.28 0.3 0.32
LL M
R
R # iterations
LLevalMRReval ( b ) Evaluation set , 1 year?s TREC.
Figure 2: MRR and LL ( average per qa pair ) vs . number of algorithm iterations for one crossvalidation fold.
6 Discussion
Manual inspection of the automatically derived clusters showed that the algorithm had constructed configurations where typically who , when and where qa pairs were put in separate clusters , as in the manual configuration . However , in some cases both who and where qa pairs occurred in the same cluster , so as to better answer questions like Who won the World Cup ?, where the answer could be a country name.
As can be seen from Table 1, there are only 4 clusters in the automatic configuration , compared to 2016 in the one-in-each configuration . Since the computational complexity of the filter model described in Section 2.2 is linear in the number of clusters , a beneficial side effect of our clustering procedure is a significant reduction in the computational requirement of the filter model.
In Figure 2 we plot LL and MRR for one of the crossvalidation folds over multiple iterations ( the while loop ) of the clustering algorithm in Sec-of LLdev leads to improvement in MRReval , and that LLeval is also well correlated with MRReval.
7 Conclusions and Future Work
In this paper we have shown that the loglikelihood of our statistical model is strongly correlated with answer accuracy . Using this information , we have clustered training qa pairs by maximizing loglikelihood on a disjoint development set of qa pairs . The experiments show that with these clusters we achieve better QA accuracy than using manually clustered training qa pairs.
In future work we will extend the types of questions that we consider , and also allow for multiword answers.
Acknowledgements
The authors wish to thank Dietrich Klakow for his discussion at the concept stage of this work . The anonymous reviewers are also thanked for their constructive feedback.
References [ Huang et al2001] Xuedong Huang , Alex Acero and Hsiao-Wuen Hon . 2001. Spoken Language Processing . Prentice-Hall , Upper Saddle River , NJ,
USA.
[Kneser and Ney1993] Reinhard Kneser and Hermann Ney . 1993. Improved Clustering Techniques for Class-based Statistical Language Modelling . Proceedings of the European Conference on Speech Communication and Technology ( EUROSPEECH).
[Merkel and Klakow2007] Andreas Merkel and Dietrich Klakow . 2007. Language Model Based Query Classification . Proceedings of the European Conference on Information Retrieval ( ECIR).
[Whittaker et al2005] Edward Whittaker , Sadaoki Furui and Dietrich Klakow . 2005. A Statistical Classification Approach to Question Answering using Web Data . Proceedings of the International Conference on Cyberworlds.
[Whittaker et al2006] Edward Whittaker , Josef Novak , Pierre Chatain and Sadaoki Furui . 2006. TREC 2006 Question Answering Experiments at Tokyo Institute of Technology . Proceedings of The Fifteenth
Text REtrieval Conference ( TREC).
[Zhang and Lee2003] Dell Zhang and Wee Sun Lee.
2003. Question Classification using Support Vector Machines . Proceedings of the Special Interest Group on Information Retrieval ( SIGIR).
240
