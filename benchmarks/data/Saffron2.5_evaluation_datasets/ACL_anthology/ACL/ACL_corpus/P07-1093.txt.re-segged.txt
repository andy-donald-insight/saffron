Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics , pages 736?743,
Prague , Czech Republic , June 2007. c?2007 Association for Computational Linguistics
A Maximum Expected Utility Framework for Binary Sequence Labeling
Martin Jansche?
jansche@acm.org
Abstract
We consider the problem of predictive inference for probabilistic binary sequence labeling models under Fscore as utility . For a simple class of models , we show that the number of hypotheses whose expected Fscore needs to be evaluated is linear in the sequence length and present a framework for efficiently evaluating the expectation of many common loss/utility functions , including the Fscore . This framework includes both exact and faster inexact calculation methods.
1 Introduction 1.1 Motivation and Scope
The weighted Fscore ( van Rijsbergen , 1974) plays an important role in the evaluation of binary classifiers , as it neatly summarizes a classifier?s ability to identify the positive class . A variety of methods exists for training classifiers that optimize the Fscore , or some similar tradeoff between false positives and false negatives , precision and recall , sensitivity and specificity , type I error and type II error rate , etc.
Among the most general methods are those of Mozer et al (2001), whose constrained optimization technique is similar to those in ( Gao et al , 2006; Jansche , 2005). More specialized methods also exist , for example for support vector machines ( Musicant et al , 2003) and for conditional random fields ( Gross et al , 2007; Suzuki et al , 2006).
All of these methods are about classifier training.
In this paper we focus primarily on the related , but orthogonal , issue of predictive inference with a fully trained probabilistic classifier . Using the weighted Fscore as our utility function , predictive inference amounts to choosing an optimal hypothesis which maximizes the expected utility . We refer to this as ? Current affiliation : Google Inc . Former affiliation : Center of Computational Learning Systems , Columbia University.
the prediction or decoding task . In general , decoding can be a hard computational problem ( Casacuberta and de la Higuera , 2000; Knight , 1999). In this paper we show that the maximum expected Fscore decoding problem can be solved in polynomial time under certain assumptions about the underlying probability model . One key ingredient in our solution is a very general framework for evaluating the expected Fscore , and indeed many other utility functions , of a fixed hypothesis.1 This framework can also be applied to discriminative classifier training.
1.2 Background and Notation
We formulate our approach in terms of sequence labeling , although it has applications beyond that . This is motivated by the fact that our framework for evaluating expected utility is indeed applicable to general sequence labeling tasks , while our decoding method is more restricted . Another reason is that the Fscore is only meaningful for comparing two ( multi)sets or two binary sequences , but the notation for multisets is slightly more awkward.
All tasks considered here involve strings of binary labels . We write the length of a given string y ? {0,1}n as | y |= n . It is convenient to view such strings as real vectors ? whose components happen to be 0 or 1 ? with the dot product defined as usual . Then y ? y is the number of ones that occur in the string y.
For two strings x,y of the same length | x | = | y | the number of ones that occur at corresponding indices is x ? y.
Given a hypothesis z and a gold standard label sequence y , we define the following quantities : 1. T = y ? y , the genuine positives ; 2. P = z ? z , the predicted positives ; 3. A = z ? y , the true positives ( predicted positives that are genuinely positive ); 1A proof-of-concept implementation is available at http : // purl.org/net/jansche/meu_framework/.
736 4. Recl = A/T , recall ( a.k.a . sensitivity or power ); 5. Prec = A/P , precision.
The ? - weighted Fscore is then defined as the weighted harmonic mean of recall and precision . This simplifies to
F ? = (? +1)A
P +? T (? > 0) (1) where we assume for convenience that 0/0 def = 1 to avoid explicitly dealing with the special case of the denominator being zero . We will write the weighted Fscore from now on as F(z,y ) to emphasize that it is a function of z and y.
1.3 Expected FScore
In Section 3 we will develop a method for evaluating the expectation of the Fscore , which can also be used as a smooth approximation of the raw Fscore during classifier training : in that task ( which we will not discuss further in this paper ), z are the supervised labels , y is the classifier output , and the challenge is that F(z,y ) does not depend smoothly on the parameters of the classifier . Gradient-based optimization techniques are not applicable unless some of the quantities defined above are replaced by approximations that depend smoothly on the classifier?s parameters.
For example , the constrained optimization method of ( Mozer et al , 2001) relies on approximations of sensitivity ( which they call CA ) and specificity2 ( their CR ); related techniques ( Gao et al , 2006; Jansche , 2005) rely on approximations of true positives , false positives , and false negatives , and , indirectly , recall and precision . Unlike these methods we compute the expected Fscore exactly , without relying on ad hoc approximations of the true positives , etc.
Being able to efficiently compute the expected Fscore is a prerequisite for maximizing it during decoding . More precisely , we compute the expectation of the function y 7? F(z,y ), (2) which is a unary function obtained by holding the first argument of the binary function F fixed . It will henceforth be abbreviated as F(z , ?), and we will denote its expected value by
E [ F(z , ?)] = ? y?{0,1}|z|
F(z,y ) Pr(y ). (3) 2Defined as [(~1? z ) ? (~1? y )] / [(~1? y ) ? (~1? y)].
This expectation is taken with respect to a probability model over binary label sequences , written as Pr(y ) for simplicity . This probability model may be conditional , that is , in general it will depend on covariates x and parameters ? . We have suppressed both in our notation , since x is fixed during training and decoding , and we assume that the model is fully identified during decoding . This is for clarity only and does not limit the class of models , though we will introduce additional , limiting assumptions shortly . We are now ready to tackle the inference task formally.
2 Maximum Expected FScore Inference 2.1 Problem Statement Optimal predictive inference under Fscore utility requires us to find an hypothesis z ? of length n which maximizes the expected Fscore relative to a given probabilistic sequence labeling model : z ? = argmax z?{0,1}n
E [ F(z , ?)] = argmax z?{0,1}n ? y
F(z,y ) Pr(y).
(4)
We require the probability model to factor into independent Bernoulli components ( Markov order zero):
Pr(y = ( y1, . . . , yn )) = n ? i=1 pyii (1? pi ) 1?yi . (5) In practical applications we might choose the overall probability distribution to be the product of independent logistic regression models , for example . Ordinary classification arises as a special case when the yi are i.i.d ., that is , a single probabilistic classifier is used to find Pr(yi = 1 | xi ). For our present purposes it is sufficient to assume that the inference algorithm takes as its input the vector ( p1, . . . , pn ), where pi is the probability that yi = 1.
The discrete maximization problem (4) cannot be solved naively , since the number of hypotheses that would need to be evaluated in a brute-force search for an optimal hypothesis z ? is exponential in the sequence length n . We show below that in fact only a few hypotheses ( n+1 instead of 2n ) need to be examined in order to find an optimal one.
The inference algorithm is the intuitive one , analogous to the following simple observation : Start with the hypothesis z = 00 . . .0 and evaluate its raw Fscore F(z,y ) relative to a fixed but unknown binary tive labels means no chance to make mistakes ), and zero recall ( unless y = z ). Switch on any bit of z that is currently off . Then precision will decrease or remain equal , while recall will increase or remain equal.
Repeat until z = 11 . . .1 is reached , in which case recall will be perfect and precision at its minimum . The inference algorithm for expected Fscore follows the same strategy , and in particular it switches on the bits of z in order of nonincreasing probability : start with 00 . . .0, then switch on the bit i1 = argmaxi pi , etc . until 11 . . .1 is reached . We now show that this intuitive strategy is indeed admissible.
2.2 Outer and Inner Maximization
In general , maximization can be carried out piecewise , since argmax x?X f ( x ) = argmax x?{argmaxy?Y f ( y)|Y?pi(X )} f ( x ), where pi(X ) is any family ( Y1,Y2, . . .) of nonempty subsets of X whose union ? iYi is equal to X . ( Recursive application would lead to a divide-and-conquer algorithm .) Duplication of effort is avoided if pi(X ) is a partition of X .
Here we partition the set {0,1}n into equivalence classes based on the number of ones in a string ( viewed as a real vector ). Define Sm to be the set
Sm = { s ? {0,1} n | s ? s = m } consisting of all binary strings of fixed length n that contain exactly m ones . Then the maximization problem (4) can be transformed into an inner maximization s?(m ) = argmax s?Sm
E [ F(s , ?)] , (6) followed by an outer maximization z ? = argmax z?{s?(0),...,s?(n)}
E [ F(z , ?)] . (7) 2.3 Closed-Form Inner Maximization The key insight is that the inner maximization problem (6) can be solved analytically . Given a vector p = ( p1, . . . , pn ) of probabilities , define z(m ) to be the binary label sequence with exactly m ones and n?m zeroes where for all indices i,k we have [ z(m)i = 1? z ( m ) k = 0 ] ? pi ? pk.
Algorithm 1 Maximizing the Expected FScore.
1: Input : probabilities p = ( p1, . . . , pn ) 2: I ? indices of p sorted by nonincreasing probability 3: z ? 0 . . .0 4: a ? 0 5: v ? expectF(z , p ) 6: for j ? 1 to n do 7: i ? I [ j ] 8: z[i ]? 1 // switch on the ith bit 9: u ? expectF(z , p ) 10: if u > v then 11: a ? j 12: v ? u 13: for j ? a+1 to n do 14: z[I [ j ]]? 0 15: return ( z,v ) In other words , the most probable m bits ( according to p ) in z(m ) are set and the least probable n?m bits are off . We rely on the following result , whose proof is deferred to Appendix A : Theorem 1. (? s ? Sm ) E [ F(z(m ), ?)]? E [ F(s , ?)].
Because z(m ) is maximal in Sm , we may equate z(m ) = argmaxs?Sm E [ F(s , ?)] = s ? ( m ) ( modulo ties , which can always arise with argmax).
2.4 Pedestrian Outer Maximization
With the inner maximization (6) thus solved , the outer maximization (7) can be carried out naively , since only n + 1 hypotheses need to be evaluated . This is precisely what Algorithm 1 does , which keeps track of the maximum value in v . On termination z = argmaxs E [ F(s , ?)]. Correctness follows directly from our results in this section.
Algorithm 1 runs in time O(n logn + n f ( n )). A total of O(n logn ) time is required for accessing the vector p in sorted order ( line 2). This dominates the O(n ) time required to explicitly generate the optimal hypothesis ( lines 13?14). The algorithm invokes a subroutine expectF(z , p ) a total of n+1 times . This subroutine , which is the topic of the next section , evaluates , in time f ( n ), the expected Fscore ( with respect to p ) of a given hypothesis z of length n.
3 Computing the Expected FScore 3.1 Problem Statement We now turn to the problem of computing the expected value (3) of the Fscore for a given hypothesis z relative to a fully identified probability model . The method presented here does not strictly require the ( a higher-order Markov assumption will suffice ), but it shall remain in effect for simplicity.
As with the maximization problem (4), the sum in (3) is over exponentially many terms and cannot be computed naively . But observe that the Fscore (1) is a ( rational ) function of integer counts which are bounded , so it can take on only a finite , and indeed small , number of distinct values . We shall see shortly that the function (2) whose expectation we wish to compute has a domain whose cardinality is exponential in n , but the cardinality of its range is polynomial in n . The latter is sufficient to ensure that its expectation can be computed in polynomial time . The method we are about to develop is in fact very general and applies to many other loss and utility functions besides the Fscore.
3.2 Expected FScore as an Integral
A few notions from real analysis are helpful because they highlight the importance of thinking about functions in terms of their range , level sets , and the equivalence classes they induce on their domain ( the kernel of the function ). A function g : ?? R is said to be simple if it can be expressed as a linear combination of indicator functions ( characteristic functions ): g(x ) = ? k?K ak ? Bk(x ), where K is a finite index set , ak ? R , and Bk ? ?.
(?S : S?{0,1} is the characteristic function of set S .) Let ? be a countable set and P be a probability measure on ?. Then the expectation of g is given by the Lebesgue integral of g . In the case of a simple function g as defined above , the integral , and hence the expectation , is defined as
E [ g ] = ? ? g dP = ? k?K akP(Bk ). (8)
This gives us a general recipe for evaluating E[g ] when ? is much larger than the range of g . Instead of computing the sum ? y ?? g(y)P({y }) we can compute the sum in (8) above . This directly yields an efficient algorithm whenever K is sufficiently small andP(Bk ) can be evaluated efficiently.
The expected Fscore is thus the Lebesgue integral of the function (2). Looking at the definition of the 0,0
Y:n , n:n 1,1Y:Y 0,1 n:Y
Y:n , n:n 2,2
Y:Y 1,2 n:Y
Y:n , n:n Y:Y 0,2 n:Y
Y:n , n:n 3,3Y:Y 2,3 n:Y
Y:n , n:n Y:Y 1,3 n:Y
Y:n , n:n Y:Y 0,3 n:Y
Y:n , n:n
Y:n , n:n
Y:n , n:n
Y:n , n:n
Figure 1: Finite State Classifier h?.
Fscore in (1) we see that the only expressions which depend on y are A = z ? y and T = y ? y ( P = z ? z is fixed because z is ). But 0 ? z ? y ? y ? y ? n = | z|.
Therefore F(z , ?) takes on at most ( n+1)(n+2)/2, i.e . quadratically many , distinct values . It is a simple function with K = {( A,T ) ? N0?N0 | A ? T ? | z |, A ? z ? z } a(A,T ) = (? +1)A z ? z +? T where 0/0 def = 1
B(A,T ) = { y | z ? y = A , y ? y = T}.
3.3 Computing Membership in Bk
Observe that the family of sets (
B(A,T ) ) ( A,T )? K is a partition ( namely the kernel of F(z , ?)) of the set ?= {0,1}n of all label sequences of length n . In turn it gives rise to a function h : ?? K where h(y ) = k iff y ? Bk . The function h can be computed by a deterministic finite automaton , viewed as a sequence classifier : rather than assigning binary accept/reject labels , it assigns arbitrary labels from a finite set , in this case the index set K . For simplicity we show the initial portion of a slightly more general two-tape automaton h ? in Figure 1. It reads the two sequences z and y on its two input tapes and counts the number of matching positive labels ( represented as Y ) as well as the number of positive labels on the second tape.
Its behavior is therefore h?(z,y ) = ( z ? y , y ? y ). The function h is obtained as a special case when z ( the first tape ) is fixed.
Note that this only applies to the special case when def start (): return (0,0) def transition(k,z , i,yi ): ( A,T )? k if yi = 1 then
T ? T +1 if z[i ] = 1 then
A ? A+1 return ( A,T ) def a(k,z ): ( A,T )? k
F ? (? +1)A z ? z +? T // where 0/0 def = 1 return F Algorithm 3 Value of a Simple Function.
1: Input : instance g of the simple function interface , strings z and y of length n 2: k ? g.start () 3: for i ? 1 to n do 4: k ? g.transition(k,z , i,y[i ]) 5: return g.a(k,z ) the family B = ( Bk)k?K is a partition of ?. It is always possible to express any simple function in this way , but in general there may be an exponential increase in the size of K when the family B is required to be a partition . However for the special cases we consider here this problem does not arise.
3.4 The Simple Function Trick
In general , what we will call the simple function trick amounts to representing the simple function g whose expectation we want to compute by : 1. a finite index set K ( perhaps implicit ), 2. a deterministic finite state classifier h : ?? K , 3. and a vector of coefficients ( ak)k?K .
In practice , this means instantiating an interface with three methods : the start and transition function of the transducer which computes h ? ( and from which h can be derived ), and an accessor method for the coefficients a . Algorithm 2 shows the Fscore instance.
Any simple function g expressed as an instance of this interface can then be evaluated very simply as g(x ) = ah(x ). This is shown in Algorithm 3.
Evaluating E [ g ] is also straightforward : Compose the DFA h with the probability model p and use an algebraic path algorithm to compute the total probability massP(Bk ) for each final state k of the resulting automaton . If p factors into independent components as required by (5), the composition is greatly sim-Algorithm 4 Expectation of a Simple Function.
1: Input : instance g of the simple function interface , string z and probability vector p of length n 2: M?Map () 3: M[g.start ()]? 1 4: for i ? 1 to n do 5: N?Map () 6: for ( k,P ) ? M do 7: // transition on yi = 0 8: k0? g.transition(k,z , i,0) 9: if k0 /? N then 10: N[k0]? 0 11: N[k0]? N[k0]+P ? (1? p[i ]) 12: // transition on yi = 1 13: k1? g.transition(k,z , i,1) 14: if k1 /? N then 15: N[k1]? 0 16: N[k1]? N[k1]+P ? p[i ] 17: M ? N 18: E ? 0 19: for ( k,P ) ? M do 20: E ? E + g.a(k,z)?P 21: return E plified . If p incorporates label history ( higher-order Markov assumption ), nothing changes in principle , though the following algorithm assumes for simplicity that the stronger assumption is in effect.
Algorithm 4 expands the following composed automaton , represented implicitly : the finite-state transducer h ? specified as part of the simple function object g is composed on the left with the string z ( yielding h ) and on the right with the probability model p . The outer loop variable i is an index into z and hence a state in the automaton that accepts z ; the variable k keeps track of the states of the automaton implemented by g ; and the probability model has a single state by assumption , which does not need to be represented explicitly . Exploring the states in order of increasing i puts them in topological order , which means that the algebraic path problem can be solved in time linear in the size of the composed automaton.
The maps M and N keep track of the algebraic distance from the start state to each intermediate state.
On termination of the first outer loop ( lines 4?17), the map M contains the final states together with their distances . The algebraic distance of a final state k is now equal to P(Bk ), so the expected value E can be computed in the second loop ( lines 18?20) as suggested by (8).
When the utility function interface g is instantiated as in Algorithm 2 to represent the Fscore , the runtime of Algorithm 4 is cubic in n , with very small inner loop iterates over the states expanded at iteration i , of which there are O(i2) many when dealing with the Fscore . The second main loop iterates over the final states , whose number is quadratic in n in this case . The overall cubic runtime of the first loop dominates the computation.
3.5 Other Utility Functions
With other functions g the runtime of Algorithm 4 will depend on the asymptotic size of the index set K.
If there are asymptotically as many intermediate states at any point as there are final states , then the general asymptotic runtime is O(n | K|).
Many loss/utility functions are subsumed by the present framework . Zero?one loss is trivial : the automaton has two states ( success , failure ); it starts and remains in the success state as long as the symbols read on both tapes match ; on the first mismatch it transitions to , and remains in , the failure state.
Hamming (1950) distance is similar to zero?one loss , but counts the number of mismatches ( bounded by n ), whereas zero?one loss only counts up to a threshold of one.
A more interesting case is given by the Pk-score ( Beeferman et al , 1999) and its generalizations , which moves a sliding window of size k over a pair of label sequences ( z,y ) and counts the number of windows which contain a segment boundary on one of the sequences but not the other . To compute its expectation in our framework , all we have to do is express the sliding window mechanism as an automaton , which can be done very naturally ( see the proof-of-concept implementation for further details).
4 Faster Inexact Computations
Because the exact computation of the expected Fscore by Algorithm 4 requires cubic time , the overall runtime of Algorithm 1 ( the decoder ) is quartic.4 3A tight upper bound on the total number of states of the composed automaton in the worst case is ? 3 + 58 n 2 + 1712 n+1 ? .
4It is possible to speed up the decoding algorithm in absolute terms , though not asymptotically , by exploiting the fact that it explores very similar hypotheses in sequence . Algorithm 4 can be modified to store and return all of its intermediate map datastructures . This modified algorithm then requires cubic space instead of quadratic space . This additional storage cost pays off when the algorithm is called a second time , with its formal parameter z bound to a string that differs from the one of the Faster decoding can be achieved by modifying Algorithm 4 to compute an approximation ( in fact , a lower bound ) of the expected F-score.5 This is done by introducing an additional parameter L which limits the number of intermediate states that get expanded.
Instead of iterating over all states and their associated probabilities ( inner loop starting at line 6), one iterates over the top L states only . We require that L ? 1 for this to be meaningful . Before entering the inner loop the entries of the map M are expanded and , using the linear time selection algorithm , the top L entries are selected . Because each state that gets expanded in the inner loop has outdegree 2, the new state map N will contain at most 2L states . This means that we have an additional loop invariant : the size of M is always less than or equal to 2L . Therefore the selection algorithm runs in time O(L ), and so does the abridged inner loop , as well as the second outer loop . The overall runtime of this modified algorithm is therefore O(n L).
If L is a constant function , the inexact computation of the expected Fscore runs in linear time and the overall decoding algorithm in quadratic time . In particular if L = 1 the approximate expected Fscore is equal to the Fscore of the MAP hypothesis , and the modified inference algorithm reduces to a variant of Viterbi decoding . If L is a linear function of n , the overall decoding algorithm runs in cubic time.
We experimentally compared the exact quartic-time decoding algorithm with the approximate decoding algorithm for L = 2n and for L = 1. We computed the absolute difference between the expected Fscore of the optimal hypothesis ( as found by the exact algorithm ) and the expected Fscore of the winning hypothesis found by the approximate decoding algorithm . For different sequence lengths n ? {1, . . . ,50} we performed 10 runs of the different decoding algorithms on randomly generated probability vectors p , where each pi was randomly drawn from a continuous uniform distribution on (0,1), or , in a second experiment , from a Beta(1/2,1/2) distribution ( to simulate an overtrained classifier).
For L = 1 there is a substantial difference of about preceding run in just one position . This means that the map datastructures only need to be recomputed from that position forward . However , this does not lead to an asymptotically faster algorithm in the worst case.
5For error bounds , see the proof-of-concept implementation.
741 0.6 between the expected Fscores of the winning hypothesis computed by the exact algorithm and by the approximate algorithm . Nevertheless the approximate decoding algorithm found the optimal hypothesis more than 99% of the time . This is presumably due to the additional regularization inherent in the discrete maximization of the decoder proper : even though the computed expected Fscores may be far from their exact values , this does not necessarily affect the behavior of the decoder very much , since it only needs to find the maximum among a small number of such scores . The error introduced by the approximation would have to be large enough to disturb the order of the hypotheses examined by the decoder in such a way that the true maximum is reordered.
This generally does not seem to happen.
For L = 2n the computed approximate expected Fscores were indistinguishable from their exact values.
Consequently the approximate decoder found the true maximum every time.
5 Conclusion and Related Work
We have presented efficient algorithms for maximum expected Fscore decoding . Our exact algorithm runs in quartic time , but an approximate cubic-time variant is indistinguishable in practice . A quadratic-time approximation makes very few mistakes and remains practically useful.
We have further described a general framework for computing the expectations of certain loss/utility functions . Our method relies on the fact that many functions are sparse , in the sense of having a finite range that is much smaller than their codomain . To evaluate their expectations , we can use the simple function trick and concentrate on their level sets : it suffices to evaluate the probability of those sets / events . The fact that the commonly used utility functions like the Fscore have only polynomially many level sets is sufficient ( but not necessary ) to ensure that our method is efficient . Because the coefficients ak can be arbitrary ( in fact , they can be generalized to be elements of a vector space over the reals ), we can deal with functions that go beyond simple counts.
Like the methods developed by Allauzen et al (2003) and Cortes et al (2003) our technique incorporates finite automata , but uses a direct threshold-counting technique , rather than a nondeterministic counting technique which relies on path multiplicities . This makes it easy to formulate the simultaneous counting of two distinct quantities , such as our A and T , and to reason about the resulting automata.
The method described here is similar in spirit to those of Gao et al (2006) and Jansche (2005), who discuss maximum expected Fscore training of decision trees and logistic regression models . However , the present work is considerably more general in two ways : (1) the expected utility computations presented here are not tied in any way to particular classifiers , but can be used with large classes of probabilistic models ; and (2) our framework extends beyond the computation of Fscores , which fall out as a special case , to other loss and utility functions , including the Pk score . More importantly , expected Fscore computation as presented here can be exact , if desired , whereas the cited works always use an approximation to the quantities we have called A and T .
Acknowledgements
Most of this research was conducted while I was affilated with the Center for Computational Learning Systems , Columbia University . I would like to thank my colleagues at Google , in particular Ryan McDonald , as well as two anonymous reviewers for valuable feedback.
References
Cyril Allauzen , Mehryar Mohri , and Brian Roark . 2003. Generalized algorithms for constructing language models . In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics.
Doug Beeferman , Adam Berger , and John Lafferty . 1999. Statistical models for text segmentation . Machine Learning , 34(1?3):177?210.
Francisco Casacuberta and Colin de la Higuera . 2000. Computational complexity of problems on probabilistic grammars and transducers . In 5th International Colloquium on Grammatical
Inference.
Corinna Cortes , Patrick Haffner , and Mehryar Mohri . 2003. Rational kernels . In Advances in Neural Information Processing
Systems , volume 15.
Sheng Gao , Wen Wu , Chin-Hui Lee , and Tai-Seng Chua . 2006.
A maximal figure-of-merit ( MFoM)-learning approach to robust classifier design for text categorization . ACM Transactions on Information Systems , 24(2):190?218. Also in ICML 2004.
Samuel S . Gross , Olga Russakovsky , Chuong B . Do , and Serafim Batzoglou . 2007. Training conditional random fields for maximum labelwise accuracy . In Advances in Neural Information Processing Systems , volume 19.
R . W . Hamming . 1950. Error detecting and error correcting codes . The Bell System Technical Journal , 26(2):147?160.
Martin Jansche . 2005. Maximum expected Fmeasure training of logistic regression models . In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing.
742
Kevin Knight . 1999. Decoding complexity in word-replacement translation models . Computational Linguistics , 25(4):607? 615.
Michael C . Mozer , Robert Dodier , Michael D . Colagrosso , Ce?sar Guerra-Salcedo , and Richard Wolniewicz . 2001. Prod-ding the ROC curve : Constrained optimization of classifier performance . In Advances in Neural Information Processing
Systems , volume 14.
David R . Musicant , Vipin Kumar , and Aysel Ozgur . 2003.
Optimizing Fmeasure with support vector machines . In Proceedings of the Sixteenth International Florida Artificial Intelligence Research Society Conference.
Jun Suzuki , Erik McDermott , and Hideki Isozaki . 2006. Training conditional random fields with multivariate evaluation measures . In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics.
C . J . van Rijsbergen . 1974. Foundation of evaluation . Journal of Documentation , 30(4):365?373.
Appendix A Proof of Theorem 1
The proof of Theorem 1 employs the following lemma : Theorem 2. For fixed n and p , let s , t ? Sm for some m with 1 ? m < n . Further assume that s and t differ only in two bits , i and k , in such a way that si = 1, sk = 0; ti = 0, tk = 1; and pi ? pk . Then E [ F(s , ?)]? E [ F(t , ?)].
Proof . Express the expected Fscore E [ F(s , ?)] as a sum and split the summation into two parts : ? y
F(s,y ) Pr(y ) = ? y yi=yk
F(s,y ) Pr(y ) +? y yi 6=yk
F(s,y ) Pr(y).
If yi = yk then F(s,y ) = F(t,y ), for three reasons : the number of ones in s and t is the same ( namely m ) by assumption ; y is constant ; and the number of true positives is the same , that is s ? y = t ? y . The latter holds because s and y agree everywhere except on i and k ; if yi = yk = 0, then there are no true positives at i and k ; and if yi = yk = 1 then si is a true positive but sk is not , and conversely tk is but ti is not . Therefore ? y yi=yk
F(s,y ) Pr(y ) = ? y yi=yk
F(t,y ) Pr(y ). (9)
Focus on those summands where yi 6= yk . Specifically group them into pairs ( y,z ) where y and z are identical except that yi = 1 and yk = 0, but zi = 0 and zk = 1. In other words , the two summations on the righthand side of the following equality are carried out in parallel : ? y yi 6=yk
F(s,y ) Pr(y ) = ? y yi=1 yk=0
F(s,y ) Pr(y )+ ? z zi=0 zk=1
F(s,z ) Pr(z).
Then , focusing on s first:
F(s,y ) Pr(y)+F(s,z ) Pr(z ) = (? +1)(A+1) m+?T Pr(y )+ (? +1)A m+?T Pr(z ) = [( A+1)pi (1? pk)+A(1? pi)pk ] (? +1) m+?T C = [ pi +( pi + pk?2pi pk)A ? pi pk ] (? +1) m+?T C = [ pi + C0]C1, where A = s ? z is the number of true positives between s and z ( s and y have an additional true positive at i by construction ); T = y ? y = z ? z is the number of positive labels in y and z ( identical by assumption ); and
C =
Pr(y ) pi (1? pk ) =
Pr(z ) (1? pi ) pk is the probability of y and z evaluated on all positions except for i and k . This equality holds because of the zeroth-order Markov assumption (5) imposed on Pr(y ). C0 and C1 are constants that allow us to focus on the essential aspects.
The situation for t is similar , except for the true positives:
F(t,y ) Pr(y)+F(t,z ) Pr(z ) = (? +1)A m+?T Pr(y )+ (? +1)(A+1) m+?T Pr(z ) = [ A pi (1? pk)+(A+1)(1? pi)pk ] (? +1) m+?T C = [ pk +( pi + pk?2pi pk)A ? pi pk ] (? +1) m+?T C = [ pk + C0]C1 where all constants have the same values as above . But pi ? pk by assumption , pk + C0 ? 0, and C1 ? 0, so we have
F(s,y ) Pr(y)+F(s,z ) Pr(z ) = [ pi + C0]C1 ? F(t,y ) Pr(y)+F(t,z ) Pr(z ) = [ pk + C0]C1, and therefore ? y yi 6=yk
F(s,y ) Pr(y )? ? y yi 6=yk
F(t,y ) Pr(y ). (10)
The theorem follows from equality (9) and inequality (10).
Proof of Theorem 1: (? s ? Sm ) E [ F(z(m ), ?)]? E [ F(s , ?)].
Observe that z(m ) ? Sm by definition ( see Section 2.3). For m = 0 and m = n the theorem holds trivially because Sm is a singleton set . In the nontrivial cases , Theorem 2 is applied repeatedly . The string z(m ) can be transformed into any other string s ? Sm by repeatedly clearing a more likely set bit and setting a less likely unset bit.
In particular this can be done as follows : First , find the indices where z(m ) and s disagree . By construction there must be an even number of such indices ; indeed there are equinumerous sets { i ? ? z(m)i = 1? si = 0 } ? { j ? ? z(m)j = 0? s j = 1 } .
This holds because the total number of ones is fixed and identical in z(m ) and s , and so is the total number of zeroes . Next , sort those indices by nonincreasing probability and represent them as i1, . . . , ik and j1, . . . , jk . Let s0 = z(m ). Then let s1 be identical to s0 except that si1 = 0 and s j1 = 1. Form s2, . . . , sk along the same lines and observe that sk = s by construction . By definition of z(m ) it must be the case that pir ? p jr for all r ? {1, . . . , k}.
Therefore Theorem 2 applies at every step along the way from z(m ) = s0 to sk = s , and so the expected utility is nonincreasing along that path.
743
