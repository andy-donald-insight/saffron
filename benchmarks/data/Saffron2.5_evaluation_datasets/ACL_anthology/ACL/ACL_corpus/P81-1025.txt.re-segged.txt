PER SPECTIVE SON
PARSINGISSUES
Christopher K . Riesbeck
Yale University
COMPUTATIONAL PEESPECT TVE
ISITUSE FULTODISTINGUISH PARSING FROM INTERPRETATION ? Since most of this posit i on paper viii be attacking the separation of parsing from interpretation  , let me first make it clear that I dobel ieve in syntactic knowledge  . In this Iam more conservative than other researchers in interpretation at Berkeley  , Carnegie-Mellon , Colombia , the universities of Connecticut and Mary l and  , and Yale . 
But believing in syntactic knowledge is not the same as believing in parsers ! The search for a way to assign  8 syntactic structure to a sentence largely independent of the meaning of that sentence has led to a terrible misdirection of labor  . And this effect has been felt on boths ides of the fence  . We find ourselves looking for ways to reduce interaction between syntax and semantics as much as possible  . How far can we drive a purely syntactic ( semantic ) analyzer , without sneaking over into the enemy camp ? Row well can we disguise syntax  ( semantics ) as semantics ( syntax ) ? How narrow a pipe between the two can we set away with ? What a waste of time  , when we should be starting with bodies of texts , considering the total language analysis picture , and looking for what kinds of knowledge need to interact to understand those texts  . 
If our intent in overext end in sour theories was to rest their muscle  , then I would have no qualms . Pushing a mechanism downablindalley is an important way to study its weaknesses  . But I really can't accept this Popperian v iew of modern computational linguistics  . 
Mechanisms are not driven beyond their limi ts to find those limits  , but rather to grabter ritory from the o ther side  . The underlying premise is " If our mechanism X can sometimes do task A  , then there is no need for someone else's mechanism Y  . " Occam's razor is used with murderous intent . 
Furthermore , the debate over whether parsers make sense has drastically reduced interaction between researchers  . Each side sees the other as avoiding fundamental issues  , and so the results from the others idea l ways seem to be beside the point  . For example , when Mirth Marcus " explains some grmamatical constraint as syntactic processing constra ints  , be doesn't answer any of the problems I'm faced with  . And I'msure Mitch has no need for frame -based  , domain-driven partial language analysis techniques  . 
This situation has not arisen because we have been forced to specialize  . We simply don't know enough to qualify for an information explosion yet  . Computational linguistics doesn't have hundreds of journals in dozens of languages  . It's a young field with only a handfulo f people working in it  . 
Noris it the case that we don't have things to say to each other  . But--end here's the rub--some of the most useful things that each of us knows are the things that we don't d are tell  . By that I mean that each of us knows where our theories fall apart  , whereve have to kludge the programs , fudge the inputs , or wince at the outputs . That kind of information could be invaluab le for suggesting to the others where to focus their attentions  . Unfortunately , even if we became brave enough to talk about , even emphasize , where we're having problems , the odds are low that we would consider acceptab lew hat someone else proposes as a solution  . 
ISSIMULATION OF HUMANPROCES SINGIMPORTANT ? Yes , very much so , even if all you are interested in is a good computer program  . The reason why was neatly captured in ~ rinciDles of Artificia ~ lnte ~ lieence : " language has evo lved as a c~unication medium between in tell iaen ~ beings "  ( Nilsson , p .  2 )  . That is , natural language usage depends on the fact that certain things can be left ambiguous  , left vague , or just left out , because the hearer knows almost as much as the speaker  . 
Natural language has been finely tuned to the co-  , -unicative needs of human beings . We may have to adapt to the limitations of our ears and our vocal chords  , but we have otherwise been the masters of our language  . This is true even if there is an innate un iversal grmmuar  ( which I don't believe in )   . A universal grammar applies few constrain ts to our use of ellipsis  , ambiguity , anaphora , and all the other aspects of language that make language an efficient means for info rmation transfer  , end a pain for the progr----er . 
Because language has been fitted to what we do best  , I believe it's improbable that there exis t processes very unlike what people use to deal with it  . Therefore , while I have no intention of trying to model reaction time data points  , I do find human behavior important for two k inds of information  . First , what do people do well , how do they do it , and how does language use depend on it ? Second , what do people do poorly , and how does language use get around it ? The question ' ~ ow can we know what human process ing is really like ?" is a non-issue  . We don't have to know what human processing i sreally like  . But if people can understand texts that leave out crucial background facts  , then our programs have to be able to infer those facts  . 
If people have trouble understanding in formation phrased in certain ways  , then our programs have to phrase it in ways they can understand  . At some level of description , our programs will have to be " doing what people do  , " i . e .   , filling in certain kinds of blanks , leaving out certain kinds of redundancies , and so on . But there is no reason for computation all inguists to worry about how deeply their p rograms correspond to human processes  . 
WILL PARALLEL PROCESSING CHANGETHINGS ?
People have been predicting ( and waiting for ) great benefits from parallelism for some time . Personally , I believe that most of the benefits will come in the area of interpretation  , where large-scale memory search , such as Scott Fahlman has been worrying about , are involved . 
And , if anything , improvements in the use of semantics will decrease the attractiveness of syntactic parsing  . 
But I also think that there are not that many gains to be had from parallel processing  . Hashcodings , discrimination trees , and so on , already yield reasonably constant speeds fo r looking up data  . It is an inconvenience to have to deal with such things  , but not an insurmountable obstacle . Our real problems at the moment are how to get our systems to make decisions  , such as " Is the question " How many times has John asked you for money ? " rhetorical or not ? " We are limited not by the number of p rocessors  , but by not knowing how to do the job . 

TtI . ~ ELINGUISTIC PERSPECTIVE
HAVEOURTOOLSAFFECTEDUS ?
Yes , and adversely . To partially contradict my statements in the last paragraph  , we've been overly concerned with how to do things with existing hardware and s of tw are  . And we've been too impressed by the success computer science has had with syntax-driven compilation of programming languages  . I1 is certainly true that work on grammars , parsers , code generators , and so on , have changed compiler generation from maes ive multi-man-year endeavors to student course projects  . If compiler technology has benefited so much from syntactic parsers  , why can't computational linguistics ? The problem here is that the technology has not do new hat people think it has  . It has allowed us to develop modern , well-structured , task-oriented languages , but it has not given us natural ones . Anyone who has had cot each an introductory progru ~ ing course knows that  . 
High-level languages , though easier to learn than machine language , are very different from human languages , such as English or Chinese . 
Programming languages , to read just Nilsson's quote , are developed for c~unication between morons  . All the useful features of language , such as ellipsis and ambiguity , have to be eliminated in order couse the technology of syntax-driven parsing  . Compilers do not point the way for computational linguistics  . They show instead what we get if we restrict ourselves to simplistic methods  . 
DOW EPARSE CONTEXT-FREELY ?
My working assumption is that the syntactic knowledge used in comprehension is at most context-free and probably a lot less  , because of memory limitations . 
This is mostly a result of semantic heuri stics taking over when constructions become too complex for our cognitive chunking capac ities  . But this is not a critical assumption form e . 
; ~ rE~AC'~ONS
Since I don't believe in the pure gran~atica l approach  , I have to replace this last set of questions with questions about the relationship between our knowledge  ( linguistic and otherwise ) and the procedures for applying i1 . Fortunately , the questions still make sense after thi ssubstitution  . 
DOOURAL GORITHMSA FFECTOURKNOWLEDGEST RUCTURES ? Of course  . In fact , it is often hard to decide whether some feature of a system is a knowledge structure or a procedural factor  . For example , is linear search a result of datast ructures or procedure designs ? CANWETEST ALGORITHMS/KNOWLEDGESTRUCTURESSEPARATELY ? We do indeed try experiments based on the shape of knowledge structures  , independently of bow they are used ( but I think that most such experiments have been inconclusive  )   . I'm not sure what it would mean , however , for a procedure to be validated independently of the knowledge structures it works with  , since until the knowledge structures were r ight  , you couldn't tell if the procedure was do ing the right thing or not  . 
WHYDOWESE PARATERECOGNITION ANDPRODUCTION ? I fI were trying to deal with this questio nonEr ratical grounds  , I wouldn't know what it meant . 
Cr ~ ars are not processes and hence have no di rection  . 
They are abstract characterizations of the set of well-formed strings  . From certain classes of gra-w-ars generators  . But such machines are not the gra-~ars , and a recognizer is manifestly not the same machine as a generator  , even though the same grammar may underlie both . 
Suppose vere phrase the question as '~ hy do we have separate knowledge structures for in terp retation and production ? " This p resupposes that there are separate knowledges t ructures  , and in our current systems this is only part ially true  . 
Interpreting and production programs abound in adhoc procedures that share very little in common near the language end  . The interpreters are full of methods for guessing at meanings  , filling in the blanks , predicting likely follow-ups , and so on . The generators are full of methods for el iminating contextual items  , picking appropriate descriptors , choosing pronouns , and so on . 
Each has a very different set of problems to deal with  . 
On the other hand , our interpreters and generators do share what we think is the important stuff  , the world knowledge , without which all the other processing wou ldn't be worth a part ridge in a parse t ree  . The world knowledge says what makes sense in onder standins and what is important to talk about  . 
Part of the separation of interpretation and generation occurs when the programs for each are developed by different people  . This Tesults in unrealistic systems that write what they can't read and read what they can't write  . Some day we'll have a good model of how knowledge the interpreter gains about unders tanding a new word is converted to knowledge the generator can use to validly pick that word in production  . This viii have account for how we can in terpret words without being ready to use them  . 
For example , from a sentence like " The cars werved off the road and struck a bridge a butment  , " we can infer that an a butment is a noun descr ibing some kind of outdoor physical objec t  , attach able to a bridge . This would be enough for interpretation , but obviously the generator will need co know more about what an a butment is before it could confidently say " Oh  , look at the cute a butment ! " A final point on sharing  . There are two standard arguments for sharing a tleast gr = mmatical information  . 
One is to save space , and the other is to maintain consistency . Without claiming that sharing doesn't occur  , I would like to point out that both arguments are very weak  . First , there is really not a lot of grammatical knowledge  , compared against all the other knowledge we have about the world  , so not that much space would be saved if shar ing occurred  . Second , if the generator derives it's linguist i cknowledge from the parser's database  , then we'll have as much consistency as we could measure in people anyway  . 

Nilsson , H .  (1980) . Princinle ~ of Artificia ~ Intellisence . Tioga Publishing Co , Palo Alto,

