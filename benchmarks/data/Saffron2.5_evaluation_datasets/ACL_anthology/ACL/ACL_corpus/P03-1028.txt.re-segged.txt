Closing the Gap : Learning-Based Information Extraction Rivaling 
Knowledge Engineering Methods
Hai Leong Chieu
DSO National Laboratories
20 Science Park Drive
Singapore 118230

Hwee Tou Ng
Department of Computer Science
National University of Singapore
3 Science Drive 2
Singapore 117543

Yoong Keok Lee
DSO National Laboratories
20 Science Park Drive
Singapore 118230


In this paper , we present a learning ap-
proach to the scenario template task of
information extraction , where information filling one template could come from multiple sentences  . When tested on the MUC4 task , our learning approach achieves accuracy competitive to the best of the  MUC4 systems , which were all built with manually engineered rules  . Our analysis reveals that our use of full parsing and state-of-the-art learning algorithms have contributed to the good performance  . 
To our knowledge , this is the first research to have demonstrated that a learning approach to the fullscale information extraction task could achieve performance rivaling that of the knowledge -engineering approach  . 
1 Introduction
The explosive growth of online texts written in natural language has prompted much research into information extraction  ( IE )  , the task of automatically extracting specific information items of interest from natural language texts  . The extracted information is used to fill database records  , also known as templates in the IE literature . 
Research efforts on IE tackle a variety of tasks.
They include extracting information from semistructured texts  , such as seminar announcements , rental and job advertisements , etc . , as well as from free texts , such as newspaper articles ( Soderland ,  1999) . IE from semistructured texts is easier than from freetexts  , since the layout and format of a semistructured text provide additional useful clues AYACUCHO  ,   19 JAN 89 ? TODAYTWOPEOPLE WEREWOUNDED WHENABOMB EXPLODEDINS ANJUANB AUTISTAMUNICIPALITY  . OFFICIALS SAIDTHATS HINING PATH MEMBERS WERE RESPONSIBLE FOR THE ATTACK  . . .   . . . POLICE SOURCES STATED THAT THE BOMB ATTACK INVOLVING THESHINING PATH 
CAUSED SERIOUSDAMAGES ......
Figure 1: Snippet of a MUC4 document to aid in extraction . Several benchmark datasets have been used to evaluate IE approaches on semistructured texts  ( Soderland , 1999; Ciravegna ,  2001;
Chieu and Ng , 2002a).
For the task of extracting information from free texts  , a series of Message Understanding Conferences ( MUC ) provided benchmark datasets for evaluation . Several subtasks for IE from freetexts have been identified  . The named entity ( NE ) task extracts person names , organization names , location names , etc . The template element ( TE ) task extracts information centered around an entity  , like the acronym , category , and location of a company . The template relation ( TR ) task extracts relations between entities . Finally , the fullscale IE task , the scenario template ( ST ) task , deals with extracting generic information items from freetexts  . To tackle the full ST task , an IE system needs to merge information from multiple sentences in general  , since the information needed to fill one template can come from multiple sentences  , and thus discourse processing is needed . 
The fullscale ST task is considerably harder than all the other IE tasks or subtasks outlined above  . 
As is the case with many other natural language processing  ( NLP ) tasks , there are two main approaches to IE , namely the knowledge-engineering approach and the learning approach  . Most early IE systems adopted the knowledge -engineering  ap-0 MESSAGE : ID TST3-MUC4-0014   1 MESSAGE : TEMPLATE 1   2 INCIDENT : DATE 19-JAN-89   3 INCIDENT:LOCATION PERU:SANJUANB AUTISTA ( MUNICIPALITY )   4 INCIDENT : TYPEBOMBING 5 INCIDENT : STAGEOFEXE CUTION ACCOMPLISHED 6 INCIDENT : INSTRUMENTID ? BOMB ? 7 INCIDENT : INSTRUMENT TYPEBOMB : ? BOMB ? 8 PERP:INCIDENT CATEGORY TERRORIST ACT 9 PERP:INDIVIDUALID ? SHINING PATH MEMBERS ? 10 PERP:ORGANIZATIONID ? SHININGPATH ? 11 PERP:ORGANIZATION SUSPECTED ORACCUSEDBY
CONFIDENCE AUTHORITIES : ? SHININGPATH ? 12 PHYSTGT : ID -13 PHYSTGT : TYPE -14 PHYSTGT : NUMBER -15 PHYSTGT : FOREIGNNATION -16 PHYSTGT : EFFECTOFINCIDENTSOMEDAMAGE : ?-? 17 PHYSTGT : TOTALNUMBER -18 HUMTGT : NAME -19 HUMTGT : DESCRIPTION ? PEOPLE ? 20 HUMTGT : TYPECIVILIAN : ? PEOPLE ? 21 HUMTGT : NUMBER 2:?PEOPLE  ?  22 HUMTGT : FOREIGNNATION -23 HUMTGT : EFFECTOFINCIDENTINJURY : ? PEOPLE ? 24 HUMTGT : TOTALNUMBER-
Figure 2: Example of a MUC4 template proach , where manually engineered rules were used for IE . More recently , machine learning approaches have been used for IE from semistructured texts  ( Califf and Mooney , 1999; Soderland , 1999; Roth and Yih , 2001; Ciravegna , 2001; Chieu and Ng , 2002a ) , named entity extraction ( Chieu and Ng , 2002b ) , template element extraction , and template relation extraction ( Miller et al ,  1998) . These machine learning approaches have been successful for these tasks  , achieving accuracy comparable to the knowledge -engineering approach  . 
However , for the fullscale ST task of generic IE from free texts  , the best reported method to date is still the knowledge-engineering approach  . For example , almost all participating IE systems in MUC used the knowledge-engineering approach for the fullscale ST task  . The one notable exception is the work of UMass at  MUC6   ( Fisher et al ,  1995) . 
Unfortunately , their learning approach did considerably worse than the best  MUC6 systems . Soderland ( 1999 ) and Chieu and Ng ( 2002a ) attempted machine learning approaches for a scaled-down version of the ST task  , where it was assumed that the information needed to fill one template came from one sentence only  . 
In this paper , we present a learning approach to the fullscale ST task of extracting information from free texts  . The task we tackle is considerably more complex than that of  ( Soderland , 1999; Chieu and Ng , 2002a ) , since we need to deal with merging information from multiple sentences to fill one template  . We evaluated our learning approach on the MUC4 task of extracting terrorist events from free texts  . We chose the MUC4 task since manually prepared templates required for training are available  . 1 When trained and tested on the official benchmark data of  MUC4  , our learning approach achieves accuracy competitive with the best  MUC4 systems , which were all built using manually engineered rules  . To our knowledge , our work is the first learning-based approach to have achieved performance competitive with the knowledge-engineering approach on the fullscale ST task  . 
2 Task Definition
The task addressed in this paper is the Scenario Template  ( ST ) task defined in the Fourth Message Understanding Conference  ( MUC-4 )  . 2 The objective of this task is to extract information on terrorist events occurring in Latin American countries from freetext documents  . For example , given the input document in Figure 1 , an IE system is to extract information items related to any terrorist events to fill zero or more database records  , or templates . Each distinct terrorist event is to fill one template  . An example of an output template is shown in Figure  2  . Each of the 25 fields in the template is called a slot , and the string or value that fills a slot is called a slot fill  . 
Different slots in the MUC4 template need to be treated differently . Besides slot 0 ( MESSAGE : ID ) and slot 1 ( MESSAGE : TEMPLATE )  , the other 23 slots have to be extracted or inferred from the text document  . These slots can be divided into the following categories : String Slots  . These slots are filled using strings extracted directly from the text document  ( slot 6 ,  9 ,  10 ,  12 ,  18 ,  19) . 
Text Conversion Slots . These slots have to be inferred from strings in the document  ( slot 2 ,  14 ,  17 ,  21 ,  24) . For example , INCIDENT : DATE has to be inferred from temporal expressions such as ?  TO-1http://www  . itl . nist . gov/iaui/894 . 02/related projects/muc/muc data/muc data index . html 2The fullscale IE task is called the ST task only in MUC6 and MUC7  , when other subtasks like NE and TE tasks were defined  . Here , we adopted this terminology also in describing the fullscale IE task for  MUC4  . 
Figure 3: ALICE : our information extraction system
DAY ?, ? LASTWEEK ?, etc.
Set Fill Slots . This category includes the rest of the slots . The value of a set fill slot comes from a finite set of possible values  . They often have to be inferred from the document . 
3 The Learning Approach
Our supervised learning approach is illustrated in Figure  3  . Our system , called ALICE ( Automated Learning-based Information Content Extraction  )  , requires manually extracted templates paired with their corresponding documents that contain terrorist events for training  . After the training phase , ALICE is then able to extract relevant templates from new documents  , using the model learnt during training . 
In the training phase , each input training document is first preprocessed through a chain of preprocessing modules  . The outcome of the preprocessing is a full parse tree for each sentence  , and coreference chains linking various coreferring noun phrases both within and across sentences  . The core of ALICE uses supervised learning to build one classifier for each string slot  . The candidates to fill a template slot are base ( nonrecursive ) noun phrases . A noun phrase   that occurs in a training document  and fills a template slot  is used to generate one positive training example for the classifier of slot   . Other noun phrases in the training document  are negative training examples for the classifier of slot   . 
The features of a training example generated from   are the verbs and other noun phrases  ( serving roles like agent and patient ) related to    in the same sentence , as well as similar features for coreferring noun phrases of    . Thus , our features for a template slot classifier encode semantic  ( agent and patient roles ) and discourse ( coreference ) information . 
Our experimental results in this paper demonstrate that such features are effective in learning what to fill a template slot  . 
During testing , a new document is preprocessed through the same chain of preprocessing modules  . 
Each candidate noun phrase    generates one test example  , and it is presented to the classifier of a template slot  to determine whether    fills the slot   . 
A separate template manager decides whether a new template should be created to include slot   , or slot  should fill the existing template . 
3.1 Preprocessing
All the preprocessing modules of ALICE were built with supervised learning techniques  . They include sentence segmentation ( Ratnaparkhi ,  1998) , part-of-speech tagging ( Charniak et al ,  1993) , named entity recognition ( Chieu and Ng , 2002b ) , full parsing ( Collins ,  1999) , and coreference resolution ( Soon et al . , 2001) . Each module performs at or near state-of-the-art accuracy  , but errors are unavoidable , and later modules in the preprocessing chain have to deal with errors made by the previous modules  . 
3 . 2 Features in Training and Test Examples As mentioned earlier  , the features of an example are generated based on a base noun phrase  ( denoted as base NP )  , which is a candidate for filling a template slot . While most strings that fill a string slot are base noun phrases  , this is not always the case . For instance , consider the two examples in Figure 4 . In the first example , ? BOMB ? should fill the string slot IN-CIDENT : INSTRUMENTID  , while in the second example , ? FMLN ? should fill the string slot PERP : ORGANIZATIONID  . However , ? BOMB ? is itself not a base NP ( the base NP is ? A BOMBEX PLO-
SION ?). Similarly for ? FMLN?.
As such , a string that fills a template slot but is itself not a base NP  ( like ? BOMB ? ) is also used to generate a training example , by using its smallest encompassing noun phrase ( like ? A BOMBEX PLO- ( 1 ) ONE PERSON WAS KILLED TONIGH TAST HERE SUL TO FA BOMBEXPLOSIONINS ANSALVADOR  . 
(2) FORTUNATELY , NOCASUAL TIES WERERE POR TEDASARE SULTO FTHIS INCIDENT  , FOR WHICH THE FMLNGUER RILL A SAREBEING HELD RESPONSIBLE  . 
Figure 4: Sentences illustrating string slots that cannot be filled by base NPs  . 
(1 ) MEMBERS OF THATSECURITY GROUPARE COMB-ING THE AREA TO DETER MINE THE FINAL OUT COME 
OF THE FIGHTING .
(2) ABOMBWASTHROWNATTHEHOUSE OFFRE-
DEMOCANDIDATEFORDE PUTY MIGUEL ANGEL
BARTRABY TERRORISTS.
Figure 5: Sample sentences for the illustration of features SION ?  ) to generate the training example features . 
During training , a list of such words is compiled for slots 6 and 10 from the training templates . During testing , these words are also used as candidates for generating test examples for slots  6 and 10  , in addition to base NPs . 
The features of an example are derived from the treebank-style parse tree output by an implementation of Collins ' parser  ( Collins ,  1999) . In particular , we traverse the full parse tree to determine the verbs  , agents , patients , and indirect objects related to a noun phrase candidate     . While a machine learning approach is used in ( Gildea and Jurafsky , 2000) to determine general semantic roles , we used a simple rule-based traversal of the parse tree instead  , which could also reliably determine the generic agent and patient role of a sentence  , and this suffices for our current purpose . 
Specifically , for a given noun phrase candidate    , the following groups of features are used : Verb of Agent NP  ( VAg ) When   is an agent in a sentence , each of its associated verbs is a VA g feature . For example , in sentence (1) of Figure 5 , if   is MEMBERS , then its VAg features are COMB and DETERMINE . 
Verb of Patient NP ( VP a ) When   is a patient in a sentence , each of its associated verbs is a VP a feature . For example , in sentence (2) of Figure 5 , if   is BOMB , then its VP a feature is THROW . 
Verb-Preposition of NP-in-PP ( V-Prep ) When   is the NP in a prepositional phrase PP , then this feature is the main verb and the preposition of PP  . 
For example , in sentence (2) of Figure 5 , if   is
HOUSE , its V-Prep feature is THROW-AT.
VP a and related NPs/PPs ( VP a Rel ) If   is a patient in a sentence , each of its VP a may have its own agents ( Ag ) and prepositional phrases ( Prep-NP )  . In this case , the tuples ( VP a , Ag ) and ( VP a , Prep-NP ) are used as features . For example , in ? GUARDSWERESHOT TO DEATH ? , if   is GUARDS , then its VP a SHOOT , and the prepositional phrase TO-DEATH form the feature  ( SHOOT , 

VAg and related NPs/PPs ( V AgRel ) This is similar to VP a above , but for VA g . 
V-Prep and related NPs ( V-PrepRel ) When   is the NP in a prepositional phrase PP , then the main verb ( V ) may have its own agents ( Ag ) and patients ( Pa )  . In this case , the tuples ( Ag , V-Prep ) and ( V-Prep , Pa ) are used as features . For example , HOUSE in sentence ( 2 ) of Figure 5 will have the features ( TERRORIST , THROW-AT ) and ( THROW-
AT , BOMB).
Noun-Preposition ( N-Prep ) This feature aims at capturing information in phrases such as ? MUR-DEROFTHEPRIESTS ?  . If   is PRIESTS , this feature will be MURDER-OF . 
Head Word ( H ) The head word of each   is also used as a feature  . In a parse tree , there is a head word at each tree node . In cases where a phrase does not fit into a parse tree node  , the last word of the phrase is used as the head word  . This feature is useful as the system has no information of the semantic class of    . From the head word , the system can get some clue to help decide if     is a possible candidate for a slot  . For example , an   with head word PEASANT is more likely to fill the human target slot compared to another     with head word CLASH  . 
Named Entity Class ( NE ) The named entity class of   is used as a feature  . 
Real Head ( RH ) For a phrase that does not fit into a parse node , the head word feature is taken to be the last word of the phrase  . The real head word of its encompassing parse node is used as another feature  . 
For example , in the NP ? FMLNGUERRILLAS ? , ? FMLN ? is a positive example for slot 10 , with head word ? FMLN ? and real head ? GUERRILLA ?  . 
Coreference features Coreference chains found by our coreference resolution module based on decision tree learning are used to determine the noun phrases that corefer with    . In particular , we use the two noun phrases    and   , where    (      ) is the noun phrase that corefers with   and immediately precedes  ( follows )     . If such a preceding ( or following ) noun phrase   exists , we generate the following features based on   : VAg  , 
VP a , and N-Prep.
To give an idea of the informative features used in the classifier of a slot  , we rank the features used for a slot classifier according to their correlation metric values  ( Chieu and Ng , 2002a ) , where informative features are ranked higher . Table 1 shows the top-ranking features for a few feature groups and template slots  . The bracketed number behind each feature indicates the rank of this feature for that slot classifier  , ordered by the correlation metric value . 
We observed that certain feature groups are more useful for certain slots  . For example , DIE is the top V Ag verb for the human target slot  , and is ranked 12 among all features used for the human target slot . 
On the other hand , VA giss oun important for the physical target slot that the top V Ag verb is due to a preprocessing error that made MONSERRAT a verb  . 
3.3 Supervised Learning Algorithms
We evaluated four supervised learning algorithms.
Maximum Entropy Classifier ( Alice-ME )
The maximum entropy ( ME ) framework is a recent learning approach which has been successfully used in various NLP tasks such as sentence segmentation  , part-of-speech tagging , and parsing ( Ratnaparkhi ,  1998) . However , to our knowledge , ours is the first research effort to have applied ME learning to the fullscale ST task  . We used the implementation of maximum entropy modeling from the open nlp  . maxent package . 3 . 
Support Vector Machine ( Alice-SVM ) The
Support Vector Machine ( SVM ) ( Vapnik ,  1995 ) has been successfully used in many recent applications such as text categorization and handwritten digit recognition  . The learning algorithm finds a hyperplane that separates the training data with the largest margin  . We used a linear kernel for all our experiments . 

Naive Bayes ( Alice-NB ) The Naive Bayes ( NB ) algorithm ( Duda and Hart ,  1973 ) assumes the independence of features given the class and assigns a test example to the class which has the highest posterior probability  . Add-one smoothing was used . 
Decision Tree ( Alice-DT ) The decision tree ( DT ) algorithm ( Quinlan ,  1993 ) partitions training examples using the feature with the highest information gain  . It repeats this process recursively for each partition until all examples in each partition belong to one class  . 
We used the WEKA package4 for the implementation of SVM , NB , and DT algorithms . 
A feature cutoff  is used for each algorithm : features occurring less than  times are rejected  . For all experiments ,  is set to 3 . For ME and SVM , no other feature selection is applied . For NB and DT , the top 100 features as determined by chi-square are selected  . While not trying to do a serious comparison of machine learning algorithms  , ME and SVM seem to be able to perform well without feature selection  , whereas NB and DT require some form of feature selection in order to perform reasonably well  . 
3.4 Template Manager
As each sentence is processed , phrases classified as positive for any of the string slots are sent to the Template Manager  ( TM )  , which will decide if a new template should be created when it receives a new slot fill  . 
The system first attempts to attach a date and a location to each slot fill     . Dates and locations are first attached to their syntactically nearest verb  , by traversing the parse tree . Then , for each string fill   , we search its syntactically nearest verb  in the same manner and assign the date and location attached to  to     . 
When a new slot fill is found , the Template Manager will decide to start a new template if one of the following conditions is true : Date The date attached to the current slot fill is different from the date of the current template  . 
Location The location attached to the current slot fill is not compatible with the location of the current template  ( one location does not contain the other )  . 

Slot V Ag VP a V-Prep N-Prep
Human Target DIE ( 12 ) KILL ( 2 ) IDENTIFY-AS ( 47 ) MURDER-OF ( 3 ) Perpetrator Individual KIDNAP ( 5 ) IMPLICATE ( 17 ) ISSUE-FOR ( 73 ) WARRANT-FOR ( 64 ) Physical Target MONSERRAT ( 420 ) DESTROY ( 1 ) THROW-AT ( 32 ) ATTACK-ON ( 11 ) Perpetrator Organization KIDNAP ( 16 ) BLAME ( 25 ) SUSPEND-WITH ( 87 ) GUERRILLA-OF ( 31 ) Instrument IDEX PLODE ( 4 ) PLACE ( 5 ) EQUIP-WITH ( 31 ) EXPLOSION-OF ( 17 ) Table 1: The top-ranking feature for each group of features and the classifier of a slot 
Incident Type Seed Words
ATTACKJES UIT , MURDER , KILL , ATTACK
BOMBING BOMB , EXPLOS , DYNAMIT , EXPLOD , INJUR
KIDNAPPING KIDNAP , ELN , RELEAS
Table 2: Stemmed seed words for each incident type This is determined by using location lists provided by the  MUC4 conference , which specify whether one location is contained in another  . An entry in this list has the format of ? PLACE-NAME1:PLACE-NAME2?  , where PLACE-NAME2 is contained in PLACE-NAME 1 ( e . g . , CUBA:HAVANA(CITY )) . 
SeedWord The sentence of the current slot fill contains a seed word for a different incident type  . 
A number of seed words are automatically learned for each of the incident types ATTACK  , BOMBING , and KIDNAPPING . They are automatically derived based on the correlation metric value used in  ( Chieu and Ng , 2002a ) . For the remaining incident types , there are too few incidents in the training data for seed words to be collected  . The seeds words used are shown in Table 2 . 
3.5 Enriching Templates
In the last stage before output , the template content is further enriched in the following manner : Removal of redundant slot fills For each slot in the template  , there might be several slot fills referring to the same thing  . For example , for HUMTGT : DESCRIPTION , the system might have found both ? PRIESTS ? and ? JESUIT PRIESTS ?  . A slot fill that is a substring of another slot fill will be removed from the template  . 
Effect/Confidence and Type Classifiers are also trained for effect and confidence slots  11  ,  16 , and 23 ( ES slots ) , as well as type slots 7 ,  13 , and 20 ( TS slots ) . ES slots used exactly the same features as string slots  , while TS slots used only head words and adjectives as features  . For such slots , each entry refers to another slot fill . For example , slot 23 may contain the entry ? DEATH ?: ? PRIESTS ? , where ? PRIESTS ? fills slot 19 . During training , each training example is a fill of a reference slot  ( e . g . , for slot 23 , the reference slots are slot 18 and 19) . For slot 23 , for example , each instance will have a class such as DEATH or INJURY  , or if there is no entry in slot 23 , UNKNOWNEFFECT . During testing , slot fills of reference slots will be classified to determine if they should have an entry in an ES or a TS slot  . 
Date and Location . If the system is unable to fill the DATE or LOCATION slot of a template  , it will use as default value the date and country of the city in the dateline of the document  . 
Other Slots . The remaining slots are filled with default values  . For example , slot5 has the default value ? ACCOMPLISHED ? , and slot 8? TERROR-ISTACT ? ( except when the perpetrator contains strings such as ? GOVERNMENT ?  , in which case it will be changed to ? STATE -SPONSORED VIO-LENCE ?  )  . Slot 15, 22, and 24 are always left unfilled . 
4 Evaluation
There are 1 , 300 training documents , of which 700 are relevant ( i . e . , have one or more event templates ) . 
There are two official test sets , i . e . , TST3 and TST4, containing 100 documents each . We trained our system ALICE using the 700 documents with relevant templates , and then tested it on the two official test sets . The output templates were scored using the scorer provided on the official website  . 
The accuracy figures of ALICE ( with different learning algorithms ) on string slots and all slots are listed in Table  3 and Table 4  , respectively . Accuracy is measured in terms of recall ( R ) , precision ( P ) , and Fmeasure ( F ) . We also list in the two tables the accuracy figures of the top  7   ( out of a total of 17 ) systems that participated in MUC4 . The accuracy figures in the two tables are obtained by running the official scorer on the output templates of ALICE  , and those of the MUC4 participating systems ( available
TST3TST 4
R P F R P F
GE 555 454GE 605 457
GE-CMU 435247 GE-CMU 485250
Alice-ME 415 145 Alice-ME 4449 46
Alice-SVM 414 543 Alice-SVM 454 444
SRI 375 143 NY U4 245 43
UMASS 364942 SRI394943
Alice-DT 315 139 Alice-DT 36 50 42
NYU 354 339 UMASS 424 242
Alice-NB 413035 Alice-NB 513239
UMICH 323634 BBN 354238
BBN 224028 UMICH 323 433
Table 3: Accuracy of string slots on the TST3 and
TST4 test set
TST3TST 4
R P F R P F
GE 5854 56GE 6253 57
GE-CMU 4855 1GE-CMU 5353 53
UMASS 455650 SRI 445147
Alice-ME 465 148 Alice-ME 4646 46
SRI 435 448NYU 4646 46
Alice-SVM 454 645 UMASS 4745 46
Alice-DT 385 344 Alice-SVM 4740 43
NYU 404643 Alice-DT 414643
UMICH 403939 BBN 404341
Alice-NB 453 439 Alice-NB 523 340
BBN 294 335 UMICH 363 435
Table 4: Accuracy of all slots on the TST3 and TST4 test set on the official website )  . The same history file downloaded from the official website is uniformly used for scoring the output templates of all systems  ( the history file contains the arbitration decisions for ambiguous cases  )  . 
We conducted statistical significance test , using the approximate randomization method adopted in  MUC4  . Table 5 shows the systems that are not significantly different from Alice-ME  . 
Our system ALICE-ME , using a learning approach , is able to achieve accuracy competitive to the best of the  MUC4 participating systems , which were all built using manually engineered rules  . We also observed that ME and SVM , the more recent machine learning algorithms , performed better than
DT and NB .
Full Parsing . To illustrate the benefit of full parsing , we conducted experiments using a subset of features  , with and without full parsing . We used ME as the learning algorithm in these experiments  . The results on string slots are summarized in Table  6  . The Test set/slots Systems in the same group
TST3/string GE-CMU , SRI , UMASS , NY U
TST 4/string GE-CMU , Alice-SVM , NYU , SRI,
Alice-DT , UMASS
TST3/all GE-CMU , UMASS , SRI , NY U
TST4/allSRI , NYU , UMASS , Alice-SVM,
Alice-DT , BBN
Table 5: Systems whose Fmeasures are not significantly different from Alice-ME at the  0  . 10 significance level with 0 . 99 confidence
TST3TST 4
System R P F R P F
H+NE 234 43018 3023
H+NE+V ( w/o parsing ) 264232284033 H+NE+V ( with parsing )   38   49   43   40   45   42 Table 6: Accuracy of string slots with and without full parsing baseline system used only two features  , head word ( H ) and named entity class ( NE ) . Next , we added three features , VAg , VP a , and V-Prep . Without full parsing , these verbs were obtained based on the immediately preceding  ( or following ) verb of a noun phrase , and the voice of the verb . With full parsing , these verbs were obtained based on traversing the full parse tree  . The results indicate that verb features contribute to the performance of the system  , even without full parsing . With full parsing , verbs can be determined more accurately , leading to better overall performance . 
5 Discussion
Although the best MUC4 participating systems , GE/GE-CMU , still outperform ALICE-ME , it must be noted that for GE ,   ?10   1/2 person months ? were spent on MUC4 using the GENLTOOLSET , after spending ?15 person months ? on MUC3 ( Rau et al . , 1992) . With a learning approach , IE systems are more portable across domains . 
Not all occurrences of a string in a document that match a slot fill of a template provide good positive training examples  . For example , in the same document , there might be the following sentences ? THE
MNR REPORTS THEKIDN APPING OF OQUELI
COLINDRES ...?, followed by ? OQUELICOLIN-
DRES ARRIVE DING UATE MAL AON 11 JAN-
UARY ? . In this case , only the first occurrence of OQUELICOLIND RES should be used as a positive example for the human target slot  . However , ALICE does not have access to such information , since the MUC4 training documents are not annotated ( i . e . , only templates are provided , but the text strings in a document are not marked  )  . Thus , ALICE currently uses all occurrences of ? OQUELI COLINDRES ? as positive training examples  , which introduces noise in the training data . We believe that annotating the string occurrences in training documents will provide higher quality training data for the learning approach and hence further improve accuracy  . 
Although part-of-speech taggers often boast of accuracy over  95%  , the errors they make can be fatal to the parsing of sentences  . For example , they often tend to confuse ? VBN ? with ? VBD ? , which could change the entire parse tree . The MUC4 corpus was provided a suppercase text , and this also has a negative impact on the named entity recognizer and part-of-speech tagger  , which both make use of case information . 
Learning approaches have been shown to perform on paror even outperform knowledge-engineering approaches in many NLP tasks  . However , the full scale scenario template IE task was still dominated by knowledge-engineering approaches  . In this paper , we demonstrate that using both state-of-art learning algorithms and full parsing  , learning approaches can rival knowledge -engineering ones  , bringing us a step closer to building fullscale IE systems in a domain-independent fashion with state-of-the-art accuracy  . 

We thank Kian Ming Adam Chai for the implementation of the full parser  . 

M . E . Califf and R . J . Mooney .  1999 . Relational learning of pattern-match rules for information extraction  . In
Proceedings of AAAI99, pages 328?334.
E . Charniak , C . Hendrickson , N . Jacobson , and M . Perkowitz .  1993 . Equations for part-of-speech tagging . In Proceedings of AAAI93, pages 784?789 . 
H . L . Chieu and H . T . Ng . 2002a . A maximum entropy approach to information extraction from semistructured and freetext  . In Proceedings of AAAI02, pages 786?791 . 
H . L . Chieu and H . T . Ng . 2002b . Named entity recognition : A maximum entropy approach using global information  . In Proceedings of COLING02, pages 190?196 . 
F . Ciravegna .  2001 . Adaptive information extraction from text by rule induction and generalisation  . In Proceedings of IJCAI01, pages 1251?1256 . 
M . Collins .  1999 . Head-driven statistical models for natural language parsing  . Ph . D . thesis , Department of Computer and Information Science , University of

R . O . Duda and P . E . Hart .  1973 . Pattern Classification and Scene Analysis . Wiley , New York . 
D . Fisher , S . Soderland , J . McCarthy , F . Feng , and W . Lehnert .  1995 . Description of the UMass system as used for MUC6 . In Proceedings of MUC6, pages 127?140 . 
D . Gildea and D . Jurafsky .  2000 . Automatic labelling of semantic roles . In Proceedings of ACL00, pages 512?520 . 
S . Miller , M . Crystal , H . Fox , L . Ramshaw , R . Schwartz , R . Stone , R . Weischedel , and the Annotation Group . 
1998 . Algorithms that learn to extract information BBN : Description of the SIFT system as used for 
MUC7. In Proceedings of MUC7.
J . R . Quinlan .  1993 . C4 . 5: Programs for Machine Learning . Morgan Kaufmann , San Francisco . 
A . Ratnaparkhi .  1998 . Maximum Entropy Models for Natural Language Ambiguity Resolution  . Ph . D . thesis , Department of Computer and Information Science , 
University of Pennsylvania.
L . Rau , G . Krupka , and P . Jacobs .  1992 . GENL-TOOLSET : MUC4 test results and analysis . In Proceedings of MUC4, pages 94?99 . 
D . Roth and W . Yih .  2001 . Relational learning via propositional algorithms : An information extraction case study  . In Proceedings of IJCAI01, pages 1257?1263 . 
S . Soderland .  1999 . Learning information extraction rules for semistructured and freetext  . Machine
Learning , 34(1/2/3):233?272.
W . M . Soon , H . T . Ng , and D . C . Y . Lim .  2001 . A machine learning approach to coreference resolution of noun phrases  . Computational Linguistics , 27(4):521?544 . 
V . N . Vapnik .  1995 . The Nature of Statistical Learning
Theory . Springer-Verlag , New York.
