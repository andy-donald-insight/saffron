A Stochastic Language Model using Dependency
and Its Improvement by Word Clustering
Shinsuke Mori *
Tokyo Research Labolatory,
IBM Japan , Ltd.
1623-14 Shimotsuruma
Yamatoshi , Japan
Makoto Nagao
Kyoto University
Yoshida-honmachi Sakyo
Kyoto , Japan
Abstract
In this paper , we present a stochastic language model for Japanese using dependency  . The prediction unit in this model is all attribute of " bunsetsu "  . 
This is represented by the product of the head of content words and that of function words  . The relation between the attributes of " bunsetsu " is ruled by a contextfree grammar  . The word sequences axe predicted from the attribute using word ngram model  . 
The spell of Unknow word is predicted using character n-grain model  . This model is robust in that it can compute the probability of an arbitrary string and is complete in that it models from unknown word to dependency at the same time  . 
1 Introduction
An effectiveness of stochastic language modeling as a methodology of natural language processing has been attested by various applications to the recognition system such as speech recognition and to the analysi system such as paxt-of-speech  ( POS ) tagger . 
In this methodology a stochastic language model with some parameters i built and they axe estimated in order to maximize its prediction power  ( minimize the crossentropy ) on an unknown input . Considering a single application , it might be better to estimate the parameters taking account of expected accuracy of recognition or analysis  . This method is , however , heavily dependent on the problem and of_fers no systematic solution  , as fax as we know . The methodology of stochastic language modeling , however , allows us to separate , from various frameworks of natural anguage processing  , the language description model common to them and enables us a systematic improvement of each application  . 
In this framework a description on a language is represented as a map from a sequence of alphabetic characters to a probability value  . The first model is C . E . Shannon's ngram model ( Shannon , 1951) . 
The parameters of the model are estimated from the frequency of n character sequences of the alphabet  ( ngram ) on a corpus containing a large number of sentences of a language  . This is the same model as 0 This work is done when the auther was at Kyoto Univ  . 
used in almost all of the recent practic M applications in that it describes only relations between sequential elements  . Some linguistic phenomena , however , axe better described by assuming relations between sep-axated elements  . And modeling this kind of phenomena , the accuracies of various application axe generally augmented  . 
As for English , there have been researches in which a stochastic on text-free grammar  ( SCFG )   ( Fuj is a kiet ~1 . , 1989) is used for model description . Recently some researchers have pointed out the importance of the lexicon and proposed lexicalized models  ( Jelinek et al , 1994; Collins ,  1997) . In these models , every head word is propagated up through the derivation tree such that every parent receives a head word from the head child  . This kind of specialization may , however , be excessive if the criterion is predictive power of the model  . Research ~ medate stimating the best specialization level for  2gram model ( Morieta L ,  1997 ) shows a classbased model is more predictive than a word-based  2gram model , a completely lexicalized model , comparing crossentropy of a POS-based 2-graxa model , a word-based 2gram model and a classbased 2-graxa model , estimated from information theoretical point of view  . 
As for a parser based on a classbased SCFG , Chax-niak ( 1997 ) reports better accuracy than the above lexicalized models  , but the clustering method is not clear enough and  , in addition , there is no report on predictive power ( crossentropy or perplexity )  . 
Hogenhout and Matsumoto ( 1997 ) propose a word-clustering method based on syntactic behavior  , but no language model is discussed . As the experiments in the present paper at test , word class relation is dependent on language model . 
In this paper , taking Japanese as the object language , we propose two complete stochastic language models using dependency between bugsetsu  , a sequence of one or more content words followed by zero  , one or more function words , and evaluate their predictive power by cross entropy  . Since the number of sorts of bunsetsu is enormous  , considering it as a symbol to be predicted would surely invoke the data sparseness problem  . To cope with this problem we model ( Brown et al ,  1992) . Each bunsetsu is represented by the class calculated from the POS of its last content word and that of its last function word  . 
The relation between bunsetsu , called dependency , is described by a stochastic on text-free grammar ( Fu , 1974) on the classes . From the class of a bunsetsu , the content word sequence and the function word sequence are independently predicted by word ngram models equipped with unknown word models  ( Mori and Yamaji ,  1997) . 
The above model assumes that the syntactic behavior of each bunsetsu depends only on POS  . The POS system invented by grammarians may not always be the best in terms of stochastic language modeling  . This is experimentally attested by the paper ( Mori et al ,  1997 ) reporting comparisons between a POS-based ngram model and a classbased ngram model induced automatically  . S Venow propose , based on this report , a word-clustering method on the model we have mentioned above to successfully improve the predictive power  . In addition , we discuss a parsing method as an application of the model  . 
We also report the result of experiments conducted on EDR corpus  ( Jap ,  1993 ) The corpus is divided into ten parts and the models estimated from nine of them axe tested on the rest in terms of crossentropy  . As the result , the crossentropy of the POS-based dependency model is  5  . 3 536 bits axt d that of the class base dependency model estimated by our method is  4  . 9944 bits . This shows that the clustering method we propose improves the predictive power of the POS-based model notably  . Additionally , a parsing experiment proved that the parser based on the improved model has a higher accuracy than the POS-based one  . 
2 Stochastic Language Model based on Dependency In this section  , we propose a stochastic language model based on dependency  . Formally this model is based on a stochastic on text-free grammar  ( SCFG )  . 
The terminal symbol is the attribute of a bunsetsu  , represented by the product of the head of the content part and that of the function part  . From the attribute , a word sequence that matches the bun . 
setsu is predicted by a word-based 2gram model , and unknown words axe predicted from POS by a character-based  2gram model . 
2.1 Sentence Model
A Japanese sentence is considered as a sequence of units called bunsetsu composed of one or more content words and function words  . Let Cont be a set of content words , Funcaset of function words and Signaset of punctuation symbols  . Then bunsetsu is defined as follows : Bnst = Cont + Func*UCont+Func*Sign  , where the signs "+" and "*" mean positive closure and Kleene closure respectively  . Since the relations between bunsetsu known as dependency are not always between sequential ones  , we use SCFG to describe them ( Fu ,  1974) . The first problem is how to choose terminal symbols  . The simplest way is to select each bunsetsu as a terminal symbol  . In this case , however , the data sparseness problem would surely be invoked  , since the number of possible bunsetsu is enormous  . To avoid this problem we use the concept of class proposed for a word ngram model  ( Brown et al ,  1992) . All bunsetsu axe grouped by the attribute defined as follows : attrib  ( b )   ( 1 ) = qast ( co . t(b )), last(f .   . c(b )), Zast(sig . ( b ))) , where the functions cont , func and sign take a bun~etsu as their argument and return its content word sequence  , its function word sequence and its punctuation respectively  . In addition , the function last ( m ) returns the POS of the last element of word sequence mor NULL if the sequence has no word  . 
Given the attribute , the content word sequence and the function word sequence of the bunsetsu axe independently generated by word-based  2gram models ( Mori and Yamaji ,  1997) . 
2.2 Dependency Model
In order to describe the relation between bunsetsu called dependency  , we make the generally accepted assumption that no two dependency relations cross each other  , and we introduce a SCFG with the attribute of bunsetsu as terminals  . It is known , as a characteristic of the Japanese language , that each bunsetsu depends on the single bunsetsu appearing just before it  . We say of two sequential bunsetsu that the first to appear is the anterior and the second is the posterior  . We assume , in addition , that the dependency relation is a binary relation -that each relation is independent of the others  . Then this relation is representing by the following form of rewriting rule of CFG : B = ~ A B  , where A is the attribute of the anterior bunsetsu and B is that of the posterior  . 
Similarly to terminal symbols , nonterminal symbols can be defined as the attribute of bunsetsu  . Also they can be defined as the product of the attribute and some additional information to reflect the characteristics of the dependency  . It is reported that the dependency is more frequent between closer bunsetsu in terms of the position in the sentence  ( Maruyama and Ogino ,  1992) . In order to model these characteristics , we add to the attribute of bunsetsuan ( noun , NULL . comma , O , 0) kyou/noun . /sign ( today ) ( noun . postp . . NULL .  0 .  0)
Kyoto/noun daigaku/noun he/postp.
( Kyoto ) ( university ) ( to)

SCFG ( verb . ending , period .  0 . 0) " ~ j ngrami/verb ku/ending . /sign ( go ) Figure 1: Dependency model based on bunsetsu additional information field holding the number of bunsetsu depending on it  . Also the fact that a bun . 
setsu has a tendency to depend on a bunsetsu with comma  . For this reason the number of bunsetsu with comma depending on it is also added  . To avoid data sparseness problem we set an upper bound for these numbers  . Let d be the number of bunsetsu depending on it and v be the number of bunsetsu with comma depending on it  , the set of terminal symbols T and that of nonterminal symbols V is represented as follows  ( see Figure 1 ) :
T = attrib(b ) ?0 ?0
V = attrib(b ) ? 1, 2, "" dmazx 0, 1, "'' Vmaz.
It should be noted that terminal symbols have no bunsetsu depending on them  . It follows that all rewriting rules are in the following forms : 
S~(a , d , v ) (2) ( ~ , d  ~ , v , ) ~( a , , d  ~ , v ~) ~3 , d  ~ , ~) (3) a1 = a3 dl = min(ds+i , dm  ~ . ) min(vs+1,v,n ~ . ) vl = if sign ( a2 ) = comma v3 otherwise where a is the attribute of bunsetsu . 
The attribute sequence of a sentence is generated through applications of these rewriting rules to the start symbol S  . Each rewriting rule has a probability and the probability of the attribute sequence is the product of those of the rewriting rules used for its generation  . Taking the example of Figure 1 , this value is calculated as follows:
P (( noun , JLL , comma , 0, 0) ( noun , postp . , NULL ,  0 , 0) ( verb , ending , period ,  0 , 0)) = P ( S ~ ( verb , ending , per lod ,  2 , 0)) ? P (( verb , ending , period ,  2 , O ) = ~( noun , NULL , comma ,  0 , 0) ( verb , ending , period ,  1 , 0)) ? P (( verb , ending , period ,  1 , 0) = ~( noun , postp . , NULL , 0, 0) ( verb , ending , period , 0, 0)) . 
The probability value of each rewriting rule is estimated from its frequency N in a syntactically annotated corpus as follows: 
P ( S~(a  ~, all , vl))
N(S : : ~( al , dl , Va))

N((al , dl , vl ) = ~( a2, d2, v~)(a3, d3, v3))
N((.I , dl , vl))
In a word ngram model , in order to cope with data sparseness problem , the interpolation technique is applicable to SCFG  . The probability of the interpolated model of grammars  G1 and G2  , whose sented as follows : P ( A = ~ a ) = ~ IPI ( A = ~ c ~ )  + , ~ P2(A = ~ a ) 0 < ~ j < l(j = l , 2) and ~ , + ~2 = 1 (4) where AEV and aE(VUT )* . The coefficients are estimated by heldout method or deleted interpolation method  ( Jelinek et al ,  1991) . 
3 Word Clustering
The model we have mentioned above uses the POS given manually for the attribute of bunsetsu  . Changing it into some class may improve the predictive power of the model  . This change needs only a slight replacement in the model representing formula  ( 1 ) : the function last returns the class of the last word of a word sequence rn instead of the POS  . The problem we have to solve here is how to obtain such classes i  . e . word clustering . In this section , we propose an objective function and a search algorithm of the word clustering  . 
3.1 Objective Function
The aim of word clustering is to build a language model with less crossentropy without referring to the test corpus  . Similar reseaxch as been successful , aiming at an improvement of a word ngram model both in English and Japanese  ( Mori et al ,  1997) . So we have decided to extend this research to obtain an optimal word class relation  . The only difference from the previous research is the language model  . In this case , it is a SCFG instead of a ngram model . Therefore the objective function , called average crossentropy , is defined as follows : my = __1 ~ H(Li , Mi ) ,   ( 5 ) m i----1 where Li is the ith learning corpus and Mi is the language model estimated from the learning corpus excluding the ith learning corpus  . 
3.2 Algorithm
The solution space of the word clustering is the set of all possible word class relations  . The caxdinality of the set , however , is too enormous for the dependency model to calculate the average crossentropy for all word class relations and select the best one  . So we abandoned the best solution and adopted a greedy algorithm as shown in Figure  2  . 
4 Syntactic Analysis
Syntactic Analysis is defined as a function which receives a character sequence as an input  , divides it into a bunsetsu sequence and determines dependency relations among them  , where the concatenation of character sequences of all the bunsetsu must Let ml  , m2 ,   .   .   . , mnbe . b4 sorted in the descending order of frequency . 
cl := ml , m2, .   .   . , m , c = Cl for each i (1, 2, . --, n ) f(mi ) := cl for each i(1, 2, .   .   .   , n ) c := argmin cecuc ,   , ~- H(move(f , mi , c )) if (- H(move(f , mi , c )) < H(f )) then / := move(/ , ms , c ) update interpolation coeff?cients . 
if ( c = c , e ~) then
C := Cuc , , , , , , i f f i l iffi2   i=3   i=4 update interpolation coefficients c !"- . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
:"  . . . . . . . . . . . . . . . . . . . i : : : ~ . ::-:~ . , update interpolation coefficients update interpolation coefficient  . 5
Figure 2: The clustering algorithm.
be equal to the input . Generally there axe one or more solutions for any input  . A syntactic analyzer chooses the structure which seems the most similar to the human decision  . There are two kinds of analyzer : one is called a rule-based analyzer  , which is based on rules described according to the intuition of grarn marians  ; the other is called a corpus-based analyzer , because it is based on a large number of analyzed examples  . In this section , we describe a stochastic syntactic analyzer , which belongs to the second category . 
4 . 1 S tochast ic Syntact i c Ana lyzer A stochastic syntactic analyzer  , based on a stochastic language model including the concept of dependency  , calculates the syntactic tree ( see Figure 1 ) with the highest probability for a given input x according to the following formula : r h = argmax P  ( Tia~ ) 
U~(T ) = Z  #sentences  #bunsetsu  #word learning 174 , 524 1 , 610 , 832 4 , 251 , 085 test 19 , 397 178 , 415 471 , 1 89  #nonterminal cross language model + #terminal entropy 
POS-based model 57 65 . 3536 classbased model 10,752 4 . 9944 = argmaxP(TIx)P ( x)
W ( T ) = Z = argmaxP(~\]T)P ( T ) ('." Bayes ' formula)
W ( T ) = : v = arg(T ) ('." P ( xlT ) = 1),
W ( T ) = Z where to ( T ) represent she character sequence of the syntactic tree T  . P ( T ) in the last line is a stochastic language model including the concept of dependency  . We use , as such a model , the POS-based dependency model described in section  2 or the classbased dependency model described in section  3  . 
4.2 Solution Search Algorithm
The stochastic on text-free grammar used for syntactic analysis consists of rewriting rules  ( see formula ( 3 ) ) in Chom~ky normal form ( Hopcroft and Ullman ,  1979 ) except for the derivation from the start symbol ( formula ( 2 ) ) . It follows that a CKY method extended to SCFG , a dynamic-programming method , is applicable to calculate the best solution in O ( n3 ) time , where n is the number of input characters . It should be noted that it is necessary to multiply the probability of the derivation from the start symbol at the end of the process  . 
5 Evaluation
We constructed the POS-based dependency model and the classbase dependency model to evaluate their predictive power  . In addition , we implemented parsers based on them which calculate the best syntactic tree from a given sequence of bun~etsu to observe their accuracy  . In this section , we present he experimental results and discuss them  . 
5.1 Conditions on the Experiments
As a syntactically annotated corpus we used EDR corpus  ( Jap ,  1993) . The corpus was divided into ten parts and the models estimated from nine of them were tested on the rest in terms of crossentropy  ( see Table 1 )  . The number of characters in the Japanese writing system is set to  6  , 879 . Two parameters which have not been determined yet in the explanation of the models  ( dmaz and v , naz ) axe both set to 1 . Although the best value for each of them can also be estimated using the average crossentropy  , they are fixed through the experiments . 
5 . 2 Eva luat ion o f Pred ic t ive Power For the purpose of evaluating the predictive power of the models  , we calculated their crossentropy on the test corpus  . In this process the annotated tree is used as the structure of the sentences in the test corpus  . Therefore the probability of each sentence in the test corpus is not the summation over all its possible derivations  . In order to compare the POS-based dependency model and the classbase depen-dency model  , we constructed these models from the same learning corpus and calculated their cross entropy on the same test corpus  . They are both interpolated with the SCFG with uniform distribution  . 
The processes for their construction areas follows : ? POS-base dependency model  1  . estimate the interpolation coefficients in Formula  ( 4 ) by the deleted interpolation method 2 . count the frequency of each rewriting rule on the whole learning corpus ? class base dependency model  1  . estimate the interpolation coefficients in Formula  ( 4 ) by the deleted interpolation method 2 . calculate an optimal word class relation by the method proposed in Section  3  . 
3 . count the frequency of each rewriting rule on the whole learning corpus The word-based  2gram model for bunsetsu generation and the character -based  2gram model as an unknown word model ( Mori and Yamaji ,  1997 ) are common to the POS-based model and classbased model  . Their contribution to the crossentropy is constant on the condition that the dependency models contain the prediction of the last word of the content word sequence and that of the function word sequence  . 
Table 2 shows the crossentropy of each model on the test corpus  . The crossentropy of the classbased dependency model is lower than that of the POS-based dependency model  . This result attests experimentally that the classbased model estimated by our clustering method is more predictive than the POS-based model and that our word clustering language model crossentropy accuracy 
POS-based model 5 . 3536 68 . 77% classbased model 4 . 9944 81 . 96% select always 53 . 1 0% the next bunsetsu method is efficient at improvement of a dependency model  . 
We also calculated the crossentropy of the classbased model which we estimated with a word  2gram model as the model M in the Formula ( 5 )  . The number of terminals and nonterminals i1 , 148 , 916 and the crossentropy is 6 . 3358 , which is much higher than that of the POS-base model  . This result indicates that the best word class relation for the dependency model is quite different from the best word class relation for the ngram model  . Comparing the number of the terminals and nonterminals  , the best word class relation for ngram model is exceedingly specialized for a dependency model  . We can conclude that word class relation depends on the language model  . 
5 . 3 Eva luat ion of Syntact i c Ana lys i s SVe implemented a parser based on the dependency models  . Since our models , equipped with a word-based 2-graan model for bunsetsu generation and the character -based  2gram as an unknown word model , can return the probability for a my input , we can build a parser , based on our model , receiving a character sequence as input . Its evaluation is not easy , however , becaus errors may occur in bunsetsu generation or in POS estimation of unknown words  . For this reason , in the following description , we assume a bunsetsu sequence as the input . 
The criterion we adopted is the accuracy of dependency relation  , but the last bunsetsu , which has no bunsetsu to depend on , and the second-to-last bunsetsu , which depends always on the last bunsetsu , are excluded from consideration . 
Table 3 shows crossentropy and parsing accuracy of the POS-based dependency model and the classbased dependency model  . This result tells us our word clustering method increases parsing accuracy considerably  . This is quite natural in the light of the decrease of crossentropy  . 
The relation between the learning corpus size and crossentropy or parsing accuracy is shown in Figure  3  . The lower bound of crossentropy is the entropy of Japanese  , which is estimated to be 4 . 3033 bit ( Mori and Yamaji , 1997) . Taking this fact into consideration , the crossentropy of both of the models has stronger tendency to decrease  . As for a c-01'0 , pOS-bm~l~M~/m~ldm=-Imset laep*t ~' ymad 100%   8O  = . 
so 101   102   10  ~  104   105   106   107  #characters in learning corpus Figure 3: Relation between crossentropy and parsing accuracy  . 
curacy , there also is a tendency to get more accurate as the learning corpus size increases  , but it is a strong tendency for the classbased model than for the POS-based model  . It follows that the classbased model profits more greatly from an increase of the learning corpus size  . 
6 Conclusion
In this paper we have presente dependency models for Japanese based on the attribute of bunsetsu  . 
They are the first fully stochastic dependency models for Japanese which describes from character sequence to syntactic tree  . Next we have proposed a word clustering method , an extension of deleted interpolation technique , which has been proven to be efficient in terms of improvement of the predictive power  . Finally we have discussed parsers based on our model which demonstrated a remarkable improvement in parsing accuracy by our word-clustering method  . 
References
Peter F . Brown , Vincent J . Della Pietra , Peter V . 
deSouza , Jennifer C . Lal , and Robert L . Mercer . 
1992 . Class-based ngram models of natural language . Computational Linguistics , 18(4):467-479 . 
Eugene Charniak .  1997 . Statistical parsing with a contextfree grammar and word statistics  . In Proceedings of the l~th National Conference on Artificial Intelligence  , pages 598-603 . 
Michael Collins .  1997 . Three generative , lexicalised models for statistical parsing . In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics  , pages 1623 . 
King Sun Fu .  1974 . Syntactic Methods in Pattern Recognition , volume 12 of Mathematics in Science and Engineering . Accademic Press . 

T . Fujisaki , F . Jelinek , J . Cocke , E . Black , and T . Nishino .  1989 . A probabilistic parsing method for sentence disambiguation  . I Proceedings of the
International Parsing Workshop.
Wide R . t to genhout and Yuji Matsumoto .  1997 . A preliminary study of word clustering based on syntactic behavior  . In Proceedings of the Computational Natural Language Learning  , pages 16-24 . 
John E . ttop croft and Jeffrey D . UU man .  1979 . Introduction to Automata Theory , Languages and Computation . Addison-~, Vesley Publishing . 
Japan Electronic Dictionary Research Institute , Ltd . , 1993 . EDR Electronic Dictionary Technical

Fredelick Jelinek , Robert L . Mercer , and Salim Roukos .  1991 . Principles of lexical language modeling for speech recognition  . In Advances in Speech Signal Processing , chapter 21 , pages 651-699 . Dekker . 
F . Jelinek , J . Lafferty , D . Magerman , R . Mercer , A . Rantnaparkhi , and S . Roukos .  1994 . Decision tree parsing using a hidden derivation model  . In Proceedings of the ARPA Workshop on Human
Language Technology , pages 256-261.
ttiroshi Maruyam and Shiho Ogino .  1992 . A statistical property of japanese phrase-to -phrase modifications  . Mathematical Linguistics , 18(7):348-352 . 
Shinsuke Mort and Osamu Yamaji .  1997 . An estimate of an upper bound for the entropy of japanese  . Transactions of Information Processing Society of Japan  ,  38(11):2191-2199 . ( In

Shinsuke Mort , Masafumi Nishimura , and Nobuyuki Ito .  1997 . l , Vord clustering for classbased language models . Transactions of Information Processing Society of Japan  ,  38(11):2200-2208 . ( In

C . E . Shannon .  1951 . Prediction and entropy of printed english . Bell System Technical Journal , 30:50-64 . 

