Convolution Kernels with Feature Selection
for Natural Language Processing Tasks
Jun Suzuki , Hideki Isozaki and Eisaku Maeda
NTT Communication Science Laboratories , NTT Corp.
24 Hikaridai , Seikacho , Sorakugun , Kyoto , 619-0237 Japan
jun , isozaki , maeda@cslab.kecl.ntt.co.jp

Convolution kernels , such as sequence and tree kernels , are advantageous for both the concept and accuracy of many natural language processing  ( NLP ) tasks . Experiments have , however , shown that the overfitting problem often arises when these kernels are used in NLP tasks  . This paper discusses this issue of convolution kernels  , and then proposes a new approach based on statistical feature selection that avoids this issue  . To enable the proposed method to be executed efficiently  , it is embedded into an original kernel calculation process by using substructure mining algorithms  . Experiments are undertaken on real NLP tasks to confirm the problem with a conventional method and to compare its performance with that of the proposed method  . 
1 Introduction
Over the past few years , many machine learning methods have been successfully applied to tasks in natural language processing  ( NLP )  . Especially , state-of-the-art performance can be achieved with kernel methods  , such as Support Vector Machine ( Cortes and Vapnik ,  1995) . Examples include text categorization ( Joachims ,  1998) , chunking ( Kudo and Matsumoto , 2002) and parsing ( Collins and Duffy ,  2001) . 
Another feature of this kernel methodology is that it not only provides high accuracy but also allows us to design a kernel function suited to modeling the task at hand  . Since natural language data take the form of sequences of words  , and are generally analyzed using discrete structures  , such as trees ( parsed trees ) and graphs ( relational graphs )  , discrete kernels , such as sequence kernels ( Lodhi et al ,  2002) , tree kernels ( Collins and Duffy ,  2001) , and graph kernels ( Suzuki et al , 2003a ) , have been shown to offer excellent results . 
These discrete kernels are related to convolution kernels  ( Haussler ,  1999) , which provides the concept of kernels over discrete structures  . Convolution kernels allow us to treat structural features without explicitly representing the feature vectors from the input object  . That is , convolution kernels are well suited to NLP tasks in terms of both accuracy and concept  . 
Unfortunately , experiments have shown that in some cases there is a critical issue with convolution kernels  , especially in NLP tasks ( Collins and Duffy , 2001; Cancedda et al , 2003; Suzuki et al , 2003b ) . 
That is , the overfitting problem arises if large ? substructures ? are used in the kernel calculations  . As a result , the machine learning approach can never be trained efficiently  . 
To solve this issue , we generally eliminate large substructures from the set of features used  . However , the main reason for using convolution kernels is that we aim to use structural features easily and efficiently  . If use is limited to only very small structures , it negates the advantages of using convolution kernels  . 
This paper discusses this issue of convolution kernels  , and proposes a new method based on statistical feature selection  . The proposed method deals only with those features that are statistically significant for kernel calculation  , large significant substructures can be used without overfitting  . Moreover , the proposed method can be executed efficiently by embedding it in an original kernel calculation process by using substructure mining algorithms  . 
In the next section , we provide a brief overview of convolution kernels  . Section 3 discusses one issue of convolution kernels , the main topic of this paper , and introduces some conventional methods for solving this issue  . In Section 4 , we propose a new approach based on statistical feature selection to offset the issue of convolution kernels using an example consisting of sequence kernels  . In Section 5 , we briefly discuss the application of the proposed method to other convolution kernels  . In Section 6 , we compare the performance of conventional methods with that of the proposed method by using real NLP tasks : question classification and sentence modality identification  . The experimental results described in Section 7 clarify the advantages of the proposed method . 
2 Convolution Kernels
Convolution kernels have been proposed as a concept of kernels for discrete structures  , such as sequences , trees and graphs . This framework defines the kernel function between input objects as the convolution of ? sub -kernels ?  , i . e . the kernels for the decompositions ( parts ) of the objects . 
Let X and Y be discrete objects . Conceptually , convolution kernels K(X , Y ) enumerate all substructures occurring in X and Y and then calculate their inner product  , which is simply written as:
K(X,Y ) = ??( X ), ?( Y ) ? = ? i ? i(X ) ? ? i(Y) .   ( 1 ) ? represents the feature mapping from the discrete object to the feature space  ; that is , ?(X ) = (?1(X ) ,   .   .   . , ? i(X ), .   .   . ) . With sequence kernels ( Lodhi et al ,  2002) , input objects X and Y are sequences , and ? i(X ) is a subsequence . With tree kernels ( Collins and Duffy ,  2001) , X and Y are trees , and ? i(X ) is a subtree . 
When implemented , these kernels can be efficiently calculated in quadratic time by using dynamic programming  ( DP )  . 
Finally , since the size of the input objects is not constant  , the kernel value is normalized using the following equation  . 
K?(X,Y ) = K(X,Y ) ?
K(X,X ) ? K(Y,Y ) (2)
The value of K?(X , Y ) is from 0 to 1 , K ?( X , Y ) = 1 if and only if X = Y . 
2.1 Sequence Kernels
To simplify the discussion , we restrict ourselves hereafter to sequence kernels  . Other convolution kernels are briefly addressed in Section  5  . 
Many kinds of sequence kernels have been proposed for a variety of different tasks  . This paper basically follows the framework of word sequence kernels  ( Cancedda et al ,  2003) , and so processes gapped word sequences to yield the kernel value  . 
Let ? be a set of finite symbols , and ? n  be a set of possible ( symbol ) sequences whose sizes are nor less that are constructed by symbols in ?  . The meaning of ? size ? in this paper is the number of symbols in the substructure  . Namely , in the case of sequence , size n means length n . S and T can represent any sequence . si and tj represent the ith and jth symbols in S and T  , respectively . Therefore , a

T ? ? a , b , c , aa , ab , ac , ba , bc , aba , aac , abc , bac , abacabcS = abacT = prod . 
1000211013 ? ?+0 ?00 ?0(a , b , c , ab , ac , bc , abc)(a , b , c , aa , ab , ac , ba , bc , aba , aac , abc , bac , abac ) u 35   3? ? ++ kernel value ? sequences su b sequences sequence S can be written as S =   s1   .   .   . si .   .   . sS , where S represents the length of S . If sequence u is contained in subsequence S[i:j ] def = si  .   .   . sj of S(allowing the existence of gaps ) , the position of u in S is written as i = ( i 1: i u )  . The length of S[i]isl(i ) = iu ? i1 + 1 . For example , if u = ab and S = cacbd , then i = (2:4) and l(i ) = 4 ? 2 + 1 = 3 . 
By using the above notations , sequence kernels can be defined as:
KSK(S , T ) = ? u??n ? i u=S[i]?? ( i ) ? ju = T [ j ] ? ? ( j )  ,   ( 3 ) where ? is the decay factor that handles the gap present in a common subsequence u  , and ?( i ) = l(i ) ? u . In this paper , means ? such that ? . Figure 1 shows a simple example of the output of this kernel  . 
However , in general , the number of features ? n , which is the dimension of the feature space , becomes very high , and it is computationally infeasible to calculate Equation  ( 3 ) explicitly . The efficient recursive calculation has been introduced in  ( Cancedda et al ,  2003) . To clarify the discussion , we redefine the sequence kernels with our notation  . 
The sequence kernel can be written as follows :
KSK(S,T ) = n ? m=1 ?1 ? i?S?1 ? j?T
Jm(Si , Tj) .   ( 4 ) where Si and Tj represent the subsequences Si = s1  , s2 ,   .   .   . , si and Tj = t1, t2, .   .   . , tj , respectively . 
Let Jm(Si , Tj ) be a function that returns the value of common subsequences if si = tj  . 
Jm(Si , Tj ) = J ? m?1(Si , Tj ) ? I(si , tj ) (5) I(si , tj ) is a function that returns a matching value between si and tj  . This paper defines I(si , tj ) as an indicator function that returns 1 if si = tj , otherwise 0 . 
Then , J ? m(Si , Tj ) and J??m(Si , Tj ) are introduced to calculate the common gapped subsequences between Si and Tj  . 
J ? m(Si , Tj ) = ? ? ? ? ? 1 if m = 0 , 0 if j = 0 and m > 0 , ? J?m(Si , Tj?1) + J ? ? m(Si , Tj?1) otherwise (6)
J ? ? m(Si , Tj ) = ? ? ? 0 if i = 0 , ? J??m(Si?1 , Tj ) + Jm(Si?1 , Tj ) otherwise ( 7 ) If we calculate Equations ( 5 ) to ( 7 ) recursively , Equation (4) provides exactly the same value as
Equation (3).
3 Problem of Applying Convolution
Kernels to NLP tasks
This section discusses an issue that arises when applying convolution kernels to NLP tasks  . 
According to the original definition of convolution kernels  , all the substructures are enumerated and calculated for the kernels  . The number of substructures in the input object usually becomes exponential against input object size  . As a result , all kernel values K ?( X , Y ) are nearly 0 except the kernel value of the object itself , K ?( X , X ) , which is 1 . 
In this situation , the machine learning process becomes almost the same as memory-based learning  . 
This means that we obtain a result that is very precise but with very low recall  . 
To avoid this , most conventional methods use an approach that involves smoothing the kernel values or eliminating features based on the substructure size  . 
For sequence kernels , ( Cancedda et al ,  2003 ) use a feature elimination method based on the size of subsequence n  . This means that the kernel calculation deals only with those subsequences whose size is nor less  . For tree kernels , ( Collins and Duffy ,  2001 ) proposed a method that restricts the features based on subtrees depth  . These methods seem to work well on the surface , however , good results are achieved only when n is very small  , i . e . n = 2 . 
The main reason for using convolution kernels is that they allow us to employ structural features simply and efficiently  . When only small sized substructures are used ( i . e . n = 2) , the full benefits of convolution kernels are missed  . 
Moreover , these results do not mean that larger sized substructures are not useful  . In some cases we already know that larger substructures are significant features as regards solving the target problem  . That is , these significant larger substructures , Table 1: Contingency table and notation for the chi -squared value cc ? ? rowuOuc = yOuc ? Ou = x u ? Ou ? cOu ? c ? Ou ? ? column Oc = MOc ? N which the conventional methods cannot deal with efficiently  , should have a possibility of improving the performance furthermore  . 
The aim of the work described in this paper is to be able to use any significant substructure efficiently  , regardless of its size , to solve NLP tasks . 
4 Proposed Feature Selection Method
Our approach is based on statistical feature selection in contrast to the conventional methods  , which use substructure size . 
For a better understanding , consider the two-class ( positive and negative ) supervised classification problem . In our approach we test the statistical deviation of all the substructures in the training samples between the appearance of positive samples and negative samples  . This allows us to select only the statistically significant substructures when calculating the kernel value  . 
Our approach , which uses a statistical metric to select features  , is quite natural . We note , however , that kernels are calculated using the DP algorithm  . 
Therefore , it is not clear how to calculate kernels efficiently with a statistical feature selection method  . 
First , we briefly explain a statistical metric , the chi-squared (?2) value , and provide an idea of how to select significant features  . We then describe a method for embedding statistical feature selection into kernel calculation  . 
4 . 1 Statistical Metric : Chi-squared Value There are many kinds of statistical metrics  , such as chi-squared value , correlation coefficient and mutual information . ( Rogati and Yang ,  2002 ) reported that chi-squared feature selection is the most effective method for text classification  . Following this information , we use ?2 values as statistical feature selection criteria . Although we selected ?2 values , any other statistical metric can be used as long as it is based on the contingency table shown in Table  1  . 
We briefly explain how to calculate the ?2 value by referring to Table 1  . In the table , c and c ? represent the names of classes , c for the positive class

T ? ? a , b , c , aa , ab , ac , ba , bc , aba , aac , abc , bac , abacabcS = abacT = prod . 
10 0 0 2 1 1 0 1 3? ?+ 0 ? 0 0 ? 0 1 . 0? = threshold 2 . 511 ?( a , b , c , ab , ac , bc , abc)(a , b , c , aa , ab , ac , ba , bc , aba , aac , abc , bac , abac ) u 35   3?  ?+ +  2  ?+  0   0   0   0   2   1   1   0   1   3?  ?+  0  ?  0   0  ?  0 kernel value kernel value unde r the feature selection featur eselection ? sequences subseq uences Figure  2: Example of statistical feature selection and c ? for the negative class  . Ouc , Ouc ? , Ou ? c and Ou ? c ? represent the number of u that appeared in the positive sample c  , the number of u that appeared in the negative sample c ?  , the number of u that did not appear in c , and the number of u that did not appear in c ? , respectively . Let y be the number of samples of positive class c that contain subsequence u  , and x be the number of samples that contain u . Let N be the total number of ( training ) samples , and M be the number of positive samples . 
Since N and M are constant for ( fixed ) data , ?2 can be written as a function of x and y , ?2(x , y ) = N ( Ouc ? Ou ? c ? ? Ou ? c ? Ouc ?) .   ( 8 )   ?2 expresses the normalized deviation of the observation from the expectation  . 
We simply represent ?2(x , y ) as ?2(u).
4.2 Feature Selection Criterion
The basic idea of feature selection is quite natural  . 
First , we decide the threshold ? of the ?2 value . If ?2(u ) < ? holds , that is , u is not statistically significant , then u is eliminated from the features and the value of u is presumed to be  0 for the kernel value . 
The sequence kernel with feature selection ( FSSK ) can be defined as follows:
KFSSK(S , T )  = ? ???2 ( u ) u ? ? n ? i u = S[i]?? ( i ) ? ju = T [ j ] ? ? ( j )  .   ( 9 ) The difference between Equations ( 3 ) and ( 9 ) is simply the condition of the first summation . FSSK selects significant subsequence u by using the condition of the statistical metric ??  ?2  ( u )  . 
Figure 2 shows a simple example of what FSSK calculates for the kernel value  . 
4.3 Efficient ?2(u ) Calculation Method
It is computationally infeasible to calculate ?2 ( u ) for all possible u with a naive exhaustive method . 
In our approach , we use a substructure mining algorithm to calculate  ?2  ( u )  . The basic idea comes from a sequential pattern mining technique  , Prefix S-pan ( Pei et al ,  2001) , and a statistical metric pruning ( SMP ) method , Apriori SMP ( Morishita and Sese ,  2000) . By using these techniques , all the significant subsequences u that satisfy ?  ?   ?2  ( u ) can be found efficiently by depth-first search and pruning  . Below , we briefly explain the concept involved in finding the significant features  . 
First , we denote uv , which is the concatenation of sequences u and v . Then , u is a specific sequence and uv is any sequence that is constructed by u with any suffix v  . The upper bound of the ?2 value of uv can be defined by the value of u ( Morishita and
Sese , 2000).
?2(uv ) ? max (?2 ( yu , yu ) , ?2(xu ? yu ,  0 )   )  =??2 ( u ) where x u and yu represent the value of x and y of u  . This in equation indicates that if ??2 ( u ) is less than a certain threshold ? , all subsequence suv can be eliminated from the features  , because no subsequence uv can be a feature . 
The PrefixSpan algorithm enumerates all the significant subsequences by using a depth-first search and constructing a TRIE structure to store the significant sequences of internal results efficiently  . 
Specifically , PrefixSpan algorithm evaluate suw , where uw represents a concatenation of a sequence u and a symbol w  , using the following three conditions . 
1 . ???2 ( uw ) 2 . ?>?2 ( uw ), ?>??2 ( uw ) 3 . ?>?2 ( uw ) , ????2 ( uw ) With 1 , subsequence u w is selected as a significant feature  . With 2 , subsequence uw and arbitrary subsequence suwv , are less than the threshold ? . Then w is pruned from the TRIE , that is , alluwv where v represents any suffix pruned from the search space  . 
With 3 , uw is not selected as a significant feature because the  ?2 value of u w is less than ? , however , uwv can be a significant feature because the upper bound  ?2 value of uwv is greater than ? , thus the search is continued touw v . 
Figure 3 shows a simple example of Prefix Span with SMP that searches for the significant features abc c d b c a b a c a c d a b d a b d a b c c d b c b a c a c d a b d ? a b c d b c  1  . 0? = b:c:d:+11+1-1-1au=w = (   ) 2 uw ? (   ) 2 ? uw ?
TRIE representation x y + 11+   11+   1 a b u = d c ? w + 11+   1-1-1 class training data suffix c : d : w = x y11   10   5  . 00 . 0 5 . 00 . 8 5 . 00 . 8 2  . 22  . 2 1  . 90 . 1 1  . 91 . 9 0 . 80 . 85 . 02  . 2a:b:c:d:+11+1-1-1u=?w=xy 5442 d1 . 91  . 9 0 . 80 . 8 ? a b c cd b c ab a ca cd a b d su f f i x su f f i xa b c cd b cb a ca cd a b  d5N  =  2M = pruned pruned Figure 3: Efficient search for statistically significant subsequences using the PrefixSpan algorithm with 
SMP by using a depth-first search with a TRIE representation of the significant sequences  . The values of each symbol represent ?2 ( u ) and ??2 ( u ) that can be calculated from the number of xu and yu  . The TRIE structure in the figure represents the statistically significant subsequences that can be shown in a path from ? to the symbol  . 
We exploit this TRIE structure and PrefixSpan pruning method in our kernel calculation  . 
4.4 Embedding Feature Selection in Kernel

This section shows how to integrate statistical feature selection in the kernel calculation  . Our proposed method is defined in the following equations  . 
KFSSK(S,T ) = n ? m=1 ?1 ? i?S?1 ? j?T
Km(Si , Tj ) (10)
Let Km(Si , Tj ) be a function that returns the sum value of all statistically significant common subsequences u if si = tj  . 
Km(Si , Tj ) = ? u??m(Si,Tj)
Ju(Si , Tj ) , (11) where ? m(Si , Tj ) represents a set of subsequences whose size u is m and that satisfy the above condition  1  . The ? m(Si , Tj ) is defined in detail in Equation (15) . 
Then , let Ju(Si , Tj ) , J?u(Si , Tj ) and J??u(Si , Tj ) be functions that calculate the value of the common subsequences between Si and Tj recursively  , as well as equations ( 5 ) to ( 7 ) for sequence kernels . We introduce a special symbol ? to represent an ? empty sequence ?  , and define ? w = w and ? w = 1 . 
Juw(Si , Tj ) = ? ? ?
J?u(Si , Tj ) ? I(w ) if uw ? ? ? uw(Si , Tj ) , 0 otherwise ( 12 ) where I ( w ) is a function that returns a matching value of w . In this paper , we define I(w ) is 1 . 
? ? m(Si , Tj ) has realized conditions 2 and 3 ; the details are defined in Equation (16) . 
J?u(Si , Tj ) = ? ? ? ? ? 1 if u = ? , 0 if j = 0 and u 6= ? , ? J?u(Si , Tj?1) + J??u(Si , Tj?1) otherwise (13)
J ? ? u(Si , Tj ) = ? ? ? 0 if i = 0 , ? J??u(Si?1 , Tj ) + Ju(Si?1 , Tj ) otherwise ( 14 ) The following five equations are introduced to select a set of significant subsequences  . ? m(Si , Tj ) and ? ? m(Si , Tj ) are sets of subsequences ( features ) that satisfy condition 1 and 3 , respectively , when calculating the value between Si and Tj in Equations  ( 11 ) and ( 12 )  . 
? m(Si , Tj ) = uu ? ? ? m(Si , Tj ) , ???2(u ) (15) ? ? m(Si , Tj ) = ? ? ? ?( ? ? ? m?1(Si , Tj ) , si ) if si = tj ? otherwise (16) ?( F , w ) = u w u ? F , ????2 ( uw ) , (17) where F represents a set of subsequences . Notice that ? m(Si , Tj ) and ? ? m(Si , Tj ) have only subsequence su that satisfy ???2 ( uw ) or ????2 ( uw )  , respectively , if si = tj (= w ) ; otherwise they become empty sets . 
The following two equations are introduced for recursive set operations to calculate ? m  ( Si , Tj ) and ? ? m(Si , Tj ) . 
??? m(Si , Tj ) = ? ? ? ? ? ? ? ? if m = 0 , ? if j = 0 and m > 0 , ??? m(Si , Tj?1) ? ? ? ? ? m(Si , Tj?1) otherwise (18) ? ? ? ? m(Si , Tj ) = ? ? ? ? if i = 0 , ??? ? m(Si?1 , Tj ) ? ? ? m(Si?1 , Tj ) otherwise (19) In the implementation , Equations ( 11 ) to ( 14 ) can be performed in the same way as those used to calculate the original sequence kernels  , if the feature selection condition of Equations ( 15 ) to ( 19 ) has been removed . Then , Equations (15) to (19) , which select significant features , are performed by the Pre-fixSpan algorithm described above and the TRIE representation of statistically significant features  . 
The recursive calculation of Equations ( 12 ) to ( 14 ) and Equations ( 16 ) to ( 19 ) can be executed in the same way and at the same time in parallel  . As a result , statistical feature selection can be embedded in oroginal sequence kernel calculation based on a dynamic programming technique  . 
4.5 Properties
The proposed method has several important advantages over the conventional methods  . 
First , the feature selection criterion is based on a statistical measure  , so statistically significant features are automatically selected  . 
Second , according to Equations (10) to (18) , the proposed method can be embedded in an original kernel calculation process  , which allows us to use the same calculation procedure as the conventional methods  . The only difference between the original sequence kernels and the proposed method is that the latter calculates a statistical metric  ?2  ( u ) by using a substructure mining algorithm in the kernel calculation  . 
Third , although the kernel calculation , which unifies our proposed method , requires a longer training time because of the feature selection  , the selected subsequences have a TRIE data structure  . 
This means a fast calculation technique proposed in  ( Kudo and Matsumoto , 2003) can be simply applied to our method , which yields classification very quickly . In the classification part , the features ( subsequences ) selected in the learning part must be known . Therefore , we store the TRIE of selected subsequences and use them during classification  . 
5 Proposed Method Applied to Other
Convolution Kernels
We have insufficient space to discuss this subject in detail in relation to other convolution kernels  . However , our proposals can be easily applied to tree kernels  ( Collins and Duffy , 2001) by using string encoding for trees . We enumerate nodes ( labels ) of tree in postorder traversal . After that , we can employ a sequential pattern mining technique to select statistically significant subtrees  . This is because we can convert to the original subtree form from the string encoding representation  . 
Table 2: Parameter values of proposed kernels and
Support Vector Machines parameter values of t margin for SVM  ( C ) 1000 decay factor of gap ( ? )  0 . 5 threshold of ?2(?) 2 . 70553 . 8415 As a result , we can calculate tree kernels with statistical feature selection by using the original tree kernel calculation with the sequential pattern mining technique introduced in this paper  . Moreover , we can expand our proposals to hierarchically structured graph kernels  ( Suzuki et al , 2003a ) by using a simple extension to cover hierarchical structures  . 
6 Experiments
We evaluated the performance of the proposed method in actual NLP tasks  , namely English question classification ( EQC ) , Japanese question classification ( JQC ) and sentence modality identification ( MI ) tasks . 
We compared the proposed method ( FSSK ) with a conventional method ( SK )  , as discussed in Section 3 , and with bag-of-words ( BOW ) Kernel ( BOW-K ) ( Joachims , 1998) as baseline methods . 
Support Vector Machine ( SVM ) was selected as the kernelbased classifier for training and classification  . Table 2 shows some of the parameter values that we used in the comparison  . We set thresholds of ? = 2 . 7055 ( FSSK1) and ? = 3 . 8415 ( FSSK2) for the proposed methods ; these values represent the 10% and 5% level of significance in the ?2 distribution with one degree of freedom , which used the ?2 significant test . 
6.1 Question Classification
Question classification is defined as a task similar to text categorization  ; it maps a given question into a question type . 
We evaluated the performance by using data provided by  ( Li and Roth , 2002) for English and ( Suzuki et al , 2003b ) for Japanese question classification and followed the experimental setting used in these papers  ; namely we use four typical question types , LOCATION , NUMEX , ORGANIZATION , and TIMETOP for JQA , and ? coarse ? and ? fine ? classes for EQC . We used the one-vs-rest classifier of SVM as the multiclass classification method for EQC  . 
Figure 4 shows examples of the question classification data used here  . 
question types input object : word sequences ( []: information of chunk and ??: named entity ) ABBREVIATION what , [ B-NP ] be , [ B-VP ] the , [ B-NP ] abbreviation , [ I-NP ] for , [ B-PP]T exas , [B-NP] , ? B-GPE ?? , [ O]DESCRIPTION what , [ B-NP ] be , [ B-VP]Ab origines , [ B-NP ]? , [ O]HUMAN who , [ B-NP ] discover , [ B-VP ] America , [B-NP] , ? B-GPE ?? , [ O ] Figure 4: Examples of English question classification data Table  3: Results of the Japanese question classification ( Fmeasure )   ( a ) TIME TOP ( b ) LOCATION ( c ) ORGANIZATION ( d ) NUMEX n



BOW-K 1234?- . 961  . 958  . 957  . 956 -  . 961  . 956  . 957  . 956 -  . 946  . 910  . 866  . 223  . 902  . 909  . 886  . 855 -1 2 3 4 ? -  . 795  . 793  . 798  . 792 -  . 788  . 799  . 804  . 800 -  . 791  . 775  . 732  . 169  . 744  . 768  . 756  . 747 -1 2 3 4 ? -  . 709  . 720  . 720  . 723 -  . 703  . 710  . 716  . 720 -  . 705  . 668  . 594  . 035  . 641 690  . 636  . 572 -1 2 3 4 ? -  . 912  . 915  . 908  . 908 -  . 913  . 916  . 911  . 913 -  . 912  . 885  . 817  . 036  . 842  . 852  . 807  . 726 -6 . 2 Sentence Modality Identification For example , sentence modality identification techniques are used in automatic text analysis systems that identify the modality of a sentence  , such as ? opinion ? or ? description ? . 
The dataset was created from Mainichi news articles and one of three modality tags  , ? opinion ? , ? decision ? and ? description ? was applied to each sentence  . The data size was 1135 sentences consisting of 123 sentences of ? opinion ? , 326 of ? decision ? and 686 of ? description ? . We evaluated the results by using 5-fold crossvalidation . 
7 Results and Discussion
Tables 3 and 4 show the results of Japanese and English question classification  , respectively . Table 5 shows the results of sentence modality identification  . n in each table indicates the threshold of the subsequence size  . n = ? means all possible subsequences are used . 
First , SK was consistently superior to BOW-K.
This indicates that the structural features were quite efficient in performing these tasks  . In general we can say that the use of structural features can improve the performance of NLP tasks that require the details of the contents to perform the task  . 
Most of the results showed that SK achieves its maximum performance when n =  2  . The performance deteriorates considerably on cen exceeds  4  . This implies that SK with larger substructures degrade classification performance  . These results show the same tendency as the previous studies discussed in Section  3  . Table 6 shows the precision and recall of SK when n = ? . As shown in Table 6 , the classifier offered high precision but low recall  . This is evidence of overfitting in learning . 
As shown by the above experiments , FSSK pro-Table 6: Precision and recall of SK : n = ?
Precision Recall F
MI : Opinion .917 .209 .339
JQA:LOCATION . 896  . 093  . 1 68 vided consistently better performance than the conventional methods  . Moreover , the experiments confirmed one important fact . That is , in some cases maximum performance was achieved with n = ?  . This indicates that subsequences created using very large structures can be extremely effective  . 
Of course , a larger feature space also includes the smaller feature spaces  , ? n ? ? n+1 . If the performance is improved by using a larger n  , this means that significant features do exist . Thus , we can improve the performance of some classification problems by dealing with larger substructures  . Even if optimum performance was not achieved with n = ?  , difference between the performance of smaller n are quite small compared to that of SK  . This indicates that our method is very robust as regards substructure size  ; It therefore becomes unnecessary for us to decide substructure size carefully  . This indicates our approach , using large substructures , is better than the conventional approach of eliminating subsequences based on size  . 
8 Conclusion
This paper proposed a statistical feature selection method for convolution kernels  . Our approach can select significant features automatically based on a statistical significance test  . Our proposed method can be embedded in the DP based kernel calculation process for convolution kernels by using substructure mining algorithms  . 
Table 4: Results of English question classification ( Accuracy )   ( a ) coarse ( b ) finen



BOW-K 1234?- . 908  . 914  . 916  . 912 -  . 902  . 896  . 902  . 906 -  . 912  . 914  . 912  . 892  . 728  . 836  . 864  . 858 -1 2 3 4 ? -  . 852  . 854  . 852  . 850 -  . 858  . 856  . 854  . 854 -  . 850  . 840  . 830  . 796  . 754  . 792  . 790  . 7 78 - Table 5: Results of sentence modality identification ( Fmeasure )   ( a ) opinion ( b ) decision ( c ) description n



BOW-K 1234?- . 734  . 743  . 746  . 751 -  . 740  . 748  . 750  . 750 -  . 706  . 672  . 577  . 058  . 507  . 531  . 438  . 368 -1 2 3 4 ? -  . 828  . 858  . 854  . 857 -  . 824  . 855  . 859  . 860 -  . 816  . 834  . 830  . 339  . 652  . 708  . 686  . 665 -1 2 3 4 ? -  . 896  . 906  . 910  . 910 -  . 894  . 903  . 909  . 909 -  . 902  . 913  . 910  . 808  . 819  . 839  . 826  . 7 93 -Experiments show that our method is superior to conventional methods  . Moreover , the results indicate that complex features exist and can be effective  . 
Our method can employ them without overfitting problems  , which yields benefits in terms of concept and performance  . 

N . Cancedda , E . Gaussier , C . Goutte , and J . -M . 
Renders .  2003 . Word-Sequence Kernels . Journal of Machine Learning Research ,  3:1059?1082 . 
M . Collins and N . Duffy .  2001 . Convolution Kernels for Natural Language . In Proc . of Neural Information Processing Systems ( NIPS?2001 )  . 
C . Cortes and V . N . Vapnik .  1995 . Support Vector Networks . Machine Learning , 20:273?297 . 
D . Haussler .  1999 . Convolution Kernels on Discrete Structures . In Technical Report UCS-CRL-99-10 . UC Santa Cruz . 
T . Joachims .  1998 . Text Categorization with Support Vector Machines : Learning with Many Relevant Features  . In Proc . of European Conference on Machine Learning ( ECML ?98 )  , pages 137?142 . 
T . Kudo and Y . Matsumoto .  2002 . Japanese Dependency Analysis Using Cascaded Chunking  . In Proc . of the 6th Conference on Natural Language
Learning ( CoNLL2002), pages 63?69.
T . Kudo and Y . Matsumoto .  2003 . Fast Methods for Kernel-based Text Analysis . In Proc . of the 41st Annual Meeting of the Association for Computational Linguistics  ( ACL 2003 )  , pages 24?31 . 
X . Li and D . Roth .  2002 . Learning Question Classifiers . In Proc . of the 19th International Conference on Computational Linguistics  ( COLING 2002 )  , pages 556?562 . 
H . Lodhi , C . Saunders , J . ShaweTaylor , N . Cristianini , and C . Watkins .  2002 . Text Classification Using String Kernel . Journal of Machine Learning Research , 2:419?444 . 
S . Morishita and J . Sese .  2000 . Traversing Item-set Lattices with Statistical Metric Pruning  . In Proc . of ACM SIGACT-SIGMOD-SIGART Symp . 
on Database Systems ( PODS?00), pages 226?236.
J . Pei , J . Han , B . Mortazavi-Asl , and H . P into . 
2001 . Prefix Span : Mining Sequential Patterns Efficiently by Prefix-Projected Pattern Growth  . 
In Proc . of the 17th International Conference on Data Engineering ( ICDE 2001 )  , pages 215?224 . 
M . Rogati and Y . Yang .  2002 . High-performing Feature Selection for Text Classification  . In Proc . of the 2002 ACM CIKM International Conference on Information and Knowledge Management  , pages 659?661 . 
J . Suzuki , T . Hirao , Y . Sasaki , and E . Maeda . 
2003a . Hierarchical Directed Acyclic Graph Kernel : Methods for Natural Language Data  . In Proc . of the 41st Annual Meeting of the Association for Computational Linguistics  ( ACL 2003 )  , pages 32?39 . 
J . Suzuki , Y . Sasaki , and E . Maeda . 2003b . Kernels for Structured Natural Language Data . In Proc . 
of the 17th Annual Conference on Neural Information Processing Systems  ( NIPS 2003 )  . 
