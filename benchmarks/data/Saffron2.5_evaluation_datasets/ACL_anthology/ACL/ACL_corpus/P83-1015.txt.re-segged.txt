On the Mathematical Properties of Linguistic Theories 
C.tgm.lm ~ ndPev-r = utt
Dept . of Computer Science
Untvermty of Toronto
Toronto , Ontario , Canada M5SIA 4

Meta-theoretical results on the decidability , genera-tire capacity , and recognition complexity o~several syntactic theories are surveyed These include contextfree grammars  , transformational grammars , lexical func-tional grammars , generalized phrase structure grammars , and tree adjunct grammars . 
i . lmt ~ tiom.
The development of new formalism sim which to express linguistic theories has been accompanied  , at\[east since Chomsky and Miller's early work on contextfree languages  , by the study of their nets-theory . In par-tioular , numerous results on the decidabttity , generative capacity , and more recently the co~aplexity of recogni-t ion of these formalisms have been published  ( and rumoured ! )  . Strangely enough , much less attention seems to have been devoted to a discussion of the significance of these mathematical results  . As a prelim-tnary to the panel on formal properties which will address the significance is sUe  , it seemed appropriate to survey the existing results  . Such is the modest gee\[of this paper . 
We wdt consider context-tree languages , transformational grammars , lexicat functional grammars , generalized phrase structure grammars , and tree adjunct grammars . Although we will not examme them here , formal studies of other syntactic the omes have been undertaken : e  . g . Warren\[51\] ~ or Montague's PTQ\[30\] , and Bor-gida \[71 for the stratifications\[grammarsOfLamb \[25\]  . 
The refollows a brief summary of some comments in the Rterature about related empirical issues  , but we avoid entirely the issue of whether one theory is more descriptively adequate than another  . 
Z . P , - elim Luary Defil ~ ons
We assume the reader is familiar with the basic definitions of regular  , context-tree ( CF ) , context-sensitive ~ CS ) , recurst ve , and recursively enumerable ( r . e . ) languages and with their accepters as car ~ be \ [ ound tn \[':-_\]  , Some elementary definitions \[ rom complexity theory may be useful  . ? urt . her details may be found tn \[2\] Complexity theory is the study of the resources required of algorithms  , usuai~yspace and time . Let e(z ) be a ~ une-Lion , say the recognition Junction \[ or a language i . The most inter~t!n ~ results we could obtain about  ) ' would be a ~ o ~ JeT bo % zn do n ~ he resources needed to compute fonamac ~ hine of a gLven architecture  , sayay on Neumann This research was sponsored by the National Science and Engineering Research Council of Cana daun der Grant 

computer or a parallel array of neurons . These results over whole classes of machines are very difficult to obtain  , and none el any significance exist for parsiD . g problems . 
Restricting ourselves to a specific machine model and an algorithm M for j '  , we can ask about the cost . ( e . g time or space ) e ( z ) of executing M on a specific input z . 
~I cally c is to of lne-gra/ned to be useful : what one studies instead ts a functio ~ cw whose argument is a ~  . 
L'Iteger n denoting the s/zs of the input to !4 , and which gives some measure of the cost of processing inputs of length n  . Complexity theorists have been most interested g % the a  . ~\]m~i~ot . ie behavlour of c~v , i . e . the behaviour of c ~ as ngets\[alge . 
:fone is interested in Upperbo~n ~ S on the behavlot : - of M  , one usuai \ [ y defines c . ( n ) as them . a:ru-n . um , of c (= over all inputs z of size n . ~his is called the worst-case convexity hJumct/on for  . &' . Notice that other de ~ rutlon : are possible : one could define the expected eomp\[exity ~ otion c  , ( n ) for / v/as the average of c ( = ) overall LnpuL . - . 
of length ~% . c might be more useful than c ~ if one had an ~ dea of what the distribution of ~ n puts to M could be  . 
Unfortunately , the introduction of probabiJistic considerations makes the study of expected complex : It  ) techm cally more difficult that of worst case comp \[ exity ? or a given problem  , expected and worst case measures may be quite ditTerent  . 
it is quited lf Tieult to get detailed descriptions otc ~ and for many purposes a cruder estimatets sufficient  . 
~" ~' le next abstraction involves " lumping " classes of cw functions into simpler ones that more clearly demon-str ~ tetheir ~ symptottc behavlour and are easier to manipulate  . This is the purpose of O-no ~ . Oon . Let f(n ) and g(n ) be two ~ ui % ct to ns . \], ts said to be O(g), . fa constant multiple of ~ is an upper bound for f  , ~ or ~ tl\[but a finite number o ~ values of n . .~\[ore precisely , ftsO(g ) ff~here , s are constants K and nO such that ~ or all ~% > ~ e\]  , ( n ) <

Given an algorithn ~ . M , we will say that tts ' . verst-case time complexity tsO ( g ) if the worst-ease time cost function cw (  . n ) : or M is O ( g ) Notice that this merely says that almost all ~ nputs to M of s  , zen can be processed in time at most a constant times g  ( n )  . It does nat ~ ay that al JL nputs requL reg\[~% ) time , or even that any do even on M , let alne on any other machine that Lmp\[ements\] ,  . Also , if two algorithms A\] and A 2 area va Hab\[e for a function \]' . 
and\[\[their worst-case complexity can be given respectively as OE  , gl ) and O(g ~) , and g2 < g2'tt may still . be the case that for a large number of cases ( may be even for all cases one is likely to encounter in practice  ) that A2 will be the preferable algorithm , simply because the constant K ! forg ! may be much smaller than Kg for  . q8 . 

In examining known results about the recognition complexity of various theories  , it is useful to consider how " robust " they are in the face of changes in the machine model from which they were derived  . These models can be divided into two classes : sequential models and parallel models  . Sequential models \[2\] include the familiar single - and multi-tape Turing Machines  ( TMs ) as well as Random Access Machines ( RAMs ) and Random/%:ces ~ Stored Prograzn Maehines ( RASPs )  . ARAM is Like a TM except that its working menory is random access rather than sequential  . ARASP is like a RAM but stores its program in its memory  . Of all these models , it is most like a y on Neumann computer . 
All these sequential models can simulate each other in ways that do not cause great changes in time complexity  . For example , ae-tape Turing Machine that runs in time O ( t ) can be simulated by a RAM in time O ( t )  . and conversely , a RAM runmng in O ( t ) can be simulated by a e-tape TM in time O ( t  ~ )  . In fact , all familiar sequential models are poIF nonm ~ Uy relate & they can su-nutate each other with at most a polynomial toss inefficiency  . 
Thus if a syntactic model is known to have a difficult recognition problem on one sequential model  , then it will not have a much easier one on another  . 
Transforl Tting a sequential algorithm to one on a parallel machine with a fixed number K of processors pro-rides at most a factor K improvement in speed  . More interesting results are obtained when the number of processors is allowed to grow with the size of the problem  , e . g . with the length of the string to be parsed . If we view these processors as connected together in a circuit  , vath inputs values entering at one end and outputs being produced at the other  , then a problem that has a solution on as sq ~ential machme in polynomial time and in spaces  w111 have a solution on a para LLeL machine with a polynomial number of processors and ci~-c~da -ptA  ( or max-Lmum number of processors data must be passed through from input to output  ) O ( s2 )   . Since the depth of a parallel circuit corresponds to the  ( parallel ) ~/ ~ tere quired to complete the computation , this means that a \[ gorlthms with sequential solutions requiring small space  ( such as deterrnimstic CSLs ) have fast parallel solutions . For a comprehensive survey of parallel computation  , see

3. Context Free languages.
Recognition techm ques for contextfree languages are wellknown  ~3\]  . The socalled " CK ~ ~ ' or " dTnarm c programming " method is attributed by Hays  \[~-51 to J Cocke , and Lt was discovered m dependent Ly by Kasami ~5 ~ . \] and Younger\[53\] who showed it to be O(nJ ) . It requires the grarm-nartobe in Chomsky Normal Form  , and putting an arbitrary grammar in CNF may square the size of the grammar  . 
Ear\[ey's algorithm recognizes strings in arbitrary CFGs in tlme O  ( n3 ) and space O ( rt2 )  , and in time O(n2) for unambiguous CF'Gs . Graham , Harrison and Ruzzo \[/3\] glvean algorithm that tlnifies ~ and Earey's \[/0\] algorithm , and discuss implementation details . 
Valiant \[50\] showed how to Interpret the Ck'Y algorithm as the finding of the transitive closure of a matrix and thus reduced CF recognition to matrix multiphca - tion  , for which subcubic a Jgorithms exist . Because of the enormous constants of proport , onality associated with thls method , it is not likely to be of much practical use , either an implementation method or as a descrt p -tlon of the function of the brain  . 
Ruzzo \[55\] has shown how CFLs can be recognized by booleanc ir cuits of depth O  ( Log ( n ) 2 )  , and thus that parallel recognition can be done in time O  ( log ( ~ ) e )  . The required circuit has size polynomial in ~ . 
So as not to get mystified by the uppe ~- bs~nW2 on CF recogmtion , it is useful to remember that no known CFL requires more than linear time  , nor is there a ( non-constructive ) proof of the existence of such alarg "- . =~ . 
For an empirical comparison of various parsing methods  , see Slocum\[44\] . 
4. Tran ~ ormational Gram.mr.
From its earliest days , discussions of transforma-t/onal grammar ( TG ) have included mention of matters computational . 
Peters and Ritchie \[3S \] provided the first nontrivial results on the generative power of TGs  . Their model reflects the " Aspects " version quite closely  , including transformations that could move and add constituents and delete them subject to recoverability  . All transformations are obligatory , and applied cycl ) cally from the bottom up . They show that every recursively enumerable ( re . ) set can be generated by a TC using a conte?t -sensitive base  . The proofts quite simple : the right hand sides of the  type-0 rules that generate ther . e . set are padded with a new " blank " symbol to make them at least as long as their lefthand sides  . Rules are added to allow the blank symbols to commute with all others  . These context-sensitive rules are then used as the base of a  T0 whose only transformation deletes the blank symbols  . 
Thus if the transformational formalism itself is supposed to cA~te ~' tze the grammatical strings of possible natural languages  , then the only languages being excluded are those which are notenurner a bie under an ~\] model of computation  . 
At the expense of a considerably more intricate argu_rnest  , the previous result can be strengthened \[32\] to show that every r . e . set can be generated by a contextfree 5 used TG , as long as a ~ Iter ( intersection with a regular set ) can be applied to the phrase-markers output by the transformations  . In fact , the base grammar can be 4 , n dependent of the language being generated . 
The proof involves simulating a TM by a TG . The transformations first generate an " input tape " for the TM being simulated  , and then apply the TM productions , one per cycle of the grammar . The filter insures that the base grammar generated just as many S nodes as necessary to generate the input string and do the simulation  . Again , if the transformational formalism is supposed to characterize the Dossibie natural languages  , then the Universal Base HYl ~) th . esis \[31\] according to which all natural \[ anguages can be generated from the same base grammarks empirically vacuous : an ? #  , recurs \[ rely enumerable language can . 
: Teverai attempts were then made to find a restricted form of the transformational model that was descmp-tively adequate arld yet whose generated languages are recurslve  ( see e . g .  \[271) . Since a key part of the proof in \[32\] involves the use of a filter on the final derivation trees  , Feters and Ritchie examined the consequences of for bidding fi/%al filtering  \[35\]  . They show that if S is the only recursive symbol in the CF base then the generated language L is predict ~ bLUen ~ zrte ~ - ~ bLeandez ' pone ' rLtZalL  . Nbo~ndec ? A language L\[s predictably en unlerable if there is an " easily " computable function t  ( n ) that gives an upper bound on the number of tape squares needed by its enumerating TM to enumerate the first n elements of L  . 
L is exponent/a Uybounded if there is a constant K such that for every string z in L there is another string z ' in L whose length is at most K times the length of z  . 

The class of non-filtering languages is quite unusual  , including all the CFLs ( obviously ) , but also some ( but not all ) Cb-l~s , some ( but not all ) reeursive languages , and some ( but not all ) r . e . languages . 
The source o~non-recursivtty in transformational \[ y generated languages is that transformations can delete arbitrarily large parts of the tree  , thus producing surface trees arbitrarily smaller than the deep structure trees they were derived from  . This ts what Chomsk'y's recover-ability of deletions condition was meant to avoid  . In his thesis , Petrick \[36\]   de6nes the following term ~ sal-\[em&d . h-i~cr , = a-in E condition on transformational derivations : consider the followi ~ g twop-markers from a derivation  , where the right one is derived from the left one by applying the cycle of transformations to subtree c producing the subtree z ~ rss Contmuing the derivation  , apply the cycle to treet yielding tree ~ . 
s$cycle2
A derivation satisfies the terminal-length -increasing condition if the yield of I ~ is always \ [ ortger than the yield of Petrick shows that if all recursion in the base " passes through S " and if all derivations satisfy the terrninal-\[ength-mcreasing condition  , then the generated language is recursive . Using a slightly more restricted model of transformations  , Rounds \[42\] strengthens this result by showing that the resulting languages are in fact context-se nsitive  . 
nanun published paper , My hill shows that Lf the condition is weakened to terrnlnal-length-non-decreasing  , then the resulting languages can be recognized in space at mostez-po~ent/o\]  , Ln the length of the input . This implies that the recognition can be done ~ n at most double-exponential time  , but Rounds\[ . ~\] shows that not only can recognition be done in ez-ponevtt/a/t/raze  , but that every language recognizable in exponential time can be generated by a TG satisfylng the terminal-length-non-decreasing condition and recoverability of deletions This Is a very stron ~ resu\]  . t , because of the closure properties of the class of exponential-time\[a_r ~ uages  . To see why this LSSO requires a ~ ew more deflnitions Let P be the classo ~ all languages that can be recog-cuzed Lnpolynom laJtime on a deterministic TM  , and NP the class of all languages that can be recognized in poly-nn miaitime on a nondeterministic T~\[P\[s obviously contained in NP  , but the converse is not known , although there is much evidence that is false . 
There is a class of problems , the socalled NP-complete problems , which are in NP and " as di ~ icuit " as any prob ' . .em in NP m the followLn ~ sense : if czn !\] of them could be shown to be mP  , then art the problems mNP would also be in P . One way to show tha La language LLs NP-complete \[sto show that L is in NP and that every other lar ~ uage Loin NP can be pol~omi ~ lly  tr-allSfOlrl0sed into L , i . e . that there is a deterministic TM , operating in polynomial time ' , that will transform an input tu to L into an input % u  0 to L 0 such that misin L if and only tu 0 tsLnLO . In practice , to show that a \[ an@uage is NP-complete , one shows that it ~ s in NP , and that some already-known NP-complete language can be polynomially transformed to it  . 
All the known NP-comp\[et . e languages can be recognized in exponential time on a deterministic machine  , and none are known to have sub-exponential solutions  . 
Thus sint : e the restricted transformational languages of Rounds characterize the exponential languages  , then i\[all of them were to be in P , then P would be equal to NP Putting it another way  , i\[P is not equal to NP , then some transformational languages ( even thoses at , sfytng the terrnlnal-length-non-increasin ~ condition  ) have ~" tractable " ( i . e . polynomial time ) recognition pro ~" cu :: , sonany deterministic TM . Note that this result also holcts for all the other ki % own sequential models of computation  , and even for parallel machines wlt has many as a poL % p to  , rt/at number o\[processors . 
5 . L=xical FUnctional Grammar , in part , transformational grammar seeks to account for a range of constraints or dependencles wtthm sentences  . Of particular interest are subcategorlzation dependencies and predicate-argunlent dependencies  . 
These dependencies can hold over arbitrarily large distances ~ everal recent theories su~o = est difTerent u/ays of accounting for these dependencies  , but without making use of transformations . We will exa /' nine three o ~ these , Lexica \[ Functional Grammar , Generalized Phrase ~ truc-ture Grammar , and Tree Adjunct Grammars , mt tte next few sections . 
Lexica \[ Functional Grammar ~ LFG ) of gap\[an and Bresnan \[24\] aims to provtd~a descriptively adequate syrttactic formalism wlthout transformations  . All the work done by transformations is instead encoded tn structures in the \[ exlcon and in \[ inks established between nodes in the constituent structure LFG languages are CS and properly include the CFLs  \[2~\]  . Berwlck \[5\] shows that a set of strings whose recognition problem is known to be NP-compIete  , namely these to \[ satis Qable boolean formulas , LsanLFGl &\[ ~ uage . 
Therefore , as was the case for Rounds's restrlcted class of TGs  , tfPLs not equal to NP , then some languages ~ en-erated by \[-~ s do not have polynomial t ~ me recognition algorithms indeed only quite " baste " parts of the LFG mecharus m are necessary to the reduction  . This includes mechanlsms necessary for feature agreement  , for forcing verbs to take certain cases , and\[exlcalambt -== uity Thus nos , mp\[echan~e to the formalism is likely to avoid the combinator laicon sequences of the ~ ull mechanism  Berw1ek has also examined the relation between LFG and the class of languages generated by iIldexed gram ~t-\[I\]  , a class kllown to be a proper subset of the C~Ls , but including some NP-complete languages \[42\] Heela/ms ( personal communication ) that the indexed languages are a proper subset of the LFG languages  . 
6. Generalized Phrase Structure Grammar.
In a series of papers , Gerald Gazdar and his colleagues\["l\]have argued for a joint account of the syntax and semantics o\[En~hshlike LFG in eschewing the use of trans  , formations but unlike it in positing , only one level of tO0 syntactic description . The syntactic apparatus is based on a nonstandard interpretation of phrase-structure rules and on the use of metarules  . The formal consequences of both these moves % ave been investigated  . 
6. I . No ~ A ~ Mmsmbtlity
There are two ways of interpreting the function of CF rules  . The first , and most usual , is as rules for , - e  ~ , T , 3b/~g strings . Derivation trees can then be seen as canonical representatives of classes of derivations producing the s~mestring  , and di~lering only in the order of application o ~ the same productions  . 
The second interpretation of CF rules is as con -straimts on derivation trees : a legal derivation tree is : he where each node is " admitted " by a rule  , i . e . each node dorm nates a sequence of nodes in a way sanctioned by a rule  . For CF rules , the two interpretations obviously generate the same strings ~  , ~ the same set of trees . 
Following a suggestion of McCawley's , Peters and ~ , it chle \[34\] showed that if one considered context-se~s ~ . ve rules from the node-admissibility point of view  , the languages defined were still CF Thus the use of CS rules in the base to impose subcategorization restric-t/oRs  , for example , does not increase the weak generatlve capacity of the base component  .   ( For some different res-trictions of context -sensitive rules that guarantee that only CFLs will be generated  , see Baker\[~:\] . ) Rounds \[40\] gives a simpler proof of Peters and ? , it chie's node-adrn is stbility result using the techniques from tree-automata theory  , a generalization to trees offmlte state automata theory for strings  . Just as a0_rote state automaton ( FSA ) accepts a strong by reading it one character at a time  , changing its state at each transition , a finite state tree automaton ( FETA ) traverses trees , propagating states . The top-do wlnF ~ TA " attaches " a starting state ( from a flnite set ) to the root o\[the tree . Tran-sltions are allowed by productions of the form  ( q ,   ,  . , ~) -->( q , .   .   .   .   .  ~,  .   .   ) such that if state q is being applied to a node Labelled and dominatmg n descendants  , then state ~ i should be applied to its ~ th descendant  . Acceptance occurs if all \[ eaves of the tree end uplabelled with states in the accepting subset  . The bottom-up Fs " rA is similar : start-\[ng states are attached to the\[eaves of the tree and the productions are of the form  ( = ,  ~ , q  ~ . . . . . q  ~ ) -> q indicating that if a node labelled a dommating n descendants each labelled wlth states qltoq  , v then node age ts labelled ~ th state q . .Acceptance occurs when the root is labelled by a state from the subset o\[accepting states  . 
.As is the ease ~ th FSAs , F ~ TAs of both flavours can be either deterministic or nondeterministic  . A set of trees i~sa~d to be recognizable if it is accepted by a nondeterministic bottom-up Fb -TA  . Again as with FSAs , any seto~trees accepted by a non-determlmstic bottom-up ~ At  . ~ accepted by a deterministic bottom-up ~ , ~ TA , but there : ~ ult does not hold for topdown F'5" FA . although the recognizable sets are exactly the languages retognized by non-determinlstic topdown FSTAs  . 
A set of trees is local if it is the set of demvation trees of a CF grammar Clearly  , every local sets recognizable by a one-state bottom-up F ~ A that checks at each node that it  satis6es a CF production . Also , the yield of a recogmzable set ol trees ( the set of strings it generetes ) is CF . .4/though not all recognizable sets are local , heycan all be mapped into local sets by a simple ~ homo ~ norphic  ) mapping . 
Rounds's proof !41\] that CS rules under node-adnussibility generate only CFLs involves showing that the set of trees accepted by the rules is recognizable  , i . e . that there is a nondeterministic bottom-up FSTA that can check at each node that some node -admissibili ~: y condition holds there  . This requires checking that the " strictly contextfree " part of the rule holds  , and that some proper analysis o\[the tree passing thr ~" g'_the node ~ atisfies the " context-sensitive " part of the rule  . 
The ditlieulty comes h'om the fact that the bottom -up automaton cannot generate the set of proper analyses  , but must instead propagate ( in its state set ) the proper analysis conditions necessary to " admit " the nodes of its subtrees  . It must , of course , also check that those rules get satisfied . 
A more intuitive proof using tree tr~nsduce ~ rsa well as FSTAs  , s sketched in the Appendix . 
Joshi and Levy \[21\] strengthened Peters and Ritchie's result by showing that the node admissibility conditions could also include arbitrary Boolean combinations of ~ mance conditions : a node could specify a bounded set of \[ abels that must occur immediately above it along a path to the root  , or un~r'nediate\[ybelo witon a path to the frontier  . 
In general the CF grammars constructed\[n . the proof of weak equivalence to the CS grammars under node admissibility are much larger than the original  , and not useful for practical recognition . Joshi , Levy and Yueh\[22\] , however , show how Eariey's a/gomthm can be extended to a parser that uses the local constraints directly  . 
8.2. Metaru Jes.
The second important mechanism used by Gazdar\[ ii \] is mp ~ es  , or rules that apply to rules to produce other rules  . Using standard notation for CF rules , one example of a metarule that could replace the transformation k ~ lown as " particle movement " is: 
V - -> VNP t X ==> V --> VP ~ ~- PRO\]X
X here is avamable behavmg like vamables in structural analyses of transformations  . If such vamables are restricted to being used as cbbTeviatic ns  , that is if they are only allowed to range over a \] ~ n ~ te subset of strings over the vocabulary  , then closing the grammar under the metarules produces only a  6nite set of derived rules , arid thus the generative power of the formalism is not increased  . If , on the other hand , X is allowed to range over strings of unbounded length  , as are the essential ~ es of transformational theory  , then the consequences are less clear . It is well known , for example , that I\[the right hand sides of phrase structure rules are allowed to be arbitrary regular expressions  , then the generated languages are still contextfree  . Might something like this not be happening wlth essential variables in metarules ? It turns out not  . 
The formal consequences of the presence of essen - tie /  , variables in metarules depends on the preserice of another device  , the socalled phantoms categories . It may be convenient informulating metarules to allo ~  , in the left hand sides of rules , occurrences of syntactic categories that are never introduced by the grammar  ,  1 . e that never appear m the mght-hand sldes of rules  . n standard CFLs , these are called % L . ~ eLesse ? tego ~ es , and rules containing them can simply be dropped , with no change Jn generative capacity Not so ~ th metarules : it is possible for metarules to rewrite rules containing phantom categories into rules without them  . Such a dev-ice was proposed at one time as a ~ ay to implement pas-tures in the GPSG framework  . 

Uszkorelt and Peters \[49\] have shown that essential variables i . n metarules are powerful devices indeed : CF grammars with metaru\[es that use at most one essential variable and allow phantom categories can generate all reeur sively enumerab\[esets  . Even if phantom categories are banned , as long as the use of at \[ east one essential variab\[es\[sallowed  , then some non-reeur sive sets can be generated . 
Possible restrictions on the use of metarules are suggested in Oazdar and Pultum  \[12\]  . Shieber et al\[45\] discuss some empirical consequences of these moves  . 
7. Tree Adjunct ~.
The Tree Adjunct Grammars ( TAGs ) of Joshi and his colleagues presents a dif ferent way of accounting for syn-tactic dependencies  ( \[17\] ,  \[19\]) . ATAG conmsts of two ( finite ) sets of ( finite ) trees , the centre trees and then djunet trees . 
The centre trees correspond to the surfaces t ruc-tures of the " kernel " sentences of the languages  . The root of the adjunct trees is labelled with a non-terminal symbol which also appears e ~c actiy once on the frontier of the tree  . All other frontier nodes are labelled with terrm-naisymbols  . Derivations in TAGs are defined by repeated application of the operation of a dune U on I ~c is a centre tree containing an occurrence of a non-tern-anal  , 4 . and if is an adjunct tree whose root ( and one node n on the fron Uer )   ; slabelled , 4 , then the adjunction of a toe is per-formed by " detaching " from c the subtree ~ rooted at A  , attaching a\[nitsplace , and reatt a chiugt at node f t . 
Adjunct to n may then be seen as a tree analogue of a contextfreeder tvatlon for strings  \[40\]  . The string \[ anguage . ~ obtamed by taking they let ds of the tree languages generated by TAGs are called Tree Adjunct ~ mg Lu ~ es  , or TALs . 
In TAGs all long-distance dependencies are the result of adjtmct tons separating nodes tb  . ~tatone point in the derivation were " cLose " .   8oth crossing and noncrossing depenctenctes can be represented\[  )  . 8\] . The formal pro-perties of TAGs are fully di scussed in  \[30\]  .  \[52\], \ [~\ ]  . Of particular interest are the ~ ollo ~ ng . 
TALs properly contain the C~Ls ~ nd are property con-rained\[n the indexed languages  , which m turn are properly contained m the CSLs . Although the indexed anguages contain NP -complete languages  , TALs are much better behaved : ~ oshi and Yokomori report ~ personale ommunication lan O  ( n  ~ ) recognition algorithm and conjecture that an O ( n  ~ ) bound may be possible . 
\[3 . A Pointer to \]~ t ~ nl ~ lrieal DLseusmons The li terature on the emptm ca\[issues under iy iug the formal results reported here ts note x ~enswe  . 
Chomsky argues convincingly \[8\] that there is no argument for natural languages neeess ~  . ' ~ l~j being recursive . This , or course , is different from the possibdity that ' ~ anguages are covtt ~ zgent tyre curst ve  . Putnam \[39\] gives three reason she claims " point in this direction ":  ( i ) ' speaker ~ can presumably classify sentences as acceptable or unacceptable  , deviant or non-deviant , etcetera , wlthout reliance on extralinguistic contexts . There are of course exceptions to this rule " ,   ( ~ ) grammatical \[ ty judgements can be made for nonsense sentences  , and \[ S ) grammars can be \[ earned .   ( e ) and ( S ) are irrelevant and ( i ) contalns zts own counterargument . 
Peters and Ritchie\[S ~\] contains a suggestive but hardly open-and-shut case ~ or contingent recurst vtty:  ( : ) every TQ has an exponentially bounded cyehng ~ unction  , and thus generates only recurs \[ relanguages ,   ( Z ) every naturalan?ua ~ e has a descriptively adequate TG  , and ( 3 ) the comp\[exlty of\[anguages investigated so farks typLca\[of the class  . 
H intikka\[16\] presents a very di\[~erent argument against the recursivity of English based on the distribution of the words ~ r ~ y and evev - y  . I/is account of why JoA ~\] cno~se'u e'mj th?~g is grarn matlcal whi\[eJohn ~ c~o~s=~y-thingks not is that = ~ y can appear only in contexts where replacing it by eve ~ changes the meaning  . Taking mean-mg to be logical equivalence , this means that grammati-eality is dependent on the determination of logical equivalence of logical formulas  , an undecidable problem . 
Chomsk'y \[8\] argues that a simpler solution ks available , namely one that replaces logical equivalence by syntactic tdentLty of some kind of logical form  . 
PuHum and Gazdar\[38\]\[sathorough survey of , and argument against , published claims ( mainly the " respectively " examples\[26\] , Dutch cross-serial dependencies , and nominallzation in Mohawk\[ , 37\] ) that some natural languages cannot be weakly generated by CF grammars  . 
NocIalms are made about the strong adequacy of CFGs  . 
9. Seeking E~gnilleanee.
When can the supporter of a weak ( syntactic ) formalism ( i . e . low recognition complexity , low gener . ~ tive capacity ) e\[alm that it superior to a competing more powerful formalism ? Ling\[astirtheories can differ along several dimensions  , wtth generative capacity and recognition capacity being only two  ( albeit related ) ones . The evaluation must take into consideration at \[ east the fot to vangothers : Coverage  . Do the theories make the same ~ ramma to tcal predictions ? Extensib dity  . The linguistic theory of which the syntactic theoryks apart will want to express wellformedness constraints other than syntactic ones These constraints may be expressed over syntactic representations  , or over different representations , presumably related to the syntactic ones . One theory may make this connection possible when another does not  . This of course underlies the arguments for strong desemptt vead equacy  , Also relevant here L show the tmguL stlc theory as a whole is decomposed  . The syntact m theory can obviously be made ampler by trans~ermng some of the explanatory burden to a  . nother constituent . The c\[asm ce?amp\[ein programming languages is the constraint that all vam-ables must be declared before they are used  . This constrain\[cannot be Lmposed by a CFG but can be by an indexed grammar  , at the cost of a dramatic increase in recognltton complexity  . Typically , however , the requirement is slmply not cen~idered part of " syntax "  , which thus remams CF , and imposed separately in this case , the overall recognit mncomp\[exlty remams ~ ome low-order polynomial  , Some arguments of this kind can be found m \[3t ~\] Separating thee on stralnts into different sub -theome ~ wlt\[nottn general make the problem of recog-ntzmg strings that satisfy all the constraints any more eHictent  , buttt may allow hailing the power of each constituent  . To take an e?treme example , every r . e . set the homomorphic image of the intersectlon of \[~  , ) contextfree languages , Implementation . This Ls probably the most subtles , ~t of issues determining the sigm fieance of the \[ or m  , ,l results , and I don't claim to understand them . 
Comparison between theories requires agreement between the machine models used to derive the complexity results As mentioned above  , the sequential models are all polynomtally related  , and no problem no thawng a likely to have one on a parallel machine limited to at most a polynomial number o\[processors  , at least if P is not equal to NP . Both these results restrict the improvement one can obtain by changing implementation  , but are of little use in comparing algorithms of low complexity  . Berwick and Weinberg \[6\] give examples of how algorithms of low complexity may have different implementations differing by large constant factors  . In particular , changes in the form of the grammar and in its representation may have this effect  . 
But of more interest I believe is the fact that implementation is often accompanied by some form of resource limitation that has two effects  . First it is also a change in speeifiea Z ~ bn . A contextfree parser implemented with a bounded stack recognizes only a finite-state language  . 
Second , very special implementations can be used if one is willing to restrict the size of the probter rt to be solved  , or even use special-purpose methods for limited problems  . Marcus's parser \[28\] with its bounded lookahead is another good example  . Sentences parsable ~ nth in the allowed lookahead have " quick " parses  , but some grammatical sentences , such as " garden path " sentences cannot be recognized without an extension to the mechanism that would distort the complexity measures  . 
There is obviously much more of this story to be told  . Allow me to speculate as to how it might go . We may end up with a space of linguistic theories  , differing in the idealization of the data they assume  , in the way they decompose constraints , and in the procedural specifications they postulate  ( I take it that two theories may differt n that the second simply provides more detail than the first as to how constraints specified by the first are to be used  . ) Our observations , in particular our measurements of necessary resources  , are drawn from the " ultimate implementation " , but this does not mean that the " ultimately low -level theory " is necessamly the most reformative  , witness many examples in the physical sciences , or that less procedural theories are not useful stepping stones to more procedural ones  . 
It is also not clear that theories of different computational power may not be useful as descriptions of different parts of the syntactic apparatus  . For example , tt may be earner to learn statements o\[ constraints within the framework of a general machine  . The constraints once learned might then be subjected to transformation to produce more ei =llcient special-purpose processors also imposing resource limitations  . Indeed , the " possible languages " of the future may be more complex than the present ones  , just as earlier ones may have been syntactically simpler We reancient languages reg-ular O Whatever we decide to make of existing formal results  , Lt is clear that continumg contact with the complexity community is important  . The driving problems there are the P = NP question  , the determination flower bounds , the study of time-space tradeot~s , and the complexity of parallel computations . We still have some m~thodological house-cleaning to do  , but I don't see how we can avoid being affected by the outcome of their investigations  . 

Thanks to BobBet ' wlck , Arawnd Joshi , Jim Hoover , and Stan Peters for their suggestions . 

Rounds \[41\] proves that context-sensitive rules under node -admissibility generate only contextfree languages by constructing a nondeterministic bottom-up  . tree automaton to recognize the accepted trees . We sketch here a proof that makes use of several d~tev-r  , tin/s-tic O " a . ,~d'uc . e~s instead . 
FSTAs can be generalized so that instead of simply accepting or rejecting trees  , they transform them , by adding constant trees , and deleting or duplicating subtrees . Such devices are called ~ nitestate tree transdue-erw  ( FSTT )  , and like the FSTA they can be topdown or bottom -up  . Flrstmotivated as models of syntax-directed translations for compilers  , they have been extensively studied ( e . g .  \[47\] ,  \[48\] , \[40\]) but a simple subset is sufficient here . 
The idea is this . Let The the set of trees accepted by the CS-based gram/nat  . Lett be in 71 F'STTs can be used to label each node ~% of t with the set of all proper analyses passing through n  . It will then be simple to check that each node satisfies one of the node admissibility conditions by sweeping through the labelled tree with a bottom-up FSTA  . 
The node labelling is done by two FST " Fs~'l and re  . Let be the maximum length of any left or right -context of any node ad I Tussibility condition  . Thus we need only label nodes with sets of strings of length at most rrL  , and over a finite alphabet there are only a fitute number of such strings  . 
rI operates bottom-up on a tree t , and labels each node 7t of t with three sets Pb'ef z ( n )  , Sufj ~ z(n) , and Y ' zeld ( n ) of proper analyses : if P is the set of all proper analyses of the subtree rooted at vt  , then Prefix ( n ) is the set of all substrings of length at most m that are prefixes of strings of P  . Similarly ,   , ~tJ % z ( n ) is the set of all suGixes of length at most ~ , ~ . , and Z'zetd ( n ) is the set of all strings of P of length at most rrL  . It can easily be shown that for any set of trees T  , T is recognizable if and only if ~/ r ) is . 
Applying to the output of " rj , the second transducer-rm operating topdown , labels each nodert with all the proper analyses going through n  , i . e . wlth a pair of sets of strings . The first set ~ nll contain all left-contexts of node n and the second all mght-e on texts  , r2 also preserves recogmzability . A bottom-up FSTA can now be defined to check at each node that both the context-~ree parto faru /e as well as its context conditions are satisfied  . 
This argument also extends easily to cover the dotal-nance predicates of ioshi and Levy : transducers can be added to label each node wlth all its " top contexts " and all its " bottom- contexts ""\[' he final ? STA must then check that the nodes satls f-y whatever Boolean combination of dorrunanee and proper analysis predicates_are requJred by the node admissibility m // es  . 
REFEEENCES\[i \] A hoA . V . ,\[ ndezedgT ~ rrLmars . , aw eztensiovt o / the co~ztezt-free ~ rr=m~rs , JACM 15 ,  647-67! . i968 . 
\[2\]AhoA . V . , Hopcroft J . E . and Ullman J . D . , The Design and Analysis of Comlmlter Algorith~q ~ Addison-Wesley  , Reading ~ ass ,  1974 . 
\[3\] A hoA . V . , and Ullman J . D , The Theory of Parsing . 
Translation . and CoKm@ili . g . volI : Parsing . Prentice Hall,
Englewood Cliffs N . J ., 1972.
103\[4\] Baker B . S . , Arbitrary g'r~mm~r ~ generat ing contezt-free languages  , TR11-72 , Center for Research in Computing Technology , Harvard Univ . , 1972 . 
\[5\] Berwick R . C . , Comp~aZional com~lezi ty ~ nd lezical 2u~t io ~ alsrammar , t9th ACL , 198 L\[6\] Berwick R . C . and Weinberg A . , Pars/rigeff / c/ . ency , com ~ tationn ~ complezity , and the eval ~ lion of gram-rna//ca ~ theor~s , l . ing . In q .  13, , . 65-191, 1962 . 
\[7\] Borgida & T . , Formal Studies of Strat/f icational Grammars , Pb_D Thesis , Universityo ? Toronto ,  1977 . 
\[8\] Chomsky N . , Rt ~ a and Representatinns . Columbia
University Press , 1980.
\[9\] CookS . A . , 7b~'ds~cornpte ~ itythso ~ dofsvnchzo-nm~spa . v ~ zt let com . ~utat ~, L'P . n-eignement
Math , bmatique 27, 99-t24, !981.
\[I0\]Eariey J . , An effic-Lent contezt-free txtrs-mg algo-~hrn , Comm_or ACM 13 . 2, 9410~, 1970 . 
\[ Ii \] Gazdar G . , PhTssestru = hzre ~ rramanar , in Ja cobson P . 
and Puilum G . ( eds . ), The Nature of Syntactic Representation . Reidel , 1982 . 
\[12\]GazdarG . and Pullum G . , Generalized l~e ~- uc-ttu ~ C~"amr ~ A'l ~oretical Synopsis  . Indiana Univ . Ling . 
Club , 1982.
\[13\]GrahamS . L . , Harrison M . A . . and Ruzzo W . L . , An iwtprovaed co~tte=t-free recognizer , ACM Trans . on Frog . 
lang . and Systems , 2, 3, 415-462, 1980.
\[14\] Hopcroft J . E . and Ullman J . , Introduction to Automata Theory , ~es and Computation , Addison Wesley ,  1979 . 
\[15\] Hays D . G . , A ~ tornat ~ cl~g~age data ~' ocess ~ g . In Computer Ai ~ Nication ~ in the Behavioral Sciences  , H Borko ( ed . ), Prentice Hail , Englewood Cli~s N . J . , 1962\[16\] Hintikka , , I . K . K . , Q~?n~e ~ in naPurai ~ ang~ges : solute logical probtems  2  , l . i . = and PhiL , 153-L72, 1977 . 
\[17\] Joshi , % K . , How much contezt-sensitiuit~j is required to pr ~ ride reasonable St ~ ttctur ~ desc ~ ptio~s:treea ~ \] oining  9-ra~nmm-s   , to appear in Dowry D . , Karttunen L . 
add Zwicky A . ( eds . ) , Naturall ~- ua ~ e Processing : l ~ Chl ) i in uui ~ c , CoH ~= m . ltatiol laJ . and TheoRt ~ aJ . Pl " Oper-ties , Cambridge Univ . Press . 
\[18\] Joshi AK . , F~to ~ .   . grecurs-m~~ndd~pendencies : an aspect of Tree Ad\]o ~ rtY  . ~ tgOra~tmar , c and a compm-~son~f some format properties of TAG's  , GPSG's , PLG'S and LFG's , these Proceedings ,  !983 . 
\[19\] Josht & K . and Levy L . S : , P/mnsest~t ~' etrees be ~ rnm'e~tt ? ~ nyouzvo ~ Id have t turught  , 18th ACL ,  1980 . 
\[20\] Joshi A . K . , Levy L . S . and Takahasht M ,, Treead funct@-ra ~ rnars , J . of C or ~ p . and Sys . Sc . "0, i , !36-163, "975 . 
!21\] Joshi A . K . , Levy L . S . , Constv~z~nts on structura l 48scr~tions : local transfc , rmations , SIAM J . on Comput-e , 1977 . 
\[Z2\] Joshi A . K , Levy L . S . and Yueh K . , L~ca/cor~traints on progrexr~m~ng ta~g~zges , Part 1 . ~ttta ~, Th . Comp . Sc . 
i2 . 2 . 65-290,:960\[~31 Josh/A ~ K . and Yokomori T ,   S0~ne characterization theore ~- sfo ~" tree ? ~ j ~ tct la'n guages and recogniz suble sets  . \ [ orthcoming\[24 Kaplan R . and Bresnan J . , Lez~cal-F~nct~nal Gram-tnwr : a fozvrt~2 system fc , rgra ~ maZ ~ cal representation , in Bre . ~ nan J(ed . ) , The Mental Repe~sentation of Grammati-cad Relatinn ~  , M\]T Press ,  ! 982 . 
\[25\]LambS . , OutlJ~e of StJratificational Gramr , '\] ar , Georgetown Umvermty Press , Washington ,  1966 . 
\[26\] Langendo en D . T . , On the inadegu~cy o / "/~ jpe-2 and TMpe-3 gr ~ rruzrs for humanl~tg ~ ages , tnP . J . Hopper ( ed . ) ~ . udiesm Historie Rl IJ ngu J~cs:festsc\]~ri Jtt\[ or W ' ~ d P  . Lehrrmrt , John Benjamin , Amsterdam ,  159-171 ,  1977 . 
\[27\] La Pointe S . , /% zc~rsive~tess and deletiovt,\[Jno . Al~d . 
3, 227-Z65, 1976.
\[28\] Marcus M . P . , A Theory of ~ tac Uc Recognition for
Natura I Language , MIT Press , 1980.
\[29\] Matthews R . , Are the ~ Fr~at~ca l sentences of a l , ,ngtmge a rec~r~ve set ? , Synthese 40 ,  2139-224 ,  1979 . 
\[30\] Montague R . , The prier treat ~ . ewt of T . Lm~t ~ fication in 0rdina ~ J English , in Hintikka , J . , Moravcsik J , and Suppes P ( eds . ), Approaches to Natural Language , Reidel,
Dordrecht , 1973.
\[31\] Peters P . S . and Ritchie R . W . , A note on the ~ iversa Zbase ~ othes ~, J . of IJ n ? ~ . ~ cs , 5, i50-2, 1969 . 
\[32\] Peters PS . and Rttchie R . W . , On restmctzng tim base component of t'r~wsform ~ztiovtatg ~ zmma~s  , Inf . and Control 18, 483-5-1, 1971 . 
\[33\] Peters P . S . arid Ritchie R . W . , On the gevter~e power of brensloz'rrta~iona ~ grcntm ~ rs  , Inf . So .  6, 49-83, 1973 . 
\[34\] Peters P . S . and Ritchte R . W . , Co . to rt-sensitive ~ . t~rwd ~ -- Ze cows t ~ tuevt tam ~ tyszs-co ~ tezt-free Languages  . 
re'wLsited.MathSys.Theory 6.324-333, "9?3.
\[35\] Peters P . S . and Ritchie R . W . , Ncrn-fitterivu \] and local fiZter ~ ggratn mm ' . ~, tnJK . K . Hinttkka , JM . E . Moravcsik , and P . Suppes ( eds . ) Approaches to Natural Ls ~ b~ge,
Reidel , 180-194.1973.
\[36\] Petrick S . R . , A Recognition Procedure tot Transformational Grammar ~ Pb_D Thesis  ,   . k-IT , 1965\[37\] Postal P . M . , Lirtt~tations of phrase-structure g~rtt-mats , mJ . A . Fodor and J . J . Katz ( eds . ) , Th ~ structure of language : Rean ~ nL/s in the philosophy of lan=~lage  , Eng~e-wood Cliffs : Prentice Hall , 137-i 51 ,  1964 . 
\[38\]PullumG . K . and Gazdar G . , Natural and contezt-~ree ~ a ~@' uages , I ~ . ~_ . and Phil . , 4, ~71-504 .  1982 . 
\[39\]Put nam H . , So~teiss~sm the theory of ~ ramrn . ~ r,ml ~ . of S~sia in ApI~lied Mathe ~ tics .   . American
Math . Soc ., 1961.
\[40\] Rounds W . C . , Mwptr~gs and gramr ~ tarso ~ trees,
Math . Sys . Th ..4, 3, 257-~87, '.9?0.
\[41\] Rounds W . C . ,  7~'ee-ov-Lented proofs of some theorems on contezt-free and indezed ls mg ~ axj es  , 2nd ACMb-'ymp , on
Th . Comp . Sc . . 109-;-!6, 1979.
\[4 ~ Rounds W . C . , Cs~r~ptezi~j of rec , Jgni ~ ionin ~ rttev-r ~ diate-levellang ta ~ ges , 14th S\]En p . or * Sw . and Aut . 

\[43\] Rounds WC . , Agraz ~ . z ~ x ~ ticaicA wr~ter ~ . a tion of e ~ pov te vt tial-ti . te Imrtg ~ ages , ~ ~ . on , Found . cf
Commp . So ., L35-!43.1975.
\[44\]b-qocum J . , A practizat co~tpar/son of pav~ing stra-tegies , !9th ACt "_981 . 
\[45\] Shieber S . M . , Stucky S . U . , Uszkoreit H . anc\[Robinson J . j . , fb~naZ constraints on metctrtdes , these Proceedings ,  1983 . 
\[46\]Thatcher JW , Charazter~ng dew . arian trees of context-free grarn rna ~' s through a generalization of finite ~ to vrtata the ov ' g  , J . of Comp . and Sys . Sc .  ~, 3~ . 7-3~2 . 
1967\[47\] Thatcher J . W . , C . ensv~/~zed~se ~ rttenti ~ troach % no maps , J . of Comp . and Sys . So . 4, 339-67,:970\[~8\]ThatcherJW . , 7~'ee mLttozrtata : an infov-rrtal survey , \[ nA . Aho ( ed . ) , Currents in the theory of CO ml ~ ting , Prentice Hall ,  148-172 ,  !973 . 
104\[49\] Uszkoreit H . and Peters P . S . , Essm~t ~ aLvar/ables in rr~ts~/es,forthcoming . 
\[50\] Valiant L . , ~ ener ~ contextfree recogTtition in less than cubict ~ z ~  , L of Comp . and b~Fs . So . i0,308-315,1975 . 
\[51\] Warren D . S . , Syntax and Sem~nLics of Paz~ing : An ~ q ~pficaUon to Montague C~ramm~m Ph  . D Thesis , University of Michigan , 1979 . 
\[52\] Yokomori T . and Joshi A . K . , Serni-liztear~ty , p~z'U ch-ba undea ~ ss ~ td tree adjunct a Tt guages , to appear \ [ ninf . 
Pr . Letter & 1983.
\[53\] Younger D . H . ,  Recoqln52io~t ~nd pars ~ ztg of context-f ~ eelamg ' az ~ es~t / . ~ u ~ nJ , Inf . and Contl~l , I0, 2, 189-208, 1967 . 
\[54\] Kasami T . , A ~ e(T/c/ent recognition a ~ d s't j . ntaxa 2go-r~thm/~r contextfreelaT ~ g~mges , Air Force Cambridge Research Laboratory report AF-CRL-65-758  , Bedford , VLA ,  \[965 . 
\[55\] Ruzzo W . L . , On ur , : Lfo~'n , c'L rc'~cora\]c~ez~tj ( extended abstract ) , Proc . of 20th $ mnualSyrup . on Found . of Com . 
S = ., 312-318, 1979.

