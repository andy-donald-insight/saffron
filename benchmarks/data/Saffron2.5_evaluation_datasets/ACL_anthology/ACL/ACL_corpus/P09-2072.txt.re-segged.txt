Proceedings of the ACL-IJCNLP 2009 Conference Short Papers , pages 285?288,
Suntec , Singapore , 4 August 2009. c?2009 ACL and AFNLP
Iterative Scaling and Coordinate Descent Methods for Maximum Entropy
Fang-Lan Huang , Cho-Jui Hsieh , Kai-Wei Chang , and Chih-Jen Lin
Department of Computer Science
National Taiwan University
Taipei 106, Taiwan
{d93011,b92085,b92084,cjlin}@csie.ntu.edu.tw
Abstract
Maximum entropy ( Maxent ) is useful in many areas . Iterative scaling ( IS ) methods are one of the most popular approaches to solve Maxent . With many variants of IS methods , it is difficult to understand them and see the differences . In this paper , we create a general and unified framework for IS methods . This framework also connects IS and coordinate descent ( CD ) methods.
Besides , we develop a CD method for
Maxent . Results show that it is faster than existing iterative scaling methods1.
1 Introduction
Maximum entropy ( Maxent ) is widely used in many areas such as natural language processing ( NLP ) and document classification . Maxent models the conditional probability as:
P w ( y|x)?S w ( x , y)/T w ( x ), (1)
S w ( x , y)?e
P t w t f t ( x,y ) , T w ( x )? ? y
S w ( x , y ), where x indicates a context , y is the label of the context , and w ? Rn is the weight vector . A function f t ( x , y ) denotes the tth feature extracted from the context x and the label y.
Given an empirical probability distribution ? P ( x , y ) obtained from training samples , Maxent minimizes the following negative loglikelihood : min w ? ? x,y ?
P ( x , y ) logP w ( y|x ) = ? x ?
P ( x ) log T w ( x )? ? t w t ?
P ( f t ), (2) where ? P ( x ) = ? y ?
P ( x , y ) is the marginal probability of x , and ? P ( f t ) = ? x,y ?
P ( x , y)f t ( x , y ) is the expected value of f t ( x , y ). To avoid overfitting the training samples , some add a regularization term and solve : min w
L(w )? ? x ?
P ( x)logT w ( x )? ? t w t ?
P(f t )+
P t w 2? (3) 1A complete version of this work is at http : // www.csie.ntu.edu.tw / ? cjlin/papers / maxent_journal.pdf.
where ? is a regularization parameter . We focus on (3) instead of (2) because (3) is strictly convex.
Iterative scaling ( IS ) methods are popular in training Maxent models . They all share the same property of solving a one-variable subproblem at a time . Existing IS methods include generalized iterative scaling ( GIS ) by Darroch and Ratcliff (1972), improved iterative scaling ( IIS ) by Della Pietra et al (1997), and sequential conditional generalized iterative scaling ( SCGIS ) by Goodman (2002). In optimization , coordinate descent ( CD ) is a popular method which also solves a one-variable subproblem at a time . With these many IS and CD methods , it is uneasy to see their differences . In Section 2, we propose a unified framework to describe IS and CD methods from an optimization viewpoint . Using this framework , we design a fast CD approach for Maxent in Section 3. In Section 4, we compare the proposed CD method with IS and LBFGS methods . Results show that the CD method is more efficient.
Notation n is the number of features . The total number of nonzeros in samples and the average number of nonzeros per feature are respectively # nz ? ? x,y ? t:f t ( x,y ) 6=0 1 and ? l ? # nz/n.
2 A Framework for IS Methods 2.1 The Framework The one-variable subproblem of IS methods is related to the function reduction L(w+ze t )? L(w ), where e t = [0, . . . , 0, 1, 0, . . . , 0]
T . IS methods differ in how they approximate the function reduction . They can also be categorized according to whether w?s components are sequentially or parallely updated . In this section , we create a framework in Figure 1 for these methods.
Sequential update For a sequential-update algorithm , once a one-variable subproblem is solved , the corresponding element in w is updated . The new w is then used to construct the next subproblem . The procedure is sketched in
Sequential update
Find A t ( z ) to approximate
L(w + ze t )? L(w)
SCGIS
Let A t ( z ) =
L(w+ze t )? L(w)
CD
Parallel update
Find a separable function A(z ) to approximate L(w + z )? L(w)
GIS , IIS
Figure 1: An illustration of various iterative scaling methods.
Algorithm 1 A sequential-update IS method
While w is not optimal
For t = 1, . . . , n 1. Find an approximate function A t ( z ) satisfying (4).
2. Approximately min z
A t ( z ) to get z ? t .
3. w t ? w t + z ? t .
Algorithm 1. If the tth component is selected for update , a sequential IS method solves the following one-variable subproblem : min z
A t ( z ), where A t ( z ) bounds the function difference:
A t ( z ) ? L(w + ze t )? L(w ) = ? x ?
P ( x ) log
T w+ze t ( x)
T w ( x ) + Q t ( z ) (4) and Q t ( z )? 2w t z+z ?
P ( f t ). (5)
An approximate function A t ( z ) satisfying (4) does not ensure that the function value is strictly decreasing . That is , the new function value L(w + ze t ) may be only the same as L(w ). Therefore , we can impose an additional condition
A t (0) = 0 (6) on the approximate function A t ( z ). If A ? t (0) 6= 0 and assume z ? t ? argmin z
A t ( z ) exists , with the condition A t (0)=0, we have A t ( z ? t )<0. This inequality and (4) then imply L(w + z ? t e t )< L(w).
If A ? t (0) = ? t
L(w ) = 0, the convexity of L(w ) implies that we cannot decrease the function value by modifying w t . Then we should move on to modify other components of w.
A CD method can be viewed as a sequential IS method . It solves the following subproblem : min z
A
CD t ( z ) = L(w + ze t )? L(w ) without any approximation . Existing IS methods consider approximations as A t ( z ) may be simpler for minimization.
Parallel update A parallel IS method simultaneously constructs n independent one-variable subproblems . After ( approximately ) solving all of them , the whole vector w is updated . Algorithm 2 gives the procedure . The differentiable function A(z ), z ? Rn , is an approximation of
L(w + z )? L(w ) satisfying
A(z ) ? L(w + z )? L(w ), A(0) = 0, and
A(z ) = ? t
A t ( z t ).
(7)
Similar to (4) and (6), the first two conditions en-Algorithm 2 A parallel-update IS method
While w is not optimal 1. Find approximate functions A t ( z t ) ? t satisfying (7).
2. For t = 1, . . . , n
Approximately min z t
A t ( z t ) to get z ? t .
3. For t = 1, . . . , n w t ? w t + z ? t .
sure that the function value is strictly decreasing.
The last condition shows thatA(z)is separable , so min z
A(z ) = ? t min z t
A t ( z t ).
That is,we can minimizeA t ( z t ),? t simultaneously , and then update w t ? t together . A parallel-update method possesses nice implementation properties.
However , since it less aggressively updates w , it usually converges slower . If A(z ) satisfies (7), taking z = z t e t implies that (4) and (6) hold for any A t ( z t ). A parallel method could thus be transformed to a sequential method using the same approximate function , but not vice versa.
2.2 Existing Iterative Scaling Methods
We introduce GIS , IIS and SCGIS via the proposed framework . GIS and IIS use a parallel update , but SCGIS is sequential . Their approximate functions aim to bound the function reduction
L(w+z)?L(w )= ? x ?
P ( x ) log
T w+z ( x)
T w ( x ) + ? t
Q t ( z t ), (8) where T w ( x ) and Q t ( z t ) are defined in (1) and (5), respectively . Then GIS , IIS and SCGIS use similar inequalities to get approximate functions . They apply log ? ? ?? 1 ?? > 0 to get (8)?? x,y ?
P ( x)P w ( y|x)(e
P t z t f t ( x,y ) ?1)+ ? t
Q t ( z t ).
(9)
GIS defines f # ? max x,y f # ( x , y ), f # ( x , y ) ? ? t f t ( x , y ), and adds a feature f n+1 ( x , y)?f # ? f # ( x , y ) with z n+1 =0. Assuming f t ( x , y ) ? 0, ? t , x , y , and using Jensen?s inequality e
P n+1 t=1 f t ( x,y ) f # ( z t f # ) ? ? n+1 t=1 f t ( x,y ) f # e z t f # and e
P t z t f t ( x,y ) ? ? t f t ( x,y ) f # e z t f # + f n+1 ( x,y ) f # , (10) we obtain n independent one-variable functions:
A
GIS t ( z t ) = e z t f # ?1 f # ? x,y ?
P ( x)P w ( y|x)f t ( x , y ) + Q t ( z t ).
286
IIS applies Jensen?s inequality e
P t f t ( x,y ) f # ( x,y ) ( z t f # ( x,y )) ? ? t f t ( x,y ) f # ( x,y ) e z t f # ( x,y ) on (9) to get the approximate function
A
IIS t ( z t ) = ? x,y ?
P ( x)P w ( y|x)f t ( x , y ) e z t f # ( x,y ) ?1 f # ( x,y ) + Q t ( z t ).
SCGIS is a sequential-update method . It replaces f # in GIS with f # t ? max x,y f t ( x , y ). Using z t e t as z in (8), a derivation similar to (10) gives e z t f t ( x,y ) ? f t ( x,y ) f # t e z t f # t + f # t ? f t ( x,y ) f # t .
The approximate function of SCGIS is
A
SCGIS t ( z t ) = e z t f # t ?1 f # t ? x,y ?
P ( x)P w ( y|x)f t ( x , y ) + Q t ( z t ).
We prove the linear convergence of existing IS methods ( proof omitted):
Theorem 1 Assume each subproblem As t ( z t ) is exactly minimized , where s is IIS , GIS , SCGIS , or CD . The sequence { wk } generated by any of these four methods linearly converges . That is , there is a constant ? ? (0, 1) such that
L(w k+1 )? L(w ? ) ? (1??)(L(w k )? L(w ? )),? k , where w ? is the global optimum of (3).
2.3 Solving one-variable subproblems
Without the regularization term , by A ? t ( z t ) = 0, GIS and SCGIS both have a simple closedform solution of the subproblem . With the regularization term , the subproblems no longer have a closedform solution . We discuss the cost of solving subproblems by the Newton method , which iteratively updates z t by z t ? z t ? A s t ? ( z t )/ A s t ?? ( z t ). (11)
Here s indicates an IS or a CD method.
Below we check the calculation of As t ? ( z t ) as the cost of As t ?? ( z t ) is similar . We have
A s t ? ( z t )= ? x,y ?
P ( x)P w ( y|x)f t ( x , y)e z t f s ( x,y ) + Q ? t ( z t ) (12) where f s ( x , y ) ? ? ? ? ? ? f # if s is GIS , f # t if s is SCGIS , f # ( x , y ) if s is IIS.
For CD,
A
CD t ? ( z t )= Q ? t ( z t )+ ? x,y ?
P ( x)P w+z t e t ( y|x)f t ( x , y).
(13)
The main cost is on calculating P w ( y|x ) ? x , y , whenever w is updated . Parallel-update approaches calculate P w ( y|x ) once every n subproblems , but sequential-update methods evaluates P w ( y|x ) after every subproblem . Consider the situation of updating w to w+z t e t . By (1),
Table 1: Time for minimizing A t ( z t ) by the Newton method CD GIS SCGIS IIS 1st Newton direction O(?l ) O(?l ) O(?l ) O(?l)
Each subsequent
Newton direction O ( ? l ) O(1) O(1) O ( ? l ) obtaining P w+z t e t ( y|x ) ? x , y requires expensive
O(#nz ) operations to evaluate S w+z t e t ( x , y ) and
T w+z t e t ( x ) ? x , y . A trick to trade memory for time is to store all S w ( x , y ) and T w ( x),
S w+z t e t ( x , y)=S w ( x , y)e z t f t ( x,y ) ,
T w+z t e t ( x)=T w ( x )+ ? y
S w ( x , y)(e z t f t ( x,y ) ?1).
Since S w+z t e t ( x , y ) = S w ( x , y ) if f t ( x , y ) = 0, this procedure reduces the the O(#nz ) operations to O(#nz/n ) = O(?l ). However , it needs extra spaces to store all S w ( x , y ) and T w ( x).
This trick for updating P w ( y|x ) has been used in SCGIS ( Goodman , 2002). Thus , the first Newton iteration of all methods discussed here takes O(?l ) operations . For each subsequent Newton iteration , CD needs O(?l ) as it calculates P w+z t e t ( y|x ) whenever z t is changed . For
GIS and SCGIS , if ? x,y ?
P ( x)P w ( y|x)f t ( x , y ) is stored at the first Newton iteration , then (12) can be done in O(1) time . For IIS , because f # ( x , y ) of (12) depends on x and y , we cannot store ? x,y ?
P ( x)P w ( y|x)f t ( x , y ) as in GIS and
SCGIS . Hence each Newton direction needs O(?l).
We summarize the cost for solving subproblems in Table 1.
3 Comparison and a New CD Method 3.1 Comparison of IS/CD methods From the above discussion , an IS or a CD method falls into a place between two extreme designs:
A t ( z t ) a loose bound ?
A t ( z t ) a tight bound
Easy to minimize A t ( z t ) Hard to minimizeA t ( z t ) There is a tradeoff between the tightness to bound the function difference and the hardness to solve the subproblem . To check how IS and CD methods fit into this explanation , we obtain relationships of their approximate functions:
A
CD t ( z t ) ? A
SCGIS t ( z t ) ? A
GIS t ( z t ),
A
CD t ( z t ) ? A
IIS t ( z t ) ? A
GIS t ( z t ) ? z t .
(14)
The derivation is omitted . From (14), CD considers more accurate subproblems than SCGIS and GIS . However , to solve each subproblem , from Table 1, CD?s each Newton step takes more time . The same situation occurs in comparing IIS and GIS . Therefore , while a tight A t ( z t ) can problems , the total time may not be less due to the higher cost of each subproblem.
3.2 A Fast CD Method
We develop a CD method which is cheaper in solving each subproblem but still enjoys fast final convergence . This method is modified from Chang et al (2008), a CD approach for linear
SVM . We approximately minimize ACD t ( z ) by applying only one Newton iteration . The Newton direction at z = 0 is now d = ? A
CD t ? (0)/A
CD t ?? (0). (15)
As taking the full Newton direction may not decrease the function value , we need a line search procedure to find ? ? 0 such that z = ? d satisfies the following sufficient decrease condition:
A
CD t ( z)?A
CD t (0) = A
CD t ( z ) ? ? zA
CD t ? (0), (16) where ? is a constant in (0, 1/2). A simple way to find ? is by sequentially checking ? = 1, ?, ? procedure is guaranteed to stop ( proof omitted).
We can further prove that near the optimum two results hold : First , the Newton direction (15) satisfies the sufficient decrease condition (16) with ?=1. Then the cost for each subproblem is O(?l ), similar to that for exactly solving subproblems of GIS or SCGIS . This result is important as otherwise each trial of z = ? d expensively costs O(?l ) for calculating ACD t ( z ). Second , taking one Newton direction of the tighter ACD t ( z t ) reduces the function L(w ) more rapidly than exactly minimizing a loose A t ( z t ) of GIS , IIS or SCGIS . These two results show that the new CD method improves upon the traditional CD by approximately solving subproblems , while still maintains fast convergence.
4 Experiments
We apply Maxent models to part of speech ( POS ) tagging for BROWN corpus ( http://www.nltk.org ) and chunking tasks for CoNLL2000 ( http://www.
cnts.ua.ac.be/conll2000/chunking).
We randomly split the BROWN corpus to 4/5 training and 1/5 testing . Our implementation is built upon OpenNLP ( http://maxent.sourceforge.net).
We implement CD ( the new one in Section 3.2), GIS , SCGIS , and LBFGS for comparisons . We include LBFGS as Malouf (2002) reported that it is better than other approaches including GIS 0 500 1000 1500 2000 10?2 10?1
Training Time ( s)
Re lat ive fu nc tio n va lue di ffe re nc e
CD
SCGIS
GIS
LBFGS ( a ) BROWN 0 50 100 150 200 10?2 10?1
Re lat ive fu nc tio n va lue di ffe re nc e
CD
SCGIS
GIS
LBFGS ( b ) CoNLL2000 0 500 1000 1500 2000
Te sti ng A cc ur ac y
CD
SCGIS
GIS
LBFGS ( c ) BROWN 0 50 100 150 200
Training Time ( s)
F1 m ea su re
CD
SCGIS
GIS
LBFGS ( d ) CoNLL2000
Figure 2: First row : time versus the relative function value difference (17). Second row : time versus testing accuracy/F1. Time is in seconds.
and IIS . We use ?2 = 10, and set ? = 0.5 and ? = 0.001 in (16).
We begin at checking time versus the relative difference of the function value to the optimum:
L(w )? L(w ? )/ L(w ? ). (17)
Results are in the first row of Figure 2. We check in the second row of Figure 2 about testing ac-curacy/F1 versus training time . Among the three IS/CD methods compared , the new CD approach is the fastest . SCGIS comes the second , while GIS is the last . This result is consistent with the tightness of their approximation functions ; see (14). LBFGS has fast final convergence , but it does not perform well in the beginning.
5 Conclusions
In summary , we create a general framework for explaining IS methods . Based on this framework , we develop a new CD method for Maxent . It is more efficient than existing IS methods.
References
K.-W . Chang , C.-J . Hsieh , and C.-J . Lin . 2008. Coordinate descent method for largescale L2-loss linear
SVM . JMLR , 9:1369?1398.
John N . Darroch and Douglas Ratcliff . 1972. Generalized iterative scaling for loglinear models . Ann.
Math . Statist ., 43(5):1470?1480.
Stephen Della Pietra , Vincent Della Pietra , and John Lafferty . 1997. Inducing features of random fields.
IEEE PAMI , 19(4):380?393.
Joshua Goodman . 2002. Sequential conditional generalized iterative scaling . In ACL , pages 9?16.
Robert Malouf . 2002. A comparison of algorithms for maximum entropy parameter estimation . In
CONLL.
288
