Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics , pages 460?469,
Portland , Oregon , June 1924, 2011. c?2011 Association for Computational Linguistics
Prefix Probability
for Probabilistic Synchronous ContextFree Grammars
Mark-Jan Nederhof
School of Computer Science
University of St Andrews
North Haugh , St Andrews , Fife
KY16 9SX
United Kingdom
markjan.nederhof@googlemail.com
Giorgio Satta
Dept . of Information Engineering
University of Padua
via Gradenigo , 6/A
I-35131 Padova
Italy
satta@dei.unipd.it
Abstract
We present a method for the computation of prefix probabilities for synchronous contextfree grammars . Our framework is fairly general and relies on the combination of a simple , novel grammar transformation and standard techniques to bring grammars into normal forms.
1 Introduction
Within the area of statistical machine translation , there has been a growing interest in socalled syntaxbased translation models , that is , models that define mappings between languages through hierarchical sentence structures . Several such statistical models that have been investigated in the literature are based on synchronous rewriting or tree transduction . Probabilistic synchronous contextfree grammars ( PSCFGs ) are one among the most popular examples of such models . PSCFGs subsume several syntaxbased statistical translation models , as for instance the stochastic inversion transduction grammars of Wu (1997), the statistical model used by the Hiero system of Chiang (2007), and systems which extract rules from parsed text , as in Galley et al (2004).
Despite the widespread usage of models related to PSCFGs , our theoretical understanding of this class is quite limited . In contrast to the closely related class of probabilistic contextfree grammars , a syntax model for which several interesting mathematical and statistical properties have been investigated , as for instance by Chi (1999), many theoretical problems are still unsolved for the class of PSCFGs.
This paper considers a parsing problem that is well understood for probabilistic contextfree grammars but that has never been investigated in the context of PSCFGs , viz . the computation of prefix probabilities . In the case of a probabilistic contextfree grammar , this problem is defined as follows . We are asked to compute the probability that a sentence generated by our model starts with a prefix string v given as input . This quantity is defined as the ( possibly infinite ) sum of the probabilities of all strings of the form vw , for any string w over the alphabet of the model . This problem has been studied by Jelinek and Lafferty (1991) and by Stolcke (1995).
Prefix probabilities can be used to compute probability distributions for the next word or part-of-speech.
This has applications in incremental processing of text or speech from left to right ; see again ( Jelinek and Lafferty , 1991). Prefix probabilities can also be exploited in speech understanding systems to score partial hypotheses in beam search ( Corazza et al , 1991).
This paper investigates the problem of computing prefix probabilities for PSCFGs . In this context , a pair of strings v1 and v2 is given as input , and we are asked to compute the probability that any string in the source language starting with prefix v1 is translated into any string in the target language starting with prefix v2. This probability is more precisely defined as the sum of the probabilities of translation pairs of the form [ v1w1, v2w2], for any strings w1 and w2.
A special case of prefix probability for PSCFGs is the right prefix probability . This is defined as the probability that some ( complete ) input string w in the source language is translated into a string in the target language starting with an input prefix v.
460
Prefix probabilities and right prefix probabilities for PSCFGs can be exploited to compute probability distributions for the next word or part-of-speech in left-to-right incremental translation , essentially in the same way as described by Jelinek and Lafferty (1991) for probabilistic contextfree grammars , as discussed later in this paper.
Our solution to the problem of computing prefix probabilities is formulated in quite different terms from the solutions by Jelinek and Lafferty (1991) and by Stolcke (1995) for probabilistic contextfree grammars . In this paper we reduce the computation of prefix probabilities for PSCFGs to the computation of inside probabilities under the same model.
Computation of inside probabilities for PSCFGs is a wellknown problem that can be solved using off-the-shelf algorithms that extend basic parsing algorithms . Our reduction is a novel grammar transformation , and the proof of correctness proceeds by fairly conventional techniques from formal language theory , relying on the correctness of standard methods for the computation of inside probabilities for PSCFG . This contrasts with the techniques proposed by Jelinek and Lafferty (1991) and by Stolcke (1995), which are extensions of parsing algorithms for probabilistic contextfree grammars , and require considerably more involved proofs of correctness.
Our method for computing the prefix probabilities for PSCFGs runs in exponential time , since that is the running time of existing methods for computing the inside probabilities for PSCFGs . It is unlikely this can be improved , because the recognition problem for PSCFG is NP-complete , as established by Satta and Peserico (2005), and there is a straightforward reduction from the recognition problem for PSCFGs to the problem of computing the prefix probabilities for PSCFGs.
2 Definitions
In this section we introduce basic definitions related to synchronous contextfree grammars and their probabilistic extension ; our notation follows
Satta and Peserico (2005).
Let N and ? be sets of nonterminal and terminal symbols , respectively . In what follows we need to represent bijections between the occurrences of nonterminals in two strings over N ??. This is realized by annotating nonterminals with indices from an infinite set . We define I(N ) = { A t | A ? N , t ? N } and VI = I(N ) ? ?. For a string ? ? V ? I , we write index (?) to denote the set of all indices that appear in symbols in ?.
Two strings ?1, ?2 ? V ? I are synchronous if each index from N occurs at most once in ?1 and at most once in ?2, and index(?1) = index(?2). Therefore ?1, ?2 have the general form : ?1 = u10A t1 11 u11A t2 12 u12 ? ? ? u1r?1A tr 1r u1r ?2 = u20A tpi(1) 21 u21A tpi(2) 22 u22 ? ? ? u2r?1A tpi(r ) 2r u2r where r ? 0, u1i , u2i ? ??, A ti 1i , A tpi(i ) 2i ? I(N ), ti 6= tj for i 6= j , and pi is a permutation of the set {1, . . . , r}.
A synchronous contextfree grammar ( SCFG ) is a tuple G = ( N,?,P , S ), where N and ? are finite , disjoint sets of nonterminal and terminal symbols , respectively , S ? N is the start symbol and P is a finite set of synchronous rules . Each synchronous rule has the form s : [ A1 ? ?1, A2 ? ?2], where A1, A2 ? N and where ?1, ?2 ? V ? I are synchronous strings . The symbol s is the label of the rule , and each rule is uniquely identified by its label . For technical reasons , we allow the existence of multiple rules that are identical apart from their labels . We refer to A1 ? ?1 and A2 ? ?2, respectively , as the left and right components of rule s.
Example 1 The following synchronous rules implicitly define a SCFG : s1 : [ S ? A 1 B 2 , S ? B 2 A 1 ] s2 : [ A ? aA 1 b , A ? bA 1 a ] s3 : [ A ? ab , A ? ba ] s4 : [ B ? cB 1 d , B ? dB 1 c ] s5 : [ B ? cd , B ? dc ] 2 In each step of the derivation process of a SCFG G , two nonterminals with the same index in a pair of synchronous strings are rewritten by a synchronous rule . This is done in such a way that the result is once more a pair of synchronous strings . An auxiliary notion is that of reindexing , which is an injective function f fromN toN . We extend f to VI by letting f(A t ) = A f(t ) for A t ? I(N ) and f(a ) = a for a ? ?. We also extend f to strings in V ? I by
X ? VI and ? ? V ? I .
Let ?1, ?2 be synchronous strings in V ? I . The derive relation [?1, ?2] ? G [?1, ?2] holds whenever there exist an index t in index(?1) = index(?2), a synchronous rule s : [ A1 ? ?1, A2 ? ?2] in P and some reindexing f such that : ( i ) index(f(?1)) ? ( index(?1) \ { t }) = ?; ( ii ) ?1 = ??1A t 1 ? ?? 1 , ?2 = ? ? 2A t 2 ? ?? 2 ; and ( iii ) ?1 = ??1f(?1)? ?? 1 , ?2 = ? ? 2f(?2)? ?? 2 .
We also write [?1, ?2] ? sG [?1, ?2] to explicitly indicate that the derive relation holds through rule s.
Note that ?1, ?2 above are guaranteed to be synchronous strings , because ?1 and ?2 are synchronous strings and because of ( i ) above . Note also that , for a given pair [?1, ?2] of synchronous strings , an index t and a rule s , there may be infinitely many choices of reindexing f such that the above constraints are satisfied . In this paper we will not further specify the choice of f .
We say the pair [ A1, A2] of nonterminals is linked ( in G ) if there is a rule of the form s : [ A1 ? ?1, A2 ? ?2]. The set of linked nonterminal pairs is denoted by N [2].
A derivation is a sequence ? = s1s2 ? ? ? sd of synchronous rules si ? P with d ? 0 (? = ? for d = 0) such that [?1i?1, ?2i?1] ? si
G [?1i , ?2i ] for every i with 1 ? i ? d and synchronous strings [?1i , ?2i ] with 0 ? i ? d . Throughout this paper , we always implicitly assume some canonical form for derivations in G , by demanding for instance that each step rewrites a pair of nonterminal occurrences of which the first is leftmost in the left component.
When we want to focus on the specific synchronous strings being derived , we also write derivations in the form [?10, ?20] ?? G [?1d , ?2d ], and we write [?10, ?20] ?? G [?1d , ?2d ] when ? is not further specified . The translation generated by a SCFG G is defined as:
T ( G ) = {[ w1, w2] | [ S 1 , S 1 ] ?? G [ w1, w2], w1, w2 ? ? ?} For w1, w2 ? ??, we write D(G , [ w1, w2]) to denote the set of all ( canonical ) derivations ? such that [ S 1 , S 1 ] ?? G [ w1, w2].
Analogously to standard terminology for contextfree grammars , we call a SCFG reduced if every rule occurs in at least one derivation ? ? D(G , [ w1, w2]), for some w1, w2 ? ??. We assume without loss of generality that the start symbol S does not occur in the righthand side of either component of any rule.
Example 2 Consider the SCFG G from example 1.
The following is a canonical derivation in G , since it is always the leftmost nonterminal occurrence in the left component that is involved in a derivation step : [ S 1 , S 1 ] ? G [ A 1 B 2 , B 2 A 1 ] ? G [ aA 3 bB 2 , B 2 bA 3 a ] ? G [ aaA 4 bbB 2 , B 2 bbA 4 aa ] ? G [ aaabbbB 2 , B 2 bbbaaa ] ? G [ aaabbbcB 5 d , dB 5 cbbbaaa ] ? G [ aaabbbccdd , ddccbbbaaa ] It is not difficult to see that the generated translation is T ( G ) = {[ apbpcqdq , dqcqbpap ] | p , q ? 1}. 2 The size of a synchronous rule s : [ A1 ? ?1, A2 ? ?2], is defined as | s | = | A1?1A2?2|. The size of G is defined as | G | = ? s?P | s|.
A probabilistic SCFG ( PSCFG ) is a pair G = ( G , pG ) where G = ( N,?,P , S ) is a SCFG and pG is a function from P to real numbers in [0, 1]. We say that G is proper if for each pair [ A1, A2] ? N [2] we have : ? s:[A1??1, A2??2] pG(s ) = 1 Intuitively , properness ensures that where a pair of nonterminals in two synchronous strings can be rewritten , there is a probability distribution over the applicable rules.
For a ( canonical ) derivation ? = s1s2 ? ? ? sd , we define pG (?) = ? d i=1 pG(si ). For w1, w2 ? ? ?, we also define : pG([w1, w2]) = ? ?? D(G,[w1,w2]) pG (?) (1) We say a PSCFG is consistent if pG defines a probability distribution over the translation , or formally : ? w1,w2 pG([w1, w2]) = 1 then also : ? w1,w2???, ?? P ? s.t . [ A 11 , A ?
G[w1, w2] pG (?) = 1 for every pair [ A1, A2] ? N [2]. The proof is identical to that of the corresponding fact for probabilistic contextfree grammars.
3 Effective PSCFG parsing
If w = a1 ? ? ? an then the expression w[i , j ], with 0 ? i ? j ? n , denotes the substring ai+1 ? ? ? aj ( if i = j then w[i , j ] = ?). In this section , we assume the input is the pair [ w1, w2] of terminal strings.
The task of a recognizer for SCFG G is to decide whether [ w1, w2] ? T ( G).
We present a general algorithm for solving the above problem in terms of the specification of a deduction system , following Shieber et al (1995). The items that are constructed by the system have the form [ m1, A1,m?1; m2, A2,m ? 2], where [ A1, A2] ?
N [2] and where m1, m?1, m2, m ? 2 are nonnegative integers such that 0 ? m1 ? m?1 ? | w1| and 0 ? m2 ? m?2 ? | w2|. Such an item can be derived by the deduction system if and only if : [ A 11 , A ?
G [ w1[m1,m ? 1], w2[m2,m ? 2]]
The deduction system has one inference rule , shown in figure 1. One of its side conditions has a synchronous rule in P of the form : s : [ A1 ? u10A t1 11 u11 ? ? ? u1r?1A tr 1r u1r,
A2 ? u20A tpi(1) 21 u21 ? ? ? u2r?1A tpi(r ) 2r u2r ] (2) Observe that , in the righthand side of the two rule components above , nonterminals A1i and A2pi?1(i ), 1 ? i ? r , have both the same index . More precisely , A1i has index ti and A2pi?1(i ) has index ti ? with i ? = pi(pi?1(i )) = i . Thus the nonterminals in each antecedent item in figure 1 form a linked pair.
We now turn to a computational analysis of the above algorithm . In the inference rule in figure 1 there are 2(r + 1) variables that can be bound to positions in w1, and as many that can be bound to positions in w2. However , the side conditions imply m?ij = mij + | uij |, for i ? {1, 2} and 0 ? j ? r , and therefore the number of free variables is only r + 1 for each component . By standard complexity analysis of deduction systems , for example following McAllester (2002), the time complexity of a straightforward implementation of the recognition algorithm is O(|P | ? | w1| rmax+1 ? | w2| rmax+1), where rmax is the maximum number of righthand side nonterminals in either component of a synchronous rule . The algorithm therefore runs in exponential time , when the grammar G is considered as part of the input . Such computational behavior seems unavoidable , since the recognition problem for SCFG is NP-complete , as reported by Satta and Peserico (2005). See also Gildea and Stefankovic (2007) and Hopkins and Langmead (2010) for further analysis of the upper bound above.
The recognition algorithm above can easily be turned into a parsing algorithm by letting an implementation keep track of which items were derived from which other items , as instantiations of the consequent and the antecedents , respectively , of the inference rule in figure 1.
A probabilistic parsing algorithm that computes pG([w1, w2]), defined in (1), can also be obtained from the recognition algorithm above , by associating each item with a probability . To explain the basic idea , let us first assume that each item can be inferred in finitely many ways by the inference rule in figure 1. Each instantiation of the inference rule should be associated with a term that is computed by multiplying the probability of the involved rule s and the product of all probabilities previously associated with the instantiations of the antecedents.
The probability associated with an item is then computed as the sum of each term resulting from some instantiation of an inference rule deriving that item . This is a generalization to PSCFG of the inside algorithm defined for probabilistic contextfree grammars ( Manning and Schu?tze , 1999), and we can show that the probability associated with item [0, S , | w1| ; 0, S , | w2|] provides the desired value pG([w1, w2]). We refer to the procedure sketched above as the inside algorithm for PSCFGs.
However , this simple procedure fails if there are cyclic dependencies , whereby the derivation of an item involves a proper subderivation of the same item . Cyclic dependencies can be excluded if it can ? 2pi?1(1)?1, A2pi?1(1),m2pi?1(1)] ...
[m?1r?1, A1r,m1r ; m ? 2pi?1(r)?1, A2pi?1(r),m2pi?1(r )] [ m10, A1,m?1r ; m20, A2,m ? 2r ] ? ??????? ??????? s:[A1 ? u10A t1 11 u11 ? ? ? u1r?1A tr 1r u1r,
A2 ? u20A tpi(1) 21 u21 ? ? ? u2r?1A tpi(r ) 2r u2r ] ? P , w1[m10,m?10] = u10, ...
w1[m1r,m?1r ] = u1r , w2[m20,m?20] = u20, ...
w2[m2r,m?2r ] = u2r
Figure 1: SCFG recognition , by a deduction system consisting of a single inference rule.
be guaranteed that , in figure 1, m?1r ? m10 is greater than m1j ? m?1j?1 for each j (1 ? j ? r ), or m?2r ? m20 is greater than m2j ? m ? 2j?1 for each j (1 ? j ? r).
Consider again a synchronous rule s of the form in (2). We say s is an epsilon rule if r = 0 and u10 = u20 = . We say s is a unit rule if r = 1 and u10 = u11 = u20 = u21 = . Similarly to contextfree grammars , absence of epsilon rules and unit rules guarantees that there are no cyclic dependencies between items and in this case the inside algorithm correctly computes pG([w1, w2]).
Epsilon rules can be eliminated from PSCFGs by a grammar transformation that is very similar to the transformation eliminating epsilon rules from a probabilistic contextfree grammar ( Abney et al , 1999). This is sketched in what follows . We first compute the set of all nullable linked pairs of nonterminals of the underlying SCFG , that is , the set of all [ A1, A2] ? N [2] such that [ A ?
G [?, ?].
This can be done in linear time O(|G |) using essentially the same algorithm that identifies nullable nonterminals in a contextfree grammar , as presented for instance by Sippu and Soisalon-Soininen (1988).
Next , we identify all occurrences of nullable pairs [ A1, A2] in the righthand side components of a rule s , such that A1 and A2 have the same index . For every possible choice of a subset U of these occurrences , we add to our grammar a new rule sU constructed by omitting all of the nullable occurrences in U . The probability of sU is computed as the probability of s multiplied by terms of the form : ? ? s.t . [ A 11 , A ?
G [?, ?] pG (?) (3) for every pair [ A1, A2] in U . After adding these extra rules , which in effect circumvents the use of epsilon-generating subderivations , we can safely remove all epsilon rules , with the only exception of a possible rule of the form [ S ? , S ? ]. The translation and the associated probability distribution in the resulting grammar will be the same as those in the source grammar.
One problem with the above construction is that we have to create new synchronous rules sU for each possible choice of subset U . In the worst case , this may result in an exponential blowup of the source grammar . In the case of contextfree grammars , this is usually circumvented by casting the rules in binary form prior to epsilon rule elimination . However , this is not possible in our case , since SCFGs do not allow normal forms with a constant bound on the length of the righthand side of each component . This follows from a result due to Aho and Ullman (1969) for a formalism called syntax directed translation schemata , which is a syntactic variant of
SCFGs.
An additional complication with our construction is that finding any of the values in (3) may involve solving a system of nonlinear equations , similarly to the case of probabilistic contextfree grammars ; see again Abney et al (1999), and Stolcke (1995).
Approximate solution of such systems might take exponential time , as pointed out by Kiefer et al (2007).
Notwithstanding the worst cases mentioned above , there is a special case that can be easily dealt with . Assume that , for each nullable pair [ A1, A2] in
G we have that [ A 11 , A ?
G [ w1, w2] does not hold for any w1 and w2 with w1 6= ? or w2 6= ?.
Then each of the values in (3) is guaranteed to be 1, and furthermore we can remove the instances of the nullable pairs in the source rule s all at the same time . This means that the overall construction of mented in linear time | G |. It is this special case that we will encounter in section 4.
After elimination of epsilon rules , one can eliminate unit rules . We define Cunit([A1, A2], [ B1, B2]) as the sum of the probabilities of all derivations deriving [ B1, B2] from [ A1, A2] with arbitrary indices , or more precisely : ? ?? P ? s.t . ? t?N , [ A 11 , A ?
G[B t 1 , B t 2 ] pG(?)
Note that [ A1, A2] may be equal to [ B1, B2] and ? may be ?, in which case Cunit([A1, A2], [ B1, B2]) is at least 1, but it may be larger if there are unit rules.
Therefore Cunit([A1, A2], [ B1, B2]) should not be seen as a probability.
Consider a pair [ A1, A2] ? N [2] and let al unit rules with lefthand sides A1 and A2 be : s1 : [ A1, A2] ? [ A t1 11 , A t1 21 ] ...
sm : [ A1, A2] ? [ A tm 1m , A tm 2m ]
The values ofCunit (?, ?) are related by the following:
Cunit([A1, A2], [ B1, B2]) = ?([ A1, A2] = [ B1, B2]) + ? i pG(si ) ? C unit([A1i , A2i ], [ B1, B2]) where ?([ A1, A2] = [ B1, B2]) is defined to be 1 if [ A1, A2] = [ B1, B2] and 0 otherwise . This forms a system of linear equations in the unknown variables Cunit (?, ?). Such a system can be solved in polynomial time in the number of variables , for example using Gaussian elimination.
The elimination of unit rules starts with adding a rule s ? : [ A1 ? ?1, A2 ? ?2] for each non-unit rule s : [ B1 ? ?1, B2 ? ?2] and pair [ A1, A2] such that Cunit([A1, A2], [ B1, B2]) > 0.
We assign to the new rule s ? the probability pG(s ) ? Cunit([A1, A2], [ B1, B2]). The unit rules can now be removed from the grammar . Again , in the resulting grammar the translation and the associated probability distribution will be the same as those in the source grammar . The new grammar has size O(|G|2), where G is the input grammar . The time complexity is dominated by the computation of the solution of the linear system of equations . This computation takes cubic time in the number of variables.
The number of variables in this case is O(|G|2), which makes the running time O(|G|6).
4 Prefix probabilities
The joint prefix probability pprefixG ([ v1, v2]) of a pair [ v1, v2] of terminal strings is the sum of the probabilities of all pairs of strings that have v1 and v2, respectively , as their prefixes . Formally : pprefixG ([ v1, v2]) = ? w1,w2??? pG([v1w1, v2w2]) At first sight , it is not clear this quantity can be effectively computed , as it involves a sum over infinitely many choices of w1 and w2. However , analogously to the case of contextfree prefix probabilities ( Jelinek and Lafferty , 1991), we can isolate two parts in the computation . One part involves infinite sums , which are independent of the input strings v1 and v2, and can be precomputed by solving a system of linear equations . The second part does rely on v1 and v2, and involves the actual evaluation of pprefixG ([ v1, v2]). This second part can be realized effectively , on the basis of the precomputed values from the first part.
In order to keep the presentation simple , and to allow for simple proofs of correctness , we solve the problem in a modular fashion . First , we present a transformation from a PSCFG
G = ( G , pG ), with G = ( N,?,P , S ), to a
PSCFG Gprefix = ( Gprefix , pGprefix ), with Gprefix = ( Nprefix , ?, Pprefix , S ?). The latter grammar derives all possible pairs [ v1, v2] such that [ v1w1, v2w2] can be derived from G , for some w1 and w2. Moreover , pGprefix([v1, v2]) = p prefix
G ([ v1, v2]), as will be verified later.
Computing pGprefix([v1, v2]) directly using a generic probabilistic parsing algorithm for PSCFGs is difficult , due to the presence of epsilon rules and unit rules . The next step will be to transform Gprefix into a third grammar G?prefix by eliminating epsilon rules and unit rules from the underlying SCFG , and preserving the probability distribution over pairs of strings . Using G?prefix one can then effectively PSCFGs , such as the inside algorithm discussed in section 3, in order to compute the desired prefix probabilities for the source PSCFG G.
For each nonterminal A in the source SCFG G , the grammar Gprefix contains three nonterminals , namely A itself , A ? and A ?. The meaning of A remains unchanged , whereas A ? is intended to generate a string that is a suffix of a known prefix v1 or v2. Nonterminals A ? generate only the empty string , and are used to simulate the generation by G of infixes of the unknown suffix w1 or w2. The two lefthand sides of a synchronous rule in Gprefix can contain different combinations of nonterminals of the forms A , A ?, or A ?. The start symbol of Gprefix is S ?. The structure of the rules from the source grammar is largely retained , except that some terminal symbols are omitted in order to obtain the intended interpretation of A ? and A?.
In more detail , let us consider a synchronous rule s : [ A1 ? ?1, A2 ? ?2] from the source grammar , where for i ? {1, 2} we have : ? i = ui0A ti1 i1 ui1 ? ? ? uir?1A tir ir uir The transformed grammar then contains a large number of rules , each of which is of the form s ? : [ B1 ? ?1, B2 ? ?2], where Bi ? ? i is of one of three forms , namely Ai ? ? i , A ? i ? ? ? i or A?i ? ? ? i , where ? ? i and ? ? i are explained below.
The choices for i = 1 and for i = 2 are independent , so that we can have 3 ? 3 = 9 kinds of synchronous rules , to be further subdivided in what follows . A unique label s ? is produced for each new rule , and the probability of each new rule equals that of s.
The righthand side ?? i is constructed by omitting all terminals and propagating downwards the ? superscript , resulting in : ?? i = A ? ti1 i1 ? ? ? A ? tir ir It is more difficult to define ?? i . In fact , there can be a number of choices for ?? i and , for each choice , the transformed grammar contains an instance of the synchronous rule s ? : [ B1 ? ?1, B2 ? ?2] as defined above . The reason why different choices need to be considered is because the boundary between the known prefix vi and the unknown suffix wi can occur at different positions , either within a terminal string uij or else further down in a subderivation involving Aij . In the first case , we have for some j (0 ? j ? r ): ?? i = ui0A ti1 i1 ui1A ti2 i2 ? ? ? uij?1A tij ij u ? ijA ? tij+1 ij+1 A ? tij+2 ij+2 ? ? ? A ? tir ir where u?ij is a choice of a prefix of uij . In words , the known prefix ends after u?ij and , thereafter , no more terminals are generated . We demand that u?ij must not be the empty string , unless Ai = S and j = 0. The reason for this restriction is that we want to avoid an overlap with the second case . In this second case , we have for some j (1 ? j ? r ): ?? i = ui0A ti1 i1 ui1A ti2 i2 ? ? ? uij?1A ? tij ij A ? tij+1 ij+1 A ? tij+2 ij+2 ? ? ? A ? tir ir Here the known prefix of the input ends within a subderivation involving Aij , and further to the right no more terminals are generated.
Example 3 Consider the synchronous rule s : [ A ? aB 1 bc C 2 d,D ? ef E 2 F 1 ]. The first component of a synchronous rule derived from this can be one of the following eight:
A ? ? B ? 1 C ? 2
A ? ? aB ? 1 C ? 2
A ? ? aB ? 1 C ? 2
A ? ? aB 1 b C ? 2
A ? ? aB 1 bc C ? 2
A ? ? aB 1 bc C ? 2
A ? ? aB 1 bc C 2 d
A ? aB 1 bc C 2 d
The second component can be one of the following six:
D ? ? E ? 2 F ? 1
D ? ? eE ? 2 F ? 1
D ? ? ef E ? 2 F ? 1
D ? ? ef E ? 2 F ? 1
D ? ? ef E 2 F ? 1
D ? ef E 2 F 1 6 = 48 synchronous rules derived from s . 2 For each synchronous rule s , the above grammar transformation produces O(|s |) left rule components and as many right rule components . This means the number of new synchronous rules is O(|s|2), and the size of each such rule is O(|s |). If we sum O(|s|3) for every rule s we obtain a time and space complexity of O(|G|3).
We now investigate formal properties of our grammar transformation , in order to relate it to prefix probabilities . We define the relation ` between P and Pprefix such that s ` s ? if and only if s ? was obtained from s by the transformation described above.
This is extended in a natural way to derivations , such that s1 ? ? ? sd ` s?1 ? ? ? s ? d ? if and only if d = d ? and si ` s?i for each i (1 ? i ? d).
The formal relation between G and Gprefix is revealed by the following two lemmas.
Lemma 1 For each v1, v2, w1, w2 ? ?? and ? ? P ? such that [ S , S ] ?? G [ v1w1, v2w2], there is a unique ?? ? P ? prefix such that [ S ?, S ?] ?? ?
Gprefix [ v1, v2] and ? ` ??. 2
Lemma 2 For each v1, v2 ? ?? and derivation ?? ? P ? prefix such that [ S ?, S ?] ?? ?
Gprefix [ v1, v2], there is a unique ? ? P ? and unique w1, w2 ? ?? such that [ S , S ] ?? G [ v1w1, v2w2] and ? ` ? ?. 2 The only nontrivial issue in the proof of Lemma 1 is the uniqueness of ??. This follows from the observation that the length of v1 in v1w1 uniquely determines how occurrences of left components of rules in P found in ? are mapped to occurrences of left components of rules in Pprefix found in ??. The same applies to the length of v2 in v2w2 and the right components.
Lemma 2 is easy to prove as the structure of the transformation ensures that the terminals that are in rules from P but not in the corresponding rules from Pprefix occur at the end of a string v1 ( and v2) to form the longer string v1w1 ( and v2w2, respectively).
The transformation also ensures that s ` s ? implies pG(s ) = pGprefix(s ?). Therefore ? ` ?? implies pG (?) = pGprefix (? ?). By this and Lemmas 1 and 2 we may conclude:
Theorem 1 pGprefix([v1, v2]) = p prefix
G ([ v1, v2]). 2
Because of the introduction of rules with lefthand sides of the formA ? in both the left and right components of synchronous rules , it is not straightforward to do effective probabilistic parsing with the grammar Gprefix . We can however apply the transformations from section 3 to eliminate epsilon rules and thereafter eliminate unit rules , in a way that leaves the derived string pairs and their probabilities unchanged.
The simplest case is when the source grammar G is reduced , proper and consistent , and has no epsilon rules . The only nullable pairs of nonterminals in Gprefix will then be of the form [ A?1, A ? 2]. Consider such a pair [ A?1, A ? 2]. Because of reduction , properness and consistency of G we have : ? w1,w2???, ?? P ? s.t.
[A 11 , A ?
G[w1, w2] pG (?) = 1
Because of the structure of the grammar transformation by which Gprefix was obtained from G , we also have : ? ?? P ? s.t.
[A ? 11 , A ? 1 2 ]? ?
Gprefix [?, ?] pGprefix (?) = 1
Therefore pairs of occurrences of A?1 and A ? 2 with the same index in synchronous rules of Gprefix can be systematically removed without affecting the probability of the resulting rule , as outlined in section 3. Thereafter , unit rules can be removed to allow parsing by the inside algorithm for PSCFGs.
Following the computational analyses for all of the constructions presented in section 3, and for the grammar transformation discussed in this section , we can conclude that the running time of the proposed algorithm for the computation of prefix probabilities is dominated by the running time of the inside algorithm , which in the worst case is exponential in | G |. This result is not unexpected , as already pointed out in the introduction , since the recognition problem for PSCFGs is NP-complete , as established by Satta and Peserico (2005), and there is a straightforward reduction from the recognition problem for PSCFGs to the problem of computing the prefix probabilities for PSCFGs.
467
One should add that , in real world machine translation applications , it has been observed that recognition ( and computation of inside probabilities ) for SCFGs can typically be carried out in low-degree polynomial time , and the worst cases mentioned above are not observed with real data . Further discussion on this issue is due to Zhang et al (2006).
5 Discussion
We have shown that the computation of joint prefix probabilities for PSCFGs can be reduced to the computation of inside probabilities for the same model.
Our reduction relies on a novel grammar transformation , followed by elimination of epsilon rules and unit rules.
Next to the joint prefix probability , we can also consider the right prefix probability , which is defined by : pr?prefixG ([ v1, v2]) = ? w pG([v1, v2w ]) In words , the entire left string is given , along with a prefix of the right string , and the task is to sum the probabilities of all string pairs for different suffixes following the given right prefix . This can be computed as a special case of the joint prefix probability.
Concretely , one can extend the input and the grammar by introducing an end-of-sentence marker $.
Let G ? be the underlying SCFG grammar after the extension . Then : pr?prefixG ([ v1, v2]) = p prefix
G ? ([ v1$, v2])
Prefix probabilities and right prefix probabilities for PSCFGs can be exploited to compute probability distributions for the next word or part-of-speech in left-to-right incremental translation of speech , or alternatively as a predictive tool in applications of interactive machine translation , of the kind described by Foster et al (2002). We provide some technical details here , generalizing to PSCFGs the approach by Jelinek and Lafferty (1991).
Let G = ( G , pG ) be a PSCFG , with ? the alphabet of terminal symbols . We are interested in the probability that the next terminal in the target translation is a ? ?, after having processed a prefix v1 of the source sentence and having produced a prefix v2 of the target translation . This can be computed as : pr?wordG ( a | [ v1, v2]) = pprefixG ([ v1, v2a ]) pprefixG ([ v1, v2]) Two considerations are relevant when applying the above formula in practice . First , the computation of pprefixG ([ v1, v2a ]) need not be computed from scratch if pprefixG ([ v1, v2]) has been computed already . Because of the tabular nature of the inside algorithm , one can extend the table for pprefixG ([ v1, v2]) by adding new entries to obtain the table for pprefixG ([ v1, v2a ]). The same holds for the computation of pprefixG ([ v1b , v2]).
Secondly , the computation of pprefixG ([ v1, v2a ]) for all possible a ? ? may be impractical . However , one may also compute the probability that the next part-of-speech in the target translation isA . This can be realised by adding a rule s ? : [ B ? b , A ? cA ] for each rule s : [ B ? b , A ? a ] from the source grammar , where A is a nonterminal representing a part-of-speech and cA is a ( pre-)terminal specific to A . The probability of s ? is the same as that of s . If G ? is the underlying SCFG after adding such rules , then the required value is pprefixG ? ([ v1, v2 cA]).
One variant of the definitions presented in this paper is the notion of infix probability , which is useful in island-driven speech translation . Here we are interested in the probability that any string in the source language with infix v1 is translated into any string in the target language with infix v2. However , just as infix probabilities are difficult to compute for probabilistic contextfree grammars ( Corazza et al ., 1991; Nederhof and Satta , 2008) so ( joint ) infix probabilities are difficult to compute for PSCFGs.
The problem lies in the possibility that a given infix may occur more than once in a string in the language . The computation of infix probabilities can be reduced to that of solving nonlinear systems of equations , which can be approximated using for instance Newton?s algorithm . However , such a system of equations is built from the input strings , which entails that the computational effort of solving the system primarily affects parse time rather than parser-generation time.
468
References
S . Abney , D . McAllester , and F . Pereira . 1999. Relating probabilistic grammars and automata . In 37th Annual Meeting of the Association for Computational Linguistics , Proceedings of the Conference , pages 542?549,
Maryland , USA , June.
A.V . Aho and J.D . Ullman . 1969. Syntax directed translations and the pushdown assembler . Journal of Computer and System Sciences , 3:37?56.
Z . Chi . 1999. Statistical properties of probabilistic contextfree grammars . Computational Linguistics , 25(1):131?160.
D . Chiang . 2007. Hierarchical phrasebased translation.
Computational Linguistics , 33(2):201?228.
A . Corazza , R . De Mori , R . Gretter , and G . Satta.
1991. Computation of probabilities for an island-driven parser . IEEE Transactions on Pattern Analysis and Machine Intelligence , 13(9):936?950.
G . Foster , P . Langlais , and G . Lapalme . 2002. User-friendly text prediction for translators . In Conference on Empirical Methods in Natural Language Processing , pages 148?155, University of Pennsylvania,
Philadelphia , PA , USA , July.
M . Galley , M . Hopkins , K . Knight , and D . Marcu . 2004.
What?s in a translation rule ? In HLTNAACL 2004, Proceedings of the Main Conference , Boston , Massachusetts , USA , May.
D . Gildea and D . Stefankovic . 2007. Worst-case synchronous grammar rules . In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics , Proceedings of the Main Conference , pages 147? 154, Rochester , New York , USA , April.
M . Hopkins and G . Langmead . 2010. SCFG decoding without binarization . In Conference on Empirical Methods in Natural Language Processing , Proceedings of the Conference , pages 646?655, October.
F . Jelinek and J.D . Lafferty . 1991. Computation of the probability of initial substring generation by stochastic contextfree grammars . Computational Linguistics , 17(3):315?323.
S . Kiefer , M . Luttenberger , and J . Esparza . 2007. On the convergence of Newton?s method for monotone systems of polynomial equations . In Proceedings of the 39th ACM Symposium on Theory of Computing , pages 217?266.
C.D . Manning and H . Schu?tze . 1999. Foundations of Statistical Natural Language Processing . MIT Press.
D . McAllester . 2002. On the complexity analysis of static analyses . Journal of the ACM , 49(4):512?537.
M.-J . Nederhof and G . Satta . 2008. Computing partition functions of PCFGs . Research on Language and
Computation , 6(2):139?162.
G . Satta and E . Peserico . 2005. Some computational complexity results for synchronous contextfree grammars . In Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing , pages 803?810.
S.M . Shieber , Y . Schabes , and F.C.N . Pereira . 1995.
Principles and implementation of deductive parsing.
Journal of Logic Programming , 24:3?36.
S . Sippu and E . Soisalon-Soininen . 1988. Parsing Theory , Vol . I : Languages and Parsing , volume 15 of EATCS Monographs on Theoretical Computer Science . Springer-Verlag.
A . Stolcke . 1995. An efficient probabilistic contextfree parsing algorithm that computes prefix probabilities.
Computational Linguistics , 21(2):167?201.
D . Wu . 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora . Computational Linguistics , 23(3):377?404.
Hao Zhang , Liang Huang , Daniel Gildea , and Kevin Knight . 2006. Synchronous binarization for machine translation . In Proceedings of the Human Language Technology Conference of the NAACL , Main Conference , pages 256?263, New York , USA , June.
469
