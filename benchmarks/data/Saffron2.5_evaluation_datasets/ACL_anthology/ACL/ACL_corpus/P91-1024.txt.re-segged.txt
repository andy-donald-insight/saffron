EXPERIMENTS ANDPROSPECTSOF
EXAMPLE-BASED MACHINE TRANS LATION
Eiichiro SUMITA *

Hitoshi HDA
ATR Interpreting Telephony Research Laboratories
Sanpeidani , Inuidani , Seikacho
Souraku-gun , Kyoto 619-02, JAPAN

EBMT ( Example-Based Machine Translation ) is proposed . EBMT retrieve similar examples ( pairs of source phrases , sentences , or texts and their translations ) from ad~t . hase of examples , adapting the examples to translate a new input . EBMT has the following features : ( 1 ) It is easily upgraded simply by inputting appropriate examples to the database  ;   ( 2 ) It assigns a reliability fact cr to the translation result  ;   ( 3 ) It is a coelerated effectively by both indexing and parallel computing  ; (4) It is robust because of best match reasoning ; ~ d(5) It well utilizes translator expertise . A prototype system has been implemented to deal with a difficult translation problem for conventional Rule-Based Machine Translation  ( RBMT )  , i . e . , translating Japanese noun phrases of the form " N ~ no  N2" into English . The system has achieved about a 78% success rate on average . This paper explains the basic idea of EBMT , illustrates the experiment in detail , explains the broad applicability of EBMT to several difficul translation problems for RBMT and discusses the advantages of integrating EBMT with 

1 INTRODUCTION
Machine Translation requires h and cmt ~ and complicated largescale knowledge  ( Nirenburg 1987 )  . 
Conventional machine translation systems use rules as the knowledge  . This framework is called Rule-Based Machine Translation  ( RBMT )  . It is difficul to scale up from a toy program to a practical system because of the problem of building such a lurge-scale rule base  . It is also difficul to improve translation performance because the effect of adding a new rule is hard to anticipate  , and because translation using a large-scule rule -based system is time-consuming  . Moreover , it is difficult to make use of situational or domain-specific information for translation  . 
their translations ) has been implemented as the knowledge ( Nagao 1984 ; Sumit and Tsutsumi 1988 ; Sato and Nagao 1989 ; Sadler 1989 a ; Sumita et al 1990 a , b ) . The translation mechanism retrieves similar examples from the database  , adapting the examples to Wanslate the new source text  . This framework is called Example-Based
Machine Translation ( EBMT).
This paper focuses on ATR's linguistic database of spoken Japanese with English translations  . The corpus contains conversations about international conferencer gistration  ( Ogura et al 1989 )  . Results of this study indicate that EBMT is a breakthrough in MT technology  . 
Our pilot EBMT system translates Japanese noun phrases of the form  '~1 xno N2" into English noun phrases . Abouta 78% success rate on average has been achieved in the experiment  , which is considered to outperform RBMT . This rate cm be improved as discussed below . 
Section 2 explains the basic idea of EBMT.
Section 3 discusses the broad applicability of EBMT and the advantages of integrating it with RBMT  . 
Sections 4 and 5 give a rationale for section 3, i . e . , section 4 illustrates the experiment of translating noun phrases of the form " Ntno  N2" in detail , and section 5 studies other phenomena through actual dam from our corpus  . Section 6 concludes this paper with detailed comparisons between RBMT and 

2 BASICIDE AOFE BMT 2 . 1 BASICFLOW In this section , the basic idea of EBMT , which is general and applicable to many phenomena dealt with by machine translation  , is shown . 
In order to conquer these problems in machine translation  , a database of examples ( pairs of source phrases , sentences , or texts and * Currently with Kyoto University
Figure 1 shows the basic flow of EBMT using translation of " kireru "\[ cut/besharp \]  . From hereon , the literal English translations are bracketed . 
(1) and (2) me examples ( pairs of
Japanese sentences and their English
Examples similar to the Japanese input sentence are retrieved in the following manner  . Syntactically , the input is similar to Japanese sentences ( 1 ) and ( 2 )  . However , semantically , "kachou"\[chief \] is far from " houchou "\[ kitchen knife\]  . But , "kachou"\[chief\]is semantically similar to " kanojo "\ [ she\]in that both are people  . In other words , the input is similar to example sentence (2) . By mimicking the similar example (2) , we finally get " The chief is sharp " . 
Although it is possible to obtain the same result by a word selection rule using fme-tuned semantic restriction  , otetha translation here is obtained by retrieving similar examples to the input  . 
? Example Database ( data for " kireru'\[cut/besharp\] )   ( 1 ) houchouwa klrsru->The kitchenknife cuts . 
(2) kano jowaki reru -> She Is sharp.
? Input kachou wakl reruo >? ? Retrieval of similar examples  ( Syntax ) Input = ( 1 )  , (2) ( Semantics ) kachou /== houehoukachou , = kanojo ( Total ) Input == ( 2 ) ? OUt0Ut -> The chief Is~har D , 
Figure I Mimicking Similar Examples 2 . 2 DISTANCE Retrieving similar examples to the input is done by measuring the distance of the input to each of examples  . The smaller a distance is , the more similar the example is to the input . To define the best distance metric is a problem of EBMT not yet completely solved  . However , one possible definition is shown in section 4 . 2 . 2 . 
From similar examples retrieved , EBMT generates the most likely translation with a reliability factor based on distance and frequency  . If there is no similar example within the given threshold  , EBMT tells the user that it cannot translate the input  . 
3 BROAD APPLICABILITY AND
INTEGRATION 3.1 BROAD APPLICABILITY
EBMT is applicable to many linguistic phenomena that are regarded as difficul to translate in conventional RBMT  . Some are wellknown among researchers of natural language processing and others have recently been given a great deal of attention  . 
When one of the following conditions holds true for a linguistic phenomenon  , RBMT is less suitable than EBMT . 
( Ca ) Translation rule formation is difficult.
(Cb ) The general rule cannot accurately describe phenomena because it represents a special case  , e . g . , idioms . 
(Cc ) Translation cannot be made in a composition alway from target words  ( Nagao 1984 ; Nitta 1986 ; Sadler 1989b ) . 
This is a list ( not exhaustive ) of phenomena in JE translation that are suitable for EBMT : ? optional cases with a case particle  ( "- de " , " ~ hi " ,  . . . )? subordinate conjunction ("- ba -" , " ~ nagara -" , " ~ tara -" ,  . . . , "- baai ~", . . . ) ? noun phrases of the form '~1 no N2" ? sentences of the form " N~waN 2 da "? sentences lacking the main verb ( eg . sentences of the form " ~ o-negaishimasu " ) ? fragmental expressions Chai " , " sou-desu " , " wakarimashita " ,  . . . ) ( Furus et al .  1990 ) ? modality represented by the sentence ending C -taino desuga "  , " ~ seteitadakimasu " ,   . . . ) ( Furus et al .  1990 ) ? simple sentences ( Sato and Nagao 1989 ) This paper discusses a detailed experiment for " N ~ no  N2" in section 4 and prospects for other phenomena ,  "  N1 wa N2 da " and " ~ o-negaishimasu " in section 5  . 
Similar phenomena in other language pairs can be found  . For example , in Spanish to English translation , the Spanish preposition " de " , with its broad usage like Japanese " no " , is also effectively Iranslated by EBMT . Likewise , in German to English translation , the German complex noun is also effectively translated by EBMT  . 
3.2 INTEGRATION
It is not yet clear whether EBMT can or shouldeal with the whole process of translation  . We assume that here are many kinds of phenomena . 
Some are suitable for EBMT , while others are suitable for RBMT . 
Integrating EBMT with RBMT is expected to be useful  . It would be more acceptable for users if RBMT were first introduced as a base system  , and then incrementally have its translation performance improved by attaching EBMT components  . This is in the line with the proposal in Nagao ( 1984 )  . Subsequently , we proposed a practical method of integration i 4 EBMTFOR " Nx no Nz " 4  . 1 THE PROBLEM " N ~ no N2" is a common Japanese noun phrase form . " no " in the " Nt no Nz " is a Japanese adnominal particle  . There are other variants , including " deno " , " karano " , " madeno " and so on . 
Roughly speaking , Japanese noun phrases of the form " N~no N2" correspond to English noun phrases of the form " N2 of N : " as shown in the examples at the top of Figure  2  . 
JapaneseEnglish youkanogo go the afternoon of the  8th kaiginomokuteki the object of the conference .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 
kaigino sankary outhe application fee for the conf  . 
? the application fee of the conf.
kyou to deno kaigi the conf , in Kyoto . '/ the conf . of Kyoto is shukannokyu kaaweek's holiday ? the holiday of a weekmittsu no hoteruthree hotels * hotels of three Figure  2 Variations in Translation of " N1 no N2" However ,  "  N2 of Nt " does not always provide a natural translation as shown in the lower examples in Figure  2  . Some translations are too broad in meaning to interpret  , others axe almost ungrammatical . For example , the fourth one , " the conference of Kyoto " , could be misconstrued as " the conference about Kyoto "  , and the last one , " hotels of three " , is not English . Natural translations often require prepositions other than " of "  , or no preposition at all . In only about one-fifth of " N~no N2" occurrences in our domain ,  "  N2 of Nt " would be the most appropriate English translation  . We cannot use any particular preposition as an effecdvede  . fault value . 
No rules for selecting the most appropriate translation for " N~no  N2" have yet been found . In other words , the condition ( Ca ) in section 3 . 1 holds . 
Selecting the translation for '~1~ no N2" is still an important and complicated problem in JE translation  . 
In contrast with the preceding research analyzing " NI no  N2"   ( Shimazu et al 1987 ; Hirai and Kitahashi 1986) , deep semantic analysis is avoided because it is assumed that translations appropriate for given domain can be obtained using domain-specific examples  ( pairs of source md target expressions )  . EBMT has the advantage that it can directly return a translation by adapting examples without reasoning through along chain of rules  . 
4.2 IMPLEMENTATION 4.2.1 OVER VIEW
The EBMT system consists of two databases : an example database and a thesaurus  ; and also three translation modules : analysis , example-based transfer , and generation ( Figure 3) . 
Examples ( pairs of source phrases and their translations ) are extracted from ATR's linguistic database of spoken Japanese with English translations  . The corpus contains conversations about registering for an international conference  ( Ogura 1989 )  . 

Database (1) Analysis I(2) Example-Based


I(3) Generation I
Figure 3 System Configuration
The thesaurus is used in calculating the semantic distance between the content words in the input and those in the examples  . It is composed of a hierarchical structure in accordance with the thesaurus of everyday Japanese written by Ohno and Hamanishi  ( 1984 )  . 
Analysisk you to deno kaigi
Example-Based Transferd JapaneseEnglish 0 . 4 touk you denotaizaithestay in Tokyo 0 . 4 honkon denotaizaithestay in Hongkong 0 . 4 touk you denogo-taizaithestay in Tokyo 1 . 0 oo saka no kaigi the conf . in Osaka1 . 0 to uk you no kaigi the conf . in Tokyo
Generation the conf . in Kyoto
Figure 4 Translation Procedure
Figure 4 illustrates the translation procedure with an actual sample  . First , morphological nalysis is performed for the input phrase  , "kyouto\[Kyoto\]denokaigi\[conference\]" . In this case , syntactical are retrieved from the database . The top five similar examples are shown . Note that he top three examples have the same distance and that hey are all translated with " in "  . Third , using this rationale , EBMT generates " the conference in Kyoto " . 
4.2.2 DISTANCE CAL CULATION
The distance metric used when retrieving examples i essential n d is explained hem in detail  . 
we suppose that the input and examples ( I , E ) in the d~tAl ~ ase ~ r ~ ted in the same data structure  , i . e . , the list of words ' syntactic and semantic attribute values  ( refeaxed to as and I ~ , E  ~) for each phrase . 
The attributes of the curren target , " NtnoN2" , 8 ~ as follows : 1 ) for the nouns " NI " and " N2": the lexical subcategory of the noun , the existence of a prefix or suffix , and its semantic code in the thesaurus ;  2 ) for the adnominal particle " no ": the kinds of variants  , " deno " , " karano " , " madeno " and so on . Here , for simplicity , only the semantic code and the kind of adnominal a = e considered  . 
Distances ae calculated using the following two expressions  ( Sumita et al 1990 a , b ) : (1) d(I , E)=d(li , E i ) " w i i (2) w i = , ~// ~ . ( freq . of t . p . when E i = l i ) 2 t . p . 
The attribute distance , d(li , E . ~ end the weight of attribute , w ~ are explained in the following sections . Each Iranslation pattern ( t . p . ) is abstracted from an example md is stored with the example in the exampled ~ m has e\[see Figure  6\]  . 
( a ) ATTRIBUTEDISTANCE
For the attribute of the adnominal particle " no " , the distance is 0 or 1 depending on whether or not they match exactly , for example , d("deno" , " deno ") = 0 and d("deno " , no ") = 1 . 
For semantic attributes , however , the distance varies between 0 and 1 . Semantic distance d ( 0 < d < 1 ) is determined by the Most Specific Common Abstract lon  ( MSCA )   ( Kolodner and Riesbeck 1989 ) obtained from the thesaurus abstraction hierarchy  . When the thesaurus i(n+l ) layered ,   ( k/n ) is assigned to the concepts in the kth layer from the bottom  . For example , as shown with the broken line in Figure 5 , the MSCAC kaigi''\[conference\] , " taizai"\[stay\] ) is " koudou "\[ actions \] and the distance is2/3  . Of course , 0 is assigned when the MSCA is the bottom class , for instance , MSCACk youto "\[ Kyoto\] , " toukyou"\[Tokyo\])="timei"\[placc\] , or when nouns are identical (
MSCA(N , N ) for any N).
Thesaurus Root\[actions\] ( 1/3 ) our aloming sgoings \] set sumeitions \] ( o ) 
Ikaisetsu\[commentary\]// .  ,  .  "  , \\ [ taizai II hatchaku I\[stays\]II\[ arrivals &\[ meetings lJ 
JJJ departures ' , II : o) , ilii kaigitaizaitouchaku\[conference\]\[ stay \]\[ arrive\] 
Figure 5Thesaurus ( portion )   ( b ) WEIGHTOFATTRIBUTE
The weight of the attribute is the degree to which the attribute influences the selection of the translation pattern  ( t . p . ) . We adopt the expression ( 2 ) used by Stanfill and Waltz ( 1986 ) for memory-based reasoning , to implement the intuition . 
t.p . freq.
BinA 12/27
AB4/27
B from A2/27
BA 2/27
BtoA1/27 ( El = timei)\[place/t.p.freq.
BinA313(E2=deno)\[in/t.p.freq.
B9/24
AB9/24
BinA 2/24
A's B1124
BonA1/24 ( E3= soudan)\[meetings\]
Figure 6 Weight of the ith attribute " deno " aze translated with the same preposition  , " in " . This implies that when El = " deno " ,   E2 is an attribute which heavily influences the selection of the translation pattern  . In contrast to this , the translation patterns of examples whose E1 = " timei "\[ place\] , = evaried . This implies that when E1--" timei"\[place\] , E ~ is an attribute which is less influential on the selection of the translation pattern  . 
According to the expression (2) , weights for attributes , E  ~ , E2 and E3 me as follows : W1 = , ~(12/27) 2+(4127 ) 2+ . . . +(1/27)2 = 0 . 49
W2=,,~(3/3)2 = 1 . 0 w3=,~(9/24) 2 + (9124) 2+ .   . .+(1/24) 2 ,= 0 . 54  ( C ) TOTALDISTANCE The distance between the input and the first example shown in Figure  4 is calculated using the weights in section 4  . 2 . 2Co ) , attribute distances as explained in section 4 . 2 . 2  ( a ) and expression ( 1 ) at the beginning of section 4 . 2 . 2 . 
d ( "kyouto'\[Kyoto\]"deno'\[in\]"kaigi'\[ conference \]  , " toukyou'\[Tokyo\]"deno'\[in\]"taizai'\ [ stay \]  )   , = d('kyouto ' , ' toukyou ") * 0 . 49 + d('deno ",' de no ') * 1 . 0 + d('kaigi ", " taizai ') * 0 . 54 = 0"0 . 49+0"1 . 0+2/3"0 . 54 = 0 . 4 4 . 3 EXPERIMENTS The current number of words in the corpus is about  300  , 000 and the number of examples i2 , 550 . 
The collection of examples from another domain is in progress  . 
4.3.1 JACKKNIFETEST
In ~ to roughly estimate translation performance , a jackknife experiment was conducted . 
We partitioned the example database (2 , 550) in groups of one hundred , then used one set as input ( 100 ) and translated them with the rest as an example database  ( 2 , 450) . This was repeated 25 times . 
Figure 7 shows that the average success rate is 78%  , the minimum 70% and the maximum 89% \[ see section 4  . 3 . 4\] . 
It is difficult to fairly compare this result with the success rate of the existing MT system  . 
However , it is believed that current conventional systems can at best output the most common translation pattern  , for example , " B of A " , as the default . In this case , the average success rate may only be about 20% . 
success (%) MAXIMUM (89%) 60 AVERAGE (78%)
MINIMUM (70%) 0 II 11121 test number
Figure 7 Result of Jackknife Test 4 . 3 . 2 SUCCES SRATEPER
NUMBER OF EXAMPLES
Figure 8 shows the relationship between the success rate and the number of examples  . Of the twentyfive cases in the previous jackknife test  , three are shown : maximum , average , and minimum . This graph shows that , in general , the more examples we have , the better the quality \[ see section 4 . 3 . 4\] . 
success (%) MAXIMUM 80t/J ~~~ ~ , ~'~'~ ' ' - - / - -  . ,, ~ sAVERAGE 70 .  _  .  -  .   .  - - -  . ''' --- no . of examples ( x 100 ) Figure 8 Success Rate per No . of Examples

Figure 9 shows the relationship between the success rate and the distance between the input and the most similar examples retrieved  . 
This graph shows that in general , the smaller the distance , the better the quality . 
In other words , EBMT assigns the distance between the input and the retrieved examples us a reliability factor  . 
SUCCESS 0 . 9r 1592/17900 . 8 0 . 7 0 . 6 0 . 5 0 . 4 0 . 3 0 . 2 0 . 1 ? = ? 35167 951162 ? 74/148 8 /24 7/14 3/56
E?IIII 0 0.2 0.4 0.6 0.81 distance
Figure 9 Success Rate per Distance 4 . 3 . 4 SUCCESSES AND FAILURES The following represents successful results :  ( 1 ) the noun phrase " kyouto-eki\[Kyoto-station \] noo-mise\[store\]"is wansta_!ed according to the translation pattern " BatA " while the similar noun phrase  , "kyouto\[Kyoto\]noshiten\[branch\]"is translated according to the translation pattern  "13 in A " ;   ( 2 ) the noun phrase of the form " N ~ no hou " is translated according to the translation pattern " A "  , in other words , the second noun is omitted . 
We ~ enow studying the results carefully ~ d are striving to improve the success rate  . 
( a ) About half of the failures are caused by a lack of similar examples  . They are easily solved by adding appropriate examples  . 
Co ) The rest are caused by the existence of similar examples :  ( 1 ) equivalent but different examples are retrieved , for instance , those of the form , "B of A " and " AB " for " rolm-gatsu\[June\] no futsu-ka\[second\]"  . This is one of the main reasons the graphs ( Figure 7 and 8 ) show an up-and-down pattern . They can be regarded as a correct translation or the distance calculation may be changed to handle the problem  ;   ( 2 ) Because the current distance calculation is inadequate  , dissimilar examples are retrieved . 
5 PHENOMENAOTHERTHAN " N1 no Nz "
This section studies the phenomena ,  "  N1 wa N2 da " and " - o-negaishimasu " with the same corpus used in the previou section  . 
5.1 " Nxwa N~da "
A sentence of the form " N\]wa N2 da " is called a " da " sentence . Here " N ' and ' ~2" ~ e nouns , " wa " is a topical particle , and " da " is a kind of verb which , roughly speaking , is the English copula " be " . 
The correspondences between " da " sentences and the English equivalents are exemplified in Figure  10  . Mainly , " N~waN2da " corresvonds to ' ~ be Nz " like ( a-l )  -  ( a -4 )  . 
However , sentences like ( b )  -  ( e ) cannot be translated according to the translation pattern  , N ~ be N2" . In example ( d) , there is no Japanese counterpart of " payment should be made-by "  . The English sentence has a modal , passive voice , the verb make , and its object , payment , while the Japanese sentence has no such correspondences  . This translation cannot be made in a composition alway from the target words which ale selected from a normal dictionary  . It is difficult of ormulate rules for the translation and to explain how the translation is made  . The conditions ( Ca ) and ( Co ) in section 3 . 1 hold true . 
Conventional pproaches lead to the understanding of " da " sentence suing contextual ndex wa -linguistic information  . However , many translations exist that are the result of human translators ' understanding  . Translation can be made by mimicking such similar examples  . 
Example(e ) is special , i . e . , idiomatic . The condition ( Co ) in section 3 . 1 holds . 
( a ) NIbeN=watashi\[I\]ko chira\[this\]denwa -bango\[tel-no  . \] sanka-hi\[fee\](b)N , costN = yokoushuu\[proc . \]
N , jonson\[Johnson\]ji mukyoku\[secretariat\] 06-951-0866106-951-0866\]   85  , 000-en\[85 , 000 yen\]3 0 , 000-en\[30 , 000 yen\](c ) for N , , the fee is N = kigyou\[companies \] 85 , 000-en\[85 , 000 yen\] ( d ) payment should be made by N = hi you\[fee\] ginnkou-furikomi\[bank-transfer \]  ( e ) the conference will end on N = saishuu-bi\[final day \]  10qatsu12nichi\[Oct   . 12th \]
Figure 10 Examples of " N1 wa N2 da "
The distribution of N\] and N2 in the examples given to 2-tuples of nouns , ( N1 , N2) . N2s of ( a-4), (13) and ( c ) are similar , i . e . , both mean " prices " . However N~s are not similar to each other . Nlsof(a-4) and ( d ) ~ esimilar , i . e . , both mean " fee " . However , the N2s ~ e not similar to each other . Thus , EBMT is applicable . 
5.2 " ~ o-negaishimasu "
Figure 11 exemplifies the conespondences between sentences of the form " ~ o-negaishimasu " and the English equivalents  . 
( a ) may I speak to N ( b ) please give meN ( c ) please pay by N ( d ) yes , please ( e ) thank You
Figure 11 jimukyoku\[secretariat\]oo-negaishlmasugo-ju usyo\[address\]o  . . . 
genkin\[cash\]de ...


Examples of " ~ o-negaishimasu "
Translations in examples ( b ) and ( c ) are possible by finding substitutes in Japanese for give me and pay by  , respectively . The conditions ( Ca ) and ( Cc ) in section 3 . 1 hold . Usually , this kind of supplement is done by contextual nalysis  . However , the connection between the missing elements and the noun in the examples i strong enough to reuse  , because it is the product of a combination of translator expertise and domain specific restriction  . 
Examples ( a ) , ( d ) and ( e ) are idiomatic expressions . 
The condition ( Cb ) holds . The distribution of the noun and the particle in the examples of our corpus varies for each case in the same way as in the " da " sentence  . EBMT is applicable . 
6 CONCLUDING REMARKS
Example-Based Machine Translation ( EBMT ) has been proposed . EBMT retrieves similar examples ( pairs of source and target expressions )  , adapting them to translate a new source text . 
The feasibility of EBMT has been shown by implementing a system which translates Japanese noun phrases of the form  '~1 no N2" into English noun phrases . The result of the experiment was encouraging . Bnaed applicability of EBMT was shown by studying the d~m from the text corpus  . The advantages of integrating EBMT with RBMT were also discussed  . The system has been written in Common Lisp , and is running on a Genera 7 . 2
Symbolics Lisp Machine at ATR.
(1) IMPROVEMENT
The more elaborate the RBMT becomes , the less expandable it is . Considerably complex rules concerning semantics , context , and the real world , are required in machine translation . This is the notorious AI bottleneck : not only is it difficulto add a new rule to the database of rules that are mutually dependent  , but it is also difficul to build such a rule database itself  . Moreover , computation using this huge and complex rule database is so slow that it forces a developer to abandon efforts to improve the system  . RBMT is not easily upgraded . 
However , EBMT has no rules , and the use of examples i relatively localized . Improvement is effected simply by inputting appropriate examples into the database  . EBMT is easily upgraded , which the experiment in section 4 . 3 . 2 has shown : the more examples we have , the better the quality . 
(2) RELIABILITY FACTOR
One of the main reasons users dislike RBMT systems i the so called " poisoned cookie " problem  . 
RBMT has no device to compute the reliability of the result  . In other words , users of RBMT cannot trust any RBMT translation , because it may be wrong without any such indication from system  . 
Consider the case where all translation processes have been completed successfully  , et , the result is incorrect . 
In EBMT , a reliability factor is assigned to the translation result according to the distance between the input and the similar examples found \[ see the experiment in section  4  . 3 . 3\] . In addition to this , retrieved examples that are similar to the input convince users that the translation is accurate  . 
(3) TRANS LATION SPEED
RBMT translates slowly in general because it is really a largescale rule-based system  , which consists of analysis , transfer , and generation modules using syntactic rules , semantic restrictions , tructural transfer rules , word selections , generation rules , and so on . For example , the Mu system has about 2 , 0 00 rewriting and word selection rules for about 70  , 000 lexical items ( Nagao et al 1986) . 
As recently pointed out ( Furus et al .  1990) , conventional RBMT systems have been biased toward syntactic  , semantic , and contextual nalysis , which consumes considerable computing time . 
However , such deep analysis is not always necessary or useful for translation  . 
In contrast with this , deep semantic analysis is avoided in EBMT because it is assumed that translations appropriate for given domain can be obtained using domain -specific examples  ( pairs of source and target expressions )  . EBMT directly returns a translation without reasoning through along chain of rules\[see There is fear that retrieval from a largescale example database will prove too slow  . However , it can be accelerated effectively by both indexing  ( Sumit and Tsutsumi 1988 ) and parallel computing ( Sumit and Iida 1991 )  . 
These processes multiply acceleration . Consequently , the computation of EBMT is acceptably efficient . 
(4) ROBUSTNESS
RBMT works on exact-match reasoning . It fails to translate when it has no knowledge that matches the input exactly  . 
However , EBMT works on best match reasoning . It intrinsically translates in a fail-safe way \[ see sections  2 and 4\]  . 
(5) TRANS LATORSEX PERTISE
Formulating linguistic rules for RBMT is a difficult job and requires a linguistically trained staff  . 
Moreover , linguistics does not deal with all phenomena occurring in real text  ( Nagao 1988 )  . 
However , examples necessary for EBMT ~ Ee easy to obtain because a large number of texts and their translations are available  . These are realization of translator expertise , which deals with all real phenomena . Moreover , a selectronic publishing increases , more and more texts will be machine-readable ( Sadler 1989b )  . 
EBMT is intrinsically biased toward a sublanguage : strictly speaking  , toward an example database . This is a good feature because it provides a way of automatically tuning itself to a sublanguage  . 

Furuse ~ O . , Sumita , E . and Iida , H .   1990 " A Method for Realizing Transfer-Driven Machine Translation "  , Reprint of W (~ L80-8 , IPSJ , ( in Japanese) . 
Hirei , M . and Kitahashi , T .  1986 , " A Semantic Classification of Noun Modifications in Japanese Sentences and Their Analysis "  , Reprint of WGNL 58-1 , IPSJ , ( in Japanese) . 
Kolodner , J . and Riesbeek , C . 1989 " Case-Based Reasoning " , Tutorial Textbook of 11th UCAI . 
Nagao , M .   1984 " A Framework of a Mechanical Translation Between Japanese and English by Analogy Principle "  , in A . Elithom and R . Banerji ( ed . ), Artificial and Human Intelligence,
North-Holland , 173-180.
Nagao , M . , Tsujii , J . , Nakamura , J .   1986 " Machine Translation from Japanese into English " , 
Proceedings of the IFI~.F ., 74, 7.
Nagao , M . (chair )   1988 " Language Engineering : The Real Bottleneck of Natural Language Processing "  , Proceedings of the 12th International Conference on
Computational Linguistics.
Nirenburg , S . 1987 Machine Translation , Cambridge
University Press , 350.
Nitta , Y .   1986 ' Idiosyncratic Gap : A Tough Problem to Structure-bound Machine Translation "  , Proceedings of the 11th International Conference on
Computational Linguistics , 107-111.
Ogura , K . , Hashimoto , K . , and Morimoto , T .   1989 " Object-Oriented User Interface for Linguistic Database "  , Proceedings of Working Conference on Data and Knowledge Base Integration  , University of
Keele , England.
Ohno , S . and Hamanishi , M . 1984 Ruigo-Shin-Jiten,
Kadokawa , 932, ( in Japanese).
Sadler , V .   1989a '' Translating with a Simulated Bilingual Knowledge Bank  ( BKB ) " , BSO/Research . 
Sadler . V . 1989 b Working with Analogical Semantics,
Foris Publications , 256.
Sato , S . and Nagao , M . 1989 " MemoryBased Translation " , Reprint of WGNL70-9 , IPSJ , ( in

Sato , S . and Nagao , M . 1990 " Toward MemoryBased Translation " , Proceedings of the 13th International Conference on Computational Linguistics  . 
Shimazu , A . , Naito , S . , and Nomura , H .   1987 " Semantic Structure Analysis of Japanese Noun Phrases with Adnominal Particles "  , Proceedings of the 25th Annual Meeting of the Association for
Computational Linguistics , 123-130.
Stanf'dl , C . and Waltz , D . 1986 ' Toward MemoryBased
Reasoning ", CACM , 29-12, 1213-1228.
Sumita , E . and Tsutsumi , Y .   1988 " A Translation Aid System Using Flexible Text Retrieval Based on Syntax-Matching "  , Proceedings of The Second International Conference on Theoretical and Methodological Issues in Machine Translation of 
Natural Languages , CMU , Pittsburgh.
Sumita , E . , Iida , H . and Kohyama , H .   1990a ' ~ l ' ranslating with Examples : A New Approach to Machine Translation "  , Proceedings of The Third International Conference on Theoretical and Methodological Issues in Machine Translation of 
Natural Languages , Texas , 203-212.
Sumita , E . /ida , H . and Kohyama , H .   1990b " Example-based Approach in Machine Translation " , Proceedings of lnfoJapan " 90 , Part 2:65-72 . 
Sumita , E . and Iida , H .   1991 " Acceleration of Example-Based Machine Translation "  , ( manuscript ) . 

