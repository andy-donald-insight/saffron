An Experiment in Hybrid Dictionary
and Statistical Sentence Alignment
Nigel Collier , Kenji Ono and Ho and Hideki Hirakawa
Communication and Information Systems Laboratories
Research and Development Center , Toshiba Corporation
1 Komukai Toshiba-cho , Kawasaki-shi , Kanagawa 210-85 S2 , Japan
nigel , ono , hirakawa@eel , rdc . toshiba , co . jp

The task of aligning sentences in parallel corpora of two languages has been well studied using pure statistical or linguistic models  . We developed a linguistic method based on lexical matching with a bilingual dictionary and two statistical methods based on sentence length ratios and sentence offset probabilities  . This paper seeks to further our knowledge of the alignment ask by comparing the performance of the alignment models when used separately and together  , i . e . as a hybrid system . Our results show that for our English-Japanese corpus of newspaper articles  , the hybrid system using lexical matching and sentence lng thratios outperforms the pure methods  . 
1 Introduction
There have been many approaches proposed to solve the problem of aligning corresponding sentences in parallel corpora  . With a few notable exceptions however , much of this work has focussed on either corpora containing European language pairs or clean -parallel corpora where there is little reformatting  . In our work we have focussed on developing a method for robust matching of English-Japanese sentences  , based primarily on lexical matching . The method combine statistical information from byte length ratios  . We show in this paper that this hybrid model is more effective than its constituent parts used separately  . 
The task of sentence alignment is a critical first step in many automatic applications involving the analysis of bilingual texts such as extraction of bilin-gum vocabulary  , extraction of translation templates , word sense disambiguation , word and phrase alignment , and extraction of parameters for statistical translation models  . Many software products which aid human translators now contain sentence alignment tools as an aid to speeding up editing and terminology searching  . 
Various methods have been developed for sentence alignment which we can categorise as either lexical such as  ( Chen ,  1993) , based on a largescale bilingual lexicon ; statistical such as ( Brown et al , 1991) ( Church , 1993) ( Gale and Church , 1903) ( Kay and RSsheheisen ,  1993) , based on distributional regularities of words or byte-length ratios and possibly inducing a bilingua lexicon as a byproduct  , or hybrid such as ( Utsuro et al , 1994) ( Wu ,  1994) , based on some combination of the other two . Neither of the pure approaches i entirely satisfactory for the following reasons : ? Text volume limits the usefulness of statistical approaches  . We would often like to be able to align small amounts of text  , or texts from various domains which do not share the same statistical properties  . 
? Bilingual dictionary coverage limitations mean that we will often encounter problems establishing a correspondence in non-general domains  . 
? Dictionary-based approaches are founded on an assumption of lexicul correspondence btween language pairs  . We cannot always rely on this for non-cognate language pairs  , such as English and Japanese . 
? Texts are often heavily reformatted in translation  , so we cannot assume that the corpus will be clean  , i . e . contain many one-to-one sentence mappings . In this case statistical methods which rely on structure correspondence such as byte-length ratios may not perform well  . 
These factors suggest that some hybrid method may give us the best combination of coverage and accuracy when we have a variety of text domains  , text sizes and language pairs . In this paper we seek to fill a gap in our understanding and to show how the various components of the hybrid method influence the quality of sentence alignment for Japanese and English newspaper articles  . 
2 Bilingual Sentence Alignment
The task of sentence alignment is to match corresponding sentences in a text from One language to sentences in a translation of that text in another language  . Of particular interest to us is the application to Asian language pairs  . Previous studies such as ( Fung and Wu ,  1994 ) have commented pairs using alphabetic haracters have not addressed important issues which occur with European-Asian language pairs  . For example , the language pairs are unlikely to be cognates , and they may place sentence boundaries at different points in the text  . It has also been suggested by ( Wu ,  1994 ) that sentence length ratio correlations may arise partly out of historic cognate-based relationships between Indo-European languages  . Methods which perform well for Indo-European language pairs have therefore been found to be less effective for non-Indo-European language pairs  . 
In our experiments the languages we use are English  ( source ) and Japanese ( translation )  . Although in our corpus ( described below ) we observe that , in general , sentences correspond one-to-one we must also consider multiple sentence correspondences a well as one-to-zero correspondences  . These cases are summarised below . 
1.1:1 The sentences match one-to-one.
2 . l : n One English sentence matches to more than one Japanese sentence  . 
3 . m : l More than one English sentence matches ot one Japanese sentence  . 
4 . m : n More than one English sentence matches to more than one Japanese sentence  . 
5 .   m:0 The English sentence/s have no corresponding Japanese sentence  . 
6 .   0:n The Japanese sentence/s have no corresponding English sentence  . 
In the case of l:n , m : l and m : n correspondences , translation has involved some reformatting and the meaning correspondence is no longer solely at the sentence level  . Ideally we would like smaller units of text to match because it is easier later onto establish word alignment correspondences  . In the worst case of multiple correspondence , the translation is spread across multiple nonconsecutives ntences  . 
3 Corpus
Our primarily motivation is knowledge acquisition for machine translation and consequently we are interested to acquire vocabulary and other bilingual knowledge which will be useful for users of such systems  . Recently there has been a move towards Internet page translation and we consider that one interesting domain for users is international news  . 
The bilingual corpus we use in our experiments i made from Reuter news articles which were translated by the Gakken translation agency from English into Japanese  1   . The translations are quite literal and the contents cover international news for I The corpus was generously made available to us by special arrangement with Gakken the period February  1995 to December 1996  . We currently have over 20 , 000 articles ( approximately 47Mb ) . From this corpus we randomly chose 50 article pairs and aligned them by hand using a human bilingual checker to form a judgement set  . The judgement set consists of 380 English sentences and 453 Japanese sentences . On average ach English article has 8 lines and each Japanese article 9 lines . 
The articles themselves form a boundary within which to align constituent sentences  . The corpus is quite well behaved . We observe many 1:1 correspondences , but also a large proportion of 1:2 and 1:3 correspondences a well as reorderings . Omis-sions seem to be quite rare , so we didn't seem anym : 0 or 0: n correspondences . 
An example news article is shown in Figure 1 which highlights several interesting points . Although the news article texts are clean and in machine-tractable format we still found that it was a significant challenge to reliably identify sentence boundaries  . A simple illustration of this is shown by the first Japanese line  J1 which usually corresponds to the first two English lines  E1 and E2  . This is a result of our general-purpo sentence segmentation algorithm which has difficulty separating the Japanese title from the first sentence  . 
Sentences usually corresponded linearly in our corpus  , with few reorderings , so the major challenge was to identify multiple correspondences and zero correspondences  . We can see an example of a zero correspondence as  E5 has no translation in the Japanese text . Al : n correspondence is shown by E7 aligning to both J5 and J6  . 
4 Alignment Models
In our investigation we examined the performance of three different matching models  ( lexical matching , byte-length ratios and offset probabilities ) . The basic models incorporate dynamic programming to find the least cost alignment path over the set of English and Japanese sentences  . Cost being determined by the model's cores . The alignment space includes all possible combinations of multiple matches up to and including  3:3 alignments . The basic models are now outlined below . 
4 . 1 Mode l 1: Lexical vector matching The lexical approach is perhaps the most robust for aligning texts in cognate language pairs  , or where there is a large amount of reformatting in translation  . It has also been shown to be particularly successful within the vector space model in multilingual information retrieval tasks  , e . g . ( Collier et al , 1998a ) , ( Collier et al , 1998b ) , for aligning texts in non-cognate languages at the article level  . 
The major limitation with lexical matching is clearly the assumption of lexical correspondence -   E2  . TAIPEI , Feb 9 ( Reuter ) -Taiwan's ruling Nationalist Party saida struggle to succeed Deng Xiaoping as China's most powerful man may have already begun  . 
E3 . " Once Deng Xiaoping dies , a hightier powers truggle among the Chinese communists is inevitable  , " a Nationalist Party report said . 
E4 . China and Taiwan have been rivals since the Nationalists lost the Chinese civil war in  1949 and fled to Taiwan . 
E5 . Both Beijing and Taipei sometimes portray each other in an unfavourable light  . 
E6 . The report said that the position of Deng's chosen successor  , President3iang Zemin , may have been subtly undermined of late . 
E7 . It based its opinion on the fact that two heavy weight political figures have recently used the phrase the " solid central collective leadership and its core " instead of the accepted " collective leadership centred on Jiang Zemin " to describe the current leadership structure  . 
E8 . " Such a sensitive statement should not be an unintentional mistake  . . . 
E9 . Does this mean the powers truggle has gradually surfaced while Deng Xiaoping is still alive ?  , " said the report , distributed to journalists . 
El0 . " At least the information sends awarning signal that the ' core of Jiang'has encountered some subtle changes  , " it added . 
31 . ~' ~ l~l ~ . ~l ~: ~, ~ P\[~:,~ . -,i~-~~'a~t . ~l~j~'~ . "~/~:i~ . fl~ . ~:/t'~'H:\]~\['~913~-I'9--\]~'~'~J2 . ~l ~ : , ~ ~ . ~6t : i ~ . ~L,/~_@~?~"e , rl-e) . , j , ~~, ~, ~- ~ . ~ . ~ e,~,~@~,,J3 . q~l~-~i'~t ~ ,  1~7)"  , 1949 ~ l:-q~I~l ~ , ~ e ) ~ l :- I ~( , ~' ~ I :-~-9~A ~ , ti ~ ~ lz~b , 5 oJ s . ~? ~ I~:,~~t2 . ,,L ~, ~ L~?~-e,~t ? ~ ~_~," ~:~- ~ J6 . .: h . ~ el : t . " i ~- : v,~t : ? . 5 q~:~l ~ J "~~, ~' 5~z~t~h . " ? ~ I : : o Figure 1: Example English-Japanese news article pair which is particularly weak for English and Asian language pairs where structural and semantic differences mean that transfer often occurs at a level above the lexicon  . This is a motivation for incorporating statistics into the alignment process  , but in the initial stage we wanted to treat pure lexical matching as our baseline performance  . 
We translated each Japanese sentence into English using dictionary term lookup  . Each Japanese content word was assigned a list of possible English translations and these were used to match against the normalised English words in the English sentences  . For an English text segment E and the English term list produced from a Japanese text segment J  , which we considered to be a possible unit of correspondence  , we calculated similarity using Dice's coefficient score shown in Equation  1  . This rather simple measure captures frequency , but not positional information , q_\]m weights of words are their frequencies inside a sentence  . 
2 fEj (1) Dice(E , . 1 ) -fE+fJ where lea is the number of lexical items which match in E and J  , fEistile number of lexical items in E and fj is the number of lexical items in J  . 
The translation lists for each Japanese word are used disjunctively  , so if one word in the list matches then we do not consider the other terms in the list  . In this way we maintain term independence . 

Our transfer dictionary contained some 79 , 0 00 English words in fullform together with the list of translations in Japanese  . Of these English words some 14 , 0 00 were proper nouns which were directly relevant to the vocabulary typically found in international news stories  . Additionally we perform lexical normalisation before calculating the matching score and remove function words with a stoplist  . 
4.2 Model 2: Byte-length ratios
For Asian language pairs we cannot rely entirely on dictionary term matching  . Moreover , algorithms which rely on matching cognates cannot be applied easily to English and some Asian language  . We were motivated by statistical alignment models such as  ( Gale and Church ,  1991 ) to investigate whether byte-length probabilities could improve or replace the lexical matching based method  . The underlying assumption is that characters in an English sentence are responsible for generating some fraction of each character in the corresponding Japanese sentence  . 
We derived a probability density function by making the assumption that English  . and Japanese sentence length ratios are normally distributed  . The parameters required for the model are the mean  , p and variance ,  ~ , which we calculated from a training set of 450 hand-aligned sentences . These are then entered into Equation 2 to find the probability of any two sentences ( or combinations of sentences for multiple alignments  ) being in an alignment relation given that they have a length ratio of x  . 
The byteleng thratios were calculated as the length of the Japanese text segment divided by the length of the English text segment  . So in this way we can incorporate multiple sentence correspondences into our model  . Bytelengths for English sentences are calculated according to the number of non-whitespace characters  , with a weighting of 1 for each valid character including punctuation . For the Japanese text we counted 2 for each non-white space character . Whitespaces were treated as having length 0 . The ratios for the training set are shown as a histogram in Figure  2 and seem to support the assumption of a normal distribution  . 
The resulting normal curve with ~ r = 0 . 33 and /1 = 0 . 76 is given in Figure 3 , and this can then be used to provide a probability score for any English and Japanese sentence being aligned in the Reuters ' corpus  . 
Clearly it is not enough simply to assume that our sentence pair lengths follow the normal distribution  . 
We tested this assumption using a standard test , by plotting the ordered ratio scores against the values calculated for the normal curve in Figure  3  . If the ~, o ? -4 . s2-1
Ill , .
o ~4 Se
Figure 2: Sentence lng thratios in training set1 . 4 1  . ao . e0 . 4 0 . 2o . . 4 + + 3 4 5 i *~1 I I +1 Figure 3: Sentence l , g thration or mal curve distribution is indeed normal then we would expect the plot in Figure  4 to y i , ? lda straight line . We can see that this is the casel : ' , rmost , although not all , of the observed scores . 
Although the curve in Figure 4 shows that our training set deviated from the normal distribution ati ! io  . m0 . , , o . , , o . , +, 2,, o , . 2 ,  .   .  ,+ ,  .  , , ,  . ? Figure 4: Sentence lngth ratio normal check curve ~ t
Oodli0-6-4
Figure 5: Sentence offsets in training set the extremes we nevertheless proceeded to continue with our simulations using this model considering that the deviations occured at the extremends of the distribution where relatively few samples were found  . The weakness of this assumption however does add extra evidence to doubts which have been raised  , e . g . ( Wu ,  1994) , about whether the byte-length model by itself can perform well  . 
4.3 Model 3: Offset ratios
We calculated the offsets in the sentence indexes for English and Japanese sentences in an alignment relation in the hand-aligned training set  . An offset difference was calculated as the Japanese sentence index minus the English sentence index within a bilingual news article pair  . The values are shown as a histogram in Figure 5 . 
As with the byte-length ratio model , we started from an assumption that sentence correspondence offsets were normally distributed  . We then calculated the mean and variance for our sample set shown in Figure  5 and used this to form a normal probability density function  ( where a = 0 . 50 and/J -1 . 45) shown in Figure 6 . 
The test for normality of the distribution is the same as for byte-length ratios and is given in Figure  7  . We can see that the assumption of normality is particularly weak for the offset distribution  , but we are motivated to see whether such a noisy probability model can improve alignment results  . 
5 Experiments
In this section we present he results of using different combinations of the three basic methods  . We combined the basic methods to make hybrid models simply by taking the product of the scores for the models given above  . Although this is simplistic we felt that in the first stage of our investigation it was better to give equal weight to each method  . 
The seven methods we tested are coded as follows :   0  . 11
O.l~t 5.20 SD 4m
Figure 6: Sentence offsets normal curve f " mO Figure 7: Sentence offscts normal check curve DICE : sentence align melit using bilingual dictionary and Dice's coefficient scores  ; LEN : sentence alignment using sentence length ratios  ; OFFSET : sentence alignment using offs , : t probabilities . 
We performed sentence alignment on our test set of  380 English sentences and 453 Japanese sentences . 
The results are shown as recall and precision which we define in the usual way as follows : recall =  #correctly matched sentences retrieved  #matched sentences in the test collection  ( a ) precision =  #correctly matched sentences retrieved matched sentences retrieved  ( 4 ) The results are shown in Table 1 . We see that the baseline method using lexical matching with a bilingual lexicon  , DICE , performs better than either of the two statistical methods LEN or OFFSET used separately  . Offset probabilities in particular performed poorly showing tl tat we cannot expect the correctly matching sentence to appear constantly in - Method Rec  . (%) Pr .  (%)
DICE ( baseline ) 84 85
LEN 8283
OFFSET 50 57
LEN+OFFSET 70 70
DICE+LEN 8987
DICE+OFFSET 80 80
DICE+LEN+OFFSET 88 85
Table 1: Sentence alignment results as recall and precision  . 
Considering the hybrid methods , we see significantly that DICE+LEN provides a clearly better e-sult for both recall and precision to either DICE or LEN used separately  . On inspection we found that DICE by itself could not distinguish clearly between many candidate sentences  . This occured for two reasons . 
1 . As a result of the limited domain in which news articles report  , there was a strong lexical overlap between candidate sentences in a news article  . 
2 . Secondly , where the lexical overlap was poor between the English sentence and the Japanese translation  , this leads to low DICE scores . 
The second reason can be attributed to low coverage in the bilingual lexicon with the domain of the news articles  . If we had set a minimum threshold limit for overlap frequency then we would have ruled out many correct matches which were found  . 
In both cases LEN provides a decisive clue and enables us to find the correct result more reliably  . Furthermore , we found that LEN was particularly effective at identifying multi-sentence or respondences compared to DICE  , possibly because some sentences are very small and provide weak evidence for lexical matching  , whereas when they are combined with neighbours they provide significant evidence for the 
LEN model.
Using all methods together however in
DICE+LEN+OFFSET seems less promising and we believe that the offset probabilities are not a reliable model  . Possibly this is due to lack of data in the training stage when we calculated ~ and p  , or the dataset may not in fact be normally distributed as indicated by Figure  7  . 
Finally , we noticed that a consistent factor in the English and Japanese text pairs was that the first two lines of the English were always matched to the first line of the Japanese  . This was because the English text separated the title and first line  , whereas our sentence segmenter could not do this for the Japanese  . This factor was consistent for all the 50 article pairs in our test collection and may have led to a small deterioration i the results  , so the figures we present are the minimum of what we can expect when sentence segmentation is performed correctly  . 
6 Conclusion
The assumption that a partial alignment at the word level from lexical correspondences can clearly indicate full sentence alignment is flawed when the texts contain many sentences with similar vocabulary  . This is the case with the news stories used in our experiments and even technical vocabulary and proper nouns are not adequate to clearly discriminate between alternative alignment choices because the vocabulary range inside the news article is not large  . Moreover , the basic assumption of the lexical approach , that the coverage of the bilingual dictionary is adequate  , cannot be relied on if we require robustness . This has shown the need for some hybrid model . 
For our corpus of newspaper articles , the hybrid model has been shown to clearly improve sentence alignment results compared with the pure models used separately  . In the future we would like to make extensions to the lexical model by incorporating term weighting methods from information retrieval such as inverse document frequency which may help to identify more importanterms for matching  . In order to test the generalisability of our method we also want to extend our investigation to parallel corpora in other domains  . 

We would like to thank Reuters and Gakken for allowing us to use the corpus of news stories in our work  . We are grateful to Miwako Shimazu for hand aligning the judgements ct used in the experiments and to Akira Kumano and Satoshi Kinoshita for useful discussions  . Finally we would also like express our appreciation to the anonymous reviewers for their helpful comments  . 
References
P . Brown , J . Lai , and R . Mercer .  1991 . Aligning sentences in parallel corpora . In P9th Annual Meeting of the Association for Computational Linguistics  , 
Berkeley , California , USA.
S . Chen .  1993 . Aligning sentences in bilingual corpora using lexical information  .   31st Annual Meeting of the Association of Computational Linguistics  , Ohio , USA , 2226 June . 
K . Church .  1993 . Char_align : a program for aligning parallel texts at the characterlevel  . In 31st Annual Meeting of the Association for Computational Linguistics  , Ohio , USA , pages 18 ,  2226


N . Collier , H . Hirakawa , and A . Kumano . 1998a . 
Creating a noisy parallel corpus from newswire articles using multilingual information retrieval  . 
Trans . of Information Processing Society of Japan ( to appear )  . 
N . Collier , H . Hirakawa , and A . Kumano . 1998b . 
Machine translation vs . dictionary term translation - a comparison for English-Japanese news article alignment  . In Proceedings of COLING-ACL'98 , University of Montreal , Canada , 10th

P . Fung and D . Wu .  1994 . Statistical augmentation of a Chinese machine readable dictionary  . In Second Annual Workshop on Very Large Corpora , pages 69-85 , August . 
W . Gale and K . Church .  1991 . A program for aligning sentences in bilingual corpora  . In Proceedings of the 29th Annual Conference of the Association for Computational Linguistics  ( ACL-91 , Berkeley , California , pages 177-184 . 
W . Gale and K . Church .  1993 . A program for aligning sentences in a bilingual corpora  . Computational Linguistics , 19(1):75-102 . 
M . Kay and M . Rbshcheisen .  1993 . Text-translation alignment . Computational Linguistics , 19:121-142 . 
T . Utsuro , H . Ikeda , M . Yamane , Y . Matsumoto , and N . Nagao .  1994 . Bilingual text matching using bilingual dictionary and statistics  . In COLING94 , 15th International Conference , Kyoto , Japan , volume 2 , August 59 . 
D . Wu .  1994 . Aligning a parallel EnglishChinese corpus statistically with lexical criteria  . In 3end Annual Meeting of the Association for Computational Linguistics  , New Mexico , USA , pages 80-87 , June 2730 . 

