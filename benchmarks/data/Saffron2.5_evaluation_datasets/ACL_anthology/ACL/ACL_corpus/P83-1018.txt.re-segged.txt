SYN'I'ACIICCONSTI ~ , ,\INTS AND F~FI:ICIFNI ' I~AI(SAI~ , II . I'I'Y
Robert C . Berwick
Room 820, MIT Artificial Intelligence l , aboratory
545 Technology Square , Cambridge , MA 02139
Amy S . Weinberg
Deparuncnt of Linguistics , MIT
Cambridge , MA 02139

A central goal of linguistic theory is to explain why natural languages are the way they are  . It has often been supposed that com0utational consideration sought to play a role in this characterization  , but rigorous arguments along these lines have been difficult to come by  . In this paper we show howake y " axiom " of certain theories of grammar  , Subjacency , can be explained by appealing to general restrictions on online parsing plus natural constraints on the rule-writing vocabulary of grammars  . The explanation avoids the problems with Marcus '   \[1980\] attempt to account for the same constraint . The argument is robust with respect to machine implementau on  , and thus avoids the problems that often arise wilen making detailed claims about parsing efficiency  . It has the added virtue of unifying in the functional domain of parsing certain grammatically disparate phenomena  , swell as making a strong claim about the way in which the grammar is actually embedded into an online sentence processor  . 
IINTRODUCTION
In its short history , computational linguistics has bccndriven by two distinct but interrelated goals  . On the one hand , it has aimed at computational explanations of distinctively human linguistic behavior--that is  , accounts of why natural languages are the way they are viewed from the perspective of computation  . On the other hand , it has accumulated a stock of engineenng methods for building machines to deal with natural  ( and artificial ) languages . Sometimes a single body of research as combined both goals  . This was true of the work of Marcus\[1980\] . for example . But all too often the goals have remained opposed -- even to the extent hat curren transformational theory has been disparaged as hopelessly " intractable " and no help at all in constructing working parsers  . 
This paper shows that modern transformational grammar  ( the " Government-Binding " or " GB " theory as described in Chomsky  \[1981\]  ) can contribute to both aims of computational linguistics  . We show that by combining simple assumptions about efficient parsability along with some assumpti  ( ms about just how grammatical theory is to be " embedded " in a model of language processing  , one can actually explain some key constraints of natural anguages  , uchas Suhjacency . 
(The a ) gumcnt is differ mltfrt ) m that used in Marcus 119801 . ) In fact , almost heentire pattern of cunstraints taken as " axioms " by the GB thct  ) ry can be accut mt cdt br . Second , contrary to what has sometimes been supposed , by exph ) iting these constraints wc can ~ how that a Gll -based theory is particularly compatil  ) lev ~ idlefficient parsing designs , in partic dlar , with extended II ~ , (k , t ) parsers ( uf the sort described by Marcus\[1980 D . Wc can ext cn dth cI,R(k . t ) design to accommodate such phenomenas antecedent -PRO and pronominal binding  . Jightward movement , gappiug , aml VP tlcletion . 
A , Functional Explanations o__fI , ocality Principles Let us consider how to explain locality constraints in natural languages  . First of all , what exactly do we mean by a " locality constraint "?"\]' he paradigm case is that of Subjacency : the distance between a displaced constituent and its " underlying " canonical argument position cannot be too large  , where the distance is gauged ( in English ) in terms of the numher of the number of S ( entence ) or NP phrase boundaries . For example , in sentence ( la ) below , John ( the socalled " antecedent " ) is just one S-boundary away from its presumably " underlying " argument position  ( denoted " x " , the " trace " ) ) as the Subject of the embedded clause , and the sentence is fine : ( la ) John seems\[Sx to like ice cream \] . 
However , all we have to dots to make the link between John and x extend over two S's  , and the sentence is ill-formed : ( lb ) John seems\[Sitiscertain\[Sx to like ice cream This restriction entails a " successive cyclic " analysis of transformational rules  ( see Chomsky\[1973\] )  . In order to derive a sentence like ( lc ) below without violating the Subjacency condition , we must move the NP from its canonical argument position through the empty Subject position in the next higher S and then to its surface slot :   ( lc ) John seems tel to be certain x to get the ice cream  . 
Since the intermediate subject position is filled in  ( lb ) there is no licit derivation for this sentence . 
More precisely , we can state the Subjacency constraint as follows : No rule of grammar can involve X and Y in a configuration like the following  ,  \[  . . . x . . . \[,,  . . . \[/ r . .Y . . . \] . . . l . X . . . \] where a and #are bounding nodes ( in l . ' nglish , S or NP phrases ) . " Why should natural language shcdcsigned Lhis way and not some other way ? Why  , that is , should a constraint like Subjaccn cy exist at all ? Our general result is that under a certain set of assumptions about grammars and their relationship to human sentence processing one can actually expect the following pattern of syntacticig cality constraints :  ( l ) The antecedent-trace relationship must obey Subjaccncy  , but other " binding " realtionships ( e . g . , NP--Pro ) need not obey

119  ( 2 ) Gapping construct it ms must be subject to a bounding condition resembling Subjacency  . but VP deletionced not be . 
(3) Rightward movem cnt must be stricdy bounded.
To the extent hat this predicted pattern of constraints i actually observed -- as it is in English and other languages -- we obtain a genuine functional explanation of these constraints and support for the assumptions themselves  . The argument is different from Man : us ' because it accounts for syntactic locality constraints  ( like Subjaceney )   , as the joint effect of a particular theory of grammar  , a theory of how that grammar is used in parsing , a criterion for efficient parsability . 
and a theory of of how the parser is buil LIn contrast  , Marcus attempted to argue that Subjaceney could be derived from just the  ( independently justified ) operating principles of a particular kind of parser  . 
B . Assumptions.
The assumptions we make are the following : ( 1 ) The grammar includes a level of annotated surface structure indicating how constituents have been displaced from their canonical predicate argument positions  . 
Further , sentence analysis is divided into two stages , along the lines indicated by tile theory of Government and Binding : the first stage is a purely syntactic analysis that rebuilds annotated surface structure  ; the second stage carries out the interpretation of variables  , binds them to operators , all making use of the " referential indices " of

(2 ) To be " visible " at a stage of analysis a linguistic representation must be written in the vocabulary of that level  . For example , to be affected by syntactic operations , a representation must be expressed in a syntactic vocabulary  ( in the usual sense )  ; to be interpreted by operations at the second stage  , the NPs in a representation must possess referential indices  .   ( This assumption is not needed to derive the Subj accn cy constraint  , but may be used to account for another " axiom " of current grammatical theory  , the socalled " constituent command " constraint on anteced cnLs and the variables that they hind  . ) This " visibility " assumption is a rather natural one  . 
(3 ) The rule-writing vocabulary of the grammar cannot make use of arithmetic predicates such as " one  "  , " two " or " three " . 
but only such predicates as " adjacent ".
Further , quzmtificational statements are not allowed mrt . les . These two assumptions are also rather standard . It has often been noted that grammars " do not count "-- that grammatical predicates are structurally based  . There is no rule of grammar that takes the just the fourth constituent of a sentence and moves it  , for example . In contrast , many different kinds of rules of grammar make reference to adjacent constituents  . ( This is a feature found in morphological , phonological , and syntactic rules . ) (4) Parsing is no . . . .! done via a method that carries along ( a representation ) of all possible derivations in parallel . In particular , an Earley-type algorithm is ruled out . To the extent that multiple options about derivations are not pursued  , the parse is " deterministic . "  ( 5 ) The left-context of the parse ( as defined in Aho and Ullman \[19721 ) is literally represented , rather than generatively represented ( as , e . g . , a regular set) . In particular , just the symbols used by the grammar ( S , NP . VP . . . ) are part of the left-context vocabulary , and not " complex " symbols serving as proxies for the set of lefl  . -contexts rings . 1 In effect , we make the ( quite strong ) assumption that the sentence processor adopts a direct  , transparent embedding of the grammar . 
Other theories or parsing methods do not meet these constraints and fail to explain the existence of locality constraints with respect to thts particular set of assumpuons  . 2 For example , as we show , there is no reason to expect a constraint like Subjacency in the Generalized Phrase Structure Grammars/GPSGsl of G  , zdar 1198 11 , because there is no inherent barrier to eastly processing a sentence where an antecedent and a trace are !  . m boundedly fart'rt ~ meach other . 
Similarly if a parsing method like Earlcy's algorithm were actually used by people  , than Sub\]acency remain samy: ; t cry on the functional grounds of efficient parsability  .   ( It could still be explained on other functional grounds  , e . g . , that of learnability . )
IIPARSING ANDLOCALITY PRINCIPLES
To begin the actual argument then , assume that online sentence processing is done by something like a deterministic parser  ) Sentences like ( 2 ) cause trouble for such a parser : ( 2 ) What i do you think that John told Mary . . . matne would like to eat % t . Recall that he suo ec . ~ i ~' elins of a left-or rightmost derivation i a contextfree grammar cnnstt tute a regular Language  .  ~ . ~ shown m . e . g . . DcRemer \[19691 . 
2 . Plainly . one is free to imagine some other set of assumptions that woul do the job  . 
3 . If one a . ssumcs a backtracking parser , then the argument can also be made to go through , but only by a . , , , , ~ ummg that backtracking Ks vcr/co~tlS , ince this son of parser clearly , ,~ ab:~umes th IR(kPt , ' , pemachines undert/leright co , mrual of ' cost " . we make the stronger assumption fIR(k)-ncss . 

The problem is that on recognizing the verb eat the parser must decide whether to expand the parse with a trace  ( the transitive reading ) or with no postverbal element (  . the intransitive reading ) . The ambiguity cannot be locally resolved since eat takes both readings  . It can only be resolved by checking to see whether there is an actual antecedent  . 
Further , observe that this is indeed a parsing decision : the machine must make some decision about how to tubuild a portion of the parse tree  . Finally , given non-parallelism , the parser is not allowed to pursue both paths at once : it must decide now how to build the parse tree  ( by inserting an empty NP trace or not )  . 
Therefore , assuming that the correct decision is to be made online  ( or that retractions of incorrect decisions are costly  ) there must be an actual parsing rule that expands a category as transitive iff there is an immediate postverbal NP in the string  ( no movement ) or if an actual antecedent is present . However , the phonologically overtantecedent can be unboundedly far away from the gap  . Therefore , it would seem that the relevant parsing rule would have to refer to a potentially unbounded left context  . Such a rule cannot be stated in the finite control table of an I  , R(k ) parser . The retbre we must find some finite way of expressing the domain over which the antecedent must be searched  . 
There are two ways of accomplishing this . First , one could express all possible left-contexts as somcregular set and then carry this representation along in the finite control table of the I  , R(k ) machine . 
This is always pu , ,;sible m the case of a contcxt-fiee grammar , and mf act is die " standard " approach . 4 However , m the case of ( e . g . )  , , h moven!enk this demands a generative encoding of the associated finite state automaton  , via the use of complex symbols like " S/wh " ( denoting the " state " that at vtt has been encountered  ) and rules to passking this nun-literal representation f the state of the parse  . Illis approach works , since wc can passakmg this state encoding through the VP  ( via the complex nonterminal symbol VP/wh ) and finally into the embedded S . This complex nonterminal is then used to trigger an expansion of eat into its transitive form  . Ill fact , this is precisely the solution method advocated by Gazdar  . We ~ ce then that if one adopts a nonterminal encoding scheme there should henop  , oble min parsing any single long-distance gap -filler relationship  . That is , there is no need for a constraint like Subjacency  . s Second , the problem of unbounded left-context is directly avoided if the search space is limited to some literally finite left context  . But this is just what the St tb jacency c ( mstraint does : it limits where an antecedent NP could be to an immediately adjacent S or S  . This constraint has a Stlll pJeint crprctatuman actual parser  ( like that built hy Murcus\[19 ; 0D . l'he IF-THEN pattern-action rules that make up the Marcus parser's ~ anite control " transi:ion table " must be finite in order to he stored ioside a machine  . The rule actions themselves are literally finite . If the role patterns must be/herally stored ( e . g . , the pattern \[ S \[ S"\[S must be stored as an actual arbitrarily long string ors nodes  , rather than as the regular set S+) , then these patterns must be literally finite . That is , parsing patterns must refer to literally hounded right and left context  ( in terms of phrasal nodes )  .   6 Note Further that 4 Following the approact l of DcRemer \[\]969\]  , one budds af in Hestale automaton Lhatreco ~ nl/es exactly I he set of i ?\[ t-  ( OII lext strings that cain arise during the course of a rightmost derivation  , the so-Gilled ch , melert . sllcf ' . nifes/aleClUlOmC ~ lott . 
5 l'laml  the same Imlds for a " hold cell " apploaeh\[o compulm  8 filler-gap relallonshipi 6  . Actually Uteri . lhJ8 k ; nd or device lall ! ; llltol Jae ( ~ itegoly of bounded contc ; ~t parsing . 
a.'~defiuedb ~. I\]oydf19(.)4\].
this constraint depends on the sheer represcntability of the parser's rule system in a finite machine  , rather than on any details of implementation . Therefore it will hold invariantly with respect to rnact fine design--no matter kind of machine we build  , if " we assume a literal representation of left -contexts  , then some kind t ) f finiteness constraint is required . The robustness of this result contrasts with the usual problems in applying " efficiency " results to explaing rm ' ~ T ""'! cal constraints  . These often fail because it is difficult to consider all possible implcment auons simultaneously  . However , if the argument is invariant with respect to machine desing  , this problem is avoided . 
Given literal left-contexts and no ( or costly ) backtracking , the arguments of armotivate some bounding condition for ambiguous sentences like these  . However , to get the lull range of cases these functional facts must interact with properties of the rule writing system as defined by the grammar  . We will derive the litct that the Imunding condition must be ~ a cenc y  ( as opposed to tri-or quad-jaccncy ) by appeal to the lhct that grammatical c~m ~ tramts and rules arc ~ tated in a vocabt dary which is non-c'vunmtg  . , ', rithmetic predicates are forbidden . But this means that since only the prediu ~ lte " ad\]  . cent " is permitted , any literal I ) ouuding rc , ~trict\]oi \] must be c . xprc , ~) cdmtcrll S of adjacent domains : t ~ e ~ ; ce Subjaccncy . INert that " , djacent " is also an arithmetic predicate . ) l : urth cr . Subjaccncymu , , tappiy ~ . o , ill traces ( not ju ' , t traces of , mlb = guously traw ~ itive/imransi\[ivevcrb , oin : cause a restriction to just the ambiguous cases would low  ) i reusing cxistent ml quantilicati . n . Ouantificatiom d predicates are barred in the rule writing vocabulary of natural grammars  .   7 Next we extend the approach to NP movement and Gapping  . 
Gapping is particularly interesting because it is difficult ~ o explain why this construction  ( tm like other deletiou rules ) is bounded . That is , why is ( 3 ) but not ( 4 ) grammatical : ( 3 ) John will hit Frank and Bill will \[ ely P George . 
* ( 4 ) John will hit Frank and I don't believe Bill will \[ elvp George  . 
The problem with gapping constructions is that the attachment of phonologically identical complements is governed by the verb that the complement follows  . Extraction tests show that in 5 ) the pilrase u/?er M'ao'attaches to V " whdein ( 6 ) it attaches to V " ( See Hornstem and
Wemberg\[\]981\] for details . (5) John will mnaft crMary . 
(6) John will arriv cafter Mary.
In gapping structures , however , the verb of the gapped constituent , s not present in the string . Therefore . correct , lltachrnento ( the complement can only be guaranteed by accessing the antecedent in the previous clause  . If this is true however , then the boundlng argument for Suhjacency applies to this ease as well : given deterministic parsing of gapping done correctly  , and a literal representation of left-context , then gapping must be comext-bounded . Note that this is a particularly 7Of course , therezsaanol hcrnatural predic . at cI hat would produce a finite bound on rule context : i\[~\]  ) alld I rate hod I . bc in tile . ameSdonlalllPrc ~ umahb ' , lhls is also an Optlllt3~l ; iI could gelreah , edinq OIICn . ' Ittlral ~ rJoln'iai ~: ll'i cresuhing languages would no  ( have ov , ,:rt nlo~ . eIII Cill OUlsideo\[an S . % o ( elllal Lhcnaltllal plcdJc ; dessimply give the rant a ? of po~ed blen diulal granmlars  . \] lo those actually rour ~ d . 
The elimination of quanllfil ', . llion predic ~ les is supportable on ground so ( acquisltton . 
1 21 interesting example bccause it shows how grammatically dissimilar operations like wh -movement and gapping can " fall together " in the functional domain of parsing  . 
NP-trace and gapl Sing constructions contrast with antecedent Y  ( pro ) nominal binding , lexical anaphore lationships , and VP deletion . These last three do not obey Subjacency . For example , a Noun Phrase can be unboundedly far from a ( phonologically empty ) 
PRO . even intenns of
John i thought it was certain that . . . \[ PROi feeding himself \] would be easy . 
Note though that in these cases the expansion of the syntactic tree does no  . _At depend on the presence or absence of an antecedent  ( Pro ) nominals and Icxical anaphors are phonologically realized in the string and can unambiguously tell the parser hew to expand the tree  . 
(After the tree is fully expanded the parser may search back to see whether the element is bound to an antecedent  , but this is not a parsing decision , ) VP deletion sites are also always locally detectable from ~ e simple fact that every sentence requires a VP  . The same argument applies to PRO . PRO is locally detectable as the only phonologically unrealized element that can appear in an ungoverned context  , and the predicate " ungoverned " is local . 8In short , there is no parsing decision that hinges on establishing the PRO-antecedent  . VP deletion-antecedent , t ) r lexical anaphor antecedent relationship . But then , we should not expect bounding principles to apply in thcse cases  , and , in fact , we do not find these elements subject o bounding . Once again then . apparently diverse grammau calphcnom c , m behave a like within a function alreal m . 
To summarize , we can explain why Subjacency applies to exactly those elements that the grammar stipulates it must apply to  . We do this using both facts about the functional design of a parsing system and properties of the formal rule writing vocabulary  , l'o the extent that the array of assumpuons about the grammar and parser actually explain this observed constraint on human linguistic behavior  , we obtain a powerful argument hat certain kinds of grammatical represenumons and parsing d c stgns are actually implicated in human sentence processing  . 
Chomsky , Noam\[19811 Lectures on Gove , nmem and Binding , For is

I ) e Rerner , Frederick\[1969\] Practical 7" nms , ': m~sJ brIR(k)I . angu , ges , Phi ) di . ~s crtation , MIT Department of Electrical Engineering and
Computer Science.
Floyd , Robert \[1964\] " Bounded-context syntactic analysis . " Communt cations of the Assoctatiotl for Computing  , l . lachinery , 7pp , 62-66 . 
Gazdar , Gerald \[19811 " Unbounded dependencies and coordinate structure  , " Linguistic Inquiry , 12:2I55-184 . 
Hornstein . Norbert and Wcinherg , Amy \[19811 " Preposition stranding and case theory , " Lingut Mic\[nquio , , 12:1 . 
Marcus , Mitchell \[19801 A Theory of Syntactic Recognition for Natural
Language , MIT Press 111 ACKNOWLEDGE IvlENTS This report describes work done at the Artificial Intelligence Laboratory of the Massachusetts Institute of l'cchnt  ) logy . Support for the Laboratory's artificial intelligence research is prey  ) deal in part by tiacAdvanced P , esearch ProjccLs Agency of the Department of Defense under Office  ( ) f Naval Research Contract N00014-80-C-0505 . 
IVREFERENCES
Aho , Alfred and Ullman , Jeffrey\[1972\] The Theory of Parsing Trnn . ~lalion , attd Cumpiiing , vo\[ .  \[ . , Prentice-(-all . 
Chumsky , Noam\[1973\]" Conditions on ' rrans formations , " in S . 
Anders ( m & P Kiparsky , eds . A Feslschr(l'l\[or Morris Halle . Holt,
Rinehart and Winston.
8 F ; hlce~~sungovcNicdfffa~ovct'llcdt:~F ; L\[:~c , and ago ~ c , ' m~J is a bounded predicate , ihcmgL cstrictcdIomu ~' , dya ~ in ~ i ? llla X1 111 ; il Droj cctlon ( at worstal S ) . 

