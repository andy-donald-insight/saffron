Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics , pages 223?231,
Jeju , Republic of Korea , 814 July 2012. c?2012 Association for Computational Linguistics
Spectral Learning of Latent-Variable PCFGs
Shay B . Cohen1, Karl Stratos1, Michael Collins1, Dean P . Foster2, and Lyle Ungar3
1Dept . of Computer Science , Columbia University
2Dept . of Statistics/3Dept . of Computer and Information Science , University of Pennsylvania
{scohen,stratos,mcollins}@cs.columbia.edu , foster@wharton.upenn.edu , ungar@cis.upenn.edu
Abstract
We introduce a spectral learning algorithm for latent-variable PCFGs ( Petrov et al , 2006).
Under a separability ( singular value ) condition , we prove that the method provides consistent parameter estimates.
1 Introduction
Statistical models with hidden or latent variables are of great importance in natural language processing , speech , and many other fields . The EM algorithm is a remarkably successful method for parameter estimation within these models : it is simple , it is often relatively efficient , and it has well understood formal properties . It does , however , have a major limitation : it has no guarantee of finding the global optimum of the likelihood function . From a theoretical perspective , this means that the EM algorithm is not guaranteed to give consistent parameter estimates . From a practical perspective , problems with local optima can be difficult to deal with.
Recent work has introduced polynomial-time learning algorithms ( and consistent estimation methods ) for two important cases of hidden-variable models : Gaussian mixture models ( Dasgupta , 1999; Vempala and Wang , 2004) and hidden Markov models ( Hsu et al , 2009). These algorithms use spectral methods : that is , algorithms based on eigenvector decompositions of linear systems , in particular singular value decomposition ( SVD ). In the general case , learning of HMMs or GMMs is intractable ( e.g ., see Terwijn , 2002). Spectral methods finesse the problem of intractibility by assuming separability conditions . For example , the algorithm of Hsu et al (2009) has a sample complexity that is polynomial in 1/?, where ? is the minimum singular value of an underlying decomposition . These methods are not susceptible to problems with local maxima , and give consistent parameter estimates.
In this paper we derive a spectral algorithm for learning of latent-variable PCFGs ( LPCFGs ) ( Petrov et al , 2006; Matsuzaki et al , 2005). Our method involves a significant extension of the techniques from Hsu et al (2009). LPCFGs have been shown to be a very effective model for natural language parsing . Under a separation ( singular value ) condition , our algorithm provides consistent parameter estimates ; this is in contrast with previous work , which has used the EM algorithm for parameter estimation , with the usual problems of local optima.
The parameter estimation algorithm ( see figure 4) is simple and efficient . The first step is to take an SVD of the training examples , followed by a projection of the training examples down to a low-dimensional space . In a second step , empirical averages are calculated on the training example , followed by standard matrix operations . On test examples , simple ( tensor-based ) variants of the insideoutside algorithm ( figures 2 and 3) can be used to calculate probabilities and marginals of interest.
Our method depends on the following results : ? Tensor form of the insideoutside algorithm.
Section 5 shows that the insideoutside algorithm for LPCFGs can be written using tensors . Theorem 1 gives conditions under which the tensor form calculates inside and outside terms correctly.
? Observable representations . Section 6 shows that under a singular-value condition , there is an observable form for the tensors required by the insideoutside algorithm . By an observable form , we follow the terminology of Hsu et al (2009) in referring to quantities that can be estimated directly from data where values for latent variables are unobserved.
Theorem 2 shows that tensors derived from the observable form satisfy the conditions of theorem 1.
? Estimating the model . Section 7 gives an algorithm for estimating parameters of the observable representation from training data . Theorem 3 gives a sample complexity result , showing that the estimates converge to the true distribution at a rate of 1/ ?
M where M is the number of training examples.
The algorithm is strikingly different from the EM algorithm for LPCFGs , both in its basic form , and in its consistency guarantees . The techniques de-be relevant to the development of spectral methods for estimation in other models in NLP , for example alignment models for translation , synchronous PCFGs , and so on . The tensor form of the insideoutside algorithm gives a new view of basic calculations in PCFGs , and may itself lead to new models.
2 Related Work
For work on LPCFGs using the EM algorithm , see Petrov et al (2006), Matsuzaki et al (2005), Pereira and Schabes (1992). Our work builds on methods for learning of HMMs ( Hsu et al , 2009; Foster et al , 2012; Jaeger , 2000), but involves several extensions : in particular in the tensor form of the insideoutside algorithm , and observable representations for the tensor form . Balle et al (2011) consider spectral learning of finite-state transducers ; Lugue et al (2012) considers spectral learning of head automata for dependency parsing . Parikh et al (2011) consider spectral learning algorithms of tree-structured directed bayes nets.
3 Notation
Given a matrix A or a vector v , we write A ? or v ? for the associated transpose . For any integer n ? 1, we use [ n ] to denote the set {1, 2, . . . n }. For any row or column vector y ? Rm , we use diag(y ) to refer to the ( m?m ) matrix with diagonal elements equal to yh for h = 1 . . . m , and off-diagonal elements equal to 0. For any statement ?, we use [[?]] to refer to the indicator function that is 1 if ? is true , and 0 if ? is false . For a random variable X , we use
E[X ] to denote its expected value.
We will make ( quite limited ) use of tensors : Definition 1 A tensor C ? R(m?m?m ) is a set of m3 parameters Ci,j,k for i , j , k ? [ m ]. Given a tensor C , and a vector y ? Rm , we define C(y ) to be the ( m ? m ) matrix with components [ C(y)]i,j = ? k?[m]Ci,j,kyk . Hence C can be interpreted as a function C : Rm ? R(m?m ) that maps a vector y ? Rm to a matrix C(y ) of dimension ( m?m).
In addition , we define the tensor C ? ? R(m?m?m ) for any tensor C ? R(m?m?m ) to have values [ C?]i,j,k = Ck,j,i Finally , for vectors x , y , z ? Rm , xy?z ? is the tensor D ? Rm?m?m where Dj,k,l = xjykzl ( this is analogous to the outer product : [ xy?]j,k = xjyk).
4 LPCFGs : Basic Definitions
This section gives a definition of the LPCFG formalism used in this paper . An LPCFG is a 5-tuple ( N , I,P,m , n ) where : ? N is the set of nonterminal symbols in the grammar . I ? N is a finite set of in-terminals.
P ? N is a finite set of preterminals . We assume that N = I ? P , and I ? P = ?. Hence we have partitioned the set of nonterminals into two subsets.
? [ m ] is the set of possible hidden states.
? [ n ] is the set of possible words.
? For all a ? I , b ? N , c ? N , h1, h2, h3 ? [ m ], we have a contextfree rule a(h1) ? b(h2) c(h3).
? For all a ? P , h ? [ m ], x ? [ n ], we have a contextfree rule a(h ) ? x.
Hence each in-terminal a ? I is always the left-hand-side of a binary rule a ? b c ; and each preterminal a ? P is always the left-hand-side of a rule a ? x . Assuming that the nonterminals in the grammar can be partitioned this way is relatively benign , and makes the estimation problem cleaner.
We define the set of possible ? skeletal rules ? as R = { a ? b c : a ? I , b ? N , c ? N }. The parameters of the model are as follows : ? For each a ? b c ? R , and h ? [ m ], we have a parameter q(a ? b c|h , a ). For each a ? P , x ? [ n ], and h ? [ m ], we have a parameter q(a ? x|h , a ). For each a ? b c ? R , and h , h ? ? [ m ], we have parameters s(h?|h , a ? b c ) and t(h?|h , a ? b c).
These definitions give a PCFG , with rule probabilities p(a(h1) ? b(h2) c(h3)|a(h1)) = q(a ? b c|h1, a )? s(h2|h1, a ? b c )? t(h3|h1, a ? b c ) and p(a(h ) ? x|a(h )) = q(a ? x|h , a).
In addition , for each a ? I , for each h ? [ m ], we have a parameter ?( a , h ) which is the probability of nonterminal a paired with hidden variable h being at the root of the tree.
An LPCFG defines a distribution over parse trees as follows . A skeletal tree ( s-tree ) is a sequence of rules r1 . . . rN where each ri is either of the form a ? b c or a ? x . The rule sequence forms a topdown , leftmost derivation under a CFG with skeletal rules . See figure 1 for an example.
A full tree consists of an s-tree r1 . . . rN , together with values h1 . . . hN . Each hi is the value for
NP2
D3 the
N4 dog
VP5
V6 saw
P7 him r1 = S ? NP VP r2 = NP ? D N r3 = D ? the r4 = N ? dog r5 = VP ? V P r6 = V ? saw r7 = P ? him Figure 1: An s-tree , and its sequence of rules . ( For convenience we have numbered the nodes in the tree .) the hidden variable for the left-hand-side of rule ri.
Each hi can take any value in [ m].
Define ai to be the nonterminal on the left-hand-side of rule ri . For any i ? {2 . . . N } define pa(i ) to be the index of the rule above node i in the tree.
Define L ? [ N ] to be the set of nodes in the tree which are the left-child of some parent , and R ? [ N ] to be the set of nodes which are the right-child of some parent . The probability mass function ( PMF ) over full trees is then p(r1 . . . rN , h1 . . . hN ) = ?( a1, h1) ?
N ? i=1 q(ri|hi , ai )? ? i?L s(hi|hpa(i ), rpa(i )) ? ? i?R t(hi|hpa(i ), rpa(i )) (1) The PMF over s-trees is p(r1 . . . rN ) = ? h1...hN p(r1 . . . rN , h1 . . . hN ).
In the remainder of this paper , we make use of matrix form of parameters of an LPCFG , as follows : ? For each a ? b c ? R , we define Qa?b c ? Rm?m to be the matrix with values q(a ? b c|h , a ) for h = 1, 2, . . . m on its diagonal , and 0 values for its off-diagonal elements . Similarly , for each a ? P , x ? [ n ], we define Qa?x ? Rm?m to be the matrix with values q(a ? x|h , a ) for h = 1, 2, . . . m on its diagonal , and 0 values for its off-diagonal elements.
? For each a ? b c ? R , we define Sa?b c ? Rm?m where [ Sa?b c]h?,h = s(h?|h , a ? b c).
? For each a ? b c ? R , we define T a?b c ? Rm?m where [ T a?b c]h?,h = t(h?|h , a ? b c).
? For each a ? I , we define the vector ? a ? Rm where [? a]h = ?( a , h).
5 Tensor Form of the Inside-Outside
Algorithm
Given an LPCFG , two calculations are central : Inputs : s-tree r1 . . . rN , LPCFG ( N , I,P , m , n ), parameters ? Ca?b c ? R(m?m?m ) for all a ? b c ? R ? c?a?x ? R(1?m ) for all a ? P , x ? [ n ] ? c1a ? R(m?1) for all a ? I.
Algorithm : ( calculate the f i terms bottom-up in the tree ) ? For all i ? [ N ] such that ai ? P , f i = c?ri ? For all i ? [ N ] such that ai ? I , f i = f?Cri(f ?) where ? is the index of the left child of node i in the tree , and ? is the index of the right child.
Return : f1c1a1 = p(r1 . . . rN)
Figure 2: The tensor form for calculation of p(r1 . . . rN ).
1. For a given s-tree r1 . . . rN , calculate p(r1 . . . rN ).
2. For a given input sentence x = x1 . . . xN , calculate the marginal probabilities ?( a , i , j ) = ? ?? T ( x):(a,i,j )?? p (?) for each nonterminal a ? N , for each ( i , j ) such that 1 ? i ? j ? N .
Here T ( x ) denotes the set of all possible s-trees for the sentence x , and we write ( a , i , j ) ? ? if nonterminal a spans words xi . . . xj in the parse tree ? .
The marginal probabilities have a number of uses.
Perhaps most importantly , for a given sentence x = x1 . . . xN , the parsing algorithm of Goodman (1996) can be used to find arg max ?? T ( x ) ? ( a,i,j )?? ?( a , i , j ) This is the parsing algorithm used by Petrov et al (2006), for example . In addition , we can calculate the probability for an input sentence , p(x ) = ? ?? T ( x ) p (?), as p(x ) = ? a?I ?( a , 1, N).
Variants of the insideoutside algorithm can be used for problems 1 and 2. This section introduces a novel form of these algorithms , using tensors . This is the first step in deriving the spectral estimation method.
The algorithms are shown in figures 2 and 3. Each algorithm takes the following inputs : 1. A tensor Ca?b c ? R(m?m?m ) for each rule a ? b c.
2. A vector c?a?x ? R(1?m ) for each rule a ? x.
225 3. A vector c1a ? R(m?1) for each a ? I .
The following theorem gives conditions under which the algorithms are correct : Theorem 1 Assume that we have an LPCFG with parameters Qa?x , Qa?b c , T a?b c , Sa?b c , ? a , and that there exist matrices Ga ? R(m?m ) for all a ? N such that each Ga is invertible , and such that : 1. For all rules a ? b c , Ca?b c(y ) =
GcT a?b cdiag(yGbSa?b c)Qa?b c(Ga)?1 2. For all rules a ? x , c?a?x = 1?Qa?x(Ga)?1 3. For all a ? I , c1a = Ga?a Then : 1) The algorithm in figure 2 correctly computes p(r1 . . . rN ) under the LPCFG . 2) The algorithm in figure 3 correctly computes the marginals ?( a , i , j ) under the LPCFG.
Proof : See section 9.1.
6 Estimating the Tensor Model
A crucial result is that it is possible to directly estimate parameters Ca?b c , c?a?x and c1a that satisfy the conditions in theorem 1, from a training sample consisting of s-trees ( i.e ., trees where hidden variables are unobserved ). We first describe random variables underlying the approach , then describe observable representations based on these random variables.
6.1 Random Variables Underlying the Approach Each s-tree with N rules r1 . . . rN has N nodes . We will use the s-tree in figure 1 as a running example.
Each node has an associated rule : for example , node 2 in the tree in figure 1 has the rule NP ? D N.
If the rule at a node is of the form a ? b c , then there are left and right inside trees below the left child and right child of the rule . For example , for node 2 we have a left inside tree rooted at node 3, and a right inside tree rooted at node 4 ( in this case the left and right inside trees both contain only a single rule production , of the form a ? x ; however in the general case they might be arbitrary subtrees).
In addition , each node has an outside tree . For node 2, the outside tree is
S
NP VP
V saw
P him
Inputs : Sentence x1 . . . xN , LPCFG ( N , I,P , m , n ), parameters Ca?b c ? R(m?m?m ) for all a ? b c ? R , c?a?x ? R(1?m ) for all a ? P , x ? [ n ], c1a ? R(m?1) for all a ? I.
Data structures : ? Each ? a,i,j ? R1?m for a ? N , 1 ? i ? j ? N is a row vector of inside terms.
? Each ? a,i,j ? Rm?1 for a ? N , 1 ? i ? j ? N is a column vector of outside terms.
? Each ?( a , i , j ) ? R for a ? N , 1 ? i ? j ? N is a marginal probability.
Algorithm : ( Inside base case ) ? a ? P , i ? [ N ], ? a,i,i = c?a?xi ( Inside recursion ) ? a ? I , 1 ? i < j ? N , ? a,i,j = j?1 ? k=i ? a?b c ? c,k+1,jCa?b c(?b,i,k ) ( Outside base case ) ? a ? I , ? a,1,n = c1a ( Outside recursion ) ? a ? N , 1 ? i ? j ? N , ? a,i,j = i?1 ? k=1 ? b?c a
Cb?c a(?c,k,i?1)?b,k,j +
N ? k=j+1 ? b?a c
Cb?a c ? (? c,j+1,k)?b,i,k ( Marginals ) ? a ? N , 1 ? i ? j ? N , ?( a , i , j ) = ? a,i,j?a,i,j = ? h?[m ] ? a,i,jh ? a,i,j h Figure 3: The tensor form of the insideoutside algorithm , for calculation of marginal terms ?( a , i , j).
The outside tree contains everything in the s-tree r1 . . . rN , excluding the subtree below node i.
Our random variables are defined as follows.
First , we select a random internal node , from a random tree , as follows : ? Sample an s-tree r1 . . . rN from the PMF p(r1 . . . rN ). Choose a node i uniformly at random from [ N ].
If the rule ri for the node i is of the form a ? b c , we define random variables as follows : ? R1 is equal to the rule ri ( e.g ., NP ? D N).
? T1 is the inside tree rooted at node i . T2 is the inside tree rooted at the left child of node i , and T3 is the inside tree rooted at the right child of node i.
? H1,H2,H3 are the hidden variables associated with node i , the left child of node i , and the right child of node i respectively.
226 ? A1, A2, A3 are the labels for node i , the left child of node i , and the right child of node i respectively . ( E.g ., A1 = NP , A2 = D , A3 = N .) ? O is the outside tree at node i.
? B is equal to 1 if node i is at the root of the tree ( i.e ., i = 1), 0 otherwise.
If the rule ri for the selected node i is of the form a ? x , we have random variables R1, T1,H1, A1, O,B as defined above , but H2,H3, T2, T3, A2, and A3 are not defined.
We assume a function ? that maps outside trees o to feature vectors ?( o ) ? Rd ? . For example , the feature vector might track the rule directly above the node in question , the word following the node in question , and so on . We also assume a function ? that maps inside trees t to feature vectors ?( t ) ? Rd.
As one example , the function ? might be an indicator function tracking the rule production at the root of the inside tree . Later we give formal criteria for what makes good definitions of ?( o ) of ?( t ). One requirement is that d ? ? m and d ? m.
In tandem with these definitions , we assume projection matices Ua ? R(d?m ) and V a ? R(d??m ) for all a ? N . We then define additional random variables Y1, Y2, Y3, Z as
Y1 = ( Ua1)??(T1) Z = ( V a1)??(O)
Y2 = ( Ua2)??(T2) Y3 = ( Ua3)??(T3) where ai is the value of the random variable Ai.
Note that Y1, Y2, Y3, Z are all in Rm.
6.2 Observable Representations
Given the definitions in the previous section , our representation is based on the following matrix , tensor and vector quantities , defined for all a ? N , for all rules of the form a ? b c , and for all rules of the form a ? x respectively : ? a = E[Y1Z?|A1 = a]
Da?b c = E [ [[ R1 = a ? b c]]Y3Z?Y ?2 | A1 = a ] d?a?x = E [ [[ R1 = a ? x]]Z?|A1 = a ] Assuming access to functions ? and ?, and projection matrices Ua and V a , these quantities can be estimated directly from training data consisting of a set of s-trees ( see section 7).
Our observable representation then consists of:
Ca?b c(y ) = Da?b c(y)(?a)?1 (2) c?a?x = d?a?x(?a)?1 (3) c1a = E [[[ A1 = a]]Y1|B = 1] (4) We next introduce conditions under which these quantities satisfy the conditions in theorem 1.
The following definition will be important : Definition 2 For all a ? N , we define the matrices
Ia ? R(d?m ) and Ja ? R(d??m ) as [ Ia]i,h = E[?i(T1) | H1 = h,A1 = a ] [ Ja]i,h = E[?i(O ) | H1 = h,A1 = a ] In addition , for any a ? N , we use ? a ? Rm to denote the vector with ? ah = P ( H1 = h|A1 = a).
The correctness of the representation will rely on the following conditions being satisfied ( these are parallel to conditions 1 and 2 in Hsu et al (2009)): Condition 1 ? a ? N , the matrices Ia and Ja are of full rank ( i.e ., they have rank m ). For all a ? N , for all h ? [ m ], ? ah > 0.
Condition 2 ? a ? N , the matrices Ua ? R(d?m ) and V a ? R(d??m ) are such that the matrices Ga = ( Ua)?Ia and Ka = ( V a)?Ja are invertible.
The following lemma justifies the use of an SVD calculation as one method for finding values for Ua and V a that satisfy condition 2: Lemma 1 Assume that condition 1 holds , and for all a ? N define ? a = E[?(T1) (?( O ))? | A1 = a ] (5) Then if Ua is a matrix of the m left singular vectors of ? a corresponding to nonzero singular values , and V a is a matrix of the m right singular vectors of ? a corresponding to nonzero singular values , then condition 2 is satisfied.
Proof sketch : It can be shown that ? a =
Iadiag(?a)(Ja )?. The remainder is similar to the proof of lemma 2 in Hsu et al (2009).
The matrices ? a can be estimated directly from a training set consisting of s-trees , assuming that we have access to the functions ? and ?.
We can now state the following theorem : For all a ? N , define Ga = ( Ua)?Ia . Then under the definitions in Eqs . 24: 1. For all rules a ? b c , Ca?b c(y ) =
GcT a?b cdiag(yGbSa?b c)Qa?b c(Ga)?1 2. For all rules a ? x , c?a?x = 1?Qa?x(Ga)?1.
3. For all a ? N , c1a = Ga?a
Proof : The following identities hold ( see section 9.2):
Da?b c(y ) = (6)
GcT a?b cdiag(yGbSa?b c)Qa?b cdiag(?a)(Ka )? d?a?x = 1?Qa?xdiag(?a)(Ka )? (7) ? a = Gadiag(?a)(Ka )? (8) c1a = Gapia (9) Under conditions 1 and 2, ? a is invertible , and (? a)?1 = (( Ka)?)?1(diag(?a))?1(Ga)?1. The identities in the theorem follow immediately.
7 Deriving Empirical Estimates
Figure 4 shows an algorithm that derives estimates of the quantities in Eqs 2, 3, and 4. As input , the algorithm takes a sequence of tuples ( r(i,1), t(i,1), t(i,2), t(i,3), o(i ), b(i )) for i ? [ M ].
These tuples can be derived from a training set consisting of s-trees ?1 . . . ? M as follows : ? ? i ? [ M ], choose a single node ji uniformly at random from the nodes in ? i . Define r(i,1) to be the rule at node ji . t(i,1) is the inside tree rooted at node ji . If r(i,1) is of the form a ? b c , then t(i,2) is the inside tree under the left child of node ji , and t(i,3) is the inside tree under the right child of node ji . If r(i,1) is of the form a ? x , then t(i,2) = t(i,3) = NULL . o(i ) is the outside tree at node ji . b(i ) is 1 if node ji is at the root of the tree , 0 otherwise.
Under this process , assuming that the s-trees ?1 . . . ? M are i.i.d . draws from the distribution p (?) over s-trees under an LPCFG , the tuples ( r(i,1), t(i,1), t(i,2), t(i,3), o(i ), b(i )) are i.i.d . draws from the joint distribution over the random variables R1, T1, T2, T3, O,B defined in the previous section.
The algorithm first computes estimates of the projection matrices Ua and V a : following lemma 1, this is done by first deriving estimates of ? a , and then taking SVDs of each ? a . The matrices are then used to project inside and outside trees t(i,1), t(i,2), t(i,3), o(i ) down to m-dimensional vectors y(i,1), y(i,2), y(i,3), z(i ); these vectors are used to derive the estimates of Ca?b c , c?a?x , and c1a.
We now state a PAC-style theorem for the learning algorithm . First , for a given LPCFG , we need a couple of definitions : ? ? is the minimum absolute value of any element of the vectors/matrices/tensors c1a , d?a?x , Da?b c , (? a)?1. ( Note that ? is a function of the projection matrices Ua and V a as well as the underlying
LPCFG .) ? For each a ? N , ? a is the value of the m?th largest singular value of ? a . Define ? = mina ? a.
We then have the following theorem:
Theorem 3 Assume that the inputs to the algorithm in figure 4 are i.i.d . draws from the joint distribution over the random variables R1, T1, T2, T3, O,B , under an LPCFG with distribution p(r1 . . . rN ) over s-trees . Define m to be the number of latent states in the LPCFG . Assume that the algorithm in figure 4 has projection matrices U?a and V ? a derived as left and right singular vectors of ? a , as defined in Eq . 5. Assume that the LPCFG , together with U?a and V ? a , has coefficients ? > 0 and ? > 0. In addition , assume that all elements in c1a , d?a?x , Da?b c , and ? a are in [?1,+1]. For any s-tree r1 . . . rN define p?(r1 . . . rN ) to be the value calculated by the algorithm in figure 3 with inputs c?1a , c??a?x , C?a?b c derived from the algorithm in figure 4. Define R to be the total number of rules in the grammar of the form a ? b c or a ? x . Define Ma to be the number of training examples in the input to the algorithm in figure 4 where ri,1 has nonterminal a on its left-hand-side . Under these assumptions , if for all a
Ma ? 128m2 ( 2N+1?1 + ?? 1 )2 ?2?4 log (2mR ? )
Then 1? ? ? ? ? ? ? p?(r1 . . . rN ) p(r1 . . . rN ) ? ? ? ? ? 1 + ? A similar theorem ( omitted for space ) states that 1? ? ? ? ? ? ??( a,i,j ) ?( a,i,j ) ? ? ? ? 1 + ? for the marginals.
The condition that U?a and V ? a are derived from ? a , as opposed to the sample estimate ?? a , follows Foster et al (2012). As these authors note , similar techniques to those of Hsu et al (2009) should be is used in place of ? a.
Proof sketch : The proof is similar to that of Foster et al (2012). The basic idea is to first show that under the assumptions of the theorem , the estimates c?1a , d??a?x , D?a?b c , ?? a are all close to the underlying values being estimated . The second step is to show that this ensures that p?(r1...rN ? ) p(r1...rN ? ) is close to 1.
The method described of selecting a single tuple ( r(i,1), t(i,1), t(i,2), t(i,3), o(i ), b(i )) for each s-tree ensures that the samples are i.i.d ., and simplifies the analysis underlying theorem 3. In practice , an implementation should most likely use all nodes in all trees in training data ; by Rao-Blackwellization we know such an algorithm would be better than the one presented , but the analysis of how much better would be challenging . It would almost certainly lead to a faster rate of convergence of p ? to p.
8 Discussion
There are several potential applications of the method . The most obvious is parsing with L-PCFGs.1 The approach should be applicable in other cases where EM has traditionally been used , for example in semisupervised learning . Latent-variable HMMs for sequence labeling can be derived as special case of our approach , by converting tagged sequences to right-branching skeletal trees.
The sample complexity of the method depends on the minimum singular values of ? a ; these singular values are a measure of how well correlated ? and ? are with the unobserved hidden variable H1. Experimental work is required to find a good choice of values for ? and ? for parsing.
9 Proofs
This section gives proofs of theorems 1 and 2. Due to space limitations we cannot give full proofs ; instead we provide proofs of some key lemmas . A long version of this paper will give the full proofs.
9.1 Proof of Theorem 1
First , the following lemma leads directly to the correctness of the algorithm in figure 2: 1Parameters can be estimated using the algorithm in figure 4; for a test sentence x1 . . . xN we can first use the algorithm in figure 3 to calculate marginals ?( a , i , j ), then use the algorithm of Goodman (1996) to find argmax??T ( x ) ? ( a,i,j )?? ?( a , i , j).
Inputs : Training examples ( r(i,1), t(i,1), t(i,2), t(i,3), o(i ), b(i )) for i ? {1 . . . M }, where r(i,1) is a context free rule ; t(i,1), t(i,2) and t(i,3) are inside trees ; o(i ) is an outside tree ; and b(i ) = 1 if the rule is at the root of tree , 0 otherwise . A function ? that maps inside trees t to feature-vectors ?( t ) ? Rd . A function ? that maps outside trees o to feature-vectors ?( o ) ? Rd ? .
Algorithm:
Define ai to be the nonterminal on the lefthand side of rule r(i,1). If r(i,1) is of the form a ? b c , define bi to be the nonterminal for the left-child of r(i,1), and ci to be the nonterminal for the right-child.
(Step 0: Singular Value Decompositions ) ? Use the algorithm in figure 5 to calculate matrices U?a ? R(d?m ) and V ? a ? R(d??m ) for each a ? N .
(Step 1: Projection ) ? For all i ? [ M ], compute y(i,1) = ( U?ai)??(t(i,1)).
? For all i ? [ M ] such that r(i,1) is of the form a ? b c , compute y(i,2) = ( U?bi)??(t(i,2)) and y(i,3) = ( U?ci)??(t(i,3)).
? For all i ? [ M ], compute z(i ) = ( V ? ai)??(o(i)).
(Step 2: Calculate Correlations ) ? For each a ? N , define ? a = 1/ ? M i=1[[ai = a ]] ? For each rule a ? b c , compute D?a?b c = ? a ? ? M i=1[[r(i,1) = a ? b c]]y(i,3)(z(i))?(y(i,2))? ? For each rule a ? x , compute d??a?x = ? a ? ? M i=1[[r(i,1) = a ? x]](z(i ))? ? For each a ? N , compute ?? a = ? a ? ? M i=1[[ai = a]]y(i,1)(z(i ))? ( Step 3: Compute Final Parameters ) ? For all a ? b c , C?a?b c(y ) = D?a?b c(y)(??a)?1 ? For all a ? x , c??a?x = d??a?x(??a)?1 ? For all a ? I , c?1a = ? M i=1[[ai=a and b(i)=1]]y(i,1) ? M i=1[[b(i)=1]] Figure 4: The spectral learning algorithm.
Inputs : Identical to algorithm in figure 4.
Algorithm : ? For each a ? N , compute ?? a ? R(d??d ) as ?? a = ? M i=1[[ai = a]]?(t(i,1))(?(o(i )))? ? M i=1[[ai = a ]] and calculate a singular value decomposition of ?? a.
? For each a ? N , define U?a ? Rm?d to be a matrix of the left singular vectors of ?? a corresponding to the m largest singular values . Define V ? a ? Rm?d ? to be a matrix of the right singular vectors of ?? a corresponding to the m largest singular values.
Figure 5: Singular value decompositions.
229
Lemma 2 Assume that conditions 13 of theorem 1 are satisfied , and that the input to the algorithm in figure 2 is an s-tree r1 . . . rN . Define ai for i ? [ N ] to be the nonterminal on the left-hand-side of rule ri , and ti for i ? [ N ] to be the s-tree with rule ri at its root . Finally , for all i ? [ N ], define the row vector bi ? R(1?m ) to have components bih = P ( Ti = ti|Hi = h,Ai = ai ) for h ? [ m ]. Then for all i ? [ N ], f i = bi(G(ai))?1.
It follows immediately that f1c1a1 = b 1(G(a1))?1Ga1?a1 = p(r1 . . . rN ) This lemma shows a direct link between the vectors f i calculated in the algorithm , and the terms bih , which are terms calculated by the conventional inside algorithm : each f i is a linear transformation ( through Gai ) of the corresponding vector bi.
Proof : The proof is by induction.
First consider the base case . For any leaf?i.e ., for any i such that ai ? P?we have bih = q(ri|h , ai ), and it is easily verified that f i = bi(G(ai))?1.
The inductive case is as follows . For all i ? [ N ] such that ai ? I , by the definition in the algorithm , f i = f?Cri(f ?) = f?Ga?T ridiag(f?Ga?Sri)Qri(Gai)?1 Assuming by induction that f ? = b?(G(a ? ))?1 and f ? = b?(G(a?))?1, this simplifies to f i = ? rdiag(?l)Qri(Gai)?1 (10) where ? r = b?T ri , and ? l = b?Sri . ? r is a row vector with components ? rh = ? h??[m ] b ? h?T ri h?,h = ? h??[m ] b ? h?t(h?|h , ri ). Similarly , ? l is a row vector with components equal to ? lh = ? h??[m ] b ? h?S ri h?,h = ? h??[m ] b ? h?s(h?|h , ri ). It can then be verified that ? rdiag(?l)Qri is a row vector with components equal to ? rh?lhq(ri|h , ai).
But bih = q(ri|h , ai )? ( ? h??[m ] b ? h?t(h?|h , ri ) ) ? ( ? h??[m ] b ? h?s(h?|h , ri ) ) = q(ri|h , ai)?rh?lh , hence ? rdiag(?l)Qri = bi and the inductive case follows immediately from Eq . 10.
Next , we give a similar lemma , which implies the correctness of the algorithm in figure 3: Lemma 3 Assume that conditions 13 of theorem 1 are satisfied , and that the input to the algorithm in figure 3 is a sentence x1 . . . xN . For any a ? N , for any 1 ? i ? j ? N , define ?? a,i,j ? R(1?m ) to have components ?? a,i,jh = p(xi . . . xj|h , a ) for h ? [ m].
In addition , define ?? a,i,j ? R(m?1) to have components ?? a,i,jh = p(x1 . . . xi?1, a(h ), xj+1 . . . xN ) for h ? [ m ]. Then for all i ? [ N ], ? a,i,j = ?? a,i,j(Ga)?1 and ? a,i,j = Ga??a,i,j . It follows that for all ( a , i , j ), ?( a , i , j ) = ?? a,i,j(Ga)?1Ga??a,i,j = ?? a,i,j ?? a,i,j = ? h ?? a,i,jh ?? a,i,j h = ? ?? T ( x):(a,i,j )?? p (?) Thus the vectors ? a,i,j and ? a,i,j are linearly related to the vectors ?? a,i,j and ?? a,i,j , which are the inside and outside terms calculated by the conventional form of the insideoutside algorithm.
The proof is by induction , and is similar to the proof of lemma 2; for reasons of space it is omitted.
9.2 Proof of the Identity in Eq . 6
We now prove the identity in Eq . 6, used in the proof of theorem 2. For reasons of space , we do not give the proofs of identities 79: the proofs are similar.
The following identities can be verified : P ( R1 = a ? b c|H1 = h,A1 = a ) = q(a ? b c|h , a ) E [ Y3,j|H1 = h,R1 = a ? b c ] = Ea?b cj,h
E [ Zk|H1 = h,R1 = a ? b c ] = Kak,h
E [ Y2,l|H1 = h,R1 = a ? b c ] = F a?b cl,h where Ea?b c = GcT a?b c , F a?b c = GbSa?b c.
Y3, Z and Y2 are independent when conditioned on H1, R1 ( this follows from the independence assumptions in the LPCFG ), hence E [[[ R1 = a ? b c]]Y3,jZkY2,l | H1 = h,A1 = a ] = q(a ? b c|h , a)Ea?b cj,h Kak,hF a?b cl,h Hence ( recall that ? ah = P ( H1 = h|A1 = a )), Da?b cj,k,l = E [[[ R1 = a ? b c]]Y3,jZkY2,l | A1 = a ] = ? h ? ahE [[[ R1 = a ? b c]]Y3,jZkY2,l | H1 = h,A1 = a ] = ? h ? ahq(a ? b c|h , a)Ea?b cj,h Kak,hF a?b cl,h (11) from which Eq . 6 follows.
230
Acknowledgements : Columbia University gratefully acknowledges the support of the Defense Advanced Research Projects Agency ( DARPA ) Machine Reading Program under Air Force Research Laboratory ( AFRL ) prime contract no . FA8750-09-C-0181. Any opinions , findings , and conclusions or recommendations expressed in this material are those of the author(s ) and do not necessarily reflect the view of DARPA , AFRL , or the US government . Shay Cohen was supported by the National Science Foundation under Grant #1136996 to the Computing Research Association for the CIFellows Project.
Dean Foster was supported by National Science Foundation grant 1106743.
References
B . Balle , A . Quattoni , and X . Carreras . 2011. A spectral learning algorithm for finite state transducers . In
Proceedings of ECML.
S . Dasgupta . 1999. Learning mixtures of Gaussians . In
Proceedings of FOCS.
Dean P . Foster , Jordan Rodu , and Lyle H . Ungar.
2012. Spectral dimensionality reduction for hmms.
arXiv:1203.6130v1.
J . Goodman . 1996. Parsing algorithms and metrics . In Proceedings of the 34th annual meeting on Association for Computational Linguistics , pages 177?183.
Association for Computational Linguistics.
D . Hsu , S . M . Kakade , and T . Zhang . 2009. A spectral algorithm for learning hidden Markov models . In
Proceedings of COLT.
H . Jaeger . 2000. Observable operator models for discrete stochastic time series . Neural Computation , 12(6).
F . M . Lugue , A . Quattoni , B . Balle , and X . Carreras.
2012. Spectral learning for nondeterministic dependency parsing . In Proceedings of EACL.
T . Matsuzaki , Y . Miyao , and J . Tsujii . 2005. Probabilistic CFG with latent annotations . In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics , pages 75?82. Association for
Computational Linguistics.
A . Parikh , L . Song , and E . P . Xing . 2011. A spectral algorithm for latent tree graphical models . In Proceedings of The 28th International Conference on Machine
Learningy ( ICML 2011).
F . Pereira and Y . Schabes . 1992. Inside-outside reestimation from partially bracketed corpora . In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics , pages 128?135, Newark , Delaware , USA , June . Association for Computational
Linguistics.
S . Petrov , L . Barrett , R . Thibaux , and D . Klein . 2006.
Learning accurate , compact , and interpretable tree annotation . In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics , pages 433?440, Sydney , Australia , July.
Association for Computational Linguistics.
S . A . Terwijn . 2002. On the learnability of hidden markov models . In Grammatical Inference : Algorithms and Applications ( Amsterdam , 2002), volume 2484 of Lecture Notes in Artificial Intelligence , pages 261?268, Berlin . Springer.
S . Vempala and G . Wang . 2004. A spectral algorithm for learning mixtures of distributions . Journal of Computer and System Sciences , 68(4):841?860.
231
