Lexicalized Context-Free Grammars
Yves Schabes and Richard C . Waters
Mitsubishi Electric Research Laboratories
201 Broadway , Cambridge , MA 02139
email : s(;habes@merl . com and dick((~merl . coin Lexicalized contextfree grammar ( LCFG ) is an attractive compromise between the parsing efficiency of contextfree grammar  ( CFC ) and the elegance and lexical sensitivity of lexicalized tree-adjoinin grammar  ( LTAG )  . LCFC is a restricted form of LTAG that can only generate contextfree languages and can be parsed in cubic time  . 
However , LCF ( I supports much of the elegance of LTAG's analysis of English and shares with LTAG the ability to lexicalize CF  ( I ; s without changing the trees generated . 

Context-free grammar ( CFG ) has been a well accepted framework for computational linguistics for a long time  . While it has drawbacks , including the inability to expressome linguistic constructions  , it has the virtue of being computationally efficient  , 
O(n3)-time in the worst case.
Recently there has been again in interest in the so called ' mildly ' context-sensitive formalisms  ( Vijay-Shanker , 1987; Weir , 1988; Joshi , Vijay-Shanker , and Weir , 1991; Vijay-Shanker and Weir , 1993a ) that generate only a small superset of contextfree languages  . One such formalism is lexicalized tree-adjoining grammar  ( LTAG )   ( Schabes , Abeill ~ , and Joshi , 1988; Abeillfi et al , 1990; Joshi and Schabes ,  1992) , which provides a number of attractive properties at the cost of decreased efficiency  , O(n6)-time in the worst case ( Vijay-Shanker , 1987; Schabes , 1991; Lang , 1990; Vijay-
Shanker and Weir , 1993b).
An LTAG lexicon consists of a set of trees each of which contains one or more lexical items  . These elementary trees can be viewed as the elementary clauses  ( including their transformational variants ) in which the lexical items participate . The trees are combined by substitution and adjunction  . 
LTAC supports context-sensitive features that can capture some language constructs not captured by CFG  . However , the greatest virtue of LTAG is that it is lexicalized and supports extended domains of locality  . The lexical nature of LTAC is of linguistic interest  , since it is believed that the descriptions of many linguistic phenomena are dependent upon lexical data  . The extended domains allow for the localization of most syntactic and semantic dependencies  ( e . g . , filler-gap and predicate-argument relationships ) . 
A fllrther interesting aspect of LTAG is its ability to lexicalize CFCs  . One can convert a CFC into an LTAG that preserves the original trees  ( Joshi and Schabes ,  1992) . 
Lexicalized contextfree grammar ( LCFG ) is an attractive compromise between LTAG and CFG , that combines many of the virtues of LTAG with the efficiency of CFG  . LCFC is a restricted form of LTAG that places further limits on the elementary trees that are possible and on the way adjunction can be performed  . These restrictions limit LCFG to producing only contextfree languages and allow LCFC to be parsed in O  ( n3 ) -time in the worst ease . However , LCFC retains most of the key features of LTAG enumerated above  . 
In particular , most of the current LTAG grammar for English ( Abeilld et al , 1990) follows the restrictions of LCFG . This is of significant practical interest because it means that the processing of these analyses does not require more computational resources than CFGs  . 
In addition , any CFG can be transformed into an equivalent LCFC that generates the same trees  ( and therefore the same strings )  . This result breaks new ground , because here to for ev-ery method of lexicalizing CFCs required context-sensitive operations  ( Joshi and Schabes ,  1992) . 
The following sections briefly , define LCFG , discuss its relationship to the current LTAG grammar for English  , prove that LC , FC can be used to lexicalize CFC , and present a simple cubic-time parser for LCFC . These topics are discussed in greater detail in Schabes and Waters  ( 1993 )  . 

Lexicalized Context-Free Grammars Like an LTAG  , an LC'FG consists of two sets of trees : initial trees  , which are combined by substitution and auxiliary trees  , which are combined by adjunction . An LCFG is lexicalized in the sense that every initial and auxiliary tree is required to contain at least one terminal symbol on its frontier  . 
More precisely , an LCFG is a five-tuple ( Z , NT , I , A ,   , 5') , where ~ is a set of terminal symbols , NT is a set of nonterminal symbols , I and A are sets of trees labeled by terminal and nonterminal symbols  , and , 5' is a distinguished nonterminal start symbol . 
Each initial tree in the set I satisfies the following requirements  . 
( i ) Interior nodes are labeled by nonterminal symbols . 
( ii ) The nodes on the frontier of the tree consist of zero or more nonterminal symbols and one or more terminal symbols  . 
( iii ) The nonterminal symbols on the frontier are marked for substitution  . By convention , this is annotated in diagrams using a downarrow ( l )  . 
Each auxiliary tree in the set A satisfies the following requirements  . 
( i ) Interior nodes are labeled by nonterminal symbols . 
( ii ) The nodes on the frontier consist of zero or more nonterminal symbols and one or more terminal symbols  . 
( iii ) All but one of the nonterminal symbols on the frontier are marked for substitution  . 
( iv ) The remaining nonterminal on the frontier of the tree is called the foot  . The label on the foot must be identical to the label on the root node of the tree  . 
By convention , the foot is indicated in diagrams using an asterisk  (  . ) . 
( v ) the foot must be in either the leftmost or the rightmost position on the frontier  . 
Figure 1 , shows seven elementary trees that might appear in an LCFG for English  . The trees containing ' boy ' , ' saw ' , and ' left ' are initial trees . 
The remainder are att xiliary trees.
Auxiliary trees whose feet are leftrnost are called left recursive  . Similarly , auxiliary trees whose feet are rightrnost are called righl recursive auxiliary trees  . The path from the root of an auxiliary tree to the foot is called the spine  . 
NP VP NVP
A/kA/X
D$N VVP*AN*VP*Adv
III iboy seems pretty smoothly

SNP i,~(+wh)SS
NP o $ VP NP o VP NP o $ VP
AI\[
VSI * NA ? iVV NPI$
III think left saw
Figure 1: Sample trees.
In LCF ( I , trees can be combined with substitution and adjunction  . As illustrated in Figure 2 , substitution replaces a node marked for substitution with a copy of an initial tree  . 
Adjunction inserts a copy of an auxiliary tree into another tree in place of an interior node that has the same label as the foot of the auxiliary tree  . 
The subtree that was previously connected to the interior node is reconnected to the foot of the copy of the auxiliary tree  . If the auxiliary tree is left recursive , this is referred to as left recursive adjunction ( see Figure 3 )  . If the auxiliary tree is right recursive , this is referred to as right recursive adjunction  ( see Figure 4 )  . 
Crucially , adjunction is constrained by requiring that a left recursive auxiliary tree cannot be adjoined on any node that is on the spine of a right recursive auxiliary tree and a right recursive auxiliary tree cannot be adjoined on the spine of a left recursive auxiliary tree  . 
An LCFG derivation must start with an initial tree rooted in S  . After that , this tree can be repeatedly extended using substitution and adjunction  . A derivation is complete when every frontier node is labeled with a terminal symbol  . 
The difference between LCFG and LTAG is
Figure 2: Substitution.
122/AA

Figure 3: Left recursive adjunction.
~ A *=" A%
Figure 4: Right recursive adjunction.
that LTAG allows the foot of an auxiliary tree to appear anywhere on the frontier and places no limitations on the interaction of auxiliary trees  . 
In this unlimited situation , adjunction encodes string wrapping and is therefore more powerful than concatenation  ( see Figure 5 )  . However , the restrictions imposed by LCFG guarantee that no context-sensitive operations can be achieved  . 
They limit the languages that can be generated by LCFGs to those that can be generated by CFGs  . 
Coverage of LCFG and LTAG
The power of LCFG is significantly less than LTAG . Surprisingly , it turns out that there are only two situations where the current LTAG grammar for English  ( Abeilld et al ,  1990 ) fails to satisfy the restrictions imposed by LCFG . 
The first situation , concerns certain verbs that take more than one sentential complement  . An example of such a verb is deduce , which is associated with the following auxiliary tree  . 

NP o $ VP
VS l * PP
IA de duce PSz , I,
I from
Since this tree contains a foot node in the center of its frontier  , it is not part of an LCFG . Having the foot on the first sential complement is convenient  , because it allows one to use the standard LTAG wh -analyses  , which depends on the w2~W4%
Figure 5: Adjunction in LTAG.
existence of an initial tree where the filler and gap are local  . This accounts nicely for the pair of sentences below  . However , other analyses of wh questions may not require the use of the auxiliary tree above  . 
(1 ) John deduced that Mary watered the grass from seeing the hose  . 
(2 ) What did John deduce that Mary wa-tered from seeing the hose  . 
The second situation , concerns the way the current LTAG explains the ambiguous attachments of adverbial modifiers  . For example , in the sentence : ( 3 ) John said Bill left yesterday . 
the attachment of yesterday is ambiguous . The two different LTAG derivations indicated in Figure  6 represent his conveniently . 
Unfortunately , in LCFG the high attachment of yesterday is for bidden since a right auxiliary tree  ( corresponding to yesterday ) is adjoined on the spines of a left auxiliary tree  ( corresponding to John said )  . However , one could avoid this problem by designing a mechanism to recover the high attachment reading from the low one  . 
Besides the two cases presented above , the current LTAG for English uses only left and right recursive auxiliary trees and does not allow any 

NP ..~", ~........

John VS * .   .   .   .   .   .   .   .   .   .   .   . VP "-:: . A said S . ""'?/\,~ to-VP*ADV
NP VPI
II yesterday
Bill V
I left
Figure 6: Two LTAG derivations for John said Bill left yesterday  . 
1 23 interaction along the spine of these two kinds of trees  . This agrees with the intuition that most English analyses do not require a context-sensitive operation  . 
LCFG . However , as shown below , combining extended substitution with restricted adjunction allows strong lexicalization of CFG  , without introducing greater parsing complexity than CFG  . 
Lexicalization of CFGs
The lexicalization of grammar formalisms is of interest from a number of perspectives  . It is of interest from a linguistic perspective , because most current linguistic theories give lexical accounts of a number of phenomena that used to be considered purely syntactic  . It is of interest from a computational perspective  , because lexicalized grammars can be parsed significantly more efficiently than nonlexicalized ones  ( Schabes and Joshi ,  1990) . 
Formally , a grammar is said ' lexicalized ' ( Schabes , Abeill ~ . , and Joshi , 1988) if it consists of: , , a finite set of elementary structures of finite size  , each of which c , ontains an overt ( i . e . , nonempty ) lexical item . 
? a finite set of operations for creating derived structures  . 
The overt lexical item in an elementary structure is referred to as its anchor  . A lexicalized grammar can be organized as a lexicon where each lexical item is associated with a finite number of structures for which that item is the anchor  . 
In general , CFGs are not lexicalized since rules such as , 5' --* NP VP that do not locally introduce lexical items are allowed  . In contrast , the wellknown Creibach Normal Form ( CNF ) for CFCs is lexicalized , because very production rule is required to be of the form A--+ac ~  ( where a is a terminal symbol , A a nonterminal symbol and a a possibly empty string of nonterminal symbols  ) and therefore locally introduces a lexical item a . 
It can be shown that for any CFG ( . 7 ( that does not derive the empty string ) , there is a CNF grammar ( . 7 ~ that derives the same language . However , it may be impossible for the set of trees produced by  ( 7 ~ to be the same as the set of trees produced by G  . 
Therefore , CNF achieves a kind of lexicalization of CFGs . However , it is only a weak lexicalization , because the set of trees is not necessarily preserved  . As discussed in the motivation section , strong lexicalization that preserves treesets is possible using LTAG  . However , this is achieved at the cost of significant additional parsing complexity  . 
Here to for e , several attempts have been made to lexicalize CFC with formalisms weaker than LTAG  , but without success . In particular , it is not sufficient omerely extend substitution so that it applies to trees  . Neither is its utficient or ely solely on the kind restricted adjunction used by Theorem If G =   ( ~ , NT , P , S ) is a finitely ambiguous CFG which does not generate the empty  . string (?) , then there is an LCFG (7~=(~ , NT , I , A , S ) generating the same language and tree set as ( 7 . Furthermore (7' can be chosen . so that it utilizes only lefl-recursive auxiliary trees  . 
As usual in the above , a CFG ( . 7 is a four-tuple , ( E , NT , P , S ) , where N is a set of terminal symbols , NT is a set of nonterminal symbols , P is a set of production rules that rewrite nonterminal symbols to strings of terminal and nonterminal symbols  , and S is a distinguished nonterminal symbol that is the start symbol of any derivation  . 
To prove the theorem we first prove a somewhat weaker theorem and then extend the proof to the flfll theorem  . In particular , we assume for the moment that the set of rules for  (  . 7 does not contain any empty rules of the form A ~ e  . 
Step 1We begin the construction of ( 7 ~ by constructing a directed graph LCG that we call the left corner derivation graph  . Paths in LCG correspond to leftmost paths from root to frontier in  ( partial ) derivation trees rooted at nonterminal symbols in  ( 1 . 
L ( TG contains a node for every symbol in EUNT and an arc for every rule in P as follows  . 
For each terminal and nonterminal symbol X in G create a node in LCG labeled with X  . For each rule X--+Ya inG create a directed arc labeled with X ~ Y a from the node labeled with X to the node labeled Y  . 
As an example , consider the example CFG in Figure 7 and the corresponding L ( TG shown in
Figure 8.
The significance of L ( ; G is that there is a one-to-one correspondence b tween paths in LCG ending on a nonterminal and left corner derivations in G  . A left corner derivation in a CFG is a partial derivation starting from any nonterminal where every expanded node  ( other than the root ) is the leftmost child of its parent and the left corner is a nonterminal  . A left corner derivation is uniquely identified by the list of rules applied  . Since G does not have any empty rules , every rule in (7 is represented in L(;'G . Therefore , every path in LCG ending on a terminal corresponds to a left corner derivation in  ( 7 and vice versa . 

S - - - + AA , 5'--+BA
A--+BB
B--+AS
B - - - - + b
Figure 7: An example grammar.
S---~BA
S-~.AA
S~-AB
B--+AS
B - -+ b b
Figure 8: The LC (; created by Step 1.
Step 2 The set of initial trees I for G'is constructed with reference to L  ( TG . In particular , an initial tree is created corresponding to each noncyclic path in L  ( /G that starts at a nonterminal symbol X and ends on a terminal symbol y  .   ( A noncyclic path is a path that does not touch any node twice  . ) For each noncyclic path in LCG from X to y , construct an initial tree T as follows . Start with a root labeled X . Apply the rules in the path one after another , always expanding the left corner node of T . While doing this , leave all the non-left corner nonterminal symbols in T unexpanded  , and label them as substitution nodes . 
Given the previous example grammar , this step produces the initial trees shown in Figure  9  . 
Each initial tree created is lexicalized , because each one has a nonterminal symbol as the left corner element of its frontier  . There are a finite number of initial trees , because the number of noncyclic paths in LCG must be finite  . Each initial tree is finite in size , because a chnoncyclic path in LCG is finite in length  . 
Most importantly , The set of initial trees is the set of nonrecursive left corner derivations in  (  , ' . 

AAS
BB $ BAS BB $
IIIbbb
Figure 9: Initial trees created by Step 2.
Step 3 This step constructs a set of left-recursive auxiliary trees corresponding to the cyclic path segments in L  ( TG that were ignored in the previous step . In particular , an att xiliary tree is created corresponding to each minimM cyclic path in LCG that starts at a non-term in M symbol  . 
For each minimal cycle in LCG from X to itself , construct an auxiliary tree T by starting with a root labeled X and repeatedly expanding left  , corner frontier nodes using the rules in the path as in Step  2  . When all the rules in the path have been used , the left corner frontier node in T will be labeled X  . Mark this as the foot node of T . While doing the above , leave all the other nonterminal symbols in T unexpanded  , and label them all substitution nodes . 
The LC ( ; in Figure 8 has two minimal cyclic paths ( one from A to Avia B and one from B to B via A )  . This leads to the the two auxiliary trees shown in Figure  10  , one for A and one for B . 
The att xiliary trees generated in this step are not  , necessarily lexicalized . There are a finite number of auxiliary trees , since the number of minimal cyclic paths in G must be finite  . Each auxiliary tree is finite in size , because a ch minimal-cycle in
LCG is finite in length.
The set of trees that can he created by corn -biding the initial trees from Step  2 with the auxiliary trees from Step 3 by adjoining auxiliary trees along the left edge is the set of every left corner derivation in  (  , ' . To see this , consider that every path in L ( ; G can be represented as an initial noncyclic path with zero or more minimal cycles inserted into it  . 
The set of trees that can be created by corn -biding the initial trees from Step  2 with the auxiliary trees from Step 3 using both substitution and adjunction is the set of every derivation in G  . To see this , consider that every derivation in G can be decomposed into a set of left corner derivations in G that are combined with substitution  . In particular , whenever a nonterminal node is not the leftmost child of its parent  , it is the head of a sep-
A B
BB $ AS $
A*S $ B*B$
Figure 10: Auxiliary trees created by Step 3.
125 arateleft corner derivation.
Step 4 This step lexicalizes the set of auxiliary trees built in step  3  , without altering the trees that can be derived . 
For each auxiliary tree T built in step 3 , consider the frontier node A just to the right of the foot  . If this node is a terminal do nothing . 
Otherwise , remove T from the set of auxiliary trees replace it with every tree that can be constructed by substituting one of the initial trees created in Step  2 for the node A in T . 
In the case of our continuing example , Step 4 results in the set of auxiliary trees in Figure 11  . 
Note that since G is finitely ambiguous , there must be a frontier node to the right of the foot of an att xiliary tree T  . If not , then T would correspond to a derivation X : ~ X in G and  6' would be infinitely ambiguous . 
After Step 4 , every auxiliary tree is lexicalized , since every tree that does not have a terminal to the right of its foot is replaced by one or more trees that do  . Since there were only a finite number of finite initial and auxiliary trees to start with  , there are still only a finite number of finite att xiliary trees  . 
The change in the auxiliary trees caused by Step 4 does not alter the set of trees that can be produced in any way  , because the only change that was made was to make substitutions that could be made anyway  , and when a substitutable node was eliminated , this was only done after every possible substitution at that node was performed  . 
Note that the initial trees are left anchored and the auxiliary trees are ahnost left anchored in the sense that the leftmost frontier node other than the foot is a terminal  . This facilitates efficient left to right parsing . 


BB $
A * SBB $
AASA * SAS $
BB $ BAS B*B
IIIbbb
Figure 1l : Auxiliary trees created by Step 4.
The procedure above creates a lexicalized grammar that generates exactly the same trees as G and therefore the same strings  . The only remaining issue is the additional assumption that G does not contain any empty rules  . 
If ( ; contains an empty rule A ~ e one first uses standard methods to transform  (  ; into an equivalent grammar H that does not have any such rule  . When doing this , create a table showing how each new rule added is related to the empty rules removed  . Lexicalize H producing H ' using the procedure above  . Derivations in H ' result in elements of the tree set of H  . By means of the table recording the relationship between  (  ; and H , these trees can be converted to derivations in G . 

Additional issues
There are several places in the algorithm where greater freedom of choice is possible  . For instance , when lexicalizing the auxiliary trees created in Step  3  , you need not do anything if there is any frontier node that is a terminal and you can choose to expand any frontier node you want  . For instance you might want to choose the node that corresponds to the smallest number of initial trees  . 
Alternatively , everywhere in the procedure , the word ' left ' can be replaced by ' right ' and vice versa  . This results in the creation of a set of right anchored initial trees and right recursive auxiliary trees  . This can be of interest when the right corner derivation graph has less cycles than the left corner one  . 
The number of trees in G ' is related to the number of noncyclic and minimal cycle paths in LCG  . In the worst case , this number rises very fast as a function of the number of arcs in LCG  , ( i . e . , in the number of rules in G ) .   ( A fully connected graph of n 2 arcs between n nodes has n!acyclic paths and n ! minimal cycles  . ) However , in the typical case , this kind of an explosion of trees is unlikely . 
Just as there can be many ways for a CF ( ~ to derive a given string , there can be many ways for an LCFG to derive a given tree  . For maximal efficiency , it would be desirable for the grammar G ' produced by the procedure above to have no ambiguity in they way trees are derived  . Unfortunately , the longer the minimal cycles in LCG , the greater the tree-generating ambiguity the procedure will introduce in G '  . However , by modifying the procedure to make use of constraints on what att xiliary trees are allowed to adjoin on what nodes in which initial trees  , it should be possible to reduce or even eliminate this ambiguity  . 
All these issues are discussed at greater length
Parsing LCFG
Since LCFG is a restricted case of tree-adjoining grammar  ( TAG )  , standard O(nG)-time TAG parsers ( Vijay-Shanker , 1987; Schabes , 1991; Lang , 1990) can be used for parsing LCFG . Further , they can be straightforwardly modified to require at most O  ( n4 ) -tirne when applied to LCFG . 
However , this still does not take fifll advantage of the context-freeness of LCFC  . 
This section describes a simple I ) ottom-up recognizer for LCFG that is in the style of the CKY parser for  ( IT ( I ;  . The virtue of this algorithm is that it shows in a simple manner how the O  ( n3 ) -time worst case complexity can be achieved for LCFG  . Schabes and Waters ( 1993 ) describes a more practical and more elaborate ( Earley-style ) recognizer for LCFC , which achieves the same bounds . 
Suppose that G = ( E , NT , I , A , S ) is an LCFG and that al '" a , ~ is an input string . We can assume without loss of generality 1 that every node in IUA has at most two children . 
Let 71 be a node in an elementary tree ( identified by the name of the tree and the position of the node in the tree  )  . The central concept of the al-go rithrn is the concepts of spanning and covering  . 
71 spans a string ai+l .   .   . aj if and only if there is some tree derived by ( ' ; for which it is the case that the fringe of the subtree rooted at  71 is ai+l """ aj . 
In particular , a nonterminal node spans ajif and only if the label on the node is a j  . A non-terrninal node spans ai+1 .   .   . aj if and only if ai + l .   .   . aj is the concatenation i left , to right order of strings spanned by the children of the node  . 
? If 7 /does not subsume the foot node of an auxiliary tree then :  71 covers the string ai + 1   .   .   . aj if and only if it span sai+l " . aj . 
? If 7 /is on the spine of a right recursive auxiliary tree T then:  71 covers ai+l . "  . aj if and only if 7 /spans some strin ~ that is the concatenation of ai+l-  .   . aj and a string spanned by the foot of T .   ( This situation is illustrated by the right drawing in Figure  12  , in which 7/ is labeled with
B . ) ? If 71 is on the spine of a left recursive auxiliary tree T then:  71 covers ai+\]" . aj if and only if 71 spans some string that is the concatenation of a string spanned by the foot of T and ai+l  .   .   . aj . 
(This situation is illustrated by the left drawing in Figure  12  , in which 71 is labeled with B . ) lit can be easily shown that by adding new nodes  ( '4"~ any L , F ( . , can be transformed into an equivalent
LC , FG satisfying this condition.
A , ai+l-.-ajai+l .., ajA*
Figure 12: Coverage of nodes on the spine.
The algorithm stores pairs of the form (71 , pos ) in an n by n array C . In a pair , posise ithert ( for top ) or b ( for bottom ) . For every node 7 line very elementary tree in ( ; , the algorithm guarantees the following . 
?(' l , b ) eC\[i , j \] if and only if , Icoversai + l .   .   . aj . 
?(' l , t)EC\[i , j \] if and only if (' l , bEC , \[i , j \] orai + l .   .   . aj is the concatenation ( in either order ) of a string covered by 7 / and a string covered by an auxiliary tree that can be adjoined on  71   . 
The algorithm fills the upper diagonal portion of the array C\[i  , j \] ( 0 < i < j_<n ) for increasing values of j-i . The proces starts by placing each foot node in every cell C'\[i  , i \] and each terminal node 71 in every cell C\[i , i + 1\] where 71 is labeled ai+l ? The algorithm then considers all possible ways of combining covers into longer covers  . In particular , it fills the cells C\[i , i+k\]for increasing values of k by combining elements from the cells C\[i  , j \] and C\[j , i + k \] for all j such that i < j < i+k . 
There are three situations where combination is possible : sibling concatenation  , left recursive concatenation , and right recursive concatenation . 
Sibling concatenation is illustrated in Figure 13 . Suppose that there is a node 7/0 ( labeled B ) with two children 711 ( labeled A ) and 712 ( labeled A ' )  . If (711 , t)EC\[i , j \] and ('12 , tE(7\[j , i + k \] then ('1 o , b ) EC\[i , i + k\] . 
Left recursive concatenation is illustrated in Figure  14  . Here , the cover of a node is combined with the cover of a left auxiliary tree that can be adjoined at the node  . Right recursive concatenation , which is shown in Figure 15 is analogous . 
For simplicity , the recognizer is written in two parts . A main procedure and a subprocedure Add ( node , pos , i , j ) , which adds the pair ( node , pos ) into C\[i , j \] . 
a . . . . a . ai+1 t + lJ aj + l'"ak "'" ak
Figure 13: Sibling concatenation.

Procedure recognizer begin ; ; foot node initialization ( , \[ z , i \] ) for i = 0 ton for all foot node 71 in Acall
Add0/ , b , i , i ); ; terminal node initialization ((;\[ i , i+1\] ) for i=0 to n-l for all node 71 in AUI labeled by ai+l call Add0/  , t , i , i+1);; induction(G'\[i , i + k\]=(;\[i , j\]+(:\[j , i + k\] ) for k = 2 to n for i = 0 to n-k for j = i + 1 to i+k-1   ; ; sibling concatenation if (711 , l ) 6C , \[ i , j \] and (712 , t ) eC\[j , i+k\]and r/1 is the left sibling of 7/2 with common parent 71o then Add ( 710  , b , i , i+k );; left recursive concatenation if 71 , b ) EC\[i , j \] and ( p , t e ( , '\[/ , i+k \] and p is the root node of a left recursive auxiliary tree that can adjoin on rl then  Add0j   , t , i , i+k );; right recursive concatenation if ' l , b ) e(;\[j , i+k\]and(p , t)EC\[i , j \] and p is the root node of a right recursive auxiliary tree that can adjoin on  7 I then Add ( r / , t , i , i+k ) if (7/ , z ) ec\[0 , 7 q and 71 is labeled by , 5' and 71 is the root node of an initial tree in I then return acceptance otherwise return rejection end Note that the sole purl  ) ose of the codest and bisto insure that only one auxiliary tree can adjoin on a node  . The procedure could easily be modified to account for other constraints on the way derivation should proceed such as those suggested for LTAGs  ( Schabes and Shieber ,  1992) . 
The procedure Add puts a pair into the array C . If the pair is already present , nothing is ( lone . 
However , if it is new , it is added to ( 7 and other pairs may be added as well . These correspond to cases where the coverage is not increased : when a node is the only child of its parent  , when the
A/2 .,+/2 ...
ai+l ... ~ A*a .... akJ + lai+l'"ak
Figure 14: Left recursive concatenation.
A/2 . ,  .   .   .   . A * . . . akaj + i . , a k ai + 1 ajai + 1 Figure 15: Right recursive concatenation . 
node is recognized without adjunction , and when substitution occurs . 
Procedure Add(r /, pos,i,j ) begin
Put(rl , pos ) in C , \[ i , j \] if pos = t and rI is the only child of a parentIt callAdd  ( # , b , i , j ) if pos = t and r ? is the root node of an initial tree  , for each substitution node pat which 71 can substitute call Add ( p , t , i , j ) ; ; no adjunction if pos = b if the node 7/does not have an OA constraint call Add ( r / , t , i , j ) end The O ( n3 ) complexity of the recognizer follows from the three nested induction loops on k  , i and j .   ( Although the procedure Add is defined recursively  , the number of pairs added to ( 7 is bounded by a constant hat is independent of sentence length  . ) By recording how each pair was introduced in each cell of the array C  , one can easily extend the recognizer to produce all derivations of the input  . 

LCFG combines much of the power of LTAG with tile computational efficiency of CFG  . It supports most of the same linguistic analysis supported by LTAC  . In particular , most of the current LTAG for English falls into LCFG  . In addition , LCFC can lexicalize CFG without altering the trees produced  . Finally , LCFG can be parsed in O(n3)-time . 
There are many directions in which the work on LCFG described here could be extended  . Insions , LP~parsing , and nondeterministic LR parsing . 

We thank John Coleman who , by questioning whether the context sensitivity of stochastic LTAG was actually being used for English  , triggered this work . We thank Aravind Joshi , Fernando Pereira , Stuart Shieber and B . Srinivas for valuable discussions . 

Abeilld , Anne , Kathleen M . Bishop , Sharon Cote , and Yves Schabes .  1990 . A lexicalized tree adjoining grammar for English . Technical Report MS-CIS-90-24 , Department of Computer and Information Science , University of Pennsylvania . 
Joshi , Aravind K . and Yves Schabes .  1992 . Tree-adjoining grammars and lexicalized grammars . In Maurice Nivat and Andreas Podel-ski , editors , Tree Automata and Languages . 
Elsevier Science.
Joshi , Aravind K ., K . Vijay-Shanker , and David
Weir .  1991 . The convergence of mildly context-sensitive grammatical formalisms  . In Peter Sells , Stuart Shieber , and Tom Wasow , editors , Foundational Issues in Nalural Language Processing  . MIT Press , Cambridge MA . 
Lang , Bernard .  1990 . The systematic on struc-tions of Earley parsers : Application to the production of O  ( n 6 ) Earley parsers for Tree Adjoining Grammars . In Proceedings of the Ist International Workshop on Tree Adjoining Grammars  , Dagstuhl C , astle , FRG , August . 
Schabes , Yves , Anne Abeill 6, and Aravind K.
Joshi .  1988 . Parsing strategies with ' lexicalized'grammars : Application to tree adjoining grammars  . In Proceedings of the 12 th International Conference on Computational Linguistics  ( COLING'88 )  , Budapest , Hungary , An-gust . 
Schabes , Yves and Aravind K . Joshi .  1990 . Parsing with lexicalized tree adjoining grammar . 
In Masaru Tomita , editor , C , urrent Issues in Parsing Technologies . Kluwer Accademic

Schabes , Yves and Stuart Shieber .  1992 . An alternative conception of tree-adjoining derivation  . In 20th Meeting of the Association for ( , ' omputational Linguistics ( ACL'92) . 
Schabes , Yves and Richard C . Waters .  1993 . Lexicalized contextfree grammar : A cubic-time parsable formalism that strongly lexicalizes contextfree grammar  . Technical Report 93-04 , Mitsubishi Electric Research Laboratories , 201 Broadway . Cambridge MA 02139 . 
Schabes , Yves .  1991 . The valid prefix property and left to right parsing of tree-adjoining grammar  . In Proceedings of the second International Workshop on Parsing Technologies  , 
Cancan , Mexico , February.
Vijay-Shanker , K . and David Weir . 1993a . The equivalence of four extensions of contextfree grammars  . To appear in Mathematical Systems Theory . 
Vijay-Shanker , K . and \[) avid Weir . 1993b . Parsing some constrained grammar formalisms . 
To appear in Computational Linguistics.
Vijay-Shanker , K .  1987 . A Study of Tree Adjoining Grammars . Ph . D . thesis , Department of Computer and Information Science , University of Pennsylvania . 
Weir , David J .  1988 . Characterizing Mildly Context- , 5 ? nsitive Grammar Formalisms . Ph . D . thesis , Department of Computer and Information Science , University of


