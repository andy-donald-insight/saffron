Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP , pages 181?189,
Suntec , Singapore , 27 August 2009. c?2009 ACL and AFNLP
Active Learning for Multilingual Statistical Machine Translation?
Gholamreza Haffari and Anoop Sarkar
School of Computing Science , Simon Fraser University
British Columbia , Canada
{ghaffar1,anoop}@cs.sfu.ca
Abstract
Statistical machine translation ( SMT ) models require bilingual corpora for training , and these corpora are often multilingual with parallel text in multiple languages simultaneously . We introduce an active learning task of adding a new language to an existing multilingual set of parallel text and constructing high quality
MT systems , from each language in the collection into this new target language.
We show that adding a new language using active learning to the EuroParl corpus provides a significant improvement compared to a random sentence selection baseline.
We also provide new highly effective sentence selection methods that improve AL for phrasebased SMT in the multilingual and single language pair setting.
1 Introduction
The main source of training data for statistical machine translation ( SMT ) models is a parallel corpus . In many cases , the same information is available in multiple languages simultaneously as a multilingual parallel corpus , e.g ., European Parliament ( EuroParl ) and U.N . proceedings . In this paper , we consider how to use active learning ( AL ) in order to add a new language to such a multilingual parallel corpus and at the same time we construct an MT system from each language in the original corpus into this new target language . We introduce a novel combined measure of translation quality for multiple target language outputs ( the same content from multiple source languages).
The multilingual setting provides new opportunities for AL over and above a single language pair . This setting is similar to the multitask AL scenario ( Reichart et al , 2008). In our case , the multiple tasks are individual machine translation tasks for several language pairs . The nature of the translation processes vary from any of the source ? Thanks to James Peltier for systems support for our experiments . This research was partially supported by NSERC , Canada ( RGPIN : 264905) and an IBM Faculty Award.
languages to the new language depending on the characteristics of each source-target language pair , hence these tasks are competing for annotating the same resource . However it may be that in a single language pair , AL would pick a particular sentence for annotation , but in a multilingual setting , a different source language might be able to provide a good translation , thus saving annotation effort . In this paper , we explore how multiple MT systems can be used to effectively pick instances that are more likely to improve training quality.
Active learning is framed as an iterative learning process . In each iteration new human labeled instances ( manual translations ) are added to the training data based on their expected training quality . However , if we start with only a small amount of initial parallel data for the new target language , then translation quality is very poor and requires a very large injection of human labeled data to be effective . To deal with this , we use a novel framework for active learning : we assume we are given a small amount of parallel text and a large amount of monolingual source language text ; using these resources , we create a large noisy parallel text which we then iteratively improve using small injections of human translations . When we build multiple MT systems from multiple source languages to the new target language , each MT system can be seen as a different ? view ? on the desired output translation . Thus , we can train our multiple MT systems using either self-training or cotraining ( Blum and Mitchell , 1998). In self-training each MT system is retrained using human labeled data plus its own noisy translation output on the unlabeled data . In cotraining each MT system is retrained using human labeled data plus noisy translation output from the other MT systems in the ensemble . We use consensus translations ( He et al , 2008; Rosti et al , 2007; Matusov et al , 2006) as an effective method for cotraining between multiple MT systems.
This paper makes the following contributions : ? We provide a new framework for multilingual MT , in which we build multiple MT systems and add a new language to an existing multilingual parallel corpus . The multilingual set-which we exploit to improve translation quality while reducing annotation effort.
? We introduce new highly effective sentence selection methods that improve phrasebased SMT in the multilingual and single language pair setting.
? We describe a novel cotraining based active learning framework that exploits consensus translations to effectively select only those sentences that are difficult to translate for all MT systems , thus sharing annotation cost.
? We show that using active learning to add a new language to the EuroParl corpus provides a significant improvement compared to the strong random sentence selection baseline.
2 AL-SMT : Multilingual Setting
Consider a multilingual parallel corpus , such as EuroParl , which contains parallel sentences for several languages . Our goal is to add a new language to this corpus , and at the same time to construct high quality MT systems from the existing languages ( in the multilingual corpus ) to the new language . This goal is formalized by the following objective function:
O =
D ? d=1 ? d ? TQ(MF d?E ) (1) where F d?s are the source languages in the multilingual corpus ( D is the total number of languages ), and E is the new language . The translation quality is measured by TQ for individual sys-temsMF d?E ; it can be BLEU score or WER/PER ( Word error rate and position independent WER ) which induces a maximization or minimization problem , respectively . The nonnegative weights ? d reflect the importance of the different translation tasks and ? d ? d = 1. AL-SMT formulation for single language pair is a special case of this formulation where only one of the ? d?s in the objective function (1) is one and the rest are zero.
Moreover the algorithmic framework that we introduce in Sec . 2.1 for AL in the multilingual setting includes the single language pair setting as a special case ( Haffari et al , 2009).
We denote the large unlabeled multilingual corpus by U := {( f1j , .., f
D j )}, and the small labeled multilingual corpus by L := {( f1i , .., f
D i , ei )}. We overload the term entry to denote a tuple in L or in U ( it should be clear from the context ). For a single language pair we use U and L.
2.1 The Algorithmic Framework
Algorithm 1 represents our AL approach for the multilingual setting . We train our initial MT systems { MF d?E}
D d=1 on the multilingual corpus L , and use them to translate all monolingual sentences in U . We denote sentences in U together with their multiple translations by U + ( line 4 of Algorithm 1). Then we retrain the SMT systems on L ? U + and use the resulting model to decode the test set . Afterwards , we select and remove a subset of highly informative sentences from U , and add those sentences together with their human-provided translations to L . This process is continued iteratively until a certain level of translation quality is met ( we use the BLEU score , WER and PER ) ( Papineni et al , 2002). In the baseline , against which we compare our sentence selection methods , the sentences are chosen randomly.
When ( re-)training the models , two phrase tables are learned for each SMT model : one from the labeled data L and the other one from pseudo-labeled data U + ( which we call the main and auxiliary phrase tables respectively ). ( Ueffing et al , 2007; Haffari et al , 2009) show that treating U + as a source for a new feature function in a loglinear model for SMT ( Och and Ney , 2004) allows us to maximally take advantage of unlabeled data by finding a weight for this feature using minimum errorrate training ( MERT ) ( Och , 2003).
Since each entry in U + has multiple translations , there are two options when building the auxiliary table for a particular language pair ( F d , E ): ( i ) to use the corresponding translation ed of the source language in a self-training setting , or ( ii ) to use the consensus translation among all the translation candidates ( e1, .., eD ) in a cotraining setting ( sharing information between multiple SMT models).
A whole range of methods exist in the literature for combining the output translations of multiple MT systems for a single language pair , operating either at the sentence , phrase , or word level ( He et al ., 2008; Rosti et al , 2007; Matusov et al , 2006).
The method that we use in this work operates at the sentence level , and picks a single high quality translation from the union of the nbest lists generated by multiple SMT models . Sec . 5 gives 1: Given multilingual corpora L and U 2: { MF d?E}
D d=1 = multrain(L , ?) 3: for t = 1, 2, ... do 4: U + = multranslate(U , { MF d?E}
D d=1) 5: Select k sentences from U +, and ask a human for their true translations.
6: Remove the k sentences from U , and add the k sentence pairs ( translated by human ) to L 7: { MF d?E}
D d=1 = multrain(L,U + ) 8: Monitor the performance on the test set 9: end for more details about features which are used in our consensus finding method , and how it is trained.
Now let us address the important question of selecting highly informative sentences ( step 5 in the
Algorithm 1) in the following section.
3 Sentence Selection : Multiple Language
Pairs
The goal is to optimize the objective function (1) with minimum human effort in providing the translations . This motivates selecting sentences which are maximally beneficial for all the MT systems . In this section , we present several protocols for sentence selection based on the combined information from multiple language pairs.
3.1 Alternating Selection
The simplest selection protocol is to choose k sentences ( entries ) in the first iteration of AL which improve maximally the first modelMF 1?E , while ignoring other models . In the second iteration , the sentences are selected with respect to the second model , and so on ( Reichart et al , 2008).
3.2 Combined Ranking
Pick any AL-SMT scoring method for a single language pair ( see Sec . 4). Using this method , we rank the entries in unlabeled data U for each translation task defined by language pair ( F d , E ). This results in several ranking lists , each of which represents the importance of entries with respect to a particular translation task . We combine these rankings using a combined score:
Score ( ( f1, .., fD ) ) =
D ? d=1 ? dRankd(f d )
Rankd (.) is the ranking of a sentence in the list for the dth translation task ( Reichart et al , 2008).
3.3 Disagreement Among the Translations
Disagreement among the candidate translations of a particular entry is evidence for the difficulty of that entry for different translation models . The reason is that disagreement increases the possibility that most of the translations are not correct.
Therefore it would be beneficial to ask human for the translation of these hard entries.
Now the question is how to quantify the notion of disagreement among the candidate translations ( e1, .., eD ). We propose two measures of disagreement which are related to the portion of shared ngrams ( n ? 4) among the translations : ? Let ec be the consensus among all the candidate translations , then define the disagreement as ? d ? d ( 1? BLEU(ec , ed ) ) .
? Based on the disagreement of every pair of candidate translations : ? d ? d ? d ? ( 1 ?
BLEU(ed ? , ed ) ) .
For the single language pair setting , ( Haffari et al ., 2009) presents and compares several sentence selection methods for statistical phrasebased machine translation . We introduce novel techniques which outperform those methods in the next section.
4 Sentence Selection : Single Language
Pair
Phrases are basic units of translation in phrasebased SMT models . The phrases which may potentially be extracted from a sentence indicate its informativeness . The more new phrases a sentence can offer , the more informative it is ; since it boosts the generalization of the model . Additionally phrase translation probabilities need to be estimated accurately , which means sentences that offer phrases whose occurrences in the corpus were rare are informative . When selecting new sentences for human translation , we need to pay attention to this tradeoff between exploration and exploitation , i.e . selecting sentences to discover new phrases v.s . estimating accurately the phrase translation probabilities . Smoothing techniques partly handle accurate estimation of translation probabilities when the events occur rarely ( indeed it is the main reason for smoothing ). So we mainly focus on how to expand effectively the lexicon or set of phrases of the model.
The more frequent a phrase ( not a phrase pair ) is in the unlabeled data , the more important it is to it in test data ( specially when the test data is indomain with respect to unlabeled data ). The more frequent a phrase is in the labeled data , the more unimportant it is ; since probably we have observed most of its translations.
In the labeled dataL , phrases are the ones which are extracted by the SMT models ; but what are the candidate phrases in the unlabeled data U ? We use the currently trained SMT models to answer this question . Each translation in the nbest list of translations ( generated by the SMT models ) corresponds to a particular segmentation of a sentence , which breaks that sentence into several fragments ( see Fig . 1). Some of these fragments are the source language part of a phrase pair available in the phrase table , which we call regular phrases and denote their set byXregs for a sentence s . However , there are some fragments in the sentence which are not covered by the phrase table ? possibly because of the OOVs ( out-of-vocabulary words ) or the constraints imposed by the phrase extraction algorithm ? called Xoovs for a sentence s . Each member of Xoovs offers a set of potential phrases ( also referred to as OOV phrases ) which are not observed due to the latent segmentation of this fragment . We present two generative models for the phrases and show how to estimate and use them for sentence selection.
4.1 Model 1
In the first model , the generative story is to generate phrases for each sentence based on independent draws from a multinomial . The sample space of the multinomial consists of both regular and
OOV phrases.
We build two models , i.e . two multinomials , one for labeled data and the other one for unlabeled data . Each model is trained by maximizing the loglikelihood of its corresponding data:
LD := ? s?D ? P ( s ) ? x?Xs logP ( x|?D ) (2) where D is either L or U , ? P ( s ) is the empirical distribution of the sentences1, and ? D is the parameter vector of the corresponding probability 1P ? ( s ) is the number of times that the sentence s is seen in D divided by the number of all sentences in D.
distribution . When x ? Xoovs , we will have
P ( x|?U ) = ? h?Hx
P ( x , h|?U ) = ? h?Hx
P ( h)P ( x|h,?U ) = ? h?Hx ? y?Y hx ? U ( y ) (3) where Hx is the space of all possible segmentations for the OOV fragment x , Y hx is the resulting phrases from x based on the segmentation h , and ? U ( y ) is the probability of the OOV phrase y in the multinomial associated with U . We let Hx to be all possible segmentations of the fragment x for which the resulting phrase lengths are not greater than the maximum length constraint for phrase extraction in the underlying SMT model.
Since we do not know anything about the segmentations a priori , we have put a uniform distribution over such segmentations.
Maximizing (2) to find the maximum likelihood parameters for this model is an extremely difficult problem2. Therefore , we maximize the following lowerbound on the loglikelihood which is derived using Jensen?s inequality:
LD ? ? s?D ? P ( s ) [ ? x?Xregs log ? D(x ) + ? x?Xoovs ? h?Hx ? y?Y hx log ? D(y ) ] (4) Maximizing (4) amounts to set the probability of each regular / potential phrase proportional to its count / expected count in the data D.
Let ? k(xi:j ) be the number of possible segmentations from position i to position j of an OOV fragment x , and k is the maximum phrase length ; ? k(x1:|x |) = ? ?? ?? 0, if | x | = 0 1, if | x | = 1 ? k i=1 ? k(xi+1:|x |), otherwise which gives us a dynamic programming algorithm to compute the number of segmentation | Hx | = ? k(x1:|x |) of the OOV fragment x . The expected count of a potential phrase y based on an OOV segment x is ( see Fig . 1.c):
E[y|x ] = ? i?j ?[ y=xi:j ]? k(x1:i?1)?k(xj+1:|x |) ? k(x ) 2Setting partial derivatives of the Lagrangian to zero amounts to finding the roots of a system of multivariate polynomials ( a major topic in Algebraic Geometry).
184 i will go to school on friday
Regular Phrases
OOV segment go to school go to to school 2/3 2/3 1/3 1/3 1/3 i will in friday
XXX
XXX .01 .004 .
.
.
.
.
.
.
.
.
(a ) potential phr.
source target prob count ( b ) ( c)
Figure 1: The given sentence in ( b ) is segmented , based on the source side phrases extracted from the phrase table in ( a ), to yield regular phrases and OOV segment . The table in ( c ) shows the potential phrases extracted from the OOV segment ? go to school ? and their expected counts ( denoted by count ) where the maximum length for the potential phrases is set to 2. In the example , ? go to school ? has 3 segmentations with maximum phrase length 2: ( go)(to school ), ( go to)(school ), ( go)(to)(school).
where ?[ C ] is 1 if the condition C is true , and zero otherwise . We have used the fact that the number of occurrences of a phrase spanning the indices [ i , j ] is the product of the number of segmentations of the left and the right sub-fragments , which are ? k(x1:i?1) and ? k(xj+1:|x |) respectively.
4.2 Model 2
In the second model , we consider a mixture model of two multinomials responsible for generating phrases in each of the labeled and unlabeled data sets . To generate a phrase , we first toss a coin and depending on the outcome we either generate the phrase from the multinomial associated with regular phrases ? regU or potential phrases ? oov
U :
P ( x|?U ) := ? U ? reg
U ( x ) + (1? ? U )? oov
U ( x ) where ? U includes the mixing weight ? and the parameter vectors of the two multinomials . The mixture model associated with L is written similarly . The parameter estimation is based on maximizing a lowerbound on the loglikelihood which is similar to what was done for the Model 1.
4.3 Sentence Scoring
The sentence score is a linear combination of two terms : one coming from regular phrases and the other from OOV phrases : ?1(s ) := ? | Xregs | ? x?Xregs log
P ( x|?U )
P ( x|?L ) + 1? ? | Xoovs | ? x?Xoovs ? h?Hx log ? y?Y hx
P ( y|?U )
P ( y|?L ) where we use either Model 1 or Model 2 for P (.|? D ). The first term is the log probability ratio of regular phrases under phrase models corresponding to unlabeled and labeled data , and the second term is the expected log probability ratio ( ELPR ) under the two models . Another option for the contribution of OOV phrases is to take log of expected probability ratio ( LEPR ): ?2(s ) := ? | Xregs | ? x?Xregs log
P ( x|?U )
P ( x|?L ) + 1? ? | Xoovs | ? x?Xoovs log ? h?Hx ? y?Y hx
P ( y|?U )
P ( y|?L)
It is not difficult to prove that there is no difference between Model 1 and Model 2 when ELPR scoring is used for sentence selection . However , the situation is different for LEPR scoring : the two models produce different sentence rankings in this case.
5 Experiments
Corpora . We preprocessed the EuroParl corpus ( http://www.statmt.org/europarl ) ( Koehn , 2005) and built a multilingual parallel corpus with 653,513 sentences , excluding the Q4/2000 portion of the data (2000-10 to 2000-12) which is reserved as the test set . We subsampled 5,000 sentences as the labeled data L and 20,000 sentences as U for the pool of untranslated sentences ( while hiding the English part ). The test set consists of 2,000 multilanguage sentences and comes from the multilingual parallel corpus built from
Q4/2000 portion of the data.
Consensus Finding . Let T be the union of the nbest lists of translations for a particular sentence.
The consensus translation tc is argmax t?T w1
LM(t ) | t | + w2
Qd(t ) | t | + w3Rd(t)+w4,d where LM(t ) is the score from a 3gram language model , Qd(t ) is the translation score generated by the decoder for MF d?E if t is produced by the dth SMT model , Rd(t ) is the rank of the translation in the nbest list produced by the dth model , w4,d is a bias term for each translation model to make their scores comparable , and | t | is the length 22.7 22.8 22.9 23.2 23.3 23.4 23.5 23.6
Added Sentences
BLE
U Sc ore
French to English
Model 2 ? LEPRModel 1 ? ELPRGeom PhraseRandom 1000 2000 3000 4000 500023.2 23.4 23.6 23.8 24.4 24.6 24.8
BLE
U Sc ore
Spanish to English
Model 2 ? LEPRModel 1 ? ELPRGeom PhraseRandom 1000 2000 3000 4000 500016.2 16.4 16.6 16.8 17.4 17.6 17.8
Added Sentences
BLE
U Sc ore
German to English
Model 2 ? LEPRModel 1 ? ELPRGeom PhraseRandom Figure 2: The performance of different sentence selection strategies as the iteration of AL loop goes on for three translation tasks . Plots show the performance of sentence selection methods for single language pair in Sec . 4 compared to the GeomPhrase ( Haffari et al , 2009) and random sentence selection baseline.
of the translation sentence . The number of weights wi is 3 plus the number of source languages , and they are trained using minimum errorrate training ( MERT ) to maximize the BLEU score ( Och , 2003) on a development set.
Parameters . We use add - smoothing where  = .5 to smooth the probabilities in Sec . 4; moreover ? = .4 for ELPR and LEPR sentence scoring and maximum phrase length k is set to 4. For the multilingual experiments ( which involve four source languages ) we set ? d = .25 to make the importance of individual translation tasks equal.
0 1000 2000 3000 4000 500018 18.5
Added Sentences
Avg
BLEU Sco re
Mulilingual da?de?nl?sv to en Self?TrainingCo?Training Figure 3: Random sentence selection baseline using self-training and cotraining ( Germanic languages to English).
5.1 Results
First we evaluate the proposed sentence selection methods in Sec . 4 for the single language pair.
Then the best method from the single language pair setting is used to evaluate sentence selection methods for AL in multilingual setting . After building the initial MT system for each experiment , we select and remove 500 sentences from U and add them together with translations to L for 10 total iterations . The random sentence selection baselines are averaged over 3 independent runs.
mode selftrain co-train
Method wer per wer per
Combined Rank 40.2 30.0 40.0 29.6
Alternate 41.0 30.2 40.1 30.1
Disagree-Pairwise 41.9 32.0 40.5 30.9
Disagree-Center 41.8 31.8 40.6 30.7
Random Baseline 41.6 31.0 40.5 30.7
Germanic languages to English mode selftrain co-train
Method wer per wer per
Combined Rank 37.7 27.3 37.3 27.0
Alternate 37.7 27.3 37.3 27.0
Random Baseline 38.6 28.1 38.1 27.6
Romance languages to English
Table 1: Comparison of multilingual selection methods with WER ( word error rate ), PER ( position independent WER).
95% confidence interval for WER numbers is 0.7 and for PER numbers is 0.5. Bold : best result , italic : significantly better.
We use three language pairs in our single language pair experiments : French-English , German-English , and Spanish - English . In addition to random sentence selection baseline , we also compare the methods proposed in this paper to the best method reported in ( Haffari et al , 2009) denoted by GeomPhrase , which differs from our models since it considers each individual OOV segment as a single OOV phrase and does not consider subsequences . The results are presented in Fig . 2. Selecting sentences based on our proposed methods outperform the random sentence selection baseline and GeomPhrase . We suspect for the situations where L is out-of-domain and the average phrase length is relatively small , our method will outperform GeomPhrase even more.
For the multilingual experiments , we use Germanic ( German , Dutch , Danish , Swedish ) and Romance ( French , Spanish , Italian , Portuguese3) lan-3A reviewer pointed out that EuroParl English-Portuguese 18.2 18.4 18.6 18.8 19.4 19.6 19.8
Avg
BLE
U Sc ore
Self?Train Mulilingual da?de?nl?sv to en AlternateCombineRankDisagree?PairwiseDisagree?CenterRandom 1000 1500 2000 2500 3000 3500 4000 4500 500019.3 19.4 19.5 19.6 19.7 19.8 19.9 20.2 20.3
Added Sentences
Avg
BLE
U Sc ore
Co?Train Mulilingual da?de?nl?sv to en
AlternateCombineRankDisagree?PairwiseDisagree?CenterRandom 0 1000 2000 3000 4000 500021.6 21.8 22.4 22.6 22.8 23.4 23.6
Added Sentences
Avg
BLE
U Sc ore
Self?Train Mulilingual fr?es?it?pt to en
AlternateCombineRankRandom 1000 1500 2000 2500 3000 3500 4000 4500 500022.6 22.8 23.4 23.6 23.8
Added Sentences
Avg
BLE
U Sc ore
Co?Train Mulilingual fr?es?it?pt to en
AlternateCombineRankRandom
Figure 4: The left/right plot show the performance of our AL methods for multilingual setting combined with self-training/co-training . The sentence selection methods from Sec . 3 are compared with random sentence selection baseline . The top plots correspond to Danish-German-Dutch-Swedish to English , and the bottom plots correspond to French-Spanish-Italian-Portuguese to English.
guages as the source and English as the target language as two sets of experiments.4 Fig . 3 shows the performance of random sentence selection for AL combined with self-training/co-training for the multisource translation from the four Germanic languages to English . It shows that the cotraining mode outperforms the self-training mode by almost 1 BLEU point . The results of selection strategies in the multilingual setting are presented in Fig . 4 and Tbl . 1. Having noticed that Model 1 with ELPR performs well in the single language pair setting , we use it to rank entries for individual translation tasks . Then these rankings are used by ? Alternate ? and ? Combined Rank ? selection strategies in the multilingual case . The ? Combined Rank ? method outperforms all the other methods including the strong random selection baseline in both self-training and cotraining modes . The disagreement-based selection methods underperform the baseline for translation of Germanic languages to English , so we omitted them for the Romance language experiments.
5.2 Analysis
The basis for our proposed methods has been the popularity of regular/OOV phrases in U and their data is very noisy and future work should omit this pair.
4Choice of Germanic and Romance for our experimental setting is inspired by results in ( Cohn and Lapata , 2007) unpopularity in L , which is measured by P ( x|?U ) P ( x|?L ) .
We need P ( x|?U ), the estimated distribution of phrases in U , to be as similar as possible to P ?( x ), the true distribution of phrases in U . We investigate this issue for regular/OOV phrases as follows : ? Using the output of the initially trained MT system on L , we extract the regular/OOV phrases as described in ?4. The smoothed relative frequencies give us the regular/OOV phrasal distributions.
? Using the true English translation of the sentences in U , we extract the true phrases . Separating the phrases into two sets of regular and OOV phrases defined by the previous step , we use the smoothed relative frequencies and form the true
OOV/regular phrasal distributions.
We use the KL-divergence to see how dissimilar are a pair of given probability distributions.
As Tbl . 2 shows , the KL-divergence between the true and estimated distributions are less than that
De2En Fr2En Es2En
KL(P ? reg ? Preg ) 4.37 4.17 4.38
KL(P ? reg ? unif ) 5.37 5.21 5.80
KL(P ? oov ? Poov ) 3.04 4.58 4.73
KL(P ? oov ? unif ) 3.41 4.75 4.99
Table 2: For regular/OOV phrases , the KL-divergence between the true distribution ( P ?) and the estimated ( P ) or uniform ( unif ) distributions are shown , where:
KL(P ? ? P ) :=
P x P ?( x ) log P ?( x)
P ( x ) .
187 100 101 102 103 104 10510 ?6 10?5 10?4 10?3 10?2 10?1
Prob abili ty
Regular Phrases in U Estimated DistributionTrue Distribution 100 101 102 103 104 10510 ?6 10?5 10?4 10?3 10?2 10?1
Prob abili ty
OOV Phrases in U Estimated DistributionTrue Distribution Figure 5: The loglog Zipf plots representing the true and estimated probabilities of a ( source ) phrase vs the rank of that phrase in the German to English translation task . The plots for the Spanish to English and French to English tasks are also similar to the above plots , and confirm a power law behavior in the true phrasal distributions.
between the true and uniform distributions , in all three language pairs . Since uniform distribution conveys no information , this is evidence that there is some information encoded in the estimated distribution about the true distribution . However we noticed that the true distributions of regu-lar/OOV phrases exhibit Zipfian ( power law ) behavior5 which is not well captured by the estimated distributions ( see Fig . 5). Enhancing the estimated distributions to capture this power law behavior would improve the quality of the proposed sentence selection methods.
6 Related Work ( Haffari et al , 2009) provides results for active learning for MT using a single language pair . Our work generalizes to the use of multilingual corpora using new methods that are not possible with a single language pair . In this paper , we also introduce new selection methods that outperform the methods in ( Haffari et al , 2009) even for MT with a single language pair . In addition in this paper by considering multilingual parallel corpora we were able to introduce cotraining for AL , while ( Haffari et al , 2009) only use self-training since they are using a single language pair.
5This observation is at the phrase level and not at the word ( Zipf , 1932) or even ngram level ( Ha et al , 2002).
(Reichart et al , 2008) introduces multitask active learning where unlabeled data require annotations for multiple tasks , e.g . they consider named-entities and parse trees , and showed that multiple tasks helps selection compared to individual tasks . Our setting is different in that the target language is the same across multiple MT tasks , which we exploit to use consensus translations and cotraining to improve active learning performance.
(Callison-Burch and Osborne , 2003b ; Callison-Burch and Osborne , 2003a ) provide a cotraining approach to MT , where one language pair creates data for another language pair . In contrast , our cotraining approach uses consensus translations and our setting for active learning is very different from their semisupervised setting . A Ph.D.
proposal by Chris Callison-Burch ( Callison-burch , 2003) lays out the promise of AL for SMT and proposes some algorithms . However , the lack of experimental results means that performance and feasibility of those methods cannot be compared to ours.
While we use consensus translations ( He et al , 2008; Rosti et al , 2007; Matusov et al , 2006) as an effective method for cotraining in this paper , unlike consensus for system combination , the source languages for each of our MT systems are different , which rules out a set of popular methods for obtaining consensus translations which assume translation for a single language pair . Finally , we briefly note that triangulation ( see ( Cohn and Lapata , 2007)) is orthogonal to the use of cotraining in our work , since it only enhances each MT system in our ensemble by exploiting the multilingual data . In future work , we plan to incorporate triangulation into our active learning approach.
7 Conclusion
This paper introduced the novel active learning task of adding a new language to an existing multilingual set of parallel text . We construct SMT systems from each language in the collection into the new target language . We show that we can take advantage of multilingual corpora to decrease annotation effort thanks to the highly effective sentence selection methods we devised for active learning in the single language-pair setting which we then applied to the multilingual sentence selection protocols . In the multilingual setting , a novel cotraining method for active learning in SMT is proposed using consensus translations which outperforms AL-SMT with self-training.
188
References
Avrim Blum and Tom Mitchell . 1998. Combining Labeled and Unlabeled Data with CoTraining.
In Proceedings of the Eleventh Annual Conference on Computational Learning Theory ( COLT 1998), Madison , Wisconsin , USA , July 2426. ACM.
Chris Callison-Burch and Miles Osborne . 2003a.
Bootstrapping parallel corpora . In NAACL workshop : Building and Using Parallel Texts : Data
Driven Machine Translation and Beyond.
Chris Callison-Burch and Miles Osborne . 2003b . Co-training for statistical machine translation . In Proceedings of the 6th Annual CLUK Research Colloquium.
Chris Callison-burch . 2003. Active learning for statistical machine translation . In PhD Proposal , Edinburgh University.
Trevor Cohn and Mirella Lapata . 2007. Machine translation by triangulation : Making effective use of multiparallel corpora . In ACL.
Le Quan Ha , E . I . Sicilia-Garcia , Ji Ming , and F.J.
Smith . 2002. Extension of zipf?s law to words and phrases . In Proceedings of the 19th international conference on Computational linguistics.
Gholamreza Haffari , Maxim Roy , and Anoop Sarkar.
2009. Active learning for statistical phrasebased machine translation . In NAACL.
Xiaodong He , Mei Yang , Jianfeng Gao , Patrick Nguyen , and Robert Moore . 2008. Indirect-hmm-based hypothesis alignment for combining outputs from machine translation systems . In EMNLP.
Philipp Koehn . 2005. Europarl : A parallel corpus for statistical machine translation . In MT Summit.
Evgeny Matusov , Nicola Ueffing , and Hermann Ney.
2006. Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment . In EACL.
Franz Josef Och and Hermann Ney . 2004. The alignment template approach to statistical machine translation . Computational Linguistics , 30(4):417?449.
Franz Josef Och . 2003. Minimum error rate training in statistical machine translation . In ACL ?03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics.
Kishore Papineni , Salim Roukos , Todd Ward , and Wei jing Zhu . 2002. Bleu : A method for automatic evaluation of machine translation . In ACL ?02: Proceedings of the 41st Annual Meeting on Association for
Computational Linguistics.
Roi Reichart , Katrin Tomanek , Udo Hahn , and Ari Rappoport . 2008. Multitask active learning for linguistic annotations . In ACL.
Antti-Veikko Rosti , Necip Fazil Ayan , Bing Xiang , Spyros Matsoukas , Richard M . Schwartz , and Bonnie Jean Dorr . 2007. Combining outputs from multiple machine translation systems . In NAACL.
Nicola Ueffing , Gholamreza Haffari , and Anoop Sarkar . 2007. Transductive learning for statistical machine translation . In ACL.
George Zipf . 1932. Selective Studies and the Principle of Relative Frequency in Language . Harvard University Press.
189
