Proceedings of the ACL-IJCNLP 2009 Conference Short Papers , pages 165?168,
Suntec , Singapore , 4 August 2009.
c
?2009 ACL and AFNLP
Hierarchical MultiClass Text Categorization
with Global Margin Maximization
Xipeng Qiu
School of Computer Science
Fudan University
xpqiu@fudan.edu.cn
Wenjun Gao
School of Computer Science
Fudan University
wjgao616@gmail.com
Xuanjing Huang
School of Computer Science
Fudan University
xjhuang@fudan.edu.cn
Abstract
Text categorization is a crucial and well-proven method for organizing the collection of large scale documents . In this paper , we propose a hierarchical multiclass text categorization method with global margin maximization . We not only maximize the margins among leaf categories , but also maximize the margins among their ancestors . Experiments show that the performance of our algorithm is competitive with the recently proposed hierarchical multiclass classification algorithms.
1 Introduction
In the past serval years , hierarchical text categorization has become an active research topic in database area ( Koller and Sahami , 1997; Weigend et al , 1999) and machine learning area ( Rousu et al ., 2006; Cai and Hofmann , 2007).
Hierarchical categorization methods can be divided in two types : local and global approaches ( Wang et al , 1999; Sun and Lim , 2001). A local approach usually proceeds in a topdown fashion , which firstly picks the most relevant categories of the top level and then recursively making the choice among the low-level categories . The global approach builds only one classifier to discriminate all categories in a hierarchy . Due that the global hierarchical categorization can avoid the drawbacks about those high-level irrecoverable error , it is more popular in the machine learning domain.
The essential idea behind global approach is that the close classes(nodes ) have some common underlying factors . Especially , the descendant classes can share the characteristics of the ancestor classes , which is similar with multitask learn-ing(Caruana , 1997). A key problem for global hierarchical categorization is how to combine these underlying factors.
In this paper , we propose an method for hierarchical multiclass text categorization with global margin maximization . We emphasize that it is important to separate all the nodes of the correct path in the class hierarchy from their sibling node , then we incorporate such information into the formulation of hierarchical support vector machine.
The rest of the paper is organized as follows.
Section 2 describes the basic model of multiclass hierarchical categorization with maximizing margin . Then we propose our improved versions in section 3. Section 4 gives the experimental analysis . Section 5 concludes the paper.
2 Hierarchical MultiClass Text
Categorization
Multiclass SVM can be generalized to the problem of hierarchical categorization ( Cai and Hofmann , 2007), which has more than two categories in most of the case . Denote Y i as the multilabels of x i and ?
Y i the multilabels set not in Y i . The separation margin of w , with respect to x i , can be approximated as : ? i ( w ) = min y?Y i ,? y ? ?
Y i ??( x i , y )? ?( x i , ? y),w ? (1)
The loss function can be accommodated to multiclass SVM to scale the penalties for margin violations proportional to the loss . This is motivated by the fact that margin violations involving an incorrect class with high loss should be penalized more severely . So the cost-sensitive hierarchical multiclass formulation takes takes the following form : min w ,? || w || n ? i=1 ? i (2) s.t.?w ,?? i ( y , ? y)??1? ? i l(y,?y ) , (? i,y?Y i , ? y ? ?
Y i ) ? i ? 0(?i ) i ( y , ? y ) = ?( x i , y ) ? ?( x i , ? y ), l(y , ? y ) > 0 and ?( x,y ) is the joint feature of input x and output y , which can be represented as : ?( x,y ) = ?( y )? ?( x ) (3) where ? is the tensor product . ?( y ) is the feature representation of y.
Thus , we can classify a document x to label y ? : y ? = arg max y
F ( w,?(x,y )) (4) where F (?) is a map function.
There are different kinds of loss functions l(y , ? y).
One is thezero-one loss , l 0/1 ( y,u ) = [ y 6= u].
Another is specially designed for the hierarchy is tree loss(Dekel et al , 2004). Tree loss is defined as the length of the path between two multilabels with positive microlabels , l tr = | path(i : y i = 1, j : u j = 1)| (5) ( Rousu et al , 2006) proposed a simplified version of l
H , namely l ?
H : l ?
H = ? j c j [ y j 6= u j & y pa ( j ) = u pa(j ) ], (6) that penalizes a mistake in a child only if the label of the parent was correct . There are some different choices for setting c j . One naive idea is to use a uniform weighting ( c j = 1). Another possible choice is to divide the loss among the sibling : c root = 1, c j = c
Parent(j ) /(| Sib(j )|+ 1) (7)
Another possible choice is to scale the loss by the proportion of the hierarchy that is in the subtree
T ( j ) rooted by j : c j = | T ( j)|/|T ( root )| (8) Using these scaling weights , the derived losses are referred as l ? uni , l ? sib and l ? sub respectively.
3 Hierarchical MultiClass Text
Categorization with Global Margin
Maximization
In previous literature ( Cai and Hofmann , 2004; Tsochantaridis et al , 2005), they focused on separating the correct path from those incorrect path.
Inspired by the example in Figure 1, we emphasize it is also important to separate the ancestor node in the correct path from their sibling node.
The vector w can be decomposed in to the set of w i for each node ( category ) in the hierarchy . In Figure 1, the example hierarchy has 7 nodes and 4 of them are leaf nodes . The category is encode as an integer , 1, . . . , 7. Suppose that the training pattern x belongs to category 4. Both w in the Figure 1a and Figure 1b can successfully classify x into category 4, since F ( w,?(x,y ? 1,2,4 ? w i , x ? is the maximal among all the possible discriminate functions . So both learned parameter w is acceptable in current hierarchical support vector machine.
Here we claim the w in Figure 1b is better than the w in Figure 1a . Since we notice in Figure 1a , the discriminate function ? w discriminate function ? w function ? w i , x ? measures the similarity of x to category i . The larger the discriminate function is , the more similar x is to category i . Since category 2 is in the path from the root to the correct category and category 3 is not , intuitively , x should be closer to category 2 than category 3. But the discriminate function in Figure 1a is contradictive to this assumption . But such information is reflected correctly in Figure 1b . So we conclude w in Fig.
1b is superior to w in 1a.
Here we propose a novel formulation to incorporate such information . Denote A i as the multilabel in Y i that corresponds to the nonleaf categories and Sib(z ) denotes the sibling nodes of z , that is the set of nodes that have the same parent with z , except z itself . Implementing the above idea , we can get the following formulation : min w ,?,? ? w ? i ? i + C i ? i (9) s.t.?w , ?? i ( y , ? y )? ? 1? ? i l(y , ? y ) , (? i , y ? Y i ? y ? ?
Y i ) ? w , ?? i ( z , ? z )? ? 1? ? i l(z , ? z ) , (? i , z ? A(i ) ? z ? Sib(z ) ) ? i ? 0(?i ) ? i ? 0(?i ) It arrives at the following Lagrangian:
L(w , ? n , ? n ) = ? w ? i ? i + C i ? i ?
X i
X y?Y i ? y ? ?
Y i ? iy?y (? w , ?? i ( y , ? y )? ? 1 + ? i l(y , ? y ) ) 2 3 4 5 6 7 10,1 xw 3,2 xw 5,3 xw 5,4 xw 1,7 xw1,5 xw 2,6 xw 4 5 6 7 10,1 xw 7,2 xw 3,3 xw 5,4 xw 1,7 xw1,5 xw 2,6 xw a ) b ) Figure 1: Two different discriminant function in a hierarchy ?
X i
X z?A i ? z?Sib(z ) ? iz?z (? w , ?? i ( z , ? z )? ? 1 + ? i l(z , ? z ) ) ?
X i c i ? i ?
X i d i ? i (10)
The dual QP becomes max ? ?(?) = ? i ? y?Y i ? y ? ?
Y i ? iy ? y + ? i ? z?A i ? z?Sib(z ) ? iz ? z ? ? i,j ? y?Y i ? y ? ?
Y i ? r?Y j ? r ? ?
Y j ? ? y,r , ? r (11) ? ? i,j ? z?A i ? z?Sib(z ) ? k?A j ? k?Sib(k ) ? ? z,k , ? k , s.t .? iy ? y ? 0, (12) ? jz ? z ? 0, (13) ? y?Y i ? y ? ?
Y i ? iy ? y l(y , ? y ) ? C ? z?A i ? z?Sib(z ) ? iz ? z l(z , ? z ) ? C where ? ? y,r , ? r = ? iy ? y ? jr ? r ??? i ( y , ? y ), ?? j ( r , ? r )? and ? ? z,k , ? k = ? iz ? z ? jk ? k ??? i ( z , ? z ), ?? j ( k , ? k)?.
3.1 Optimization Algorithm
The derived QP can be very large , since the number of ? and ? variables is up to O(n?2
N ), where n is number of training pattern and N is the number of nodes in the hierarchy . But two properties of the dual problem can be exploited to design a much more efficient optimization.
First , the constraints in the dual problem Eq . 11 - Eq . 15 factorize over the instance index for both ?- variables and ?- variables . The constraints in Eq . 14 do not couple ?- variables and ?- variables together . Further , dual variables ? iy ? y and ? jy ? ? y ? belonging to different training instances i and j do not join in a same constraints . This inspired an optimization procedure which iteratively performs subspace optimization over all dual variables ? iy ? y belonging to the same training instance . This will in general reduced to a much smaller QP , since it freezes all ? jy ? y with j 6= i and ?- variables at their current values . This strategy can be applied in solving ?- variables.
Secondly , the number of active constraints at the solution is expected to be relatively small , since only a small fraction of categories ? y ? ?
Y i ( or ? y ? Sib(y ) when y ? A i ) will typically fail to achieve the required margin . The expected sparseness of the variable for the dual problem can be exploited by employing a variable selection strategy . Equivalently , this corresponds to a cutting plane algorithm for the primal QP . Intuitively , we will identify the most violated margin constraint with index ( i,y , ? y ) and then add the corresponding variable to the optimization problem . This means that we start with extremely sparse problems and only successively increase the number of variables in the active set . This general approach to deal with large linear or quadratic optimization problems is also known as column selection . In practice , it is often not necessary to optimize until final convergence , which adds to the attractiveness of this approach.
We have used the LOQO optimization package ( Vanderbei , 1999) in our experiments.
4 Experiment
We evaluate our proposed model on the section D in the WIPO-alpha collection the 1372 training and 358 testing document . The The values per column is calculated with the different loss function.
X
X
X
X
X
X
X
Train
Test l 0/1 l ? l tr l uni l sib l sub
HSVM 48.6 188.8 94.4 97.2 5.4 7.5 l 0/1
HSVM-S 48.3 186.6 93.3 96.6 5.2 7.4
HSVM 49.7 187.7 93.9 99.4 5.0 7.1 l ?
HSVM-S 47.8 165.3 89.7 90.5 4.8 6.9
HM3 70.9 167.0 - 89.1 5.0 7.0
HSVM 49.4 186.0 93.0 98.9 5.0 7.5 l tr
HSVM-S 48.9 181.4 90.2 97.8 4.9 7.1
HSVM 47.2 181.0 90.5 94.4 5.0 7.0 l ? uni
HSVM-S 46.9 179.3 88.7 91.9 4.9 6.9
HM3 70.1 172.1 - 88.8 5.2 7.4
HSVM 49.4 184.9 92.5 98.9 4.8 7.4 l ? sib
HSVM-S 48.9 170.2 91.6 90.8 4.7 7.4
HM3 64.8 172.9 - 92.7 4.8 7.1
HSVM 50.6 189.9 95.0 101.1 5.2 7.5 l ? sub
HSVM-S 47.2 169.4 85.2 89.4 4.3 6.6
HM3 65.0 170.9 - 91.9 4.8 7.2 number of nodes in the hierarchy is 188, with maximum depth 3.
We compared the performance of our proposed method HSVM-S with two algorithms : HSVM(Cai and Hofmann , 2007) and HM3(Rousu et al , 2006).
4.1 Effect of Different Loss Function
We compare the methods based on different loss functions , l 0/1 , l ? , l tr , l u?ni , l s?ib and l s?ub . The performances for three algorithms can be seen in Table 1. Those empty cells , denoted by ?-?, are not available in ( Rousu et al , 2006).
As expected , l 0/1 is inferior to other hierarchical losses by getting poorest performance in all the testing losses , since it can not take into account the hierarchical information between categories . The results suggests that training with a hierarchical losses function , like l s?ib or l u?ni , would lead to a better reduced l 0/1 on the test set as well as in terms of the hierarchical loss . In Table 1, we can also point out that when training with the same hierarchical loss , the performance of HSVM-S is better than HSVM under the measure of most hierarchical losses , since HSVM-S includes more hierarchical information,the relationship between the sibling categories , than HSVM which only separate the leave categories.
5 Conclusion
In this paper we present a hierarchical multiclass document categorization , which focus on maximize the margin of the classes at the different levels in the class hierarchy . In future work , we plan to extend the proposed hierarchical learning method to the case where the hierarchy is a DAG instead of tree and scale up the method further.
Acknowledgments
This work was ( partially ) funded by Chinese NSF 60673038, Doctoral Fund of Ministry of Education of China 200802460066, and Shanghai Science and Technology Development Funds 08511500302.
References
L . Cai and T Hofmann . 2004. Hierarchical document categorization with support vector machines.
In Proceedings of the ACM Conference on Information and Knowledge Management.
L . Cai and T . Hofmann . 2007. Exploiting known taxonomies in learning overlapping concepts . In Proceedings of International Joint Conferences on Artificial Intelligence.
R . Caruana . 1997. Multitask learning . Machine
Learning , 28(1):41?75.
Ofer Dekel , Joseph Keshet , and Yoram Singer . 2004.
Large margin hierarchical classification . In Proceedings of the 21 st International Conference on
Machine Learning.
D . Koller and M Sahami . 1997. Hierarchically classifying documents using very few words . In Proceedings of the International Conference on Machine
Learning ( ICML).
Juho Rousu , Craig Saunders , Sandor Szedmak , and John ShaweTaylor . 2006. Kernel-based learning of hierarchical multilabel classification models . In
Journal of Machine Learning Research.
A . Sun and E.-P Lim . 2001. Hierarchical text classification and evaluation . In Proceedings of the IEEE International Conference on Data Mining ( ICDM).
Ioannis Tsochantaridis , Thorsten Joachims , Thomas Hofmann , and Yasemin Altun . 2005. Large margin methods for structured and interdependent output variables . In Journal of Machine Learning.
R . J . Vanderbei . 1999. Loqo : An interior point code for quadratic programming . In Optimization Methods and Software.
K . Wang , S . Zhou , and S Liew . 1999. Building hierarchical classifiers using class proximities . In Proceedings of the International Conference on Very
Large Data Bases ( VLDB).
A . Weigend , E . Wiener , and J Pedersen . 1999. Exploiting hierarchy in text categorization . In Information
Retrieval.
168
