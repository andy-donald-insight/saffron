ADISCO VERY PROCE DURE
FOR CERTAIN PHONOLOGICAL RULES
Mark Johnson
Linguistics , UCSD.

Acquisition of phonological systems can be insightfully studied in terms of discovery procedures  . This paper describes a discovery procedure , implemented in Lisp , capable of determining a set of ordered phonological rules  , which may be in opaque contexts ~ from a set of surface forms arranged in paradigms  . 
1. INTRODUCTION
For generative grammarians , uchas Chomsky (1965) , a primary problem of linguistics is to explain how the language learner can acquire the grammar of his or her language on the basis of the limited evidence available to himor her  . Chomsky introduced the idealization of instantaneous acquisition  , which 1 adopt here , in order to model the language acquisition device as a function from primary linguistic data to possible grammars  , rather than as a process . 
Assuming that the set of possible human languages is small  , rather than large , appears to make acquisition easier , since there are fewer possible grammars to choose from  , and less data should be required to choose between them  . Accordingly , generative linguists are interested in delimiting the class of possible human languages  . This is done by looking for properties common to all human languages  , or universals . 
Together , these universals form universal grammar , a set of principles that all human languages obey  . Assuming that universal grammar is innate , the language learner can use it to restrict the number of possible grammars he or she must consider when learning a language  . 
As part of universal grammar , the language learner is supposed to innately possess an evaluation metric  , which is used to " decide " between two grammars when both are consistent with other principles of universal grammar and the available language data  . 
2. DISCOVERY PROCEDURES
This approach deals with acquisition without reference to a specific discovery procedure  , and so in some sense the results of such research are general ~ in that in principle they apply to all discovery procedures  . Still , I think that there is some utility in considering the problem of acquisition in terms of actual discovery procedures  . 
Firstly , we can identify the parts of a grammar that are underspeeified with respect to the available data  . Parts of a grammar or a rule are strongly data determined if they are fixed or uniquely determined by the data  , given the requirement that overall grammar be empirically correct  . 
By contrast , a part of a grammar or of a rule is weakly data determined if there is a large class of grammar or rule parts that are all consistent with the available data  . For example , if there are two possible analyses that equally well account for the available data  , then the choice of which of these analyses should be incorporated in the final grammar is weakly data determined  . Strong or weak data determination is therefore a property of the grammar formalism and the data combined  , and independent of the choice of discovery procedure  . 
Secondly , a discovery procedure may partition a phonological system in an interesting way  . For instance , in the discovery procedure described here tile evaluation metric is not called apon to compare one grammar with another  , but rather to make smaller , more local , comparisons . This leads to a factoring of the evaluation metric that may prove useful for its further investigation  . 
Thirdly , focussing on discovery procedures forces us to identify what the surface indications of the various constructions in the grammar are  . Of course , this does not mean one should look for a one-to -one correspondence between individual grammar constructions and the surface data  ; but rather complexes of grammar constructions that interact to yield particular patterns on the surface  . One is then investigating the logical implications of the existence of a particular constructions in the data  . 
Following from the last point ,   1 think a discovery procedure should have a deductive rather than enumerative structure  . In particular , procedures that work essentially by enumerating all possible  ( sub ) grammars and seeing which ones work are not only in general very inefficient  , but . also not . very insightful . These discovery by enumeration procedure simply give us a list of all rule systems that are empirically adequate as a result  , but they give us no idea as to what properties of these systems were crucial in their being empirically adequate  . 
This is because the structure imposed on the problem by a simple recursive enumeration procedure is in general not related to the intrinsic structure of the rule discovery problem  . 
3. A PHONOLOGICAL RULEDISCOVERY PRO-

Below and in Appendix A I outline a discovery procedure : which I have fully implemented in Franz Lispona VAX  11/750 computer , for a restricted class of phonological rules , namely rules of the type shown in (1) . 
(1) ~~ b/c
Rule ( 1 ) means that any segment a that appears in context C in the input to the rule appears as a bin the rule's output  . Context C is a feature matrix , and to say that a appears in context C means that C is a subse ! of the fvature malrix consists of an ordered  2 set of such rules , where the rules are considered to apply in a cascaded fashion  , that . is , the output of one rule is the input to the next . .
The problem the discovery procedure must solve is , given some data , to determine the set of rules . As an idealization , I assume that the input to the discovery procedure is a set of surface paradigms  , a two dimensional array of words with all words in the same row possessing the same stem and all words in the same column the same affix  . Moreover , l assume the root and suffix morphemes are already identified  , a hhough I admit this task may be nontrivial . 
4. DETERMINING THECONTEXTTHATCONDI-
TIONSANAL TERNATION
Consider the simplest phonological system : one in which only one phonological rule is operative  . In this system the alternating segements a and b can be determined by inspection  , since a and b will be the only alternating segments in the data  ( although there will be a systematic ambiguity as to which is a and which is b  )  . Thus a and b are strongly data determined . 
Given a and b . we can write a set of equations that the rule context C that conditions this alternation must obey  . 
Our rule rnust apply in all contexts Cb where ab appears that alternates with an a  , since by hypothesis b was produced by this rule . We can represent this by equation (2) . 
(2) ~7\] Cb , C matches Cb
The second condition that our rule must obey is that it doesn't apply in any context  . Caw here an a appears . If it did , of course , we would expect ab , not ana , in this position on the surface . We can write this condition by equation (3) . 
(3) ~? C ,, C does not match 6',
These two equations define the rule context C . Note that in general these equations do not yield a unique value for C  ; depending a ponthed at at bere may be no C that simultaneously satisfies  ( 2 ) and ( 3 )  . or there may be several different C that simultaneously satisfies  ( 2 ) and ( 3 )  . We cannot appeal further to the data to decide which C to use  , since they all are equally consistent with the data  . 
Let us call the set of C that simultaneously satisfies  ( 2 ) and ( 3 ) So Then Sc is strongly data determined ; in fact , there is an efficient algorithm for computing S c from the C  , s and Cbs that does not involve enumerating and testing all ima-ginable C  ( the algorithm is described in Appendix A )  . 
However , if Sc contains more than one 6' , the choice of which C from Sc to actually use as the rule's context is weakly  1 What is crucial for what follows is that saying context C matches a portion of a word W is equivalent osaying that C is a subset of W  . Since both rule contexts and words can be written assets of features  ,   1 use " contexts " to refer both to rule contexts and to words  . 
zI make this assumption as a first approximation . In fact , in real phonological systems phonological rules may be unordered with respect o each other  . 
data determined . Moreover . the choice of v , hich ( ' from Sclouse does not affect any other decisions that the discovery procedure has to make-that is  . nothing else in the complete grammar must change if we decide to use one C instead of another  . 
Plausibly , the evaluation metric and universal principles decide which C to use in this situation  . For example , if the alternation involves n as a fization of a vowel  , something that usually only occurs in the context  , of an asal , and one of the contexts in Sc involves the feature nasal but the other C in Sc do not  , a reasonable requirement is that the discovery procedure should select the context involving the feature nasal as the appropriate context C for the rule  . 
Another possibility is that . qc'S containing more than one , member indicates to the discovery procedure that it simply has too little data to determine the grammar  , and it defers making a decision on which C to use until it has the relevant data  . 
The decision as to which of these possibilities is correct is is not unimportant  , and may have interesting empirical consequences regarding language acquisition  . 
McCarthy (1981) gives some data on a related issue . 
Spanish does not tolerate word initials C clusters  , a fact . which might be accounted for in two ways ; either with a rule that inserts e before word initials C clusters  , or by a constraint on wellformed underlying structures  ( a redundancy rule ) barring word initials C . McCarthy reports that either constraint is adequate to account for Spanish morphop bonemics  , and there is no particular language internal evidence to prefer one over the other  . 
The two accounts make differing predictions regarding the treat rnent of loan words  . The e insertion rule predicts that loan words beginning with sC should receive an initiale  ( as they do : esnob , esmoking , esprey ) , while the wellformedness constraint makes no such prediction  . 
McCarthy's evidence from Spanish therefore suggests that the human acquisition procedure can adopt one potential analysis and rejects another without empirical evidence to distinguish between them  . l to wever , in the Spanish case , the two potential analyses differ as to which components of the grammar they involve  ( active phonological processes versus lexical redundancy rules  ) which affects the overall structure of the adopted grammar to a much greater degree than the choice of one C from Sc over another  . 
5. RULEORDERING
In the last section 1 showed that a single phonological rule can be determined from the surface data  . In practice , very few , if any , phonological systems involve only one rule . 
Systems involving more than one rules how complexity that single rule systems do not  . In particular , a rules may be ordered in such a fashion that one rule affects segments that are part of the context that conditions the operation of another rule  . If a rule's context is visible on the surface ( i e . 
has not been destroyed by the operation of another rule  ) it is said to be transparent , while if a rule's context is no longer visible on the surface it is opaque  . On the face of it , opaque contexts could pose problems for discovery procedures  . 
345 () r < h , rillg ( , ir , lh , ~h ~ u-b <'( q + a topic ~ , ul > , ,l+jlilial re . ~ e~-~r\[hit + ? h . .<,h,g ', . Xl ' . mai , , , d , i , . cli' . .c , i . thi-~,,rti . . is t (, shov . that e?trirls i cally ordered ruh , si , , prilu'iph ' pose t~o prohlem for a discover ) prl , tt ' durl ' .   ( ' ~ l ' n if later ruh's obscure I he ( ' on text of earlier ones . I don't make any elaitn that I he procedure presented here is opt in lal-in fact I can think of at least two ways to make it perform its job more effil'ienlly  . The output of this ( lisc < ~ very procedure is the set of all possible ordered ruh  .   s3stelllS zaud their correspondiHgulderhing forms that can pr  (  , due e the given surface for t , is . 
As before . I as s , lnle thal the data is in the form of sets of paradigms  . I also as sunu , that for e ~ er ) ruh , ct langing anatoab . anaheri , aiion hetween a and b appears in the data : thus ++ e know hylisting the alternations in ttw data just what the possihle as and bs of the ruh ' are  4  . 
Frorn the assump xi on thairuh , s are extins\[(ally ordered il folh , ws lhat one of the ruh's must have appli ( , ( tlast : that is . 
there is a urJique " most surfaev " rule . The (' on text or this ruh . 
+ ~ illne < essari LyI , r transl ) are t , ( visible in the surface hJrms ) , as there is ill ) later rule to n lake its context opaque . 
Of coHrse , till ' ( liscover . ' , procedure has no a priori way of tell hJg + ~ hit'h alt  (  . rnati . n ( . , r responds In then lost surfacy rule . 
Thl Ly > althought ilt , identh ) of till'segnlelitS involved in tileniosl suffal "  , ruleilia)"he strictly data delerlnined , at this stall , I h l s i n f t l r n l a l i i l n i . "; Ill ) availahle to the discovery pro-('edure . 
SO at this point , tile discovery pr ( lcedure proposed here systematically investigates all of the surface ahernations : fi  ) reach alternation it makes the hypothesis that h , is the the alternation ( if lilt , n lost sllrfa (') rub ' .   ( ' herks that a context Call be fouud thai conditions this alternation  ( this lnus the so if the hypothesis is correct ) using the sirigle rule algorithm presented earlier  , and then investigates if it , is possible to con-strut ( an empirically correct set of rules based on this hylitlt  . hesis . 
Given thai we have found a potential Il III OSI surfacy " ruh  , , all of the surface alternates are replaced by the putative underlying segment to fornl a set of intermediate forms  , in whi <' h the rule just discovered has been undone  . We can undo this rule berause we previously identified tile alternating seg-nlents  , ull ) ,  . rtantly , undoing this rule means that all other Thus if then rules in the systet n are unoi'dered  , this procedure returns n ! solutions corresponding to then ways of ordering these rules  . 
The reason why the class of phonological rules considered in this paper was restricted to those mapping segments into segments was so that all alternations could be identified by simply comparing surface forms segment by segment  . Thus in this discovery procedure the algorithm for identifying possible alternates can be of a particularly simple form  . If we are willing It ) complicate the rnachinery that deterlnines the possi-bh ' a hernations in some data  . we can relax the restriction prohibiting epe+nt , hesis and deletion rules , and the requirement that all alternations are visible on tile surface  . That is , if the approach here is correct , the problem of identifying which segments alternate is a different problem to discovering the  (   ( Ull ' ~ tlllll lltl ~ hdlll ~ ll ~ , lhl ~ , flitIhilll , i l , ruh . swhl ) secot , texts had been made opaque in the surfaced a lab . v the operation of the most surfacy rule will now be transparent  . 
The hypothesis tester proceeds to look for another alternation  , this tilne in the intermediate forms , rather than in the surface fi ) rms , and so on until all alternations have been accounted for  . 
If at an . ' , ' stage the hypothesis tester fails to find a rule I  , odr's cribe the alternation it is currently working with  , that is , the single-rule algorithm determines thain orule context exists that can capture this alternation  , the hypothesis tester discards ttte current hypothesis  , and tries auother . 
The hypothesis tester is responsible for proposing different rule order\[ass  , which are tested by applying the rules in reverse to arrive at progressively more renloved represent a-lions  , with the single-ruh ' algorithm being applied at each step to deterl nine if a rule exists that relates one level of intermediate representation with the next  . We ran regard the hyp ( itiles is tester as systematically searching throughtile space of different rule orderings  , seeking rub ' orderings that successfully accounts for the ohserved data  . 
q't Je output of this procedure is therefore a list of all possible rule orderings  . As \] tnentioned before , I think that tile et lumer at lve approacit adopted here is basically flawed  . So althougit this procedure is relatively efficient  , in situations where rule ordering is strictly data determined  ( that is , where only one nile ordering is consistent with the data  )  , in situations where the rules are tmordered ( any rule ordering will do )  , the procedure will generate all possible n ! orderings of then rules  . 
This was most striking while working with some Japanese data  . with 6 dislincl alternations ,   4 of which were unordered with respect to each other  . The discovery procedure , as presented above , required approximately 1 hour of CPU time to completely analyse this data : it  . found < l different underlying forms and 512 different rules . v stems that generate the Japanese data , differing primarily in tile ordering of the rules . 
This demonstrates that a discovery procedure that simply enumerates all possible rule ordering is failing to capture some in l portant insight regarding rule ordering  , since unordered rules are much more difficult for this type of procedure to handle  , yet , unordered rules are the most comtnon situation in naturallangnage phonology  . 
This problem may be traced back to the assumption made above that a phonological system consists of an ordered set of rules  . The Japanese example shows that in many real phonological systems  , the ordering of particular rules is simply not strongly data determined  . What we need is some way of partitioning different  , rule orderings into equivalence classes , as was done with this the different rule contexts in the single rule algorithm  , and then compute with these equivalence classes rather than individual rule systems  ; that is . seek to localize the weak data determinacy . 
Looking at the problem in another way , we asked the discovery procedure to find all sets of ordered rules that generate the surface data  , which it did . However , it seems that this simply was no trigllt question  , since the answer to this question , a set of 512 different systems , is virtually phonologists in general have not yet agreed what exactly the principles of rule ordering ares  . 
Still , the present discovery procedure , whatever its deficiencies , does demonstrate that rule ordering in phonology does not pose any principled in surmountable problems for discovery procedures  ( although the procedure presented here is certainly practically lacking in certain situations  )  , even if a later rule is allowed to disturb the context of an earlier rule  , so that the rule's context is no longer " surface true "  . Nonetheless , it is an empirical question as to whether phonology is best described in terms of ordered interacting rules ~ all that l have shown is that such systems are not in principle unlearnable  . 
6. CONCLUSION
In this paper I have presented the details of a discovery procedure that can determine a limited class of phonological rules with arbitrary rule ordering  . The procedure has the interesting property that it can be separated into two separate phases  , the first , phase being superificial data analysis , that is , collecting the sets C , and Cb of equations (2) and (3) , and the second phase being the application of the procedure proper  , which need never reference the data directly , but can do all of its calculations using C , and Cb ~ . This property is interesting because it is likely that  6"  , and Ca have limiting values , as the number of forms in the surface data increases  . That is , presumably the language only has a fixed number of alternations  , and each of these only occurs in some fixed contexts  , and assoon as we have enough data to see all of these contexts we will have determined C  , and Cb . and extra data will not . make these sets larger . Thus the computational complexity of the second phase of the discovery procedure is more or less independent  , of the size the lexicon , making the entire procedure require linear time with respect to the size of the data  . 
i think this is a desirable result , since there is something counterintuitive to a situation in which the difficulty of discovering a grammar increases rapidly with the size of the lexicon  . 
7. APPENDIXA : DETERMININGARULE'SCON-

In this appendix ! describe an algorithm for calculating the set of rule contexts Sc = C that satisify equations  ( 2 ) and ( 3 ) repeated below in set notation as ( 4 ) and ( 5 )  . Recall that Cb are the contexts in which the alternation did take place  , and C a are the contexts in which the alternations did not take place  . We want to find ( the set , of ) contexts that . 
simultaneously match all the Cb , while not matching any C . .
(4) VC ~, CC_Cb
In this paper 1 adopted strict ordering of all rules because it is one of the more stringent rule ordering hypotheses available  . 
e In fact , the sets Ca and Cb as defined above do not contain quite enough information alone  . We must also indicate which segments in these contexts alternate  , and what they alternate to . This may form the basis of a very different rule order discovery procedure  . 
(5) Vc , .c;c,
We can manipulat . e these into computationally more tractable forms . Starting with (4), we have c ~, c c c ~(= (4))
VCb,\/fEC,f ~ Cb~/ec,fcACbCCI"3Cb
PutC,=f " lCb-Then CC6"i.
Now consider equation (5).
~' c ,, c ~ c,
Vc , , ~ i ~( c-c.)
But since C~C1, iff ~( C-C0) . then fE(C1-C , ) NC . Then ~/ c . ,q_ /~ ( c , - c ,  )  , / ~ c This last equation says thalever ) , context thaiful fills the conditions above contains at least one feature that distinguishes it from each  C0  , and that this feature must be in the intersection of all the Cb  . If for any C, . C\]-Ce = O(the null set of features) , then there are no contexts C that simultaneously match all the Cb and none of the C  , , implying that no rule exists that accounts for the observed a h  . ernation . 
We can construct the set Sc using this last formula by first  , calculating C1 , the intersection of all the Cb , and then for each C , , calculating C I : ( C I - C ? ) , a member of which must be in every 6' . The idea is to keep a set of the minimal C needed to account for the C  , so far ; if Cconl . a in s a member of C ! we don't need to modify it ; if C does not contain a member of CI then we have to add a member of CI to it in order for it to satisfy the equations above  . The algorithm below a complishes this . 
set C1:\[" ICbsetSc = ~ for each C.
set %= c,-c.
if % - O return " No rule contezts " for each C in Sc if cn  el=-0 remove C from Sc for eaeh/in 6'/ add CU/t?Sc return Sc where the subroutine " add " adds a set to Sconly if it or its subset is not already present  . 
After this algorithm has applied , Sc will contain all the minimal different C that satisfy equations  ( 4 ) and ( 5 ) above . 

