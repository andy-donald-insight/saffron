Proceedings of the ACL-HLT 2011 Student Session , pages 122?126,
Portland , OR , USA 1924 June 2011. c?2011 Association for Computational Linguistics
Kmeans Clustering with Feature Hashing
Hajime Senuma
Department of Computer Science
University of Tokyo
7-3-1 Hongo , Bunkyoku , Tokyo 113-0033, Japan
hajime.senuma@gmail.com
Abstract
One of the major problems of Kmeans is that one must use dense vectors for its centroids , and therefore it is infeasible to store such huge vectors in memory when the feature space is highdimensional . We address this issue by using feature hashing ( Weinberger et al ., 2009), a dimension-reduction technique , which can reduce the size of dense vectors while retaining sparsity of sparse vectors . Our analysis gives theoretical motivation and justification for applying feature hashing to Kmeans , by showing how much will the objective ofK-means be ( additively ) distorted . Furthermore , to empirically verify our method , we experimented on a document clustering task.
1 Introduction
In natural language processing ( NLP ) and text mining , clustering methods are crucial for various tasks such as document clustering . Among them , Kmeans ( MacQueen , 1967; Lloyd , 1982) is ? the most important flat clustering algorithm ? ( Manning et al , 2008) both for its simplicity and performance.
One of the major problems of Kmeans is that it has K centroids which are dense vectors where K is the number of clusters . Thus , it is infeasible to store them in memory and slow to compute if the dimension of inputs is huge , as is often the case with NLP and text mining tasks . A wellknown heuristic is truncating after the most significant features ( Manning et al , 2008), but it is difficult to analyze its effect and to determine which features are significant.
Recently , Weinberger et al (2009) introduced feature hashing , a simple yet effective and analyzable dimension-reduction technique for largescale multitask learning . The idea is to combine features which have the same hash value . For example , given a hash function h and a vector x , if h(1012) = h(41234) = 42, we make a new vector y by setting y42 = x1012 + x41234 ( or equally possibly x1012?x41234,?x1012+x41234, or?x1012?x41234).
This trick greatly reduces the size of dense vectors , since the maximum index value becomes equivalent to the maximum hash value of h . Furthermore , unlike random projection ( Achlioptas , 2003; Boutsidis et al , 2010), feature hashing retains sparsity of sparse input vectors . An additional useful trait for NLP tasks is that it can save much memory by eliminating an alphabet storage ( see the preliminaries for detail ). The authors also justified their method by showing that with feature hashing , dotproduct is unbiased , and the length of each vector is well-preserved with high probability under some conditions.
Plausibly this technique is useful also for clustering methods such as Kmeans . In this paper , to motivate applying feature hashing to Kmeans , we show the residual sum of squares , the objective of Kmeans , is well-preserved under feature hashing.
We also demonstrate an experiment on document clustering and see the feature size can be shrunk into 3.5% of the original in this case.
122 2 Preliminaries 2.1 Notation
In this paper , || ? || denotes the Euclidean norm , and ??, ?? does the dot product . ? i,j is the Kronecker?s delta , that is , ? i,j = 1 if i = j and 0 otherwise.
2.2 Kmeans
Although we do not describe the famous algorithm of Kmeans ( MacQueen , 1967; Lloyd , 1982) here , we remind the reader of its overall objective for later analysis . If we want to group input vectors into K clusters , Kmeans can surely output clusters ?1, ...? K and their corresponding vectors ?1, ...,? K such that they locally minimize the residual sum of squares ( RSS ) which is defined as
K ? k=1 ? x??k || x ? ? k || 2.
In the algorithm , ? k is made into the mean of the vectors in a cluster ? k . Hence comes the name Kmeans.
Note that RSS can be regarded as a metric since the sum of each metric ( in this case , squared Euclidean distance ) becomes also a metric by constructing a 1-norm product metric.
2.3 Additive distortion
Suppose one wants to embed a metric space ( X , d ) into another one ( X ?, d ?) by a mapping ?. Its additive distortion is the infimum of  which , for any observed x , y ? X , satisfies the following condition : d(x , y )?  ? d?(?(x ), ?( y )) ? d(x , y ) + .
2.4 Hashing tricks
According to an account by John Langford 1, a coauthor of papers on feature hashing ( Shi et al , 2009; Weinberger et al , 2009), hashing tricks for dimension-reduction were implemented in various machine learning libraries including Vowpal Wabbit , which he realesed in 2007.
Ganchev and Dredze (2008) named their hashing trick random feature mixing and empirically supported it by experimenting on NLP tasks . It is similar to feature hashing except lacking of a binary hash 1http://hunch.net/?jl/projects/hash_ reps/index.html function . The paper also showed that hashing tricks are useful to eliminate alphabet storage.
Shi et al (2009) suggested hash kernel , that is , dot product on a hashed space . They conducted thorough research both theoretically and experimentally , extending this technique to classification of graphs and multiclass classification . Although they tested Kmeans in an experiment , it was used for classification but not for clustering.
Weinberger et al (2009) 2 introduced a technique feature hashing ( a function itself is called the hashed feature map ), which incorporates a binary hash function into hashing tricks in order to guarantee the hash kernel is unbiased . They also showed applications to various realworld applications such as multitask learning and collaborative filtering . Though their proof for exponential tail bounds in the original paper was refuted later , they reproved it under some extra conditions in the latest version . Below is the definition.
Definition 2.1. Let S be a set of hashable features , h be a hash function h : S ? {1, ..., m }, and ? be ? : S ? {?1}. The hashed feature map ?( h ,?) : R|S | ? Rm is a function such that the ith element of ?( h,?)(x ) is given by ?( h,?)i ( x ) = ? j:h(j)=i ?( j)xj .
If h and ? are clear from the context , we simply write ?( h ,?) as ?.
As well , a kernel function is defined on a hashed feature map.
Definition 2.2. The hash kernel ??, ??? is defined as ? x,x ??? = ??( x ), ?( x ?)?.
They also proved the following theorem , which we use in our analysis.
Theorem 2.3. The hash kernel is unbiased , that is,
E?[?x,x ???] = ? x,x??.
The variance is
V ar?[?x,x ???] = ? ? ? i 6=j x2ix ?2 j + xix ? ixjx ? j ? ? .
2The latest version of this paper is at arXiv http :// arxiv.org/abs/0902.2206, with correction to Theorem 3 in the original paper included in the Proceeding of ICML ?09.
123 2.4.1 Eliminating alphabet storage
In this kind of hashing tricks , an index of inputs do not have to be an integer but can be any hashable value , including a string . Ganchev and Dredze (2008) argued this property is useful particularly for implementing NLP applications , since we do not anymore need an alphabet , a dictionary which maps features to parameters.
Let us explain in detail . In NLP , features can be often expediently expressed with strings . For instance , a feature ? the current word ends with - ing ? can be expressed as a string cur:end:ing ( here we suppose : is a control character ). Since indices of dense vectors ( which may be implemented with arrays ) must be integers , traditionally we need a dictionary to map these strings to integers , which may waste much memory . Feature hashing removes this memory waste by converting strings to integers with on-the-fly computation.
3 Method
For dimension-reduction to Kmeans , we propose a new method hashed Kmeans . Suppose you have N input vectors x1, ..., xN . Given a hashed feature map ?, hashed Kmeans runs Kmeans on ?( x1), ..., ?( xN ) instead of the original ones.
4 Analysis
In this section , we show clusters obtained by the hashed Kmeans are also good clusters in the original space with high probability . While Weinberger et al (2009) proved a theorem on ( multiplicative ) distortion for Euclidean distance under some tight conditions , we illustrate ( additive ) distortion for RSS . Since Kmeans is a process which monotonically decreases RSS in each step , if RSS is not distorted so much by feature hashing , we can expect results to be reliable to some extent.
Let us define the difference of the residual sum of squares ( DRSS).
Definition 4.1. Let ?1, ...? K be clusters , ?1, ...,? K be their corresponding centroids in the original space , ? be a hashed feature map , and ??1 , ...,? ?
K be their corresponding centroids in the hashed space.
Then , DRSS is defined as follows:
DRSS = |
K ? k=1 ? x??k ||?( x )? ?? k ||
K ? k=1 ? x??k || x ? ? k || 2|.
Before analysis , we define a notation for the ( Euclidean ) length under a hashed space : Definition 4.2. The hash length || ? ||? is defined as || x ||? = ||?( x )|| = ? ??( x ), ?( x )? = ? ? x,x??.
Note that it is clear from Theorem 2.3 that
E?[||x||2?] = || x || 2, and equivalently E?[||x||2? ? || x||2] = 0.
In order to show distortion , we want to use Cheby-shev?s inequality . To this end , it is vital to know the expectation and variance of the sum of squared hash lengths . Because the variance of the sum of random variables derives from each covariance between pairs of variables , first we show the covariance between the squared hash length of two vectors.
Lemma 4.3. The covariance between the squared hash length of two vectors x,y ? Rn is
Cov?(||x || ?( x,y ) m , where ?( x,y ) = 2 ? i 6=j xixjyiyj .
This lemma can be proven by the same technique described in the Appendix A of Weinberger et al (2009).
Now we see the following lemma.
Lemma 4.4. Suppose we have N vectors x1, ..., xN . Let us define X = ? i || xi || ? i || xi || 2 = ? i ( || xi||2? ? || xi || . Then , for any  > 0,
P ? ?| X | ?  ? m ? ? ? ?
N ? i=1
N ? j=1 ?( xi,xj ) ? ? ? .
124
Proof . This is an application of Chebyshev?s inequality . Namely , for any  > 0,
P ( | X ? E?[X ]| ?  ?
V ar?[X ] ) ? .
Since the expectation of a sum is the sum of expectations we readily know the zero expectation:
E?[X ] = 0.
Since adding constants to the inputs of covariance does not change its result , from Lemma 4.3, for any x,y ? Rn,
Cov?(||x || 2, || y||2? ? || y || 2) = ?( x,y ) m .
Because the variance of the sum of random variables is the sum of the covariances between every pair of them,
V ar?[X ] =
N ? i=1
N ? j=1 ?( xi,xj).
Finally , we see the following theorem for additive distortion.
Theorem 4.5. Let ? be the sum of ?( x,y ) for any observed pair of x,y , each of which expresses the difference between an example and its corresponding centroid . Then , for any ,
P (| DRSS | ? ) ? ? 2m .
Thus , if m ? ??1??2 where 0 < ? <= 1, with probability at least 1??, RSS is additively distorted by .
Proof . Note that a hashed feature map ?( h ,?) is linear , since ?( x ) = Mx with a matrix M such that Mi,j = ?( i)?h(i),j . By this liearlity , ? ? k = |? k|?1 ? x??k ?( x ) = ?(|? k|?1 ? x??k x ) = ?(? k ). Reapplying linearlity to this result , we have ||?( x)???k || 2 = || x??k||2?. Lemma 4.4 completes the proof.
The existence of ? in the theorem suggests that to use feature hashing , we should remove useless features which have high values from data in advance.
For example , if frequencies of words are used as 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65 0.7 10 100 1000 10000 100000 1e+06
F5 me asu re hash size m hashed kmeans Figure 1: The change of F5-measure along with the hash size features , function words should be ignored not only because they give no information for clustering but also because their high frequencies magnify distortion.
5 Experiments
To empirically verify our method , from 20 Newsgroups , a dataset for document classification or clustering 3, we chose 6 classes and randomly drew 100 documents for each class.
We used unigrams and bigrams as features and ran our method for various hash sizes m ( Figure 1). The number of unigrams is 33,017 and bigrams 109,395, so the feature size in the original space is 142,412.
To measure performance , we used the F5 measure ( Manning et al , 2008). The scheme counts correctness pairwisely . For example , if a document pair in an output cluster is actually in the same class , it is counted as true positive . In contrast , if it is actually in the different class , it is counted as false positive . Following this manner , a contingency table can be made as follows:
Same cluster Diff . clusters
Same class TP FN
Diff . classes FP TN
Now , F ? measure can be defined as
F ? = (?2 + 1)PR ?2P + R where the precision P = TP/(TP + FP ) and the recall R = TP/(TP + FN).
3http://people.csail.mit.edu/jrennie/20Newsgroups / recall . Manning et al (2008) stated that in some cases separating similar documents is more unfavorable than putting dissimilar documents together , and in such cases the F ? measure ( where ? > 1) is a good evaluation criterion.
At the first look , it seems odd that performance can be higher than the original where m is low . A possible hypothesis is that since Kmeans only locally minimizes RSS but in general there are many local minima which are far from the global optimal point , therefore distortion can be sometimes useful to escape from a bad local minimum and reach a better one . As a rule , however , large distortion kills clustering performance as shown in the figure.
Although clustering is heavily case-dependent , in this experiment , the resulting clusters are still reliable where the hash size is 3.5% of the original feature space size ( around 5,000).
6 Future Work
Arthur and Vassilvitskii (2007) proposed Kmeans ++, an improved version of Kmeans which guarantees its RSS is upper-bounded . Combining their method and the feature hashing as shown in our paper will produce a new efficient method ( possibly it can be named hashed Kmeans ++). We will analyze and experiment with this method in the future.
7 Conclusion
In this paper , we argued that applying feature hashing to Kmeans is beneficial for memory-efficiency.
Our analysis theoretically motivated this combination . We supported our argument and analysis by an experiment on document clustering , showing we could safely shrink memory-usage into 3.5% of the original in our case . In the future , we will analyze the technique on other learning methods such as Kmeans ++ and experiment on various real-data NLP tasks.
Acknowledgements
We are indebted to our supervisors , Jun?ichi Tsujii and Takuya Matsuzaki . We are also grateful to the anonymous reviewers for their helpful and thoughtful comments.
References
Dimitris Achlioptas . 2003. Database-friendly random projections : Johnson-Lindenstrauss with binary coins.
Journal of Computer and System Sciences , 66(4):671? 687, June.
David Arthur and Sergei Vassilvitskii . 2007. kmeans ++ : The Advantages of Careful Seeding . In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on
Discrete Algorithms , pages 1027?1035.
Christos Boutsidis , Anastasios Zouzias , and Petros Drineas . 2010. Random Projections for kmeans Clustering . In Advances in Neural Information Processing Systems 23, number iii , pages 298?306.
Kuzman Ganchev and Mark Dredze . 2008. Small Statistical Models by Random Feature Mixing . In Proceedings of the ACL08 HLT Workshop on Mobile Language
Processing , pages 19?20.
Stuart P . Lloyd . 1982. Least Squares Quantization in PCM . IEEE Transactions on Information Theory , 28(2):129?137.
J MacQueen . 1967. Some Methods for Classification and Analysis of Multivariate Observations . In Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability , pages 281?297.
Christopher D . Manning , Prabhakar Raghavan , and Hinrich Schu?tze . 2008. Introduction to Information Retrieval . Cambridge University Press.
Qinfeng Shi , James Petterson , Gideon Dror , John Langford , Alex Smola , and S.V.N . Vishwanathan . 2009.
Hash Kernels for Structured Data . Journal of Machine
Learning Research , 10:2615?2637.
Kilian Weinberger , Anirban Dasgupta , John Langford , Alex Smola , and Josh Attenberg . 2009. Feature Hashing for Large Scale Multitask Learning . In Proceedings of the 26th International Conference on Machine
Learning.
126
