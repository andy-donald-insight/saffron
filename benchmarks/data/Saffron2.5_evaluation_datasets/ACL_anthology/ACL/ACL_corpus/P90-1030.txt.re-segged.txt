Computational structure of generative phonology
and its relation to language comprehension.
Eric Sven Ristad *
MIT Artificial Intelligence Lab
545 Technology Square
Cambridge , MA 02139

We analyse the computational complexity of phonological models as they have developed over the past twenty years  . The major result sate that generation and recognition are undecidable for segmental models  , and that recognition is NP-hard for that portion of segmental phonology subsumed by modern autosegmental models  . Formal restrictions are evaluated . 
1 Introduction
Generative linguistic theory and human language comprehension may both be thought of as computations  . The goal of language comprehension is to construct structural descriptions of linguistic sensations  , while the goal of generative theory is to enumerate all and only the possible  ( grammatical ) structural descriptions . These computations are only indirectly related . For one , the input to the two computations is not the same  . As we shall see below , the most we might say is that generative theory provides an extensional chatacterlsation f language comprehension  , which is a function from surface forms to complete representations  , including underlying forms . The goal of this article is to reveal exactly what generative linguistic theory says about language comprehension in the domain of phonology  . 
The article is organized as follows . In the next section , we provide a brief overview of the computational structure of generative phonology  . In section 3 , we introduce the segmental model of phonology , discuss its computational complexity , and prove that even restricted segmental models are extremely powerful  ( undecidable )  . Subsequently , we consider various proposed and plausible restrictions on the model  , and conclude that even the maximally restricted segmental model is likely to be intractable  . The fourth section in - . 
troduces the modern autosegmental ( nonlinear ) model and discusses its computational complexity . 
" The author is supported by a IBM graduate fellowship and eternally indebted to Morris Halle and Michael Kenstowicz for teaching him phonology  . Thanks to Noam Chomsky , Sandiway Fong , and Michael Kashket for their comments and assistance  . 

We prove that the natural problem of constructing an autosegmental representation fan underspecified surface form is NP-hard  . The article concludes by arguing that the complexity proofs are unnatural despite being true of the phonological models  , because the formalism of generative phonology is itself unnatural  . 
The central contributions of this article ate : ( i ) to explicate the relation between generative theory and language processing  , and argue that generative theories are not models of language users primarily because they do not consider the inputs naturally available to language users  ; and ( ii ) to analyze the computational complexity of generative phonological theory  , as it has developed over the past twenty years , including segmental and autosegmental models . 
2 Computational structure of generative phonology The structure of a computation may be described at many levels of abstraction  , principally including : ( i ) the goal of the computation ;   ( ii ) its in-put/output specification ( the problem statement )  ,   ( iii ) the algorithm and representation for achieving that specification  , and ( iv ) the primitive operations in which terms the algorithm is implemented  ( the machine architecture )  . 
Using this framework , the computational structure of generative phonology may be described as follows : ? The computational goal of generative phonology  ( as distinct from it's research goals ) is to enumerate the phonological dictionaries of all and only the possible human languages  . 
? The problem statement is to enumerate the observed phonological dictionary of sparticu-lax language from some underlying dictionary of morphemes  ( roots and affixes ) and phonological processes that apply to combinations of underlying morphemes  . 
? The algorithm by which this is accomplished is a derivational process g  ( ' the grammar ' ) from underlying forms z to surface forms y = g ( z )   . Underlying forms are constructed by combining ( typically , with concatenation or substitution ) the forms stored in the underlying dictionary of morphemes  . Linguistic relations are represented both in the structural descriptions and the derivational process  . 
The structural descriptions of phonology are representations of perceivable distinctions between linguistic sounds  , such as stress levels , syllable structure , tone , and articulatory gestures . The underlying and surface forms are both drawn from the same class of structural descriptions  , which consist of both segmental strings and autosegmental relations  . A segmental string is a string of segments with some representation of constituent structur  . In the SPE theory of Chomsky and Halle ( 1968 ) concrete boundary symbols are used ; in Lexical Phonology , abstract brackets are used . Each segment is a set of phonological features , which are abstract as compared with phonetic representations  , although both are given in terms of phonetic features  . Supra segmental relations are relations among segments  , rather than properties of individual segments . For example , a syllable is a hierarchical relation between a sequence of segments  ( the nucleus of the syllable ) and the less sonorous segments that immediately preceed and follow it  ( the onset and coda , respectively ) . Syllables must satisfy certain universal constraints  , such as the sonority sequencing constraint , as well as language particular ones . 
a The derivntional process is implemented by an ordered sequence of unrestricted rewriting rules that are applied to the current derivation string to obtain surface forms  . 
According to generative phonology , comprehension consists of finding a structural description for a given surface form  . In effect , the logical problem of language comprehension is reduced to the problem of searching for the underlying form that generates a given surface form  . When the surface form does not transparently identify its corresponding underlying form  , when the space of possible underlying forms is large  , or when the grammar g is computationally complex , the logical problem of language comprehension can quickly become very difficult  . 
In fact , the language comprehension problem is intractable for all segmental theories  . For example , in the formal system of The Sound Pat . 
tern of English ( SPE ) the comprehension problem is undecidable . Even if we replace the segmental representation of cyclic boundaries with the abstract constituents of Lexical Phonology  , and prohibit derivational rules from read justing constituent boundaries  , comprehension remains PSPACE-complete . Let us now turn to the technical details . 
3 Segmental Phonology
The essential components of the segmental model may be briefly described as follows  . The set of features includes both phonological features and diacritics and the distinguished feature segment that marks boundaries  . ( An example diacritic is a blaut , a feature that marks stems that must undergo a change vowel quality  , such as tense-conditioned a blaut in the English sing  , sang , sung alternation . ) As noted in SPE , " technically speaking , the number of diacritic feature should be at least as large as the number of rules in the phonology  . Hence , unless there is a bound on the length of a phonology  , the set\[o features \] should be unlimited . "( f n . 1, p . 390 ) Features may be specified q-or-or by an integral value  1  ,  2  ,   .   .   .   , N where N is the maximal deg/ee of differentiation permitted for any linguistic feature  . Note that N may vary from language to language , because languages admit different degrees of differentiation i such features as vowel height  , stress , and tone . A set of feature specifications i called a unit or sometimes a segment  . A string of units is called a matrizor a segmental string  . 
A elementary rule is of the form ZXAYW
ZXBYW where A and B may be ~ borany unit , A  ~ B ; X and Y may be matrices ( strings of units ) , and Z and W may be thought of a brackets labelled with syntactic ategorie such as ' S ' or ' N' and so forth  . A comple=rule is a finite schema for generating a  ( potentially infinite ) set of elementary rules .   1 The rules are organised into 1Following   3ohnson   ( 1972 )  , we may define schenm as follows . The empty string and each unit is sschema ; schema may be combined by the operations of union  , intersection , egation , kleene star , and exponentiation over the set of units . Johnson also introduces variables and Boolean conditions into the schema  . This " schema language " is a extremely powerful characterisation f the class of regular languages over the alphabet of units  ; it is not used by practicing phonologists . Because a given complex rule can represent an in finite set of element aryules  , Johnson shows how the iterated , exhaustive application of one complex rule to a given segmental string can " effect virtually any computable mapping  , "( p . 10) ie . , can simulate any TNI computation . Next , he proposes a more restricted " simultaneous " mode of application for a complex rule  , which is only capable of performing a finite -state mapping in any application  . This article considers the independent question of what computations can be performed by a set of element aryules  , and hence provides loose lower bounds for Johnson 's model  . We note in passing , however , that the problem of simply determining whether a given rule is subsumed by one of Johnson's schema is itself intractable  , requiring at least exponen-plied in order to an underlying matrix to obtain a surface matrix  . 
Ignoring a great many issues that are important for linguistic reasons but iz relevant for our purposes  , we may think of the derivational process as follows  . The input to the derivation , or " underlying form , " is a bracketed string of morphemes , the output of the syntax . The output of the derivation is the " surface form  , " a string of phonetic units . 
The derivation consists of a series of cycles . On each cycle , the ordered sequence of rules is applied to every maximal string of units containing no internal brackets  , where each P~+ , applies ( or doesn't apply ) to the result of applying the immediately preceding rule Ri  , and so forth . Each rule applies simultaneously to all units in the current derivations \] string  . For example , if we apply the rule A--*B to the string AA , the result is the string BB . At the end of the cycle , the last rule P ~ erases the inner most brackets , and then the next cycle begins with the rule R1 . The derivation terminates when all the brackets ateerased  . 
Some phonological processes , such as the assimilation of voicing across morpheme boundaries  , are very common across the world's languages . 
Other processes , such as the atbitraty insertion of consonants or the substitution of one unit for another entirely distinct unit  , ate extremely rate or entirely unattested . For this reason , all adequate phonological theories must include an explicit measure of the naturalness of a phonological process  . A phonological theory must also define a criterion to decide what constitutes two independent phonological processes and what constitutes a legitimate phonological generalization  . 
Two central hypotheses of segmental phonology are ( i ) that the most natural grammaxs contain the fewest symbols and  ( ii ) a set of rules represent independent phonological processes when they cannot be combined into a single rule schema according to the intricate notational system first described in SPE  .   ( Chapter 9 of Kenstowicz and Kisseberth ( 1979 ) contains a less technical sum-maty of the SPE system and a discussion of subsequent modifications and emendations to it  . ) 3  . 1 Complex i ty o f segmenta l recogn i t ion and generation  . 
Let us say a dictionary D is a finite set of the underlying phonological forms  ( matrices ) of morphemes . These morphemes may be combined by concatenation adsimple substitution  ( a syntactic category is replaced by a morpheme of that category  ) to form a possibly in finite set of underlying forms  . Then we may characterize the two central computations of phonology as follows  . 
tial space.
The phonological generation problem ( PGP ) is : Given a completely specified phonological matrix z and a segmental grammar g  , compute the surface form y : g(z ) of z . 
The phonological recognition problem ( PRP ) is : Given a ( partially specified ) surface form y , a dictionary D of underlying forms , and a segmental grammar g , decide if the surface form y = g ( = ) can be derived from some underlying form z according to the grammar g  , where z constructed from the forms in D . 
Lenuna 3 . 1 The segmental model can directly simulate the computation of any deterministic ~ Turing machine Monanyin put w  , using only elementary rules . 
Proof . We sketch the simulation . The underlying form z will represent the TM input w  , while the surface formy will represent the halted state of M on w  . The immediate description of the machine ( tape contents , head position , state symbol ) is represented in the string of units . Each unit represents the contents of a tape square  . The unit representing the currently scanned tape square will also be specified for two additional features  , to represent the state symbol of the machine and the direction in which the head will move  . Therefore , three feature sate needed , with a number of specifications determined by the finite control of the machine M  . Each transition of M is simulated by a phonological rule  . A few rules ate also needed to move the head position around  , and to erase the entire derivation string when the simulated m~chine halts  . 
There are only two key observations , which do not appear to have been noticed before . The first is that contraty to populat misstatement  , phonological rules at e not context-sensitive . Rather , theyate unrestricted rewriting rules because they can perform deletions as well as insertions  . ( This is essential to the reduction , because it allows the derivation string to become at bitatily long  . ) The second observation is that segmental rules can f ~ eely manipulate  ( insert and delete ) boundary symbols , and thus it is possible to prolong the derivation indefinitely : we need only employ a rule R  , ~_ , at the end of the cycle that adds an extra boundary symbol to each end of the derivation string  , unless the simulated machine has halted . 
The remaining details are omitted , but may be found in Ristad (1990) .  \ [ \ ]
The immediate consequences are :
Theorem IPGP is undecidable.
Proof . By reduction to the undecidable problem w6L ( M ) ? of deciding whether a given TMM accepts an input w  . The input to the generation problem consists of an underlying form z that represents w and a segmental grammaring to \] emma  3  . 1 . The output is a surface form y : g ( z ) that represents he halted configuration of the TM  , with all but the accepting uniterased . 

Theorem 2PRP is undecidable.
Proof . By reduction to the undecidable problem L ( M ) = ? ~ b of deciding whether a given TMM accepts any inputs  . The input to the recognition problem consists of a surface formy that represents he halted accepting state of the TM  , a trivial dictionary capable of generating E * , and a segmental grammarg that simulates the computations of the TM according to lemma  3  . 1 . The output is an underlying form z that represents he input that M accepts  . The only trick is to construct a ( trivial ) dictionary capable of generating all possible underlying forms E *  . \[\] An important corollary to lemma 3 . 1 is that we can encode a universal Turing machine in a segmental grammax  . If we use the four-symbol seven-state " smallest UTM " of Minsky  ( 1969 )  , then the resulting segmental model contains no more than three features  , eight specifications , and 36 very simple rules ( exact details in Ristad ,  1990) . As mentioned above , a central component of the segmental theory is an evaluation metric that favors simpler  ( i e . , shorter ) grammars . This segmental grammar of universal computation appears to contain significantly fewer symbols than a segmental grammar for any natural language  . Therefore , this corollary presents severe conceptual nd empirical problems for the segmental theory  . 
Let us now turn to consider the range of plausible restrictions on the segmental model  . At first glance , it may seem that the single most important computational restriction is to prevent rules from inserting boundaries  . Rules that manipulate boundaries axe called read just ment rules  . 
They axe needed for two reasons . The first is to reduce the number of cycles in a given derivation by deleting boundaries and flattening syntactic structure  , for example to preven the phonology from assigning too many degrees of stress to a highly -embedded sentence  . The second is to reax range the boundaries given by the syntax when the intonational phrasing of an utterance does not correspond to its syntactic phrasing  ( socalled " bracketing paradoxes " )  . In this case , boundaries are merely moved around , while preserving the total number of boundaries in the string  . The only way to accomplish this kind of bracket read justment in the segmental model is with rules that delete brackets and rules that insert brackets  . Therefore , if we wish to exclude rules that insert boundaries  , we must provide an alternate mechanism for boundary read justment  . 
For the sake of axgument--and because it is not too hard to construct such a boundary read just -ment mechanism--letushence for thad op this restriction  . Now how powerful is the segmental model ? Although the generation problem is nowcer-taiuly decidable  , the recognition problem remains undecidable , because the dictionary and syntax are both potentially infinite sources of boundaries : the underlying form z needed to generate any given surface form according to the grammarg could be axbitradly long and contain an axbi-traxy number of boundaries  . Therefore , the complexity of the recognition problem is unaffected by the proposed restriction on boundary read just-ments  . The obvious restriction then is to additionally limit the depth of embeddings by some fixed constant  .   ( Chomsky and Halle flirt with this restriction for the linguistic reasons mentioned above  , but view it as a performance limitation , and hence choose not to adopt it in their theory of linguistic ompetence  . ) Lernma 3 . 2 Each derivational cycle can directly simulate any polynomial time alternating Turing machine  ( ATM ) M computation . 
Proof . By reduction from a polynomial-depth ATM computation  . The input to the reduction is an ATMM on input w  . The output is a segmen-tadgrammarg and underlying form zs  . t . the surface form y = g ( z ) represents a halted accepting computation iffM accepts ~ v in polynomial time  . 
The major change from lemma 3 . 1 is to encode the entire instantaneous description of the ATM state  ( i e . , tape contents , machine state , head position ) in the features of a single unit . To do this requires a polynomial number of features  , one for each possible tapes quaxe , plus one feature for the machine state and another for the head position  . 
No weach derivation string represents a level of the ATM computation tree  . The transitions of the ATM computation axe encoded in a block B as follows  . An AND-transition is simulated by a triple of rules  , one to insert a copy of the current state , and two to implement the two transitions . An OR-transition is simulated by a pair of disjunctively-ordered rules  , one for each of the possible successor states . The complete rule sequence consists of a polynomial number of copies of the block B  . 
The last rules in the cycle delete halting states , so that the surface form is the empty string ( or reasonably-sized string of ' accepting ' units  ) when the ATM computation halts and accepts . If , on the other hand , the surface form contains any non-halting or nonaccepting units  , then the ATM does not accept its input w in polynomial time  . The reduction may clearly be performed in time polynomial in the size of the ATM and its input  . \[\] Because we have restricted the number of embeddings in an underlying form to be no more than can consist of more than a constant number of cycles  . Therefore , lemma 3 . 2 establishes the following theorems : Theorem 3 PGP with bounded embeddings is
PSPACE . hard.
Proof . The proof is an immediate consequence of lemma 3 . 2 and a corollary to the Chandra-Kosen-Stockmeyer theorem  ( 1981 ) that equates polynomial time ATM computations and PSPACEDTM computations  . \[\] Theozem 4PRP with bounded embeddings is
PSPACE-hard.
Proof . The proof follows from lemma 3 . 2 and the Chandra-Kosen-Stockmeyer result . The dictionary consists of the lone unit that encodes the ATM starting configuration  ( i e . , input w , start state , head on leftmost square ) . The surface string is either the empty string or a unit that represents the halted accepting ATM configuration  . \[\] There is some evidence that this is the most we can do  , at least for the PGP . The requirement that the reduction be polynomial time limits us to specifying a polynomial number of features and a polynomial number of rules  . Since each feature corresponds to a tape square , i e . , the ATM space resource , we are limited to PSPACE ATM computations . Since each phonological rule corresponds to a next-move relation  , i e . , one time step of the ATM , we are thereby limited to specifying PTIME
ATM computations.
For the PRP , the dictionary ( or syntax-interface ) provides the additional ability to nondeterministically guess an arbitrarily long  , boundary-free underlying form z with which to generate a given surface form g  ( z )  . This ability remains unused in the preceeding proof  , and it is not too hard to see how it might lead to undecidability  . 
We conclude this section by summarizing the range of linguistically plausible formal restrictions on the derivational process : Feature system  . As Chomsky and Hallenoted , the SPE formal system is most naturally seen as having a variable  ( unbounded ) set of features and specifications . This is because languages differ in the diacritics they employ  , as well as differing in the degrees of vowel height  , tone , and stress they allow . Therefore , the set of features must be allowed to vary from language to language  , and in principle is limited only by the number of rules in the phonology  ; the set of specifications must likewise be allowed to vary from language to language  . 
It is possible , however , to postulate the existence of a large , fixed , language-universal set of phonological features and a fixed upper limit to the number N of perceivable distinctions any one feature is capable of supporting  . 
If we take these upper limits seriously , then the class of reductions described in lemma 3 . 2 would no longer be allowed .   ( It will be possible to simulate any ~ computation i a single cycle  , however . ) Ruleform__At . Rules that delete , change , exchange , or insert segments -- as well as rules that manipulate boundaries -- are crucial to phonological theorizing  , and therefore cannot be crudely constrained . More subtle and indirect restrictions are needed . One approach is to formulate language-universal constraints on phonological representations  , and to allow a segment to be altered only when it violates some constraint  . 
McCarthy ( 1981:405 ) proposes a morpheme rule constraint ( MRC ) that requires all morphological rules to be of the form A---  , B/X where A is a unitor ~ b , and B and X are ( possibly null ) strings of units . ( X is the immediate context of A , to the right or left . ) It should be obvious that the MRC does not constrain the computational complexity of segmental phonology  . 
4 Autosegmental Phonology
In the past decade , generative phonology has seen a revolution in the linguistic treatment of suprasegmental phenomena such as tone  , harmony , infixation , and stress assignment . Although these autosegmental models have yet to be formalised  , they may be briefly described as follows . 
Rather than one-dimensional strings of segments , representations may be thought of as " a three -dimensional object hat for concreteness one might picture as a spiral-bound notebook  , " whose spine is the segmental string and whose pages contain simple constituent structures that are indendent of the spine  ( Halle 1985 )  . One page represent she sequence of tones associated with a given articulation  . By decoupling the representation ftonal sequences from the articulation sequence  , it is possible for segmental sequences of different lengths to nonetheless be associated to the same tone sequence  . For example , the tonal sequence Low-High-High , which is used by English speakers to express urprise when answering a question  , might be associated to a word containing any number of syllables  , from two ( Brazi 0 totwelve ( floccin-auccinihili pilification ) and beyond . Other pages ( called " planes " ) represent morphemes , yllable structure , vowels and consonants , and the tree of articulatory ( ie . , phonetic ) features . 
239 4 . 1 Complexity of autosegmental recognition . 
In this section , we prove that the PRP for autosegmental models is NP-hard  , a significant reduction in complexity from the undecidable and PSPACE-hard computations of segmental theories  .   ( Note however that autosegmental representations have augmented -- but not replaced -- portions of the segmental model  , and therefore , unless something can be done to simplify segmental derivations  , modern phonology inherits the intractability of purely segmental p proaches  . ) Let us begin by thinking of the NP-complete 3-Satisfiability problem ( 3SAT ) as a set of interacting constraints . In particular , every satisfiable Boolean formula in 3-CNF is a string of clauses C1  , C2 ,   .   .   . , Cp in the variables z l , z = , .   .   .   , z , that satisfies the following three constraints : ( i ) negation : a variable = j and its negation ~ have opposite truth values  ;   ( ii ) clausal satisfaction : every clause C ~ = ( a ~ V bi V c/ ) contains a true literal ( a literal is a variable or its negation )  ;   ( iii ) consistency of truth assignments : every unnegated literal of a given variable is assigned the same truth value  , either 1 or 0 . 
Lemma 4 . 1 Autosegmental representations can enforce the 3SAT constraints . 
ProoLThe idea of the proof is to encode negation and the truth values of variables in features  ; to enforce clausal satisfication with a local autosegmental process  , such as syllable structure ; and to ensure consistency of truth assignments with a nonlocal autosegmental process  , such as a nonconcatenative morphology or long -distance assimilation  ( harmony )  . To implement these ideas we must examine morphology  , harmony , and syllable structure . 
Morphology . In the more familiar languages of the world , such as Romance languages , morphemes are concatenated to form words . In other languages , uchas Semitic languages , a morpheme may appear more that once inside another morpheme  ( this is called infixation )  . For example , the Arabic word kata b , meaning ' he wrote ' , is formed from the active perfective morpheme a doubly in-fixed to the ktb morpheme  . In the autosegmental model , each morpheme is assign edits own plane . 
We can use this system of representation to ensure consistency of truth assigments  . Each Boolean variable z ~ is represented by a separate morpheme p ~  , and every literal of = i in the string of formula literals is associated to the one underlying morpheme p ~  . 
Harmony . Assimilation is the common phonological process whereby some segment comes to share properties of an adjacent segment  . In English , consonant nasality assimilates to immediately preceding vowels  ; assimilation also occurs forms of the prefx in- demonstrate : in + logical-  , illogical and in-l-probable -- , improbable . In other languages , assimilation is unbounded and can affect nonadjacent segments : these assimilation processes are called harmony systems  . In the Turkic languages all sutFt x vowels assimilate the backnesss feature of the last stem vowel  ; in Capanshua , vowels and glides that precede a word-final deleted nasal  ( an underlying nasal segment absent from the surface form  ) are all nasalized . In the autosegmental model , each harmonic feature is assign edits own plane . As with morpheme-infixation , we can represent each Boolean variable by a harmonic feature  , and thereby ensure consistency of truth assignments  . 
Syllable structure . Words are partitioned into syllables . Each syllable contains one or more vow-ds V ( its nucleus ) that may be preceded or followed by consonants C . For example , the Arabic word ka . tab consists of two syIlabhs , the two-segment syllable CV and the three-segment dosed syllable CVC  . Every segment is assigned a sonority value , hr hich ( intuitively ) is proportional to the openness of the vocal cavity  . For example , vowels are the most sonorou segments , while stops such as porb are the least sonorous  . Syllables obey a language-universalonority sequencing constraint  ( SSC )  , which states that the nucleus is the sonority peak of a syllable  , and that the sonority of adjacent segments wiftly and monotonically decreases  . We can use the SSC to ensure that every clause C ~ contains a true literal as follows  . The centred idea is to make literal truth correspond to the stricture feature  , so that a true literal ( represented as a vowel ) is more sonorous than a false literal ( represented as a consonant )  . Each clause C~- ( a ~ Vb ~ Vc ~ ) is encoded as a segmental string C-z , - zb-zc , where C is a consonant of sonority 1 . SegmentzGhasson ority 10 when literal at is true , 2 otherwise ; segment=s has sonority 9 when literal bi is true , 5 otherwise ; and segment z chasson ority 8 when literal q is true , 2 otherwise . 
Of the eight possible truth values of the three literals and ~ he corresponding syllabifications  ,   0nly the syllabification corresponding to three false literals is excluded by the SSC  . In that case , the corresponding string of four consonants C-C-C -C has the sonority sequence  1-2-5-2  . No immediately preceeding or following segment of any sonority can result in a syllabification that obeys the SSC  . 
Therefore , all Boolean clauses must contain a true literal . ( Complete proof in Ristad ,  1990 ) \[\] The direct consequence of this lemma 4 . 1 is : Theorem 5 PRP for the autose graental model is

Proof . By reduction to 3SAT . The idea is to construct a surface form that completely identities the variables and their negation or lack of it  , but does not specify the truth values of those variables  . The dictionary will generate all possible underlying forms  ( in fixed morphemes or harmonic strings )  , one for each possible truth assignment , and the autosegmental representation of lemma 4 . 1 will ensure that generated formulas are in fact satisfiable  . \[\] 5 Conclusion . 
In my opinion , the preceding proofs are unnatural , despite being true of the phonological models , because the phonological models themselves are unnatural  . Regarding segmental models , the unde-cidability results tell us that the empirical content of the SPE theory is primarily in the particular rules postulated for English  , and not in the extremely powerful and opaque formal system  . We have also seen that symbol-minimization is a poor metric for naturalness  , and that the complex no-rational system of SPE ( not discussed here ) is an inadequate characterization f the notion of " appropriate phonological generalisation  .   "2 Because not every segmental grammarg generates a natural set of sound patterns  , why should we have any faith or interest in the formal system ? The only justification for these formal systems then is that they are good programming languages for phonological processes  , that clearly capture our intuitions about human phonology  . But segmental theories are not such good programming languages  . They are notationally-constrained an highly -articulated  , which limits their expressive power ; they obscurely represent phonological relations in rules and in the derivation process itself  , and hide the dependency relations and interactions among phonological processes in rule ordering  , disjunctive ordering , blocks , and cyclicity , sYet , despite all these opaque notational constraints , it is possible to write a segmental grammar for any decidable set  . 
A third unnatural feature is that the goal of enumerating structural descriptions has an indirect and computationally costly connection to the goal of language comprehension  , which is to construct a structural description of a given utterance  . When information is missing from the surface form  , the generative model obligates itself to enumerate all possible underlying forms that might generate the surface form  . When the generative process is lengthy , capable of deletions , or capable of enforcing complex interactions between nonlocal and local relations  , then the logical problem of language comprehension will be intractable  . 
Natural phonological processe seem to avoid complexity and simplify interactions  . It is hard to find an phonological constraint that is absolute and inviolable  . There are always exceptions , exceptions to the exceptions , and so forth . Deletion processes like a pocope , syncopy , clusters implica-tion and strayer asure , as well as insertions , seem to be motivated by the necessity of modifying a representation to satisfy a phonological constraint  , not to exclude representations or to generate complex sets  , as we have used them here . 
Finally , the goal of enumerating structural descriptions might not be appropriate for phonology and morphology  , because the set of phonological words is only finite and phrase-level phonology is computationally simple  . There is no need or rational for employing such a powerful derivational system when all we are trying to do is capture the relatively little systematicity in a finite set of representations  . 
6 References.
2The explication of what constitutes a " natural rule " is significantly more elusive than the symbol-minimization metric suggests  . Explicit symbol-counting is rarely performed by practicing phonolo-gists  , and when it is , it results in unnatural rules . 
Moreover , the goal of constructing the smallest grammar for a given  ( infinite ) set is not attainable in principle , because it requires us to solve the undecidable TM equivalence problem  . Nor does the symbol-counting metzlc constrain the generative or computational power of the formalism  . Worst of all , the UTM simulation suggested above shows that symbol count does not correspond to " naturalness  . " In fact , two of the simplest grammars generate ~ and ~' , both of which are extremely unnatural . 
3 A further difficulty for autosegmental models ( not brought out by the proof ) is that the interactions among planes is obscured by the current practice of imposing an absolute order on the construction of planes in the derivation process  . For example , in English phonology , syllable structure is constructed be-Ch and ra , A . , D . Kozen , and L . Stockmeyer , 1981 . 
Alternation . 3. ACM 28(1):114-133.
Chomsky , Noam and Morris Halle .  1968 . The Sound Pattern of English . New York : Harper

Halle , Morris .  1985 . " Speculations about the representation of words in memory  . " In Phonetic Linguistics , Essays in Honor of Peter Lade-\]oged , V . Fromkin , ed . Academic Press . 
Johnson , C . Douglas .  1972 . Formal Aspects of Phonological Description . The Hague : Mouton . 
Kenstowicz , Michael and Charles Kisseberth.
1979 . Generative Phonology . New York : forestress is assigned , and then recomputed on the basis of the resulting stress assignment  . A more natural approach would be to let stress and syllable structure computations intermingle in a nondirectional process  . 

Academic Press.
McCarthy , John .  1981 . "A prosodic theory of nonconcatenative morphology  . " Linguistic
Inquiry 12, 373-418.
Minsky , Marvin .  1969 . Computation : finite and in finite machines . Englewood Cliffs : Prentice

Ristad , Eric S .  1990 . Computational structure of human language . Ph . D dissertation , MIT Department of Electrical Engineering and Computer Science  . 

