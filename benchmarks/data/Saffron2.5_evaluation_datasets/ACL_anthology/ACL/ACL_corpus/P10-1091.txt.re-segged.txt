Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics , pages 886?896,
Uppsala , Sweden , 1116 July 2010. c?2010 Association for Computational Linguistics
Estimating Strictly Piecewise Distributions
Jeffrey Heinz
University of Delaware
Newark , Delaware , USA
heinz@udel.edu
James Rogers
Earlham College
Richmond , Indiana , USA
jrogers@quark.cs.earlham.edu
Abstract
Strictly Piecewise ( SP ) languages are a subclass of regular languages which encode certain kinds of long-distance dependencies that are found in natural languages . Like the classes in the Chomsky and Subregular hierarchies , there are many independently converging characterizations of the SP class ( Rogers et al , to appear ). Here we define SP distributions and show that they can be efficiently estimated from positive data.
1 Introduction
Long-distance dependencies in natural language are of considerable interest . Although much attention has focused on long-distance dependencies which are beyond the expressive power of models with finitely many states ( Chomsky , 1956; Joshi , 1985; Shieber , 1985; Kobele , 2006), there are some long-distance dependencies in natural language which permit finite-state characterizations.
For example , although it is wellknown that vowel and consonantal harmony applies across any arbitrary number of intervening segments ( Ringen , 1988; Bakovic ?, 2000; Hansson , 2001; Rose and Walker , 2004) and that phonological patterns are regular ( Johnson , 1972; Kaplan and Kay , 1994), it is less wellknown that harmony patterns are largely characterizable by the Strictly Piecewise languages , a subregular class of languages with independently-motivated , converging characterizations ( see Heinz (2007, to appear ) and especially
Rogers et al (2009)).
As shown by Rogers et al ( to appear ), the Strictly Piecewise ( SP ) languages , which make distinctions on the basis of ( potentially ) discontiguous subsequences , are precisely analogous to the Strictly Local ( SL ) languages ( McNaughton and Papert , 1971; Rogers and Pullum , to appear ), which make distinctions on the basis of contiguous subsequences . The Strictly Local languages are the formal-language theoretic foundation for ngram models ( Garcia et al , 1990), which are widely used in natural language processing ( NLP ) in part because such distributions can be estimated from positive data ( i.e . a corpus ) ( Jurafsky and Martin , 2008). N - gram models describe probability distributions over all strings on the basis of the Markov assumption ( Markov , 1913): that the probability of the next symbol only depends on the previous contiguous sequence of length n ? 1. From the perspective of formal language theory , these distributions are perhaps properly called Strictly k-Local distributions ( SLk ) where k = n . It is wellknown that one limitation of the Markov assumption is its inability to express any kind of long-distance dependency.
This paper defines Strictly k-Piecewise ( SPk ) distributions and shows how they too can be efficiently estimated from positive data . In contrast with the Markov assumption , our assumption is that the probability of the next symbol is conditioned on the previous set of discontiguous subsequences of length k ? 1 in the string . While this suggests the model has too many parameters ( one for each subset of all possible subsequences ), in fact the model has on the order of |?| k+1 parameters because of an independence assumption : there is no interaction between different subsequences.
As a result , SP distributions are efficiently computable even though they condition the probability of the next symbol on the occurrences of earlier ( possibly very distant ) discontiguous subsequences . Essentially , these SP distributions reflect a kind of longterm memory.
On the other hand , SP models have no shortterm memory and are unable to make distinctions on the basis of contiguous subsequences . We do not intend SP models to replace ngram models , but instead expect them to be used alongside of scope of this paper and is left for future research.
Since SP languages are the analogue of SL languages , which are the formal-language theoretical foundation for ngram models , which are widely used in NLP , it is expected that SP distributions and their estimation will also find wide application . Apart from their interest to problems in theoretical phonology such as phonotactic learning ( Coleman and Pierrehumbert , 1997; Hayes and Wilson , 2008; Heinz , to appear ), it is expected that their use will have application , in conjunction with ngram models , in areas that currently use them ; e.g . augmentative communication ( Newell et al , 1998), part of speech tagging ( Brill , 1995), and speech recognition ( Jelenik , 1997).
?2 provides basic mathematical notation . ?3 provides relevant background on the subregular hierarchy . ?4 describes automata-theoretic characterizations of SP languages . ?5 defines SP distributions . ?6 shows how these distributions can be efficiently estimated from positive data and provides a demonstration . ?7 concludes the paper.
2 Preliminaries
We start with some mostly standard notation . ? denotes a finite set of symbols and a string over ? is a finite sequence of symbols drawn from that set . ? k , ?? k , ?? k , and ?? denote all strings over this alphabet of length k , of length less than or equal to k , of length greater than or equal to k , and of any finite length , respectively . ? denotes the empty string . | w | denotes the length of string w . The prefixes of a string w are Pfx(w ) = { v : ? u ? ?? such that vu = w}.
When discussing partial functions , the notation ? and ? indicates that the function is undefined , respectively is defined , for particular arguments.
A language L is a subset of ??. A stochastic language D is a probability distribution over ??.
The probability p of word w with respect to D is written PrD(w ) = p . Recall that all distributions D must satisfy ? w ??? PrD(w ) = 1. If L is language then PrD(L ) = ? w?L PrD(w).
A Deterministic Finitestate Automaton ( DFA ) is a tuple M = ? Q ,?, q0, ?, F ? where Q is the state set , ? is the alphabet , q0 is the start state , ? is a deterministic transition function with domain Q ? ? and codomain Q , F is the set of accepting states . Let d ? : Q ? ?? ? Q be the ( partial ) path function of M , i.e ., d?(q , w ) is the ( unique ) state reachable from state q via the sequence w , if any , or d?(q , w )? otherwise . The language recognized by a DFA M is
L(M ) def = { w ? ?? | d?(q0, w )? ? F}.
A state is useful iff for all q ? Q , there exists w ? ?? such that ?( q0, w ) = q and there exists w ? ?? such that ?( q , w ) ? F . Useless states are not useful . DFAs without useless states are trimmed.
Two strings w and v over ? are distinguished by a DFA M iff d?(q0, w ) 6= d?(q0, v ). They are Nerode equivalent with respect to a language L if and only if wu ? L ?? vu ? L for all u ? ??. All DFAs which recognize L must distinguish strings which are inequivalent in this sense , but no DFA recognizing L necessarily distinguishes any strings which are equivalent . Hence the number of equivalence classes of strings over ? modulo Nerode equivalence with respect to L gives a ( tight ) lower bound on the number of states required to recognize L.
A DFA is minimal if the size of its state set is minimal among DFAs accepting the same language . The product of n DFAs M1 . . . Mn is given by the standard construction over the state space Q1 ? . . .? Qn ( Hopcroft et al , 2001).
A Probabilistic Deterministic Finitestate Automaton ( PDFA ) is a tuple M = ? Q ,?, q0, ?, F , T ? where Q is the state set , ? is the alphabet , q0 is the start state , ? is a deterministic transition function , F and T are the final-state and transition probabilities . In particular , T : Q ? ? ? R + and F : Q ? R + such that for all q ? Q , F ( q ) + ? a??
T ( q , a ) = 1. (1)
Like DFAs , for all w ? ??, there is at most one state reachable from q0. PDFAs are typically represented as labeled directed graphs as in Figure 1.
A PDFA M generates a stochastic language DM . If it exists , the ( unique ) path for a word w = a0 . . . ak belonging to ?? through a PDFA is a sequence ?( q0, a0), ( q1, a1), . . . , ( qk , ak )?, where qi+1 = ?( qi , ai ). The probability a PDFA assigns to w is obtained by multiplying the transition probabilities with the final probability along w?s path if b:2/10 c:3/10
B:4/9a:3 /10 a:2/9 b:2/9 c:1/9
Figure 1: A picture of a PDFA with states labeled A and B . The probabilities of T and F are located to the right of the colon.
it exists , and zero otherwise.
PrDM(w ) = ( k ? i=1
T ( qi?1, ai?1) ) ? F ( qk+1) (2) if d?(q0, w )? and 0 otherwise A probability distribution is regular deterministic iff there is a PDFA which generates it.
The structural components of a PDFA M are its states Q , its alphabet ?, its transitions ?, and its initial state q0. By structure of a PDFA , we mean its structural components . Each PDFA M defines a family of distributions given by the possible instantiations of T and F satisfying Equation 1. These distributions have | Q |? (|?| + 1) independent parameters ( since for each state there are |?| possible transitions plus the possibility of finality .) We define the product of PDFA in terms of co-emission probabilities ( Vidal et al , 2005a).
Definition 1 Let A be a vector of PDFAs and let | A | = n . For each 1 ? i ? n let Mi = ? Qi ,?, q0i , ? i , Fi , Ti ? be the ith PDFA in A . The probability that ? is co-emitted from q1, . . . , qn in
Q1, . . . , Qn , respectively , is
CT (??, q1 . . . qn ?) = n ? i=1
Ti(qi , ?).
Similarly , the probability that a word simultaneously ends at q1 ? Q1 . . . qn ? Qn is
CF (? q1 . . . qn ?) = n ? i=1
Fi(qi).
Then ? A = ? Q ,?, q0, ?, F , T ? where 1. Q , q0, and ? are defined as with DFA product.
2. For all ? q1 . . . qn ? ? Q , let
Z(?q1 . . . qn ?) =
CF (? q1 . . . qn ?) + ? ???
CT (??, q1 . . . qn ?) be the normalization term ; and ( a ) let F (? q1 . . . qn ?) = CF (? q1 ... qn?)Z(?q1 ... qn ?) ; and ( b ) for all ? ? ?, let T (? q1 . . . qn ?, ?) = CT (??, q1 ... qn?)Z(?q1 ... qn ?) In other words , the numerators of T and F are defined to be the co-emission probabilities ( Vidal et al ., 2005a ), and division by Z ensures that M defines a wellformed probability distribution . Statistically speaking , the co-emission product makes an independence assumption : the probability of ? being co-emitted from q1, . . . , qn is exactly what one expects if there is no interaction between the individual factors ; that is , between the probabilities of ? being emitted from any qi . Also note order of product is irrelevant up to renaming of the states , and so therefore we also speak of taking the product of a set of PDFAs ( as opposed to an ordered vector).
Estimating regular deterministic distributions is well-studied problem ( Vidal et al , 2005a ; Vidal et al ., 2005b ; de la Higuera , in press ). We limit discussion to cases when the structure of the PDFA is known . Let S be a finite sample of words drawn from a regular deterministic distribution D . The problem is to estimate parameters T and F of M so that DM approaches D . We employ the widely-adopted maximum likelihood ( ML ) criterion for this estimation.
(T ? , F ? ) = argmax
T,F ( ? w?S
PrM(w ) ) (3)
It is wellknown that if D is generated by some PDFA M ? with the same structural components as M , then optimizing the ML estimate guarantees that DM approaches D as the size of S goes to infinity ( Vidal et al , 2005a ; Vidal et al , 2005b ; de la Higuera , in press).
The optimization problem (3) is simple for deterministic automata with known structural components . Informally , the corpus is passed through the PDFA , and the paths of each word through the corpus are tracked to obtain counts , which are then normalized by state . Let M = ? Q ,?, ?, q0, F , T ? be the PDFA whose parameters F and T are to be estimated . For all states q ? Q and symbols a ? ?, The ML estimation of the probability of T ( q , a ) is obtained by dividing the number of times this transition is used in parsing the sample S by the b:2 c:3
B:4a :3 a :2 b:2 c:1
Figure 2: The automata shows the counts obtained by parsing M with sample
S = { ab , bba , ?, cab , acb , cc}.
SL SP
LT PT
LTT
SF
FO
Reg MSO
Prop +1 <
Figure 3: Parallel Subregular Hierarchies.
number of times state q is encountered in the parsing of S . Similarly , the ML estimation of F ( q ) is obtained by calculating the relative frequency of state q being final with state q being encountered in the parsing of S . For both cases , the division is normalizing ; i.e . it guarantees that there is a wellformed probability distribution at each state . Figure 2 illustrates the counts obtained for a machine M with sample S = { ab , bba , ?, cab , acb , cc}.1 Figure 1 shows the PDFA obtained after normalizing these counts.
3 Subregular Hierarchies
Within the class of regular languages there are dual hierarchies of language classes ( Figure 3), one in which languages are defined in terms of their contiguous substrings ( up to some length k , known as k-factors ), starting with the languages that are Locally Testable in the Strict Sense ( SL ), and one in which languages are defined in terms of their not necessarily contiguous subsequences , starting with the languages that are Piecewise 1Technically , this acceptor is neither a simple DFA or PDFA ; rather , it has been called a Frequency DFA . We do not formally define them here , see ( de la Higuera , in press).
Testable in the Strict Sense ( SP ). Each language class in these hierarchies has independently motivated , converging characterizations and each has been claimed to correspond to specific , fundamental cognitive capabilities ( McNaughton and Papert , 1971; Brzozowski and Simon , 1973; Simon , 1975; Thomas , 1982; Perrin and Pin , 1986; Garc??a and Ruiz , 1990; Beauquier and Pin , 1991; Straubing , 1994; Garc??a and Ruiz , 1996; Rogers and Pullum , to appear ; Kontorovich et al , 2008; Rogers et al ., to appear).
Languages in the weakest of these classes are defined only in terms of the set of factors ( SL ) or subsequences ( SP ) which are licensed to occur in the string ( equivalently the complement of that set with respect to ?? k , the forbidden factors or forbidden subsequences ). For example , the set containing the forbidden 2-factors { ab , ba } defines a Strictly 2-Local language which includes all strings except those with contiguous substrings { ab , ba }. Similarly since the parameters of ngram models ( Jurafsky and Martin , 2008) assign probabilities to symbols given the preceding contiguous substrings up to length n ? 1, we say they describe Strictly n-Local distributions.
These hierarchies have a very attractive modeltheoretic characterization . The Locally Testable ( LT ) and Piecewise Testable languages are exactly those that are definable by propositional formulae in which the atomic formulae are blocks of symbols interpreted factors ( LT ) or subsequences ( PT ) of the string . The languages that are testable in the strict sense ( SL and SP ) are exactly those that are definable by formulae of this sort restricted to conjunctions of negative literals . Going the other way , the languages that are definable by First-Order formulae with adjacency ( successor ) but not precedence ( less-than ) are exactly the Locally Threshold Testable ( LTT ) languages . The Star-Free languages are those that are First-Order definable with precedence alone ( adjacency being FO definable from precedence ). Finally , by extending to Monadic Second-Order formulae ( with either signature , since they are MSO definable from each other ), one obtains the full class of Regular languages ( McNaughton and Papert , 1971; Thomas , 1982; Rogers and Pullum , to appear ; Rogers et al , to appear).
The relation between strings which is fundamental along the Piecewise branch is the subse-w ? v def ?? w = ? or w = ?1 ? ? ? ? n and (? w0, . . . , wn ? ??)[ v = w0?1w1 ? ? ? ? nwn].
in which case we say w is a subsequence of v.
For w ? ??, let
Pk(w ) def = { v ? ? k | v ? w } and
P?k(w ) def = { v ? ?? k | v ? w }, the set of subsequences of length k , respectively length no greater than k , of w . Let Pk(L ) and P?k(L ) be the natural extensions of these to sets of strings . Note that P0(w ) = {?}, for all w ? ??, that P1(w ) is the set of symbols occurring in w and that P?k(L ) is finite , for all L ? ??.
Similar to the Strictly Local languages , Strictly Piecewise languages are defined only in terms of the set of subsequences ( up to some length k ) which are licensed to occur in the string.
Definition 2 ( SPk Grammar , SP ) A SPk grammar is a pair G = ??, G ? where G ? ? k . The language licensed by a SPk grammar is
L(G ) def = { w ? ?? | P?k(w ) ? P?k(G)}.
A language is SPk iff it is L(G ) for some SPk grammar G . It is SP iff it is SPk for some k.
This paper is primarily concerned with estimating Strictly Piecewise distributions , but first we examine in greater detail properties of SP languages , in particular DFA representations.
4 DFA representations of SP Languages
Following Sakarovitch and Simon (1983),
Lothaire (1997) and Kontorovich , et al (2008), we call the set of strings that contain w as a subsequence the principal shuffle ideal2 of w:
SI(w ) = { v ? ?? | w ? v}.
The shuffle ideal of a set of strings is defined as
SI(S ) = ? w?SSI(w)
Rogers et al ( to appear ) establish that the SP languages have a variety of characteristic properties.
Theorem 1 The following are equivalent:3 2Properly SI(w ) is the principal ideal generated by { w } wrt the inverse of ?.
3For a complete proof , see Rogers et al ( to appear ). We only note that 5 implies 1 by DeMorgan?s theorem and the fact that every shuffle ideal is finitely generated ( see also
Lothaire (1997)).
1 b c 2a b c
Figure 4: The DFA representation of SI(aa).
1. L = ? w?S [ SI(w )], S finite , 2. L ? SP 3. (? k)[P?k(w ) ? P?k(L ) ? w ? L ], 4. w ? L and v ? w ? v ? L ( L is subsequence closed ), 5. L = SI(X ), X ? ?? ( L is the complement of a shuffle ideal).
The DFA representation of the complement of a shuffle ideal is especially important.
Lemma 1 Let w ? ? k , w = ?1 ? ? ? ? k , and MSI(w ) = ? Q ,?, q0, ?, F ?, where Q = { i | 1 ? i ? k }, q0 = 1, F = Q and for all qi ? Q ,? ? ?: ?( qi , ?) = ? ? ? qi+1 if ? = ? i and i < k , ? if ? = ? i and i = k , qi otherwise.
Then MSI(w ) is a minimal , trimmed DFA that recognizes the complement of SI(w ), i.e ., SI(w ) =
L(MSI(w)).
Figure 4 illustrates the DFA representation of the complement of SI(aa ) with ? = { a , b , c }. It is easy to verify that the machine in Figure 4 accepts all and only those words which do not contain an aa subsequence.
For any SPk language L = L (??, G ?) 6= ??, the first characterization (1) in Theorem 1 above yields a nondeterministic finite-state representation of L , which is a set A of DFA representations of complements of principal shuffle ideals of the elements of G . The trimmed automata product of this set yields a DFA , with the properties below ( Rogers et al , to appear).
Lemma 2 Let M be a trimmed DFA recognizing a SPk language constructed as described above.
Then : 1. All states of M are accepting states : F = Q.
890 ab c b c b a c a b b c b b a b ? ?, a ?, b ?, c ?, a,b ?, b,c ?, a,c ?, a,b,c Figure 5: The DFA representation of the of the SP language given by G = ?{ a , b , c }, { aa , bc}?.
Names of the states reflect subsets of subsequences up to length 1 of prefixes of the language.
Note this DFA is trimmed , but not minimal.
2. For all q1, q2 ? Q and ? ? ?, if d?(q1, ?)? and d?(q1, w ) = q2 for some w ? ?? then d?(q2, ?)?. ( Missing edges propagate down .) Figure 5 illustrates with the DFA representation of the of the SP2 language given by G = ?{ a , b , c }, { aa , bc }?. It is straightforward to verify that this DFA is identical ( modulo relabeling of state names ) to one obtained by the trimmed product of the DFA representations of the complement of the principal shuffle ideals of aa and bc , which are the prohibited subsequences.
States in the DFA in Figure 5 correspond to the subsequences up to length 1 of the prefixes of the language . With this in mind , it follows that the DFA of ?? = L(?,?k ) has states which correspond to the subsequences up to length k ? 1 of the prefixes of ??. Figure 6 illustrates such a DFA when k = 2 and ? = { a , b , c}.
In fact , these DFAs reveal the differences between SP languages and PT languages : they are exactly those expressed in Lemma 2. Within the state space defined by the subsequences up to length k ? 1 of the prefixes of the language , if the conditions in Lemma 2 are violated , then the DFAs describe languages that are PT but not SP . Pictorially , PT2 languages are obtained by arbitrarily removing arcs , states , and the finality of states from the DFA in Figure 6, and SP2 ones are obtained by non-arbitrarily removing them in accordance with Lemma 2. The same applies straightforwardly for any k ( see Definition 3 below).
a b c a b c b a c c a b a b c a c b b c a a b c ? ?, a ?, b ?, c ?, a,b ?, b,c ?, a,c ?, a,b,c Figure 6: A DFA representation of the of the SP2 language given by G = ?{ a , b , c},?2?. Names of the states reflect subsets of subsequences up to length 1 of prefixes of the language . Note this
DFA is trimmed , but not minimal.
5 SP Distributions
In the same way that SL distributions ( ngram models ) generalize SL languages , SP distributions generalize SP languages . Recall that SP languages are characterizable by the intersection of the complements of principal shuffle ideals . SP distributions are similarly characterized.
We begin with Piecewise-Testable distributions.
Definition 3 A distribution D is k-Piecewise Testable ( written D ? PTDk ) def ?? D can be described by a PDFA M = ? Q ,?, q0, ?, F , T ? with 1. Q = { P?k?1(w ) : w ? ??} 2. q0 = P?k?1(?) 3. For all w ? ?? and all ? ? ?, ?( P?k?1(w ), a ) = P?k?1(wa ) 4. F and T satisfy Equation 1.
In other words , a distribution is k-Piecewise Testable provided it can be represented by a PDFA whose structural components are the same ( modulo renaming of states ) as those of the DFA discussed earlier where states corresponded to the subsequences up to length k ? 1 of the prefixes of the language . The DFA in Figure 6 shows the bution as long as the assigned probabilities satisfy
Equation 1.
The following lemma follows directly from the finite-state representation of PTk distributions.
Lemma 3 Let D belong to PTDk and let M = ? Q ,?, q0, ?, F , T ? be a PDFA representing D defined according to Definition 3.
PrD(?1 . . . ? n ) = T ( P?k?1(?), ?1) ? ? ? ? 2?i?n
T ( P?k?1(?1 . . . ? i?1), ? i ) ? ? (4) ? F ( P?k?1(w))
PTk distributions have 2|?| k?1 (|?|+1) parameters ( since there are 2|?|k?1 states and |?|+1 possible events , i.e . transitions and finality).
Let Pr (? | #) and Pr (# | P?k(w )) denote the probability ( according to some D ? PTDk ) that a word begins with ? and ends after observing P?k(w ). Then Equation 4 can be rewritten in terms of conditional probability as
PrD(?1 . . . ? n ) = Pr(?1 | #) ? ? ? ? 2?i?n
Pr(?i | P?k?1(?1 . . . ? i?1)) ? ?(5) ? Pr (# | P?k?1(w )) Thus , the probability assigned to a word depends not on the observed contiguous sequences as in a Markov model , but on observed subsequences.
Like SP languages , SP distributions can be defined in terms of the product of machines very similar to the complement of principal shuffle ideals.
Definition 4 Let w ? ? k?1 and w = ?1 ? ? ? ? k?1.
Mw = ? Q ,?, q0, ?, F , T ? is a w-subsequence-distinguishing PDFA ( w-SD-PDFA ) iff
Q = Pfx(w ), q0 = ?, for all u ? Pfx(w ) and each ? ? ?, ?( u , ?) = u ? iff u ? ? Pfx(w ) and u otherwise and F and T satisfy Equation 1.
Figure 7 shows the structure of Ma which is almost the same as the complement of the principal shuffle ideal in Figure 4. The only difference is the additional self-loop labeled a on the rightmost state labeled a . Ma defines a family of distributions over ??, and its states distinguish those b c a a a b c ? Figure 7: The structure of PDFA Ma . It is the same ( modulo state names ) as the DFA in Figure 4 except for the self-loop labeled a on state a.
strings which contain a ( state a ) from those that do not ( state ?). A set of PDFAs is a k-set of SD-PDFAs iff , for each w ? ?? k?1, it contains exactly one w-SD-PDFA.
In the same way that missing edges propagate down in DFA representations of SP languages ( Lemma 2), the final and transitional probabilities must propagate down in PDFA representations of SPk distributions . In other words , the final and transitional probabilities at states further along paths beginning at the start state must be determined by final and transitional probabilities at earlier states non-increasingly . This is captured by defining SP distributions as a product of k-sets of
SD-PDFAs ( see Definition 5 below).
While the standard product based on co-emission probability could be used for this purpose , we adopt a modified version of it defined for k-sets of SD-PDFAs : the positive co-emission probability . The automata product based on the positive co-emission probability not only ensures that the probabilities propagate as necessary , but also that such probabilities are made on the basis of observed subsequences , and not unobserved ones . This idea is familiar from ngram models : the probability of ? n given the immediately preceding sequence ?1 . . . ? n?1 does not depend on the probability of ? n given the other ( n ? 1)-long sequences which do not immediately precede it , though this is a logical possibility.
Let A be a k-set of SD-PDFAs . For each w ? ?? k?1, let Mw = ? Qw ,?, q0w , ? w , Fw , Tw ? be the w-subsequence-distinguishing PDFA in A.
The positive co-emission probability that ? is simultaneously emitted from states q ?, . . . , qu from the statesets Q ?, . . . Qu , respectively , of each SD-
PCT (??, q ? . . . qu ?) = ? qw??q?...qu ? qw=w
Tw(qw , ?) (6)
Similarly , the probability that a word simultaneously ends at n states q ? ? Q ?, . . . , qu ? Qu is
PCF (? q ? . . . qu ?) = ? qw??q?...qu ? qw=w
Fw(qw ) (7)
In other words , the positive co-emission probability is the product of the probabilities restricted to those assigned to the maximal states in each Mw . For example , consider a 2-set of SD-PDFAs A with ? = { a , b , c }. A contains four
PDFAs M?,Ma,Mb,Mc . Consider state q = ??, ?, b , c ? ?? A ( this is the state labeled ?, b , c in
Figure 6). Then
CT ( a , q ) = T ?(?, a )? Ta (?, a )? Tb(b , a )? Tc(c , a ) but PCT ( a , q ) = T ?(?, a )? Tb(b , a )? Tc(c , a ) since in PDFA Ma , the state ? is not the maximal state.
The positive co-emission product (?+) is defined just as with co-emission probabilities , substituting PCT and PCF for CT and CF , respectively , in Definition 1. The definition of ?+ ensures that the probabilities propagate on the basis of observed subsequences , and not on the basis of unobserved ones.
Lemma 4 Let k ? 1 and let A be a k-set of SD-PDFAs . Then ?+ S defines a wellformed probability distribution over ??.
Proof Since M ? belongs to A , it is always the case that PCT and PCF are defined . Wellformedness follows from the normalization term as in Definition 1. ? Definition 5 A distribution D is k-Strictly Piecewise ( written D ? SPDk ) def ?? D can be described by a PDFA which is the positive co-emission product of a k-set of subsequence-distinguishing
PDFAs.
By Lemma 4, SP distributions are wellformed.
Unlike PDFAs for PT distributions , which distinguish 2|?|k?1 states , the number of states in a k-set of SD-PDFAs is ? i<k(i + 1)|?|i , which is ?(|?| k+1). Furthermore , since each SD-PDFA only has one state contributing |?|+1 probabilities to the product , and since there are |?? k | = |?| k?1|?|?1 many SD-PDFAs in a k-set , there are |?| k ? 1 |?| ? 1 ? (|?|+ 1) = |?| k+1 + |?| k ? |?| ? 1 |?| ? 1 parameters , which is ?(|?| k).
Lemma 5 Let D ? SPDk . Then D ? PTDk.
Proof Since D ? SPDk , there is a k-set of subsequence-distinguishing PDFAs . The product of this set has the same structure as the PDFA given in Definition 3. ? Theorem 2 A distribution D ? SPDk if D can be described by a PDFA M = ? Q ,?, q0, ?, F , T ? satisfying Definition 3 and the following.
For all w ? ?? and all ? ? ?, let
Z(w ) = ? s?P?k?1(w)
F ( P?k?1(s )) + ? ???? ? ? ? s?P?k?1(w)
T ( P?k?1(s ), ??) ? ? (8) ( This is the normalization term .) Then T must satisfy : T ( P?k?1(w ), ?) = ? s?P?k?1(w ) T ( P?k?1(s ), ?)
Z(w ) (9) and F must satisfy : F ( P?k?1(w )) = ? s?P?k?1(w ) F ( P?k?1(s))
Z(w ) (10)
Proof That SPDk satisfies Definition 3 Follows directly from Lemma 5. Equations 810 follow from the definition of positive co-emission probability . ? The way in which final and transitional probabilities propagate down in SP distributions is reflected in the conditional probability as defined by Equations 9 and 10. In terms of conditional probability , Equations 9 and 10 mean that the probability that ? i follows a sequence ?1 . . . ? i?1 is not only a function of P?k?1(?1 . . . ? i?1) ( Equation 4) but further that it is a function of each subsequence in ?1 . . . ? i?1 up to length k ? 1.
893
In particular , Pr(?i | P?k?1(?1 . . . ? i?1)) is obtained by substituting Pr(?i | P ? k?1(s )) for T ( P ? k?1(s ), ?) and Pr (# | P ? k?1(s )) for F ( P?k?1(s )) in Equations 8, 9 and 10. For example , for a SP2 distribution , the probability of a given P?1(bc ) ( state ?, b , c in Figure 6) is the normalized product of the probabilities of a given P?1(?), a given P?1(b ), and a given P?1(c).
To summarize , SP and PT distributions are regular deterministic . Unlike PT distributions , however , SP distributions can be modeled with only ?(|?| k ) parameters and ?(|?| k+1) states . This is true even though SP distributions distinguish 2|?| k?1 states ! Since SP distributions can be represented by a single PDFA , computing Pr(w ) occurs in only ?(| w |) for such PDFA . While such PDFA might be too large to be practical , Pr(w ) can also be computed from the k-set of SD-PDFAs in ?(| w|k ) ( essentially building the path in the product machine on the fly using Equations 4, 8, 9 and 10).
6 Estimating SP Distributions
The problem of ML estimation of SPk distributions is reduced to estimating the parameters of the SD-PDFAs . Training ( counting and normalization ) occurs over each of these machines ( i.e . each machine parses the entire corpus ), which gives the ML estimates of the parameters of the distribution.
It trivially follows that this training successfully estimates any D ? SPDk.
Theorem 3 For any D ? SPDk , let D generate sample S . Let A be the k-set of SD-PDFAs which describes exactly D . Then optimizing the MLE of S with respect to each M ? A guarantees that the distribution described by the positive co-emission product of ?+ A approaches D as | S | increases.
Proof The MLE estimate of S with respect to SPDk returns the parameter values that maximize the likelihood of S . The parameters of D ? SPDk are found on the maximal states of each M ? A.
By definition , each M ? A describes a probability distribution over ??, and similarly defines a family of distributions . Therefore finding the MLE of S with respect to SPDk means finding the MLE estimate of S with respect to each of the family of distributions which each M ? A defines , respectively.
Optimizing the ML estimate of S for each M ? A means that as | S | increases , the estimates T?M and F?M approach the true values TM and FM . It follows that as | S | increases , T?N + A and F?N + A approach the true values of TN + A and FN + A and consequently DN + A approaches D . ? We demonstrate learning long-distance dependencies by estimating SP2 distributions given a corpus from Samala ( Chumash ), a language with sibilant harmony.4 There are two classes of sibilants in Samala : [- anterior ] sibilants like [ s ] and [> ts ] and [+ anterior ] sibilants like [ S ] and [> tS].5 Samala words are subject to a phonological process wherein the last sibilant requires earlier sibilants to have the same value for the feature [ anterior ], no matter how many sounds intervene ( Applegate , 1972). As a consequence of this rule , there are generally no words in Samala where [- anterior ] sibilants follow [+ anterior ]. E.g.
[StojonowonowaS ] ? it stood upright ? ( Applegate 1972:72) is licit but not *[ Stojonowonowas].
The results of estimating D ? SPD2 with the corpus is shown in Table 6. The results clearly demonstrate the effectiveness of the model : the probability of a [? anterior ] sibilant given P?1([-? anterior ]) sounds is orders of magnitude less than given P?1(? anterior ]) sounds.
x
Pr(x | P?1(y )) s > ts S > tS s 0.0335 0.0051 0.0011 0.0002 ? ts 0.0218 0.0113 0.0009 0.
y S 0.0009 0. 0.0671 0.0353 > tS 0.0006 0. 0.0455 0.0313 Table 1: Results of SP2 estimation on the Samala corpus . Only sibilants are shown.
7 Conclusion
SP distributions are the stochastic version of SP languages , which model long-distance dependencies . Although SP distributions distinguish 2|?|k?1 states , they do so with tractably many parameters and states because of an assumption that distinct subsequences do not interact . As shown , these distributions are efficiently estimable from positive data . As previously mentioned , we anticipate these models to find wide application in NLP.
4The corpus was kindly provided by Dr . Richard Applegate and drawn from his 2007 dictionary of Samala.
5Samala actually contrasts glottalized , aspirated , and plain variants of these sounds ( Applegate , 1972). These laryngeal distinctions are collapsed here for easier exposition.
894
References
R.B . Applegate . 1972. Inesen?o Chumash Grammar.
Ph.D . thesis , University of California , Berkeley.
R.B . Applegate . 2007. Samala-English dictionary : a guide to the Samala language of the Inesen?o Chumash People . Santa Ynez Band of Chumash Indians.
Eric Bakovic ?. 2000. Harmony , Dominance and Control . Ph.D . thesis , Rutgers University.
D . Beauquier and Jean-Eric Pin . 1991. Languages and scanners . Theoretical Computer Science , 84:3?21.
Eric Brill . 1995. Transformation-based error-driven learning and natural language processing : A case study in part-of-speech tagging . Computational Linguistics , 21(4):543?566.
J . A . Brzozowski and Imre Simon . 1973. Characterizations of locally testable events . Discrete Mathematics , 4:243?271.
Noam Chomsky . 1956. Three models for the description of language . IRE Transactions on Information
Theory . IT-2.
J . S . Coleman and J . Pierrehumbert . 1997. Stochastic phonological grammars and acceptability . In Computational Phonology , pages 49?56. Somerset , NJ : Association for Computational Linguistics . Third Meeting of the ACL Special Interest Group in Computational Phonology.
Colin de la Higuera . in press . Grammatical Inference : Learning Automata and Grammars . Cambridge University Press.
Pedro Garc??a and Jose ? Ruiz . 1990. Inference of k-testable languages in the strict sense and applications to syntactic pattern recognition . IEEE Transactions on Pattern Analysis and Machine Intelligence , 9:920?925.
Pedro Garc??a and Jose ? Ruiz . 1996. Learning k-piecewise testable languages from positive data . In Laurent Miclet and Colin de la Higuera , editors , Grammatical Interference : Learning Syntax from Sentences , volume 1147 of Lecture Notes in Computer Science , pages 203?210. Springer.
Pedro Garcia , Enrique Vidal , and Jose ? Oncina . 1990.
Learning locally testable languages in the strict sense . In Proceedings of the Workshop on Algorithmic Learning Theory , pages 325?338.
Gunnar Hansson . 2001. Theoretical and typological issues in consonant harmony . Ph.D . thesis , University of California , Berkeley.
Bruce Hayes and Colin Wilson . 2008. A maximum entropy model of phonotactics and phonotactic learning . Linguistic Inquiry , 39:379?440.
Jeffrey Heinz . 2007. The Inductive Learning of Phonotactic Patterns . Ph.D . thesis , University of
California , Los Angeles.
Jeffrey Heinz . to appear . Learning long distance phonotactics . Linguistic Inquiry.
John Hopcroft , Rajeev Motwani , and Jeffrey Ullman.
2001. Introduction to Automata Theory , Languages , and Computation . Addison-Wesley.
Frederick Jelenik . 1997. Statistical Methods for
Speech Recognition . MIT Press.
C . Douglas Johnson . 1972. Formal Aspects of Phonological Description . The Hague : Mouton.
A . K . Joshi . 1985. Tree-adjoining grammars : How much context sensitivity is required to provide reasonable structural descriptions ? In D . Dowty , L . Karttunen , and A . Zwicky , editors , Natural Language Parsing , pages 206?250. Cambridge University Press.
Daniel Jurafsky and James Martin . 2008. Speech and Language Processing : An Introduction to Natural Language Processing , Speech Recognition , and Computational Linguistics . Prentice-Hall , 2nd edition.
Ronald Kaplan and Martin Kay . 1994. Regular models of phonological rule systems . Computational Linguistics , 20(3):331?378.
Gregory Kobele . 2006. Generating Copies : An Investigation into Structural Identity in Language and Grammar . Ph.D . thesis , University of California,
Los Angeles.
Leonid ( Aryeh ) Kontorovich , Corinna Cortes , and Mehryar Mohri . 2008. Kernel methods for learning languages . Theoretical Computer Science , 405(3):223 ? 236. Algorithmic Learning Theory.
M . Lothaire , editor . 1997. Combinatorics on Words.
Cambridge University Press , Cambridge , UK , New
York.
A . A . Markov . 1913. An example of statistical study on the text of ? eugene onegin ? illustrating the linking of events to a chain.
Robert McNaughton and Simon Papert . 1971.
Counter-Free Automata . MIT Press.
A . Newell , S . Langer , and M . Hickey . 1998. The ro?le of natural language processing in alternative and augmentative communication . Natural Language
Engineering , 4(1):1?16.
Dominique Perrin and Jean-Eric Pin . 1986. First-Order logic and Star-Free sets . Journal of Computer and System Sciences , 32:393?406.
Catherine Ringen . 1988. Vowel Harmony : Theoretical
Implications . Garland Publishing , Inc.
895
James Rogers and Geoffrey Pullum . to appear . Aural pattern recognition experiments and the subregular hierarchy . Journal of Logic , Language and Information.
James Rogers , Jeffrey Heinz , Matt Edlefsen , Dylan Leeman , Nathan Myers , Nathaniel Smith , Molly Visscher , and David Wellcome . to appear . On languages piecewise testable in the strict sense . In Proceedings of the 11th Meeting of the Assocation for
Mathematics of Language.
Sharon Rose and Rachel Walker . 2004. A typology of consonant agreement as correspondence . Language , 80(3):475?531.
Jacques Sakarovitch and Imre Simon . 1983. Subwords . In M . Lothaire , editor , Combinatorics on Words , volume 17 of Encyclopedia of Mathematics and Its Applications , chapter 6, pages 105?134.
Addison-Wesley , Reading , Massachusetts.
Stuart Shieber . 1985. Evidence against the context-freeness of natural language . Linguistics and Philosophy , 8:333?343.
Imre Simon . 1975. Piecewise testable events . In Automata Theory and Formal Languages : 2nd Grammatical Inference conference , pages 214?222,
Berlin ; New York . Springer-Verlag.
Howard Straubing . 1994. Finite Automata , Formal Logic and Circuit Complexity . Birkha?user.
Wolfgang Thomas . 1982. Classifying regular events in symbolic logic . Journal of Computer and Systems
Sciences , 25:360?376.
Enrique Vidal , Franck Thollard , Colin de la Higuera , Francisco Casacuberta , and Rafael C . Carrasco.
2005a . Probabilistic finite-state machines-part I.
IEEE Transactions on Pattern Analysis and Machine
Intelligence , 27(7):1013?1025.
Enrique Vidal , Frank Thollard , Colin de la Higuera , Francisco Casacuberta , and Rafael C . Carrasco.
2005b . Probabilistic finite-state machines-part II.
IEEE Transactions on Pattern Analysis and Machine
Intelligence , 27(7):1026?1039.
896
