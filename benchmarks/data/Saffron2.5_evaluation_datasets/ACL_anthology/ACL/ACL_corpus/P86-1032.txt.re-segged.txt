A MODEL OF PLANINFERENCE THAT DIST IN GUISHES
BETWEENTHEBELIEFS OF ACTORS AND OBSER VERS
Martha E . Pollack
Artificial Intelligence Center

Center for the Study of Language and Information
SRI International
333 Ravenswood Avenue
Menlo Park , CA 94025

Existing models of plan inference ( PI ) in conversation have assumed that the agent whose plan is being inferred  ( the actor ) and the agent drawing the inference ( the observer ) have identical beliefs about actions in the domain  . I argue that this assumption often results in failure of both the PI process and the communicative process that PI is meant to support  . In particular , it precludes the principled generation of appropriate responses to queries that arise from invalid plans  . I describe a model of P1 that abandons this assumption . It rests on an analysis of plans as mental phenomena  . Judgements that a plan is invalid are associated with particular discrepancies between the beliefs that the observer ascribes to the actor when the former believes that the latter has some plan  , and the beliefs that the observer herself holds . I show that the content of an appropriate response to a query is affected by the types of any such discrepancies of belief judged to be present in the plan inferred to underlie that query  . The PI model described here has been implemented in SPIRIT  , a small demonstration system that answers questions about the domain of computer mail  . 

The importance of plan inference ( PI ) in models of conversation has been widely noted in the computational-linguistics literature  . Incorporating PI capabilities into systems that answer users ' questions has enabled such systems to handle indirect speech acts  \[13\]  , supply more information than is actually requested in a query  \[2\]  , provide helpful information in response to a yes /no query answered in the negative  \[2\]  , disambiguate requests \[17\] , resolve certain forms of intersentential e lipsis  \[6  , 11\] , and handle such discourse phenomenas clarification subdialogues  \[11\]  , and correction or " de bugging ~ subdialogues The research reported in this paper has been made possible in part by an IBM Graduate Fellowship  , in part by a gift from the Systems Development Foundation  , and in part by support from the Defense Advanced Research Projects Agency under Contract  N00039-84-K  . 0078 with the Space and Naval Warfare Command . The views and conclusions contained in this document are those of the author and should not be interpreted as representative of the official policies  , either expressed or implied , of the Defense Advanced Research Projects Agency or the United States Government  . 
! amgrateful to Barbara Grosz , James Allen , Phil Cohen , Amy Lansky , Candy Sidner and Bonnie Webber for their comments on an earlier draft  . 

The PI process in each of these systems , however , has assumed that the agent whose plan is being inferred  ( to whom I shall refer as the actor )  , and the agent drawing the inference ( to whom I shall refer as the observer )  , have identical beliefs about the actions in the domain  . Thus , Allen's model , which was one of the earliest accounts of PI in conversation  1 and impired a great deal of the work done subsequently  , includes , as a typical PI rule , the following : " SBAW ( P ) ~iSBAW ( ACT ) if P is a precondition of ACT"\[2 , page 120\] . 
This rule can be glossed as " if the system ( observer ) believes that an agent ( actor ) wants some proposition P to be true , then the system may draw the inference that the agent wants to perform some action ACT of which P is a precondition  . " Note that it is left unstated precisely who it is -- the observer or the actor --- that believes that P is a precondition of ACT  . 
If we take this to be a belief of the observer , it is not clear that the latter will infer the actor's plan  ; on the other hand , if we consider it to heabelief of the actor , it is unclear how the observer comes to have direct access to it  . In practice , there is only a single set of operators relating preconditions and ac-/tion * in Allen's system  ; the belief in question is regarded as being both the actor's and the observer's  . 
In many situations , an assumption that the re~v~nt beliefs of the actor are identical with those of the observer esults in failure not only of the PI process  , but also of : ~ he communicative process that PI is meant to suppgrt  . -In particular , it precludes the principled generation of appropriate responses to queries that arise from invalid plans  . In this paper , I report on a model of Plin conversation that distinguishes between the beliefs of the actor and those of the observer  . The model rests on an analysis of plans as mental phenomena : ~ having a plans is analyzed as having a particular configuration of k  , c-lids and intentions . Judgements that a plan is invalid are associated with particular discrepancies between the beliefs that the observer ascribes to the actor when the former believes that the latter has some plan  , and the beliefs observer herself holds . I give an account of differentypes of plan in -validities  , and show how this account provides an explanation for certain regularities that are observable in cooperative responses to questions  . The PI model described here has been implemented in SPIRIT  , a small demonstration system that answers questions about the domain of computer mail  . More ' Allen's article Izl summarizes his dissertation r  .   .   .   .   . chIll . 
2 07 extensive discussion of both the PI model and SPIRIT can be found in my dissertation  \[14\]  . 
PLANSASMENT ALPHENOMENA
We can distinguish between two views of plans . AsBratman\[5 , page 271\] has observed , there is an ambiguity in speaking of an agent's plan : " On the one hand  , \[ this\]could mean an appropriate abstract strncture -- some sort of partial function from circumstances to actions  , perhaps . On the other hand , \[ it\]could mean an appropriate state of mind , one naturally describable in terms of such structures ! We might call the former sense the data structure view of plans  , and the latter the mental phenomenon view of plans  . Work in plan synthesis ( e . g . , Fikes and Nilsson \[8\] , Sacerdoti\[15\] , Wilkins\[18\] , and Pednault\[12\]) , has taken the data structure view , considering plans to be structures encoding aggregates of actions that  , when performed in circumstances satisfying some specified preconditions  , achieve some specified results . For the purposes of PI , however , it is much more useful to adoptamental phenomenon view and consider plans to be particular configurations of beliefs and intentions that some agent has  . After all , inferring another agent's plan means figuring out what action she " has in mind  , " and he may well be wrong about the effects of those intended actions  . 
Consider , for example , the plan I have to find out how Kathy is feeling . Believing that Kathy is at the hospital , I plan to do this by finding out the phone number of the hospital  , calling there , asking to be connected to Kathy's room , and finally saying " How are you doing ?" If , unbeknownst tome , Kathy has already been discharged , then executing my plan will not lead to my goal of finding out hows he is feeling  . Forme to have a plan to do fl that consists of doing some collection of actions  I1  , it is not necessary that the performance of II actually lead to the performance of ft  . What is necessary is that I believe that its performance will do so  . This insight is at the core of a view of plans as mental phenomena  ; in this viewaplan " exists "-- i . e . , gains its status as a plan--by virtue of the beliefs  , as well as the intentions , of the person whose plan it is . 
Further consideration of our common sense conceptions of what it means to have a plan leads to the following analysis  \[14  , Chap . 312: ( PO ) An agent G has a plantod of l , that consists in doing some set of acts II , provided that 1 . G believes that he can execute a chact in I1 . 
2 . G believes that executing the acts in I1 will entail the performance of f t . 
3 . G believes that each act in I/plays a role in his plan  . 
( See discussion below . ) 4 . C in tends to execute a chact in I1 . 
5. G in tends to execute II as a way of doing B.
2Although this definition ignores ome important issues of commitment over time  , as discussed by Bratman \[4\] and Cohen and Levesque \[71  , it is sufficiento supporthe PI process needed for many question-answering situations  . This is because , in such situations , unexpected changes in the world that would force are consideration f the actor's intentions can usually be safely ignored  . 
6 . Gintends each act in II to play a role in his plan  . 
The notion of an act playing a role in a plan is defined in terms of two relationships over acts : generation  , in the sense defined by Goldman \[9\] , and enablement . Roughly , one act generates another if , by performing the first , the agent also does the second ; thus , saying to Kathy " How are you doing ?" may generate asking her hows he is feeling  . Or , to take an example from the computer-mail domain , typing DEL . at the prompt for a computer mail system may generate deleting the current message  , which may in turn generate cleaning out one's mail file  . In contrast , one act enables the generation of a second by a third if the first brings about circumstances that are necessary for the generation  . Thus , typing HEADER 15 may enable the generation of deleting the fifteenth message by typing DEL  .   , because it makes message 15 be the current message , to which ' . ' refers , s The difference between generation and enablement consists largely in the fact that  , when an acta generates an act ~ , the agent need only do a , and will automatically be done also . However , when a enables the generation of some "1 by fl , the agent needs to do something more than just a to have done either flor " t  . In this paper , I consider only the inference of a restricted subset of plans  , which I shall call simple plans . An agent has a simple plan if and only if he believes that all the acts in that plan play a role in it by generating another act  ; i . e . , if it includes no acts that he believes are related to one another by enablement  . 
It is important to distinguish between types of actions  ( act-types )  , such as typing DEL . , and actions themselves , uch as my typing DE / .   . right now . Actions or acts -- I will use the two terms interchangeahly -- can be thought of as triples of act  . type , agent , and time . Generation is a relation over actions , not overact-types . Not every case of an agent typing DEL ? will result in the agent deleting the current message  ; for example , my typing it just now did not , because I was not typing it to a computer mail system  . Similarly , executability -- the relation expressed in Clause ( 1 ) of ( P0 ) as " can execute "-- applies to actions , and the objects of an agent's intentions are , in this model , also actions . 
Using the representation language specified in my thesis  \[14\]  , which builds upon Allen's interval-based temporal logic  \[3\]  , the conditions on G's having a simple plan to do fl can be encoded as follows:  ( P1 ) SIMPLE-PLAN ( G , a  ~ , \[ a ~ ,  . . . , a~-i 1 , t2 , tl ) ~( i ) BEL(G , EXEC(ai , G , t2) , tl ) , for i = 1 .   .   .   .   . nA(ii ) BEL(G , GEN(ai , cq+I , G , t2) , tl ) , for i = 1 . . . . , n1 A ( iii ) INT(G,al , t2,tl ), for i = 1 .   .   .   .   . nA(iv ) INT(G , by ( ai , ai+l ) , t2 , tl ) , for i = 1 . . . .  , n1 The left hand side of ( P1 ) denotes that the agent G has , at time t l , a simple plan to do an , consisting of doing these to factsel ,   .   .   . , an-latt2 . Note that all these are simultaneous acts ; this is a consequence of the restriction to simple plans  . 
The right hand side of ( P1 ) corresponds directly to ( PO )  , except that , in keeping with the restriction to simple plans , specific assertions about each act generating anothe replace the SE nablement here thus differs from the usual binary relation in which one action enables another  . Since this paper does not further consider plans with enabling actions  , the advantages of the alternative definition will not be discussed  . 
2 08 more general statement regarding the fact that each act plays a role in the plan  . The relation BEL(G , P , t ) should be taken to mean that agent G believes proposition P throughout time intervalt  ; INT ( G , a , tz , tl ) means that at time tlG intends to do a att 2 . The relation EXEC(a , G , t ) is true if and only if the act of G doing a att is ezecutable  , and the relation GEN(a , / /  , G , t ) is true if and only if the act of G doing a att generates the act of G doing // att  . The function by maps two act-type terms into a third act-type term : if an agent G intends to do by  ( a , //) , then G intends to do the complex act//-by-a , i . e . , he intends to do a in order to do // . Further discussion of these relations and functions can be found in Pollack  \[14  , Chap .  4\] . 
Clause(i ) of ( P1) captures clause (1) of ( P0) . 4 Clause ( iS ) of ( P1 ) captures both clauses ( 2 ) and ( 3 ) of ( P0 ) : when i takes the value n-l , clause ( iS ) of ( P1) captures the requirement , stated in clause (2) of ( P01 , that G believes his acts will entail his goal ; when i takes values between 1 and n2 , it captures the requirement of clause ( 3 ) of ( P0 )  , that G believes each of his acts plays a role in his plan  . Similarly , clause ( iii ) of ( Pl ) captures clause ( 4 ) of ( P0 )  , and clause ( iv ) of ( P1 ) captures clauses ( 5 ) and ( 6 ) of ( PO )  . 
(P1 ) can be used to state what it means for an actor to have an invalid simple plan : G has an invalid simple plan if and only if he has the configuration of beliefs and intentions listed in  ( P1 )  , where one or more of those beliefs is incorrect , and , consequently , one or more of the intentions i unrealizable . The correctness of the actor's beliefs thus determines the validity of his plan : if all the beliefs that are part of his plan are correct  , then all the intentions in it are realizable , and the plan is valid . Validity in this absolute sense , however , is not of primary concern in modeling plan inference in conversation  . 
What is important here is rather the observer's judgement of whether the actor's plan is valid  . It is to the analysis of such invalidity judgements  , and their effect on the question-answering process  , that we now turn . 
PLANINFERENCE IN
QUESTION-ANSWERING
Models of the question-answering process often include a claim that the respondent  ( R ) must infer the plans of the questioner ( Q )  . SoR is the observer , and Qtheactor . Building on the analysis of plans as mental phenomena  , we can say that , if R believes that she has inferred Q's plan , there is some set of beliefs and intentions at is fying  ( P1 ) that R believes Q has ( or is at least likely to have )  . Then there are particular discrepancies that may arise between the beliefs that Rascribes to Q when she believes he has some plan  , and the beliefs that Rherself holds . Specifically , R may nother self believe one or more of the beliefs  , corresponding to Clauses ( i ) and ( iS ) of ( P1 )  , that sheascribes to Q . We can associate such discrepancies with 41n fact , it captures more : to encode Clause ( i ) of ( P0 )  , the pacameter 1 in Clause ( i ) of ( PI ) need only vary between I and n-l . However , given the relationship between EXEC and GEN specified in Pollack\[  t4\]  , namely EXEC(a , G , t)AGEN(a ,  ~ , G , t ) ~ EXEC(~ , G , t ) the instance of Clause ( i ) of ( P1 ) with i = n is a consequence of the instance of Clause  ( i ) with i = n-1 and the instance of Clause ( iS ) with i = n-l . A similar argument can be made about Clause ( iii )  . 
R's judgement that the plans he has inferred is invalid  , s The type of any invalidities , defined in terms of the clauses of ( PI ) that contain the discrepant beliefs , can be shown to influence the content of a cooperative response  . However , they do not fully determine it : the plan inferred to underlie a query  , along with any invalidities it is judged to have , are but two factors affecting the response -generation process  , the most significant others being factors of relevance and salience  . 
I will illustrate the effect of invalidity judgements on response content with a query of the form " I want to perform an act of ~  , so I need to find out how to perform an act of a , " in which the goal is explicit , as in example ( 1 ) below ?: ( I ) "I want to prevent Tom from reading my mail file  . How can I set the permissions on it to faculty -read only ? ~ In questions in which no goal is mentioned explicitly  , analysis depends upon inferring a plan leading to a goal that is reasonable in the domain situation  . Let us assume that , given query (1) , R has inferred that Q has the simple plan that consists only in setting the permissions to faculty-read only  , and thereby directly preventing Tom from reading the file  , i . e . : (2) BEL(R , SIMPLE-PLAN(Q , prevent ( mmfile , read , tom ) , \[set-permissions ( mmfile , read , faeulty )\] , t2 , tl ) , tz ) Later in this paper , I will describe the process by which R can come to have this belief  . Bear in mind that , by ( P1) ,   ( 2 ) can be expanded into a set of beliefs that R has about Q's beliefs and intentions  . 
The first potential discrepancy is that R may believe to be false some belief  , corresponding to Clause ( i ) of ( PI ) , that , by virtue of (2) , sheascribes to Q . In such a case , I will say that she believes that some action in the inferred plan is un-e=~utable  . Examples of responses in which R conveys this information are  ( 3 )   ( in which R believes that at least one intended act is unexecutable  ) and ( 4 )   ( in which R believes that at least two intended acts are unexeeutable  ) :  ( 3 ) " There ianoway for you to set the permissions on a tile t of a culty-read only  . What you can do is move it into a password -protected subdirectory  ; that will prevent Tom from reading it . "  ( 4 ) " There is no way fary out oset the permissions on a file to faculty  . read only , nor is there anyway for you to prevent
Tom from reading it."
ST hleauumee that R always believes that her own beliefs are complete and correct  . Such an usumption is not an unreasonable one for question-answering systems to make  . More general conversational systems must abandon this usumption  , sometimes updating their own beliefs upon detecting a discrepancy  . 
e The analysis below is related to that provided by  2oshi   , Webber , and Weischedel\[10 . There are significant differences in my approach , however , which involve ( i ) a different structural analysis , which applies ane=-scala6111lll to agtions rather than plans and introduces incoherence  ( this latter notion Idellne in the next section )  ; ( ii ) a claim that the types of invtlldlties ( e . g . , formedness , executability of the queried action , and ex-ecuts bility of a goal action ) are independent of one another ; and ( iii ) a claim that recognition of any invalidities , while necessary for determining what information to include in an appropriate response  , is not in itself sufficient for this purpose . Also , Joshietel . do not consider the question of how invalid plans can be inferred  . 

The discrepancy resulting in ( 3 ) is represented in ( 5 )  ; the discrepancy in ( 4 ) is represented in ( 5 ) plus ( 6 ) :  ( 5 ) BEL ( R , BEL(Q , EXEC(set-permissions(mmfile , read , faculty ) , 
Q , tz ), tl ), t ~)

BEL(R , - , EXEC(set-permissions(mmfile , read , faculty ) , 
Q , t2) , t ~) (6) BEL(R , BEL(Q , EXEC(prevent(mmfile , read , tom) , 
Q , t2), tl ), ti)

BEL(R,--EXgC(prevent(ramfile , read , tom),
Q , t2), h)
The second potential discrepancy is that R may believe false some belief corresponding to Clause  ( ii ) of ( P1 ) that , by virtue of (2) , sheascribes to Q . I will then say that she believes the plan to be ill-formed  . In this ease , her response may con ~' ey that the intended acts in the plan will not fit together as expected  , as in (7) , which might be uttered if R believes it to be mutually believed by R and Q that Tom is the system manager :  ( 7 ) " Well , the command is SETPROTECTION ---- ( Fac-ulty : Read )  , but that won't keep Tomout : file permissions don't apply to the system manager  . " The discrepancy resulting in ( 7 ) is ( 8 ) :  ( 8 ) BEL ( R , BEL(Q , GEN(set-permissions(mmfile , read , faculty ) , prevent ( ramfile , read , tom) , 
Q , t2), tl ), h)

BEL(R , - ~ GEN ( set-permissions ( mmfile , read , faculty ) , prevent ( mmfile , read , tom) , 
Q , t2), h)
Alternatively , there may be some combination of these discrepancies between R's own beliefs and those that R attributes to Q  , as reflected in a response such as ( 9 ) :  ( 9 ) " There is no way for you to set the permissions to faculty-read only  ; and even if you could , it wouldn't keep Tomout : tile permissions don't apply to the system manager  . " The discrepancies n coded in ( 5 ) and ( 8 ) together might result in ( 9 )  . 
Of course , it is also possible that no discrepancy exists at all  , in which ease I will say that R believes that Q's plan is valid  . A response such as ( 10 ) can be modeled as arising from an inferred plan that R believes valid :  ( 10 ) " Type SET PROTECTION = ( Faculty : Read )   . " Of the eight possible combinations of formedness  , exe-curability of the queried act and executability of the goal act  , seven are possible : the only logically incompatible combination is a wellformed plan with an executable queried act  , but unexecutable goal act . This range of invalidities accounts for a great deal of the information conveyed in naturally occurring dialogues  . But there is an important regularity that the PI model does not yet explain  . 
A PROBLEM FOR PLAN

In all of the preceding cases , R has intuitively " made sense " of Q's query , by determining some underlying plan whose components he understands  , though shemay also believe that the plan is flawed  . For instance in (7) , R has determined that Q may mistakenly believe that  , when one sets the permissions on a file to allow a particular access to a particular group  , no one who is not a member of that group can gain access to the file  . 
This ( incorrect ) belief explains whyQ believes that setting the permissions will prevent Tom from reading the file  . 
There are also cases in which R may not even be able to " make sense " of Q's query  . As a somewhat whimsical example , imagine Q saying: ( 11 ) ~I want to talk to Kathy , so I need to Fredout how to stand on my head . ~In many contexts , a perfectly reasonable response to this query is ~ Huh ? ~  . Q's query is incoherent : R cannot understand why Q believes that finding out how to stand on his head  ( or standing on his head ) will lead to talking with Kathy . One can , of course , construct scenarios in which Q's query makes perfect sense : Kathy might  , for example , be currently hanging by her feeting ravity boots  . The point here is not to imagine such circumstances in which Q's query would be coherent  , but instead to realize that there are many circumstances in which it would not  . 
The judgement that a query is incoherent is not the same as a judgement that the plan inferred to underlie it is ill-formed  . 
To see this , contrast example ( 11 ) with the following : ( 12 ) alwant to talk to Kathy . Do you know the phone number at the hospital ?" Here  , if R believes that Kathy has already been discharged from the hospital  , she may judge the plans he infers to underlie Q's query to be ill-formed  , and may inform him that calling the hospital will not lead to talking to Kathy  . She can even in form him why the plan is ill -formed  , namely , because Kathy is no longer at the hospital . This differs from (11) , in which R cannot inform Q of the reason his plan is invalid  , because she cannot , on an intuitive level , even determine what his plan is . 
Unfortunately , the model as developed so far does not distinguish between in coherence and ill -formedness  . The reason is that , given a reasonable account of semantic interpretation  , it is transparent from the query in ( 11 ) that Q intends to talk to Kathy , intends to find out how to stand on his head , and intends his doing the latter to play a role in his plan to do the former and that he also believes that he can talk to Kathy  , believes that he can find out how to stand on his head  , and believes that his doing the latter will play a role in his precisely what are required to have a plan according to  ( P0 )  . 
Consequently , after hearing (11) , R can , in fact , infer a plan underlying Q's query , namely the obvious one : to find out how to stand on his head  ( or to stand on his head ) in order to talk to Kathy . Then , since R does not herself believe that the former act will lead to the latter  , on the analysis so far given , we would regard R as judging Q's plan to be ill -formed  . But this is not the desired analysis : the model should instead capture the fact that R cannot make sense of Q's query here--that it is incoherent  . 
Let us return to the set of examples about setting the permissions on a file  , discussed in the previou section . Inher semantic interpretation of the query in ( 1 )  , R may come to have a number of beliefs about Q's beliefs and intentions  . Specifically , all of the following may betr ~ e : (13) BEL(R , BgL(Q , gXEC(set-permissions(mmfile , read , faculty ) , q , tz ) , tl ) , t ~) (14) BEL(R , BEL(Q , gXEC(prevent(mmfile , read , tom) , 
Q , t2) , tl ) , t ~) (15) BEL(R , BEL(Q , GEN ( set-permissions ( mmfile , read , faculty ) , prevent ( mmfile , read , tom) , 
Q , tz ) , tl ) , t ~) (16) BEL(R , INT ( Q , set-permissions ( mmfile , read , faculty ) , t2 , ~ l ) , t t ) (17) BEL(R , INT(Q , prevent ( mmfile , read , tom ) , t2 , tl ) , t ~) (18) BEL(R , IiT(Q , by(set-permissions(mmfile , read , faculty ) , prevent ( mmfile , read , tom )) , t2 , tl ) , tl ) Together ,   ( 13 ) - ( 18 ) are sumcient for R's believing that Q has the simple plan as expressed in  ( 2 )  . This much is not surprising . In effect Q has stated in his query what his plan is -- to prevent Tom from reading the file by setting the permission on it to faculty-read only -- so  , of course , R should be able to infer just that . And if R further believes that the system manager can override file permissions and that Tom is the system manager  , but also that Q does not know the former fact , R will judge that Q's plan is ill-formed , and may provide a response such as that in (7) . There is a discrepancy here between the belief R ascribes to Qinsatisfaction of Clause  ( ii ) of ( Pl ) -- namely , that expressed in ( 15 ) -- and R ' sown beliefs about the domain . 
But what if R , instead of believing that it is mutually believed by Q and R that Tom is the system manager  , believes that they mutually believe that he is a faculty member ? In this case  , (13)-(18) may still be true . However we do not want to say that this case is indistinguishable from the previous one  . 
7 Actually , the requirement that Q have these beliefs may be slightly too strong  ; see Pollack\[14 , Chap . 3\] for discussion . 
In the previous case , Runderstood the source of Q's erroneous belief : she realized that Q did not know that the system manager could override file protections  , and therefore though that , by setting permissions to restrict access to a group that Tom is not a member of  , he could prevent Tom from reading the file . 
In contrast , in the current ease , R cannot really understand Q's plan : she cannot determine whyQ believes that he will prevent Tom from reading the file by setting the permissions on it to faculty-read only  , given that Q believes that Tom is a faculty member  . This current case is like the case in ( 11 ) : Q's query is incoherent to R . 
To capture the difference between i il-formedness and inco-herence  , I will claim that , when an agent R is asked a question by a nactor Q  , R needs to attemp to a scribe to Q more than just a set of beliefs and intention satisfying  ( Pl )  . Specifically , for each belief satisfying Clause ( ii ) of ( Pl )  , R must also ascribe to Q another belief that explains the former in a certain specifiable way  . The beliefs that satisfy Clause ( ii ) are beliefs about the relation between two particular actions : for instance  , the plan underlying query ( 12 ) includes Q's belief that his action of calling the hospital attz will generate his action of establishing a communication channel to Kathy at  t2  . This belief can be explained by a belief Q has about the relation between the act-types ~ calling a location " and ~ establishing a communication channel to an agent  . " Q may believe that sets of the former type generate acts of the latter type provided that the agent to whom the communication channel is to be established is at the location to be called  . Such a belief can be encoded using the predicate CGEN  , which can be read " conditionally generates , " as follows : (19) BEL(Q , CGEN(call(X ) , establish-channel(Y) , at(X , Y )) , t l The relation CGEN(a , B , C ) is true if and only if acts of type a performed when condition Cholds will generate acts of type  #  . 
Thus , the sentence CGEN(a , B , C ) can be seen as one possible interpretation of a hieran=hical panning operator with header B  , preconditions C , and body a . Conditional generation is a relation between two act-types and a set of conditions  ; generation , which is a relation between two actions , can be defined in terms of conditional generation . 
In reasoning about (12) , R can attribute to Q the belief expressed in (19) , combined with a belief that Kathy will be at the hospital at time  t2  . Together , these beliefs explain Q's belief that , by calling the hospital att 2 , he will establish a com-mt mieation channel to Kathy  . Similarly , in reasoning about query ( 1 ) in the case in which R does not believe that Q knows that Tom is a faculty member  , R can ascribe to Q the beliefs that , by setting the permissions on a file to restrict access to a partieulacgroup  , one denies access to everyone who is neithers member of that group nor the system manager  , as expressed in (20): (20) BEL(R , BEL(Q , CGEN(set-permissions(X , P , Y ) , prevent(X , P , Z ) ,  - , member(g , Y )) , h ) , tt ) She can also ascribe to Q the belief that Tom is not a member of the faculty  , ( or more precisely , that Tom will not be a member of the faculty at the intended performance time tz  )  , i . e . , (21)BEL(R , BEL(Q , HOLDS (- ~ member(tom , facuity) , t2) , tl) , tl The conjunction of these two beliefs explains Q's further belief  , expressed in (15) , that , by setting the permissions to faculty-read only at  t2  , he can prevent Tom from reading the file . 
In contrast , in example (11) , R has no basis for a scribing to Q beliefs that will explain why he thinks that standing on his head will lead to talking with Kathy  . And , in the version of example ( 1 ) in which R believes that Q believes that Tom is a faculty member  , R has no basis for a scribing to Qa belief that explains Q's belief that setting the permissions t of a culty-read only will prevent Tom from reading the file  . 
Explanatory beliefs are incorporated in the PI model by the introduction of ezplanatory plans  , ore plans . Saying that an agent R believes that another agent Q has some eplan is shorthand for describing a set of beliefs possessed by R  , specifically : ( P2) ( R , EPLAN(Q , ~n , \[ al .   .   .   .   . an-l\],\[pl .   .   .   .   . Pn-l\] , t2 , tl ) , tl)(i)BEL(R , BEL(Q , EXEC(cq , Q , t2) , tl) , tl ) , for i = 1 ,  . . . ,n A ( ii ) BEL(R , BEL(Q , GEN(~ , ai+t , Q , t2) , tt) , tl ) , for i = 1 ,  . . . ,n-I A ( iii ) BEL(R , INT(Q , ~ I , tz , tl ) , tl ) , for i = 1 ,  . . .   , nA(iv ) BEL(R , INT(Q , by ~ al , ai+l ) , t2 , tl ) , tl ) , for i = 1 ,  . . .   , n1A(v)BEL(R , BEL(Q , pi , tl ) , tl ) , where each Pi is
CGEN(ai , cq+l,Ci)AHOLDS(Ci,t2)
I claim that the PI process underlying cooperative question-answering can be modeled as an attempto infer an eplan  , i . e . , to form a set of beliefs about the questioner's beliefs and intentions that satisfies  ( P2 )  . Thus the next question to ask is : how can R come to have such a set of beliefs ? 
THE INFERENCE PROCESS
In the complete PI model , the inference of an eplan is a twostage process . First , Rinfers beliefs and intentions that Q plausibly has  . Then when she has found some set of theme that is large enough to account for Q's query  , their epistemie status can be upgraded , from beliefs and intentions that R believes Q plausibly has  , to beliefs and intentions that R will , for the purposes of forming her response , consider Q actually to have . Within this paper , however , I will blur the distinction between attitudes that R believes Q plausibly has and attitudes that R believes Q indeed has  ; in consequence I will also omit discussion of the second stage of the PI process  . 
A set of plan inference rules encodes the principles by which an inferring agent R can reason from some set of beliefs and intentions - -call this the antecedenteplan--that she thinks Q has  , to some further set of beliefs and intentions - -call this the consequent eplan--that she also thinks he has  . The beliefs and intentions that the antecedent eplan comprises are a proper subset of those that the consequente plan comprises  . To reason from antecedente plan to consequent eplan  , R must attribute some explanatory belief to Q on the basis of something other than just Q's query  . In more detail , if part of R's belief that Q has the antecedent eplan is a belief that Q intends to do some act a  , and R has reason to believe that Q believes that act-type a conditionally generates act-type  3' under condition C , then R can infer that Q in tends to do a in order to do % believing as well that C will hold at performance time  . R can also reason in the other direction : if part of her belief that Q has some plausible eplan is a belief that Q intends to do some act a and R has reason to believe that Q believes that act-type conditionally generates act-type a under condition C  , then R can infer that Q intends to do " ~ in order to do a  , believing that C will holds t performance time . 
The plan inference rules encode the pattern of reasoning expressed in the last two sentences  . Different plan inference rules encode the different bases upon which R may decide that Q may believe that a conditional generation relation holds between some a  , an act of which is intended as part of the antecedente plan  , and some % This a scription of beliefs , as well as the ascription of intentions , is a nonmonotonic process . For arbitrary proposition P , R will only decide that Q may believe that P if R has no reason to believe Q believes that-~P  . 
In the most straightforward case , R will a scribe to Q a belief abouts conditional generation relation that she herself believes true  . This reasoning can be encoded in the representation language in rule  ( PI1 ) :  ( PII ) BEL ( R , EPLAN(Q , an , \[ al .   .   .   .   . an-a \],\[ pl .   .   .   .   . On-t\], t2, h ), h)

BEL(R , CGEN(an , %C ), q)
BEL(R , EPLAN(Q , %\[al .   .   .   .   . a , \],\[ pl .   .   .   .   . p,\], t2, tl ) where p,~ . CGEN(ar , ,"I , C ) ^ HOLDS(C , t2) This rule says that , if R's belief that Q has so meep lan includes a belief that Q intends to do an act an  , and R also believes that act-type a ~ conditionally generates some " ~ under condition C  , then R can ( nonmonotonically ) infer that Q has the additional intention of doing a  , in order to do ~-- i . e . , that he intends to do by ( an , "~) . Q's having this intention depends upon his also having the supporting belief that an conditionally generates ~' under some condition C  , and the further belief that this C will hold at performance time  . A rule symmetric to ( PI1 ) is also needed since R cannot only reason about what acts might be generated by an act that she already believes Q in tends  , but also about what acts might generate such an act  . 
Consider R's use of ( PI1 ) in attempting to infer the plan underlying query ( 1 ) ) R herself has a particular belief about the relation between the act-types " setting the permissions on ? file " and " preventing someone access to the file  , " a belief we can encode as follows : (22) BELR , CGEN(met-permissions(X , P , Y ) , prevent(X , P , Z ) , - ~ member(Z , Y ) A--system-mgr(Z )) , q ) From query (1 , R can directly attribute to Q two triviale plans : sI have simplified somewhat in the following account for presentational purposes  . A step-by-step account of this inference process is given in 
Poll ~ ck\[14, Chap . 6\].
212 (23) BEL(R , EPbAN(Q , set-permissions ( mmfile , read , faculty ) ,  \[ \] , t2 , t , ) , tl ) (24) BEL(R , EPLAN(Q , prevent(mmfile , read , tom ) , \[ \] , t2tl ) , tl ) The belief in ( 23 ) is justified by the fact that ( 13 ) satisfies Clause ( i ) of ( P2 )  , (16) satisfies Clause ( iv ) of ( P2) , and Clauses ( ii ) , ( iii ) , and ( v ) are vacuously satisfied . An analogous argument applies to (24) . 
Now , if R applies ( PII ) , she will attribute to Q exactly the same belief as she herself has  , as expressed in (22) , along with a belief that the condition C specified there will hold at  t2  . 
That is , as part of her belief that a particular eplan underlies  ( 1 )  , R will have the following belief : (25) BEL(R , BEL(Q , CGEN ( set-permissions(X , P , Y ) , prevent(X , P , Z ) ,  - , member ( Z , Y ) A-~system-mrg(Z ))


A--system-mgr(tom ), tz ), tl ), q)
The belief that R attributes to Q , as expressed in (25) , is an explanatory belief supporting (15) . Note that it is not the same explanatory belief that was expressed in  ( 20 ) and ( 21 )  . In (25) , the discrepancy between R's beliefs and R's beliefs about Q's beliefs is about whether Tom is the system manager  . This discrepancy may result in a response like ( 26 )  , which conveys different information than does ( 7 about the source of the judged ill-formedness . 
(26) " Well , the command is SETPROTECTION = ( Fac-ulty : Read )  , but that wo n't keep Tom out : he's the system manager  . "  ( PI1 )   ( and its symmetric partner ) are not sufficient o model the inference of the eplant hat results in  ( 7 )  . This is because , in using ( PI1) , R is restricted to a scribing to Q the same beliefs about the relation between domain act -types as she herself has  . ~ Theeplant hat results in ( 7 ) includes a belief that R attributes to Q involving a relation between act-types that R believes false  , specifically , the CGEN relation in (20) . What is needed to derive this is a rule such as ( PI2 ) :  ( PI2 ) BEL ( R , EPLAN(Q , on , \[ al .   .   .   .   . an-l\],\[pl .   .   .   .   . Pn-l\],t2,t ,), q)

BEL(R , CGEN(an , 7, C~A .   .   . ACm ), tl)--4BEL(R,EPLAN(Q , 7,\[al , .   .   . , a \],\[ pl .   .   .   .   . p,\], tz,q ) where p ,= CGEN(an , % CIA . .  . ACi-IACi+IA . .  . ACm ) AHOLDS(C,A .   .   . ACi1ACi+lh .   .   . ACm , t2) ~ Hence , existing PI systems that equate R's and Q's beliefs about actions could  , in principle , have handled example such as (26) which r , : ~: Sreonly the use of ( PI1) , although they have not done so . Further , whi \] ~ they could have handled the particular type of invalidity that can be inferred using  ( PII )  , without an analysis of the general problem of invalid plans and their effects on cooperative responses  , these systems would need to treat this as a special case in which a variant response is required  . 
What ( PI2 ) expresses i that R may ascribe to Q a belief about a relation between act-types that is a slight variation of ones he herself has  . What ( PI2) asserts is that , if there is some CGEN relation that R believes true  , she may attribute to Qa belief in a similar CGEN relation that is stronger  , in that it is missing one of the required conditions  . If R uses ( PI2 ) in attempting to infer the plan that underlies query  ( 1 )  , she may decide that Q's belief about the conditions under which setting the permissions on a file prevent someone from accessing the file do not include the person's not being the system manager  . 
This can result in R attributing to Q the explanatory belief in  ( 20 ) and ( 21 )  , which , in turn , may result in a response such as that in (7) . 
Of course , both the kind of discrepancy that may be introduced by  ( PI1 ) and the kind that is always introduced by ( PI2 ) may be present simultaneously , resulting in a response like (27): (27) " Well , the command is SETPROTECTION = ( Fac-ulty : Read )  , but that wo n't keep Tom out : he's the system manager  , and file permissions don't apply to the system manager  . "  ( PI2 ) represents just one kind of variation of her own beliefs that R may consider attributing to Q  . Additional PI rules encode other variations and can also be used to encode any typical misconceptions that R may attribute to Q  . 

The inference process described in this paper has been implemented in SPIRIT  , a System for Plan Inference that Reasons about Invalidities Too  . SPIRIT infers and evaluates the plans underlying questions asked by users about the domain of computer mail  . It also uses the result of its inference and evaluation to generate simulated cooperative responses  . SPIRIT is implemented in C-Prolog , and has run on several different machines , in eluding a Sun Workstation , a Vax 11-750 , and a DEC-20 . SPIRIT is a demonstration system , implemented to demonstrate the PI model developed in this work  ; consequently only a few key examples , which are sufficient o demonstrate SPIRIT's capabilities  , have been implemented . 
Of course , SPIRIT's knowledge base could be expanded in a straightforward manner  . SPIRIT has no mechanisms for computing relevance or salience and  , consequently , always produces as complete an answer as possible . 

In this paper I demonstrated that modeling cooperative conversation  , in particular cooperative question-answcring , requires a model of plan inference that distinguishes between the beliefs of actors and those of observers  . I reported on such a model , which rests on an analysis of plans as mental phenomena  . Under this analysis there can be discrepancies between an agent's own beliefs and the beliefs that sheascribes to an actor when she think she has some plan  . Such discrepancies were associated with the observer's judgement that the actor's plan is invalid  . Then the types of any invalidities judged to be present in a plan inferred to underlie a query were shown to affect the content of a cooperative response  . 1 further suggested that , to to ascribe to the questioner more than just a set of beliefs and intentions sufficient to believe that he has some plan : she must also attempt to a scribe to him beliefs that explain those beliefs and intentions  . The eplan construct was introduced to capture this requirement  . Finally , I described the process of inferring eplans--that is  , of ascribing to another agent beliefs and intentions that explain his query and can influence a response to it  . 
REFERENCES\[1\]JamesF . Allen . A Plan Based Approach to Speech Act Recognition . Technical Report TR 121/79, University of
Toronto , 1979.
\[2\] James F . Allen . Recognizing intentions from natural language utterances  . In Michael Brady and Robert C . 
Berwlck , editors , Computational Models of Discourse , pages 107-166 , MIT Press , Cambridge , Mass . , 1983 . 
\[3\] James F . Allen . Towards a general theory of action and time . Artificial Intelligence , 23(2):123-154, 1984 . 
\[4\] Michael Bratman . Intention , Plans and Practical Reason . 
Harvard University Press , Cambridge , Ma . , forthcoming . 
\[5\] Michael Bratman . Taking plans seriously . Social Theory and Practice , 9:271-287, 1983 . 
\[6\] M . Sandra Carberry . Pragmatic Modeling in Information System Interfaces  . PhD thesis , University of Delaware , 1985 . 
\[7\] Philip R . Cohen and Hector J . Levesque . Speech acts and rationality . In Proceedings of the e3rd Conference of the Association for Computational Linguistics  , pages 49-59 , 
Stanford , Ca ., 1985.
\[8\]R . E . Fikes and Nils J . Nilsson . Strips : a new approach to the application of theorem proving to problem solving  . 
Artificial Intelligence , 2:189-208, 1971.
\[9\] Alvin I . Goldman . A Theory of Human Action . Prentice-
Hail , Englewood Cliffs , N.J ., 1970.
\[10\]A ravin dK . Joshi , Bonnie Webber , and Ralph Weischedel . 
Living up to expectations : computing expert responses  . 
In Proceedings of the Fourth National Conference on Artificial Intelligence  , pages 169-175 , Austin , T x . , 1984 . 
\[11\]Diane Litman . Plan Recognition and Discourse Analysis : An Integrated Approach for Understanding Dialogues  . 
PhD thesis , University of Rochester , 1985.
\[12\]Ed w in P . D . Pednault . Preliminary Report on a Theory of Plan Synthesis . Technical Report 358, SRI International , 1985 . 
\[13\]C . Raymond Perranlt and James F . Allen . A plan-based analysis of indirect speech acts . American Journal of Computational Linguistics ,  6:167-182 ,  1980 . 
\[14\] Martha E . Pollack . Inferring Domain Plans in @ ~ estion-Answering . PhD thesis , University of Pennsylvania , 1986 . 
\[15\]Earl D . Sacerdoti . A Structure for Plans and Behavior . 
American Elsevier , New York , 1977.
\[16\]C and aee L . Sidner . Plan parsing for intended response recognition in discourse  . Computational Intelligence,
I(I ), 1985.
\[17\]C and a ceL . Sidner . What the speaker means : the recognition of speakers ' plans in discourse  . International Journal of Computers and Mathematics  ,  9:71-82 ,  1983 . 
\[18\]David E . Wilkins . Domain-independent planning : representation and plan generation  . Artificial Intelligence , 22:269--301, 984 . 

