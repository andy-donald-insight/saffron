Dual Decomposition
for Natural Language Processing
Alexander M . Rush and Michael Collins
Decoding complexity
focus : decoding problem for natural language tasks
y
?
= arg max
y
f ( y)
motivation:
? richer model structure often leads to improved accuracy ? exact decoding for complex models tends to be intractable
Decoding tasks many common problems are intractable to decode exactly high complexity ? combined parsing and part-of-speech tagging ( Rush et al , 2010) ? ? loopy ? HMM part-of-speech tagging ? syntactic machine translation ( Rush and Collins , 2011)
NP-Hard ? symmetric HMM alignment ( DeNero and Macherey , 2011) ? phrasebased translation ? higher-order nonprojective dependency parsing ( Koo et al , 2010) in practice : ? approximate decoding methods ( coarse-to-fine , beam search , cube pruning , gibbs sampling , belief propagation ) ? approximate models ( mean field , variational models)
Motivation cannot hope to find exact algorithms ( particularly when NP-Hard ) aim : develop decoding algorithms with formal guarantees method : ? derive fast algorithms that provide certificates of optimality ? show that for practical instances , these algorithms often yield exact solutions ? provide strategies for improving solutions or finding approximate solutions when no certificate is found dual decomposition helps us develop algorithms of this form Dual Decomposition ( Komodakis et al , 2010; Lemare?chal , 2001) goal : solve complicated optimization problem y ? = arg max y f ( y ) method : decompose into subproblems , solve iteratively benefit : can choose decomposition to provide ? easy ? subproblems aim for simple and efficient combinatorial algorithms ? dynamic programming ? minimum spanning tree ? shortest path ? mincut ? bipartite match ? etc.
Related work there are related methods used NLP with similar motivation related methods : ? belief propagation ( particularly max-product ) ( Smith and
Eisner , 2008) ? factored A * search ( Klein and Manning , 2003) ? exact coarse-to-fine ( Raphael , 2001) aim to find exact solutions without exploring the full search space
Tutorial outline focus : ? developing dual decomposition algorithms for new NLP tasks ? understanding formal guarantees of the algorithms ? extensions to improve exactness and select solutions outline : 1. worked algorithm for combined parsing and tagging 2. important theorems and formal derivation 3. more examples from parsing , sequence labeling , MT 4. practical considerations for implementing dual decomposition 5. relationship to linear programming relaxations 6. further variations and advanced examples 1. Worked example aim : walk through a dual decomposition algorithm for combined parsing and part-of-speech tagging ? introduce formal notation for parsing and tagging ? give assumptions necessary for decoding ? step through a run of the dual decomposition algorithm Combined parsing and part-of-speech tagging
S
NP
N
United
VP
V flies
NP
D some
A large
N jet goal : find parse tree that optimizes score(S ? NP VP ) + score(VP ? V NP ) + ...+ score(United1,N ) + score(V,N ) + ...
Constituency parsing notation : ? Y is set of constituency parses for input ? y ? Y is a valid parse ? f ( y ) scores a parse tree goal : arg max y?Y f ( y ) example : a contextfree grammar for constituency parsing
S
NP
N
United
VP
V flies
NP
D some
A large
N jet
Part-of-speech tagging notation : ? Z is set of tag sequences for input ? z ? Z is a valid tag sequence ? g(z ) scores of a tag sequence goal : arg max z?Z g(z ) example : an HMM for partof speech tagging
United1 flies2 some3 large4 jet5
N V D A N
Identifying tags notation : identify the tag labels selected by each model ? y(i , t ) = 1 when parse y selects tag t at position i ? z(i , t ) = 1 when tag sequence z selects tag t at position i example : a parse and tagging with y(4,A ) = 1 and z(4,A ) = 1
S
NP
N
United
VP
V flies
NP
D some
A large
N jet y
United1 flies2 some3 large4 jet5
N V D A N z
Combined optimization goal : arg max y?Y,z?Z f ( y ) + g(z ) such that for all i = 1 . . . n , t ? T , y(i , t ) = z(i , t ) i.e . find the best parse and tagging pair that agree on tag labels equivalent formulation : arg max y?Y f ( y ) + g(l(y )) where l : Y ? Z extracts the tag sequence from a parse tree
Dynamic programming intersection can solve by solving the product of the two models example : ? parsing model is a contextfree grammar ? tagging model is a first-order HMM ? can solve as CFG and finite-state automata intersection replace S ? NP VP with
SN,N ? NPN,V VPV , N
S
NP
N
United
VP
V flies
NP
D some
A large
N jet
Parsing assumption the structure of Y is open ( could be CFG , TAG , etc .) assumption : optimization with u can be solved efficiently arg max y?Y f ( y ) + ? i , t u(i , t)y(i , t ) generally benign since u can be incorporated into the structure of f example : CFG with rule scoring function h f ( y ) = ?
X?Y Z?y h(X ? Y Z ) + ? ( i , X )? y h(X ? wi ) where arg maxy?Y f ( y ) + ? i , t u(i , t)y(i , t ) = arg maxy?Y ?
X?Y Z?y h(X ? Y Z ) + ? ( i , X )? y ( h(X ? wi ) + u(i , X ))
Tagging assumption we make a similar assumption for the set Z assumption : optimization with u can be solved efficiently arg max z?Z g(z )? ? i , t u(i , t)z(i , t ) example : HMM with scores for transitions T and observations O g(z ) = ? t?t??z
T ( t ? t ?) + ? ( i , t)?z
O(t ? wi ) where arg maxz?Z g(z )? ? i , t u(i , t)z(i , t ) = arg maxz?Z ? t?t??z
T ( t ? t ?) + ? ( i , t)?z ( O(t ? wi )? u(i , t))
Dual decomposition algorithm
Set u (1) ( i , t ) = 0 for all i , t ? T
For k = 1 to K y ( k ) ? arg max y?Y f ( y ) + ? i , t u ( k ) ( i , t)y(i , t ) [ Parsing ] z ( k ) ? arg max z?Z g(z )? ? i , t u ( k ) ( i , t)z(i , t ) [ Tagging ] If y ( k)(i , t ) = z(k)(i , t ) for all i , t Return ( y ( k ), z(k))
Else u(k+1)(i , t )? u(k)(i , t )? ? k(y ( k ) ( i , t )? z(k)(i , t))
Algorithm step-by-step [ Animation]
Main theorem theorem : if at any iteration , for all i , t ? T y ( k ) ( i , t ) = z(k)(i , t ) then ( y ( k ), z(k )) is the global optimum proof : focus of the next section 2. Formal properties aim : formal derivation of the algorithm given in the previous section ? derive Lagrangian dual ? prove three properties
I upper bound
I convergence
I optimality ? describe subgradient method
Lagrangian goal : arg max y?Y,z?Z f ( y ) + g(z ) such that y(i , t ) = z(i , t)
Lagrangian:
L(u , y , z ) = f ( y ) + g(z ) + ? i , t u(i , t ) ( y(i , t )? z(i , t )) redistribute terms
L(u , y , z ) = ? ? f ( y ) + ? i , t u(i , t)y(i , t ) ? ?+ ? ? g(z )? ? i , t u(i , t)z(i , t ) ? ?
Lagrangian dual
Lagrangian:
L(u , y , z ) = ? ? f ( y ) + ? i , t u(i , t)y(i , t ) ? ?+ ? ? g(z )? ? i , t u(i , t)z(i , t ) ? ?
Lagrangian dual:
L(u ) = max y?Y,z?Z
L(u , y , z ) = max y?Y ? ? f ( y ) + ? i , t u(i , t)y(i , t ) ? ?+ max z?Z ? ? g(z )? ? i , t u(i , t)z(i , t ) ? ?
Theorem 1. Upper bound define : ? y ?, z ? is the optimal combined parsing and tagging solution with y ? ( i , t ) = z?(i , t ) for all i , t theorem : for any value of u
L(u ) ? f ( y ?) + g(z?)
L(u ) provides an upper bound on the score of the optimal solution note : upper bound may be useful as input to branch and bound or
A * search
Theorem 1. Upper bound ( proof ) theorem : for any value of u , L(u ) ? f ( y ?) + g(z ?) proof:
L(u ) = max y?Y,z?Z
L(u , y , z ) (1) ? max y?Y,z?Z:y=z
L(u , y , z ) (2) = max y?Y,z?Z:y=z f ( y ) + g(z ) (3) = f ( y ? ) + g(z ? ) (4)
Formal algorithm ( reminder)
Set u (1) ( i , t ) = 0 for all i , t ? T
For k = 1 to K y ( k ) ? arg max y?Y f ( y ) + ? i , t u ( k ) ( i , t)y(i , t ) [ Parsing ] z ( k ) ? arg max z?Z g(z )? ? i , t u ( k ) ( i , t)z(i , t ) [ Tagging ] If y ( k)(i , t ) = z(k)(i , t ) for all i , t Return ( y ( k ), z(k))
Else u(k+1)(i , t )? u(k)(i , t )? ? k(y ( k ) ( i , t )? z(k)(i , t))
Theorem 2. Convergence notation : ? u ( k+1) ( i , t )? u(k)(i , t ) + ? k(y ( k ) ( i , t )? z(k)(i , t )) is update ? u ( k ) is the penalty vector at iteration k ? ? k is the update rate at iteration k theorem : for any sequence ?1, ?2, ?3, . . . such that lim t ?? ? t = 0 and ?? t=1 ? t =?, we have lim t??
L(u t ) = min u
L(u ) i.e . the algorithm converges to the tightest possible upper bound proof : by subgradient convergence ( next section)
Dual solutions define : ? for any value of u yu = arg max y?Y ? ? f ( y ) + ? i , t u(i , t)y(i , t ) ? ? and zu = arg max z?Z ? ? g(z )? ? i , t u(i , t)z(i , t ) ? ? ? yu and zu are the dual solutions for a given u
Theorem 3. Optimality theorem : if there exists u such that yu(i , t ) = zu(i , t ) for all i , t then f ( yu ) + g(zu ) = f ( y ? ) + g(z ? ) i.e . if the dual solutions agree , we have an optimal solution ( yu , zu)
Theorem 3. Optimality ( proof ) theorem : if u such that yu(i , t ) = zu(i , t ) for all i , t then f ( yu ) + g(zu ) = f ( y ? ) + g(z ? ) proof : by the definitions of yu and zu
L(u ) = f ( yu ) + g(zu ) + ? i , t u(i , t)(yu(i , t )? zu(i , t )) = f ( yu ) + g(zu ) since L(u ) ? f ( y ?) + g(z ?) for all values of u f ( yu ) + g(zu ) ? f ( y ? ) + g(z ? ) but y ? and z ? are optimal f ( yu ) + g(zu ) ? f ( y ? ) + g(z ? )
Dual optimization
Lagrangian dual:
L(u ) = max y?Y,z?Z
L(u , y , z ) = max y?Y ? ? f ( y ) + ? i , t u(i , t)y(i , t ) ? ?+ max z?Z ? ? g(z )? ? i , t u(i , t)z(i , t ) ? ? goal : dual problem is to find the tightest upper bound min u
L(u)
Dual subgradient
L(u ) = max y?Y ? ? f ( y ) + ? i,t u(i , t)y(i , t ) ? ?+ max z?Z ? ? g(z )? ? i,t u(i , t)z(i , t ) ? ? properties : ? L(u ) is convex in u ( no local minima ) ? L(u ) is not differentiable ( because of max operator ) handle non-differentiability by using subgradient descent define : a subgradient of L(u ) at u is a vector gu such that for all v
L(v ) ? L(u ) + gu ? ( v ? u)
Subgradient algorithm
L(u ) = max y?Y ? ? f ( y ) + ? i,t u(i , t)y(i , t ) ? ?+ max z?Z ? ? g(z )? ? i,j u(i , t)z(i , t ) ? ? recall , yu and zu are the argmax?s of the two terms subgradient : gu(i , t ) = yu(i , t )? zu(i , t ) subgradient descent : move along the subgradient u ? ( i , t ) = u(i , t )? ? ( yu(i , t )? zu(i , t )) guaranteed to find a minimum with conditions given earlier for ? 3. More examples aim : demonstrate similar algorithms that can be applied to other decoding applications ? contextfree parsing combined with dependency parsing ? corpus-level part-of-speech tagging ? combined translation alignment Combined constituency and dependency parsing setup : assume separate models trained for constituency and dependency parsing problem : find constituency parse that maximizes the sum of the two models example : ? combine lexicalized CFG with second-order dependency parser
Lexicalized constituency parsing notation : ? Y is set of lexicalized constituency parses for input ? y ? Y is a valid parse ? f ( y ) scores a parse tree goal : arg max y?Y f ( y ) example : a lexicalized contextfree grammar
S(flies)
NP(United)
N
United
VP(flies)
V flies
NP(jet)
D some
A large
N jet
Dependency parsing define : ? Z is set of dependency parses for input ? z ? Z is a valid dependency parse ? g(z ) scores a dependency parse example : *0 United1 flies2 some3 large4 jet5
Identifying dependencies notation : identify the dependencies selected by each model ? y(i , j ) = 1 when constituency parse y selects word i as a modifier of word j ? z(i , j ) = 1 when dependency parse z selects word i as a modifier of word j example : a constituency and dependency parse with y(3, 5) = 1 and z(3, 5) = 1
S(flies)
NP(United)
N
United
VP(flies)
V flies
NP(jet)
D some
A large
N jet y *0 United1 flies2 some3 large4 jet5 z
Combined optimization goal : arg max y?Y,z?Z f ( y ) + g(z ) such that for all i = 1 . . . n , j = 0 . . . n , y(i , j ) = z(i , j)
Algorithm step-by-step [ Animation]
Corpus-level tagging setup : given a corpus of sentences and a trained sentence-level tagging model problem : find best tagging for each sentence , while at the same time enforcing intersentence soft constraints example : ? test-time decoding with a trigram tagger ? constraint that each word type prefer a single POS tag
Corpus-level tagging full model for corpus-level tagging
He saw an American man
The smart man stood outside
Man is the best measure
N
Sentence-level decoding notation : ? Yi is set of tag sequences for input sentence i ? Y = Y1? . . .? Ym is set of tag sequences for the input corpus ? Y ? Y is a valid tag sequence for the corpus ? F ( Y ) = ? i f ( Yi ) is the score for tagging the whole corpus goal : arg max
Y?Y
F ( Y ) example : decode each sentence with a trigram tagger
He
P saw
V an
D
American
A man
N
The
D smart
A man
N stood
V outside
R
Inter-sentence constraints notation : ? Z is set of possible assignments of tags to word types ? z ? Z is a valid tag assignment ? g(z ) is a scoring function for assignments to word types ( e.g . a hard constraint - all word types only have one tag ) example : an MRF model that encourages words of the same type to choose the same tag z1 man
N man
N man
N
N z2 man
N man
N man
A
N g(z1) > g(z2)
Identifying word tags notation : identify the tag labels selected by each model ? Ys(i , t ) = 1 when the tagger for sentence s at position i selects tag t ? z(s , i , t ) = 1 when the constraint assigns at sentence s position i the tag t example : a parse and tagging with Y1(5,N ) = 1 and z(1, 5,N ) = 1
He saw an American man
The smart man stood outside
Y man man man z
Combined optimization goal : arg max
Y?Y,z?Z
F ( Y ) + g(z ) such that for all s = 1 . . . m , i = 1 . . . n , t ? T ,
Ys(i , t ) = z(s , i , t)
Algorithm step-by-step [ Animation]
Combined alignment ( DeNero and Macherey , 2011) setup : assume separate models trained for English-to-French and
French-to-English alignment problem : find an alignment that maximizes the score of both models with soft agreement example : ? HMM models for both directional alignments ( assume correct alignment is one-to-one for simplicity)
English-to-French alignment define : ? Y is set of all possible English-to-French alignments ? y ? Y is a valid alignment ? f ( y ) scores of the alignment example : HMM alignment
The1 ugly2 dog3 has4 red5 fur6 1 3 2 4 6 5
French-to-English alignment define : ? Z is set of all possible French-to-English alignments ? z ? Z is a valid alignment ? g(z ) scores of an alignment example : HMM alignment
Le1 chien2 laid3 a4 fourrure5 rouge6 1 2 3 4 6 5
Identifying word alignments notation : identify the tag labels selected by each model ? y(i , j ) = 1 when e-to-f alignment y selects French word i to align with English word j ? z(i , j ) = 1 when f-to-e alignment z selects French word i to align with English word j example : two HMM alignment models with y(6, 5) = 1 and z(6, 5) = 1
The1 ugly2 dog3 has4 red5 fur6 1 3 2 4 6 5 y
Le1 chien2 laid3 a4 fourrure5 rouge6 1 2 3 4 6 5 z
Combined optimization goal : arg max y?Y,z?Z f ( y ) + g(z ) such that for all i = 1 . . . n , j = 1 . . . n , y(i , j ) = z(i , j)
Algorithm step-by-step [ Animation ] 4. Practical issues aim : overview of practical dual decomposition techniques ? tracking the progress of the algorithm ? extracting solutions if algorithm does not converge ? lazy update of dual solutions
Tracking progress at each stage of the algorithm there are several useful values track : ? y ( k ) , z ( k ) are current dual solutions ? L(u ( k ) ) is the current dual value ? y ( k ) , l(y ( k ) ) is a potential primal feasible solution ? f ( y ( k ) ) + g(l(y ( k ) )) is the potential primal value useful signals : ? L(u ( k ) )? L(u(k?1)) is the dual change ( may be positive ) ? min k
L(u ( k ) ) is the best dual value ( tightest upper bound ) ? max k f ( y ( k ) ) + g(l(y ( k ) )) is the best primal value the optimal value must be between the best dual and primal values
Approximate solution upon agreement the solution is exact , but this may not occur otherwise , there is an easy way to find an approximate solution choose : the structure y ( k ?) where k ? = arg max k f ( y ( k ) ) + g(l(y ( k ) )) is the iteration with the best primal score guarantee : the solution yk ? is non-optimal by at most ( min t
L(u t ))? ( f ( y ( k ?) ) + g(l(y ( k ?) ))) there are other methods to estimate solutions , for instance by averaging solutions ( see Nedic ? and Ozdaglar (2009))
Lazy decoding idea : don?t recompute y ( k ) or z(k ) from scratch each iteration lazy decoding : if subgradient u(k ) is sparse , then y ( k ) may be very easy to compute from y ( k?1) use : ? very helpful if y or z factors naturally into several parts ? decompositions with this property are very fast in practice example : ? in corpus-level tagging , only need to recompute sentences with a word type that received an update 5. Linear programming aim : explore the connections between dual decomposition and linear programming ? basic optimization over the simplex ? formal properties of linear programming ? full example with fractional optimal solutions ? tightening linear program relaxations
Simplex define : ? ? y is the simplex over Y where ? ? ? y implies ? y ? 0 and ? y ? y = 1 ? ? z is the simplex over Z ? ? y : Y ? ? y maps elements to the simplex example:
Y = { y1, y2, y3} vertices ? ? y ( y1) = (1, 0, 0) ? ? y ( y2) = (0, 1, 0) ? ? y ( y3) = (0, 0, 1) ? y ( y1) ? y ( y2) ? y ( y3) ? y
Linear programming optimize over the simplices ? y and ? z instead of the discrete sets
Y and Z goal : optimize linear program max ??? y ,??? z ? y ? y f ( y ) + ? z ? zg(z ) such that for all i , t ? y ? yy(i , t ) = ? z ? zz(i , t)
Lagrangian
Lagrangian:
M(u , ?, ?) = ? y ? y f ( y ) + ? z ? zg(z ) + ? i,t u(i , t ) ( ? y ? yy(i , t )? ? z ? zz(i , t ) ) = ( ? y ? y f ( y ) + ? i,t u(i , t ) ? y ? yy(i , t ) ) + ( ? z ? zg(z )? ? i,t u(i , t ) ? z ? zz(i , t ) )
Lagrangian dual:
M(u ) = max ??? y ,??? z
M(u , ?, ?)
Strong duality define : ? ??, ?? is the optimal assignment to ?, ? in the linear program theorem : min u
M(u ) = ? y ?? y f ( y ) + ? z ?? zg(z ) proof : by linear programming duality
Dual relationship theorem : for any value of u,
M(u ) = L(u ) note : solving the original Lagrangian dual also solves dual of the linear program
Primal relationship define : ? Q ? ? y ?? z corresponds to feasible solutions of the original problem
Q = {(? y ( y ), ? z(z )): y ? Y , z ? Z , y(i , t ) = z(i , t ) for all ( i , t )} ? Q ? ? ? y ?? z is the set of feasible solutions to the LP
Q ? = {(?, ?): ? ? ? Y , ? ? ? Z , ? y ? yy(i , t ) = ? z ? zz(i , t ) for all ( i , t )} ? Q ? Q ? solutions : max q?Q h(q ) ? max q?Q ? h(q ) for any h
Concrete example ? Y = { y1, y2, y3} ? Z = { z1, z2, z3} ? ? y ? R x a
He a is y1 x b
He b is y2 x c
He c is y3
Z a
He b is z1 b
He a is z2 c
He c is z3
Simple solution
Y x a
He a is y1 x b
He b is y2 x c
He c is y3
Z a
He b is z1 b
He a is z2 c
He c is z3 choose : ? ?(1) = (0, 0, 1) ? ? y is representation of y3 ? ?(1) = (0, 0, 1) ? ? z is representation of z3 confirm : ? y ?(1)y y(i , t ) = ? z ?(1)z z(i , t ) ?(1) and ?(1) satisfy agreement constraint
Fractional solution
Y x a
He a is y1 x b
He b is y2 x c
He c is y3
Z a
He b is z1 b
He a is z2 c
He c is z3 choose : ? ?(2) = (0.5, 0.5, 0) ? ? y is combination of y1 and y2 ? ?(2) = (0.5, 0.5, 0) ? ? z is combination of z1 and z2 confirm : ? y ?(2)y y(i , t ) = ? z ?(2)z z(i , t ) ?(2) and ?(2) satisfy agreement constraint , but not integral
Optimal solution weights : ? the choice of f and g determines the optimal solution ? if ( f , g ) favors (?(2), ?(2)), the optimal solution is fractional example : f = [1 1 2] and g = [1 1 ? 2] ? f ? ?(1) + g ? ?(1) = 0 vs f ? ?(2) + g ? ?(2) = 2 ? ?(2), ?(2) is optimal , even though it is fractional
Algorithm run [ Animation]
Tightening ( Sherali and Adams , 1994; Sontag et al , 2008) modify : ? extend Y , Z to identify bigrams of part-of-speech tags ? y(i , t1, t2) = 1 ? y(i , t1) = 1 and y(i + 1, t2) = 1 ? z(i , t1, t2) = 1 ? z(i , t1) = 1 and z(i + 1, t2) = 1 all bigram constraints : valid to add for all i , t1, t2 ? T ? y ? yy(i , t1, t2) = ? z ? zz(i , t1, t2) however this would make decoding expensive single bigram constraint : cheaper to implement ? y ? yy(1, a , b ) = ? z ? zz(1, a , b ) the solution ?(1), ?(1) trivially passes this constraint , while ?(2), ?(2) violates it
Dual decomposition with tightening tightened decomposition includes an additional Lagrange multiplier yu,v = arg max y?Y f ( y ) + ? i , t u(i , t)y(i , t ) + v(1, a , b)y(1, a , b ) zu,v = arg max z?Z g(z )? ? i , t u(i , t)z(i , t )? v(1, a , b)z(1, a , b ) in general , this term can make the decoding problem more difficult example : ? for small examples , these penalties are easy to compute ? for CFG parsing , need to include extra states that maintain tag bigrams ( still faster than full intersection)
Tightening step-by-step [ Animation ] 6. Advanced examples aim : demonstrate some different relaxation techniques ? higher-order nonprojective dependency parsing ? syntactic machine translation Higher-order nonprojective dependency parsing setup : given a model for higher-order nonprojective dependency parsing ( sibling features ) problem : find nonprojective dependency parse that maximizes the score of this model difficulty : ? model is NP-hard to decode ? complexity of the model comes from enforcing combinatorial constraints strategy : design a decomposition that separates combinatorial constraints from direct implementation of the scoring function
Nonprojective dependency parsing structure : ? starts at the root symbol * ? each word has a exactly one parent word ? produces a tree structure ( no cycles ) ? dependencies can cross example : *0 John1 saw2 a3 movie4 today5 that6 he7 liked8 *0 John1 saw2 a3 movie4 today5 that6 he7 liked8
Arc-Factored *0 John1 saw2 a3 movie4 today5 that6 he7 liked8 f ( y ) = score(head =?0,mod = saw2) + score(saw2, John1) + score(saw2,movie4) + score(saw2, today5) + score(movie4, a3) + ...
e.g . score(?0, saw2) = log p(saw2|?0) ( generative model ) or score(?0, saw2) = w ? ?( saw2, ?0) ( CRF/perceptron model ) y ? = arg max y f ( y ) ? Minimum Spanning Tree Algorithm
Sibling models *0 John1 saw2 a3 movie4 today5 that6 he7 liked8 f ( y ) = score(head = ?0, prev = NULL,mod = saw2) + score(saw2,NULL , John1)+score(saw2,NULL,movie4) + score(saw2,movie4, today5) + ...
e.g . score(saw2,movie4, today5) = log p(today5|saw2,movie4) or score(saw2,movie4, today5) = w ? ?( saw2,movie4, today5) y ? = arg max y f ( y ) ? NP-Hard Thought experiment : individual decoding *0 John1 saw2 a3 movie4 today5 that6 he7 liked8 score(saw2,NULL , John1) + score(saw2,NULL,movie4) + score(saw2,movie4, today5) score(saw2,NULL , John1) + score(saw2,NULL , that6) score(saw2,NULL , a3) + score(saw2, a3,he7) possibilities under sibling model , can solve for each word with Viterbi decoding.
Thought experiment continued *0 John1 saw2 a3 movie4 today5 that6 he7 liked8 idea : do individual decoding for each head word using dynamic programming if we?re lucky , we?ll end up with a valid final tree but we might violate some constraints
Dual decomposition structure goal : y ? = arg max y?Y f ( y ) rewrite : arg max y ? Y z ? Z , f ( y ) + g(z ) such that for all i , j y(i , j ) = z(i , j)
Algorithm step-by-step [ Animation]
Syntactic translation decoding setup : assume a trained model for syntactic machine translation problem : find best derivation that maximizes the score of this model difficulty : ? need to incorporate language model in decoding ? empirically , relaxation is often not tight , so dual decomposition does not always converge strategy : ? use a different relaxation to handle language model ? incrementally add constraints to find exact solution
Syntactic translation example [ Animation]
Summary presented dual decomposition as a method for decoding in NLP formal guarantees ? gives certificate or approximate solution ? can improve approximate solutions by tightening relaxation efficient algorithms ? uses fast combinatorial algorithms ? can improve speed with lazy decoding widely applicable ? demonstrated algorithms for a wide range of NLP tasks ( parsing , tagging , alignment , mt decoding)
References I
J . DeNero and K . Macherey . Model-Based Aligner Combination Using Dual Decomposition . In Proc . ACL , 2011.
D . Klein and C.D . Manning . Factored A * Search for Models over Sequences and Trees . In Proc IJCAI , volume 18, pages 1246?1251. Citeseer , 2003.
N . Komodakis , N . Paragios , and G . Tziritas . Mrf energy minimization and beyond via dual decomposition . IEEE Transactions on Pattern Analysis and Machine Intelligence , 2010. ISSN 0162-8828.
Terry Koo , Alexander M . Rush , Michael Collins , Tommi Jaakkola , and David Sontag . Dual decomposition for parsing with nonprojective head automata . In EMNLP , 2010. URL http://www.aclweb.org/anthology/D10-1125.
B.H . Korte and J . Vygen . Combinatorial Optimization : Theory and
Algorithms . Springer Verlag , 2008.
References II
C . Lemare?chal . Lagrangian Relaxation . In Computational Combinatorial Optimization , Optimal or Provably Near-Optimal Solutions [ based on a Spring School ], pages 112?156, London , UK , 2001. Springer-Verlag . ISBN 3-540-42877-1.
Angelia Nedic ? and Asuman Ozdaglar . Approximate primal solutions and rate analysis for dual subgradient methods . SIAM Journal on Optimization , 19(4):1757?1780, 2009.
Christopher Raphael . Coarse-to-fine dynamic programming . IEEE Transactions on Pattern Analysis and Machine Intelligence , 23: 1379?1390, 2001.
A.M . Rush and M . Collins . Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation . In Proc.
ACL , 2011.
A.M . Rush , D . Sontag , M . Collins , and T . Jaakkola . On Dual Decomposition and Linear Programming Relaxations for Natural Language Processing . In Proc . EMNLP , 2010.
References III
Hanif D . Sherali and Warren P . Adams . A hierarchy of relaxations and convex hull characterizations for mixed-integer zero?one programming problems . Discrete Applied Mathematics , 52(1):83 ? 106, 1994.
D.A . Smith and J . Eisner . Dependency Parsing by Belief Propagation . In Proc . EMNLP , pages 145?156, 2008. URL http://www.aclweb.org/anthology/D08-1016.
D . Sontag , T . Meltzer , A . Globerson , T . Jaakkola , and Y . Weiss.
Tightening LP relaxations for MAP using message passing . In
Proc . UAI , 2008.
