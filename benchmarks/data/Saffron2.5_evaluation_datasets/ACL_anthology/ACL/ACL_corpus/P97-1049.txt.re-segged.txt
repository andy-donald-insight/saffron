Hierarchical Non-Emitting Markov Models
Eric Sven Ristad and Robert G . Thomas
Department of Computer Science
Princeton University
Princeton , NJ 08544-2087
rist ad , rgt ) ? cs.princeton , edu
Abstract
We describe a simple variant of the inter-
polated Markov model with non-emitting
state transitions and prove that it is strictly
more powerful than any Markov model.
Empirical results demonstrate that the
non-emitting model outperforms the inter-
polated model on the Brown corpus and
on the Wall Street Journal under a wide range of experimental conditions  . The non-emitting model is also much less prone to over training  . 
1 Introduction
The Markov model has long been the core technology of statistical anguage modeling  . Many other models have been proposed , but none has offered a better combination of predictive performance  , computational efficiency , and ease of implementation . 
Here we add hierarchical non-emitting state transitions to the Markov model  . Although the states in our model remain Markovian  , the model itself is no longer Markovian because it can represent unbounded dependencies in the state distribution  . 
Consequently , the non-emitting Markov model is strictly more powerful than any Markov model  , including the context model ( Rissanen , 1983; Rissanen ,  1986) , the backoff model ( Cleary and Witten , 1984; Katz ,  1987) , and the interpolated Markov model ( Jelinek and Mercer , 1980; MacKay and Peto ,  1994) . 
More importantly , the non-emitting model consistently outperforms the interpolated Markov model on natural language texts  , under a wide range of experimental conditions . We believe that the superior performance of the non-emitting model is due to its ability to better model conditional independence  . Thus , the non-emitting model is better able to represent both conditional independence and long -distance dependence  , i e . , it is simply a better statistical model . The non-emitting model is also nearly as computationally eff?cient and easy to implement as the interpolated model  . 
The remainder of our article consists of four sections  . In section 2 , we review the interpolated Markov model and briefly demonstrate hat all interpolated models are equivalent to some basic Markov model of the same model order  . Next , we introduce the hierarchical non-emitting Markov model in section  3  , and prove that even a lowly second order non -emitting model is strictly more powerful than any basic Markov model  , of any model order . In section 4 , we report empirical results for the interpolated model and the non-emitting model on the Brown corpus and Wall Street Journal  . Finally , in section 5 we conjecture that the empirical success of the non-emitting model is due to its ability to better model a point of apparent independence  , such as may occur at a sentence boundary . 
Our notation is as follows . Let A be a finite alphabet of distinct symbols , \[ A\[=k , and let zT 6 AT denote an arbitrary string of length T over the alphabet A  . Then z ~ denotes the substring of zT that begins at position i and ends at position j  . For convenience , we abbreviate the unit length substring z~aszi and the length t prefix of zT as z *  . 
2 Background
Here we review the basic Markov model and the interpolated Markov model  , and establish their equivalence . 
A basic Markov model ? = ( A , n , 6 , ) consists of an alphabet A , a model order n , n > 0 , and the state transition probabilities 6 , : AnxA---*\[0 ,  1\] . 
With probability 6 , ( y\[zn ) , a Markov model in the state z ' ~ will emit the symboly and transition to the state z ' ~ y  . Therefore , the probability Prn(ZtlXt1 ,  ? ) assigned by an order n basic Markov model ? to a symbol z ' in the history z  t1 depends only on the last n symbols of the history . 
?,'~I,Tt-l\pm(z,lz'-l , ?) = ~ . ~, I , - . J(1) An interpolated Markov model ? = ( A , n , A , 6) consists of a finite alphabet A , a maximal model order n , the state transition probabilities 6= 60 . .  .  6 , , 6i : A i x A ~ \[0 ,  1\] , and the state-conditional interpolation parameters A =  A0  . . . An , Ai : Ai---*\[0,1\] . 

The probability assigned by an interpolated model is a linear combination of the probabilities assigned by all the lower order Markov models  . 
p0(yl ' , ?) = + (1-Ai(zi )) p?(ylz ~ , ?) (2) where ) q(zi ) = 0 for i > n , and and therefore p~(z , lzt-1 ,  ?)  , -7 = p?(ztlzt_ , ~ , ?) , i e . , the prediction depends only on the last n symbols of the history  . 
In the interpolated model , the interpolation parameter smooth the conditional probabilities estimated from longer histories with those estimated from shorter histories  ( :lelinek and Mercer ,  1980) . 
Longer historie support stronger predictions , while shorter histories have more accurate statistics  . Interpolating the predictions from histories of different lengths results in more accurate predictions than can be obtained from any fixed history length  . 
A quick glance at the form of ( 2 ) and ( 1 ) reveals the fundamental simplicity of the interpolated Markov model  . Every interpolated model ? is equivalent to some basic Markov model ?'  ( temma 2 . 1) , and every basic Markov model ? is equivalent o some interpolated context model ?'  ( lemma 2 . 2) . 
Lemma 2.1
V?3qJ'VZTEA*~m(ZTI ?' , T ) : pe(zTI ? , T )\] Proof . We may convert the interpolated model ? into a basic model ?' of the same model order n  , simply by setting 6"( ylzn ) equal top c(y\[zn , ?) for all states znEAn and symbols y6A .  \[\]
Lemma 2.2
V ? ~ ? tvzT6A*\[pc(zTI ?' , T ) = pm(xT\]? , T )\] Proof . Every basic model is equivalent to an interpolated model whose interpolation values are unity for states of order n  . \[\] The lemma suffice to establish the following theorem  . 
Theorem 1 The class of interpolated Markov models is equivalent to the class of basic Markov models  . 
Proof . By lemmas 2.1 and 2.2. f " l
A similar argument applies to the backoff model.
Every backoff model can be converted into an equivalent basic model  , and every basic model is a backoff model . 
3 Non-Emitting Markov Models
A hierarchical non-emitting Markov model ? = ( A , n , A , 5) consists of an alphabet A , a maximal model order n , the state transition probabilities , 5=5 o . . . 6 n , 6i : AixA ~\[0 , 1\] , and the non-emitting state transition probabilities A =  A0   .   .   . An , hi:Ai---*\[0,1\] . With probability 1-Ai(zi ) , a non-emitting model will transition from the state zi to the state z ~ without emitting a symbol  . With probability A/(z')~i(Y\[Zi ), a . non-emitting model will transition from the state z * to the state z ' y and emit the symbol y  . 
Therefore , the probability pe(yJ\[zi ,  ? ) assigned to a string yJ in the history xi by a non-emitting model ? has the recursive form  ( 3 )  ,  = + ( 1 - ? )   ( 3 ) where Ai ( zi ) = 0 for i > n and A0 ( e )  = 1 . Note that , unlike the basic Markov model , p~(ztlzt-l , ?)  #t--1pe(ztlzt_n ,  ? ) because the state distribution of the non -emitting model depends on the prefix zi-n : This simple fact will allow us to establish that there exists a non-emitting model that is not equivalent to any basic model  . 
Lemma 3 . 1 states that there exists a non-emitting model ? that cannot be converted into an equivalent basic model of any order  . There will always be a string zT that distinguishes the non-emitting model ? from any given basic model ?' because the non-emitting model can encode unbounde dependencies in its state distribution  . 
Lemma 3 . 13 ? V ?'3 zTEA*\[p , (zTI? , T )  #pm(zT\[?' , T )\] Proof . The idea of the proof is that our non-emitting model will encode the first symbol Zl of the string zT in its state distribution  , for an unbounded distance . This will allow it to predict the last symbol ZT using its knowledge of the first symbol zl  . The basic model will only be able predict the last symbol ZT using the preceding n symbols  , and therefore when T is greater than n , we can arrange for p , ( zTl ? , T ) to differ from any p , ~(zT\[?' , T ) , simply by our choice of zl . 
The smallest non-emitting model capable of exhibiting the required behavior has order  2  . The non-emitting transition probabilities A and the interior of the string z  T1 will be chosen so that the non-emitting model is either in an order  2 state or an order 0 state , with no way to transition from one to the other . The first symbol zl will determine whether the non-emitting model goes to the order  2 state or stays in the order 0 state . No matter what probability the basic model assigns to the final symbol ZT  , the non-emitting model can assign a different probability by the appropriate choice of Zl  , 6O(ZT ) , and Consider the second order non-emitting model over a binary alphabet with  ) ~ ( 0 )  = 1 , A(1) = 0 , and A(ll ) = 1 on strings in AI'A . When z l = 0 , then x2 will be predicted using the 1st order model 61  ( x21 xl )  , and all subsequent zt will be predicted by the second order model  62  ( ztlxtt_-~ )  . When z l = 0 , then all subsequent z , will be predicted by the 0th order model t-1~5 o ( xt )  . Thus for all t > p , pc(x~\[x~-x ) ? p~(t\[xt_v ) for any fixed p , and no basic model is equivalent to this simple non-emitting model  . \[\] It is obvious that every basic model is also a non-emitting model  , with the appropriate choice of non-
Lemma 3.2
V ?3~' V2:TEA*\[pe(xTJ ?' , T ) = prn(zTl ? , T ) \] These lemmas suffice to establish the following theorem  . 
Theorem 2 The class of non-emitting Markov models is strictly more powerful than the class of basic Markov models  , because it is able to represent a larger class of probability distributions on strings  . 
Proof . By lemmas 3.1 and 3.2. r-I
Since interpolated models and backoff models are equivalent obasic Markov models  , we have as a corollary that non-emitting Markov models are strictly more powerful than interpolated models and backoff models as well  . Note that non-emitting Markov models are considerably less powerful than the full class of stochastic finite state automaton  ( SFSA ) because their states are Markovian . Non-emitting models are also less powerful than the full : class of hidden Markov models  . 
Algorithms to evaluate the probability of a string according to a non-emitting model  , and to optimize the non-emitting state transitions on a training corpus are provided in related work  ( Ristad and
Thomas , 1997).
4 Empirical Results
The ultimate measure of a statistical model is its predictive performance in the domain of interest  . 
To take the true measure of non-emitting models for natural language texts  , we evaluate their performance as character models on the Brown corpus  ( Francis and Kucera ,  1982 ) and as word models on the Wall Street Journal . Our results show that the non-emitting Markov model consistently gives bet  . ter predictions than the traditional interpolated Markov model under equivalent experimental conditions : In all cases we compare non-emitting and interpolated models of identical model orders  , with the same number of parameters . Note that the non-emitting bigram and the interpolated bigram are equivalent  . 
Corpus Size Alphabet Blocks
Brown 6,004,032 9021
WSJ 1989 6,219,350 20,293 22
WSJ 1987-894 2,373,513 20,092 152
All , ~ values were initialized uniformly to 0 . 5 and then optimized using deleted estimation on the first  90% of each corpus ( Jelinek and Mercer ,  1980) . 
DEr . ET~D-ESTIMATIoN(B , ?) 1 . Until convergence 2 . Initialize A + , , ~- to zero ; 3 . For each block Bi in B4 . Initialize 6 using B-Bi ; 5 . EXPECTATION-STEP(Bi , ?,~+,~-); 6 . MAXIMIZATION-STEP ( ~ b,~+,)~-); 7 . Initialize ~ using B ; Here , ~+  ( zi ) accumulates the expectations of emit-ting a , symbol from state zi while ) ~- ( zi ) accumulates the expectations of transitioning to the state z ~ without emitting a symbol  . 
The remaining 10% percent of each corpus was used to evaluate model performance  . No parameter tying was performed . 1 4 . 1 Brown Corpus Our first set of experiments were with character models on the Brown corpus  . The Brown corpus is an eclectic collection of English prose  , containing 6 , 004 , 032 characters partitioned into 500 files . Deleted estimation used 21 blocks . Results are reported as per-character test message entropies  ( bits/char )  , -L log2 p(yvjv ) . The non-tlemitting model outperforms the interpolated model for all nontrivial model orders  , particularly for larger m . odel orders . The non-emitting model is considerably less prone to overtraining  . After 10 EM iterations , the order 9 non-emitting model scores 2 . 0 085 bits/char while the order 9 interpolated model scores 2  . 3338 bits/charafter 10 EM iterations . 
Bto ~, m Comus 3 . B .   .   .   .   .   .   . N < ~ . .e~ , nlngI do ~ kBe ~ EM Itorltio~1 - e --- 6   ~1~ Inta~t ~ lno Model : ~ iIEM hemtio ~~- ,  3 . Not ~ emJflJnMod ~ l:10th ~ Mlte/itlon . --" Interpo4ate ~ Model : lOt PIEM nerit k ) 41-m--I\3"4f ~ . ~2 J2 . ~ ~ - ~  .   .  :  . . . . . . .  :- . .--- . :  . . . . . 
2 tii is ai1 . 8  2   3   4   5   6   7   8 ~ ol On~r Figure 1: Test mess agentropies as a function of model order on the Brown corpus  . 
4.2 WSJ 1989
The second set of exPeriments was on the 1989 Wall Street Journal corpus , which contains 6 , 219 , 350 words . Our vocabulary consisted of the 20 , 2 93 words that occurred at least 10 times in the entire WSJ 1989 corpus . All out-of-vocabulary words 1 In forthcoming work , we compare the performance of the interpolated and non-emitting models on the Brown corpus and Wall Street Journal with ten different parameter tying schemes  . Our experiments confirm that some parameter tying schemes improve model performance  , although only slightly . The non-emitting model consistently outperformed the interpolated model on all the corpora for all the parameter tying schemes that we evaluated  . 

WS . .I 1987-'89   160 were mapped to a unique OOV symbol . Deleted estimation used 22 blocks . Following standard practice in the speech recognition community  , results are reported as per word test message perplexities  , p(yVlv ) -? . Again , the non-emitting model outperforms the interpolated Markov model for all nontriv-ial model orders  . 
WSJ 1989.--,,,
Norl-emc ~ ng Model : But EMI tor = tk ~
Intsrp ~ ated Model : ~ EMI ~ er ~ ion ~- -30 ' k 11o  "*~  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 
Iooii " Li ,, 12 Model 30; , der 4
Figure 2: Test message perplexities as a function of model order on WSJ  1989  . 
4.3 WSJ 1987-89
The third set of experiments was on the 1987-89 Wall Street Journal corpus , which contains 42 , 373 , 513 words . Our vocabulary consisted of the 20 , 0 92 words that occurred at least 63 times in the entire WSJ 1987-89 corpus . Again , all out-of-vocabulary words were mapped to a unique OOV symbol  . Deleted estimation used 152 blocks . Results are reported as test message perplexities  . As with the WS31989 corpus , the non-emitting model outperforms the interpolated model for all nontrivial model orders  . 
5 Conclusion
The power of the non-emitting model comes from its ability to represent additional information in its state distribution  . In the proof of lemma 3 . 1 above , we used the state distribution to represent a long distance dependency  . We conjecture , however , that the empirical success of the non-emitting model is due to its ability to remember to ignore  ( i e . , to forget ) a misleading history at a point of apparent independence  . 
A point of apparent independence occurs when we have adequate statistics for two strings z  n1 and yn but not yet for their concatenation z , ,- lyn . In the most extreme case , the frequencies of z n1 and yn are high , but the frequency of even the medial bigram zn -lylislow  . In such a situation , we would like to ignore the entire history z n1 when predicting y ' ~ , because all di ( yjlx n-l~-1 ) will be close to zerox

J ; SO
Non-4mitting Modot : Be=tEM  #erat ) o 41
Lnterpolatod Moflel : Best EMI torlt ~ on ~- Figure 3: Test message perplexities as a function of model order on WSJ  1987-89  . 
for i < n . To simplify the example , we assume that 6 ( yjlz~-l~-1 ) = 0 for j_>1 and i < n . 
In such a situation , the interpolated model must repeatedly transition past some suffix of the history z  ~-1 for each of the next n -1 predictions , and so the total probability assigned top c ( ynle ) by the interpolated model is a product of n ( n-1 ) /2 probabilities . 
po ( y ~ I ~"- ~ )  "-~  ) )\] = \[ i=~l ( 1-A ( x ~*-1P ( Y  ~ I  ~ ) n--1\] . . . 
(1-a(~_~yi~-l )) p(ynl~-~)
F ,,- Ir ' . -- i\]:"\[k ~= li ~ = ( 1--A ( X ' ~- ly ~- I ) ) Pc ( Y n ' ~ )   ( 4 ) In contrast , the non-emitting model will immediately transition to the empty context in order to predict the first symbol Yl  , and then it need never again transition past any suffix of xn-\]  . Consequently , the total probability assigned to p e ( yn\[e ) by the non-emitting model is a product of only n -   1 probabilities . 
n--1\]
Given the same state transition probabilities , note that ( 4 ) must be considerably ess than ( 5 ) because probabilities lie in \[0 ,  1\] . Thus , we believe that the empirical success of the non -emitting model comes from its ability to effectively ignore a misleading history rather than from its ability to remember distant events  . 

Finally , we note the use of hierarchical non-emitting transition si a general technique that may be employed in any time series model  , including context models and backoff models . 

Both authors are partially supported by Young Investigator Award  IRI-0258517 to Eric Ristad from the National Science Foundation  . 
References
Lalit R . Bahl , Peter F . Brown , Peter V . deSouza , Robert L . Mercer , and David Nahamoo .  1991 . A fast algorithm for deleted interpolation . In Proc . 
EUROSPEECH'91, pages 1209-1212, Genoa.
J . G . Cleary and I . H . Witten .  1984 . Data compression using adaptive coding and partial string matching  . IEEE Trans . Comm . , COM-32(4):396-402 . 
W . Nelson Francis and Henry Kucera .  1982 . Frequency analysis of English usage : lexicon and grammar  . Houghton Mifflin , Boston . 
Fred Jelinek and Robert L . Mercer .  1980 . Interpolated estimation of Markov source parameters from sparse data  . In Edzard S . Gelsema and Laveen N . Kanal , editors , Pattern Recognition in Practice , pages 381-397 , Amsterdam , May 2123 . 
North Holland.
Slava Katz .  1987 . Estimation of probabilities from sparse data for the language model component of a speech recognizer  . IEEE Trans . ASSP , 35:400-401 . 
David J.C . MacKay and Linda C . Bauman Peto.
1994. A hierarchical Dirichlet language model.
Natural Language Engineering , 1(1).
Jorma Rissanen .  1983 . A universal data compression system . IEEE Trans . Information Theory,

Jorma Rissanen .  1986 . Complexity of strings in the class of Markov sources  . IEEE Trans . Information
Theory , IT-32(4):526-532.
Eric Sven Ristad and Robert G . Thomas .  1997 . Hierarchical non-emitting Markov models . Technical Report CS-TR-544-96 , Department of Computer Science , Princeton University , Princeton , 
NJ , March.
Frans M . J . Willems , Yuri M . Shtarkov , and Tjalling J . Tjalkens .  1995 . The context-tree weighting method : basic properties  . IEEE Trans . 
Inf . Theory , 41(3):653-664.

