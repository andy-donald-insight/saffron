SOMECHART-BASED TECHNIQUESFOR
PARSINGILL-FOR MEDINPUT
Chris S . Mellish
Deparlment of Artificial Intelligence,
University of Edinburgh,
80 South Bridge,
EDINBURGHEH 11HN,


We argue for the usefulness of an active chart as the basis of a system that searches for the globally most plausibl explanation of failure to syntactically parse a given input  . We suggest semantics-free , grammar-independent techniques for parsing inputs displaying simple kinds of ill-formedness and discuss the search issues involved  . 
THE PROBLEM
Although the ultimate solution to the problem of processing ill-formed input mustake into account semantic and pragmatic factors  , nevertheless it is important to understand the limits of recovery stra-tegies that age based entirely on syntax and which are independent of any particular grammar  . The aim of Ot is work is therefore to explore purely syntactic and granmmr-independent t chniques to enable a to recover from simple kinds of i il-formedness in rex  . 
tual inputs . Accordingly , we present a generalised parsing strategy based on an active chart which is capable of diagnosing simple ? nvrs  ( unknown/mi . uq ~ elled words , omitted words , extra noise words ) in sentences ( from languages described by contextfree phrase slructur ? grammars without e-productions  )  . This strategy has the advantage that the recovery process can run after a standard  ( active chart ) parser has terminated unsuccessfully , without causing existing work to be reputed or the original parser to be slowed down in any way  , and that , unlike previou systems , it allows the full syntactic context obe exploited in the determination of a " best " parse for an ill-formed sentence  . 
EXPLOITING SYNTACTICCONTEXT
Weischedel and Sondheimer ( 1983 ) present an approach to processing ill-formed input based on a modified ATN parser  . The basic idea is , when an initial p~s@fails , to selec the incomplete parsing path that consumes the longest initial portion of the input  , apply a special rule to allow the blocked parse to continue  , and then to iterate this process until a successful parse is generated  . The result is a " hilloc limbing " search for the " best " parse  , relying at each point on the " longest path " heuristic  . Unfortunately , sometimes this heuristic will yield several possible parses  , for instance with the sentence :
The snow blocks I " teroad ( no partial parse getting past the point shown ) where the parser can fail expecting either a verb or a determiner  . Moreover , sometimes the heuristic will cause the most " obvious " error to be missed : 
He said that he snow the road T
The paper will T the best news is the Times where we might suspect that there is a missing verb and a misspelled " with " respectively  . In all these cases , the " longest path " heuristic fails to indicate unambiguously the minimal change that would be necessary to make the whole input acceptable as a sentence  . This is not surprising , as the left-fight bias of an ATN parser allows the system to take no account of the right context of a possible problem element  . 
Weischedel and Sond heimer's use of the " longest path " heuristic is similar to the use of locally least-cost error recovery in Anderson and Backhouse's  ( 1981 ) scheme for compilers . It seems to be generally accepted that any form of globally " minimum-distance " error correction will be too costly to implement  ( Aho and Ullman ,  1977) . Such work has , however , not considered heuristic approaches , such as the one we are developing . 
Another feature of Weischedel and
Sond heimer's system is the use of grammar-slx ~ ific recovery rules  ( " metarules " in their terminology )  . 
The same is true of many other systems for dealing with ill-formed input  ( e . g . Carhonell and Hayes (1983), Jensen et al (1983)) . Although grammar-specific recovery rules are likely in the end always to be more powerful than grammar-independent rules  , it does seem to be worth investigating how far one can get with rules that only depend on the grammar for-ma//sm used  . 

IOTt be Tgardener Tc?llects Tmanure TffT the T  antumn7 TJ ,  1 2 3 4 5 6  , < Need S from 0 to 7> < Need NP+VP from 0 to 7> < Need VP from 2 to 7> < Need VP+PP from 2 to 7> < Need PP from 4 to 7> < Need P+NP from 4 to 7> < Need P from4 to 5>   ( hypoth ~ s )   ( by topdown rule )   ( by fundamental rule with NP found bottom-up )   ( by topdown rule )   ( by fundamental rule with VP found bottom-up )   ( by topdown rule )   ( by fundamental rule with NP found bottom-up ) 
Figure 1: Focusing on an emx.
In _ ~ . ~_pting an ATN parser to compare partial parses , We is chedel and Sond heimer have already introduced machinery to represent several alternative partial parse simultaneously  . From this , it is a relatively small step to introduce a wellformed substring table  , or even an active chart , which allows for a glo-hal assessment of the state of the parser  . If the grammar form ~ fi~m is also changed to a declarative formalism  ( e . g . CF-PSGs , DCGs ( Pereira and Warren 1980) , patr-ll ( Shieber 1984)) , then there is a possibility of constructing other partial parses that do not start at the beginning of the input  . In this way , right context can play a role in the determination f the ~ est " parse  . 
WHATACHART PARSERLE A VESBEHIND
The information that an active chart parser leaves behind for consideration by a " postmortem " obviously depends on the parsings Wategy used  ( Kay 1980 , Gazdar and Mellish 1989) . Act/reedges are particularly important fx~n the point of view of diagnosing errors  , as an unsatisfied active dge suggests a place where an input error may have occurred  . So we might expect o combine violated expectations with found constituents to hypothesis e complete parses  . For simplicity , we assume here that the grammar is a simple CF -PSG  , although there are obvious generalisations . 
(Left-right ) topdown pars/ng is guaranteed to create active edges for each kind of phrase that could continue a partial parses tarling at the beginning of the input  . On the other hand , bottom-uparsing ( by which we mean left corner parsing without op -down filtering  ) is guaranteed to find all complete consti-merits of every possible parse  . In addition , whenever a nonempty initial segment of a rule RHS has been found  , the parser will create active edges for the kind of phrase predicted to occur after this segment  . Top-down parsing will always create an edge for a phrase that is needed for a parse  , and so it will always indicate by the presence of an unsatisfied active edge the firstester point  , if there is one . If a subsequent error is present , topdown parsing will not always create an active edge corresponding to it  , because the second may occur within a constituent that will not be predicted until the first error is corrected  . Similarly , fight-to-left topdown parsing will always indicate the last error point  , and a combination of the two will find the first and last  , but not necessarily any error points in between . On the other hand , bottom-up parsing will only create an active edge for each error point that comes immediately after a sequence of phrases corresponding to an initial segment of the RI-IS of a grammar rule  . Moreover , it will not necessarily refine its predictions to the most detailed level  ( e . g . having found an NP , it may predict the existence of a following VP , but not the existence of types of phrases that can start a VP  )  . Weisobe del and Sond heimer's approach can be seen as an incremental topdown parsing  , where at each stage the rightmost t in . riffled active edge is artificially allowed to be safistied in some way  . As we have seen , there is no guarantee that this sort of hillclimbing will find the " best " solution for multiple errors  , or even for single errors . How can we combine bottom-up and topdown parsing for a more effective solution ? 
FOCUSING ONANERROR
Our basic stramgy is to run a bottom-up arser over the input and then  , if this fails to find a complete parse , to run a modified topdown parser over the resulting chart to hypothesis e possible complete parses  . The modified topdown parser attempts to find the minimal errors that  , when taken account of , enable a complete parse to be constructed . Imagine that a bottom-u parser has already run over the input " the gardener collects manure if the autumn "  . Then Figure 1 shows ( informally ) how a topdown parser might focus on a possible rror  . To implement this kind of reasoning , we need a topdown parsing rule that knows how to refine a set of global needs and a constituents from either directim  . When we may encounter multiple rotors , however , we need to express multiple needs ( e . g . < NeedN from 3 to 4 and PP from 8 to I0>) . We also need to have a fimda-mental rule that can absorb found phrases firomany wherein a relevant portion of the chart  ( e . g . given a rule " NP - -+ Det Adj N " and a sequence " as marvelloussihgt "  , we need to be able to hypothesi ~ that " as " should be a Det and " sihgt " aN  )  . To save repealing work , we need a version of the topdown rule that stops when it reaches an appropriate category that has already been found bottom-up  . 
Finally , we need to handle both " anchored " and " unanchored " needs  . In an anchored need ( e . g . 
< Need NP from 0 to 4> ) we know the beginning and end of the portion of the chart within which the search is to take place  . In looking for a NP VP sequence in " the happy blage on su'mpled the bait "  , however , we can't initially find a complete ( initial ) NP or ( final ) VP and hence don't know where in the chart these phrases mee LWe express this by < Need NP from  0 to * , VP f~om * to 6> , the symbol "*" denoting a position in the chart that remains to be determined  . 
GENERALISED TOP-DOWN PARSING
If we adopt a chart parsing suategy with only edges that carry in form a fim about global needs  , thee will be considerable dupficated effort . For instance , the further efinement of the two edges : < Need NP hem  0 to 3 and V from 9 to 10> < Need NP from 0 to 3 and Adj from 10 to 11> can lead to any analysis of possible NPs between 0 and 3 being done twice . Restricting the possible format of edges in this way would be similar to allowing the " functional composition rule "  ( Steedman 1987 ) in standard chart parsing , and in general this is not done for efficiency reasons  . Instead , we need to produce a single edge that is " in charge " of the computation looking for NPs between  0 and 3  . When poss?ole NPs are then found , these then need to be combined with the original edges by an appropriate form of the fundamental rule  . We are thus led to the following form for a generalised edge in our chart parser :  . 
< C from S to E needs
C$1fi'om$1 toel , cs2 from s2 to e2.
. o.
C $ , from $ . to e , > where C is a category , the c$~are lists of categories ( which we will show inside square brackets )  ,  . S , E , the si and the e ~ atepositions in the chart ( or the special symbol "* ~ )  . The presence of an edge of this kind in the chart indicates that the parser is attempting to find a phrase of category C covering the portion of the chart from S to E  , but that in order to succeed it must still satisfy all the needs listed  . Each need specifies a sequence of categories csl that must be found contiguously to occupy the portion of the chart extending from st to ei  . 
Now that he format of the edges is defined , we can be precise about the parsing rules used . Our modified chart parsing rules are shown in Figure  2  . 
The modified topdown ru/e allows us to refine a need into a more precise one  , using a rule of the grammar ( the extra conditions on the rule prevent further efinement where a phrase of a given category has already been found within the precise part of the chart being considezed  )  . The modified fundamental ru/e allows a need to be satisfied by an edge that is completely ~ ti ~ fied  ( i . e . an inactivedge , in the standard terminology ) . A new rule , the simplification ru/~ , is now required to do the relevant house keeping when one of an edge's needs has been completely satisfied  . One way that these rules could run would be as follows  . The chart starts off with the inactive edges left by bottom-uparsing  , together with a single " seed " edge for the topdown phase < GOAL from  0 ton needs\[S\] from 0 to n > , where n is the final position in the chart . At any point the fundamental rule is run as much as possible  . When we can proceed no further , the first need is refined by the topdown rule ( hopefully search now being anchored )  . The fundamental rule may well again apply , taking account of smaller phrases that have already been found  . When this has run , the topdown rule may then furthe refine the system's expectations about the parts of the phrase that cannot be found  . 
And so on . This is just the kind of " focusing " that we discussed in the last section  . . If an edge expresses needs in several separate places  , the first will eventually get resolved , the simplification rule will then apply and the rest of the needs will then be worked on  . 
For this all to make sense , we must assume that all hypothesised needs can eventually be resolved  ( otherwise the rules do not suffice for more than one error to be narrowed own  )  . We can ensure this by introducing special rules for recoguising the most primitive kinds of errors  . The results of these rules must obviously be scored in some way  , so that errors are not wildly hypothesised in all sorts of places  . 

Top-down rule : < C from Stoeneeds\[cl . . . csl\]fromsltoe:,cs2fzoms2 to e2 . . . . cs . from s . to e . > cI~RHS ( in the grammar ) < cl from sl to e needs RHS from sx to e > where e = ff csl is not empty or e  1 ffi * then * else ex ( e l =* or CS l is nonempty or there is no category cl from sl to e :  ) 
Fundamental rule : < C from SmE needs\[ . . . csncl . . . csn\]fromsltoex , cs2 . . . > < c ~ from S ~ to Elneeds < nothing >> < Cfxom Stoeneed scsn from sxtoSt  , csx2fxomEttoel , cs2 . . . >( sl < Sx , el = * or El < e :)
Simplification rule : < CfxomS to E needs ~ from stos  , c$2 from s2 to e2 ,   . . . cs . from s . me , , > < C from S to eneed scs2 from s2 to e2, . . . cs . fxom s . to e . >
Garbage rule : < C fronts to E needs I \] from sl to el  , c$2 from s2 to e2 ,   . . . cs . froms , to e . > < C fronts to E needs cs2 from s2 to e2, . . . cs . from s . me . >( s , ~ el)
Empty category rule : < C from S to E needs\[cl . . . csl\]fromstos,cs2 froms2 to e2 . . . . ca . from s . to e . > < CfxomS to E needs cs2 from s2 to e2 .   . . . cs . f ~ oms , to e , >
Unknown word rule : < C from Stoeneeds\[cl . . . csl\]fromsltoex , cs2 froms2 to e2 . . . . cs . fzom s . to e . > < C from S to E needs cs ~ from st + l to ex , cs2 from s2 to e2 ,   . . . cs . from s . to e . > ( clalexical category , sl < the end of the chart and the word at si not of category c ~  )  . 
Figure 2: Generalised Top-down Parsing Rules
SEARCHCONTROL ANDEV ALUATION

Even without the extra rules for recognising primitive errors  , we have now introduced a large parsing search space  . For instance , the new fundamental rule means that topdown processing can take place in many different parts of the chart  . Chart parsers already use the notion of an agenda  , in which possible additions to the chart are given priority  , and so we have soughto make use of this in organising a heuristic search for the " best " poss ~ leparse  . We have considered a number of parameters for deciding which edge should have priority : MDE  ( mode of formation ) We prefer edges that arise from the fundamental rule to those that arise from the rap-down rule  ; we disprefer edges that arise from unanchored applications of the topdown nile  . 
PSF ( penalty so far ) Edges resulting from the garbage , empty category and unknown word rules are given penalty scores  . PSF counts the penalties that have been accumulated so far in an edge  . 
PB ( best penalty ) This is an estimate of the best possible penalty that this edge  , when complete . 
could have . This score can use the PSF , together with information about the parts of the chart covered-for do not have lexical entries  . 
GU $ ( the ma~um number of words that have been used so far in a partial parse using this edge  ) We prefer edges that lead to parses accounting for more words of the input  . 
PBG ( the best possible penalty for any complete hypothesis nvolving this edge  )  . This is a short-fall score in the sense of Woeds ( 1982 )  . 
UBG ( the best possible number of words that could be used in any complete hypothesis containing this edge  )  . 
In our implementation , each rule calculates each of these scores for the new edge from those of the contributing edges  . We have experimented with a number of ways of using these scores in comparing two possible edges to be added to the chart  . At present , he most promising approach seems to be to compare in mm the scores for PBG  , MDE , UBG , GUS , PSF and PB . As soon as a difference in scores is encountered , the edge that wins on this account is chosen as the preferred one  . Putting PBG first in this sequence ensures that the first solution found will be a solution with a minimal penalty score  . 
The rules for computing scores need to make estimates about he possible penalty scores that might arise from attempting to find given types of phrases in given parts of the chart  . We use a number of heuristics to compute these . For instance , the pres . 
ence of a word not appearing in the lexicon means that every parse covering that word must have a nonzero penalty score  . In general , an attempt to find an instance of a given category in a given portion of the chart must produce a penalty score if the boltom-up parsing phase has not yielded an inactive dge of the correct kind within that portion  . Finally , the fact that the grammar is assumed to have no e -productions means that an attemp to find along sequence of categories in a short piece of chart is doomed to produce a penalty score  ; similarly a sequence of lexical categories cannot be found without penalty in a pord on of charthat is too long  . 
Some of the above scoring parameters score an edge according what sorts of parses it could contribute to  , not just according to bow internally plausible it seems  . This is desirable , as we wish the construction of globally most plausible solutions to drive the parsing  . On the other hand , it introduces a number of problems for chart organisation  . As the same edge ( apart from its score ) may be generated in different ways , we may end up with multiple possible scores for it  . It would make sense at each point to consider the best of the possible scores associated with an edge to be the current score  . In this way we would not have to repeat work for every differently scored version of an edge  . But consider the following scenario : Edge A is added to the chart  . Later edge B is spawned using A and is placed in the agenda  . Subsequently A's scc~e increases because it is derived in a new and better way  . This should affect B's score ( and hence B's position on the agenda )  . 
If the score of an edge increases then the scores of edges on the agenda which were spawned from it should also increase  . To cope with this sort of problem , we need some sort of dependency analysis , a mechanism for the propagation of changes and an easily resorted agenda  . We have not addressed these problem so far-our cterent implementation treats the score as an integral part of an edge and suffers fiom the resulting duplication problem  . 
PRELIMINARYEX PERIMENTS
To see whether the ideas of this paper make sense in practice  , we have performed some very prel-iminaw experiments  , with an inefficient implementation of the chart parser and a small CF-PSG  ( 84 rules and 34 word lexicon , 18 of whose entries indicate category ambiguity ) for a fragment of English . We generated random sentences ( 30 of each length considered ) from the grammar and then introduced random ocxunences of specific types of errors into these sentences  . The errors considered were none ( i . e . leaving the correct sentence as it was ) , deleting a word , adding a word ( either a completely unknown word or a word with an entry in the lexicon  ) and substituting a completely unknown word for one word of the sentence  . For each length of original sentence , there , ~ ts were averaged over the 30 sentences randomly generated . We collected the following statistics ( see Table 1 for the results ) : BU cyc/e$-the number of cycles taken ( see below ) to exhaus the chart in the initial ( standard ) bottom-u parsing phase . 
#$ olns - the number of different " solutions " found  . A " solution " was deemed to be a description of a possible set of errors which has a minimal penalty score and if corrected would enable a complete parse to be constructed  . Possible errors were adding an extra word , deleting a word and substituting a word for an instance of a given lexical category  . 

Table 1: Preliminary experimental results


Delete one word
Add unknown word
Add known word
Substunk nown word
Length of original
BU cycles ,   ,  #Solns 31 i 69   1   135   1   198   1   17   5   50   5   105   6   155   7   '3   29   1   6   60   2   9   105   2   12   156   3   37   3   72   3   137   3   170   5   17   2   49   2   96   2   150   3 
First Last TD cycles 0   0   0   0   0   0   0   0   0   0   0   0   14   39   50   18   73   114   27   137   350   33   315   1002   9   17   65   24   36   135   39   83   526   132   289   1922   29   51   88   . d 43   88   216   58   124   568   99   325   1775   17   28   46   23   35   105   38   56   300   42   109   1162 The penalty associated with a given set of errors was the number of  em3~ in the set . 
First-the number of cycles of generalised topdown parsing required to find the first solution  . 
Last - the number of cycles of generalised topdown parsing required to find the last solution  . 
TD cyc/es-the number of cycles of generalised topdown parsing required to exhaust all possibilities of sets of errors with the same penalty as the first solution found  . 
It was important to have an implementation -independent measure of the amount of work done by the parser  , and for this we used the concept of a " cycle " of the chart parser  . A " cycle " in this context represents he activity of the parser in removing one item from the agenda  , adding the relevant edge to the chart and adding to the agendany new edges that are suggested by the rules as a result of the new addition  . For instance , in conventional topdown chart parsing a cycle might consist of removing the edge < S from  0 to 6 needs\[NPVI'\] from 0 to 6> from the front of the agenda , adding this to the chart and then adding new edges to the agenda  , as follows . Ftrst of all , for each edge of the form < NP from 0 to a needs 0> in the chart the fundamental rule determines that <S from  0 to 6 needs\[VP \] from ct to 6> should be added . Secondly , for each rule NP- . . ,   7 in the grammar the topdown rule determines that < NP from  0 to * needs y from 0 to *> should be added . With generalised topdown parsing , there are more rules to be considered , but the idea is the same . Actually , for the topdown rule our implementation schedules a whole collection of single additions  ( " apply the topdown rule to edge a " ) as a single item on the agenda . When such a request reaches the front of the queue  , the actual new edges are then computed and themselves added to the agenda  . The result of this strategy is to make the agenda smaller but more structured  , at the cost of some extracy cles . 
EVALUATION AND FUTURE WORK
The preliminary results show that , for small sentences and only one error , enumerating all the possible minimum-penalty errors takes no worse than  10 times as long as parsing the correct sentences . 
Finding the first minimal-penalty error can also be quite fast  . There is , however , a great variability between the types of error . Errors involving completely unknown words can be diagnosed reasonably allows the estimation of penalty scores to be quite accurate  ( the system still has to work out whether the word can be an addition and for what categories it can substitute for an instance of  , however ) . We have not yet considered multiple errors in a sentence  , and we can expect he behaviour to worsten dramatically as the number of errors increases  . Although Table 1 does not show this , there is also a great deal of variability between sentences of the same length with the same kind of introduced error  . It is noticeable that errors towards the end of a sentence are harder to diagnose than those at the start  . This reflects the leR-fight orientation of the parsing rules-an attempt to find phrases tarting to the right of an error will have a PBG score at least one more than the estimated PB  , whereas an attemptm find phrases in an open -ended portion of the chart starting before an error may have a PBG score the same as the PB  ( as the error may occur within the phrases to be found  )  . Thus more parsing attempts will be relegated to the lower parts of the agenda in the first case than in the second  . 
One disturbing fact about the statistics is that the number of minimal-penalty solutions may be quite large  . For instance , the ill-formed sentence : who has John seen on that had was formed by adding the extra word " had " to the sentence " who has John seen on that  "  . Our parser found three other possible single errors to account for the sentence  . The word " on " could have been an added word , the word " on " could have been a substitution for a complement iser and there could have been a missing NP after " on "  . This large number of solutions could be an arte fact of our particular gram-ram " and lexicon  ; certainly it is unclear how one should choose between possible solutions in a grammar -independent way  . In a few cases , the introduction of a random error actually produced a grammatical sentence-this occurred  , for instance , twice with sentences of length 5 given one random A__ddedword . 
At this stage , we cannot claim that our experiments have done anything more than indicate a certain concreteness to the ideas and point to a number of unresolved problems  . It remains to be seen how the performance will scale up for a realistic grammar and parser  . There are a number of detailed issues to resolve before a really practical implementation f the above ideas can be produced  . The indexing strategy of the chart needs to be altered to take into accoun the new parsing rules  , and remaining problems of duplication of effort need to be addressed  . 
For instance , the generalised version of the fundamental rule allows an active edge to combine with a set of in active dgesatisfying its needs in any order  . 
The scoring of errors is another ar ~ which should be better investigated  . Where extra words are introduced accidentally into a text  , in practice they are perhaps unlikely to be words that are already in the lexicon  . Thus when we gave our system sentences with known words added  , this may not have been a fair test . Perhaps the scoring system should prefer added words to be words outside the lexicon  , substituted words to substitute for words in open categories  , deleted words to be noncontent words , and so on . Perhaps also the confidence of the system about possible substitutions could take into account whether a standard spelling correct or can rewrite the acnmiword to a known word of the hypothesised category  . A more sophisticated rror scoring strategy could improve the system's behaviour considerably for real examples  ( it might of course makeless difference for random examples like the ones in our experiments  )  . 
Finally the behaviour of the approach with realistic grammars written in more expressive notations needs to be established  . At present , we are investigating whether any of the current ideas can be used in conjunction with Allport's  ( 1988 ) " interesting corner " parser . 

This work was done in conjunction with the SERC -supported project  GR/D/16130  . I am currently supported by an SERC Advanced Fellowship  . 

Aho , Alfred V . and Ullman , Jeffrey D . 1977 Principles of Compiler Design . Addison-Wesley . 
Allpo ~ David . 1988 The TICC : Parsing Interesting Text . In : Proceedings of the Second ACL Conference on Applied Natural Language 
Processing , Austin , Texas.
Anderson , S.O . and Backhouse , Roland C . 1981
Locally Least-Cost Error-Recovery in
Earley's Algorithm . ACM TOPIAS3(3): 318-347.
Carbonell , Jaime G . and Hayes , Philip J . 1983
Recovery Strategies for Parsing 123-146.
Gazdar , Gerald and Mellish , Chris .   1989 Natura/Language Processing in LISP-An Introduction to Computational Linguistics  . 

Jensen , Karen , Heidom , George E . , Miller , Lance A . 
and Ravin , Yael . 1983 Parse Fitting and
Prose Fitting : Getting a Hold on Ill.
Formedness . A/C/,9(3-4):147-160.
Kay , Matin . 1980 Algorithm Schemat and Data
Structures in Syntactic Processing.
Research Report CSL-80-12, Xerox PARC.
Pereir & Fernando C.N . and Warren , David ILD.
1980 Definite Clause Grammars for
Language Analysis - A Survey of the Formalism and a Comparison with Augmented Transition Networks  . Artifu:ial Intelligence 13(3): 231-278 . 
Shieber , Stuart M .   1984 The Design of a Computer Language for Linguistic Information  . In
Proceedings of COLING-84, 362-366.
Steedman , Mark . 1987 Combina tow Grammars and
Human Language ~ g . In : Garfield,
J ., Ed ., Modularity in Knowledge
Representation ad Natural Language Pro-ceasing . Bradford Books/MIT Press . 
Weischedel , Ralph M . and 5 on dheimer . Norman K.
1983 Meta-rules as a Basis for ~ g
HI-Formed Input . AICL 9(3-4): 161-177.
Woods , Williant A .   1982 Optimal Search Strategies for Speech Understanding Control  . 
Artificial Intelligence 18(3): 295-326.

