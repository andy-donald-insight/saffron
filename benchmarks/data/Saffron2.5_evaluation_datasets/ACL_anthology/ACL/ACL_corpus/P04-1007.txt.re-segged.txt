Discriminative Language Modeling with
Conditional Random Fields and the Perceptron Algorithm 
Brian Roark Murat Saraclar
AT&T Labs-Research

Michael Collins Mark Johnson
MIT CSAIL Brown University
m collins@csail.mit.edu Mark Johnson@Brown.edu

This paper describes discriminative language modeling for a large vocabulary speech recognition task  . We contrast two parameter estimation methods : the perceptron algorithm  , and a method based on conditional random fields ( CRFs )  . The models are encoded as deterministic weighted finite state automata  , and are applied by intersecting the automata with word-lattices that are the output from a baseline recognizer  . The perceptron algorithm has the benefit of automatically selecting a relatively small feature set in just a couple of passes over the training data  . However , using the feature set output from the perceptron algorithm  ( initialized with their weights )  , CRF training provides an additional 0 . 5% reduction in word error rate , for a total 1 . 8% absolute reduction from the baseline of 39 . 2% . 
1 Introduction
A crucial component of any speech recognizer is the language model  ( LM )  , which assigns scores or probabilities to candidate output strings in a speech recognizer  . The language model is used in combination with an acoustic model  , to give an overall score to candidate word sequences that ranks them in order of probability or plausibility  . 
A dominant approach in speech recognition has been to use a ? source-channel ?  , or ? noisy-channel ? model . In this approach , language modeling is effectively framed as density estimation : the language model?s task is to define a distribution over the source ? i  . e . , the possible strings in the language . Markov ( ngram ) models are often used for this task , whose parameters are optimized to maximize the likelihood of a large amount of training text  . Recognition performance is a direct measure of the effectiveness of a language model  ; an indirect measure which is frequently proposed within these approaches is the perplexity of the LM  ( i . e . , the log probability it assigns to some heldout dataset  )  . 
This paper explores alternative methods for language modeling  , which complement the source-channel approach through discriminatively trained models  . The language models we describe do not attempt to estimate a generative model P  ( w ) over strings . Instead , they are trained on acoustic sequences with their transcriptions  , in an attempt to directly optimize error rate . Our work builds on previous work on language modeling using the perceptron algorithm  , described in Roark et al (2004) . 
In particular , we explore conditional random field methods , as an alternative training method to the perceptron  . 
We describe how these models can be trained over lattices that are the output from a baseline recognizer  . We also give a number of experiments comparing the two approaches  . The perceptron method gave a 1 . 3% absolute improvement in recognition error on the Switchboard domain  ; the CRF methods we describe give a further gain , the final absolute improvement being 1 . 8% . 
A central issue we focus on concerns feature selection  . 
The number of distinct ngrams in our training data is close to  45 million , and we show that CRF training converges very slowly even when trained with a subset  ( of size 12 million ) of these features . Because of this , we explore methods for picking a small subset of the available features  . 1 The perceptron algorithm can be used as one method for feature selection  , selecting around 1 . 5 million features in total . The CRF trained with this feature set , and initialized with parameters from perceptron training  , converges much more quickly than other approaches  , and also gives the optimal performance on the heldout set  . 
We explore other approaches to feature selection , but find that the perceptron-based approach gives the best results in our experiments  . 
While we focus on ngram models , we stress that our methods are applicable to more general language modeling features ? for example  , syntactic features , as explored in , e . g . , Khudanpur and Wu (2000) . We intend to explore methods with new features in the future  . Experimental results with ngram models on 1000-best lists show a very small drop in accuracy compared to the use of lattices  . This is encouraging , in that it suggests that models with more flexible features than ngram models  , which therefore cannot be efficiently used with lattices  , may not be unduly harmed by their restriction to nbest lists  . 
1.1 Related Work
Large vocabulary ASR has benefitted from discriminative estimation of Hidden Markov Model  ( HMM ) parameters in the form of Maximum Mutual Information Estimation  ( MMIE ) or Conditional Maximum Likelihood Estimation ( CMLE )  . Woodland and Povey ( 2000 ) have shown the effectiveness of lattice-based MMIE/CMLE in challenging largescale ASR tasks such as Switchboard  . 
In fact , state-of-the-art acoustic modeling , as seen , for example , at annual Switchboard evaluations , invariably includes some kind of discriminative training  . 
Discriminative estimation of language models has also been proposed in recent years  . Jelinek ( 1995 ) suggested an acoustic sensitive language model whose parameters  1Note also that in addition to concerns about training time  , a language model with fewer features is likely to be considerably more efficient when decoding new utterances  . 
are estimated by minimizing H(WA ) , the expected uncertainty of the spoken text W , given the acoustic sequence A . Stolcke and Weintraub ( 1998 ) experimented with various discriminative approaches including MMIE with mixed results  . This work was followed up with some success by Stolcke et al  ( 2000 ) where an ? anti-LM ? , estimated from weighted Nbest hypotheses of a baseline ASR system  , was used with a negative weight in combination with the baseline LM  . Chen et al ( 2000 ) presented a method based on changing the trigram counts discriminatively  , together with changing the lexicon to add new words  . Kuo et al ( 2002 ) used the generalized probabilistic descent algorithm to train relatively small language models which attempt to minimize string error rate on the DARPA Communicator task  . Banerjee et al ( 2003 ) used a language model modification algorithm in the context of a reading tutor that listens  . Their algorithm first uses a classifier to predict what effect each parameter has on the error rate  , and then modifies the parameters to reduce the error rate based on this prediction  . 
2 Linear Models , the Perceptron
Algorithm , and Conditional Random

This section describes a general framework , global linear models , and two parameter estimation methods within the framework  , the perceptron algorithm and a method based on conditional random fields  . The linear models we describe are general enough to be applicable to a diverse range of NLP and speech tasks ? this section gives a general description of the approach  . In the next section of the paper we describe how global linear models can be applied to speech recognition  . In particular , we focus on how the decoding and parameter estimation problems can be implemented over lattices using finite-state techniques  . 
2.1 Global linear models
We follow the framework outlined in Collins (2002 ;  2004) . The task is to learn a mapping from inputs x ? X to outputs y ? Y  . We assume the following components : ( 1 ) Training examples ( xi , yi ) for i = 1 .   .   . N . 
(2 ) A function GEN which enumerates a set of candidates GEN  ( x ) for an input x . (3) A representation ? mapping each ( x , y ) ? X ? Y to a feature vector ?( x , y ) ? Rd . (4) A parameter vector ?? ? Rd . 
The components GEN , ? and ?? define a mapping from an input x to an output F  ( x ) through
F ( x ) = argmax y?GEN(x ) ?( x , y ) ? ? ? (1) where ?( x , y ) ? ? ? is the inner product ? s?s(x , y ) . 
The learning task is to set the parameter values ? ? using the training examples as evidence  . The decoding algorithm is a method for searching for they that maximizes 
Eq . 1.
2.2 The Perceptron algorithm
We now turn to methods for training the parameters ? ? of the model  , given a set of training examples
Inputs : Training examples ( xi , yi)
Initialization : Set ?? = 0

For t = 1 . . . T , i = 1 . . . N
Calculate zi = argmax z?GEN(xi ) ?( xi , z ) ? ? ? If(zi 6= yi ) then ?? = ? ? + ?( xi , yi )? ?( xi , zi )
Output : Parameters ? ?
Figure 1: A variant of the perceptron algorithm.
( x1, y1) .   .   . ( xN,yN ) . This section describes the perceptron algorithm , which was previously applied to language modeling in Roark et al  ( 2004 )  . The next section describes an alternative method , based on conditional random fields . 
The perceptron algorithm is shown in figure 1 . At each training example ( xi , yi ) , the current best-scoring hypothesis zi is found , and if it differs from the reference yi , then the cost of each feature2 is increased by the count of that feature in zi and decreased by the count of that feature in yi  . The features in the model are updated , and the algorithm moves to the next utterance . 
After each pass over the training data , performance on a heldout dataset is evaluated , and the parameterization with the best performance on the heldout set is what is ultimately produced by the algorithm  . 
Following Collins (2002) , we used the averaged parameters from the training algorithm in decoding heldout and test examples in our experiments  . Say ?? ti is the parameter vector after the i?th example is processed on the t?th pass through the data in the algorithm in figure  1  . Then the averaged parameters ? ? AVG are defined as ? ? AVG = ? i  , t ? ? ti/NT . Freund and Schapire ( 1999 ) originally proposed the averaged parameter method  ; it was shown to give substantial improvements in accuracy for tagging tasks in Collins  ( 2002 )  . 
2.3 Conditional Random Fields
Conditional Random Fields have been applied to NLP tasks such as parsing  ( Ratnaparkhi et al , 1994; Johnson et al ,  1999) , and tagging or segmentation tasks ( Lafferty et al , 2001; Sha and Pereira , 2003; McCallum and Li , 2003; Pinto et al ,  2003) . CRFs use the parameters ? ? to define a conditional distribution over the members of 
GEN ( x ) for a given input x : p ? ? ( yx ) = exp ( ? ( x , y ) ? ? ?) where Z(x , ??) = ? y?GEN(x ) exp(?(x , y )  ? ?? ) is a normalization constant that depends on x and  ??  . 
Given these definitions , the loglikelihood of the training data under parameters ?? is 
LL (??) =
N ? i=1 log p??(yi  xi ) =
N ? i=1 [ ?( xi , yi ) ? ? ? ? logZ(xi ,  ?? ) ]  ( 2 )   2Note that here lattice weights are interpreted as costs  , which changes the sign in the algorithm presented in figure  1  . 
Following Johnson et al ( 1999 ) and Lafferty et al ( 2001 )  , we use a zero-mean Gaussian prior on the parameters resulting in the regularized objective function : 
LLR (??) =
N ? i=1 [ ?( xi , yi ) ? ? ? ? logZ(xi ,  ?? ) ]? ??2 2?2  ( 3 ) The value ? dictates the relative influence of the loglikelihood term vs  . the prior , and is typically estimated using heldout data . The optimal parameters under this criterion are ? ?? = argmax ? ? LLR  ( ?? )  . 
We use a limited memory variable metric method ( Benson and More ? , 2002) to optimize LLR . There is a general implementation of this method in the Tao/PETSc software libraries  ( Balay et al , 2002; Benson et al ,  2002) . This technique has been shown to be very effective in a variety of NLP tasks  ( Malouf , 2002; Wallach ,  2002) . The main interface between the optimizer and the training data is a procedure which takes a parameter vector ?? as input  , and in turn returns LLR ( ?? ) as well as the gradient of LLR at ?? . The derivative of the objective function with respect to a parameter?s at parameter values ?? is ? LLR ? ? s = 
N ? i=1 ? ? ? s(xi , yi )? ? y?GEN(xi)p??(yxi ) ? s(xi , y ) ? ??? s?2 ( 4 ) Note that LLR ( ?? ) is a convex function , so that there is a globally optimal solution and the optimization method will find it  . The use of the Gaussian prior term ??2/2?2 in the objective function has been found to be useful in several NLP settings  . It effectively ensures that there is a large penalty for parameter values in the model becoming too large ? as such  , it tends to control over training . The choice of LLR as an objective function can be justified as maximum a posteriori  ( MAP ) training within a Bayesian approach . An alternative justification comes through a connection to support vector machines and other large margin approaches  . SVM-based approaches use an optimization criterion that is closely related to LLR ? see 
Collins (2004) for more discussion.
3 Linear models for speech recognition
We now describe how the formalism and algorithms in section  2 can be applied to language modeling for speech recognition  . 
3.1 The basic approach
As described in the previous section , linear models require definitions of X , Y , xi , yi , GEN , ? and a parameter estimation method . In the language modeling setting we take X to be the set of all possible acoustic inputs  ; Y is the set of all possible strings ,  ?? , for some vocabulary ? . Each xi is an utterance ( a sequence of acoustic feature-vectors )  , and GEN ( xi ) is the set of possible transcriptions under a first pass recognizer  . ( GEN(xi ) is a huge set , but will be represented compactly using a lattice ? we will discuss this in detail shortly  )  . We take y i to be the member of GEN ( xi ) with lowest error rate with respect to the reference transcription of xi  . 
All that remains is to define the feature-vector representation  , ?( x , y ) . In the general case , each component ? i(x , y ) could be essentially any function of the acoustic input x and the candidate transcription y  . The first feature we define is ?0(x , y ) as the log-probability of y given x under the lattice produced by the baseline recognizer  . Thus this feature will include contributions from the acoustic model and the original language model  . The remaining features are restricted to be functions over the transcription y alone and they track all ngrams up to some length  ( say n = 3 )  , for example : ?1(x , y ) = Number of times ? the the of ? is seeniny At an abstract level  , features of this form are introduced for all ngrams up to length  3 seen in some training data lattice , i . e . , ngrams seen in any word sequence within the lattices  . In practice , we consider methods that search for sparse parameter vectors ??  , thus assigning many ngrams 0 weight . This will lead to more efficient algorithms that avoid dealing explicitly with the entire set of ngrams seen in training data  . 
3.2 Implementation using WFA
We now give a brief sketch of how weighted finite -state automata  ( WFA ) can be used to implement linear models for speech recognition  . There are several papers describing the use of weighted automata and transducers for speech in detail  , e . g . , Mohri et al (2002) , but for clarity and completeness this section gives a brief description of the operations which we use  . 
For our purpose , a WFAA = (? , Q , q s , F , E ,  ?) , where ? is the vocabulary , Q is a ( finite ) set of states , qs ? Q is a unique start state , F?Q is a set of final states , E is a ( finite ) set of transitions , and ?: F?R is a function from final states to final weights  . Each transition e ? E is a tuple e = ( l[e] , p[e ] , n[e ] , w[e ]) , where l[e ]? ? is a label ( in our case , words ) , p[e ] ? Q is the origin state of e , n[e ] ? Q is the destination state of e , and w[e]?R is the weight of the transition . A successful path pi = e1 .   .   . ej is a sequence of transitions , such that p[e1] = qs , n[ej]?F , and for 1 < k ? j , n[ek?1] = p[ek] . Let ? A be the set of successful paths pi in a WFAA  . For any pi = e1 .   .   . ej , l[pi ] = l[e1] .   .   . l[ej] . 
The weights of the WFA in our case are always in the log semiring  , which means that the weight of a path pi = e1 .   .   . ej ? ? A is defined as : wA[pi ] = ( j ? k=1 w[ek] )  + ? ( ej )   ( 5 ) By convention , we use negative log probabilities as weights , so lower weights are better . All WFA that we will discuss in this paper are deterministic  , i . e . there are no  transitions , and for any two transitions e , e ? ? E , if p[e ] = p[e ?] , then l[e ] 6= l[e ?] . Thus , for any string w = w1 .   .   . wj , there is at most one successful path pi ? ? A , such that pi = e1 .   .   . ej and for 1 ? k ? j,l[ek ] = wk , i . e . l[pi ] = w . The set of string sw such that there exists api ? ? A with l[pi ] = w define a regular language LA  ? ?  . 
We can now define some operations that will be used in this paper  . 
? ? A . For a set of transitions E and ? ? R , define ? E = ( l[e] , p[e ] , n[e ] , ? w[e ]) : e?E . Then , for any WFAA = (? , Q , q s , F , E ,  ?) , define ? A for ? ? R as follows : ? A = (? , Q , q s , F , ? E ,  ??) . 
? A ? A ? . The intersection of two deterministic WFAsA?A ? in the log semiring is a deterministic WFA such that LA ? A ? = LA ? 
LA ? . For any pi ? ? A ? A ? , wA?A ? [ pi ] = wA[pi1] + wA ? [ pi2] , where l[pi]=l[pi1] = l[pi2] . 
? BestPath(A ) . This operation takes a WFAA , and returns the best scoring path p?i = argmin pi ? ? AwA[pi ]  . 
? M in Err(A , y) . Given a WFAA , a string y , and an error-function E(y , w ) , this operation returns p?i = argmin pi??AE(y , l[pi]) . This operation will generally be used with y as the reference transcription for a particular training example  , and E(y , w ) as some measure of the number of errors in w when compared to y  . In this case , the M in Err operation returns the path pi ? ? A such l[pi ] has the smallest number of errors when compared to y  . 
? Norm(A ) . Given a WFAA , this operation yields a WFAA ? such that LA = LA ? and for every pi ? ? A there is a pi ? ? ? A ? such that l[pi]=l[pi ?] and wA?[pi ?] = wA [ pi ] + log  ( ? p ? i ? ? A exp ( ? wA[p?i ] )   )   ( 6 ) 
Note that ? pi ? Norm ( A ) exp ( ?wNorm ( A ) [ pi ] )  = 1  ( 7 ) In other words the weights define a probability distribution over the paths  . 
? ExpCount(A,w ) . Given a WFAA and an ngram w , we define the expected count of w in A as
ExpCount(A , w ) = ? pi??AwNorm(A)[pi]C(w , l[pi ]) where C(w , l[pi ] ) is defined to be the number of times the ngram w appears in a string l[pi]  . 
Given an acoustic input x , let Lx be a deterministic word lattice produced by the baseline recognizer  . The lattice Lx is an acyclic WFA , representing a weighted set of possible transcriptions of x under the baseline recognizer  . The weights represent the combination of acoustic and language model scores in the original recognizer  . 
The new , discriminative language model constructed during training consists of a deterministic WFA which we will denote D  , together with a single parameter ?0 . 
The parameter ?0 is the weight for the log probability feature ?0 given by the baseline recognizer . The WFAD is constructed so that LD = ?? and for all pi ? ? DwD[pi ] = d ?  j=1 ? j ( x , l[pi ]) ? j Recall that ? j(x , w ) for j > 0 is the count of the j?th ngram in w , and ? j is the parameter associated with that w wi2   i1 w wi1 iwi wi1 ? wi?wi??wi Figure 2: Representation of a trigram model with failure transitions  . 
ngram . Then , by definition , ?0L ? D accepts the same set of strings as L , but w?0L ? D[pi ] = d ? j=0 ? j(x , l[pi ]) ? j and argmin pi ? L?(x , l[pi ]) ? ? ? = BestPath(?0L?D ) . 
Thus decoding under our new model involves first producing a lattice L from the baseline recognizer  ; second , scaling L with ?0 and intersecting it with the discriminative language model D  ; third , finding the best scoring path in the new WFA . 
We now turn to training a model , or more explicitly , deriving a discriminative language model ( D , ?0) from a set of training examples . Given a training set ( xi , ri ) for i = 1 .   .   . N , where xi is an acoustic sequence , and ri is a reference transcription , we can construct lattices Li for i = 1 .   .   . N using the baseline recognizer . We can also derive target transcriptions yi = Min Err  ( Li , ri ) . The training algorithm is then a mapping from ( Li , yi ) for i = 1 .   .   . N to a pair ( D , ?0) . Note that the construction of the language model requires two choices  . The first concerns the choice of the set of ngram features ? i for i =  1   .   .   . d implemented by D . The second concerns the choice of parameters ? i for i =  0   .   .   . d which assign weights to the ngram features as well as the baseline feature  ?0  . 
Before describing methods for training a discriminative language model using perceptron and CRF algorithms  , we give a little more detail about the structure of D  , focusing on how ngram language models can be implemented with finite-state techniques  . 
3 . 3 Representation of ngram language models An ngram model can be efficiently represented in a deterministic WFA  , through the use of failure transitions ( Allauzen et al ,  2003) . Every string accepted by such an automaton has a single path through the automaton  , and the weight of the string is the sum of the weights of the transitions in that path  . In such a representation , every state in the automaton represents an ngram history h  , e . g . wi?2wi?1 , and there are transitions leaving the state for every word wi such that the feature h wi has a weight  . 
There is also a failure transition leaving the state  , labeled with some reserved symbol ? , which can only be traversed if the next symbol in the input does not match any transition leaving the state  . This failure transition points to the backoff state h ?  , i . e . the ngram history h minus its initial word . Figure 2 shows how a trigram model can be represented in such an automaton  . See Allauzen et al (2003) for more details . 
Note that in such a deterministic representation , the entire weight of all features associated with the word wi following history h must be assigned to the transition labeled with wile aving the state h in the automaton  . For example , if h = wi?2wi?1 , then the trigram wi?2wi?1wi is a feature , as is the bigram wi?1wi and the unigram wi . In this case , the weight on the transition wile aving state h must be the sum of the trigram  , bigram and unigram feature weights . If only the trigram feature weight were assigned to the transition  , neither the unigram nor the bigram feature contribution would be included in the path weight  . In order to ensure that the correct weights are assigned to each string  , every transition encoding an order kngram must carry the sum of the weights for all ngram features of orders ? k  . To ensure that every string in ?? receives the correct weight  , for any ngram hw represented explicitly in the automaton  , h?w must also be represented explicitly in the automaton  , even if its weight is 0 . 
3.4 The perceptron algorithm
The perceptron algorithm is incremental , meaning that the language model D is built one training example at a time  , during several passes over the training set . Initially , we build D to accept all strings in ?? with weight  0  . For the perceptron experiments , we chose the parameter ?0 to be a fixed constant , chosen by optimization on the heldout set . The loop in the algorithm in figure 1 is implemented as:
For t = 1 .   .   . T , i = 1 .   .   . N : ? Calculate zi = argmax y?GEN(x ) ?( x , y ) ??? = BestPath ( ?0 Li ? D ) ? If zi 6= M in Err ( Li , ri ) , then update the feature weights as in figure 1 ( modulo the sign , because of the use of costs ) , and modify D so as to assign the correct weight to all strings  . 
In addition , averaged parameters need to be stored ( see section 2 . 2) . These parameters will replace the un-averaged parameters in Donce training is completed  . 
Note that the only ngram features to be included in Dat the end of the training process are those that occur in either a best scoring path ziora minimum error pathy i at some point during training  . Thus the perceptron algorithm is in effect doing feature selection as a byproduct of training  . Given N training examples , and T passes over the training set , O ( NT ) ngrams will have nonzero weight after training . Experiments in Roark et al .   ( 2004 ) suggest that the perceptron reaches optimal performance after a small number of training iterations  , for example T = 1 or T = 2 . Thus O ( NT ) can be very small compared to the full number of ngrams seen in all training lattices  . In our experiments , the perceptron method chose around 1 . 4 million ngrams with nonzero weight . This compares to 43 . 6 5 million possible ngrams seen in the training data . 
This is a key contrast with conditional random fields  , which optimize the parameters of a fixed feature set  . Feature selection can be critical in our domain , as training and applying a discriminative language model over all ngrams seen in the training data  ( in either correct or incorrect transcriptions ) may be computationally very demanding . One training scenario that we will consider will be using the output of the perceptron algorithm  ( the averaged parameters ) to provide the feature set and the initial feature weights for use in the CRF algorithm  . This leads to a model which is reasonably sparse , but has the benefit of CRF training , which as we will see gives gains in performance . 
3.5 Conditional Random Fields
The CRF methods that we use assume a fixed definition of the ngram features ? i for i =  1   .   .   . d in the model . 
In the experimental section we will describe a number of ways of defining the feature set  . The optimization methods we use begin at some initial setting for ??  , and then search for the parameters ? ? ? which maximize LLR  ( ?? ) as defined in Eq .  3 . 
The optimization method requires calculation of LLR  ( ?? ) and the gradient of LLR ( ?? ) for a series of values for ?? . The first step in calculating these quantities is to take the parameter values ??  , and to construct an acceptor D which accepts all strings in ??  , such that wD[pi ] = d ? j=1 ? j(x , l[pi ]) ? j For each training lattice Li , we then construct a new lattice L?i = Norm ( ?0 Li ? D )  . The lattice L?i represents ( in the log domain ) the distribution p ? ? ( yxi ) over strings y ? GEN ( xi )  . The value of log p ? ? ( yixi ) for any i can be computed by simply taking the path weight of pi such that l[pi ] = yi in the new lattice L?i  . Hence computation of LLR (??) in Eq . 3 is straightforward . 
Calculating the ngram feature gradients for the CRF optimization is also relatively simple  , once L?i has been constructed . From the derivative in Eq . 4, for each i = 1 .   .   . N , j=1 .   .   . d the quantity ? j(xi , yi )? ? y?GEN(xi)p??(yxi ) ? j(xi , y ) (8) must be computed . The first term is simply the number of times the j ? th ngram feature is seen in y i  . The second term is the expected number of times that the j?th ngram is seen in the acceptor L?i  . If the j?th ngram is w1 .   .   . wn , then this can be computed as ExpCount ( L?i , w1 .   .   . wn ) . The GRM library , which was presented in Allauzen et al (2003) , has a direct implementation of the function Exp Count  , which simultaneously calculates the expected value of all ngrams of order less than or equal to a given n in a lattice L  . 
The one non-ngram feature weight that is being estimated is the weight  ?0 given to the baseline ASR negative log probability  . Calculation of the gradient of LLR with respect to this parameter again requires calculation of the term in Eq  . 8 for j = 0 and i = 1 .   .   . N . Computation of ? y?GEN(xi)p??(yxi ) ? 0(xi , y ) turns out to be not as straightforward as calculating ngram expectations  . To do so , we rely upon the fact that ?0( xi , y ) , the negative log probability of the path , decomposes to the sum of negative log probabilities of each transition in the path  . We index each transition in the lattice Li , and store its negative log probability under the baseline model  . We can then calculate the required gradient from L ? i  , by calculating the expected value in L?i of each indexed transition in Li  . 
We found that an approximation to the gradient of ?0  , however , performed nearly identically to this exact gradient  , while requiring substantially less computation . 
Let wn1 be a string of n words , labeling a path in word lattice L?i . For brevity , let Pi ( wn1 ) = p?? ( wn1  xi ) be the conditional probability under the current model  , and let Qi ( wn1 ) be the probability of wn1 in the normalized baseline ASR lattice Norm ( Li )  . Let Li be the set of strings in the language defined by Li  . Then we wish to compute
Ei for i = 1 . . . N , where
Ei = ? wn1 ? Li
Pi(wn1) logQi(wn1 ) = ? wn1 ? Li ? k=1 . . . n
Pi(wn1) logQi(wk  wk?11 ) (9)
The approximation is to make the following Markov assumption : 
Ei ? ? wn1 ? Li ? k=1 . . . n
Pi(wn1) logQi(wkwk?1k?2) = ? xyz?Si
ExpCount(L ? i , xyz ) logQi ( zxy ) (10 ) where Si is the set of all trigrams seen in Li . The term logQi ( zxy ) can be calculated once before training for every lattice in the training set  ; the ExpCount term is calculated as before using the GRM library  . We have found this approximation to be effective in practice  , and it was used for the trials reported below . 
When the gradients and conditional likelihoods are collected from all of the utterances in the training set  , the contributions from the regularizer are combined to give an overall gradient and objective function value  . These values are provided to the parameter estimation routine  , which then returns the parameters for use in the next iteration  . The accumulation of gradients for the feature set is the most time consuming part of the approach  , but this is parallelizable , so that the computation can be divided among many processors  . 
4 Empirical Results
We present empirical results on the Rich Transcription  2002 evaluation test set ( rt02 )  , which we used as our development set , as well as on the Rich Transcription 2003 Spring evaluation CTS test set ( rt03 )  . The rt02 set consists of 6081 sentences ( 63804 words ) and has three subsets : Switchboard1 , Switchboard 2 , Switchboard Cellular . The rt03 set consists of 9050 sentences ( 76083 words ) and has two subsets : Switchboard and Fisher . 
We used the same training set as that used in Roark et al  ( 2004 )  . The training set consists of 276726 transcribed utterances ( 3047805 words )  , with an additional 20854 utterances ( 249774 words ) as heldout data . For 0 500 1000 3737 . 5
Word error rate
Baseline recognizer Perceptron , Feat = PL , Lattice Perceptron , Feat = PN , N = 1000 CRF ,  ? = ? , Feat = PL , Lattice CRF ,  ? = 0 . 5, Feat=PL , Lattice CRF , ?=0 . 5 , Feat = PN ,   N=1000 Figure 3: Word error rate on the rt02 evalset versus training iterations for CRF trials  , contrasted with baseline recognizer performance and perceptron performance  . Points are at every 20 iterations . Each point(x , y ) is the WER at the iteration with the best objective function value in the interval  ( x-20 , x] . 
each utterance , a weighted word lattice was produced , representing alternative transcriptions , from the ASR system . From each word lattice , the oracle best path was extracted , which gives the best word-error rate from among all of the hypotheses in the lattice  . The oracle word-error rate for the training set lattices was  12  . 2% . 
We also performed trials with 1000-best lists for the same training set , rather than lattices . The oracle score for the 1000-best lists was 16 . 7% . 
To produce the word-lattices , each training utterance was processed by the baseline ASR system  . However , these same utterances are what the acoustic and language models are built from  , which leads to better performance on the training utterances than can be expected when the ASR system processes unseen utterances  . To somewhat control for this , the training set was partitioned into 28 sets , and baseline Katz backoff trigram models were built for each set by including only transcripts from the other  27 sets . Since language models are generally far more prone to overtrain than standard acoustic models  , this goes a long way toward making the training conditions similar to testing conditions  . 
There are three baselines against which we are comparing  . The first is the ASR baseline , with no reweighting from a discriminatively trained ngram model  . The other two baselines are with perceptron -trained ngram model reweighting  , and were reported in Roark et al (2004) . The first of these is for a pruned-lattice trained trigram model  , which showed a reduction in word error rate ( WER ) of 1 . 3%, from 39 . 2% to 37 . 9% on rt02 . 
The second is for a 1000-best list trained trigram model , which performed only marginally worse than the lattice-trained perceptron  , at 38 . 0% on rt02 . 
4.1 Perceptron feature set
We use the perceptron-trained models as the starting point for our CRF algorithm : the feature set given to the CRF algorithm is the feature set selected by the perceptron algorithm  ; the feature weights are initialized to those of the averaged perceptron  . Figure 3 shows the performance of our three baselines versus three trials of  0   500   1000   1500   2000   250037   37  . 5
Word error rate
Baseline recognizer Perceptron , Feat = PL , Lattice CRF ,  ? = 0 . 5, Feat=PL , Lattice CRF , ?=0 . 5, Feat = E , ? = 0 . 01 CRF , ? = 0 . 5, Feat = E , ? = 0 . 9 Figure 4: Word error rate on the rt02 evalset versus training iterations for CRF trials  , contrasted with baseline recognizer performance and perceptron performance  . Points are at every 20 iterations . Each point(x , y ) is the WER at the iteration with the best objective function value in the interval  ( x-20 , x] . 
the CRF algorithm . In the first two trials , the training set consists of the pruned lattices , and the feature set is from the perceptron algorithm trained on pruned lattices  . There were 1 . 4 million features in this feature set . 
The first trial set the regularizer constant ? = ? , so that the algorithm was optimizing raw conditional likelihood  . 
The second trial is with the regularizer constant ? =   0  . 5 , which we found empirically to be a good parameterization on the heldout set  . As can be seen from these results , regularization is critical . 
The third trial in this set uses the feature set from the perceptron algorithm trained on  1000-best lists , and uses CRF optimization on these on these same 1000-best lists . 
There were 0 . 9 million features in this feature set . For this trial , we also used ? = 0 . 5 . As with the perceptron baselines , the nbest trial performs nearly identically with the pruned lattices  , here also resulting in 37 . 4% WER . This may be useful for techniques that would be more expensive to extend to lattices versus nbest lists  ( e . g . models with unbounded dependencies ) . 
These trials demonstrate that the CRF algorithm can do a better job of estimating feature weights than the perceptron algorithm for the same feature set  . As mentioned in the earlier section , feature selection is a byproduct of the perceptron algorithm  , but the CRF algorithm is given a set of features . The next two trials looked at selecting feature sets other than those provided by the perceptron algorithm  . 
4.2 Other feature sets
In order for the feature weights to be nonzero in this approach  , they must be observed in the training set . The number of unigram , bigram and trigram features with nonzero observations in the training set lattices is  43  . 65 million , or roughly 30 times the size of the perceptron feature set . Many of these features occur only rarely with very low conditional probabilities  , and hence cannot meaningfully impact system performance  . We pruned this feature set to include all unigrams and bigrams  , but only those trigrams with an expected count of greater than  0  . 01 in the training set . That is , to be included , a
Trial Iterrt02rt03
ASR Baseline - 39.2 38.2
Perceptron , Lattice - 37.9 36.9
Perceptron , Nbest-3 8.0 37.2
CRF , Lattice , PercepFeats (1 . 4M ) 769 37 . 4 36 . 5 CRF , Nbest , PercepFeats (0 . 9M ) 946 37 . 4 36 . 6 CRF , Lattice , ? = 0 . 01(12M ) 2714 37 . 6 36 . 5 CRF , Lattice , ? = 0 . 9 (1 . 5M ) 1679 37 . 5 36 . 6 Table 1: Word-error rate results at convergence iteration for various trials  , on both Switchboard 2002 test set ( rt02) , which was used as the dev set , and Switchboard 2003 test set ( rt03) . 
trigram must occur in a set of paths , the sum of the conditional probabilities of which must be greater than our threshold ? =  0  . 01 . This threshold resulted in a feature set of roughly  12 million features , nearly 10 times the size of the perceptron feature set . For better comparability with that feature set , we set our thresholds higher , so that trigrams were pruned if their expected count fell below ? =  0  . 9 , and bigrams were pruned if their expected count fell below ? =  0  . 1 . We were concerned that this may leave out some of the features on the oracle paths  , so we added back in all bigram and trigram features that occurred on oracle paths  , giving a feature set of 1 . 5 million features , roughly the same size as the perceptron feature set  . 
Figure 4 shows the results for three CRF trials versus our ASR baseline and the perceptron algorithm baseline trained on lattices  . First , the result using the perceptron feature set provides us with a WER of  37  . 4%, as previously shown . The WER at convergence for the big feature set ( 12 million features ) is 37 . 6% ; the WER at convergence for the smaller feature set  ( 1 . 5 million features ) is 37 . 5% . While both of these other feature sets converge to performance close to that using the perceptron features  , the number of iterations over the training data that are required to reach that level of performance are many more than for the perceptron -initialized feature set  . 
Table 1 shows the word-error rate at the convergence iteration for the various trials  , on both rt02 and rt03 . All of the CRF trials are significantly better than the perceptron performance  , using the Matched Pair Sentence Segment test for WER included with SCTK  ( NIST ,  2000) . 
On rt02 , the Nbest and perceptron initialized CRF trials were were significantly better than the lattice perceptron at p <  0  . 001 ; the other two CRF trials were significantly better than the lattice perceptron at p <  0  . 01 . On rt03 , the Nbest CRF trial was significantly better than the lattice perceptron at p <  0  . 002 ; the other three CRF trials were significantly better than the lattice perceptron at p <  0  . 001 . 
Finally , we measured the time of a single iteration over the training data on a single machine for the perceptron algorithm  , the CRF algorithm using the approximation to the gradient of  ?0  , and the CRF algorithm using an exact gradient of ?0  . Table 2 shows these times in hours . Because of the frequent update of the weights in the model  , the perceptron algorithm is more expensive than the CRF algorithm for a single iteration  . Further , the CRF algorithm is parallelizable , so that most of the work of an

Features Percep approx exact
Lattice , PercepFeats (1 . 4M ) 7 . 10 1 . 69 3 . 61 Nbest , PercepFeats (0 . 9M ) 3 . 40 0 . 96 1 . 40
Lattice , ? = 0.01(12M ) -2.24 4.75
Table 2: Time ( in hours ) for one iteration on a single Intel
Xeon 2.4 Ghz processor with 4GBRAM.
iteration can be shared among multiple processors . Our most common training setup for the CRF algorithm was parallelized between  20 processors , using the approximation to the gradient . In that setup , using the 1 . 4M feature set , one iteration of the perceptron algorithm took the same amount of realtime as approximately  80 iterations of CRF . 
5 Conclusion
We have contrasted two approaches to discriminative language model estimation on a difficult large vocabulary task  , showing that they can indeed scale effectively to handle this size of a problem  . Both algorithms have their benefits . The perceptron algorithm selects a relatively small subset of the total feature set  , and requires just a couple of passes over the training data  . The CRF algorithm does a better job of parameter estimation for the same feature set  , and is parallelizable , so that each pass over the training set can require just a fraction of the realtime of the perceptron algorithm  . 
The best scenario from among those that we investigated was a combination of both approaches  , with the output of the perceptron algorithm taken as the starting point for CRF estimation  . 
As a final point , note that the methods we describe do not replace an existing language model  , but rather complement it . The existing language model has the benefit that it can be trained on a large amount of text that does not have speech transcriptions  . It has the disadvantage of not being a discriminative model  . The new language model is trained on the speech transcriptions  , meaning that it has less training data , but that it has the advantage of discriminative training ? and in particular  , the advantage of being able to learn negative evidence in the form of negative weights on ngrams which are rarely or never seen in natural language text  ( e . g . , ? the of ?) , but are produced to ofrequently by the recognizer  . The methods we describe combines the two language models  , allowing them to complement each other . 

Cyril Allauzen , Mehryar Mohri , and Brian Roark .  2003 . Generalized algorithms for constructing language models  . In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics  , pages 40?47 . 
Satish Balay , William D . Gropp , Lois Curfman McInnes , and Barry F . 
Smith .  2002 . Petscusers manual . Technical Report ANL-95/11-Revision 2 . 1 . 2, Argonne National Laboratory . 
Satanjeev Banerjee , Jack Mostow , Joseph Beck , and Wilson Tam . 
2003 . Improving language models by learning from speech recognition errors in a reading tutor that listens  . In Proceedings of the Second International Conference on Applied Artificial Intelligence  , 
Fort Panhala , Kolhapur , India.
Steven J . Benson and Jorge J . More ? .  2002 . A limited memory variable metric method for bound constrained minimization  . Preprint ANL/ACSP 909-0901 , Argonne National Laboratory . 
Steven J . Benson , Lois Curfman McInnes , Jorge J . More ?, and Jason Sarich .  2002 . Taousers manual . Technical Report ANL/MCS-TM-242-Revision 1 . 4, Argonne National Laboratory . 
Zheng Chen , Kai-Fu Lee , and MingJing Li .  2000 . Discriminative training on language model . In Proceedings of the Sixth International Conference on Spoken Language Processing  ( ICSLP )  , Beijing , China . 
Michael Collins .  2002 . Discriminative training methods for hidden markov models : Theory and experiments with perceptron algorithms  . In Proceedings of the Conference on Empirical Methods in Natural Language Processing  ( EMNLP )  , pages 1?8 . 
Michael Collins .  2004 . Parameter estimation for statistical parsing models : Theory and practice of distribution-free methods  . In Harry Bunt , John Carroll , and Giorgio Satta , editors , New Developments in Parsing Technology . Kluwer . 
Yoav Freund and Robert Schapire .  1999 . Large margin classification using the perceptron algorithm  . Machine Learning , 3(37):277?296 . 
Frederick Jelinek .  1995 . Acoustic sensitive language modeling . Technical report , Center for Language and Speech Processing , Johns
Hopkins University , Baltimore , MD.
Mark Johnson , Stuart Geman , Steven Canon , Zhiyi Chi , and Stefan Riezler .  1999 . Estimators for stochastic ? unification-based ? grammars  . In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics  , pages 535?541 . 
Sanjeev Khudanpur and Jun Wu .  2000 . Maximum entropy techniques for exploiting syntactic  , semantic and collocational dependencies in language modeling  . Computer Speech and Language , 14(4):355?372 . 
Hong-Kwang Jeff Kuo , Eric Fosler-Lussier , Hui Jiang , and Chin-Hui Lee .  2002 . Discriminative training of language models for speech recognition  . In Proceedings of the International Conference on Acoustics  , Speech , and Signal Processing ( ICASSP ) , Orlando , 

John Lafferty , Andrew McCallum , and Fernando Pereira .  2001 . Conditional random fields : Probabilistic models for segmenting and labeling sequence data  . In Proc . ICML , pages 282?289, Williams
College , Williamstown , MA , USA.
Robert Malouf .  2002 . A comparison of algorithms for maximum entropy parameter estimation  . In Proc . CoNLL , pages 49?55 . 
Andrew McCallum and Wei Li .  2003 . Early results for named entity recognition with conditional random fields  , feature induction and web-enhanced lexicons . In Proc . CoNLL . 
Mehryar Mohri , Fernando C . N . Pereira , and Michael Riley .  2002 . 
Weighted finite-state transducers in speech recognition  . Computer
Speech and Language , 16(1):69?88.
NIST .  2000 . Speech recognition scoring toolkit ( sc  tk ) version 1 . 2c . 
Available at http://www.nist.gov/speech/tools.
David Pinto , Andrew McCallum , Xing Wei , and W . Bruce Croft .  2003 . 
Table extraction using conditional random fields . In Proc . ACM SI-

Adwait Ratnaparkhi , Salim Roukos , and R . Todd Ward .  1994 . A maximum entropy model for parsing . In Proceedings of the International Conference on Spoken Language Processing  ( ICSLP )  , pages 803?806 . 
Brian Roark , Murat Saraclar , and Michael Collins .  2004 . Corrective language modeling for large vocabulary ASR with the perceptron algorithm  . In Proceedings of the International Conference on Acoustics  , Speech , and Signal Processing ( ICASSP ) , pages 749?752 . 
Fei Sha and Fernando Pereira .  2003 . Shallow parsing with conditional random fields . In Proc . HLTNAACL , Edmonton , Canada . 
A . Stolcke and M . We intraub .  1998 . Discriminitive language modeling . In Proceedings of the 9th   Hub5 Conversational Speech Recognition Workshop . 
A . Stolcke , H . Bratt , J . Butzberger , H . Franco , V . R . Rao Gadde , M . Plauche , C . Richey , E . Shriberg , K . Sonmez , F . Weng , and J . Zheng .  2000 . The SRI March 2000   Hub5 conversational speech transcription system . In Proceedings of the NIST Speech Transcription Workshop  . 
Hanna Wallach .  2002 . Efficient training of conditional random fields . 
Master?s thesis , University of Edinburgh.
P . C . Woodland and D . Povey .  2000 . Large scale discriminative training for speech recognition  . In Proc . ISCAITRWASR 2000, pages 7?16 . 
