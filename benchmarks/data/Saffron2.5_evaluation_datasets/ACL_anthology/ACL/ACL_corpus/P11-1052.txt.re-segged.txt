Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics , pages 510?520,
Portland , Oregon , June 1924, 2011. c?2011 Association for Computational Linguistics
A Class of Submodular Functions for Document Summarization
Hui Lin
Dept . of Electrical Engineering
University of Washington
Seattle , WA 98195, USA
hlin@ee.washington.edu
Jeff Bilmes
Dept . of Electrical Engineering
University of Washington
Seattle , WA 98195, USA
bilmes@ee.washington.edu
Abstract
We design a class of submodular functions meant for document summarization tasks.
These functions each combine two terms , one which encourages the summary to be representative of the corpus , and the other which positively rewards diversity . Critically , our functions are monotone nondecreasing and submodular , which means that an efficient scalable greedy optimization scheme has a constant factor guarantee of optimality.
When evaluated on DUC 2004-2007 corpora , we obtain better than existing state-of-art results in both generic and query-focused document summarization . Lastly , we show that several well-established methods for document summarization correspond , in fact , to submodular function optimization , adding further evidence that submodular functions are a natural fit for document summarization.
1 Introduction
In this paper , we address the problem of generic and querybased extractive summarization from collections of related documents , a task commonly known as multidocument summarization . We treat this task as monotone submodular function maximization ( to be defined in Section 2). This has a number of critical benefits . On the one hand , there exists a simple greedy algorithm for monotone submodular function maximization where the summary solution obtained ( say S ?) is guaranteed to be almost as good as the best possible solution ( say Sopt ) according to an objective F . More precisely , the greedy algorithm is a constant factor approximation to the cardinality constrained version of the problem , so that F(S ?) ? (1 ? 1/e)F(Sopt ) ? 0.632F(Sopt ). This is particularly attractive since the quality of the solution does not depend on the size of the problem , so even very large size problems do well . It is also important to note that this is a worst case bound , and in most cases the quality of the solution obtained will be much better than this bound suggests.
Of course , none of this is useful if the objective function F is inappropriate for the summarization task . In this paper , we argue that monotone nondecreasing submodular functionsF are an ideal class of functions to investigate for document summarization.
We show , in fact , that many well-established methods for summarization ( Carbonell and Goldstein , 1998; Filatova and Hatzivassiloglou , 2004; Takamura and Okumura , 2009; Riedhammer et al , 2010; Shen and Li , 2010) correspond to submodular function optimization , a property not explicitly mentioned in these publications . We take this fact , however , as testament to the value of submodular functions for summarization : if summarization algorithms are repeatedly developed that , by chance , happen to be an instance of a submodular function optimization , this suggests that submodular functions are a natural fit . On the other hand , other authors have started realizing explicitly the value of submodular functions for summarization ( Lin and Bilmes , 2010; Qazvinian et al , 2010).
Submodular functions share many properties in common with convex functions , one of which is that they are closed under a number of common combination operations ( summation , certain compositions , restrictions , and so on ). These operations give us the tools necessary to design a powerful submodular objective for submodular document summarization that extends beyond any previous work . We demonstrate this by carefully crafting a class of submodular func-tasks , both generic and query-focused . In doing so , we demonstrate better than existing state-of-the-art performance on a number of standard summarization evaluation tasks , namely DUC04 through to DUC-07. We believe our work , moreover , might act as a springboard for researchers in summarization to consider the problem of ? how to design a submodular function ? for the summarization task.
In Section 2, we provide a brief background on submodular functions and their optimization . Section 3 describes how the task of extractive summarization can be viewed as a problem of submodular function maximization . We also in this section show that many standard methods for summarization are , in fact , already performing submodular function optimization.
In Section 4, we present our own submodular functions . Section 5 presents results on both generic and query-focused summarization tasks , showing as far as we know the best known ROUGE results for DUC04 through DUC06, and the best known precision results for DUC-07, and the best recall DUC-07 results among those that do not use a web search engine.
Section 6 discusses implications for future work.
2 Background on Submodularity
We are given a set of objects V = { v1, . . . , vn } and a functionF : 2V ? R that returns a real value for any subset S ? V . We are interested in finding the subset of bounded size | S | ? k that maximizes the function , e.g ., argmaxS?V F(S ). In general , this operation is hopelessly intractable , an unfortunate fact since the optimization coincides with many important applications . For example , F might correspond to the value or coverage of a set of sensor locations in an environment , and the goal is to find the best locations for a fixed number of sensors ( Krause et al , 2008).
If the function F is monotone submodular then the maximization is still NP complete , but it was shown in ( Nemhauser et al , 1978) that a greedy algorithm finds an approximate solution guaranteed to be within e?1 e ? 0.63 of the optimal solution , as mentioned in Section 1. A version of this algorithm ( Minoux , 1978), moreover , scales to very large data sets . Submodular functions are those that satisfy the property of diminishing returns : for anyA ? B ? V \ v , a submodular functionF must satisfyF(A+v)?F(A ) ? F(B + v)?F(B ). That is , the incremental ? value ? of v decreases as the context in which v is considered grows from A to B . An equivalent definition , useful mathematically , is that for any A,B ? V , we must have that F(A ) + F(B ) ? F(A ? B ) + F(A ? B).
If this is satisfied everywhere with equality , then the function F is called modular , and in such case
F(A ) = c + ? a?A ~ fa for a sized | V | vector ~ f of real values and constant c . A set function F is monotone nondecreasing if ? A ? B , F(A ) ? F(B ). As shorthand , in this paper , monotone nondecreasing submodular functions will simply be referred to as monotone submodular.
Historically , submodular functions have their roots in economics , game theory , combinatorial optimization , and operations research . More recently , submodular functions have started receiving attention in the machine learning and computer vision community ( Kempe et al , 2003; Narasimhan and Bilmes , 2005; Krause and Guestrin , 2005; Narasimhan and Bilmes , 2007; Krause et al , 2008; Kolmogorov and Zabin , 2004) and have recently been introduced to natural language processing for the tasks of document summarization ( Lin and Bilmes , 2010) and word alignment ( Lin and Bilmes , 2011).
Submodular functions share a number of properties in common with convex and concave functions ( Lova?sz , 1983), including their wide applicability , their generality , their multiple options for their representation , and their closure under a number of common operators ( including mixtures , truncation , complementation , and certain convolutions ). For example , if a collection of functions { Fi}i is submodular , then so is their weighted sum F = ? i ? iFi where ? i are nonnegative weights . It is not hard to show that submodular functions also have the following composition property with concave functions : Theorem 1. Given functions F : 2V ? R and f : R ? R , the composition F ? = f ? F : 2V ? R ( i.e ., F ?( S ) = f(F(S ))) is nondecreasing submodular , if f is nondecreasing concave and F is nondecreasing submodular.
This property will be quite useful when defining submodular functions for document summarization.
511 3 Submodularity in Summarization 3.1 Summarization with knapsack constraint Let the ground set V represents all the sentences ( or other linguistic units ) in a document ( or document collection , in the multidocument summarization case ). The task of extractive document summarization is to select a subset S ? V to represent the entirety ( ground set V ). There are typically constraints on S , however . Obviously , we should have | S | < | V | = N as it is a summary and should be small . In standard summarization tasks ( e.g ., DUC evaluations ), the summary is usually required to be length-limited . Therefore , constraints on S can naturally be modeled as knapsack constraints : ? i?S ci ? b , where ci is the nonnegative cost of selecting unit i ( e.g ., the number of words in the sentence ) and b is our budget . If we use a set function F : 2V ? R to measure the quality of the summary set S , the summarization problem can then be formalized as the following combinatorial optimization problem:
Problem 1. Find
S ? ? argmax
S?V
F(S ) subject to : ? i?S ci ? b.
Since this is a generalization of the cardinality constraint ( where ci = 1,?i ), this also constitutes a ( wellknown ) NP-hard problem . In this case as well , however , a modified greedy algorithm with partial enumeration can solve Problem 1 near-optimally with (1?1/e)-approximation factor ifF is monotone submodular ( Sviridenko , 2004). The partial enumeration , however , is too computationally expensive for real world applications . In ( Lin and Bilmes , 2010), we generalize the work by Khuller et al (1999) on the budgeted maximum cover problem to the general submodular framework , and show a practical greedy algorithm with a (1? 1/?e)-approximation factor , where each greedy step adds the unit with the largest ratio of objective function gain to scaled cost , while not violating the budget constraint ( see ( Lin and Bilmes , 2010) for details ). Note that in all cases , submodularity and monotonicity are two necessary ingredients to guarantee that the greedy algorithm gives near-optimal solutions.
In fact , greedy-like algorithms have been widely used in summarization . One of the more popular approaches is maximum marginal relevance ( MMR ) ( Carbonell and Goldstein , 1998), where a greedy algorithm selects the most relevant sentences , and at the same time avoids redundancy by removing sentences that are too similar to ones already selected.
Interestingly , the gain function defined in the original MMR paper ( Carbonell and Goldstein , 1998) satisfies diminishing returns , a fact apparently unnoticed until now . In particular , Carbonell and Goldstein (1998) define an objective function gain of adding element k to set S ( k /? S ) as : ? Sim1(sk , q )? (1? ?) max i?S
Sim2(si , sk ), (1) where Sim1(sk , q ) measures the similarity between unit sk to a query q , Sim2(si , sk ) measures the similarity between unit si and unit sk , and 0 ? ? ? 1 is a tradeoff coefficient . We have : Theorem 2. Given an expression forFMMR such that FMMR(S ?{ k})?FMMR(S ) is equal to Eq . 1, FMMR is non-monotone submodular.
Obviously , diminishing-returns hold since max i?S
Sim2(si , sk ) ? max i?R
Sim2(si , sk ) for all S ? R , and therefore FMMR is submodular.
On the other hand,FMMR , would not be monotone , so the greedy algorithm?s constant-factor approximation guarantee does not apply in this case.
When scoring a summary at the subsentence level , submodularity naturally arises . Concept-based summarization ( Filatova and Hatzivassiloglou , 2004; Takamura and Okumura , 2009; Riedhammer et al , 2010; Qazvinian et al , 2010) usually maximizes the weighted credit of concepts covered by the summary.
Although the authors may not have noticed , their objective functions are also submodular , adding more evidence suggesting that submodularity is natural for summarization tasks . Indeed , let S be a subset of sentences in the document and denote ?( S ) as the set of concepts contained in S . The total credit of the concepts covered by S is then
Fconcept(S ) , ? i??(S ) ci , where ci is the credit of concept i . This function is known to be submodular ( Narayanan , 1997).
512
Similar to the MMR approach , in ( Lin and Bilmes , 2010), a submodular graph based objective function is proposed where a graph cut function , measuring the similarity of the summary to the rest of document , is combined with a subtracted redundancy penalty function . The objective function is submodular but again , non-monotone . We theoretically justify that the performance guarantee of the greedy algorithm holds for this objective function with high probability ( Lin and Bilmes , 2010). Our justification , however , is shown to be applicable only to certain particular non-monotone submodular functions , under certain reasonable assumptions about the probability distribution over weights of the graph.
3.2 Summarization with covering constraint Another perspective is to treat the summarization problem as finding a low-cost subset of the document under the constraint that a summary should cover all ( or a sufficient amount of ) the information in the document . Formally , this can be expressed as
Problem 2. Find
S ? ? argmin
S?V ? i?S ci subject to : F(S ) ? ?, where ci are the element costs , and set function F(S ) measure the information covered by S . When F is submodular , the constraint F(S ) ? ? is called a submodular cover constraint . When F is monotone submodular , a greedy algorithm that iteratively selects k with minimum ck/(F(S ? { k }) ? F(S )) has approximation guarantees ( Wolsey , 1982). Recent work ( Shen and Li , 2010) proposes to model document summarization as finding a minimum dominating set and a greedy algorithm is used to solve the problem . The dominating set constraint is also a submodular cover constraint . Define ?( S ) be the set of elements that is either in S or is adjacent to some element in S . Then S is a dominating set if |?( S )| = | V |. Note that
Fdom(S ) , |?( S )| is monotone submodular . The dominating set constraint is then also a submodular cover constraint , and therefore the approaches in ( Shen and Li , 2010) are special cases of Problem 2. The solutions found in this framework , however , do not necessarily satisfy a summary?s budget constraint . Consequently , a subset of the solution found by solving Problem 2 has to be constructed as the final summary , and the near-optimality is no longer guaranteed . Therefore , solving Problem 1 for document summarization appears to be a better framework regarding global optimality . In the present paper , our framework is that of Problem 1.
3.3 Automatic summarization evaluation
Automatic evaluation of summary quality is important for the research of document summarization as it avoids the labor-intensive and potentially inconsistent human evaluation . ROUGE ( Lin , 2004) is widely used for summarization evaluation and it has been shown that ROUGE-N scores are highly correlated with human evaluation ( Lin , 2004). Interestingly , ROUGE-N is monotone submodular , adding further evidence that monotone submodular functions are natural for document summarization.
Theorem 3. ROUGE-N is monotone submodular.
Proof . By definition ( Lin , 2004), ROUGE-N is the ngram recall between a candidate summary and a set of reference summaries . Precisely , let S be the candidate summary ( a set of sentences extracted from the ground set V ), ce : 2V ? Z + be the number of times ngram e occurs in summary S , and Ri be the set of ngrams contained in the reference summary i ( suppose we have K reference summaries , i.e ., i = 1, ? ? ? , K ). Then ROUGE-N can be written as the following set function:
FROUGE-N(S ) , ? K i=1 ? e?Ri min(ce(S ), re,i ) ? K i=1 ? e?Ri re,i , where re,i is the number of times ngram e occurs in reference summary i . Since ce(S ) is monotone modular and min(x , a ) is a concave nondecreasing function of x , min(ce(S ), re,i ) is monotone submodular by Theorem 1. Since summation preserves submodularity , and the denominator is constant , we see that FROUGE-N is monotone submodular.
Since the reference summaries are unknown , it is of course impossible to optimize FROUGE-N directly.
Therefore , some approaches ( Filatova and Hatzivassiloglou , 2004; Takamura and Okumura , 2009; Riedhammer et al , 2010) instead define ? concepts ?. Alter-modular functions that naturally models the quality of a summary while not depending on an explicit notion of concepts , as we will see in the following section.
4 Monotone Submodular Objectives
Two properties of a good summary are relevance and non-redundancy . Objective functions for extractive summarization usually measure these two separately and then mix them together trading off encouraging relevance and penalizing redundancy . The redundancy penalty usually violates the monotonicity of the objective functions ( Carbonell and Goldstein , 1998; Lin and Bilmes , 2010). We therefore propose to positively reward diversity instead of negatively penalizing redundancy . In particular , we model the summary quality as
F(S ) = L(S ) + ? R(S ), (2) where L(S ) measures the coverage , or ? fidelity ?, of summary set S to the document , R(S ) rewards diversity in S , and ? ? 0 is a tradeoff coefficient.
Note that the above is analogous to the objectives widely used in machine learning , where a loss function that measures the training set error ( we measure the coverage of summary to a document ), is combined with a regularization term encouraging certain desirable ( e.g ., sparsity ) properties ( in our case , we ? regularize ? the solution to be more diverse ). In the following , we discuss how both L(S ) andR(S ) are naturally monotone submodular.
4.1 Coverage function
L(S ) can be interpreted either as a set function that measures the similarity of summary set S to the document to be summarized , or as a function representing some form of ? coverage ? of V by S . Most naturally , L(S ) should be monotone , as coverage improves with a larger summary . L(S ) should also be submodular : consider adding a new sentence into two summary sets , one a subset of the other . Intuitively , the increment when adding a new sentence to the small summary set should be larger than the increment when adding it to the larger set , as the information carried by the new sentence might have already been covered by those sentences that are in the larger summary but not in the smaller summary . This is exactly the property of diminishing returns . Indeed , Shannon entropy , as the measurement of information , is another wellknown monotone submodular function.
There are several ways to define L(S ) in our context . For instance , we could use L(S ) = ? i?V,j?S wi,j where wi,j represents the similarity between i and j . L(S ) could also be facility location objective , i.e ., L(S ) = ? i?V maxj?S wi,j , as used in ( Lin et al , 2009). We could also use
L(S ) = ? i??(S ) ci as used in concept-based summarization , where the definition of ? concept ? and the mechanism to extract these concepts become important . All of these are monotone submodular.
Alternatively , in this paper we propose the following objective that does not reply on concepts . Let
L(S ) = ? i?V min { Ci(S ), ? Ci(V )} , (3) where Ci : 2V ? R is a monotone submodular function and 0 ? ? ? 1 is a threshold coefficient . Firstly , L(S ) as defined in Eqn . 3 is a monotone submodular function . The monotonicity is immediate . To see that L(S ) is submodular , consider the fact that f(x ) = min(x , a ) where a ? 0 is a concave nondecreasing function , and by Theorem 1, each summand in Eqn . 3 is a submodular function , and as summation preserves submodularity , L(S ) is submodular.
Next , we explain the intuition behind Eqn . 3. Basically , Ci(S ) measures how similar S is to element i , or how much of i is ? covered ? by S . Then Ci(V ) is just the largest value that Ci(S ) can achieve . We call i ? saturated ? by S when min{Ci(S ), ? Ci(V )} = ? Ci(V ). When i is already saturated in this way , any new sentence j can not further improve the coverage of i even if it is very similar to i ( i.e ., Ci(S ? { j }) ? Ci(S ) is large ). This will give other sentences that are not yet saturated a higher chance of being better covered , and therefore the resulting summary tends to better cover the entire document.
One simple way to define Ci(S ) is just to use
Ci(S ) = ? j?S wi,j (4) where wi,j ? 0 measures the similarity between i and j . In this case , when ? = 1, Eqn . 3 reduces to the case where L(S ) = ? i?V,j?S wi,j . As we will see in Section 5, having an ? that is less than to the case when ? = 1, which coincides with our intuition that using a truncation threshold improves the final summary?s coverage.
4.2 Diversity reward function
Instead of penalizing redundancy by subtracting from the objective , we propose to reward diversity by adding the following to the objective:
R(S ) =
K ? i=1 ? ? j?Pi?S rj . (5) where Pi , i = 1, ? ? ? K is a partition of the ground set V ( i.e ., ? i Pi = V and the Pis are disjoint ) into separate clusters , and ri ? 0 indicates the singleton reward of i ( i.e ., the reward of adding i into the empty set ). The value ri estimates the importance of i to the summary . The functionR(S ) rewards diversity in that there is usually more benefit to selecting a sentence from a cluster not yet having one of its elements already chosen . As soon as an element is selected from a cluster , other elements from the same cluster start having diminishing gain , thanks to the square root function . For instance , consider the case where k1, k2 ? P1, k3 ? P2, and rk1 = 4, rk2 = 9, and rk3 = 4. Assume k1 is already in the summary set S . Greedily selecting the next element will choose k3 rather than k2 since ? 13 < 2 + 2. In other words , adding k3 achieves a greater reward as it increases the diversity of the summary ( by choosing from a different cluster ). Note,R(S ) is distinct from L(S ) in that R(S ) might wish to include certain outlier material that L(S ) could ignore.
It is easy to show that R(S ) is submodular by using the composition rule from Theorem 1. The square root is nondecreasing concave function.
Inside each square root lies a modular function with nonnegative weights ( and thus is monotone).
Applying the square root to such a monotone submodular function yields a submodular function , and summing them all together retains submodularity , as mentioned in Section 2. The monotonicity ofR(S ) is straightforward . Note , the form of Eqn . 5 is similar to structured group norms ( e.g ., ( Zhao et al , 2009)), recently shown to be related to submodularity ( Bach , 2010; Jegelka and Bilmes , 2011).
Several extensions to Eqn . 5 are discussed next : First , instead of using a ground set partition , intersecting clusters can be used . Second , the square root function in Eqn . 5 can be replaced with any other nondecreasing concave functions ( e.g ., f(x ) = log(1 + x )) while preserving the desired property ofR(S ), and the curvature of the concave function then determines the rate that the reward diminishes . Last , multiresolution clustering ( or partitions ) with different sizes ( K ) can be used , i.e ., we can use a mixture of components , each of which has the structure of Eqn . 5. A mixture can better represent the core structure of the ground set ( e.g ., the hierarchical structure in the documents ( Celikyilmaz and Hakkani-tu?r , 2010)). All such extensions preserve both monotonicity and submodularity.
5 Experiments
The document understanding conference ( DUC ) ( http://duc.nist.org ) was the main forum providing benchmarks for researchers working on document summarization . The tasks in DUC evolved from single-document summarization to multidocument summarization , and from generic summarization (2001?2004) to query-focused summarization (2005?2007). As ROUGE ( Lin , 2004) has been officially adopted for DUC evaluations since 2004, we also take it as our main evaluation criterion . We evaluated our approaches on DUC data 2003-2007, and demonstrate results on both generic and query-focused summarization . In all experiments , the modified greedy algorithm ( Lin and Bilmes , 2010) was used for summary generation.
5.1 Generic summarization
Summarization tasks in DUC03 and DUC04 are multidocument summarization on English news articles . In each task , 50 document clusters are given , each of which consists of 10 documents.
For each document cluster , the system generated summary may not be longer than 665 bytes including spaces and punctuation . We used DUC03 as our development set , and tested on DUC04 data.
We show ROUGE1 scores1 as it was the main evaluation criterion for DUC03, 04 evaluations.
1ROUGE version 1.5.5 with options : - a - c 95 - b 665 - m - n 4 - w 1.2 tences and stemming words using the Porter Stemmer.
Each sentence was represented using a bag-of-terms vector , where we used context terms up to bigrams.
Similarity between sentence i and sentence j , i.e ., wi,j , was computed using cosine similarity : wi,j = ? w?si tfw,i ? tfw,j ? idf2w ?? w?si tf2w,si idf ?? w?sj tf2w,j idf , where tfw,i and tfw,j are the numbers of times that w appears in si and sentence sj respectively , and idfw is the inverse document frequency ( IDF ) of term w ( up to bigram ), which was calculated as the logarithm of the ratio of the number of articles that w appears over the total number of all articles in the document cluster.
Table 1: ROUGE1 recall ( R ) and Fmeasure ( F ) results (%) on DUC04. DUC03 was used as development set.
DUC04 R F
P i?V
P j?S wi,j 33.59 32.44
L1(S ) 39.03 38.65
R1(S ) 38.23 37.81
L1(S ) + ? R1(S ) 39.35 38.90
Takamura and Okumura (2009) 38.50 -
Wang et al (2009) 39.07 -
Lin and Bilmes (2010) - 38.39
Best system in DUC04 ( peer 65) 38.28 37.94 We first tested our coverage and diversity reward objectives separately . For coverage , we use a modular Ci(S ) = ? j?S wi,j for each sentence i , i.e.,
L1(S ) = ? i?V min ? ? ? ? j?S wi,j , ? ? k?V wi,k ? ? ? . (6)
When ? = 1, L1(S ) reduces to ? i?V,j?S wi,j , which measures the overall similarity of summary set S to ground set V . As mentioned in Section 4.1, using such similarity measurement could possibly over-concentrate on a small portion of the document and result in a poor coverage of the whole document.
As shown in Table 1, optimizing this objective function gives a ROUGE1 Fmeasure score 32.44%.
On the other hand , when using L1(S ) with an ? < 1 ( the value of ? was determined on DUC03 using a grid search ), a ROUGE1 Fmeasure score 38.65% 36.2 36.4 36.6 36.8 37.4 37.6 0 5 10 15 20
RO
UG
E1 F-me asu re (% )
K=0.05N
K=0.1N
K=0.2N a
Figure 1: ROUGE1 Fmeasure scores on DUC03 when ? and K vary in objective function L1(S ) + ? R1(S ), where ? = 6 and ? = aN .
is achieved , which is already better than the best performing system in DUC04.
As for the diversity reward objective , we define the singleton reward as ri = 1N ? j wi,j , which is the average similarity of sentence i to the rest of the document . It basically states that the more similar to the whole document a sentence is , the more reward there will be by adding this sentence to an empty summary set . By using this singleton reward , we have the following diversity reward function:
R1(S ) =
K ? k=1 ? ? j?S?Pk ? i?V wi,j . (7)
In order to generate Pk , k = 1, ? ? ? K , we used CLUTO2 to cluster the sentences , where the IDF-weighted term vector was used as feature vector , and a direct K-mean clustering algorithm was used . In this experiment , we set K = 0.2N . In other words , there are 5 sentences in each cluster on average.
And as we can see in Table 1, optimizing the diversity reward function alone achieves comparable performance to the DUC04 best system.
Combining L1(S ) and R1(S ), our system outperforms the best system in DUC04 significantly , and it also outperforms several recent systems , including a concept-based summarization approach ( Takamura and Okumura , 2009), a sentence topic model based system ( Wang et al , 2009), and our MMR-styled submodular system ( Lin and Bilmes , 2010). Figure 1 illustrates how ROUGE1 scores change when ? and K vary on the development set ( DUC03).
2http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview (%) on DUC05, where DUC05 was used as training set.
DUC05 R F
L1(S ) + ? RQ(S ) 8.38 8.31
Daume ? III and Marcu (2006) 7.62 -
Extr , Daume ? et al (2009) 7.67 -
Vine , Daume ? et al (2009) 8.24 -
Table 3: ROUGE2 recall ( R ) and Fmeasure ( F ) results on DUC05 (%). We used DUC06 as training set.
DUC05 R F
L1(S ) + ? RQ(S ) 7.82 7.72
Daume ? III and Marcu (2006) 6.98 -
Best system in DUC05 ( peer 15) 7.44 7.43 5.2 Query-focused summarization We evaluated our approach on the task of query-focused summarization using DUC 05-07 data . In DUC05 and DUC06, participants were given 50 document clusters , where each cluster contains 25 news articles related to the same topic . Participants were asked to generate summaries of at most 250 words for each cluster . For each cluster , a title and a narrative describing a user?s information need are provided . The narrative is usually composed of a set of questions or a multi-sentence task description.
The main task in DUC-07 is the same as in DUC06.
In DUC 05-07, ROUGE2 was the primary criterion for evaluation , and thus we also report ROUGE-23 ( both recall R , and precision F ). Documents were processed as in Section 5.1. We used both the title and the narrative as query , where stop words , including some function words ( e.g ., ? describe ?) that appear frequently in the query , were removed . All queries were then stemmed using the Porter Stemmer.
Note that there are several ways to incorporate query-focused information into both the coverage and diversity reward objectives . For instance , Ci(S ) could be query-dependent in how it measures how much query-dependent information in i is covered by S . Also , the coefficient ? could be query and sentence dependent , where it takes larger value when a sentence is more relevant to query ( i.e ., a larger value of ? means later truncation , and therefore more possible coverage ). Similarly , sentence clustering and singleton rewards in the diversity function can also 3ROUGE version 1.5.5 was used with option - n 2 - x - m -2 4 - u - c 95 - r 1000 - f A - p 0.5 - t 0 - d - l 250 Table 4: ROUGE2 recall ( R ) and Fmeasure ( F ) results (%) on DUC06, where DUC05 was used as training set.
DUC06 R F
L1(S ) + ? RQ(S ) 9.75 9.77
Celikyilmaz and Hakkani-tu?r (2010) 9.10 -
Shen and Li (2010) 9.30 -
Best system in DUC06 ( peer 24) 9.51 9.51 Table 5: ROUGE2 recall ( R ) and Fmeasure ( F ) results (%) on DUC-07. DUC05 was used as training set for objective L1(S ) + ? RQ(S ). DUC05 and DUC06 were used as training sets for objective L1(S ) +? ? ?? RQ,?(S).
DUC-07 R F
L1(S ) + ? RQ(S ) 12.18 12.13
L1(S ) +
P3 ?=1 ?? RQ,?(S ) 12.38 12.33
Toutanova et al (2007) 11.89 11.89
Haghighi and Vanderwende (2009) 11.80 -
Celikyilmaz and Hakkani-tu?r (2010) 11.40 - Best system in DUC-07 ( peer 15) 12.45 12.29 be query-dependent . In this experiment , we explore an objective with a query-independent coverage function ( R1(S )), indicating prior importance , combined with a query-dependent diversity reward function , where the latter is defined as:
RQ(S ) =
K ? k=1 ? ? ? ? ? j?S?Pk ( ?
N ? i?V wi,j + (1? ?) rj,Q ) , where 0 ? ? ? 1, and rj,Q represents the relevance between sentence j to query Q . This query-dependent reward function is derived by using a singleton reward that is expressed as a convex combination of the query-independent score ( 1N ? i?V wi,j ) and the query-dependent score ( rj,Q ) of a sentence . We simply used the number of terms ( up to a bigram ) that sentence j overlaps the query Q as rj,Q , where the IDF weighting is not used ( i.e ., every term in the query , after stop word removal , was treated as equally important ). Both query-independent and query-dependent scores were then normalized by their largest value respectively such that they had roughly the same dynamic range.
To better estimate of the relevance between query and sentences , we further expanded sentences with synonyms and hypernyms of its constituent words . In particular , part-of-speech tags were obtained for each sentence using the maximum entropy part-of-speech tagger ( Ratnaparkhi , 1996), and all nouns were then WordNet ( Fellbaum , 1998). Note that these expanded documents were only used in the estimation rj,Q , and we plan to further explore whether there is benefit to use the expanded documents either in sentence similarity estimation or in sentence clustering in our future work . We also tried to expand the query with synonyms and observed a performance decrease , presumably due to noisy information in a query expression.
While it is possible to use an approach that is similar to ( Toutanova et al , 2007) to learn the coefficients in our objective function , we trained all coefficients to maximize ROUGE2 Fmeasure score using the Nelder-Mead ( derivative-free ) method.
Using L1(S)+?RQ(S ) as the objective and with the same sentence clustering algorithm as in the generic summarization experiment ( K = 0.2N ), our system , when both trained and tested on DUC05 ( results in Table 2), outperforms the Bayesian query-focused summarization approach and the search-based structured prediction approach , which were also trained and tested on DUC05 ( Daume ? et al , 2009).
Note that the system in ( Daume ? et al , 2009) that achieves its best performance (8.24% in ROUGE2 recall ) is a so called ? vine-growth ? system , which can be seen as an abstractive approach , whereas our system is purely an extractive system . Comparing to the extractive system in ( Daume ? et al , 2009), our system performs much better (8.38% v.s . 7.67%).
More importantly , when trained only on DUC06 and tested on DUC05 ( results in Table 3), our approach outperforms the best system in DUC05 significantly.
We further tested the system trained on DUC05 on both DUC06 and DUC-07. The results on DUC06 are shown in Table . 4. Our system outperforms the best system in DUC06, as well as two recent approaches ( Shen and Li , 2010; Celikyilmaz and Hakkani-tu?r , 2010). On DUC-07, in terms of ROUGE2 score , our system outperforms PYTHY ( Toutanova et al , 2007), a state-of-the-art supervised summarization system , as well as two recent systems including a generative summarization system based on topic models ( Haghighi and Vanderwende , 2009), and a hybrid hierarchical summarization system ( Celikyilmaz and Hakkani-tu?r , 2010). It also achieves comparable performance to the best DUC-07 system . Note that in the best DUC-07 system ( Pingali et al , 2007; Jagarlamudi et al , 2006), an external web search engine ( Yahoo !) was used to estimate a language model for query relevance . In our system , no such web search expansion was used.
To further improve the performance of our system , we used both DUC05 and DUC06 as a training set , and introduced three diversity reward terms into the objective where three different sentence clusterings with different resolutions were produced ( with sizes 0.3N , 0.15N and 0.05N ). Denoting a diversity reward corresponding to clustering ? as RQ,?(S ), we model the summary quality as
L1(S ) + ?3 ?=1 ?? RQ,?(S ). As shown in Table 5, using this objective function with multiresolution diversity rewards improves our results further , and outperforms the best system in DUC-07 in terms of
ROUGE2 Fmeasure score.
6 Conclusion and discussion
In this paper , we show that submodularity naturally arises in document summarization . Not only do many existing automatic summarization methods correspond to submodular function optimization , but also the widely used ROUGE evaluation is closely related to submodular functions . As the corresponding submodular optimization problem can be solved efficiently and effectively , the remaining question is then how to design a submodular objective that best models the task . To address this problem , we introduce a powerful class of monotone submodular functions that are well suited to document summarization by modeling two important properties of a summary , fidelity and diversity . While more advanced NLP techniques could be easily incorporated into our functions ( e.g ., language models could define a better Ci(S ), more advanced relevance estimations for the singleton rewards ri , and better and/or overlapping clustering algorithms for our diversity reward ), we already show top results on standard benchmark evaluations using fairly basic NLP methods ( e.g ., term weighting and WordNet expansion ), all , we believe , thanks to the power and generality of submodular functions . As information retrieval and web search are closely related to query-focused summarization , our approach might be beneficial in those areas as well.
518
References
F . Bach . 2010. Structured sparsity-inducing norms through submodular functions . Advances in Neural
Information Processing Systems.
J . Carbonell and J . Goldstein . 1998. The use of MMR , diversity-based reranking for reordering documents and producing summaries . In Proc . of SIGIR.
A . Celikyilmaz and D . Hakkani-tu?r . 2010. A hybrid hierarchical model for multidocument summarization . In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics , pages 815?824, Uppsala , Sweden , July . Association for Computational
Linguistics.
H . Daume ?, J . Langford , and D . Marcu . 2009. Search-based structured prediction . Machine learning , 75(3):297?325.
H . Daume ? III and D . Marcu . 2006. Bayesian query-focused summarization . In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for
Computational Linguistics , page 312.
C . Fellbaum . 1998. WordNet : An electronic lexical database . The MIT press.
E . Filatova and V . Hatzivassiloglou . 2004. Event-based extractive summarization . In Proceedings of ACL Workshop on Summarization , volume 111.
A . Haghighi and L . Vanderwende . 2009. Exploring content models for multidocument summarization . In Proceedings of Human Language Technologies : The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics , pages 362?370, Boulder , Colorado , June . Association for Computational Linguistics.
J . Jagarlamudi , P . Pingali , and V . Varma . 2006. Query independent sentence scoring approach to DUC 2006.
In DUC 2006.
S . Jegelka and J . A . Bilmes . 2011. Submodularity beyond submodular energies : coupling edges in graph cuts.
In Computer Vision and Pattern Recognition ( CVPR),
Colorado Springs , CO , June.
D . Kempe , J . Kleinberg , and E . Tardos . 2003. Maximizing the spread of influence through a social network.
In Proceedings of the 9th Conference on SIGKDD International Conference on Knowledge Discovery and
Data Mining ( KDD).
S . Khuller , A . Moss , and J . Naor . 1999. The budgeted maximum coverage problem . Information Processing
Letters , 70(1):39?45.
V . Kolmogorov and R . Zabin . 2004. What energy functions can be minimized via graph cuts ? IEEE Transactions on Pattern Analysis and Machine Intelligence , 26(2):147?159.
A . Krause and C . Guestrin . 2005. Near-optimal nonmy-opic value of information in graphical models . In Proc.
of Uncertainty in AI.
A . Krause , H.B . McMahan , C . Guestrin , and A . Gupta.
2008. Robust submodular observation selection . Journal of Machine Learning Research , 9:2761?2801.
H . Lin and J . Bilmes . 2010. Multidocument summarization via budgeted maximization of submodular functions . In North American chapter of the Association for Computational Linguistics/Human Language Technology Conference ( NAACL/HLT-2010), Los Angeles,
CA , June.
H . Lin and J . Bilmes . 2011. Word alignment via submodular maximization over matroids . In The 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies ( ACL-HLT ), Portland , OR , June.
H . Lin , J . Bilmes , and S . Xie . 2009. Graphbased submodular selection for extractive summarization . In Proc.
IEEE Automatic Speech Recognition and Understanding ( ASRU ), Merano , Italy , December.
C.-Y . Lin . 2004. ROUGE : A package for automatic evaluation of summaries . In Text Summarization Branches Out : Proceedings of the ACL04 Workshop.
L . Lova?sz . 1983. Submodular functions and convexity.
Mathematical programming-The state of the art,(eds . A.
Bachem , M . Grotschel and B . Korte ) Springer , pages 235?257.
M . Minoux . 1978. Accelerated greedy algorithms for maximizing submodular set functions . Optimization
Techniques , pages 234?243.
M . Narasimhan and J . Bilmes . 2005. A submodular-supermodular procedure with applications to discriminative structure learning . In Proc . Conf . Uncertainty in Artifical Intelligence , Edinburgh , Scotland , July . Morgan Kaufmann Publishers.
M . Narasimhan and J . Bilmes . 2007. Local search for balanced submodular clusterings . In Twentieth International Joint Conference on Artificial Intelligence ( IJ-
CAI07), Hyderabad , India , January.
H . Narayanan . 1997. Submodular functions and electrical networks . North-Holland.
G.L . Nemhauser , L.A . Wolsey , and M.L . Fisher . 1978. An analysis of approximations for maximizing submodular set functions I . Mathematical Programming , 14(1):265? 294.
P . Pingali , K . Rahul , and V . Varma . 2007. IIIT Hyderabad at DUC 2007. Proceedings of DUC 2007.
V . Qazvinian , D.R . Radev , and A . Ozgu?r . 2010. Citation Summarization Through Keyphrase Extraction . In Proceedings of the 23rd International Conference on Computational Linguistics ( Coling 2010), pages 895? 903.
519
A . Ratnaparkhi . 1996. A maximum entropy model for part-of-speech tagging . In EMNLP , volume 1, pages 133?142.
K . Riedhammer , B . Favre , and D . Hakkani-Tu?r . 2010.
Long story short-Global unsupervised models for keyphrase based meeting summarization . Speech Communication.
C . Shen and T . Li . 2010. Multidocument summarization via the minimum dominating set . In Proceedings of the 23rd International Conference on Computational Linguistics ( Coling 2010), pages 984?992, Beijing , China , August . Coling 2010 Organizing Committee.
M . Sviridenko . 2004. A note on maximizing a submodular set function subject to a knapsack constraint . Operations Research Letters , 32(1):41?43.
H . Takamura and M . Okumura . 2009. Text summarization model based on maximum coverage problem and its variant . In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics , pages 781?789. Association for
Computational Linguistics.
K . Toutanova , C . Brockett , M . Gamon , J . Jagarlamudi , H . Suzuki , and L . Vanderwende . 2007. The PYTHY summarization system : Microsoft research at DUC 2007. In the proceedings of Document Understanding
Conference.
D . Wang , S . Zhu , T . Li , and Y . Gong . 2009. Multidocument summarization using sentence-based topic models . In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers , pages 297?300, Suntec , Singapore , August . Association for Computational Linguistics.
L.A . Wolsey . 1982. An analysis of the greedy algorithm for the submodular set covering problem . Combinatorica , 2(4):385?393.
P . Zhao , G . Rocha , and B . Yu . 2009. Grouped and hierarchical model selection through composite absolute penalties . Annals of Statistics , 37(6A):3468?3497.
520
