On the Evaluation and Comparison of Taggers : the Effect of 
Noise in Testing Corpora.
Llufs Padr 6 and Llufs M~rquez
Dep . LSI . Technical University of Catalonia
c/Jordi Girona 13.08034 Barcelona
padro , lluism@lsi.upc.es

This paper addresses the issue of POS tagger evaluation  . Such evaluation is usually performed by comparing the tagger output with a reference test corpus  , which is assumed to be error-free . Currently used corpora contain noise which causes the obtained performance to be a distortion of the real value  . We analyze to what extent this distortion may invalidate the comparison between taggers or the measure of the improvement given by a new system  . The main conclusion is that a more rigorous testing experimentation setting/designing is needed to reliably evaluate and compare tagger accuracies  . 
1 Introduction and Motivation
Part of Speech ( POS ) Tagging is a quite well defined NLP problem , which consists of assigning to each word in a text the proper morphosyntactic tag for the given context  . Although many words are ambiguous regarding their POS  , in most cases they can be completely disambiguated taking into account an adequate context  . Successful taggers have been built using several approaches  , uch as statistical techniques , symbolic machine learning techniques , neural networks , etc . The accuracy reported by most curren taggers ranges from  9697% to almost 100% in the linguistically motivated Constraint Grammar environment  . 
Unfortunately , there have been very few direct comparisons of alternative taggers  1 on identical test data . However , in most current papers it is argued that the performance of some taggers is better than others as a result of some kind of indirect comparisons between them  . We IOne of the exceptions is the work by ( Samuelsson and Voutilainen ,  1997) , in which a very strict comparison between taggers is performed  . 
think that there are a number of not enough controlled/considered factors that make these conclusions dubious in most cases  . 
In this direction , the present paper aims to point out some of the difficulties arising when evaluating and comparing tagger performances against a reference test corpus  , and to make some criticism about common practices followed by the NLP researchers in this issue  . 
The abovementioned factors can affect either the evaluation or the comparison process  . 
Factors affecting the evaluation process are : ( 1 ) Training and test experiments are usually performed over noisy corpora which distorts the obtained results  ,   ( 2 ) performance figures are too often calculated from only a single or very small number of trials  , though average results from multiple trials are crucial to obtain reliable estimations of accuracy  ( Mooney ,  1996) ,   ( 3 ) testing experiments are usually done on corpora with the same characteristics a the training data -usually a small fresh portion of the training corpus-but no serious attempts have been done in order to determine the reliability of the results when moving from one domain to another  ( Krovetz ,  1997) , and ( 4 ) no figures about computational effort-space /time complexity-are usually reported  , even from an empirical perspective . A factors affecting the comparison process is that comparisons between taggers are often indirect  , while they should be compared under the same conditions in a multiple-trial experiment with statistical tests of significance  . 
For these reasons , this paper calls for a discussion on POS taggers evaluation  , aiming to establish a more rigorous test experimentation setting/designing  , indispensable to extract reliable conclusions . As a starting point , we will focus only on how the noise in the test corpus can affect the obtained results  . 
9972 Noise in the testing corpus
From a machine learning perspective , the relevant noise in the corpus is that of non systematically mistagged words  ( i . e . different annotations for words appearing in the same syntac-tic/semantic on texts  )  . 
Commonly used annotated corpora have noise . See , for instance , the following examples from the Wall Street Journal  ( wsJ ) corpus : Verb participle forms are sometimes tagged as such  ( VBN ) and also as adjectives ( JJ ) in other sentences with no structural differences : la  )   . . . failing_VBG to_TO voluntarily_RB submit_VB the_DT requested_VBN ? n formation _~N  . . . 
ib ) . . . a_DT large JJ sample_NN of_IN married_2J women_b ~ IS with __IN at_IN least_3JS one_CD child_NN .   .   . 
Another structure not coherently tagged are noun chains when the nouns  ( NN ) are ambiguous and can be also adjectives ( JJ ) : 2a )   . . . Mr . _NNP Hahn_NNP ,  _  , the_DT 62-year-oldJJ chairman_NN and_CC chief_NN executive_JJ officer_NN of_IN Georgia-Pacific_NNP Corp  . _NNP .   .   . 
2b ) . . . Burger_NNP King_NNP's_POS chief_JJ executive_NN o \] ficer_NN  , _ , 
Barry_NNP Gibbons_NNP ,_, stars_VBZ in_ . IN ads_NNS saying_VBG . . . 
The noise in the test set produces a wrong estimation of accuracy  , since correct answers are computed as wrong and vice versa  . In following sections we will show how this uncertainty in the evaluation may be  , in some cases , larger than the reported improvements from one system to another  , so invalidating the conclusions of the comparison . 
3 Model Setting
To study the appropriateness of the choices made by a POS tagger  , a reference tagging must be selected and assumed to be correct in order to compare it with the tagger output  . This is usually done by assuming that the disambiguated test corpora being used contains the right POS disambiguation  . This approach is quite right when the tagger error rate is larger enough than the test corpus error rate  , nevertheless , the current POS taggers have reached a performance level that invalidates this choice  , since the tagger error rate is getting too close to the error rate of the test corpus  . 
Since we want to study the relationship between the tagger error rate and the test corpus error rate  , we have to establish an absolute reference point . Although ( Church , 1992) questions the concept of correct analysis , ( Samuelsson and Voutilainen ,  1997 ) establish that there exists a-statistically significant-absolute correct disambiguation  , respect o which the error rates of either the tagger or the test corpus can be computed  . What we will focus on is how distorted is the tagger error rate by the use of a noisy test corpus as a reference  . 
The cases we can find when evaluating the performance of a certain tagger are presented in table  1  . OK/--aOK stand for a right/wrong tag ( respect othe absolute correct disambiguation )  . When both the tagger and the test corpus have the correct tag  , the tag is correctly evaluated as right . When the test corpus has the correct tag and the tagger gets it wrong  , the occurrence is correctly evaluated as wrong . 
But problems arise when the test corpus has a wrong tag : If the tagger gets it correctly  , it is evaluated as wrong when it should be right ( false negative )  . If the tagger gets it wrong , it will be rightly evaluated as wrong if the error commited by the tagger is other than the error in the test corpus  , but wrongly evaluated as right ( false positive ) if the error is the same . 
Table 1 shows the computation of the percent-corpus tagger eval : righteval : wrong 
OKcOKt(1-C ) t
OKc " aOKt- ( 1-C )   ( 1- t ) " aOKcOKt-Cu ~ OKc ~ OKtC ( 1-u ) pC ( 1-u ) (1-p ) Table 1: Possible cases when evaluating a tagger . 
ages of each case . The meanings of the used variables are : C : Test corpus error rate  . Usually an estimation is supplied with the corpus . 
t : Tagger performance rate on words rightly tagged in the test corpus  . It can be seen as

u : Tagger performance rate on words wrongly tagged in the test corpus  . It can be seen as

9 98 p : Probability that the tagger makes the same error as the test corpus  , given that both get a wrong tag . 
x : Real performance of the tagger , i . e . what would be obtained on an error-free test set . 
K : Observed performance of the tagger , computed on the noisy test corpus . 
For simplicity , we will consider only performance on ambiguous words  . Considering unambiguous words will make the analysis more complex  , since it should be taken into account hat neither the behaviour of the tagger  ( given by u , t , p ) nor the errors in the test corpus ( given by c ) are the same on ambiguous and unambiguous words . Nevertheless , this is an issue that must be further addressed . 
If we knew each one of the above proportions , we would be able to compute the real performance of our tagger  ( x ) by adding up the OKtrows from table 1 , i . e . the cases in which the tagger got the right disambiguation independently from the tagging of the test set : x =  ( 1-C ) t+Cu ( 1 ) The equation of the observed performance can also be extracted from table  1  , adding up what is evaluated as right :
K = (1-C)t+C(1-u)p(2)
The relationship between the real and the observed performance is derived from  1 and 2: x = K-C ( 1-u ) p + Cu Since only K and C are known ( or approximately estimated ) we cannot compute the real performance of the tagger  . All we can do is to establish some reasonable bounds for t  , u and p , and see in which range is x . 
Since all variables are probabilities , they are bounded in \[0 ,  1\] . We also can assume 2 that K > C . We can use this constraints and the above equations to bound the values of all variables  . From 2 , we obtain : u = 1K-t ( 1-C ) K - t ( 1-C ) K-C ( 1-u ) p , p- , t = CpC(l-u)1-CT hus , u will be maximum when p and t are maximum ( i . e .  1) . This gives an upper bound 2In the cases we are interested in - that is , current systems - the tagger observed performance  , I ( , is over 90% , while the corpus error rate , C , is below 10% . 
for u of (1-K)/C . When t = 0 , u will range in \[- oo , 1-K/C \] depending on the value of p . 
Since we are assuming K > C , the most informative lower bound for u keeps being zero  . Similarly , p is minimum when t = 1 and u = 0 . When t = 0 the value for p will range in \[ K/C , + c ~\] depending on u . Since K > C , the most informative upper bound for p is still 1 . Finally , t will be maximum when u-1 and p = 0 , and minimum when u = O and p = l . Summarizing : 0 < u < min 1 , ~-~(3) K+C-llma=0 , ~ j < p_<l ( 4 )   1 -CSince the values of the variables are mutually constrained  , it is not possible that , for instance , u and t have simultaneously their upper bound values  ( if ( 1-I (   ) /C < 1 thenK/ ( 1-C ) >1 and vice versa )  . Any bound which is out of \[0 ,   1\] is not informative and the appropriate boundary , 0 or 1 , is then used . Note that the lower bound for t will never be negative under the assumption K > C  . 
Once we have established these bounds , we can use equation 1 to compute the range for the real performance value of our tagger : x will be minimum when u and t are mimmum  , which produces the following bounds : ~ , , , , = Xr-Cp (6)
K+C if K_<1-Cx , , , ~ == 1--g+c-1 if K>1-C (7) pAs an example , let's suppose we evaluate a tagger on a test corpus which is known to contain about  3% of errors ( C = 0 . 03)  , and obtain a reported performance of 93% 3 ( K=0 . 93) . In this case , equations 6 and 7 yield a range for the real performance x that varies from  \[0  . 93, 0 . 96\] when p = Oto\[0 . 90, 0 . 96\] when p = 1 . 
This results suggest hat although we observe a performance of K  , we cannot be sure of how well is our tagger performing without taking into account the values of t  , u and p . 
It is also obvious that the intervals in the above example are too wide  , since they consider all the possible parameter values  , even when they correspond to very unlikely param- ~ This is a realistic case obtained by  ( M?rquez and Padr6 , 1997) tagger . Note that 93% is the accuracy on ambiguous words ( the equivalent overall accuracy was about 97% )  . 
999 eter combinations 4 . In section 4 we will try to narrow those intervals , limiting the possibilities to reasonable cases . 
4 Reasonable Bounds for the Basic

In real cases , not all parameter combinations will be equally likely  . In addition , the bounds for the values of t , u and p are closely related to the similarities between the training and test corpora  . That is , if the training and test sets are extracted from the same corpus  , they will probably contain the same kind of errors in the same kind of situations  . This may cause the training procedure to learn the errors-especially if they are systematic- and thus the resulting tagger will tend to make the same errors that appear in the test set  . On the contrary , if the training and test sets come from different sources - sharing only the tagset-the behaviour of the resulting tagger will not depend on the right or wrong tagging of the test set  . 
We can try to establish narrower bounds for the parameters than those obtained in section  3  . 
First of all , the value of t is already constrained enough , due to its high contribution ( 1-C ) to the value of K , which forcest to take a value close to K . For instance , applying the boundaries in equation 5 to the case C- -0  . 03 and K--0 . 93, we obtain that t belongs to \[0 . 928, 0 . 959\] . 
The range for u can be slightly narrowed considering the following : In the case of independent test and training corpora  , u will tend to be equal to t . Otherwise , the more biased towards the corpus errors is the language model  , the lower u will be . Note than u > t would mean that the tagger disambiguates better the noisy cases than the correct ones  . Concerning to the lower bound , only in the case that all the errors in the training and test corpus were systematic  ( and thus can be learned ) could ur each zero . 
However , not only this is not a likely Situation , but also requires a perfect-learning tagger . It seems more reasonable that , in normal cases , errors will be random , and the tagger will behave 4For instance , it is not reasonable that u = 0 , which would mean that the tagger never disambiguates correctly a wrong word in the corpus  , or p--1 , which would mean that it always makes the same error when both are wrong  . 
randomly on the noisy occurrences . This yields a lower bound for u of 1/a , being a the average ambiguity ratio for ambiguous words  . 
The reasonable bounds for u are thus _1 <_u < mint , a Finally , the value of p has similar constraints to those of u  . If the test and training corpora are independent , the probability of making the same error , given that both are wrong , will be the random1/(a-1) . If the corpora are not independent , he errors that can be learned by the tagger will cause ptorise up to  ( potentially )  1 . Again , only in the case that all errors where systematic  , could preach 1 . 
Then , the reasonable bounds for pare : 1   K+C-1 max < p < 1 a-l'C- -5 On ' Comparing Tagger

As stated above , knowing which are the reasonable limits for the u  , p and t parameters enables us to compute the range in which the real performance of the tagger can vary  . 
So , given two different aggers T1 and T2 , and provided we know the values for the test corpus error rate and the observed performance of both cases  ( C1 , C  ~ , K 1 , K s ) , we can compare them by matching the reasonable intervals for the respective real performances xl and  x2  . 
From a conservative position , we cannot strongly state than one of the taggers performs better than the other when the two intervals overlap  , since this implies a chance that the real performances of both taggers are the same  . 
The following real example has been extracted from  ( M?rquez and Padrd ,  1997 ) : The tagger T1 uses only bigram information and has an observed performance on ambiguous words  K1  =  0  . 9135 (96 . 86% overall ) . The tagger 2"2 uses trigrams and automatically acquired context constraints and has an accuracy of  K2  =  0  . 9282 (97 . 39% overall ) . Both taggers have been evaluated on a corpus ( wsJ ) with an estimated error rate 5C1 = C2 = 0 . 03 . The average ambiguity ratio of the ambiguous words in the corpus is  a=2  . 5 tags/word . 
5 The ( wsJ ) corpus error rate is estimated over all words . We are assunfing that the errors distribute uniformly among all words  , although ambiguous words sonable intervals for the real performance of the taggers  . 
for pi = (1/a ) = 0 . 4 IxxE [91 . 35, 94 . 05] x2 ? [92:82, 95 . 60] for pi = lxlE[90 . 75, 93 . 99] x2E[92 . 22, 95 . 5 5] The same information is included in figure 1 which presents the reasonable accuracy intervals for both taggers  , for pranging from 1/a = 0 . 4 to 1 ( the shadowed part corresponds to the overlapping region between intervals  )  . 

I " IIIII % acc qracy 1/a = 0.4 I . . . .
909192939495 ( X)
Figure 1: Reasonable intervals for both taggers The obtained intervals have a large overlap region which implies that there are reasonable parameter combinations that could cause the taggers to produce different observed performances though their real accuracies were very similar  . From this conservative approach , we would not be able to conclude that the tagger 7"2 is better than T1  , even though the 95% confidence intervals for the observed performances did allow us to do so  . 
6 Discussion
The presented analysis of the effects of noise in the test corpus on the evaluation of POS taggers leads us to conclude that when a tagger is evaluated as better than another using noisy test corpus  , there are reasonable chances that they are in fact very similar but one of them is just adapting better than the other to the noise in the corpus  . 
probably have a higher error rate . Nevertheless , a higher value for C would cause the intervals to be wider and to overlap even more  . 
We believe that the widespread practice of evaluating taggers against a noisy test corpus has reached its limit  , since the performance of current aggers is getting too close to the error rate usually found in test corpora  . 
An obviou solution - and may be not as costly as one might think  , since small test sets properly used may yield enough statistical evidence-is using only error -freetest corpora  . Another possibility is to further study the influence of noise in order to establish a criterion-e  . g . a threshold depending on the amount of overlapping between intervals-to decide whether a given tagger can be considered better than another  . 
There is still much to be done in this direction . This paper does not intend to establish a new evaluation method for POS tagging  , but to point out that there are some issues- such as the noise in test corpus - that have been paid little attention and are more important than what they seem to be  . 
Some of the issues that should be further considered are : The effect of noise on unambiguous words  ; the reasonable intervals for overall real performance  ; the-probably-different values of C , p , u and t for ambiguous/unambiguous words ; how to estimate the parameter values of the evaluated tagger in order to constrain as much as possible the intervals  ; the statistical significance of the interval overlappings  ; a more informed ( and less conservative ) criterion to re-ject/accept the hypothesis that both taggers are different  , etc . 
References
Church , K . W .  1992 . Current Practice in Part of Speech Tagging and Suggestions for the Future  . 
In Simmons ( ed . ), Sbornikpraci : In Honor of Henry Kudera . Michigan Slavic Studies . 
Krovetz , R .  1997 . Homonymy and Polysemy in Information Retrieval . In Proceedings of joint
E/ACL meeting.
M~trquez , L . and Padr6,L .  1997 . A Flexible POS Tagger Using an Automatically Acquired Language Model  . In Proceedings of joint E/ACL meeting . 
Mooney , R . J .  1996 . Comparative Experiments on Disambiguating Word Senses : An Illustration of the Role of Bias in Machine Learning  . In Proceedings of EMNLP'96 conference . 
Samuelsson , C . and Voutilainen , A .  1997 . Comparing a Linguistic and a Stochastic Tagger . In Proceedings of joint E/ACL meeting . 


A quest article versa so bre l'avaluaci6 dedesam-biguad or smorfosint ~ ctics . Normalment ,   l'ava-luaci6 esfa comparant lasortidadel de sam-biguad or a rabun corpus de refer ~ ncia  , queses upos all iured ' errors . Detotamanera , els corpus ques ' usen habitual ment contenen so roll que causa que el rendiment ques ' obt ~ dels desam-biguad or siguiuna  distorsi6 delval or real . Enaque starticle analitzem fins a quin punt a ques-ta  distorsi6 pot invalidarla comparaci6 entredes ambiguad or solames ura de lamillor a a por-tada per unnousi stem a  . La conclusi6 principal ~ squecal establir procediments alternatius  d'experimentaci6 milsrigorosos , per poder ava-luari comparar fiablement les precisions des ambiguad or smor fosint ? ctics  . 

Artikulu haudesanbiguatzai lemorfosintak-tiko en ebaluazio areninguru and at za  . Nor-male an , ebaluazioa , desanbiguatzaile are nirte-era et au stez errorerik gabekoer referent ziako corpus batk on paratuz egit end a  . Halaere , maiz corpus et an error eake goten diraeta horrek de-sanbiguatzaile are nemaitz aren benetako balioan eraginaiz at end u  . Artikuluhon et an , hainzuze nere , horixeazter tuko dugu , alegia , zerneurritan distorts io horrekjar dezakeen auzit and esanbiguatzai leen artekokon parazioaedo sistemaberribate ke kardezake en hobekuntza -maila  . Konklusiorik nagusiena hauxe da:de -sanbiguatzai lemorfos in taktiko a kztert zeko et a moduzi urrago batez konparatuahalizateko  , azterketa-bideaksakonagoaketaze hatzagoakizan be hark olirate keel a  . 

