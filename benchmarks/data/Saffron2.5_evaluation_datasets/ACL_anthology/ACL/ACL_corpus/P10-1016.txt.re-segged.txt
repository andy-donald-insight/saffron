Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics , pages 148?156,
Uppsala , Sweden , 1116 July 2010. c?2010 Association for Computational Linguistics
Pseudoword for Phrase-based Machine Translation


Xiangyu Duan Min Zhang Haizhou Li
Institute for Infocomm Research , A-STAR , Singapore
{Xduan , mzhang , hli}@i2r.a-star.edu.sg



Abstract

The pipeline of most Phrase-Based Statistical Machine Translation ( PBSMT ) systems starts from automatically word aligned parallel corpus . But word appears to be too finegrained in some cases such as noncompositional phrasal equivalences , where no clear word alignments exist . Using words as inputs to PBSMT pipeline has inborn deficiency . This paper proposes pseudoword as a new start point for PBSMT pipeline . Pseudoword is a kind of basic multiword expression that characterizes minimal sequence of consecutive words in sense of translation . By casting pseudoword searching problem into a parsing framework , we search for pseudowords in a monolingual way and a bilingual synchronous way . Experiments show that pseudoword significantly outperforms word for PBSMT model in both travel translation domain and news translation domain.
1 Introduction
The pipeline of most Phrase-Based Statistical Machine Translation ( PBSMT ) systems starts from automatically word aligned parallel corpus generated from word-based models ( Brown et al , 1993), proceeds with step of induction of phrase table ( Koehn et al , 2003) or synchronous grammar ( Chiang , 2007) and with model weights tuning step . Words are taken as inputs to PBSMT at the very beginning of the pipeline . But there is a deficiency in such manner that word is too finegrained in some cases such as noncompositional phrasal equivalences , where clear word alignments do not exist . For example in Chinese-to-English translation , ??? and ? would like to ? constitute a 1-to-n phrasal equivalence , ??? ?? and ? how much is it ? constitute a m-to-n phrasal equivalence . No clear word alignments are there in such phrasal equivalences . Moreover , should basic translational unit be word or coarse-grained multiword is an open problem for optimizing SMT models.
Some researchers have explored coarse-grained translational unit for machine translation.
Marcu and Wong (2002) attempted to directly learn phrasal alignments instead of word alignments . But computational complexity is prohibitively high for the exponentially large number of decompositions of a sentence pair into phrase pairs . Cherry and Lin (2007) and Zhang et al (2008) used synchronous ITG ( Wu , 1997) and constraints to find noncompositional phrasal equivalences , but they suffered from intractable estimation problem . Blunsom et al (2008; 2009) induced phrasal synchronous grammar , which aimed at finding hierarchical phrasal equivalences.
Another direction of questioning word as basic translational unit is to directly question word segmentation on languages where word boundaries are not orthographically marked . In Chinese-to-English translation task where Chinese word boundaries are not marked , Xu et al (2004) used word aligner to build a Chinese dictionary to resegment Chinese sentence . Xu et al (2008) used a Bayesian semisupervised method that combines Chinese word segmentation model and Chinese-to-English translation model to derive a Chinese segmentation suitable for machine translation . There are also researches focusing on the impact of various segmentation tools on machine translation ( Ma et al 2007; Chang et al 2008; Zhang et al 2008). Since there are many 1-to-n phrasal equivalences in Chinese-to-English translation ( Ma and Way . 2009), only focusing on Chinese word as basic translational unit is not adequate to model 1-to-n translations . Ma and Way (2009) tackle this problem by using word aligner to bootstrap bilingual segmentation suitable for machine translation . Lambert and Banchs (2005) detect bilingual multiword ex-Spanish-English sentence pair into bilingual units , where word aligner is also used.
IBM model 3, 4, 5 ( Brown et al , 1993) and Deng and Byrne (2005) are another kind of related works that allow 1-to-n alignments , but they rarely questioned if such alignments exist in word units level , that is , they rarely questioned word as basic translational unit . Moreover , m-to-n alignments were not modeled.
This paper focuses on determining the basic translational units on both language sides without using word aligner before feeding them into PBSMT pipeline . We call such basic translational unit as pseudoword to differentiate with word.
Pseudoword is a kind of multiword expression ( includes both unary word and multiword).
Pseudoword searching problem is the same to decomposition of a given sentence into pseudowords . We assume that such decomposition is in the Gibbs distribution . We use a measurement , which characterizes pseudoword as minimal sequence of consecutive words in sense of translation , as potential function in Gibbs distribution.
Note that the number of decomposition of one sentence into pseudowords grows exponentially with sentence length . By fitting decomposition problem into parsing framework , we can find optimal pseudoword sequence in polynomial time . Then we feed pseudowords into PBSMT pipeline , and find that pseudowords as basic translational units improve translation performance over words as basic translational units . Further experiments of removing the power of higher order language model and longer max phrase length , which are inherent in pseudowords , show that pseudowords still improve translational performance significantly over unary words.
This paper is structured as follows : In section 2, we define the task of searching for pseudowords and its solution . We present experimental results and analyses of using pseudowords in PBSMT model in section 3. The conclusion is presented at section 4.
2 Searching for Pseudowords
Pseudoword searching problem is equal to decomposition of a given sentence into pseudowords . We assume that the distribution of such decomposition is in the form of Gibbs distribution as below : ) exp ( where X denotes the sentence , Y denotes a decomposition of X . Sig function acts as potential function on each multiword yk , and ZX acts as partition function . Note that the number of yk is not fixed given X because X can be decomposed into various number of multiwords.
Given X , ZX is fixed , so searching for optimal decomposition is as below : ?== k y
YY kK
SigARGMAXXYPARGMAXY where Y1K denotes K multiword units from decomposition of X . A multiword sequence with maximal sum of Sig function values is the search target ? pseudoword sequence . From (2) we can see that Sig function is vital for pseudoword searching . In this paper Sig function calculates sequence significance which is proposed to characterize pseudoword as minimal sequence of consecutive words in sense of translation . The detail of sequence significance is described in the following section.
2.1 Sequence Significance
Two kinds of definitions of sequence significance are proposed . One is monolingual sequence significance . X and Y are monolingual sentence and monolingual multiwords respectively in this monolingual scenario . The other is bilingual sequence significance . X and Y are sentence pair and multiword pairs respectively in this bilingual scenario.
2.1.1 Monolingual Sequence Significance
Given a sentence w1, ?, wn , where wi denotes unary word , monolingual sequence significance is defined as : 1,1 , , +? = ji ji ji Freq
Freq
Sig (3) where Freqi , j ( i?j ) represents frequency of word sequence wi , ?, wj in the corpus , Sigi , j represents monolingual sequence significance of a word sequence wi , ?, wj . We also denote word sequence wi , ?, wj as span[i , j ], whole sentence as span[1, n ]. Each span is also a multiword expression.
Monolingual sequence significance of span[i , j ] is proportional to span[i , j]?s frequency , while is inversely proportion to frequency of expanded span ( span[i-1, j+1]). Such definition characterizes minimal sequence of consecutive words which we are looking for . Our target is to find pseudoword sequence which has maximal sum of spans ? significances : kX kZ (1) where pw denotes pseudoword , K is equal to or less than sentence?s length . spank is the kth span of K spans span1K . Equation (4) is the rewrite of equation (2) in monolingual scenario . Searching for pseudowords pw1K is the same to finding optimal segmentation of a sentence into K segments span1K ( K is a variable too ). Details of searching algorithm are described in section 2.2.1.
We firstly search for monolingual pseudowords on source and target side individually.
Then we apply word alignment techniques to build pseudoword alignments . We argue that word alignment techniques will work fine if nonexistent word alignments in such as noncompositional phrasal equivalences have been filtered by pseudowords.
2.1.2 Bilingual Sequence Significance
Bilingual sequence significance is proposed to characterize pseudoword pairs . Cooccurrence of sequences on both language sides is used to define bilingual sequence significance . Given a bilingual sequence pair : span-pair[is , js , it , jt ] ( source side span[is , js ] and target side span[it , jt ]), bilingual sequence significance is defined as below : ,1,1,1 ,,, ,,, +?+? = ttss ttss ttss jiji jiji jiji Freq
Freq
Sig (5) where Freq denotes the frequency of a span-pair.
Bilingual sequence significance is an extension of monolingual sequence significance . Its value is proportional to frequency of span-pair[is , js , it , jt ], while is inversely proportional to frequency of expanded span-pair[is-1, js+1, it1, jt+1].
Pseudoword pairs of one sentence pair are such pairs that maximize the sum of span-pairs ? bilingual sequence significances : ? = ??= Kk pairspanpairspanK K SigARGMAXpwp 11 1 (6) pwp represents pseudoword pair . Equation (6) is the rewrite of equation (2) in bilingual scenario.
Searching for pseudoword pairs pwp1K is equal to bilingual segmentation of a sentence pair into optimal span-pair1K . Details of searching algorithm are presented in section 2.2.2.
2.2 Algorithms of Searching for Pseudowords Pseudoword searching problem is equal to decomposition of a sentence into pseudowords.
But the number of possible decompositions of the sentence grows exponentially with the sentence length in both monolingual scenario and bilingual scenario . By casting such decomposition problem into parsing framework , we can find pseudoword sequence in polynomial time.
According to the two scenarios , searching for pseudowords can be performed in a monolingual way and a synchronous way . Details of the two kinds of searching algorithms are described in the following two sections.
2.2.1 Algorithm of Searching for Monolingual Pseudowords ( SMP ) Searching for monolingual pseudowords is based on the computation of monolingual sequence significance . Figure 1 presents the search algorithm . It is performed in a way similar to
CKY ( Cocke-Kasami-Younger ) parser.

Initialization : Wi , i = Sigi , i;
Wi , j = 0, ( i?j ); 1: for d = 2 ? n do 2: for all i , j s.t . j-i=d-1 do 3: for k = i ? j ? 1 do 4: v = Wi , k + Wk+1, j 5: if v > Wi , j then 6: Wi , j = v ; 7: u = Sigi , j 8: if u > Wi , j then 9: Wi , j = u ; Figure 1. Algorithm of searching for monolingual pseudowords ( SMP).

In this algorithm , Wi , j records maximal sum of monolingual sequence significances of sub spans of span[i , j ]. During initialization , Wi , i is initialized as Sigi,i ( note that this sequence is word wi only ). For all spans that have more than one word ( i?j ), Wi , j is initialized as zero.
In the main algorithm , d represents span?s length , ranging from 2 to n , i represents start position of a span , j represents end position of a span , k represents decomposition position of span[i,j ]. For span[i , j ], Wi , j is updated if higher sum of monolingual sequence significances is found.
The algorithm is performed in a bottom-up way . Small span?s computation is first . After maximal sum of significances is found in small spans , big span?s computation , which uses small spans ? maximal sum , is continued . Maximal sum of significances for whole sentence ( W1,n , n is sentence?s length ) is guaranteed in this way , and optimal decomposition is obtained correspondingly.
150
The method of fitting the decomposition problem into CKY parsing framework is located at steps 79. After steps 36, all possible decompositions of span[i , j ] are explored and Wi , j of optimal decomposition of span[i , j ] is recorded.
Then monolingual sequence significance Sigi,j of span[i , j ] is computed at step 7, and it is compared to Wi , j at step 8. Update of Wi , j is taken at step 9 if Sigi,j is bigger than Wi , j , which indicates that span[i , j ] is non-decomposable . Thus whether span[i , j ] should be non-decomposable or not is decided through steps 79.
2.2.2 Algorithm of Synchronous Searching for Pseudowords ( SSP ) Synchronous searching for pseudowords utilizes bilingual sequence significance . Figure 2 presents the search algorithm . It is similar to ITG ( Wu , 1997), except that it has no production rules and nonterminal nodes of a synchronous grammar . What it cares about is the span-pairs that maximize the sum of bilingual sequence significances.

Initialization : if is = js or it = jt then ttssttss ttss jijijiji SigW ,,,,,, = ; else 0,,, = jijiW ; 1: for ds = 2 ? ns , dt = 2 ? nt do 2: for all is , js , it , jt s.t . js-is=ds-1 and jt-it=dt-1 do 3: for ks = is ? js ? 1, kt = it ? jt ? 1 do 4: v = max { , ttssttss jkjkkiki
WW ,1,,1,,, +++ ttsst tjiji ,,, tj ,,, tj ,,, jiji ,,, tss kijkjkki
WW ,,,1,1,, ++ + } 5: if v > W then tss 6: W = v ; tss iji 7: u = ttss jiji
Sig ,,, 8: if u > W then tss iji 9: W = u ; ttss Figure 2. Algorithm of Synchronous Searching for
Pseudo-words(SSP).

In the algorithm , records maximal sum of bilingual sequence significances of sub span-pairs of span-pair[i ttss jiji
W ,,, s , js , it , jt ]. For 1-to-m span-pairs , Ws are initialized as bilingual sequence significances of such span-pairs . For other span-pairs , Ws are initialized as zero.
In the main algorithm , ds/dt denotes the length of a span on source/target side , ranging from 2 to ns/nt ( source/target sentence?s length ). is/it is the start position of a span-pair on source/target side , js/jt is the end position of a span-pair on source/target side , ks/kt is the decomposition position of a span-pair[is , js , it , jt ] on source/target side.
Update steps in Figure 2 are similar to that of Figure 1, except that the update is about span-pairs , not monolingual spans . Reversed and non-reversed alignments inside a span-pair are compared at step 4. For span-pair[is , js , it , jt ], is updated at step 6 if higher sum of bilingual sequence significances is found.
ttss jiji
W ,,,
Fitting the bilingually searching for pseudowords into ITG framework is located at steps 79.
Steps 36 have explored all possible decompositions of span-pair[is , js , it , jt ] and have recorded maximal ttss of these decompositions . Then bilingual sequence significance of span-pair[i jijiW ,,, s , js , it , jt ] is computed at step 7. It is compared to ttss at step 8. Update is taken at step 9 if bilingual sequence significance of span-pair[i jijiW ,,, s , js , it , jt ] is bigger than ttss , which indicates that span-pair[i jijiW ,,, s , js , it , jt ] is non-decomposable.
Whether the span-pair[is , js , it , jt ] should be non-decomposable or not is decided through steps 79.
In addition to the initialization step , all span-pairs ? bilingual sequence significances are computed . Maximal sum of bilingual sequence significances for one sentence pair is guaranteed through this bottom-up way , and the optimal decomposition of the sentence pair is obtained correspondingly.
z Algorithm of Excluded Synchronous
Searching for Pseudowords ( ESSP)
The algorithm of SSP in Figure 2 explores all span-pairs , but it neglects NULL alignments , where words and ? empty ? word are aligned . In fact , SSP requires that all parts of a sentence pair should be aligned . This requirement is too strong because NULL alignments are very common in many language pairs . In SSP , words that should be aligned to ? empty ? word are programmed to be aligned to real words.
Unlike most word alignment methods ( Och and Ney , 2003) that add ? empty ? word to account for NULL alignment entries , we propose a method to naturally exclude such NULL alignments . We call this method as Excluded Synchronous Searching for Pseudowords ( ESSP).
The main difference between ESSP and SSP is in steps 36 in Figure 3. We illustrate Figure 3?s span-pair configuration in Figure 4.
151
Initialization : if is = js or it = jt then ttssttss jijijiji ,,,,,, ,,, jijiW
SigW = ; else 0= ttss ; 1: for ds = 2 ? ns , dt = 2 ? nt do 2: for all is , js , it , jt s.t . js-is=ds-1 and jt-it=dt-1 do 3: for ks1=is+1 ? js , ks2=ks1-1 ? js-1 kt1=it+1 ? jt , kt2=kt1-1 ? jt-1 do 4: v = max{W , ttssttss jkjkkiki
W ,1,,11,,1, 2211 ++?? + 1,,,1,1, 122 ?++ + ttsstt kijkjk W tt j ,,, tj,,,
Sig tt ji ,,, ttss jiji ,,, 1, 1?ss kiW } 5: if v > W then ss iji 6: W = v ; tss iji 7: u = ttss jiji ,,, 8: if u > W then ss ji 9: W = u ; Figure 3. Algorithm of Excluded Synchronous
Searching for Pseudowords ( ESSP).

The solid boxes in Figure 4 represent excluded parts of span-pair[is , js , it , jt ] in ESSP . Note that , in SSP , there is no excluded part , that is , ks1=ks2 and kt1=kt2.
We can see that in Figure 4, each monolingual span is configured into three parts , for example : span[is , ks1-1], span[ks1, ks2] and span[ks2+1, js ] on source language side . ks1 and ks2 are two new variables gliding between is and js , span[ks1, ks2] is source side excluded part of span-pair[is , js , it , jt ]. Bilingual sequence significance is computed only on pairs of blank boxes , solid boxes are excluded in this computation to represent NULL alignment cases.

Figure 4. Illustration of excluded configuration.

Note that , in Figure 4, solid box on either language side can be void ( i.e ., length is zero ) if there is no NULL alignment on its side . If all solid boxes are shrunk into void , algorithm of
ESSP is the same to SSP.
Generally , span length of NULL alignment is not very long , so we can set a length threshold for NULL alignments , eg . ks2-ks1?EL , where EL denotes Excluded Length threshold . Computational complexity of the ESSP remains the same to SSP?s complexity O(ns3.nt3), except multiply a constant EL2.
There is one kind of NULL alignments that ESSP can not consider . Since we limit excluded parts in the middle of a span-pair , the algorithm will end without considering boundary parts of a sentence pair as NULL alignments.
3 Experiments and Results
In our experiments , pseudowords are fed into PBSMT pipeline . The pipeline uses GIZA ++ model 4 ( Brown et al , 1993; Och and Ney , 2003) for pseudoword alignment , uses Moses ( Koehn et al , 2007) as phrasebased decoder , uses the SRI Language Modeling Toolkit to train language model with modified Kneser-Ney smoothing ( Kneser and Ney 1995; Chen and Goodman 1998). Note that MERT ( Och , 2003) is still on original words of target language . In our experiments , pseudoword length is limited to no more than six unary words on both sides of the language pair.
We conduct experiments on Chinese-to-
English machine translation . Two data sets are adopted , one is small corpus of IWSLT-2008 BTEC task of spoken language translation in travel domain ( Paul , 2008), the other is large corpus in news domain , which consists Hong Kong News ( LDC2004T08), Sinorama Magazine ( LDC2005T10), FBIS ( LDC2003E14), Xinhua ( LDC2002E18), Chinese News Translation ( LDC2005T06), Chinese Treebank ( LDC2003E07), Multiple Translation Chinese ( LDC2004T07). Table 1 lists statistics of the corpus used in these experiments.
is ks1 ks2 js it kt1 kt2 jt is ks1 ks2 js it kt1 kt2 jt a ) non-reversed b ) reversed small large
Ch ? En Ch ? En
Sent . 23k 1,239k word 190k 213k 31.7m 35.5m
ASL 8.3 9.2 25.6 28.6
Table 1. Statistics of corpora , ? Ch ? denotes Chinese , ? En ? denotes English , ? Sent .? row is the number of sentence pairs , ? word ? row is the number of words , ? ASL ? denotes average sentence length.
opment set , use IWSLT08 official test set for test.
A 5gram language model is trained on English side of parallel corpus . For large corpus , we use NIST02 as development set , use NIST03 as test set . Xinhua portion of the English Gigaword3 corpus is used together with English side of large corpus to train a 4gram language model.
Experimental results are evaluated by case-insensitive BLEU4 ( Papineni et al , 2001).
Closest reference sentence length is used for brevity penalty . Additionally , NIST score ( Doddington , 2002) and METEOR ( Banerjee and Lavie , 2005) are also used to check the consistency of experimental results . Statistical significance in BLEU score differences was tested by paired bootstrap resampling ( Koehn , 2004).
3.1 Baseline Performance
Our baseline system feeds word into PBSMT pipeline . We use GIZA ++ model 4 for word alignment , use Moses for phrasebased decoding.
The setting of language model order for each corpus is not changed . Baseline performances on test sets of small corpus and large corpus are reported in table 2.
small Large
BLEU 0.4029 0.3146
NIST 7.0419 8.8462
METEOR 0.5785 0.5335
Table 2. Baseline performances on test sets of small corpus and large corpus.
3.2 Pseudoword Unpacking
Because pseudoword is a kind of multiword expression , it has inborn advantage of higher language model order and longer max phrase length over unary word . To see if such inborn advantage is the main contribution to the performance or not , we unpack pseudoword into words after GIZA ++ aligning . Aligned pseudowords are unpacked into m?n word alignments.
PBSMT pipeline is executed thereafter . The advantage of longer max phrase length is removed during phrase extraction , and the advantage of higher order of language model is also removed during decoding since we use language model trained on unary words . Performances of pseudoword unpacking are reported in section 3.3.1 and 3.4.1. Ma and Way (2009) used the unpacking after phrase extraction , then reestimated phrase translation probability and lexical reordering model . The advantage of longer max phrase length is still used in their method.
3.3 Pseudoword Performances on Small
Corpus
Table 3 presents performances of SMP , SSP , ESSP on small data set . pwchpwen denotes that pseudowords are on both language side of training data , and they are input strings during development and testing , and translations are also pseudowords , which will be converted to words as final output . wchpwen/pwchwen denotes that pseudowords are adopted only on Eng-lish/Chinese side of the data set.
We can see from table 3 that , ESSP attains the best performance , while SSP attains the worst performance . This shows that excluding NULL alignments in synchronous searching for pseudowords is effective . SSP puts overly strong alignment constraints on parallel corpus , which impacts performance dramatically . ESSP is superior to SMP indicating that bilingually motivated searching for pseudowords is more effective.
Both SMP and ESSP outperform baseline consistently in BLEU , NIST and METEOR.
There is a common phenomenon among SMP,
SSP and ESSP . wchpwen always performs better than the other two cases . It seems that Chinese word prefers to have English pseudoword equivalence which has more than or equal to one word . pwchpwen in ESSP performs similar to the baseline , which reflects that our direct pseudoword pairs do not work very well with GIZA ++ alignments . Such disagreement is weakened by using pseudowords on only one language side ( wchpwen or pwchwen ), while the advantage of pseudowords is still leveraged in the alignments.
Best ESSP ( wchpwen ) is significantly better than baseline ( p<0.01) in BLEU score , best SMP ( wchpwen ) is significantly better than baseline ( p<0.05) in BLEU score . This indicates that pseudowords , through either monolingual searching or synchronous searching , are more effective than words as to being basic translational units.
Figure 5 illustrates examples of pseudowords of one Chinese-to-English sentence pair . Gold standard word alignments are shown at the bottom of figure 5. We can see that ? front desk ? is recognized as one pseudoword in ESSP . Because SMP performs monolingually , it can not consider ???? and ? front desk ? simultaneously.
SMP only detects frequent monolingual multiwords as pseudowords . SSP has a strong constraint that all parts of a sentence pair should be aligned , so source sentence and target sentence have same length after merging words into pseudowords . We can see that too many pseudowords are detected by SSP.

Figure 5. Outputs of the three algorithms ESSP , SMP and SSP on one sentence pair and gold standard word alignments . Words in one pseudoword are concatenated by ?_?.
3.3.1 Pseudoword Unpacking Performances on Small Corpus We test pseudoword unpacking in ESSP . Table 4 presents its performances on small corpus.
unpackingESSP pwchpwen wchpwen pwchwen baseline
BLEU 0.4097 0.4182 0.4031 0.4029
NIST 7.5547 7.2893 7.2670 7.0419
METEOR 0.5951 0.5874 0.5846 0.5785
Table 4. Performances of pseudoword unpacking on small corpus.

We can see that pseudoword unpacking significantly outperforms baseline . wchpwen is significantly better than baseline ( p<0.04) in BLEU score . Unpacked pseudoword performs comparatively with pseudoword without unpacking.
There is no statistical difference between them . It shows that the improvement derives from pseudoword itself as basic translational unit , does not rely very much on higher language model order or longer max phrase length setting.
3.4 Pseudoword Performances on Large
Corpus
Table 5 lists the performance of using pseudowords on large corpus . We apply SMP on this task . ESSP is not applied because of its high computational complexity . Table 5 shows that all three configurations ( pwchpwen , wchpwen , pwchwen ) of SMP outperform the baseline . If we go back to the definition of sequence significance , we can see that it is a datadriven definition that utilizes corpus frequencies . Corpus scale has an influence on computation of sequence significance in long sentences which appear frequently in news domain . SMP benefits from large corpus , and wchpwen is significantly better than baseline ( p<0.01). Similar to performances on small corpus , wchpwen always performs better than the other two cases , which indicates that Chinese word prefers to have English pseudoword equivalence which has more than or equal to one word.

SMP pwchpwen wchpwen pwchwen baseline
BLEU 0.3185 0.3230 0.3166 0.3146
NIST 8.9216 9.0447 8.9210 8.8462
METEOR 0.5402 0.5489 0.5435 0.5335
Table 5. Performance of using pseudowords on large corpus.
3.4.1 Pseudoword Unpacking Performances on Large Corpus Table 6 presents pseudoword unpacking performances on large corpus . All three configurations improve performance over baseline after pseudoword unpacking . pwchpwen attains the best BLEU among the three configurations , and is significantly better than baseline ( p<0.03).
wchpwen is also significantly better than baseline ( p<0.04). By comparing table 6 with table 5, we can see that unpacked pseudoword performs comparatively with pseudoword without unpacking . There is no statistical difference be-
SMP SSP ESSP pwchpwen wchpwen pwchwen pwchpwen wchpwen pwchwen pwchpwen wchpwen pwchwen baseline BLEU 0.3996 0.4155 0.4024 0.3184 0.3661 0.3552 0.3998 0.4229 0.4147 0.4029 NIST 7.4711 7.6452 7.6186 6.4099 6.9284 6.8012 7.1665 7.4373 7.4235 7.0419 METEOR 0.5900 0.6008 0.6000 0.5255 0.5569 0.5454 0.5739 0.5963 0.5891 0.5785 ?? ? ?? ? ? ?? ? The guy at the front desk is pretty rude .
?? ? ?? ? ? ?? ?
The guy_at the front_desk is pretty_rude .
?? ? ?? ? ? ?? ?
The guy at the front_desk is pretty rude .
ESSP ?? ? ?? ? ? ?? ?
The guy at the front desk is pretty rude .
Gold standard word alignments
SMP
SSP rives from pseudoword itself as basic translational unit , does not rely very much on higher language model order or longer max phrase length setting . In fact , slight improvement in pwchpwen and pwchwen is seen after pseudoword unpacking , which indicates that higher language model order and longer max phrase length impact the performance in these two configurations.

UnpackingSMP pwchpwen wchpwen pwchwen
Baseline
BLEU 0.3219 0.3192 0.3187 0.3146
NIST 8.9458 8.9325 8.9801 8.8462
METEOR 0.5429 0.5424 0.5411 0.5335
Table 6. Performance of pseudoword unpacking on large corpus.
3.5 Comparison to English Chunking
English chunking is experimented to compare with pseudoword . We use FlexCRFs ( Xuan-Hieu Phan et al , 2005) to get English chunks.
Since there is no standard Chinese chunking data and code , only English chunking is executed.
The experimental results show that English chunking performs far below baseline , usually 8 absolute BLEU points below . It shows that simple chunks are not suitable for being basic translational units.
4 Conclusion
We have presented pseudoword as a novel machine translational unit for phrasebased machine translation . It is proposed to replace too finegrained word as basic translational unit . Pseudoword is a kind of basic multiword expression that characterizes minimal sequence of consecutive words in sense of translation . By casting pseudoword searching problem into a parsing framework , we search for pseudowords in polynomial time . Experimental results of Chinese-to-English translation task show that , in phrasebased machine translation model , pseudoword performs significantly better than word in both spoken language translation domain and news domain . Removing the power of higher order language model and longer max phrase length , which are inherent in pseudowords , shows that pseudowords still improve translational performance significantly over unary words.
References
S . Banerjee , and A . Lavie . 2005. METEOR : An automatic metric for MT evaluation with improved correlation with human judgments . In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization ( ACL?05). 65?72.
P . Blunsom , T . Cohn , C . Dyer , M . Osborne . 2009. A
Gibbs Sampler for Phrasal Synchronous
Grammar Induction . In Proceedings of ACL-
IJCNLP , Singapore.
P . Blunsom , T . Cohn , M . Osborne . 2008. Bayesian synchronous grammar induction . In Proceedings of NIPS 21, Vancouver , Canada.
P . Brown , S . Della Pietra , V . Della Pietra , and R.
Mercer . 1993. The mathematics of machine translation : Parameter estimation . Computational Linguistics , 19:263?312.
P.-C . Chang , M . Galley , and C . D . Manning . 2008.
Optimizing Chinese word segmentation for machine translation performance . In Proceedings of the 3rd Workshop on Statistical Machine
Translation ( SMT?08). 224?232.
Chen , Stanley F . and Joshua Goodman . 1998. An empirical study of smoothing techniques for language modeling . Technical Report TR-10-98, Harvard University Center for Research in Computing Technology.
C . Cherry , D . Lin . 2007. Inversion transduction grammar for joint phrasal translation modeling . In Proc . of the HLTNAACL Workshop on Syntax and Structure in Statistical Translation ( SSST 2007), Rochester , USA.
D . Chiang . 2007. Hierarchical phrasebased translation.Computational Linguistics , 33(2):201? 228.
Y . Deng and W . Byrne . 2005. HMM word and phrase alignment for statistical machine translation . In Proc . of HLTEMNLP , pages 169?176.
G . Doddington . 2002. Automatic evaluation of machine translation quality using ngram cooccurrence statistics . In Proceedings of the 2nd International Conference on Human Language Technology ( HLT?02). 138?145.
Kneser , Reinhard and Hermann Ney . 1995. Improved backing-off for M-gram language modeling . In Proceedings of the IEEE International Conference on Acoustics , Speech , and Signal Processing , pages 181?184, Detroit , MI.
P . Koehn , H . Hoang , A . Birch , C . Callison-Burch , M.
Federico , N . Bertoldi , B . Cowan,W . Shen , C.
Moran , R . Zens , C . Dyer , O . Bojar , A . Constantin , E . Herbst . 2007. Moses : Open source toolkit for statistical machine translation . In Proc . of the
Prague.
P . Koehn , F . J . Och , D . Marcu . 2003. Statistical phrasebased translation . In Proc . of the 3rd International conference on Human Language Technology Research and 4th Annual Meeting of the NAACL ( HLTNAACL 2003), 81?88, Edmonton,
Canada.
P . Koehn . 2004. Statistical Significance Tests for Machine Translation Evaluation . In Proceedings of EMNLP.
P . Lambert and R . Banchs . 2005. Data Inferred Multiword Expressions for Statistical Machine Translation . In Proceedings of MT Summit
X.
Y . Ma , N . Stroppa , and A . Way . 2007. Bootstrapping word alignment via word packing . In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics ( ACL?07).
304?311.
Y . Ma , and A . Way . 2009. Bilingually Motivated Word Segmentation for Statistical Machine Translation . In ACM Transactions on Asian Language Information Processing , 8(2).
D . Marcu,W.Wong . 2002. A phrasebased , joint probability model for statistical machine translation . In Proc . of the 2002 Conference on Empirical Methods in Natural Language Processing ( EMNLP2002), 133?139, Philadelphia . Association for Computational Linguistics.
F . J . Och . 2003. Minimum error rate training in statistical machine translation . In Proc . of ACL , pages 160?167.
F . J . Och and H . Ney . 2003. A systematic comparison of various statistical alignment models.
Computational Linguistics , 29(1):19?51.
Xuan-Hieu Phan , Le-Minh Nguyen , and Cam-Tu Nguyen . 2005. FlexCRFs : Flexible Conditional Random Field Toolkit , http://flexcrfs.sourceforge.
net
K . Papineni , S . Roukos , T . Ward , W . Zhu . 2001. Bleu : a method for automatic evaluation of machine translation , 2001.
M . Paul , 2008. Overview of the IWSLT 2008 evaluation campaign . In Proc . of Internationa Workshop on Spoken Language Translation , 2021
October 2008.
A . Stolcke . (2002). SRILM - an extensible language modeling toolkit . In Proceedings of
ICSLP , Denver , Colorado.
D . Wu . 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora . Computational Linguistics , 23(3):377? 403.
J . Xu , Zens ., and H . Ney . 2004. Do we need Chinese word segmentation for statistical machine translation ? In Proceedings of the ACL Workshop on Chinese Language Processing
SIGHAN?04). 122?128.
J . Xu , J . Gao , K . Toutanova , and H . Ney . 2008.
Bayesian semisupervised chinese word segmentation for statistical machine translation.
In Proceedings of the 22nd International Conference on Computational Linguistics ( COLING?08).
1017?1024.
H . Zhang , C . Quirk , R . C . Moore , D . Gildea . 2008.
Bayesian learning of noncompositional phrases with synchronous parsing . In Proc . of the 46th Annual Conference of the Association for Computational Linguistics : Human Language Technologies ( ACL-08:HLT ), 97?105, Columbus,
Ohio.
R . Zhang , K . Yasuda , and E . Sumita . 2008. Improved statistical machine translation by multiple Chinese word segmentation . In Proceedings of the 3rd Workshop on Statistical Machine Translation ( SMT?08). 216?223.

