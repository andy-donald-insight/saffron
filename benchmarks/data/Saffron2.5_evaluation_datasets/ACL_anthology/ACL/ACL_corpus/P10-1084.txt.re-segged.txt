Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics , pages 815?824,
Uppsala , Sweden , 1116 July 2010. c?2010 Association for Computational Linguistics
A Hybrid Hierarchical Model for Multi-Document Summarization
Asli Celikyilmaz
Computer Science Department
University of California , Berkeley
asli@eecs.berkeley.edu
Dilek Hakkani-Tur
International Computer Science Institute
Berkeley , CA
dilek@icsi.berkeley.edu
Abstract
Scoring sentences in documents given abstract summaries created by humans is important in extractive multidocument summarization . In this paper , we formulate extractive summarization as a two step learning problem building a generative model for pattern discovery and a regression model for inference . We calculate scores for sentences in document clusters based on their latent characteristics using a hierarchical topic model . Then , using these scores , we train a regression model based on the lexical and structural characteristics of the sentences , and use the model to score sentences of new documents to form a summary . Our system advances current state-of-the-art improving ROUGE scores by ?7%. Generated summaries are less redundant and more coherent based upon manual quality evaluations.
1 Introduction
Extractive approach to multidocument summarization ( MDS ) produces a summary by selecting sentences from original documents . Document Understanding Conferences ( DUC ), now TAC , fosters the effort on building MDS systems , which take document clusters ( documents on a same topic ) and description of the desired summary focus as input and output a word length limited summary . Human summaries are provided for training summarization models and measuring the performance of machine generated summaries.
Extractive summarization methods can be classified into two groups : supervised methods that rely on provided document-summary pairs , and unsupervised methods based upon properties derived from document clusters . Supervised methods treat the summarization task as a classifica-tion/regression problem , e.g ., ( Shen et al , 2007; Yeh et al , 2005). Each candidate sentence is classified as summary or non-summary based on the features that they pose and those with highest scores are selected . Unsupervised methods aim to score sentences based on semantic groupings extracted from documents , e.g ., ( Daume?III and Marcu , 2006; Titov and McDonald , 2008; Tang et al , 2009; Haghighi and Vanderwende , 2009; Radev et al , 2004; Branavan et al , 2009), etc . Such models can yield comparable or better performance on DUC and other evaluations , since representing documents as topic distributions rather than bags of words diminishes the effect of lexical variability . To the best of our knowledge , there is no previous research which utilizes the best features of both approaches for MDS as presented in this paper.
In this paper , we present a novel approach that formulates MDS as a prediction problem based on a two-step hybrid model : a generative model for hierarchical topic discovery and a regression model for inference . We investigate if a hierarchical model can be adopted to discover salient characteristics of sentences organized into hierarchies utilizing human generated summary text.
We present a probabilistic topic model on sentence level building on hierarchical Latent Dirichlet Allocation ( hLDA ) ( Blei et al , 2003a ), which is a generalization of LDA ( Blei et al , 2003b ). We construct a hybrid learning algorithm by extracting salient features to characterize summary sentences , and implement a regression model for inference ( Fig.3). Contributions of this work are : ? construction of hierarchical probabilistic model designed to discover the topic structures of all sentences . Our focus is on identifying similarities of candidate sentences to summary sentences using a novel tree based sentence scoring algorithm , concerning topic distributions at different levels of the discovered hierarchy as described in ? 3 and ? 4, ? representation of sentences by meta-features to mary text . Our aim is to find features that can best represent summary sentences as described in ? 5, ? implementation of a feasible inference method based on a regression model to enable scoring of sentences in test document clusters without retraining , ( which has not been investigated in generative summarization models ) described in ? 5.2.
We show in ? 6 that our hybrid summarizer achieves comparable ( if not better ) ROUGE score on the challenging task of extracting the summaries of multiple newswire documents . The human evaluations confirm that our hybrid model can produce coherent and nonredundant summaries.
2 Background and Motivation
There are many studies on the principles governing multidocument summarization to produce coherent and semantically relevant summaries . Previous work ( Nenkova and Vanderwende , 2005; Conroy et al , 2006), focused on the fact that frequency of words plays an important factor . While , earlier work on summarization depend on a word score function , which is used to measure sentence rank scores based on ( semi-)supervised learning methods , recent trend of purely datadriven methods , ( Barzilay and Lee , 2004; Daume?III and Marcu , 2006; Tang et al , 2009; Haghighi and Vanderwende , 2009), have shown remarkable improvements . Our work builds on both methods by constructing a hybrid approach to summarization.
Our objective is to discover from document clusters , the latent topics that are organized into hierarchies following ( Haghighi and Vanderwende , 2009). A hierarchical model is particularly appealing to summarization than a ? flat ? model , e.g.
LDA ( Blei et al , 2003b ), in that one can discover ? abstract ? and ? specific ? topics . For instance , discovering that ? baseball ? and ? football ? are both contained in an abstract class ? sports ? can help to identify summary sentences . It follows that summary topics are commonly shared by many documents , while specific topics are more likely to be mentioned in rather a small subset of documents.
Feature based learning approaches to summarization methods discover salient features by measuring similarity between candidate sentences and summary sentences ( Nenkova and Vanderwende , 2005; Conroy et al , 2006). While such methods are effective in extractive summarization , the fact that some of these methods are based on greedy algorithms can limit the application areas . Moreover , using information on the hidden semantic structure of document clusters would improve the performance of these methods.
Recent studies focused on the discovery of latent topics of document sets in extracting summaries . In these models , the challenges of inferring topics of test documents are not addressed in detail . One of the challenges of using a previously trained topic model is that the new document might have a totally new vocabulary or may include many other specific topics , which may or may not exist in the trained model . A common method is to rebuild a topic model for new sets of documents ( Haghighi and Vanderwende , 2009), which has proven to produce coherent summaries.
An alternative yet feasible solution , presented in this work , is building a model that can summarize new document clusters using characteristics of topic distributions of training documents . Our approach differs from the early work , in that , we combine a generative hierarchical model and regression model to score sentences in new documents , eliminating the need for building a generative model for new document clusters.
3 Summary-Focused Hierarchical Model
Our MDS system , hybrid hierarchical summarizer , HybHSum , is based on an hybrid learning approach to extract sentences for generating summary . We discover hidden topic distributions of sentences in a given document cluster along with provided summary sentences based on hLDA described in ( Blei et al , 2003a)1. We build a summary-focused hierarchical probabilistic topic model , sumHLDA , for each document cluster at sentence level , because it enables capturing expected topic distributions in given sentences directly from the model . Besides , document clusters contain a relatively small number of documents , which may limit the variability of topics if they are evaluated on the document level . As described in ? 4, we present a new method for scoring candidate sentences from this hierarchical structure.
Let a given document cluster D be represented with sentences O={om}|O|m=1 and its corresponding human summary be represented with sentences S={sn}|S|n=1. All sentences are comprised of words
V = { w1, w2, .. w|V | } in { O ? S}.
1Please refer to ( Blei et al , 2003b ) and ( Blei et al , 2003a ) for details and demonstrations of topic models.
816
Summary hLDA ( sumHLDA ): The hLDA represents distribution of topics in sentences by organizing topics into a tree of a fixed depth L ( Fig.1.a ). Each candidate sentence om is assigned to a path com in the tree and each word wi in a given sentence is assigned to a hidden topic zom at a level l of com . Each node is associated with a topic distribution over words . The sampler method alternates between choosing a new path for each sentence through the tree and assigning each word in each sentence to a topic along that path . The structure of tree is learnt along with the topics using a nested Chinese restaurant process ( nCRP ) ( Blei et al , 2003a ), which is used as a prior.
The nCRP is a stochastic process , which assigns probability distributions to infinitely branching and infinitely deep trees . In our model , nCRP specifies a distribution of words into paths in an L-level tree . The assignments of sentences to paths are sampled sequentially : The first sentence takes the initial L-level path , starting with a single branch tree . Later , mth subsequent sentence is assigned to a path drawn from the distribution : p(pathold , c|m,mc ) = mc ?+ m?1 p(pathnew , c|m,mc ) = ? ?+ m?1 (1) pathold and pathnew represent an existing and novel ( branch ) path consecutively , mc is the number of previous sentences assigned to path c , m is the total number of sentences seen so far , and ? is a hyperparameter which controls the probability of creating new paths . Based on this probability each node can branch out a different number of child nodes proportional to ?. Small values of ? suppress the number of branches.
Summary sentences generally comprise abstract concepts of the content . With sumHLDA we want to capture these abstract concepts in candidate sentences . The idea is to represent each path shared by similar candidate sentences with representative summary sentence(s ). We let summary sentences share existing paths generated by similar candidate sentences instead of sampling new paths and influence the tree structure by introducing two separate hyperparameters for nCRP prior : ? if a summary sentence is sampled , use ? = ? s , ? if a candidate sentence is sampled , use ? = ? o.
At each node , we let summary sentences sample a path by choosing only from the existing children of that node with a probability proportional to the number of other sentences assigned to that child.
This can be achieved by using a small value for ? s (0 < ? s ? 1). We only let candidate sentences to have an option of creating a new child node with a probability proportional to ? o . By choosing ? s ? ? o we suppress the generation of new branches for summary sentences and modify the ? of nCRP prior in Eq.(1) using ? s and ? o hyperparameters for different sentence types . In the experiments , we discuss the effects of this modification on the hierarchical topic tree.
The following is the generative process for sumHLDA used in our HybHSum : (1) For each topic k ? T , sample a distribution ? k v Dirichlet(?).
(2) For each sentence d ? { O ? S }, ( a ) if d ? O , draw a path cd v nCRP(?o ), else if d ? S , draw a path cd v nCRP(?s).
(b ) Sample L-vector ? d mixing weights from
Dirichlet distribution ? d ? Dir(?).
(c ) For each word n , choose : ( i ) level zd,n|?d and ( ii ) word wd,n | { zd,n , cd , ?} Given sentence d , ? d is a vector of topic proportions from L dimensional Dirichlet parameterized by ? ( distribution over levels in the tree .) The nth word of d is sampled by first choosing a level zd,n = l from the discrete distribution ? d with probability ? d,l . Dirichlet parameter ? and ? o control the size of tree effecting the number of topics.
(Small values of ? s do not effect the tree .) Large values of ? favor more topics ( Blei et al , 2003a).
Model Learning : Gibbs sampling is a common method to fit the hLDA models . The aim is to obtain the following samples from the posterior of : ( i ) the latent tree T , ( ii ) the level assignment z for all words , ( iii ) the path assignments c for all sentences conditioned on the observed words w.
Given the assignment of words w to levels z and assignments of sentences to paths c , the expected posterior probability of a particular word w at a given topic z=l of a path c=c is proportional to the number of times w was generated by that topic : p(w|z , c,w , ?) ? n(z=l,c=c,w=w ) + ? (2) Similarly , posterior probability of a particular topic z in a given sentence d is proportional to number of times z was generated by that sentence : p(z|z , c , ?) ? n(c=cd,z=l ) + ? (3) n (.) is the count of elements of an array satisfying the condition . Note from Eq.(3) that two sentences d1 and d2 on the same path c would have probabilities . Posterior probabilities are normalized with total counts and their hyperparameters.
4 Tree-Based Sentence Scoring
The sumHLDA constructs a hierarchical tree structure of candidate sentences ( per document cluster ) by positioning summary sentences on the tree . Each sentence is represented by a path in the tree , and each path can be shared by many sentences . The assumption is that sentences sharing the same path should be more similar to each other because they share the same topics . Moreover , if a path includes a summary sentence , then candidate sentences on that path are more likely to be selected for summary text . In particular , the similarity of a candidate sentence om to a summary sentence sn sharing the same path is a measure of strength , indicating how likely om is to be included in the generated summary ( Algorithm 1): Let com be the path for a given om . We find summary sentences that share the same path with om via : M = { sn ? S|csn = com }. The score of each sentence is calculated by similarity to the best matching summary sentence in M : score(om ) = maxsn?M sim(om , sn ) (4) If M =?, then score(om )=?. The efficiency of our similarity measure in identifying the best matching summary sentence , is tied to how expressive the extracted topics of our sumHLDA models are.
Given path com , we calculate the similarity of om to each sn , n=1..|M | by measuring similarities on : ? sparse unigram distributions ( sim1) at each topic l on com : similarity between p(wom,l|zom = l , com , vl ) and p(wsn,l|zsn = l , com , vl ) ?? distributions of topic proportions ( sim2); similarity between p(zom | com ) and p(zsn | com).
? sim1: We define two sparse ( discrete ) unigram distributions for candidate om and summary sn at each node l on a vocabulary identified with words generated by the topic at that node , vl ? V . Given wom = { w1, ..., w|om | } , let wom,l ? wom be the set of words in om that are generated from topic zom at level l on path com . The discrete unigram distribution poml = p(wom,l|zom = l , com , vl ) represents the probability over all words vl assigned to topic zom at level l , by sampling only for words in wom,l . Similarly , psn,l = p(wsn,l|zsn , com , vl ) is the probability of words wsn in sn of the same topic . The probability of each word in pom,l and psn,l are obtained using Eq . (2) and then normalized ( see Fig.1.b).
Algorithm 1 Tree-Based Sentence Scoring 1: Given tree T from sumHLDA , candidate and summary sentences : O = { o1, ..., om } , S = { s1, ..., sn } 2: for sentences m ? 1, ..., | O | do 3: - Find path com on tree T and summary sentences 4: on path com : M = { sn ? S|csn = com } 5: for summary sentences n ? 1, ..., | M | do 6: - Find score(om)=maxsn sim(om , sn ), 7: where sim(om , sn ) = sim1 ? sim2 8: using Eq.(7) and Eq.(8) 9: end for 10: end for 11: Obtain scores Y = { score(om )} | O | m=1 The similarity between pom,l and psn,l is obtained by first calculating the divergence with information radius - IR based on Kullback-Liebler(KL ) divergence , p=pom,l , q=psn,l :
IRcom , l(pom,l , psn,l)=KL(p || p+q 2 )+ KL(q || p+q 2 ) (5) where , KL(p||q)=Pi pi log pi qi . Then the divergence is transformed into a similarity measure ( Manning and Schuetze , 1999):
Wcom,l(pom,l , psn,l ) = 10 ? IRcom,l(pom,l,psn,l ) (6) IR is a measure of total divergence from the average , representing how much information is lost when two distributions p and q are described in terms of average distributions . We opted for IR instead of the commonly used KL because with IR there is no problem with infinite values since pi+qi 2 6=0 if either pi 6=0 or qi 6=0. Moreover , unlike KL , IR is symmetric , i.e ., KL(p,q ) 6=KL(q,p).
Finally sim1 is obtained by average similarity of sentences using Eq.(6) at each level of com by : sim1(om , sn ) = 1L ? L l=1 Wcom , l(pom,l , psn,l ) ? l (7) The similarity between pom,l and psn,l at each level is weighted proportional to the level l because the similarity between sentences should be rewarded if there is a specific word overlap at child nodes.
?sim2: We introduce another measure based on sentence-topic mixing proportions to calculate the concept-based similarities between om and sn.
We calculate the topic proportions of om and sn , represented by pzom = p(zom | com ) and pzsn = p(zsn | com ) via Eq.(3). The similarity between the distributions is then measured with transformed IR document cluster on ? global warming ?. ( Duc06) z z
Posterior Topic
Distributions v z1 z .
.
.
.
.
.
.
.
.
w .
.
.
.
.
.
.
w z .
.
.
.
.
.
w candidate o m summary s n ( b ) Magnified view of sample path c [ z o m ={ w n ={ w 6, w ...
...
z
K1 z
K z warming incidence research global predict health change disease forecast temperature slow malaria sneeze starving middle-east siberia o m : ? Global s n :? Global level:3 level:1 level:2 v z1 v z2 v z2 v z3 v z3 w w w w w .
w .
p o m z p s n z p(w | z s n,1 s n p(w | z o m,1 o m p(w | z s n,2 s n p(w | z o m,2 o m p(w | z s n,3 s n p(w | z o m,3 o m Figure 1: ( a ) A sample 3-level tree using sumHLDA . Each sentence is associated with a path c through the hierarchy , where each node zl,c is associated with a distribution over terms ( Most probable terms are illustrated ). ( b ) magnified view of a path ( darker nodes ) in ( a ). Distribution of words in given two sentences , a candidate ( om ) and a summary ( sn ) using sub-vocabulary of words at each topic vzl . Discrete distributions on the left are topic mixtures for each sentence , pzom and pzsn .
as in Eq.(6) by : sim2 ( om , sn ) = 10 ? IRcom ( pzom , pzsn ) (8) sim1 provides information about the similarity between two sentences , om and sn based on topic-word distributions . Similarly , sim2 provides information on the similarity between the weights of the topics in each sentence . They jointly effect the sentence score and are combined in one measure : sim(om , sn ) = sim1(om , sn ) ? sim2 ( om , sn ) (9) The final score for a given om is calculated from Eq.(4). Fig.1.b depicts a sample path illustrating sparse unigram distributions of om and sm at each level as well as their topic proportions , pzom , and pzsn . In experiment 3, we discuss the effect of our tree-based scoring on summarization performance in comparison to a classical scoring method presented as our baseline model.
5 Regression Model
Each candidate sentence om , m = 1..|O | is represented with a multidimensional vector of q features fm = { fm1, ..., fmq }. We build a regression model using sentence scores as output and selected salient features as input variables described below : 5.1 Feature Extraction We compile our training dataset using sentences from different document clusters , which do not necessarily share vocabularies . Thus , we create ngram meta-features to represent sentences instead of word ngram frequencies : ( I ) nGram Meta-Features ( NMF ): For each document cluster D , we identify most frequent ( nonstop word ) unigrams , i.e ., vfreq = { wi } r i=1 ? V , where r is a model parameter of number of most frequent unigram features . We measure observed unigram probabilities for each wi ? vfreq with pD(wi ) = nD(wi )/ ?| V | j=1 nD(wj ), where nD(wi ) is the number of times wi appears in D and | V | is the total number of unigrams . For any ith feature , the value is fmi = 0, if given sentence does not contain wi , otherwise fmi = pD(wi ). These features can be extended for any ngrams . We similarly include bigram features in the experiments.
(II ) Document Word Frequency Meta-
Features ( DMF ): The characteristics of sentences at the document level can be important in summary generation . DMF identify whether a word in a given sentence is specific to the document in consideration or it is commonly used in the document cluster . This is important because summary sentences usually contain abstract terms rather than specific terms.
To characterize this feature , we reuse the r most frequent unigrams , i.e ., wi ? vfreq . Given sentence om , let d be the document that om belongs to , i.e ., om ? d . We measure unigram probabilities for each wi by p(wi ? om ) = nd(wi ? om)/nD(wi ), where nd(wi ? om ) is the number of timeswi appears in d and nD(wi ) is the number of times wi appears in D . For any ith feature , the value is fmi = 0, if given sentence does not contain wi , otherwise fmi = p(wi ? om ). We also include bigram extensions of DMF features.
819 ( III ) Other Features ( OF ): Term frequency of sentences such as SUMBASIC are proven to be good predictors in sentence scoring ( Nenkova and Vanderwende , 2005). We measure the average unigram probability of a sentence by : p(om ) =
P w?om
PD(w ), where PD(w ) is the observed unigram probability in the document collection D and | om | is the total number of words in om . We use sentence bigram frequency , sentence rank in a document , and sentence size as additional features.
5.2 Predicting Scores for New Sentences
Due to the large feature space to explore , we chose to work with support vector regression ( SVR ) ( Drucker et al , 1997) as the learning algorithm to predict sentence scores . Given training sentences { fm , ym } | O | m=1, where fm = { fm1, ..., fmq } is a multidimensional vector of features and ym=score(om )? R are their scores obtained via Eq.(4), we train a regression model . In experiments we use nonlinear Gaussian kernel for SVR.
Once the SVR model is trained , we use it to predict the scores of ntest number of sentences in test ( unseen ) document clusters , Otest = { o1, ... o|Otest | } .
Our HybHSum captures the sentence characteristics with a regression model using sentences in different document clusters . At test time , this valuable information is used to score testing sentences.
Redundancy Elimination : To eliminate redundant sentences in the generated summary , we incrementally add onto the summary the highest ranked sentence om and check if om significantly repeats the information already included in the summary until the algorithm reaches word count limit . We use a word overlap measure between sentences normalized to sentence length . A om is discarded if its similarity to any of the previously selected sentences is greater than a threshold identified by a greedy search on the training dataset.
6 Experiments and Discussions
In this section we describe a number of experiments using our hybrid model on 100 document clusters each containing 25 news articles from DUC2005-2006 tasks . We evaluate the performance of HybHSum using 45 document clusters each containing 25 news articles from DUC2007 task . From these sets , we collected v80K and v25K sentences to compile training and testing data respectively . The task is to create max . 250 word long summary for each document cluster.
We use Gibbs sampling for inference in hLDA and sumHLDA . The hLDA is used to capture abstraction and specificity of words in documents ( Blei et al , 2009). Contrary to typical hLDA models , to efficiently represent sentences in summarization task , we set ascending values for Dirichlet hyperparameter ? as the level increases , encouraging mid to low level distributions to generate as many words as in higher levels , e.g ., for a tree of depth=3, ? = {0.125, 0.5, 1}. This causes sentences share paths only when they include similar concepts , starting higher level topics of the tree.
For SVR , we set  = 0.1 using the default choice , which is the inverse of the average of ?( f)T?(f ) ( Joachims , 1999), dot product of kernelized input vectors . We use greedy optimization during training based on ROUGE scores to find best regularizer C = { 10?1..102 } using the Gaussian kernel.
We applied feature extraction of ? 5.1 to compile the training and testing datasets . ROUGE is used for performance measure ( Lin and Hovy , 2003; Lin , 2004), which evaluates summaries based on the maxium number of overlapping units between generated summary text and a set of human summaries . We use R1 ( recall against unigrams ), R2 ( recall against bigrams ), and RSU4 ( recall against skip-4 bigrams).
Experiment 1: sumHLDA Parameter Analysis : In sumHLDA we introduce a prior different than the standard nested CRP ( nCRP ). Here , we illustrate that this prior is practical in learning hierarchical topics for summarization task.
We use sentences from the human generated summaries during the discovery of hierarchical topics of sentences in document clusters . Since summary sentences generally contain abstract words , they are indicative of sentences in documents and should produce minimal amount of new topics ( if not none ). To implement this , in nCRP prior of sumHLDA , we use dual hyperparameters and choose a very small value for summary sentences , ? s = 10e?4  ? o . We compare the results to hLDA ( Blei et al , 2003a ) with nCRP prior which uses only one free parameter , ?. To analyze this prior , we generate a corpus ofv1300 sentences of a document cluster in DUC2005. We repeated the experiment for 9 other clusters of similar size and averaged the total number of generated topics . We show results for different values of ? and ? o hyperparameters and tree depths.
820 ? = ? o 0.1 1 10 depth 3 5 8 3 5 8 3 5 8 hLDA 3 5 8 41 267 1509 1522 4080 8015 sumHLDA 3 5 8 27 162 671 1207 3598 7050 Table 1: Average # of topics per document cluster from sumHLDA and hLDA for different ? and ? o and tree depths.
?s = 10e?4 is used for sumHLDA for each depth.
Features Baseline HybHSum
R1 R2 RSU4 R1 R2 RSU4
NMF (1) 40.3 7.8 13.7 41.6 8.4 12.3
DMF (2) 41.3 7.5 14.3 41.3 8.0 13.9
OF (3) 40.3 7.4 13.7 42.4 8.0 14.4 (1+2) 41.5 7.9 14.0 41.8 8.5 14.5 (1+3) 40.8 7.5 13.8 41.6 8.2 14.1 (2+3) 40.7 7.4 13.8 42.7 8.7 14.9 (1+2+3) 41.4 8.1 13.7 43.0 9.1 15.1 Table 2: ROUGE results ( with stopwords ) on DUC2006 for different features and methods . Results in bold show statistical significance over baseline in corresponding metric.
As shown in Table 1, the nCRP prior for sumHLDA is more effective than hLDA prior in the summarization task . Less number of top-ics(nodes ) in sumHLDA suggests that summary sentences share preexisting paths and no new paths or nodes are sampled for them . We also observe that using ? o = 0.1 causes the model to generate minimum number of topics (# of top-ics=depth ), while setting ? o = 10 creates excessive amount of topics . ?0 = 1 gives reasonable number of topics , thus we use this value for the rest of the experiments . In experiment 3, we use both nCRP priors in HybHSum to analyze whether there is any performance gain with the new prior.
Experiment 2: Feature Selection Analysis Here we test individual contribution of each set of features on our HybHSum ( using sumHLDA).
We use a Baseline by replacing the scoring algorithm of HybHSum with a simple cosine distance measure . The score of a candidate sentence is the cosine similarity to the maximum matching summary sentence . Later , we build a regression model with the same features as our HybHSum to create a summary . We train models with DUC2005 and evaluate performance on DUC2006 documents for different parameter values as shown in Table 2.
As presented in ? 5, NMF is the bundle of frequency based meta-features on document cluster level , DMF is a bundle of frequency based meta-features on individual document level and OF represents sentence term frequency , location , and size features . In comparison to the baseline , OF has a significant effect on the ROUGE scores . In addition , DMF together with OF has shown to improve all scores , in comparison to baseline , on average by 10%. Although the NMF have minimal individual improvement , all these features can statistically improve R2 without stop words by 12% ( significance is measured by ttest statistics).
Experiment 3: ROUGE Evaluations
We use the following multidocument summarization models along with the Baseline presented in
Experiment 2 to evaluate HybSumm.
? PYTHY : ( Toutanova et al , 2007) A state-of-the-art supervised summarization system that ranked first in overall ROUGE evaluations in DUC2007. Similar to HybHSum , human generated summaries are used to train a sentence ranking system using a classifier model.
? HIERSUM : ( Haghighi and Vanderwende , 2009) A generative summarization method based on topic models , which uses sentences as an additional level . Using an approximation for inference , sentences are greedily added to a summary so long as they decrease KL-divergence.
? HybFSum ( Hybrid Flat Summarizer ): To investigate the performance of hierarchical topic model , we build another hybrid model using flat LDA ( Blei et al , 2003b ). In LDA each sentence is a superposition of all K topics with sentence specific weights , there is no hierarchical relation between topics . We keep the parameters and the features of the regression model of hierarchical HybHSum intact for consistency . We only change the sentence scoring method . Instead of the new tree-based sentence scoring (? 4), we present a similar method using topics from LDA on sentence level . Note that in LDA the topic-word distributions ? are over entire vocabulary , and topic mixing proportions for sentences ? are over all the topics discovered from sentences in a document cluster . Hence , we define sim1 and sim2 measures for LDA using topic-word proportions ? ( in place of discrete topic-word distributions from each level in Eq.2) and topic mixing weights ? in sentences ( in place of topic proportions in Eq.3) respectively . Maximum matching score is calculated as same as in HybHSum.
? HybHSum1 and HybHSum2: To analyze the effect of the new nCRP prior of sumHLDA on sum-
R1 R2 R4 R1 R2 R4
Baseline 32.4 7.4 10.6 41.0 9.3 15.2
PYTHY 35.7 8.9 12.1 42.6 11.9 16.8
HIERSUM 33.8 9.3 11.6 42.4 11.8 16.7
HybFSum 34.5 8.6 10.9 43.6 9.5 15.7
HybHSum1 34.0 7.9 11.5 44.8 11.0 16.7
HybHSum2 35.1 8.3 11.8 45.6 11.4 17.2
Table 3: ROUGE results of the best systems on DUC2007 dataset ( best results are bolded .) marization model performance , we build two different versions of our hybrid model : HybHSum1 using standard hLDA ( Blei et al , 2003a ) and
HybHSum2 using our sumHLDA.
The ROUGE results are shown in Table 3. The HybHSum2 achieves the best performance on R1 and R4 and comparable on R2. When stop words are used the HybHSum2 outperforms state-of-the-art by 2.5-7% except R2 ( with statistical significance ). Note that R2 is a measure of bigram recall and sumHLDA of HybHSum2 is built on unigrams rather than bigrams . Compared to the HybFSum built on LDA , both HybHSum1&2 yield better performance indicating the effectiveness of using hierarchical topic model in summarization task . HybHSum2 appear to be less redundant than HybFSum capturing not only common terms but also specific words in Fig . 2, due to the new hierarchical tree-based sentence scoring which characterizes sentences on deeper level.
Similarly , HybHSum1&2 far exceeds baseline built on simple classifier . The results justify the performance gain by using our novel tree-based scoring method . Although the ROUGE scores for HybHSum1 and HybHSum2 are not significantly different , the sumHLDA is more suitable for summarization tasks than hLDA.
HybHSum2 is comparable to ( if not better than ) fully generative HIERSUM . This indicates that with our regression model built on training data , summaries can be efficiently generated for test documents ( suitable for online systems).
Experiment 4: Manual Evaluations
Here , we manually evaluate quality of summaries , a common DUC task . Human annotators are given two sets of summary text for each document set , generated from two approaches : best hierarchical hybrid HybHSum2 and flat hybrid HybFSum models , and are asked to mark the better summary
New federal rules for organic food will assure consumers that the products are grown and processed to the same standards nationwide . But as sales grew more than 20 percent a year through the 1990s , organic food came to account for $1 of every $100 spent on food , and in 1997 t h e a g e n c y t o o k n o t i c e , proposing national organic standards for all food.
By the year 2001, organic products are projected to command 5 percent of total food sales in the United States . The sale of organics rose by about 30 percent last year , driven by concerns over food safety , the environment and a fear of genetically engineered food . U.S.
sales of organic foods have grown by 20 percent annually for the last seven years.
(c ) HybFSum Output ( b ) HybHSum
The Agriculture Department began to propose standards for all organic foods in the late 1990's because their sale had grown more than 20 per cent a year in that decade . In January 1999 the USDA approved a " certified organic " label for meats and poultry that were raised without growth hormones , pesticide-treated feed , and antibiotics.
(a ) Ref . Output word organic 6 6 6 genetic 2 4 3 allow 2 2 1 agriculture 1 1 1 standard 5 7 0 sludge 1 1 0 federal 1 1 0 bar 1 1 0 certified 1 1 0 s p e c i f i c
H y b
H
S u m y b
F
S u m
R e f
Figure 2: Example summary text generated by systems compared in Experiment 3. ( Id:D0744 in DUC2007). Ref.
is the human generated summary.
Criteria HybFSum HybHSum2 Tie
Non-redundancy 26 44 22
Coherence 24 56 12
Focus 24 56 12
Responsiveness 30 50 12
Overall 24 66 2
Table 4: Frequency results of manual quality evaluations.
Results are statistically significant based on ttest . T ie indicates evaluations where two summaries are rated equal.
according to five criteria : non-redundancy ( which summary is less redundant ), coherence ( which summary is more coherent ), focus and readability ( content and not include unnecessary details ), responsiveness and overall performance.
We asked 4 annotators to rate DUC2007 predicted summaries (45 summary pairs per annotator ). A total of 92 pairs are judged and evaluation results in frequencies are shown in Table 4. The participants rated HybHSum2 generated summaries more coherent and focused compared to HybFSum . All results in Table 4 are statistically significant ( based on ttest on 95% confidence level .) indicating that HybHSum2 summaries are rated significantly better.
822 ...
Document Cluster
Document Cluster
Document Cluster n ...
...
f q f-input features ...
f q f-input features ...
f q f-input features h(f,y ) : regression model for sentence ranking ...
...
...
.
.
z z
K zz z z sumHLDA ...
...
.
.
z z
K zz z z sumHLDA ...
...
.
.
z z
K zz z z sumHLDA ...
...
y-output candidate sentence scores 0.02 0.01 0.0 .
.
y-output candidate sentence scores 0.35 0.09 0.01 .
.
y-output candidate sentence scores 0.43 0.20 0.03 .
.
Figure 3: Flow diagram for Hybrid Learning Algorithm for Multi-Document Summarization.
7 Conclusion
In this paper , we presented a hybrid model for multidocument summarization . We demonstrated that implementation of a summary focused hierarchical topic model to discover sentence structures as well as construction of a discriminative method for inference can benefit summarization quality on manual and automatic evaluation metrics.
Acknowledgement
Research supported in part by ONR N00014-02-1-0294, BT Grant CT1080028046, Azerbaijan Ministry of Communications and Information Technology Grant , Azerbaijan University of Azerbaijan Republic and the BISC Program of UC Berkeley.
References
R . Barzilay and L . Lee . Catching the drift : Probabilistic content models with applications to generation and summarization . In In Proc . HLT-
NAACL?04, 2004.
D . Blei , T . Griffiths , M . Jordan , and J . Tenenbaum.
Hierarchical topic models and the nested chinese restaurant process . In In Neural Information Processing Systems [ NIPS ], 2003a.
D . Blei , T . Griffiths , and M . Jordan . The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies . In
Journal of ACM , 2009.
D . M . Blei , A . Ng , and M . Jordan . Latent dirichlet allocation . In Jrnl . Machine Learning Research , 3:993-1022, 2003b.
S.R.K . Branavan , H . Chen , J . Eisenstein , and R . Barzilay . Learning document-level semantic properties from freetext annotations . In Journal of Artificial Intelligence Research , volume 34, 2009.
J.M . Conroy , J.D . Schlesinger , and D.P . O?Leary.
Topic focused multi-cument summarization using an approximate oracle score . In In Proc.
ACL?06, 2006.
H . Daume?III and D . Marcu . Bayesian query focused summarization . In Proc . ACL06, 2006.
H . Drucker , C.J.C . Burger , L . Kaufman , A . Smola , and V . Vapnik . Support vector regression machines . In NIPS 9, 1997.
A . Haghighi and L . Vanderwende . Exploring content models for multidocument summarization.
In NAACL HLT-09, 2009.
T . Joachims . Making largescale svm learning practical . In In Advances in Kernel Methods - Support Vector Learning . MIT Press ., 1999.
C.-Y . Lin . Rouge : A package for automatic evaluation of summaries . In In Proc . ACL Workshop on Text Summarization Branches Out , 2004.
823
C.-Y . Lin and E.H . Hovy . Automatic evaluation of summaries using ngram cooccurance statistics . In Proc . HLTNAACL , Edmonton , Canada , 2003.
C . Manning and H . Schuetze . Foundations of statistical natural language processing . In MIT
Press . Cambridge , MA , 1999.
A . Nenkova and L . Vanderwende . The impact of frequency on summarization . In Tech . Report MSR-TR-2005-101, Microsoft Research , Redwood , Washington , 2005.
D.R . Radev , H . Jing , M . Stys , and D . Tam.
Centroid-based summarization for multiple documents . In In Int . Jrnl . Information Processing and Management , 2004.
D . Shen , J.T . Sun , H . Li , Q . Yang , and Z . Chen.
Document summarization using conditional random fields . In Proc . IJCAI?07, 2007.
J . Tang , L . Yao , and D . Chens . Multitopic based query-oriented summarization . In SIAM International Conference Data Mining , 2009.
I . Titov and R . McDonald . A joint model of text and aspect ratings for sentiment summarization.
In ACL-08:HLT , 2008.
K . Toutanova , C . Brockett , M . Gamon , J . Jagarlamudi , H . Suzuki , and L . Vanderwende . The ph-thy summarization system : Microsoft research at duc 2007. In Proc . DUC , 2007.
J.Y . Yeh , H.-R . Ke , W.P . Yang , and IH . Meng.
Text summarization using a trainable summarizer and latent semantic analysis . In Information Processing and Management , 2005.
824
