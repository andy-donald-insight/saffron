Document Classification Using a Finite Mixture Model 
Hang Li Kenji Yamanishi
C&C Res . Labs ., NEC
4-1-1 Miyazaki Miyamae-ku Kawasaki , 216, Japan
Email : lihang , yamanisi@sbl.cl.nec.co.jp
Abstract
We propose a new method of classifying
documents into categories . We define for each category a finite mixture model based on soft clustering of words  . We treat the problem of classifying documents as that of conducting statistical hypothesis testing over finite mixture models  , and employ the EM algorithm to efficiently estimate parameters in a finite mixture model  . Experimental results indicate that our method outperforms existing methods  . 
1 Introduction
We are concerned here with the issue of classifying documents into categories  . More precisely , we begin with a number of categories ( e . g . , ' tennis , soccer , skiing ') , each already containing certain documents . 
Our goal is to determine into which categories newly given documents ought to be assigned  , and to do so on the basis of the distribution of each document's words  .   1 Many methods have been proposed to address this issue  , and a number of them have proved to be quite effective  ( e . g . ,(Apte , Damerau , and Weiss , 1994; Cohen and Singer , 1996; Lewis , 1992; Lewis and Ringuette , 1994; Lewis et al , 1996; Schutze , Hull , and Pedersen , 1995; Yang and Chute ,  1994)) . 
The simple method of conducting hypothesis testing over word-base distributions in categories  ( defined in Section 2 ) is not efficient in storage and suffers from the data sparseness problem  , i . e . , the number of parameters in the distributions is large and the data size is not sufficiently large for accurately estimating them  . In order to address this difficulty , ( Guthrie , Walker , and Guthrie ,  1994 ) have proposed using distributions based on what we refer to as hard  1A related issue is the retrieval , from a database , of documents which are relevant to a given query ( pseudo-document )   ( e . g . ,(Deerwester et al , 1990; Fuhr , 1989; Robertson and Jones , 1976; Salton and McGill ,  1983;
Wong and Yao , 1989)).
clustering of words , i . e . , in which a word is assigned to a single cluster and words in the same cluster are treated uniformly  . The use of hard clustering might , however , degrade classification results , since the distributions it employs are not always precise enough for representing the differences between categories  . 
We propose here to employ softchster inf , i . e . , a word can be assigned to several different clusters and each cluster is characterized by a specific word probability distribution  . We define for each category a finite mixture model  , which is a linear combination of the word probability distributions of the clusters  . We thereby treat the problem of classifying documents as that of conducting statistical hypothesis testing over finite mixture models  . In order to accomplish ypothesis testing , we employ the EM algorithm to efficiently and approximately calculate from training data the maximum likelihood estimates of parameters in a finite mixture model  . 
Our method overcomes the major drawbacks of the method using word-base distributions and the method based on hard clustering  , while retaining their merits ; it in fact includes those two methods as special cases  . Experimental results indicate that our method outper for rrLs them  . 
Although the finite mixture model has already been used elsewhere in natural language processing  ( e . g . ( Jelinek and Mercer , 1980; Pereira , Tishby , and Lee ,  1993)) , this is the first work , to the best of knowledge , that uses it in the context of document classification  . 
2 Previous Work
Word-based method
A simple approach to document classification is to view this problem as that of conducting hypothesis testing over word-base distributions  . In this paper , we refer to this approach as the word-based method  ( hereafter , eferred to as WBM ) . 
2 We borrow from ( Pereira , Tishby , and Lee ,  1993 ) the terms hard clustering and soft clustering , which were used therein a different ask . 

Letting W denote a vocabulary ( a set of words ) , and w denote a random variable representing any word in it  , for each category ci(i = 1 ,   .   .   .   , n ) , we define its word-based distribution P ( wIci ) as a histogram type of distribution over W .   ( The number of free parameters of such a distribution is thus IW\[-  1  )  . WBM then views a document as a sequence of words : d = W l  ,  ' ' "  , WN ( 1 ) and assumes that each word is generated independently according to a probability distribution of a category  . It then calculates the probability of a document with respect o a category as 

P ( dlc , ) = P ( w,, .   .   .   , ~ Nle , ) = 1-~P ( w , lc , ) ,   ( 2 ) t = l and classifies the document into that category for which the calculated probability is the largest  . We should note here that a document's probability with respecto each category is equiv Men to the likelihood of each category with respect othe document  , and to classify the document into the category for which it has the largest probability is equivalent to classifying it into the category having the largest likelihood with respect oit  . Hereafter , we will use only the term likelihood and denote it as L  ( dlci )  . 
Notice that in practice the parameters in a distribution must be estimated from training data  . In the case of WBM , the number of parameters is large ; the training data size , however , is usually not sufficiently large for accurately estimating them  . This is the data . sparseness problem that so often stands in the way of reliable statistical language processing  ( e . g . ( Gale and Church , 1990)) . Moreover , the number of parameters in word-based istributions i too large to be efficiently stored  . 
Method based on hard clustering
In order to address the above difficulty , Guthrie et . al , have proposed a method based on hard clustering of words  ( Guthrie , Walker , and Guthrie ,  1994 )   ( hereafter we will refer to this method as HCM )  . Letcl, . . . , c,~be categories . HCM first conducts hard clustering of words . Specifically , it ( a ) defines a vocabulary as a set of words W and defines as clusters its subsets k l  ,   .   .  -  , k , n satisfying t3~=xkj = W and kifqkj = 0 ( ij )   ( i . e . , each word is assigned only to a single cluster ) ; and ( b ) treats uniformly all the words assigned to the same cluster  . HCM then defines for each category cia distribution of the clusters P  ( kj\[ci )   ( j = 1 ,   .   .   . , m ) . It replaces each word wt in the document with the cluster kt to which it belongs  ( t = 1 , -- . , N ) . It assumes that a cluster k t is distributed according to P  ( kj\[ci ) and calculates the likelihood of each category ci with respect to the document by 

L(dle ,) -- L(kl , .   .   . , kNlci ) = He(k , le ,) . 
t = l(3)
Table 1: Frequencies of words racket strokes hotgo alkick ball cl  4   1   2   1   0   2   c2   0   0   0   3   2   2 Table 2: Clusters and words ( L=5 , M = 5)'k lracket , stroke , shotkskick . k3 goal , ball
Table 3: Frequencies of clusters klks k3 c 1   7   0   3   c2   0   2   5 There are any number of ways to create clusters in hard clustering  , but the method employed is crucial to the accuracy of document classification  . Guthrie et . al . have devised away suitable to documentation classification  . Suppose that there are two categories cl = ' tennis ' and  c2='soccer   , ' and we obtain from the training data ( previously classified documents ) the frequencies of words in each category , such as those in Tab .  1 . Letting L and M be given positive integers , HCM creates three clusters : kl , k2 and k3 , in which kl contains those words which are among the L most frequent words in cl  , and not among the M most frequent in c2 ;   k2 contains those words which are among the L most frequent words in cs  , and not among the M most frequent in Cl ; and k3 contains all remaining words ( see Tab .  2) . HCM then counts the frequencies of clusters in each category  ( see Tab .  3 ) and estimates the probabilities of clusters being in each category  ( see Tab .  4) . 3 Suppose that a newly given document , liked in Fig . i , is to be classified . HCM cM culates the likelihood values 3We calculate the probabilities here by using the socalled expected likelihood estimator  ( Gale and Church ,  1990):  . f(kjlc ,) + 0 . 5 ,
P ( k3 lc ~ ) = f-~--~-~xm ( 4 ) where f ( kjlci ) is the frequency of the cluster kj in ci , f(ci ) is the total frequency of clusters in cl , and m is the total number of clusters . 

Table 4: Probability distributions of clusters k l k2   k3 cl 0  . 65 0 . 04 0 . 30 cs 0 . 06 0 . 29 0 . 65L ( dlCl ) and L(dlc2) according to Eq .  (3) . ( Tab .   5 shows the logarithms of the resulting likelihood values  . ) It then classifies dintocs , as logs L(dlcs ) is larger than logs L(dlc1) . 
d = kick , goal , goal , ball
Figure 1: Example document
Table 5: Calculating loglikelihood values log2L ( dlct ) = 1 x logs . 04+3 ? logs . 30 = -9 . 85 logs L(d\]cs ) = 1 ? logs . 29+3x logs . 65 = -3 . 6 5 HCM can handle the data sparseness problem quite well  . By assigning words to clusters , it can drastically reduce the number of parameters to be estimated  . It can also save space for storing knowledge . We argue , however , that the use of hard clustering still has the following two problems :  1  . HCM cannot assign a word ?0 more than one cluster at a time . Suppose that there is another category c3 =' skiing ' in which the word ' ball ' does not appear  , i . e . , ' ball ' will be indicative of both cl and c2 , but not cs . If we could assign ' ball'to both kt and k2 , the likelihood value for classifying a document containing that word to clor  c2 would become larger , and that for classifying it into c3 would become smaller . HCM , however , cannot do that . 
2 . HCM cannot make the best use of information about the differences among the frequencies of words assigned to an individual cluster  . For example , it treats ' racket ' and ' shot'uniformly because they are assigned to the same cluster k t  ( see Tab .  5) . ' Racket'may , however , be more indicative of Cl than'shot , ' because it appears more frequently in clthan ' shot  . ' HCM fails to utilize this information . This problem will become more serious when the values L and M and M in word clustering are large  , which renders the clustering itself relatively meaningless  . 
From the perspective of number of parameters , HCM employs models having very few parameters , and thus may not sometimes represent much useful information for classification  . 
3 Finite Mixture Model
We propose a method of document classification based on soft clustering of words  . Letcl , -- . , cn  be categories . We first conduct the soft clustering . Specifically , we ( a ) define a vocabulary as a set W of words and define as clusters a number of its subsets k l  ,   .  - -  , k , n satisfying u ' ~= lkj = W ;   ( notice that k it 3 k j = 0 ( i ~ j ) does not necessarily hold here , i . e . , a word can be assigned to several different clusters  )  ; and ( b ) define for each cluster k j ( j = 1 ,   .   .   .   , m ) a distribution Q ( w\[kj ) over its words ( )"\]~ wekj Q ( w\[kj )  = 1 ) and a distribution P ( wlkj ) satisfying : ! Q ( wlki )  ; weki , P ( wlkj ) 0 ; w ? ( 5 ) where w denotes a random variable representing any word in the vocabulary  . We then define for each category ci(i = 1, .   .   .   , n ) a distribution of the clusters P ( kj Ici ) , and define for each category a linear combination of P  ( w\]kj ) :
P ( wlc ~) = ~ P ( kjlc ~) xP ( wlk . i ) (6) j = l as the distribution over its words , which is referred to as a finite mixture model ( e . g . , ( Everitt and Hand , 1981)) . 
We treat the problem of classifying a document as that of conducting the likelihood ratio test over finite mixture models  . That is , we view a document as a sequence of words , d = wl ,  " "  , WN (7) where wt(t = 1 ,   .  -  . , N ) represents a word . We assume that each word is independently generated according to an unknown probability distribution and determine which of the finite mixture models P  ( w\[ci ) ( i = 1 ,   .   .   .   , n ) is more likely to be the probability distribution by observing the sequence of words  . Specifically , we calculate the likelihood value for each category with respect o the document by: 
L(d\[ci ) = L(w l , .   .   .   , wglci ) = I-\[~=1P ( wtlc , ) : n = lP ( kic , ) xP(w , lk ) )  ( 8 ) We then classify it into the category having the largest likelihood value with respect oit  . Hereafter , we will refer to this method as FMM . 
FMM includes WBM and HCM as its special cases . If we consider the specific case ( 1 ) in which a word is assigned to a single cluster and P  ( wlkj ) is given by 1 . 
(9) P ( wlkj ) = O ; w~k ~ , to kj , then we will get the same classification result as in HCM  . In such a case , the likelihood value for each category ci becomes : L  ( dlc , ) = I-I ; : x ( P ( ktlci ) xP ~ wtlkt ) ) = 1-It =~ P ( ktlci ) xl-It = lP ( Wtlkt )  ,   ( lo ) where kt is the cluster corresponding to wt . Since the probability P ( wt\]kt ) does not depend one ate-Ngories , we can ignore the second term YIt = lP ( wtIkt ) in hypothesis testing , and thus our method essentially becomes equivalent to HCM  ( c . f . Eq .  (3)) . 
Further , in the specific case (2) in which m = n , for each j , P ( w lkj ) hasIW l parameters : P ( w l k j ) =
P ( w l c j ) , and P ( kjlci ) is given by 1; i = j , P ( kjlci ) = O;i#j ,   ( 11 ) the likelihood used in hypothesis testing becomes the same as that in Eq  . (2), and thus our method becomes equivalent to WBM . 
4 Estimation and Hypothesis
Testing
In this section , we describe how to implement our method . 
Creating clusters
There are any number of ways to create clusters on a given set of words  . As in the case of hard clustering , the way that clusters are created is crucial to the reliability of document classification  . Here we give one example approach to cluster creation  . 
Table 6: Clusters and words
Ikl Iracket , stroke , shot , balllkskick , goal , ball We let the number of clusters equal that of categories  ( i . e . , m = n ) 4 and relate each cluster ki to one category ci ( i = 1 ,  - -  . , n ) . We then assign individual words to those clusters in whose related categories they most frequently appear  . Letting 7 ( 0 _< 7 < 1 ) be a predetermined threshold value , if the following inequality holds : f(wlci ) > 7 , ( t2) f(w ) then we assign w to k i , the cluster related to ci , where f ( wlci ) denotes the frequency of the word w in category ci  , and f(w ) denotes the total frequency of w . Using the data in Tab . l , we create two clusters : k t and k 2 , and relate them to ct and c2 , respectively . 
4One can certainly assume that m > n.
For example , when 7= 0 . 4 , we assign'go al'tok2 only , as the relative frequency of ' goal ' in c ~ is 0 . 75 and that incx is only 0 . 25 . We ignore in document classification those words which cannot be assigned to any cluster using this method  , because they are not indicative of any specific category  . ( For example , when 7>_0 . 5' ball'will not be assigned into any cluster . ) This helps to make classification efficient and accurate  . Tab . 6 shows the results of creating clusters . 
Estimating P ( w l k j )
We then consider the frequency of a word in a cluster  . If a word is assigned only to one cluster , we view its total frequency as its frequency within that cluster  . For example , because ' goal ' is assigned only toks , we use as its frequency within that cluster the total count of its occurrence in all categories  . If a word is assigned to several different clusters  , we distribute its total frequency among those clusters in proportion to the frequency with which the word appears in each of their respective related categories  . For example , because ' ball ' is assigned to both kl and k2 , we distribute its total frequency among the two clusters in proportion to the frequency with which ' ball ' appears in cl and  c2  , respectively . After that , we obtain the frequencies of words in each cluster as shown in Tab  .  7 . 
Table 7: Distributed frequencies of words rackets troke shot goal kick ball kl  4   1   2   0   0   2   k2   0   0   0   4   2   2 We then estimate the probabilities of words in each cluster  , obtaining the results in Tab .  8 .   5 Table 8: Probability distributions of words rackets troke shot goal kick ball kl  0  . 44 0 . 11 0 . 22 0 0 0 . 22k 20000 . 50 0 . 25 0 . 25
Estimating P ( kj\]ci)
Let us next consider the estimation of P ( kj\[ci).
There are two common methods for statistical estimation  , the maximum likelihood estimation method 5We calculate the probabilities by employing the maximum likelihood estimator :/  ( kAc0 ( 13 ) P ( kilci ) - f ( ci ) ' where f ( kj \] cl ) is the frequency of the cluster kj in ci , and f ( cl ) is the total frequency of clusters in el . 

Table 10: Calculating loglikelihood values\[log ~ L ( d\[cl ) = log2 (  . 14?  . 25) +2x log2( . 14 x . 50)+log2( . 86x . 22 + . 14 x . 25) : -14  . 67\[I logSL(dlc2) 1 og2 ( . 96 x . 25) +2x log2( . 96 x . 50) + 1og2( . 04 x . 22T . 96 ?  . 25) -6 . 1 8 I Table 9: Probability distributions of clusters k l k2 
Cl 0 . 86 0 . 14 c2 0 . 04 0 . 96 and the Bayes estimation method . In their implementation for estimating P ( kj Ici ) , however , both of them suffer from computational intractability  . The EM algorithm ( Dempster , Laird , and Rubin ,  1977 ) can be used to efficiently approximate he maximum likelihood estimator of P  ( kj\[c ~ )  . We employ here an extended version of the EM algorithm  ( Helmbold et al . , 1995) . ( We have also devised , on the basis of the Markov chain Monte Carlo ( MCMC ) technique ( e . g . ( Tanner and Wong , 1987; Yamanishi ,  1996)) 6 , an algorithm to efficiently approximate the Bayes estimator of P  ( kj\[c ~ )  . ) For the sake of notational simplicity , for a fixed i , let us write P ( kjlci ) as Oj and P ( wlkj ) as Pj ( w )  . 
Then letting 9= (01 ,  ' "  , 0 m ) , the finite mixture model in Eq . (6) may be written as rn
P ( wlO ) = ~0 ~ xPj(w ). (14) j = l
For a given training sequence wl ' " WN , the maximum likelihood estimator of 0 is defined as the value which maximizes the following loglikelihood function  ) L ( O ) = ~' logOjPj ( wt )   .   ( 15 ) ~-\ j=l The EM algorithm first arbitrarily sets the initial value of  0  , which we denote as 0(0) , and then successively calculates the values of 6 on the basis of its most recent values . Lets be a predetermined number . At the lth iteration ( l-:1, .   .  -  , s ) , we calculate = by 0~' )  : 0~ '-1 )   ( ~? ( VL ( 00-1 ) ) j - 1 ) + 1 )  , (16) where ~> 0 ( when ~ = 1 , Hembold et al's version simply becomes the standard EM algorithm  )  , and 6We have confirmed in our preliminary experiment that MCMC perform slightly better than EM in document classification  , but we omit the details here due to space limitations  . 
~TL(O ) denotes vL(O ) = (0L001"'"O0, nOL) . (17) Afters numbers of calculations , the EM algorithm outputs 00) = (0 ~ O ,  . . . , 0~)) as an approximate of 0 . It is theoretically guaranteed that the EM algorithm converges to a local minimum of the given likelihood  ( Dempster , Laird , and Rubin ,  1977) . 
For the example in Tab . 1, we obtain the results as shown in Tab .  9 . 
Testing
For the example in Tab . 1, we can calculate according to Eq .   ( 8 ) the likelihood values of the two categories with respect o the document in Fig  . 1 ( Tab . 10 shows the logarithms of the likelihood values )  . We then classify the document into category c2 , as log2 L(d\]c2) is larger than log2L(dlcl ) . 
5 Advantages of FMM
For a probabilistic approach to document classification  , the most important thing is to determine what kind of probability model  ( distribution ) to employ as a representation of a category . It must (1) appropriately represent a category , as well as ( 2 ) have a proper preciseness in terms of number of parameters  . The goodness and badness of selection of a model directly affects classification results  . 
The finite mixture model we propose is particularly well-suited to the representation f a category  . 
Described in linguistic terms , a cluster corresponds to a topic and the words assigned to it are related to that topic  . Though documents generally concentrate on a single topic  , they may sometimes refer for a time to others , and while a document is discussing anyone topic , it will naturally tend to use words strongly related to that topic  . A document in the category of ' tennis'is more likely to discuss the topic of ' tennis  , ' i . e . , to use words strongly related to ' tennis , ' but it may sometimes briefly shift to the topic of's occer  , ' i . e . , use words strongly related to's occer . ' A human can follow the sequence of words in such a document  , associate them with related topics , and use the distributions of topics to classify the document  . Thus the use of the finite mixture model can be considered as a stochastic implementation f this process  . 
The use of FMM is also appropriate from the viewpoint of number of parameters  . Tab . 11 shows the numbers of parameters in our method ( FMM )  , 
WBM O(n . IW l )
HCM O(n.m)
FMMo(Ikl+n'm)
HCM , and WBM , where IW\] is the size of a vocabulary , Ikl is the sum of the sizes of word clusters m ( i . e . ,Ikl -- E~=I Ikil ) , n is the number of categories , and m is the number of clusters . The number of parameters in FMM is much smaller than that in WBM  , which depends on IW l , a very large number in practice ( notice that Ikl is always smaller than IWI when we employ the clustering method  ( with 7>0 . 5) described in Section 4 . As a result , FMM requires less data for parameter estimation than WBM and thus can handle the data sparseness problem quite well  . Furthermore , it can economize on the space necessary for storing knowledge  . On the other hand , the number of parameters in FMM is larger than that in HCM  . It is able to represent the differences between categories more precisely than HCM  , and thus is able to resolve the two problems , described in Section 2 , which plague HCM . 
Another advantage of our method may be seen in contrast to the use of latent semantic analysis  ( Deerwester et al ,  1990 ) in document classification and document retrieval  . They claim that their method can solve the following problems : synonymy problem how to group synonyms  , like'stroke ' and'shot , ' and make each relatively strongly indicative of a category even though some may individually appear in the category only very rarely  ; polysemy problem how to determine that a word like ' ball ' in a document refers to a ' tennis ball ' and not a ' soccer ball  , ' so as to classify the document more accurately ; dependence problem how to use dependent words , like'kick ' and ' goal , ' to make their combined appearance in a document more indicative of a category  . 
As seen in Tab . 6 , our method also helps resolve all of these problems  . 
6 P re l iminary Exper imenta l Resu l t s In this section  , we describe the results of the experiments we have conducted to compare the performance of our method with that of HCM and others  . 
As a first dataset , we used a subset of the Reuters newswire data prepared by Lewis  , called Reuters-21578 Distribution 1 . 0 . 7We selected nine overlapping categories , i . e . in which a document may be-rReuters-21578 is available at http://www . research . att . com/lewis . 
long to several different categories . We adopted the Lewis Split in the corpus to obtain the training data and the test data  . Tabs . 12 and 13 give the details . We did not conduct stemming , or use stop words . We then applied FMM , HCM , WBM , and a method based on cosine-similarity , which we denote as COS9 , to conduct binary classification . In particular , we learn the distribution for each category and that for its complement category from the training data  , and then determine whether or not to classify into each category the documents in the test data  . When applying FMM , we used our proposed method of creating clusters in Section  4 and set 7 to be 0  ,  0 . 4, 0 . 5, 0 . 7, because these are representative values . For HCM , we classified words in the same way as in FMM and set  7 to be 0  . 5, 0 . 7, 0 . 9, 0 . 95 . 
( Notice that in HCM , 7 cannot be set less than 0 . 5 . )
Table 12: The first dataset
Num . of doc . in training data 707
Num . of doc in test data 228
Num . of ( type of ) words 10902
Avg . num . of words per doe . 310.6
Table 13: Categories in the first dataset
I wheat , corn , oilseed , sugar , coffee soybe an , cocoa , rice , cott on \]
Table 14: The second dataset
Num . of doc . training data 13625
Num . of doc . in test data 6188
Num . of ( type of ) words 50301
Avg . num . of words per doc . 181.3
As a second dataset , we used the entire Reuters-21578 data with the Lewis Split . Tab . 14 gives the details . Again , we did not conduct stemming , or use stop words . We then applied FMM , HCM , WBM , and COS to conduct binary classification . When applying FMM , we used our proposed method of creating clusters and set  7 to be 0  ,  0 . 4, 0 . 5, 0 . 7 . For HCM , we classified words in the same way as in FMM and set  7 to be 0  . 5, 0 . 7, 0 . 9, 0 . 95 . We have not fully completed these experiments , however , and here we only 8'Stop words ' refers to a predetermined list of words containing those which are considered not useful for document classification  , such as articles and prepositions . 
9In this method , categories and document so be classified are viewed as vectors of word frequencies  , and the cosine value between the two vectors reflects similarity  ( Salton and McGill ,  1983) . 

Table 15: Tested categories in the second dataset earn , acq , crude , money-fx , grain interest , trade , ship , wheat , corn \] give the results of classifying into the ten categories having the greatest numbers of documents in the test data  ( see Tab .  15) . 
For both datasets , we evaluated each method in terms of precision and recall by means of the socalled microaveraging  10 When applying WBM , HCM , and FMM , rather than use the standard likelihood ratio testing  , we used the following heuristics . For simplicity , suppose that there are only two categories cl and  c2  . Letting ? be a given number larger than or equal 0  , we assign a new document d in the following way :  ~   ( logL ( dlcl ) -logL ( dlc2 ) ) > e ; d--*cl , (logL(dlc2) logL(dlct )) > ~; d---+ cu , otherwise ; unclassify d , ( is ) where N is the size of document d .   ( One can easily extend the method to cases with a greater ~ umber of categories  . ) 11 For COS , we conducted classification in a similar way . 
Figs .   2 and 3 show precisionrecall curves for the first data set and those for the second dataset  , respectively . In these graphs , values given after FMM and HCM represent 3' in our clustering method ( e . g . 
FMM0 . 5, HCM 0 . 5, etc ) . We adopted the break even point as a single measure for comparison  , which is the one at which precision equals recall  ; a higher score for the break even point indicates better performance  . Tab .   16 shows the break even point for each method for the first dataset and Tab  . 17 shows that for the second dataset . For the first dataset ,   FMM0 attains the highest score at break even point ; for the second dataset , FMM 0 . 5 attains the highest . 
We considered the following questions : ( 1 ) The training data used in the experimentation may be considered sparse  . Will a word-clustering-based method ( FMM ) outperform a word-based method ( WBM ) here ? ( 2 ) Is it better to conduct soft clustering ( FMM ) than to do hard clustering ( HCM ) ?  ( 3 ) With our current method of creating clusters , as the threshold 7 approaches 0 , FMM behaves much like WBM and it does not enjoy the effects of clustering at all  ( the number of parameters is as largel ? In micro -averaging  ( Lewis and Ringuette ,  1994) , precision is defined as the percentage of classifie documents in all categories which are correctly classified  . Recall is defined as the percentage of the total documents in all categories which are correctly classified  . 
n Notice that words which are discarded in the duster-ing process should not to be counted in document size  . 
I0 . g0 . 8 0 . 7 ~ 0 . 6 0 . 5 0 . 4 0 . 3 0 . 2 ~"  .   .   .   .  _ ' :~  .   . " HCM 0 . S " - e- . 
.~"  .   . . . . :: . ': . ~? HCM 0 . 7"  . -v , --, . -" "  .   . ~ " '~"~ .   . " HCMO . 9" ~- - . ~/-""-~, " HCM 0 . g5"-~'--? . " ~, " . , " FMM0"-e- . -/ ~ . ~" FMM 0 . 4" "+--~-  .   . /' ~~--~" FMM 0 . 5"-e--y .   .   .   . -,," FMMO . 7" / . ~::::~: . .-~-- '-, . 

0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 recall Figure 2: Precision-recall curve for the first dataset c
II
O . g0 . 8 0 . 7 0 . 6 0,5 0 . 4 0 . 3 0 . 2 0 . 1" WBM ""+ --" HCM 0 . 5"-D-"HCM 0 . 7 = K --
GI , """ HCMO . g " ~ .  - . . " " '~-  . . . . " HCMO . g5""~---"'"' l ~ ~3~" FMMO"-e .  - . 
" .  "~ .  ~  . .- Q ? FMM0 . 4" -+--?  .   .   .   .  , . :"  .   .  ",,, . " FMM 0 . 5"-Q--%"-,~"FMM 0 . 7 ~ "- . . .  " . . .  ~ ,  ~ ?0 ,  012 0:~ 01 , 0: s0:00: ,  0:8 01 , recall Figure 3: Precision-recall curve for the second dataset as in WBM  )  . This is because in this case ( a ) a word will be assigned into all of the clusters ,   ( b ) the distribution of words in each cluster will approach that in the corresponding category in WBM  , and ( c ) the likelihood value for each category will approach that in WBM  ( recall case ( 2 ) in Section 3 )  . Since creating clusters in an optimal way is difficult  , when clustering does not improve performance we can at least make FMM perform as well as WBM by choosing  7  =  0  . The question now is " does FMM perform better than WBM when  7 is 0?" In looking into these issues , we found the following : (1) When 3'> > 0 , i . e . , when we conduct clustering , FMM does not perform better than WBM for the first dataset  , but it performs better than WBM for the second dataset  . 
Evaluating classification results on the basis of each individual category  , we have found that for three of the nine categories in the first dataset  , 









FMM0 . 7 for thq first dataset 0 . 60 0 . 62 0 . 32 0 . 42 0 . 54 0 . 51 0 . 66 0 . 54 0 . 52 0 . 42
Table 17: Break even point for the
COS 10.52
WBM ! 0.62
HCM 0.51 0.47
HCM 0.7i 0.51
HCM 0.91 0.55
HCM 0.95 0.31
FMM 0 i 0.62
FMM 0.4 0.54
FMM 0.5 0.67
FMM 0.7 0.62 second dataset
FMM0 . 5 performs best , and that in two of the ten categories in the second dataset  FMM0  . 5 performs best . These results indicate that clustering sometimes does improve classification results when we use our current way of creating clusters  . ( Fig .   4 shows the best result for each method for the category ' corn ' in the first dataset and Fig  . 5 that for ' grain ' in the second dataset . )(2) When 3'>>0, i . e . , when we conduct clustering , the best of FMM almost always outperforms that of

(3) When 7= 0 , FMM performs better than WBM for the first data set  , and that it performs as well as WBM for the second dataset  . 
In summary , FMM always outperforms HCM ; in some cases it performs better than WBM ; and in general it performs at least as well as WBM  . 
For both datasets , the best FMM results are superior to those of COS throughout  . This indicates that the probabilistic approach is more suitable than the cosine approach for document classification based on word distributions  . 
Although we have not completed our experiments on the entire Reuters dataset  , we found that the results with FMM on the second dataset are almost as good as those obtained by the other approaches reported in  ( Lewis and Ringuette ,  1994) . ( The results are not directly comparable , because ( a ) the results in ( Lewis and Ringuette ,  1994 ) were obtained from an older version of the Reuters data  ; and ( b ) they t0 , 9 0 . 8 0 . 7 0 . 8 0 . 8'COS""'~/,"HCMO . 9" ~-  . 
? ' ~ "~ . , " FMMO . 8", /"-~ o ' . , ?' . , o ' . ~ o ' . , o . ~ oi ? oi , o ' . 8 o ' . 8 ror , ~ Figure 4: Precision-recall curve for category ' corn ' 0  . 8 0 . 7 0,6 0 . 5 0 . 4 0 . 3 0 . 2
O.t " " .. k ~, ? . . . ~" h ~ MO . 7"" e --",

I0' . , 0' . , 0' . , 0' . , 0' . 8 0' . , 0 . , 0 . ? 01 , Figure 5: Precision-recall curve for category ' grain ' used stop words  , but we did not . ) We have also conducted experiments on the Susanne corpus data  t2 and confirmed the effectiveness of our method . We omit an explanation of this work here due to space limitations  . 
7 Conclusions
Let us conclude this paper with the following remarks :  1  . The primary contribution of this research is that we have proposed the use of the finite mixture model in document classification  . 
2 . Experimental results indicate that our method of using the finite mixture model outperforms the method based on hard clustering of words  . 
3 . Experimental results also indicate that in some cases our method outperforms the word-based  12The Susanne corpus , which has four nonoverlapping categories , is ~ va ~ lable at ftp://ota . ox . ac . ukating clusters . 
Our future work is to include : 1 . comparing the various methods over the entire Reuters corpus and over other databases  ,  2 . developing better ways of creating clusters . 
Our proposed method is not limited to document classification  ; it can also be applied to other natural language processing tasks  , like word sense disambiguation , in which we can view the context surrounding a ambiguous target word as a document and the word senses to be resolved as categories  . 

We are grateful to Tomoyuki Fujita of NEC for his constant encouragement  . We also thank Naoki Abeof NEC for his important suggestions  , and Mark Peter sen of Meiji Univ . for his help with the English of this text . We would like to expresspecial appreciation to the six ACL anonymous reviewers who have provided many valuable comments and criticisms  . 

Apte , Chidanand , Fred Damerau , and Sholom M.
Weiss .  1994 . Automated learning of decision rules for text categorization  . ACM Tran . on Information Systems , 12(3):233-251 . 
Cohen , William W . and Yoram Singer . 1996.
Context-sensitive learning methods for text categorization  . Proc . of SIGIR'96 . 
Deerwester , Scott , Susan T . Dumais , George W.
Furnas , ThomasK . Landauer , and Richard Harshman .  1990 . Indexing by latent semantic analysis . 
Journ . of the American Society for Information
Science , 41(6):391-407.
Dempster , A . P . , N . M . Laird , and D . B . Rubin .  1977 . 
Maximum likelihood from incomplete data via the em algorithm  . Journ . of the Royal Statistical Society , Series B ,  39(1):1-38 . 
Everitt , B . and D . H and .  1981 . Finite Mixture Distributions . London : Chapman and Hall . 
Fuhr , Norbert .  1989 . Models for retrieval with probabilistic indexing . Information Processing and
Management , 25(1):55-72.
Gale , Williams A . and Kenth W . Church . 1990.
Poor estimates of context are worse than none.
Proc . of the DARPA Speech and Natural Language
Workshop , pages 283-287.
Guthrie , Louise , Elbert Walker , and Joe Guthrie . 
1994 . Document classification by machine : Theory and practice  . Proc . of COLING'94, pages 1059-1063 . 
Helmbold , D . , R . Schapire , Y . Siuger , and M . Warmuth .  1995 . A comparison of new and old algorithm for a mixture estimation problem  . Proc . of
COLT'95, pages 61-68.
Jelinek , F . and R . I . Mercer .  1980 . Interpolated estimation of markov source parameters from sparse data  . Proc . of Workshop on Pattern Recognition in Practice , pages 381-402 . 
Lewis , David D .  1992 . An evaluation of phrasal and clustered representations on a text categorization task  . Proc . of SIGIR '9~, pages 37-50 . 
Lewis , David D . and Marc Ringuette .  1994 . A comparison of two learning algorithms for test categorization  . Proc . of 3rd Annual Symposium on Document Analysis and Information Retrieval  , pages 81-93 . 
Lewis , David D ., Robert E . Schapire , James P.
Callan , and Ron Papka .  1996 . Training algorithms for linear text classifiers . Proc . of SI-
GIR'96.
Pereira , Fernando , Naftali Tishby , and Lillian Lee . 
1993. Distributional clustering of english words.
Proc . of ACL'93, pages 183-190.
Robertson , S . E . and K . SparckJones .  1976 . Relevance weighting of search terms . Journ . of the American Society for Information Science ,  27:129-146 . 
Salton , G . and M . J . McGi U .  1983 . Introduction to Modern Information Retrieval . New York : Mc-
Graw Hill.
Schutze , Hinrich , David A . Hull , and Jan O . Pedersen .  1995 . A comparison of classifiers and document representations for the routing problem  . 
Proc . of SIGIR'95.
Tanner , Martin A . and Wing Hung Wong . 1987.
The calculation of posterior distributions by data augmentation  . Journ . of the American Statistical
Association , 82(398):528-540.
Wong , S . K . M . and Y . Y . Ya  ~ .  1989 . A probability distribution model for information retrieval  . Information Processing and Management ,  25(1):39-53 . 
Yamanishi , Kenji .  1996 . A randomized approximation of them dl for stochastic models with hidden variables  . Proc . of COLT'96, pages 99-109 . 
Yang , Yiming and Christoper G . Chute .  1994 . An example-based mapping method for text categorization and retrieval  . ACM Tran . on Information
Systems , 12(3):252-277.

