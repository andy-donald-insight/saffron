Automatic Acquisition of Language Model
based on Head-Dependent Relation between Words
Seungmi Lee and Key-Sun Choi
Department of Computer Science
Center for Artificial Intelligence Research Korea Advanced Institute of Science and Technology email : leesm  , kschoi@world , kaist , ac . kr
Abstract
Language modeling is to associate a sequence of words with a priori probability  , which is a key part of many natural anguage applications such as speech recognition and statistical machine translation  . In this paper , we present a language modeling based on a kind of simple dependency grammar  . The grammar consists of head-dependent relations between words and can be learned automatically from a raw corpus using the reestimation algorithm which is also introduced in this paper  . Our experiments show that the proposed model performs better than ngram models at  11% to 11  . 5 ~ reductions in test corpus entropy . 
1 Introduction
Language modeling is to associate a priori probability to a sentence  . It is a key part of many natural language application such as speech recognition and statistical machine translation  . 
Previous works for language modeling can be broadly divided into two approaches  ; one is n-gram-based and the other is grammar -based  . 
Ngram model estimates the probability of a sentence as the product of the probability of each word in the sentence  . It assumes that probability of the nth word is dependent on the previous n-  1 words . The ngram probabilities are estimated by simply counting the ngram frequencies in a training corpus  . In some cases , class ( or part of speech ) ngrams are used instead of word n-grams ( Brown et al , 1992; Chang and Chen ,  1996) . Ngram model has been widely used so far , but it has always been clear that ngram cannot represent long distance dependencies  . 
In contrast with ngram model , grammar-based approach assigns syntactic structures to a sentence and computes the probability of the sentence using the probabilities of the structures  . Long distance dependencies can be represented well by means of the structures  . The approach usually makes use of phrase structure grammars such as probabilistic on text -free grammar and recursive transition network  ( Lari and Young , 1991; Sneff , 1992; Chen ,  1996) . In the approach , however , a sentence which is not accepted by the grammar is assigned zero probability  . Thus , the grammar must have broadcoverage so that any sentence will get nonzero probability  . But acquisition of such a robust grammar has been known to be very difficult  . 
Due to the difficulty , some works try to use an integrated model of grammar and ngram compensating each other  ( McCandless , 1994; Meteer and Rohlicek ,  1993) . Given a robust grammar , grammar-based language modeling is expected to be more powerful and compact in model size than n -gram-based one  . 
In this paper we present a language modeling based on a kind of simple dependency grammar  . The grammar consists of head-dependent relations between words and can be learned automatically from a raw corpus using the reestimation algorithm which is also introduced in this paper  . Based on the dependencies , a sentence is analyzed and assigned syntactic structures by which long distance dependences are represented  . Because the model can be thought of as a linguistic bigram model  , the smoothing functions of ngram models can be applied to it  . 
Thus , the model can be robust , adapt easily to new domains , and be effective . 
The paper is organized as follows . We introduce some definitions and notations for the dependency grammar and the reestimation algorithm in section  2  , and explain the algorithm in section 3 . In section 4 , we show the experimental results for the suggested model compared to ngram models  . Finally , section 5 concludes this paper . 
2 A Simple Dependency Grammar
In this paper , we assume a kind of simple dependency grammar which describes a language words  . A sentence is analyzed by establishing dependency links between individual words in the sentence  . A dependency analysis , : D , of a sentence can be represented with arrows pointing from head to dependent as depicted in Figure  1  . For structural generality , we assume that there is always a marking tag , "EOS " ( End of Sentence) , at the end of a sentence and it has the head word of the sentence as its own depen-dent  ( " gave " in Figure 1 )  . 
I gave him a book EOS
Figure 1: An example dependency analysis A/ ) is a set of interwordependencies which satisfy the following conditions :  ( 1 ) every word in the sentence has its head in the sentence x-cept the head word of the sentence  . (2) every word can have only one head .   ( 3 ) there is neither crossing nor cycle of dependencies  . 
The probabilistic model of the simple dependency grammar is given by p  ( sentence ) = ~-'~ p ( D )  2 ) = 2II2 ) x . - . + y6D where p(x--+y ) = p(ylx ) freq(x--+y)
E , z ) "
CompleteLink and Complete-Sequence
Here , we define complete-link and complete-sequence which represent partial : Ds for substrings  . They are used to construct overall 79s and used as the basic structures for the reestimation algorithm in section  3  . 
A set of dependency relations on a word sequence , wijl , is a complete-link when the following conditions are satisfied : ? there is  ( wi-+wi ) or ( wie--wj ) exclusively . 
? Every inner word has a head in the word sequence . 
? Neither crossing nor cycle of dependency relations is allowed  . 
tWe use wi for ith word in a sentence and wi , j for the word sequence from wl to wj(i<j ) . 
k her second child the bus
Figure 2: Example complete-links
A complete-link has direction . A complete-link on wij is said to be " rightward " if the outermost relation is  ( wi--+wj )  , and " leftward " if the relation is ( wie--wj )  . Unit complete-link is defined on a string of two adjacent words  , wi , ; + l . In Figure 2 , ( a ) is a rightward complete-link , and both of ( b ) and ( c ) are leftward ones . 
bird in the cage the bus book
Figure 3: Example complete-sequences
A complete-sequence is a sequence of 0 or more adjacent complete-links that have the same direction  . A unit complete-sequence is defined on a string of one word  . It is 0 sequence of complete-links . The direction of a complete-sequence is determined by the direction of the component complete-links  . In Figure 3 ,   ( a ) is a rightward complete-sequence composed of two complete-links  , and ( b ) is a left ward one .   ( c ) is a complete-sequence composed of zero complete -links  , and it can be both leftward and rightward . 
The word of " complete " means that the dependency relations on the inner words are completed and that consequently there is no need to process further on them  . From now on , we use Lr(i , j)/Lt(i , j ) for rightward/leftward complete-links and Sr ( i , j)/St(i , j ) for right-ward/leftward complete-sequences on wi , j . 
Any complete-link on wi , j can be viewed as the following combination . 
? L ~( i , j ) : ( wi--+wj ) , S ~( i , m ) , St(m+l , j)?L l(i , j ) : ( wie--wj ) , St(i , m ) , St(m+l , j ) for am(i < m < j) . 
Otherwise , the set of dependencies does not satisfy the conditions of no crossing  , no cycle and no multiple heads and is not a complete-link any more  . 
Similarly , any complete-sequence on wi , j can be viewed as the following combination . 
? S~(i , j ) : Sr(i , m ) , L ~( m , j ) ? St(i , j ) : Lt(i , m ) , St(m , j ) for am(i < m < j) . 
In the case of complete-sequence , we can prevent multiple constructions of the same restriction  . 
Figure 4: Abstract representation f /)
Figure 4 shows an abstract representation fa/ ) of an nword sentence . When wk(1 < k <_n ) is the head of the sentence , any D of the sentence can be represented by a St ( l , EOS ) uniquely by the assumption that there is always the dependency relation  , ( wk+--wEos ) . 
3 Reestimation Algorithm
The reestimation algorithm is a variation of Inside-Outside algorithm  ( Jelinek et al , 1990) adapted to dependency grammar . In this section we first define the insideoutside probabilities of complete-links and complete -sequences  , and then describe the reestimation algorithm based on them  2  . 
In the followings , ~ indicates inside probability and a , is for outside probability . The superscripts , l and s , are used for " complete-link " and " complete -sequence " respectively  . The subscripts indicate direction : r for " rightward " and 
I for " leftward ".
The inside probabilities of complete-links(n~(i , j ) , Lt(i , j )) and complete-sequences ( Sr(i , j ) , Sl(i , j )) are as follows . 
j-1/3 t ~( i , j ) = ~ p(wi--+wj)/3~(i , m)t3~(m+1 , j) . 
rn = ij--I/3\[(i , j ) = Ep(wi6 . -wj ) t3~(i , m ) 13 ?( m+1, j) . 
rn = ij--1fl~(i , j ) = ~/ 3~( i , ( m,j).

J/3?(i , j ) = ~/ 3\[(i,m ) t3?(m,j).

The basis probabilities are :/31 r(i , i+1) = p(wi"~wi+l)/3\[(i , i+1) = p(wi(-"wi+l)/3~(i , i ) = f l ?( i , i ) = 1/37 (1 , EOS ) = p(wL ,   ) ~ A little more detailed explanation of the expressions can be found in  ( Lee and Choi ,  1997) . 
/3~(i , i+1) = p(L~(i , i+1)) = p(wi+wi+t)/37 ( i , i+1) = p(Lt(i , i+1)) = p(wi+--wi+t ) . 
/37(1 , EOS ) is the sentence probability because every dependency analysis  , D , is represented by a St(l , EOS ) and /37 (1 , EOS ) is sum of the probability of every St(l , EOS ) . 
probabilities for complete-(i , j )) and complete-sequences are as follows . 
The outside links ( L, . (i , j ) , Lt(S ~ ( i , j ) , St(i , j )) i at ~( i , j ) = nc~(v , j)/3 i ~( v , i ) . 
a ~( i , h)/3 ?( j , h).
h = ja ~( i , j ) = ~ a~(i , h)/3tr(j , h)h = j + l + at r(i , h)/3 i~(j+1 , h)p(wi-+Wh ) + al(i , h)/3?(j+1 , h)p(wi~wh ) . 
i-Ia ~ ( i , j ) = ~ a~(v , j)fl~(v , i)v----I+dr(v , j)Z ; ( v , i-t ) p(wvwA+al(v , j)t3;(v , i-1) p(wve-wj ) . 
The basis probability is ~(1, EOS ) = 1.
Given a training corpus , the initial grammar is just a list of all pairs of unique words in the corpus  . The initial pairs represent the tentative head -dependent relations of the words  . 
And the initial probabilities of the pairs can be given randomly  . The training starts with the initial grammar . The train corpus is analyzed with the grammar and the occurrence frequency of each dependency relation is calculated  . Based on the frequencies , probabilities of dependency relations are recalculated by 
C(wp--+w ~) The process w , ) = C ( w continues until the entropy of the training corpus becomes the minimum  . The frequency of occurrence , C(wi--+wj) , is calculated by w ) = -+ 1 t ? ? t = p(wt ,  . )a . ( , ,3)/3~(i , j ) where O ~( wi ~ wj , D , wl , n ) is 1 if the dependency relation , ( wi--+wj ) , is used in the D , quency of the dependency relation , ( wi+-wj ) , is computed by ~---- L---o ~ l(i , j)~\[(i , j ) . 
4 Preliminary experiments
We have experimented with three language models , trigram model ( TRI ) , bigram model ( BI ) , and the proposed model ( DEP ) on a raw corpus extracted from KAIST corpus3 . The raw corpus consists of 1 , 589 sentences with 13 , 139 words , describing animal life in nature . We randomly divided the corpus into two parts : a training set of  1  , 445 sentences and a test set of 144 sentences . And we made 15 partial training sets which include the firsts sentences in the whole training set  , for s ranging from 100 to 1 , 445 sentences . We trained the three language models for each partial training set  , and tested the training and the test corpus entropies  . 
TRI and BI was trained by counting the occurrence of trigrams and bigrams respectively  . 
DEP was trained by running the reestimation algorithm iteratively until it converges to an optimal dependency grammar  . On the average , 26 iterations were done for the training sets . 
Smoothing is needed for language modeling due to the sparse data problem  . It is to compensate for the overestimated and the underestimated probabilities  . Smoothing method itself is an important factor . But our goal is not to find out a better smoothing method  . So we fixed on an interpolation method and applied it for the three models  . It can be represented as ( McCandless , 1994) . . . , w,-x ) =,\ P , ( wilw,-,+l, . . . , wi_l ) + (1- . . . , where = C(w l , . . . , w,-1)
C(w , , . . . , + K , "
The Ks is the global smoothing factor . The bigger the Ks , the larger the degree of smoothing . 
For the experiments we used 2 for Ks.
We take the performance of a language model to be its crossentropy on test corpus  , 1s
IV lE-l?g2Pm ( Si ) i = 13K AIST ( Korean Advanced Institute of Science and Technology  ) corpus has been under construction since 1994 . It consists of raw text collection (45 , 000 , 000 words ) , POS-tagged collection (6 , 750 , 000 words ) , and tree-tagged collection (30 , 000 sentences ) at present . 
where the test corpus contains a total of IV\] words and is composed of S sentences  . 
3.4 ii ! I3.2 32.8> "2.6 O.
2 . 4 u ~2 . 2 ~( DEP model ) o2a ( TRI model ) i1 . 8 1 . 6 1 . 4 0 200 400 600 800 1000 1200 1400 600
No . of training sentences
Figure 5: Training corpus entropies
Figure 5 shows the training corpus entropies of the three models  . It is not surprising that DEP performs better than BI  . DEP can be thought of as a kind of linguistic bigram model in which long distance dependencies can be represented through the head-dependent relations between words  . TRI shows better performance than both BI and DEP  . We think it is because TRI overfits the training corpus  , judging from the experimental results for the test corpus  . 
9 . 5 iIIIIII 8 . 5 uJ 7 . 5  . =(( TRI model ) 7/ ( DEP model ) o 6 . 5 a i I I I I I 0   200   400   600   800   1000   1200   1400   1600 
No . of training sentences
Figure 6: Test corpus entropies
For the test corpus , BI shows slightly better performance than TRI as depicted in Figure  6  . Increase in the order of ngram from two to to three shows no gains in entropy reduction  . DEP , however , Shows still better performance than the ngram models  . It shows about 11 . 5% entropy reduction to BI and about 11% entropy reduction to TRI . Figure 7 shows the entropies for the mixed corpus of training and test sets  . From the results , we can see that head-dependent relations between words are more useful information than the naive ngram sequences  , for language modeling . We can see also that the reestimation algorithm can find out properly the hidden head-dependent relations between words  , from a raw corpus . 
726, r,f-u J(no
Z ( BI model )   ( TRI model )   ( DEP model )  0 200 400 600 800 1000 1200 1400
No . of training sentences
Figure 7: Mixed corpus entropiesi ! iii ! ( DEP model ) o ( TRI model ) "*'-- rTIIIIII 200   400   600   800   1000   1200   1400   1600 
No . of training sentences
Figure 8: Model size
Related to the size of model , however , DEP has much more parameters than TRI and BI as depicted in Figure  8  . This can be a serious problem when we create a language model from a large body of text  . In the experiments , however , DEP used the grammar acquired automatically as it is  . In the grammar , many interword dependencies have probabilities near  0  . If we exclude such dependencies a was experimented for ngrams by Seymore and Rosenfeld  ( 1996 )  , we may get much more compact DEP model with very slight increase in entropy  . 
5 Conclusions
In this paper , we presented a language model based on a kind of simple dependency grammar  . The grammar consists of head-dependent relations between words and can be learned automatically from a raw corpus by the reestimation algorithm which is also introduced in this paper  . By the preliminary experiments , it was shown that the proposed language model performs better than ngram models in test corpus entropy  . This means that the reestimation algorithm can find out the hidden information of head-dependent relation between words in a raw corpus  , and the information is more useful than the naive word sequences of ngram  , for language modeling . 
We are planning to experiment the performance of the proposed language model for large corpus  , for various domains , and with various smoothing methods . For the size of the model , we are planning to test the effects of excluding the dependency relations with near zero probabilities  . 

P . F . Brown , V . J . Della Pietra , P . V . deSouza , J . C . Lai , and R . L . Mercer .  1992 . " Class-Based ngram Models of Natural Language " . 
Computational Linguistics , 18(4): 467-480.
C . Chang and C . Chen .  1996 . " Application Issues of SA-class Bigram Language Models "  . 
Computer Processing of Oriental Languages , io (1): i-i5 . 
S . F . Chen .  1996 . " Building Probabilistic Models for Natural Language "  . Ph . D . thesis , Havard University , Cambridge , Massachusetts . 
F . Jelinek , J . D . Lafferty , and R . L . Mercer . 
1990 . " Basic Methods of Probabilistic ContextFree Grammars "  . Technical report , IBM-T . J . Watson Research Center . 
K . Lari and S . J . Young .  1991 . " Applications of stochastic on text-free grammars using the insideoutside algorithm "  . Computer Speech and Language , 5:237-257 . 
S . Lee and K . Choi .  1997 . " Reestimation and Best First Parsing Algorithm for Probabilistic Dependency Grammar "  . In WVLC-5, pages 1121 . 
M . K . McCandless .  1994 . " Automatic Acquisition of Language Models for Speech Recognition "  . Master's thesis , Massachusetts Institute of Technology . 
M . Meteer and J . R . Rohlicek .  1993 . " Statistical Language Modeling Combining Ngram and Context-free Grammars "  . In ICASSP-93, volume II , pages 3740, January . 
K . Seymore and R . Rosenfeld .  1996 . "Scalable Trigram Backoff Language Models " . Technical Report CMU-CS-96-139, Carnegie Mellon

S . Sneff .  1992 . " TINA : A natural anguage system for spoken language applications "  . Computational Linguistics , 18(1):61-86 . 

