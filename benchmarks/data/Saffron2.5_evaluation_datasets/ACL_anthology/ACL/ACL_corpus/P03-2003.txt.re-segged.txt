On the Applicability of Global Index Grammars
Jose ? M . Castan?o
Computer Science Department
Brandeis University


We investigate Global Index Grammars ( GIGs ) , a grammar formalism that uses a stack of indices associated with productions and has restricted context-sensitive power  . We discuss some of the structural descriptions that GIGs can generate compared with those generated by LIGs  . We show also how GIGs can represent structural descriptions corresponding to HPSGs  ( Pollard and Sag , 1994) schemas . 
1 Introduction
The notion of Mildly context sensitivity was introduced in  ( Joshi ,  1985 ) as a possible model to express the required properties of formalisms that might describe Natural Language  ( NL ) phenomena . It requires three properties : 1 a ) constant growth property ( or the stronger semi-linearity property )  ; b ) polynomial parsability ; c ) limited cross-serial dependencies , i . e . some limited context sensitivity . The canonical NL problems which exceed context free power are : multiple agreements  , reduplication , crossing de-pendencies . 2 Mildly Context-sensitive Languages ( MCSLs ) have been characterized by a geometric hierarchy of grammar levels  . A level 2 MCSL ( eg . 
1See for example , ( Joshi et al ,  1991) , ( Weir ,  1988) . 
2 However other phenomena ( e . g . scrambling , Georgian Case and Chinese numbers ) might be considered to be beyond certain mildly context-sensitive formalisms  . 
TALs/LILs ) is able to capture up to 4 counting dependencies ( includes L4 = a n b n c n d n n ? 1 but not L5 = anbncndnenn ? 1  )  . They were proven to have recognition algorithms with time complexity O  ( n 6 )   ( Satta ,  1994) . In general for a level-kMCSL the recognition problem is in O  ( n3?2 k?1 ) and the descriptive power regarding counting dependencies is  2k   ( Weir ,  1988) . 
Even the descriptive power of level 2 MCSLs ( Tree Adjoining Grammars ( TAGs )  , Linear In-dexed Grammars ( LIGs ) , Combinatory Categorial Grammars ( CCGs ) might be considered insufficient for some NL problems  , therefore there have been many proposals3 to extend or modify them . On our view the possibility of modeling coordination phenomena is probably the most crucial in this respect  . 
In ( Castan?o ,  2003 ) we introduced Global Index Grammars ( GIGs ) - and GILs the corresponding languages-as an alternative grammar formalism that has a restricted context sensitive power  . We showed that GIGs have enough descriptive power to capture the three phenomena mentioned above  ( reduplication , multiple agreements , crossed agreements ) in their generalized forms . Recognition of the language generated by a GIG is in bounded polynomial time : O  ( n 6 )  . 
We presented a Chomsky-Schu?tzenberger representation theorem for GILs  . In ( Castan?o , 2003c ) we presented the equivalent automaton model : LR-2PDA and provided a characterization the-3There are extensions or modifications of TAGs , CCGs , IGs , and many other proposals that would be impossible to mention here  . 
orems of GILs in terms of the LR-2PDA and GIGs . The family of GILs is an Abstract Fam-ily of Language  . 
The goal of this paper is to show the relevance of GIGs for NL modeling and processing  . This should not be understood as claim to propose GIGs as a grammar model with ? linguistic content ? that competes with grammar models such as HPSG or LFG  . It should be rather seen as a formal language resource which can be used to model and process NL phenomena beyond contextfree  , or beyond the level 2 MCSLs ( like those mentioned above ) or to compile grammars created in other framework into GIGs  . LIGs played a similar role to model the treatment of the SLASH feature in GPSGs and HPSGs  , and to compile TAGs for parsing . GIGs offer additional descriptive power as compared to LIGs or TAGs regarding the canonical NL problems mentioned above  , and the same computational cost in terms of asymptotic complexity  . They also offer additional descriptive power in terms of the structural descriptions they can generate for the same set of string languages  , being able to produce dependent paths . 4 This paper is organized as follows : section 2 reviews Global Index Grammars and their properties and we give examples of its weak descriptive power  . Section 3 discusses the relevance of the strong descriptive power of GIGs  . We discuss the structural description for the palin-drome  , copy and the multiple copies languages ww+w ??? . Finally in section 4 we discuss how this descriptive power can be used to encode HPSGs schemata  . 
2 Global Index Grammars 2 . 1 Linear Indexed Grammars Indexed grammars , (IGs ) ( Aho ,  1968) , and Linear Index Grammars , ( LIGs ; LILs ) ( Gazdar ,  1988) , have the capability to associate stacks of indices with symbols in the grammar rules  . IGs are not semilinear . LIGs are Indexed Grammars with an additional constraint in the form of the productions : the stack of indices can be ?  trans-4For the notion of dependent paths see for instance ( Vijay-Shanker et al , 1987) or ( Joshi ,  2000) . 
mitted ? only to one nonterminal . As a consequence they are semiline ar and belong to the class of MCSGs  . The class of LILs contains L4 but not L5 ( see above )  . 
A Linear Indexed Grammar is a 5-tuple ( V , T , I , P , S ) , where V is the set of variables , T the set of terminals , I the set of indices , S in V is the start symbol , and P is a finite set of productions of the form , where A , B ? V ,  ? , ??( V?T )? , i ? I : a . A [ . .] ? ? B [ . .] ? b . A[i . .] ? ? B [ . .] ? c . A [ . .] ? ? B[i . .] ? Example 1 L(Gwcw ) =  wcw  w ?  a , b ? , Gww = ( S , R , a , b , i , j , S , P ) and P is : 1 . S [ . .] ? aS[i . .] 2 . S [ . .] ? bS[j . .] 3 . S [ . .] ? cR [ . .] 4 . R[i . .] ? R[ . .]a 5 . R[j . .] ? R[ . .]b 6 . R [] ? ?2 . 2 Global Indexed Grammars GIGs use the stack of indices as a global control structure  . This formalism provides a global but restricted context that can be updated at any local point in the derivation  . GIGs are a kind of regulated rewriting mechanisms  ( Dassow and Pa?un ,  1989 ) with global context and history of the derivation  ( or ordered derivation ) as the main characteristics of its regulating device  . 
The introduction of indices in the derivation is restricted to rules that have terminals in the right hand side  . An additional constraint that is imposed on GIGs is strict leftmost derivation whenever indices are introduced or removed by the derivation  . 
Definition 1 AGIG is a 6-tuple G = ( N , T , I , S , # , P ) where N , T , I are finite pairwise disjoint sets and 1 ) N are nonterminals 2 ) T are terminals 3 ) I a set of stack indices 4 ) S ? N is the start symbol 5 )  #is the start stack symbol ( not in I , N , T ) and 6) P is a finite set of productions , having the following form , 5 where 5The notation in the rules makes explicit that operation on the stack is associated to the production and neither to terminals nor to nonterminals  . It also makes explicit that the operations are associated to the computation of a Dyck language  ( using such notation as used in e . g . ( Harrison , 1978)) . In another notation : a . 1[ y . .]A ? [ y . .]?, a . 2[ y . .]A ? [ y . .]?, b .  [ . .] A ? [ x . .]a ? and c . [ x . .]A ? [ . .]? x ? I , y ? I ? # , A ? N ,  ? , ??( N?T )? and a ? T . 
a . iA ??? ( epsilon ) a . iiA?[y ]? ( epsilon with constraints ) b . A ? x a ? ( push ) c . A ? ? x?a ? ( pop ) Note the difference between push ( type b ) and pop rules ( type c ) : push rules require the right hand side of the rule to contain a terminal in the first position  . Pop rules do not require a terminal at all . That constraint on push rules is a crucial property of GIGs  . Derivations in a GIG are similar to those in a CFG except that it is possible to modify a string of indices  . We define the derives relation ? on sential forms  , which are strings in I ? #( N ? T ) ? as follows . Let ? and ? bein(N ? T ) ? , ? be in I ? , x in I , w be in T ? and X i in ( N ? T ) . 
1 . If A??X1 . . . X n is a production of type ( a . )( i . e . ? = ? or ? = [ x ], x ? I ) then : i . ?#?A ? ??? X1 . . . X n?i i . x?#?A???x?#?X1 . . . X n?2 . If A??a X1 . . . X n is a production of type ( b . ) or push : ? = x , x ? I , then : ? #wA ? ? ? x?#wa X1 . . . X n?3 . If A??X1 . . . X n is a production of type ( c . ) or pop : ? = x ? , x ? I , then : x ? #wA ? ? ? ? #wX1 . . . . . . Xn ? The reflexive and transitive closure of ? is denoted  , as usual by ?? . We define the language of a GIG , G , L ( G ) to be : w  #S ? ? #w and w is in T ? The main difference between  , IGs , LIGs and GIGs , corresponds to the interpretation of the derives relation relative to the behavior of the stack of indices  . In IGs the stacks of indices are distributed over the nonterminals of the right hand side of the rule  . In LIGs , indices are associated with only one nonterminal at right hand side of the rule  . This produces the effect that there is only one stack affected at each derivation step  , with the consequence of these milinearity property of LILs  . GIGs share this uniqueness of the stack with LIGs : there is only one stack to be considered  . Unlike LIGs and IGs the stack of indices is independent of nonterminals in the GIG case  . GIGs can have rules where the right hand side of the rule is composed only of terminals and affect the stack of indices  . Indeed push rules ( type b ) are constrained to start the righthand side with a terminal as specified in  ( 6 . b ) in the GIG definition . The derives definition requires a leftmost derivation for those rules  ( push and pop rules ) that affect the stack of indices . The constraint imposed on the push productions can be seen as constraining the context sensitive dependencies to the introduction of lexical information  . This constraint prevents GIGs from being equivalent to a Turing Machine as is shown in  ( Castan?o , 2003c ) . 
2.2.1 Examples
The following example shows that GILs contain a language not contained in LILs  , nor in the family of MCSLs . This language is relevant for modeling coordination in NL  . 
Example 2 ( Multiple Copies).
L(Gwwn ) = w w + w ? a,b ?
Gwwn = ( S , R , A , B , C , L , a , b , i , j , S , # , P ) and where P is : S ? ASBSCC ? RCL
R ? ? i
RAR ? ? j
RBR ? [#] ?
A ? i a B ? j bL ? ? i
LaaL ? ? j
Lbb
The derivation of a babab:#S?#AS?i  #aS?i  #aBS ? ji#abS?ji#abC?ji#abRC?i#abRBC ? # abRABC ? #abABC?i  #abaBC?ji  #ababC?ji  #ababL ? i  #abab Lb ?  #ababab The next example shows the MIX  ( or Bach ) language . ( Gazdar , 1988) conjectured the MIX language is not an IL . GILs are semilinear , ( Castan?o , 2003c ) therefore ILs and GILs could be incomparable under set inclusion  . 
Example 3 ( MIX language) . L(Gmix ) = w w ? a , b , c ? and aw = bw = cw?1Gmix = ( S , D , F , L , a , b , c , i , j , k , l , m , n , S , # , P ) where P is:
S?FSDSLS ? F?icF?j bF?ka
D ? ? i a S b b S a D ? ? j a S c c S a D ? ? k b S c c S b
D ? l a S b b S a D ? m a S c c S a D ? n b S c c S b
L??lc L??mb L??na
The following example shows that the family of GILs contains languages which do not belong to the MCSL family  . 
Example 4 ( Multiple dependencies )
L(G g d p ) = a n ( b n c n ) + n ? 1,
G g d p = ( S , A , R , E , O , L , a , b , c , i , S , # , P ) and P is:
S ? ARA ? aAEA ? aE ? i b
R ? i bLL?ORCC ? ? i cCc
O ? ? icOEc
The derivation of the string a abbcc b cc c shows five dependencies  . 
 #S ? #AR ? #aAER ? #aa ER ? i  #aabR ? ii  #aabbL ? ii  #aabbOR?i  #aabbcOER ?  #aabbccER?i  #aabbcc bR ? a abbcc b b c c  2  . 3 GILs Recognition The recognition algorithm for GILs we presented in  ( Castan?o , 2003) is an extension of Earley?s algorithm ( cf . ( Earley , 1970)) for CFLs . It has to be modified to perform the computations of the stack of indices in a GIG  . In ( Castan?o , 2003) a graph-structured stack ( Tomita ,  1987 ) was used to efficiently represent ambiguous index operations in a GIG stack  . Earley items are modified adding three parameters  ?  , c , o : [? , c , o , A ? ? ? A ? , i , j ] The first two represent a pointer to an active node in the graph-structured stack  ( ? ? I and c ? n )  . The third parameter ( o?n ) is used to record the ordering of the rules affecting the stack  . 
The O ( n 6 ) time-complexity of this algorithm reported in ( Castan?o , 2003) can be easily verified . The complete operation is typically the costly one in an Earley type algorithm  . It can be verified that there are at most n6 instances of the indices ( c1 , c2 , o , i , k , j ) involved in this operation . The counter parameters c1 and c2 , might be state bound , even for grammars with ambiguous indexing . In such cases the time complexity would be determined by the CFG backbone properties  . The computation of the operations on the graph -structured stack of indices are performed at a constant time where the constant is determined by the size of the index vocabulary  . 
O(n6) is the worst case ; O ( n3 ) holds for grammars with state-bound indexing ( which includes unambiguous indexing ) 6 ; O ( n2 ) holds for unambiguous contextfree backbone grammars with state-bound indexing and O  ( n ) for bounded-state7 contextfree backbone grammars with state-bound indexing  . 
3 GIGs and structural description ( Gazdar ,  1988 ) introduces Linear Indexed Grammars and discusses its applicability to Natural Language problems  . This discussion is addressed not in terms of weak generative capacity but in terms of strong -generative capacity  . 
Similar approaches are also presented in ( Vijay-Shanker et al , 1987) and ( Joshi , 2000) ( see ( Miller ,  1999 ) concerning weak and strong generative capacity )  . In this section we review some of the abstract configurations that are argued for in  ( Gazdar ,  1988) . 
3.1 The palind rome language
CFGs can recognize the language wwRw ??? but they cannot generate the structural description depicted in figure  1   ( we follow Gazdar?s notation : the leftmost element within the brackets corresponds to the top of the stack  ) : a [ . .] [ a ] [ b , a ] [ c , b , a]bcd[d , c , b , a ] dc [ b , a ] ba[a ] [ . .] [ c , b , a ] Figure 1: A noncontextfree structural description for the language w wR  ( Gazdar ,  1988 ) Gazdar suggests that such configuration would be necessary to represent Sc and in avian  6Unambiguous indexing should be understood as those grammars that produce for each string in the language a unique indexing derivation  . 
7Context Free grammars where the set of items in each state set is bounded by a constant  . 
unbounded dependencies . Such an structure can be obtained using a GIG ( and of course a LIG )  . 
But the mirror image of that structure cannot be generated by a GIG because it would require to allow push productions with a nonterminal in the first position of the right hand side  . However the English adjective constructions that Gazdar argues that can motivate the LIG derivation  , can be obtained with the following GIG productions as shown in figure  2  . 
Example 5 ( Comparative Construction).
AP ? APNPAP ? A ? A ? ? A ? A
A ? i a A ? j b A ? k c
NP ? ? i a NP NP ? ? j b NP NP ? ? k c NP




AA





Aba[a,b,c]a NP bNP
NP cc [ . .] [ b,c ] [ b,c ] [ c ] [ . .] [ c ] [ . .] Figure 2: A GIG structural description for the language w wR It should be noted that the operations on indices follow the reverse order as in the LIG case  . On the other hand , it can be noticed also that the introduction of indices is dependent on the presence of lexical information and its transmission is not carried through a topdown spine  , as in the LIG or TAG cases . The arrows show the leftmost derivation order that is required by the operations on the stack  . 
3.2 The Copy Language
Gazdar presents two possible LIG structural descriptions for the copy language  . Similar structural descriptions can be obtained using GIGs  . 
However hear gues that another tree structure could be more appropriate for some Natural Language phenomenon that might be modeled with a copy language  . Such structure cannot be generated by a LIG , and can by an IG ( see ( Castan?o , 2003b ) for a complete discussion and comparasion of GIG and LIG generated trees  )  . 
GIGs cannot produce this structural description , but they can generate the one presented in figure  3  , where the arrows depict the leftmost derivation order  . GIGs can also produce similar structural descriptions for the language of multiple copies  ( the language ww+w ??? as shown in figure 4 , corresponding to the grammar shown in example 2 . 
[] [] b[a]a[b , a][a]abcd[b , a ] a b [ a , b , a ] [ b , a , b , a ] [ b , a , b , a ] [ a , b , a ] Figure 3: AGIG structural description for the copy language [][][][] [ a][a][c  , b , a][b , a ] [ b , a ] [ b , a]ab[a ] [ b , a ] a b ? a b [ b , a ] [ a ] [ b , a]b [ a]a ba [ a , b , a][b , a , b , a ] [ a , b , a ] [ b , a , b , a ] [ b , a , b , a ] [ b , a , b , a ] [ a , b , a ] [ b , a , b , a ] [ b , a , b , a][a , b , a ] [ a , b , a ] abaabb Figure 4: A GIG structural description for the multiple copy language  4 GIGs and HPSGs We showed in the last section how GIGs can produce structural descriptions similar to those of LIGs  , and others which are beyond LIGs and TAGs descriptive power  . Those structural descriptions corresponding to figure  1 we recorrelated to the use of the SLASH feature in GPSGs and HPSGs  . In this section we will show how the structural description power of GIGs  , is not only able to capture those phenomena but also additional structural descriptions  , compatible with those generated by HPSGs . This follows from the ability of GIGs to capture dependencies through different paths in the derivation  . 
There has been some work compiling HPSGs into TAGs  ( cf . ( Kasper et al ,  1995) , ( Becker and Lopez ,  2000)) . One of the motivations was the potential to improve the processing efficiency of HPSG  , performing HPSG derivations at compile time . Such compilation process allowed to identify significant parts of HPSG grammars that were mildly context-sensitive  . 
We will introduce informally some slight modifications to the operations on the stacks performed by a GIG  . We will allow the productions of a GIG to be annotated with finite strings in I?I ? instead of single symbols  . This does not change the power of the formalism . It is a standard change in PDAs ( cf . ( Harrison ,  1978 ) ) to allow to push/pop several symbols from the stack  . Also the symbols will be interpreted relative to the elements in the top of the stack  ( as a Dyck set )  . Therefore different derivations might be produced using the same production according to what are the topmost elements of the stack  . This is exemplified with the productions X??nv x and X ? [ n]vx  , in particular in the first three cases where different actions are taken  ( the actions are explained in the parenthesis ) : nn ?  #wX ? ? ? nvvn ?  #wx ? ( pop n and push v ) nv ? ? #wX ? ? ? nv ?  #wx ? ( pop n and v ? ) vn ? #wX ? ? ? nvvn ? vn ?  #wx ? ( pushn ? and v ) n ?  #wX ? ? [ n ] vvn ?  #wx ? ( check and push ) We exemplify how GIGs can generate similar structural descriptions as HPSGs do  , in a very over simplified and abstract way . We will ignore many details and try give an rough idea on how the transmission of features can be carried out from the lexical items by the GIG stack  , obtaining very similar structural descriptions . 

Figure 5 depicts the tree structure corresponding to the Head-Subject Schema in HPSG  ( Pollard and Sag ,  1994) . 

HEAD12 HEAD < >



Figure 5: Head-Subject Schema
Figure 6 shows an equivalent structural description corresponding to the GIG productions and derivation shown in the next example  ( which might correspond to an intransitive verb )  . The arrows indicate how the transmission of features is encoded in the leftmost derivation order  , an how the elements contained in the stack can be correlated to constituents or lexical items  ( terminal symbols ) in a constituent recognition process . 


XP XP

Yy[n ..] [ n . .] [ ..] [ v . .] [ v . .] [ v . .]
Figure 6: Head-Subject in GIG format
Example 6 ( Intransitive verb ) XP ? YP XP
XP ? X Y P ? Y X ? ? nv xY ? n y #XP ? #YP XP ?  #yXP ? n#Y XP ? n  #yX ? v#yx 
Head-Comps-Schema Figure 7 shows the tree structure corresponding to the Head- 
Complement schema in HPSG.
HEAD1HEAD <2>
H <> 13, n
CC1 n23 n

Figure 7: Head-Comps Schematree representation The following GIG productions generate the structural description corresponding to figure  8  , where the initial configuration of the stack is assumed to be [ n]: 
Example 7 ( transitive verb).
XP ? X CP CP ? Y CP X ? ? nv n?x CP ? ?
Y?ny
The derivation : n  #XP ? n  #XCP ? n ? v#x CP ? n ? v#xY CP ? v#xyCP ? v#xy 
CPXP
X x C P Y y [ n ] [ nv ] [ nv ] ? [ v ] [ v ] [ v  ] 
Figure 8: Head-Comp in GIG format
The productions of example 8 ( which use some of the previous examples ) generate the structural description represented in figure  9  , corresponding to the derivation given in example 8  . We show the contents of the stack when each lexical item is introduced in the derivation  . 
Example 8 ( SLASH in GIG format).
XP ? YP XP XP ? X CP XP ? X XP
CP ? YPCPX ? ? nvn ? hates CP ? ?
X ? ? nv ? know X ? ? nvv ? claims
YP ? n

A derivation of ? Kimweknow Sandy claims Dana hates ?: #XP ? #YP XP ? n  #Kim XP ? n  #Kim YP XP ? nn  #Kim we XP ? nn  #Kim we X XP ? v ? n  #Kim we know XP ? v?n#Kim we know YPXP ? nv?n#Kim we knowS and yXP ? nv?n#Kim we knowS and yXXP ? v?n  #Kimweknow S and y claims XP ? v ? n  #Kim we know S and y claims Y PXP ? nv ? n  #Kim we know S and y claims Dana XP ?? #Kimweknow Sandy claims Dana hates Finally the last example and figure  10 show how coordination can be encoded . 
Example 9 ( SLASH and Coordination )
XP ? YP XP XP ? X CP XP ? X XP
CP ? YPCPCP ? ? X ? [ nv?n ] cvisit
X ? ? nvn ? talk to C ? and CXP ? XP CXP
CXP ? CXPX ? ? nv?didYP ? n
Whoyou 5 Conclusions
We presented GIGs and GILs and showed the descriptive power of GIGs is beyond CFGs  . 
CFLs are properly included in GILs by definition . We showed also that GIGs include

YP XP



XP XP





YP [ n ] [ nn ] [ nvn ] [ nvn ] [] we know
S and y claims
Danahates
Kim ? [ n]
CP [ vn ] [ vn ] []
Figure 9: SLASH in GIG format some languages that are not in the LIL/TAL family  . GILs do include those languages that are beyond contextfree and might be required for NL modelling  . The similarity between GIGs and LIGs , suggests that LILs might be included in GILs . We presented a succinct comparison of the structural descriptions that can be generated both by LIGs and GIGs  , we have shown that GIGs generate structural descriptions for the copy language which cannot be generated by LIGs  . We showed also that this is the case for other languages that can be generated by both LIGs and GIGs  . This corresponds to the ability of GIGs to generate dependent paths without copying the stack  . We have shown also that those nonlocal relationships that are usually encoded in HPSGs as feature transmission  , can be encoded in GIGs using its stack , exploiting the ability of Global stacks to encode dependencies through dependent paths and not only through a spine  . 

Thanks to J . Pustejovsky for his continuous support and encouragement on this project  . Many thanks also to the anonymous reviewers who provided many helpful comments  . This work was partially supported by NLMG rant [  ] 

XP XP
XPYP[nv]X did
Whoyou
YP visit

CXP and

XP talk to [ nvn ] ? ? [ nvn][][n][nv n ] 
CP [ cnvn ]
Figure 10: SLASH in GIG format
R01 LM06649-02.

A . V . Aho .  1968 . Indexed grammars - an extension of contextfree grammars  . Journal of the Association for Computing Machinery  ,  15(4):647?671 . 
T . Becker and P . Lopez .  2000 . Adapting hpsg-to-tag compilation to wide-coverage grammars  . 
J . Castan?o .  2003 . GIGs : Restricted context-sensitive descriptive power in bounded polynomial-time  . In Proc . of Cicling 2003,
Mexico City , February 1622.
J . Castan?o . 2003b . Global index grammars and descriptive power . In R . Oehrle and J . Rogers , editors , Proc . of Mathematics of Language , MOL8 . 
Bloomington , Indiana , June.
J . Castan?o . 2003c . LR Parsing for Global Index Languages ( GILs ) . In In Proceeding of CIAA 2003,
Santa Barbara , CA.
J . Dassow and G . Pa?un .  1989 . Regulated Rewriting in Formal Language Theory . Springer , Berlin,
Heidelberg , New York.
J . Earley .  1970 . An Efficient Context-free Parsing Algorithm . Communications of the ACM , 13:94?102 . 
G . Gazdar .  1988 . Applicability of indexed grammars to natural languages  . In U . Reyle and C . Rohrer , editors , Natural Language Parsing and Linguistic Theories , pages 69?94 . D . Reidel , Dordrecht . 
M . H . Harrison .  1978 . Introduction to Formal Language Theory . Addison-Wesley Publishing Company , Inc . , Reading , MA . 
A . Joshi , K . Vijay-Shanker , and D . Weir .  1991 . The convergence of mildly context-sensitive grammatical formalisms  . In Peter Sells , Stuart Shieber , and Thomas Wasow , editors , Foundational issues in natural language processing  , pages 31?81 . MIT
Press , Cambridge , MA.
A . Joshi .  1985 . Tree adjoining grammars : How much context sensitivity is required to provide reasonable structural description ? In D  . Dowty , L . Karttunen , and A . Zwicky , editors , Natural language processing : psycholinguistic , computational and theoretical perspectives , pages 206?250 . Chicago
University Press , New York.
A . Joshi .  2000 . Relationship between strong and weak generative power of formal systems  . In Proceedings of the Fifth International Workshop on Tree Adjoining Grammars and Related Formalisms  ( TAG+5 )  , pages 107?114 , Paris , France . 
R . Kasper , B . Kiefer , K . Netter , and K . Vijay-Shanker .  1995 . Compilation of HPSG into TAG . 
In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics  , pages 92?99 . Cambridge , Mass . 
P . Miller .  1999 . Strong Generative Capacity . CSLI Publications , Stanford University , Stanford CA , 

C . Pollard and I . A . Sag .  1994 . Head-driven Phrase Structure Grammar . University of Chicago Press,
Chicago , IL.
G . Satta .  1994 . Tree-adjoining grammar parsing and boolean matrix multiplication  . Computational linguistics , 20, No .  2 . 
M . Tomita .  1987 . An efficient e augmented-context-free parsing algorithm  . Computational linguistics , 13:31?46 . 
K . Vijay-Shanker , D . J . Weir , and A . K . Joshi .  1987 . 
Characterizing structural descriptions produced by various grammatical formalisms  . In Proc . of the 25th ACL , pages 104?111, Stanford , CA . 
D . Weir .  1988 . Characterizing mildly context-sensitive grammar formalisms  . Ph . D . thesis , University of Pennsylvania . 
