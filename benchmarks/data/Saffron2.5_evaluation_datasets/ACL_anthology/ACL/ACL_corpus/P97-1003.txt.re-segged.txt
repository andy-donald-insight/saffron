Three Generative , Lexicalised Models for Statistical Parsing
Michael Collins *
Dept . of Computer and Information Science
University of Pennsylvania
Philadelphia , PA , 19104, U.S.A.
m collins ~ gradient , c is . upenn , edu
Abstract
In this paper we first propose a new statistical parsing model  , which is a generative model of lexicalised contextfree grammar  . We then extend the model to include a probabilistic treatment of both subcategorisation and wh-movement  . Results on Wall Street Journal text show that the parser performs at  88  . 1/87 . 5% constituent precision/recall , an average improvement of 2 . 3% over ( Collins 96) . 
1 Introduction
Generative models of syntax have been central in linguistics since they were introduced in  ( Chomsky 57 )  . Each sentence-tree pair ( S , T ) in a language has an associated topdown derivation consisting of a sequence of rule applications of a grammar  . These models can be extended to be statistical by defining probability distributions at points of nondeterminism in the derivations  , thereby assigning a probability 7) ( S , T ) to each ( S , T ) pair . Probabilistic contextfree grammar ( Booth and Thompson 73 ) was an early example of a statistical grammar . 
A PCFG can be lexicalised by associating a head word with each nonterminal in a parse tree  ; thus far , ( Magerman 95 ; Jelinek et al94) and ( Collins 96) , which both make heavy use of lexical information , have reported the best statistical parsing performance on Wall Street Journal text  . Neither of these models is generative , instead they both estimate 7) ( T\]S ) directly . 
This paper proposes three new parsing models.
Model 1 is essentially a generative version of the model described in  ( Collins 96 )  . In Model 2 , we extend the parser to make the complement /adjunct distinction by adding probabilities over subcategorisation frames for head words  . In Model 3 we give a probabilistic treatment of wh-movement , which This research was supported by ARPA Grant

is derived from the analysis given in Generalized Phrase Structure Grammar  ( Gazdar et al 95 )  . The work makes two advances over previous models : First  , Model 1 performs significantly better than ( Collins 96 )  , and Models 2 and 3 give further improvements -- our final results are  88  . 1/87 . 5% constituent precision/recall , an average improvement of 2 . 3% over ( Collins 96) . Second , the parsers in ( Collins96) and ( Magerman 95 ; Jelinek et al 94 ) produce trees without information about wh -movement or subcategorisation  . Most NLP applications will need this information to extract predicate-argument structure from parse trees  . 
In the remainder of this paper we describe the 3 models in section 2  , discuss practical issues in section 3 , give results in section 4 , and give conclusions in section 5 . 
2 The Three Parsing Models 2 . 1 Model 1 In general , a statistical parsing model defines the conditional probability  , 7) ( T\]S ) , for each candidate parse tree T for a sentence S . The parser itself is an algorithm which searches for the tree  , Tb~st , that maximises 7~( TIS ) . A generative model uses the observation that maximising  7V  ( T , S ) is equivalent to maximising 7~ ( T\]S ) : 1T be , t = argm~xT~(TlS ) = argmTax ? ~( T , S ) ~( s ) = argm~x 7~( T , S ) (1) 7~( T , S ) is then estimated by attaching probabilities to a topdown derivation of the tree  . In a PCFG , for a tree derived by n applications of contextfree rewrite rules LHSi ~ RHSi  , 1 < i < n , 7 ~( T , S ) = H7)(RHSiILHSi ) (2) i = l .   . n The rewrite rules are either internal to the tree  , where LHS is a nonterminal ndRHS is a string 7~ ( T , S ) 17~(S ) is constant , hence maximising ~ is equivalent to maximising " P  ( T , S ) . 

TOPiS ( bought)
NP ( w ~ ought ) tVB/~NpmJJ NNNNPIIIooks ) 
Last week Marks I1 bought NNP f

TOP->S ( bought)
S(bought ) -> NP ( week)
NP ( week ) -> JJ(Last)
NP ( Marks ) -> NNP ( Marks)
VP ( bought ) -> VB(bought)
NP ( Brooks ) -> NNP ( Brooks)
NP ( Marks ) VP ( bought)


Figure 1: A lexicalised parse tree , and a list of the rules it contains . For brevity we omit the POS tag associated with each word  . 
of one or more nonterminals ; or lexical , where LHS is a part of speech tag and RHS is a word  . 
A PCFG can be lexicalised 2 by associating a word w and a part-of-speech ( POS ) tag t with each nonterminal X in the tree . Thus we write a nonterminal as X(x ) , where x = ( w , t ) , and X is a constituent label . Each rule now has the form 3: P ( h)->Ln(In ) . . . ni(ll)H(h)Rl(rl ) . . . Rm(rm ) (3) H is the head child of the phrase , which inherits the head word h from its parent P . L 1 . . . L ~ and R1 . . . Rm are left and right modifiers of H . Either norm may be zero , and n = m = 0 for unary rules . Figure 1 shows a tree which will be used as an example throughout this paper  . 
The addition of lexical heads leads to an enormous number of potential rules  , making direct estimation of ? ) ( RHS  LHS ) infeasible because of sparse data problems . We decompose the generation of the RHS of a rule such as  ( 3 )  , given the LHS , into three steps -- first generating the head , then making the independence assumptions that the left and right modifiers are generated by separate  0th-order markov processes 4:   1  . Generate the head constituent label of the phrase  , with probability 7) H(HIP , h ) . 
2 . Generate modifiers to the right of the head with probability  1-Ii=1  . .m+1 ~ n(Ri(ri )  P , h , H) . 
R , ~+l(r , ~+ l ) is defined as STOP--the STOP symbol is added to the vocabulary of nonterminals  , and the model stops generating right modifiers when it is generated  . 
2We find lexical heads in Penn treebank data using rules which are similar to those used by  ( Magerman 95 ; 
Jelinek et al94).
S With the exception of the top rule in the tree , which has the form TOP--+H(h ) . 
4 An exception is the first rule in the tree , T0P-+H(h) , which has probability Prop(H , hlTOP ) 3 . Generate modifiers to the left of the head with probability rL = l  . .n + l ?) L ( L ~( li ) l P , h , H ), where
Ln+l(ln+l ) = STOP.
For example , the probability of the rule S ( bought ) -> NP ( week ) NP ( Marks ) YP ( bought ) would be es-timated as 7~h ( YPIS , bought ) x~l(NP ( Marks ) IS , YP , bought ) x 7~ , ( NP ( week ) S , VP , bought ) x7 ~ z(STOPIS , VP , bought ) x ~ r(STOPIS , VP , bought ) We have made the 0th order markov assumptions 7~ , ( Li(li ) H , P , h , L 1 ( l l ) . . . Li-1(/i1)) =
P~(Li(li)H,P , h ) (4)
Pr(Ri(ri)H,P , h,R1(rl) . . . R~-1(ri-1)) = ? ~ r(Ri(ri)H , P , h )   ( 5 ) but in general the probabilities could be conditioned on any of the preceding modifiers  . In fact , if the derivation order is fixed to be depth -first--that is  , each modifier recursively generates the subtree below it before the next modifier is generated - -then the model can also condition on any structure below the preceding modifiers  . For the moment we exploit this by making the approximations  7~l   ( Li ( li ) H , P , h , Ll(ll ) . . . Li_l(l~_l )) = ?) l(ni(li)lH , P , h , distance z(i-1)) (6) ?) r(ai(ri)\]H , P , h , R 1 ( r l ) . . . Ri-1(ri-l )) = ? ~ T(Ri(ri)\[H,P . h , distancer ( i-1 ) )  ( 7 ) where distance z and distance rare functions of the surface string from the head word to the edge of the constituent  ( see figure 2 )  . The distance measure is the same as in ( Collins 96 )  , a vector with the following 3 elements : ( 1 ) is the string of zero length ? ( Allowing the model to learn a preference for right-branching structures  )  ;   ( 2 ) does the string contain a modification of the most recent verb  )  . (3) Does the string contain 0 ,  1 , 2 or >2 commas ? ( where a comma is anything tagged as " , " or ":") . 
P ( h ) distance-I
Figure 2: The next child , Ra(r3) , is generated with probability 7~( R3(r3)\[P , H , h , distance r(2)) . The distance is a function of the surface string from the word after h to the last word of  R2  , inclusive . In principle the model could condition on any structure dominated by H  , R1 or R2 . 
2 . 2 Model 2: The complement/adjunct distinction and subcategorisation The tree in figure  1 is an example of the importance of the complement /adjunct distinction  . It would be useful to identify " Marks " as a subject  , and " Last week " as an adjunct ( temporal modifier )  , but this distinction is not made in the tree , as both NPs are in the same position 5 ( sisters to a VP under an S node )  . From here on we will identify complements by attaching a "- C " suffix to nonterminals -- figure  3 gives an example tree . 
TOP1S ( bought)
NP ( w ~ ought)
Last week Marks VBD NP-C(B rooks )
Ilbought Brooks
Figure 3: A tree with the "- C " suffix used to identify complements  . " Marks " and " Brooks " are in subject and object position respectively  . " Last week " is an adjunct . 
A postprocessing stage could add this detail to the parser output  , but we give two reasons for making the distinction while parsing : First  , identifying complements i complex enough to warrant a probabilistic treatment  . Lexical information is needed 5Except " Marks " is closer to the VP , but note that " Marks " is also the subject in " Mark slast week bought 

-- for example , knowledge that " week '' is likely to be a temporal modifier  . Knowledge about subcategorisation preferences -- for example that a verb takes exactly one subject -- is also required  . These problems are not restricted to NPs , compare " The spokes woman said ( SBAR that the asbestos was dangerous ) " vs . " Bonds beat short term investments ( SBAR because the market is down ) " , where an SBAR headed by " that " is a complement , but an SBAI : the aded by " because " is an adjunct . 
The second reason for making the comple-ment /adjunct distinction while parsing is that it may help parsing accuracy  . The assumption that complements are generated independently of each other often leads to incorrect parses -- see figure  4 for further explanation . 
2.2.1 Identifying Complements and
Adjuncts in the Penn Treebank
We add the "- C " suffix to all nonterminals in training data which satisfy the following conditions :  1  . The nonterminal must be : (1) an NP , SBAR , or S whose parent is an S ; (2) an NP , SBAR , S , or VP whose parent is a VP ; or (3) an S whose parent is an SBAR . 
2 . The nonterminal must not have one of the following semantic tags : ADV  , VOC , BNF , DIR , 
EXT , LOC , MNR , TMP , CLR or PRP . See ( Marcus et al 94 ) for an explanation of what these tags signify . For example , the NP " Last week " in figure 1 would have the TMP ( temporal ) tag ; and the SBAR in " ( SBAR because the market is down ) " , would have the ADV ( adverbial ) tag . 
In addition , the first child following the head of a prepositional phrase is marked as a complement  . 
2 . 2 . 2 Probabilities over Subcategorisation
Frames
The model could be retrained on training data with the enhanced set of nonterminals  , and it might learn the lexical properties which distinguish complements and adjuncts  ( " Marks " vs " week " , or " that " vs . " because ") . However , it would still suffer from the bad independence assumptions illustrated in figure  4  . To solve these kinds of problems , the generative process is extended to include a probabilistic choice of left and right subcategorisation frames :  1  . Choose ahead H with probability ~ H(H\[P , h) . 
2 . Choose left and right subcat frames , LC and RC , with probabilities 7) ~ c(LC\[P , H , h ) and
NP CVP
NP CNP CVP
II~f ~. was ADJP
NP NP Drey fus the best fund was ADJP\[
III low low Drey fus the best fund 2 . ( a ) Incorrect S(b ) Correct S
NP CVP
NP CVP l
I ~ The issue / ~
The issue was NPCw-CNP VP a bill a bill funding NPC funding NPC 
II Congress Congress
Figure 4: Two examples where the assumption that modifiers are generated independently of each other leads to errors  . In ( 1 ) the probability of generating both " Dreyfus " and " fund " as subjects  , 7 ~( NP-C ( Dreyfus ) IS , VP , was )* T'(NP-C(fund)IS , VP , was ) is unreasonably high . (2) is similar : 7~ ( NPC(bill ) , VPC(funding ) IVP , VB , was ) = P ( NP-C(bill ) IVP , VB , was )*7~( VP-C(funding ) IVP , VB , was ) is a bad independence assumption . 
Prc(RCIP , H , h) . Each subcat frame is a multiset 6 specifying the complements which the head requires in its left or right modifiers  . 
3 . Generate the left and right modifiers with probabilities  7  ) l ( Li , liIH , P , h , distance t(i-1) , LC ) and 7~ r(R  ~ , riIH , P , h , distance r(i-1) , RC ) respectively . Thus the subcat requirements are added to the conditioning context  . As complements are generated they are removed from the appropriate subcat multiset  . Most importantly , the probability of generating the STOP symbol will be  0 when the subcat frame is nonempty , and the probability of generating a complement will be  0 when it is not in the subcat frame ; thus all and only the required complements will be generated  . 
The probability of the phrase S ( bought ) -> NP ( week ) NP-C ( Marks ) VP ( bought ) is now : 7 ) h ( VPIS , bought ) x to ( NP-CIS , VP , bought ) xtS , VP , bought ) ? 7~/( NP-C(M arks ) IS , VP , bought , NPC ) x 7: ~ I(NP ( week ) IS , VP , bought , ) x7) l(STO eIS , ve , bought ,  ) ?
Pr(STOPIS , VP , bought,)
Here the head initially decides to take a single NPC  ( subject ) to its left , and no complements ~ Arnultis et , or bag , is a set which may contain duplicate nonterminal labels  . 
to its right . NP-C ( Marks ) is immediately generated as the required subject , and NPC is removed from LC , leaving it empty when the next modifier , NP ( week ) is generated . The incorrect structures in figure 4 should now have low probability because ~ Ic ( NP-C , NP-C\[S , VP , bought ) and " Prc(NP-C , VP-CI VP , VB , was ) are small . 
2.3 Model 3: Traces and Wh-Movement
Another obstacle to extracting predicate-argument structure from parse trees is wh-movement  . This section describes a probabilistic treatment of extraction from relative clauses  . Noun phrases are most often extracted from subject position  , object position , or from within PPs : Example 1The store ( SBAR which TRACE bought Brooks Brothers ) Example 2The store ( SBAR which Marks bought

Example 3The store ( SBAR which Marks bought
Brooks Brothers/tom TRACE)
It might be possible to write rule-based patterns which identify traces in a parse tree  . However , we argue again that this task is best integrated into the parser : the task is complex enough to warrant a probabilistic treatment  , and integration may help parsing accuracy . A couple of complexities are that modification by an SBAR does not always involve extraction  ( e . g . , " the fact ( SBAR that be so boru is
NP ( store ) SBAR(that ) (+ gap)
The store


I that ( i ) NP->NP ( 2 ) SBAR ( + gap ) -> WHNP ( 3 ) S ( + gap ) -> NPC ( 4 ) VP ( + gap ) -> VB
S(bought ) (-- gap)
NP-C(~ht ) (--- gap)
IB ~ w

Veek )
II bought last week



TRACENP
Figure 5: A + gap feature can be added to nonterminals to describe NP extraction  . The toplevel NP initially generates an SBAR modifier  , but specifies that it must contain an NP trace by adding the + gap feature  . The gap is then passed down through the tree , until it is discharged as a TRACE complement to the right of bought  . 
played with a ball and a bat )") , and it is not uncommon for extraction to occur through several constituents  , ( e . g . , " The changes ( SBAR that he said the government was prepared to make TRACE  ) " )  . 
The second reason for an integrated treatment of traces is to improve the parameterisation f the model  . In particular , the subcategorisation probabilities are smeared by extraction  . In examples 1 , 2 and 3 above ' bought ' is a transitive verb , but without knowledge of traces example 2 in training data will contribute to the probability of ' bought ' being an intransitive verb  . 
Formalism similar to GPSG ( Gazdar et al 95 ) handle NP extraction by adding a gap feature to each nonterminal in the tree  , and propagating gaps through the tree until they are finally discharged as a trace complement  ( see figure 5 )  . In extraction cases the Penn treebank annotation coindexes a TRACE with the WHNP head of the SBAR  , so it is straightforward to add this information to trees in training data  . 
Given that the LHS of the rule has a gap , there are 3 ways that the gap can be passed down to the

Head The gap is passed to the head of the phrase , as in rule (3) in figure 5 . 
Left , Right The gap is passed on recursively to one of the left or right modifiers of the head  , or is discharged as a trace argument to the left /right of the head  . In rule (2) it is passed onto a right modifier , the S complement . In rule ( 4 ) a trace is generated to the right of the head VB . 
We specify a parameter 7~c(GIP , h , H ) where G is either Head , Left or Right . The generative process is extended to choose between these cases after generating the head of the phrase  . The rest of the phrase is then generated in different ways depending on how the gap is propagated : In the Head case the left and right modifiers are generated as normal  . In the Left , Right cases a gap requirement is added to either the left or right SUBCAT variable  . This requirement is fulfilled ( and removed from the subcat list ) when a trace or a modifier nonterminal which has the + gap feature is generated  . For example , Rule (2) , SBAR ( that )   ( + gap ) -> WHNP ( that ) S-C ( bought )   ( + gap )  , has probability ~ h(WHNPISBAR , that ) ?7 ~ G(RightISBAR , WHNP , that ) x
T ~ LC(ISBAR , WHNP , that ) x
T'Rc(S-C\[SBAR , WHNP , that ) x7 ~ R(SC(bought)(+gap)\[SBAR , WHNP , that , SC , + gap)x7 ~ R(STOPISBAR , WHNP , that , ) x
PC(STOPISBAR , WHNP , that,)
Rule (4), VP ( bought )(+ gap)->VB(bought)
TRACENP ( week ) , has probability 7~ h(VBIVP , bought ) xPG(RightIVP , bought , VB ) x PLC(IVP , bought , VB ) x ~ PRc(NP-CIvP , bought , VB ) x7 ~ R(TRACEIVP , bought , VB , NPC , + gap)x
PR(NP ( week ) IVP , bought , VB , )?7) L(STOPIVP , bought , VB , ) x7 ~ R(STOPIVP , bought , VB , ) In rule (2) Right is chosen , so the + gap requirement is added to RC . Generation of S-C(bought )(+ gap ) ? H(+)
Prob = XPr ? b = X'X ~ H(HIP, .   .   . )(b)P (-) + Ri (+) = ~
HR1
Prob-=XProb = Y
Figure 6: The life of a constituent in the chart.
(c)P (-) = ~ P (+)
Prob = XProb=XX'PL(STOPI . . . . ) xPR(STOP I . . . . )
P (-) ?. HR1Ri
Prob = X x Y x ~ R(Ri(ri)IP,H, . . . )(+) means a constituent is complete ( i . e . it includes the stop probabilities ) , (-) means a constituent is incomplete .   ( a ) a new constituent is started by projecting a complete rule upwards  ;   ( b ) the constituent then takes left and right modifiers  ( or none if it is unary )  . (c ) finally , STOP probabilities are added to complete the constituent  . 
Backoff " PH(HI "-) Pa(GI . . . ) PL~(Li(It ,) I . .-)
Level PLc(LC t . . . ) Pm(Ri(rti)I . . . )7) Rc(RCI . . . ) 1 P , w , t P , H , w , t P , H , w , t , A , LC2 P , t P , H , t P , H , t , A , LC3 PP , HP , H ,  & , LC4--
PL2 ( lwil . . . )
PR2 ( rwiI...)
Li , Iti , P , H , w , t , A , LC
L , , lti , P , H , t , A , LC
LI , lti

Table 1: The conditioning variables for each level of backoff  . For example , T'H estimation interpolates el = ~? H(HIP , w , t ) , e2 = 7 ~ H(HIP , t ) , and e3 = PH(HIP ) . A is the distance measure . 
: ul fills both the SC and + gap requirements in RC.
In rule (4) Right is chosen again . Note that generation of trace satisfies both the NPC and + gap subcat requirements  . 
3 Practical Issues 3 . 1 Smooth ing and Unknown Words Table 1 shows the various levels of back of for each type of parameter in the model  . Note that we decompose " PL(Li(lwi , lti ) IP , H , w , t , ~  , LC )   ( where lwi and Iti are the word and POS tag generated with nonterminal Li  , A is the distance measure ) into the product 79L 1 ( Li ( lti ) IP , H , w , t , Zx , LC)x7 ~ L2(lwiILi , lti ,  19 , H , w , t , A , LC ) , and then smooth these two probabilities separately  ( Jason Eisner , p . c . ) . In each case 7 the final estimate is e----Ale1+ ( 1-& l ) (A2e2 + ( 1 - &2 ) e a ) where ex ,   e2 and e3 are maximum likelihood estimates with the context at levels  1  , 2 and 3 in the table , and , kl ,   , k2 and )~3 are smoothing parameters where 0_ < , ki_<1 . All words occurring less than 5 times in training data , and words in test data which rEx cept cases L2 and R2  , which have 4 levels , so that e = ~ let+(1--*X1)() ~2 e2+(1- , ~2) (&3 e3+(1-~3) e4)) . 
have never been seen in training , are replaced with the " UNKNOWN " token . This allows the model to robustly handle the statistics for rare or new words  . 
3 . 2 Par t o f Speech Tagg ing and Pars ing Part of speech tags are generated along with the words in this model  . When parsing , the POS tags allowed for each word are limited to those which have been seen in training data for that word  . For unknown words , the output from the tagger described in ( Ratnaparkhi 96 ) is used as the single possible tag for that word . A CKY style dynamic programming chart parser is used to find the maximum probability tree for each sentence  ( see figure 6 )  . 
4 Results
The parser was trained on sections 02  -  21 of the Wall Street Journal portion of the Penn Treebank  ( Marcus et al 93 )   ( approximately 40 , 000 sentences ) , and tested on section 23 (2 , 416 sentences ) . We use the PAR . SEVAL measures ( Black et al 91 ) to compare performance :
Labeled Precision = number of correct constituents in proposed parse number of constituents in proposed parse  ( Magerman 95 )   ( Collins 96 ) 
Model 1
Model 2
Model 3 ~ ce ~) 2 CBs 84 . 6% 84 . 9% 1 . 26 56 . 6% 81 . 4% 84 . 0% 84 . 3% 1 . 46 54 . 0% 85 . 8% 86 . 3% 1 . 14 59 . 9% 83 . 6% 85 . 3% 85 . 7% 1 . 32 57 . 2% 87 . 4% 88 . 1% 0 . 96 65 . 7% 86 . 3% 86 . 8% 87 . 6% 1 . 11 63 . 1% 88 . 1% 88 . 6% 0 . 91 66 . 5% 86 . 9% 87 . 5% 88 . 1% 1 . 07 63 . 9% 88 . 1% 88 . 6% 0 . 91 66 . 4% 86 . 9% 87 . 5% 88 . 1% 1 . 07 63 . 9% 78 . 8% 80 . 8% 84 . 1% 84 . 6% 84 . 6% Table 2: Results on Section 23 of the WSJ Treebank . LR/LP = labeled recall/precision . CBs is the average number of crossing brackets per sentence  . 0CBs ,  <  2 CBs are the percentage of sentences with 0 or < 2 crossing brackets respectively . 
Labeled Recall-~numbero/correct constituents in proposed parse number of constituents in treebank parse 
Crossing Brackets - - - - number of constituents which violate constituent boundaries with a constituent in the treebank parse  . 
For a constituent to be ' correct ' it must span the same set of words  ( ignoring punctuation , i . e . all tokens tagged as commas , colons or quotes ) and have the same labels as a constituent in the treebank parse  . Table 2 shows the results for Models 1, 2 and 3 . The precision/recall of the traces found by Model  3 was 93  . 3%/90 . 1%  ( out of 436 cases in section 23 of the treebank )  , where three criteria must be met for a trace to be " correct ":  ( 1 ) it must be an argument to the correct head word ;   ( 2 ) it must be in the correct position in relation to that head word  ( preceding or following )  ;   ( 3 ) it must be dominated by the correct nonterminal label  . For example , in figure 5 the trace is an argument to bought , which it follows , and it is dominated by a VP . Of the 436 cases ,   342 we restring-vacuous extraction from subject position  , recovered with 97 . 1%/98 . 2% precision/recall ; and 94 were longer distance cases , recovered with 76%/60 . 6% precision/recall 94 . 1 Compar i son to previous work Model 1 is similar in structure to ( Collins 96 ) - - the major differences being that the " score " for each bigram dependency is  7't  ( L , liIH , P , h , distance t )  8 ( Magerman 95 ) collapses ADVP and PRT to the same label , for comparison we also removed this distinction when calculating scores  . 
9We exclude infinitival relative clauses from these figures  , for example " I called a plumber TRACE to fix the sink " where ' plumber ' is coindexed with the trace subject of the infinitival  . The algorithm scored 41%/18% precision/recall on the 60 cases in section 23 --but infinitival relatives are extremely difficult even for human annotators to distinguish from purpose clauses  ( in this case , the infinitival could be a purpose clause modifying ' called '  )   ( Ann Taylor , p . c . ) rather than Pz(Li , P , HI li , h , distance l ) , and that there are the additional probabilities of generating the head and the STOP symbols for each constituent  . However , Model 1 has some advantages which may account for the improved performance  . 
The model in ( Collins 96) is deficient , that is for most sentences S , Y ~ T7~(T\]S ) < 1 , because probability mass is lost to dependency structures which violate the hard constraint that no links may cross  . 
For reasons we do not have space to describe here , Model 1 has advantages in its treatment of unary rules and the distance measure  . The generative model can condition on any structure that has been previously generated -- we exploit this in models  2 and 3 -- whereas ( Collins 96 ) is restricted to conditioning on features of the surface string alone  . 
(Charniak 95 ) also uses a lexicalised generative model . In our notation , he decomposes P ( RHSil LHSi ) as " P(R , ~ . . . R1HL1 . .Lm \] P,h ) x 1-L=I . .~ 7~(r~l P , Ri , h ) x I-L=l . .m 7)(lil P , Li , h ) . The Penn treebank annotation style leads to a very large number of contextfree rules  , so that directly estimating 7~( R . . . . R1HL1 . .Lm I P , h ) may lead to sparse data problems , or problems with coverage ( a rule which has never been seen in training may be required for a test data sentence  )  . The com-plement/adjunct distinction and traces increase the number of rules  , compounding this problem . 
( Eisner 96) proposes 3 dependency models , and gives results that show that a generative model similar to Model  1 performs best of the three . However , a pure dependency model omits nonterminal information  , which is important . For example , " hope " is likely to generate a VP ( T0 ) modifier ( e . g . , I hope \[ VP to sleep \] ) whereas "' require " is likely to generate an S ( T0 ) modifier ( e . g . , I require IS Jim to sleep \]) , but omitting nonterminals conflates these two cases  , giving high probability to incorrect structures such as " I hope\[Jim to sleep \] " or " I require\[to sleep\]"  .   ( Alshawi 96 ) extends a generative dependency model to include an additional state variable which is equivalent to having nonterminals -- hishe does not fully specify the details of his model  , and doesn't give results for parsing accuracy . ( Miller et al .  96 ) describe a model where the RHS of a rule is generated by a Markov process  , although the process is not head-centered . They increase the set of nonterminals by adding semantic labels rather than by adding lexical head words  . 
( Magerman 95 ; Jelinek et al 94 ) describe a history-based approach which uses decision trees to estimate  7a  ( T\[S )  . Our models use much less sophisticated ngram estimation methods  , and might well benefit from method such as decision-tree estimation which could condition on richer history than just surface distance  . 
There has recently been interest in using dependency-based parsing models in speech recognition  , for example ( Stolcke 96) . It is interesting to note that Models 1 , 2 or 3 could be used as language models . The probability for any sentence can be estimated as P  ( S )  = ~~ . TP ( T , S ) , or ( making a Viterbi approximation for efficiency reasons  ) as 7 ) ( S )   . ~P ( Tb~st , S) . We intend to perform experiments to compare the perplexity of the various models  , and a structurally similar ' pure ' PCFG 1? . 
5 Conclusions
This paper has proposed a generative , lexicalised , probabilistic parsing model . We have shown that linguistically fundamental ideas  , namely subcategorisation and wh-movement , can be given a statistical interpretation . This improves parsing performance , and , more importantly , adds useful information to the parser's output . 
6 Acknowledgements
I would like to thank Mitch Marcus , Jason Eisner , Dan Melamed and Adwait Ratnaparkhi for many useful discussions  , and comments on earlier versions of this paper . This work has also benefited greatly from suggestions and advice from Scott Miller  . 

H . Alshawi .  1996 . Head Automata and Bilingual Tiling : Translation with Minimal Representations  . Proceedings of the 3~th Annual Meeting of the Association for Computational Linguistics  , pages 167-176 . 
E . Black et al 1991 . A Procedure for Quantitatively Comparing the Syntactic Coverage of English Grammars  . Proceedings of the February 1991 DARPA Speech and Natural Language Workshop . 
1?Thanks to one of the anonymous reviewers for suggesting these experiments  . 
T . L . Booth and R . A . Thompson .  1973 . Applying Probability Measures to Abstract Languages  . IEEE Transactions on Computers , C-22 (5) , pages 442-450 . 
E . Charniak .  1995 . Parsing with ContextFree Grammars and Word Statistics  . Technical Report CS-95-28, Dept . of Computer Science , Brown University . 
N . Chomsky . 1957. Syntactic Structures , Mouton,
The Hague.
M . J . Collins .  1996 . A New Statistical Parser Based on Bigram Lexical Dependencies  . Proceeding so / the 34th Annual Meeting o/the Association for Computational Linguistics  , pages 184-191 . 
J . Eisner .  1996 . Three New Probabilistic Models for Dependency Parsing : An Exploration  . Proceedings o/COLING96, pages 340-345 . 
G . Gazdar , E . H . Klein , G . K . Pullum , I . A . Sag .  1985 . 
Generalized Phrase Structure Grammar . Harvard
University Press.
F . Jelinek , J . Lafferty , D . Magerman , R . Mercer , A . 
Ratnaparkhi , S . Roukos .  1994 . Decision Tree Parsing using a Hidden Derivation Model  . Proceeding so / the 1994 Human Language Technology Workshop , pages 272-277 . 
D . Magermaa .  1995 . Statistical Decision-Tree Models for Parsing . Proceeding so / the 33rd Annual Meetingo \] the Association for Computational 
Linguistics , pages 276-283.
M . Marcus , B . Santorini and M . Marcinkiewicz.
1993 . Building a Large Annotated Corpus of English : the Penn Treebank  . Computational Linguistics , 19(2):313-330 . 
M . Marcus , G . Kim , M.A . Marcinkiewicz , R.
MacIntyre , A . Bies , M . Ferguson , K . Katz , B . 
Schasberger .  1994 . The Penn Treebank : Annotating Predicate Argument Structure  . Proceedings of the 1994 Human Language Technology Workshop , pages 110~115 . 
S . Miller , D . Staliard and R . Schwartz .  1996 . A Fully Statistical Approach to Natural Language Interfaces  . Proceeding so / the 34th Annual Meeting of the Association for Computational Linguistics  , pages 55-61 . 
A . Ratnaparkhi .  1996 . A Maximum Entropy Model for Part-Of-Speech Tagging  . Conference on Empirical Methods in Natural Language Processing  . 
A . Stolcke .  1996 . Linguistic Dependency Modeling . 
Proceedings of ICSLP96 , Fourth International Conference on Spoken Language Processing  . 

