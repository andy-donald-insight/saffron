Formal aspects and parsing issues of dependency theory 
Vincenzo Lombardo and Leonardo Lesmo
Dipartimento di Informatica and Centrodi Scienza Cognitiva 
Universita ' di Torino
c . so Svizzera 185-10149 Torino-Italy
vincenzo , lesmo@di.unito.it

The paper investigates the problem of providing a formal device for the dependency approach to syntax  , and to link it with a parsing model . After reviewing the basic tenets of the paradigm and the few existing mathematical results  , we describe a dependency formalism which is able to deal with long-distance dependencies  . Finally , we present an Earley-style parser for the formalism and discuss the  ( polynomial ) complexity results . 
1. Introduction
Many authors have developed dependency theories that cover crosslinguistically the most significant phenomena of natural language syntax : the approaches range from generative formalisms  ( Sgall et al 1986 )  , to lexically-based descriptions ( Mel'cuk 1988) , to hierarchical organizations of linguistic knowledge  ( Hudson 1990 )   ( Fraser , Hudson 1992) , to constrained categorial grammars ( Milward 1994 )  . Also , a number of parsers have been developed for some dependency frameworks  ( Covington 1990 )   ( Kwon , Yoon 1991) ( Sleator , Temperley 1993) ( Hahn et al1994) ( Lombardo , Lesmo 1996) , including a stochastic treatment ( Eisner 1996 ) and an object-oriented parallel parsing method ( Neuhaus , Hahn 1996) . 
However , dependency theories have never been explicitly linked to formal models  . Parsers and applications usually refer to grammars built around a core of dependency oncepts  , but there is a great variety in the description of syntactic on straints  , from rules that are very similar to CFG productions  ( Gaifman 1965 ) to individual binary relations on words or syntactic ategories  ( Covington 1990 )   ( Sleator , Temperley 1993) . 

SUB/SCOMP
I ~ likes
SUB?~OBJ
John beans
Figure 1 . A dependency tree for the sentence " I know John likes beans "  . The leftward or rightward orientation of the edges represents the order constraints : the dependents that precede  ( respectively , follow ) the head stand on its left ( resp . right ) . 
The basic idea of dependency is that the syntactic structure of a sentence is described in terms of binary relations  ( dependency relations ) on pairs of words , a head ( parent ) , and a dependent ( daughter ) , respectively ; these relations usually form a tree , the dependency tree ( fig .  1) . 
The linguistic merits of dependencys n tax have been widely debated  ( e . g . ( Hudson 1990)) . 
Dependency syntax is attractive because of the immediate mapping of dependency trees on the predicate-arguments structure and because of the treatment of free-word order constructs  ( Sgall et al 1986 )   ( Mercuk 1988 )  . Desirable properties of lexicalized formalisms ( Schabes 1990 )  , like finite ambiguity and decidability of string acceptance  , intuitively hold for dependency sntax . 
On the contrary , the formal studies on dependency theories are rare in the literature  . 
Gaifman ( 1965 ) showed that projective dependency grammars , expressed by dependency rules on syntactic ategories  , are weakly equivalent to contextfree grammars . And , in fact , it is possible to devise O ( n3 ) parsers for this formalism ( Lombardo , Lesmo 1996) , or other projective variations ( Milward 1994 )   ( Eisner 1996 )  . On the controlled relaxation of projective constraints  , Nasr ( 1995 ) has introduced the condition of pseudo ? projectivity  , which provide some controlled looser constraints on arc crossing in a dependency tree  , and has developed a polynomial parser based on a graph-structured stack  . Neuhaus and Broker ( 1997 ) have recently showed that the general recognition problem for nonprojective dependency grammars  ( whathey call discontinuous DG ) is NP-complete . 
They have devised a discontinuous DG with exclusively lexical categories  ( no traces , as most dependency theories do ) , and dealing with free word order constructs through a looser subtree ordering  . This formalism , considered as the most straightforward extension to a projective formalism  , permits the reduction of the vertex cover problem to the dependency recognition problem  , thus yielding the NP-completeness result . 
However , even if banned from the dependency literature , the use of nonlexical categories i only a notational variant of some graph structures already present in some formalisms  ( see , e . g . , Word Grammar ( Hudson 1990)) . This paper introduces a lexicalized dependency formalism  , which deals parsing algorithm . The formalism is projective , and copes with long-distance dependency phenomena through the introduction of nonlexical categories  . 
The nonlexical categories allow us to keep in alterate the condition of projectivity  , encoded in the notion of derivation . The core of the grammar relies on predicate -argument structures associated with lexical items  , where the head is a word and dependents are categories linked by edges labelled with dependency relations  . Free word order constructs are dealt with by constraining displacements via a set data structure in the derivation relation  . The introduction of nonlexical categories also permits the resolution of the inconsistencies pointed out by Neuhaus and Broker in Word Grammar  ( 1997 )  . 
The parser is an Earley type parser with a polynomial complexity  , that encodes the dependency trees associated with a sentence  . 
The paper is organized as follows . The next section presents a formal dependencys stem that describes the linguistic knowledge  . Section 3 presents an Earley-type parser : we illustrate the algorithm  , trace an example , and discuss the complexity results . Section 4 concludes the paper . 
2. A dependency formalism
The basic idea of dependency is that the syntactic structure of a sentence is described in terms of binary relations  ( dependency relations ) on pairs of words , a head ( or parent ) , and a dependent ( daughter ) , respectively ; these relations form a tree , the dependency tree . In this section we introduce a formal dependency system  . The formalism is expressed via dependency rules which describe one level of a dependency tree  . Then , we introduce a notion of derivation that allows us to define the language generated by a dependency grammar of this form  . 
The grammar and the lexicon coincide , since the rules are lexicalized : the head of the rule is a word of a certain category  , i . e . the lexical anchor . From the linguistic point of view we can recognize two types of dependency rules : primitive dependency rules  , which represent subcategorization frames , and non-primitive dependency rules , which result from the application of lexical metarules to primitive and non-primitive dependency rules  . 
Lexical metarules ( not dealt with in this paper ) obey general principles of linguistic theories . 
A dependency grammar is a six-tuple < W,C,S,
D , I , H >, where
W is a finite set of symbols ( words of a natural language )  ; C is a set of syntactic ategories ( among which the special category E )  ; S is a nonempty set of root categories ( C~S ) ; D is the set of dependency relations , e . g . SUBJ , OBJ , XCOMP , POBJ , PRED ( among which the special relation VISITOR 1) ; I is a finite set of symbols ( among which the special symbol 0 )  , called u-indices ; H is a set of dependency rules of the form x : X ( < rlYlUl ' Cl > . . . < ri-1Yi-l Ui-l'Ci-1>#<a'i+lYi+fl;i+l > . . . < rmY mum ' ~ m >) 1) xeW , is the head of the rule ; 2) XeC , is its syntactic ategory ;  3 ) an element < riYiuix j > is a d-quadruple ( which describbs a-de-pefident )  ; he sequence of d-quads , including the symbol # ( representing the linear position of the head ,  #is a special symbol ) , is called the d-quad sequence . We have that 3a ) rie D , j e1 . . . . . i-l,i + l . . . . . m ; 3b)Y jeC,je1 . . . . . i-l,i + l . . . . . m ; 3c ) ujeI , j  e1 . . . . . i-l,i + l . . . . . m ; 3d ) x \] is a ( possibly empty ) set of triples < u , r , Y > , called u-triples , where ue I , reD , 

Finally , it holds that:
I ) For each ue I that appears in au-triple < u , r , Y > ~ Uj , there exists exactly oned-quad < riYiu:\]xi > in the same rule such that u = ui  , i ~ j . 
II ) For each u = ui of ad-quad<ri Yiuixi > , there exists exactly one u-triple < u , r , Y > exj , i ~ j , in the same rule . 
Intuitively , a dependency rule constrains one node ( head ) and its dependents in a dependency tree : the d -quad sequence states the order of elements  , both the head (  #position ) and the dependents ( d-quads )  . 
The grammar is lexicalized , because each dependency rule has a lexical anchor in its head  ( x : X )  . Ad-quad<rj Y jujxj > identifies a dependent of category Yi  ,   co-rm-ect6d with the head via a dependency relation ri . Each element of the d-quad sequence is possibly f issociated with au-index  ( uj ) and a set of u-triples ( xj )  . Both uj and xj can be mill elements , i . e . 0 and ~, respectively . Au-triple ( x-component of the d-quad ) < u , R , Y > bounds the area of the dependency tree where the trace can be located  . Given the constraints I and II , there is a one-to-one correspondence btween the u-indices and the u-triples of the d-quads  . Given that a dependency rule constrains one head and its direct dependents in the dependency tree  , we have that the dependent indexed by uk is coindexed with a The relation VISITOR  ( Hudson 1990 ) accounts for displaced elements and , differently from the other relations , is not semantically interpreted . 
7 88 trace node in the subtree rooted by the dependent containing the u-triple < Uk  , R , Y > . 
Now we introduce a notion of derivation for this formalism  . As one dependency rule can be used more than once in a derivation process  , it is necessary to replace the u-indices with unique symbols  ( progressive integers ) before the actual use . The replacement must be consistent in the u and the x components  . When all the indices in the rule are replaced , we say that the dependency rule ( as well as the u-triple ) is instantiated . 
A triple consisting of a word w ( aW ) or the trace symbole ( ~ W ) and two integers g and v is a word object of the grammar  . 
Given a grammar G , the set of word objects of G is
Wx(G ) = ~ tXv/IX , v_>0, xeWue.
A pair consisting of a category X ( eC ) and a string of instantiated u-triples T is a category object of the grammar  ( X ( T ) ) . 
A4-tuple consisting of a dependency relation r ( eD )  , a category object X(yl ) , an integer k , a set of instantiated u-triples T2 is a derivation object of the grammar . Given a grammar G , the set of derivation objects of G is
Cx(G ) = < r , Y(y1) , u , y2>/reD , Ye C , u is an integer ,  )'1  , T2 are strings of instantiated u-triples . 
Let a , ~ eWx(G )* and Ve(Wx(G)uCx(G ))* . The derivation relation holds as follows : 1 ) a < R , X(Tp) , U , Tx > ~ := ~? I < rl , Yl(Pl ) , U 1 , ' ~1><: r2 , Y2(P2) , u2 , '~2>  .   .   . 
< ri-1 , Yi1(Pi-1) , ui1 , ' q1 > uX0 < ri + l , Yi+l(Pi+l) , Ui+l , ' Ci+l > .   ,   , < rm , Ym(Pm) , Um , ' Cm > where x : X(<rlYlUl ' Cl > . . . < ri-lYi-l Ui-lZi-l > #< ri + lYi + l ' ~ i + l > . . . < rmYmUm'Cm >) is a dependency rule , and Plu . . . uPm--'q'puTx . 
2) ct < r,X(<j,r,X >), u , O >=, auej
We define = ~* as the reflexive , transitive closure of ~ . 
Given a grammar G , L ' ( G ) is the language of sequences of word objects :
L '( G ) = aeWx(G)*/<TOP , Q(i ~) ,  0 ,  0> :=>*  ( x and QeS ( G ) where TOP is a dummy dependency relation . The language generated by the grammar G , L(G ) , is defined through the function t : L ( G ) = we Wx ( G ) */ w = t ( ix ) and oceL ' ( G )  , where t is defined recursively as t (-) = - ; (tWva ) = wt(at(~teva ) = t(a) . 
where - is meempty sequence.
As an example , consider the grammar

W(G1) = I , John , beans , know , likes
C(G1) = V , V+EX , N
S(GO = v,V+EX
D(G1) = SUBJ , OBJ , VISITOR , TOP1((31) = 0 , Ul
T ( G1) > , where T ( G1 ) includes the following dependency rules : 1 . I : N(#); 2 . John : N(#); 3 . beans : N(#); 4 . likes : V(<SUBJ , N ,  0 , 0>#< OBJ , N ,  0 ,  0)>); 5 . knOW:V+EX(<VISITOR , N , ul , 0> < SUB\]' , N ,  0 , 0>#< SCOMP , V ,  0 , < ul , OBJ , N >>) . 
A derivation for the sentence " Beans Iknow John likes " is the following : < TOP  , V + EX(O ) ,  0 , 0>::~< VISITOR , N(IE l ) , I , 0> < SUBJ , N(~) ,  0 , 0> know < SCOMP , V (~) ,  0 , < I , OBJ , N >> := ~ l beans < SUBJ , N(O ) ,  0 , 0> know < SCOMP , V(O ) ,  0 , < I , OBJ , _N '>> l be ans I know < SCOMP , V(O ) ,  0 , < I , OBJ , N >> = : ~ l be ans I know < SUBJ , N(O ) ,  0 , O > likes < OBJ , N(<I , OBJ , N > ) ,  0 , 0> = : ~ l be ans I know John likes < OBJ , N(<I , OBJ , N > ) ,  0 ,   0>   1beans Iknow John like sel The dependency tree corresponding to this derivation is in fig  .  2 . 
3. Parsing issues
In this section we describe an Earley-style parser for the formalism in section  2  . The parser is an offline algorithm : the first step scans the input sentence to selec the appropriate dependency rules know 
VISITOSUB " SCOMP
John E1
Figure 2 . Dependency tree of the sentence " Beans I know John likes "  , given the grammar G1 . 
789 from the grammar . The selection is carried out by matching the head of the rules with the words of the sentence  . The second step follows Earley's phases on the dependency rules  , together with the treatment of u-indices and u -triples  . This offline technique is not uncommon in lexicalized grammars  , since each Earley's prediction would waste much computation time  ( a grammar factor ) in the body of the algorithm , because dependency rules do not abstract over categories  ( cf . (Schabes1990)) . 
In order to recognize a sentence of n words , n + l sets Si of items are built . An item represents a subtree of the total dependency tree associated with the sentence  . An item is a 5-tuple < Dotted-rule , Position , g-index , v-index , T-stack > . Dotted-rule is a dependency rule with a dot between two d-quads of the d-quad sequence  . Position is the input position where the parsing of the subtree represented by this item began  ( the leftmost position on the spanned string )  . It-index and v-index are two integers that correspond to the indices of a word object in a derivation  . T-stack is a stack of sets of u-triples to be satisfied yet : the sets of u-triples  ( including empty sets , when applicable ) provided by the various items are stacked in order to verify the consumption of one u-triple in the appropriate subtree  ( cf . the notion of derivation above ) . Each time the parser predicts a dependent , the set of u-triples associated with it is pushed onto the stack  . In order for an item to enter the completer phase  , the top of T-stack must be the empty set , that means that all the u-triples associated with that item have been satisfied  . The sets of u-triples in T-stack are always inserted at the top  , after having checked that each u-triple is not already present in T-stack  ( the check neglects the u-index )  . In case au-triple is present , the deeper u-triple is deleted , and the T-stack will only contain the u-triple in the top set  ( see the derivation relation above )  . When satisfying au-triple , the T-stack is treated as a set data structure , since the formalism does not pose any constraint on the order of consumption of the u-triples  . 
Following Earley's style , the general idea is to move the dotrightward , while predicting the dependents of the head of the rule  . The dot can advance a crossad-quadruple<rj Yiui ' q > or across the special symbol #  . The d-q-ua-d-irh mediately following the dot can be indexed uj  . This is acknowledged by predicting an item ( representing the subtree rooted by that dependent  )  , and inserting a new progressive integer in the fourth component of the item  ( v-index )  . xj is pushed onto T-stack : the substructure ooted by a node of category Yi must contain the trace nodes of the type licensed by the u -triples  . The prediction of a trace occurs as in the case 2 ) of the derivation process . When an item P contains a dotted rule with the dotat its end and a T-stack with the empty set @ as the top symbol  , the parser looks for the items that can advance the dot  ,   . given the completion of the dotte dependency rule mP  . Here is the algorithm . 
Sentence : w0w1...wn1
Grammar G = < W , C , S , D , I , H > initialization for each x : Q(5) eH(G) , where QeS ( G ) replace achu-index in 8 with a progressive integer ; INSERT < x : Q(*8) ,  0 ,  0 ,  0 , \[\]> into SO body for each set Si(O ~_ i . gn ) do for each P--<y : Y('q?5) , j , Ix , v , T-stack > in Si - - -> completer ( including pseudo completer )  . 
if 5 is the empty sequence and TOP ( T-stack ) = O for each < x : X ( T t?<R , Y , ux , X x > 4) , j  ~ , Ix ' , v ' , T-stack '> in Sj
T-stack "<- POP ( T-stack);
INSERT < x : X(~<ILY , Ux , X x > ? Q , j ' , Ix ' , v ' , T-stack "> into Si ; - - -> predictor : if 5 =< R&Z&u & " c5>   I1 then for each rule z : Z8  ( 0 ) replace a chu-index in 0 with a prog . integer ;
T-stack'<-PUSH-UNION(x&T-stack);
INSERT < z:Z5( ,  0) , i ,  0 , u&T-stack '> intoSi; .   .   .   .   .   .   .   .   .   .   . > pseudopredictor : if < u , R&Z8>e UNION(setiIseti in T-stack);
DELETE<u , R&Z&>fromT-stack;
T-stack'<-PUSH(Q , T-stack);
INSERT < e:ZS( . #), i , u . uS . T-stack '> into Si ; - - -> scanner : if ~= #Tl then if y = wi
INSERT < y:Y (7#"" q) , j , Ix , v , T-stack > into Si + l . . . . . . . . . . . > pseudoscanner : else if y = EINSERT < E : Y(#") , j , IX , v , T-stack > into Si ; end for end for , termination if < x : Q(ct , ) .  0 . is . v . \[\] > esn , where QaS(G ) then accept else reject end if . 
At the beginning ( initialization ) , the parser initializes the set So , by inserting all the dotted rules ( x : Q ( 8 ) eH ( G ) ) that have a head of a root category ( QeS ( G ) ) . The dot precedes the whole d-quad sequence ($) . Each u-index of the rule is replaced by a progressive integer  , in both the u and indices are null (0) , and T-stack is empty (\[\]) . 
The body consists of an external loop on the sets Si  ( 0 < i < n ) and an inner loop on the single items of the set Si  . Let
P = < y:Y (~*8) , j , IX , v , T-stack > be a generic item . Following Eafley's schema , the parser invokes three phases on the item P : completer  , predictor and scanner . Because of the derivation of traces ( 8 ) from the u-triples in T-stack , we need to add some code ( the socalled pseudo-phases ) that deals with completion , prediction and scanning of these ntities . 
Completer : When 8 is an empty sequence ( all the d-quad sequence has been traversed ) and the top of T-stack is the empty set O ( all the triples concerning this item have been satisfied  )  , the dotted rule has been completely analyzed . The completer looks for the items in Si which we rewaiting for completion  ( return items ; j is the retum Position of the item P ) . The return items must contain a dotted rule where the dot immediately precedes ad-quad<R  , Y , ux , X x > , where Y is the head category of the dotted rule in the item P  . Their generic form is < x : X(X*<R , Y , ux , X x > 4) , J ' , IX ' , v ' , T-stack '> . These items from Sj are inserted into Si after having advanced the dot to the right of < R  , Y , ux , %x> . 
Before inserting the items , we need updating the T-stack component , because some u-triples could have been satisfied ( and , then , deleted from the T-stack ) . The new T-stack " is the T-stack of the completed item after popping the top element E ~  . 
Predictor : If the dotted rule of Phasad-quad < Rs  , Z&us , xS > immediately after the dot , then the parser is expecting a subtree headed by a word of category ZS  . This expectation is encoded by inserting a new item  ( predicted item ) in the set , for each rule associated with 28 ( of the form z : Zs ( 0 ) ) . Again , each u-index of the new item ( d-quad sequence 0 ) is replaced by a progressive integer . The v-index component of the predicted item is set to us  . Finally , the parser prepares the new T-stack ' , by pushing the newu-triples introduced by %8 , that are to be satisfied by the items predicted after the current item  . This operation is accomplished by the primitive PUSH-UNION  , which also accounts for the nonrepetition of u -triples in T-stack  . As stated in the derivation relation through the UNION operation  , there cannot be two u-triples with the same relation and syntactic ategory in T-stack  . 
In case of a repetition of au-triple , PUSH deletes the oldu-triple and inserts the new one  ( with the same u-index ) in the topmost set . Finally , INSERT joins the new item to the set Si . 
The pseudopredictor accounts for the satisfaction of the theu-triples when the appropriate conditions hold  . The current d-quad in P , < Rs , Zs , us , xS > , can be the dependent which satisfies the u-triple < u  , Rs , Zs > in T-stack ( the UNION operation gathers all the u-triples scattered through the T-stack  ) : in addition to updating T-stack ( PUSH ( O , T-stack ) ) and inserting the u-index u8 in the v component as usual , the parser also inserts the u-index u in the IX component to coindex the appropriate distant element  . Then it inserts an item ( trace item ) with a fictitious dotte dependency rule for the trace  . 
Scanner : When the dot precedes the symbol # , the parser can scan the current input word wi ( if y , the head of the item P , is equal to it ) , or pseudos can a trace item , respectively . The result is the insertion of a new item in the subsequent set  ( Si + l ) or the same set ( Si )  , respectively . 
At the end of the external loop ( termination ) , the sentence is accepted if an item of a root category Q with a dotted rule completely recognized  , spanning the whole sentence ( Position = 0) , an empty T-stack must be in the set Sn . 
3.1. An example
In this section , we trace the parsing of the sentence " Beans I know John likes "  . In this example we neglec the problem of subject verb agreement : i can be coded by inserting the AGR features in the category label  ( in a similar way to the + EX feature in the grammar  G1  )  ; the comments on the right help to follow the events  ; the separator symbol I helps to keep trace of the sets in the stack  ; finally , we have left in plain text the d-quad sequence of the dotted rules  ; the other components of the items appear in boldface  . 
S0 < know:V . F . , X(*<VISITOR . N ,  1 , 6> < SUBJ , N ,  0 , 6>#< PCOMP , V ,  0 ,  <1 , OBJ , N >>) , 
O , O , O , H > < likes : V(*<SUBJ , N , O , 6>#< OBJ , V , O ,  0>) ,  0 ,  0 ,  0 , H > < beans:N (*#) ,  0 ,  0 ,  1 , \[ O\]><I:N (*#) , 0 , 0 ,  1 , \[ O\]><John:N (*#) ,  0 ,  0 ,  1 , \[0\]> < beans:N (*#) ,  0 ,  0 ,  0 , \[0\]> < I:N (*#) , 0 , 0 , 0 , \[~\]> < John:N (*#) ,  0 ,  0 ,  0 , \[0l > ( initialization )   ( initialization )   ( predictor " know " )   ( predictor " know " )   ( predictor " know " )   ( predictor " likes " )   ( predictor " likes " )   ( predictor " likes " ) 
Sl\[beans\]<beans:N(#o) , O , O ,  1 , \[0l > O carmer ) < know:V+EX(<VISITOR , N ,  1 , Q~>(completer " beans ")* < SUBJ , N , O , 9>#< SCOMP , V ,  0 ,  <1 , OBLN >>) ,  0 ,  0 ,  0 , ~> < likes : V(<SUBJ , N ,  0 , 9> ( completer " beans ")*#< OBJ , V ,  0 , Q )>) ,  0 ,  0 ,  0 , \[\]> < beans:N (*#) ,  1 ,  0 ,  0 , \[ O\]>(predictor"know")<I:N (*#) ,  1 ,  0 ,  0 , \[?~\]> ( predictor " know ") < John : N(*#) ,  1 ,  0 ,  0 ,  \[~\]>  ( predictor " know " ) $2\[I\]<I:N ( # * )  ,  1 ,  0 ,  0 , \[~10\]>(scanner ) < know:V+v . x(<VISITOR , N ,  1 , O > ( completer " I ") < SUBJ , N ,  0 , Q~>*#<SCOMP0V ,  0 ,  <1 , OBJ , N >>) ,  0 ,  0 ,  0 , H>$3\[know\]<know:V+EX(<VISITOR , N ,  1  . 9> ( scanner ) < SUBJ , N ,  0 , ~>#? < SCOMP , V ,  0 ,  <1 , OBJ , N >>) ,  0 ,  0 ,  0 , H > < likes : V(?<SUBJ , N ,  0 . 9> ( predictor " know ")#< OBJ , V ,  0 ,  ~>) ,  3 ,  0 ,  0 ,  \[ <1 , OBJ , N >\] > < beans : N(?#) ,  3 ,  0 ,  2 , \[<1 , OBJ , N > I~\]>(p . " know ") < I : N(?#) , 3 ,  0 ,  2 ,  \[<1 , OBJ , N > I~\]>(p . " know ") < John : N(*#) ,  3 ,  0 ,  2 ,  \[<1 , OBJ , N > 1~\]>(p . " know ") < beans : N(?#) ,  3 ,  0 ,  0 ,  \[<1 , OBJ , N > I~\]>(p . " likes ") < I:N (*#) ,  3 ,  0 ,  0 ,  \[<1 , OBJ , N > I~\]>(p . " likes ") < John : N(*#) ,  3 ,  0 ,  0 ,  \[<1 , OBJ , N > I~l > ( p . " likes")$4\[John\]<John:N(#*) ,  3 ,  0 ,  2 ,  \[ <1 , OBJ , N > 19\]>(scanner ) < John:N(#*) ,  3 ,  0 ,  0 ,  \[ <1 , OBJ , N > I ~\] > ( scanner ) < know:V+EX ( < VISITOR , N ,  2 , 9> ( completer " John ")* < SUBJ , N ,  0 , 9>#< SCOMP , V ,  0 ,  <2 , OBJ , N >>) ,  3 ,  0 ,  0 ,  \[<1 , OBJ , N >\] > < likes : V(<SUBJ , N ,  0 , ~> ( completer " John ")*#< OBJ , V ,  0 ,  ~>) ,  3 ,  0 ,  0 ,  \[ <1 , OBJ , N>\]>$5\[likes\]<likes:V(<SUBJ , N ,  0 , 9> ( scanner ) #? < OBLN ,  0 ,  ~>)  ,  0 ,  0 ,  0 ,  \[<1 , OBJ , N >\] > < beans:N (*#) ,  5 ,  0 ,  0 ,  \[<1 , OBJ , N > I~\]><I : N(?#) ,  5 ,  0 ,  0 ,  \[<1 , OBJ , N > I~\]><John:N(?#) ,  5 ,  0 ,  0 ,  \[<1 , OBJ , N>1 ~\] > <~: N(?#) ,  5 ,  0 ,  0 , \[ O\]><E:N(#*) ,  5 ,  0 ,  0 , \[~\]> < likes : V(<SUBJ , N ,  0 , Q ~> #< OBJ , N ,  0 , O >*) ,  3 ,  0 ,  0 , \[?~'\]> < know:V+EX(<VISITOR . N ,  1 , O > < SUBJ , N ,  0 , ~>#< SCOMP , V ,  0 , < I , OBJ . N >>?),0,0,0,\[\]> (13 . " likes ") ( p . " likes ") ( p . " likes " )   ( pseudopredietor )   ( pseudopredictor )   ( completer )   ( completer )  3 . 2 . Complexity results The parser has a polynomial complexity  . The space complexity of the parser , i . e . the number of items , is O(n3+a DllCl) . Each item is a 5-tuple < Dotted-rule , Position , Ix-index , v-index , T-stack >: Dotted rules are in a number which is a constant of the grammar  , but in offline parsing this number is bounded by O  ( n )  . Position is bounded by O(n ) . Ix-index and v-index are two integers that keep trace of u-triple satisfaction  , and do not add a nown contribution to the complexity count  . T-stack has a number of elements which depends on the maximum length of the chain of predictions  . Since the number of rules is O(n ) , the size of the stack is O(n ) . The elements of T-stack contain all the u-triples introduced up to an item and which are to be satisfied  ( deleted ) yet . Au-triple is of the form < u , R , Y >: u is an integer that is in influent , Ra D , Ya C . Because of the PUSH-UNION operation on T-stack , the number of possible u-triples cattered throughout the elements of T-stack is IDIICI  . The number of different stacks is given by the dispositions of IDllCIu-triples on O  ( n ) elements ; so , O(nlOllCl) . Then , the number of items in a set of items is bounded by O  ( n2+IDIICI ) and there are n sets of items ( O ( n3+IDIICI ) ) . 
The time complexity of the parser is O ( n7+3 IDIICI )  . Each of the three phases executes an INSERT of an item in a set  . The cost of the INSERT operation depends on the implementation f the set data structure  ; we assume it to be linear ( O ( n2+IDIICI ) ) to make easy calculations . The phase completer xecutes at most O ( n2+IDIICI ) ) actions per each pair of items ( two for-loops )  . The pairs of items are O(n6+2 IDIICI) . But to execute the action of the completer , one of the sets must have the index equal to one of the positions  , so O(n5+21 DIICI) . Thus , the completer costs O(n7+3 ID\[ICI) . The item to introduce the predictions ( " for each rule " loop )  ; then , the loop of the pseudopredictor is O ( IDIICI )   ( UNION+DELETE )  , a grammar factor . 
Finally it inserts the new item in the set ( O ( n2+IDIICI ) ) . The total number of items is O ( n3+tDJ ) o ) and , so , the cost of the predictor O(n(i+21 DICl) . 
The phases canner executes the INSERT operation per item  , and the items are at most O(n3+IDIICI ) . 
THUS , the scanner costs O(ns+2IDII?1) . The total complexity of the algorithm is O ( n7+3 ID ttCt )  . 
We are conscious that the ( grammar dependent ) exponent can be very high , but the treatment of the set data structure for the u-triples requires expensive operations  ( cf . a stack ) . Actually this formalism is able to deal a high degree of free word order  ( for a comparabler sult , see ( Becker , Rambow 1995)) . Also , the complexity factor due to the cardinalities of the sets D and C is greatly reduced if we consider that linguistic constraints restric the displacement of several categories and relations  . A better estimation of complexity can only be done when we consider empirically the impact of the linguistic constraints in writing a wide coverage grammar  . 
4. Conclusions
The paper has described a dependency formalism and an Earley-type parser with a polynomial complexity  . 
The introduction of nonlexical categories in a dependency formalism allows the treatment of long distance dependencies and of free word order  , and to a ovid the NP-completeness . The grammar factor at the exponent can be reduced if we furtherly restric the long distance dependencies through the introduction of a more restrictive data structure than the set  , as it happens in some constrained phrase structure formalisms  ( Vijay-
Schanker , Weir 1994).
A compilation step in the parser can produce parse tables that account for left-corner information  ( this optimization of the Earley algorithm has already been proven fruitful in  ( Lombardo , Lesmo 1996)) . 

Becker T . , Rambow O . , Parsing non-immediate dominance relations , Proc . IWPT 95, Prague , 1995, 2633 . 
Covington M.A ., Parsing Discontinuous
Constituents in Dependency Grammar,
Computational Linguistics 16, 1990, 234-236.
Earley J ., An Efficient Context-free Parsing
Algorithm . CACM 13, 1970, 94-102.
Eisner J . , Three New Probabilistic Models for Dependency Parsing : An Exploration  , Proc . 
COLING96, Copenhagen , 1996, 340-345.
Fraser N . M . , Hudson R . A . , Inheritance in Word Grammar , Computational Linguistics 18 ,  1992 ,  133-158 . 
Gaifman H . , Dependency Systems and Phrase Structure Systems , Information and Control 8 ,  1965 ,  304-337 . 
Hahn U . , Schacht S . , Broker N . , Concurrent , Object-Oriented Natural Language Parsing : The Parse Talk Model  , CLIF Report 9/94 , Albert-Ludwigs-Univ . , Freiburg , Germany ( also in Journal of Human Computer Studies )  . 
Hudson R ., English Word Grammar , Basil
Blackwe U , Oxford , 1990.
Kwon H ., Yoon A ., Unification-Based
Dependency Parsing of Governor-Final
Languages , Proc . IWPT 91, Cancun , 1991, 182-192 . 
Lombardo V . , Lesmo L . , An Earley-type recognizer for dependency grammar  , Proc . 
COLING96, Copenhagen , 1996, 723-728.
Mercuk I ., Dependency Syntax : Theory and
Practice , SUNY Press , Albany , 1988.
Milward D ., Dynamic Dependency Grammar,
Linguistics and Phylosophy , December 1994.
Nasr A . , A formalism and a parser for lexicalized dependency grammar  , Proc . IWPT 95, Prague , 1995, 186-195 . 
Neuhaus P . , Broker N . , The Complexity of Recognition of Linguistically Adequate Dependency Grammars  , Proc . ACL/EACL97,
Madrid , 1997, 337-343.
Neuhaus P . , Hahn U . , Restricted Parallelism in Object-Oriented Parsing  , Proc . COLING96,
Copenhagen , 1996, 502-507.
Rambow O ., Joshi A ., A Formal Look at
Dependency Grammars and Phrase Structure Grammars , with Special Consideration of Word-Order Phenomena  , Int . Workshop on The
Meaning-Text Theory , Darmstadt , 1992.
Schabes Y . , Mathematical nd Computational Aspects Of Lexicalized Grammars  , Ph . D . 
Dissertation MS-CIS-90-48, Dept . of Computer and Information Science , University of Pennsylvania , Philadelphia ( PA ) , August 1990 . 
SgaUP . , Haijcova E . , Panevova J . , The Meaning of Sentence in its Semantic and Pragmatic Aspects  , Dordrecht Reidel Publ . Co . , Dordrecht , 1986 . 
Sleator D . D . , Temperley D . , Parsing English with a Link Grammar , Proc . of IWPT 93,199 3, 277-29i . 
Vijay-Schanker K . , Weir D . J . , Parsing some constrained grammar formalisms,
Computational Linguistics 19/4, 1994
