Growing Semantic Grammars
Marsal Gavaldh and Alex Waibel
Interactive Systems Laboratories
Carnegie Mellon University
Pittsburgh , PA 15213, U.S.A.
marsal@cs , cmu.edu

A critical path in the development of natural language understanding  ( NLU ) modules lies in the difficulty of defining a mapping from words to semantics : Usually it takes in the order of years of highly-skilled labor to develop a semantic mapping  , e . g . , in the form of a semantic grammar , that is comprehensive enough for a given domain . Yet , due to the very nature of human language , such mappings invariably fail to achieve full coverage on unseen data  . Acknowledging the impossibility of stating a priori all the surface forms by which a concept can be expressed  , we present GsG : an empathic computer system for the rapid deployment of NLU front-ends and their dynamic customization by nonexpert end-users  . 
Given a new domain for which an NLU frontend is to be developed  , two stages are involved . In the authoring stage , GSQ aids the developer in the construction of a simple domain model and a kernel analysis grammar  . Then , in the runtime stage , GSG provides the end-user with an interactive environment in which the kernel grammar is dynamically extended  . Three learning methods are employed in the acquisition of semantic mappings from unseen data  :   ( i ) parser predictions , ( ii ) hidden understanding model , and ( iii ) end-user paraphrases . A baseline version of GsG has been implemented and pre-llminary experiments show promising results  . 
1 Introduction
The mapping between words and semantics , be it in the form of a semantic grammar , tor of a set of rules that transform syntax trees onto  , say , a frame-slot structure , is one of the major bottlenecks in the development of natural anguage understanding  ( NLU ) systems . A parser will work for any domain but the semantic mapping is domain-dependent  . Even after the domain model has been established , the daunting task of trying to come up with all the possible surface forms by which each concept can  1 Semantic grammars are grammars whose nonterminals correspond to semantic concepts  ( e . g . ,\[ greeting \] or \[ suggest . time\] ) rather than to syntactic on stituents ( such as Verb or Woun Phrase )  . They have the advantage that the semantics of a sentence can be directly read off its parse tree  , and the disadvantage that a new grammar must be developed for each domain  . 
be expressed , still lies a head . Writing such mappings takes in the order of years  , can only be performed by qualified humans ( usually computational linguists ) and yet the final result is often fragile and non -adaptive  . 
Following a radically different philosophy , we propose rapid ( in the order of days ) deployment of NLU modules for new domains with on -needbas is learning : let the semantic grammar grow automatically when and where it is needed  . 
2 Grammar development
If we analyze the traditional method of developing a semantic grammar for a new domain  , we find that the following stages are involved . 
1 . Data collection . Naturally-occurring data from the domain at hand are collected  . 
2 . Design of the domain model . A hierarchical structuring of the relevant concepts in the domain is built in the form of an ontology or domain model  . 
3 . Development of a kernel grammar . A grammar that covers a small subset of the collecte data is constructed  . 
4 . Expansion of grammar coverage . Lengthy , ar-duous task of developing the grammar to extend its coverage over the collected at a and beyond  . 
5 . Deployment . Release of the final grammar for the application at hand  . 
The GsG system described in this paper aids all but the first of these stages : For the second stage  , we have built a simple editor to design and analize the Domain Model  ; for the third , a semiautomated way of constructing the Kernel Grammar  ; for the fourth , an interactivenvironment in which new semantic mappings are dynamically acquired  . As for the fifth ( deployment ) , it advances one place : after the short initial authoring phase  ( stages 2 and 3 above ) the final application can already be launched , since the semantic grammar will be extended , at runtime , by the nonexpert end-user . 
3 System architecture
As depicted in Fig .  1 , GsG is composed of the following modules : the Domain Model Editor and the run  . ~mestage .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 
Figure 1: System architecture of GSG.
Kernel Grammar Editor , for the authoring stage , and the SouP parser and the IDIGA environment , for the runtime stage . 
3.1 Authoring stage
In the authoring stage , a developers creates the Domain Model ( DM ) with the aid of the DME ditor . 
In our present formalism , the DM is simply a directed acyclic graph in which the vertices correspond to concept-labels and the edges indicate concept-subconcept relations  ( see Fig . 2 for an example ) . 
Once the DM is defined , the Kernel Grammar Editor drives the development of the Kernel Grammar by querying the developer to instantiate into grammar rules the rule templates derived from the DM  . 
For instance , in the DM in Fig .  2 , given that concept suggest_time requires ubconcept\[time\]  , the rule template\[suggest_time\]<\[time\] is generated  , which the developer can instantiate into , say , rule (2) in Fig .  3 . 
The Kernel Grammar Editor follows a concrete-to -abstract ordering of the concepts obtained via a topological sort of the DM to query the developer  , after which the Kernel Grammar is complete 3 and 2Understood here as a qualified person ( e . g . , knowledge engineer or software developer ) who is familiar with the domain at hand and has access to some sample sentences that the NLU frontend is supposed to understand  . 
3We say that grammar G is complete with respect o domain model DM if and only if for each arc from concept i to concept j in DM there is at least one gramma rule headed by concept i that contains concept j  . This ensures that any idea expressible in DM has a surface form  , or , seen it from another angle , that any indomain utterance has a paraphrase- . o-\[namel suggestion l\[rejectionl\[ acceptance \] 
Tv~\[suggest_time l\[rejecteime\] accept_time l\[time\[interval?star t_point\[end  . .point  ' , point\[day_of week\[time_of_dayI Figure  2: Fragment of a domain model for a scheduling task . A dashed edge indicates optional subconcept ( default is required )  , a dashed angle indicates inclusive subconcepts ( default is exclusive )  . 
(1 ) \[ suggestion \] ~-- suggest_time ( 2 ) suggest_time ~-- how about \[ time\] ( 3 ) \[ time\]~\[point\] ( 4 ) \[ point\] 4---- * on day_of_week * time_of_day ( 5 ) day_of_week ~--- Tuesday ( 6 ) time_of_day 6--- afterno on Figure 3: Fragment of a grammar for a scheduling task . 
A '*' indicates optionality.
the NLU frontend is ready to be deployed.
It is assumed that : ( i ) after the authoring stage the DM is fixed , and ( ii ) the communicative goal of the end-user is expressible in the domain  . 
3.2 Run-time stage
Instead of attempting " universal coverage " we rather accep the fact that one can never know all the surface forms by which the concepts in the domain can be expressed  . What GsG provides in the runtime stage are mechanisms that allow a nonexpertend-user to " teach " the meaning of new expressions  . 
The tight coupling between the SouP parser 4 and the IDIGAs environment allows for a rapid and multifaceted analysis of the input string  . If the parse , or rather , the paraphrase automatically generated by GSG6 , is deemed incorrect by the end-user , a learning episodensues . 
that is covered by G .
4 Very fast , stochastic topdown chart parser developed by the first author incorporating heuristics to  , in this order , maximize coverage , minimize tree complexity and maximize tree probability  . 
5 Acronym for interactive , distributed , incremental grammar acquisition . 
6In order for all the interactions with the end -user to be performed in natural anguage only  , a generation grammar is needed to transform semantic representations i to surface forms  . To that effect GSG is able to cleverly use the analysis grammar in " reverse  . " Bybringing to bear contextual constraints , Gso can make predictions as to what a sequence of unparsed words might mean  , thereby exhibiting an " empathic " behavior toward the end-user  . To this aim , three different learning methods are employed : parser predictions  , hidden understanding model , and end-user paraphrases . 
3.2.1 Learning
Similar to Lehman (1989) , learning in Gs Q takes place by the dynamic reation of grammar rules that capture the meaning of unseen expressions  , and by the subsequent update of the stochastic models  . Acquiring a new mapping from an unparsed sequence of words onto its desired semantic representation involves the following steps  . 
1 . Hypothesis formation and filtering . Given the context of the sentence at hand , Gsc constructs hypotheses in the form of parse trees that cover the unparsed sequence  , discards those hypotheses that are not approved by the DMr and ranks the remaining by likelihood  . 
2 . Interaction with the end-user . The ranked hypotheses are presented to the end -user in the form of questions about  , or rephrases of , the original utterance . 
3 . Dynamic rule creation . If the end-user is satisfied with one of the options  , a new grammar rule is dynamically created and becomes part of the end-user's grammar until further notice  . 
Each new rule is annotated with the learning episode that gave rise to it  , including end-user ID , timestamp , and a counter that will keep track of how many times the new rule fires in successful parses  ,  3 . 2 . 2 Parser predict ions As suggested by Kiyono and Tsujii  ( 1993 )  , one can make use of parse failures to acquire new knowledge  , both about the nature of the unparsed words and about he inadequacy of the exist in grammar rules  . 
GsG uses incomplete parses to predict what can come next  ( i . e . after the partially-parsed sequence 7I . e . , parse trees containing concept-subconcept relations that are inconsistent with the stipulations of the DM  . 
S The degree of generalization r level o . fabstraction that a new rule should exhibit is an open question but currently a Principle of Maximal Abstraction is followed :  ( a ) Parse the lexical items of the new rule's right -hand-side with all concepts granted toplevel status  , i . e . , able to stand at the root of a parse tree . 
( b ) If a word is not covered by any tree , take it as is into the final right hand side . Else , take the root of the parse tree with largest span  ; if tie , prefer the root that ranks higher in the DM . 
For example , with the DM in Fig . 2 and the grammar in Fig .  3 , What about Tuesday f is abstracted to the maximally general what about \[ time\]  ( as opposed to what about \[ day_of_week\]or what about \[ point \]  )   . 

Figure 4: Example of a learning episode using parser predictions  . Initially only the temporal expression is understood  . . . 
in left-to-right parsing , or before the partially-parsed sequence in right -to-left parsing  )  . This allows two kinds of grammar acquisition : 1 . Discovery of expression equivalence . E . g . , with the grammar in Fig .   3 and input sentence What about Tuesday afternoon ? GsQ is able to ask the end-user whether the utterance means the same as How about Tuesday afternoon ?  ( See Figs . 4, 5 and 6) . That is because in the process of parsing What about Tuesday afternoon ? right-to-left  , he parser has been able to match rule (2) in Fig . 2 up to about , and thus it hypothesizes the equivalence of what and how since that would allow the parse to complete  .  9 2 . Discovery of an ISA relation . Similarly , from input sentence How about no on ? Gs Gisable to predict  , in left-to-right parsing , that no on isa\[time\] . 
3.2.3 Hidden understanding model
As another way of bringing contextual information to bear in the process of predicting the meaning  9For realworld grammars , of , say , over 1000 rules , it is necessary to bound the number of partial parses by enforcing a maximum beam size at the lefthand side level  , i . e . , placing a limit on the number of subparses under each nonterminal to curb the exponential explosion  . 
YNNO : ";-""" < i
Figure 5: ... but a correct prediction is made ...
Pm does . Sin~n ? ~ ~ V hatabout Tuesda yaftar ~ ooo ?
What~t Tuesaay aftemo ~? I
I*-\[su:J gos Lttl\]
I + --, lsit
I?-about
I+-\[tlm\]
I+-\[polntl
I ? -\ [ day_of_woekl
II
I+-ttml day
I4.-\[tii . . el_day \]
Illutoml ~ Refilla , hati ~ ut ~ ue ~ l ~ aftemooniiokIf8 , a---q
L . . . Z . .J . . . . . . . . . ; lst~a ~ LlJ ,  '~  <- - " , , mat about \[ ume \]  I Figure 6: . . . and a new rule is acquired . 
of unparsed words , the following stochastic models , inspired in Miller et al (1994) and Seneff (1992) , and collectively referred to as hidden understanding model  ( HUM )  , are employed . 
? Speech-act ngram . Top-level concepts can be seen as speech acts of the domain  . For instance , in the DM in Fig . 2 toplevel concept such as \[ greeting \] , Cf are well \] or \[ suggestion \] , correspond to discourse speech acts , and innormally-occurring conversation , they follow a distribution that is clearly nonuniform  . 1? ? Concept-subconcept HMM . Discrete hidden Markov model in which the states correspond l ? Needless to say  , speech actransition distributions are empirically estimated  , but , intuitively , the sequence <\[ greeting \] , [ suggestion \]> is more likely than the sequence <\[ greeting \]  , \[farewell \] > . 
to the concepts in the DM(i . e . , equivalent to grammar nonterminals ) and the observations to the embedded concepts appearing as immediate daughters of the state in a parse tree  . 
For example , the parse tree in Fig . 4 contains the following set of < state , observation > pairs : <\[ time\] , \[ point \] > , <\[ point\] , \[ day_of_week\]> , <\[ point\] , \[ time_of_day \] > . 
? Concept-word HMM . Discrete hidden Markov model in which the states correspond to the concepts in the DM and the observations to the embedded lexical items  ( i . e . , grammar terminals ) appearing as immediate daughters of the state in a parse tree  . For example , the parse tree in Fig . 4 contains the pairs : <\[ day_of_week\] , tuesday > , <\[ time_of_day\] , afternoon > . 
The HUM thus attempts to capture the recurring patterns of the language used in the domain in an a synchronous mode  , i . e . , independent ofword order ( as opposed to parser predictions that heavily depend on word order  )  . Its aim is , again , to provide predictive power at runtime : upon encountering an unparsable expression  , the HUM hypothesizes possible intended meanings in the form of a ranked list of the most likely parse trees  , given the current state in the discourse , the subparses for the expression and the lexical items present in the expression  . 
Its parameters can be best estimated through training over a given corpus of correct parses  , but in order not to compromise our established goal of rapid deployment  , we employ the following techniques . 
1 . In the absence of a training corpus , the HUM parameters are seeded from the Kernel Grammar itself  . 
2 . Training is maintained at runtime through dynamic updates of all model parameters after each utterance and learning episode  . 
3.2.4 End-user paraphrases
If the end-user is not satisfied with the hypotheses presented by the parser predictions or the HUM  , a third learning method is triggered : learning from a paraphrase of the original utterance  , given also by the end-user . Assuming the paraphrase is understood ,   11 GsG updates the grammar in such a fashion so that the semantics of the first sentence are equivalent to those of the paraphrase  . 1211 Precisely , the requirement that the grammar be complete ( seenote 3 ensures the existence of a suitable paraphrase for any utterance expressible in the domain  . In practice , however , it may take too many attempts to find an appropriate paraphrase  . Currently , if the first paraphrase is not understood , no further equests are made . 
12 Presently , the root of the paraphrase's parse tree directly becomes the left-hand-side of the new rule  . 

Perfect OkBad
Expert before 55.41 17.58 27.01
Expert after 75.68 10.81 13.51
A+?0.?7 - - 6.77 - -13.50
End-user1 before 58.11 18.92 22.97
End-user1 after 64.86 22.97 12.17
A + 6.75 +. ~.05 - -10.80
End-user2 before 41.89 16.22 41.89
End-user2 after 48.64 28.38 22.98
A + 6.75+1?.16 - -18.91
Table 1: Comparison of parse grades ( in %) . Expert using traditional method vs . nonexperts using GSG . 
4 Preliminary results
We have conducted a series of preliminary experiments in different languages  ( English , German and Chinese ) and domains ( scheduling , travel reservations ) . We present here the results for an experiment involving the comparison of expert vs  . nonexpert grammar development on a spontaneous travel reservation task in English  . The grammar had been developed over the course of three months by a full-time expert grammar writer and the experiment consisted in having this expert develop on an unseen set of  72 sentences using the traditional environment and asking two nonexpert users is to " teach "  Gs6 the meaning of the same 72 sentences through interactions with the system . Table 1 compares the correct parses before and after development  . 
It took the expert 15 minutes to add 8 rules and reduce bad coverage from 27  . 01% to 13 . 51% . As for the nonexperts , end-user 1 , starting with a similar grammar , reduced bad parses from 22 . 97% to 12 . 1 7% through a 30-minute session 14 with GsG that gave rise to 8 new rules ; end-user2 , starting with the smallest possible complete grammar  , reduced bad parses from 41 . 89% to 22 . 9 8% through a 35-minute session 14 that triggered the creation of 17 new rules . 
60% of the learning episodes were successful , with an average number of questions of 2 . 91 . The unsuccessful learning episodes had an average number of questions of  6  . 1 9 and their failure is mostly due to unsuccessful paraphrases  . 
As for the nature of the acquired rules , they differ in that the expert makes use of optional and repeatable tokens  , an expressive power not currently available to GSG  . On the other hand this lack of generality can be compensated by the Principle of Maximal Abstraction  ( seenote 8 )  . As an example , to cover the new construction And your last name  ?  , the expert chose to create the rule :\[ request mame \] ~* and your last name tS Undergraduate sudents not majoring in computer science or linguistics  . 
14 Including a 5-minute introduction.
whereas both end-user1 and end-users induced the automatic acquisition of the rule :\[ requostmame\]~CONJPOSS\[ last \] name  .   15   5 Discussion Although preliminary and limited in scope  , these results are encouraging and suggest hat grammar development by nonexperts through GsG is indeed possible and cost-effective  . It can take the nonexper twice as long as the expertogo through a set of sentences  , but the main point is that it is possible at all for a user with no background in computer science or linguistics to teach G so the meaning of new expressions without being aware of the underlying machinery  . 
Potential applications of GSG are many , most notably a very fast development of NLU components for a variety of tasks including speech recognition and NL interfaces  . Also , the IDIGA environment enhances the usability of any system or application that incorporates it  , for the end-users are able to easily " teach the computer " their individual anguage patterns and preferences  . 
Current and future work includes further development of the learning methods and their integration  , design of a rule-merging mechanism , comparison of individual vs . collective grammars , distributed grammar development over the World Wide Web  , and integration of GSG's runtime stage into the JANUS speech recognition system  ( Lavie et al 1997 )  . 

The work reported in this paper was funded in part by a grant from ATR Interpreting Telecommunications Research Laboratories of Japan  . 

Kiyono , Masaki and Junichi Tsujii .  1993 . " Linguistic knowledge acquisition from parsing failures  . " In Proceedings of the 6th Conference of the European Chapter of the ACL . 
Lavie , Alon , Alex Waibel , Lori Levin , Michael Finke , Donna Gates , Marsal Gavaldh , Torsten Zeppenfeld , and Puming Zhan .  1997 . " JANus II hspeech-to-speech translation i multiple languages  . " In Proceedings of ICASSP-97 . 
Lehman , Jill Fain .  1989 . Adaptive parsing : Self-extending natural anguage interfaces  . Ph . D . dissertation , School of Computer Science , Carnegie Mellon

Miller , Scott , Robert Bobrow , Robert Ingria , and Richard Schwartz .  1994 . " Hidden understanding models of natural anguage . " In Proceedings of ACL-9$ . 
Seneff , Stephauie .  1992 . " TINA : a natural anguage system for spoken language applications  . " In Computational Linguistics , vol . 18, no . 1, pp .  61-83 . 
15 Uppercased nonterminals ( uch as COIJ and POSS ) are more syntactical in nature and do not depend on the DM  . 


Undels camins critics enel desenvolupament de mbduls de  comprensi6 dell lenguatge natural pass a perladificult at de definirla  funci6 que assign a , a unase qii~ncia demots , la represent a ci6sem~ntica desit jada . Elsm~todes tradicionals per definira questa correspond~nci are quereix enl'es for q de lingiiistes computacionals  , que de diquen mesoso ~ dhu canys construint , per exemple , unagram ~ ticasem~ntica ( formalismeene lqualels s~mbols no terminals de lagram ~ tica corresponen directamentals concept es del dominide  l'aplicaci6 determinada )  , i , tanmate ix , degut precisamental aprbpia natura del l lenguatge hum ~  , lagram ~ tica resultant main o 4s capaq decobrir to tsels mots i expressions que ocor-ren natural mental dominien  qiiesti6  . 
Recone ixent pertant laim possibilitat d'establir a priori to tesles for mesuperficial sambqu ~ un con-cepte potser express at  , presentemena quest tre-ball GsG : unsistema computacional emp ~ ticper alr ~ piddesplegament demb duls de  comprensi6 dell lenguatge naturalillur adaptaci6 din & micaales particularitatsi prefertncies d ' usuaris finals in ex-perts  . 
El proc4s de construcci6 d ' unmb dulde com-prensi6 dell lenguatge natural per a unnoudominipot set dividiten dues parts  . Primerament , durantla fase decomposici5 , Gs Gaju dael desenvolup a dor experten l'estructuraci6 dels conceptes del domini ( ontologia ) ie nl ' establiment d ' unagram & tica minimal . To tseguit , durantla fased'execuci5 , Gs ~ forneixl ' usuari final in expert d ' unmed i interactiue nqu & lagram & tica  4s augment a dain & micament . 
Tresm ~ todes d ' a prenent at geautom & tic s6n uti-litzatsen l'adquisici6 deregles gramaticals a partir denoves frases i construccions :  ( i ) prediccions del'analitzador ( GSG empraan & lisis in completes per conjecturar quins roots poden apar & ixertant desprds del ' arbred ' an M is incomplet  , en an Misid'es querraadreta , cornabansdel ' arbred ' an M is incomplet , en an Misided retaaes querra ) ,   ( ii ) cadenes de Markov ( m ~ to deses to chstics que model en , independent ment del ' or dredels mots , la distribuci6 dels conceptes illurstransicions , emprats per calcularel concepte global m4s probable do nats un contextiuns arbres d ' an Misi parcials determinats  )  , i ( iii ) par & frasis ( em-prades per assign a rllur representaci6 sem & ntical afrase original )  . 
Hem implement at una primera versi6 de Gs Giels result at sobting uts , perb4 que preliminars ,   6n benencor at ja dors car demostren que unusuari nexpert pot " ensenyar " aGs Gel significat de noves expression sicausar una  extensi6 delagram & tica comparable alad ' unexpert . 
Actual mentestem treballanten lamill or a delsm & to desautom & tics d ' a prenent at geillur  inte-graci6  , en el dissen yd ' un me can is me de corn-binaci6 autom~ticaderegles gramaticals , enla comparaci6 degram & tiques individual sambgram & tiques col . lectives , en el de senvolup ament distribu ' it degram ~ tiques a  trav4s de la World Wide Web , i en la integraci6 delafase d'execuci6 de Gs Genel sistema de recone ixe-ment de la parlai  traducci6 autom~tica JANUS . 

