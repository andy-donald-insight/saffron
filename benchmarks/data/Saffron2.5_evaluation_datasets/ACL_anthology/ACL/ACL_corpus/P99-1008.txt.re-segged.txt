Finding Parts in Very Large Corpora
Matthew Berland , Eugene Charniak
rob , ec@cs.brown , edu
Department of Computer Science
Brown University , Box 1910
Providence , RI 02912

We present a method for extracting parts of objects from wholes  ( e . g . " speedometer " fom " car ") . Given a very large corpus our method finds part words with  55% accuracy for the top 50 words as ranked by the system . The part list could be scanned by an end-user and added to an existing ontology  ( such as WordNet )  , or used as a part of a rough semantic lexicon . 
1 Introduction
We present a method of extracting parts of objects from wholes  ( e . g . " speedometer " fom " car ") . To be more precise , given a single word denoting some entity that has recognizable parts  , the system finds and rank-orders other words that may denote parts of the entity in question  . Thus the relation found is strictly speaking between words  , a relation Miller\[1\] calls " meronymy . " In this paper we use the more colloquial " part of " terminology  . 
We produce words with 55?? accuracy for the top 50 words ranked by the system , given a very large corpus . Lacking an objective definition of the part of relation  , we use the majority judgment of five human subjects to decide which proposed parts are correct  . 
The program's output could be scanned by an end -user and added to an existing ontology  ( e . g . , WordNet ) , or used as a part of a rough semantic lexicon . 
To the best of our knowledge , there is no published work on automatically finding parts from unlabeled corpora  . Casting our nets wider , the work most similar to what we present here is that by Hearst  \[2\] on acquisition of hyponyms ( " isa " relations )  . In that paper Hearst ( a ) finds lexical correlates to the hyponym relations by looking in text for cases where known hyponyms appear in proximity  ( e . g . , in the construction ( NP , NP and ( NP other NN )) as in " boats , cars , and other vehicles ") , ( b ) tests the proposed patterns for validity , and ( c ) uses them to extract relations from a corpus . In this paper we apply much the same methodology to the part of relation  . Indeed , in \[2\] He arst states that she tried to apply this strategy to the part of relation  , but failed . We comment later on the differences in our approach that we believe were most important to our comparative success  . 
Looking more widely still , there is an ever-growing literature on the use of statistical/corpus-based techniques in the automatic acquisition of lexical-semantic knowledge  ( \[38\] )  . We take it as axiomatic that such knowledge is tremendously useful in a wide variety of tasks  , from lower-level tasks like noun phraser ference , and parsing to user-level tasks such as web searches  , question answering , and digest-ing . Certainly the large number of projects that use WordNet  \[1\] would support his contention . And although WordNet is handbuilt , here is general agreement that corpus-based methods have an advantage in the relative completeness of their coverage  , particularly when used as supplement so the more labor-intensive methods  . 
2 Finding Parts 2.1 Parts
Webster's Dictionary defines " part " as " one of the often in definite or unequal subdivisions into which something is or is regarded as divided and which together constitute the whole  . " The vagueness of this definition translates into a lack of guidance on exactly what constitutes a part  , which in turn translates into some doubts about evaluating the results of any procedure that claims to find them  . More specifically , note that the definition does not claim that parts must be physical objects  . Thus , say , " novel " might have " plot " as a part . 
In this study we handle this problem by asking informants which words in a list are parts of some target word  , and then declaring majority opinion to be correct  . We give more details on this aspect of the study later  . Here we simply note that while our subjects often disagreed  , there was fair consensus that what might count as apart depends on the nature of the stitution yields its members  , and a concept yields its characteristics and processes  . In other words , " floor " is part of " building " and " plot " is part of " book  . " 2  . 2 Pat terns Our first goal is to find lexical patterns that tend to indicate part-whole relations  . Following Hearst\[2\] , we find possible patterns by taking two words that are in a part-whole relation  ( e . g , basement and building ) and finding sentences in our corpus ( we used the North American News Corpus ( NANC ) from LDC ) that have these words within close proximity . The first few such sentences are : . . . the basement of the building . 
. . . the basement in question is in a four-story apartment building  . . . 
... the basement of the apartment building.
From the building's basement ...
... the basement of a building ...
... the basements of buildings ...
From these examples we construct he five patterns shown in Table  1  . We assume here that parts and wholes are represented by individual lexical items  ( more specifically , as head nouns of noun phrases ) as opposed to complete noun phrases , or as a sequence of " important " noun modifiers together with the head  . 
This occasionally causes problems , e . g . , " conditioner " was marked by our informants as not part of " car "  , whereas " air conditioner " probably would have made it into a part list  . Nevertheless , in most cases head nouns have worked quite well on their own  . 
We evaluated these patterns by observing how they performed in an experiment on a single example  . 
Table 2 shows the 20 highestranked part words ( with the seed word " car " ) for each of the patterns AE . 
( We discuss later how the rankings were obtained . ) Table 2 shows patterns A and B clearly outperform patterns C  , D , and E . Although parts occur in all five patterns ~ the lists for A and B are predominately parts -oriented  . The relatively poor performance of patterns C and E was ant ! cipated  , as many things occur " in " cars ( or buildings , etc . ) other than their parts . Pattern D is not so obviously bad as it differs from the plural case of pattern B only in the lack of the determiner " the " or " a "  . However , this difference proves critical in that pattern D tends to pick up " counting " nouns such as " truck load  . " On the basis of this experiment we decided to proceed using only patterns A and B from Table  1  . 
A . whole NN\[-PL\]'s POS part NN\[-PL\] .   .   . building's basement .   .   . 
B . part NN\[-PL\] of PREP the IaDE Troods\[ JJINN \]* whole NN  .   .   . basement of a building .   .   . 
C . part NN in PREP the laDE Troods\[JJINN\]* whole NN  .   .   . basement in a building .   .   . 
D . parts NN-PL of PREP wholes NN-PL .   .   . basements of buildings .   .   . 
E . parts NN-PL in PREP wholes NN-PL .   .   . basements in buildings .   .   . 
Format : type_of_wordTAG type_of_wordTAG ...
NN=Noun,NN-PL=PluralNoun
DET = Determiner , PREP = Preposition
POS = Possessive , JJ = Adjective
Table h Patterns for partOf(basement , building ) 3 Algorithm 3 . 1 Input We use the LDC North American News Corpus ( NANC )  . which is a compilation of the wire output of several US newspapers  . The total corpus is about 100,000,000 words . We ran our program on the whole dataset , which takes roughly four hours on our network . The bulk of that time ( around 90% ) is spent tagging the corpus . 
As is typical in this sort of work , we assume that our evidence ( occurrences of patterns A and B ) is independently and identically distributed ( lid )  . We have found this assumption reasonable , but its break down has led to a few errors . In particular , a drawback of the NANC is the occurrence of repeated articles  ; since the corpus consists of all of the articles that come over the wire  , some days include multiple , updated versions of the same story , containing identical paragraphs or sentences . We wrote programs to weed out such cases , but ultimately found them of little use . First , " update " articles still have substantial variation  , so there is a continuum between these and articles that are simply on the same topic  . 
Second , our data is so sparse that any such repeats are very unlikely to manifes themselves as repeated examples of part-type patterns  . Nevertheless since two or three occurrences of a word can make it rank highly  , our results have a few anomalies that stem from failure of the i id assumption  ( e . g . , quite appropriately , " clunker ") . 

PatternAheadlight windshiel dignition shifter dashboardra-diator brake tail pipe pipeair bag speedometer converterhood trunk visorvent wheel occupant enginety re 
PatternBtrunk wheeldriverhood occupants eat bumper back seat dashboard jalopy fender earro of wind-shield back clunker windowshipment reen act men taxle 
Pattern Cp as senger gunmen leafle thophouse plant air baggunk or an cocain eget away motorist phone men in decency person ride woman detonator kid key 
PatternD import caravan make dozen car cass hipment hundred thousand sale export model truck load queue million boatload inventory hood registration trunkten 
Pattern Eairbag packet switch gemamate ur device handgun passenger fires muggler phone tag driver we apon meal compartment croatian defect refuge e delay Table  2: Grammatical Pattern Comparison Our seeds are one word  ( such as " car " ) and its plural . We do not claim that all single words would fare as well as our seeds  , as we picked highly probable words for our corpus  ( such as " building " and " hospital " ) that we thought would have parts that might also be mentioned therein  . With enough text , one could probably get reasonabler sults with any noun that met these criteria  . 
3.2 Statistical Methods
The program has three phases . The first identifies and records all occurrences of patterns A and B in our corpus  . The second filters out all words ending with " ing '  , " ness ' , or " ity ' , since these suffixes typically occur in words that denote a quality rather than a physical object  . Finally we order the possible parts by the likelihood that they are true parts according to some appropriate metric  . 
We took some care in the selection of this metric . At an intuitive level the metric should be something like p  ( w\[p )  .   ( Here and in what follows w denotes the outcome of the random variable generating wholes  , and p the outcome for parts . W ( w ) states that w appears in the patterns AB as a whole  , while P ( p ) states that p appears as a part . ) Metrics of the form p ( wIP ) have the desirable property that they are invariant over p with radically different base frequencies  , and for this reason have been widely used in corpus-based lexical semantic research  \[3  , 6 , 9\] . 
However , in making this intuitive idea someone more precise we found two closely related versions : p  ( w , W ( w)IP ) p(w , w ( ~ , ) Ip , e ( p ) ) We call metrics based on the first of these " loosely conditioned " and those based on the second " strongly conditioned "  . 
While invariance with respect of requency is generally a good property  , such invariant metrics can lead to bad results when used with sparse data  . In particular , if a part word p has occurred only once in the data in the AB patterns  , then perforce p ( w\[P ) = 1 for the entity w with which it is paired . Thus this metric must be tempered to take into account the quantity of data that supports its conclusion  . To put this another way , we want topic k(w , p ) pairs that have two properties , p(wIP ) is high and \[ w , pl is large . We need a metric that combines these two desiderata in a natural way  . 
We tried two such metrics . The first is Dun-ning's \[10\] loglikelihood metric which measures how " surprised " one would be to observe the data counts Iw  , p\[ , \[ - , w , pl , \[ w ,  -  , pl and I-'w , -' Plifone assumes that p(wIP ) = p(w ) . Intuitively this will be high when the observed p ( wIP ) >> p ( w ) and when the count supporting this calculation are large  . 
The second metric is proposed by Johnson ( personal communication )  . He suggests asking the question : how far apart can we besure the distributions p  ( w\[p ) and p ( w ) are if we require a particular significance level  , say . 05 or . 01 . We call this new test the " significant -difference " test  , or sigdiff . Johnson observes that compared to sigdiff , loglikelihood tends to overestimate the importance of data frequency at the expense of the distance between p  ( wIP ) and 3 . 3 Compar i son Table 3 shows the 20 highestranked words for each statistical method , using the seed word " car . " The first group contains the words found for the method we perceive as the most accurate  , sigdiff and strong conditioning . The other groups show the differences between them and the first group  . The + category means that this method adds the word to its list  , -means the opposite . For example , " back " is on the sigdiff-loose list but not the sigdiff-strong list  . 
In general , sigdiff worked better than surprise and strong conditioning worked better than loose conditioning  . In both cases the less favored methods tend to promote words that are less specific  ( " back " over " airbag " , " use " over " radiator ") . Furthermore , the air bag brake bumper dashboard driver fender headlighthoodignition occupant piperadi-ator seat shifter speedometer tail pipe trunkvent wheelwinds hield 
Sigdiff , Loose+backback seat over steerre arro of vehicle visor-air bag brake bumper pipespeedometer tailpip event 
Surprise , Strong+back cost engine owner pricer earro of use value window-air bag bumper fenderignition piperadiator shifter speedometer tailpip event 
Surprise , Loose+back cost engine frontowner price rear roofside value version window-air bag brake bumper dashboard fenderig-nition piperadiator shifter speedometer tailpip event 
Table 3: Methods Comparison combination of sigdiff and strong conditioning worked better than either by itself  . Thus all results in this paper , unless explicitly noted otherwise , were gathered using sigdiff and strong conditioning combined  . 
4 Results 4.1 Testing Humans
We tested five subjects ( all of whom were unaware of our goals ) for their concept of a " part . " We asked them to rate sets of 100 words , of which 50 were in our final results set . Tables 6  -  11 show the top 50 words for each of our six seed words along with the number book  10   8   20   14   30   20   40   24   50   28   5O hospital school of subjects who marked the word as a part of the seed concept  . The score of individual words vary greatly but there was relative consensus on most words  . We put an asterisk next to words that the majority subjects marked as correct  . Lacking a formal definition of part , we can only define those words as correct and the rest as wrong  . While the scoring is admittedly not perfect 1 , it provides an adequate reference result . 
Table 4 summarizes these results . There we show the number of correct part words in the top  10  ,  20 ,  30 ,  40 , and 50 parts for each seed ( e . g . , for " book " , 8 of the top 10 are parts , and 14 of the top 20) . Overall , about 55% of the top 50 words for each seed are parts , and about 70% of the top 20 for each seed . The reader should also note that we tried one ambiguous word  , " plant " to see what would happen . Our program finds parts corresponding to both senses  , though given the nature of our text , the industrial use is more common . Our subjects marked both kinds of parts as correct  , but even so , this produced the weakest part list of the six words we tried  . 
As a baseline we also tried using as our " pattern " the head nouns that immediately surround our target word  . We then applied the same " strong conditioning , sigdiff " statistical test to rank the candidates . 
This performed quite poorly . Of the top 50 candidates for each target , only 8% were parts , as opposed to the 55% for our program . 
4.2 WordNet
WordNet + door engine floorboard gear grille horn mirror rooftail fin window-brake bumper dashboardriver headlightig-nition occupant pipe radiators eat shifter speedometer tailpip event wheelwinds hield 
Table 5: WordNet Comparison
We also compared out parts list to those of WordNet  . Table 5 shows the parts of " car " in WordNet that are not in our top  20   ( + ) and the words in our top 20 that are not in WordNet (  -  )   . There are definite tradeoffs , although we would argue that our top-20 set is both more specific and more comprehensive . 
Two notable words our top 20 lack are " engine " and " door " , both of which occur before 100 . More generally , all WordNet parts occur somewhere before 500 , with the exception of " tailfin ' , which never occurs with car . It would seem that our program would be lFor instance  , " shifter " is undeniably part of a car , while " production " is only arguably part of a plant  . 
60 a good tool for expanding Wordnet , as a person can scan and mark the list of part words in a few minutes  . 
5 Discussion and Conclusions
The program presented here can find parts of objects given a word denoting the whole object and a large corpus of unmarked text  . The program is about 55% accurate for the top 50 proposed parts for each of six examples upon which we test edit  . There does not seem to be a single cause for the 45% of the cases that are mistakes . We present here a few problems that have caught our attention  . 
Idiomatic phrases like " a jalopy of a car " or " the sonofagun " provide problems that are not easily weeded out  . Depending on the data , these phrases can be as prevalent as the legitimate parts  . 
In some cases problems arose because of tagger mistakes  . For example , " re-enactment " would be found as part of a " car " using pattern B in the phrase " there -enactment of the car crash " if " crash " is tagged as a verb  . 
The program had some tendency to find qualities of objects  . For example , " driveability " is strongly correlated with car . We try to weed out most of the qualities by removing words with the suffixes " hess "  , " ing ' , and " ity . " The most persistent problem is sparse data , which is the source of most of the noise . More data would almost certainly allow us to produce better lists  , both because the statistics we are currently collecting would be more accurate  , but also because larger numbers would allow us to find other reliable indicators  . 
For example , idiomatic phrases might be recognized as such . So we see " jalopy of a car " ( two times ) but not , of course , " the car's jalopy " . Words that appear in only one of the two patterns are suspect  , but to use this rule we need sufficient counts on the good words to be sure we have a representative sample  . At 100 million words , the NANC is not exactly small , but we were able to process it in about four hours with the machines at our disposal  , so still larger corpora would not be out of the question  . 
Finally , as noted above , Hearst \[2\] tried to find parts in corpora but did not achieve good results  . 
She does not say what procedures were used , but assuming that the work closely paralleled her work on hyponyms  , we suspec that our relative success was due to our very large corpus and the use of more refined statistical measures for ranking the output  . 
6 Acknowledgments
This research was funded in part by NSF grant IRI-9319516 and ONR Grant N0014-96-1-0549  . Thanks to the entire statistical NLP group at Brown  , and particularly to Mark Johnson , Brian Roark , Gideon Mann , and Ann-Maria Popescu who provided invaluable help on the project  . 
References\[1\] George Miller , Richard Beckwith , Cristiane Fellbaum , Derek Gross & Katherine J . Miller , " WordNet : an online lexicai database , " International Journal of Lexicography 3 (1990) ,  235-245 . 
\[2\] Marti Hearst , " Automatic acquisition of hyponyms from large text corpora  , " in Proceedings of the Fourteenth International Conference on Computational Linguistics  , , 1992 . 
\[3\] Ellen Riloff & Jessica Shepherd , "A corpus-based approach for building semantic lexicons  , " in Proceedings of the Second Conference on Empirical Methods in Natural Language Processing  ,  1997 ,  117-124 . 
\[4\] Dekang Lin , " Automatic retrieval and clustering of similar words  , " in 36th Annual Meeting of the Association for Computational Linguistics and  17th International Conference on Computational Linguistics  ,  1998 ,  768-774 . 
\[5\] Gregory Grefenstette , " SEXTANT : extracting semantics from raw text implementation details  , " Heuristics : The Journal of Knowledge Engineering  ( 1993 )  . 
\[6\] Brian Roark & Eugene Charniak , " Noun-phrase cooccurrence statistics for semiautomatic semantic lexicon construction  , " in 36th Annual Meeting of the Association for Computational Linguistics and  17th International Conference on Computational Linguistics  ,  1998 ,  1110-1116 . 
\[7\] Vasileios Hatzivassiloglou & Kathleen R . McKeown , " Predicting the semantic orientation of adjectives  , " in Proceedings of the 35th Annual Meeting of the ACL ,  1997 ,  174-181 . 
\[8\] Stephen D . Richardson , William B . Dolan & Lucy Vanderwende , " MindNet : acquiring and structuring semantic information from text  , " in 36th Annual Meeting of the Association for Computational Linguistics and  17th International Conference on Computational Linguistics  ,  1998 ,  1098-1102 . 
\[9\] William A . Gale , Kenneth W . Church & David Yarowsky , "A method for disambiguating word senses in a large corpus  , " Computers and the Humanities (1992) . 
\[10\] Ted Dunning , "Accurate methods for the statistics of surprise and coincidence  , " Computational
Linguistics 19(1993), 6174.



Word author subtitle coauthor foreword publication epigraph co-editor cover copy page title authorship manuscript chapte repilogue publisher jacket subject double-pages ale excerpt content plot galley edition protagonist co-publisher spine premise revelation theme fallacy editor translation character tone flaw section introduction release diarist preface narrator format facsimile mock-up essay back heroine pleasure 
Table 6: book x/5   5*   4*   4*   5*   4*   5*   5*   5*   5*   4*   5*   5*   5*   4*   3*   5*   5*   4*   5*   4*   4*   5*   4*   72   154   527   2116   42   156   85   456   100   577   9   23   32   162   28   152   12   45   49   333   7   20   30   250   14   89   14   93   10   60   23   225   4   9   10   62   36   432   7   37   82   1449   23   276   37   572   12   120   3   6   13   156   9   83   32   635   219   6612   7   58   11   143   2   2   2   2   2   2   47   1404   9   115   14   285   129   5616   17   404   25   730   15   358   3   11   6   72   3   12   37   1520   10   207   39   1646   2   3   38   1736   4   31 
Word rubble ~ oor faca debase mentro of atrium exterior tenantro of top wreckages tair wells hell demolition balcony hallway renovation janitor rotundaent rance hulkwall ruin lobby courtyard tenancy debrispipe interior frontelevator evacuation website airshaft cornice construction l and lord occupant owner rear destruction super intendent stair way cellar half-mile step corrid or window subbasement doors pire 
Table 7: building x/5   4*   5*   5*   4*   5*   5*   5*   5*   3*   4*   4*   5*   4*   3*   3*   5*   5*   5*   5*   5*   4*   3* 

Word trunk winds hield dashboard headlight wheel ignition hood driver radiator shifter occupant brak event fender tail pipe bumper pipeair bag seat speedometer converter backs eat window roof  . 
jalopy enginer earvisor deficiency back over steer plate cigarette clunker battery interior speed shipment re-enactment conditioner axletank attribute location cost paintantenn as ocket corsatire 
Table 8: car x/5   4*   5*   5*   5*   5*   4*   5*   5*   3*   5*   5*   5*   3*   5*   4*   4*   5*   5*   4*   3*   5*   3*   5*   5*   5*   5* 


Word ward radiologist trograncic mortuary hope well clinicane as the tist ground patient floor unit room entrance doctor administrator corrid or staff department bed pharmacist director superintendent storage chief lawn compound head nurse switchboard debrisexecutive pediatrician board are aceoyard front reputation in mate procedure overhead committee mile center pharmacy laboratory programs hah president ruin 
Table 9: hospital x/5   5*   5*   5*   4*   4*   5*   5*   4*   3*   5*   5*   4*   5*   3*   3*   5*   4*   4*   3*   4*   4*   5* 
Frame 98O constructions talk reactor emission modernization melters hut downstart up worker root closure completion operator inspection location gate sproutle af output turbine equipment residue zen foliage conversion workforce seed design fruit expansion pollution cost tour employees ite owner roof manager operation characteristic production shoot unit to werco-owner instrumentation ground fiance e economics energy 
Table 10: plant x/5   3*   3*   4*   3*   3*   5*   3*   4*   3*   4*   5*   5*   4*   3*   3*   3* 


Word dean principal graduate prom head mistress
Mumnicurriculums eventh-gradergy mnasium faculty criten dowment ~ umn ~ cade ten rollment in fwmary vale dictorian commandant student feet auditorium jamies on year book cafeteria teacher grader wennbergjeffepupil campus class trustee counselor benefactor berthhallwaym as cotfounder rask in playground program ground courtyard hall championship accreditation fellow freund rector class room 
Table 1 I : school 5*   3*   3*   4*   3*   5*   3*   5*   5*   4*   4*   5* o ' 3*   4*   5*   3*   4*   4*   3*   4*   3*   3*   3*   4* 
