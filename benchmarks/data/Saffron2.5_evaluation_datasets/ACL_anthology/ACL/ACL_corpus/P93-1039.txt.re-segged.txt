RESPONDING TOUSER QUERIESIN A COLLABORATIVE ENVIRONMENT * 
Jennifer Chu
Department of Computer and Information Sciences
University of Delaware
Newark , DE 19716, USA
Internet : jchu@cis.udel.edu

We propose a plan-based approach for responding to user queries in a collaborative environment  . We argue that in such an environment , the system should not accep the user's query automatically  , but should consider it a proposal open for negotiation  . In this paper we concentrate on cases in which the system and user disagree  , and discuss how this disagreement can be detected  , negotiated , and how final modifications should be made to the existing plan  . 
1 Introduction
In task-oriented consultation dialogues , the user and expert jointly construct a plan for achieving the user's goal  . 
In such an environment , it is important hat the agents agree on the domain plan being constructed and on the problem -solving actions being taken to develop it  . This suggests that he participants communicate their disagreements when they arise lest the agents work on developing different plans  . We are extending the dialogue understanding system in  \[6\] to include a system that responds to the user's utterances in a collaborative manner  . 
Each utterance by a participant constitutes a proposal intended to affect heagents'shared plan  . One component of our architecture , the evaluator , examines the user's proposal and decides whether to accept or reject it  . Since the user has knowledge about his/her particular circumstances and preferences that influence the domain plan and how it is constructed  , the evaluator must be are active planner that interacts with the user to obtain information used in building the evaluation meta-plan  . Depending on the evaluation , the system can acceptor rejec the proposal , or suggest what it considers to be a better alternative  , leading to an embedded negotiation subdialogue . 
In addition to the evaluator , our architecture consists of a goal selector , an intentional planner , and a discourse realizer . The goal selector , based on the result of the evaluation and the current dialogue model  , selects an appropriate intentional goal for the system to pursue  . The intentional planner builds a plan to achieve the intentional goal  , and the discourse realizer generates utterances to convey information based on the intentional plan  . 
This paper describes the evaluator , concentrating on cases in which the system and user disagree  . We show how the system determines that the user's proposed additions are erroneous and  , instead of directly responding to the user's utterances  , conveys the disagreement . Thus , our work contributes to an overall dialogue system by  1  ) extending the model in \[6\] to eliminate the assumption that the system will automatically answer the user's questions or follow the user's proposals  , and 2 ) capturing the notion * This material is based upon work supported by the National Science Foundation under Grant No  . IRI-9122026 . 
of cooperativer sponses within an overall collaborative framework that allows for negotiation  . 
2 The Tripartite Model
Lambert and Carberry proposed a plan-based tripartite model of expert/novice on sultation dialogue which includes a domain level  , a problem-solving level , and a discourse level\[6\] . The domain level represent she sys-tem's beliefs about the user's plan for achieving some goal in the application domain  . The problem-solving level encodes the system's beliefs about how both agents are going about constructing the domain plan  . The discourse level represents the system's beliefs about both agents ' communicative actions  . Lambert developed a plan recognition algorithm that uses contextual knowledge  , world knowledge , linguistic clues , and a library of generic recipes for actions to analyze utterances and construct a dialogue  model\[6\]  . 
Lambert ' system automatically adds to the dialogue model all actions inferred from an utterance  . However , we argue that in a collaborative environment , he system should only accep the proposed additions if the system believes that they are appropriate  . Hence , we separate the dialogue model into an existing dialogue model and a proposed model  , where the former constitutes the shared plan agreed upon by both agents  , and the latter the newly proposed actions that have not yet been confirmed  . 
Suppose arlier dialogue suggests that the user has the goal of getting a Master's degree in CS  ( Get-Masters ( U , CS )) . Figure 1 illustrates the dialogue model that would be built after the following utterances by Lam-bert ' splan recognition algorithm odified to accommodate the separation of the existing and propose dialogue models  , and augmented with a relaxation algorithm to recognize ill-formed  plans\[2\]  . 
U : I wanto satisfy my seminar course requirement . 
Who's teaching CS 689?3 The Evaluator
A collaborative system should only incorporate proposed actions into an existing plan if they are considered appropriate  . This decision is made by the evaluator , which will be discussed in this section . This paper only considers cases in which the user 's proposal contains an infeasible action  ( one that cannot be performed ) or would result in an ill-formed plan ( one whose actions do not contribute to one another as intended  ) \[9\] . 
We argue that the evaluator , in order to check for erroneous plans/goals , only needs to examine actions in the proposed model  , since actions in the existing model would have been checked when they were proposed  . 
When a chain of actions is proposed , the evaluator starts examining from the topmost action so that the most general action that is inappropriate will be addressed  . 

Domain Level ~~'-\]-i_~o_,o_-_~ . .~_o ~ : m_, .   .   .   .   .   .   . 
. . . . . . . .  ~  . . . . ?--' ! Is~-s,~,,~-co~,~,cs ) ~ .   . . . . . . . . . 
t ~, " .
P ~ b , 1era-Solv-mg_Lev?1 .   .   .   .   .   .   .   .   .   .   .   .   .   . " ~, i Tal ~_ Com , ~( U , CS 689) p . . . ~  .   .   .   .   .   .   .   .   . 
.  .   .   .   .   .   .   . S - - - - . - . - . ~ . ~- - . ~---- i . ~ . , ~9~d-r ~ c ~ . s . s~-s*,mo ,, ~ Co , ~ eJ , cs ) ~\[-- i ": .  -  . . . . . . . . .  , # :  . . . . . . . . i \[ Build - Plma ( U , S , TaI~?-Course(U , ( ~ S689)) I .   .   .   .   .   .   . : :"" ~ o " on'lna ~ tiat*-Single ~ Vat ~ l , S , _fae , Tca?bts~fae , CS689))' ,   . : V ~ po~d ~: ~__ Ao ~, " .   .   .   .   .   .   .   .   .   .  :  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . Goal :,: .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .  -,7::  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 
iIObtafin-hffo . Rcf(U , &_f~e , Teach? , (_fae , CS689))\]\[Ask-Rcf(U , S , _fac , Tcaehes(_fa? , CS6 89 ) ) \[ i \[ Mal~Q-Accq'tablc ~' s'Teae ~- fae ' c s689  ) ) I ? \[ Givc-B ack~r~u ? d ( U . S , Tcael~-fae , CS689))\["7\]Infono(U , S , want0J , Satls~-Scminar-Coua~U , CS )))\] IT ~ CO , S . wa*t(O,S~'Scr~*~C??~*?0J . CS )))\[ I~*f-R*q~a~J , sJ~'TCaev~-f~'cs689))\[?
Suffacc-Say-Prop(U , S , wa at fU . I
Satiffy . Seaninar-C~0J , CS ))) ISurfae ~ WH-QfU , S , _fac , Tcachcs(_fae , CS689 ) ) I I want to * atirfy my seminar cours ~ rttluir ~ rntnts  . Who's r~aching ( : $689? Figure 1: The Structure of the User's Utterances The evaluator checks whether the existing and proposed actions together constitute a wellformed plan  , one in which the children of each action contribute to their parent action  . Therefore , for each pair of actions , the evaluator checks against its recipe library to determine if their parent-child relationship holds  . The evaluator also checks whether each additional action is feasible by examining whether its applicability conditions are satisfied and its preconditions ~ can be satisfied  . 
We contend that wellformednes should be checked before feasibility since the feasibility of an action that does not contribute to its parent action is irrelevant  . Similarly , the wellformedness of a plan that attempts to achieve an infeasible goal is also irrelevant  . Therefore , we argue that the processes of checking wellformedness and feasibility should be interleaved in order to address the most general action that is inappropriate  . We show how this interleaved process works by referring back to figure  1  . 
Suppose the system believes that CS689 is not a seminar course . The evaluation process starts from Satisfy -Seminar-Course  ( U , CS ) , the topmost action in the proposed domain model . The system's knowledge indicates that Satisfy -Seminar-Course  ( U , CS ) contributes to Get-Masters ( U , CS ) . The system also believes that the applicability conditions and the preconditions for the Satisfy -Seminar-Course domain plan are satisfied  , indicating that the action is feasible . However , the sys-tem's recipe library gives no reason to believe that Take-Course  ( U , CS689) contributes to Satisfy-Seminar-Course(U , CS ) , since CS 689 is not a seminar course . The evaluator then decides that this pair of proposed actions would make the domain planill -formed  . 
4 When the Proposal is Erroneous
The goal selector's task is to determine , based on the current dialogue model , an intentional goal \[8\] that is most appropriate for the system to pursue  . An intentional goal could be to directly respond to the user's utterance  , a Both applicability conditions and preconditions are prerequisites for executing a recipe  . However , it is unreasonable to attempt to satisfy an applicability condition whereas preconditions can be planned for  . 
Action : Correct-Inference (..s 1,_2,_proposed)
Recipe-Type : Decomposition
Appl Cond : believe(_sl,~contributes(_actl, . . act2)) believe(_s2 , contributes (_ actl , _act 2)) Constraints : in-plan(_actl , _ proposed ) Vin-plan(_act 2 , _proposed ) Body : Modify-Acts (_s 1 , _s2 , _proposed , _actl , _act2) Insert-Correction ( . .s I ,_ 2,_proposed)
Effects : modified ( _proposed ) well-formed ( _proposed ) Action : Modify-Acts ( _sl , _s2 , _proposed , _actl , _act2)
Recipe-Type : Specialization
Appl Cond : believe(_s1 , - ~ contributes (_ actl , _act 2)) Preconditions : believe(_s2 , - , contributes(_actl , _act 2)) Body:Remove-Act(_sl , _s2 , _proposed , _actl )
Alter-Act(_sl,_s2,_proposed,-actl)
Effects : modified (_ proposed )
Goal : modified (_ proposed )
Figure 2: Two Problem-Solving Recipes to correct a user's misconception  , to provide a better alternative , etc . In this paper we only discuss the goal selector's task when the user has an erroneous plan/goal  . 
In a collaborative environment , if the system decides that the proposed model is infeasible/ill-formed  , it should refuse to accep the additions and suggest modifications to the proposal by entering a negotiation subdialogue  . For this purpose , we developed recipes for two problem-solving actions  , Correct-Goal and Correct-Inference , each a specialization of a Modify-Proposal action  . We illustrate the Correct-Inference action in more detail  . 
We show two problem-solving recipes , Correct-Inference and Modify-Acts , in figure 2 . The Correct-Inference recipe is applicable when _s2 believes that_act l contributes to achieving _act2  , while _ sl believes that such a relationship does not hold  . The goal is to make the resultant plan a wellformed plan  ; therefore , its body consists of an action Modify-Acts that deletes the problematic omponents of the plan  , and Insert-Correction , that inserts new actions/variables into the plan . One precondition in Modify-Acts is be-lieve (_s2 , - ~ contributes (_ actl , -act 2)) ( note that in Correct-Inference , _s2 believes contributes (- actl , -act2)) , and the change in _s2's belief can be accomplished by invoking the discourse level action In form so that_sl can convey the ill-formedness  to_s2  . This Informact may lead to further negotiation about whether_actl contributes to  _act2  . 
Only when_sl receives a positive feedback from _s2  , indicating that_s2 accepts_sl's belief , can_sl assume that the proposed actions can be modified  . 
Earlier discussion shows that the proposed actions in figure  1 would make the domain plan ill-formed . Therefore , the goal selector posts a goal to modify the proposal  , which causes the Correct-Inferencercipe in figure  2 to be selected . The variables_act l and _act2 are bound to Take-Course ( U , CS689) and Satisfy-Seminar-Course(U , CS ) , respectively , since the system believes that the former does not contribute to the latter  . 
Figure 3 shows how we envision the planner to expand on the Correct-Inferencercipe  , which results in the generation of the following two utterances :  ( 1 ) S " Taking CS689 does not contribute to satisfying the seminar course requirement  , (2) CS689 is not a seminar course . 

Dialogue Model in Figttre1~t
Problem-Solving Level J .   .   .   .   .   .   .   .   . E .   .   .   .   .   .   .   .   . J , - .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . -  .   .   .   .   .   .   .   .   .   .   . ,-  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 
, ~ Ce n ? rate-Respo nse(S , U , Proposed-Model ) J ' , \[Evaluale-Proposalf S , U , Pro\[n: , sed-Model IJ Modif ' - Proposal(S . U , Proposed-Model ) It ii ', IC or reot'lnfer ? ncc(S . U , Pr?P?sed'Model)I , ' Modify-Acts ( S , U , Proposcd-Mod?l , Take-Coul'se(U . CS6g9), f,S afsfv . s~ainar . Course(U . CS l ~ r ~', ~ f + ~- ~; q .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .  -- : .  :---~-  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 
In for ra(S , U , -infenm ce(Tak-?-Course(n , c $689) , I_S aris P/-Scminar-Coorse(U ~ Te\[~'~S . U . -infetence ( Take , Course(U , CS689) . Address-Believability(S , U ' , -( infefence ( L__ . __Satis ~- Seminar-Coutse(U , CS ))) IT a . k?-Cotn'sed UCS 689"~ Satis ~- Seminat-Course(U . CS ) )l VS ur face'Say'Pr ? P ( S'U " izff crenee ( IJlaform ( S , U . -isa(CS689, seminar-course ))\] Take-Cours~(U . CS689) .   .   .   . 
\[ Satisf C-Serainar-Course(U , CS 111\[T?II(S , U , - is a ( CS 689 , serain at . -cottrs ?)) Jf .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . js_~ . _~_r:~e~_s . y . ~=sc_s_+sg___~_ . ,_~_ . ~_o~ ) )___ Taking CS689 does not contribute to satisfying CS689 is not a seminar ? ours ? the seminar course requirement Figure  3: The Dialogue Model for the System's Response The action Inform  ( _sl , _s2 , _prop ) has the goal be-lieve (_s 2 , _prop ); therefore , utterance ( 1 ) is generated by executing the Informaction as an attempt to satisfy the preconditions for the Modify-Acts recipe  . Utterance ( 2 ) results from the Address-Believability action , which is a subaction of Inform , to suppor the claim in (1) . The problem-solving and discourse levels in figure  3 operate on the entire dialogue model shown in figure  1  , since the evaluation process acts upon this model  . Due to this nature , the evaluation process can be viewed as a meta -planning process  , and when the goal of this process is achieved , the modified dialogue model is returned to . 
Now consider the case in which the user continues by accepting utterances  ( 1 ) and ( 2 )  , which satisfies the precondition of Modify-Acts . Modify-Acts has two specializations , Remove-Act , which removes the incorrect action ( and all of its children )  , and Alter-Act , which generalizes the proposed action so that the plan will be wellformed  . 
Since Take-Course contributes to Satisfy-Seminar -Course as long as the course is a seminar course  , the system generalizes the user's proposed action by replacing  CS689 with a variable . This variable may be instantiated by the Insert -Correction subaction of Correct-Inference when the dialogue continues  . Note that our model accounts for why the user's original question about the instructor of  CS689 is never answered--a conflict was detected that made the question superfluous  . 
5 Related Work
Several researchers have studied collaboration \[1 ,  3 ,   10\] and Allen propose different plan modalities depending on whether a plan fragment is shared  , proposed and acknowledged , or merely private\[1\] . However , they have emphasize discourse analysis and none has provided a plan-based framework for proposal negotiation  , specifled appropriate system response during collaboration  , or accounted for why a question might never be answered  . 
Litman and Allen used discourse meta-plans to handle a class of correction subdialogues  \[7\]  . However , their Correct-Plan only addressed cases in which an agent adds a repair step to a preexisting plan that does not execute as expected  . Thus their meta-plans do not handle correction of proposed additions to the dialogue model  ( since this generally does not involve adding a step to the proposal  )  . 
Furthermore , they were only concerned with understanding utterances  , not with generating appropriater sponses . 
The work in \[5 , 1I ,   9\] addressed generating cooperative responses and responding to plan-based misconceptions  , but did not capture these within an overall collaborative system that must negotiate proposals with the user  . Heeman \[4\] used meta-plans to account for collaboration on referring expressions  . We have addressed collaboration i constructing the user's task-related plan  , captured cooperative responses and negotiation of how the plan should be constructed  , and provided an accounting for why a user's question may never be answered  . 
6 Confusions and Future Work
We have presented a plan-based framework for generating responses in a collaborative environment  . Our framework improves upon previous ones in that  ,  1 ) it captures cooperative responses as a part of collaboration  ,  2 ) it is capable of initiating negotiation subdialogues to determine what actions should be added to the shared plan  , 3) the correction process , instead of merely pointing out problematic plans /goals to the user  , modifies the plan into its most specific form accepted by both participants  , and 4 ) the evaluation/correction process operates at a metalevel which keeps the negotiation subdialogue separate from the original dialogue model  , while allowing the same plan-inference mechanism to be used at both levels  . 
We intend to enhance our evaluator so that it also recognize sub-optimal solutions and can suggest better alternatives  . We will also study the goal selector's task when the user's plan/goal is well-formed/feasible  . 
This includes identifying a set of intentional goals and a strategy for the goal selector to choose amongst them  . 
Furthermore , we need to develop the intentional planner which constructs a plan to achieve the posted goal  , and a discourse realizer to generate natural anguage text  . 
References\[1\]James Allen . Discourse structure in the TRAINS project . 
In Darpa Speech and Natural Language Workshop ,  1991 . 
\[2\] Rhonda Eller and Sandra Carberry . A metarule approach to flexible plan recognition in dialogue  . User Modeling and User-Adapted lnteraction ,  2:27--53 ,  1992 . 
\[3\] Barbara Grosz and Candace Sidner . Plans for discourse . In Cohen et al , editor , Intentions in Communication , pages 417--444 .  1990 . 
\[4\] Peter Heeman . A computational model of collaboration on referring expressions  . Master's thesis , University of
Toronto , 1991.
\[5\]A ravind Joshi , Bonnie Webber , and Ralph Weischedel . 
Living up to expectations : Computing expert responses  . In
Proc . AAAL pages 169--175, 1984.
\[6\]Lynn Lambert and Sandra Carberry . A tripartite plan-based model of dialogue . In Proc . ACL , pages 47--54, 1991 . 
\[7\] Diane Litman and James Allen . A plan recognition model for subdialogues in conversation  . Cognitive Science , 11:163--200, 1987 . 
\[8\] Johanna Moore and Cecile Paris . Planning text for advisory dialogues . In Proc . ACL , pages 203--211, 1989 . 
\[9\] Mart . ha Pollack . A model of plan inference that distinguishes between the beliefs of actors and observers  . In
Proc . ACL , pages 207--214, 1986.
\[10\]Candace Sidner . Using discourse to negotiate in collaborative activity : An artificial anguage  . In Workshop Notes: AAAI-92 Cooperation Among Heterogeneous Intelligent
Systems , pages 121--128, 1992.
\[11\] Peter van Beek . A model for generating better explanations . 
In Proc . ACL , pages 215--220, 1987.

