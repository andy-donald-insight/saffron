Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics , pages 1385?1394,
Portland , Oregon , June 1924, 2011. c?2011 Association for Computational Linguistics
A Stacked Sub-Word Model for Joint Chinese Word Segmentation and
Part-of-Speech Tagging
Weiwei Sun
Department of Computational Linguistics , Saarland University
German Research Center for Artificial Intelligence ( DFKI)
D-66123, Saarbru?cken , Germany
wsun@coli.uni-saarland.de
Abstract
The large combined search space of joint word segmentation and Part-of-Speech ( POS ) tagging makes efficient decoding very hard . As a result , effective high order features representing rich contexts are inconvenient to use . In this work , we propose a novel stacked subword model for this task , concerning both efficiency and effectiveness . Our solution is a two step process . First , one word-based segmenter , one character-based segmenter and one local character classifier are trained to produce coarse segmentation and POS information . Second , the outputs of the three predictors are merged into subword sequences , which are further bracketed and labeled with POS tags by a finegrained subword tagger . The coarse-to-fine search scheme is efficient , while in the subword tagging step rich contextual features can be approximately derived . Evaluation on the Penn Chinese Treebank shows that our model yields improvements over the best system reported in the literature.
1 Introduction
Word segmentation and part-of-speech ( POS ) tagging are necessary initial steps for more advanced Chinese language processing tasks , such as parsing and semantic role labeling . Joint approaches that resolve the two tasks simultaneously have received much attention in recent research . Previous work has shown that joint solutions led to accuracy improvements over pipelined systems by avoiding segmentation error propagation and exploiting POS information to help segmentation . A challenge for joint approaches is the large combined search space , which makes efficient decoding and structured learning of parameters very hard . Moreover , the representation ability of models is limited since using rich contextual word features makes the search intractable . To overcome such efficiency and effectiveness limitations , the approximate inference and reranking techniques have been explored in previous work ( Zhang and Clark , 2010; Jiang et al , 2008b).
In this paper , we present an effective and efficient solution for joint Chinese word segmentation and POS tagging . Our work is motivated by several characteristics of this problem . First of all , a majority of words are easy to identify in the segmentation problem . For example , a simple maximum matching segmenter can achieve an fscore of about 90.
We will show that it is possible to improve the efficiency and accuracy by using different strategies for different words . Second , segmenters designed with different views have complementary strength.
We argue that the agreements and disagreements of different solvers can be used to construct an intermediate subword structure for joint segmentation and tagging . Since the subwords are large enough in practice , the decoding for POS tagging over subwords is efficient . Finally , the Chinese language is characterized by the lack of morphology that often provides important clues for POS tagging , and the POS tags contain much syntactic information , which need context information within a large window for disambiguation . For example , Huang et al (2007) showed the effectiveness of utilizing syntactic information to rerank POS tagging results . As a result , the capability to represent rich contextual features is crucial to a POS tagger . In this work , we use a representation-efficiency tradeoff through stacked learning , a way of approximating rich nonlocal fea-This paper describes a novel stacked subword model . Given multiple word segmentations of one sentence , we formally define a subword structure that maximizes the agreement of non-word-break positions . Based on the subword structure , joint word segmentation and POS tagging is addressed as a two step process . In the first step , one word-based segmenter , one character-based segmenter and one local character classifier are used to produce coarse segmentation and POS information . The results of the three predictors are then merged into subword sequences , which are further bracketed and labeled with POS tags by a finegrained subword tagger . If a string is consistently segmented as a word by the three segmenters , it will be a correct word prediction with a very high probability . In the subword tagging phase , the finegrained tagger mainly considers its POS tag prediction problem . For the words that are not consistently predicted , the finegrained tagger will also consider their bracketing problem . The coarse-to-fine scheme significantly improves the efficiency of decoding . Furthermore , in the subword tagging step , word features in a large window can be approximately derived from the coarse segmentation and tagging results . To train a good subword tagger , we use the stacked learning technique , which can effectively correct the training/test mismatch problem.
We conduct our experiments on the Penn Chinese Treebank and compare our system with the state-of-the-art systems . We present encouraging results.
Our system achieves an fscore of 98.17 for the word segmentation task and an fscore of 94.02 for the whole task , resulting in relative error reductions of 14.1% and 5.5% respectively over the best system reported in the literature.
The remaining part of the paper is organized as follows . Section 2 gives a brief introduction to the problem and reviews the relevant previous research.
Section 3 describes the details of our method . Section 4 presents experimental results and empirical analyses . Section 5 concludes the paper.
2 Background 2.1 Problem Definition
Given a sequence of characters c = ( c1, ..., c#c ), the task of word segmentation and POS tagging is to predict a sequence of word and POS tag pairs y = (? w1, p1?, ? w#y , p#y ?), where wi is a word , pi is its POS tag , and a ?#? symbol denotes the number of elements in each variable . In order to avoid error propagation and make use of POS information for word segmentation , the two tasks should resolved jointly . Previous research has shown that the integrated methods outperformed pipelined systems ( Ng and Low , 2004; Jiang et al , 2008a ; Zhang and Clark , 2008).
2.2 Character-Based and Word-Based
Methods
Two kinds of approaches are popular for joint word segmentation and POS tagging . The first is the ? character-based ? approach , where basic processing units are characters which compose words . In this kind of approach , the task is formulated as the classification of characters into POS tags with boundary information . Both the IOB2 representation ( Ramshaw and Marcus , 1995) and the Start/End representation ( Kudo and Matsumoto , 2001) are popular . For example , the label BNN indicates that a character is located at the begging of a noun . Using this method , POS information is allowed to interact with segmentation . Note that word segmentation can also be formulated as a sequential classification problem to predict whether a character is located at the beginning of , inside or at the end of a word . This character-by-character method for segmentation was first proposed in ( Xue , 2003), and was then further used in POS tagging in ( Ng and Low , 2004). One main disadvantage of this model is the difficulty in incorporating the whole word information.
The second kind of solution is the ? word-based ? method , where the basic predicting units are words themselves . This kind of solver sequentially decides whether the local sequence of characters makes up a word as well as its possible POS tag . In particular , a word-based solver reads the input sentence from left to right , predicts whether the current piece of continuous characters is a word token and which class it belongs to . Solvers may use previously predicted words and their POS information as clues to find a new word . After one word is found and classified , solvers move on and search for the next possible word . This word-by-word method for segmentation was first proposed in ( Zhang and Clark , 2007), and Clark , 2008).
In our previous work(Sun , 2010), we presented a theoretical and empirical comparative analysis of character-based and word-based methods for Chinese word segmentation . We showed that the two methods produced different distributions of segmentation errors in a way that could be explained by theoretical properties of the two models . A system combination method that leverages the complementary strength of word-based and character-based segmentation models was also successfully explored in their work . Different from our previous focus , the diversity of different models designed with different views is utilized to construct subword structures in this work . We will discuss the details in the next section.
2.3 Stacked Learning
Stacked generalization is a metalearning algorithm that was first proposed in ( Wolpert , 1992) and ( Breiman , 1996). The idea is to include two ? levels ? of predictors . The first level includes one or more predictors g1, ... gK : Rd ? R ; each receives input x ? Rd and outputs a prediction gk(x ). The second level consists of a single function h : Rd+K ? R that takes as input ? x , g1(x ), ..., gK(x )? and outputs a final prediction y ? = h(x , g1(x ), ..., gK(x)).
Training is done as follows . The training data S = {( xt,yt ) : t ? [1, T ]} is split into L equal-sized disjoint subsets S1, ..., SL . Then functions g1, ..., gL ( where gl = ? gl1, ..., g l
K ?) are seperately trained on
S ? Sl , and are used to construct the augmented dataset S ? = {(? xt , y?1t , ..., y?
K t ?, yt ) : y ? k t = g l k(xt ) and xt ? Sl }. Finally , each gk is trained on the original dataset and the second level predictor h is trained on S ?. The intent of the crossvalidation scheme is that ykt is similar to the prediction produced by a predictor which is learned on a sample that does not include xt.
Stacked learning has been applied as a system ensemble method in several NLP tasks , such as named entity recognition ( Wu et al , 2003) and dependency parsing ( Nivre and McDonald , 2008). This framework is also explored as a solution for learning nonlocal features in ( Torres Martins et al , 2008). In the machine learning research , stacked learning has been applied to structured prediction ( Cohen and Carvalho , 2005). In this work , stacked learning is used to acquire extended training data for subword tagging.
3 Method 3.1 Architecture
In our stacked subword model , joint word segmentation and POS tagging is decomposed into two steps : (1) coarse-grained word segmentation and tagging , and (2) finegrained subword tagging . The workflow is shown in Figure 1. In the first phase , one word-based segmenter ( SegW ) and one character-based segmenter ( SegC ) are trained to produce word boundaries . Additionally , a local character-based joint segmentation and tagging solver ( SegTagL ) is used to provide word boundaries as well as inaccurate POS information . Here , the word local means the labels of nearby characters are not used as features . In other words , the local character classifier assumes that the tags of characters are independent of each other . In the second phase , our system first combines the three segmentation and tagging results to get subwords which maximize the agreement about word boundaries . Finally , a finegrained subword tagger ( SubTag ) is applied to bracket subwords into words and also to obtain their POS tags.
Raw sentences
Character-based segmenter SegC
Local character classifier
SegTagL
Word-based
Segmenter SegW
Segmented sentences
Segmented sentences
Segmented sentences
Merging
Subword sequences
Subword tagger SubTag
Figure 1: Workflow of the stacked subword model.
In our model , segmentation and POS tagging interact with each other in two processes . First , although SegTagL is locally trained , it resolves the word generating stage , segmentation and POS tagging help each other . Second , in the subword tagging stage , the bracketing and the classification of subwords are jointly resolved as one sequence labeling problem.
Our experiments on the Penn Chinese Treebank will show that the word-based and character-based segmenters and the local tagger on their own produce high quality word boundaries . As a result , the oracle performance to recover words from a subword sequence is very high . The quality of the final tagger relies on the quality of the subword tagger . If a high performance subword tagger can be constructed , the whole task can be well resolved.
The statistics will also empirically show that subwords are significantly larger than characters and only slightly smaller than words . As a result , the search space of the subword tagging is significantly shrunken , and exact Viterbi decoding without approximately pruning can be efficiently processed.
This property makes nearly all popular sequence labeling algorithms applicable.
Zhang et al (2006) described a subword based tagging model to resolve word segmentation . To get the pieces which are larger than characters but smaller than words , they combine a character-based segmenter and a dictionary matching segmenter.
Our contributions include (1) providing a formal definition of our subword structure that is based on multiple segmentations and (2) proposing a stacking method to acquire subwords.
3.2 The Coarse-grained Solvers
We systematically described the implementation of two state-of-the-art Chinese word segmenters in word-based and character-based architectures , respectively ( Sun , 2010). Our word-based segmenter is based on a discriminative joint model with a first order semi-Markov structure , and the other segmenter is based on a first order Markov model . Exact Viterbi-style search algorithms are used for decoding . Limited to the document length , we do not give the description of the features . We refer readers to read the above paper for details . For parameter estimation , our work adopt the Passive-Aggressive ( PA ) framework ( Crammer et al , 2006), a family of margin based online learning algorithms . In this work , we introduce two simple but important refinements : (1) to shuffle the sample orders in each iteration and (2) to average the parameters in each iteration as the final parameters.
Idiom In linguistics , idioms are usually presumed to be figures of speech contradicting the principle of compositionality . As a result , it is very hard to recognize out-of-vocabulary idioms for word segmentation . However , the lexicon of idioms can be taken as a close set , which helps resolve the problem well . We collect 12992 idioms1 from several online Chinese dictionaries . For both word-based and character-based segmentation , we first match every string of a given sentence with idioms . Every sentence is then splitted into smaller pieces which are seperated by idioms . Statistical segmentation models are then performed on these smaller character sequences.
We use a local classifier to predict the POS tag with positional information for each character.
Each character can be assigned one of two possible boundary tags : ? B ? for a character that begins a word and ? I ? for a character that occurs in the middle of a word . We denote a candidate character token ci with a fixed window ci?2ci?1cici+1ci+2. The following features are used : ? character unigrams : ck ( i ? 2 ? k ? i + 2) ? character bigrams : ckck+1 ( i ? 2 ? k ? i+1) To resolve the classification problem , we use the linear SVM classifier LIBLINEAR2.
3.3 Merging Multiple Segmentation Results into Sub-Word Sequences A majority of words are easy to identify in the segmentation problem . We favor the idea treating different words using different strategies . In this work we try to identify simple and difficult words first and to integrate them into a subword level . Inspired by previous work , we constructed this subword structure by using multiple solvers designed from different views . If a piece of continuous characters is consistently segmented by multiple segmenters , it will 1This resource is publicly available at http://www.
coli.uni-saarland.de/?wsun/idioms.txt.
2Available at http://www.csie.ntu.edu.tw / ? cjlin/liblinear/.
1388 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
Answer : [ P ] [ JJ ] [ NN ] [ CD ] [ M ] [ VV ] [ JJ ] [ NN ]
SegW : [] [] [ ] [ ] [ ] [ ] [ ] [ ]
SegC : [] [] [ ] [ ] [] [ ] [ ]
SegTagL : [ P ] [ JJ ] [ NN ] [ CD ] [ NT ] [ CD ] [ NT ] [ VV ] [ VV ] [ NN ] Subwords : [ P ] [ JJ ] [ NN ] [ BCD ] [ ICD ] [ NT ] [ CD ] [ NT ] [ VV ] [ VV ] [ NN ] Figure 2: An example phrase : ???????????????? ( Being in front with a total score of 355.35 points).
not be separated in the subword tagging step . The intuition is that strings which are consistently segmented by the different segmenters tend to be correct predictions . In our experiment on the Penn Chinese Treebank ( Xue et al , 2005), the accuracy is 98.59% on the development data which is defined in the next section . The key point for the intermediate subword structures is to maximize the agreement of the three coarse-grained systems . In other words , the goal is to make merged subwords as large as possible but not overlap with any predicted word produced by the three coarse-grained solvers.
In particular , if the position between two continuous characters is predicted as a word boundary by any segmenter , this position is taken as a separation position of the subword sequence . This strategy makes sure that it is still possible to resegment the strings of which the boundaries are disagreed with by the coarse-grained segmenters in the finegrained tagging stage.
The formal definition is as follows . Given a sequence of characters c = ( c1, ..., c#c ), let c[i : j ] denote a string that is made up of characters between ci and cj ( including ci and cj ), then a partition of the sentence can be written as c[0 : e1], c[e1 + 1 : e2], ..., c[em : # c ]. Let sk = { c[i : j ]} denote the set of all segments of a partition . Given multiple partitions of a character sequence S = { sk }, there is one and only one merged partition sS = { c[i : j ]} s.t.
1. ? c[i : j ] ? sS ,? sk ? S , ? c[s : e ] ? sk , s ? i ? j ? e.
2. ? C ? satisfies the above condition , | C ?| > | C|.
The first condition makes sure that all segments in the merged partition can be only embedded in but do not overlap with any segment of any partition from S . The second condition promises that segments of the merged partition achieve maximum length.
Figure 2 is an example to illustrate the procedure of our method . The lines SegW , SegC and SegTagL are the predictions of the three coarse-grained solvers . For the three words at the beginning and the two words at the end , the three predictors agree with each other . And these five words are kept as subwords . For the character sequence ?? ????????, the predictions are very different . Because there are no word break predictions among the first three characters ?????, it is as a whole taken as one subword . For the other five characters , either the left position or the right position is segmented as a word break by some predictor , so the merging processor seperates them and takes each one as a single subword . The last line shows the merged subword sequence . The coarse-grained POS tags with positional information are derived from the labels provided by SegTagL.
3.4 The Fine-grained Sub-Word Tagger
Bracketing subwords into words is formulated as a IOB-style sequential classification problem . Each subword may be assigned with one POS tag as well as two possible boundary tags : ? B ? for the beginning position and ? I ? for the middle position . A tagger is trained to classify subword by using the features derived from its contexts.
The subword level allows our system to utilize features in a large context , which is very important for POS tagging of the morphologically poor language . Features are formed making use of subword contents , their IOB-style inaccurate POS tags . In the following description , ? C ? refers to the content of the subword , while ? T ? refers to the IOB-style POS tags . For convenience , we denote a subword with its context ... si?2si?1sisi+1si+2..., where si is
C(si )=?????; T(si)=?B-CD?
C(si+1)=???; T(si+1)=?I-CD?
C(si?1)C(si )=??? ????
T(si?1)T(si)=?NN BCD?
C(si)C(si+1)=???? ??
T(si)T(si+1)=?B-CD ICD?
C(si?1)C(si+1)=??? ??
T(si?1)T(si+1)=?B-NN ICD?
Prefix(1)=???; Prefix(2)=????; Prefix(3)=????? Suffix(1)=???; Suffix(2)=????; Suffix(3)=????? Table 1: An example of features used in the subword tagging.
the current token . We denote lC , lT as the sizes of the window.
? Unigram features : C(sk ) (? lC ? k ? lC),
T(sk ) (? lT ? k ? lT ) ? Bigram features : C(sk)C(sk+1) (? lC ? k ? lC ? 1), T(sk)T(sk+1) (? lT ? k ? lT ? 1) ? C(si?1)C(si+1) ( if lC ? 1), T(si?1)T(si+1) ( if lT ? 1) ? T(si?2)T(si+1) ( if lT ? 2) ? In order to better handle unknown words , we also extract morphological features : character ngram prefixes and suffixes for n up to 3.
These features have been shown useful in previous research ( Huang et al , 2007).
Take the subword ????? in Figure 2 for example , when lC and lT are both set to 1, all features used are listed in Table 1.
In the following experiments , we will vary window sizes lC and lT to find out the contribution of context information for the disambiguation . A first order Max-Margin Markov Networks model is used to resolve the sequence tagging problem . We use the SVM-HMM3 implementation for the experiments in this work . We use the basic linear model without applying any kernel function.
3Available at http://www.cs.cornell.edu/
People/tj/svm_light/svm_hmm.html.
Algorithm 1: The stacked learning procedure for the subword tagger.
input : Data S = {( ct,yt ), t = 1, 2, ..., n}
Split S into L partitions { S1, ... SL } for l = 1, ..., L do
Train SegWl , SegCl and SegTagLl using
S ? Sl.
Predict Sl using SegW l , SegCl and
SegTagLl.
Merge the predictions to get subwords training sample S?l .
end
Train the subword tagger SubTag using S?.
3.5 Stacked Learning for the Sub-Word Tagger The three coarse-grained solvers SegW , SegC and SegTagL are directly trained on the original training data . When these three predictors are used to produce the training data , the performance is perfect . However , this does not hold when these models are applied to the test data . If we directly apply SegW , SegC and SegTagL to extend the training data to generate subword samples , the extended training data for the subword tagger will be very different from the data in the run time , resulting in poor performance.
One way to correct the training/test mismatch is to use the stacking method , where a K-fold crossvalidation on the original data is performed to construct the training data for subword tagging . Algorithm 1 illustrates the learning procedure . First , the training data S = {( ct,yt )} is split into L equal-sized disjoint subsets S1, ..., SL . For each subset Sl , the complementary set S ? Sl is used to train three coarse solvers SegWl , SegCl and SegTagLl , which process the Sl and provide inaccurate predictions.
Then the inaccurate predictions are merged into subword sequences and Sl is extended to S?l . Finally , the subword tagger is trained on the whole extended data set S?.
4 Experiments 4.1 Setting
Previous studies on joint Chinese word segmentation and POS tagging have used the Penn Chinese Treebank ( CTB ) in experiments . We follow this set-corpus and define the training , development and test sets according to ( Jiang et al , 2008a ; Jiang et al , 2008b ; Kruengkrai et al , 2009; Zhang and Clark , 2010). Table 2 shows the statistics of our experimental settings.
Data set CTB files # of sent . # of words
Training 1270 18,089 493,939 400-931 1001-1151
Devel . 301-325 350 6821
Test 271-300 348 8008
Table 2: Training , development and test data on CTB 5.0 Three metrics are used for evaluation : precision ( P ), recall ( R ) and balanced fscore ( F ) defined by 2PR/(P+R ). Precision is the relative amount of correct words in the system output . Recall is the relative amount of correct words compared to the gold standard annotations . For segmentation , a token is considered to be correct if its boundaries match the boundaries of a word in the gold standard . For the whole task , both the boundaries and the POS tag have to be correctly identified.
4.2 Performance of the Coarse-grained Solvers Table 3 shows the performance on the development data set of the three coarse-grained solvers . In this paper , we use 20 iterations to train SegW and SegC for all experiments . Even only locally trained , the character classifier SegTagL still significantly outperforms the two state-of-the-art segmenters SegW and SegC . This good performance indicates that the POS information is very important for word segmentation.
Devel . Task P (%) R (%) F
SegW Seg 94.55 94.84 94.69
SegC Seg 95.10 94.38 94.73
SegTagL Seg 95.67 95.98 95.83
Seg&Tag 87.54 91.29 89.38
Table 3: Performance of the coarse-grained solvers on the development data.
4.3 Statistics of Sub-Words
Since the base predictors to generate coarse information are two word segmenters and a local character classifier , the coarse decoding is efficient . If the length of subwords is too short , i.e . the decoding path for subword sequences are too long , the decoding of the finegrained stage is still hard . Although we cannot give a theoretical average length of subwords , we can still show the empirical one . The average length of subwords on the development set is 1.64, while the average length of words is 1.69. The number of all IOB-style POS tags is 59 ( when using 5-fold crossvalidation to generate stacked training samples ). The number of all POS tags is 35. Empirically , the decoding over subwords is 1.691.64?( n+1 times as slow as the decoding over words , where n is the order of the markov model . When a first order markov model is used , this number is 2.93. These statistics empirically suggest that the decoding over subword sequence can be efficient.
On the other hand , the subword sequences are not perfect in the sense that they do not promise to recover all words because of the errors made in the first step . Similarly , we can only show the empirical upper bound of the subword tagging . The oracle performance of the final POS tagging on the development data set is shown in Table 4. The upper bound indicates that the coarse search procedure does not lose too much.
Task P (%) R (%) F
Seg&Tag 99.50% 99.09% 99.29
Table 4: Upper bound of the subword tagging on the development data.
One main disadvantage of character-based approach is the difficulty to incorporate word features.
Since the subwords are on average close to words , subword features are good approximations of word features.
4.4 Rich Contextual Features Are Useful
Table 5 shows the effect that features within different window size has on the subword tagging task.
In this table , the symbol ? C ? means subword content features while the symbol ? T ? means IOB-style POS tag features . The number indicates the length
C:?0 T:?0 92.52 92.83 92.67
C:?1 T:?0 92.63 93.27 92.95
C:?1 T:?1 92.62 93.05 92.83
C:?2 T:?0 93.17 93.86 93.51
C:?2 T:?1 93.27 93.64 93.45
C:?2 T:?2 93.08 93.61 93.34
C:?3 T:?0 93.12 93.86 93.49
C:?3 T:?1 93.34 93.96 93.65
C:?3 T:?2 93.34 93.96 93.65
Table 5: Performance of the stacked subword model ( K = 5) with features in different window sizes.
of the window . For example , ? C:?1? means that the tagger uses one preceding subword and one succeeding subword as features . From this table , we can clearly see the impact of features derived from neighboring subwords . There is a significant increase between ? C:?2? and ? C:?1? models . This confirms our motivation that longer history and future features are crucial to the Chinese POS tagging problem . It is the main advantage of our model that making rich contextual features applicable . In all previous solutions , only features within a short history can be used due to the efficiency limitation.
The performance is further slightly improved when the window size is increased to 3. Using the labeled bracketing fscore , the evaluation shows that the ? C:?3 T:?1? model performs the same as the ? C:?3 T:?2? model . However , the subword classification accuracy of the ? C:?3 T:?1? model is higher , so in the following experiments and the final results reported on the test data set , we choose this setting.
This table also suggests that the IOB-style POS information of subwords does not contribute . We think there are two main reasons : (1) The POS information provided by the local classifier is inaccurate ; (2) The structured learning of the subword tagger can use real predicted subword labels during its decoding time , since this learning algorithm does inference during the training time . It is still an open question whether more accurate POS information in rich contexts can help this task . If the answer is YES , how can we efficiently incorporate these features ? 4.5 Stacked Learning Is Useful Table 6 compares the performance of ? C:?3 T:?1? models trained with no stacking as well as different folds of crossvalidation . We can see that although it is still possible to improve the segmentation and POS tagging performance compared to the local character classifier , the whole task just benefits only a little from the subword tagging procedure if the stacking technique is not applied . The stacking technique can significantly improve the system performance , both for segmentation and POS tagging.
This experiment confirms the theoretical motivation of using stacked learning : simulating the test-time setting when a subword tagger is applied to a new instance . There is not much difference between the 5-fold and the 10-fold crossvalidation.
Devel . Task P (%) R (%) F
No stacking Seg 95.75 96.48 96.12
Seg&Tag 91.42 92.13 91.77
K = 5 Seg 96.42 97.04 96.73
Seg&Tag 93.34 93.96 93.65
K = 10 Seg 96.67 97.11 96.89
Seg&Tag 93.50 94.06 93.78
Table 6: Performance on the development data . No stacking and different folds of crossvalidation are separately applied.
4.6 Final Results
Table 7 summarizes the performance of our final system on the test data and other systems reported in a majority of previous work . The final results of our system are achieved by using 10-fold crossvalidation ? C:?3 T:?1? models . The left most column indicates the reference of previous systems that represent state-of-the-art results . The comparison of the accuracy between our stacked subword system and the state-of-the-art systems in the literature indicates that our method is competitive with the best systems . Our system obtains the highest fscore performance on both segmentation and the whole task , resulting in error reductions of 14.1% and 5.5% respectively.
1392
Test Seg Seg&Tag ( Jiang et al , 2008a ) 97.85 93.41 ( Jiang et al , 2008b ) 97.74 93.37 ( Kruengkrai et al , 2009) 97.87 93.67 ( Zhang and Clark , 2010) 97.78 93.67
Our system 98.17 94.02
Table 7: Fscore performance on the test data.
5 Conclusion and Future Work
This paper has described a stacked subword model for joint Chinese word segmentation and POS tagging . We defined a subword structure which maximizes the agreement of multiple segmentations provided by different segmenters . We showed that this subword structure could explore the complementary strength of different systems designed with different views . Moreover , the POS tagging could be efficiently and effectively resolved over subword sequences . To train a good subword tagger , we introduced a stacked learning procedure . Experiments showed that our approach was superior to the existing approaches reported in the literature.
Machine learning and statistical approaches encounter difficulties when the input/output data have a structured and relational form . Research in empirical Natural Language Processing has been tackling these complexities since the early work in the field . Recent work in machine learning has provided several paradigms to globally represent and process such data : linear models for structured prediction , graphical models , constrained conditional models , and reranking , among others . A general expressivity-efficiency trade off is observed . Although the stacked subword model is an ad hoc solution for a particular problem , namely joint word segmentation and POS tagging , the idea to employ system ensemble and stacked learning in general provides an alternative for structured problems.
Multiple ? cheap ? coarse systems are used to provide diverse outputs , which may be inaccurate . These outputs are further merged into an intermediate representation , which allows an extractive system to use rich contexts to predict the final results . A natural avenue for future work is the extension of our method to other NLP tasks.
Acknowledgments
The work is supported by the project TAKE ( Technologies for Advanced Knowledge Extraction ), funded under contract 01IW08003 by the German Federal Ministry of Education and Research . The author is also funded by German Academic Exchange Service ( DAAD).
The author would would like to thank Dr . Jia Xu for her helpful discussion , and Regine Bader for proofreading this paper.
References
Leo Breiman . 1996. Stacked regressions . Mach . Learn ., 24:49?64, July.
William W . Cohen and Vitor R . Carvalho . 2005. Stacked sequential learning . In Proceedings of the 19th international joint conference on Artificial intelligence , pages 671?676, San Francisco , CA , USA . Morgan
Kaufmann Publishers Inc.
Koby Crammer , Ofer Dekel , Joseph Keshet , Shai ShalevShwartz , and Yoram Singer . 2006. Online passive-aggressive algorithms . JOURNAL OF MACHINE
LEARNING RESEARCH , 7:551?585.
Zhongqiang Huang , Mary Harper , and Wen Wang.
2007. Mandarin part-of-speech tagging and discriminative reranking . In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ( EMNLPCoNLL ), pages 1093?1102, Prague , Czech Republic , June . Association for Computational Linguistics.
Wenbin Jiang , Liang Huang , Qun Liu , and Yajuan Lu?.
2008a . A cascaded linear model for joint Chinese word segmentation and part-of-speech tagging . In Proceedings of ACL08: HLT , pages 897?904, Columbus , Ohio , June . Association for Computational Linguistics.
Wenbin Jiang , Haitao Mi , and Qun Liu . 2008b . Word lattice reranking for Chinese word segmentation and part-of-speech tagging . In Proceedings of the 22nd International Conference on Computational Linguistics ( Coling 2008), pages 385?392, Manchester , UK , August . Coling 2008 Organizing Committee.
Canasai Kruengkrai , Kiyotaka Uchimoto , Jun?ichi Kazama , Yiou Wang , Kentaro Torisawa , and Hitoshi Isahara . 2009. An error-driven word-character hybrid model for joint Chinese word segmentation and pos tagging . In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Process-August . Association for Computational Linguistics.
Taku Kudo and Yuji Matsumoto . 2001. Chunking with support vector machines . In NAACL ?01: Second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies 2001, pages 1?8, Morristown , NJ , USA . Association for Computational Linguistics.
Hwee Tou Ng and Jin Kiat Low . 2004. Chinese part-of-speech tagging : One-at-a-time or all-at-once ? word-based or character-based ? In Dekang Lin and Dekai Wu , editors , Proceedings of EMNLP 2004, pages 277? 284, Barcelona , Spain , July . Association for Computational Linguistics.
Joakim Nivre and Ryan McDonald . 2008. Integrating graphbased and transition-based dependency parsers.
In Proceedings of ACL08: HLT , pages 950?958, Columbus , Ohio , June . Association for Computational
Linguistics.
L . A . Ramshaw and M . P . Marcus . 1995. Text chunking using transformationbased learning . In Proceedings of the 3rd ACL/SIGDAT Workshop on Very Large Corpora , Cambridge , Massachusetts , USA , pages 82?94.
Weiwei Sun . 2010. Word-based and character-based word segmentation models : Comparison and combination . In Coling 2010: Posters , pages 1211?1219, Beijing , China , August . Coling 2010 Organizing Committee.
Andre ? Filipe Torres Martins , Dipanjan Das , Noah A.
Smith , and Eric P . Xing . 2008. Stacking dependency parsers . In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing , pages 157?166, Honolulu , Hawaii , October . Association for Computational Linguistics.
David H . Wolpert . 1992. Original contribution : Stacked generalization . Neural Netw ., 5:241?259, February.
Dekai Wu , Grace Ngai , and Marine Carpuat . 2003. A stacked , voted , stacked model for named entity recognition . In Walter Daelemans and Miles Osborne , editors , Proceedings of the Seventh Conference on Natural Language Learning at HLTNAACL 2003, pages 200?203.
Nianwen Xue , Fei Xia , Fu-Dong Chiou , and Martha Palmer . 2005. The penn chinese treebank : Phrase structure annotation of a large corpus . Natural Language Engineering , 11(2):207?238.
Nianwen Xue . 2003. Chinese word segmentation as character tagging . In International Journal of Computational Linguistics and Chinese Language Processing.
Yue Zhang and Stephen Clark . 2007. Chinese segmentation with a word-based perceptron algorithm . In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics , pages 840?847, Prague , Czech Republic , June . Association for Computational
Linguistics.
Yue Zhang and Stephen Clark . 2008. Joint word segmentation and POS tagging using a single perceptron . In Proceedings of ACL08: HLT , pages 888?896, Columbus , Ohio , June . Association for Computational Linguistics.
Yue Zhang and Stephen Clark . 2010. A fast decoder for joint word segmentation and POStagging using a single discriminative model . In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing , pages 843?852, Cambridge , MA , October . Association for Computational Linguistics.
Ruiqiang Zhang , Genichiro Kikui , and Eiichiro Sumita.
2006. Subword-based tagging by conditional random fields for Chinese word segmentation . In Proceedings of the Human Language Technology Conference of the NAACL , Companion Volume : Short Papers , pages 193?196, New York City , USA , June . Association for
Computational Linguistics.
1394
