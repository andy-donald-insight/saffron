Structural Disambiguation Based on Reliable
Estimation of Strength of Association
Haodong Wu Eduar do de Paiva Alves
Teiji Furugori
Department of Computer Science
University of Electro-Communications
1-5-1 , Chofugaoka , Chofu , Tokyo 1828 585 , JAPA Nwu , ealves , furugori@phaeton . cs . uec . ac . jp
Abstract
This paper proposes a new classbased method to estimate the strength of association in word cooccurrence for the purpose of structural disambiguation  . To deal with sparseness of data , we use a conceptual dictionary as the source for acquiring upper classes of the words related in the cooccurrence  , and then uset-scores to determine a pair of classes to be employed for calculating the strength of association  . We have applied our method to determining dependency relations in Japanese and prepositional phrase attachments in English  . The experimental results show that the method is sound  , effective and useful in resolving structural ambiguities  . 
1 Introduction
The strength of association between words provides lexical preferences for ambiguity resolution  . It is usually estimated from statistics on word cooccurrences in large corpora  ( Hindle and Rooth ,  1993) . A problem with this approach is how to estimate the probability of word cooccurrences that are not observed in the training corpus  . There are two main approaches to estimate the probability : smoothing methods  ( e . g . , Church and Gale , 1991; Jelinek and Mercer , 1985; Katz , 1987) and classbased methods ( e . g . , Brown et al , 1992; Pereira and Tishby , 1992; Resnik , 1992; Yarowsky ,  1992) . 
Smoothing methods estimate the probability of the unobserved cooccurrences by using frequencies of the individual words  . For exam-pie , when eat and bread do not cooccur , the probability of ( eat , bread ) would be estimated by using the frequency of ( eat ) and ( bread )  . 
A problem with this approach is that it pays no attention to the distributional characteristics of the individual words in question  . Using this method , the probability of ( eat , bread > and ( eat , cars ) would become the same when bread and cars have the same frequency  . It is unacceptable from the linguistic point of view  . 
Class-based methods , on the other hand , estimate the probabihties by associating a class with each word and collecting statistics on word class cooccurrences  . For instance , instead of calculating the probability of ( eat , bread ) directly , these methods associate e at with the class \ [ ingest\]and bread with tile class\[food\] and collect statistics on the classes\[ingest\ ] and\[food\]  . The accuracy of the estimation depends on the choice of classes  , however . Some classbased methods ( e . g . , Yarowsky ,  1992 ) associate each word with a single class without consid cr-ing the other words in the cooccurrence  . However , a word may need to be replaced by different class depending on the cooccurrence  . Some classes may not have enough occurrences to allow a reliable estimation  , while other classes may be too general and include too many words not relevant othe estimation  . An alternative is to obtain various classes associated in a taxonomy with the words in question and select the classes according to a certain criteria  . 
There are a number of ways to select the classes used in the estimation  . Weischedel et al ( 1993 ) chose the lowest classes in a taxonomy can be estimated  . This approach may result in unreliable estimates , since some of the class cooccurrences used may be attributed to chance  . 
Resnik ( 1993 ) selected all pairs of classes corresponding to the head of a prepositional phrase and weighted them to bias the computation of the association in favor of higher-frequency cooccurrences which he considered " more reliable  . " Contrary to this assumption , high frequency cooccurrences axe unreliable when the probability that the cooccurrence may be at -tributed to chance is high  . 
In this paper we propose a classbased method that selects the lowest classes in a taxonomy for which the cooccurrence confidence is above a threshold  . We subsequently apply the method to solving structural ambiguities in Japanese dependency structures and English prepositional phrase attachments  . 
2 Class-based Estimation of
Strength of Association
The strength of association ( SA ) may be measured using the frequencies of word cooccurrences in large corpora  . For instance , Church and Hanks ( 1990 ) calculated SA in terms of mutual information between two words wl and  w2: N * f ( wl , w2) I(w l , w2 ) = log2 ( 1 ) f ( wl ) f ( w2 ) here N is the size of the corpus used in the estimation  , f(W l , w2) is the frequency of the cooccurrence , f(wl ) and f(w2) that of each word . 
When no cooccurrence is observed , SA may be estimated using the frequencies of word classes that contain the words in question  . The mutual information in this case is estimated by: 
I(CI , C2) = log2N*f(Cl , C2 )   ( 2 ) f ( Cl ) f ( C2 ) here Cl and C2 are the word classes that respectively contain W l and  w2  , f ( C1 ) and f ( C2 ) the numbers of occurrences of all the words included in the word classes  C1 and C2  , and f(C1 , C2 ) is the number of cooccurrences of the word classes 
C1 and C2.
Normally , the estimation using word classes needs to select classes  , from a taxonomy , for which cooccurrences are significant . We use t-scores for this purpose 1 . 
For a class cooccurrence ( C1 , C2) , the tscore may be approximated by : ~ f(C , ,C2) - -~ f(Cl)f(C2) (3)

We use the lowest class cooccurrence for which the confidence measured with t-scores is above a threshold  2  . Given a cooccurrence containing the word w , our method selects a class for w in the following way : Step  1: Obtain the classes C 1  , C2 . . . . , C n associated with w in a taxonomy . 
Step 2: Setito0.
Step 3: Set i to iq-1.
Step 4: Compute tusing formula (3).
Step 5: If t < threshold.
If i ~ ngo to step 3.
Otherwise xit.
Step 6: Select the class Ci to replace w.
Let us see what this means with an example . Suppose we try to estimate SA for ( produce , telephone ) 3 . See Table 1 . Here f(v ) , f ( n ) and f ( vn ) axe the frequencies for the verb produce , classes for the noun telephone , and cooccurrences between the verb and the classes for telephone  , respectively ; and t is the tscore 4 . 
' The tscore ( Church and Mercer ,  1993 ) compares the hypothesis that a cooccurrence is significan ~ agains the null hypothesis that the cooccurrence an be attributed to chance  . 
2 The default threshold for tscore is 1 . 28 which corresponds to a confidence lvel of 90% . t-scores are often inflated due to certain violations of assumptions  . 
a The data was obtained from 68 , 623 verb-noun pairs in EDR Corpus ( EDR ,  1993) . 
4In our theory , we are to use each pair of ( Ci , Ci ) , where i = l , 2 ,  . . . m , j-l , 2, .   .   .   , n , to calculate strengths of lexical associations . But our experiments show that upper classes of a verb are very unreliable to be used to measure the strengths  . The reason may be that , unlike nouns , the verbs would not have a " neat " hierarchy or that the upper classes of a verb become too general as they contain too many concepts underneath tem  . Because of this observation , we use , for the classes of a produce concrete thing 671   18926   100   -4  . 6 produce inanimate object 671 559 369 0 . 83 produce implement/tool 6712 1383 51 . 91 produce machine 6716 641 92 . 86 produce communication machine 67 183 10 . 2 5 produce telephone 671   24   0 - Table 1 Estimation of ( produce telephone ) The lowest class cooccurrence ( produce , communication machine ) has a low t score and produces a bad estimation . The most frequent cooccurrence ( produce , concrete thing ) has a low t score also reflecting the fact that it may be attributed to chance  . The t-scores for ( produce , machine ) and ( produce , implement/too O are high and show that these cooccurrences are significant  . Among them , our method selects the lowest class cooccurrence for which the tscore is above the threshold :  ( produce , machine ) . 
3 Disambiguation Using
Class-Based Estimation
We now apply our method to estimate SA for two different types of syntactic constructions and use the results in resolving structural ambiguities  . 
3.1 Disambiguation of Dependency
Relations in Japanese
Identifying the dependency structure of a Japanese sentence is a difficult problem since the language allows relatively free word orders  . A typical dependency relation in Japanese appears in the form of modifier-particle-modific and triplets  . When a modifier is followed by a number of possible modific and s  , verb , the verb itself or , when it does not give us a good result , only the lowest class of the verb in calculating the strength of association  ( SA )  . Thus , for an example , the verb eath as a sequence of eat ~ ingest ~ put something into body --%  . . . --" event-"concept in the class hierarchy , but we use only eat and ingest for the verb eat when calculating SA for  ( eat , apple ) . 
there arise situations in which syntactic roles may be unable to determine the dependency relation or the modifier-modific and relation  . For instance , in ' ~0' ( vigorous ) may modify either ' q ~ ~' ( middleaged ) or ' tll ~' ( health care )  . 
But which one is the modifl c and of ' ~ ~ ~ 0 '? We solve the ambiguity comparing the strength of association for the two or more possible dependency relations  . 
Calculation of Strength of Association We calculate the Strength of Association  ( SA ) score for modifier-particle-modific and by : SA  ( rn / ; p .   .   . m . ) = log2\/(C . , li . r)/(p . .trn . ) \]  ( a ) where Cmfier stands for the classes that include the modifier word  , Part is the particle following the modifier , mc the content word in the modific and phrase , and f the frequency . 
Let us see the process of obtaining SA score in an example  (  ~ - ? ) ~- ~  (   )   ( literally : professor-subject . marker-work ) . To calculate the frequencies for the classes associated with '~'  , we obtain from the Cooccurrence Dictionary ( COD ) 5 the number of occurrences for ( w - 3 ?- SCOD and CD are provided by Japan Electronic Dictionary Research Institute  ( EDR ,  1993) . COD contains the frequencies of individual words and of the modifier-obtain from the Concept Dictionary  ( CD )   6 the closes that include '$~' and then sum up all the occurrences of words included in the classes  . 
The relevant portion of CD for '$~' in (  ~ -$~-~ <  ) is shown in Figure 1 . The numbers in parenthesis here indicate the summed-up frequencies  . 
We then calculate the tscore between '$~-<' and all the classes that include ' ~'  . See
Table 2.
Classes for the t-particle-modifier ~ score modific and 
A ~ ~$#~4.57 h ? ~<
A ~5.14$~<~O ~ A ~1.74~<~~A ~0.74 ~<
Table 2t-scores for (~-~-~<)
The tscore for the cooccurrence of the modifier and particle-modific and pair  , ' ~~ ' and '~) ~-~<' , is higher than the threshold when ' ~' is replaced with \[~ J ~ C~_t ~  ) kr~\] . 
Using (4) , the strength of ~ sociation for the cooccurrence of  (  ~ - ~ ) ~ - ~ <  ) is calculated from the SA between the cl~s\[~R ~lJ'C ~_? cgk~\] and  ,  ~_~< . ' When the word in question has more than one sense  , we estimate SA corresponding to each sense and choose the one that results in the highest SA score  . For instance , we estimate SA between ' ~' and the various senses of '~<'  , and choose the highest value : in this case the one corresponding to the sense ' to be employed  . ' Determination of Most Strongly Associated Structure After calculating SA for each possible construction  , we choose the construction with highest SA score as the most probable struc-pm-ticle-modific and triplets in a corpus that includes  220  , 000 parsed Japanese sentences . 
6 CD provides a hierarchical structure of concepts corresponding to all the words in COD  . The number of concepts in CD is about 400,000 . 
ture . See the following example : ?? . ~?) ~ ~' C~<)kc ) ~ b ~ : ~ .  ? ?  . technic : al progress work people stress nnovatlon Here  , the arrows show possible dependency relations , the numbers on the arrows the estimated SA , and the thick arrows the dependency with highest mutual information that means the most probable dependency relation  . In the example , '~ d : ~~' modifies ' j ~ A . ' C ' and '~<' modifes'A' . The estimated mutual information for (~ g ~#~ , ~ A , ~ C ) is 2 . 79 and that for ( ff~i , A ) is 6 . 13 . Thus , we choose ' ~_/ , ~ C ' as the modific and for ' ~$?' and ' , k ' as that for ' ~ i '
In the example shown in Figure 2 , our method selects the most likely modifier -modific and relation  . 
Experiment Disambiguation of dependency relations was done using  75 anlbiguous constructions from Fukumoto ( 1992 )  . Solving the ambiguity in the constructions involves choosing among two or more modifier -particle-modific and relations  . The training data consists of all 568 , 000 modifier-particle-modific and triplets in COD . 
Evaluation We evaluated the performance of our method comparing its results with those of other methods using the same test and training data  . Table 3 shows the various results ( success rates )  . Here ,   ( 1 ) indicates the performance obtained using the principle of Closest Attachment  ( Kimball ,  1973) ;   ( 2 ) shows the performance obtained using the lowest observed class cooccurrence  ( Weischedel et al ,  1993) ;   ( 3 ) is the result from the maximum mutual information over all pairs of classes corresponding to the words in the cooccurrence  ( Resnik , 1993; Alves ,  1996) ; and (4) shows the performance of our method 7 . 
7 The precision is for the 1 . 28 default threshold . The precision was 81 . 2% and 84 . 1% when we set the threshold to . 84 and . 95 . In all these cases the coverage was 92 . 0% . 
1419 (3) person (3)
I human or similar (42) I
AM ( 39 ) human defined by race or origin ( 3 ) Japanese ( 2 ) worker ( 5 ) person defined by role ( I ) person defined by position . .? .   . . . . . . . 
(I ) slave (0) professor
Figure 1 An Extract of CD\[~9.19\[4.48
F -' ) It national investigation based cause prompt study expect Figure  2 An example of parsing a Japanese sentence method precision  ( 1 ) closest attachment 70 . 6% (2) lowest classes 81 . 2% (3) maximum MI 82 . 6% (4) our method 87 . 0% Table 3 Results for determining dependency relations Closest attachment  ( 1 ) has a low performance since it fails to take into consideration the identity of the words involved in the decision  . Selecting the lowest classes ( 2 ) often produces unreliablestimates and wrong decisions due to data sparseness  . Selecting the classes with highest mutual information  ( 3 ) results in overgeneralization that may lead to incorrect attachments  . Our method avoids both estimating from unreliable classes and overgeneralization and results in better estimates and a better performance  . 
A qualitative analysis of our results shows two causes of errors  , however . Some errors occurred when there were not enough occurrences of the particle-modific and pattern to estimate any of the strength of association necessary for resolving ambiguity  . Other errors occurred when the decision could not be made without surrounding context  . 
3 . 2 P repos i t iona l Phrase At tachment in Eng l i sh Prepositional phrase  ( PP ) attachment is a paradigm case of syntactic ambiguity  . The most probable attachment may be chosen comparing the SA between the PP and the various attachment elements  . Here SA is measured by : SA ( v_attach lv , p , n2) = log2\-\]-(C ~ ~' , 2)) (5) SA(n_attach ln , , p , n)--log , \7-( C - ~ , ~-C , --~2  )  \]  ( 6 ) where Cw stands for the class that includes the word w and f is the frequency in a training data containing  verb-nounl-preposition-noun2 constructions . 
Our method selects from a taxonomy the classes to be used to calculate the SA score and score as the most probable  . 
Experiment We performed a PP attachment experiment on the data that consists of all the  21  , 0 46 semantically annotated verb-noun-preposition-noun constructions found in EDR English Corpus  . We set aside 500 constructions for test and used the remaining 20  , 546 as training data . We first performed the experiment using various values for the threshold  . Table 4 shows the results . The first line here shows the default which corresponds to the most likely attachment for each preposition  . For instance , the preposition of is attached to the noun , reflecting the fact that PP's led by of are mostly attached to nouns in the training data  . The ' confidence ' values correspond to a binomial distribution and are given only as a references  . 
confidence t coverage precision success 100% 68 . 0% 68 . 0% 50%  . 00 82% 82 . 2% 79 . 4% 70%  . 52 75% 87 . 3% 83 . 4% 80%  . 84 65% 88 . 6% 84 . 2% 85%  . 95 57% 89 . 6% 84 . 8% 90% 1 . 28 50% 91 . 3% 85 . 6% Table 4 Results for PP attachment with various thresholds for tscore The precision grows with t-scores  , while coverage decreases . In order to improve coverage , when the method cannot find a class cooccurrence for which the tscore is above the threshold  , we recursiv clytried to find a cooccurrence using the threshold immediately smaller  ( see Table 4 )  . When the method could not find cooccurrences with tscore above the smalles threshold  , the default was used . The overall success rates are shown in " success " column in Table  4  . 
SAs another way of reducing the sparse data problem  , we clustered prepositions using the method escribed in " ~ Vu and Furugori  ( 1996 )  . Prepositions like synonyms and antonyms are clustered into groups and replaced by a representative preposition  ( e . g . , till and pending are replaced by until ; amongst , a mid and a midst are replaced by among . ) . 
Evaluation We evaluated the performance of our method comparing its results with those of other methods with the same test and training data  . The results are given in Table 5 . Here ,   ( 5 ) shows the performance of two native speakers who we rejust presented quadruples of four head words without surrounding contexts  . 
Method Success Rate (1) closest Attachment 59 . 6% (2) lowest classes 80 . 2% (3) maximum MI 79 . 0% (4) our method 85 . 6% (5) human ( head words only ) 87 . 0%
Table 5 Comparison with other methods
The lower bound and the upper bound on the performance of our method seem to be  59  . 6% scored by the simple heuristic of closest attachment  ( 1 ) and 87 . 0% by human beings (4) . 
Obviously , the success rate of closest attachment ( 1 ) is low as it always attaches a word to the noun without considering the words in question  . The unanticipated low success rate of human judges is partly due to the fact that sometimes constructions were inherently ambiguous so that their choices differed from the annotation in the corpus  . 
Our method ( 4 ) performed better than the lowest classes method ( 2 ) and maximum MI method ( 3 )  . I to we smainly to the fact that our method makes the estimation from class cooccurrences that are more reliable  . 
4 Concluding Remarks
We proposed a classbased method that selects classes to be used to estimate the strength of association for word cooccurrences  . The classes selected by our method can be used to estimate various types of strength of association i different applications  . The method iffers from other classbased methods in that it allows identification of a reliable and specific class for each cooccurrence in consideration and can deal with date sparseness problem more efficiently  . It ods : overgeneralization and employment of unreliable class cooccurrences  . 
We applied our method to two structural disambiguation experiments  . In both experiments the performance is significantly better than those of others  . 
References\[1\]Alves , E .  1996 . " The Selection of the Most Probable Dependency Structure in Japanese Using Mutual Information  . " In Proc . of the 34th ACL , pages 372-374 . 
\[2\] Brown , P . , Della Pietra , V . and Mercer , R .  (1992) . " Word Sense Disambiguation Using Statistical Methods  . " Proceedings of the 30th ACL , pages 264-270 . 
\[3\] Church , K . , and Mercer , R .  1993 . " Introduction to the Special Issue on Computational Linguistics Using Large Corpora  . " Computational Linguistics , 19(1):124 . 
\[4\] Church , K . , and Hanks , P .  1990 . " Word Association Norms , Mutual Information and Lexicography . " Computational Linguistics , 16(1):22-29 . 
\[5\] Church , K . , and Gale , W .  1991 . " A Comparison of the Enhanced Good Turing and Deleted Estimation Methods for Estimating Probabilities of English Bigrams  . " Computer Speech and Language , 5:19-54 . 
\[6\] Fukumoto , F . , Sano , H . , Saitoh , Y . and Fukumoto J .  1992 . " A Framework for Dependency Grammar Based on the Word's Modifiability Level-Restricted Dependency Grammar  . " Trans . IPSJapan , 33(10):1211-1223 ( in Japanese) . 
\[7\] Hindle , D . , and Rooth , M .  1993 . " Structural Ambiguity and Lexical Relations . " Computational Linguistics , 19(1):103-120 . 
\ [8\] Japan Electronic Dictionary Research Institute , Ltd .  1993 . EDR Electronic Dictionary
Specifications Guide ( in Japanese).
\[9\] Jelinek , F . , and Mercer , R .  1985 . " Probability Distribution Estimation from Sparse Data  . " IBM Technical Disclosure Bulletin , 28:2591-2594 . 
\[10\]Katz , S .  1987 . " Estimation of Probabilities from Sparse Data for Language Model Component of a Speech Recognizer  . " IEEE Transactions on Acoustics , Speech and Signal Processing , ASSP-35(3): 400-401 . 
\[11\] Kimball , J .  1973 . "Seven Principles of Surface Structure Parsing in Natural Language  . " Cognition , 2:15-47 . 
\[12\]Pereira , F . and Tishby , N .  1992 . " Distributional Similarity , Phrase Transitions and Hierarchical Clustering . " In Proc . of the 30th
ACL , pages 183-190.
\[13\] Resnik , P .  1992 . " Wordnet and Distributional Analysis : A Class -Based Approach to Lexical Discovery  . " AAAI Workshop on Statistically-based Natural Language Processing Techniques  , pages 56-64 . 
\[14\] Resnik , P .  1993 . " Selection and Information : A Class-Based Approach to Lexical Relationships  . " PhD . thesis , University of

\[15\] Weischedel , R . , Meteer , M . , Schwartz , R . , Ramshaw , L . , and Palmucci , J .  1993 . " Coping with Ambiguity and Unknown Words Through Probabilistic Models  . " Computational Linguistics , 19(2):359-382 . 
\[16\]Wu , H . and Furugori , T .  1996 . " A Hybrid Disambiguation Model for Prepositional Phrase Attachment  . " Literary and
Linguistic Computing . 11(4):187-192.
\[17\] Yarowsky , D .  1992 . " Word Sense Disambiguation using Statistical Models of Roget's Categories Trained on Large Corpora  . " Proceedings of COLING-92, pages 454-460 . 

