GENERALIZED CHART ALGORITHM :
ANEFFICIENT PROCE DURE FOR
COST-BASED ABDUCTION
Yasuharu Den
ATR Interpreting Telecommunications Research Laboratories  22 Hikaridai , Seikacho , Sorakugun , Kyoto 619-02 , JAPANTel:+81-7749-5-1328 , Fax:+81-7749-5-1308 , email : den Qitl . atr . co . jp
Abstract
We present an efficient procedure for cost-based abduction  , which is based on the idea of using chart parsers as proof procedures  . We discuss in detail three features of our algorithm--goal-driven bottom-up derivation  , tabulation of the partial results , and agenda control mechanism -- and report the results of the preliminary experiments  , which show how these features improve the computational efficiency of cost-based abduction  . 

Spoken language understanding is one of the most challenging research areas in natural language processing  . Since spoken language is incomplete in various ways  , i . e . , containing speech errors , ellipsis , metonymy , etc . , spoken language understanding systems hould have the ability to process incomplete inputs by hypothesizing the underlying information  . The abduction-based approach ( Hobbs et al . , 1988 ) has provided a simple and elegant way to realize such a task  . 
Consider the following 3 apanese sentence : ( 1 ) Sfsekikat-ta ( a famous writer ) buy PAST This sentence contains two typical phenomena arising in spoken language  , i . e . , metonymy and the ellipsis of a particle . When this sentence is uttered under the situation where the speake reports his experience  , its natural interpretation is the speaker bought aSS seki novel  . To derive this interpretation , we need to resolve the following problems : ? The metonymy implied by the noun phrase  S6seki is expanded to a S6seki novel , based on the pragmatic knowledge that the name of a writer is sometimes used to refer to his novel  . 
? The particle-less thematic relation between the verb katta and the noun phrase SSseki is determined to be the object case relation  , based on the semantic knowledge that the object case relation between a trading action and a commodity can be linguistically expressed as a thematic relation  . 
This interpretation is made by abduction . For instance , the above semantic knowledge is stated , in terms of the predicate logic , as follows : (2) sem(e , x ) Ctrade(e ) A commodity(x ) A obj(e , x ) Then , the inference process derives the consequent sem ( e , x ) by hypothesizing an antecedent obj ( e , x ) , which is never proved from the observed facts . This process is called abduction . 
Of course , there may be several other possibilities that support the thematic relation sem  ( e , x) . 
For instance , the thematic relation being determined to be the agent case relation  , sentence (1) can have another interpretation , i . e . , Sfseki bought something , which , under some other situations , might be more feasible than the first interpretation  . 
To cope with feasibility , the abduction-based model usually manages the mechanism for evaluating the goodness of the interpretation  . This is known as cost-based abduction ( Hobbs et al ,  1988) . 
In cost-based abduction , each assumption bears a certain cost . For instance , the assumption obj ( e , x ) , introduced by applying rule (2) , is specified to have a cost of , say ,  $2 . The goodness of the interpretation is evaluated by accumulating the costs of all the assumptions involved  . The whole process of interpreting an utterance is depicted in the following schema :  1  . Find all possible interpretations , and 2 . Select the one that has the lowest cost . 
In our example , the interpretation that assumes the thematic relation to be the object case relation  , with the metonymy being expanded to a S6seki novel , is cheaper than the interpretation that assumes the thematic relation to be the agent case relation  ; hence , the former is selected . 
An apparent problem here is the high computational cost  ; because abduction allows many possibilities , the schema involves very heavy computation . Particularly in the spoken language understanding task  , we need to consider a great number of possibilities when hypothesizing various underlying information  . This makes the abduction processticality of abduction-based systems  . The existing models do not provide any basic solution to this problem  . Charniak ( Charniak and Husain , 1991; Charniak and Santos Jr . , 1992) dealt with the problem , but those solutions are applicable only to the propositional case  , where the search space is represented as a directed graph over ground formulas  . 
In other words , they did not provide a way to build such graphs from rules  , which , in general , contain variables and can be recursive . 
This paper provides a basic and practical solution to the computation problem of cost-based abduction  . The basic idea comes from the natural language parsing literature  . As Pereira and Warren (1983) pointed out , there is a strong connection between parsing and deduction  . They showed that parsing of DCG can be seen as a special case of deduction of Horn clauses  ; conversely , deduction can be seen as a generalization of parsing  . Their idea of using chart parsers as deductive -proof procedures can easily be extended to the idea of using chart parsers as abductive-proof procedures  . Because chart parsers have many advantages from the viewpoint of computational efficiency  , chart-based abductive-proof procedures are expected to nicely solve the computation problem  . Our algorithm , proposed in this paper , has the following features , which considerably enhance the computational efficiency of cost-based abduction :  1  . Goal-driven bottom-up derivation , which reduces the search space . 
2 . Tabulation of the partial results , which avoids the recomputation of the same goal . 
3 . Agenda control mechanism , which realizes various search strategies to find the best solution efficiently  . 
The rest of the paper is organized as follows.
First , we explain the basic idea of our algorithm , and then present he details of the algorithm along with simple examples  . Next , we report the results of the preliminary experiments  , which clearly show how the above features of our algorithm improve the computational efficiency  . Then , we compare our algorithm with Pereira and Warren 's algorithm  , and finally conclude the paper . 
Head-driven Derivation
Pereira and Warren showed that chart parsers can be used as proof procedures  ; they presented the Earley deduction proof procedure  , that is a generalization of topdown chart parsers  . However , they mentioned only topdown chart parsers , which is not always very efficient compared to bottom-up  ( left-corner ) chart parsers . It seems that using left-corner parsers as proof procedures in ot so easy  ,   . . . . . . . . . . . ':"'"'" Figure 1: Concept of Head-driven Derivation unless the rules given to the provers have a certain property  . Here , we describe under what conditions left-corner parsers can be used as proof procedures  . 
Let us begin with the general problems of Horn clause deduction with naive topdown and bottom-up derivations : ? Deduction with topdown derivation is affected by the frequent backtracking necessitated by the inadequate selection of rules to be applied  . 
? Deduction with bottom-up derivation is affected by the extensive vacuous computation  , which never contributes to the proof of the initial goal  . 
These are similar to the problems that typically arise in natural language parsing with naive topdown and bottom-u parsers  . In natural language parsing , these problems are resolved by introducing a more sophisticated derivation mechanism  , i . e . , left-corner parsing . We have attempted to apply such a sophisticated mechanism to deduction  . 
Suppose that the proof of a goal g(x , y ) can be represented in the manner in Figure 1 ; the first argument x of the goal g(x , y ) is shared by all the formulas along the path from the goalg  ( z , y ) to the left corner am(z , zm ) . In such a case , we can think of a derivation process that is similar to left-corner parsing  . We call this derivation head-driven derivation , which is depicted as follows : Step 1 Finda fact a ( w , z ) whose first argument wunifies with the first argument x of the goalg  ( x , y ) , and place it on the left corner . 
Step 2 Find a rule am-l(W , Zrn-l ) Ca(W , Zm)/~BZ^ . . . ABn whose leftmost antecedent a(W , Zm ) unifies with the left-corner keya(x , z ) , and introduce the new goals B1 ,   . . . , and Bn . If all these goals are recursively derived , then create the consequent a , ,~_ 1( z , zm_1) , which dominates a(x , zm ) , B 1 ,   . . . , and Bn , and place it on the left corner instead of a(x , z) . 
Step 3 If the consequent a m-l(x , zm_l ) unifies with the goal g(z , y ) , then finish the process . Otherwise , go back to step 2 with am-1(x , zm_l ) being the new left-corner key . 

Left-corner parsing of DCG is just a special case of head-driven derivation  , in which the input string is shared along the left border  , i . e . , the path from a nonterminal to the leftmost word in the string that is dominated by that nonterminal  . 
Also , semantic-head-driven generation ( Shieber el al . , 1989; van Noord , 1990) and head-corner parsing ivan Noord , 1991; Sikkel and op den Akker , 1993) can be seen as head-driven derivation , when the semantic-head/syntactic-head is moved to the leftmost position in the body of each rule and the argument representing the semantic -feature/head-feature is moved to the first position in the argument list of each formula  . 
To apply the above procedures , all rules must be in chain form arn--l(W , Zrn-~)Carn(W , Zm ) AB1A . . . ABn ; that is , in every rule , the first argument of the leftmost antecedent must be equal to the first argument of the consequent  . This is the condition under which left-corner parsers can be used as proof procedures  . Because this condition is overly restrictive , we extend the procedures so that they allow non -chain rules  , i . e . , rules not in chain form . Step 1 is replaced by the following : Step 1 Finda non-chain rule a ( w , z ) CB1 A .   .   . A B ~ such that the first argument w of the consequent a  ( w , z ) unifies with the first argument z of the goalg ( x , y ) , and introduce the new goals B1 ,   . . . , and /3, . A fact is regarded as a non-chain rule with an empty antecedent  . If all these goals are recursively derived , then create the consequent a(z , z ) , which dominates B1 ,   . . . , and B , , and place it on the left corner . 
Generalized Chart Algorithm
The idea given in the previous section realizes the goal-driven bottom-up derivation  , which is the first feature of our algorithm . Then , we present a more refined algorithm based upon the idea  , which realizes the other two features as well as the first one  . 
Chart Parsing and its Generalization Like left-corner parsing  , which has the drawback of repeatedly recomputing partial results  , head-driven derivation will face the same problem when it is executed in a depth-first manner with backtracking  . In the case of left-corner parsing , the problem is resolved by using the tabulation method  , known as chart parsing ( Kay ,  1980) . A recent study by Haruno et al ( 1993 ) has shown that the same method is applicable to semantic-head-driven generation  . The method is also applicable to head-driven derivation  , which is more general than semantic-head-driven generation  . 
To generalize charts to use in proof procedures , m(<\[AJ , \[B\]> , \[A , B \]) , oO . O*?"?"?O?Oo . ., . 


/ /  . . k i / .  '- . \/\\ h(<IA\],\[BI>A ~>) I<II . \[BI > ~ . . m ( <\[\],~ f ~ . .)\]) g -" .   .   .   .   . -%-"A .   .   .   .   . C '""" . . . . . . . . ":':~ ~\[ A1JBI ~(" -- ~ / <\[ l \ [ l>~
Z\u " . . . .  . . . . . I(Some labels . . . . . m (<\[ A \],\[_\] . .~',tA \]) are omitted ) . . . . . . . . . . . . . . . . . . . .  . .
m (<\[ A \], IB\]>,\[B , A\])
Figure 2: Example of Generalized Charts we first define the chart lexicons  . In chart parsing , lexicons are the words in the input string , each of which is used as the index for a subset of the edges in the chart  ; each edge incident from ( the start-point of ) lexicon w represents the substructure dominating the substring starting from w  . In our case , from the-similarity between left-corner parsing and head-driven derivation  , lexicons are the terms that occur in the first argument position of any formula  ; each edge incident from ( the start-point of ) lexicon x represents the substructure dominating the successive sequence of the derived formulas starting from the fact in which z occupies the first argument position  . For example , in the chart representing the proof in Figure 1 , all the edges corresponding to the formulas on the left border  , i . e . am(X , Zrn ), am--l(Z,Zm--1), . . . , al(x , zl ) and g(z , y ) , are incident from ( the start-point of ) lexicon z , and , hence , x is the index for these edges . 
Following this definition of the chart lexicons , there are two major differences between chart parsing and proof procedures  , which Haruno also showed to be the differences between chart parsing and semantic-head-driven generation  . 
1 . In contrast ochart parsing , where lexicons are determined immediately upon input  , in proof procedures lexicons should be incrementally introduced  . 
2 . In contrast ochart parsing , where lexicons are connected one by one in a linear sequence  , in proof procedures lexicon should be connected in many-to-many fashion  . 
In proof procedures , the chart lexicons are not determined at the beginning of the proof  ( because the proof )  , rather they are dynamically extracted from the subgoals as the process goes  . In addition , if the rules are nondeterministic , it sometimes happens that there are introduced , from one left-corner key , a(x , z ) , two or more distinct successive subgoals , bl(wl , y ~) , b2(w2 , y2) , etc . , that have different first arguments , w1 , w2 , etc . In such a case , one lexicon x should be connected to two or more distinct lexicons  , w1 , w2 , etc . Furthermore , it can happen that two or more distinct left -corner keys  , al(xl , zl ) , a2(x2 , z2) , etc . , incidentally introduce the successive subgoals , bl(w , yl ) , b2(w , y ~) , etc . , with the same first argument w . In such a case , two or more distinct lexicons , x1 , x2 , etc . , should be connected to one lexicon w . Therefore , the connections among lexicons should be many-to -many  . Figure 2 shows an example of charts with many-to-many connections  , where the connections are represented by pointers A  , B ; etc . 
The Algorithm
We , so far , have considere deduction but not abduction . Here , we extend our idea to apply to abduction , and present he definition of the algorithm . 
The extension for abduction is very simple.
First , we add a new procedure , which introduces an assumption G for a given goal G  . An assumption is treated as if it were a fact . This means that an assumption , as well as a fact , is represented as a passive edge in terms of the chart algorithm  . Second , we associate a set S of assumptions with each edge e in the chart  ; S consists of all the assumptions that are contained in the completed part of the  ( partial ) proof represented by the edge e . More formally , the assumption set 5 associated with an edge e is determined as follows:  1  . If e is a passive edge representing an assumption
A , then S--A.
2 . If e is a passive/active edge introduced from a non-chain rule  , including fact , then S is empty . 
3 . If e is a passive/active edge predicted from a chain rule with a passive edge e ' being the left -corner key  , then S is equal to the assumption set
S ' of e'.
4 . If e is a passive/active edge created by combining an active edge el and a passive edge  e2  , then , -q = $1 U $2 where 81 and ~ q2 are the assumption sets of el and e2  , respectively . 
Taking these into account , the definition of our algorithm is as follows , f is a function that assigns a unique vertex to each chart lexicon  . The notation A : S stands for the label of an edge e  , where A is the label of e in an ordinary sense and S is the assumption set associated with e  . 
Initialization Add an active edge\[\[?IG \]- I-:?to the chart  , looping at vertex 0 , where G is the initial goal . 
Apply the following procedures repeatedly until no procedures are applicable  . 
Introduction Let e be an active edge labeled\[ . . . \[?\] Bj . . . \] A : Sincident from vertex s to t , where Bj = bj(zj , yj ) is the first open boxine . 
1 . If the lexicon xj is never introduced in the chart  , then introduce it and run a pointer from tt of ( zj )   . Then , do the following : ( a ) For every non-chain rule a ( w , z ) CB1 A .   .   . ABn , including fact , such that w unifies with z i , create an active edge labeled\[\[?\]Bl '"\[? lS , ~\]a(xj , z ): ? between vertex f(x j ) and f(zj)+1 . ( Create , instead , a passive edge labeled a(x j , z ): ? when the rule is a fact , i . e . n = 0 . )  ( b ) Create a passive edge labeled Bj : Bj between vertex f  ( xj ) and f ( zj )  + 1 . 
2 . If the lexicon zj was previously introduced in the chart  , then run a pointer from t t of ( x j ) . 
In addition , if the passivedge Bj : Bj never exists in the chart  , create it between vertex f(rj ) and f(xj)+1 . 
Prediction Lete be a passive edge labeled C : S incident from vertex s to t  . For every chain rule A ' CAAB1A .   .   . A Bn such that A unifies with C , create an active edge labeled\[A\[?\]B1 . . . \[?\] Bn\]A':,~between vertex s and t . 
( Create , instead , a passive edge labeled A ': S when A is the single antecedent  , i . e . , n = 0 . ) Combinat ion Let ez be an active edge labeled \[ '" "\[?\] Bj\[?\]Bj+I '"  . \[?\] B , ~\]A : $1 incident from vertex s to t , where Bj is the first open boxine z and let e2 be a passive edge labeled C : S ~ incident from vertex u to v  . If Bj and Cunify and there is a pointer from t to u  , then create an active edge labeled \[- . . Bj\[?\]Bj+I . . . \[?\] Bn\]A:S1US2 between vertex s and v . ( Create , instead , a passive edge labeled A:S1 US : when B 1 is the last element , i . e . , j = n . ) Each passive edge T : S represents an answer . 

Here , we present a simple example of the application of our algorithm to spoken language understanding  . Figure 3 provides the rules for spoken Japanese understanding  , with which the sentence ( 1 ) is parsed and interpreted . They include the pragmatic , semantic and knowledge rules as well as the syntactic and lexical rules  . 
The syntactic rules allow the connection between a verb and a noun phrase with or with-s  ( i , k , e)Cvp(i , k , e ) v p(i , k , e)Cnp(i , j , c , x ) Avp(j , k , e ) A depend((c , e , x ) d ) v p(i , k , e)Cn p(i , j , x ) Avp(j , k , e ) A depend((c , e , X ) d ) np(i , k , c , x)Cnp(i , j , x ) Ap(j , k , c ) depend((c , e , x ) d ) Cprag((x ) p , y ) ^ sem(c , e , y ) ,  )
Lexical Rules np(\[S6seki \] k\] , k , x)Csoseki(x )$ ~ vp(\[katta\]k\] , k , e)C buy(e)*1 p(\[galk\] , k , c)cga(e)p(\[olk\] , k , c)C wo(c ) . 1
Pragmatic Rules prag((x)p , ) prag((x ) p , y)Crter(x)^wrte(^novel(y)Sl
Semantic Rulessem(s ) Cga(s , e ) Aga(e ) $ 3 sem(s ) Cwo(s , e)^o(e ) . 3ga((c , e , x ) 8 , c ) C in tend(e ) A person(x ) A agt((e , x)e )$ 2? wo((c , e , x ) , c)Ctrade(e ) A commodity(z ) ^ obj((e , x ) , ) $~
Knowledge Rules person ( x ) Csoseki ( x ) w~ter ( x ) Csoseki ( x ) book ( x ) Cnovd ( x ) eommodity (  ~  ) C book ( z ) trade ( e ) Cbuy ( e ) intend ( e ) Ctrade ( e ) 
Figure 3: Example of Rules out a particle , which permit structures like \[ VP\[NpS6sek2\]\[vpkatla  \]\] . Such a structure is evaluated by the pragmatic and semantic riteria  . That is , the dependency between a verbal concept e and a nominal concept x is supported if there is an entity y such that x and y have a pragmatic relation  , i . e . , a metonymy relation , and e and y have a semantic relation , i . e . , a thematic relation . The metonymy relation is defined by the pragmatic rules  , based on certain knowledge , such as that the name of a writer is sometimes used to refer to his novel  . Also , the thematic relation is defined by the semantic rules  , based on certain knowledge , such as that the object case relation between a trading action and a commodity can be linguistically expressed as a thematic relation  . 
The subscript $ c of a formula A represents the cost of assuming formula A  . A is easy to assume when c is small , while A is difficult to assume when c is large . For instance , the cost of interpreting the thematic relation between a trading action and a commodity as the object case relation is low  , say $2 , while the cost of interpreting the thematic relation between an intentional action and a third person as the agent case relation is high  , say $ 20 . This assignment of costs is suitable for a situation in which the speaker reports his experience  . In spite of the difficulty of assigning suitable costs in general  , the cost-based interpretation is valuable , because it provides a uniform criteria for syntax  , semantics and pragmatics . Hopefully , several techniques , independently developed in these areas , e . g . , stochastic parsing , example-based/corpus-based techniques for word sense/structural disambiguation  , etc . , will be usable for better cost assignment . Probability will also be a key technique for the cost assignment  ( Charniak and Shimony ,  1990) . 
Figure 4 and Table 1 show the chart that is created when a sentence ( 1 ) is parsed and interpreted using our algorithm . Although the diagram seems complicated , it is easy to understand if we break down the diagram  . Included are the syntactic parsing of the sentence  ( indicated by edges 2 ,  6 ,  7 ,  14 , 52 and 53) , the pragmatic interpretation of the metonymy by S6seki S ( indicated by edges 17 ,  18 , 20 and 24) , the semantic interpretation of the thematic relation between a buying event B and a novel N written by  S6seki   ( indicated by edges 42 ,  44 ,  45 ,  47 , 48 and 50) , and so on . In the pragmatic interpretation , assumption ovel ( N ) ( edge 21) is introduced , which is reused in the semantic interpretation . In other words , a single assumption is used more than once . Such a tricky job is naturally realized by the nature of the chart algorithm  . 
Agenda Control
Since the aim of cost-based abduction is to find out the best solution  , not all solutions , it is reasonable to consider combining heuristic search strategies with our algorithm to find the best solution efficiently  . Our algorithm facilitate such an extension by using the agenda control mechanism  , which is broadly used in advanced chart parsing systems  . 
The agenda is a storage for the edges created by any of the three procedures of the chart algorithm  , out of which edges to be added to the chart are selected  , one by one , by a certain criterion . The simplest strategy is to select the edge which has the minimal cost at that time  , i . e . , ordered search . 
Although ordered search guarantees that the first solution is the best one  , it is not always very efficient . We can think of other search strategies , like bestfirst search , beamsearch , etc . , which are more practical than ordered search . To date , we have not investigated any of these practical search strategies  . 
However , it is apparent hatour chart algorithm , together with the agenda control mechanism , will provide a good way to realize these practical search strategies  . 
222\[ Soseki , katta\]i

I i


I i



Il 39, ,, 5 . .4 .  -  .  - ~? . .? . ?-??? o??g ~ gllIRmBSml ~00-?-- . . . . . . . 
6"7,8%\ f . l -*- o . ~ i . . . . . . . . . . . . . . . ! i 35 . .  . . . . 34,49 " i ! a F < B,S>s i ~ 1 . ~ iii .   .   .   .   .   .   .   . < s . v
I "..' L_3,4,5 ~. S
Q . -"20/24g/k"\_XZ :" .   .   .   .   . L ???'~???' o0o ?' . . . . . .






Ii D !
P n\\13 ~ . 1-'-)2i-"I"J . . . 10 , 11 , 1 pass ive edge act ive edge pointer Figure 4: Chart Diagram for SSsekikatta
Preliminary Experiments
We conducted preliminary experiments o compare four methods of cost-based abduction : topdown algorithm  ( TD )  , head-driven algorithm ( HD ) , generalized chart algorithm with full-search ( GCF )  , and generalized chart algorithm with ordered search  ( GCO )  . The rules used for the experiments are in the spoken language understanding task  , and they are rather small ( 51 chain rules +35 non-chain rules )  . The test sentences include one verb and 14 noun phrases , e . g . , sentence (1) . 
Table 2 shows the results . The performance of each method is measured by the number of computation steps  , i . e . , the number of derivation steps in TD and HD , and the number of passive and active edges in GCF and GCO  . The decimals in parentheses show the ratio of the performance of each method to the performance of TD  . The table clearly shows how the three features of our algorithm improve the computational efficiency  . The improvement from TD to HD is due to the first feature  , i . e . , goal-driven bottom-up derivation , which eliminates about 50% of the computation steps ; the improvement from HD to GCF is due to the second feature  , i . e . , tabulation of the partial results , Table 2: Comp . among TD , HD , GCF , and GCO
Ns IIT DItt DGC F12 151 12 (0 . 52) 83 (0 . 39) 2 432 218 (0 . 50) 148 (0 . 34) 3 654 330 (0 . 50) 193 (0 . 30) 4 876 442 (0 . 50) 238 (0 . 27)
GCO 75 (0 . 35) 113 (0 . 26) 160 (0 . 24) 203 (0 . 23 ) which decreases the number of steps another 13%-23%  ; the improvement from GCF to GCO is due to the last feature  , i . e . , the agenda control mechanism , which decreases the number of steps another 4%-8% . In short , the efficiency is improved , maximally , about four times . 
Comparison with Earley Deduction We describe  , here , some differences between our algorithm and Earley deduction presented by Pereira and Warren  . First , as we mentioned before , our algorithm is mainly based on bottom-up ( left-corner ) derivation rather than topdown derivation , that Earley deduction is based on . Our experiments showed the superiority of this approach in our par-Second  , our algorithm does not use sub-sumption-checking of edges  , which causes a serious computation problem in Earley deduction  . Our algorithm needs subsumption-checking oly when a new edge is introduced by the combination procedure  . In the parsing of augmented grammars , even when two edges have the same nonterminal symbol  , they are different in the annotated structures associated with those edges  , e . g . , feature structures ; in such a case , we cannot use one edge in place of another . Likewise , in our algorithm , edges are always annotated by the assumption sets  , which , in most cases , prevent those edges from being reused . 
Therefore , in this case , subsumption-checking is ot effective . In our algorithm , reuse of edges only becomes possible when a new edge is introduced by the introduction procedure  . However , this is done only by adding a pointer to the edge to be reused  , and , to invoke this operation , equality-checking of lexicons , not edges , is sufficient . 
Finally , our algorithm has a stronger connection with chart parsing than Earley deduction does  . 
Pereira and Warren noted that the indexing of formulas is just an implementation technique to increase efficiency  . However , indexing plays a considerable role in chart parsing  , and how to index formulas in the case of proof procedures in ot so obvious  . In our algorithm , from the consideration of head-driven derivation , the index of a formula is determined to be the first argument of that formula  . All formulas with the same index are derived the first time that index is introduced in the chart  . 
Pointers among lexicons are also helpful in avoiding nonproductive attempts at applying the combination procedure  . All the devices that were originally used in chart parsers in a restricted way are included in the formalism  , not in the implementation , of our algorithm . 
Concluding Remarks
In this paper , we provided a basic and practical solution to the computation problem of cost-based abduction  . We explained the basic concept of our algorithm and presented the details of the algorithm along with simple examples  . We also showed how our algorithm improves computational efficiency on the basis of the results of the preliminary experiments  . 
We are now developing an abduction-based spoken language understanding system using our algorithm  . The main problem is how to find a good search strategy that can be implemented with the agenda control mechanism  . We are investigating this issue using both theoretical ndempirical approaches  . We hope to report good results along these lines in the future  . 

The author would like to thank Prof . Yuji Matsumoto of Nara Institute of Science and Technology and Masahiko Haruno of NTT Communication Science Laboratories for their helpful discussions  . 
References\[Charniak and Husain , 1991\] Eugene Charniak and Saadia Husain . A new admissible heuristic for minimal-cost proofs  . Proceedings of the 12th
IJCAI , pages 446-451, 1991.
\[Charniak and Santos Jr . , 1992\] Eugene Charniak and Eugene Santos Jr . Dynamic MAP calculations for abduction . Proceedings of the l Oth
AAAI , pages 552-557, 1992.
\[Charniak and Shimony , 1990\] Eugene Charniak and Solom on E . Shimony . Probabilistic semantics for cost based abduction . Proceedings of the 8th AAAI , pages 106-111, 1990 . 
\[ Haruno et al , 1993\] Masahiko Haruno , Yasuharu Den , Yuji Matsumoto , and Makoto Nagao . Bidirectional chart generation of natural language texts  . Proceedings of the 11th AAAI , pages 350-356 ,  1993 . 
\[ Hobb set at . , 1988\] Jerry R . Hobbs , Mark Stickel , Paul Martin , and Douglas Edwards . Interpretation as abduction . Proceedings of the 26th Annual Meeting of ACL , pages 95-103 ,  1988 . 
\[Kay , 1980\] Martin Kay . Algorithm schemata and data structures in syntactic processing  . Technical Report CSL-80-12 , XEROX Palo Alto Research
Center , 1980.
\[Pereir and Warren , 1983\] Fernando C . N . Pereira and David H . D . Warren . Parsing as deduction . 
Proceedings of the 21st Annual Meeting of ACL , pages 137-144 ,  1983 . 
\[Shieber et at . , 1989\] Stuart M . Shieber , Gertjan van Noord , Robert C . Moore , and Fernando C . N . 
Pereira . A semantic-head-drivenge ration algorithm for unification-based formalisms  . Proceedings of the 27th Annual Meeting of ACL , pages 717 ,  1989 . 
\[ Sikkel and op den Akker , 1993\] Klaas Sikkel and Rieks op den Akker . Predictive head-corner chart parsing . The 3rd International Workshop on Parsing Technologies , pages 267-276 ,  1993 . 
\[van Noord , 1990\] Gertjan van Noor d . An overview of head-driven bottom-up generation . Current Research in Natural Language Generation , chapter 6 , pages 141-165 . Academic Press , 1990 . 
\[van Noord , 1991\] Gertjan van Noord . Head corner parsing for discontinuous constituency  . Proceedings of the 29th Annual Meeting of ACL , pages 114-121 ,  1991 . 

Table 1: Table Representation of the Chart
I1 #I  #IArcA-Set\[From1\[\[?\]s( , ~ , \[\] , e ) \] T?1-'2'\[\[?\]s?seki ( S ) $1\] d'1 np (  ~ , q2 , S )' 3' s?seki(S ) ~ l~l2'4 I person(S ) a , 3'5' writer(S)c ~ , 3'6' np(Cb , ffl , S)c~2+B+3''6\[?lvp(@ , k , e )\[?\] depend((c , e , S ) a)\]'8 I\[np(~ , ff2 , S)a'6\[?lp(kO , k , e)\]np(d2 , k , c , S ) 9I\[\[?\]buy(B)Sl\]?7 . vp ( ~ , ~ , B ) 10' buy ( B ) ~lf li9!11'trade ( B ) /3I10'i'2 lintend ( B ) /3I11i3IvP ( ql ' ~' B ) /3I9+D+10141\[np ( rb , ff2 , S)c ~ , /37 + C+13vp(~ , ~ , B )\[?\] depend((P , B , S ) d)\]vp(q ~ , ~ , B ) 15\[\[?\]prag((S)p , X ) ?14\[?\]sem((P , S , x ) . .)\] depend(<P , S , S ) a ) 16 prag((S ) p , S ) ? , 15 i7J\[\[?\]writer ( S ) '?15\[?\] write (   ( S , x ) p )$1 ?\[7\] novel(c ) . 1\]\[prag((s ) ~ , ~ ) as '\[ writer ( S ) ' a'17 + G+5\[?\] write (   ( S , N ) p ) ~1?\[?\] novel ( N ) ~1\] prag ( (S>v'N ) i19 i write ( (S , N ) p ) ' a ?'7'18'20\[ writer ( S ) ' oqT ' 18 + H + 19 write ( S , N > . )\[?\] novel(N ) ~1\]prag((S)p , N ) ,   , '' novel ( N ) "2022 book ( N ) t 62 123' commodity ( N ) '6'2224' prag ( IS ) v , N ) ' a , 7 , 8'20 + I+2125\[prag((S)v , S ") ?15 + F+16 t?\]sem((P , B , S > ~ , )\]  , depend (( P , B , S ) d ) 26\[\[?\]intend ( By25\[?\]person ( S ) \[?\] agt (   ( B , S ) , ) ~2?\] ga(<P , B , S) , ,P ) 27'\[\[?\]trade(B ) ! ~ b , ~25\[?\] commodity(S ) wo((P , B , S ) ,   , P )
Arc\[A-Set IFrom J28\[intend(B ) , /326 + K+12\[?\]person(S)\[?\]agt(<B , S ) ,  )  . 2?\]ga(<P , B , S)~ , P ) 29 I\[trade ( B ) I/3I27+K+11\[?\] commodity ( S ) \[ e\]obj ( (B , S ) ~) ~ 2\]wo(<P , B , S) , ,P ) '30 ' \[ intend(B ) '  a , /3'28 + L+4' person(S ) it ?\] agt((B , S ) , ) ~2?\] ga(<P , B , S)~ , P ) ,   ,   , 31'agt((BS ) s ) ~2 ? e3032 ~ ga((P , B , S ) s P )' a , fl , e'33 I\[ga((P , B , S ) ,   , P )' a , /3 , eI30+M+31I32\[?\]ga(P)~3\]sem((P , B , S ) , )  , 34' ga(P ) ~3I , 35\[sem((P , B , S)s)Ia , ? I33\[/3 , e , ? t33+N+34I36I depe~d((P , B , S ) d ) a , /3 , e , ~' I25+J+35 l37Ivp((~ , ~ , B ) ' a , /3 , e ,  ~  ,   , 14 + E + 3638'~? , ~ , B ) or , /3 , e ,  ~  ,  37  , 39'' a , fl , e , ( ~1 + A + 3840\[prag((S)p , N ) a , 7  , 515 + F+24\[?\]sem((P , S , g ) s )\] depend((P , B , S ) a ) 41\[\[?\]intend ( B ) ~40\[?\] person ( N ) \[?\] agt ( (B , N) , )'2 ?\] : ga((P , B , NI ~ , P ) ) 42 '\[\[?\] trade ( B ) ~b'40j\[?\] commodity ( N ) \[?\] obj ( (B , N ) , )$2\]wo((P , B , i ) s , P ) '\[ intend ( B ) /3'41 + P+12'43i\[?\]person ( N ) i \[?\] agt ( (s , g ) s )$ 2?\]! iga((P , B , Nl~ , P ) 44'\[trade ( B ) '/3'42 + P+11'\[?\] commodity ( N ) \[?\] obj ( (B , N ) , ) ~2\]wo((P , B , N) . ,P ) 45 \[ trade(B ) /3 , 844 + Q+23: commodity(N)\[?\]obj(<B , N ) s ) ' 2\]wo((P , B , N ) ,   , P ) 46 obj((B , g ) ~3 s2r/ , 45 t4 7' wo((P , B , N ) ~ , P ) tfl ,  6 , r / 45 + R + 4648\[wo((P , B , lg)s , P ) / 3 ,  6 , r/47\[?\]wo(P ) s3\]sem(<P , B , NL ) 49'wo(P ) ~3"0i48'50 , I sem((P , BN ) s)t/3 , 6 , r / , 048 + S+49t51t depend((P , B , S ) a ) ' o ~ , /3 ,  7 , 6 , rt , 0 t40+0+50't52 Jvp(?~B)'"ta ,  /3  , 7  , 6  , r / , 0i14+E+5153s (? , D , B ) a , /3 , v , e , , ,  0 52 '  , a , fl ,  7  , 5  , n ,  0  , 1 + A+53 , ? = iS 6 s e k i , katta\] , ? =\[ katta\]~54 , T ,   . . 0 o ~ = soseki(S ) $1 , fl = buy(B )$1 , 7= write((S , N ) p )$1 ? , 6= novel(N )$ 1 , e = agt((B , S ) ,  ) $2? , ? = ga(P ) $ 3 ,   , = obj((B , N)s)$2 , 0 = wo(P ) $ 3
