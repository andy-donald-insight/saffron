Conditions on Consistency of
Probabilistic Tree Adjoining Grammars *
Anoop Sarkar
Dept . of Computer and Information Science
University of Pennsylvania
200 South 33rd Street,
Philadelphia , PA 19104-6389 USA
a noop@linc , c is . upenn , edu
Abstract
Much of the power of probabilistic methods in modelling language comes from their ability to compare several derivations for the same string in the language  . An important starting point for the study of such cross-derivational properties is the notion of consistency  . The probability model defined by a probabilistic grammar is said to be consistent if the probabilities assigned to all the strings in the language sum to one  . 
From the literature on probabilistic on text-free grammars  ( CFGs )  , we know precisely the conditions which ensure that consistency is true for a given CFG  . This paper derives the conditions under which a given probabilistic Tree Adjoining Grammar  ( TAG ) can be shown to be consistent . It gives a simple algorithm for checking consistency and gives the formal justification for its correctness  . The conditions derived here can be used to ensure that probability models that use TAGs can be checked for deficiency  ( i . e . whether any probability mass is assigned to strings that cannot be generated  )  . 
1 Introduction
Much of the power of probabilistic methods in modelling language comes from their ability to compare several derivations for the same string in the language  . This cross-derivational power arises naturally from comparison of various derivational paths  , each of which is a product of the probabilities associated with each step in each derivation  . A common approach used to assign structure to language is to use a probabilistic grammar where each elementary rule * This research was partially supported by NSF grant  SBR8920230 and ARO grant DAAH0404-94-G-0426  . 
The author would like to thank Aravind Joshi , Jeff Rey-nat , Giorgio Satta , B . Srinivas , Fei Xia and the two anonymous reviewers for their valuable comments  . 
or production is associated with a probability.
Using such a grammar , a probability for each string in the language is computed  . Assuming that the probability of each derivation of a sentence is well-defined  , the probability of each string in the language is simply the sum of the probabilities of all derivations of the string  . In general , for a probabilistic grammar G the language of G is denoted by L  ( G )  . Then if a string v is in the language L ( G ) the probabilistic grammar assigns v some nonzero probability  . 
There are several cross-derivational properties that can be studied for a given probabilistic grammar formalism  . An important starting point for such studies is the notion of consistency  . The probability model defined by a probabilistic grammar is said to be consistent if the probabilities assigned to all the strings in the language sum to  1  . That is , if Pr defined by a probabilistic grammar , assigns a probability to each string v6E * , where Pr(v ) = 0 if v ~ L(G ) , then
Pr(v ) = i(i ) veL(G )
From the literature on probabilistic on text-free grammars  ( CFGs ) we know precisely the conditions which ensure that  ( 1 ) is true for a given CFG . This paper derives the conditions under which a given probabilistic TAG can be shown to be consistent  . 
TAGs are important in the modelling of natural language since they can be easily lexicalized  ; more over the trees associated with words can be used to encode argument and adjunct relations in various syntactic environments  . This paper assume some familiarity with the TAG formalism  . ( Joshi , 1988) and ( Joshi and Schabes ,  1992 ) are good introductions to the formalism and its linguistic relevance  . TAGs have structure grammars and dependency grammars  ( Rambow and Joshi ,  1995 ) and can handle ( nonprojective ) long distance dependencies . 
Consistency of probabilistic TAGs has practical significance for the following reasons : ? The conditions derived here can be used to ensure that probability models that use 
TAGs can be checked for deficiency.
? Existing EM based estimation algorithms for probabilistic TAGs assume that the property of consistency holds  ( Schabes ,  1992) . EM based algorithms begin with an initial ( usually random ) value for each parameter . If the initial assignment causes the grammar to be inconsistent  , then iterative reestimation might converge to an inconsistent grammar  1  . 
? Techniques used in this paper can be used to determine consistency for other probability models based on TAGs  ( Carroll and
Weir , 1997).
2 Notation
In this section we establish some notational conventions and definitions that we use in this paper  . Those familiar with the TAG formalism only need to give a cursory glance through this section  . 
A probabilistic TAG is represented by ( N , E ,  2: , A , S , ?) where N , E are , respectively , nonterminal and terminal symbols . 2:U , 4 is a set of trees termed as elementary trees . We take V to be the set of all nodes in all the elementary trees  . For each leaf AEV , label(A ) is an element from EU e , and for each other node A , label(A ) is an element from N . S is an element from N which is a distinguished start symbol  . 
The root node A of every initial tree which can start a derivation must have label  ( A ) = S . 
2: axetermed initial trees and , 4 are auxiliary trees which can rewrite a tree node A E V  . 
This rewrite step is called adjunction . ? is a function which assigns each adjunction with a probability and denotes the set of parameters  1Note that for CFGs it has been shown in ( Chaudhari et al , 1983; S~nchez and Bened ~ ,  1997 ) that insideoutside reestimation can be used to avoid inconsistency  . 
We will show later in the paper that the method used to show consistency in this paper precludes a straightforward extension of that result for TAGs  . 
in the model . In practice , TAGs also allow a leaf nodes A such that label ( A ) is an element from N . Such nodes A are rewritten with initial trees from I using the rewrite step called substitut ion  . Except in one special case , we will not need to treat substitution as being distinct from adjunction  . 
Fort E2: U . 4 ,  `4 ( t ) are the nodes in tree t that can be modified by adjunction  . For label ( A ) EN we denote Adj ( label ( A ) ) as the set of trees that can adjoin at node A E V  . 
The adjunction of t into NEV is denoted by N~-~ t  . No adjunction at NEV is denoted by N~n il . We assume the following properties hold for every probabilistic TAGG that we consider :  1  . G is lexicalized . There is at least one leaf node a that lexicalizes each elementary tree  , i . e . aEE . 
2 . Gisproper . For each NEV , ?(g~-~nil)+~?(g~-+t ) = 1t . 

Adjunction is prohibited on the foot node of every auxiliary tree  . This condition is imposed to avoid unnecessary ambiguity and can be easily relaxed  . 
There is a distinguished nonlexicalized initial tree T such that each initial tree rooted by a node A with label  ( A ) = S substitutes into T to complete the derivation  . This ensures that probabilities assigned to the input string at the start of the derivation are wellformed  . 
We use symbols S,A,B, .   .   . to range over V , symbols a , b , c, .   . , to range over E . We use tl , t2, .   .   , to range over IUA and e to denote the empty string  . We use Xito range over all i nodes in the grammar  . 
3 Applying probability measures to
Tree Adjoining Languages
To gain some intuition about probability assignments to languages  , let us take for example , a language wellknown to be a tree adjoining language : 
L ( G ) = anbncndnln > 1 tion ? to assign any probability distribution to the strings in L  ( G ) and then expect hat we can assign appropriate probabilites to the adjunctions in G such that the language generated by G has the same distribution as that given by ?  . However a function ? that grows smaller by repeated multiplication as the inverse of an exponential function cannot be matched by any TAG because of the constant growth property of TAGs  ( see ( Vijay-Shanker ,  1987) , p .  104) . An example of such a function ? is a simple Poisson distribution  ( 2 )  , which in fact was also used as the counterexample in  ( Booth and Thompson , 1973) for CFGs , since CFGs also have the constant growth property . 
1? ( anbncndn ) = e.n!(2)
This shows that probabilistic TAGs , like CFGs , are constrained in the probabilistic languages that they can recognize or learn  . As shown above , a probabilistic language can fail to have a generating probabilistic TAG  . 
The reverse is also true : some probabilistic TAGs  , like some CFGs , fail to have a corresponding probabilistic language  , i . e . they are not consistent . There are two reasons why a probabilistic TAG could be inconsistent : " dirty " grammars  , and destructive or incorrect probability assignments  . 
"Dirty " grammars . Usually , when applied to language , TAGs are lexicalized and so probabilities assigned to trees are used only when the words anchoring the trees are used in a derivation  . However , if the TAG allows nonlexicalized trees , or more precisely , auxiliary trees with no yield , then looping adjunctions which never generate a string are possible  . However , this can be detected and corrected by a simple search over the grammar  . Even in lexicalized grammars , there could be some auxiliary trees that are assigned some probability mass but which can never adjoin into another tree  . 
Such auxiliary trees are termed unreachable and technique similar to the ones used in detecting unreachable productions in CFGs can be used here to detect and eliminate such trees  . 
Destructive probability assignments.
This problem is a more serious one , and is the main subject of this paper . Consider the probabilistic TAG shown in (3) 2 . 
tl~1t2 $2! S312-o?(S1t2) = 1 . o ? ($2~+ t2) = 0 . 99-+ nil ) = 0 . 01 ?($ 3~-+ t2) = 0 . 98 ?($ 3 ~ nd ) = 0 . 02  ( 3 ) Consider a derivation in this TAG as a generative process  . It proceeds as follows : node $1 intl is rewritten as t2 with probability 1  . 0 . Node $2 in t2 is 99 times more likely than not to be rewritten as t2 itself , and similarly node $3 is 49 times more likely than not to be rewritten as t2  . 
This however , creates two more instances of $2 and $3 with same probabilities . This continues , creating multiple instances of t2 at each level of the derivation process with each instance of  t2 creating two more instances of itself . The grammar itself is not malicious ; the probability assignments are to blame . It is important onote that inconsistency is a problem even though for any given string there are only a finite number of derivations  , all halting . Consider the probability mass function ( pmf ) over the set of all derivations for this grammar . An inconsistent grammar would have apmf which assigns a large portion of probability mass to derivations that are non-terminating  . This means there is a finite probability the generative process can enter a generation sequence which has a finite probability of non -termination  . 
4 Conditions for Consistency
A probabilistic TAGG is consistent if and only if : 
Pr ( v )  = 1  ( 4 ) ve LCG ) where Pr ( v ) is the probability assigned to a string in the language  . If a grammar G does not satisfy this condition , G is said to be inconsistent . 
To explain the conditions under which a probabilistic TAG is consistent we will use the TAG  2The subscripts are used as a simple notation to uniquely refer to the nodes in each elementary tree  . They are not part of the node label for purposes of adjunction  . 
1166 in (5) as an example.
tl~t2?(A1~-~t2) = 0.8 ?( A1~-+ nil ) = 0.2
B1A *

I a 2
B*a3?(A2~-~t2)=0 . 2?(B2~-~t3)=0 . 1 ?( A2~+ nil ) = 0 . 8 ?( B2~nil ) = 0 . 9?(B1~+t3)=0 . 2?(B1~-+nil ) = 0 . 8 ?( A3~-~t2)=0 . 4 ?( A3~-~nil ) = 0 . 6 (5) From this grammar , we compute a square matrix A4 which of size IVI , where V is the set of nodes in the grammar that can be rewritten by adjunction  . Each AzIij contains the expected value of obtaining node Xj when node Xi is rewritten by adjunction at each level of a TAG derivation  . We call Ad the stochastic expectation matrix associated with a probabilistic 

To get A4 for a grammar we first write a matrix P which has IVI rows and IIUA\[columns  . 
An element Pij corresponds to the probability of adjoining tree tj at node X i  , i . e . ?(Xi ~'+ tj ) 3 . 
tlt2
A 10 0.8
A2 0 0.2
P = BI00
A 30 0.4
B200t 30.2
We then write a matrix N which has\[IUA\[ rows and IV\[columns  . An element Nij is 1 . 0 if node X j is a node in tree ti . 
N =
A1A2B1A3B2t1\[1 . 00000\]t2\[01 . 0 1 . 0 1 . 00\]t 30000 1 . 0 Then the stochastic expectation matrix A4 is simply the product of these two matrices . 
3Note that P is not a row stochastic matrix . This is an important difference in the construction of  . h4 for TAGs when compared to CFGs . We will return to this point in ?5 . 






A1 A2 B1 A3 B200 . 8 0 . 8 0 . 8 0 0 0 . 2 0 . 2 0 . 2 0 0 0 0 0 0 . 2 0 0 . 4 0 . 4 0 . 4 0 0 0 0 0 0 . 1 By inspecting the values of A4 in terms of the grammar probabilities indicates that  . h4ij contains the values we wanted , i . e . expectation of obtaining node Aj when node Ai is rewritten by adjunction at each level of the TAG derivation process  . 
By construction we have ensured that the following theorem from  ( Booth and Thompson , 1973) applies to probabilistic TAGs . A formal justification for this claim is given in the next section by showing a reduction of the TAG derivation process to a multitype Galton -Watson branching process  ( Harris ,  1963) . 
Theorem 4 . 1 A probabilistic grammar is consistent if the spectral radius p  ( A4 )  < 1 , where , h , 4 is the stochastic expectation matrix computed from the grammar  . ( Booth and Thompson , 1973; Soule ,  1974 ) This theorem provides a way to determine whether a grammar is consistent  . All we need to do is compute the spectral radius of the square matrix  A4 which is equal to the modulus of the largest eigenvalue of   . If this value is less than one then the grammar is consistent  4  . Computing consistency can by pass the computation of the eigenvalues for  A4 by using the following theorem by Ger~gor in ( see ( Horn and Johnson , 1985; Wether ell ,  1980)) . 
Theorem 4 . 2 For any square matrix . h4, p( . M )  <  1 if and only if there is an n > 1 such that the sum of the absolute values of the elements of each row of  . Mn is less than one . Moreover , any n ' > n also has this property . ( GerSgor in , see ( Horn and Johnson , 1985;
We therell ,  1980 ) )  4The grammar may be consistent when the spectral radius is exactly one  , but this case involves many special considerations and is not considered in this paper  . In practice , these complicated tests are probably not worth the effort  . See ( Harris ,  1963 ) for details on how this special case can be solved  . 

This makes for a very simple algorithm to check consistency of a grammar  . We sum the values of the elements of each row of the stochastic expectation matrix  A4 computed from the grammar . If any of the row sums are greater than one then we compute  A42  , repeat the test and compute :~422 if the test fails , and so on until the test succeeds5 . The algorithm does not halt if p(A4)_>1 . In practice , such an algorithm works better in the average case since computation of eigenvalues i more expensive for very large matrices  . An upper bound can be set on the number of iterations in this algorithm  . Once the bound is passed , the exact eigenvalues can be computed . 
For the grammar in ( 5 ) we computed the following stochastic expectation matrix :  0   0  . 8 0 . 8 0 0 . 2 0 . 2
A4 = 0000 0.4 0.4 0 0 0
The first row sum is 2.4.

Since the sum of each row must be less than one , we compute the power matrix , ~ v/2 . However , the sum of one of the rows is still greater than 1  . Continuing we compute A422 . 
j ~ 2200 . 1728 0 . 1728 0 . 1728 0 . 0688 0 0 . 0432 0 . 0432 0 . 0432 0 . 0172 0 0 0 0 0 . 0002 0 0 . 0864 0 . 0864 0 . 0864 0 . 0344 0 0 0 0 0 . 0001 This time all the row sums are less than one , hence p ( , ~4) < 1 . So we can say that the grammar defined in ( 5 ) is consistent . We can confirm this by computing the eigenvalues for  A4 which are 0  ,  0 ,  0 . 6, 0 and 0 . 1, allless than 1 . 
Now consider the grammar ( 3 ) we had considered in Section 3 . The value of . ? 4 for that grammar is computed to be: $1   s2   s3   slI0   10   10\]   . A ~ (3):$200 . 99 0 . 99 $3 0 0 . 98 0 . 9 8 S We compute A422 and subsequently only successive powers of 2 because Theorem 4  . 2 holds for any n ' > n . 
This permits us to use a single matrix at each step in the algorithm  . 
The eigenvalues for the expectation matrix M computed for the grammar  ( 3 ) are 0 ,  1 . 97 and 0 . The largest eigenvalue is greater than 1 and this confirms ( 3 ) to be an inconsistent grammar . 
5 TAG Derivations and Branching

To show that Theorem 4 . 1 in Section 4 holds for any probabilistic TAG , it is sufficient o show that the derivation process in TAGs is a Galton- 
Watson branching process.
A Galton-Watson branching process ( Harris ,  1963 ) is simply a model of processes that have objects that can produce additional objects of the same kind  , i . e . recursive processes , with certain properties . There is an initial set of objects in the 0th generation which produces with some probability a first generation which in turn with some probability generates a second  , and so on . We will denote by vectors Z0, Z1, Z2, .   .   . 
the 0th , first , second, . . . generations . There are two assumptions made about Z0, Z1, Z2, .   .   .  :  . The size of the nth generation does not influence the probability with which any of the objects in the  ( n + 1 ) -th generation is produced . In other words , Z0, Z1, Z2, .   .   . 
form a Markov chain.
. The number of objects born to a parent object does not depend on how many other objects are present at the same level  . 
We can associate a generating function for each level Zi  . The value for the vector Zn is the value assigned by the nth iterate of this generating function  . The expectation matrix A4 is defined using this generating function . 
The theorem attributed to Galton and Watson specifies the conditions for the probability of extinction of a family starting from its  0th generation , assuming the branching process represents a family tree  ( i . e , respecting the conditions outlined above ) . The theorem states that p( . ~4 ) <1 when the probability of extinction is tlt2 ( 0 ) t2 ( 0 ) t3 ( 1 ) t2 ( 1 . 1)
IIt2 (1.1) t3(o)
BIA
A2B2A
B1ABa3al
A3 a2 Ba3
II as AS
BIA
II , ~ as
I level 0 level 1 level 2 level 3 level 4 (6) . s ( ~ ) The assumptions made about the generating process intuitively holds for probabilistic TAGs  . 
(6) , for example , depicts a derivation of the string a2a2a2a2a3a3al by a sequence of adjunctions in the grammar given in  ( 5 )  6 . The parse tree derived from such a sequence is shown in Fig  .  7 . In the derivation tree (6) , nodes in the trees at each leveliaxe rewritten by adjunction to produce a level i +  1  . There is a final level 4 in ( 6 ) since we also consider the probability that a node is not rewritten further  , i . e . Pr(A~-~nil ) for each node A . 
We give a precise statement of a TAG derivation process by defining a generating function for the levels in a derivation tree  . Each level i in the TAG derivation tree then corresponds to Zi in the Mi in the Maxkov chain of branching  pro-6The numbers in parentheses next to the tree names are node addresses where each tree has adjoined into its parent  . Recall the definition of node addresses in
Section 2.
cesses . This is sufficient o justify the use of Theorem 4 . 1 in Section 4 . The conditions on the probability of extinction then relates to the probability that TAG derivations for a probabilistic TAG will not recurse in finitely  . Hence the probability of extinction is the same as the probability that a probabilistic TAG is consistent  . 
For each XjEV , where V is the set of nodes in the grammar where adjunction can occur  , we define the k-argument adjunction generating \] unction over variables i  ,   .   .   . , Sk corresponding to the knodes in V . 
gj(sl , ..., 8k ) =
Ete Adj ( Xj ) uniQ?(xjt) . k ?*) where , rj(t ) = 1 iff node X j is in tree t , rj(t ) = 0 otherwise . 
For example , for the grammar in ( 5 ) we get the following adjunction generating functions taking the variables l  , s2 ,  83 ,  84 , 85 to represent the nodes A1 , A2 , B 1 , A3 , B2 respectively . 
g1(81, .   .   .   , 85 )  = ? ( A1 ~" ~ t2 ) "82"83"s4+? ( A1~--~nil ) g2 ( 81 ,  . .  .   , 8 ~) = ?( A2~-~t2)?82"83"s4+?(A2~--~nil)g~(81 ,  . .  . , 85) = ?( B1~-~t3)"85+?(B1~nil)g4(81, .   .   . , 85) = ?( A3~-+t2)"82"83"844 . ?( A3~-+ nil)g5 (81, . . . , s ~) = ?( B2~-~t3)"ss+?(B2~-~nil)
The nth level generating function
Gn(sl , ..., sk ) is defined recursively as follows.
G0(81, . . . , Sk ) = 81
Gl(sl , ..., sk ) = gl(sl , ..., Sk)
G , ( sl , .   .   . , sk ) = G,-l\[gl(sl , .   .   . , sk ), .   .   . , gk(sl , . . . ,Sk ) \] For the grammar in ( 5 ) we get the following level generating functions . 
O0(sl , .   .   . , 85) = 81 = ?( A1~-+ t2)"se . 83"84+?(A1~-+nil ) = 0 . 8  . s2 . s3 . s4 + 0 . 2
G2(sl , .   .   . , 85) = ?( A2~-+ t2)\[g2(sy, .   .   . , 85)\]\[g3(81, . . . , 85)\]\[g4(81, .   .   . , 85)\]-\[-?(A2~nil ) 222222 = 0 . 0882838485 + 0 . 03828384 + 0 . 0482838485 + 0 . 18828384-t-0 . 04 s 5+ 0 . 196 Examining this example , we can express Gi(s1 ,   .   .   . , Sk ) as a sum Di(sl , .   .   . , Sk ) + Ci , where Ci is a constant and Di ( . ) is a polynomial with no constanterms . A probabilistic TAG will be consistent if these recursive qua-tions terminate  , i . e . iff limi + ooDi(sl , .   .   .   , 8k )  --+  0 We can rewrite the level generation functions in terms of the stochastic expectation matrix Ad  , where each element mi , j of . A4 is computed as follows ( cf . ( Booth and Thompson , 1973)) . 
Ogi (81, .   .   . , 8k)mi , j=08 jsl, . . . ,sk=l ( 8 ) The limit condition above translates to the condition that the spectral radius of  34 must be less than 1 for the grammar to be consistent . 
This shows that Theorem 4 . 1 used in Section 4 to give an algorithm to detect inconsistency in a probabilistic holds for any given TAG  , hence demonstrating the correctness of the algorithm  . 
Note that the formulation of the adjunction generating function means that the values for ?  ( X ~ 4 n i l ) for all XEV do not appear in the expectation matrix  . This is a crucial difference between the test for consistency in TAGs as compared to CFGs  . For CFGs , the expectation matrix for a grammar G can be interpreted as the contribution of each nonterminal to the derivations for a sample set of strings drawn from L  ( G )  . Using this it was shown in ( Chaudhari et al , 1983) and ( S?nchez and Bened ~ ,  1997 ) that a single step of the insideoutside algorithm implies consistency for a probabilistic CFG  . However , in the TAG case , the inclusion of values for ? ( X ~ -+ nil )   ( which is essen-timif we are to interprethe expectation matrix in terms of derivations over a sample set of strings  ) means that we cannot use the method used in ( 8 ) to compute the expectation matrix and furthermore the limit condition will not be convergent  . 
6 Conclusion
We have shown in this paper the conditions under which a given probabilistic TAG can be shown to be consistent  . We gave a simple algorithm for checking consistency and gave the formal justification for its correctness  . The result is practically significant for its applications in checking for deficiency in probabilistic TAGs  . 

T . L . Booth and R . A . Thompson .  1973 . Applying probability measures to abstract languages  . IEEE Transactions on Computers , C-22(5):442-450 , May . 
J . Carroll and D . Weir .  1997 . Encoding frequency information in lexicalized grammars  . In Proc .   5th Int'l Workshop on Parsing Technologies IWPT-97  , Cambridge , Mass . 
R . Chaudhari , S . Pham , and O . N . Garcia .  1983 . Solution of an open problem on probabilistic grammars  . 
IEEE Transactions on Computers , C-32(8):748-750,

T . E . Harris .  1963 . The Theory of Branching Processes . 
Springer-Verlag , Berlin.
R . A . Horn and C . R . Johnson .  1985 . Matrix Analysis . 
Cambridge University Press , Cambridge.
A . K . Joshi and Y . Schabes .  1992 . Tree-adjoining ram-mar and lexicalized grammars . In M . Nivat and A . Podelski , editors , Tree automat and languages , pages 409-431 . Elsevier Science . 
A . K . Joshi .  1988 . An introduction to tree adjoining grammars . In A . Manaster-Ramer , editor , Mathematics of Language . John Benjamins , Amsterdam . 
O . Rainbow and A . Joshi .  1995 . A formal look at dependency grammars and phrase -structure grammars  , with special consideration of word-order phenomena  . 
In Leo Wanner , editor , Current Issues in Meaning-
Text Theory . Pinter , London.
J . -A . S?nchez and J . -M . Bened\[ .  1997 . Consistency of stochastic on text-free grammars from probabilistic estimation based on growth transformations  . IEEE Transactions on Pattern Analysis and Machine Intelligence  ,  19(9):1052-1055 , September . 
Y . Schabes .  1992 . Stochastic lexicalized tree-adjoining grammars . In Proc . of COLING'92 , volume 2 , pages 426-432 , Nantes , France . 
S . Soule .  1974 . Entropies of probabilistic grammars . Inf . 
Control , 25:55-74.
K . Vijay-Shanker .  1987 . A Study of Tree Adjoining Grammars . Ph . D . thesis , Department of Computer and Information Science , University of Pennsylvania . 
C . S . We therell .  1980 . Probabilistic languages : A review and some open questions  . Computing Surveys , 12(4):361-379 . 

