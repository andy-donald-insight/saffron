The Complexity of Recognition
of Linguistically Adequate Dependency Grammars
Peter Neuhaus
Norbert Briiker
Computational Linguistics Research Group
Freiburg University , Friedrichstrage 50
D-79098 Freiburg , Germany
email : neuhaus , nobi@coling.uni-freiburg.de

Results of computational complexity exist for a wide range of phrase structure-based grammar formalisms  , while there is an apparent lack of such results for dependency-based formalisms  . We here adapt a result on the complexity of ID/LP -grammars to the dependency framework  . Contrary to previou studies on heavily restricted ependency grammars  , we prove that recognition ( and thus , parsing ) of linguistically adequate dependency grammars is ~A/T'-complete  . 
1 Introduction
The introduction of dependency grammar ( DG ) into modern linguistics is marked by Tesni~re ( 1959 )  . His conception addressed id actic goals and , thus , did not aim at formal precision , but rather at an intuitive understanding of semantically motivate dependency relations  . An early formalization was given by Gaifman ( 1965 )  , who showed the generative capacity of DG to be ( weakly ) equivalent to standard contextfree grammars . 
Given this equivalence , interest in DG as a linguistic framework diminished considerably  , although many dependency grammarians view Gaifman 's conception as an unfortunate one  ( cf . Section 2) . To our knowledge , there has been no other formal study of DG . This is reflected by a recent study ( Lombardo & Lesmo ,  1996) , which applies the Earley parsing technique ( Earley , 1970) to DG , and thereby achieves cubic time complexity for the analysis of DG  . In their discussion , Lombardo & Les mo express their hope that slight increases in generative capacity will correspond to equally slight increases in com-putational complexity  . It is this claim that we challenge here . 
After motivating nonprojective analyses for DG , we investigate various variants of DG and identify the separation of dominance and precedence as a major part of current DG theorizing  . Thus , no current variant of DG ( not even Tesni ~ re's original formulation ) is compatible with Gaifman's conception , which seems to be motivated by formal considerations only  ( viz . , the proof of equivalence ) . Section 3 advances our proposal , which cleanly separates dominance and precedence rlations  . This is illustrated in the fourth section , where we give a simple encoding of an A/P -complete problem in a discontinuous DG  . Our proof of A/79-completeness , however , does not rely on discontinuity , but only requires unordered trees . 
It is adapted from a similar proof for unordered contextfree grammars  ( UCFGs ) by Barton ( 1985 )  . 
2 Versions of Dependency Grammar
The growing interest in the dependency concept ( which roughly corresponds to the O-roles of GB , subcategorization in HPSG , and the socalle domain of locality of TAG ) again raises the issue whether nonlexical categories are necessary for linguistic analysis  . After reviewing several proposals in this section  , we argue in the next section that word order -- the description of which is the most prominent difference between PSGs and DGs--can adequately be described without reference to nonlexical categories  . 
Standard PSG trees are projective , i . e . , no branches cross when the terminal nodes are projected onto the input string  . In contrast oPSG approaches , DG requires nonprojective analyses . As DGs are restricted to lexical nodes , one cannot , e . g . , describe the socalled unbounded ependencies without giving up projectivity  . First , the categorial approach employing partial constituents  ( Huck , 1988; Hepple , 1990) is not available , since there are no phrasal categories . Second , the coin-dexing ( Haegeman , 1994) or structure-sharing ( Pollard & Sag , 1994) approaches are not available , since there are no empty categories . 
Consider the extracted NP in " Beans , Iknow John likes "( cf . also to Fig . 1 in Section 3) . A projective tree would require " Beans " to be connected to either " I " or " know "- none of which is conceptually directly related to " Beans  "  . It is " likes " that determines syntactic fea-it . The only connection between " know " and " Beans " is that the finite verb allows the extraction of " Beans "  , thus defining order restrictions for the NP . This has led some DG variants to adopt a general graph structure with multiple heads instead of trees  . We will refer to DGs allowing nonprojective analyses as discontinuous DGs  . 
Tesni~re ( 1959 ) devised a bipartite grammar theory which consists of a dependency omponent and a translation component  ( ' translation'used in a technical sense denoting a change of category and grammatical function  )  . The dependency omponent defines four main categories and possible dependencies between them  . What is of interest here is that there is no mentioning of order in Tesni Sre's work  . Some practition eers of DG have allowed word order as a marker for translation  , but they do not prohibit nonprojective trees . 
Gaifman ( 1965 ) designed his DG entirely analogous to contextfree phrase structure grammars  . Each word is associated with a category , which functions like the nonterminals in CFG . He then defines the following rule format for dependency grammars :  ( 1 ) X ( Y ,   ,   .   .   . , Y  ~ , , , Y ~+ I , .   .   .   , Y , , ) This rule states that a word of category X governs words of category  Y1  ,  . . . , Yn which occur in the given order . 
The head ( the word of category X ) must occur between the ith and the ( i+1 ) -th modifier . The rule can be viewed as an ordered tree of depth one with node labels  . 
Trees are combined through the identification of the root of one tree with a leaf of identical category of another tree  . This formalization is restricted to projective trees with a completely specified order of sister nodes  . As we have argued above , such a formulation cannot capture semantically motivate dependencies  . 
2.1 Current Dependency Grammars
Today's DGs differ considerably from Gaifman's conception  , and we will very briefly sketch various order descriptions  , showing that DGs generally dissociate dominance and precedence by some mechanism  . All variants share , however , the rejection of phrasal nodes ( although phrasal features are sometimes allowed ) and the introduction of edge labels ( to distinguish different dependency relations )  . 
Meaning-Text Theory ( Mer5uk , 1988) assumes seven strata of representation . The rules mapping from the unordere dependency trees of surface-syntactic representations onto the annotated lexeme sequences of deep -morphological representations include global ordering rules which allow discontinuities  . These rules have not yet been formally specified ( Mel'5uk & Pertsov ,  1987 , p . 187 f ) , but see the proposal by Rambow & Joshi (1994) . 
Word Grammar ( Hudson , 1990) is based on general graphs . The ordering of two linked words is specified together with their dependency relation  , as in the proposition " object of verb succeeds it "  . Extraction is analyzed by establishing another dependency  , visitor , between the verb and the extracte e , which is required to precede the verb , as in " visitor of verb precedes it " . Resulting inconsistencies , e . g . in case of an extracted object , are not resolved , however . 
Lexicase ( Starosta ,  1988 ;  1992 ) employs complex feature structures to represent lexical and syntactic entities  . Its word order description is much like that of Word Grammar  ( at least at some level of abstraction )  , and shares the above inconsistency . 
Dependency Unification Grammar ( Hellwig ,  1988 ) defines a treelike data structure for the representation fsyntactic analyses  . Using morphosyntactic features with special interpretations  , a word defines abstract positions into which modifiers are mapped  . Partial orderings and even discontinuities can thus be described by allowing a modifier to occupy a position defined by some transitive head  . The approach cannot restrict discontinuities properly  , however . 
Slot Grammar ( McCord , 1990) employs a number of rule types , some of which are exclusively concerned with precedence  . So-called head/slot and slot/slot ordering rules describe the precedence in projective trees  , referring to arbitrary predicates overhead and modifiers  . Ex-tractions ( i . e . , discontinuities ) are merely handled by a mechanism built into the parser  . 
This brief overview of current DG flavors shows that various mechanisms  ( global rules , general graphs , procedural means ) are generally employed to lift the limitation to projective trees  . Our own approach presented below improves on these proposals because it allows the lexicalized and declarative formulation of precedence constraints  . The necessity of nonprojective analyses in DG results from examples like " Beans  ,   1 know John likes " and the restriction to lexical nodes which prohibits gap-threading and other mechanisms tied to phrasal categories  . 
3 A Dependency Grammar with Word
Order Domains
We now sketch a minimal DG that incorporates only word classes and word order as descriptional dimensions  . 
The separation of dominance and precedence presented here grew out of our work on German  , and retains the local flavor of dependency specification  , while at the same time covering arbitrary discontinuities  . It is based on a ( modal ) logic with model theoretic interpretation , which is presented in more detail in ( Br~ker ,  1997) . 
338 fknow//~,,,@xi~es ~)
Idld 2
Figure 1: Word order domains in " Beans , Iknow John likes "3 . 1 Order Specification Our initial observation is that DG cannot use binary precedence constraints as PSG does  . Since DG analyses are hierarchically flatter , binary precedence constraints result in inconsistencies  , as the analyses of Word Grammar and Lexicase illustrate  . In PSG , on the other hand , the phrasal hierarchy separates the scope of precedence restrictions  . This effect is achieved in our approach by defining word order domains assets of words  , where precedence restrictions apply only to words within the same domain  . Each word defines a sequence of order domains , into which the word and its modifiers are placed . 
Several restrictions are placed on domains . First , the domain sequence must mirror the precedence of the words included  , i . e . , words in a prior domain must precede all words in a subsequent domain  . Second , the order domains must be hierarchically ordered by set inclusion  , i . e . , be projective . Third , a domain ( e . g . , dl in Fig . l ) can be constrained to contain at most one partial dependency tree  . l We will write singleton domains as "_" , while other domains are represented by "-" . The precedence of words within domains is described by binary precedence rstrictions  , which must be locally satisfied in the domain with which they are associated  . Considering Fig . 1 again , a precedencerstriction for " likes " to precede its object has no effect  , since the two are in different domains . The precedence constraints are formulated as a binary relation " ~" over dependency labels  , including the special symbol " self " denoting the head  . 
Discontinuities can easily be characterized , since a word may be contained in any domain of ( nearly ) any of its transitive heads . If a domain of its direct head contains the modifier  , a continuous dependency results . If , however , a modifier is placed in a domain of some transitive head  ( as " Beans " in Fig . 1), discontinuities occur . Bound-ing effects on discontinuities are described by specifying that certain dependencies may not be crossed  . 2 For thet For details , cf . ( Br6ker , 1997) . 
2 German data exist that cannot be captured by the ( more common ) bounding of discontinuities by nodes of a certain purpose of this paper  , we need not formally introduce the bounding condition  , though . 
A sample domain structure is given in Fig . l , with two domains dl and d2 associated with the governing verb " know " ( solid ) and one with the embedded verb " likes " ( dashed )  . dl may contain only one partial dependency tree , the extracted phrase , d2 contains the rest of the sentence . Both domains are described by (2) , where the domain sequence is represented as "<<" .   d2 contains two precedence rstrictions which require that " know "  ( represented by self ) must follow the subject ( first precedence constraint ) and precede the object ( second precedence constraint )  . 
(2) __   << ---- . ( subject- . < self ), ( self--<object ) 3 . 2 Formal Description The following notation is used in the proof  . A lexicon Lez maps words from an alphabet E to word classes  , which in turn are associated with valencies and domain sequences  . The set C of word classes is hierarchically ordered by a subclass relation  ( 3 ) is accCxCA word w of class c inherits the valencies  ( and domain sequence ) from c , which are accessed by (4) w . valencies A valency ( b , d , c ) describes a possible dependency relation by specifying a flag bindicating whether the dependency may be discontinuous  , the dependency named ( a symbol ) , and the word class cEC of the modifier . A word h may governa word m in dependency d if h defines a valency  ( b , d , c ) such that ( misaoc ) and m can consistently be inserted into a domain of h  ( for b = - ) or a domain of a transitive head of h ( for b = + )  . This condition is written as (5) governs(h , d,m)
ADG is thus characterized by (6) G = ( Lex , C , is ac , E ) The language L ( G ) includes any sequence of words for which a dependency tree can be constructed such that for each word h governing a word m in dependency d  , governs ( h , d , m ) holds . The modifier of hindependency disaccessed by ( 7 ) h . mod(d ) category . 
339 4 The complexity of DG Recognition
Lombardo & Lesmo (1996, p . 728 ) convey their hope that increasing the flexibility of their conception of DG will "  .   .   . imply the restructuring of some parts of the recognizer  , with a plausible increment of the complexity " . 
We will show that adding a little ( linguistically required ) flexibility might well ren de recognition A/P -complete  . 
To prove this , we will encode the vertex cover problem , which is known to be A/P-complete , in a DG . 
4.1 Encoding the Vertex Cover Problem in
Discontinuous DG
A vertex cover of a finite graph is a subset of its vertices such that  ( at least ) one endpoint of every edge is a member of that set  . The vertex cover problem is to decide whether for a given graph there exists a vertex cover with at most k elements  . The problem is known to be A/7~-complete ( Garey & Johnson ,  1983 , pp . 53-56) . 
Fig . 2 gives a simple example where c , disa vertex cover . 
ab
X d
Figure 2: Simple graph with vertex cover c,d.
A straightforward encoding of a solution in the DG formalism introduced in Section  3 defines a root words of class S with k valencies for words of class O  . O has IW l subclasses denoting the nodes of the graph  . An edge is represented by two linked words ( one for each endpoint ) with the governing word corresponding to the node included in the vertex cover  . The subordinated word is assigned the class R , while the governing word is assigned the subclass of O denoting the node it represents  . The latter word classes define a valency for words of class R  ( for the other endpoint ) and a possibly discontinuous valency for another word of the identical class  ( representing the endpoint of another edge which is included in the vertex cover  )  . This encoding is summarized in Table 1 . 
The input string contains an initials and for each edge the words representing its endpoints  , e . g . " sacc dadb-dcb " for our example . If the grammar allows the construction of a complete dependency tree  ( cf . Fig . 3 for one solution ) , this encodes a solution of the vertex cover problem  . 

Illllllllb
Iltllllll I
IIIIIIIIIII$accdadbdcb Figure 3: Encoding a solution to the vertex cover problem from Fig  .  2 . 
4.2 Formal Proof using Continuous DG
The encoding outlined above uses nonprojective trees  , i . e . , crossing dependencies . In anticipation of counterarguments such as that the present e dependency grammar was just too powerful  , we will present he proof using only one feature supplied by most DG formalisms  , namely the free order of modifiers with respect o their head  . Thus , modifiers must be inserted into an order domain of their head  ( i . e . , no+mark invalencies ) . This version of the proof uses a slightly more complicated encoding of the vertex cover problem and resembles the proof by Barton  ( 1985 )  . 
Definition 1 ( Measure)
Let II ? II be a measure for the encoded input length of a computational problem  . We require that if S is a set or string and kEN then IS l > k implies Il Sll___Ilkll and that for any tuple  I1  ( ""  , z ,   .   . ") 11-Ilzll holds .  <
Definition 2 ( Vertex Cover Problem)
A possible instance of the vertex cover problem is a triple  ( V , E , k ) where ( V , E ) is a finite graph and IvI>kN . The vertex cover problem is the set VC of all instances  ( V , E , k ) for which there exists a subset V ' C_V and a function f : E - - - > VI such that IV ' l <_k and V  ( Vm , Vn ) EE : f((vm , Vn )) EVm , V n .  <1
Definition 3 ( DG recognition problem )
A possible instance of the DG recognition problem is a tuple  ( G , a ) where G = ( Lex , C , is ac ,  ~ ) is a dependency grammar as defined in Section 3 and a EE + . The DG recognition problem DGR consists of all instances  ( G , a ) such that a EL(G ) .   <1 For an algorithm to decide the VC problem consider a data structure representing the vertices of the graph  ( e . g . , a set ) . We separate the elements of this data structure S  ( -  , markl , O ) ,  (-  , mark 2 , 0) -- ( self-~mark 1) , ( mark1- . < mark2)A is a c0(- , unmrk , R ) ,  (+ , same , A ) = ( unmrk-Ksame ) , ( self-4 same ) B is a cO(- , unmrk , R ) ,  (+ , same , B ) = ( unmrk--<same) , ( self- . < same ) (7 is a cO(- , unmrk , R ) ,  (+ , same , C ) ~ ( unmrk--4 same) , ( self-4 same ) D is a cO(- , unmrk , R ) ,  (+ , same , D ) -( unmrk-- . < same ), ( self-~same )
R--\[word\[classesIssaA , RbB , RcC , RdD , R Table 1: Word classes and lexicon for vertex cover problem from Fig  . 2 into the ( maximal ) vertex cover set and its complement set . Hence , one endpoint of every edge is assigned to the vertex cover  ( i . e . , it is marked ) . Since ( at most ) all IEI edges might share a common vertex , the data structure has to be a multiset which contains IEI copies of each vertex  . Thus , marking the IVI-k complement vertices actually requires marking IVI-ktimes IE\[identical vertices  . This will leave ( k-1 ) * IE I unmarked vertices in the input structure . To achieve this algorithm through recognition of a dependency grammar  , the marking process will be encoded as the filling of appropriate valencies of a words by words representing the vertices  . 
Before we prove that this encoding can be generated in polynomial time we show that : 
Lemma 1
The DG recognition problem is in the complexity class 
Alp.\[\]
Let G = ( Lex , C , is ac , Z ) and a E\]E+ . We give a nondeterministic algorithm for deciding whether a =  ( Sl- . - sn ) is in L(G ) . Let H be an empty set initially : 1 . Repeat until IHI = Iol(a ) i . For every SiEOr choose a lexicon entry ciE Lex  ( si )  . 
ii . From the ci choose one word as the head h0.
iii . Let H := ho and M := ciliE\[1, IO rl\]\H . 
( b ) Repeat until M = 0: i . Choose a head hEH and a valency ( b , d , c ) Eh . valencies and a modifier mE

ii . If governs ( h , d , m ) holds then establish the dependency relation between h and them  , and add m to the set H . 
iii . Removem from M.
The algorithm obviously is ( nondeterministically ) polynomial in the length of the input . Given that ( G , g ) EDGR , a dependency tree covering the whole input exists and the algorithm will be able to guess the dependents of every head correctly  . If , conversely , the algorithm halts for some input ( G , or ) , then there necessarily must be a dependency tree rooted in hocompletely covering a  . Thus , ( G , a ) EDGR .  \[\]
Lemma 2
Let ( V , E , k ) be a possible instance of the vertex cover problem  . Then a grammar G(V , E , k ) and an input a(V , E , k ) can be constructed in time polynomial in
II(v , E , k ) II such that ( V , E , k ) EVC ? : : : : : v(G(V , E , k ) , a(V , E , k )) EDGR\[\]For the proof , we first define the encoding and show that it can be constructed in polynomial time  . Then we proceed showing that the equivalence claim holds  . The set of classes is G = a efS , R , UUH die\[1 , IEI\]UU ~ , ?1ie\[1 , IVI\] . In the is achierarchy the classes Uish are the superclass U  , the classes V ~ the superclass R . 
Valencies are defined for the classes according to Table  2  . 
Furthermore , we define E = dee SUvii/E\[1 , IV l\] . 
The lexicon Lex associates words with classes as given in Table  2  . 
We set
G(V , E , k ) = clef(Lex , C , is ac , ~) and a(V , E , k ) = defsVl''"Vl "'" yIV\["""VlV ~
IEIIEI
For an example , cf . Fig .   4 which shows a dependency tree for the instance of the vertex cover problem from Fig  .  2 . The two dependencies Ul and u2 represent the complement of the vertex cover . 
It is easily seen 3 that \[\[( G(V , E , k) , a(V , E , k ))\[\[ is polynomial in\[\[V\[\[ , \[\[ E\[\[ and k . From\[El_>k and Definition 1 it follows that H ( V , E , k)\[I>_\[IE\]\[_>\]\[k\[\[_>k . 
3The construction requires 2 ?\[ V\[+\[El + 3 word classes , IV\[+ 1 terminals in at most \[ El + 2 readings each . S defines IV\[+k ? IE\[-kvalencies , Uidefines \[ E\[-1valencies . The length of a is IV\[?\[E\[+1 . 
341 word class valencies
Vvi?V Vi is acR
Vvi?V Ui is acU(-, rz,V/),-- . , (-, rlEl_l , V/)
Vei EE Hi
S (-, u , , u), .   .   .   ,  ( -  , u , v , _ , , v ) ,  ( -  , hi , Hi ) ,  - ' -  ,  ( -  , hie I , HIEI ) ,  ( -  , n , R ) ,  ? ? ?  ,  ( -  , r(k- , ) l ~ l , R )
I order I = word\]="i--word classes U . ~UHjl3vm , v . ? v : ej = ( vm , v , , ) ^ ss Table 2: Word classes and lexicon to encode vertex cover problem $ aaa abbbb Figure  4: Encoding a solution to the vertex cover problem from Fig  .  2 . 
Hence , the construction of ( G(V , E , k ) , a(V , E , k ) ) can be done in worst-case time polynomial in II ( V , E , k)ll . 
We next show the equivalence of the two problems.
Assume ( V , E , k ) ? VC : Then there exists a subset V ' C_V and a function f : E--+V ' such that IV ' l <_k and V  ( vm , v , ~) ? E : f((vm , vn )) ?( vm , V n ) . A dependency tree for a(V , E , k ) is constructed by : 1 . For ever yei ? E , one word f(ei ) is assigned class
Hi and governed by s invalency hi.
2 . For each vi ? V\V ' , IEI-I words vi are assigned class R and governed by the remaining copy of vi in reading Uithrough valencies rltor lEl_l  . 
3 . The vi in reading Ui are governed by sthrough the valencies uj  ( j ?\[1 , IWl-k\]) . 
4 . ( k-1) ? IEI words remain in a . These receive reading R and are governed by s in valencies r ~  ( j ?\[1 , ( k-1)IEI \]) . 
The dependency tree rooted in scovers the whole input a  ( V , E , k ) . Since G(V , E , k ) does not give any further restrictions this implies a  ( V , E , k ) ? L(G(V , E , k )) and , thus , (G(V , E , k ) , a(V , E , k )) ? DGR . 
Conversely assume ( G(V , E , k ) , a(V , E , k )) ? DGR : Then a(V , E , k ) ? L(G(V , E , k )) holds , i . e . , there exists a dependency tree that covers the whole input  . Since s cannot be governed in any valency , it follows that s must be the root . The instances of S has IEI valencies of class H , ( k-1)*\[EI valencies of class R , and IWl-kvalencies of class U , whose instances in turn have IEI- 1 valencies of class R . This sums up to IEI * IVl potential dependents , which is the number of terminals in a besidess . Thus , all valencies are actually filled . We define a subset VoC_V by Vo = def VE VI3i e \[1  , IYl-k\]8 . mod(ul ) = v . I . e . ,  ( 1 ) IVol=IVI-k The dependents of s invalencies hl are from the set V ' Vo  . We define a function f : E--+V\V o by f ( ei ) = defs . mod(hi ) for all ei EE . By construction f ( ei ) is an end point of edge ei , i . e . 
(2) V(v , , , , v , de E : f((v , . , , , v , 4, ) ev , , , , v, . , We define a subset V ' C V by V ' = def  f ( e ) le ? E . 
Thus (3) Ve ? E : f(e)?V '
By construction of V ' and by ( 1 ) it follows ( 4 ) IV ' l<IYl-IVol=kFrom ( 2 )  ,  (3) , and (4) we induce(V , E , k ) ? VC .  ?
Theorem 3
The DG recognition problem is in the complexity class 
Afl ) C.\[\]
The Af : P-completeness of the DG recognition problem follows directly from lemmata  1and   2  .  ?  5 Conclusion We have shown that current DG theorizing exhibits a feature not contained in previous formal studies of DG  , namely the independent specification of dominance and precedence constraints  . This feature leads to a A/'7% complete recognition problem . The necessity of this extension approved by most current DGs relates to the fact that DG must directly characterize dependencies which in PSG are captured by a projective structure and additional processes such as coindexing or structure sharing  ( most easily seen in treatments of socalled unbounded linear order  , as we have done in Section 3 , nevertheless seems to be a promising approach for PSG as well  ; see a very similar proposal for HPSG ( Reape ,  1989) . 
The .   N'79-completeness result also holds for the discontinuous DG presented in Section  3  . This DG can characterize at least some context -sensitive languages such as an bncn  , i . e . , the increase in complexity corresponds to an increase of generative capacity  . We conjecture that , provided a proper formalization of the other DG versions presented in Section  2  , their . A/P-completeness can be similarly shown . With respect to parser design , this result implies that the wellknown polynomial time complexity of chart-ortabular -based parsing techniques cannot be achieved for these DG formalisms in general  . This is the reason why the PARSETALK text understanding system  ( Neuhaus & Hahn ,  1996 ) utilize special heuristics in a heterogeneous chart-and backtracking-based parsing approach  . 

Barton , Jr . , G . E .  (1985) . On the complexity of ID/LP parsing . Computational Linguistics , 11(4):205-218 . 
Br6ker , N .  (1997) . Eine Dependenz grammatik zur Kopplung heterogener Wissens system eauf moda Uogischer Basis  , ( Dissertation ) . Freiburg , DE : Philosophische Fakult ~ it , Albert-Ludwigs-

Earley , J .  (1970) . An efficient contextfree parsing algorithm . Communications of the ACM , 13(2):94-102 . 
Gaifman , H .  (1965) . Dependencys stems and phrase-structure systems . Information & Control , 8:304--337 . 
Garey , M . R . & D . S . Johnson (1983) . Computers and Intractability : A Guide to the Theory of NP-completeness  ( 2 . ed . ) . New York , NY : Freeman . 
Haegeman , L .  (1994) . Introduction to Government and
Binding . Oxford , UK : Basil Blackwell.
Hellwig , E (1988) . Chart parsing according to the slot and filler principle  . In Proc . of the 12th Int . Conf . 
on Computational Linguistics . Budapest , HU , 22-27 Aug 1988, Vol . 1, pp .  242-244 . 
Hepple , M .  (1990) . Word order and obliqueness in categorial grammar . In G . Barry & G . Morill ( Eds . ), Studies in categorial grammar , pp .  47--64 . Edinburgh , UK : Edinburgh University Press . 
Huck , G .  (1988) . Phrasal verbs and the categories of postponement . In R . Oehrle , E . Bach & D . Wheeler ( Eds . ) , Categorial Grammars and Natural Language Structures  , pp .  249-263 . Studies in Linguistics and Philosophy 32 . Dordrecht , NL:D . Reidel . 
Hudson , R .  (1990) . English Word Grammar . Oxford,
UK : Basil Blackwell.
Lombardo , V . & L . Lesmo (1996) . An earley-typercog-nizer for dependency grammar  . In Proc . of the 16th Int . Conf . on Computational Linguistics . Copenhagen , DK , 59 Aug 1996, Vol . 2, pp .  723-728 . 
McCord , M .  (1990) . Slot grammar : A system for simpler construction of practical natural language grammars  . In R . Studer ( Ed . ), Natural Language and Logic , pp .  118-145 . Berlin , Heidelberg : Springer . 
Mer~uk , I .  (1988) . Dependency Sntax : Theory and Practice . New York , NY : SUNY State University
Press of New York.
Mel'6uk , I . & N . Pertsov (1987) . Surface Syntax of English : A Formal Model within the MTT Framework  . 
Amsterdam , NL : John Benjamins.
Neuhaus , R&U . Hahn (1996) . Restricted parallelism in object-oriented lxical parsing  . In Proc . of the 16th Int . Conf . on Computational Linguistics . Copenhagen , DK , 59 Aug 1996, pp .  502-507 . 
Pollard , C . & I . Sag (1994) . Head-Driven Phrase Structure Grammar . Chicago , IL : University of Chicago

Rambow , O . & A . Joshi (1994) . A formalook at DGs and PSGs , with consideration of word-order phenomena . In L . Wanner ( Ed . ), Current Issues in
Meaning-Text-Theory . London : Pinter.
Reape , M . (I989) . A logical treatment of semi-free word order and discontinuous constituents  . In Proc . of the 27 th Annual Meeting of the Association for Computational Linguistics  . Vancouver , BC , 1989, pp .  103-110 . 
Starosta , S .  (1988) . The Case for Lexicase . London :

Starosta , S .  (1992) . Lexicase revisited . Department of
Linguistics , University of Hawaii.
Tesni`re , L .  ((1969) 1959) . Elements de Syntaxe Structurale (2 . ed . ) . Paris , FR : Klincksieck . 

