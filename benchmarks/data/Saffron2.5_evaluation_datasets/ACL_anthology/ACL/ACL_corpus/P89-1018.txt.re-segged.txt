The Structure of Shared Forests
in Ambiguous Parsing
Sylvie Billott " Bernard Lang *

r and Universit ~ d ' Orl ~ ans
billot Ginria . in ria . frlang Ginria . in ria . fr
Abstract
The ContextFree backbone of some naturalanguage analyzers produces all possible CF parses as some kind of shared forest  , from which a single tree is to be chosen by a disambiguation process that may be based on the finer features of the language  . We study the structure of these forests with respecto optimality of sharing  , and in relation with the parsing schema used to produce them  . In addition to a theoretical and experimental framework for studying these issues  , the main results presented are :- sophistication i chart parsing schemata  ( e . g . use of lookahead ) may reduce time and space efficiency instead of improving it  , -there is a shared forest structure with at most cubic size for any CF grammar  , -when O(n3) complexity is required , the shape of a shared forest is dependent on the parsing schema used  . 
Though analyzed on CF grammars for simplicity , these results extend to more complex formalisms such as unification based grammars  . 
Keywords : ContextFree Parsing , Ambiguity , Dynamic Programming , Earley Parsing , Chart Parsing , Parsing Strategies , Parsing Schemata , Parse Tree , Parse Forest . 
1 Introduction
Several natural language parser start with & pure Conte ~ zt  . 
Free ( CF ) backbone that makes a first sketch of the structure of the analyzed sentence  , before it is handed to a more elaborate analyzer ( possibly a coroutine )  , that takes into account the finer grammatical structure to filter out undesirable parses  ( see for example \[24 , 28\]) . In \[28\] , Shieber surveys existing variants to this approach before giving his own tunable approach based on restrictions that ~ split up the infinite nonterminal domain into a finite set of equivalence classes that can be used for parsing  "  . The basic motivation for this approach is to benefit from the CF parsing technology whose development over  30 years has lead to powerful and ei~cient parsers \ [ I  , 7\] . 
A parser that takes into account only an approximation f the grammatical features will often find ambiguities it cannot resolve in the analyzed sentences I  . A natural solution * Address : INRIA , B . P . 105,78153 Le Chesn~y , France . 
The work reported here was partially supported by the Eureka 
Software Factory project.
1 Ambiguity may also have a semantical origin . " is then to produce all possible parses , according to the CF backbone , and then select among them on the basis of the complete features information  . One hitch is that the number of parses may be exponential in the size of the input sentence  , or even in fuite for cyclic grammars or incomplete sentences  \[16\]  . However chart parsing techniques have been developed that produce an encoding of all possible parses as a data structure with a size polynomial in the length of the input sentence  . These techniques are all based on a dynamic programming paradigm  . 
The kind of structure they produce to represent all parses of the analyzed sentence is an essential characteristic of these algorithm  . Some of the published algorithms produce only a chart as described by Kayin  \[14\]  , which only associates nonterminal categories to segments of the analyzed sentence  \[11  , 39 , 13 , 3 , 9\] , and which thus still requires nontrivial pro -ceasing to extract parse-trees  \[26\]  . The worst size complexity of such a chart is only a square function of the size of the input  2  . 
However , practical parsing algorithms will often produce a more complex structure that explicitly relates the instances of nonterminals asociated with sentence fragments to their constituents  , possibly in several ways in case of ambiguity , with a sharing of some common subtrees between the distinct ambiguous parses  \[7  , 4 , 24 , 31 , 2 5\] ~ One advantage of this structure is that the chart retains only these constituents hat can actually participate in a parse  . Furthermore it makes the extraction of parse -trees a trivial matter  . A drawback is that this structure may be cubic in the length of the parsed sentence  , and more generally polynomial ' for some proposed algorithms  \[31\]  . However , these algorithms are rather well behaved in practice  , and this complexity is not a problem . 
In this paper we shall call shared forest such data  struc-2 We do not consider CF reco~zers that have asymptotically the lowest complexity  , but are only of theoretical interest here\[~S , 5\] . 
3 There are several other published implementation fchart parsers  \[23  , 20 , 33\] , hut they often do not give much detail on the output of the parsing process  , or even sidestep the problem ~1 . 
together\[33\] . We do not consider here the well . formed s ~ bs ~ ring fable a of Shell \[26\] which falls somewhere in between i our class i -ficgtlon  . They do not use pointers and parse-trees are only " indirectly " visible  , but may be extracted rather simply in linear time . 
? The table may contain useless constituents.
4 Space cubic algorithms often require the lan ~tage grammar to be in Chomsky Normal Form  , and some authors have incorrectly conjectured tha ~cubic complexity can no the obtained otherwise  . 
1 43 tures used to represent simultaneously all parse trees for a given sentence  . 
Several question ? may be asked in relation with shared forests : ? How to construct hem during the parsing process ? ? Can the cubic complexity be attained without modifying the grammar  ( e . g . into Chomsky Normal Form ) ?s What is the appropriate data structure to improve sharing and reduce time and space complexity ? ? How good is the sharing of tree fragments between ambiguous parses  , and how can it be improved ? ? Is there a relation between the coding of parse-trees in the shared forest and the parsing schema used ? ? How wellformalized is their definitions nd construction ? These questions are of importance in practical systems because the answers impact both the performance and the implementation techniques  . For example good sharing may allow a better factorization of the computation that filters parse trees with the secondary features of the language  . The representation needed for good sharing or low space complexity may be incompatible with the needs of other components of the system  . These components may also make assumptions about this representation that are incompatible with some parsing schemata  . The issue of formalization is of course related to the formal tractability of correctness proof for algorithms using shared forests  . 
In section 2 we describe a uniform theoretical framework in which various parsing strategies are expressed and compared with respect othe above questions  . This approach as been implemented into a system intended for the experimental study and comparison of parsing strategies  . This system is described in section 3 . Section 4 contain ~ a detailed example produced with our implementation which illustrates both the working of the system and the underlying theory  . 
2 A Uniform Framework
To discus ? the above issue ? in a uniform way , we need a gen-era \] framework that encompasses all forms of chart parsing and shared forest building in a unique formalism  . We shall take a ? al~ska formalism developed by the second author in previous papers  \[15  , 16\] . The idea of this approach is to separate the dynamic programming construct ? needed for efficient chart parsing from the chosen parsing schema  . Comparison between the classifications of Kay \[14\] and Gritfith & Petrick \[10\] shows that a parsing schema ( or parsing strategy ) may be expressed in the construction of a Push -Down Transducer  ( PDT )  , a well studied formalization of left-to-right CF parsers  5  . These PDTs are usually nondeterministic and cannot be used as produced for actual parsing  . Their backtrack simulation does not alway ? terminate  , and is often time-exponential when it does , while breadth-first simulation is usually exponential for both time and space  . However , by extending Earley's dynamic programming construction to PDTs  , Long provided in\[15\] a way of simulating all possible computations of any PDT in cubic time and space complex-s Grifllth & Petrick actually use Turingma  , ' hines for pedagogical reasons . 
ity . This approach may thus be used as a uniform framework for comparing chart parsers s  . 
2.1 The algorithm
The following is a formal overview of parsing by dynamic programming interpretation of PDT ?  . 
Our ahn is to parse sentences in the language ? ( G ) generated by a CF phrase structure grammar G-- ( V ,  ~ , H , N ) according to its syntax . The notation used is V for the set of nontermln ~ l  , ~ for the set of terminals , H for the rules , for the initial nonterminal , and e for the empty string . 
We assume that , by some appropriate parser construction technique  ( e . g .  \[12 , 6 , 1\] ) we mechanically produce from the grammar Ga parser for the language ?  ( G ) in the form of a ( possibly nondeterministic ) push . down transducer ( PDT ) TG . The output of each possible computation of the parser is a sequence of rules in rl ~ to be used in a left-to-right reduction of the input sentence  ( this is obviously equivalent to producing a parse tree  )  . 
We assume for the PDTTG a very general formal definition that can fit most usual PDT construction techniques  . It is defined as an 8-tuple TG--(Q ,  \]~ , A , H ,  6 ,  ~ ,  ; , F ) where : Q is the set of states , ~ is the set of input word symbols , A is the set of stack symbols , H is the set of output symbols s(i . e . rule ? of G ) , q is the initial state , $ is the initial stack symbol , F is the set of final states , 6 is afnite set of transitions of the form : ( pAa ~-* qBu ) with p , qEQ , x , s ? Aue , aE~:u ~ , and . ~ H * . 
Let the PDT be in a configuration p-- ( pAaazu ) where p is the current state , A a is the ? tack contents with A on the top , az is the remaining input where the symbol a is the next to be shifted and zE ~*  , and u is the already produced output . The application of a transition r = ( pAa ~-* qBv ) result ? in a new configuration p'---- ( q Botzuv ) where the terminal symbol a has been scanned ( i . e . shifted ) , A has been popped and B has been pushed , and t , has been concatenated to the existing output , ~ If the terminal symbol a is replaced by e in the transition  , no input symbol is scanned . If A(reap . 
B ) is replaced by ? then no stack symbol is popped from  ( resp . 
pushed on ) the ? tack.
Our algorithm consist ? in an Earley-like 9 simulation of the PDTTG . Using the terminology of \[1\] , the algorithm builds an item set , ~ successively for each word symbol z ~ holding position i in the input sentence z  . An item is constituted of two modes of the form ( pAi ) where p is a PDT state , A is a stack symbol , and i . is the index of an input symbol . 
The item set & contains items of the form ( ( p A i )   ( qBj ) )  . 
These item ? are used as nontermineds of an output grammar S The original intent of  \[15\] was to show how one can generate efficient general CF chart parsers  , by first producing the PDT with the efllcient echniques for deterministic parsing developed for the compiler technology  \[6  , 12 , 1\] . This idea was later successfu/ly used by Tomits \[31\] who applied it to LR ( 1 ) parsers\[6 , 1\] , and later to other puelulown based parsers\[32\] . 
7 Implomczxtations u ually dc~ote these rules by their index in these trl  . 
sActual implementations use output symbols from r Iu ~  , since rules alone do not distinguish words in the same lexical category  . 
sWeasmune the reader to be familiar with some variation of Earley's algorithm  . Earley's original paper uses the words tere ( from dynamic programming terminology ) instead of item . 
144 = (8 , l'I ,  ~ , U ~) , where 8 is the set of all items ( i . e . the union of &) , and the rules in ~ are constructed together with their left-hand-side item by the algorithm  . The initial nonterminal Ut of ~ derives on the last items produced by a successful computation  . 
Appendix A gives the details of the construction of items and rules in G by interpretation f the transitions of the PDT  . 
More details may be found in \[15, 16\].
2.2 The shared forest
An apparently major difference between the above algorithm and other parsers is that it represents a parse as the string of the grammar rules used in a leftmost reduction of the parsed sentence  , rather than as a parse tree ( cf . section 4) . When the sentence has several distinct paxses , the set of all possible parse strings is represented in finite shared form by a CF grammar that generates that possibly in finite set  . Other published algorithms produce instead a graph structure representing all paxse-trees with sharing of common subpaxts  , which corresponds well to the intuitive notion of a shared forest  . 
This difference is only appearance . We show here in section 4 that the CF grammar of all leftmost parses is just a theoretical formalization of the shared  . forest graph . ContextFree grammars can be represented by AND-OR graphs that are closely related to the syntax diagrams often used to describe the syntax of programming languages  \[37\]  , and to the transition et works of Woods\[22\] . In the case of our grammar of leftmost parses , this AND-OR graph ( which is acyclic when there is only finite ambiguity  ) is precisely the shaxed-forest graph . In this graph , AND-nodes correspond to the usual parse tree nodes  , whil ~ OR-nodes correspond to xmbi-guities , i . e . distinct possible subtrees occurring in the same context  . Sharing of subtrees in represented by nodes accessed by more than one other node  . 
The grammar viewpoint is the following ( cf . the example in section 4) . Nonterminal ( reap . terminal ) symbols correspond to nodes with ( reap . without ) outgoing arcs . AND-nodes correspond to right hand sides of gramma rules  , and OR-nodes ( i . e . ambiguities ) correspond to nonterminals defined by several rules  . Subtree sharing is represented byse Voeral uses of the same symbol in rule right hand sides  . 
To our knowledge , this representation fparse-forests a grammars i the simplest and most tractable theoretical formalization proposed so far  , and the parser presented here is the only one for which the correctness of the output grammar-- i  . e . of the shared-forest--has ever been proved . 
Though in the examples we use graph ( ical ) representations for intuitive understanding ( grammars axe also sometimes represented as graphs  \[37\]  )  , they are not the proper formal tool for manipulating shared forests  , and developing formalized ( proved ) algorithms that use them . Graph formalization is considerably more complex and awkward to manipulate than the well understood  , specialized and few concepts of CF grammars . Furthermore , unlike graphs , this grammar formalization of the shared forest may be tractably extended to other grammatical formalisms  ( ct:section 5 )  . 
More importantly , our work on the parsing of incomplete sentences \[16\] has exhibited the fundamental character of our grammatical view of shared forests : when parsing the completely unknown sentence  , the shared forest obtained is precisely the complete grammar of the analyzed language  . 
This also leads to connections with the work on partial eval-nation  \[8\]  . 
2.3 The shape of the forest
For our shared-forest , x cubic space complexity ( in the worst case--space complexity is often linear in practice  ) is achieved , without requiring that the language grammar be in Chonmky Normal Form  , by producing a grammar of parses that has at most two symbols on the right hand side of its rules  . This amounts to representing the list of sons of a parse tree node as a Lisplike list built with binary nodes  ( see figures 1L-2 )  , and it allows partial sharing of the sons i0 The structure of the parse grammar , i . e . the shape of the parse forest , is tightly related to the parsing schema used , hence to the structure of the possible computation of the nondeterministic PDT from which the parser is constructed  . 
First we need a precise characterization fparsing strategies  , whose distinction is often blurred by superimposed optimizations  . We call bottom-up a strategy in which the PDT decides on the nature of a constituent  ( i . e . on the grammar rule that structures it ) , after having made this decision first on its subconstituents  . It corresponds to a postfix left-to-right walk of the parse tree  . Top-Down parsing recognizes a constituent before recognition of its subconstituents  , and corresponds to a prefix walk . Intermediate strategies are also possible . 
The sequence of operations of a bottom-u parser is basically of the following form  ( up to possible simplifying oi > . 
timizations ): To parse a constituent A , the parser first parses and pushes on the stack each subconstituent B ~  ; at some point , it decides that it has all the constituents of A on the stack and it pops them all  , and then it pushes A and outputs the ( rule number ~- of the ) recognized rule f : A-*Bl .   .   . Bn , . Dynamic programming interpretation of such a sequence results in a shared forest containing parse-trees with the shape described in figure  1  , i . e . where each node of the forest points to the beginning of the llst of its sons  . 
A topdown PDT uses a different sequence of operations  , detailed in appendix B , resulting in the shape of figure 2 where a forest node points to the end of the list of sons  , which is itself chained backward . These two figures are only simple examples . Many variations on the shape of parse trees and forests may be obtained by changing the parsing schema  . 
Sharing in the shared forest may correspond to sharing of a complete subtree  , but also to sharing of atail of all stofs on s : this is what allows the cubic complezity  . Thus bottom-up parsing may share only the rightmost subconstituents of a constituent  , while topdown parsing may share only the leftmost subconstituents  . This relation between parsing schema and shape of the shared forest  ( and type of sharing ) is a consequence of intrinsic properties of chart parsing  , and not of our specific implementation . 
It is for example to be expected that the bidirectional nature of island parsing leads to irregular structure in shared forests  , when optimal sharing is sought for . 
3 Implementation and Experimental

The ideas presented above have been implemented in an experimental system called T in  ( after the woodman of OZ )  . 
1 0 This was noted by Shell \[26\] and is implicit in his use of "2-form ~grammars . 

AA
Figure 1: Bottom-u parse-tree '
Figure 2: Top-down parse tree
The intent is to provide a uniform f~amework for the construction and experimentation fchart parsers  , somewhat as systems like MCHART\[29\] , but with a more systematic theoretical foundation  . The kernel of the system is a virtual parsing machine with a stack and a set of primitive commands corresponding essentially to the operation of a practical Push-Down Transducer  . These commands include for example : push(resp . pop ) to pus hasymbol on the stack ( reap . 
pop one ) , check ~ in dowto compare the lookahead symbol ( s ) to some given symbol , chs ck stack to branch depending on the top of the sta~k  , scan to read an input word , out pu $ to output a rule number ( or a terminal symbol )  , got of or uncon-ditional jumps , and a few others . However theae commands are never used directly to program parsers  . They are used as machine instructions for compilers that compile grammatical definitions into T in code according to some parsing schema  . 
A characteristic of these commands i that they may all be marked as non-determlnistic  . The intuitive interpretation is that there is a nondeterministic choice between a command thus marked and another command whose address in the virtual machine code is then specified  . However execution of the virtual machine code is done by an all-paths interpreter that follows the dynamic programming strategy described in section  2  . 1 and appendix A . 
The Tininterpreter is used in two different ways :   1  . to study the effectiveness for chart parsing of known parsing schemata designed for deterministic parsing  . 
We have only considered formally defined parsing schemata  , corresponding to established PDA construction techniques that we use to mechanically translate CF grammars into T in code  . ( e . g . LALR (1) and LALR(2)\[6\] , weak precedence\[12\] , LL(0) topdown ( recursive descent) , LR (0) , LR(1)\[1\] . . . ) . 
2 . to study the computational behavior of the generated code  , and the optimization techniques that could be used on the T in code--and more generally chart parser code--with respect to code size  , execution speed and better sharing in the parse forest  . 
Experimenting with several compilation schemata has shown that sophistication may have a negative effect on the ej ~ iciency of all path  parsin911   . Sophisticated PDT construction techniques tend to multiply the number of special cases  , thereby increasing the code size of the chart parser  . Sometimes it also prevent sharing of locally identical subcomputations because of differences in context analysis  . This in turn may result in lesser sharing in the parse forest and sometimes longer computation  , as in example $ BBL in appendix C , but of course it does not change the set of parse -trees encoded in the forest  12  . Experimentally , weak precedence gives slightly better sharing than LALR  ( 1 ) parsing . 
The latter is often v/ewed as more efficient , whereas it only has a larger deterministic domain . 
One essential guideline to achieve better sharing ( and often also reduced computation time ) is to try to recognize very grammar rule in only one place of the generated chart parser code  , even at the cost of increasing nondeterminism . 
Thus simpler schemata such as precedence , LL ( 0 )   ( and probably LR ( 0 ) I ~ ) produce the best sharing . However , since they correspond to a smaller deterministic domain within the CF grammar realm  , they may sometimes be computationally less efficient because they produce a larger number of useless items  ( Le . edges ) that correspond to dead-end computational paths . 
Slight sophistication ( e . g . LALR (1) used by Tomita in \[31\] , or LR ( 1 )   ) may slightly improve computational performance by detecting earlier dead-end computations  . This may however beat the expense of the forest sharing quality  . 
More sophistication ( say LR ( 2 ) ) is usually losing on both accounts as explained earlier  . The duplication of computational pg sths due to distinct context analysis over weights he  11 We mean here the sophistication f the CF parser construction technique rather than the sophistication f the language fa-ture schop into be used by this parser  . 
l ~ This negative behavior of some techniques originally intended to preserve determl ni~n had beam remarked and analyzed in a special case by Bouckaert  , Pirotte and Shelling\[3\] . However we believe their result o be weaker than ours  , since it seems to rely on the fact that they directly interpret ~' anuuars rather than first compile them  . Hence each interpretive step include in some sense compilation steps  , which are more expensive when lookahead is increased  . Their paper presents several examples that run less efficiently when lookahead is increased  . For all these examples , this behavior disappears in our compiled setting . However the grammar SBBL in appendix C shows a loss of eltlciency with increased lookahead that is due exclusively to loss of sharing caused by irrelevant contextual distinctions  . This effect is particularly visible when parsing incompletes ntences  \[16\]  . 
Eiticiency loss with increased lookahead is mainly due to state splitting  \[6\]  . This should favor LALR techniques ova-LR ones . 
is Our resnlts do not take into account a newly found optimization of PDT interpretation that applies to all and only to bottom-up PDTs  . This should make simple bottom-up schemes competitive for sharing quality  , and even increase their computationalei ~ ciency . However it should not change qualitatively the relative performances of bottom-up arsers  , and n~y emphasize even more the phenomenon that reduces efficiency when lookahead in-can be no absolute rule : ffagrammar is a close " to the LR  ( 2 ) domain , an LR ( 2 ) schema is likely to give the best result for most parsed sentences  . 
Sophisticated schemata correspond also to larger parsers  , which may be critical in some natural language applications with very large grammars  . 
The choice of a parsing schema depends in fine on the grammar used  , on the corpus ( or kind ) of sentences to be analyzed , and on a balance between computational and sharing efficiency  . It is best decided on an experimental basis with a system such as ours  . Furthermore , we do not believe that any firm conclusion limited to CF grammars would be of real practical usefulness  . The real purpose of the work presented is to get a qualitative insight in phenomena which are best exhibited in the simpler framework of CF parsing  . 
This insight should help us with more complex formalisms  ( cf . section 5 ) for which the phenomena might be less easily evidenced  . 
Note that the evidence gained contradicts he common be-lid that parsing schemata with a large deterministic domain  ( see for example the remarks on LR parsing in \[31\]  ) are more effective than simpler ones . Most experiments in this area were based on in comparable implementations  , while our uniform framework gives us a common theoretical yardstick  . 
4 A Simple Bottom-Up Example
The following is a simple example based on a bottom-up PDT generated by our LALR  ( 1 ) compiler from the following grammar taken from \[31\]: I ( 0 ) '$ ax ::=$' s$ ( 1 ) ' s : := ' up ' v p ( 2 ) ' e::-' s'pp ( 3 ) ' up::=n ( 4 ) ' up::-detn ( 5 ) ' up::-'up'pp ( 6 ) ' pp::-prep'up ( 7 ) ' vp::=v'up Nonterminals are prefixed with a quote symbol The first rule is used for initialization and handlhg of the delimiter symbol  8  . The $ delimiters are implicit in the actual input sentence  . 
The sample input is a ( nv detn prep n ) ~ . It figures ( for example ) the sentence : a T see a man at home ~ . 
4 . 1 Output grammar produced by the parser The grammar of parses of the input sentence is given in figure  3  . 
The initial nonterminal is the lefthand side of the first rule  . For readability , the nonterminals have been given computer generated names of the form  at2  , where z is an integer . 
All other symbols are terminal . Integer terminals correspond to rule numbers of the input language grammar given above  , and the other terminals are symbols of the parsed language  , except for the special terminal % i1" which indicates the end of the list of subconstituents of a sentence constituent  , and may also be read as the empty string ~ . Note the ambiguity for nontermlnal t 4 . 
It is possible to simplify this grammar to 7 rules without losing the sharing of common subparses  . However it would no longer exhibit the structure that makes it readable as a shared-forest  ( though this structure could be retrieved )  . 
n t0 : := ntl 0 nt l9  : :=  nt20 nilntl : := nt2   nt3   nt20 : := n nt2  : : - $  at21  : : -  nt22 nil nt3  : :=  nt4   nt37   nt22  : :=  nt23   6   at4  : :=  at5   2   at23  : :=  at24   nt25   nt4  : :=  nt29   1   nt24 : := prep nt5  : :=  nt6   nt21   nt25  : :=  nt26 nil nt6  : :=  nt7   1   nt26  : :=  nt27   3 nit : := nt8 ntll nt27  ::=  nt28 nil at8  ::-  at9   3   nt28 : := n nt9 : := ntl Onil at29  ::-  nt8   nt30   nil0 : :- n at30  ::=  nt31 nilntll : :- nil2 nil at31  ::=  at32   7   nil2  ::=  nil3   7   at32  ::=  nil4   at33   n~13  ::=  nil4   nil5   nt33  ::=  nt34 nil nil4 : :- v at34  ::=  nt35   5   nt15  ::=  nil6 nil nt35  ::=  nil6   nt36   nil6  ::=  at17   4   nt36  ::=  nt22 nil nil7  ::=  ntl8   ntl9   nt37  ::=  nt38 nil nt18 : := det nt38 : :=$ Figure 3: Grammar of parses of the input sentence The two parses of the input sentence defined by this grammar are : $ n  3 v det n 4   7   1 prep n 3   6   2 $$ n 3 v det n 4 prep n 3   6   5   7   1 $ Here again the two $ symbols must be read as delimiters  . 
The ~1" symbols , no longer useful , have been omitted in these two parses . 
4 . 2 Parse shared - fo res t const ructed f i ' om that grnnalx larTo explain the structure of the shared forest  , we first build a graph from the grammar , as shown in figure 4 . Each node corresponds to one terminal or nonterminal of the grammar in figure  3  , and is labelled by it . The labels at the right of small dashes are rule numbers from the parsed language grammar  ( see beginning of section 4 )  . The basic structure is that of figure 1 . 
From this first graph , we can trivially derive the more traditional shared forest given in figure  5  . Note that this simplified representation is not always adequate since it does not allow partial sharing of their sons between two nodes  . Each node includes a label which is a nonterminal of the parsed language grammar  , and for each possible derivation ( several in case of ambiguity ) there is the number of the grammar rule used for that derivation  . Though this simplified version is more readable , the representation f figure 5 is not adequate to represent partial sharing of the subconstituents of a constituent  . 
Of course , the ~ constructions ~ given in this section are purely virtual  . In an implementation , the data structure representing the grammar of figure  3 may be directly interpreted and used as a shared -forest  . 
A similar construction for topdown parsing is sketched in appendix B  . 

I1~337"~I"238 $$521 . --\] l . 29~30--~""-r'i . ?"-') ?' f -'9 ~ . 13--f-~~-~,"z . t-I , 3s3e--l,"101416B428 rtV\]flE
I 17~19"-~'
II 1820 det n
Figure 4: Graph of the output grammar


Figure 5: The shared forest 5 Extensions As indicated earlier , our intent is mostly to understand phenomena that would be harder to evidence in more complex grammatical formalisms  . 
This statement implies that our approach can be extended  . 
This is indeed the case . It is known that many simple parsing schemata can be expressed with stack based machines  \[32\]? This is certainly the case for M!left-to-right CF chart parsing schemata  . 
We have formally extended the concept of PDA into that of Logical PDA which is an operational pushdown stack device for parsing unification based grammars  \[17  , 1 8\] or other non-CF grammar such as Tree Adjoining Grammars  \[19\]  . 
Hence we axe reusing and developing our theoretical  \[18\] and experimental \[38\] approach in this much more general setting which is more likely to be effectively usable for natural language parsing  . 
Furthermore , these extensions can also express , within the PDA model , non-left-to-fight behavior such as is used in island parsing  \[38\] or in Shei \]' s approach \[26\]? More generally they allow the formal analysis of agenda strategies  , which we have not considered here . In these extensions , the counterpart of parse forests are proof forests of definite clause programs  . 
6 Conclusion
An Mysis of Ml-path parsing schemata within a common framework exhibits in comparable terms the properties of these schemata  , and gives objective criteria for chosing a given schema when implementing a language analyzer  . The approach taken here supports both theoretic M analysis and actuM experimentation  , both for the computational behavior of pLmers and for the structure of the resulting shared forest  . 
Many experiments and extension still remaint 9 be made : improved dynamic programming interpretation of bottom-up parsers  , more extensive xperimental measurements with a variety of languages and parsing schemata  , or generalization of this approach to more complex situations  , such as word lattice parsing \[21 , 30\] , or even handling of " secondary " language features  . Early research in that latter direction is promising : our framework and the corresponding paradigm for parser construction have been extended to full first-order Horn clauses  \[17  , 18\] , and are hence applicable to unification based grammatical formalisms  \[27\]  . Shared forest construction and analysis can be generalized in the same way to these more advanced formalisms  . 
Acknowledgements : We are grateful to V ~ ronique Donzeau-Gouge for many fruitful discussions  . 
This work has been partially supported by the Eureka 
Software Factory ( ESF ) project.
References\[1\]Aho , A . V . ; and Ullman , J . D?1972 The Theory of Parsing , Trar~lation and Compiling . Prentice-
Hall , Englewood Cliffs , New Jersey.
\[2\] Billot ~ S . 1988 Analyseurs Syntaxiques et Non . 
D6 terminigme . Th ~ se de Doctor at , Universit ~ d'Ofl~nsla Source , Orleans ( France) . 
148\[3\] Bouckaert , M . ; Pirotte , A ~; and Sn~lllng , M .   1975 Efficient Parsing Algorithms for General ContextFree Grammars  . Information Sciences 8(1):1-26\[4\] Cooke , J . ; ~ nd Schwartz , J . T . 1970 Programming Languages and Their Compilers . Courant Institute of Mathematical Sciences , New York University , New York . 
\[5\] Coppersmith , D . ; and Winograd , S .   1982 On the Asymptotic Complexity of Matrix Multiplication  . 
SIAM Journal on Computing , 11(3):472-492.
\[6\]DeRemer , F.L .1971 SimpleLR(k ) Grammars.
Communications ACM 14(7): 453-460.
\[7\] Earley , J . 1970 An Efficient ContextFree Parsing Algorithm . Communications ACM 13(2): 94-102 . 
\[8\] Fntamura , Y . ( ed . )  1988 Proceedings of the Workshop on Paxtial Evaluation and Mixed Computation  . New Generation Computing 6(2, 3) . 
\[9\] Graham , S . L . ; Harrison , M . A . ; and Ruzzo W . L . 
1980 An Improved ContextFree Recognizer . ACM Transactions on Programming Languages and Systems  2  ( 3 ) : 415-462 . 
\[10\]Griffiths,L ; and Petrick , S .   1965 On the Relative Efficiencies of ContextFree Grammar Recogniz-ers  . Communications ACM 8(5): 289-300 . 
\[11\] Hays , D . G . 1962 Automatic Language-Data Proceesing . In Computer Applications in the Behavioral Sciences  , ( H . Borkoed . ), Prentice-Hall , pp . 

\[12\] Ichbiah , J . D . ; and Morse , S . P .   1970 A Technique for Generating Almost Optimal Floyd -Evans Pro-ductions for Precedence Grammars  . Communications ACM 13(8): 501-508 . 
\[13\] Kuami , J .   1965 An E~icient Recognition and Slmtax Analysis Algorithm  . for ContextFree Lan . 
geages . Report of Univ . of Hawaii , also AFCRL-65-758 , Air Force Cambridge Research Labor ~- tory , Bedford ( Massachusetts ) , also 1968 , University of Illinois Coordinated Science Lab . Report,
No . R-257.
\[14\] Kay , M .   1980 Algorithm Schemat and Data Structures in Syntactic Processing  . Proceeding soy the Nobel Symposium on Text Processing  , Gothenburg . 
\[15\] Lung , B .   1974 Deterministic Techniques for Efficient Nondeterministic Parsers  . Proc . oy the 2" ~ Colloquium on Automata , Languages and Programming , J . Loeckx ( ed . ) , Saarbrflcken , Springer Lecture Notes in Computer Science 14:   255-269  . 
Also : Rapport de Recherche 72, IRIA-Laboris,
Rocquencourt ( France).
\[16\] Lung , B . 1988 Parsing Incomplete Sentences . Proc . 
of the 12en Internat . Cony . on Computational Linguistics ( COLING'88 ) " CoL1:365-371 , D . Vargha ( ed . ), Budapest ( Hungary ) . 
\[17\] Lung , B . 1988 Datalog Automata . Proc . of the rd 3Internat . Cony . on Data and Knowledge Bases , C . Beeri , J . W . Schmidt , U . Dayal ( eds . ), Morgan Kanfmann Pub . , pp . 389-404, Jerusalem ( Israel ) . 
\[18\] Lung , B . 1988 Complete Evaluation of Horn Clauses , an Automata Theoretic Approach . INRIA
Research Report 913.
\[19\]LanK , B .   1988 The Systematic Construction of Eadey Parsers : Application to the Production o/O  ( n 6 ) Earle ~ Parsers for Tree Adjoining Grammars . In preparation . 
\[20\]Li , T . ; and Chun , H . W .   1987 A Massively Psral-lel Network-Based Natural Language Parsing System  . Proc . ol ? nd Int . Cony . on Computers and Applications Beijing ( Peking) ,  : 401-408 . 
\[21\]Nakagawa , S .   1987 Spoken Sentence Recognition by Time-Synchronous Parsing Algorithm of ContextFree Grammar  . Proc . ICASSP 87, Dallas ( Texas ), Vol .  2 : 829-832 . 
\[22\] Pereira , F . C . N . ; and Warren , D . H . D .   1980 Deft-uite Clause Grammars for Language Analysis - -A survey of the Formalism and a Comparison with Augmented Transition Networks  . Artificial Intel . 
ligence 13: 231-278.
\[23\]Phillips , J . D .   1986 A Simple Efficient Parser for Phrase Structure Grammars  . Quarterly Newslet-ter of the Soc . for the Study of Artificial Intelligence ( AISBQ )  59: 1419 . 
\[24\]Pratt,V . R . 1975 LINGOL--A Progress Report . 
In Proceedings of the Jth IJCAI : 422-428.
\[25\] Rekers , J .   1987 A Parser Generator for Finitely Ambiguous ContextFree Grammars  . Report CS-R8712, Computer Science/Dpt . of Software Technology , Centrum voor Wiskundeen Informatica , 
Amsterdam ( The Netherlands).
\[26\] Sheil , B . A . 1976 Observations on ContextFree Parsing . in Statistical Methods in Linguistics : . 71-109, Stockholm ( Sweden ), Pros . of Internat . Conf . 
on Computational Linguistics ( COLING-76) , Or-taw'4 ( Canada ) . 
Also : Techuical Report TR12-76 , Center for Research in Computing Technology , Alken Computation Laboratory , Harvard Univ . , Cambridge ( Massachusetts) . 
\[27\] Shieber , S . M .   1984 The Design of a Computer Language for Linguistic Information  . Proc . of the 10'h Internat . Cony . on Computational Linguistics -- COLING'84:   362-366  , Stanford ( California ) . 
\[28\] Shieber , S . M .   1985 Using Restriction to Extend Parsing Algorithms for Complex-Feature-Based Formalisms  . Proceeding soythe ~3rd Annual Meeting of the Association for Computational Linguistics :  145-152  . 
\[29\] Thompson , H . 1983 MCHART : A Flexible , Mod-ular Chart Parsing System . Proc . of the National Conf . on Artificial Intelligence ( AAAI-83) , Washington(D . C . ), pp .  408-410 . 
\[30\] Tomita , M .   1986 An Efficient Word Lattice Parsing Algorithm for Continuous Speech Recognition  . 
In Proceeding so y IEEE-IECE-ASJ International Conference on Acoustics  , Speech , and Signal Pro-?essing ( ICASSP86) , Vol .  3: 1569-1572 . 
\[31\]Tomita , M .   1987 An Efficient Augmented-Context-Free Parsing Algorithm  . Computational
Linguistics 13(1-2):3146.
\[32\] Tomita , M .   1988 Graph-structured Stack and Natural Language Parsing  . Proceeding soythe 26 th Annual Meeting Of the Association for Computa . 
tional Linguistics : 249-257.
149\[33\] Uehaxa , K . ; Ochitani , R . ; Kaknsho , 0 . ; Toyoda , J .   1984 A Bottom-Up Parser based on Predicate Logic : A Survey of the Formalism and its Implementation Technique  . 198 ~ In?ernst . Syrup . on Logic P~mming , Atlantic City ( New Jersey) ,  : 220-227 . 
\[34\]U . S . Department of Defense 1983 Reference Manual for the Ada Programming Language . 
ANSI/MIL-STD-1815A.
I35\]Valiant , L . G .   1975 General ContextFree Recognition in Less than Cubic Time  . Journal of Computer and System Sciences ,  10: 308-315 . 
\[36\] Villemonte de la Clergerie , E . ; and Zanchetta , A . 
1988 Eealuateur de Clauaes de Horn . Rapport de Staged ' Option , Ecole Polyte chulque , Palaise & u(n'auce) . 
\[37\] Wirth,N . 1971 The Programming Language Pascal . Acta Informatica , 1(1) . 
\[38\]Ward , W . H . ; Hauptmann , A . G . ; Stern , R . M . ; and Chanak , T .   1988 Parsing Spoken Phrases Despite Missing Words . In Proceedings of the 1988 International Conference on Acot ~ tics , Speech , and Signal Processing ( ICASSP88) , Vol .  1: 275-278 . 
\[39\] Younger , D . H .   1967 Recognition and Parsing of ContextFree Languages in Timen  3  . Information and Control , 10(2):189-208
A The algorithm
This is the formal description of a minimal dynamic programming PDT interpreter  . The actual T in interpreter has a larger instruction set  . Comments are prefixed with ~ . 
--Begin parse with input sentence x of length n step-A:--Initialization := 
So := ~; 7 ,  := ~ ; i := 0 ; -- initial item -- first rule of output grammar -- initialize item  . set So--rules of output grammar--input -scanner indez is set--before the first input symbol step-B :-- Iteration while i < n loop for everyires Uf  (   ( pAi )   ( qBj )   ) in S , do for every ~ zanaition r in 6 dome consider four kinds of transitions , corresponding to the instructions of a minimal PDT interpreter  . 
i ~ rf(p?e~-*fez ) then ~ OUTPUTz
Y : ----(( rAi ) CqBj )); & :=8 , uv ; 7':=Pu(v-u ~) ; if r ----( p e ? ~- , roe ) then -- PUSHC
V : ----(( rOi ) ( pAi )); s , := & u(V ; := 7' u(v-?) ; if r = ( pAe~teen -- PAPA for every il ; enY = (( qBj ) ( SDk )) in Sj do
V :=(( rBi)(sVk )); s , :=& uv ; 7':=7 , u((v-Yu ); if r = ( p?a~-~r ??) then
V := (( rAi + l ) ( qej));
S ,+ x :=&+ xuV ; := 7, u((v- . u ); i := i+1; . nd loop ; step-C:--Termination : J are very item O = (   ( f ; n )( ~ ; 0)) such that fEF do := ~' u(U~- . U );--Uf is the initial nonterminal of ~ . 
-- End of parse--SHIF Tain S .
B Interpretation of a top-down PDT To illustrate the creation of the shared forest  , we present here informally a simplified sequence of transitions in their order of execution by a topdown parser  . We indicate the transitions as T in instructions on the left  , as defined in appendix A . On the right we indicate the item and the rule produced by execution of each instruction : the item is the left-hand-side of the rule  . 
The pseudo-instructions can is given in italics because it does not exist  , and stands for the parsing of a subconstituent : either several transitions for a complex constituent or a single shift instruction for a lexical constituent  . 
The global behavior of scan is the same as that of e h if % and it may be understood as a shift on the whole subconstituent  . 
Items axe represented by a pair of integer . Hence we give no details about states or input , but keep just enough information to see how items axe interrelated when applying a pop transition : it must use two items of the form  ( a , b ) and ( b , c ) as indicated by the algorithm . 
The symbol r stands for the rule used to recognize a constituents  , and ~ rist and s for the rule used to recognize itsi'h subconstituentei  . The whole sequence , minus the first and the last two instructions , would be equivalent to " scans ' . 
?  . . (6  , 6) pushr (7 , 6) -> epushrl (8 , 7) -> ? scan 81 (9 , 7) -> (8 , 7) slout rl (10 , 7) -> (9 , 7) rlpop(11 , 6) -> (7 , 6) (10 , 7) ptm hf2 (12 , 11) -> ? scans ~ (13 , 11) -> (12 , 11) ssoutr2 (14 , 11) -> (13 , 11) r2pop(15 , 6) -> (11 , 6) (14 , 11) push r ~ (16 , 15) -> ? scansa (17 , 15) -> (16 , 15) s3 outr3 (18 , 15) -> (17 , 15) ~3 pop (19 , 6) -> (15 , 6) (18 , 15) outf (20 , 6 ) -> (19  , 6) pop (21 , 5) -> (6  , 5) (20 , 6)  ,   ,   . 
This grammar may be simplified by eliminating useless nonterminals  , deriving on the empty stringe or on a single other nonterminal  . As in section 4 , the simplified grammar may then be represented as a graph which is similar  , with more details ( the rules used for the subconstituents )  , to the graph given in figure 2 . 

CExperimental Comparisons
This appc~dlx gives some of the experimental data gathered to c~npa ~ compilation achemata ~ For each grammar  , the first table gives the size of the PDT soh-t ~ dned by compiling it accordlnZ to several compilation schemat L This size corresponds to the number of instructions genca'ated for the PDT  , which is roughly then , mher of possible PDT states . 
The second table gives two figures far each schema nd for sevm-alinput sentences  . The first figure is the number of items computed to parse that sentence with the given schema : it may be read as the number of computation steps and is thus ? measure of computational ei ~ ciency  . The second figure is then , ,ml~er of items r~n~in ; ngafter simp/ification of the output grarn m ~ , it is thus an indicator of shsx~g quality . Sharing is better when this second figure is low . 
In these tables , columns beaded with LR/LALR stands for the LR (0) , LR (1) , LALR ( 1 ) and LALR ( 2 ) cases ( which often give the same results )  , unlesa one of these cases has its own expl ; clt column . 
Tests were run on the GRE , NSE , UBDA and RR gramman of \[3\]: they did not exhibit the loss of eRiciency with incre~md lookahead that was reported for the bottom-up lookahead of  \[3\]  . 
We believe the results presented here axe consistent and give an accurate comparison of performances of the parsers considered  , despite some implementation departure from the strict theoretical model required by performance onsiderations  . At int version of our LL(0) compiler & , ave results that were inconsistent with the results of the bottom-u parsers  . This was , , due to & weakness in that LL ( 0 ) compiler which was then corrected . We consider this experience to be a conflrm ~ ion of then sefuln ~ of our uniform framework  . 
It must be stressed that these ~ reprellmi ~ L~-y experiments  . On the basis of thd r .  ~ , dysis , we intend a new set of experiments that will better exhibi the phenomena discussed in the paper  . In particular we wish to study variants of the schen~ta and dynamic progr~nming interpretation that give the best p  , ~d ble sharing . 
C . IGr-mmarUBDA it : :' A ? J ?
LR ( 0 ) \[ LR ( 1 ) LALR ( 1 ) LALR ( 2 ) 38604141 input string ma&a a aam
LR/LAL R14-923-15249-156 prece ( L15-929-15226-124 preced . LL(0) 36 46
LL (0) 41-975-15391-112
C . 2 Gr-mmarRR ?: :- x?l ? gramm ~ is LALR ( 1 ) but not LR ( 0 )  , which explains the lower performance of the LR ( O ) parser . 
LB . (0) LR(1) LALR (1) LALR (2) preced . LL ( 0 ) 343737374846 input string LR ( 0 ) LR/LALR preced . 
?  14   -9   14-9   15-9 xx 23-   13   20-   13   25  -  13 x x z x x z 99-   29   44  -  29   56  -  29 
C . 3 Picogrsmmar of English
S::8EP VP\[SPP
IP : : " nJ deca\[lipPP
VP : := virP
PP : : ? preplip
LL ( O ) 28-943-13123-29
LR(0) LR(1) LALR(1) LALR(2) preced . LL ( 0 )   110   341   104   104   90   116 input string LFt/LALR preced . LL(0) n?n prep n 71 . 4772-47169-43n?n ( prep n ) 2146-97141-93260-77 n?u ( Fepn ) 3260-172245-161371-122n?n ( prep n ) s 854-541775-491844-317 C . 4 Grammar of Ada expressions This grimm & r , too long for inclusion h ~ e , is the grammar of expressions of the \] an ~ cru~e Ads N as given in the reference manual  \[3@ This grammar is ambiguous . 
In these examples , the use of lookahead give approximately a 25% gain in speed elliciency over LR ( 0 ) parsing , with the same fo~ts hadng . 
However the use of lookahead rn~y increase the LR ( 1 ) parser size quadratically with the granunar size . Still , a better engineered LR ( 1 ) construction should not usually increase that size as dra-nmticaily as indicated by our experimental figure  . 
LR(0) LR(1) LALR (1) preced.
587322 10534323 input string LIt ( 0 ) LR ( 1 ) LALR ( 1 ) a * 37'4-3959-3959-39 ( ae3 ) + b 137-   75   113  -  75   113-   75   &*3"I-b**4   169-   81   122  -  81   122  -  81 
C .5 Grnmmar PB
E ::= aAd\[?Be\[ba?\[bBd?::me
B :: me preced.

LR(0) LR(1) LALR(1)~(2) p~'cd . LL ( 0 ) 76 i008084122 Thin~p-ammar is LR ( 1 ) but is not LALR . For each compilations cb , ' mait gives the same result on all possible inputs : aed  , a e ? , be candbed . 
LR(0) LR(1) LALR(1) & (2) preced . LL (0) 26-1523-1526-1529-1547-15
C .6 Grammar SBBL
E :: 8X ? dJIB c\[Y ? c\[YB d
X : : mr
Y :: mr
A : := ei\[g
B ::= eAJs
LR(0) LR(1) LALR(1) LALR(2) preced.
159 294 158 158 104 input string LR ( 0 ) LR ( 1 ) LALR ( 1 )  &  ( 2 ) preced . 
fegd 50-   21   57 o 37   50  -  21   84  -  36 feee ~ Fl 62  -  29   7'5  -  49   62  -  29   II0  -  44 The term ln , df may be ambiguously parsed as X or as Y . This ambiguous left context increases uselessly the complexity of the LR  ( 1 ) ~during recognition of the A and B constituents . Hence LR ( 0 ) performs better in this case since it ignores the context  . 

