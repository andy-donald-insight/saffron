Relating Probabilistic Grammars and Automata
Steven Abney David McAllester Fernando Perei ra 
AT&T Labs-Research
180 Park Ave
Florham Park NJ 07932
abney , dmac , pereira @ research.att.com
Abstract
Both probabilistic contextfree grammars ( PCFGs ) and shift-reduce probabilistic pushdown automata ( PPDAs ) have been used for language modeling and maximum likelihood parsing  . We investigate the precise relationship between these two formalisms  , showing that , while they define the same classes of probabilistic languages  , they appear to impose different inductive biases . 
1 Introduction
Current work in stochastic language models and maximum likelihood parsers falls into two main approaches  . The first approach ( Collins , 1998; Charniak ,  1997 ) uses directly the definition of stochastic grammar  , defining the probability of a parse tree as the probability that a certain topdown stochastic generative process produces that tree  . The second approach ( Briscoe and Carroll , 1993; Black et al , 1992; Magerman , 1994; Ratnaparkhi , 1997; Chelba and Jelinek ,  1998 ) defines the probability of a parse tree as the probability that a certain shift-reduce stochastic parsing automaton outputs that tree  . These two approaches correspond to the classical notions of contextfree grammars and nondeterministic pushdown automata respectively  . It is wellknown that these two classical formalisms define the same language class  . 
In this paper , we show that probabilistic on text-free grammars ( PCFGs ) and probabilistic pushdown automata ( PPDAs ) define the same class of distributions on strings  , thus extending the classical result to the stochasticase  . We also touch on the perhaps more interesting question of whether PCFGs and shift-reduce parsing models have the same inductive bias with respecto the automatic learning of model parameters from data  . Though we cannot provide a definitive answer , the constructions we use to answer the equivalence question involve blow-ups in the number of parameters in both directions  , suggesting that the two models impose different inductive biases  . 
We are concerned here with probabilistic shift -reduce parsing models that define probability distributions over word sequences  , and in particular the model of Chelba and Jelinek  ( 1998 )  . Most other probabilistic shift-reduce parsing models  ( Briscoe and Carroll , 1993; Black et al , 1992; Magerman , 1994; Ratnaparkhi ,  1997 ) give only the conditional probability of a parse tree given a word sequence  . 
Collins ( 1998 ) has argued that those models fail to capture the appropriate dependency relations of natural anguage  . Furthermore , they are not directly comparable to PCFGs , which define probability distributions over word sequences  . 
To make the discussion somewhat more concrete , we now present a simplified version of the Chelba -Jelinek model  . Consider the following sentence :
The small woman gave the fat manhers and wich.
The model under discussion is based on shift -reduce PPDAs  . In such a model , shift transitions generate the next word w and its associated syntactic category X and push the pair  ( X , w ) on the stack . Each shift transition is followed by zero or more reduce transitions that combine topmost stack entries  . For example the stack elements ( Det , the ) , ( hdj , small ) , ( N , woman ) can be combined to form the single entry ( NP , woman ) representing the phrase " the small woman " . In general each stack entry consists of a syntactic ategory and a head word  . 
After generating the prefix " The small woman gave the fatman " the stack might contain the sequence  ( NP , woman ) < Y , gave ) ( NP , man ) . The Chelba-Jelinek model then executes a shift tran-  ( S , admired ) --+ ( NP , Mary ) ( VP , admired ) ( VP , admired ) -+( V , admired ) ( Np , oak ) ( NP , oak ) -+ ( Det , the ) ( N , oak ) ( N , oak ) -+ ( Adj , to wering > ( N , oak > ( N , oak > - ~( Adj , strong > ( N , oak > ( N , oak ) -+ ( hdj , old > ( N , oak ) ( NP , Mary ) -+ Mary(N , oak ) -+ oak Figure 1: Lexicalized contextfree grammarsition by generating the next word  . This is done in a manner similar to that of a trigram model except that  , rather than generate the next word based on the two preceding words  , it generates the next word based on the two topmost stack entries  . In this example the Chelba-Jelinek model generates the word " her " from  ( V , gave ) ( NP , man ) while a classical trigram model would generate " her " from " fatman "  . 
We now contrast Chelba-Jelinek style models with lexicalized PCFG models  . A PCFG is a contextfree grammar in which each production is associated with a weight in the interval  \[0  ,   1\] and such that the weights of the productions from any given nonterminal sum to  1  . For instance , the sentence Mary admired the towering strong old oak can be derived using a lexicalized PCFG based on the productions in Figure  1  . Production probabilities in the PCFG would reflec the likelihood that a phrase headed by a certain word can be expanded in a certain way  . Since it can be difficult to estimate fully these likelihoods  , we might restrict ourselves to models based on bilexical relationships  ( Eisner ,  1997) , those between pairs of words . The simplest bilexical relationship is a bigram statistic  , the fraction of times that " oak " follows " old  "  . Bilexical relationships for a PCFG include that between the head word of a phrase and the head word of a nonhead immediate constituent  , for instance . 
In particular , the generation of the above sentence using a PCFG based on Figure  1 would exploit a bilexical statistic between " to wering " and " oak " contained in the weight of the fifth production  . This bilexical relationship between " to wering " and " oak " would not be exploited in either a trigram model or in a Chelba-Jelinek style model  . In a Chelba-Jelinek style model one must generate " towering " before generating " oak " and then " oak " must be generated from  ( Adj , strong ) , ( Adj , old ) . In this example the Chelba-Jelinek model behaves more like a classical trigram model than like a PCFG model  . 
This contrast between PPDAs and PCFGs is formalized in theorem  1  , which exhibits a PCFG for which no stochastic parameterization of the corresponding shift -reduce parser yields the same probability distribution over strings  . 
That is , the standard shift-reduce translation from CFGs to PDAs cannot be generalized to the stochastic as e  . 
We give two ways of getting around the above difficulty  . The first is to construct a topdown PPDA that mimics directly the process of generating a PCFG derivation from the start symbol by repeatedly replacing the leftmost nonterminal in a sentential form by the right hand side of one of its rules  . Theorem 2 states that any PCFG can be translated into a topdown PPDA  . Conversely , theorem 3 states that any PPDA can be translated to a PCFG , not just those that are topdown PPDAs for some PCFG  . Hence PCFGs and general PPDAs define the same class of stochastic languages  . 
Unfortunately , topdown PPDAs do not allow the simple left-to -right processing that motivates shift-reduce PPDAs  . A second way around the difficulty formalized in theorem  1 is to encode additional information about the derivation context with richer stack and state alphabets  . Theorem 7 shows that it is thus possible to translate an arbitrary PCFG to a shift-reduce PPDA  . The construction requires a fair amount of machinery including proofs that any PCFG can be put in Chomsky normal form  , that weights can be renormalized to ensure that the result of grammar transformations can be made into PCFGs  , that any PCFG can be put in Greibach normal form , and , finally , that a Greibach normal form PCFG can be converted to a shift-reduce PPDA  . 
The construction also involves a blow up in the size of the shift-reduce parsing automaton  . 
This suggests that some languages that are concisely describable by a PCFG are not concisely describable by a shift-reduce PPDA  , hence that the class of PCFGs and the class of shift-reduce PPDAs impose different inductive biases on the reduce PPDAs to PCFGs  , there is also a blow up , if a less dramatic one , leaving open the possibility that the biases are incomparable  , and that neither formalism is inherently more concise  . 
Our main conclusion is then that , while the generative and shift-reduce parsing approaches are weakly equivalent  , they impose different inductive biases . 
2 Probabilistic and Weighted

For the remainder of the paper , we fix a terminal alphabet E and a nonterminal alphabet N  , to which we may add auxiliary symbols as needed . 
A weighted contextfree grammar ( WCFG ) consists of a distinguished start symbol SEN plus a finite set of weighted productions of the form X-~a  , ( alternately , u:X--~a ) , where XEN , aE ( Nt2E ) * and the weight u is a nonnegative real number . A probabilistic on text-free grammar ( PCFG ) is a WCFG such that for all X , ) - ~ u : x-~au = 1 . Since weights are nonnegative , this also implies that u <_ 1 for any individual production . 
A PCFG defines a stochastic process with sentential forms as states  , and leftmost rewriting steps as transitions . In the more general case of WCFGs , we can no longer speak of stochastic processes ; but weighted parse trees and sets of weighted parse trees are still well-defined notions  . 
We define a parse tree to be a tree whose nodes are labeled with productions  . Suppose node ~ is labeled X-~a\[Y1, . . . , Yn \], where we write a\[Y1, . . .   , Yn \] for a string whose nonterminal symbols are Y1  ,  . . . , Y  ~ . We say that ~' s nonterminal label is X and its weight is u  . The subtree rooted at ~ is said to be rooted in X  . ~ is well-labeled just in case it has n children , whose nonterminal labels are Y1 ,  .   .   . , Yn , respectively . 
Note that a terminal node is well-labeled only if a is empty or consists exclusively of terminal symbols  . We say a WCFGG admits a treed just in case all nodes of d are well-labeled  , and all labels are productions of G . Note that no requirement is placed on the nonterminal of the root node of d  ; in particular , it need not be S . 
We define the weight of a treed , denoted Wa(d ) , or W ( d ) if G is clear from context , to be the product of weights of its nodes . The depth r ( d ) of d is the length of the longest path from root to leaf in d  . The root production it ( d ) is the label of the root node . The root symbol p ( d ) is the lefthand side of ~ r ( d )  . The yield a ( d ) of the treed is defined in the standard way as the string of terminal symbols " parsed " by the tree  . 
It is convenient to treat the functions 7 r , p , a , and ras random variables over trees . We write , for example , p = X as an abbreviation for dip(d ) = X ; and WG ( p = X ) represents the sum of weights of such trees . If the sum diverges , we set WG ( p = X ) = o o . We call IIXHG = WG(p = X ) the norm of X , and IIGII =
IIS lla the norm of the grammar.
AWCFGG is called convergent if \[\[ G\[\[<o o.
If G is a PCFG then \[\[ G\[\[=WG(p "- S ) < 1 , that is , all PCFGs are convergent . A PCFGG is called consistent if \]\] GII=1 . A sufficient condition for the consistency of a PCFG is given in  ( Booth and Thompson ,  1973) . If ( I ) and ? are two sets of parse trees such that 0 < WG ( ~ ) < co we define PG ( ( I ) \]~ ) to be WG ( ~Nqt ) /WG ( kO )  . 
For any terminal string y and grammar G such that 0 < WG ( p--S ) < co we define PG ( Y ) to be Pa ( a = YIP = S )  . 
3 Stochastic Push-Down Automata
We use a somewhat nonstandar definition of pushdown automaton for convenience  , but all our results hold for a variety of essentially equivalent definitions  . In addition to the terminal alphabet ~ , we will use sets of stack symbols and states as needed  . A weighted pushdown automaton ( WPDA ) consists of a distinguished start state q0 , a distinguished start stack symbol X0 and a finite set of transitions of the following form where p and q are states  , a EEL . Je , X and Z1, . . . , Zn are stack symbols , and w is a nonnegative ralweight : x , pa ~ Zl . . . Zn , qAWPDA is a probabilistic pushdown automaton ( PPDA ) if all weights are in the interval \[0 ,   1\] and for each pair of a stack symbol X and a state q the sum of the weights of all transitions of the form X  , p~Z1 . . . Z = , q equal s1 . A machine configuration is a pair ( fl , q ) of a finite sequence fl of stack symbols ( a stack ) and a machine state q . A machine configuration is called halting if the stack is empty  . If M is a PPDA containing the transition X , p ~ Z1 . . . Zn , q then any configuration of the form ( fiX , p ) has figuration ( f ~ Z1 . . . Zn , q > where this transformation has the effect of " outputting " a if a ? e  . 
A complet execution of M is a sequence of transitions between configurations starting in the initial configuration <  X0  ,   q0> and ending in a configuration with an empty stack  . The probability of a complet execution is the product of the probabilities of the individual transitions between configurations in that execution  . For any PPDAM and yEE*we define PM ( Y ) to be the sum of the probabilities of all complete executions outputting y  . APPDAM is called consistent if ) - ~ ye ~* PM ( Y )  = 1 . 
We first show that the wellknown shift-reduce conversion of CFGs into PDAs cannot be made to handle the stochasticase  . Given a ( nonprobabilistic ) CFGG in Chomsky normal form we define a ( nonprobabilistic ) shift-reduce PDASIt ( G ) as follows . The stack symbols of SIt ( G ) are taken to be nonterminals of G plus the special symbols T and ?  . The states of SR ( G ) are in one-to-one correspondence with the stack symbols and we will abuse notation by using the same symbols for both states and stack symbols  . The initial stack symbol is 1 and the initial state is ( the state corresponding to ) _L . For each production of the form X--+a inG the PDASIt  ( G ) contains all shift transitions of the following form 
Y , Z-~Y Z , X
The PDASR ( G ) also contains the following termination transitions where S is the start symbol of G  . 
E1, S-+,T
I,T-~,T
Note that if G consists entirely of productions of the form S -+ a these transition suffice  . More generally , for each production of the form X-+Y Z in G the PDASR  ( G ) contains the following reduce transitions . 
Y , Z-~, X
All reachable configurations are in one of the following four forms where the first is the initial configuration  , the second is a template for all intermediate configurations with a EN *  , and the last two are terminal configurations . 
< 1,1>, <11., x >, < I,T >, T >
Furthermore , a configuration of the form ( l_l_a , X ) can be reached after outputting y if and only if a X : ~ y  . In particular , the machine can reach configuration (?_L , S ) outputting y if and only if S : ~ y . So the machine SR ( G ) generates the same language as G . 
We now show that the shift-reduce translation of CFGs into PDAs does not generalize to the stochasticase  . For any PCFGG we define the underlying CFG to be the result of erasing all weights from the productions of G  . 
Theorem 1 There exists a consistent PCFG G in Chomsky normal  . form with underlying CFG G ' such that no consistent weighting M of the PDASR  ( G  ~ ) has the property that PM ( Y )  =
Pa(u ) for all Ue
To prove the theorem take G to be the following grammar  . 

S-~AX1, S3+BY1
X ,-~ CX2, X2-~CA
YlCy2, Y2A,CB
A-~a , S-~b , C-~c
Note that G generates acca and bcc beach with probability ?  . Let M be a consistent PPDA whose transitions consist of some weighting of the transitions of SR  ( G ' )  . We will assume that PM ( Y ) = PG ( Y ) for all yEE * and derive a contradiction . C all the nonterminals A , B , and C preterminals . Note that the only reduce transitions in SR ( G  ~ ) combining two preterminals are C , A - ~ , X2 and C , B - ~ , Y2 . Hence the only machine configuration reachable after outputting the sequence ace is  (  . I__LAC , C > . If PM ( acca ) --? and PM ( accb ) --0 then the machine in configuration (  . I_?AC , C > must deterministically move to configuration ( I?ACC , A > . But this implies that configuration ( IIBC , C > also deterministically moves to configuration <?? BCC  , A > so we have PM ( bccb ) -=0 which violates the assumptions about M .   , , Although the standard shift-reduce translation of CFGs into PDAs fails to generalize to the stochasticase  , the standard topdown conversion easily generalizes  . A topdown PPDA is one in which only ~ transitions can cause the stack to grow and transitions which output a word must pop the stack  . 

Theorem 2 Any string distribution definable by a consistent PCFG is also definable by a topdown PPDA  . 
Here we consider only PCFGs in Chomsky normal form -- the generalization to arbitrary PCFGs is straightforward  . Any PCFG in Chomsky normal form can be translated to a topdown PPDA by translating each weighted production of the form X--~YZ to the set of expansion moves of the form W  , X ~ W Z , Y and each production of the form X-~a to the set of pop moves of the form Z  , X 72 -' ~ , Z . ? We also have the following converse of the above theorem  . 
Theorem 3 Any string distribution definable by a consistent PPDA is definable by a PCFG  . 
The proof , omitted here , uses a weighted version of the standard translation of a PDA into a CFG followed by a renormalization step using lemma  5  . We note that it does in general involve an increase in the number of parameters in the derived PCFG  . 
In this paper we are primarily interested in shift -reduce PPDAs which we now define formally  . In a shift-reduce PPDA there is a one-to-one correspondence bt ween states and stack symbols and every transition has one of the following two forms  . 
Y , Za-~Y Z , Xa?E

Y , Z-~+, X
Transitions of the first type are called shift transitions and transitions of the second type are called reduce transitions  . Shift transitions output a terminal symbol and push a single symbol on the stack  . Reduce transitions are e-transitions that combine two stack symbols  . 
The above theorems leave open the question of whether shift-reduce PPDAs can express arbitrary contextfree distributions  . Our main theorem is that they can . To prove this some additional machinery is needed . 
4 Chomsky Normal Form
A PCFG is in Chomsky normal form ( CNF ) if all productions are either of the form X-St a  , aEE or X-~Y1Y2 , Y 1 , Y2EN . Our next theorem states , in essence , that any PCFG can be converted to Chomsky normal form  . 
Theorem 4For any consistent PCFGG with PG ( e ) <1 there exists a consistent PCFGC ( G ) in Chomsky normal form such that , for all yE

Pa(y)-ea(yly#e)
PC(G)(Y)--1-Pa(e)
To prove the theorem , note first that , without loss of generality , we can assume that all productions in G are of one of the forms X--~Y Z  , X-5 tY , X - ~ a , or X-Y+e . More specifically , any production ot in one of these forms must have the form X  -5t ? rfl where a and fl are nonempty strings . Such a production can be replaced by X-~AB , A-~a , and B 2+ fl where A and B are fresh nonterminal symbols . 
By repeatedly applying this binarization transformation we get a grammar in the desired form defining the same distribution on strings  . 
We now assume that all productions of G are in one of the above four forms  . This implies that a node in a G-derivation has at most two children  . A node with two children will be called a branching node  . Branching nodes must be labeled with a production of the form X-~YZ  . Because G can contain productions of the form X- - ~ e there may be arbitrarily large G-derivations with empty yield  . 
Even G-derivations with nonempt yield may contain arbitrarily large subtrees with empty yield  . A branching node in the G-derivation will be called ephemeral if either of its children has empty yield  . Any G-derivation d with la ( d ) l_ 2 must contain a unique shallowest non-ephemeral branching node  , labeled by some production X ~ Y Z . In this case , define fl(d ) = YZ . Otherwise ( la(d)l < 2), let fl(d ) = a(d) . We say that a nonterminal X is nontrivial in the grammar G if Pa  ( a  #eIP = X ) > O . 
We now define the grammar G'to consist of all productions of the following form where X  , Y , and Z are nontrivial nonterminals of Ganda is a terminal symbol appearing in G  . 
XPG (~= Y Z ~ p = x,~#~) Y Z
XPG (~= a12 += x,~?~)a
We leave it to the reader to verify that G ' has the property stated in theorem  4  .  ?
The above proof of theorem 4 is non-constructive in that it does not provide any 
PG(Z = Y ZIp = x ,  #and Pa(Z = a\[p = X , a?e ) . However , it is not difficult to compute probabilities of the form PG  ( ?\[ p = X , r <_ t+1 ) from probabilities of the form PG ( ( I ) \] p = X , v_<t ) , and PG ( ? IP = X ) is the limit ast goes to infinity of P a ( ( I ) \] p = X , r_<t ) . We omit the details here . 
from X equals 1:=~: x-~\[Y1 . . . . . y . \] u ~-- E . x -, oIv, . . . . . Y . lIIlla = . . . . . y . \] ul-LwG ( p == wo ( p=x ) Wa ( p = X )  -  1   5 Renormalization A nonterminal X is called reachable in a grammar G if either X is S or there is some  ( recursively ) reachable nonterminal Y such that G contains a production of the form Y-~a where contains X  . A nonterminal X is nonempty in G if G contains X - ~ awhereu >  0 and a contains only terminal symbols , or G contains X-~o ~\ [ Y1 ,   .   .   .   , Yk\]where u > 0 and each 1~is ( recursively ) nonempty . A WCFG G is proper if every nonterminal is both reachable and nonempty  . It is possible to efficiently compute the set of reachable and nonempty nonterminals in any grammar  . Furthermore , the subset of productions involving only nonterminals that are both reachable and nonempty defines the same weight distribution on strings  . 
So without loss of generality we need only consider proper WCFGs  . A reweighting of G is any WCFG derived from G by changing the weights of the productions of G  . 
Lemma 5 For any convergent proper WCFG
G , there exists are weighting G t of G such that G ~ is a consistent PCFG such that for all terminal strings y we have PG '  ( Y ) = Pa ( Y )  . 
Proof . " Since G is convergent , and every nonterminal X is reachable , we must have IIXIla < oo . We now renormalize all the productions from X as follows  . For each production X-~a\[Y1, .   .   . , Yn \] we replace uby ? = IIIIG

To show that G'is a PCFG we must show that the sum of the weights of all productions For any parse treed admitted by Gletd ~ be the corresponding tree admitted by G ~  , that is , the result of reweighting the productions in d . One can show by induction on the depth of parse trees that if p  ( d ) = X then Wc , ( d ') =\[- ~ GWG(d) . 
Therefore IIXIIG , = ~~ d\[p(d ) = XWG , ( d ') - ~ ~ ~ alo(e ) = xW a(d ) == 1 . In particular , Ilaql = Il Slla , - 1 , that is , G ' is consistent . This implies that for any terminal string Y we have PG '  ( Y ) = li-~Wa , ( a = y , p = S ) = Wa , ( a = y , p = S) . Furthermore , for any treed with p(d ) = S we have Wa , ( d ') = ~\ [ ~ cW a(d ) and so WG , ( a = y , p = S)-~WG(a = y , p = S ) = Pc(Y ) .  "  6 Greibach Normal Form A PCFG is in Greibach normal form  ( GNF ) if every production X-~a satisfies ( ~ EEN * . 
The following holds :
Theorem 6For any consistent PCFG G in
CNF there exists a consistent PCFG G ~ in GNF such that Pc  , ( Y ) = Pa(Y ) for ye E * . 
Proof : A left corner G-derivation from X to Y is a G-derivation from X where the leftmost leaf  , rather than being labeled with a production , is simply labeled with the nonterminal Y . For example , if G contains the productions X ~ Y Z and Z-~a then we can construct a left corner G-derivation from X to Y by building a tree with a root labeled by XZ  . ~Y Z , a left child labeled with Y and a right child labeled with Z-~a  . The weight of a left corner G-derivation is the product of the productions on the nodes  . A tree consisting of a single node labeled with X is a left corner G-derivation from 
X to X .
For each pair of nonterminals X , Y in G we introduce a new nonterminal symbol X/Y . 

The H-derivations from X/Y will be in one to one correspondence with the left-corner G-derivations from X to Y  . For each production in G of the form X ~ a we include the following in H where S is the start symbol of G: 
S--~a S/X
We also include in Hall productions of the following form where X is any nonterminal in G : x/x If G consists only of productions of the form S - ~ a these production suffice  . More generally , for each nonterminal X/Y of H and each pair of productions U~Y Z  , W~-~a we include in H the following :
X/Y ~2aZ/WX/U
Because of the productions X/X-~e , WH(#:X/X ) > 1 , and H is not quite in GNF . These two issues will be addressed momentarily . 
Standard arguments can be used to show that the H -derivations from X/Y are in one-to-one correspondence with the left corner G-derivations from X to Y  . Furthermore , this one-to-one correspondence preserves weight- - if d is the H-derivation rooted at X/Y corresponding to the left corner G-derivation from X to Y then WH  ( d ) is the product of the weights of the productions in the G-derivation  . 
The weight-preserving one-to-one correspondence between left-corner G-derivations from X to Y and H-derivations from X/Y yields the following  . 
WH ( ao ~ )  : ~'~ ( S_U + aS/X ) EHUWH ( ~ r : Ollp - - - S/X ) 
Po(a )
Theorem 5 implies that we can reweight the proper subset of H  ( the reachable and nonempty productions of H ) so as to construct a consistent PCFG g with Pj ( (~ ) = PG ( ~ )  . To prove theorem 6 it now suffices to show that the productions of the form X/X-~e can be eliminated from the PCFG J  . Indeed , we can eliminate the eproductions from J in a manner similar to that used in the proof of theorem  4  . A node in an J-derivation is ephemeral if it is labeled X-~e for some X  . We now define a function 7 on J-derivations d as follows . If the root of d is labeled with X-~a Y Z then we have four subcases  . If neither child of the rootise phemeral then 7 ( d ) is the string a Y Z . If only the left child is ephemeral then 7 ( d ) is a Z . If only the right child is ephemeral then 7 ( d ) is a Y and if both children are ephemeral then7 ( d ) is a . Analogously , if the root is labeled with X-~a Y , then 7 ( d ) is a Y if the child is note phemeral and a otherwise  . If the root is labeled with X-~ethen7 ( d ) is e . 
A nonterminal X in K will be called trivial if P j  ( 7 = eIP = X )  = 1 . We now define the final grammar G'to consist of all productions of the following form where X  , Y , and Z are nontrivial nonterminals appearing in J and a is a terminal symbol appearing in J  . 
XPj(a = aI__~=X,"y ? ?) a
X p j ( a = a Y ~_ ~ = X , " y C e ) a Y
XPJ(a = a Y Zl - ~ p = X ' ~?) a Y Z
As in section 4 , for every nontrivial nonterminal X in K and terminal string  ( ~ we have PK ( a = ( ~IP = X ) = Pj ( a = aIP = X , a~e ) . In particular , since Pj(e ) = PG(()=0 , we have the following :== Pj ( a = alp = S ) = Pj ( a ) = Pa (   ) The PCFGK is the desired PCFG in Greibach normal form  . ? The construction in this proof is essentially the standard left-corner transformation  ( Rosenkrantz and II ,  1970) , as extended by Salomaa and Soittola (1978 , theorem 2 . 3) to algebraic formal power series . 
7 The Main Theorem
We can now prove our main theorem.
Theorem 7 For any consistent PCFGG there exists a shift -reduce PPDAM such that 
PM(Y ) = PG(Y ) for all yE~*.
Let G be an arbitrary consistent PCFG . By theorems 4 and 6~ we can assume that G consists of productions of the form S-~e and form not mentioning S  . We can then replace the rule S 1_:+~ S ~ with all rules of the form S 0-__~  ) ~' a where G contains S~~'-+a . We now assume without loss of generality that G consists of a single production of the form S - ~ eplus productions in Greibach normal form not mentioning S on the right hand side  . 
The stack symbols of M are of the form W ~ where ceEN * is a proper suffix of the right hand side of some production in G  . For example , if G contains the production X-~a Y Y Z then the symbols of M include Wy z  , Wy , and We . The initial state is Ws and the initial stack symbol is ?  . We have assumed that G contains a unique production of the form S-~e  . We include the following transition in M corresponding to this production  . 

Then , for each rule of the form X-~a ~ in G and each symbol of the form Wx  , ~ we include the following in M :
Z , Wx . ~ Z Wx ., Wz
We also include all " postprocessing " rules of the following form: 
Wx~W ~ W ~ ~., 1?,W ~ ~, T
I,T-:+, T
Note that all reduction transitions are deterministic with the single exception of the first rule listed above  . The nondeterministic shift transitions of M are in one-to-one correspondence with the productions of G  . This yields the property that PM(Y ) = PG(Y ) .  ?  8 Conclusions The relationship between PCFGs and PPDAs is subtler than a direct application of the classical constructions relating general CFGs and PDAs  . Although PCFGs can be concisely translated into topdown PPDAs  , we conjecture that there is no concise translation of PCFGs into shift-reduce PPDAs  . Conversely , there appears to be no concise translation of shift-reduce PP-DAs to PCFGs  . Our main result is that PCFGs and shift-reduce PPDAs are intertranslatable  , hence weakly equivalent . However , the non-conciseness of our translations i consistent with the view that stochastic topdown generation models are significantly different from shift-reduce stochastic parsing models  , affecting the ability to learn a model from examples  . 
References
Alfred V . Aho and Jeffrey D . Ullman .  1972 . The Theory of Parsing , Translation and Compiling , volume I . Prentice-Hall , Englewood Cliffs , New

Ezra Black , Fred Jelinek , John Lafferty , David Magerman , Robert Mercer , and Salim Roukos . 
1992 . Towards history-based grammars : Using richer models for probabilistic parsing  . In Proceedings of the 5th DARPA Speech and Natural
Language Workshop.
Taylor Booth and Richard Thompson .  1973 . Applying probability measures to abstract languages  . 
IEEE Transactions on Computers , C-22(5):442-450.
Ted Briscoe and John Carroll .  1993 . Generalized probabilistic LR parsing of natural language  ( corpora ) with unification-based grammars . Computational Linguistics , 19(1):25-59 . 
Eugene Charniak .  1997 . Statistical parsing with a contextfree grammar and word statistics  . 
In Fourteenth National Conference on Artificial Intelligence  , pages 598-603 . AAAI Press/MIT

Ciprian Chelba and Fred Jelinek .  1998 . Exploiting syntactic structure for language modeling  . In
COLINGACL'98, pages 225-231.
Michael Collins .  1998 . Head-Driven Statistical Models for Natural Language Parsing  . Ph . D . thesis,
University of Pennsylvania.
Jason Eisner .  1997 . Bilexical grammars and a cubic-time probabilistic parser  . In Proceedings of the International Workshop on Parsing Technologies  . 
David M . Magerman .  1994 . Natural Language Parsing as Statistical Pattern Recognition  . Ph . D . thesis , Department of Computer Science , Stanford

Adwait Ratnaparkhi .  1997 . A linear oberved time statistical parser based on maximum entropy models  . In Claire Cardie and Ralph Weischedel , editors , Second Conference on Empirical Methods in Natural Language Processing  ( EMNLP-2 )  , Somerset , New Jersey . Association For Computational Linguistics . 
Daniel J . Rosenkrantz and Philip M . Lewis II .  1970 . 
Deterministic left corner parser . In IEEE Conference Record of the 11th Annual Symposium on Switching and Automata Theory  , pages 139-152 . 
Arto Saloma and Matti Soittola .  1978 . Automata-Theoretic Aspects of Formal Power Series . 
Springer-Verlag , New York.

