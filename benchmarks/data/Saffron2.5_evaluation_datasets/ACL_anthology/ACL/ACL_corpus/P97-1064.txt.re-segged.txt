A Structured Language Model
Ciprian Chelba
The Johns Hopkins University
CLSP , Barton Hall 320
3400 N . Charles Street , Baltimore , MD-21218
chelba@jhu.edu
Abstract
The paper presents a language model that develops yntactic structure and uses it to extract meaningful information from the word history  , thus enabling the use of long distance dependencies  . The model assigns probability to every joint sequence of words-binary-parse-structure with head word annotation  . The model , its probabilistic parametrization , and a set of experiments meant to evaluate its predictive power are presented  . 
the dog I heard yesterday barked
Figure 1: Partial parse '?" ~ . ( ~ I h_-=*l)~_-I\[h_O ~ w_l . . . w . .p . . . . . . . . w q . .  . w ~ rw_lr + ll . . . w_k w_lk + l . . . . . w_n </ s >
Figure 2: A word-parse k-prefix 1 Introduction The main goal of the proposed project is to develop a language model  ( LM ) that uses syntactic structure . 
The principles that guided this propo ? alwere : ? the model will develop syntactic knowledge as a builtin feature  ; it will assign a probability to every joint sequence of words-binary-parse-structure  ; ? the model should operate in a left-to-right manner so that it would be possible to decode word lattices provided by an automatic speech recognizer  . 
The model consists of two modules : a next word predictor which makes use of syntactic structure as developed by a parser  . The operations of these two modules are intertwined  . 
2 The Basic Idea and Terminology
Consider predicting the word barked in the sentence : the dog I heard yesterday barked again  . 
A 3gram approach would predict barked from ( heard , yesterday ) whereas it is clear that the predictor should use the word dog which is outside the reach of even  4grams   . Our assumption is that what enables us to make a good prediction of barked is the syntactic structure in the past  . The correct partial parse of the word history when predicting barked is shown in Figure  1  . 
The word dog is called the head word of the constituent  ( the ( dog (   .   .   .   )   ) ) and dog is an exposed headword when predicting barked--topmost head word in the largest constituent that contains it  . The syntactic structure in the past filters out irrelevant words and points to the important ones  , thus enabling the use of long distance information when predicting the next word  . Our model will assign a probability P ( W , T ) to every sentence W with every possible binary branching parse T and every possible head word annotation for every constituent of T  . Let W be a sentence of length I words to which we have prepended < s > and appended </ s > so that wo = < s > and wl + l = </ s >  . Let Wk be the word k-prefix w0 . .  . wk of the sentence and WkT ~ the word-parse k -prefix  . To stress this point , a word-parse k-prefix contains only those binary trees whose span is completely included in the word k-prefix  , excluding wo = < s > . Single words can be regarded as root-only trees . Figure 2 shows a word-parse k-prefix ; h_0 . . h_-m  are the exposed headwords . A complete parse -- Figure 3 -- is any binary parse of the wl . .  . wi </ s > sequence with the restriction that </ s > is the only allowed head word  . 
498 ~ D < s > w_l . . . . . . w </ s >
Figure 3: Complete parse
Note that ( wl .   .   . w i ) needn't be a constituent , but for the parses where it is , there is no restriction on which of its words is the head word  . 
The model will operate by means of two modules : ? PREDICTOR predicts the next word wk+l given the word-parse k-prefix and then passes control to the PARSER  ; ? PARSER grows the already existing binary branching structure by repeatedly generating the transitions adjoin-left or adjoin-r ight until it passes control to the PREDICTOR by taking a null transition  . 
The operations performed by the PARSER ensure that all possible binary branching parses with all possible head word assignments for the w ~  . . . wk word sequence can be generated . They are illustrated by Figures 46 . The following algorithm describes how the model generates a word sequence with a complete parse  ( see Figures 36 for notation ) :
Transition t ; // aPARSER transition generate < s > ; do predict next_word ; // PREDICTOR do // PARSER if ( T_-l !=< s > ) if ( h_0==</s > ) t = adjoin-right ; else t = adjoin-left , right , null ; else I ; = null ; while ( t != null ) while ( ! ( h_0==</s > & ET_-1 == < s > ) ) t = adjoin-right ; // adjoin < s > ; DONE It is easy to see that any given word sequence with a possible parse and head word annotation is generated by a unique sequence of model actions  . 
3 Probabilistic Model
The probability P ( W , T ) can be broken into : 1+1pP(W , T ) = l-L=1\[(wk/Wk-lTk-1)"~\]~21P ( tklwk , Wk- , Tk-1 , t  ~ .   .   . t ~_ l ) \] where : ? Wk-lTk-1 is the word-parse ( k-1 ) -prefix ? wk is the word predicted by PP~EDICTOR ? Nk-  1 is the number of adjoin operations the PARSER executes before passing control to the PREDICTOR  ( the N~-th operation at position k is the null transition  )  ; N ~ is a function of Th_-2h_-Ih_O
Figure 4: Before an adjoin operation h . ~(- z)--h_(-2)h . _ o . h . _(-x )
Figure 5: Result of adjoin-left h'_*t ) . h_(o2) h*_O--n_O h_ . 

Figure 6: Result of adjoin-right?t ~ denotes the ith PARSER operation carried out at position k in the word string  ; tkEadjoin-left , adjoin-right , i < Nk , = null , i = Nk Our model is based on two probabilities :
P ( wk/Wk-lTk-1) (1)
P ( t~/Wk,Wk-lTk-1, t ~... t ~_l ) (2)
As can be seen ( wk , Wk-lTk-1, tkk .   .   . ti_l ) is one of the Nk word-parse k-prefixes of WkTk , i = 1 , Nk at position k in the sentence . 
To ensure a proper probabilistic model we have to make sure that  ( 1 ) and ( 2 ) are well defined conditional probabilities and that the model halts with probability one  . A few provisions need to be taken : ? P ( null/WkTk )  = 1 , if T_-1 ==< s > ensures that < s > is adjoined in the last step of the parsing process  ; ? P ( adjoin-right/WkTk ) = 1 , if h_0 ==</ s > ensures that the head word of a complete parse is < Is >  ; ?3~> Os . t . P ( wk = </ s >/ Wk-lT~-l ) >_ e ,   VWk-lTk-1 ensures that the model halts with probability one . 
3.1 The first model
The first term (1) can be reduced to an ngram LM , P ( w~/W~-lTk-1) = P ( wk/W~-l .   .   . Wk-n+l ) . 
A simple alternative to this degenerate approach would be to build a model which predicts the next word based on the preceding  p1 exposed head words and n1 words in the history , thus making the following equivalence classification :\[ WkTk\]=h_O  .   . h_-p+2, iUk-l .   . Wk-n+1 . 

The approach is similar to the trigger LM ( Lau93) , the difference being that in the present work triggers are identified using the syntactic structure  . 
3.2 The second model
Model ( 2 ) assigns probability to different binary parses of the word k-prefix by chaining the elementary operations described above  . The workings of the PARSER are very similar to those of Spat-ter  ( Jelinek 94 )  . It can be brough to the full power of S patter by changing the action of the adjoin operation so that it takes into accounthe termi-nal /nontermin all bels of the constituent proposed by adjoin and it also predicts the nonterminal label of the newly created constituent  ; PREDICTOR will now predic the next word along with its POS tag  . The best equivalence classification of the WkTk word-parse k-prefix is yet to be determined  . The Collins parser ( Collins 96 ) shows that dependency-grammar-like bigram constraints may be the most adequate  , so the equivalence classification \[ WkTk\] should contain at least  ( h_0 , h_-1 . 
4 Preliminary Experiments
Assuming that the correct partial parse is a function of the word prefix  , it make sense to compare the word level perplexity  ( PP ) of a standard ngram LM with that of the P ( wk/Wk-ITk-1 ) model . We developed and evaluated four LMs : ? 2 bigram LMs P ( wk/Wk-lTk-1 ) = P ( Wk/Wk-1 ) referred to as W and w , respectively ; wk-1 is the previous ( word , POS tag ) pair ; ?2P ( wk/Wk-ITk--1) = P(wjho ) models , referred to as H and h , respectively ; h0 is the previous exposed ( head word , POS/non-term tag ) pair ; the parses used in this model were those assigned manually in the Penn Treebank  ( Marcus 95 ) after undergoing head word percolation and binarization  . 
All four LMs predict a word wk and they were implemented using the Maximum Entropy Modeling Toolkit  1   ( Ristad 97 )  . The constraint templates in the W , H models were : 4<=<*>_<*> <7> ; P-<=<7>_<*><7> ;  2 <= <?>_<7> <?> ;  8 <= <*>_<?> <7> ; and in the w , h models they were : 4<=<*>_<*> <7> ;  2 <= <7>_<*> <7> ;  < . > denotes a don't care position , < 7>_<7 > a(word , tag ) pair ; for example , 4 <=< 7>_<*> <7> will trigger on all (( word , any tag ) , predicted-word ) pairs that occur more than 3 times in the training data . 
The sentence boundary is not included in the PP calculation  . Table 1 shows the PP results along with Iftp://ftp . cs . princeton . edu/pub/packages/memt the number of parameters for each of the  4 models described . 
HLMPP\[parara HLM PP paramI
H3 12 206 540 h4 10 10 24 37
Table 1: Perplexity results 5 Acknowledgements The author thanks to Frederick Jelinek  , Sanjeev Khudanpur , Eric Ristad and all the other members of the Dependency Modeling Group  ( Stolcke 97 )  , WS96 DoD Workshop at the Johns Hopkins University . 
References
Michael John Collins .  1996 . A new statistical parser based on bigram lexical dependencies  . In Proceedings of the 3~th Annual Meeting of the Association for Computational Linguistics  ,  184-191 , 
Santa Cruz , CA.
Frederick Jelinek .  1997 . Information extraction from speech and text-- course notes  . The Johns Hopkins University , Baltimore , MD . 
Frederick Jelinek , John Lafferty , David M . Magerman , Robert Mercer , Adwait Ratnaparkhi , Salim Roukos .  1994 . Decision Tree Parsing using a Hidden Derivational Model  . In Proceedings of the Human Language Technology Workshop  ,  272-277 . 

Raymond Lau , Ronald Rosenfeld , and Salim Roukos .  1993 . Trigger-based language models : a maximum entropy approach  . In Proceedings of the IEEE Conference on Acoustics  , Speech , and Signal Processing , volume 2 ,  4548 , Minneapolis . 
Mitchell P . Marcus , Beatrice Santorini , Mary Ann Marcinkiewicz .  1995 . Building a large annotated corpus of English : the Penn Treebank  . Computational Linguistics , 19(2):313-330 . 
Eric Sven Ristad .  1997 . Maximum entropy modeling toolkit . Technical report , Department of Com-puter Science , Princeton University , Princeton , 
NJ , January 1997, v . 1.4 Beta.
Andreas Stolcke , Ciprian Chelba , David Engle , Frederick Jelinek , Victor Jimenez , Sanjeev Khudanpur , Lidia Mangu , Harry Printz , Eric Sven Ristad , Roni Rosenfeld , Dekai Wu .  1997 . Structure and Performance of a Dependency Language Model  . In Proceedings of Eurospeech'97, PJ aodes,
Greece . To appear.

