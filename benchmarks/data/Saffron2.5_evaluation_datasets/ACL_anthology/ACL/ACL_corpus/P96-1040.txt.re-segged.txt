The Rhythm of Lexical Stress in Prose
Doug Beeferman
School of Computer Science
Carnegie Mellon University
Pittsburgh , PA 15213, USA
dougb+@cs , cmu.edu
Abstract
" Proser hythm " is a widely observed but
scarcely quantified phenomenon . We de-
scribe an information-theoretic model for measuring the regularity of lexical stress in English texts  , and use it in combination with trigram language models to demonstrate a relationship between the probability of word sequences in English and the amount of rhythm present in them  . We find that the stream of lexical stress in text from the Wall Street Journal has an entropy rate of less than  0  . 75 bits per syllable for common sentences . We observe that the average number of syllables per word is greater for rarer word sequences  , and to normalize for this effect we run control experiments to show that the choice of word order contributes significantly to stress regularity  , and increasingly with lexical probability . 
1 Introduction
Rhythminheres in creative output , asserting itself as the meter in music , theiambs and trochees of poetry , and the uniformity in distances between objects in art and architecture  . More subtly there is widely believed to be rhythm in English prose  , reflecting the arrangement of words , whether deliberate or subconscious , to enhance the perceived acoustic signal or reduce the burden of remembrance for the reader or author  . 
In this paper we describe an information-theoretic model based on lexical stress that substantiates this common perception and relates stress regularity in written speech  ( which we shall equate with the intuitive notion of " rhythm "  ) to the probability of the text itself . By computing the stress entropy rate for both a set of Wall Street Journal sentences and a version of the corpus with randomized intrasentential word order  , we also find that word order contributes significantly to rhythm  , particularly within highly probable sentences . We regard this as a first step in quantifying the extent to which metrical properties influence syntactic hoice in writing  . 
1.1 Basics
In speech production , syllables are emitted as pulses of sound synchronized with movements of the mus-culature in the ribcage  . Degrees of stress arise from variations in the amount of energy expended by the speaker to contract hese muscles  , and from other factors such as intonation . Perceptually stress is more abstractly defined , and it is often associated with " peaks of prominence " in some representation of the acoustic input signal  ( Ochsner ,  1989) . 
Stress as a lexical property , the primary concern of this paper , is a function that maps a word to a sequence of discrete levels of physical stress  , approximating the relative emphasis given each syllable when the word is pronounced  . Phonologists distinguish between three levels of lexical stress in English : primary  , secondary , and what we shall call weak for lack of a better substitute for unstressed  . For the purposes of this paper we shall regard stresses as symbols fused serially in time by the writer or speaker  , with words acting as building blocks of predefined stress sequences that may be arranged arbitrarily but never broken apart  . 
The culminative property of stress states that every content word has exactly one primary -stressed syllable  , and that whatever syllables remain are subordinate to it  . Monosyllabic function words such as the and of usually receive weak stress  , while content words get one strong stress and possibly many secondary and weak stresses  . 
It has been widely observed that strong and weak tend to alternate at " rhythmically ideal disyllabic distances "  ( Kager , 1989a ) . " Ideal " here is a complex function involving production  , perception , and many unknowns . Our concern is not top in point his ideal , nor to answer precisely why it is sought by speakers and writers  , but to gauge to what extent it is sought . 
We seek to investigate , for example , whether the avoidance of primary stress clash , the placement of two or more strongly stressed syllables in succession  , influence syntactic hoice . In the Wall Street Jour-ing issues re-cent-ly were filed with the Se-cur-i-ties and Exchange Com-mis-sion "  . The phrase " recently were filed " can be syntactically permuted as " were filed recently  "  , but this clashes filed with the first syllable of recently  . The chosen sentence avoids consecutive primary stresses  . Kager postulates with a decidedly information theoretic under-tone that the resulting binary alternation is " simply the maximal degree of rhythmic organization compatible with the requirement that adjacent stresses are to be avoided  . " ( Kager , 1989a ) Certainly we are not proposing that a hard decision based only on metrical properties of the output is made to resolve syntactic hoice ambiguity  , in the case above or in general . Clearly semantic emphasis has its say in the decision  . But it is our belief that rhythm makes a nontrivial contribution  , and that the tools of statistics and information theory will help us to estimate it formally  . Words are the building blocks . How much do their selection ( diction ) and their arrangement ( syntax ) act to enhancer hythm?1 . 2 Past mode ls and quant i f i cat ions Lexical stress is a well-studied subject at the intra word level  . Rules governing how to map a word's orthographic or phonetic transcription to a sequence of stress values have been searched for and studied from rules-based  , statistical , and connectionist perspectives . 
Word-external stress regularity has been denied this level of attention  . Patterns in phrases and compound words have been studied by Halle  ( Halle and Vergnaud , 1987) and others , who observe and reformulate such phenomenas the emphasis of the penultimate constituent in a compound noun  ( National Center for Supercomputing Applications , for example . ) Treatment of lexical stress across word boundaries is carce in the literature  , however . 
Though proser hythminquiry is more than a hundred years old  ( Ochsner ,  1989) , it has largely been dismissed by the linguistic community as irrelevant to formal models  , a same recuriosity for literary analysis . This is partly because formal methods of inquiry have failed to present a compelling case for the existence of regularity  ( Harding ,  1976) . 
Past attempts to quantify proser hythmmay be classified as perception-oriented or signal -oriented  . 
In both cases the studies have typically focussed on regularities in the distance between peaks of prominence  , or interstress intervals , either perceived by a human subject or measured in the signal  . The former class of experiments relies on the subjective segmentation of utterances by a necessarily limited number of participants - -subject stapping out the rhythms they perceive in a wave form on a recording device  , for example ( Kager , 1989b ) . To say nothing of the psychoacoustic biases this methodology introduces  , it relies onto olittle data for anything but a sterile set of means and variances  . 
Signal analysis , too , has not yet been applied to very large speech corpora for the purpose of investigating prose rhythm  , though the technology now exists to lend efficiency to such studies  . The experiments have been of smaller scope and geared toward detecting is ochrony  , regularity in absolute time . Jassem et al ( Jassem , Hill , and Witten ,  1984 ) use statistical techniques such as regression to analyze the duration of what they term rhythm units  . 
Jassem postulates that speech is composed of extra -syllable narrow rhythm units with roughly fixed duration independent of the number of syllable constituents  , surrounded by varia . ble-length an acruses . 
Abercrombie ( Abercrombie ,  1967 ) views speech as composed of metrical feet of variable length that begin with and are conceptually highlighted by a single stressed syllable  . 
Many experiments lead to the common conclusion that English is stress-timed  , that there is some regularity in the absolute duration between strong stress events  . In contrast to postulated syllable-timed languages like French in which we find exactly the inverseffect  , speakers of English tend to expand and to contract syllable streams so that the duration between bounding primary stresses matches the other intervals in the utterance  . It is unpleasant for production and perception alike  , however , when too many weak-stressed syllables are forced into such an interval  , or when this amount of " p adding " varies wildly from one interval to the next  . Proser hythmanalyst so far have not considered the syllable stream independent from syllabic  , phonemic , or interstress duration . In particular they haven't measured the regularity of the purely lexical stream  . 
They have instead continually re-answered questions concerning is ochrony  . 
Given that speech can be divided into interstress units of roughly equal duration  , we believe the more interesting question is whether a speaker or writer modifies his diction and syntax to fit a regular number of syllables into each unit  . This question can only be answered by a lexical approach  , an approach that pleasingly lends itself to efficient experimentation with very large amounts of data  . 
2 Stress entropy rate
We regard every syllable as having either strong or weak stress  , and we employ a purely lexical , context independent mapping , a pronunciation dictionary a , to tell us which syllables in a word receive which level of stress  . We base our experiments on a binary-valued symbol set  E1 = W , Sandonaternary-valued symbol set E2 = W , S , P , where ' W ' indicates weak stress , ' S ' indicate strong stress , 1 We use the ll 6 , 000-entry CMU Pronouncing Dictionary version 0 . 4 for all experiments in this paper . 
303 i ( ,
Figure 2: A 5gram model viewed as a first-order
Markov chain and ' P ' indicates a pause . Abstractly the dictionary maps words to sequences of symbols from primary  , secondary , unstressed , which we interpret by downsampling to our binary system -- primary stress is strong  , non-stressi weak , and secondary stress ( '2' ) we allow to be either weak or strong depending on the experiment we are conducting  . 
We represent a sentence as the concatenation of the stress sequences of its constituent words  , with ?' P'symbols ( for the N2 experiments ) breaking the stream where natural pauses occur . 
Traditional approaches to lexicailanguage modeling provide insight on our analogous problem  , in which the input is a stream of syllables rather than words and the values are drawn from a vocabulary N of stress levels  . We wish to create a model that yields approximate values for probabilities of the form p  ( sklso , sl ,   .   .   .   , Sk-1) , where siE ~ is the stress symbol at syllable i in the text  . A model with separate parameters for each history is prohibitively large  , as the number of possible histories grows exponentially with the length of the input  ; and for the same reason it is impossible to train on limited data  . Consequently we partition the history space into equivalence classes  , and the stochastic ngram approach that has served lexicailanguage modeling so well treats two histories as equivalent if the yend in the same n  -   1 symbols . 
As Figure 2 demonstrates , an ngram model is simply a stationary Markov chain of order k = n  -1  , or equivalently a first-order Markov chain whose states are labeled with tuples from Ek  . 
To gauge the regularity and compressibility of the training data we can calculate the entropy rate of the stochastic process as approximated by our model  , an upper bound on the expected number of bits needed to encode each symbol in the best possible encoding  . Techniques for computing the entropy rate of a stationary Markov chain are wellknown in information theory  ( Cover and Thomas ,  1991) . If Xi is a Markov chain with stationary distribution tt and transition matrix P  , then its entropy rate is
H(X ) = -~. i,jI'tiPij logpij.
The probabilities in P can be trained by accumulating  , for each ( sx , s2 ,   .   .   . , sk)EEk , the k-gram count in C(sl,sz, .   .   .   , sk ) in the training data , and normalizing by the ( k-1)-gram count
C(sl , s2, . . . , sl , -1).
The stationary distribution p satisfies pP = # , or equivalently#k = ~ j#jPj , k ( Parzen ,  1962) . In general finding p for a large state space requires an eigenvector computation  , but in the special case of an ngram model it can be shown that the value in p corresponding to the state  ( sl , s2 ,   .   .   . , sk ) is simply the k-gram frequency C(sl , s2, .   .   .   , sk)/N , where N is the number of symbols in the data .   2 We therefore can compute the entropy rate of a stress sequence in time linear in both the amount of data and the size of the state space  . This efficiency will enable us to experiment with values of n as large as seven  ; for larger values the amount of training data , not time , is the limiting factor . 
3 Methodology
The training procedurentails simply counting the number of occurrences of each ngram for the training data and computing the stress entropy rate by the method described  . As we treat each sentence as an independent event  , no cross-sentence ngrams are kept : only those that fit between sentence boundaries are counted  . 
3 . 1 The mean ing o f s t ress ent ropy ra te We regard these experiments as computing the entropy rate of a Markov chain  , estimated from training data , that approximately models the emission of symbols from a random source  . The entropy rate bounds how compressible the training sequence is  , and not precisely how predictable unseen sequences from the same source would be  . To measure the efficacy of these models in prediction it would be necessary to divide the corpus  , train a model on one subset , and measure the entropy rate of the other with respect o the trained model  . Compression can take place offline , after the entire training set is read , while prediction cannot " cheat " in this manner . 
But we claim that our results predict how effective prediction would be  , for the small state space in our Markov model and the huge amount of training data translate to very good state coverage  . In language modeling , unseen words and unseen ngrams are a serious problem  , and are typically combatted with smoothing techniques such as the backoff model and the discounting formula offered by Good and Turing  . In our case , unseen " words " never occur , for 2This ignores edge effects , for ~--~ sC(sl , s2 ,  .   .   .   , sa ) = N-k+1 , but this discrepancy is negligible when N is very large  . 

Listen tome closely I'l lendeavor to explain  / 
SWSSS WS WS WS WSP what separates a charlatan from a Charlemagne 
WSW2WS WW WS WS W2P
Figure 1: A songly rice x emplifies a highly regular stress stream  ( from the musical Pippin by Stephen
Schwartz . ) the tiniest of realistic training sets will cover the binary or ternary vocabulary  . Coverage of the ngram set is complete for our prose training texts for n as high as eight  ; nor do singleton states ( counts that occur only once )  , which are the bases of Turing's es-timate of the frequency of untrained states in new data  , occur until n = 7 . 
3.2 Lexicallzing stress
Lexical stress is the " backbone of speech rhythm " and the primary tool for its analysis  . ( Baum ,  1952 ) While the precise acoustical prominences of syllables within an utterance are subject to certain word-external hierarchical constraints observed by Halle  ( Halle and Vergnaud , 1987) and others , lexical stress is a local property . The stress patterns of individual words within a phrase or sentence are generally context independent  . 
One source of error in our method is the ambiguity for words with multiple phonetic transcriptions that differ in stress assignment  . Highly accurate techniques for part-of-speech labeling could be used for stress pattern disambiguation when the ambiguity is purely lexical  , but often the choice , in both production and perception , is dialectal . It would be straightforward to divide among all alternatives the count for each ngram that includes a word with multiple stress patterns  , but in the absence of reliable frequency information to weight each pattern we chose simply to use the pronunciation listed first in the dictionary  , which is judged by the lexicographer to be the most popular  . Very little accuracy is lost in making this assumption  . Of the 115 , 966 words in the dictionary , 4635 have more than one pronunciation ; of these , 1269 have more than one distinct stress pattern ; of these , 525 have different primary stress placements . This smallest class has a few common words ( such as " refuse " used as a noun and as a verb )  , but most either occur infrequently in text ( obscure proper nouns , for example ) , or have a primary pronunciation that is overwhelmingly more common than the rest  . 
4 Experiments
The efficiency of the ngram training procedure allowed us to exploit a wealth of data--over  60 million syllables -- from 38 million words of Wall Street Journal text . We discarded sentences not completely covered by the pronunciation dictionary  , leaving 36 . 1 million words and 60 . 7 million syllables for experimentation . 
Our first experiments used the binary ~1 alphabet . The maximum entropy rate possible for this process is one bitper syllable  , and given the unigram distribution of stress values in the data  ( 55 . 2% are primary ) , an upper bound of slightly over 0 . 99 bits can be computed . Examining the 4gram frequencies for the entire corpus ( Figure 3a ) sharpens this substantially , yielding an entropy rate estimate of 0 . 846 bits per syllable . Most frequent among the 4grams are the patterns WSWS and SWSW , consistent with the principle of binary alternation mentioned in section  1  . 
The 4gram estimate matches quite closely with the estimate of  0  . 8 52 bits that can be derived from the distribution of word stress patterns excerpted in Figure  3b   . But both measures overestimate the entropy rate by ignoring longer-range dependencies that become vident when we use larger values of n  . 
For n = 6 we obtain a rate of 0 . 795 bits per syllable over the entire corpus . 
Since we had several thousand times more data than is needed to make reliable estimates of stress entropy rate for values of n less than  7  , it was practical to subdivide the corpus according to some criterion  , and calculate the stress entropy rate for each subset as well as for the whole  . We chose to divide at the sentence level and to partition the  1  . 5 9 million sentences in the data based on a likelihood measure suitable for testing the hypothesis from section  1  . 
A lexical trigram backoff-smoothed language model was trained on separate data to estimate the language perplexity of each sentence in the corpus  . 
Sentence perplexity PP ( S ) is the inverse of sentence P ( S ) is the probability of the sentence according to the language model and ISI is its word count  . This measure gauges the average " surprise " after revealing each word in the sentence as judged by the trigram model  . The question of whether more probable word sequences are also more rhythmic can be approximated by asking whether sentences with lower perplexity have lower stress entropy rate  . 
Each sentence in the corpus was assigned to one of one hundred bins according to its perplexity - -sentences with perplexity between  0 and 10 were assigned to the first b in ; between 10 and 20 , the sec-W'u'4W : 0 . 78 ~ WSk'-~:6 . 91 ~ SWt,/W : 2 . 96 ~ SSWW:3 . 94 ~~ S : 2 . 94 ~ WSWS : 11 . 00 ~ S~F ~ S : 7 . 80 ~ SSWS : 8 . 59 ~ I~SW : 6 . 97 ~ WSSW : 6 . 16 ~ SWSW : 11 . 21 ~ SSSW : 6 . 25 ~ k ' WSS : 3 . 71 ~ WSSS : 6 . 06 ~ SWSS : 8 . 48 ~ SSSS : 6 . 27~
S 45.87 ~
SW 18.94 ~
W 9.54~(b)s~r ~ s.74 ~ ws 5.14 ~
WSW 4.54 ~
Figure 3: ( a ) The corpus frequencies of all binary stress 4grams   ( based on 60 . 7 million syllables ) , with secondary stress mapped to " weak " ( W ) .   ( b ) The corpus frequencies of the top six lexical stress patterns  . 
Wail Sb ' eet Jouinal sylal despert mtd , by perpledty bin
Wan Street Journal sentences 2 . 5 e+Q6  ==  13  ! :~  te~6   5QO000 Wall Street Journal Iraining symbols ( sylabl = )  , by perple ~ dtyb in
Wall Street Journalse ~ llences 100   2~0   300   400   500   600   700   800   900   1000 L = ~ gu ~ epeq  ~ zay 1  . 78 1,76 1,74 1 . 72 t . 7  ~ .  1 . 68 _= 1 . 66 1 . 64 1,62 1 . 6 1 . 58 1 . 56 ( a )
Iifi IIIiI 100   200   300   400   500   600   700   8\]0   900   1000 
Language peq 31 e~y
Figure 4: The amount of training data , in syllables , in each perplexity bin . The bin at perplexity level pp contains all sentences in the corpus with perplexity no less than pp and no greater than pp +  10  . The smallest count ( at bin 990) is 50662 . 
ond ; and so on . Sentences with perplexity greater than 1000 , which numbered roughly 106 thousand out of 1 . 59 million , were discarded from all experiments , as 10-unit bins at that level captured too little data for statistical significance  . A histogram showing the amount of training data ( in syllables ) per perplexity bin is given in Figure 4 . 
It is crucial to detect and understand potential sources of bias in the methodology so far  . It is clear that the perplexity bins are well trained  , but not yet that they are comparable with each other  . Figure 5 shows the average number of syllables per word in sentences that appear in each bin  . That this function is roughly increasing agrees with our intuition that sequences with longer words are rarer  . But it biases our perplexity bins at the extremes  . Early bins , with sequences that have a small syllable rate per word  ( 1 . 57 in the 0 bin , for example ) , are pre-disposed to a lower stress entropy rate since primary stresses  , which occur roughly once per word , are more frequent . Later bins are also likely to be prej-udiced in that direction  , for the inverse reason : The Figure 5: The average number of syllables per word for each perplexity bin  . 
increasing frequency of multisyllabic words makes it more and more fashion able to trans it to a weak -stressed syllable following a primary stress  , sharp en-ing the probability distribution and decreasing entropy  . 
This is verified when we run the stress entropy rate computation for each bin  . The results for ngram models of orders 3 through 7  , for the case in which secondary lexical stress is mapped to the " weak " level  , are shown in Figure 6 . 
All of the rates calculated are substantially less than a bit  , but this only reflects the stress regularity inherent in the vocabulary and in word selection  , and says nothing about word arrangement . 
The atomic elements in the text stream , the words , contribute regularity independently . To determine how much is contributed by the way they are glued together  , we need to remove the bias of word choice . 
For this reason we settled on a model size , n = 6 , and performed a variety of experiments with both the original corpus and with a control set that contained exactly the same bins with exactly the same sentences  , but mixed up . Each sentence in the control set was permuted with a pseudorandom sequence of swaps based on an insensitive function of the original  ; that is to say , identical sentences in the 0 . 05 0 . 9 "~ 0 . 85 ? i 0 . 8 0 . 7 8 Wall Street Journal BINARY stress entropy rates , bype ~ exily bin , i 3gram model ~ 4gr ~ model - . 
5gram model
S-gramr nodd~-.
7gram model ~.--?..., il,
Jiii IIIi 100   200   300   400   500   600   700   800 goo 1000 
Language ~ Olexity
Figure 6: ngram stress entropy rates for ~ z , weak secondary stress corpus were shuffled the same way and sentences differing by only one word were shuffled similarly  . 
This allowed us to keep steady the effects of multiple copies of the same sentence in the same perplexity bin  . More importantly , these tests hold everything constant--diction , syllable count , syllable rate per word--except for syntax , the arrangement of the chosen words within the sentence  . Comparing the unrandomized results with this control experiment allows us  , therefore , to factor out everything but word order . In particular , subtracting the stress entropy rates of the original sentences from the rates of the randomized sentences gives us a figure  , relative entropy , that estimates how many bits we save by knowing the proper word order given the word choice  . The results for these tests for weak and strong secondary stress are shown in Figures  7 and 8  , including the difference curves between the randomized-word and original entropy rates  . 
The consistently positive difference function demonstrates that there is some extrastress regularity to be had with proper word order  , about a hundred tho fabit on average . The difference is small indeed , but its consistency over hundreds of well-trained at a points puts the observation on statistically solid ground  . 
The negative slopes of the difference curves suggests a more interesting conclusion : As sentence perplexity increases  , the gap in stress entropy rate between syntactic sentences and randomly permuted sentences narrows  . Restated inversely , using entropy rates for randomly permuted sentences as a baseline  , sentences with higher sequence probability are relatively more rhythmical in the sense of our definition from section  1  . 
To supplement the ~ z binary vocabulary tests we ran the same experiments with  ~2  =  0  ,  1 , P , introducing a pause symbol to examine how stress behaves near phrase boundaries  . Commas , dashes , semicolons , colons , ellipses , and all sentence-terminating punctuation in the text  , which were removed in the E1 tests , were mapped to a single pause symbol for E ~ . Pauses in the text arise not only from semantic on straints but also from physiological limitations  . These include the " breath groups " of syllables that influence both vocalized and written production  . ( Ochsner , 1989) . The results for these experiments are shown in Figures  9 and 10  . 
Expectedly , adding the symbol increases the confusion and hence the entropy  , but the rates remain less than a bit . The maximum possible rate for a ternary sequence is log  2   3  ~  1  . 58 . 
The experiments in this section were repeated with a larger perplexity interval that partitioned the corpus into  20 bins , each covering 50 units of perplexity . The resulting curves mirrored the finer-grain curves presented here  . 
5 Conclusions and future work
We have quantified lexical stress regularity , measured it in a large sample of written English prose  , and shown there to be a significant contribution from word order that increases with lexical perplexity  . 
This contribution was measured by comparing the entropy rate of lexical stress in natural sentences with randomly permuted versions of the same  . Ran-domizing the word order in this way yields a fairly crude baseline  , as it produces a syntactic sequences in which , for example , single-syllable function words can unnaturally clash  . To correct for this we modified the randomization algorithm to permute only open-class words and to fix in place determiners  , particles , pronouns , and other closed-class words . 
We found the entropy rates to be consistently midway between the fully randomized and unrandom -ized values  . But even this constrained randomization is weaker than what we'd like  . Ideally we should factor out semantics as well as word choice  , comparing each sentence in the corpus with its grammatical variations  . While this is a difficult experiment to do automatically  , we ' rehoping to approximate it using a natural anguage generation system based on link grammar under development by the author  . 
Also , we're currently testing other data sources such as the Switchboard corpus of telephone speech  ( Godfrey , Holliman , and McDaniel ,  1992 ) to measure the effects of rhythm in more spontaneous and grammatically relaxed texts  . 
6 Acknowledgments
Comments from John Lafferty , Georg Niklfeld , and Frank Dellaert contributed greatly to this paper  . 
The work was supported in part by an ARPA AASERT award  , number DAAH04-95-1-0475 . 

Wall Street Journal BINARY stressent mpJ rates , by pefide agy bin ; second ap/slxess mapped to WEAK 0 . 81  .   .   .   . 
Randomized Wall Street Journal sentences - h-, o . o ~' N*v ~, 0 . 70 0 . 75 0 . 74 IIIIIIIII #00 200 300 400 5006 ( : 070080090010 ~ O
Lan~a9 ~ pe~pl?alyi 0,780,77
WalStreet Journal BINARY stpess entropy rate differences  , by perplexity b~n ; secondary stress mapped 1 o WEAK 0 . 025 ~ , , , ,
WSJ randomized ~ nusnomaed or nized 0 . 02 0 . 015 0 . 0 05 iIIiiiiii 100   200   300   400   500   600   700   800   900   1000 
Langu ~ jepe ~ p ~ ex ~
Figure 7:   6-gram stress entropy rates and difference curve for El , weak secondary stress 0 . 76 2 0 . 75 i 0 . 74 " Wail SlmetJ ~ mal BINARY alress enbopy rates , bypel~e~ty bin ; seco~daly stress map ~ to ~ RONG Wall Street Journal BINARYs Vessentropy into differences  , bype ~ ~; s ~ daws tm ~ ~ to STRONG 0 . 79 0 . 024 , , / , ,  . 
Wag Street Journal sentences ~ IWSJ rand on ~ zed minus nonmndon ~ zed Randomized Wall Street Journal sentences -~----  0  . 022/~* V , ? ~ oo2 ~ ~", 0 . 018 0 . 014 0,012 0 . 01 0 . 73 0 . 008 0 . 72, 0 . 006 0 , 71 IIIIIIIII 0 , 0 04 IIIIIIII 100   200   300   400   500   600   700   800   900   1000   0   100   200   300   400   500   600   700   800   900   1000 Language pelple agy Language perplexity Figure 8:   6-gram entropy rates and difference curve for El , strong secondary stress 0 . 9 3 Wall Street Journal TER NARY stress entropy rates  , by per ~ ex ~ tyb in ; secomlary stress mapped to STRONG 0 . 97 ~ , Randomized wW2i , Sstl:eel , ~Ur ~ all : ene ~ e ~ c : . + --~- o . ~' ~+ ~ t **, z ~~; ; i '*'" 0 . 92 ~ % 0 . 9 1 ii II liiiI 100   200   300   400   500   600   700   800   900   10OO 
Language perplexity
Wall Street Journal TERNARY stress entropy rated ifferences  , by pmple ~ tyb in ; secondary stress mapped to WEAK 0 . 05  .  , 0 . 045 0 . 04 0 . 0~5 0 . 03 0 . 025 0 . 02 0 . 015 0 . 01 0 . 005  , i ,   , WSJ randomized minusnon randomized- , , , --
IIIIIIII 100   200   300   400   800   600   700   800   900   1000 
Language pe~plexily
Figure 9:   6-gram entropy rates and difference curve for ~ , weak secondary stress Wall Street Journal TERNARY sYess entropy rates  , by per pleiffy bin ; secorldaPjs b'ess mapped to STRONG Wall Street Journal TERNARY stressent rppyrate differences  , by perplexity b~n ; seco~a~=~s~toSTRONG 0 . 94 + . L , , , , , , , , 0 . 05 . , , , ~ a % * ~ . . * W~dl Street Journal senterces ~ WSJ randomized minusn~mmloralzed ' " ~ ~ Randomized WaOStreet Journal sentences -*--  . /" U + "*% o . o ~ o8~"~!~0 . 92 ~$~: ~0814, I . ~~' ~, j j~v ~ . , : ~'" IN, . Ii ~ Vi ~ i , . * 0 . 9 ' ; ' ' 0 . 89 ~ ~ 0 . 88 0 . 87 0  . ~ 0 . 0 . ', 0 . 025 0 . 0~ 0 . 01~ 0 . 01 0 . 0 05 i i i i i ~ i i i i i i i i i i i i 100   200   300   400   500   000   700   800   ~0   1000   0   100   200   300 ~ xO 5  ( \]0600 700 800 900 1000 Language pelple . , dly La . ~geperplexity Figure 10:   6-gram entropy rates and difference curve for E2  , strong secondary stress

Abercrombie , D .  1967 . Elements of general phonetics . Edinburgh University Press . 
Baum , P.F . 1952. The Other Harmony of Prose.
Duke University Press.?
Cover , T . M . and J . A . Thom ~ .  1991 . Elements of information theory . John Wiley & Sons , Inc . 
Godfrey , J . , E . Holliman , and J . McDaniel .  1992 . 
Switchboard : Telephone speech corpus for research development  . In Proc . ICASSP-92, pages 1-517-520 . 
Halle , M . and J . Vergnaud .  1987 . An essay on stress . The MIT Press . 
Harding , D . W .  1976 . Words intor hythm : English speech rhythm in verse and prose  . Cambridge University Press . 
Jassem , W . , D . R . Hill , and I . H . Witten .  1984 . 
Isochrony in English speech : its statistical validity and linguistic relevance  . In D . Gibbon and H . Richter , editors , Intonation , rhythm , and accent : Studies in Discourse Phonology . Walter de
Gruyter , pages 203-225.
Kager , R . 1989a . A metrical theory of stress and destressing in English and Dutch  . For is Publications . 
Kager , R . 1989b . Therhythm of English prose.
Foris Publications.
Ochsner , R.S . 1989. Rhythmand writing . The
Whitson Publishing Company.
Parzen , E .  1962 . Stochastic processes . Holden-Day . 

