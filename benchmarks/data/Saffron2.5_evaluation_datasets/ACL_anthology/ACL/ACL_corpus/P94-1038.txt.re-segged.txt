Similarity-Based Estimation of Word Cooccurrence 
Probabilities
Ido Dagan Fernando Pereira
AT&T Bell Laboratories
600 Mountain Ave.
Murray Hill , NJ 07974, USA
dagan ? research , att.com
pereira ? research , att . com

In many applications of natural language processing it is necessary to determine the likelihood of a given word combination  . For example , a speech recognizer may need to determine which of the two word combinations " eatapeach " and " eatabeach " is more likely  . Statistical NLP methods determine the likelihood of a word combination according to its frequency in a training corpus  . However , the nature of language is such that many word combinations are infrequent and do not occur in a given corpus  . In this work we propose a method for es-timating the probability of such previously unseen word combinations using available information on " most similar " words  . 
We describe a probabilistic word association model based on distributional word similarity  , and apply it to improving probability estimates for unseen word bigrams in a variant of Katz's backoff model  . The similarity-based method yields a 20% perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech recognition error  . 

Data sparseness is an inherent problem in statistical methods for natural language processing  . Such methods use statistics on the relative frequencies of configurations of elements in a training corpus to evaluate alternative analyses or interpretations of new samples of text or speech  . The most likely analysis will be taken to be the one that contains the most frequent configurations  . The problem of data sparseness arises when analyses contain configurations that never occurred in the training corpus  . Then it is not possible to estimate probabilities from observed frequencies  , and some other estimation scheme has to be used . 
We focus here on a particular kind of configuration  , word cooccurrence . Examples of such cooccurrences include relationships between head words in syntactic constructions  ( verb-object or adjective-noun , for example ) and word sequences ( ngrams ) . In commonly used models , the probability estimate for a previously unseen cooccurrence is a function of the probability esti- 
Lillian Lee
Division of Applied Sciences
Harvard University 33 Oxford St . Cambridge MA 02138 , US Allee?das , harvard , edumates for the words in the cooccurrence . For example , in the bigram models that we study here , the probability P ( w21 wl ) of a conditioned word w2 that has never occurred in training following the conditioning word wl is calculated from the probability of w ~  , as estimated by w2's frequency in the corpus ( Jelinek , Mercer , and Roukos , 1992; Katz ,  1987) . This method depends on an independence assumption on the cooccurrence of Wl and  w2: the more frequent w2 is , the higher will be the estimate of P ( w2\[w l ) , regardless of W l . 
Class-based and similarity-based models provide an alternative to the independence assumption  . In those models , the relationship between given words is modeled by analogy with other words that are in some sense similar to the given ones  . 
Brown et a \] .   ( 1992 ) suggest a classbased ngram model in which words with similar cooccurrence distributions are clustered in word classes  . The cooccurrence probability of a given pair of words then is estimated according to an averaged cooccurrence probability of the two corresponding classes  . Pereira , Tishby , and Lee ( 1993 ) propose a " soft " clustering scheme for certain grammatical cooccurrences in which membership of a word in a class is probabilistic  . Cooccurrence probabilities of words are then modeled by averaged cooccurrence probabilities of word clusters  . 
Dagan , Markus , and Markovitch ( 1993 ) argue that reduction to a relatively small number of predetermined word classes or clusters may cause a substantial loss of information  . Their similarity-based model avoids clustering altogether  . Instead , each word is modeled by its own specific class , a set of words which are most similar to it ( a sink-nearest neighbor approaches in pattern recognition  )  . Using this scheme , they predict which unobserved cooccurrences are more likely than others  . 
Their model , however , is not probabilistic , that is , it does not provide a probability estimate for unobserved cooccurrences  . It cannot therefore be used in a complete probabilistic framework  , such as ngram language models or probabilistic lexicalized grammars  ( Schabes , 1992; Lafferty , Sleator , and Temperley ,  1992) . 
We now give a similarity-based method for estimating the probabilities of cooccurrences unseen in training  . 

Similarity-based estimation was first used for language modeling in the cooccurrence smoothing method of Es-sen and Steinbiss  ( 1992 )  , derived from work on acoustic model smoothing by Sugawara et al  ( 1985 )  . We present a different method that takes as starting point the backoff scheme of Katz  ( 1987 )  . We first allocate an appropriate probability mass for unseen cooccurrences following the backoff method  . Then we redistribute that mass to unseen cooccurrences according to an averaged cooccurrence distribution of a set of most similar conditioning words  , using relative entropy as our similarity measure . This second step replaces the use of the independence assumption in the original backoff model  . 
We applied our method to estimate unseen bigram probabilities for Wall Street Journal text and compared it to the standard backoff model  . Testing on a heldout sample , the similarity model achieved a 20% reduction in perplexity for unseen bigrams . These constituted just 10 . 6% of the test sample , leading to an overall reduction in test set perplexity of  2  . 4% . We also experimented with an application to language modeling for speech recognition  , which yielded a statistically significant reduction in recognition error  . 
The remainder of the discussion is presented in terms of bigrams  , but it is valid for other types of word cooccurrence as well  . 
Discounting and Redistribution
Many low-probability bigrams will be missing from any finite sample  . Yet , the aggregate probability of all these unseen bigrams is fairly high  ; any new sample is very likely to contain some . 
Because of data sparseness , we cannot reliably use a maximum likelihood estimator  ( MLE ) for bigram probabilities . The MLE for the probability of a bigram ( wi , we ) is simply:
PML(W i , we)--c(w , we)N , (1) where c(wi , we ) is the frequency of ( wi , we ) in the training corpus and N is the total number of bigrams  . However , this estimates the probability of any unseen hi -gram to be zero  , which is clearly undesirable . 
Previous proposals to circumvent the above problem  ( Good , 1953; Jelinek , Mercer , and Roukos , 1992; Katz , 1987; Church and Gale ,  1991 ) take the MLE as an initial estimate and adjust it so that the total probability of seen bigrams is less than one  , leaving some probability mass for unseen bigrams . Typically , the adjustment involves either interpolation , in which the new estimator is a weighted combination of the MLE and an estimator that is guaranteed to be nonzero for unseen bigrams  , or discounting , in which the MLE is decreased according to a model of the unreliability of small frequency counts  , leaving some probability mass for unseen bigrams . 
The backoff model of Katz ( 1987 ) provides a clear separation between frequent events  , for which observed frequencies are reliable probability estimators  , and low-frequency events , whose prediction must involve additional information sources  . In addition , the backoff model does not require complex estimations for interpolation parameters  . 
A hack-off model requires methods for ( a ) discounting the estimates of previously observed events to leave out some positive probability mass for unseen events  , and ( b ) redistributing among the unseen events the probability mass freed by discounting  . For bigrams the resulting estimator has the general form fPd  ( w21 wl ) if c ( wi , w2) > 0D(w21wt ) = ~ . a(Wl ) Pr(w2\]wt ) oherwise ,   ( 2 ) where Pd represents the discounted estimate for seen bigrams  , P ~ the model for probability redistribution among the unseen bigrams  , and a(w ) is a normalization factor . Since the overall mass left for unseen bigrams starting with w i is given by ~  , P , ~( welwi ) , w ~ : c(wi , w ~) > 0 ~( wi ) = 1 - the normalization
Ew2P ( w2\[wl ) :1 is = factor required to ensure ( wl ) 1-~: c ( ~i , w2 ) > 0 Pr ( we\[wi ) The second formulation of the normalization is computationally preferable because the total number of possible bigram types far exceeds the number of observed types  . Equation ( 2 ) modifies slightly Katz's presentation to include the placeholder Pr for alternative models of the distribution of unseen bigrams  . 
Katz uses the GoodTuring formula to replace the actual frequency c  ( wi , w2) of a bigram ( or an event , in general ) with a discounted frequency , c*(wi , w2) , defined by c*(wi , w2) = ( C(W l , w2) + 1) nc(wl ' ~) + i , (3) nc(wl , w2 ) where nc is the number of different bigrams in the corpus that have frequency c  . He then uses the discounted frequency in the conditional probability calculation for a bigram : c *  ( wi , w2) (4)
Pa(w21wt)-C(W l )
In the original GoodTuring method ( Good ,  1953 ) the free probability mass is redistributed uniformly among all unseen events  . Instead , Katz's backoff scheme redistributes the free probability mass nonuniformly in proportion to the frequency of  w2  , by setting
Pr ( we Jwi ) = P ( w  ~ )   ( 5  ) wl the probability of an unseen following word w2 is proportional to its unconditional probability . However , the overall form of the model ( 2 ) does not depend on this assumption , and we will next investigate an estimate for P ~ ( w21 wl ) derived by averaging estimates for the conditional probabilities that  w2 follows words that are distributionally similar to wl  . 
The Similarity Model
Our scheme is based on the assumption that words that are " similar " to wl can provide good predictions for the distribution of wl in unseen bigrams  . Let S ( Wl ) denote a set of words which are most similar to wl  , as determined by some similarity metric . We define PsiM(W21Wl ) , the similarity-based model for the conditional distribution of wl  , as a weighted average of the conditional distributions of the words in S  ( Wl ) :
PsiM(W21wl ) = - , ?'-'~ w(~i , ~') (6) ZWleS(Wl ) 2\[--~'( ~\] ~ l'\['/fll)~"~W/w , ~ j ) ' where W ( W ~ l , wl ) is the ( unnormalized ) weight given to w ~ , determined by its degree of similarity to wl . According to this scheme ,   w2 is more likely to follow wl if it tends to follow words that are most similar to wl  . To complete the scheme , it is necessary to define the similarity metric and  , accordingly , S(wl ) and W(w ~ , Wl ) . 
Following Pereira , Tishby , and Lee (1993) , we measure word similarity by the relative ntropy  , or KullbackLeibler ( KL ) distance , between the corresponding conditional distributions D  ( w ~ II w ~ ) = ZP ( w2\]wl ) ogP ( w2Iwl )   ( 7 ) ~ P ( w2lw ~ ) " The KL distance is 0 when wl = w ~ , and it increases as the two distribution are less similar  . 
To compute ( 6 ) and ( 7 ) we must have nonzero estimates of P ( w21 wl ) whenever necessary for ( 7 ) to be defined . We use the estimates given by the standard backoff model  , which satisfy that requirement . Thus our application of the similarity model averages together standard backoff estimates for a set of similar conditioning words  . 
We define S ( wl ) as the set of at most k nearest words to wl ( excluding w l itself )  , that also satisfy D(WlIIw ~) < t . k and t are parameters that control the contents of $  ( wl ) and are tuned experimentally , as we will see below . 
W ( w ~, wl ) is defined as
W ( w ~, W l ) --- exp-/3D(W lII~i)
The weight is larger for words that are more similar  ( closer ) to wl . The parameter fl controls the relative contribution of words in different distances from wl : as the value of fl increases  , the nearest words to Wlget relatively more weight  . As fl decreases , remote words get a larger effect . Like k and t,/3 is tuned experimentally . 
Having a definition for PSIM(W2\[Wl ) , we could use it directly as Pr ( w2\[wl ) in the backoff scheme ( 2 )  . We found that it is better to smooth PsiM ( W ~\ [ Wl ) by interpolating it with the unigram probability P  ( w2 )   ( recall that Katz used P ( w2 ) as Pr ( w2\[wl ) ) . Using linear interpolation we get P , ( w2\[wl ) = 7P(w2)+(1-7) PsiM(W2lWl) ,   ( 8 ) where " f is an experimentally-determined i terpolation parameter  . This smoothing appears to compensate for inaccuracies in Pslu  ( w2\]wl )  , mainly for infrequent conditioning words . However , as the evaluation below shows , good values for 7 are small , that is , the similarity-based model plays a stronger role than the independence assumption  . 
To summarize , we construct a similarity-based model for P ( w2\[wl ) and then interpolate it with P ( w2 )  . The interpolated model ( 8 ) is used in the backoff scheme as Pr ( w2\[wl )  , to obtain better estimates for unseen bigrams . Four parameters , to be tuned experimentally , are relevant for this process : k and t , which determine the set of similar words to be considered  , /3 , which determines the relative effect of these words  , and 7 , which determines the overall importance of the similarity-based model  . 
Evaluation
We evaluated our method by comparing its perplexity  1 and effect on speech recognition accuracy with the baseline bigram backoff model developed by MIT Lincoln Laboratories for the Wall Streel Journal  ( WSJ ) text and dictation corpora provided by ARPA's HLT pro-grain  ( Paul ,  1991) .   2 The baseline backoff model follows closely the Katz design  , except that for compactness all frequency one bigrams are ignored  . The counts used ill this model and in ours were obtained from  40  . 5 million words of WSJ text from the years 1987-89 . 
For perplexity evaluation , we tuned the similarity model parameters by minimizing perplexity on an additional sample of  57  . 5 thousand words of WSJ text , drawn from the ARPAHLT development test set . The best parameter values found were k = 60 , t = 2 . 5,/3=4 and 7=0 . 15 . For these values , the improvement in perplexity for unseen bigrams in a held out  18 thousand word sample , in which 10 . 6% of the bigrams are unseen , is just over 20% . This improvement on unseen 1The perplexity of a conditional bigram probability model  /5 with respect to the true bigram distribution is an information-theoretic measure of model quality  ( Jelinek , Mercer , and Roukos ,  1992 ) that can be empirically estimated by exp--~~-~ i logP  ( w , tu , i_l ) for a test set of length N . Intuitively , the lower the perplexity of a model the more likely the model is to assign high probability to bigrams that actually occur  . In our task , lower perplexity will indicate better prediction of unseen bigrams  . 
2The ARPA WSJ development corpora come in two versions  , one with verbalized punctuation and the other without  . We used the latter in all our experiments . 
274k t ~7 training reduction ( % ) test reduction ( % )  60 2 . 5 4 0 . 15 18 . 4 20 . 51 50 2 . 5 4 0 . 15 18 . 38 20 . 45 40 2 . 5 4 0 . 2 18 . 34 20 . 03 30 2 . 5 4 0 . 25 18 . 33 19 . 76 70 2 . 5 4 0 . 1 18 . 3 20 . 53 80 2 . 5 4 . 5 0 . 1 18 . 25 20 . 55 100 2 . 5 4 . 5 0 . 1 18 . 23 20 . 54 90 2 . 5 4 . 5 0 . 1 18 . 23 20 . 59 20 1 . 5 4 0 . 3 18 . 04 18 . 7 10 1 . 5 3 . 5 0 . 3 16 . 64 16 . 9 4 Table 1: Perplexity Reduction on Unseen Bigrams for Different Model Parameters bigrams corresponds to an overall test set perplexity improvement of  2  . 4% ( from 237 . 4 to 231 . 7) . Table 1 shows reductions in training and test perplexity , sorted by training reduction , for different choices in the number k of closest neighbors used  . The values of f ~ , 7 and t are the best ones found for each k . 3 From equation (6) , it is clear that the computational cost of applying the similarity model to an unseen bigram is O  ( k )  . Therefore , lower values for k ( and also for t ) are computationally preferable . From the table , we can see that reducing k to 30 incurs a penalty of less than 1% in the perplexity improvement , so relatively low values of k appear to be sufficient to achieve most of the benefit of the similarity model  . As the table also shows , the best value of 7 increases ask decreases , that is , for lower k a greater weight is given to the conditioned word's frequency  . This suggests that the predictive power of neighbors beyond the closest  30 or so can be modeled fairly well by the overall frequency of the conditioned word  . 
The bigram similarity model was also tested as a language model in speech recognition  . The test data for this experiment were pruned word lattices for  403 WSJ closed-vocabulary test sentences . Arc scores in those lattices are sums of an acoustic score  ( negative loglikelihood ) and a language model score , in this case the negative log probability provided by the baseline bigram model  . 
From the given lattices , we constructed new lattices in which the arc scores were modified to use the similarity model instead of the baseline model  . We compared the best sentence hypothesis in each original attice and in the modified one  , and counted the word disagreements in which one of the hypotheses i correct  . There were a total of 96 such disagreements . The similarity model was correct in 64 cases , and the backoff model in 32 . This advantage for the similarity model is statistically significant at the  0  . 01 level . The overall reduction in error rate is small , from 21 . 4% to 20 . 9% , because the number of disagreements is small compared with  3Values of fl and t refer to base 10 logarithms and exponentials in all calculations . 
the overall number of errors in our current recognition setup  . 
Table 2 shows some examples of speech recognition disagreements between the two models  . The hypotheses are labeled'B'for backoff and ' S ' for similarity  , and the boldface words are errors . The similarity model seems to be able to model better regularitie such as semantic parallelism in lists and avoiding a past tense form after " to  . " On the other hand , the similarity model makes several mistakes in which a function word is inserted in a place where punctuation would be found in written text  . 
Related Work
The cooccurrences mooihing technique ( Essen and Steinbiss ,  1992) , based on earlier stochastic speech modeling work by Sugawara et al  ( 1985 )  , is the main previous attempto use similarity to estimate the probability of unseen events in language modeling  . In addition to its original use in language modeling for speech recognition  , Grishman and Sterling ( 1993 ) applied the cooccurrence smoothing technique to estimate the likelihood of selectional patterns  . We will outline here the main parallels and differences between our method and cooccurrence smoothing  . A more detailed analysis would require an empirical comparison of the two methods on the same corpus and task  . 
In cooccurrence smoothing , as in our method , a baseline model is combined with a similarity -based model that refines some of its probability estimates  . The similarity model in cooccurrence smoothing is based on the intuition that the similarity between two words w and w'can be measured by the confusion probability Pc  ( w'lw ) that w'can be substituted for w in an arbitrary context in the training corpus  . Given a baseline probability model P , which is taken to be the MLE , the confusion probability Pc ( w ~ lwl ) between conditioning words w ~ and wl is defined as l P c  ( w l l w l )  - -1  ( 9 ) P ( l ) p ( wllw2 ) p ( wl12 ) P (  2 ) ' the probability that wl is followed by the same context words as w ~  . Then the bigram estimate derived by S\] commitments  . .  . from leaders fell to three point six billion dollars BI followed bv France the US agreed in ltalv  , yIyS\[followed by France the US Greece .   .   . Italy
B\[he whispers to made a
S\[he whispers to a naide
B the necessity for change exist
S\[the necessity for change exists
B \] without .   .   . additional reserves Centrust would have reported S\[without  .   .   . additional reserves of Centrust would have reported 
B \] in the darkness past the church
S in the darkness passed the church
Table 2: Speech Recognition Disagreements between Models cooccurrence smoothing is given by 
Ps(w21wl ) = ~ P ( w~lw'l)Pc(w'llwO
Notice that this formula has the same form as our similarity model  ( 6 )  , except that it uses confusion probabilities where we use normalized weights  . 4 In addition , we restrict the summation to sufficiently similar words  , whereas the cooccurrence smoothing method sums over all words in the lexicon  . 
The similarity measure ( 9 ) is symmetric in the sense that Pc ( w'lw ) and Pc ( w\[w ' ) are identical up to fre-Pc ( w'lw ) _P ( w ) quency normalization , that is Pc(wlw')-P(w , ) " In contrast , D ( wHw ' )   ( 7 ) is a symmetric in that it weighs each context in proportion to its probability of occurrence with w  , but not with w q In this way , if w and w ' have comparable frequencies but w' has a sharper context distribution than w  , then D(w'I\[w ) is greater than D(w\[\[w') . Therefore , in our similarity model w ' will play a stronger role in estimating w than vice versa  . These properties motivated our choice of relative entropy for similarity measure  , because of the intuition that words with sharper distributions are more informative about other words than words with flat distributions  . 
4This presentation corresponds to model 2B in Essen and Steinbiss ( 1992 )  . Their presentation follows the equivalent model l -A  , which averages over similar conditioned words , with the similarity defined with the preceding word as context  . In fact , these equivalent models are symmetric in their treatment of conditioning and conditioned word  , as they can both be rewritten as Ps(w2lwl ) , ~ ,   ,   ,   ,   , P ( w2\[Wl ) P ( Wl = Iw ~ ) P ( w21 wl ) They also consider other definitions of confusion probability and smoothed probability estimate  , but the one above yielded the best experimental results  . 
Finally , while we have used our similarity model only for missing bigrams in a backoff scheme  , Essen and Steinbiss ( 1992 ) used linear interpolation for all bigrams to combine the cooccurrence smoothing model with MLE models of bigrams and unigrams  . Notice , however , that the choice of back of for interpolation is independent from the similarity model used  . 
Further Research
Our model provides a basic scheme for probabilistic similarity-based estimation that can be developed in several directions  . First , variations of (6) may be tried , such as different similarity metrics and different weighting schemes  . Also , some simplification of the current model parameters may be possible  , especially with respect to the parameters t and k used to select the nearest neighbors of a word  . A more substantial variation would be to base the model on similarity between conditioned words rather than on similarity between conditioning words  . 
Other evidence may be combined with the similarity -based estimate  . For instance , it may be advantageous to weigh those estimates by some measure of the reliability of the similarity metric and of the neighbor distributions  . A second possibility is to take into account negative vidence : if Wl is frequent  , but w2 never follow edit , there may be enough statistical evidence to put an upper bound on the estimate of P  ( w21wl )  . 
This may require an adjustment of the similarity based estimate  , possibly along the lines of ( Rosenfeld and Huang ,  1992) . Third , the similarity-based estimate can be used to smooth then a aximum likelihood estimate for small nonzero frequencies  . If the similarity-based estimate is relatively high  , a bigram would receive a higher estimate than predicted by the uniform discounting method  . 
Finally , the similarity-based model may be applied to configurations other than bigrams  . For trigrams , it is necessary to measure similarity between different conditioning bigrams  . This can be done directly , form P ( w31 wl , w2) , corresponding to different bigrams ( wl , w ~) . Alternatively , and more practically , it would be possible to define a similarity measure between bigrams as a function of similarities between corresponding words in them  . Other types of conditional cooccurrence probabilities have been used in probabilistic parsing  ( Black et al ,  1993) . If the configuration i question includes only two words  , such as P ( object l verb ) , then it is possible to use the model we have used for bigrams  . 
If the configurationic ludes more elements , it is necessary to adjust the method , along the lines discussed above for trigrams . 
Conclusions
Similarity-based model suggest an appealing approach for dealing with data sparseness  . Based on corpus statistics , they provide analogies between words that often agree with our linguistic and domain intuitions  . In this paper we presented a new model that implements the similarity-based approach to provide estimates for the conditional probabilities of unseen word cooccur-fences  . 
Our method combines similarity-based estimates with Katz's backoff scheme  , which is widely used for language modeling in speech recognition  . Although the scheme was originally proposed as a preferred way of implementing the independence assumption  , we suggest that it is also appropriate for implementing similarity-based models  , as well as classbased models . It enables us to rely on direct maximum likelihood estimates when reliable statistics are available  , and only otherwise resort to the estimates of an " indirect " model  . 
The improvement we achieved for a bigram model is statistically significant  , hough modest in its overall effect because of the small proportion of unseen events  . 
While we have used bigrams as an easily-accessible platform to develop and test the model  , more substantial improvements might be obtainable for more informative configurations  . An obvious case is that of trigrams , for which the sparse data problem is much more severe  . ~ Our longer-term goal , however , is to apply similarity techniques to linguistically motivated word cooccurrence configurations  , as suggested by lexicalized approaches to parsing  ( Schabes , 1992; Lafferty , Sleator , and Temperley ,  1992) . In configurations like verb-object and adjective -noun  , there is some evidence ( Pereira , Tishby , and Lee ,  1993 ) that sharper word cooccurrence distributions are obtainable  , leading to improved predictions by similarity techniques  . 

We thank Slava Katz for discussions on the topic of this paper  , Doug McIlroy for detailed comments , DougPaul5 For WSJ trigrams , only 58 . 6% of test set trigrams occur in 40M of words of training ( Doug Paul , personal communication ) . 
for help with his baseline backoff model , and Andre Ljolje and Michael Riley for providing the word lattices for our experiments  . 
References
Black , Ezra , Fred Jelinek , John Lafferty , David M . 
Magerman , David Mercer , and Salim Roukos . 1993.
Towards history-based grammars : Using richer models for probabilistic parsing  . In 30th Annual Meeting of the Association for Computational Linguistics  , pages 31-37 , Columbus , Ohio . Ohio State University , Association for Computational Linguistics , Morristown , New Jersey . 
Brown , Peter F . , Vincent J . Della Pietra , Peter V . 
deSouza , Jenifer C . Lai , and Robert L . Mercer.
1992 . Class-based ngram models of natural language . Computational Linguistics , 18(4):467-479 . 
Church , Kenneth W . and William A . Gale .  1991 . A comparison of the enhanced GoodTuring and deleted estimation methods for estimating probabilities of English bigrams  . Computer Speech and Language , 5:19-54 . 
Dagan , Ido , Shaul Markus , and Shaul Markovitch.
1993 . Contextual word similarity and estimation from sparse data  . In 30th Annual Meeting of the Association for Computational Linguistics  , pages 164-171 , Columbus , Ohio . Ohio State University , Association for Computational Linguistics , Morristown , 
New Jersey.
Essen , Ute and Volker Steinbiss .  1992 . Coocurrence smoothing for stochastic language modeling  . In Proceedings of ICASSP , volume I , pages 161-164 . IEEE . 
Good , I . J .  1953 . The population frequencies of species and the estimation of population parameters  . 
Biometrika , 40(3):237-264.
Grishman , Ralph and John Sterling .  1993 . Smoothing of automatically generated selectional constraints  . In Human Language Technology , pages 254-259 , San Francisco , California . Advanced Research Projects Agency , Software and Intelligent Systems Technology
Office , Morgan Kaufmann.
Jelinek , Frederick , Robert L . Mercer , and Salim Roukos .  1992 . Principles of lexical language modeling for speech recognition  . In Sadaoki Furui and M . Mohan Sondhi , editors , Advances in Speech Signal Processing . Mercer Dekker , Inc . , pages 651-699 . 
Katz , Slava M .  1987 . Estimation of probabilities from sparse data for the language model component of a speech recognizer  . IEEE Transactions on Acoustics , Speeech and Signal Processing ,  35(3):400-401 . 
Lafferty , John , Daniel Sleator , and Davey Temperley . 
1992 . Grammatical trigrams : a a probabilistic model of link grammar  . In Robert Goldman , editor , AAA Iral Language Processing , Cambridge , Massachusetts . 
American Association for Artificial Intelligence.
Paul , Douglas B .  1991 . Experience with a stack decoder-based HMMCSR and backoff ngram language models  . In Proceedings of the Speech and Natural Language Workshop  , pages 284-288 , Palo Alto , California , February . Defense Advanced Research Projects Agency , Information Science and Technology Office , Morgan Kaufmann . 
Pereira , Fernando C . N . , Naftali Z . Tishby , and Lillian Lee .  1993 . Distributional custering of English words . In $ Oth Annual Meeting of the Association for Computational Linguistics  , pages 183-190 , Co\]urn-bus , Ohio . Ohio State University , Association for Computational Linguistics , Morristown , New Jersey . 
Rosenfeld , Ronald and Xuedong Huang .  1992 . Improvements in stochastic language modeling . In DARPA Speech and Natural Language Workshop , pages 107-111 , Harriman , New York , February . Morgan Kaufmann , San Mateo , California . 
Sehabes , Yves .  1992 . Stochastic lexiealized tree-adjoining grammars . In Proceeedings of the 14th International Conference on Computational Linguistics  , Nantes , France . 
Sugawara , K . , M . Nishimura , K . Toshioka , M . Okoehi , and T . Kaneko .  1985 . Isolated word recognition using hidden Markov models  . In Proceedings of ICASSP , pages 14 , Tampa , Florida . IEEE . 

