Using Decision Trees to Construct a Practi calParser 
Masahiko Haruno * Satoshi Shirait Yoshi fumiOoyamat 
mharuno ~ hlp . atr . co . jp shirai,~cslab . kecl . ntt . co . j poov a ma~cslal ) . kecl . ntt . co . jp
* ATR Human Information Processing Research Laboratories 
22 Hikaridai , Seikacho , Sorakugun , Kyoto 619-02 , Japan . 
tNTT Communication Science Laboratories 24 Hikaridai , Seikacho , Sorakugun , Kyoto 619-02 , Japan . 
Abstract
This paper describes novel and practical Japanese parsers that uses decision trees  . First , we construct a single decision tree to estimate modification probabilities  ; how one phrase tends to modify another . Next , we introduce a boosting algorithm in which several decision trees are constructed and then combined for probability estimation  . The two constructed parsers are evaluated by using the EDR Japanese annotated corpus  . The single-tree method outperforms the conventional  . Japanese stochastic methods by 4% . Moreover , the boosting version is shown to have significant advantages  ;  1 ) better parsing accuracy than its single-tree counterpart for any amount of training data and  2  ) no overfitting to data for various iterations . 
1 Introduction
Conventional parsers with practical levels of performance require a number of sophisticated rules that have to be handcrafted by human linguists  . It is time-consunaing and cumbersometon aa intain these rules for two reasons  . 
? The rules are specific to the application domain . 
? Specific rules handling collocational expressions create side effects  . Such rules often deteriorate t , he overall performance of the parser . 
The stochastic approach , on the other hand , has the potential to overcome these difficulties . Because it . induce stochastic rules to maximize over all performance against raining data  , it not only adapts to any application domain but . also may avoid overfitting to the data . In the late 80 s and early 90 s , the induction and parameter estimation of probabilistic contextfree grammars  ( PCFGs ) from corpora were intensively studied . Because these grammars comprise only nonterminal and part-of-speech tag symbols  , their performances were not enough to be used in practical applications  ( Charniak ,  1993) . A broader ange of information , in particular lexical information , was found to be essential in disambiguating the syntactic structures of realworld sentences  . 
SPATTER ( Magerman ,  1995 ) augmented the pure PCFG by introducing a number of lexical attributes  . 
The parser controlled applications of each rule by using the lexical constraints induced by decision tree algorithm  ( Quinlan ,  1993) . The SPATTER parser attained 87% accuracy and first made stochastic parsers a practical choice  . The other type of high precision parser , which is based on dependency analysis was introduced by Collins  ( Collins ,  1996) . Dependency analysis first segments a sentence into syntactically meaningful sequences of words and then considers the modification of each segment  . Collins ' parser computes the likelihood that each segment modifies the other  ( 2 term relation ) by using large corpora . These modification probabilities are conditioned by head words of two segments  , distance between the two segments and other syntactic features  . Although these two parsers have shown similar performance  , the keys of their success are slightly different . SPATTER parser performance greatly depends on the feature selection ability of the decision tree algorithm rather than its linguistic representation  . On the other hand , dependency analysis plays an essential role in Collins ' parser for efficiently extracting information from corpora  . 
In this paper , we describe practical Japanese dependency parsers that uses decision trees  . In the Japanese language , dependency analysis has been shown to be powerful because segment  ( bunsetsu ) order in a sentence is relatively free compared to European languages  . .Japanese dependency parsers generally proceed in three steps  . 
1. Segment a sentence into a sequence of bunsetsu.
2 . Prep are a modification matrix , each value of which represents how one bunsetsu is likely to modify another  . 
3 . Find optimal modifications in a sentence by a dynamic programming technique  . 
The most difficult part is the second ; how to construct a sophisticated modification matrix  . With conventional Japanese parsers , the linguistnms t classify the bunsetsu and select appropriate fatures to compute modification values  . The parsers thus suffer from application domain diversity and the side effects of specific rules  . 

Stochastic dependency parsers like Collins ' , on the other hand , define a set of attributes for conditioning the modification probabilities  . The parsers consider all of the attributes regardless of bunsetsu type  . 
These methods can encompass only a small number of features if the probabilities are to be precisely evaluated from finite number of data  . Our decision tree method constructs a more sophisticated modification matrix  . It automatically selects a sufficient number of significant attributes according to bunsetsu type  . We can use arbitrary numbers of the attributes which potentially increase parsing accuracy  . 
Natural anguages are full of exception aln d collocational expressions  . It is difficult for machine learning algorithms , as well as human linguists , to judge whether a specific rule is relevant in terms of overall performance  . To tackle this problem , we test the mixture of sequentially generated ecision trees  . 
Specifically , we use the AdaBoost algorithm ( Freund and Schapire ,  1996 ) which iteratively performs two procedures : 1 . construct a decision tree based on the current data distribution and  2  . updating the distribution by focusing on data that are not well predicted by the constructed tree  . The final modification probabilities are computed by mixing all the decision trees according to their performance  . 
The sequential decision trees gradually change from broadcoverage to specific exceptional trees that  . cannot be captured by a single general tree . In other words , the method incorporates not only general expressions but also infrequent specific ones  . 
The rest of the paper is constructed as follows.
Section 2 summarizes dependency analysis for the Japanese language  . Section 3 explains our decision tree models that compute modification probabilities  . Section 4 then presents experimental results obtained by using EDR Japanese annotated corpora  . 
Finally , section 5 concludes the paper.
2 Dependency Analysis in Japanese

This section overviews dependency analysis in the Japanese language  . The parser generally performs the following three steps  . 
1. Segment a sentence into a sequence of bunsetsu.
2 . Prep are modification matrix each value of which represents how one bunsetsu is likely to modify the other  . 
3 . Find optimal modifications in a sentence by a dynamic programming technique  . 
Because there are no explicit delimiters between words in Japanese  , input sentences are first word segmented , part-of-speech tagged , and then chunked into a sequence of bunsetsus . The first step yields , for the following example , the sequence of bunsetsu displayed below . The parenthesis in the Japanese expressions represent the internal structures of the bunsetsu  ( word segmentations )  . 
Example : a ~ lqe ) ~7~12 ~ . ~: C)-~U~o75~r7-1'Y-~~r,A . t  ~ ( (~ l~ ) (e ~ ) )  (   ( Y  ~ ) ( I : )   )   (   ( ~ ) i ) ( e ) ) ) kinou-no yuu gata-ni kinjo-no yesterday-NO even in ~Nl neighbor-No  (   ( ~? ~ ) (~ ) )  ( ( v -? : - ) (? ) )  ( (~2 , z , ) ( t : ) ko do mo-gawain-won or nu Tta children-GA wine-WO drink+PAST The second step of parsing is to construct a modification matrix whose values represent the likelihood that one bunsetsu modifies another in a sentence  . 
In the Japanese language , we usually make two assumptions : 1 . Every bunsetsu except the last one modifies only one posterior bunsetsu  . 
2 . No modification crosses to other modifications in a sentence  . 
Table 1 illustrates a modification matrix for the example sentence  . In the matrix , columns and rows represent anterior and posterior bunsetsus  , respectively . For example , the first bunsetsu " kinou-no " modifics the second ' yuugala-ni ' with score  0  . T0 and the third ' kinjo-no ' with score 0 . 07 . The aim of this paper is to generate a modification matrix by using decision trees  . 
kfnou-no~tul#ata . ni0 . 70 yvu gata-ni**njo-no 0 . 07 0  . 10 kfnjo . noko dorna-#a 0, 100 . 10 0  . 70 kadomo*~a ~ ain-~o 0, 10 0 . 10 0  . 20 0  . 05 no mu . ta 0 . 03 0  . 70 0  . 10 0  . 95 i , aln . mlo1 . 0 0 Table 1: Modification Matrix for Sample Sentence The final step of parsing optimizes the entire dependency structure by using the values in the modification matrix  . 
Before going into our model , we introduce the notations that will be used in the model  . Let S be the input sentence . S comprises a bunsetsu set B of length m (< bl , f ~> , -  .  -  , < bm , f , , > ) in which bi and fi represent the ith bunsetsu and its features  , respectively . We define D to be a modification set ; D = rood(l ) ,  . . . , mod ( m-1 ) in which rood ( i ) indicates the number of busetsu modified by the ith bunsetsu  . Because of the first assumption , the length of D is always m-1 . Using these notations , the result of the third step for the example can be given as D =  2  ,  6 ,  4 ,  6 , 6 as displayed in Figure 1 . 
3 Decision Trees for Dependency
Analysis 3 . 1 Stochast ic Mode l and Decis ion Trees The stochastic dependency parser assigns the most plausible modification set D be  , t to a sentence Sinkmou-nouu gat 3   4 jc-noko domo-ga , ll 56 t ' ain-'0 n0 mu . tat Figure 1: Modification Set for Sample Sentence terms of the training data distribution  . 
Dbest = argmax DP ( D\[S ) = arg , naxDP ( D\[B ) By assuming the independence of modifications , P ( D\[B ) can be transformed as follows . 
P ( yeslbi , bj , fl ,  " '  , fro ) means the probability that a pair of bunsetsu bi and bj have a modification relation  . Note that each modification is constrained by all features f  ,   ,  - -  .   , froin a sentence despite of the assumption of independence  . We use decision trees to dynamically select appropriate features for each combination of bunsetsus from f  ,   ,  - - -  , fm . 
mi-~P ( yes\[bi ,  ""  , fro)P ( DIB ) = 1-I-bj , f ,   ,   . 
Let us first consider the single tree case . The training data for the decision tree comprise any unordered combination of two bunsetsu in a sentence  . 
Features used for learning are the linguistic information associated with the two bunsetsu  . The next section will explain these features in detail  . The class set for learning has binary values yes and no which delineate whether the data  ( the two bunstsu ) has a modification relation or not . In this setting , the decision tree algorithm automatically and consecutively selects the significant  , features for discriminating modify/non-modify relations  . 
We slightly changed C4 . 5 ( Quinlan ,  1993 ) programs to be able to extract class frequencies at every node in the decision tree because our task is regression rather than classification  . By using the class distribution , we compute the probability PDT ( yesl bi , bj , f  ~ ,   .   .   .   , fro ) which is the Laplace estimate of empirical likelihood that bimodifies bj in the constructed eci-sion tree DT  . Note that it . is necessary to nor-realize PDT ( yes\[bi , bj , f ,   ,   .   .   .   , fro ) to approximate P ( yes\[bi , bj , fx ,  " '  , fm ) . By considering all candidates posterior to bi , P ( yeslbi , b . i , fl ,  ' "  , fm ) is computed using a heulistic rule (1) . It is of course reasonable to normalize class frequencies instead of the probability PoT  ( yesl bi , bj , , f ,   ,   .   .   . , fro ) . Equation ( 1 ) tends to emphasize long distance dependencies more than is true for frequency-based normalization  . 
P ( yeslbi , bj , f , , . . . , f . ~) ~_
PDT ( yeslbi , bj , fl ,  ' "  , fro ) (1) ~> imPDT ( yeslbl , by , f  ~ ,   .   .   .   , f , ,  ) Let us extend the above to use a set of decision trees  . As briefly mentioned in Section 1 , a number of infrequent and exceptional expressions appear in any natural language phenomena  ; they deteriorate the overall performance of application systems  . It is also difficult for automated learning systems to detect and handle these expressions because x cep-tional expressions are placed ill the same class as frequent ones  . To tackle this difficulty , we generate a set of decision trees by adaboost ( Freund and Schapire , 1996) algorithm illustrated in Table 2 . The algorithm first sets the weights to 1 for all exana-pies ( 2 in Table 2 ) and repeats the following two procedures T times ( 3 in Table 2 )  . 
1 . A decision tree is constructed by using the current weight vector  ( ( a ) in Table 2 )  2 . Example data are then parsed by using the tree and the weights of correctly handled examples are reduced  ( ( b )  , ( c ) in Table 2) 1 . 


Input : sequence of N examples < eL , u , ~> .   .   .   . , < eN , . wN > in whichel and wi represent an example and its weight  , respectively . 
Initialize the weight vector w i = 1 for i = 1, .   .   . , N
Do for t = l , 2, .   .   . , T ( a ) Call C 4 . 5 providing it with the weight vector w , s and Construct a modification probability set ht ( b ) Let Error be a set of examples that are not . 
identified by lit
Compute the pseudo error rate of ht:e'=EiCE .   .   .   . wi/~ , = IN w , if et > 5' then abort loopl--et ( c ) For examples correctly predicted by ht , update the weights vector to be wi = wiflt4 . Output a final probability set : hl = Zt = , T ( log ~) ht/Zt = , T ( Iog ~ ) Table 2: Combining Decision Trees by Ada-boost

The final probability set hI is then computed by mixing T trees according to their performance  ( 4 in Table 2 )  . Using h : instead of PoT ( yesl bi , bj , fl ,  ' "  , f , ,~) , in equation ( 1 ) generates a boosting version of the dependency parser  . 
3.2 Linguistic Feature Types Used for
Learning
This section explains the concrete feature setting we used for learning  . The feature set mainly focuses on 2 part-of-speech of head word 7 particle ' wa ' between two bunsetsu 3 type of bunsetsu 8 punctuation between two bunsetsu 4 punctuation 5 parentheses Table 3: Linguistic Feature Types Used for Learning
Feature Type Vanet $') ,  <6~'  , ~ tE , t ~'~ t ~' , l ~' t t ~"6 ,   . : ~,-'~', 5, a ~ . , L , L ? ~', E' . '," tr . ,'t ~ L , "16, " t ', " ~ , " ~ , " ~ st ' ~- .  \ ]  . ' ~ , % * ~ t . t,-","~,\]_'0'), t . ?l ~*, ~** ?9" C,\]' . g t ~ , gl~ , 9\] '*~ , 9" C ,  99 ~ ,  ~?~ , , & ~ ,  __% ,  ~ , ~ a  ~ , @ t ,   , @ t , L , @ t , Ll2 ,  @~6 ,  ~'~" , t ? 6 , @6 Ul : , to 0 , ~k ~' , ~k ' C ,  : :  ,  ~  ,  0~ , d ) h , tl , I ~ . /J ':), ~, IE , It : , tt : : ~ . , t-C,~b,~L < I/, l . t  ~ . ~, ~-, ~ I . ~ R~I ~' ~ , . ~1~ . , ~, . ~l ~; l~\]f't it , lg'~,$1"tf ~, t~l, . V , ? IL ~\[\] glllql~\] . e~i ~\], non , k ~ . , . X , ~ J . ? ~ non , ", ~, ~ .  \ [ , \ [  . \[, ~, l , ", ', ~, , , I, . I , \], J
A(0), B(;~4), C(>5) 70,180,1
Table 4: Values for Each Feature Type ? 3 . Sie3a2sa2"graph . dirt-sooo*occo ~ Sooo 2oo00   2scoo   3o00o asooo 4ooco   45ooo soooo
N ~ bet of Ttammg Data
Figure 2: Learning Curve of Single-Tree Parser the two bunsetsu constituting each data  . . Tile class set consists of binary values which delineate whether a sample  ( the two bunsetsu ) have a modification relation or not . We use 13 features for the task ,   10 directly from the 2 bunsetsu under consideration and 3 for other bunsetu information as summarized in
Table 3.
Each bunsetsu ( anterior and posterior ) has the 5 features : No . 1 to No . 5 in Table 3 . Features No . 6 to No . 8 are related to bunsetsu pairs . Both No . 1 and No . 2 concern the head word of the bunsetsu . 
No . 1 takes values of frequent words or thesaurus categories  ( NLRI ,  1964) . No . 2 , on the other hand , takes values of part-of-speech tags . No . 3 deals with bull-setsu types which consist of functional word chunks or tile part-of-speech tags that dominate tile bull-sets u's syntactic haracteristics  . No . 4 and No . 5 are binary features and correspond to punctuation and parentheses  , respectively . No . 6 represents how many bunsetsus exist , between the two bunsetsus . Possible values are A(0), B(0--4) and C(>5) . No . 7 deals with the postpositional particle ' wa ' which greatly influences the long distance dependency of subject verb modifications  . Finally , No . 8 addresses tile punctuation between the two bunsetsu  . Tile detailed values of each feature type are summarized ill Table  4  . 
4 Experimental Results
We evaluated the proposed parser using the EDR Japanese annotated corpus  ( EDR ,  199 . 5) . The experiment consisted of two parts . One evaluated the single-tree parser and the other tile boosting counterpart  . Intilerest of this section , parsing accuracy refers only to precision ; how many of tile system's output are correct in terms of the annotated corpus  . 
We do not show recall because we assume very bunsetsu modifies only one posterior bunsetsu  . The features used for learning were nonhead word features  , ( i . e . , type 2 to 8 in Table 3) . Section 4 . 1 . 4 investigates lexical information of head words such as frequent  , words and thesaurus categories . Before going into details of tile experimental results  , we sunnna-rize here how training and test data were selected  . 
1 . After all sentences in the EDR corpus were word -segmented and part-of-speech tagged  ( Matsumoto and others ,  1996) , they were then chunked into a sequence of bunsetsu  . 
2 . All bunsetsu pairs were compared with EDR bracketing annotation  ( correct segmentations Parsing Accuracy 82 . 01% ~3 . 43~, 83 . 52% 83 . 35% Table 5: Number of Training Sentences v . s . Parsing Accuracy I Number of Training Sentences H  3000   6000   10000   20000   30000   50000 I\[\[Parsing Accuracy' 82  . 07% 82 . 70% 83 . 52% 84 . 07% 84 . 27% 84 . 33% Table 6: Pruning Confidence Level v . s . Parsing Accuracy and modifications ) . If a sentence contained a pair inconsistent with the EDR annotation  , the sentence was removed from the data . 
3 . All data examined ( total number of sen-tences : 207802 , total number of bun-set . su : 1790920) were divided into 20 files , The training data were same number of first sentences of the  20 files according to the training data size . Test data ( 10000 sentences ) were the 2501th to 3000th sentences of each file . 
4.1 Single Tree Experiments
In the single tree experiments , we evaluated the following 4 properties of the new dependency parser . 
? Tree pruning and parsing accuracy ? Number of training data and parsing accuracy ? Significance of features other than Head word 
Lexical Information ? Significance of Head word Lexical Information  4  . 1 . 1 P run ing and Pars ing Accuracy Table 5 summarizes the parsing accuracy with various confidence levels of pruning  . The number of training sentences was 10000 . 
In C4 . 5 programs , a larger value of confidence means weaker pruning and  25% is connnonly used in various domains ( Quinlan ,  1993) . Our experimental results show that 75% pruning attains the best performance , i . e . weaker pruning than usual . In the remaining single tree experiments , we used the 75% confidence level . Although strong pruning treats infrequent data as noise  , parsing involves many exceptional and infrequent modifications as mentioned before  . Our result means that only information included in small numbers of samples are useful for disambiguating the syntactic structure of sentences  . 
4.1.2 The amount of Training Data and
Parsing Accuracy
Table 6 and Figure 2 show how the number of training sentences influences parsing accuracy for the same  10000 test . sentences . They illustrate tile following two characteristics of the learning curve  . 
1 . The parsing accuracy rapidly rises up to 30000 sentences and converges at around 50000 sentences . 
2 . The maximum parsing accuracy is 84 . 33% at 50000 training sentences . 
We will discuss the maximum accuracy of 84.33%.
Compared to recent stochastic English parsers that yield  86 to 87% accuracy ( Collins , 1996; Magerman ,  1995) ,  84 . 33% seems unsatisfactory at the first glance . The main reason behind this lies in the difference between the two corpora used : Penn Treebank  ( Marcus et al , 1993) and EDR corpus ( EDR ,  1995) . Penn Treebank ( Marcus et al ,  1993 ) was also used to induce part-of-speech ( POS ) taggers because the corpus contains very precise and detailed POS markers as well as bracket  , annotations . In addition , English parsers incorporate the syntactic tags that are contained in the corpus  . The EDR corpus , on the other hand , contains only coarse POS tags . We used another Japanese POS tagger ( Matsumoto and others ,  1996 ) to make use of well-grained information for disambiguating syntactic structures  . Only the bracket information in the EDR corpus was considered  . We conjecture that the difference between the parsing accuracies i due to the difference of the corpus information  . ( Fujio and Matsumoto ,  1997 ) constructed an EDR-based dependency parser by using a similar method to Collins '  ( Collins ,  1996) . The parser attained 80 . 48% accuracy . Although thier training and test . sentences are not exactly same as ours , the result seems to support our conjecture on the data difference between EDR and Penn Treebank  . 
4.1.3 Significance of NonHead Word

We will now summarize tile significance of each nonhead word feature introduced in Section  3  . The influence of the lexical information of head words will be discussed in the next section  . Table 7 illustrates how the parsing accuracy is reduced when each feature is removed  . The number of training sentences was 10000 . In the table , ant and post . represent , heanterior and the posterior bunsetsu , respectively . 
Table 7 clearly demonstrates that the most signifi-ant POS of head  -0  . 07% post punctuation +1 . 62(7( , ant bunsetsu type ant punctuation ant parentheses post POS of head post bunsetsu type  +9  . 34% +1 . 15% +0 . 00% +2 . 13% +0 . 52% post parentheses - e0 . 00% distance between two bunsetsus +5 . 21% punctuation between two bunsetsus +0 . 01% ' wa'between two bunsetsus+1 . 7 9% Table 7: Decrease of Parsing Accuracy When Each Attribute Removed 
Head Word Information
Parsing Accuracy l\] 100words   200words Levell Level2 I 83  . 34% 82 . 68%82 . 51%81 . 67% Table 8: Head Word Information v . s . Parsing Accuracy cant features are anterior bunsetsu type and distance between the two bunsetsu  . This result may partially support an often used heuristic  ; bunsetsu modification should be as short range as possible  , provided the modification is syntactically possible  . In particular , we need to concentrate on the types of bunsetsu to attain a higher level of accuracy  . Most features contribute , to some extent , to the parsing performance . In our experiment , information on parentheses has no effect on the performance  . The reason may be that EDR contains only a small number of parentheses  . One exception in our features is anterior POS of head  . We currently hypothesize that this drop of accuracy arises from two reasons  . 
? In many cases , the POS of head word can be determined from bunsetsu type  . 
? Our POS tagger sometimes assigns verbs for verb -derived nouns  . 
4.1.4 Significance of Head-words Lexical
Information
We focused on the head word feature by testing the following  4 lexical sources . The first and the second are the 100 and 200 most frequent words , respectively . The third and the fourth are derived from a broadly used Japanese thesaurus  , Word List by Semantic Principles ( NLRI ,  1964) . Level 1 and Level 2 classify words into 15 and 67 categories , respectively . 
1 . 100 most Frequent words 2 . 200 most Frequent words 3 . Word List Level 14 . Word List Level 2 Table 8 displays the parsing accuracy when each head word information was used in addition to the previous features  . The number of training sentences was 10000 . In all cases , the performance was worse than 83 . 5 2% which was attained without head word lexical information  . More surprisingly , more head word information yielded worse performance  . From this result , it . may be safely said , at least , for the Japanese language , ' that we cannot expect , lexica \] information to always improve the performance  . Further investigation of other thesaurus and clustering  ( Charniak ,  1997 ) technique sinecessary to fully understand the influence of lexical information  . 
4.2 Boosting Experiments
This section reports experimental results on the boosting version of our parser  . In all experiments , pruning confidence levels were set . to 55% . Table 9 and Figure 3 show the parsing accuracy when the number of training examples was increased  . Because the number of iterations in each dataset changed between  5 and 8  , we will show the accuracy by combining the first 5 decision trees . In Figure 3 , the dotted line plots the learning of the single tree case  ( identical to Figure 2 ) for reader's convenience . The characteristics of the boosting version can be summarized as follows compared to the single tree version  . 
? The learning curverises more rapidly with a small number of examples  . It is surprising that the boosting version with 10000 sentences performs better than the single tree version with  50000 sentences . 
? The boosting version significantly outperforms the single tree counterpart for any number of sentences although they use the same features for learning  . 
Next , we discuss how the number of iterations influences the parsing accuracy  . Table 10 shows the parsing accuracy for various iteration numbers when  50000 sentences were used as training data . The re-suits have two characteristics . 
? Parsing accuracy rose uprapidly at the second iteration  . 
* No overfitting to data was seen although the performance of each generated tree fell around  30% at the final stage of iteration . 

IN ombe . oT . i , , i , , gSe , l*e , , co . I 3OO0   6OOO   I'0000   2OOOO   3OO0O   5O0OO IParsing Accuracy 83  . 10% 84 . 03% 84 . 44% 84 . 74% 84 . 91% 85 . 03% Table 9: Number of Training Sentences v . s . Parsing Accuracy Parsing Accuracy\[\[84 . 32% 84 . 93% 84 . 89% 84 . 86% 85 . 03% 85 . 01% I Table 10: Number of Iteration v . s . Parsing Accuracy 5 Conclusion We have described a new Japanese dependency parser that uses decision trees  . First , we introduced the single tree parser to clarify the basic characteristics of our method  . The experimental result show that it outperforms conventional stochastic parsers by  4%  . Next , the boosting version of our parser was introduced  . The promising results of the boosting parser can be summarized as follows  . 
? The boosting version outperforms the single-tree counterpart regardless of training data amount  . 
? No data overfitting was seen when the number of iterations changed  . 
We now plan to continue our research in two directions  . One is to make our parser available to a broad range of researchers and to use their feedback to revise the features for learning  . Second , we will apply our method to other languages , say English . Although we have focused on the Japanese language  , it is straightforward to modi ~" our parser to work with other languages  . 

B2"laoostJng.O = r "///'/

N ~ berOtTra~mg Oata
Proc .   15th National Conference on Artificial 172-telligence   , pages 598-603 . 
Michael Collins .  1996 . A New Statistical Parser based on bigram lexical dependencies  . In Proc . 
3 4th Annual Meeting of Association for Computational Linguistics  , pages 184-191 . 
Japan Electronic Dictionary Reseaech Institute Ltd . 
EDR , 1995 . the EDR Electronic Dictionary Technical Guide . 
Yoav Freund and Robert Schapire .  1996 . A decision-theoretic generalization of online learning and an application to boosting  . 
M . Fujio and Y . Matsumoto .  1997 . Japanese dependency structure analysis based on statistics  . 
In SIGNLNL 117-12, pages 83-90. ( in Japanese).
David M . Magerman .  1995 . Statistical Decision-Tree Models for Parsing . In Proc . 3 3rd Annual Meeting of Association for Computational Linguistics  , pages 276-283 . 
Mitchell Marcus , Beatrice Santorini , and Mary Ann Marcinkiewicz .  1993 . Building a large annotated corpus of English : The Penn Treebank  . Computational Linguistics , 19(2):313-330, June . 
Y . Matsumoto et al1996. Japanese Morphological
Analyzer Chasen 2.0 User's Manual.
NLRI . 1964. Word List by Semantic Principles.
Syuei Syuppan . ( in Japanese).
J . Ross Quinlan .  1993 . C4 . 5 Programs for Machine Learning . Morgan Kaufinann Publishers . 
Figure 3: Learning Curve of Boosting Parser

Eugene Charniak .  1993 . Statistical Language Learning . The MIT Press . 
Eugene Charniak .  1997 . Statistical Parsing with a Context-free Grammar and Word Statistics  . In
