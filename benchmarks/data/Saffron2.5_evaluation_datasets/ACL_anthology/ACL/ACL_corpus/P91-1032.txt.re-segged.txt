FINITE-STATE APPROXIMATION
OF PHRASEST RUCTUREG RAMM ARS
Fernando C . N . Pereira
AT&T Bell Laboratories
600 Mountain Ave.
Murray Hill , NJ 07974
Rebecca N . Wright
Dept . of Computer Science , Yale University
PO Box 2158 Yale Station
New Haven , CT06520

Phrase-structure grammars are an effective representation for important syntactic and semantic aspects of natural anguages  , but are computationally too demanding for use as language models in realtime speech recognition  . An algorithm is described that computes finite -state approximations for contextfree grammars and equivalent augmented phrase-structure grammar formalisms  . 
The approximation is exact for certain contextfree grammars generating regular languages  , including all left-linear and right-linear contextfree grammars  . The algorithm has been used to construct finite -state language models for limited-domain speech recognition tasks  . 
1 Motivation
Grammars for spoken language systems are subject to the conflicting requirements of language modeling for recognition and of language analysis for sentence interpretation  . Current recognition algorithms can most directly use finite-state acceptor  ( FSA ) language models . However , these models are inadequate for language interpretation  , since they cannot express the relevant syntactic and semantic regularities  . Augmented phrase structure grammar ( APSG ) formalisms , such as unification-based grammars ( Shieber , 1985a ) , can express many of those regularities , but they are computationally ess suitable for language modeling  , because of the inherent cost of computing state transitions in APSG parsers  . 
The above problems might be circumvented by using separate grammars for language modeling and language interpretation  . Ideally , the recognition grammar should not reject sentences acceptable by the interpretation grammar and it should contain as much as reasonable of the constraints built into the interpretation grammar  . 
However , if the two grammars are built independently , those goals are difficult to maintain . For this reason , we have developed a method for constructing automatically a finite-state approximation for an APSG  . Since the approximation serves as language model for a speech recognition frontend to the real parser  , we require it to be sound in the sense that the it accepts all strings in the language defined by the APSG  . Without qualification , the term " approximation " will always mean here " sound approximation  . " If no further constraints were placed on the closeness of the approximation  , the trivial algorithm that assigns to any APSG over alphabet E the regular language E * would do  , but of course this language model is useless . One possible criterion for " goodness " of approximation arises from the observation that many interesting phrase-structure grammars have substantial parts that accept regular languages  . That does not mean that the grammar rules are in the standard forms for defining regular languages  ( left-linear or right-linear )  , because syntactic and semantic onsiderations often require that strings in a regular set be assigned structural descriptions not definable by left-or right-linear rules  . A useful criterion is thus that if a grammar generates a regular language  , the approximation algorithm yields an acceptor for that regular language  . In other words , one would like the algorithm to be exact for APSGs yielding regular languages  .   1 While we have not proved that in general our method satisfies the above exactness criterion  , we show in Section 3 . 2 that the method is exact for left-linear and right-linear grammars  , two important classes of contextfree grammars generating regular languages  . 
1 At first sight , this requirement may be seen as conflicting with the undecidability of determining whether a CFG generates a regular language  ( Harrison ,  1978) . However , note that the algorithm just produces an approximation  , but cannot say whether the approximation is exact . 
2462 The Algorithm
Our approximation method applies to any contextfree grammar  ( CFG )  , or any unification-based grammar ( Shieber , 1985a ) that can be fully expanded into a contextfree grammar  .   2 The resulting FSA accepts all the sentences accepted by the input grammar  , and possibly some non-sentences as well . 
The current implementation accepts as input a form of unification grammar in which features can take only atomic values drawn from a specified finite set  . Such grammars can only generate contextfree languages  , since an equivalent CFG can be obtained by instantiating features in rules in all possible ways  . 
The heart of our approximation method is an algorithm to conver the LR  ( 0 ) characteristic machine . Ad(G ) ( Aho and Ullman , 1977; Backhouse ,  1979 ) of a CFGG into an FSA for a superset of the language L  ( G ) defined by G . The characteristic machine for a CFG G is an FSA for the viable prefixes of G  , which are just the possible stacks built by the standard shift-reduce recognizer for 
G when recognizing strings in L(G).
This is not the place to review the characteristic machine construction in detail  . However , to explain the approximation algorithm we will need to recall the main aspects of the construction  . The states of . ~4(G ) are sets of dotted rules A---*a . \[3 where A---, a/~ is some rule of G .   . A4 ( G ) is the determinization by the standard subset construction  ( Aho and Ullman ,  1977 ) of the FSA defined as follows : ? The initial state is the dotted rule ff ---  , -S where S is the start symbol of G and S ' is a new auxiliary start symbol  . 
? The final state is S'--~S..
? The other states are all the possible dotted rules of G  . 
? There is a transition labeled X , where X is a terminal or nonterminal symbol , from dotted rule A-+a . X ~ toA--+c ~ X . // . 
? There is an e-transition from A--~a ? B/~to B - - ~  "7  , where B is a nonterminal symbol and B -+ 7 a rule in G . 
2Unification-based grammars not in this class would have to be weakened first  , using techniques akin to those of Sato and Tamaki  ( 1984 )  , Shieber (1985b ) and Haas (1989) . 
IS'->.SS->.AbA->.AaA->.

Is '-> s . \]' Aqk ~ SA '> A' . ba Ja ~\ [ A . > Aa . j Figure 1: Characteristic Machine for G1 . A  ~ ( G ) can be seen as the finite state control for a nondeterministic shift-reduce pushdown recognizer TO  ( G ) for G . A state transition labeled by a terminal symbol z from states to state s ' licenses a shift move  , pushing onto the stack of the recognizer the pair  ( s , z ) . Arrivalata state containing a completed dotted rule A--~a  . licenses a reduction move . This pops from the stack as many pairs as the symbols in a  , checking that the symbols in the pairs match the corresponding elements of a  , and then takes the transition out of the last state poppeds labeled by A  , pushing(s , A ) onto the stack .   ( Full definitions of those concepts are given in Section  3  . ) The basic ingredient of our approximation algorithm is the flattening of a shift-reduce recognizer for a grammar G into an FSA by eliminating the stack and turning reduce moves in toe-transitions  . 
It will be seen below that flattening 7~ ( G ) directly leads to poor approximations in many interesting cases  . Instead, . bq ( G ) must first be unfolded into a larger machine whose states carry information about the possible stacks of g  ( G )   . The quality of the approximation is crucially influenced by how much stack information is encoded in the states of the unfolded machine : too little leads to coarse approximations  , while too much leads to redundant automata needing very expensive optimization  . 
The algorithm is best understood with a simple example  . Consider the left-linear grammar G1
S - - - . A b
A---*A aJe
AJ(G1) is shown on Figure 1 . Unfolding is not required for this simple example  , so the approximating FSA is obtained from . Ad(G1) by the flattening method outlined above . The reducing states in AJ(G1) , those containing complete dotted rules , are states 0 , 3 and 4 . For instance , the reduction at state 4 would lead to a transition on nonter-
Figure 3: Minimal Acceptorminal A , to state 2 , from the state that activated the rule being reduced  . Thus the corresponding e-transition goes from state  4 to state 2  . Adding all the transitions that arise in this way we obtain the FSA in Figure  2  . From this point on , the arcs labeled with nonterminals can be deleted  , and after simplification we obtain the deterministic finite automaton  ( DFA ) in Figure 3 , which is the minimal DFA for L(G1) . 
If flattening were always applied to the LR ( 0 ) characteristic machine as in the example above , even simple grammars defining regular languages might be in exactly approximated by the algorithm  . The reason for this is that in general the reduction at a given reducing state in the characteristic machine transfers to different states depending on context  . In other words , the reducing state might be reached by different routes which use the result of the reduction in different ways  . 
Consider for example the grammar G2
S ~ aX a \] bXb
X -'* c which accepts just the two strings a ca and bcb  . 
Flattening J ~4 ( G2 ) will produce an FSA that will also accept a cb and bca  , an undesirable outcome . 
The reason for this is that the e-transitions leaving the reducing state containing X~c  . do not distinguish between the different ways of reaching that state  , which are encoded in the stack of Oneway of solving the above problem is to unfold each state of the characteristic machine into a set of states corresponding to different stacks at that state  , and flattening the corresponding recognizer rather than the original one  . However , the set of possible stacks at a state is in general in finite  . Therefore , it is necessary to do the unfolding not with respect os tacks  , but with respect o a finite partition of the set of stacks possible at the state  , induced by an appropriate equivalence rla-tion . The relation we use currently makes two stacks equivalent if they can be made identical by collapsing loops  , that is , removing portions of stack pushed between two arrivals at the same state in the finite-state control of the shift-reduce recognizer  . The purpose of collapsing loops is to ~ forget " stack segments that may be arbitrarily repeated  , s Each equivalence class is uniquely defined by the shortest stack in the class  , and the classes can be constructed without having to consider all the  ( infinitely ) many possible stacks . 
3 Formal Properties
In this section , we will show here that the approximation method described informally in the previous section is sound for arbitrary CFGs and is exact for left -linear and right-linear CFGs  . 
In what follows , G is a fixed CFG with terminal vocabulary ~ , nonterminal vocabulary N , and start symbol S ; V = ~ UN . 
3.1 Soundness
Let J~4 be the characteristic machine for G , with state set Q , start state so , set of final states F , and transition function ~: SxV--*S . As usual , transition function such as 6 are extended from input symbols to input strings by defining  6  ( s , e)--s and 6 is , a /~) = 5 (6(s , a ) , /~) . The shift-reduce recognizer 7~ associated to A4 has the same states , start state and final states . Its configurations are triples Is , a , w ) of a state , a stack and an input string . The stack is a sequence of pairs/s , X ) of a state and a symbol . The transitions of the shift-reduce recognizer are given as follows : Shift : is  , a , zw)t-(s ' , a/s , z ) , w ) if 6(s , z ) = s'Reduce : is , err , w)~-/5(s' , A ) , cr / s ' , A / , w ) if either ( 1 ) A--~? is a complete dotted rule 3Since possible stacks can be shown to form a regular language  , loop collapsing has a direct connection to the pumping lemma for regular languages  . 
248 in s , s "= s and r is empty , or (2) AX 1 .   .   . X n . is a completed dotted rule in s,T = is 1, Xl ) .   .   . ( sn,Xn ) and s "= 81 . 
The initial configurations of ~ are ( so , e , w for some input string w , and the final configurations are ( s , ( so , S ) , e ) for some states EF . A derivation of a string w is a sequence of configurations  c0   ,   .   .   .   , cm such that c0 = ( s0 , e , w ) , c , ~= ( s , ( so , S ) , e ) for some final states , and ei1l-ci for l < i < n . 
Lets be a state . We define the set Stacks ( s ) to contain every sequence ( s0 , X 0) .   .   . ( sk , Xk ) such that si = 6 ( si-l , Xi-1) , l < i < k and s = 6 ( st , Xk ) . In addition , Stacks(s0) contains the empty sequence . By construction , it is clear that if ( s , a , w ) is reachable from an initial configuration in ~ , the no-ES tacks ( s ) . 
A stack congruence on 7? is a family of equivalence relations_=o on Stacks  ( s ) for each states E8 such that if o-= , a ' and/f(s , X ) = d the no-(s , X = , ,  , r(s , X ) . A stack congruence ---- partitions each set Stacks  ( s ) into equivalence classes\[<r \]? of the stacks in Stacks  ( s ) equivalentoo-under --_ ,  . 
Each stack congruence-on ~ induces a corresponding unfolded recognizer  7~-  . The states of the unfolded recognizer axe pairs i s  , M ,  )  , notated more concisely as\[~\]? , of a state and stack equivalence class at that state  . The initial state is \[ e\] , o , and the final states are all\[o-\]?withsEF and o-ES tacks  ( s )  . The transition function 6- of the unfolded recognizer is defined by t- ( \[ o-\]' , x ) =\[ o-is , x )  \ ] ' ( '' x ) That this is well-defined follows immediately from the definition of stack congruence  . 
The definitions of dotted rules in states , configurations , shift and reduce transitions given above carry over immediately to unfolded recognizers  . 
Also , the characteristic recognizer can also be seen as an unfolded recognizer for the trivial coarsest congruence  . 
Unfolding a characteristic recognizer does not change the language accepted : Proposition  1 Let G be a CFG ,   7~ its characteristic recognizer with transition function ~  , and = a stack congruence on T ? . Then the unfolded recognizer ~=_ and 7~ are equivalent recognizers . 
Proof : We show first that any string w accepted by T ? --- is accepted by  7~  . Let do , .   .   . , dm be a derivation of w in ~= . Each di has the form di = (\[ P/\]" , o'i , ul ) , and can be mapped to an T ? configuration di = ( sl , 8 i , ul ) , where ? = E and (( s , C ) , X ) = 8 is , X ) . It is straightforward to verify that do , .   .   . , d , , is a derivation of w in ~ . 
Conversely , let wEL(G ), and c0, .   .   .   , em be a derivation of w in 7~ , with ci = isl , o-i , ui ) . 
We define el = (\[ ~ ri\]s ~ , hi , ui ) , where ~= e and o-is , x ) = aito-\]' , x ) . 
If ci1P ci is a shift move , then ui-1 = zui and 6(si-l , z ) = si . Therefore ,  6-@ , _  ,  \ ] " - '  , ~) =\[ o -~- , ( s ~ - , ,~)\]~("- '") = \[ o- , \]' , 
Furthermore , ~= o-~-l(S ,  - 1 ,  ~) = ~ , -1(\[o- , - 1 \ ] " - '  ,  ~)
Thus we have ~' , -x = (\[ o-l-d "-' , ai-x , * u ,  ) ~ , =@ d " , e~-l(P~-d"-' ,  * )  , ~'~) with 6_=(\[o-i-1\]"-' , z ) =\[ o-i \]" . Thus , by definition of shift move , 6i-1I-6i in 7?_-- . 
Assume now that ei1I-ci is a reduce move in ~ . Then ui = ui-1 and we have a states in 7~ , a symbol AEN , a stacko - and a sequencer of state-symbol pairs such that si =  6  ( s , A ) o-i-1 = o"1"o- , = o-(s , a ) and either ( a ) A --* ? is insi-t , s = si1 and r = e , or ( b)A--- , XI .   .   . X n . is in si1, r = ( ql,X d .   .   . ( q . , X . ) and s = ql-
Let ~=\[ o-\]* . Then 6= ( ~ , A ) =\[ o-(s , A)p0 , A ) =\[ o-d " We now define a pair sequence ~ to play the same role in  7~- asr does in ~ . In case ( a ) above , ~= e . Otherwise , let rl = e and ri = ri-l(qi-l , Xi-1) for 2 < i(n , and define ~ by = (\[ dq ' , xl ) .   .   . @hiq ', xi ) ???(\[ ~ . p -, x .  )

O'i--1--~-0"7" = o-(q1, X1) .   .   . ( q . -x , x . -x ) x . )--? r(q ~, X , .   .   . ( qi-hXi-l ) xd-- . x . ) == a(\[d ' , A ) = a(# , A ) ~ i = (~ f = (& A) , a(~ , A) , ui ) which by construction of e immediately entails that ~_  1 ~- C i is a reduce move in ~= . fl For any unfolded state p , letPop ( p ) be the set of states reachable from p by a reduce transition  . 
More precisely , Pop ( p ) contains any state pl such that there is a complete dotted rule A--*  ( ~ . in p and a state pll such that 6-(pI ~ , ~)-p and 6-(f * , A ) -- f . Then the flattening ~ r = of ~- is a nondeterministic FSA with the same state set  , start state and final states as ~- and nondeterministic transition function @= defined as follows : ? If  6=  ( p , z ) - p t for some zEE , then fE?If p ~ EPop(p ) then fE~b = ( p ,  ~) . 
Letco, .   .   .   , cm be a derivation of string w in ~ , and put ei--(q ~ , ~ q , wl ) , and p~=\[~\]~' . By construction , if ci_~F ci is a shift move on z ( wi-x--zw ~) , then 6= ( pi-l , Z ) = Pi , and thus p~~~-(p ~_ ~ , z ) . Alternatively , assume the transition is a reduce move associated to the completed dotted rule A--*a  . . We consider first the case a ~ ~ . Puta--X 1 .   .   . X  ~ . By definition of reduce move , there is a sequence of states r l ,   .   .   . , r ~ and a stack#such that o'i-x = ?( r ~, X1) . . . ( rn , Xn ) , qi--#(r  ~ , A ) , 5(r  ~ , A ) = qi , and 5(rj , X 1 ) - ri + ~ for 1 ~ j < n . By definition of stack congruence , we will then have = where rx = ? and rj = ( r ~ , X ,  )  .   .   . ( r~-x,X~-,)for j > 1 . Furthermore , again by definition of stack congruence we have 6=  ( \[cr\]r * , A ) = Pi . Therefore , Pi 6 Pop(pi_l ) and thus pie ~_--( pi-x , ?) . A similar but simpler argument allows us to reach the same conclusion for the case a = e  . Finally , the definition of final state for g = and ~ r__ makes Pma final state  . Therefore the sequence P0, .  -  . , Pm is an accepting path for w in ~ r_ . We have thus proved Proposition 2 For any CFGG and stack congruence =_ on the canonical LR  ( 0 ) shift-reduce recognizer 7~ ( G ) of G , L(G)C_L(~r-(G )) , where ~ r-(G ) is the flattening of of T~(G ) -- . 
Finally , we should show that the stack collapsing equivalence described informally earlier is indeed a stack congruence  . A stack r is a loop if '/"-"(81, X1) . . . ( sk,Xk ) and 6(sk,Xt ) = sz . A stack ~ collapses to a stack ~' if cr = pry , cr ~= pv and r is a loop . Two stacks are equivalent if they can be collapsed to the same stack  . This equivalence relation is closed under suffixing  , therefore it is a stack congruence . 
3.2 Exactness
While it is difficult to decide what should be meant by a " good " approximation  , we observed earlier that a desirable feature of an approximation algo-rithm would be that it be exact for a wide class of CFGs generating regular languages  . We show in this section that our algorithm is exact both for left-linear and for right-linear contextfree grammars  , which as is wellknown generate regular languages . 
The proofs that follow rely on the following basic definitions and facts about the LR  ( 0 ) construction . Each LR ( 0 ) states is the closure of a set of a certain set of dotted rules  , its core . The closure \[ R\] of a set R of dotted rules is the smallest set of dotted rules containing R that contains B--~  "7 whenever it contains A--~a ? Bfl and B---* 7 is in G . The core of the initial states o contains just the dotted rule ff ~  . S . For any other states , there is a state 8 ~ and a symbol X such that 8 is the closure of the set core consisting of all dotted rules A ~ a X  . / ~ where A --* a . X / ~ belongs to s ' . 
3.3 Left-Linear Grammars
In this section , we assume that the CFGG is left-linear , that is , each rule in G is of the form AB/~or A--+/~ , where A , BEN and/3E ~* . 
Proposition 3 Let G be a left-linear CFG , and let gz be the FSA produced by the approximation algorithm from G  . Then L(G ) = L(3r ) . 
Proof : By Proposition 2, L(G ) C . L ( . ~') . Thus we need only show L(~)C_L(G ) . 
The proofhing es on the observation that each states of At  ( G ) can be identified with a string EV * such that every dotted rule in s is of the form A ~ ~  . a for some AEN and c~EV * . 

Clearly , this is true for so =\[ S'--* . S\], with ~0 = e . 
The core k of any other states will by construction contain only dotted rules of the form A ~ a  . 
with a ~ e . Since G is left linear , /3 must be a terminal string , ensuring that s =\[ h\] . Therefore , every dotted rule A--*a . fins must result from dotted rule A ~ . aft in so by the sequence of transitions determined by a  ( since ? tq ( G ) is deterministic )  . This means that if A~a . f and A'--*a ' . fl ' are in s , it must be the case that a-a ~ . In the remainder of this proof , let ~= s whenever a = ~ . 
To go from the characteristic machine . M(G ) to the FSA ~' , the algorithm first unfolds Ad ( G ) using the stack congruence relation , and then flat-tens the unfolded machine by replacing reduce moves with e-transitions  . However , the above argument shows that the only stack possible at a states is the one corresponding to the transitions given by $  , and thus there is a single stack congruence state at each state  . Therefore, . A4(G ) will only be flattened , not unfolded . Hence the transition function ? for the resulting flattened automaton ~" is defined as follows  , where aE
N ~* U \] ~* , aE  ~ , and AEN : ( a ) ?( ~ , a ) = ~( b ) ? (5 , e ) = . 4 IA --, aeGThe start state of ~" is ~ . The only final state is S . 
We will establish the connection between Y ~ derivations and G derivations  . We claim that if there is a path from ~ to S labeled by w then either there is a rule A--*a such that w = xy and S : ~ Ay = ~ azy  , or a = S and w = e . The claim is proved by induction on Iw\[ . 
For the base case , suppose . \[ wI = 0 and there is a path from & to . ~ labeled by w . Then w = e , and either a-S , or there is a path of e-transitions from ~ to S . In the latter case , S = ~ A=~e for some AEN and rule A--~e , and thus the claim holds . 
Now , assume that the claim is true for all I wl < k , and suppose there is a path from & to , ~ labeled wI , for some \[ wl\[=k . Then wI-aw for some terminal a and Iw\[<k , and there is a path from ~-~ to S labeled by w . By the induction hypothesis , S = ~ . Ay = ~ aaz'y , where A-- . * aaz ~ is a rule and zly-w ( since aay ? S) . Letting z--axI , we have the desired result . 
If wEL (~) , then there is a path from ~ to labeled by w . Thus , by claim just proved , S = ~ Ay : : ~: cy , where A ~? is a rule and w = ~ y ( since e#S ) . Therefore , S = ~ w,sow~L(G ), as desired . 
3.4 Right-Linear Grammars
A CFGG is right linear if each rule in G is of the form A--~f BorA --*  /3  , where A , BEN and Proposition 4 Let G be a right-linear CFG and 9 e be the unfolded , flattened automaton produced by the approximation algorithm on input G  . Then
L(G ) = L(Yz).
Proof : As before , we need only show L(~') C

Let ~ be the shift-reduce recognizer for G . The key fact to notice is that , because G is right-linear , no shift transition may follow a reduce transition  . 
Therefore , no terminal transition in 3 c may follow ane-transition , and after any e-transition , there is a sequence of G-transitions leading to the final state \[$'--* S  . \] . Hence ~" has the following kinds of states : the start state  , the final state , states with terminal transitions entering or leaving them  ( we call these reading states )  , states with e-transitions entering and leaving them  ( prefinal states )  , and states with terminal transitions entering them and e-transitions leaving them  ( cr0ss over states )  . Any accepting path through ~" will consist of a sequence of a start state  , reading states , across over state , prefinal states , and a final state . The exception to this is a path accepting the empty string  , which has a start state , possibly some prefinal states , and a final state . 
The above argument also shows that unfolding does not change the set of strings accepted by ~  , because any reduction in 7~ = ( ore-transition in jc )  , is guaranteed to be part of a path of reductions ( e-transitions ) leading to a final state of 7~_- ( ~ )  . 
Suppose now that w = w : .   .   . wn is accepted by ~' . Then there is a path from the start state So through reading states sl  ,   .   .   .   , s , ,-1 , to cross over states n , followed bye-transitions to the final state . We claim that if there there is a path from sl to sn labeled wi + l  .   .   . wn , then there is a dotted rule A---*x ? yB in si such B : ~ z and y z =  w~+1  . .  . wn , where AEN , BENU ~* , y , z ~ ~* , and one of the following holds : ( a ) z is a nonempty suffix of w t .   .   . wi , ( b ) z = e,A "= ~ A,A'--*z' . A " is a dotted rule in sl , and zt is a nonempty suffix of T1 .   .   . wi , or ( c ) z = e , si = s0, and S = ~ A . 
We prove the claim by induction on n-i . For the base case , suppose there is an empty path from must be some dotted rule A~x  . in sn . Letting y = z = B = e , we get that A---*z . yB is a dotted rule of s , and B =  z . The dotted rule A--', z . yB must have either been added to 8n by closure or by shifts . If it arose from a shift , z must be a nonempty suffix of w l .   .   . wn . If the dotted rule arose by closure , z = e , and there is some dotted rule A~--~zt ? A " such that A "= ~ A and ~ l is a nonempty suffix of W l  .   .   . wn . 
Now suppose that the claim holds for paths from si to sn  , and look at a path labeled wi .   .   . wn from si1to sn . By the induction hypothesis , A ~ z ? yB is a dotted rule of st , where B = ~ z , uz = wi + l .   .   . wn , and ( since st~s0) , either z is a nonempty suffix of w l .   .   . wiorz = e,A~- . z  ~ . A " is a dotted rule of si , A ": ~ A , and z ~ is a nonempty suffix of wl .   .   . wl . 
In the former case , when z is a nonempty suffix of w l .   .   . wl , then z = wj .   .   . wi for some 1 < j < i . Then A---, wj .   .   . wl ? yB is a dotted rule of sl , and thus A---*wj .   .   . wi-1?wiy B is a dotted rule of si_l . If j < i-1, then wj .   .   . wi_l is a nonempty suffix of w l .   .   . wi-1, and we are done . 
Otherwise , wj .   .   . wi-1=e , and so A--* . wiy B is a dotted rule of si-1 . Let y ~= wiy . Then A  ~ . yJB is a dotted rule of si1 , which must have been added by closure . Hence there are nonterminals AI and A " such that A ": ~ A and AI~zI?A " is a dotted rule of st-l  , where z ~ is a nonemptys UtTLX of W l . . ? wi - 1 . 
In the latter case , there must be a dotted rule A ~ ~ wj .   .   . wi-1 ? wiA " insi1 . The rest of the conditions are exactly as in the previous case  . 
Thus , if w-wl .   .   . wn is accepted by ~ c , then there is a path from so to sn labeled by wl .   .   . w , . 
Hence , by the claim just proved , A ~ z . yB is a dotted rule of sn , and B : ~ z , where yz-"wl .   .   . wa--w . Because the st in the claim is so , and all the dotted rules of si can have nothing before the dot  , and z must be the empty string . 
Therefore , the only possible case is case 3 . Thus , S : ~ A - - - , yz = w , and hence wEL(G ) . The proof that the empty string is accepted by ~" only if it is in L  ( G ) is similar to the proof of the claim . 
D4A Complete Example
The appendix shows an APSG for a small fragment of English  , written in the notation accepted by the current version of our grammar compiler  . 
The categories and features used in the grammar are described in Tables  1 and 2   ( categories without features are omitted )  . Features enforce person-number agreement , personal pronoun case , and a limited verb subcategorization scheme . 
Grammar compilation has three phrases : ( i ) construction of an equivalent CFG , ( ii ) approximation , and ( iii ) determinization and minimization of the resulting FSA  . The equivalent CFG is derived by finding all full instantiations of the initial APSG rules that are actually reachable in a derivation from the grammar's start symbol  . In the current implementation , the construction of the equivalent CFG is is done by a Prolog program  , while the approximator , determinizer and minimizer are written in C . 
For the example grammar , the equivalent CFG has 78 nonterminals and 157 rules , the unfolded and flattened FSA 2615 states and 4096 transitions , and the determinized and minimized final DFA 16 states and 97 transitions . The runtime for the whole process is 4 . 91 seconds on a Sun
Sparc Station 1.
Substantially larger grammars , with thousands of instantiated rules , have been developed for a speech-to-speech translation project  . Compilation times vary widely , but very long compilations appear to be caused by a combinatorial explosion in the unfolding of right recursions that will be discussed further in the next section  . 
5 Informal Analysis
In addition to the cases of left-linear and right -linear grammars discussed in Section  3  , our algo-rithm is exact in a variety of interesting cases  , including the examples of Church and Patil (1982) , which illustrate how typical attachment ambiguities arise as structural ambiguities on regular string sets  . 
The algorithm is also exact for some self -embedding rammars  4 of regular languages , such as
S--+a SlSbl c defining the regular language a *eb*  . 
A more interesting example is the following simplified grammar for the structure of English noun  4 A grammar is self-embedding if and only if licenses the derivation X ~ c ~ X ~ for nonempty c ~   and/3  . A language is regular if and only if it can be described by some non-self-embedding grammar  . 

Figure 4: Acceptor for Noun Phrases phrases :
NP-+Det Nom\[PN
Det-+Art\]NP's
Nom-+NIN om PPJ Adj Nom
PP--*PNP
The symbols Art , N , PN and P correspond to the parts of speech article  , noun , proper noun and preposition . From this grammar , the algorithm derives the DFA in Figure 4 . 
As an example of inexact approximation , consider the the self-embedding CFG
S-+a SbI ~ for the nonregular language a ' ~ b ' ~ , n > O . This grammar is mapped by the algorithm into an FSA accepting ~ I a + b +  . The effect of the algorithm is thus to " forget " the pairing between a's and b's mediated by the stack of the grammar's characteristic recognizer  . 
Our algorithm has very poor worst-case performance  . First , the expansion of an APSG into a CFG , not described here , can lead to an exponential blow up in the number of nonterminals and rules  . Second , the subset calculation implicit in the LR ( 0 ) construction can make the number of states in the characteristic mach in exponential on the number of CF rules  . Finally , unfolding can yield another exponential bow-up in the number of states  . 
However , in the practical examples we have considered , the first and the last problems appear to be the most serious  . 
The rule instantiation problem may be alleviated by avoiding full instantiation of unification grammar rules with respecto " don't care " features  , that is , features that are not constrained by the rule . 
The unfolding problem is particularly serious in grammars with subgrammars of the form 
S-+XISI "" JX ,, SJY(I)
It is easy to see that the number of unfolded states in the subgrammar is exponential in n  . This kind of situation often arises indirectly in the expansion of an APSG when some features in the right hand side of a rule are unconstrained and thus lead to many different instantiated rules  . In fact , from the proof of Proposition 4 it follows immediately that unfolding is unnecessary for right-linear grammars  . Ultimately , by dividing the grammar into non-mutually recursive  ( strongly connected ) components and only unfolding center-embedded components  , this particular problem could heavoided , sIn the meanwhile , the problem can be circumvented by left factoring  ( 1 ) as follows:
S-+ZS\[Yz-+x , I . . . IX .
6 Related Work and Conclusions
Our work can be seen as an algorithmic realization of suggestions of Church and Patil  ( 1980 ;  1982 ) on algebraic simplifications of CFGs of regular languages  . Other work on finite state approximations of phrase structure grammars has typically relied on arbitrary depth cutoffs in rule application  . 
While this is reasonable for psycholinguistic modeling of performancer strictions on center embedding  ( Pulman ,  1986) , it does not seem appropriate for speech recognition where the approximating FSA is intended to work as a filter and not reject inputs acceptable by the given grammar  . For instance , depth cutoffs in the methodes cribed by Black ( 1989 ) lead to approximating FSAs whose language is neither a subset nor a superset of the language of the given phrase-structure grammar  . 
In contrast , our method will produce an exact FSA for many interesting grammars generating regular languages  , uchas those arising from systematic attachment ambiguities  ( Church and Patil ,  1982) . 
It important to note , however , that even when the result FSA accepts the same language  , the original grammar is still necessary because interpret a-SWe have already implemented a version of the algorithm that splits the grammar into strongly connected components  , approximates and minimizes separately each component and combines the results  , but the main purpose of this version is to reduce approximation addeterminization costs for some grmmmars  . 
2 53 tion algorithms are generally expressed in terms of phrase structures described by that grammar  , not in terms of the states of the FSA . 
Although the algorithm described here has mostly been adequate for its intended application -- grammar sufficiently complex not to be approximated within reasonable time and space bounds usually yield automata that are far too big for our current realtime speech recognition hardware--it would be eventually of interest to handle right-recursion ialess profligate way  . In a more theoretical vein , it would also be interesting to characterize more tightly the class of exactly approximable grammars  . Finally , and most spec-ulatively , one would like to develop useful notions of degree of approximation of a language by a regular language  . Formal-language-theoretic notions such as the rational index  ( Boason et al , 1981) or probabilistic ones ( Soule ,  1974 ) might be profitably investigated for this purpose . 

We thank Mark Liberman for suggesting that we look into finite-state approximations and Pedro Moreno  , David Roe , and Richard Sproat for trying out several prototypes of the implementation and supplying test grammars  . 

Alfred V . Aho and Jeffrey D . Ullman .  1977 . Princi . 
pies of Compiler Design . Addison-Wesley , Reading,

Rol and C . Backhouse .  1979 . Syntazo \] Programming Languages -- The orll and Practice  . Series in Computer Science . Prentice-Hall , Englewood Cliffs , New

Alan W . Black .  1989 . Finite state machines from feature grammars . In Masaru Tomita , editor , Inter . 
national Workshop on Parsing Technologies , pages 277-285 , Pittsburgh , Pennsylvania . Carnegie Mellon University . 
Luc Boason , Bruno Courcelle , and Maurice Nivat.
1981 . The rational index : a complexity measure for languages  . SIAM Journalo \] Computing , 10(2):284-296 . 
Kenneth W . Church and Ramesh Patil .  1982 . Coping with syntactic ambiguity or how to put the block in the box on the table  . Computational Linguistics , 8(3--4):139-149 . 
Kenneth W . Church .  1980 . On memory\]imitations in ? naturalanguage processing  . Master's thesis , M . I . T . 
Published as Report MIT/LCS/TR-245.
Andrew Haas .  1989 . A parsing algorithm for unification grammar . Computational Linguistics , 15(4):219-232 . 
Michael A . Harrison .  1978 . Introduction to Formal Language The or ~ l . Addison-Wesley , Reading , Mas-sachussets . 
Steven G . Pulman .  1986 . Grammars , parsers , and memory limitations . Language and Cognitive Processes , 1(3):197-225 . 
Taisuke Sato and Hisao Tamaki .  1984 . Enumeration of success patterns in logic programs  . Theoretical
Computer Science , 34:227-240.
Stuart M . Shieber . 1985a . An Introduction to Unification-Based Approaches to Grammar  . Number 4 in CSLI Lecture Notes . Center for the Study of Language and Information , Stanford , California . 
Distributed by Chicago University Press.
Stuart M . Shieber . 1985b . Using restriction to extend parsing algorithms for complex-feature-based formalisms  . In ~3rd Annual Meeting of the Association \] or Computational Linguistics  , pages 145-152 , Chicago , Illinois . Association for Computa-tionai Linguistics , Morristown , New Jersey . 
Stephen Soule .  1974 . Entropies of probabilistic grammars . In \] ormation and Control , 25:57-74 . 
Appendix APSG Formalism and Example
Nonterminal symbols ( syntactic ategories ) may have features that specify variants of the category  ( eg . singular or plural noun phrases , intransitive or transitive verbs ) . A category cat with feature constraints i written cat#\[ca  ,  ? ? ? , em3 . 
Feature constraints for feature f have one of the forms  . f =, , (2)\] = c(3) . f = ( c ~ .   .   .   .   . c .   )   ( 4 ) where v is a variable name ( which must be capitalized ) and c , cl ,   .   .   . , c , are feature values . 
All occurrences of a variable v in a rulestand for the same unspecified value  . A constraint with form ( 2 ) specifies a feature as having that value . A constraint of form ( 3 ) specifies an actual value for a feature , and a constraint of form ( 4 ) specifies that a feature may have any value from the specified set of values  . The symbol "!" appearing as the value of a feature in the righthand side of a rule indicates that that feature must have the same value as the feature of the same name of the category in the lefthand side of the rule  . 
This notation , as well as variables , can be used to enforce feature agreement between categories in a rule  , ?s sentence npv pargs detn pron
V noun phrase verb phrase verb arguments determiner noun pronoun verb n  ( number )  , p(person)n , p , c(case)n , p , t(verb type ) tnnn , p , Cn , p , t Table 1: Categories of Example Grammar
Feature n ' ( number ) p ( person ) c ( case ) t ( verb type ) 
Valuess ( singular) , p(plural)!(first ) , 2 ( second ) , 3 ( third ) s ( subject ) , o(non subject ) i ( intransitive ) , t ( transitive ) , d ( ditransitive ) Table 2: Features of Example Grammar for instance , number agreement between Subject and verb . 
It is convenient to decl are the features and possible values of categories with category declarations appearing before the gramma rules  . Category declarations have the form cat C at S\[ /1  =  ( Vll .   .   .   . , V2kl), .   . o , fm = ( vml .   .   .   . , Vmk ,)\] . 
giving all the possible values of all the features for the category  . 
The declaration start cat.
declares cat as the start symbol of the grammar.
In the gramma rules , the symbol "'" prefixes terminal symbols , commas are used for sequencing and \[" for alternation  . 
starts.
catsg\[n = Cs , p = (1, 2, 3)\].
cat npg\[n = ( s,p ), p = (1, 2, 3), c = ( s , o)\].
catvpg\[n = ( s,p ), l >= (1, 2, 3), type = ( i , t,d)\].
catargsg\[type=(i.t,d)\].
cat detg\[n = ( s,p)\].
cat ng\[n = ( s,p)\].
cat prong\[n = ( s,p ), p = (1, 2, 3), c = ( s , o)\].
catvg\[n-(s , p ) , p = (1 , 2 , 3)  , type = ( i , t , d )\] . 
s => npg\[n = ! , pffi ! , c = s\] , vpg\[n = ! , p = !\] . 
npg\[p=3\]=>detg\[n=!\] , adjs , ng\[n = !\] . 
nl~\[n=s,p-3\]->pn.
np => prongIn = !, p = !, c = !\].
prong\[n=s,p-1, c=s\]=>'i.
prong\[p=2\]=>'you.
prong\[n=s,p=3, c=s\]=>' heI'she.
prong\[n-s,p3\]=>'it.
prong\[nffip,l~l,c-s\]=>'vs.
prong\[n = p,p = 3, c=s\]>'they.
prong\[n=s,p-l,c=o\]=>'me.
prong\[n=s , p=3 , c = o \] => ' him\[prong\[n = p , p=1 , c = o \] >' us . 
prong\[n = p , p-3, c = o \] => ' them.

vp => vg\[n = ! , p = ! , type=:\] , argsg\[type=!\] . 
adjs -> ~.
adjs => adj , adjs.
args#\[ type=i\]=>\[\].
args#\[ type=t\]=>npg\[c = o\].
argsg\[type-d\]=>npg\[c=o\] , ' to , npg\[cfo\] . 
pn=>'tonI'dick\['harry.
det =>' soae J ' the.
det#\[n=s\]=>'every\['a , det#\[n-p\]=>'all\['most . 
n#\[n=s\]=>' child\['cake.
n#\[n ~ p \] =>' children I ' cakes.
adj .->' niceJ'sgeet.
v#\[n=s,l~3, type=i \]>' sleeps.
v#\[nffip , type=i \]>' sleep.
v#\[n=s,l ~, (1, 2), type=/\]=>' sleep.
v#\[n-s,p3, type=t\]->'eats.
v#\[n~p , type-t\]=>'eat.
v#\[n=s,p-(1,2), type=t\]ffi > ' eat.
v#\[n=s,pffi3, type=d\]>'gives.
v#\[nffip , type-d\]=>'give.
v#\[n=s,p = (1, 2), type=d\]=>'give.

