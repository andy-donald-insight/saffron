AFLEXIBLEEX AMPLE-BAS EDPARSER BASE DONTHESS TC "
Mosleh Hmoud A1-Adhaileh & Tang Enya Kong
Computer Aided Translation Unit
School of computer sciences
University Sains Malaysia
11800 PEN ANG , MAL AYSIA
mosleh@cs.usm.my , enyakong@cs.usm.my

In this paper we sketch an approach for Natural Language parsing  . Our approach is an example-based approach , which relies mainly on examples that already parsed to their representation structure  , and on the knowledge that we can get from these examples the required information to parse a new input sentence  . In our approach , examples are annotated with the Structured String Tree Correspondence  ( SSTC ) annotation schema where each SSTC describes a sentence  , a representation tree as well as the correspondence btween substrhzgs in the sentence and subtrees in the representation tree  . In the process of parsing , we first try to build subtrees for phrases in the input sentence which have been successfully found in the example-base-a bottom up approach  . 
These subtrees will then be combined together to form a single rooted representation tree based on an example with similar representation structure -atopdown approach  . 
Keywords : Example-based parsing , SSTC.
1. INTRODUCTION
In natural anguage processing ( NLP ) , one key problem is how to design an effective parsing system  . 
Natural language parsing is the process of analyzing or parsing that takes sentences in a natural anguage and converts them to some representation form suitable for further interpretation towards some applications might be required  , for example , translation , text abstraction , question-answering , etc . 
The generated representation tree structure can be a phrase structure tree  , a dependency tree or a logical structure tree , as required by the application involved . 
Here we design an approach for parsing natural language to its representation structure  , which depends on related examples already parsed in the example-base  . This approach is called example-based parsing , as oppose to the traditional approaches of natural language parsing which normally are based on rewriting rules  . Here linguistic knowledg extracted directly from the example-base will be used to parse a natural language sentence  ( i . e . using past language experiences instead of rules )  . For a new sentence , to build its analysis ( i . e . representation structure tree ) , ideally if the sentence is already in the example -base  , its analysis is found there too , but in general , the input sentence will not be found in the example-base  . 
In such case , a method is used to retrieve close related examples and use the knowledge from these examples to build the analysis for the input sentence  . 
In general , this approach relies on the assumption that if two strings  ( phrase or sentence ) are " close " , their analysi should be " close " too . If the analysis of the first one is known , the analysis of the other can be obtained by making some modifications in the analysis of the first one  . 
The example-based approach has become a common technique for NLP applications  , especially in MT as reported in \[1\] , \[2\] or\[3\] . However , a main problem normally arises in the current approaches which indirectly limits their applications in the development of a largescale and practical example-based system  . Namely the lack of flexibility in creating the representation tree due to the restriction that correspondences betwee nodes  ( terminal or nonterminal ) of the representation tree and words of the sentence must be one-to-one and some even restrict it to only in projective manner according to certain traversai order  . This restriction or mally results to the inefficient usage of the example-base  . In this paper , we shall first discuss on certain cases where projective representation trees are inadequate for characterizing representation structures of some natural linguistic phenomena  , i . e . featurisation , lexicalisation and crossed dependencies . Next , we ? The work reported in this paper is supported by the IRPA research programs  , under project number 04-02-05-6001 funded by the Ministry of Science , Technology and Environment , Malaysia . 
6 87 propose to overcome the problem by introducing a flexible annotation schema called Structured String-Tree Correspondence  ( SSTC ) which describes a sentence lare presentation tree  , and the correspondence between substrings in the sentence and subtrees in the representation tree  . Finally , we present a algorithm to parse natural language sentences based on the SSTC annotation schema  . 
2 . NON-PROJECTIVE CORRES PONDE-NCE SINN A TURAL LANGUAGE 

In this section , we shall present some cases where projective representation tree is found to be inadequate for characterizing representation tree of some natural language sentences  . The cases illustrated here are featurisation , lexicalisation and crossed dependencies . An example containing mixture of these nonprojective correspondences alo will be presented  . 
2.1 Featurisation
Featurisation occurs when a linguist decides that a particular substring in the sentence  , should not be represented as a subtree in the representation tree but perhaps as a collection of features  . For example , as illustrated in figure 1 , this would be the case for prepositions in arguments which can be interpreted as part of the predicate and not the argument  , and should be featurised into the predicate ( e . g . " up " in " picks-up ") , the particle " up " is featurised as a part of the feature properties of the verb " pick "  . 
picks up
He picks up the ball
Figure 1: Featurisation 2 . 2 Lex ica l i sa t ion Lexicalisation is the case when a particular subtree in the representation tree presents the meaning of some part of the string  , which is not orally realized in phonological form  . Lexicalisation may result from the correspondence of a subtree in the tree to an empty substring in the sentence  , or substring in the sentence to more than one subtree in the tree  . Figure 2 illustrates the sentence " John eats the apple and Mary the pear " where " eats " in the sentence corresponds to more than one node in the tree  . 
and ea_./"oO ~ ~ eats
John eats the apple and Mary tile pear
Figure 2: Lexicalisation 2 . 3 Crossed dependenc ies The most complicated case of string-tree correspondence is when dependencies are intertwined with each other  . It is a very common phenomenon in a turalanguage . In crossed dependencies , subtree in the tree corresponds to single substring in the sentence  , but the words in a substring are distributed over the whole sentence in a discontinuous manner  , in relation to the subtree they correspond to . An example of crossed dependencies i occurred in the bncn sentences of the form  ( anvIn > 0 )  , figure 3 illustrates the representation tree for the string " a a v b b c c "  ( also written a . la . 2vb . lb . 2c . lc . 2 to show the positions ) , this akin to the ' respectively ' problem in English sentence like " John and Mary give Paul and Anntrousers and dresses respectively "  \[4\]  . 
va . 1b . 1\[c . 1__v1'4 ?
Figure 3: Crossed dependencies
Sometimes the sentence contains mixture of these nonprojective correspondences  , figure 4 illustrates the sentence " He picks the ball up " , which contains both featurisation and crossed dependencies  . Here , the particle " up " is separated from its verb " picks " by a noun phrase " the ball " in the string  . And " up " is featurised into the verb " picks " ( e . g . " up " in " picks-up ") . 
picl/pick:sup
Figure 4: Mixture of featurisation and crosse dependencies
CORRESPONDENCE ( SSTC)
The correspondence bt ween the string on one hand , and its representation fmeaning on the other hand  , is defined in terms of finer subcorrespondences between substrings of the sentence and subtrees of the tree  . Such correspondence is made of two interrelated correspondences  , one between nodes and substrings , and the other between subtrees and substrings ,   ( the substrings being possibly discontinuous in both cases  )  . 
The notation used in SSTC to denote a correspondence on sists of a pair of intervals X/Y attached to each node in the tree  , where X ( SNODE ) denotes the interval containing the substring that corresponds to the node  , and Y ( STREE ) denotes the interval containing the substring that corresponds to the subtree having the node as root  \[4\]  . 
Figure 5 illustrates the sentence " all cat seatmice " with its corresponding SSTC  . It is a simple projective correspondence . An interval is assigned to each word in the sentence  , i . e . (01) for " all " , (12) for " cats " , (23) for " eat " and (34) for " mice " . A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of the node  , e . g . 
the node " cats " with SNODE interval ( 12 ) corresponds to the word " cats " in the string with the similar interval  . The correspondence between subtrees and substrings are denoted by the interval assigned to the STREE of each node e  . g . the subtree rooted at node " eat " with STREE interval  ( 04 ) corresponds to the whole sentence " all cat seat mice "  . 
Treeeat (2-3/0-4) 3.4, 3.4, all (0-1/0-1)~t
String all cat seat mice ( 01 )   ( 12 )   ( 23 )   ( 34 ) Figure 5: An SSTC recording the sentence " all cat seat mice " and its Dependency tree together with the correspondences between substrings of the sentence and subtrees of the tree  . 
4. USES OF SST CANNOTATION IN
EXAMPLE-BASED PARSING
In order to enhance the quality of example-based systems  , sentences in the example-base are normally annotated with theirs constituency or dependency structures which in turn allow example-based parsing to be established at the structural level  . To facilitate such structural annotation , here we annotate the examples based on the Structured String-Tree Correspondence  ( SSTC )  . The SSTC is a general structure that can associate  , to string in a language , arbitrary tree structure as desired by the annotator to be the interpretation structure of the string  , and more importantly is the facility to specify the correspondence between the string and the associated tree which can be interpreted for both analysis and synthesis in NLP  . These features are very much desired in the design of an annotation scheme  , in particular for the treatment of linguistic phenomena which are not-standard e  . g . crossed dependencies \[5\] . 
Since the example in the example-base are described in terms of SSTC  , which consists of a sentence ( the text ) , a dependency tree ' ( the linguistic representation ) and the mapping between the two ( correspondence )  ; example-based parsing is performed by giving a new input sentence  , followed by getting the related examples ( i . e , examples that contain same words in the input sentence  ) from the example-base , and used them to compute the representation tree for the input sentence guided by the correspondence bt ween the string and the tree as discussed in the following sections  . Figure 6 illustrates the general schema for example-based NL parsing based on the SSTC schema  . 


Example . Iibased/\
Parsing Output
Figure 6: Example-based natural language parsing based on the SSTC schema  . 
4.1 The parsing algorithm
The example-based approach in MT\[1\] , \[2\] or\[3\] , relies on the assumption that if two sentences are " close "  , their analysis should be " close " too . If the analysis of the first one is known , the analysis of the other can be obtained by making some modifications in the analysis of the first one  ( i . e . 
iEach node is tagged with syntactic category to enable substitution at category level  . 
689 close : distance not too large , modification : edit operations ( insert , delete , replace )\[6\] . 
In most of the cases , similar sentence might not occurred in the example-base  , so the system utilized some close related examples to the given input sentence  ( i . e . similar structure to the input sentence or contain some words in the input sentence  )  . For that it is necessary to construct several subSSTCs  ( called substitutions hereafter ) for phrases in the input sentence according to their occurrence in the examples from the example -base  . These substitutions are then combined together to form a complete SSTC as the output  . 
Suppose the system in tends to parse the sentence " the oldman picks the green lamp up "  , depending on the following set of examples representing the example-base  . 
picks v\]uplp\](1-2+4-5/0-5)
He\[hiballn\](0-1/0-1) (3-4/2-4)
Ithe\[detl(2-3/2-3)
He picks the ballup 011223 3445 ( 1 ) tums\[v\] ( 3-4/0-5 ) signal n\]on\[adv\] ( 2-3/0-3 )   ( 4-5/4-5 ) / ~ theldet\]green\[adj\] ( 0-1/0-1 )   ( 1-2/1-2 ) 
The green signal turns on 01I2233445 ( 2 ) isv\] ( 2-3/0-4 ) lamp\[nloff\[adv\] ( 1-2/0-2 )   ( 3-4/3-4 ) 
Itheld et l(0-1/0-1)
The lamp is of f01I22334 diedv\] ( 3-4/0-4 ) mJn\[n\] ( 2-3/0-3 ) the\[det\]old\[adj\] ( 0-1/0-1 )   ( 1-2/1-2 ) 
The old mandied 0112 2334 ( 3 )   ( 4 ) The example-base is first processed to retrieve some knowledge related to each word in the example-base to form a knowledge index  . Figure 7 shows the knowledge index constructed based on the example-base given above  . The knowledge retrieved for each word consists of  :   1  . Example number : The example number of one of the examples which containing this word with this knowledge  . Note that each example in the example-base is assigned with a number as its identifier  . 
2 . Frequency : The frequency of occurrence in the example-base for this word with the similar knowledge  . 
3. Category : Syntactic ategory of this word.
4 . Type : Type of this word in the dependency tree ( 0: terminal , l : nonterminal ) . 
-Terminal word : The word which is at the bottom level of the tree structure  , namely the word without any son/s under it ( i . e . 
STREE = SNODE in SSTC annotation).
-Nonterminal word : The word which is linked to other word/s at the lower level  , namely the word that has son/s(i . e . 
STREE ~: SNODE in SSTC annotation).
5 . Status : Status of this word in the dependency tree  ( 0: root word , 1: nonroot word , 2: friend word ) -Friend word : In case of featurisation , if a word is featurised into other word , this word is called friend for that word , e . g . the word " up " is a friend for the word " picks " in figure  1  . 
6 . Parent category : Syntactic ategory of the parent node of this word in the dependency tree  . 
7 . Position : The position of the parent node in the sentence  ( 0: after this word , 1: before this word ) . 
8 . Next knowledge : A pointer pointing to the next possible knowledge of this word  . Note that a word might have more than one knowledge  , e . g . "man " could be a verb or a noun . 
Based on the constructed knowledge index in figure  7  , the system built the following table of knowledge for the input sentence : The input sentence : the old man picks the green  01   12   23   34   45   56 the 0   1   1 old 1   2   4 man 2   3   4 picks 3   4   1 the 4   5   1 green 5   6   2 lamp 6   7   3 up 7   8   1   4 det 0   1 nlad j0   1 n 1 n 1   1 v 1 v 1   0   4 det 0   1 nlad j0   1 v 1 n 1 iv 1 pl 2 v lamp up 67   78   0 nil 0 nil 0 nilnil 0 nil 0 nil 0 nil 1 nilNote that to each word in the input sentence , the system built a record which contain the word , SNODE interval , and a linked list of possible knowledge related to the word as recorded in the knowledge index  . The following figure describes an example record for the word < the >: 
This mean : the word < the > , snode (0-1) , one of the examples that contain the word with this knowledge is example l  , this knowledgerpeated 4 time in the example-base , the category of the word is < det > , it is a terminal node , nonroot node , the parent category is < n > , and the parent appear after it in the sentence . 
690 = gl Example No . If requeneyl category It peIs~tus IP arent category l Position IN extKn  . \[ the ~ I4 det0In 0 nil . 
old-~41 adj 0 In 0 nil.
he-~IIn0Iv0nil.
turns-~21vI0nil.
ball-~IIn1 Iv Inil.
green-~2 I adj01 n0 nil.
signal-~2In IIv 0 nil.
on-~21adv0Iv1nil.
ticks-~I1vI0nil.
off-~31 adv01 v1 nil.
man-~4 In 11v0nil.
died-~4 Iv I0 nil.
lamp-~3 In 1Iv 0 nil.
up-~11 pI2 v1 nil.
Figure 7: The knowledge index for the words in the example -base  . 
This knowledge will be used to build the substitutions for the input sentence  , as we will discuss in the next section . 
4.1.1 Substitutions generation
In order to build substitutions , the system first classifies the words in the input sentence into terminal words and nonterminal words  . For each terminal word , the system tries to identify the nonterminal word it may be connected to based on the syntactic category and the position of the nonterminal word in the input sentence  ( i . e . before or after the terminal word ) guided by SNODE interval . 
In the input sentence given above , the terminal words are " the " , " old " and " green " and based on the knowledge table for the words in the input sentence  , they may be connected as sonnode to the first nonterminal with category\[n\]which appear after them in the input sentence  . 
For (" the "01 , and " old "12 ) they are connected as sons to the word ( " man "23 )  . 
nowledge I\]N on terminal I able II wordStn\]I For  ( " the "45 , and " green "56 ) they are connected as sons to the word ( " lamp " 67 )  . 
~nowledge II Non-terminal I
I -, ~"-", pv---lamp\[n\]
I'he'~-~SU~ebnStl_~ertaUttio ? ? I ~ ....
Igreen ~" ~ generator
The remainder nonterminal words , which are not connected to any terminal word , will be treated as separate substitutions . 
From the input sentence the system builds the following substitutions respectively : man\[n \] picks\[v\]lamp\[n\]up\[p\]  ( 2 -3/0-3 )   ( 3-4/0-8 )   ( 6-7/4-7 )   ( 7 -8 / -  ) the l det\]old\[adj\]the\[de ( \] green\[adj \] ( 0-1/0-1 )   ( 1-2/1-2 )   ( 4-5/4-5~  ( 5-6/5-6 )   ( 1 )   ( 2 )   ( 3 )   ( 4 ) Note that this approach is quite similar to the generation of constituents in bottom-up chart parsing except that the problem of handling multiple overlapping constituents is not addressed here  . 
4.1.2 Substitutions combination
In order to combine the substitutions to form a complete SSTC  , the system first finds nonterminal words of input sentence  , which appear as root word of some dependency trees in the example SSTCs  . If more than one example are found ( in most cases )  , the system will calculate the distance between the input sentence and the examples  , and the closest example to proceed further . 
In our example , the word " picks " is the only word in the sentence which can be the root word  , so example ( 1 ) which containing " pick " as root will be used as the base to construc the output SSTC  . The system first generates the substitutions for example  ( 1 ) based on the same assumptions mentioned earlier in substitutions generation  , which are : heln\]Picks\[v\]ball\[n\]uplPl ( 0-1/0-1 )   ( 1-2/0-5 )   ( 3-4~2-4 )   ( 45/- ) 
Ithe\[det\](2-3/2-3)(1)(2)(3) (4)
Distance calculation:
Here the system utilizes distance calculation to determine the plausible example  , which SSTC structure will be used as a base to combine the substitutions at the input sentence  . We define a heuristic to calculate the distance , in terms of editing operations . Editing operations are insert ( E-->p) , deletion ( p--)E ) and replacing ( a "-) s) . Edition distances , which have been proposed in many works\[7\] , \[8\] and\[9\] , reflect a sensible notion , and it can be represented as metrics under some hypotheses  . They defined the edition distances as number of editing operations to transfer one word to another form  , i . e . 
how many characters needed to be edited based on insertion  , deletion or replacement . Since words are strings of characters , entences are strings of words , editing distances hence are not confined to words  , they may be used on sentences\[6\] . 
With the similar idea , we define the edition distance as: ( i ) The distance is calculated at level of substitutions  ( i . e . only the root nodes of the substitutions will be considered  , not all the words in the sentences ) .   ( ii ) The edit operations are done based on the syntactic ategory of the root nodes  , ( i . e . the comparison between the input sentence and an example is based on the syntactic ategory of the root nodes of their substitutions  , not based on the words ) . 
The distance is calculated based on the number of editing operations  ( deletions and insertion ) needed to transfer the input sentence substitutions to the example substitutions  , by assigning weight to each of these operations :   1to insertion and 1 to deletion . 
e.g.:a ) S1: The old maneats an apple.
$2: He eats a sweet cake.
man\[n\]eats\[v\]f'a pllein ) the ~\[ adj\]ea ~ ~ ~ an\[det\]
HeIn\]Iv\]cakeln\]aldet\]sweet\[adj\ ] In  ( a )  , the distance between S1 and $2 is 0 . 

He(nlboy\[nl

The\[detl
S1: He eats an apple in the garden.
$2: The boy who drink stea eats the cake.
eats\[v\]~~garden\[n\]who~\[~l\]dri ~: : ~~ ~ ln\] 
Ithe\[det\]
In ( b) , the distance between S1 and $2 is (3+2) = 5 . 
Note that when a substitution is decided to be deleted from the example  , all the words of the related substitutions ( i . e . the root of the substitutions and all other words that may link to it as brothers  , or son/s ) , are deleted too . This series is determined by referring to an example containing this substitution in the example-base  . For example in ( b ) above , the substitution rooted with " who " must be deleted  , hence substitutions " drinks " and " tea " must be deleted too  , similarly " in " must be delete dhence " garden " must be deleted too  . 
Before making the replacement , the system must first check that the root nodes categories for substitutions in both the example and the input sentence are the same  , and that these substitutions are occurred in the same order  ( i . e . the distance is 0) . If there exist additional substitutions in the input sentence  ( i . e . the distance ~:0) , the system will either combine more than one substitution into a single substitution based on the knowledge index before placement is carried out or treat it as optional substitution which will be added as additional subtree under the root  . On the other hand , additional substitutions appear in the example will be treated as optional substitutions and hence can be removed  . 
Additional substitutions are determined during distance calculation  . 

Next the substitutions in example ( 1 ) will be replaced by the corresponding substitutions generated from the input sentence to form a final SSTC  . The replacement for the example in preorder traversal  , and each substitution in the tree structure replaced with its corresponding substitution i the input sentence  . This approach is analogous to topdown parsing technique  . 
Figure 8 , illustrates the parsing schema for the input sentence " The old malt picks the green lamp up "  . 
Input sentence
The old man picks the green lamp up substitutions I im I  ( I ) ~thelde qold ladj \] (  2  )  ~
Ithe\[det\]greenladjl\[(4) k ~ ~ p . -picks lvlup\[Pl(1-2+4-5/0-5)/\
He\[hiballlnl(0-1/0-1) (3-4/2-4)
Itheld et l(2-3/2-3)
He picks the ballup 011223 3445
SSTC base\[i ; i structure ~, , , ~ . . . . .
? II . J
Replacement \] l~-q

SSTC example substitutions
I , tl , oln lI-(2) ~
Iuptp ) Ic4) I !
OutputSSTC ~ , structure picks\[v\]uplp\]man\[n\] ( 2-3/0-3 ) lamp\[n\] ( 6-7/4-7 ) /\/\ the\[det\]oldladj \] the\[det\]green\ [ adj \]  ( O-I/0-l )   ( 1-2/1-2 )   ( 4-5/4-5 )   ( 5-6/5-6 ) 
The oldman picks the green lamp up 01   I2   23   34   45   56   67   78 
I ............
Figure 8: The parsing schema based on the SSTC for the sentence " the old man picks the green lamp up " using example  (  1  )  . 
5. CONCLUSION
In this paper , we sketch an approach for parsing NL string , which is an example-based approach relies on the examples that already parsed to their representation structures  , and on the knowledge that we can get from these examples information eeded to parse the input sentence  . 
A flexible annotation schema called Structured String-Tree Correspondence  ( SSTC ) is introduced to express linguistic phenomena such as featurisation  , lexicalisation and crossed dependencies . We also present an overview of the algorithm to parse natural language sentences based on the SSTC annotation schema  . However , to obtain a full version of the parsing algorithm  , there are several other problems which needed to be considered further  , i . e . the handling of multiple substitutions , an efficient method to calculate the distance between the input sentence and the examples  , and lastly a detailed formula to compute the resultant SSTC obtained from the combination process especially when deletion of optional substitutions are involved  . 
References : \[1\] M . Nagao , " A Framework of a mechanical translation between Japanese and English by analogy principle "  , in ; A . Elithorn , R . Benerji , ( Eds . ), Artificial and Human Intelligence , Elsevier :

\[2\]V . Sadler & Vendelmans , " Pilot implementation fabilingual knowledge bank "  , Proc . of Coling-90,
Helsinki , 3, 1990, 449-451.
\[3\] S . Sato & M . Nagao , " Example-based Translation of technical Terms " , Proc . of TMI-93, Koyoto , 1993, 58-68 . 
\[4\]Y . Zaharin & C . Boitet , " Representation trees and string-tree correspondences "  , Proc . of Coling-88,
Budapest , 1988, 59-64.
\[5\]E . K . Tang & Y . Zaharin , " Handling Crossed Dependencies with the STCG " , Proc . of NLPRS'95,
Seoul , 1995,\[6\]Y . Lepage & A . Shin-ichi , " Saussurian analogy : a theoritical account and its application "  , Proc . of Coling-96, Copenhagen , 2, 1996, 717-722 . 
\[7\] V . I . Levenshtein , " Binary codes capable of correcting deletions , insertions and reversals " , Dokl . 
Akad.SSR , 163, No . 4, 1965, 845-848.
English translation hz Soviet Physics-dok lady ,  10 , 
No . 8, 1966, 707710.
\[8\] Robert A . Wagner & Michael J . Fischer , " The String-toString Correction Problem " , Journal for the Association of Computing Machinery  ,  21 , No .  1, 1974, 168-173 . 
\[9\] Stanley M . Selkow , " The Tree-to-Tree Editing Problem " , Information Processing Letters ,  6 , No .  6, 1977, 184-186 . 

