Computing Optimal Descriptions for Optimality Theory 
Grammars with ContextFree Position Structures
Bruce Tesar
The Rutgers Center for Cognitive Science /
The Linguistics Department
Rutgers University
Piscataway , NJ 08855 USA
tesar@ruccs , rutgers , edu
Abstract
This paper describes an algorithm for
computing optimal structural descriptions
for Optimality Theory grammars with
contextfree position structures . This
algorithm extends Tesar's dynamic pro-
gramming approach ( Tesar , 1994) ( Tesar,
1995@to computing optimal structural
descriptions from regular to contextfree structures  . The generalization to contextfree structures creates several complications  , all of which are overcome without compromising the core dynamic programming approach  . The resulting algorithm has a time complexity cubic in the length of the input  , and is applicable to grammars with universal constraints that exhibit contextfree locality  . 
1 Comput ing Optimal Descriptions in Opt ima l i ty Theory In Optimality Theory  ( Prince and Smolensky ,  1993) , grammaticality is defined in terms of optimization  . 
For any given linguistic input , the grammatical structural description of that input is the description  , selected from a set of candidate descriptions for that input  , that best satisfies a ranked set of universal constraints  . The universal constraints often conflict : satisfying one constraint may only be possible at the expense of violating another one  . These conflicts are resolved by ranking the universal constraints in a strict dominance hierarchy : one violation of a given constraint is strictly worse than any number of violations of a lower-ranked constraint  . 
When comparing two descriptions , the one which better satisfies the ranked constraints has higher Harmony  . Crosslinguistic variation is accounted for by differences in the ranking of the same constraints  . 
The term linguistic input should here be understood as something like an underlying form  . In phonology , an input might be a string of segmental material ; in syntax , it might be a verb's argument structure , along with the arguments . For exposi-tional purposes , this paper will assume linguistic inputs to be ordered strings of segments  . A candidate structural description for an input is a full linguistic description containing that input  , and indicating what the ( pronounced ) surface realization is . An important property of Optimality Theory ( OT ) grammars is that they do not accept or reject inputs  ; every possible input is assigned a description by the grammar  . 
The formal definition of Optimality Theory posits a function  , Gen , which maps an input to a large ( often in finite ) set of candidate structural descriptions , all of which are evaluated in parallel by the universal constraints  . An OT grammar does not itself specify an algorithm  , it simply assigns a grammatical structural description to each input  . However , one can ask the computational question of whether efficient algorithms exist to compute the description assigned to a linguistic input by a grammar  . 
The most apparent computational challenge is posed by the allowance of faithfulness violations : the surface form of a structural description may not be identical with the input  . Structural positions not filled with input segments constitute over pars-ing  ( epen thesis )  . Input segments not parsed into structural positions do not appear in the surface pronunciation  , and constitute under parsing ( deletion ) . 
To the extent hat under parsing and over parsing are avoided  , the description is said to be faithful to the input  . Crucial to Optimality Theory are faithfulness constraints  , which are violated by under parsing and over parsing  . The faithfulness constraints ensure that a grammar will only tolerate deviations of the surface form from the input form which are necessary to satisfy structural constraints dominating the faithfulness constraints  . 
Computing an optimal description means considering a space of candidate descriptions that include structures with a variety of faithfulness violations  , and evaluating those candidates with respect o a ranking in which structural and faithfulness constraints may be interleaved  . This is parsing in the generic sense : a structural description is being as-what is traditionally thought of as parsing in com -putation M linguistics  . Traditional parsing attempts to construct a grammatical description with a surface form matching the given input string exactly  ; if a description cannot be fit exactly , the input string is rejected as ungrammatical . Traditional parsing can be thought of as enforcing faithfulness absolutely  , with no faithfulness violations are allowed . Partly for this reason , traditional parsing is usually understood as mapping a surface form to a description  . In the computation of optimal descriptions considered here  , a candidate that is fully faithful to the input may be tossed aside by the grammar in favor of a less faithful description better satisfying other  ( dominant ) constraints . Computing an optimal description in Optimality Theory is more naturally thought of as mapping an underlying form to a description  , perhaps as part of the process of language production  . 
Tesar ( Tesar , 1994) ( Tesar , 1995a ) has developed algorithms for computing optimal descriptions  , based upon dynamic programming . The details laid out in ( Tesar , 1995a ) focused on the case where the set of structures underlying the Genfunction are formally regular  . In this paper , Tesar's basic approach is adopted , and extended to grammars with a Genfunction employing fully contextfree structures  . Using such contextfree structures introduces some complications not apparent with the regular case  . This paper demonstrates that the complications can be dealt with  , and that the dynamic programming case may be fully extended to grammars with contextfree structures  . 
2 Context-Free Position Structure

Tesar ( Tesar , 1995a ) formalizes Gen as a set of matchings between an ordered string of input segments and the terminals of each of a set of position structures  . The set of possible position structures is defined by a formal grammar  , the position structure grammar . A position structure has as terminals structural positions  . In a valid structural description , each structural position may be filled with at most one input segment  , and each input segment may be parsed into at most one position  . The linear order of the input must be preserved in all candidate structural descriptions  . 
This paper considers Optimality Theory grammars where the position structure grammar is contextfree  ; that is , the space of position structures can be described by a formal contextfree grammar  . 
As an illustration , consider the grammar in Examples 1 and 2 ( this illustration is not intended to represent any plausible natural language theory  , but does use the " peak/margin " terminology sometimes employed in syllable theories  )  . The set of inputs is C,V + . The candidate descriptions of an input consist of a sequence of pieces  , each of which has a peak ( p ) surrounded by one or more pairs of margin positions  ( m )  . These structures exhibit prototypical contextfree behavior  , in that margin positions to the left of a peak are balanced with margin positions to the right  . ' e ' is the empty string , and ' S ' the start symbol . 
Example 1 The Position Structure Grammar
S := ~ Fie
F = ~ Y IY F
Y ~ PIMFM
M : : ~ m
P = : ~ p
Example 2 The Constraints - ( m/V ) Do not parse V into a margin position - ( p/C ) Do not parse C into a peak position
PARSE Input segments must be parsed
FILL mA margin position must be filled
FILL pA peak position must be filled
The first two constraints a restructur M , and mandate that V not be parsed into a margin position  , and that C not be parsed into a peak position . The other three constraints are faithfulness constraints  . 
The two structural constraints are satisfied by descriptions with each V in a peak position surrounded by matched C's in margin positions : CC VCC  , V , CV CC CV CC , etc . If the input string permits such an analysis , it will be given this completely faithful description  , with no resulting constraint violations ( ensuring that it will be optimal with respect o any ranking  )  . 
Consider the constraint hierarchy in Example 3.
Example 3 A Constraint Hierarchy-(m/V ) , -( p/C ) , PARSE ~> FILLp > FILLm This ranking ensures that in optimal descriptions  , a V will only be parsed as a peak , while a C will only be parsed as a margin . Further , all input segments will be parsed , and unfilled positions will be included only as necessary to produce a sequence of balanced structures  . For example , the input/VC/receives the description 1 shown in Example 4  . 
Example 4The Optimal Description for/VC/

The surface string for this description is CVC : the first C was " epen the sized " to balance with the one following the peak V  . This candidate is optimal because it only violates FILL m  , the lowest-ranked constraint . 
Tesar identifies locality as a sufficient condition on the universal constraints for the success of hisl In this paper  , tree structures will be denoted with parentheses : a parent node X with child nodes Y and Z is denoted X  ( Y , Z) . 
102 approach . For formally regular position structure grammars , he defines a local constraint as one which can be evaluated strictly on the basis of two consecutive positions  ( and any input segments filling those positions ) in the linear position structure . That idea can be extended to the contextfree case as follows  . 
A local constraint is one which can be evaluated strictly on the basis of the information contained within a local region  . A local region of a description is either of the following : ? a  non4erminal nd the child nonterminals that it immediately dominates  ; ? a nonterminal which dominates a terminal symbol  ( position )  , along with the terminal and the input segment ( if present ) filling the terminal position . 
It is important okeep clear the role of the position structure grammar  . It does not define the set of grammatical structures  , it defines the Space of candidate structures . Thus , the computation of descrip-tions addressed in this paper should be distinguished from robust  , or error-correcting , parsing ( Anderson and Backhouse ,  1981 , for example ) . There , the input string is mapped to the grammatical structure that is ' closest '  ; if the input completely matches a structure generated by the grammar  , that structure is automatically selected . In the OT case presented here , the full grammar is the entire OT system , of which the position structure grammar is only a part  . 
Error-correcting parsing uses optimization only with respecto the faithfulness of predefined grammatical structures to the input  . OT uses optimization to define grammaticality . 
3 The Dynamic Programming Table
The Dynamic Programming ( DP ) Table is here a three-dimensional , pyramid-shaped data structure . 
It resembles the tables used for contextfree chart parsing  ( Kay ,  1980 ) and maximum likelihood computation for stochastic on text-free grammars  ( Lari and Young , 1990) ( Charniak ,  1993) . Each cell of the table contains a partial description  ( a part of a structural description )  , and the Harmony of that partial description . A partial description is much like an edge in chart parsing  , covering a contiguous substring of the input . A cell is identified by three indices , and denoted with square brackets ( e . g . ,\[ X , a , c\]) . The first index identifying the cell ( X ) indicates the cell category of the cell . The other two indices ( a and c ) indicate the contiguou substring of the input string covered by the partial description contained in the cell  ( input segments i a through ic )  . 
In chart parsing , the set of cell categories i precisely the set of nonterminals in the grammar  , and thus a cell contains a subtree with a root nonterminal corresponding to the cell category  , and with leaves that constitute precisely the input substring covered by the cell  . In the algorithm presented here , the set of cell categories are the nonterminals of the position structure grammar  , along with a category for each left-aligned substring of the right hand side of each position gramma rule  . Example 5 gives the set of cell categories for the position structure grammar in Example  1  . 
Example 5 The Set of Cell Categories
S , F , Y , M , P , MF
The last category in Example 5 , MF , comes from the rule Y = : ~ MFM of Example 1 , which has more than two nonterminals on the right hand side  . Each such category corresponds to an incomplete dgein normal chart parsing  ; having a table cell for each such category eliminates the need for a separate data structure containing edges  . The cell\[MF , a , c \] may contain an ordered pair of subtrees , the first with root M covering input \ [ a , b \] , and the second with root F covering input \[ b+l , c\] . 
The DP Table is perhaps best envisioned as a set of layers  , one for each category . A layer is a set of all cells in the table indexed by a particular cell category  . 
Example 6 A Layer of the Dynamic Programming
Table for Category M ( input i 1"i3)\[U , l , 3\]\[M , 1 , 2\]\[M , 2 , 3\]\[M , I , 1\]\[M , 2 , 2\]\[M , 3 , 3\] I ili2i 3For each substring length , there is a collection of rows , one for each category , which will collectively be referred to as a level  . The first level contains the cells which only cover one input segment  ; the number of cells in this level will he the number of input segments multiplied by the number of cell categories  . 
Level two contains cells which cover input substrings of length two  , and so on . The top level contains one cell for each category  . One other useful partition of the DP table is into blocks  . A block is a set of all cells covering a particular input subsequence  . A block has one cell for each cell category . 
A cell of the DP Table is filled by comparing the results of several operations  , each of which try to fill a cell . The operation producing the partial description with the highest Harmony actually fills the cell  . 
The operations themselves are discussed in Section  4  . 
The algorithm presented in Section 6 fills the table cells level by level : first , all the cells covering only one input segment are filled  , then the cells covering two consecutive segments are filled  , and so forth . When the table has been completely filled , cell\[S , 1 , J \] will contain the optimal description of the input  , and its Harmony . The table may also be filled in a more left-to -right manner  , bottom-up , in the spirit of CKY . First , the cells covering only segment il , and then i2 , are filled . Then , the cells entries in the cells covering each of il and is  . The cells of the next diagonal are then filled . 
4 The Operations Set
The Operations Set contains the operations used to fill DP Table cells  . The algorithm proceeds by considering all of the operations that could be used to fill a cell  , and selecting the one generating the partial description with the highest Harmony to actually fill the cell  . There are three main types of operations , corresponding to under parsing , parsing , and over parsing actions . These actions are analogous to the three primitive actions of sequence comparison  ( Sankoff and Kruskal , 1983): deletion , correspondence , and insertion . 
The discussion that follows makes the assumption that the right hand side of every production is either a string of nonterminals or a single terminal  . Each parsing operation generates a new element of structure  , and so is associated with a position structure grammar production  . The first type of parsing operation involves productions which generate a single terminal  ( e . g . , P := ~ p) . Because we are assuming that an input segment may only be parsed into at most one position  , and that a position may have at most one input segment parsed into it  , this parsing operation may only fill a cell which covers exactly one input segment  , in our example , cell\[P , I , 1\] could be filled by an operation parsing il into a p position  , giving the partial description P ( p filled with il )  . 
The other kinds of parsing operations are matched to position grammar productions in which a parent nonterminal generates child nonterminals  . One of these kinds of operations fills the cell for a category by combining cell entries for two factor categories  , in order , so that the substrings covered by each of them combine  ( concatenatively , with no overlap ) to form the input substring covered by the cell being filled  . For rule Y = ~ MFM , there will be an operation of this type combining entries in \[ M  , a , b \] and\[F , b+l , c \] , creating the concatenated structures \[ M , a , b\]+\[F , b+l , c \] , to fill \[ MF , a , c \] . The final type of parsing operation fills a cell for a category which is a single nonterminal on the left hand side of a production  , by combining two entries which jointly form the entire right hand side of the production  . This operation would combining entries in \[ MF , a , c \] and\[M , c?l , d\] , creating the structure Y(\[MF , a , c\] , \[M , c+l , d\]) , to fill \[ Y , a , d\] . Each of these operations involves filling a cell for a target category by using the entries in the cells for two factor categories  . 
The resulting Harmony of the partial description created by a parsing operation will be the  combina-2This partial description is not a single tree , but an ordered pair of trees . In general , such concatenated structures will be ordered lists of trees  . 
tion of the marks assessed each of the partial descriptions for the factor categories  , plus any additional marks incurred as a result of the structure added by the production itself  . This is true because the constraints must be local : any new constraint violations are determinable on the basis of the cell category of the factor partial descriptions  , and not any other internal details of those partial descriptions  . 
All possible ways in which the factor categories , taken in order , may combine to cover the substring , must be considered . Because the factor categories must be contiguous and in order  , this amounts to considering each of the ways in which the substring can be split into two pieces  . This is reflected in the parsing operation descriptions given in Section  6  . 2 . 
Under parsing operations are not matched with position grammar productions  . ADP Table cell which covers only one input segment may be filled by an under parsing operation which marks the input segment as under parsed  . In general , any partial description covering any substring of the input may be extended to cover an adjacent input segment by adding that additional segment marked as under-parsed  . Thus , a cell covering a given substring of length greater than one may be filled in two mirror -image ways via under parsing : by taking a partial description which covers all but the leftmost input segment and adding that segment as under parsed  , and by taking a partial description which covers all but the rightmost input segment and adding that segment as under parsed  . 
Over parsing operations are discussed in Section 5.
5 The Overparsing Operations
Over parsing operations consume no input ; they only add new unfilled structure . Thus , a block of cells ( the set of cells each covering the same input substring  ) is interdependent with respect o over parsing operations  , meaning that an over parsing operation trying to fill one cell in the block is adding structure to a partial description from a different cell in the same block  . The first consequence of this is that the over parsing operations must be considered after the under parsing and parsing operations for that block  . 
Otherwise , the cells would be empty , and the over-parsing operations would have nothing to add onto  . 
The second consequence is that over parsing operations may need to be considered more than once  , because the result of one over parsing operation ( if it fill sacell ) could be the source for another over pars-ing operation  . Thus , more than one pass through the over parsing operations for a block may be necessary  . 
In the description of the algorithm given in Section  6  . 3 , each Repeat-Until loop considers the over pars-ing operations for a block of cells  . The number of loop iterations is the number of passes through the overparsing operations for the block  . The loop iterations stop when none of the over parsing operations is less harmonic than the partial description already in the cell  )  . 
In principle , an unbounded number of over pars-ing operations could apply  , and in fact descriptions with arbitrary numbers of unfilled positions are contained in the output space of Gen  ( as formally defined )  . The algorithm does not have to explicitly consider arbitrary amounts of over parsing  , however . 
A necessary property of the faithfulness constraints  , given constraint locality , is that a partial description cannot have over parsed structures repeatedly added to it until the resulting partial description falls into the same cell category as the original prior to over-parsing  , and be more Harmonic . Such a sequence of over parsing operations can be considered a over pars-ing cycle  . Thus , the faithfulness constraints must ban over parsing cycles  . This is not solely a computational requirement , but is necessary for the grammar to be well -defined : over parsing cycles must be har -monically suboptimal  , otherwise arbitrary amounts of over parsing will be permitted in optimal descriptions  . In particular , the constraint should prevent over parsing from adding an entire over parsed nonterminal more than once to the same partial description while passing through the over parsing operations  . In Example 2 , the constraints FILLm and FILL peffectively ban over parsing cycles : no matter where these constraints are ranked  , a description containing an over parsing cycle will be less harmonic  ( due to additional FILL violations ) than the same description with the cycle removed . 
Given that the universal constraints meet this criterion  , the overparsing operations may be repeatedly considered for a given level until none of them increase the Harmony of the entries in any of the cells  . 
Because each over parsing operation maps a partial description in one cell category to one for another cell category  , a partial description cannot undergo more consecutive over parsing operations than there are cell categories without repeating at least one cell category  , thereby creating a cycle . Thus , the number of cell categories places a constant bound on the number of passes made through the over parsing operations for a block  . 
A single nonterminal may dominate an entire subtree in which none of the syllable positions at the leaves of the tree are filled  . Thus , the optimal " unfilled structure " for each nonterminal  , and in fact each cell category , must be determined , for use by the overparsing operations . The optimal over-parsing structure for category X is denoted with IX  , 0\] , and such an entity is referred to as a base over parsing structure  . A set of such structures must be computed , one for each category , before filling input-dependent DP table cells . Because these values are not dependent upon the input  , base over pars-ing structures may be computed and stored in advance  . Computing them is just like computing other cell entries  , except that only over parsing operations are considered  . First , consider ( once ) the overpars-ing operations for each nonterminal X which has a production rule permitting it to dominate a terminal x : each tries to set IX  , 0\] to contain the corresponding partial description with the terminal x left unfilled  . 
Next consider the other over parsing operations for each cell  , choosing the most Harmonic of those operations ' partial descriptions and the prior value of 

6 The Dynamic Programming
Algorithm 6 . 1 Notat ion maxH  returns the argument with maximum Harmony  ( i  ~ ) denotes input segment i ~ under parsed
X t is a nonterminal x t is a terminal + denotes concatenation  6  . 2 The Operations Underparsing Operations for \[ X t  , a , a \]: create(i ~/+\[ X* , 0\] Under parsing Operations for IX t , a , c\]:create(ia ) +\[ X ~ , a+l , c \] create\[Xt , a , e-1\]+(ia)
Parsing operations for\[Xt , a , a \]: for each production X t : : ~ x k create X t ( xk filled with i a ) 
Parsing operations for \[ X * , a , c \] , where c > a and all X are cell categories : for each production X t = ~ X k X m for b = a + l to  c1 create X * ( \[Xk , a , b\] , \[X'~ , b+1c\]) for each production Xu := ~ X/:xmxn . . . 
where X t = X k X ' ~ ~ : for b = a + l to c1 create\[X k , a , b\]+\[X'~ , b+l , c\]
Over parsing operations for\[Xt , 0\]: for each production X t = ~ x k create X t ( xkunfilled ) for each production X t = ~ X k X m creatext ( \[Xk , 0\] , \[Xm , 0\]) for each production X~~X kXmXn . . . 
where Xt--xkxm : create\[Xk , 0\]+\[Xm , 0\] Over parsing operations for\[Xt , a , a \]: same as for \[ X* , a , c\]Over parsing operations for\[Xt , a , c \]: for each production X t ~ X k create X t ( \[Xk , a , c \]) createXt(\[Xk , 0\] , \[X'~ , a , c \]) create X ~(\ [ X k , a , c\] , \[X'~ , 0\]) for each production Xu := ~ XkXmX ~ . . . 
where X t = X k X ' ~ ~ : create\[X k , a , c\]+\[Xm , 0\] create\[Xk , 0\]+\[Xm , a , c \] 6 . 3 The Main A lgor i thm /* create the base over parsing structures */ 

For each X t , Set\[Xt , 0\] to maxH\[Xt , 0\] , over parsing ops for\[Xt , 0\] Until no IX t , 0\] has changed uring a pass /* fill the cells covering only a single segment */ 
For a = 1 to J
For each X t , Set\[Xt , a , a \] to maxHunder parsing ops for\[Xt , a , a\]
For each X t , Set\[Xt , a , a \] to maxH\[Xt , a , a \] , parsing ops for\[Xt , a , a\]

For each X t , Set\[Xt , a , a \] to maxH\[Xt , a , a \] , over parsing ops for\[Xt , a , a \] Until no\[Xt , a , a \] has change during a pass /* fill the rest of the cells */ 
For d = lto(J-l)
For a = lto(J-d )
For each X t , Set\[Xt , a , a + d\]to maxHunder parsing ops for\[Xt , a , a+d\]
For each X ~ , Set\[Xt , a , a + d\]maxH\[Xt , a , a+d\] , parsing ops for\[Xt , a , a+d\]

For each X t ,
Set\[Xt , a , a+d\]tomaxH\[Xt , a , a+d\] , over parsing ops for\[Xt , a , a+d\]Untilno\[Xt , a , a + d\]has change during a pass Return \[ S , 1 , J \] as the optimal description 6 . 4 Complex i ty Each block of cells for an input subsequence is processed in time linear in the length of the subsequence  . This is a consequence of the fact that in general parsing operations filling such a cell must consider all ways of dividing the input subsequence into two pieces  . The number of over parsing passes through the block is bounded from above by the number of cell categories  , due to the fact that over-parsing cycles are suboptimal  . Thus , the number of passes is bounded by a constant , for any fixed position structure grammar . The number of such blocks is the number of distinct  , contiguous input subsequences ( equivalently , the number of cells in a layer ) , which is on the order of the square of the length of the input  . If N is the length of the input , the algorithm has computational complexity O(N3) . 
7 Discussion 7.1 Locality
That locality helps processing should he no great surprise to computation a lists  ; the computational significance of locality is widely appreciated  . Further , locality is often considered a desirable property of principles in linguistics  , independent of computational concerns . Nevertheless , locality is a sufficient but not necessary restriction for the applicability of this algorithm  . The locality restriction is really a special case of a more general sufficient condition  . 
The general condition is a kind of " Markov " property  . This property requires that , for any substring of the input for which partial descriptions are constructed  , the set of possible partial descriptions for that substring may be partitioned into a finite set of classes  , such that the consequences in terms of constraint violations for the addition of structure to a partial description may he determined entirely by the identity of the class to which that partial description belongs  . The special case of strict locality is easy to understand with respect to contextfree structures  , because it states that the only information needed about a subtree to relate it to the rest of the tree is the identity of the root nonterminal  , so that the ( necessarily finite ) set of nonterminals provides the relevant set of classes  . 
7.2 Under parsing and Derivational

The treatment of the under parsing operations given above creates the opportunity for the same partial description to be arrived at through several different paths  . For example , suppose the input is i a .   .   . ibicid .   .   . i e , and there is a constituent in \[ X , a , b \] and a constituent\[Y , d , e\] . Further suppose the input segmentic is to be marked under parsed  , so that the final description \[ S , a , e\]contains\[X , a , b \]( i ~)\ [ Y , d , e\] . 
That description could be arrived at either by combining\[X  , a , b \] and ( ic ) to fill \[ X , a , c \] , and then combine \[ X , a , c \] and\[Y , d , e\] , or it could be arrived at by combining ( i  ~ ) and \[ Y , d , e \] to fill \[ Y , c , e\] , and then combine \[ X , a , b \] and\[Y , c , e\] . The potential confusion stems from the fact that an underparsed segment is part of the description  , but is not a proper constituent of the tree . 
This problem can be avoided in several ways . An obvious one is to only permit under parsings to be added to partial descriptions on the right side  . One exception would then have to be made to permit input segments prior to any parsed input segments to be underparsed  ( i . e . , if the first input segment is un-derparsed , it has to be attached to the left side of some constituent because it is to the left of everything in the description  )  . 
1068 Conclusions
The results presented here demonstrate that the basic cubic time complexity results for processing contextfree structures are preserved when Optimality Theory grammars are used  . If Gencan be specified as matching input segments to structures generated by a contextfree position structure grammar  , and the constraints are local with respect o those structures  , then the algorithm presented here may be applied directly to compute optimal descriptions  . 
9 Acknowledgments
I would like to thank Paul Smolensky for his valuable contributions and support  . I would also like to thank David I-I aussler , Clayton Lewis , Mark Liberman , Jim Martin , and Alan Prince for useful discussions , and three anonymous reviewers for helpful comments  . This work was supported in part by an NSF Graduate Fellowship to the author  , and NSF grant IRI-9213894 to Paul Smolensky and Geraldine

Bruce Tesar .  1994 . Parsing in Optimality Theory : A dynamic programming approach  . Technical Report CU-CS-714-94, April 1994 . Department of Computer Science , University of Colorado , Boulder . 
Bruce Tesar . 1995a . Computing optimal forms in Optimality Theory : Basic syllabification  . Technical Report CU-CS-763-95, February 1995 . Department of Computer Science , University of Colorado , Boulder . 
Bruce Tesar . 1995b . Computational Optimality Theory . Unpublished Ph . D . Dissertation . Department of Computer Science , University of Colorado , 
Boulder . June 1995.
A . J . Viterbi .  1967 . Error bounds for convolution codes and an asymptotically optimal decoding algorithm  . IEEE Trans . on Information Theory 13:260-269 . 
References
S . O . Anderson and R . C . Backhouse .  1981 . Locally least-cost error recovery in Earley's algorithm  . ACM Transactions on Programming Languages and Systems  3:   318-347  . 
Eugene Charniak .  1993 . Statistical anguage learning . Cambridge , MA : MIT Press . 
Martin Kay .  1980 . Algorithmic schemat and data structures in syntactic processing  . CSL-80-12, October 1980 . 
K . Lari and S . J . Young .  1990 . The estimation of stochastic contextfree grammars using the insideoutside algorithm  . Computer Speech and Language 4:3536 . 
Harry R . Lewis and Christos H . Papadimitriou.
1981 . Elements of the theory of computation . Englewood Cliffs , New Jersey : Prentice-Hall , Inc . 
Alan Prince and Paul Smolensky .  1993 . Optimality Theory : Constraint interaction in generative grammar  . Technical Report CU-CS-696-93 , Department of Computer Science , University of Coloradoat Boulder , and Technical Report TR2 , Rutgers Center for Cognitive Science , Rutgers University , New Brunswick , NJ . March . To appear in the Linguistic Inquiry Monograph Series  , 
Cambridge , MA : MIT Press.
David Sankoff and Joseph Kruskal .  1983 . Timewarps , string edits , and macromolecules : The theory and practice of sequence comparison  . Reading,
MA : Addison-Wesley.

