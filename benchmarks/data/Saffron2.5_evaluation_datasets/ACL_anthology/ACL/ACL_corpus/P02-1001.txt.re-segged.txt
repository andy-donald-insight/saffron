Parameter Estimation for Probabilistic Finite State Transducers ? 
Jason Eisner
Department of Computer Science
Johns Hopkins University
Baltimore , MD , USA 21218-2691


Weighted finite-state transducers suffer from the lack of a training algorithm  . Training is even harder for transducers that have been assembled via finite-state operations such as composition  , minimization , union , concatenation , and closure , as this yield stricky parameter tying . We formulate a ? parameterized FST ? paradigm and give training algorithms for it  , including a general bookkeeping trick ( ? expectation semirings ? ) that cleanly and efficiently computes expectations and gradients  . 
1 Background and Motivation
Rational relations on strings have become widespread in language and speech engineering  ( Roche and Schabes ,  1997) . Despite bounded memory they are well-suited to describe many linguistic and textual processes  , either exactly or approximately . 
A relation is a set of ( input , output ) pairs . Relations are more general than functions because they may pair a given input string with more or fewer than one output string  . 
The class of socalled rational relations admits a nice declarative programming paradigm  . Source code describing the relation ( a regular expression ) is compiled into efficient object code ( in the form of a 2-tape automaton called a finite-state transducer )  . The object code can even be optimized for runtime and codesize  ( via algorithms such as determinization and minimization of transducers  )  . 
This programming paradigm supports efficient nondeterminism  , including parallel processing over infinite sets of input strings  , and even allows ? reverse ? computation from output to input  . Its unusual flexibility for the practiced programmer stems from the many operations under which rational relations are closed  . It is common to define further useful operations ( as macros )  , which modify existing relations not by editing their source code but simply by operating on them ? from outside  . ? ? A brief version of this work , with some additional material , first appeared as ( Eisner , 2001a ) . Alei surely journal-length version with more details has been prepared and is available  . 
The entire paradigm has been generalized to weighted relations  , which assign a weight to each ( input , output ) pair rather than simply including or excluding it  . If these weights represent probabilities P ( input , output ) or P ( output input ) , the weighted relation is called a joint or conditional  ( probabilistic ) relation and constitutes a statistical model . Such models can be efficiently restricted , manipulated or combined using rational operations as before  . An artificial example will appear in ?2 . 
The availability of toolkits for this weighted case  ( Mohri et al , 1998; van Noord and Gerdemann , 2001) promises to unify much of statistical NLP . 
Such tools make it easy to run most current approaches to statistical markup  , chunking , normalization , segmentation , alignment , and noisy-channel decoding , 1 including classic models for speech recognition ( Pereira and Riley ,  1997 ) and machine translation ( Knight and Al-Onaizan ,  1998) . Moreover , once the models are expressed in the finite-state framework  , it is easy to use operators to tweak them , to apply them to speech lattices or other sets , and to combine them with linguistic resources . 
Unfortunately , there is a stumbling block : Where do the weights come from ? After all  , statistical models require supervised or unsupervised training  . Currently , finite-state practitioners derive weights using exogenous training methods  , then patch them onto transducer arcs . Not only do these methods require additional programming outside the toolkit  , but they are limited to particular kinds of models and training regimens  . For example , the forwardbackward algorithm ( Baum , 1972) trains only Hidden Markov Models , while ( Ristad and Yianilos , 1996) trains only stochastic edit distance . 
In short , current finite-state toolkits include no training algorithms  , because none exist for the large space of statistical models that the toolkits can in principle describe and run  . 
1 Given output , find input to maximize P ( input , output ) . 
Computational Linguistics ( ACL ) , Philadelphia , July 2002 , pp .  18 . 
Proceedings of the 40th Annual Meeting of the Association for ( a )   ( b )  0/ . 15 a:x/ . 63 1/ . 15 a :/ . 07? 2/ . 5 b:/ . 003 ? b : z / . 12 3/ . 5b : x/ . 027 a :/ . 7 ? b :/ . 03 ? b : z / . 12 b :/ . 1 ? b : z / . 4 b:/ . 01 ? b : z / . 4b:x/ . 09 4/ . 15 a:p/ . 7 5/ . 5b:p/ . 03b:q/ . 12b : p/ . 1b:q/ . 4(c ) 6/1 p : x/ . 97/1p:/ . 1 ? q:z/1 p:/1 ? q:z/1 Figure 1: ( a ) A probabilistic FST defining a joint probability distribution  . ( b ) A smaller joint distribution . (c ) A conditional distribution . Defining ( a ) = ( b ) ? ( c ) means that the weights in ( a ) can be altered by adjusting the fewer weights in ( b ) and ( c )  . 
This paper aims to provide a remedy through a new paradigm  , which we call parameterized finite-state machines  . It lays out a fully general approach for training the weights of weighted rational relations  . First ?2 considers how to parameterize such models , so that weights are defined in terms of underlying parameters to be learned  .   ?3 asks what it means to learn these parameters from training data  ( what is to be optimized ? )  , and notes the apparently formidable bookkeeping involved  .   ?4 cuts through the difficulty with a surprisingly simple trick  . Finally ,   ?5 removes inefficiencies from the basic algorithm , making it suitable for inclusion in an actual toolkit  . Such a toolkit could greatly shorten the development cycle in natural language engineering  . 
2 Transducers and Parameters
Finite state machines , including finite-state automata ( FSAs ) and transducers ( FSTs )  , are a kind of labeled directed multigraph . For ease and brevity , we explain them by example . Fig .   1a shows a probabilistic FST with input alphabet ? = a  , b , output alphabet ? = x , z , and all states final . It may be regarded as a device for generating a string pair in ????? by a random walk from  0?  . Two paths exist that generate both input a abb and output x z :  0?a:x/  . 63 ??0 ? a : / . 07? ?1 ? b : / . 03 ? ?2 ? b : z / . 4 ?? 2/ . 5 ?0 ? a:x/ . 63 ??0 ? a : / . 07 ? ? 1 ? b : z / . 12 ? ?2 ? b : / . 1 ?? 2/ . 5 ? Each of the paths has probability . 0002646 , so the probability of somehow generating the pair  ( a abb , xz ) is . 0002646 +  . 0002646 =  . 0005292 . 
Abstracting away from the idea of random walks , arc weights need not be probabilities . Still , define a path?s weight as the product of its arc weights and the stopping weight of its final state  . Thus Fig . 1 a defines a weighted relation f where f(a abb , x z ) = . 0005292 . This particular relation does happen to be probabilistic  ( see ?1 )  . It represents a joint distribution ( since ? x , y f(x , y ) = 1) . Meanwhile , Fig . 1c defines a condition alone (? x?y f(x , y ) = 1) . 
This paper explains how to adjust probability distributions like that of Fig  . 1 a so as to model training data better . The algorithm improves an FST?s numeric weights while leaving its topology fixed  . 
How many parameters are there to adjust in Fig .   1a ? That is up to the user who built it ! An FST model with few parameters is more constrained  , making optimization easier . Some possibilities : ? Most simply , the algorithm can be asked to tune the 17 numbers in Fig . 1 a separately , subject to the constraint that the paths retain total probability  1  . A more specific version of the constraint requires the FST to remain Markovian : each of the  4 states must present options with total probability  1   ( at state 1? ,  15+ . 7+ . 03 . + . 12=1) . This preserves the random walk interpretation and ( we will show ) entails no loss of generality . The 4 restrictions leave 13 free params . 
? But perhaps Fig .   1a was actually obtained as the composition of Fig . 1b ? c , effectively defining P ( input , output ) = ? midP ( input , mid ) ? P ( output mid ) . If Fig . 1b?c are required to remain Markovian , they have 5 and 1 degrees of freedom respectively , so now Fig . 1 a has only 6 parameters total . 2 In general , composing machines multiplies their arc counts but only adds their parameter counts  . We wish to optimize just the few underlying parameters  , not independently optimize the many arc weights of the composed machine  . 
? Perhaps Fig .   1b was itself obtained by the probabilistic regular expression  ( a:p ) ?? ( b : ( p + ? q ) )?? with the 3 parameters ( ? ,  ? ,  ?) = ( . 7,  . 2,  . 5) . With ? = . 1 from footnote 2 , the composed machine 2Why does Fig .   1c have only 1 degree of freedom ? The Markovian requirement means something different in Fig  . 1c , which defines a conditional relation P ( output mid ) rather than a joint one . A random walk on Fig . 1c chooses among arcs with a given input label . So the arcs from state 6? within put p must have total probability 1   ( currently . 9+ . 1) . All other arc choices are forced by the input label and so have probability  1  . 
The only tunable value is . 1(denote it by ?), with . 9 = 1? ? . 
( Fig . 1a ) has now been described with a total of just 4   parameters!3 Here , probabilistic union E + ? F def = ? E + ( 1 ? ? ) Fmeans ? flip a ?- weighted coin and generate E if heads  , Fiftails . ? E??def = ( ? E ) ? ( 1?? ) means ? repeatedly flip an ?- weighted coin and keep repeating E as long as it comes upheads  . ? These 4 parameters have global effects on Fig . 1a , thanks to complex parameter tying : arcs 4? b : p ? ? 5?  , 5 ? b : q ? ?5 ? in Fig . 1b get respective probabilities ( 1? ? ) ? ? and ( 1 ? ? ) ? , which covary with ? and vary oppositely with ? . Each of these probabilities in turn affects multiple arcs in the composed FST of Fig  . 1a . 
We offer a theorem that highlights the broad applicability of these modeling techniques  . 4 Iff ( input , output ) is a weighted regular relation , then the following statements are equivalent : ( 1 ) f is a joint probabilistic relation ;   ( 2 ) f can be computed by a Markovian FST that halts with probability  1  ; (3) f can be expressed as a probabilistic regexp , i . e . , a regexp built up from atomic expressions a : b ( for a ???  , b ??? ) using concatenation , probabilistic union + p , and probabilistic closure ? p . 
For defining conditional relations , a good regexplanguage is unknown to us , but they can be defined in several other ways : ( 1 ) via FSTs as in Fig . 1c ,   ( 2 ) by compilation of weighted rewrite rules ( Mohri and Sproat ,  1996) ,   ( 3 ) by compilation of decision trees ( Sproat and Riley ,  1996) ,   ( 4 ) as a relation that performs contextual left-to -right replacement of input substrings by a smaller conditional relation  ( Gerdemann and van Noord ,  1999) , 5  ( 5 ) by conditionaliza-tion of a joint relation as discussed below  . 
A central technique is to define a joint relation as a noisy-channel model  , by composing a joint relation with a cascade of one or more conditional relations as in Fig  . 1 ( Pereira and Riley , 1997; Knight and Graehl ,  1998) . The general form is illustrated by 3Conceptually , the parameters represent the probabilities of reading another a  ( ? )  ; reading another b(?) ; transducing b top rather than q(?) ; starting to transduce p to  rather than x (?) . 
4 To prove (1) ? (3) , express f as an FST and apply the wellknown Kleene-Schu?tzenberger construction  ( Berstel and Reutenauer ,  1988) , taking care to write each regexp in the construction as a constant times a probabilistic regexp  . A full proof is straightforward , as are proofs of (3) ? (2) ,  (2)?(1) . 
5In (4) , the randomness is in the smaller relation?s choice of how to replace a match  . One can also get randomness through the choice of matches  , ignoring match possibilities by randomly deleting markers in Gerdemann and van Noord?s construction  . 
P ( v , z ) def = ? w , x , yP ( vw)P ( w , x)P ( yx)P ( zy ) , implemented by composing 4 machines . 6 , 7 There are also procedures for defining weighted FSTs that are not probabilistic  ( Berstel and Reutenauer ,  1988) . Arbitrary weights such as 2 . 7 may be assigned to arcs or sprinkled through a reg-exp  ( to be compiled into : /2 . 7? ? arcs ) . A more subtle example is weighted FSAs that approximate PCFGs  ( Nederhof , 2000; Mohri and Nederhof ,  2001) , or to extend the idea , weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation  . These are parameterized by the PCFG?s parameters , but add or remove strings of the PCFG to leave an improper probability distribution  . 
Fortunately for those techniques , an FST with positive arc weights can be normalized to make it jointly or conditionally probabilistic : ? An easy approach is to normalize the options at each state to make the FST Markovian  . Unfortunately , the result may differ for equivalent FSTs that express the same weighted relation  . Undesirable consequences of this fact have been termed ? label bias ?  ( Lafferty et al ,  2001) . Also , in the conditional case such per-state normalization is only correct if all states accept all input suffixes  ( since ? dead ends ? leak probability mass )  . 8 ? A better-founded approach is global normalization  , which simply divides each f(x , y ) by ? x ? , y ? f(x ? , y ?) ( joint case ) or by ? y ? f(x , y ?) ( conditional case ) . To implement the joint case , just divide stopping weights by the total weight of all paths  ( which ?4 shows how to find )  , provided this is finite . 
In the conditional case , let g be a copy off with the output labels removed  , so that g(x ) finds the desired divisor ; determinize g if possible ( but this fails for some weighted FSAs )  , replace all weights with their reciprocals , and compose the result with f . 96P ( w , x ) defines the source model , and is often an ? identity FST ? that requires w = x  , really just an FSA . 
7We propose also using n-tape automata to generalize to ? branching noisy channels ?  ( a case of dendroid distributions )  . 
In ? w , xP(vw)P ( v?w)P ( w , x)P ( yx ) , the true transcription w can be triply constrained by observing speechy and two errorful transcriptions v  , v ? , which independently depend on w . 
8 A corresponding problem exists in the joint case , but may be easily avoided thereby first pruning non-coaccessible states  . 
9It suffices to make gun ambiguous ( one accepting path per string )  , a weaker condition than determinism . When this is not possible ( as in the inverse of Fig . 1b , whose conditionaliza-Normalization is particularly important because it enables the use of loglinear  ( maximum-entropy ) parameterizations . Here one defines each arc weight , coinweight , or regexpweight in terms of meaningful features associated by hand with that arc  , coin , etc . Each feature has a strength ? R > 0 , and a weight is computed as the product of the strengths of its features  . 1 0 It is now the strengths that are the learnable parameters  . This allows meaningful parameter tying : if certain arcs such as u : i ? ?  , o : e ? ? , and a : ae ?? share a contextual ? vowel-fronting ? feature  , then their weights rise and fall together with the strength of that feature  . The resulting machine must be normalized , either per-state or globally , to obtain a joint or a conditional distribution as desired  . Such approaches have been tried recently in restricted cases  ( McCallum et al , 2000; Eisner , 2001b ; Lafferty et al ,  2001) . 
Normalization may be postponed and applied instead to the result of combining the FST with other FSTs by composition  , union , concatenation , etc . A simple example is a probabilistic FSA defined by normalizing the intersection of other probabilistic FSAs  f1  , f2 ,   .   .   . .  ( This is in fact a loglinear model in which the component FSAs define the features : string x has log fi  ( x ) occurrences of feature i . ) In short , weighted finite-state operators provide a language for specifying a wide variety of parameterized statistical models  . Let us turn to their training . 
3 Estimation in Parameterized FSTs
We are primarily concerned with the following training paradigm  , novel in its generality . Let f ? : ? ? ? ? ? ? R?0 be a joint probabilistic relation that is computed by a weighted FST  . The FST was built by some recipe that used the parameter vector ?  . 
Changing ? may require us to rebuild the FST to getup dated weights  ; this can involve composition , reg-exp compilation , multiplication of feature strengths , etc .   ( Lazy algorithms that compute arcs and states of tion cannot be realized by any weighted FST  )  , one can sometimes succeed by first intersecting g with a smaller regular set in which the input being considered is known to fall  . In the extreme , if each input string is fully observed ( not the case if the input is bound by composition to the output of a one-to-many FST  )  , one can succeed by restricting g to each input string in turn  ; this amounts to manually dividing f(x , y ) by g(x ) . 
10 Traditionally log ( strength ) values are called weights , but this paper uses ? weight ? to mean something else  . 
89 a : x/ . 631 0 a : x/ . 6311 b : x/ . 027 a :/ . 7 ? b :/ . 0051? 12/ . 5b:z/ . 1284 b :/ . 1 ? b : z / . 404b :/ . 1 ? Figure 2: The joint model of Fig . 1 a constrained to generate only input ? a ( a+b ) ? and output = xxz . 
f ? on demand ( Mohri et al , 1998) can pay off here , since only part off ? may be needed subsequently  . ) As training data we are given a set of observed ( input , output ) pairs , ( xi , yi ) . These are assumed to be independent random samples from a joint distribution of the form f ??  ( x , y ); the goal is to recover the true ?? . Samples need not be fully observed ( partly supervised training ) : thus xi ? ? ? , yi ?? ? may be given as regular sets in which input and output were observed to fall  . For example , in ordinary HMM training , xi = ?? and represents a completely hidden state sequence  ( cf . Ristad (1998) , who allows any regular set ) , while yi is a single string representing a completely observed emission sequence  . 1 1 What to optimize ? Maximum-likelihood estimation guesses ?? to be the ? maximizing ? if ?  ( xi , yi ) . Maximum-posterior estimation tries to maximize P ( ? ) ? ? if ? ( xi , yi ) where P (?) is a prior probability . In a loglinear parameterization , for example , a prior that penalizes feature strengths far from  1 can be used to do feature selection and avoid overfitting  ( Chen and Rosenfeld ,  1999) . 
The EM algorithm ( Dempster et al , 1977) can maximize these functions . Roughly , the E step guesses hidden information : if ( xi , yi ) was generated from the current f ? , which FST paths stand a chance of having been the path used ?  ( Guessing the path also guesses the exact input and output  . ) The M step updates ? to make those paths more likely  . 
EM alternates these steps and converges to a local optimum  . The M step?s form depends on the parameterization and the E step serves the M step?s needs  . 
Let f?be Fig . 1a and suppose ( xi , yi ) = ( a(a + b ) ?, xxz) . During the E step , we restrict to paths compatible with this observation by computing xi ? f??yi  , shown in Fig .  2 . To find each path?s posterior probability given the observation  ( xi , yi ) , just conditionalize : divide its raw probability by the total probability  ( ? 0 . 1003) of all paths in Fig .  2 . 
11 To implement an HMM by an FST , compose a probabilistic FSA that generates a state sequence of the HMM with a conditional FST that transduces HMM states to emitted symbols  . 
But that is not the full E step . The M step uses not individual path probabilities  ( Fig . 2 has infinitely many ) but expected counts derived from the paths . 
Crucially ,   ?4 will show how the E step can accumulate these counts effortlessly  . We first explain their use by the M step , repeating the presentation of ?2: ? If the parameters are the 17 weights in Fig . 1a , the M step reestimates the probabilities of the arcs from each state to be proportional to the expected number of traversals of each arc  ( normalizing at each state to make the FST Markovian  )  . So the E step must count traversals . This requires mapping Fig . 2 back onto Fig .   1a : to traverse either 8? a : x ? ? 9? or 9? a : x ? ? 10? in Fig . 2 is ? really ? to traverse0 ? a:x ? ?0? in Fig . 1a . 
? If Fig . 1 a was built by composition , the M step is similar but needs the expected traversals of the arcs in Fig  . 1b ? c . This requires further unwinding of Fig .   1a?s   0? a : x ? ? 0?: to traverse that arc is ? really ? to traverse Fig  . 1b?s4?a:p ? ?4? and Fig . 1 c?s 6 ? p : x ? ? 6 ? . 
? If Fig . 1b was defined by the regexp given earlier , traversing 4? a:p ? ? 4? is in turn ? really ? just evidence that the ?- coincame upheads  . To learn the weights ? ,  ? ,  ? ,  ? , count expected heads/tails for each coin . 
? If arc probabilities ( or even ? ,  ? ,  ? , ?) have loglinear parameterization , then the E step must compute c = ? i ecf(xi , yi ) , where ec(x , y ) denotes the expected vector of total feature counts along a random path in f ? whose  ( input , output ) matches(x , y ) . The M step then treats c as fixed , observed data and adjusts ? until the predicted vector of total feature counts equals c  , using Improved Iterative Scaling ( Della Pietra et al , 1997; Chen and Rosenfeld ,  1999) . 12 For globally normalized , joint models , the predicted vector is ecf (?? , ??) . If the loglinear probabilities are conditioned on the state and/or the input  , the predicted vector is harder to describe ( though usually much easier to compute )  . 131 2 IIS is itself iterative ; to avoid nested loops , run only one iteration at each M step , giving a GEM algorithm ( Riezler ,  1999) . 
Alternatively , discard EM and use gradient-based optimization . 
13 For per-state conditional normalization , let Dj , a be the set of arcs from state j with input symbol a ??  ; their weights are normalized to sum to 1 . Besides computing c , the E step must count the expected number dj , a of traversals of arcs in each Dj , a . Then the predicted vector given ? is ? j , adj , a ? ( expected feature counts on a randomly chosen arc in Dj  , a ) . Per-state joint normalization ( Eisner , 2001b ,  ?8 . 2) is similar but drops the dependence on a . The difficult case is global conditional normalization  . Itarises , for example , when training a joint model of the form f ? = ??  ?   ( g ? ? h ? )  ? ? ? , where h ? is a conditional It is also possible to use this EM approach for discriminative training  , where we wish to maximize ? iP ( yixi ) and f ? ( x , y ) is a conditional FST that defines P(yx ) . The trick is to instead train a joint model g ? f  ?  , where g(x i ) defines P ( xi ) , thereby maximizing ? iP(xi ) ? P ( yixi ) . ( Of course , the method of this paper can train such compositions  . ) If x1, .   .   . xn are fully observed , just define each g(xi ) = 1/n . But by choosing a more general model of g , we can also handle incompletely observed xi : training g?f ? then forces g and f ? to cooperatively reconstruct a distribution over the possible inputs and do discriminative training of f ? given those inputs  .   ( Any parameters of g may be either frozen before training or optimized along with the parameters off ?  . ) A final possibility is that each xi is defined by a probabilistic FSA that already supplies a distribution over the inputs  ; then we consider x i ? f ?? y i directly , just as in the joint model . 
Finally , note that EM is not all-purpose . It only maximizes probabilistic objective functions  , and even there it is not necessarily as fast as ( say ) conjugate gradient . For this reason , we will also show below how to compute the gradient of f ?  ( xi , yi ) with respect to ? , for an arbitrary parameterized FST f ? . 
We remark without elaboration that this can help optimize task-related objective functions  , such as ? i ? y(P ( xi , y )? / ? y?P ( xi , y ?)?) ? error(y , yi ) . 
4 The EStep : Expectation Semirings
It remains to devise appropriate E steps , which looks rather daunting . Each path in Fig . 2 weaves together parameters from other machines , which we must untangle and tally . In the 4-coin parameterization , path 8? a : x ? ? 9? a : x ? ? 10? a : ? ? 10? a : ? ? 10? b : z ? ? 12? must yield up a vector ? H ? , T ? , H ? , T ? , H ? , T ? , H ? , T ?? that counts observed heads and tails of the 4 coins . This non-trivially works out to ?4 ,  1 ,  0 ,  1 ,  1 ,  1 ,  1 ,  2? . For other parameterizations , the path must instead yield a vector of arc traversal counts or feature counts  . 
Computing a count vector for one path is hard enough  , but it is the Estep?s job to find the expected value of this vector ? an average over the infinitely loglinear model of P  ( vu ) for u ? ? ? ? , v ? ? ? ? . Then the predicted count vector contributed by h is ? i ? u ? ? ? ? P  ( uxi , yi ) ? ech(u , ???) . The term ? iP(uxi , yi ) computes the expected count of each u ???? . It may be found by a variant of ?4 in which path values are regular expressions over  ???  . 
many paths pi through Fig .   2 in proportion to their posterior probabilities P ( pixi , yi ) . The results for all ( xi , yi ) are summed and passed to the M step . 
Abstractly , let us say that each path pi has not only a probability P  ( pi )  ? [0 , 1] but also a value val(pi ) in a vector space V , which counts the arcs , features , or coin flips encountered along path pi . The value of a path is the sum of the values assigned to its arcs  . 
The E step must return the expected value of the unknown path that generated  ( xi , yi ) . For example , if every arc had value 1 , then expected value would be expected path length  . Letting ? denote the set of paths in x i ? f ?? yi  ( Fig . 2), the expected value is 14
E[val(pi)xi , yi ] = ? pi??P ( pi ) val ( pi ) ? pi??P ( pi )   ( 1 ) The denominator of equation ( 1 ) is the total probability of all accepting paths in x i ? f ? y i  . But while computing this , we will also compute the numerator . 
The idea is to augment the weight data structure with expectation information  , so each weight records a probability and a vector counting the parameters that contributed to that probability  . We will enforce an invariant : the weight of any path set ? must be  ( ? pi??P ( pi )  , ? pi??P ( pi ) val(pi )) ? R?0?V , from which (1) is trivial to compute . 
Berstel and Reutenauer ( 1988 ) give a sufficiently general finite-state framework to allow this : weights may fall in any set K  ( instead of R )  . Multiplica-tion and addition are replaced by binary operations ? and ? on K  . Thus ? is used to combine arc weights into a path weight and ? is used to combine the weights of alternative paths  . To sum over in finite sets of cyclic paths we also need a closure operation ?  , interpreted as k ? = ? ? i=0 ki . The usual finite-state algorithms work if ( K , ? , ? , ?) has the structure of a closed semiring . 15 Ordinary probabilities fall in the semiring ( R ? 0 , + , ? ,  ?) . 1 6 Our novel weights fall in a novel 14Formal derivation of ( 1 ) : ? piP ( pixi , yi)val(pi ) = (? piP(pi , xi , yi)val(pi))/P ( xi , yi ) = (? piP(xi , yipi)P ( pi ) val(pi )) / ? piP(xi , yipi)P ( pi ); now observe that P ( xi , yipi ) = 1 or 0 according to whether pi ? ? . 
15 That is : ( K , ?) is a monoid ( i . e . , ?: K?K ? K is associative ) with identity 1 . ( K , ?) is a commutative monoid with identity 0 . ? distributes over ? from both sides , 0 ? k=k ?0 = 0 , and k ? = 1 ? k ? k ? = 1 ? k ? ? k . For finite-state composition , commutativity of ? is needed as well . 
16 The closure operation is defined for p ? [0 , 1) as p ? = 1/(1 ? p) , so cycles with weights in [0 , 1) are allowed . 
V-expectation semiring , ( R?0?V , ? , ? , ?) : ( p1 , v1)? ( p2 , v2) def = ( p1 p2 , p1 v2 + v1 p2) (2) ( p1 , v1)? ( p2 , v2) def = ( p1 + p2 , v1+v2) (3) if p ? defined , ( p , v ) ? def = ( p ? , p ? v p ? )   ( 4 ) If an arc has probability p and value v , we give it the weight ( p , pv ) , so that our invariant ( see above ) holds if ? consists of a single length-0 or length1 path . The above definitions are designed to preserve our invariant as we build up larger paths and path-sets  . ? lets us concatenate ( e . g . ) simple paths pi1 , pi2 to get a longer path pi with P ( pi ) = P ( pi1 ) P ( pi2 ) and val ( pi ) = val ( pi1 ) + val ( pi2 )  . The definition of ? guarantees that path pi?s weight will be  ( P ( pi )  , P ( pi ) ? val(pi)) . ? lets us take the union of two disjoint path sets  , and ? computes in finite unions . 
To compute (1) now , we only need the total weight ti of accepting paths in x i ? f ? y i  ( Fig .  2) . 
This can be computed with finite-state methods : the machine  (  ? xi ) ?f? ( yi ?  ) is a version that replaces all input : output labels with :   , so it maps( , ) to the same total weight ti . Minimizing it yields a one-state FST from which ti can be read directly ! The other ? magical ? property of the expectation semiring is that it automatically keeps track of the tangled parameter counts  . For instance , recall that traversing 0? a : x ? ? 0? should have the same effect as traversing both the underlying arcs  4? a:p ? ? 4? and 6? p : x ? ? 6?  . And indeed , if the underlying arcs have values v1 and v2 , then the composed arc0 ? a:x ? ?0? gets weight ( p1 , p1v1)? ( p2 , p2v2) = ( p1 p2 , p1 p2(v1 + v2)) , just as if it had value v1 + v2 . 
Some concrete examples of values may be useful : ? To count traversals of the arcs of Figs  . 1b ? c , number the searcs and let arc ` have value e ` , the ` th basis vector . Then the ` the lement of val ( pi ) counts the appearances of arc ` in path pi , or underlying path pi . 
? A regexp of form E + ? F = ? E + ( 1?? ) F should be weighted as ( ? , ? ek)E+(1? ? , (1? ?) ek+1)F in the new semiring . Then elements k and k + 1 of v al ( pi ) count the heads and tails of the ?- coin . 
? For a global loglinear parameterization , an arc?s value is a vector specifying the arc?s features  . Then val ( pi ) counts all the features encountered along pi . 
Really we are manipulating weighted relations , not FSTs . We may combine FSTs , or determinize or minimize them , with any variant of the semiring-weighted algorithms  . 1 7 As long as the resulting FST computes the right weighted relation  , the arrangement of its states , arcs , and labels is unimportant . 
The same semiring may be used to compute gradients  . We would like to find f?(xi , yi ) and its gradient with respect to ? , where f ? is real-valued but need not be probabilistic  . Whatever procedures are used to evaluate f?(xi , yi ) exactly or approximately ? for example , FST operations to compile f ? followed by minimization of  (  ? xi ) ? f ?? ( yi ?  ) ? can simply be applied over the expectation semiring  , replacing each weight p by ( p , ? p ) and replacing the usual arithmetic operations with ?  ,  ? , etc . 18  ( 2 ) ? ( 4 ) preserve the gradient ( (2 ) is the derivative product rule )  , so this computation yields ( f?(xi , yi ) , ? f?(xi , yi )) . 
5 Removing Inefficiencies
Now for some important remarks on efficiency : ? Computing ti is an instance of the wellknown algebraic path problem  ( Lehmann , 1977; Tarjan , 1981a ) . Let T i = x i ? f ? y i . Then ti is the total semiring weight w0n of paths in T i from initial state 0 to final state n ( assumed WLOG to be unique and unweighted )  . It is wasteful to compute ti as suggested earlier  , by minimizing (  ? xi ) ? f?(yi ? ) , since then the real work is done by an - closure step  ( Mohri ,  2002 ) that implements the all-pairs version of algebraic path  , whereas all we need is the single-source version . If n and m are the number of states and edges , 19 then both problems are O(n3) in the worst case , but the single-source version can be solved in essentially O  ( m ) time for acyclic graphs and other reducible flow graphs  ( Tarjan , 1981b ) . For a general graph Ti , Tarjan ( 1981b ) shows how to partition into ? hard ? subgraphs that localize the cyclicity or irreducibility  , then run the O ( n3 ) algorithm on each subgraph ( thereby reducing n to as little as 1 )  , and recombine the results . The overhead of partitioning and recombining is essentially only O  ( m )  . 
? For speeding up the O(n3) problem on subgraphs , one can use an approximate relaxation technique 17Eisner   ( submitted ) develops fast minimization algorithms that work for the real and V-expectation semirings  . 
18 Division and subtraction are also possible : ? ( p , v ) = (? p , ? v ) and ( p , v ) ?1 = ( p ? 1 , ?p?1vp?1) . Division is commonly used in defining f ? ( for normalization )  . 
1 9Multiple edges from j to k are summed into a single edge . 
( Mohri , 2002) . Efficient hardware implementation is also possible via chip-level parallelism  ( Rote ,  1985) . 
? In many cases of interest , Ti is an acyclic graph . 2 0 Then Tarjan?s method computes w0j for each j in topologically sorted order , thereby finding ti in a linear number of ? and ? operations  . For HMMs ( footnote 11) , Ti is the familiar trellis , and we would like this computation of tito reduce to the forwardbackward algorithm  ( Baum ,  1972) . But notice that it has no backward pass . In place of pushing cumulative probabilities backward to the arcs  , it pushes cumulative arcs ( more generally , values in V ) forward to the probabilities . This is slower because our ? and ? are vector operations  , and the vectors rapidly lose sparsity as they are added together  . 
We therefore reintroduce a backward pass that lets us avoid ? and ? when computing ti  ( so they are needed only to construct Ti )  . This speedup also works for cyclic graphs and for any V  . Write wjk as ( pjk , vjk ) , and let w1jk = ( p1jk , v1jk ) denote the weight of the edge from j to k . 19 Then it can be shown that w0n = ( p0n , ? j , kp0jvward and backward probabilities , p0j and pkn , can be computed using single-source algebraic path for the simpler semiring  ( R , + , ? , ?)? or equivalently , by solving a sparse linear system of equations over R  , a much-studied problem at O(n ) space , O(nm ) time , and faster approximations ( Greenbaum ,  1997) . 
? A Viterbi variant of the expectation semiring exists : replace  ( 3 ) with if ( p1> p2 , ( p1 , v1) , ( p2 , v2)) . 
Here , the forward and backward probabilities can be computed in time only O  ( m + n log n )   ( Fredman and Tarjan ,  1987) . kbest variants are also possible . 
6 Discussion
We have exhibited a training algorithm for parameterized finite-state machines  . Some specific consequences that we believe to be novel are  ( 1 ) an EM algorithm for FSTs with cycles and epsilons  ;   ( 2 ) training algorithms for HMMs and weighted contextual edit distance that work on incomplete data  ;   ( 3 ) end-to-end training of noisy channel cascades , so that it is not necessary to have separate training data for each machine in the cascade  ( cf . Knight and Graehl , 20 If x i and y i are acyclic ( e . g . , fully observed strings ) , and f(or rather its FST ) has no :  cycles , then composition will ? unroll ? f into an acyclic machine  . If only xi is acyclic , then the composition is still acyclic if domain ( f ) has no  cycles . 
1998) , although such data could also be used ;   ( 4 ) training of branching noisy channels ( footnote 7 )  ; (5) discriminative training with incomplete data ;   ( 6 ) training of conditional MEMMs ( McCallum et al ,  2000 ) and conditional random fields ( Lafferty et al , 2001) on unbounded sequences . 
We are particularly interested in the potential for quickly building statistical models that incorporate linguistic and engineering insights  . Many models of interest can be constructed in our paradigm  , without having to write new code . Bringing diverse models into the same declarative framework also allows one to apply new optimization methods  , objective functions , and finite-state algorithms to all of them . 
To avoid local maxima , one might try deterministic annealing ( Rao and Rose ,  2001) , or randomized methods , or place a prior on ? . Another extension is to adjust the machine topology  , say by model merging ( Stolcke and Omohundro ,  1994) . Such techniques build on our parameter estimation method  . 
The key algorithmic ideas of this paper extend from forward-backward-style to inside-outside -style methods  . For example , it should be possible to do end-to-end training of a weighted relation defined by an interestingly parameterized synchronous CFG composed with tree transducers and then FSTs  . 

L . E . Baum .  1972 . An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process  . In equalities , 3 . 
Jean Berstel and Christophe Reutenauer .  1988 . Rational Series and their Languages . Springer-Verlag . 
Stanley F . Chen and Ronald Rosenfeld .  1999 . A Gaussian prior for smoothing maximum entropy models  . 
Technical Report CMU-CS-99-108, Carnegie Mellon.
S . Della Pietra , V . Della Pietra , and J . Lafferty .  1997 . 
Inducing features of random fields . IEEE Transactions on Pattern Analysis and Machine Intelligence  ,  19(4) . 
A . P . Dempster , N . M . Laird , and D . B . Rubin .  1977 . 
Maximum likelihood from incomplete data via the EM algorithm  . J . Royal Statist . Soc . Ser . B , 39(1):1?38 . 
Jason Eisner . 2001a . Expectation semirings : Flexible EM for finite -state transducers  . In G . van Noord , ed . , Proc . of the ESSLLI Workshop on FiniteState Methods in Natural Language Processing  . Extended abstract . 
Jason Eisner . 2001b . Smoothing a Probabilistic Lexicon via Syntactic Transformations  . Ph . D . thesis , University of Pennsylvania . 
D . Gerdemann and G . van Noord .  1999 . Transducers from rewrite rules with back references  . Proc . of EACL . 
Anne Greenbaum .  1997 . Iterative Methods for Solving Linear Systems . Soc . for Industrial and Applied Math . 
Kevin Knight and Yaser Al-Onaizan .  1998 . Translation with finite-state devices . In Proc . of AMTA . 
Kevin Knight and Jonathan Graehl .  1998 . Machine transliteration . Computational Linguistics , 24(4) . 
J . Lafferty , A . McCallum , and F . Pereira .  2001 . Conditional random fields : Probabilistic models for segmenting and labeling sequence data  . Proc . of ICML . 
D . J . Lehmann .  1977 . Algebraic structures for transitive closure . Theoretical Computer Science , 4(1):59?76 . 
A . McCallum , D . Freitag , and F . Pereira .  2000 . Maximum entropy Markov models for information extraction and segmentation  . Proc . of ICML , 591?598 . 
M . Mohri and M . -J . Nederhof .  2001 . Regular approximation of contextfree grammars through transformation  . In J . -C . Junqua and G . van Noord , eds . , Robustness in Language and Speech Technology . Kluwer . 
Mehryar Mohri and Richard Sproat .  1996 . An efficient compiler for weighted rewrite rules . In Proc . of ACL . 
M . Mohri , F . Pereira , and M . Riley .  1998 . A rational design for a weighted finite-state transducer library  . Lecture Notes in Computer Science , 1436 . 
M . Mohri .  2002 . Generic epsilon-removal and input epsilon -normalization algorithms for weighted transducers  . Int . J . of Foundations of Comp . Sci . , 1(13) . 
Mark-Jan Nederhof .  2000 . Practical experiments with regular approximation of contextfree languages  . 
Computational Linguistics , 26(1).
Fernando C . N . Pereira and Michael Riley .  1997 . Speech recognition by composition of weighted finite automata  . In E . Roche and Y . Schabes , eds . , FiniteState Language Processing . MIT Press , Cambridge , MA . 
A . Rao and K . Rose .   2001 Deterministically annealed design of hidden Markov movel speech recognizers  . 
In IEEE Trans . on Speech and Audio Processing , 9(2) . 
Stefan Riezler .  1999 . Probabilistic Constraint Logic Programming . Ph . D . thesis , Universita?t Tu?bingen . 
E . Ristad and P . Yianilos .  1996 . Learning string edit distance . Tech . Report CS-TR-532-96, Princeton . 
E . Ristad .  1998 . Hidden Markov models with finite state supervision  . In A . Kornai , ed . , Extended FiniteState Models of Language . Cambridge University Press . 
Emmanuel Roche and Yves Schabes , editors . 1997.
FiniteState Language Processing . MIT Press.
Gu?nter Rote .  1985 . Asys to licarray algorithm for the algebraic path problem  ( shortest paths ; matrix inversion ) . Computing , 34(3):191?219 . 
Richard Sproat and Michael Riley .  1996 . Compilation of weighted finite-state transducers from decision trees  . 
In Proceedings of the 34th Annual Meeting of the ACL . 
Andreas Stolcke and Stephen M . Omohundro . 1994.
Best-first model merging for hidden Markov model induction  . Tech . Report ICSITR-94-003, Berkeley , CA . 
Robert Endre Tarjan . 1981a . A unified approach to path problems . Journal of the ACM , 28(3):577?593, July . 
Robert Endre Tarjan . 1981b . Fast algorithms for solving path problems . J . of the ACM , 28(3):594?614, July . 
G . van Noord and D . Gerdemann .  2001 . An extendible regular expression compiler for finite-state approaches in natural language processing  . In Automata Implementation , no . 22 in Springer Lecture Notes in CS . 
