Dependency Parsing with an Extended Finite State Approach 
Kemal Oflazer
Department of Computer Engineering
Bilkent University
Ankara , 06533, Turkey
ko?cs , bilkent , edu.tr
Computing Research Laboratory
New Mexico State University
Las Cruces , NM , 88003 USA
ko@crl , nmsu.edu

This paper presents a dependency parsing scheme using an extended finite state approach  . The parser augments input representation with " channels " so that links representing syntactic dependency relations among words can be accommodated  , and iterates on the input a number of times to arrive at a fixed point  . Intermediate configurations violating various constraints of projective dependency representations such as no crossing links  , no independent items except sential head , etc , are filtered via finite state filters . We have applied the parser to dependency parsing of Turkish  . 
1 Introduction
Recent advances in the development of sophisticated tools for building finite state systems  ( e . g . , XRCE Finite State Tools ( Karttunen et al ,  1996) , ATgzT Tools ( Mohri et al ,  1998 ) ) have fostered the development of quite complex finite state systems for natural language processing  . In the last several years , there have been a number of studies on developing finite state parsing systems  , ( Koskenniemi , 1990; Koskenniemi et al , 1992; Grefenstette , 1996; Ait-Mokhtar and Chanod ,  1997) . There have also been a number of approaches to natural anguage parsing using extended finite state approaches in which a finite state engine is applied multiple times to the input  , or various derivatives thereof , until some stopping condition is reached . Roche ( 1997 ) presents an approach for parsing in which the input is iteratively bracketed using a finite state transducer  . Ab-ney ( 1996 ) presents a finite state parsing approach in which a tagged sentence is parsed by transducers which progressively transform the input to sequences of symbols representing phrasal constituents  . This paper presents an approach to dependency parsing using an extended finite state model resembling the approaches of Roche and Abney  . The parser produces outputs that encode a labeled ependency tree representation f the syntactic relations between the words in the sentence  . 
We assume that the reader is familiar with the basic concepts of finite state transducers  ( FS Thereafter )  , finite state devices that map between two regular languages U and L  ( Kaplan and Kay ,  1994) . 
2 Dependency Syntax
Dependency approaches to syntactic representation use the notion of syntactic relation to associate surface lexical items  . The book by Mel~uk ( 1988 ) presents a comprehensive exposition of dependency syntax  . Computational pproaches to dependency syntax have recently become quite popular  ( e . g . , a workshop dedicated to computational pproaches to dependency grammars has been held at  COL-ING/ACL'98 Conference )  . J ~ irvinen and Tapana-ninen have demonstrated an efficient wide-coverage dependency parser for English  ( Tapanainen and J ~ irvinen , 1997; J?rvinen and Tapanainen ,  1998) . 
The work of Sleator and Temperley ( 1991 ) on link grammar , an essentially exicalized variant of dependency grammar  , has also proved to be interesting in a number of aspects  . Dependency-based statistical language modeling and analysis have also become quite popular in statistical natural language processing  ( Lafferty et al , 1992; Eisner , 1996; Chelbaand et al ,  1997) . 
Robinson ( 1970 ) gives four axioms for wellformed dependency structures  , which have been assumed in almost all computational pproaches  . In a dependency structure of a sentence ( i ) one and only one word is independent , i . e . , not linked to some other word , ( ii ) all others depend directly on some word , ( iii ) no word depends on more than one other , and , ( iv ) if a word A depends directly on B , and some word C intervenes between them ( in linear order )  , then C depends directly on A or on B , or on some other intervening word . This last condition of projectivity ( or various extensions of it ; see e . g . , Lau and Huang ( 1994 ) ) is usually assumed by most computational approaches to dependency grammars as a constraint for filtering configurations  , and has also been used as a simplifying condition in statistical approaches for inducing dependencies from corpora  ( e . g . , Yiiret (1998) . )  3 Turkish Turkish is an agglutinative language where a sequence of inflectional and derivational morphemes get affixed to a root  ( Oflazer ,  1993) . Derivations are very productive , and the syntactic relations that a word is involved in as a dependent or head element  , are determined by the inflectional properties of the  1 II
IG3 io , 1
Figure hLinks and Inflectional Groups one or more  ( intermediate ) derived forms . In this work , we assume that a Turkish word is represented as a sequence of inflectional groups  ( IGs hereafter )  , separated by " DBs denoting derivation boundaries  , in the following general form : root+Infl1"DB+Infl2"DB  + .  ?  . ' DB + Infl . 
where Inflide note relevant inflectional features including the part-of-speech for the root  , or any of the derived forms . For instance , the derived determiner saglam la?t lrdzgzmz zdakiI would be represented  as:2 saglam+hdj"DB+Verb+Become"DB+Verb+Caus+Pos " DB+Adj+PastPart+Pisg*DB+  Noun+Zero+A3sg+Pnon+Loc'DB+Det 
This word has 6IGs :
I . sa ~ lam+Adj 2 . + Verb+Become 3 . + Verb+Caus+Pos4 . + Adj + PastPart + Plsg 5 . + Noun+Zero+A3sg 6 . +Det+Pnon+LocA sentence would then be represented as a sequence of the IGs making up the words  . 
An interesting observation that we can make about Turkish is that  , when a word is considered as a sequence of IGs , syntactic relation links only emanate from the last IG of a  ( dependent ) word , and land on one of the IG's of the ( head ) word on the right ( with minor exceptions )  , as exemplified in Figure 1 . A second observation is that , with minor exceptions , the dependency links between the IGs , when drawn above the IG sequence , do not cross . 
Figure 2 shows a dependency tree for a sentence laid on top of the words segmented along IG boundaries  . 
4 F in i te S ta te Dependency Pars ing The approach relies on augmenting the input with " channels " that  ( logically ) reside above the IG sequence and " laying " links representing dependency relations in these channels  , as depicted Figure 3a ) . 
The parser operates in a number of iterations : At each iteration of the parser  , an new empty channel 1Literally ,  " ( the thing existing ) at the time we caused ( something ) to become strong " . Obviously this is not a word that one would use everyday  . Turkish words found in typical text average about  34 morphemes including the stem . 
2 The morphological features other than the obvious POSe are : + Become : become verb  , + Caus : causative verb , PastPart : Derived past participle , Ptsg : leg possessive agreement , A3sg:3sg number-person agreement , + Zero : Zero derivation with no overt morpheme , + Pnon : No possessive agreement , + Loc : Locative case , + Poe : Positive Polarity . 
a ) Input sequence of IGs amaugmented with symbols to represent Channels  . 
( IGl ) ( IG2) ( IG3) . . . (IGi ) . . . ( IGn_)(IG , ) b ) Links are embedded in channels . 
,  .   .  -  .   .   .  ,  .  , ,% , , , :  .   .   .  ,  .   .   . r , .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . ~ , ,  .   .   .   .   . ~  .   .   .   .   .   . 
( IGl ) ( IG2) ( IG3) . . . (IGi ) . . . ( IG . _l ) ( IG . ) c ) New channels are " stacked on top of each other " . 
? u .   .   .   .   .   .  ~  .   .   . T .  ' ,  .   . , L .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .  ~  .   .   .   .   .   .   . ~  .   .   .   .   . 
.n .   .   .   .   .   .   . r .   .  ,  .  : ,  .   .   .   .   .   . ~  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . ~  .   .   .   .  ,  .   . ~  .   .   .   .   .   . 
(IGI ) ( IG2) ( IG3) . . . (IGi ) . . . ( IG . .I ) ( IG . ) d ) So that links that cannot be accommodated in lower channels can be established  . 
.  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .  ?  . l .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .  ;  .   .   .   .   .   . 
( IGl ) ( IG2) ( IG3) . . . (IGi ) . . . (IG , . l ) (1G ,)? . ~  .  - -  . --~-" A '"" ~ .   .   .   .   .   .   .   .   .   .   .   .   .  ~  .   .   .   .   .   .   .   .   .   .   .   .  ~"""1~  .   .   .   .   . 
(IG ,) ( IG ,) ( IG 0... ( IG ~) ... ( IG ?.,)0 G,)
Figure 3: Channels and Links is " stacked " on top of the input  , and any possible links are established using these channels  , until no new links can be added . An abstract view of this is presented in parts b ) throughe ) of Figure 3 . 
4.1 Representing Channels and Syntactic
Relations
The sequence ( or the chart ) of IGs is produced by a a morphological naly zer FST  , with each IG being augmented by two pairs of delimiter symbols  , as <( IG ) > . Word final IGs , IGs that links will emanate from , are further augmented with a special marker ? . 
Channels are represented by pairs of matching symbols that surround the <  .   .   . ( and the ) .   .   . > pairs . 
Symbols for new channels ( upper channels in Figure 3 ) are stacked so that the symbols for the topmost channels are those closes to the  (   .   .   .  )  . a The channel symbol 0 indicates that the channel segment is not used while  1 indicates that the channel is used by a link that starts at some IG on the left and ends at some IG on the right  , that is , the link is just crossing over the IG . If a link starts from an IG ( ends on an IG ) , then a start ( stop ) symbol denoting the syntactic relation is used on the right  ( left ) side of the IG . The syntactic relations ( along with symbols used ) that we currently encode in our parser are the following :  4 S ( Subject )  , 0 ( Object ) , M(Modifier , adv/adj ) , P ( Possessor) , C(Classifier ) , D ( Determiner ) , T ( Dative Adjunct ) , L ( Locative Adjunct) , A : ( Ablative Adjunct ) and I ( Instrumental Adjunct )  . For instance , with three channels , the two IGs of bahgedeki in Figure 2 , would be represented as < MD 0 ( bah~e+Noun+h3sg+Pnon+Loc ) 000> <000 ( +Det? ) 00 d > . The M and the D to the left of 3At any time , the number of channel symbols on both sides of an IG are the same  . 
4We use the lower case symbol to mark the start of the link and the uppercase symbol to encode the end of the link  . 

DetPosSubj
DADJ NDN ADV VNP NAD VV
Last line shows the final POS for each word.
Figure 2: Dependency Links in an example Turkish Sentence the first IG indicate the incoming modifier and determiner links  , and the d on the right of the second IG indicates the outgoing determiner link  . 
4.2 Components of a Parser Stage
The basic strategy of a parser stage is to recognize by a rule  ( encoded as a regular expression ) a dependent IG and a head IG , and link them by modifying the " topmost " channel between those two  . To achieve this : 1 . we put temporary brackets to the left of the dependent IG and to the right of the head IG  , making sure that ( i ) the last channel in that segment is free , and ( ii ) the dependent is not already linked ( at one of the lower channels )  ,  2 . we mark the channels of the start , intermediate and ending IGs with the appropriate symbols encoding the relation thus established by the brackets  ,  3 . we remove the temporary brackets . 
A typical linking rule looks like the following : 5 \[ LLIGILR\]\[ML IG2 MR\]*\[RL IG3 RR\] ( -> ) " s " .   .   . " s " This rule says: ( optionally ) bracket ( with S and S )  , any occurrence of morphological pattern IG1 ( dependent )  , skipping over any number of occurrences of pattern  IG2  , finally ending with a pattern IG3 ( governor ) . The symbols L(eft ) L(eft ) , LR , ML , MR , RL and RR are regular expressions that encode constraints on the bounding channel symbols  . For instance , LI ~ is the pattern "? . .  .   .  )  .   .   .   . 0"\["0" I1\]*">" which checks that ( i ) this is a word-final IG ( hasa "?" )  ,   ( ii ) the right side " topmost " channel is empty ( channel symbol nearest o " ) " is "0" )  , and ( iii ) the IG is not linked to any other in any of the lower channels  ( the only symbols on the right side are 0s and ls . )
For instance the example rule \[ LL NominativeNominalA3pl LR\]\[MLAnyIGMR\]*\[RL\[ FiniteVerbA3sg I FiniteVerbl3pl \] RR\] ( -> ) " s .   .   .   . s " S We use the XRCE Regular Expression Language Syntax  ; see http://www . xrce . xerox , com/resea . vch/taltt/fst/fs syntax . htral for details . 
is used to bracket a segment starting with a plural nominative nominal  , as subject of a finite verb on the right with either +  A3sg or + A3pl number-person agreement ( allowed in Turkish . ) The regular expression NominativeNominalA3pl matches any nominal IG with nominative case and A3pl agreement , while the regular expression \[ F initeVerbA3sg J Fini teVerbA3pl \] matches any finite verb IG with either A3sg or A3pl agreement . The regular expression Any IG matches any IG . 
All the rules are grouped together into a parallel bracketing rule defined as follows: 
Bracket =\[
Patternl (->) " Rell " . . . " Rell ", Pattern2(->) " Rel2" . . . " Rel2" ,  \ ]  ; which will produce all possible bracketing of the input IG sequence  .  6 4 . 3 F i l te r ing Cross ing L ink Conf igurat ions The bracketings produced by Bracket contain configurations that may have crossing links  . This happens when the left side channel symbols of the IG immediately right of a open bracket contains the symbol  1 for one of the lower channels , indicating a linkentering the region , or when the right side channel symbols of the IG immediately to the left of a close bracket contains the symbol  1 for one of the lower channels , indicating a link exiting the segment , i . e . , either or both of the following patterns appear in the bracketed segment :  ( i ) S < .   .   .  1  .   .   .  0 (  .   .   .  )  .   .   . 
( ii ) ... (...) 0...1 ...> S
Configurations generated by bracketing are filtered by FSTs implementing suitable regular expressions that reject inputs having crossing links  . 
A second configuration that may appear is the following : A rule may attempt to put a link in the topmost channel even though the corresponding segment is not utilized in a previous channel  , e . g . , the corresponding segment one of the previous channels may be all Os  . This constraint filters such cases to 6Reli and Rolia repairs of brackets ; there is a distinct pair for each syntactic relation to be identified by these rules  . 
2 56 prevent redundant configurations from proliferating for later iterations of the parser  .   7 For these two configuration constraints we define Filtera on figs as sFilter Configs =\[ Fi lter Crossing Links  . o . 
Filter Empty Segments\];
We can now define one phase ( of one iteration ) of the parser as : Phase = Bracket . o . Filter Con2igs . o . 
Mark Channels . o . Remove Temp Brackets ; The transducer Mark Channels modifies the channel symbols in the bracketed segments to either the syntactic relation start or end symbol  , or a 1 , depending on the IG . Finally , the transducer Remove TempBrackets , removes the brackets .   9 The formulation up to now does not allow us to bracket an IG on two consecutive nonoverlapping links in the same channel  . We would need a bracketing configuration like . . . S < . . . > H < . . . > S . . .  <  . . . > M . . . 
but this would not be possible within Brack et , as patterns check that no other brackets are within their segment of interest  . Simply composing the Phase transducer with itself without introducing a new channel solves this problem  , giving usa one-stage parser , i . e . ,
Parse = Phase . o . Phase ; 4 . 4 Enforc ing Syntact i c Const ra in ts The rules linking the IGs are overgenerating in that they may generate configurations that may violate some general or language specific constraints  . 
For instance , more than one subject or one object may attach to a verb  , or more that one determiner or possessor may attach to a nominal  , an object may attach to a passive verb ( conjunctions are handled in the manner described in J?rvinen and Tapanainen  ( 1998 ) ) , or a nominative pronoun may be linked as a direct object  ( which is not possible in Turkish )  , etc . Constraints preventing these may can be encoded in the bracketing patterns  , but doing so results in complex and unreadable rules  . Instead , each can be implemented as a finite state filter which operate on the outputs of Parse by checking the symbols denoting the relations  . For instance we can define the following regular expression for filtering out configurations where two determiners are attached to the same IG : l ?   7This constraint is a bittrickier since one has to check that the same number of channels on both sides are empty  ; we limit ourselves to the last 3 channels in the implementation . 
8 . o . denotes the transducer composition operator . We also use , for exposition purposes ,  = , instead of the XRCE define command . 
9 The detai ls of these regular expressions are quite uninteresting  . 
l ? Left Channel Symbols and Right Channel Symbols denote the sets of symbols that can appear on the left and right side channels  . 
A tMost0neDet =\["<"\[~\[\[$" D"\]'I\]& Left Charmel Symbols *\]"  ( " Any IG ( "@" )  " ) "
Right Channel Symbols *"> "\]*;
The FST for this regular expression makes sure that all configurations that are produced have at most one D symbol among the left channel symbols  , nM any other syntactic on straints ( e . g . , only one object to a verb ) can be formulated similar to above . 
All such constraints Consl , Cons 2 . .  . Cons N , can then be composed to give one FST that enforces all of these : Syntactic Filter=\ [ Consl  . o . Cons 2 . o . 
Cons 3 . o .   .   .   .   . o . Cons N\] 4 . 5 I te ra t lve app l i ca t ion of the parser Full parsing consists of iterative applications of the Parser and Syntact icF i l ter FSTs  . Let Input be a transducer that represents the word sequence  . Let
Last Channel Not Empty=\["<"Left Channel Symbels+  "  ( " Any IG ( "@" )  " ) "
Right Charmel Symbols +"> "\]* -\["<" Left Channel Symbols *  0  " ( " Any IG ( "@" )  " ) "0 Right Channel Symbols *"> "\]* ; be a transducer which detects if any configuration has at least one link established in the last channel added  ( i . e . , not all of the " topmost " channel symbols are O ' s  . ) Let Morpholog ica lD isambiguator be a reductionistic finite state disambiguator which performs accurate but very conservative local disambiguation and multiword construct coalescing  , to reduce morphological mbiguity without making any errors  . 
The iterative applications of the parser can now be given  ( in pseudo code ) as :  #Map sentence to a transducer representing a chart of IGs M = \[ Sentence  . o . Morphological Analyzer \] . o . 
Morphological Disambi ~ nlator ; repeat
M = M.o . Add Channel.o . Parse.o.
Syntactic Filter ; until(\[M . o . Last Channel Not Empty\] . l ==)
M = M.o.0nly0neUn linked;
Parses = M.I;
This procedure iterates until the most recently added channel of every configuration generated is unused  ( i . e . , the ( lower regular ) language recognized by M . o . Last Channel Not Empty is empty . )
The step after the loop , M = M.o.
0 nly0neUn linked , enforces the constraint that 11 The crucial portion at the beginning says " For any IG it is not the case that there is more than one substring containing D among the left channel symbols of that IG  . " the word final IGs have to link as a dependent to some head  . This transduction filters all those configurations  ( and usually there are many of them due to the optionality in the bracketing step  . ) Then , Parses defined as the ( lower ) language of the resulting FST has all the strings that encode the 
IGs and the links.
4.6 Robust Parsing
It is possible that either because of grammar coverage  , or ungrammatical input , a parse with only one unlinked word final IG may not be found  . In such cases Parses above would be empty . One may however opt to accept parses with k > 1 unlinked word final IGs when there are no parses with < k unlinked word final IGs  ( for some small k . ) This can be achieved by using the lenient composition operator  ( Karttunen ,  1998) . Lenient composition , notated as .  0 . , is used with a generator-filter combination . 
When a generator transducer G is leniently composed with a filter transducer  , F , the resulting transducer , G .  0 . F , has the following behavior when an input is applied : If any of the outputs of G in response to the input string satisfies the filter F  , then G . 0 . F produces just these as output . Otherwise , G . 0 . Foutputs what G outputs . 
Let Unlinked_i denote a regular expression which accepts parse configurations with less than or equaliun linked word final IGs  . For instance , for i = 2 , this would be defined as follows : -\[\[$\[ "<" Left Channel Symbols *"  ( " Any IG "@ .   .   .   .  )"
E "0" I13 .  ">"\]3" > 2 \] ; which rejects configurations having more than 2 word final IGs whose right channel symbols contain only  0s and is , i . e . , they do not link to some other
IG as a dependent.
Replacing line M = H . o . Only 0ne Unlinked , with , for instance , M = M . 0 . Unlinked_l . 0 . 
Unlinked_2 . 0 . Unlinked_3 ; will have the parser produce outputs with up to 3 unlinked word final IGs , when there are no outputs with a smaller number of unlinked word final IGs  . Thus it is possible to recover some of the partial dependency structures when a full dependency structure is not available for some reason  . The caveat would be however that since Unlinked_l is a very strong constraint  , any relaxation would increase the number of outputs substantially  . 
5 Exper iments w i th dependency pars ing o f Turk ish Our work to date has mainly consisted of developing and implementing the representation a d finite state techniques involved here  , along with a nontrivial grammar component . We have tested the resulting system and grammar on a corpus of  50 Turkish sentences ,   20 of which were also used for developing and testing the grammar  . These sentences had 4 to 24 words with an average 10 about 12 words . 
The grammar has two major components . The morphological analyzer is a full coverage analyzer built using XRCE tools  , slightly modified to generate outputs as a sequence of IGs for a sequence of words  . When an input sentence ( again represented as a transducer denoting a sequence of words  ) is composed with the morphological analyzer ( seepseudo code above )  , a transducer for the chart representing all IGs for all morphological ambiguities  ( remaining after morphological disambiguation ) is generated . The dependency relations are described by a set of about  30 patterns much like the ones exemplified above . The rules are almost all nonlexical establishing links of the types listed earlier  . 
Conjunctions are handled by linking the left conjunct to the conjunction  , and linking the conjunction to the right conjunct  ( possibly at a different channel )  . There are an additional set of about 25 finite state constraints that impose various syntactic and configurational constraints  . The resulting Parser transducer has 2707 states 27  , 7 13 transitions while the Syntactic Constraints transducer has  28  , 894 states and 302 , 354 transitions . The combined transducer for morphological nalys is and  ( very limited ) disambiguation has 87 , 475 states and 218 , 082 arcs . 
Table 1 presents our results for parsing this set of 50 sentences . The number of iterations also count the last iteration where no new links are added  . Inspired by Lin's notion of structural complexity  ( Lin ,  1996) , measured by the total length of the links in a dependency parse  , we ordered the parses of a sentence using this measure  . In 32 out of 50 sentences (64%) , the correct parse was either the top ranked parse or among the top ranked parses with the same measure  . In 13 out of 50 parses ( 26% ) the correct parse was not among the top ranked parses  , but was ranked lower . Since smaller structural complexity requires , for example , verbal adjuncts , etc . to attach to the nearest verb wherever possible , topicalization of such items which brings them to the beginning of the sentence  , will generate along ( er ) link to the verb ( at the end ) increasing complexity . In 5 out of 50 sentences (5%) , the correct parse was not available among the parses generated  , mainly due to grammar coverage . The parses generated in these cases used other ( morphological ) ambiguities of certain lexical items to arrive at some parse within the confines of the grammar  . 
The finite state transducers compile in about 2 minutes on Apple Macintosh 250 Mhz Power-book . Parsing is about a second per iteration including lookup in the morphological naly zer  . With completely ( and manually ) morphologically disambiguated input , parsing is instantaneous . Figure 4 presents the input and the output of the parser for a sample Turkish sentence  . Figure 5 shows the output h/ik fimetinizle di ~ ie konomik program lnsonucund a sult of the economic program fo nowed by the government  , 5 nemfiach mlann at flchg\]ms6yledi , important steps were taken . 
Parser Output after 3 iterations :
Parsel : < O00 ( dUnya+Noun+A3sg+Pnon+Nom@ ) OOc><COO ( banka+Noun+A3sg+P3sg+Bom~ ) OcO > < OlO ( tUrkiye+Noun+Prop+A3sg+Pnon+Nom@ ) Olc > < CC ~ ( direkt~r+N~un+A3sg+~3sg+N~m@ ) s~><~1 ( hUkUmet+B~un+A3sg+~n~n+Gen@ ) 1~s><~1 ( iz1e+verb+p~s ) 1~> <~ ( + Adj+Past ~ art + p3sg@ ) 1m~><~11 ( ek~n~mik+Adj@ ) 1~m><MM1 ( pr~gram+B~un+A3sg+~n~n+Gen~ ) ~ p > < P ~ ( s~nuC+N~un+A3sg+P3s~?L~c@ ) ~1~><~ ( ~nem+N~un ) ~><~11 ( +Adj+with@ ) 1~m><M1~ ( adIm+N~un+A3p1+Pn~n+Gen~ ) 1 ~ s > < S ~ ( at+Verb ) ~><~ ( +verb+~ass+P~s ) ~><~ ( +I~un+~ast~art+A3sg+~3s~Acc@ ) ~1~><~L~ ( s~y1e+verb+p~s+~ast+A3sg@ ) ~>
Parse 2: < ~ ( dUnya+I ~ un+A3 sg+~n~n+N~m@ ) ~c><C~ ( banka+N~un+A3sg+~3sg+I~m~ ) ~c~><~1~ ( tUrkiye+N~un+pr~p+A3sg+pn~n+l~m@ ) ~c > < CC ~ ( direkt~r+N~un+A3sg+p3sg+N~m@ ) s~><~ ( hUkUmet+l~un+A3sg+pn~n+Gen@ ) 1~s><~ ( iz1e+Verb+p~s ) 1~> <~ ( + Adj+Past ~ art + ~3 sg@ ) ~m~><~ ( ek~n~mik+AdjQ ) ~m><RM1 ( pr~ram+N~un+A3s~+pn~n+GenQ ) ~ p > < p ~ ( s~nuC+N~un+A3sg+~3sg+L~ ) ~1~><~1~ ( ~nem+~un ) ~><~1 ( +Adj+with@ ) 1~m><M~1 ( adIm+N~un+A3p1+~n~n+Gen~ ) 1 ~ s > < SL 1 ( at+Verb ) 1~><~1 ( +Verb+~ass+~s ) 1~><~ ( +N~un+~astpart+A3sg+~3sg+Acc@ ) 1~><~ ( s~y1e+verb+p~s+~ast+A3sg@ ) ~> The only difference in the two are parses are in the locative adjunct attachment  ( to verbs at and sSyle , highlighted with ***) . 
Figure 4: Sample Input and Output of the parser
Avg . Words/Sentence:
Avg . IGs/Sentence:
Avg . Parser Iterations :
Avg . Parses/Sentence : 11 . 7 (4 - 24) 16 . 4 (5 - 36) 5 . 2 (3 - 8) 23 . 9  ( 1 - 132 ) Table 1: Statistics from Parsing 50 Turkish Sentences of the parser processed with a Perl script to provide a more human-consumable presentation :  6 Discussion and Conclusions We have presented the architecture and implementation of novel extended finite state dependency parser  , with results from Turkish . We have formulated , but not yet implemented at this stage , two extensions . Crossing dependency links are very rare in Turkish and almost always occur in Turkish when an adjunct of a verb cuts in a certain position of a  ( discontinuous ) noun phrase . We can solve this by allowing such adjuncts to use a special channel " below " the IG sequence so that limited crossing link configurations can be allowed  . Links where the dependent is to the right of its head  , which can happen with some of the word order variations  ( with back-grounding of some dependents of the main verb  ) can similarly be handled with a right-to-left version of Parser which is applied during each iteration  , but these cases are very rare . 
In addition to the reductionistic disambiguator that we have used just prior to parsing  , we have implemented a number of heuristics to limit the number of potentially spurious configurations that result because of optionality in bracketing  , mainly by enforcing obligatory bracketing for immediately sequential dependency configurations  ( e . g . , the complement of a postposition is immediately before it  . ) Such heuristics force such dependencies to appear in the first channel and hence prune many potentially useless configurations popping up in later stages  . 
The robust parsing technique has been very instrumental during the process mainly in the debugging of the grammar  , but we have not made any substantial experiments with it yet  . 
7 Acknowledgments
This work was partially supported by a NATO Science for Stability Program Project Grant  , TU-LANGUAGE made to Bilkent University . A portion of this work was done while the author was visiting Computing Research Laboratory at New Mexico State University  . The author thanks Lauri Karttunen of Xerox Research Centre Europe  , Grenoble for making available XRCE Finite State Tools  . 
References
Steven Abney .  1996 . Partial parsing via finite state cascades . In Proceedings of the ESSLLI'96 Robust
Parsing Workshop.
Salah Ait-Mokhtar and Jean-Pierre Chanod . 1997.
Incremental finite-state parsing . In Proceedings of
ANLP'97, pages 72-79, April.
Ciprian Chelbaand et al 1997 . Structure and estimation of a dependency language model  . In Processings of Eurospeech'97 . 
Jason Eisner .  1996 . Three new probabilistic models for dependency parsing : An exploration  . In Proceedings of the 16th International Conference on Computational Linguistics  ( COLING96 )  , pages 340-345 , August . 

S .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 
c .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . Css .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . Rc---Ccc---CC ss---Sms---NR p-dUnyabank at Urki yedirekt Orh Uk Umetiz leek on omik program Noun Noun Noun Noun Noun Noun Verb AdSAdS@Noun  A3sg   A3sg Prop A3sg   A3sg Pos Past Part A3sg 
Pn on P3sgA3sgP3sgPn on P3sg@Pn on
Nos@Sos@Pn on Nom@Gen@Gen@
Nom @ .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . S1 .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . LS--P 1 m---Rs---SLo-- -0 Ssonu COnesad Isats Oyle Noun Noun Adj Noun Verb Verb Noun Verb 
A3 sgWith@A3 pIPassPastPartPos
P3sgPnonPosA3sgPast
Loc@Gen@P3sgA3sg@

Figure 5: Dependency tree for the second parse Gregory Grefenstette  .  1996 . Light parsing as finite-state filtering . In ECAI '96 Workshop on Extended finite state models of language  . August . 
Timo J ~ irvinen and Pasi Tapanainen .  1998 . Towards an implementable dependency grammar . In Proceedings of COLING/ACL'98 Workshop on Processing Dependency-based Grammars , pages 110 . 
Ronald M . Kaplan and Martin Kay .  1994 . Regular models of phonological rule systems . Computational Linguistics ,  20(3):331-378 , September . 
Lauri Karttunen , Jean-Pierre Chanod , Gregory Grefenstette , and Anne Schiller .  1996 . Regular expressions for language ngineering . Natural
Language Engineering , 2(4):305-328.
Lauri Karttunen .  1998 . The proper treatment of optimality theory in computational linguistics  . In Lauri Karttunen and Kemal Oflazer , editors , Proceedings of the International Workshop on FiniteState Methods in Natural Language Processing - 
FSMNLP , June.
Kimmo Koskenniemi , Pasi Tapanainen , and Atro Voutilainen .  1992 . Compiling and using finite-state syntactic rules . In Proceedings of the 14th International Conference on Computational Linguistics  , COLING-92 , pages 156-162 . 
Kimmo Koskenniemi .  1990 . Finite state parsing and disambiguation . In Proceedings of the 13th International Conference on Computational Linguistics  , COLING'90 , pages 229-233 . 
John Lafferty , Daniel Sleator , and Davy Temperley .  1992 . Grammatical trigrams : A probabilistic model of link grammars  . In Proceedings of the 1992 AAAI Fall Symposium on Probablistic Approaches to Natural Language  . 
Bong Yeung Tom Lai and Changning Huang . 1994.
Dependency grammar and the parsing of Chinese sentences  . In Proceedings of the 1994 Joint Conference of 8th ACLIC and 2nd PaFo Col . 
Dekang Lin .  1996 . On the structural complexity of natural anguage sentences  . In Proceedings of the 16th International Conference on Computational
Linguistics ( COLING96).
Igor A . Mel~uk .  1988 . Dependency Syntax : Theory and Practice . State University of New York Press . 
Mehryar Mohri , Fernando Pereira , and Michael Riley .  1998 . A rational design for a weighted finite-state transducer library  . In Lecture Notes in Computer Science , 1 . ~36 . Springer Verlag . 
Kemal Oflazer .  1993 . Two-level description of Turkish morphology . In Proceedings of the Sixth Conference of the European Chapter of the Association for Computational Linguistics  , April . A full version appears in Literary and Linguistic Computing  , Vol . 9 No . 2, 1994 . 
Jane J . Robinson .  1970 . Dependency structures and transformational rules . Language , 46(2):259-284 . 
Emmanuel Roche .  1997 . Parsing with finite state transducers . In Emmanuel Roche and Yves Schabes , editors , FiniteState Language Processing , chapter 8 . The MIT Press . 
Daniel Sleator and Davy Temperley .  1991 . Parsing English with a link grammar . Technical Report CMU-CS-91-196 , Computer Science Department , 
Carnegie Mellon University.
Pasi Tapanainen and Timo J ~ rvinen .  1997 . A nonprojective dependency parser . In Proceedings of
ANLP'97, pages 64-71, April.
DenizY/ir et .  1998 . Discovery of Linguistic Relations Using Lexical Attraction  . Ph . D . thesis , Department of Electrical Engineering and Computer Science  , Massachusetts Institute of Technology . 

