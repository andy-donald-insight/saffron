An Algorithm for Simultaneously Bracketing Parallel Texts 
by Aligning Words
Dekai Wu

Department of Computer Science
University of Science & Technology
Clear Water Bay , Hong Kong
dekai@cs , ust.hk

We describe a grammarless method for simultaneously bracketing both halves of a parallel text and giving word alignments  , assuming only a translation lexicon for the language pair  . We introduce inversion-invariant transduction grammars which serve as generative models for parallel bilingual sentences with weak order constraints  . Focusing on Wans-duction grammars for bracketing , we formulate a normal form , and a stochastic version amenable to a maximum -likelihood bracketing algorithm  . Several extensions and experiments are discussed . 
1 Introduction
Parallel corpora have been shown to provide an extremely rich source of constraints for statistical analysis  ( e . g . , Brown et al 1990 ; Gale & Church 1991 ; Gale et al 1992 ; Church 1993 ; Brown et al 1993 ; Dagan et al 1993 ; Dagan & Church 1994 ; Fung&Church 1994 ; Wu&Xia 1994 ; Fung & McKeown 1994) . Our thesis in this paper is that he lexical information actually gives sufficient information to extract not merely word alignments  , but also bracketing constraints for both parallel texts  . 
Aside from purely linguistic interest , bracket structure has been empirically shown to be highly effective at constraining subsequent training of  , for example , stochastic contextfree grammars ( Pereira & ~1992 ; Black et al 1993) . Previous algorithms for automatic bracketing operate on monolingual texts and hence require more grammatical constraints  ; for example , tactics employing mutual information have been applied to tagged text  ( Magerumn & Marcus 1990 )  . 
Algorithms for word alignment attempt to find the matching words between parallel sentences  .   1 Although word alignments are of little use by themselves  , they provide potential anchor points for other applications  , or for subsequent learning stages to acquire more interesting structures  . Our technique views word alignment 1 Word matching is a more accurate rm than word alignment since the matchings may cross  , but we follow the literature . 
and bracket annotation for both parallel texts as an integrated problem  . Although the examples and experiments herein are on Chinese and English  , we believe the model is equally applicable to other language pairs  , especially those within the same family ( say Indo-European )  . 
Our bracketing method is based on a new formalism called an inversion  . invariant transduction grammar . By their nature inversion-invariant transduction grammars over generate  , because they permitoo much constituent-ordering freedom  . Nonetheless , they turn out to be very useful for recognition when the true grammar is not fully known  . Their purpose is not to flagungrammatical in -pots  ; instead they assume that he inputs are grammatical  , the aim being to extract structure from the input data  , in kindred spirit with robust parsing . 
2 Inversion-Invariant Transduction

A Wansduction grammar is a bilingual model that generates two output streams  , one for each language . The usual view of transducers a having one input stream and one output stream is more appropriate for restricted or deterministic fnite-state machines  . Although finite-state transducers have been well studied  , they are insufficiently powerful for bilingual models  . The models we consider here are nondeterministic models where the two languages ' role is symmetric  . 
We begin by generalizing transduction to contextfree form  . In a contextfree transduction grammar , terminal symbols come in pairs that ~ are emitted to separate output streams  . It follows that each rewrite rule emits not one but two streams  , and that every nonterminal stands for a class of derivable substring pairs  . For example , in the rewrite rule
A ~ Bx/yCz/e the terminal symbols z and z are symbols of the language Lx and are emitted on stream  1  , while the terminal symboly is a symbol of the language  L2 and is emitted on stream 2  . This rule implies that z/y must be a valid entry in the translation lexicon  . A matched terminal symbol pair such as z/y is called a couple  . As aspe , Aal case , the null symbole in either language means that no output 











NP VP
Prep NP
ProIDet Class NN
Mod N\[NN PP
VV\[VVNNIVP PP
V\]AdvV
I/~I you/f$~-*for/~~.book/n
Figure 1: Example IITG.
token is generated . We call a symbol pair such as x/eanLl -singleton  , and elyan L2-singleton . 
We can employ contextfree transduction grammars in simple attempts at generative models for bilingual sentence pairs  . For example , pretend for the moment that the simple ttans due tion grammar shown in Figure  1 is a contextfree transduction grammar , ignoring the ~ symbols that are in place of the usual ~ symbols  . This grammar generates the following example pair of English and 
Chinese sentences in translation : (1) a . \[ I\[\[took\[abook\]so\]vp\[foryon\]~ \] vp\]sb  . \[~i\[\[~T\[--*W\]so\]w\[~\]~\]vt , \]s Each instance of a nonterminal here actually derives two subsltings  , one in each of the sentences ; these two substrings are translation counterparts  . This suggests writing the parse trees together : ( 2 ) ~\[\[ took/~Y\[a/~d~: book/1\[\]so \] vp\[for/~\[~you/~\]pp\]vv\]s The problem with contextfree transduction granunars is that  , just as with finite-state ransducers , both sentences in a translation pair must share exactly the same gram-matic ~ d structure  ( except for optional words that can be handled with lexical singletons  )  . For example , the following sentence pair with a perfectly valid  , alternative Chinese translation cannot be generated :  ( 3 ) a . \[ I\[\[took\[abook\]so\]vp\[for you\]v ~\] vP\]sb  . \[~\[\[~?~\] ~\[~ T\[--~\]so\]vt , \] vP\]s We introduce the device of an inversion-invafiant transduction grammar  ( IITG ) to get around the inflexibility of contextfree txansduction grammars  . Productions are interpreted as rewrite rules just as with contextfree transduction grammars  , with one additional proviso : when generating output for stream  2  , the constituents on a rule's righthand side may be emitted either left-to-right  ( as usual ) or right-to-left ( in inverted order )  . We use instead of --~ to indicate this . Note that inversion is permitted at any level of rule expansion  . 
With this simple proviso , the transduction grammar of Figure 1 straightforwardly generates sentence-pair ( 3 )  . 
However , the IITG's weakened ordering constraints now also permit the following sentence pairs  , where some constituents have been reversed : ( 4 ) &*\[ I\[\[for youlpp\[\[a bookl~p tooklvp\] vp\]sb  .  \ [~  \[\[~?~\]1~ \[~ tT\[--:*:It\]so\]w\]vp\]s ( 5 ) a . *\[\[\[ yonfor\]re\[\[abook\]so took\]w\]vp I \] sb  . *\[~\[\[~\] rp\[\[tl\[:~--\]so ~ T\] vP\]VP\]SAs a bilingual generative linguistic theory  , therefore , IITGs are not well-motivated ( at least for most natural language pairs )  , since the majority of constructs do not have freely revexsable constituents  . 
We refer to the direction of a production's L2 constituent ordering as an orientation . It is sometimes useful to explicitly designate one of the two possible orientations when writing productions  . We do this by distinguishing two varieties of concatenation perators on string-pairs  , depending on timode atation . Tim operator\[\] performs the " usual " paitwise concatenation so that \[ AB\] yields the string -pair  ( C x , C2) where Cx =  A1Bx and (52 = A2B2 . But the operator 0 concatema ~ constituents on output stream 1 while reversing the monstream 2  , so that Ci = AxBx but C2 = B2A2 . For example , the NP . - . DetClass NN rule in the transduction grammar above actually expands to two standard rewrite rules : -  . \[Bet NN\] ( DetClass NN ) Before turning to bracketing , we take note of three lemmas for IITGs ( proofs omitted ) : Lemmal For any inversion-invarian transduction grammar G  , there exists an equivalent inversion-invariant transduction grammar G ' where T  ( G )  =
T ( G '), such that : 1 . lfe ELI(G ) and eEL2(G ) , then G ' contains a single production of the form S'--~e/c  , where S ' is the start symbol of G ' and does not appear on the right hand side of any production of G '  ;  2 . otherwise G ' contains no productions of the form
A ~ e / e.
L emma2 For any inversion-invarian transduction grammar G  , there exists an equivalent inversion-invariant transduction gratrm ~ rG ' where T  ( G ) = T ( G ' )  , T ( G ) = T ( G') , such that the right hand side of any production of G'contains either a single terminal-pair or a list of nonterminals  . 
L emma3 For any inversion-invarian transduction grammar G  , there exists an equivalent inversion transduction grammar G ' where T  ( G ) = T ( G ' )  , such that G ' does not contain any productions of the form A--  , B . 
3 Bracketing Transduction Grammars
For the remainder of this paper , we focus our attention on pure bracketing . We confine ourselves to bracketing constituent categories aten ot differentiated  . Aside from the start symbol S , BTGs contain only one nonterminal symbol , A , which rewrites eitherecursively as a string of A 's or as a single terminal-pair  . In the former case , the productions has the form A~- , A ! where we use A ! to abbreviate A .   .   . A , where the fanout f denotes the number of A's . Each A corresponds to a level of bracketing and can be thought of as demarcating some unspecified kind of syntactic category  .   ( This same " repetitive expansion " restriction used with standard contextfree grammars and transdue tion grammars yields bracketing grammars without orientation ivariauce  . ) A full bracketing transduction grammar of degree f contains A productions of every fanout between  2 and f , thus allowing constituents of any lengthupt of . In principle , a full BTG of high degree is preferable , having the greatest flexibility to a cx~mmdate arbitrarily long matching sequences  . However , the following theorem simplifies our algorithms by allowing us to get away with  degree2 BTGs . I~t ~ we will see how postprocessing restores the fanout flexibility  ( Section 5 . 2) . 
Theorem 1 For any full bracketing transduction grammar T , there exists an equivalent bracketing transduction grammar T ' in normal form where every production takes one of the following forms : 
S ~ e / e
S ~ A
A ~ AA
A~z/y
A ~ ~ :/ e
A ~ ely
ProofByLemmas1 ,  2 , and 3 , we may assume T contains only productions of the form S~-*e/e  , A z/y , A~z/e , A ~-* e/y , and A , --* AA .   .   . A . For proof by induction , we need only show that any full BTGT of degree f  >   2 is equivalent to a full BTGT ' of degree f - 1  . It suffices to show that he production A~- , A ! call be removed without any loss to the generated language  , i . e . , tha ! the remaining productions in T ' can still derive any string-pair derivable by T  ( removing a production cannot increase the set of derivable string-pairs  )  . Let ( E , C ) be any siring-pair derivable from A~A1 , where E is output on stream1 and Constream 2 . Define Ei as the substring of E derived from the ith A of the production  , and similarly define Ci . There are two cases depending on the concatenation rientation  , but ( E , C ) is derivable by T ' in either case . 
In the first case , if the derivation used was A . .-, \[ A !\], thenE = E 1 .   .   . El and C =  C1 .   .   . C1 . Let ( E ', C ') = ( E1 .   .   . E !- x,C1 .   .   . C1-1) . Then ( E' , C ') is derivable from A--~\[A!-I\] , and thus ( E , C ) = ( E~E1 , C  ~ C ! ) is derivable from A~\[AA\]:In the second case  , the derivation used was A--- . A !), and we still have E = E1 .   .   . E ! but now C--CY .   .   . C1 . Now let ( E ', C ") =
A ~ accountable / ~ tJ\[
A , ---+ anthority/~t ~
A ~ finaucia Yl\[#l ~
A.-* secretary / ~
A ~ to / ~
A ~-, wfll\]~
A  ~ Jo
A , -, be Je
A ~ thele
Figure 2: Some relevant lexical productions.
.  . E1-1, C1-1 .   .   . C 1 ) . ~( E ' , C ") is derivable (~ A--*(A!-I) , and thus ( E , e)-(E'E ! , C ! C ") is derivable from A - - - , ( AA ) . \[74 Stochastic Bracketing Transduction

In a stochastic BTG ( SBTG ) , each rewrite rule has a probability . Leta ! denote the probability of the A -production with f an outdegree f  . For the remaining ( lexical ) pro-dnctions , we use b(z , y ) to denote P\[A~z/vlA\] . The probabiliti ~ obey the constraint that
Ea !+ Eb(z'Y ) = 1l ~?, Y
For our experiments we employed a normal form transduction grammar  , so a !=0 for all f#2 . The A-productions used were :
A ~-* AA
Ab(&~)z/v
Ab~Ox/e
A ~% ~) e/V for all z , y lexical translations for all z English vocabulary for all y Chinese vocabulary Theb  ( z , y ) distribution actually encodes the EnglishChinese translation lexicon  . As discussed below , the lexicon we employed was automatically learned from a parallel corpus  , giving us the b(z , y ) probabilities directly . The latter two singleton forms permit any word in either sentence to be unmatched  . A smalle-constant is chosen for the probabilities b  ( z , e ) and b(e , y ) , so that the optimal bracketing resorts to these productions only when it is otherwise impossible to match words  . 
With BTGs , to parse means to build matched bracketings for senmnce-pairs rather than sentences  . Tiffs means that the adjacency constraints given by the nested levels must be obeyed in the bracketings of both languages  . The result of the parse gives bracketings for both input sentences  , as well as a bracket algnment indicating the corresponding brackets between the sentences  . The bracket alignment includes a word alignment as a byproduct  . 
Consider the following sentence pair from our corpus : will / ~\ [ #~ 
The/cAuthority/~t ~ belt accountabl ~ the J ~

Figure 3: Bracketing tree.
Secretary/--~(6) a . The Authority will be accountable to the Financial Secretary  . 
b . If t ~ l~t ' ~ l~t ~ t ~ o
Assume we have the productions in Figure 2 , which is a fragment excerpted from our actual BTG  . Ignoring capitalization , an example of a valid parse that is consistent with our linguistic ideas is:  ( 7 ) \[\[\[ The/eAuthority/~t ~\]\[ will / ~ ( \[ be&accountable/~t ~\]\ [ to/~\[the/?\[\ [ Financial / ~ l ~ 
Secretary / ~\]\]\]\])\]\] J.\]
Figure 3 shows a graphic representation f the same brac & et ing  , where the 0 level of lrac , keting is marked by the horizontal line . The English is read in the usual depth-first left -to-right order  , but for the Chinese , a hori-zontaline means the right subtree is traversed before the left  . 
The ( ) notation concisely displays the common structure of the two sentences  . However , the bracketing is clearer if we view the sentences monolingually  , which allows us to inver the Chinese constituents within the  0 so that only \[\] brackets need to appear . .
(8) a . \[\[\[ The Authority\]\[will\[\[be accountable\]\[to\[the\[\[Financial Secretary  \]\]\]\]\]\]1  . \]k\[\[\[\["~ , ' ~\]\[~ t'\[\[I~\[\[~~\]\]\]\]\]\[~ . l \]\]\]\] o\]In the monolingual view , extra brackets appear in one language whenever there is a singletonithe other language  . 
If the goal is just to obtain ~ for monolingual sentences  , the extra brackets can be discarded aft ~ parsing  :   ( 9 )  \[\[\[ ~  , ~\]\[~R\[~\[Igil~~\]\]\[~ttt\]\]\ ] o \] The basis of the bracketing strategy can be seen as choosing the bracketing that maximizes the  ( probabilistically weighted ) number of words matched , subject othe BTG representational constraint , which has the effect of limiting the possible crossing patterns in the word alignment  . A simpler , related idea of penalizing distortion from some ideal matching pattern can be found in the statistical translation  ( Brown et al 1990 ; Brown et al 1993 ) and word alignment ( Dagan et al 1993 ; Dagan & Church 1994) models . Unlike these models , however , the BTG aims m model constituents ruc-ture when determining distortion penalties  . In particular , crossings that are consistent with the constituent tree structure are not penalized  . The implicit assumption is that core arguments of frames remain similar across languages  , and tha ! core arguments of the same frame will surface adjacently  . The accuracy of the method on a particular language pair will therefore depend upon the extent to which this language universals hypothesis holds  . 
However , the approach is robust because if the assumption is violated  , damage will be limited to dropping the fewest possible crossed word matchings  . 
We now describe how a dynzmic-programming parser can compute an optimal bxackcting given a sentence-pair and a stochastic BTG  . In bilingual parsing , just as with ordinary monolingual parsing , probabilizing the grammar imum likelihood parse . Our algorithm is similar in spirit to the recognition algorithm for HMMs  ( Viterbi 1967 )  . 
Denote the input English sentence by el , ?? .   , er and the corresponding input Chinese sentence by el  ,   .   .   . , cv . 
As an abbreviation we write co . ., for the sequence of words eo+l,e,+2, .   .   . , e  ~, and similarly for c ~ . .~ . Let 6 . tu ~ = maxP\[e , . .t/e ~ . .~\] be the maximum probability of any derivation from A that__successfully parses both substrings es  . .t and ? u . .v . The best parse of the sentence pair is that with probability  60  , T , 0y . 
The algorithm computes 6 o , T , 0 , V following the recur-fences below . 2 The time complexity of this algorithm is O ( Ta Va ) where T and V are the lengths of the two sen ~ . 
1 . Initialization 6 t -- l , t , v - - l , v " -2 . Recursion 6 ttuv " - -
Ottuu - - " where l < t < T b(e ,  /~ ) , 1 < v < V maxr / ~\[\] 60 1 tstuv~stuvJ .   , 6\[\] 61 1s~stuv~stuv6\[\] uv=maxa 26 , suu 6stuv s < S < ~ u < V < va\[lstuv " - axg smax 6sSut  . r6$tUvs<S < tu<U < vv\[\]--argUmax 6 , s uu6stuv sgut~s < S < tu < U < v 6J~uv --max a 2   6sSU  ~  6StuU s <$ < t u < U < v * r ! ~ u v = argsmax 6  , SV ~6 Stuffs < S < t u < U < v
V ~ uv = argUmax 6, su~6S , uV s < S < t u < V < v 3 . Reconstrm:tion Using 4-tuples to name each node of the parse tree , initially set qx = (0 , T ,  0 , V ) to be the root . The remaining descendants in the optimal parse tree are then given recursively for any q =  ( s , t , u , v ) by:
LEFT'""s ~ r\[\]uv\[\]~/~q )  =  (  ' \[~ '"~' '\[\] ''"~ ) f if 0 , t ~ =\[\] mGHT(q ) = t , 
LEF r '"" so "0 v0v "
RIGHT(q ) = ( a!~uv , t , u , v ~ u ~)) if O , tuv = 0 Several additional extensions on this algorithm were found to be useful  , and are briefly described below . Details are given in Wu (1995) . 
2We are gene~!izing argmax as to allow arg to specify the index of interest  . 
4.1 Simultaneous segmentation
We often find the same concept realized using different numbers of words in the two languages  , creating potential difficulties for word alignment  ; what is a single word in English may be realized as a compound in Chinese  . Since Chinese text is not orthographically separated into words  , the standard methodology is to first preproce ~ inputexts through a segmentation module  ( Chiang et al 1992 ; Lin et al 1992 ; Chang & Chert 1993 ; Lin et al 1993 ; Wu&Tseng 1993 ; Sproat et al 1994) . However , this se-rionsly degrades our algorithm's performance  , since the the segmenter may encounter ambiguities that are unresolvable monolingually and thereby introduc errors  . 
Even if the Chinese segmentation is acceptable moaolin-gually  , it may not agree with the division of words present in the English sentence  . Moreover , conventional compounds are frequently and unlmxlictably missing from translation lexicons  , and this can furllu ~ degrade perfor-

To avoid such problems we have extended the algorithm to optimize the segmentation f the Chinese sentence in parallel with the ~ tinglm ~ : ess  . Note that this treatment of segmentation does not attempt to address the open linguistic question of what constitutes a Chinese " word "  . Our definition of a correct " segmentation " is purely task-driven : longer segments are desirable if and only ffno compositional translation is possible  . 
4.2 Pre/post-positional biases
Many of the bracketing errors are caused by singletons  . 
With singletons , there is no crosslingual discrimination to increase the certainty between alternative brackeaings  . 
A heuristic to deal with this is to specify for each of the two languages whether prepositions or postpositions more common  , where " preposition " here is meant not in the usual part-of-speech sense  , but rather in a broad sense of the tendency of function words to attach left or right  . This simples wategcm is effective because the majority of unmatched singletons are function words that counterparts in the other language  . This observation holds assuming that the translation lexicon's coverage is reasonably good  . For both English and Chinese , we specify a prepositional bias , which means that singletons are attached to the right whenever possible  . 
4.3 Punctuation constraints
Certain punctuation characters give strong constituency indications with high reliability  . " Perfect separators " , which include colons and Chinese full stops , and " pet-feet delimiters " , which include parentheses and quotation marks , can be used as bracketing constraints . We have extended the algorithm to precluded hypotheses that are inconsistent with such constraints  , by initializing those entries in the DP table corresponding to illegal subhypotheses with zero probabilities  , These entries are blocked from recomputation during the DP phase  . As their probabilities always remain zero , the illegal bracketing scannever participate in any optimal bracketing  . 
248 5 Postprocessing 5 . 1 A Singleton-Rebalancing Algorithm We now introduce an algorithm for further improving the bracketing accuracy in cases of singletons  . Consider the following bracketing produced by the algorithm of the previou section :  ( 10 ) \[ t The/~\[\[Authority/~f ~\[ wilg ~ ad ( \[be/~accountable/~t ~\]\ [ to the/~\[~/~\[ Financial/~i ~ 
Seaetary/-nl\]\]\])\]ll\]Jo\]
The prepositional bias has already correctly restricted the singleton " T be/d'to attach to the right  , but of course " The " does not belong outside the rest of the sentence  , but rather with " Authority " . The problem is that singletons have no discriminative power between alternative bracket matching s--they only contribute to the ambiguity  . However , we can minimize the impact by moving singletons as deep as possible  , closer to the individual word they precede or succeed  , by widening the scope of the brackets immediately following the singleton  . In general this improves precision since wide -scope brackets are less constraining  . 
The algorithm employs are balancing strategy rem -niscent of balanced-tree structures using left and right rotations  . Aleftrotation changes a ( A ( B C ) ) structure to a ( ( AB ) C ) structure , and vice versa for a right rotation . 
The task is complicated by the presence of both \[ \] and  0 brackets with both LI-and L2-singletons   , ince each combination presents different interactions  . To be legal , a rotation must preserve symbol order on both output streams  . However , the following lemma shows that any subtree can always be rebalanced at its root if either of its children is a singleton of either language  . 
Lenuna 4 Let x be a L 1 singleton , y be a L2 singleton , and A , B , C be arbitrary constituent subtrees . Then the following properties hold for the\[\] and  0 operators : ( Associativity ) \[A\[BC\]\]=\[\[AB\]C\] ( A ( B C ) ) =  ( ( AB ) C )   ( L , -singleton bidirectionality ) lax\]~-- ( A  ~ )  \[ , A \]: ( xA )   ( L2-singleton flipping commutativity ) \[ Av \] = ( vA ) \[ uA \] = ( Av )   ( L1-singleton rotation properties ) \[ z ( A B ) \] ~-  ( x ( A B ) ) ~--  ( ( zA ) B )  ~-  ( \[xA\]B )   ( x\[aB\] ) ~---\[ x\[AB\]\]~---\[\[zA\]B\] . ~ \[ ( xA ) B \]\ [ ( A B ) x \] = ( ( AB ) ~ )  =  ( A ( B  ~ ) ) =  ( A\[B ~\] )   ( lAB\]x ) ~-\[\[ AB\]x\]=\[A\[Bx\]\]~---\[A ( B x ) \]  ( L~-singleton rotation properties ) \[ v ( A B ) \] =  ( ( AB ) v )  =  ( A ( Bv ) ) =  ( AtvB\] )   ( y\[AB\] ) ~--\[\[ AB\]y\]~\[A\[By\]\]~--\[A ( yB ) \] \[ ( A B ) v\] , ~  ( y ( A B ) ) ~  ( ( vA ) B )  ~-  ( M y \] B )   ( \[ AB\]v ) ~\[ v\[AB\]\]=ttvA\]B\]=\[ ( Av ) B \] The method of Figure 4 modifies the inputree to attach singletons as closely as possible to couples  , but remaining consistent with the inputree in the following sense : singletons cannot " escape " their inmmdiately surrounding brackets  . The key is that for any given subtree , if the outermost bracket involves a singleton that should be rotated into a subtree  , then exactly one of the singleton rotation properties will apply  . The method proceeds depth-first , sinking each singleton as deeply as possible . 
For example , after ebalm ~ cing , sentence ( 10 ) is bracketed as follows: ( 11 ) \[\[\[\[ The/eAuthority/~\]\[witV~1t ' ( \[ be/eaccountable/~tft\]\[othe/~\[dFBJ\[ Fhumciai/ll~'i ~ 
Secretary/--~111) 111 Jo\]5 . 2 Flattening the Bracketing Because the BTG is in normal form  , each bracket can only hold two constituents . This improves parsing efficiency , but requires over commiUnent since the algorithm is always forced to choose between  ( A ( B C ) ) and ( ( AB ) C ) statures even when no choice is clearly better . In the worst case , both senteau : ~ might have perfectly aligned words  , lending no discriminative lverage whatsoever to the bfac~ter  . This leaves a very large number of choices : if both sentences are of length i = m  , then the l ~ ~ ( 21 ) 1 possible lracJ w~ngs with fanout 2 , none of which is better justitied than any other . Thus to improve accuracy , we should reduce the specificity of the bracketing's commitment i such cases  . 
We implement this with another postprocessing stage  . 
The algorithm proceeds bottom-up , elimiDming a smalay brackets as possible , by making use of the associa fiv-ity equivalences \[ ABel=\[A\[BC\]\]=\[lAB\]C\] and 
SINK-SINGLETON ( node )   1 ff node is not a leaf 2 if a rotation property applies at node 3 apply the rotation to node 4 ch // d ~-- the child into which the singleton 5 was rotated 6 SINK-SINGLETON ( chi/d ) 
RE~AL~CE-aXEE ( node ) 1 if node is not a leaf2 REBAL ANCE-TREE ( left-child\[node\] ) 3 REeALANCE-TREE ( right-child\[node\] ) 4 S~K-SXNGI . ,E'ro~ ( node ) Figure 4: The singleton rebalancing schema . 
2 49 \[ These/~arrangements / ~ will/eef ~ enhance / ~ q  ~ our / ~  ( \[ d ~ Jability / ~ ; 0\] [ tok dEt ~ maintain/~t ~ monetary/~t s ab i l i ty /~ in the years to come/e \]  ) do\]\[The/eAuthority / ~\] ~ will / ~ ( \[ be/eaccountable/gt ~\]\ [ to the/eelm Financial/l~i ~ Secretary / ~\]  ) Jo\]\[They/~t ! l~J ( are/eright/iE~d-l-Jfftok do/~e/~so/e ) io \]\[ ( \[ Evenk more ~ important / l ~\]\ [ Jehowever / ~_ \]  ) \[Jee/~ , is/~to make the very best of our/ee/~ffl ~ own/~$~e/~Jtalent/X ~\] J  . \] hope/ee/o ! ~ lemployers/l\[~l ~ will/~make full/edg~rj'~use / ~\[ offe those/\]Jl ~ a ~___\ ]   ( (\[ dJfJ-V who /&\] \[ have aequired/e e /$~ new/ ~i skills/tS~l ~ \]  ) \[ through/L~i ~ t thisJ ~ l programme / ~ l ' ~\] ) J . \] have / ~ oat/elength / ~ l ( on/ehow/~g ~ we/~e/~ll ~ ) \[ can FaJJ ) ~ boostkd ~ iltour / ~: ~ e / ~ prosperity/$~\]Jo \] Figure  5: Bracketing/alignment ouput examples . ( ~= unrecognized input token . )( ABC ) = ( A(BC )) = (( AB)C) . Tim singleton bidi-rectionality and flipping eommutativity equivalences  ( see Lemma 4 ) are also applied , whenever they render the as-sociativity equivalences applicable  . 
The final result after flattening sentence ( 11 ) is as follows: ( 12 ) \[ The/eAuthority / ~\] ~ will/g ~' ( \[ be/eaccountable/J~t J!\[\] to tl ~/ eelm Financial/l ~ 
Secretary /--~1) jo \] 6 Experiments
Evaluation methodology for bracketing is controversial because of varying perspectives on what the " gold standard " should be  . We identify two prototypical positions , and give results for both . One position uses a linguistic evaluation criterion  , where accuracy is measured against some theoretic notion of constituent sructure  . The other position uses a functional evaluation criterion  , where the " correctness " of a bracketing depends on its utility with respect to the application task at hand  . For example , here we consider a bracket-pair functionally useful if it correctly identifies phrasal translations --- especially where the phrases in the two languages are not compositionally derivable solely from obvious word translations  . Notice that in contrast , the linguistic evaluation criterion is insensitive to whether the bracketings of the two sentences match each other in any semantic way  , as long as the monolingual bracketings in each sentence are correct  . In either case , the bracket precision gives the proportion of found br ~ & ets that agree with the chosen correctness criterion  . 
All experiments reported in this paper were performed on sentence pairs fom the HKUST English Chinese Parallel Bilingual Corpus  , which consists of governmental transcripts ( Wu 1994 )  . The translation lexicon was automatically learned from the same corpus via statistical sentence alignment  ( Wu 1994 ) and statistical Chinese word and collocation extraction  ( Fung&Wu 1994 ; Wu&Fung 1994) , followed by an EM word-translation learning procedure  ( Wu&Xia 1994 )  . The translation lexicon contains an English vocabulary of approximately  6  , 5 00 words and a Chinese vocabulary of approximately 5  , 500 words . The mapping is many-to-many , with an average of 2 . 25 Chinese translations per English word . 
The translation accuracy is imperfect ( about 86% percent weighted precision )  , which turns out to cause many of the bracketing errors  . 
Approximately 2 , 0 00 sentence pairs with both English and Chinese lengths of  30 words or less were extracted from our corpus and bracketed using the algorithm described  . Several additional criteria were used to filter out unsuitable sentence pairs  . If the lengths of the pair of sentences differed by more thm la  2:1 ratio , the pair was rejected ; such a difference usually arises as the result of an earlier error in automatic sentence alignment  . 
Sentences containing more than one word absent from the translation lexicon were also rejected  ; the bracketing method is not intended to be robust against lexicon in a de-quacies  . We also rejected sentence pairs with fewer than two matching words  , since this gives the bracketing algorithm nodiso ' iminative lverage  ; such pairs ~ c ~ ounted for less than 2% of the input data . A random sample of the b~keted sentence pairs was then drawn  , and the bracket precision was computed under each criterion for correctness  . Additional examples are shown in Figure 5 . 
Under the linguistic riterion , the monolingual bracket precision was 80 . 4% for the English sentences , and 78 . 4% for the Chinese sentences . Of course , monolinguai grammar-based bracketing methods can achieve higher precision  , but such tools assume grammar resources that may not be available  , such as good Chinese granuna ~ . 
Moreover , if a good monolingual bracketer is available , its output can easily be incorporated in much the same way as punct n ~ ion constraints  , thereby combining the best of both worlds . Under the functional criterion , the parallel bracket precision was 72 . 5% , lower than the monolingual precision since brackets can be correct in one language but not the other  . Grammar-based bracketing methods cannot directly produce results of a comparable nature  . 
250 7 Conclusion
We have proposed a new tool for the corpus linguist's arsenal : a method for simultaneously bracketing both halves of a parallel bilingual corpus  , using only a word translation lexicon . The method can also be seen as a word alignment algorithm that employs a realistic distortion model and aligns consituents as well as words  . 
The basis of the approach is a new inversion -invariant transduction grammar formalism  . 
Various extension strategies for simultaneous segmentation  , positional biases , punctuation constraints , ingle-tonrebalancing , and bracket flattening have been introduced . Parallel bracketing exploits a relatively untapped source of constraints  , in that parallel bilingual sentences are used to mutually analyze each other  . The model nonetheless retains a high degree of compatibility with more conventional monolingual formalisms and methods  . 
The bracketing and alignment of parallel corpora can be fully automatized with zero initial knowledge resources  , with the aid of automatic procedures for learning word translation lexicons  . This is particularly valuable for work on languages for which online knowledge resources are relatively scarce compared with English  . 

I would like to thank Xuanyin Xia , Eva Wai-Man Foug , 
Pascale Fung , and Derick Wood.
References
BLACK , EZRA , ROGERGARSIDE , & GEoF~EYI~(eds . ) . 
1993 . Statistically-driven computer grammars of English : The IB ~ aster approach  . Amsterdam : Editions Rodopi . 
BROWN , Pt ~ reRF . , JOHNCOCKE , STEPHENA . D~1APt~rgA,VINCENTJ . ~t ~ rttA , FR ~ ERICKJ ~ LnqWK , JOHND . 
~, ROBERTL . MERCER , & PAULS . RoossiN.
1990 . A statistical approach to machine translation . Computational Linguistics , 16(2):29-85 . 
BROWN , PETERE , STEPHENA . DIKLAP mTxA,VINCENTJ . DEL-LAPteTgA , & ROBERTL . M~CER .  1993 . The mathematics of statistical machine translation : Parameter estimation  . 
Computational Linguistics , 19(2):263-311.
CHANG , CHAO-HUANG & CHE~G-DERCHEN .  1993 . HMM-based part-of-speech tagging for Chinese corpora  . In Proceedings of the Workshop on Very Large Corpora  ,  40-47 , 
Columbus , Ohio.
CHIANG , TUNG-HUI , JING-SHINCHANG , MING-YULIN , & KEH-YIHSu .  1992 . Statistical models for word segmentation and unknown resolution  . In Proceedings of ROCLING-92, 121-146 . 
CHURCH,~W .  1993 . Char-align : A program for aligning parallel texts at the characterlevel  . In Proceedings of the 31st Annual Conference of the Association for Computational Linguistics  ,  18 , Columbus , OH . 
DAGAN , IDO&KENNETHW . CHURCH .  1994 . Termight : Identifying and translating technical terminology  . In Proceedings of the Fourth Conference on Applied Natural Language Processing  ,  34-40 , Stuttgart . 
DAGAN , IDO , KENNETHW . CHURCH , & W\[\]\[JJ ~ A . GAL ~ . 
1993 . Robust bilingual word alignment for machine aided translation  . In Proceedings of the Wor~hop on Very Large
Corpora , 18, Columbus , OH.
FUNO , PASCALE & KENNETHW . CHURCH .  1994 . Kvec : A new approach for aligning parallel texts  . In Proceedings of the Fifteenth International Conference on Computational 
Linguistics , 1096-1102, Kyoto.
FUNG , PASCALE & KATI~J ~ McKE oWN .  1994 . Aligning noisy parallel corpora cross language groups : Word pair feature matching by dynamic time warping  . In AMTA-94 , Association for Machine Translation i the Americas  ,  8188 , Columbia , Maryland . 
FUNO , PASCALE & DEKAI Wu .  1994 . Statistical augmentation of a Chinese machine -readable dictionary  . In Proceedings of the Second Annual Workshop on Very Large Corpora  ,  69-85 , Kyoto . 
GALE , Wn H~MA . & ~ W . CHURCH .  1991 . A program for aligning sentences in bilingual corpora  . In Proceedings of the 29th Annual Conference of the Association for Computational Linguistics  ,  177-184 , Berkeley . 
GALE , Wn HAMA . , KENNETHW . CHURCH , & DAVIDY AROWSKY .  1992 . Using bilingual materials to develop word sense disambiguatlon methods  . In Fourth International Conference on Theoretical nd Methodological Issues in Machine Translation  ,  101-112 , Montreal . 
I ~, M~o-Yu , Tt~o-Hta~o , & K~-YmSu . 1993.
A preliminary study on unknown word problem in Chinese word segmentation  . I Proceedings of ROCLING-93, 119-141 . 
LIN , YI-CHUNG , TUNG-HUICHIANG , & KEH-YmSU .  1992 . 
Discrimination oriented pm babilistic tagging . In Proceedings of ROCLING-92, 85-96 . 
MAGERMAN , DAVIDM . & ~ L p . MARCUS .  1990 . Parsing a natural language using mutual information statistics  . In Proceedings of AAAI-90 , Eighth National Conference on
Artificial Intelligence , 984--989.
PEREIRA , FEXNANDO & YVESSCHABES .  1992 . Inside-outsider e , estimation from partially bracketed corpora . In Proceedings of the 30th Annual Conference of the Association for Computational Linguistic:  ,  128-135 , Newark , DE . 
SPROAT , RICHARD , CHnJNSHItl , WnIJAMGALE , & N . CHANG . 
1994 . A stochastic word segmentation algorithm for a Mandarin text-to-speech system  . In Proceedings of the 32nd Annual Conference of the Association for Computational Linguistics  , Lag Cruces , New Mexico . To appear . 
VITERBI , AND REWJ .  1967 . Error bounds for convolutional codes and an asymptotically optimal decoding algorithm  . 
IEEE Transactions on Information Theory ,  13:260-269 . 
WU , DEKAL 1994 . Aligning a parallel EnglishChinese corpus statistically with lexical criteria  . In Proceedings of the 32ndAnnual Conference of the Association for Computational Linguistics  ,  80-87 ,  \[ , as Cruces , New Mexico . 
WU , DEKAI , 1995 . Stochastic nversion transduction grammars and bilingual parsing of parallel corpora  . In preparation . 
WU , DEKAI & PASCALE FUNG .  1994 . Improving Chinese tokenization with linguistic filters on statistical lexical acquisition  . In Proceedings of the Fourth Conference on@plied Natural Language Processing  ,  180-181 , Stuttgart . 
Wu , D ~, AI&XUANTINXIA .  1994 . Learning an EnglishChinese lexicon from a parallel corpus  . In AMTA-94 , Association for Machine Translation i the Americas  ,  206-213 , Columbia , Maryland . 
Wu , ZIMIN&GWYI~THTSI~G .  1993 . Chinese text segmentation for text retrieval : Achievements and problems  . 
Journal of The American Society for Information Science  ,  44(9):532-542 . 

