Similarity-Based Methods For Word Sense Disambiguation 
Ido Dagan
Dept . of Mathematics and
Computer Science
Bar Ilan University
Ramat Gan 52900, Israel
dagan?macs , biu.ac.il
Lillian Lee Fernando Pereira
Div . of Engineering and AT&T Labs-Research
Applied Sciences 600 Mountain Ave.
Harvard University Murray Hill , NJ 07974 , USA Cambridge , MA 01238 , USA pereira ? research , att . cornllee ? eecs , harvard , edu
Abstract
We compare four similarity-based sti-mation methods against backoff and maximum-likelihood estimation methods on a pseudoword sense disambiguation task in which we controlled for both unigram and bigram frequency  . The similarity-based methods perform up to 40% better on this particular task . We also conclude that events that occur only once in the training set have major impact on similarity-based estimates  . 
1 Introduction
The problem of data sparseness affects all statistical methods for natural language processing  . Even large training sets tend to misrepresent low -probability events  , since rare events may not appear in the training corpus at all  . 
We concentrate here on the problem of estimating the probability of unseen word pairs  , that is , pairs that do not occur in the training set . Katz's backoff scheme ( Katz ,  1987) , widely used in bigram language modeling , estimates the probability of an unseen bigram by utilizing unigram estimates  . This has the undesirable result of assigning unseen bigrams the same probability if they are made up of unigrams of the same frequency  . 
Class-based methods ( Brown et al , 1992;
Pereira , Tishby , and Lee , 1993; Resnik ,  1992 ) cluster words into classes of similar words , so that one can base the estimate of a word pair 's probability on the averaged cooccurrence probability of the classes to which the two words belong  . However , a word is therefore modeled by the average behavior of many words  , which may cause the given word's idiosyncrasies to be ignored  . For instance , the word " red " might well act like a generic ol or word in most cases  , but it has distinctive cooccurrence patterns with respect to words like " apple  , "" banana , " and so on . 
We therefore consider similarity-based estimation schemes that do not require building general word classes  . Instead , estimates for the most similar words to a word w are combined  ; the evidence provided by word w ' is weighted by a function of its similarity to w  . 
Dagan , Markus , and Markovitch ( 1993 ) propose such a scheme for predicting which unseen cooccurrences are more likely than others  . 
However , their scheme does not assign probabilities . In what follows , we focus on probabilistic similarity-based estimation methods  . 
We compared several such methods , including that of Dagan , Pereira , and Lee ( 1994 ) and the cooccurrence smoothing method of Essen and Steinbiss  ( 1992 )  , against classical estimation methods , including that of Katz , in a decision task involving unseen pairs of direct objects and verbs  , where unigram frequency was eliminated from being a factor  . We found that all the similarity-based schemes performed almost  40% better than backoff , which is expected to yield about 50% accuracy in our experimental setting . Furthermore , a scheme based on the total divergence of empirical dis-significant improvement in error rate over cooccurrence smoothing  . 
We also investigated the effect of removing extremely low-frequency events from the training set  . We found that , in contrast o backoff smoothing , where such events are often discarded from training with little discernible f-fect  , similarity-based smoothing methods suffer noticeable performance degradation when singletons  ( events that occur exactly once ) are omitted . 
2 D is t r ibut iona l S imi la r i ty Mode ls We wish to model conditional probability distributions arising from the coocurrence of linguistic objects  , typically words , in certain configurations . We thus consider pairs ( wl , w2) EVi?V2 for appropriate sets 1/1 and V2 , not necessarily disjoint . In what follows , we use subscript i for the ith element of a pair  ; thus P ( w21 wi ) is the conditional probability ( or rather , some empirical estimate , the true probability being unknown ) that a pair has second element w2 given that its first element is wl ; and P ( wll w2) denotes the probability estimate , according to the base language model , that wl is the first word of a pair given that the second word is  w2  . 
P ( w ) denotes the base estimate for the unigram probability of word w  . 
A similarity-based language model consists of three parts : a scheme for deciding which word pairs require a similarity-based estimate  , a method for combining information from similar words  , and , of course , a function measuring the similarity between words  . We give the details of each of these three parts in the following three sections  . We will only be concerned with similarity between words in  V1  . 
1 To the best of our " knowledge , this is the first use of this particular distribution dissimilarity function in statistical language processing  . The function itself is implicit in earlier work on distributional custering  ( Pereira , Tishby , and Lee ,  1993 , has been used by Tish by ( p . e . ) in other distributional similarity work , and , as suggested by Yo av Freund(p . c . ) , it is related to results of Hoeffding ( 1965 ) on the probability that a given sample was drawn from a given joint distribution  . 
2.1 Discounting and Redistribution
Data sparseness makes the maximum likelihood estimate  ( MLE ) for word pair probabilities unreliable . The MLE for the probability of a word pair ( W l , w2) , conditional on the appearance of word wl , is simply
PML(W2\[wl)--c(wl , w2) (1) c(i ) where c(wl , w2) is the frequency of ( wl , w2 ) in the training corpus and c ( wl ) is the frequency of wt . However , PML is zero for any unseen word pair , which leads to extremely inaccurate estimates for word pair probabilities  . 
Previous proposals for remedying the above problem  ( Good , 1953; Jelinek , Mercer , and Roukos , 1992; Katz , 1987; Church and Gale ,  1991 ) adjust the MLE in so that the total probability of seen word pairs is less than one  , leaving some probability mass to be redistributed among the unseen pairs  . In general , the adjustment involves either interpolation , in which the MLE is used in linear combination with an estimator guaranteed to be nonzero for unseen word pairs  , or discounting , in which a reduced MLE is used for seen word pairs  , with the probability mass left over from this reduction used to model unseen pairs  . 
The discounting approach is the one adopted by Katz  ( 1987 ) :/ Pd ( w2\]wx ) C ( Wl , w2) > 0/5(w2lwl ) =\[ o ~( wl ) Pr(w2\[wl)o . w . 
(2 ) where Pd represents the GoodTuring discounted estimate  ( Katz , 1987) for seen word pairs , and Pr denotes the model for probability redistribution among the unseen word pairs  . 
c~(wl ) is a normalization factor.
Following Dagan , Pereira , and Lee (1994) , we modify Katz's formulation by writing Pr ( w2\]wl ) instead P ( w2 )  , enabling us to use similarity-based stimates for unseen word pairs instead of basing the estimate for the pair on unigram frequency P  ( w2 )  . Observe that similarity estimates are used for unseen word pairs only  . 
We next investigate stimates for Pr ( w21 wl ) that are distributionally similar to W l . 
2.2 Combining Evidence
Similarity-based models assume that if word w~is " similar " to word wl  , then w ~ can yield information about the probability of unseen word pairs involving wl  . We use a weighted average of the evidence provided by similar words  , where the weight given to a particular word w ~ depends on its similarity to wl  . 
More precisely , let W(wl , W ~ l ) denote an increasing function of the similarity between wl and w\[  , and let $ ( Wl ) denote the set of words most similar to W l . Then the general form of similarity model we consider is a W-weighted linear combination of predictions of similar words : 
PSIM('W2IW l ) = ~ V(W l , W  ~ ) E ~ ~ s ( ~1  )   ( 3 ) where = is a normalization factor . According to this formula ,   w2 is more likely to occur with wl if it tends to occur with the words that are most similar to 

Considerable latitude is allowed in defining the set $  ( Wx )  , as is evidenced by previous work that can be put in the above form  . Essen and Steinbiss ( 1992 ) and Karov and Edelman ( 1996 )   ( implicitly ) set 8 ( wl ) = V1 . However , it may be desirable to restrict , 5 ( wl ) in some fashion , especially if 1/1 is large . For instance , Dagan . Pereira , and Lee ( 1994 ) use the closest k or fewer words w ~ such that the dissimilarity between wl and w ~ is less than a threshold value t  ; k and t are tuned experimentally . 
Now , we could directly replace P , . (w2\[wl ) in the backoff equation ( 2 ) with PSIM ( W21Wl )  . 
However , other variations are possible , such as interpolating with the unigram probability 

P, . ( w2lwl ) = 7P(w2)+(1-7) PsiM(W2lWl) , where 7 is determined experimentally ( Dagan , Pereira , and Lee ,  1994) . This represents , in effect , a linear combination of the similarity estimate and the backoff estimate : if  7  --  1  , then we have exactly Katz's backoff scheme . 
As we focus in this paper on alternatives for PS lM  , we will not consider this approach here ; that is , for the rest of this paper , Pr(w2\]wl ) =

2.3 Measures of Similarity
We now consider several word similarity functions that can be derived automatically from the statistics of a training corpus  , as opposed to functions derived from manually -constructed word classes  ( Resnik ,  1992) . All the similarity functions we describe below depend just on the base language model P  ( ' I ' )  , not the discounted model/5 ( . \[ . ) from Section 2 . 1 above . 
2.3.1 KL divergence
KullbackLeibler ( KL ) divergence is a standard information-theoretic measure of the dissimilarity between two probability mass functions  ( Cover and Thomas ,  1991) . We can apply it to the conditional distribution P (  . \[ wl ) induced by Wlon words in V2:D ( wx\[lW ) = P ( w2lwl ) logP ( wu\[wx ) P ( w21 wl ) "  ( 4 ) For D ( w x H w ~ l ) to be defined it must be the case that P ( w2\]w  ~ l ) >0 whenever P ( w21 wl )  > 0 . Unfortunately , this will not in general be the case for MLEs based on samples  , so we would need smoothed estimates of P ( w2\]w ~ ) that redistribute some probability mass to zero -frequency events  . However , using smoothed estimates for P ( w2\[wl ) as well requires a sum over all w 26172 , which is expensive \[' or the large vocabularies under consideration  . Given the smoothe denominator distribution , we set l/V ( wl , w ~) = lO-~D ( wlll w'l) , where /3 is a free parameter . 
2 . 3 . 2 Total d ivergence to the average A related measure is based on the total KL divergence to the average of the two distributions : + wlA  ( wx , W 11) = D(w , wl ) + D ( w ~\ [+ w ~ ) where ( W l ? w ~ ) /2 shorthand for the distribution ? ( P (  . IwJ + P ( . Iw ~)) Since D('II-)>O,A(W l,W ~)>_O . Furthermore , letting p(w2) = P ( w2\[wJ , p'(w2) = P ( w2lw ~) and C:w2:p(w2) > O , p'(w2) > O , it is straightforward to show by grouping terms appropriately that 
A(wi , wb = - H(p(w2))-H(p'(w2)) + 2 log2 , where H(x ) = - x log x . Therefore , d(wl , w ~) is bounded , ranging between 0 and 2 log2 , and smoothed estimates are not required because probability ratios are not involved  . In addition , the calculation of A(wl , w  ~ ) require sum-ming only over those w2 for which P ( w2iwJ and P ( w2\]w ~ ) are both nonzero , which , for sparse data , makes the computation quite fast . 
As in the KL divergence case , we set
W ( W l , W ~ l ) to be 10-~A ( ~' wl).
2.3.3 LI norm
The L1 norm is defined as n(wi , wl ) : ~ IP ( w2l wj-P ( w21 w'J l .  (6)

By grouping terms as before , we can express L(wI , w  ~ ) in a form depending only on the " common " w2:n ( wl , w  ~ ) = 2-Ep ( w2 ) -Ep ' ( w2 ) w26 Cw2EC?Ip ( w2 ) -p' ( w2 ) t . 

This last form makes it clear that 0 <
L(Wl , w\[)_<2 , with equality if and only if there are no words w2 such that both P ( w2lwJ and
P ( w2lw\[)a restrictly positive.
Since we require a weighting scheme that is decreasing in L  , we set
W ( w l , w ~) = (2-n(w l , W/l )) fl with fl again free . 
2.3.4 Confusion probability
Essen and Steinbiss ( 1992 ) introduced confusion probability 2 , which estimates the probability that word w ~ can be substituted for word 

Pc(wlWl ) = w(wl ,  = ~ , P ( wllw2 ) P ( w~\[w2 ) P ( w2 ) w2P ( Wl ) Unlike the measures described above , wl may not necessarily be the " closest " word to itself  , that is , there may exist a word w ~ such that
Pc(W'l\[W l ) > P c(w,\[w l).
The confusion probability can be computed from empirical estimates provided all unigram estimates are nonzero  ( as we assume throughout )  . In fact , the use of smoothed estimates like those of Katz 's backoff scheme is problematic  , because those estimates typically do not preserve consistency with respect to marginal estimates and Bayes's rule  . However , using consistent estimates ( such as the MLE ) , we can rewrite Pc as follows : ' wP ( w2l wl ) . P ( w21 w'JP ( w'J . Pc(W1\[1) = ~ P(w2)

This form reveals another important difference between the confusion probability and the functions D  , A , and L described in the previousec-tions . Those functions rate w ~ as similar to wlif , roughly , P ( w21 w ~) is high when P ( w21 ' wj is . 
Pc(w ~\[ wl) , however , is greater for those w ~ for which P ( w ~ , wJ is large when P ( w21 wJ/P ( w2) is . When the ratio P ( w21 wl)/P ( w2) is large , we may think of w2 as being exceptional , since if w2 is infrequent , we do not expect P ( w21 wJ to be large . 
2.3.5 Summary
Several features of the measures of similarity listed above are summarized in table  1  . " Base LM constraints " are conditions that must be satisfied by the probability estimates of the base  2Actually   , they present wo alternative definitions . 
We use their model 2B , which they found yielded the best experimental results  . 
59 language model . The last column indicates whether the weight W ( wl , w  ~ ) associated with each similarity function depends on a parameter that needs to be tuned experimentally  . 
3 Experimental Results
We evaluated the similarity measures listed above on a word sense disambiguation task  , in which each method is presented with a noun and two verbs  , and decides which verb is more likely to have the noun as a direct object  . Thus , we do not measure the absolute quality of the assignment of probabilities  , as would be the case in a perplexity evaluation , but rather the relative quality . We are therefore able to ignore constant factors , and so we neither normalize the similarity measures nor calculate the denominator in equation  ( 3 )  . 
3.1 Task : Pseudoword Sense

In the usual word sense disambiguation problem , the method to be tested is presented with an ambiguous word in some context  , and is asked to identify the correct sense of the word from the context  . For example , a test instance might be the sentence fragment " robbed the bank "  ; the disambiguation method must decide whether " bank " refers to a river bank  , a saving sbank , or perhap some other alternative . 
While sense disambiguation is clearly an important task  , it presents numerous experimental difficulties . First , the very notion of " sense " is not clearly defined  ; for instance , dictionaries may provide sense distinctions that are too fine or too coarse for the data at hand  . Also , one needs to have training data for which the correct senses have been assigned  , which can require considerable human effort . 
To circumven these and other difficulties , we set up a pseudow or disambiguation experiment ( Schiitze , 1992; Gale , Church , and Yarowsky , 1992) the general format of which is as follows . We first construct a list of pseudowords , each of which is the combination of two different words in  V2  . Each word in V2 contributes to exactly one pseudoword . Then , we replace each w2 in the test set with its corresponding pseudoword  . For example , if we choose to create a pseudoword out of the words " make " and " take "  , we would change the test data like this : make plans=~make  , take plans take action = ~ make , take action The method being tested must choose between the two words that make up the pseudoword  . 
3.2 Data
We used a statistical part-of-speech tagger ( Church ,  1988 ) and pattern matching and concordancing tools ( due to David Yarowsky ) to identify transitive main verbs and head nouns of the corresponding direct objects in  44 million words of 1988 Associated Press newswire . 
We selected the noun-verb pairs for the 1000 most frequent nouns in the corpus . These pairs are undoubtedly somewhat noisy given the errors inherent in the part-of-speech tagging and pattern matching  . 
We used 80% , or 587833 , of the pairs so derived , for building base bigram language models , reserving 20 . o /0 for testing purposes . As some , but not all , of the similarity measures requires moothed language models  , we calculated both a Katz backoff language model ( P = 15 ( equation ( 2 ) ) , with Pr(w2\[wl ) = P ( w2)) , and a maximum-likelihood model ( P = PML ) -Furthermore , we wished to investigate Katz's claim that one can delete singletons  , word pairs that occur only once , from the training set without affecting model performance  ( Katz ,  1987) ; our training set contained 82407 singletons . We therefore built four base language models , summarized in Table 2 . 

Katz with singletons no singletons ( 587833 pairs )   ( 505426 pairs ) 
MLE-1 MLE-ol
BO-1BO-ol
Table 2: Base Language Models
Since we wished to test the effectiveness of using similarity for unseen word cooccurrences  , we removed from the test set any verb-object pairs 



Pcrange\[0 , co\]\[0 , 2 log2\]\[0 ,  2\] \[0 , ? max w , P ( w2)\] base LM constraints
P ( w21 w~l ) ? 0 if P ( w2\[wx ) ~ : 0 none none
Bayes consistency
Table 1: Summary of similarity function properties tune ? yes yes yes no that occurred in the training set  ; this resulted in 17152 unseen pairs ( some occurred multiple times )  . The unseen pairs were further divided into five equal-sized parts  , T1 through : /'5 , which formed the basis for fivefold crossvalidation : in each of five runs  , one of the Ti was used as a performance test set , with the other 4 sets combined into one set used for tuning parameters  ( if necessary ) via a simple grid search . Finally , test pseudowords were created from pairs of verbs with similar frequencies  , so as to control for word frequency in the decision task  . We use error rate as our performance metric , defined as (  #incorrect choices + (  #ofties ) /2 ) of where N was the size of the test corpus . A tie occurs when the two words making up a pseudoword are deemed equally likely  . 
3.3 Baseline Experiments
The performances of the four base language models are shown in table  3  .   MLE-1 and MLE-ol both have error rates of exactly . 5 because the test sets consist of unseen bigrams , which are all assigned a probability of 0 by maximum-likelihood estimates , and thus are all ties for this method . The backoff models BO-1 and BO-ol also perform similarly . 



BO-ol 7'1T ~% T 4% . 5  . 5  . 5  . 5  . 5 i r 0 . 517 0 . 520 0 . 512 0 . 513 0 . 516 0 . 517 0 . 520 0 . 512 0 . 513 0 . 5 16 Table 3: Base Language Model Error Rates Since the backoff models consistently performed worse than the MLE models  , we chose to use only the MLE models in our subsequent experiments  . Therefore , we only ran comparisons between the measures that could utilize unsmoothed at a  , namely , the Lt norm , L(wx , w ~); the total divergence to the average , A(wx , w ~); and the confusion probability , Pc(w ~ lwx) . 3 In the full paper , we give detailed example showing the different neighborhoods induced by the different measures  , which we omit here for reasons of space . 
3.4 Performance of Similarity-Based

Figure 1 shows the results on the five test sets , using MLE-1 as the base language model . The parameter/3 was always set to the optimal value for the corresponding training set  . RAND , which is shown for comparison purposes , simply chooses the weights W(wl , w ~) randomly . 
S(wl ) was set equal to Vt in all cases.
The similarity-based methods consistently outperform the MLE method  ( which , recall , always has an error rate of . 5 ) and Katz's backoff method ( which always had an error rate of about . 51) by a huge margin ; therefore , we conclude that information from other word pairs is very useful for unseen pairs where unigram frequency is not informative  . The similarity-based methods also do much better than RAND  , which indicates that it is not enough to simply combine information from other words arbitrarily : it is quite important otake word similarity into account  . In all cases , A edged out the other methods . The average improvement in using A instead of Pc is  . 0082; this difference is significant to the . 1 level ( p < . 085), according to the paired ttest . 
3 It should be noted , however , that on BO-1 data , KL-divergence performed slightly better than the L1 norm . 

T1 T2
Err ~ Rates on T~t Sets ,   8aN Language MociJ MLEI " RANOMLEI " --" CONFMU ~ I " -  .   .   .   . 
"I . MLEI "?....
? AMLEI ? -- ii
T3 T4 T5
Figure 1: Error rates for each test set , where the base language model was MLE-1 . The methods , going from left to right , are RAND , Pc , L , and A . The performances shown are for settings of fl that were optimal for the corresponding training set  . I3 ranged from 4 . 0 to 4 . 5 for L and from 10 to 13 for A . 
The results for the MLE-olcase are depicted in figure  2  . Again , we see the similarity-based methods achieving far lower error rates than the MLE  , backoff , and RAND methods , and again , A always performed the best . However , with singleton somitted the difference between A and Pc is even greater  , the average difference being . 024, which is significan to the . 01 level ( paired ttest) . 
An important observation is that all methods , including RAND , were much more effective if singletons were included in the base language model  ; thus , in the case of unseen word pairs , Katz's claim that singletons can be safely ignored in the backoff model does not hold for similarity-based models  . 
4 Conclusions
Similarity-based language models provide an appealing approach for dealing with data sparseness  . We have described and compared the performance of four such models agains two classical estimation methods  , the MLE method and Katz's backoff scheme , on a pseudoword disambiguation task . We observed that the similarity-based methods perform much better on unseen word pairs  , with the measure based E ~ or ~ tes on TeSt ~ . ~ Umgua 91 Model MLE . ot . . . . F - \ ]
T t ;)- I
T21"3T4"RANDMLEol*--"CONFMLE01"-....
" LMLEol "-....
"7" AML Eol . . . . . .

ii ! :' F

Figure 2: Error rates for each test set , where the base language model was MLE-ol . /~ ranged from 6 to 11 for L and from 21 to 22 for A . 
on the KL divergence to the average , being the best overall . 
We also investigated Katz's claim that one can discard singletons in the training data  , resulting in a more compact language model , without significant loss of performance . Our results indicate that for similarity-based language modeling  , singletons are quite important ; their omission leads to significant degradation of performance  . 

We thank Hiyan Alshawi , Joshua Goodman,
Rebecca Hwa , Stuart Shieber , and Yoram
Singer for many helpful comments and discussions . Part of this work was done while the first and second authors were visiting AT&:TLabs  . 
This material is based upon work supported in part by the National Science Foundation under Grant No  . IRI-9350192 . The second author also gratefully acknowledges support from a National Science Foundation Graduate Fellowship and an AT&TGRPW/ALFP grant  . 
References
Brown , Peter F . , Vincent J . Della Pietra , Peter V . 
deSouza , Jennifer C . Lai , and Robert L . Mercer . 
1992 . Class-based ngram models of naturalan-guage . Computational Linguistics , 18(4):467-479,


Church , Kenneth .  1988 . A stochastic parts program and noun phrase parser for unrestricted text  . In Proceedings of the Second Conference on Applied Natural Language Processing  , pages 136143 . 
Church , Kenneth W . and William A . Gale . 1991.
A comparison of the enhanced GoodTuring and deleted estimation methods for estimating probabilites of english bigrams  . Computer Speech and
Language , 5:19-54.
Cover , Thomas M . and Joy A . Thomas .  1991 . Elements of Information Theory . John Wiley . 
Dagan , Ido , Fernando Pereira , and Lillian Lee .  1994 . 
Similarity-based stimation of word cooccurrence probabilities  . In Proceedings of the 32nd Annual Meeting of the ACL , pages 272-278 , Las Cruces , 

Essen , Ute and Volker Steinbiss .  1992 . Cooccurrence smoothing for stochastic language modeling  . In Proceedings of ICASSP , volume 1 , pages 161-164 . 
Gale , William , Kenneth Church , and David Yarowsky .  1992 . Work on statist cal methods for word sense disambiguation  . In Working Notes , AAAI Fall Symposium Series , Probabilistic Approaches to Natural Language , pages 54-60 . 
Good , I . J .  1953 . The population frequencies of species and the estimation of population parameters  . Biometrika , 40(3 and 4):237-264 . 
Hoeffding , Wassily .  1965 . Asymptotically optimal tests for nmttinomial distributions  . Annals of Mathematical Statistics , pages 369-401 . 
Jelinek , Frederick , Robert L . Mercer , and Salim Roukos .  1992 . Principles of lexical language modeling for speech recognition  . In In Sadaoki Furui and M . Mohan Sondhi , editors , Advances in Speech Signal Processing . Mercer Dekker , Inc . , pages 651-699 . 
Karov , Yael and Shimon Edelman .  1996 . Learning similarity-based word sense disambiguation from sparse data  . In 4rth Workshop on Very Large

Katz , Slava M .  1987 . Estimation of probabilities from sparse data for the language model component of a speech recognizer  . IEEE Transactions on Acoustics , Speech and Signal Processing , 
ASSP-35(3):400-401, March.
Pereira , Fernando , Naftali Tishby , and Lillian Lee . 
1993. Distributional custering of English words.
In Proceedings of the 31st Annual Meeting of the
ACL , pages 183-190, Columbus , OH.
Resnik , Philip .  1992 . Wordnet and distributional analysis : A classbased approach to lexical discovery  . AAAI Workshop on Statistically-based Natural Language Processing Techniques  , pages 56-64 , 

Schiitze , Hinrich .  1992 . Context space . In Working Notes , AAAI Fall Symposium on Probabilistic
Approaches to Natural Language.

