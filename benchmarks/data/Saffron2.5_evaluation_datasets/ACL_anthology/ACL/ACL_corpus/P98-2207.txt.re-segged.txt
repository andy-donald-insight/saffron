Keyword Extraction using Term-Domain Interdependence for 
Dictation of Radio News
Yoshimi Suzuki Fumiyo Fukumoto Yoshihiro Sek iguch i 
Dept . of Computer Science and Media Engineering
Yamanashi University
4-3-11 Takeda , Kofu 400 Japan
ysuzuki@suwa , fukumoto@skyo , sokiguti?saiko , osi . yamanashi , ac . jp
Abstract
In this paper , we propose keyword extraction method for dictation of radionews which consists of several domains  . In our method , newspaper articles which are automatically classified into suitable domains are used in order to calculate feature vectors  . The feature vector shows term-domain terd ependence and are used for selecting a suitable domain of each part of radio news  . Keywords are extracted by using the selected omain  . The results of keyword extraction experiments showed that our methods are robust and effective for dictation of radionews  . 
1 Introduction
Recently , many speech recognition systems are designed for various tasks  . However , most of them are restricted to certain tasks , for example , a tourist information and a hamburger shop . Speech recognition systems for the task which consists of various domain seems to be required for some tasks  , e . g . a closed caption system for TV and a transcription system of public proceedings  . In order to recognize spoken discourse which has several domains  , the speech recognition system has to have large vocabulary  . Therefore , it is necessary to limit word search space using linguistic restricts  , e . g . domain identification . 
There have been many studies of domain identification which used term weighting  ( J . McDonough et al , 1994; Yokoi et al , 1997) . McDonough proposed a topic identification method on switchboard corpus  . He reported that the result was best when the number of words in keyword dictionary was about  800  . In his method , duration of discourses of switchboard corpora is rather long and there are many keywords in the discourse  . However , for a short discourse , there are few keywords in a short discourse . Yokoi also proposed a topic identification method using cooccurrence of words for topic identification  ( Yokoi et al ,  1997) . He classified each dictated sentence of news into  8 topics . In TV or Radio news , however , it is difficult to segment each sentence automatically  . Sekine proposed a method for selecting a suitable sentence from sentences which were extracted by a speech recognition system using statistical language model  ( Sekine ,  1996) . 
However , if the statistical model is used for extraction of sentence candidates  , we will obtain higher recognition accuracy . 
Some initial studies of transcription of broad-cast news proceed  ( Bakis et al ,  1997) . However there are some remaining problems , e . g . speaking styles and domain identification . 
We conducte domain identification and keyword extraction experiment  ( Suzuki et al , 1997) for radionews . In the experiment , we classified radionews into 5 domains ( i . e . 
accident , economy , international , politics and sports ) . The problems which we faced with are ; 1 . Classification of newspaper articles into suitable domains could not be performed automatically  . 
2 . Many incorrect keywords are extracted , because the number of domains was few . 
In this paper , we propose a method for keyword extraction using term-domainter depen-dence in order to cope with these two problems  . 
The results of the experiments demonstrated the effectiveness of our method  . 
2 An overview of our method
Figure 1 shows an overview of our method.
Our method consists of two procedures . In the procedure of term-domainter dependence calculation  , the system calculates feature vectors clopedia of currenterm and newspaper articles  . 
In the procedure of keyword extraction in radio news  , firstly , the system divides radio news into segments according to the length of pauses  . We call the segments units . The domain which has the largest similarity between the unit of news and the feature vector of each domain is selected as domain of the unit  . Finally , the system extracts keywords in each unit using the feature vector of selected omain which is selected by domain identification  . 
Explanations of ~~ Radio Newsnen , ~ pedia JL ar~icle ~= i "~' jQ::::::::::: : : : l 
Feature vectors caVe)

D 7...\[~

Feature vectors ( Fea Va ), ~ . . . D1\[~Domain identification
D 7"0" .   .   . ID3ID 7D 181 ?\[ ~ Keyword Extraction
D 14 1 * : ~
I President \]
I\[, Democratic party Jl
Keyword extraction Calculation of term-domain interdependence 
Figure 1: An overview of our method 3 Calculating feature vectors In the procedure of term-domainter depen-dence calculation  , We calculate likelihood of appearance of each noun in each domain  . Figure 2 shows how to calculate feature vectors of term -domain interdependence  . 
In our previous experiments , we used 5 domains which were sorted manually and calculated  5 feature vectors for classifying domains of each unit of radionews and for extracting keywords  . Our previou system could not extract some keywords because of many noisy keywords  . 
In our method , newspaper articles and units of radionews are classified into many domains  . At each domain , a feature vector is calculated by an encyclopedia of currenterms and newspaper articles  . 
3 . 1" Sort ing newspaper articles according to the i r domains Firstly  , all sentences in the encyclopedia are analyzed morpheme by Chasen  ( Matsumoto et
An encyclopedia of currenterms 141 domains 10 , 236 explanations )?
I Sorting explanations \]
QNewspaper articles about 110,000 articles . ,/ ? \[ Separa~articles I \[ Extra~:~nouns I IE ~rac ~'~ n?unsl i Calculating f requ ~ vectors  ( Freq Va ) II Calculating frequency vectors ( Freq Ve ) l . _1 Calculating similarity I . ~' ~ between Fea Ve and Freq VaI Calculating X values of '_r-L 
J each noun on domains JX , , 7
IIS orting articles into domains . V . Il according to simitarity I~41 feature vectors ( Fea Ve ) ~ -  . -I .  :~
Calculating x : values of each noun on domans ? 041 feature vectors ( Fea Va ) ~
Figure 2: Calculating feature vectors al . , 1997 ) and nouns which frequently appear are extracted . A feature vector is calculated by frequency of each noun at each domain  . We call the feature vector FeaVe . Each element of FeaVe is a X2 value ( Suzuki et al ,  1997) . 
Then , nouns are extracted from newspaper articles by a morphological naly si system  ( Matsumoto et al ,  1997) , and frequency of each noun are counted . Next , similarity between FeaVe of each domain and each newspaper article are calculated by using formula  ( 1 )  . Finally , a suitable domain of each newspaper article are selected by using formula  ( 2 )  . 
Sirn(i , j ) = Fea Vej . Freq Vai (1)
Dornainl = argmaxSim(i , j ) (2)
I~j ~ N where i means a newspaper article and j means a domain  .  ( . ) means operation of inner vector . 
3 . 2 Term-domain in terdependence represented by feature vectors Firstly  , at each newspaper articles , less than 5 domains whose similarities between each article and each domain are large are selected  . 
Then , at each selecte domain , the frequency vector is modified according to similarity value and frequency of each noun in the article  . For example , If an article whose selecte domains are " political party " and " election "  , and similarity between the article and " political party " tion " are  100 and 60 respectively , each frequency vector is calculated by formula ( 3 ) and formula ( 4 )  . 
100 FreqVm=FreqV ~+ FreqVal x1 - ~ ( 3 ) 60 freqV ~ l = FreqV ~ z + freqVaix1 - ~ ( 4 ) where i means a newspaper article . 
Then , we calculate feature vectors FeaVa using FreqV using the method mentioned in our previous paper  ( Suzuki et al ,  1997) . Each element of feature vector shows X 2 value of the domain and word k . All word k ( 1 < k < M : M means the number of elements of a feature vector  ) are put into the keywor dictionary . 
4 Keyword extraction
Input news stories are represented by phoneme lattice  . There are no marks for word boundaries in input news stories  . Phoneme lattices are segmented by pauses which are longer than  0  . 5 second in recorded radio news . The system selects a domain of each unit which is a segmented phoneme lattice  . At each frame of phoneme lattice , the system selects maximum 20 words from keyword dictionary . 
4 . 1 Simi lar i ty between a domain and an unit We define the words whose X  2 values in the feature vector of domain j are large as keywords of the domain j  . In an unit of radio news about " political party  "  , there are many keywords of " political party " and the X  2 value of keywords in the feature vector of " political  2 party " is large . Therefore , sum of X w , polltical party tends to be large ( w : a word in the unit )  . In our method , the system selects a word path whose 2 is maximized in the word lattice sum of Xkj at domain i  . The similarity between unit/and domain j is calculated by formula  ( 5 )  . 
Sim(i , j ) = maxSim '( i , j ) all paths = maxnp(wordk)xXk , 15) all paths In formula (5) , word k is a word in the word lattice , and each selected word does not share any frames with any other selected words  . 
np(wordk ) is the number of phonemes of wordk.
2Xk , jisx 2 value of word k for domain j.
The system selects a word path whose
Siml(i , j ) is the largest among all word paths for domain j . 
Figure 3 shows the method of calculating similarity between unit/and  domainD1  . The system selects a word path whose Sim~(uniti , D1) is larger than those of any other word paths . 
phoneme lattice of uni ~ andidates i-
Si ~ unit . DI ) = max (3 . 2 x3 + 0 . 5 x 6,3,2 x3 +4 . 3 x4 +0 . 7? 2, 3 . 2 x3+4 . 3 x4+4 . 3 x3,1 . 2 x3 + 0 . 3x 4, -- . ) Figure 3: Calculating similarity between unit/and D1   4  . 2 Domain ident i f icat ion and keyword extraction In the domain identification process  , the system identifies each unit to a domain by formula  ( 5 )  . If Sim(i , j ) is larger than similarities between an unit and any other domains  , domain j seems to be the domain of unit ~ . The system selects the domain which is the largest of all similarities in N of domains as the domain of the unit  ( formula ( 6 ) )  . The words in the selected word path for selected omain are selected as keywords of the unit  . 
Domain i = argmaxSim(i , j ) (6)
X < j < N " 5 Experiments 5.1 Test data
The test data we have used is a radio news which is selected from NHK  6 o'clock radio news in August and September of 1995  . Some news stories are hard to be classified into one domain in radio news by human  . For evaluation of domain identification experiments  , we fied into the same domains are selected . The units which were used as test data are segmented by pauses which are longer than  0  . 5 second . We selected 50 units of radionews for the experiments . The 50 units consisted of 10 units of each domain . We used two kinds of test data . One is described with correct phoneme sequence . The other is written in phoneme lattice which is obtained by a phoneme recognition system  ( Suzuki et al ,  1993) . In each frame of phoneme lattice , the number of phoneme candidates did not exceed 3 . The following equations show the results of phoneme recognition  . 
the number of correct phonemes in phoneme lattice the number of uttered phonemes the number of correct phonemes in phoneme lattice phoneme segments in phoneme lattice =  95  . 6% = 81 . 2% 5 . 2 Training data In order to classify newspaper articles into small domain  , we used an encyclopedia of current terms " Chiezo "  ( Yamamoto ,  1995) . In the encyclopedia , there are 141 domains in 9 large domains . There are 10 , 2 36 head words and those explanations in the encyclopedia  . In order to calculate feature vectors of domains , all explanations in the encyclopedia are performed morphological nalys is by Chasen  ( Matsumoto et al ,  1997) .  9 , 8 05 nouns which appeared more than 5 times in the same domains were selected and a feature vector of each domain was calculated  . Using 141 feature vectors which were calculated in the encyclopedia  , we identified o-mains of newspaper articles . We identified o-mains of 110 , 0 00 articles of newspaper for calculating feature vectors automatically  . We selected 61 , 7 27 nouns which appeared at least 5 times in the newspaper articles of same domains and calculated  141 feature vectors . 
5 . 3 Domain identi f icat ion exper iment The system select suitable domain of each unit for keyword extraction  . Table I shows the results of domain identification  . We conducted domain identification experiments u ing two kinds of input data  , i . e . correct phoneme sequence and phoneme lattice and two kinds of domains  , i . e . 141 domains and 9 large domains . 
We also compared the results and the result using previous method  ( Suzuki et al ,  1997) . For comparison , we selected 5 domains which are used by previous method in our method  . In previous method , we used a keywor dictionary which has 4 , 212 words . 
Table 1: The result of domain identification number of Correct Phoneme method domains phoneme lattice our  141   62%   40% method 9   78%   54%   5   90%   82% previous 5   86%   78% method 5  . 4 Keyword ext ract ion exper iment We have conducted keyword extraction experiment using the method with  141 feature vectors ( our method )  , 5 feature vectors ( previous method ) and without domain identification . Table 2 shows recall and precision which are shown in formula  ( 7 )  , and formula (8) , respectively , when the input data was phoneme lattice . 
the number of correct words in recall = MSKP the number of selected words in  ( 7 ) 
MSKP the number of correct words precision = in MSKP the number of correct nouns  ( 8 ) in the unit MSKP : the most suitable keyword path for selected domain  6 Discussion 6  . 1 Sort ing newspaper art ic les according to the i r domains For using X  2 values in feature vectors , we have good result of domain identification of newspaper articles  . Even if the newspaper articles which are classified into several domains  , the suitable domains are selected correctly . 
6 . 2 Domain ident i f i cat ion o f radio news Table I shows that when we used  141 kinds of domains and phoneme lattice ,   40% of units were identified as the most suitable domains by our tion 
Method R/P our method R ( 141 domains ) P previous method R ( 5 domains ) P without DIR . (1 domain ) P
Correct phoneme
Phoneme lattice 88 . 5% 48 . 9% 69 . 0% 38 . 1% 80 . 0% 63 . 1% 77 . 0% 60 . 1% 24 . 0% 33 . 0% 12 . 2% 9 . 5% R : recall P : precision Dh domain identification method and shows that when we used  9 kinds of domains and phoneme lattice ,   54% of units are identified as the most suitable domains by our method  . When the number of domains was 5 , the results using our method are better than our previous experiment  . The reason is that we use small domains . Using small domains , the number of words whose X 2 values of a certain domain are high is smaller than when large domains are used  . 
For further improvement of domain identification , it is necessary to use larger newspaper corpus in order to calculate feature vectors precisely and have to improve phoneme recognition  . 
6 . 3 Keyword ext ract ion o f rad io news When we used our method to phoneme lattice  , recall was 48 . 9% and precision was 38 . 1% . 
We compared the result with the result of our previous experiment  ( Suzuki et al ,  1997) . The result of our method is better than the our previous result  . The reason is that we used domains which are precisely classified  , and we can limit keyword search space . Howeve recall was 48 . 9% using our method . It shows that about 50% of selected keywords were incorrect words , because the system tries to find keywords for all parts of the units  . In order to raise recall value , the system has to use cooccurrence between keywords in the most suitable keyword path  . 
7 Conclusions
In this paper , we proposed keyword extraction in radio news using term-domain interdependence  . In our method , we could obtain sorted large corpus according to domains for keyword extraction automatically  . Using our method , the number of incorrect keywords in extracted words was smaller than the previous method  . 
In future , we will study how to select correct words from extracted keywords in order to apply our method for dictation of radionews  . 
8 Acknowledgments
The authors would like to thank Mainichi Shimbun for permission to use newspaper articles on CD -Mainichi Shimbun  1994 and 1995  , As a hiShimbun for permission to use the data of the encyclopedia of current terms " Chiezo  1996" and Japan Broadcasting Corporation ( NHK ) for permission to user adionews . The authors would also like to thank the anonymous reviewers for their valuable comments  . 
References
Baimo Bakis , Scott Chen , Ponani Gopalakrishnan , Ramesh Gopinath , Stephane Maes , and Lazaros Pllymenakos .  1997 . Transcription of broadcast news-system robustness is ues and adaptation techniques  . In Proc . ICASSP'97, pages 711-714 . 
J . McDonough , K . Ng , P . Jeanrenaud , H . Gish , and J . R . Rohlicek .  1994 . Approaches to topic identification on the switchboard corpus  . In Proc . IEEE
ICASSP'94, volume 1, pages 385-388.
Yuji Matsumoto , Akira Kitauchi , Tatuo Yamashita , Osamu Imaichi , and Tomoaki Imamura ,  1997 . 
Japanese Morphological Analysis System ChaSen Manual  . Matsumoto Lab . Nara Institute of Science and Technology . 
Satoshi . Sekine .  1996 . Modeling topic coherence for speech recognition . In Proc . COLING96, pages 913-918 . 
Yoshimi Suzuki , Chieko Furuichi , and Satoshi Imai . 
1993 . Spoken japanese sentence recognition using dependency relationship with systematical semantic ategory  . Trans . of IEICEJ 76D-II , 11:2264-2273 . ( in Japanese) . 
Yoshimi Suzuki , Fumiyo Fukumoto , and Yoshihiro Sekiguchi .  1997 . Keyword extraction of radio news using term weighting for speech recognition  . 
In NLPRS 97, pages 301-306.
Shin Yamamoto , editor .  1995 . The Asahi Encyclopedia of Current Terms ' Chiezo  '  . Asahi Shimbun . 
Kentaro Yokoi , Tatsuya Kawahara , and Shuji Doshita .  1997 . Topic identification of news speech using word cooccurrence statistics  . In Technical Report of IEICE SP96-I05 , pages 71-78 . ( in Japanese) . 

