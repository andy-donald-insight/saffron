Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers , pages 1?5,
Portland , Oregon , June 1924, 2011. c?2011 Association for Computational Linguistics
Lexicographic Semirings for Exact Automata Encoding of Sequence Models
Brian Roark , Richard Sproat , and Izhak Shafran
{roark,rws,zak}@cslu.ogi.edu
Abstract
In this paper we introduce a novel use of the lexicographic semiring and motivate its use for speech and language processing tasks . We prove that the semiring allows for exact encoding of backoff models with epsilon transitions . This allows for offline optimization of exact models represented as large weighted finite-state transducers in contrast to implicit ( online ) failure transition representations . We present preliminary empirical results demonstrating that , even in simple intersection scenarios amenable to the use of failure transitions , the use of the more powerful lexicographic semiring is competitive in terms of time of intersection.
1 Introduction and Motivation
Representing smoothed ngram language models as weighted finite-state transducers ( WFST ) is most naturally done with a failure transition , which reflects the semantics of the ? otherwise ? formulation of smoothing ( Allauzen et al , 2003). For example , the typical backoff formulation of the probability of a word w given a history h is as follows
P(w | h ) = {
P(w | h ) if c(hw ) > 0 ? hP(w | h ?) otherwise (1) where P is an empirical estimate of the probability that reserves small finite probability for unseen ngrams ; ? h is a backoff weight that ensures normalization ; and h ? is a backoff history typically achieved by excising the earliest word in the history h . The principle benefit of encoding the WFST in this way is that it only requires explicitly storing ngram transitions for observed ngrams , i.e ., count greater than zero , as opposed to all possible ngrams of the given order which would be infeasible in for example large vocabulary speech recognition . This is a massive space savings , and such an approach is also used for nonprobabilistic stochastic language models , such as those trained with the perceptron algorithm ( Roark et al , 2007), as the means to access all and exactly those features that should fire for a particular sequence in a deterministic automaton . Similar issues hold for other finite-state sequence processing problems , e.g ., tagging , bracketing or segmenting.
Failure transitions , however , are an implicit method for representing a much larger explicit automaton ? in the case of ngram models , all possible ngrams for that order . During composition with the model , the failure transition must be interpreted on the fly , keeping track of those symbols that have already been found leaving the original state , and only allowing failure transition traversal for symbols that have not been found ( the semantics of ? otherwise ?). This compact implicit representation cannot generally be preserved when composing with other models , e.g ., when combining a language model with a pronunciation lexicon as in widely-used FST approaches to speech recognition ( Mohri et al , 2002). Moving from implicit to explicit representation when performing such a composition leads to an explosion in the size of the resulting transducer , frequently making the approach intractable.
In practice , an offline approximation to the model is made , typically by treating the failure transitions as epsilon transitions ( Mohri et al , 2002; Allauzen et al , 2003), allowing large transducers to be composed and optimized offline . These complex approximate transducers are then used during first-pass decoding , and the resulting pruned search graphs ( e.g ., word lattices ) can be rescored with exact language models encoded with failure transitions.
Similar problems arise when building , say , POS-taggers as WFST : not every postag sequence will have been observed during training , hence failure transitions will achieve great savings in the size of models . Yet discriminative models may include complex features that combine both input stream ( word ) and output stream ( tag ) sequences in a single feature , yielding complicated transducer topologies for which effective use of failure transitions may not nisms is required in such cases to allow for offline representation and optimization.
In this paper , we introduce a novel use of a semiring ? the lexicographic semiring ( Golan , 1999) ? which permits an exact encoding of these sorts of models with the same compact topology as with failure transitions , but using epsilon transitions . Unlike the standard epsilon approximation , this semiring allows for an exact representation , while also allowing ( unlike failure transition approaches ) for offline composition with other transducers , with all the optimizations that such representations provide.
In the next section , we introduce the semiring , followed by a proof that its use yields exact representations . We then conclude with a brief evaluation of the cost of intersection relative to failure transitions in comparable situations.
2 The Lexicographic Semiring
Weighted automata are automata in which the transitions carry weight elements of a semiring ( Kuich and Salomaa , 1986). A semiring is a ring that may lack negation , with two associative operations ? and ? and their respective identity elements 0 and 1. A common semiring in speech and language processing , and one that we will be using in this paper , is the tropical semiring ( R ? {?}, min ,+,?, 0), i.e ., min is the ? of the semiring ( with identity ?) and + is the ? of the semiring ( with identity 0). This is appropriate for performing Viterbi search using negative log probabilities ? we add negative logs along a path and take the min between paths.
A ? W1,W2 . . . Wn?-lexicographic weight is a tuple of weights where each of the weight classes W1,W2 . . . Wn , must observe the path property ( Mohri , 2002). The path property of a semiring K is defined in terms of the natural order on K such that : a < K b iff a ? b = a . The tropical semiring mentioned above is a common example of a semiring that observes the path property , since : w1 ? w2 = min{w1, w2} w1 ? w2 = w1 + w2 The discussion in this paper will be restricted to lexicographic weights consisting of a pair of tropical weights ? henceforth the ? T , T ?- lexicographic semiring . For this semiring the operations ? and ? are defined as follows ( Golan , 1999, pp . 223?224): ? w1, w2? ? ? w3, w4? = ? ???? ???? if w1 < w3 or ? w1, w2? ( w1 = w3 & w2 < w4) ? w3, w4? otherwise ? w1, w2? ? ? w3, w4? = ? w1 + w3, w2 + w4? The term ? lexicographic ? is an apt term for this semiring since the comparison for ? is like the lexicographic comparison of strings , comparing the first elements , then the second , and so forth.
3 Language model encoding 3.1 Standard encoding For language model encoding , we will differentiate between two classes of transitions : backoff arcs ( labeled with a ? for failure , or with  using our new semiring ); and ngram arcs ( everything else , labeled with the word whose probability is assigned ). Each state in the automaton represents an ngram history string h and each ngram arc is weighted with the ( negative log ) conditional probability of the word w labeling the arc given the history h . For a given history h and ngram arc labeled with a word w , the destination of the arc is the state associated with the longest suffix of the string hw that is a history in the model . This will depend on the Markov order of the ngram model . For example , consider the trigram model schematic shown in Figure 1, in which only history sequences of length 2 are kept in the model.
Thus , from history hi = wi?2wi?1, the word wi transitions to hi+1 = wi?1wi , which is the longest suffix of hiwi in the model.
As detailed in the ? otherwise ? semantics of equation 1, backoff arcs transition from state h to a state h ?, typically the suffix of h of length | h | ? 1, with weight (? log?h ). We call the destination state a backoff state . This recursive backoff topology terminates at the unigram state , i.e ., h = , no history.
Backoff states of order k may be traversed either via ?- arcs from the higher order ngram of order k + 1 or via an ngram arc from a lower order ngram of order k?1. This means that no ngram arc can enter the zeroeth order state ( final backoff ), and full-order states ? history strings of length n ? 1 for a model of order n ? may have ngram arcs entering from other full-order states as well as from backoff states of history size n ? 2.
3.2 Encoding with lexicographic semiring For an LM machineM on the tropical semiring with failure transitions , which is deterministic and has the wi1 ?/- log ? hi wi ?/- log ? h i+1 wi /- logP(wi|wi-1) ?/- log ? w i1 wi /- logP(wi ) Figure 1: Deterministic finite-state representation of ngram models with negative log probabilities ( tropical semiring ). The symbol ? labels backoff transitions . Modified from Roark and
Sproat (2007), Figure 6.1.
path property , we can simulate ?- arcs in a standard LM topology by a topologically equivalent machine M ? on the lexicographic ? T , T ? semiring , where ? has been replaced with epsilon , as follows . For every ngram arc with label w and weight c , source state si and destination state sj , construct an ngram arc with label w , weight ?0, c ?, source state s?i , and destination state s?j . The exit cost of each state is constructed as follows . If the state is nonfinal , ??,??.
Otherwise if it final with exit cost c it will be ?0, c?.
Let n be the length of the longest history string in the model . For every ?- arc with ( backoff ) weight c , source state si , and destination state sj representing a history of length k , construct an - arc with source state s?i , destination state s ? j , and weight ???( n?k ), c ?, where ? > 0 and ??( n?k ) takes ? to the ( n ? k)th power with the ? operation . In the tropical semiring , ? is +, so ??( n?k ) = ( n ? k)?.
For example , in a trigram model , if we are backing off from a bigram state h ( history length = 1) to a unigram state , n ? k = 2 ? 0 = 2, so we set the backoff weight to ?2?,? log?h ) for some ? > 0.
In order to combine the model with another automaton or transducer , we would need to also convert those models to the ? T , T ? semiring . For these automata , we simply use a default transformation such that every transition with weight c is assigned weight ?0, c ?. For example , given a word lattice L , we convert the lattice to L ? in the lexicographic semiring using this default transformation , and then perform the intersection L ? ? M ?. By removing epsilon transitions and determinizing the result , the low cost path for any given string will be retained in the result , which will correspond to the path achieved with ?- arcs . Finally we project the second dimension of the ? T , T ? weights to produce a lattice in the tropical semiring , which is equivalent to the result of L ? M , i.e.,
C2(det(eps-rem(L ? ? M ?))) = L ? M where C2 denotes projecting the second-dimension of the ? T , T ? weights , det (?) denotes determinization , and eps-rem (?) denotes - removal.
4 Proof
We wish to prove that for any machine N , ShortestPath(M ? ? N ?) passes through the equivalent states in M ? to those passed through in M for ShortestPath(M ? N ). Therefore determinization of the resulting intersection after - removal yields the same topology as intersection with the equivalent ? machine . Intuitively , since the first dimension of the ? T , T ? weights is 0 for ngram arcs and > 0 for backoff arcs , the shortest path will traverse the fewest possible backoff arcs ; further , since higher-order backoff arcs cost less in the first dimension of the ? T , T ? weights in M ?, the shortest path will include ngram arcs at their earliest possible point.
We prove this by induction on the state-sequence of the path p/p ? up to a given state si/s?i in the respective machines M/M ?.
Base case : If p/p ? is of length 0, and therefore the states si/s?i are the initial states of the respective machines , the proposition clearly holds.
Inductive step : Now suppose that p/p ? visits s0...si/s?0...s ? i and we have therefore reached si/s ? i in the respective machines . Suppose the cumulated weights of p/p ? are W and ??, W ?, respectively . We wish to show that whichever sj is next visited on p ( i.e ., the path becomes s0...sisj ) the equivalent state s ? is visited on p ? ( i.e ., the path becomes s?0...s ? is ? j).
Let w be the next symbol to be matched leaving states si and s?i . There are four cases to consider : (1) there is an ngram arc leaving states si and s?i labeled with w , but no backoff arc leaving the state ; (2) there is no ngram arc labeled with w leaving the states , but there is a backoff arc ; (3) there is no ngram arc labeled with w and no backoff arc leaving the states ; and (4) there is both an ngram arc labeled with w and a backoff arc leaving the states . In cases (1) and (2), there is only one possible transition to take in either M or M ?, and based on the algorithm for construction of M ? given in Section 3.2, these transitions will point to sj and s?j respectively . Case (3) leads to failure of intersection with either machine . This leaves case (4) to consider . In M , since there is a transition leaving state si labeled with w , not be traversed , hence the destination of the ngram arc sj will be the next state in p . However , in M ?, both the ngram transition labeled with w and the backoff transition , now labeled with , can be traversed . What we will now prove is that the shortest path through M ? cannot include taking the backoff arc in this case.
In order to emit w by taking the backoff arc out of state s?i , one or more backoff () transitions must be taken , followed by an ngram arc labeled with w . Let k be the order of the history represented by state s?i , hence the cost of the first backoff arc is ?( n ? k )?,? log(?s?i )? in our semiring . If we traverse m backoff arcs prior to emitting the w , the first dimension of our accumulated cost will be m(n ? k + m?12 )?, based on our algorithm for construction of M ? given in Section 3.2. Let s?l be the destination state after traversing m backoff arcs followed by an ngram arc labeled with w . Note that , by definition , m ? k , and k ? m + 1 is the order of state s?l . Based on the construction algorithm , the state s?l is also reachable by first emitting w from state s?i to reach state s ? j followed by some number of backoff transitions . The order of state s?j is either k ( if k is the highest order in the model ) or k + 1 ( by extending the history of state s?i by one word ). If it is of order k , then it will require m ? 1 backoff arcs to reach state s?l , one fewer than the path to state s?l that begins with a backoff arc , for a total cost of ( m ? 1)(n ? k + m?12 )? which is less than m(n ? k + m?12 )?. If state s?j is of order k + 1, there will be m backoff arcs to reach state s?l , but with a total cost of m(n ? ( k + 1) + m?12 )? = m(n ? k + m?3 2 )? which is also less than m(n ? k + m?12 )?. Hence the state s?l can always be reached from s ? i with a lower cost through state s?j than by first taking the backoff arc from s?i . Therefore the shortest path on
M ? must follow s?0...s ? is ? j . 2
This completes the proof.
5 Experimental Comparison of , ? and ? T , T ? encoded language models For our experiments we used lattices derived from a very large vocabulary continuous speech recognition system , which was built for the 2007 GALE Arabic speech recognition task , and used in the work reported in Lehr and Shafran (2011). The lexicographic semiring was evaluated on the development set (2.6 hours of broadcast news and conversations ; 18K words ). The 888 word lattices for the development set were generated using a competitive baseline system with acoustic models trained on about 1000 hrs of Arabic broadcast data and a 4gram language model . The language model consisting of 122M ngrams was estimated by interpolation of 14 components . The vocabulary is relatively large at 737K and the associated dictionary has only single pronunciations.
The language model was converted to the automaton topology described earlier , and represented in three ways : first as an approximation of a failure machine using epsilons instead of failure arcs ; second as a correct failure machine ; and third using the lexicographic construction derived in this paper.
The three versions of the LM were evaluated by intersecting them with the 888 lattices of the development set . The overall error rate for the systems was 24.8%?comparable to the state-of-the-art on this task1. For the shortest paths , the failure and lexicographic machines always produced identical lattices ( as determined by FST equivalence ); in contrast , 81% of the shortest paths from the epsilon approximation are different , at least in terms of weights , from the shortest paths using the failure LM . For full lattices , 42 (4.7%) of the lexicographic outputs differ from the failure LM outputs , due to small floating point rounding issues ; 863 (97%) of the epsilon approximation outputs differ.
In terms of size , the failure LM , with 5.7 million arcs requires 97 Mb . The equivalent ? T , T ?- lexicographic LM requires 120 Mb , due to the doubling of the size of the weights.2 To measure speed , we performed the intersections 1000 times for each of our 888 lattices on a 2993 MHz Intel R ? Xeon R ? CPU , and took the mean times for each of our methods . The 888 lattices were processed with a mean of 1.62 seconds in total (1.8 msec per lattice ) using the failure LM ; using the ? T , T ?- lexicographic LM required 1.8 seconds (2.0 msec per lattice ), and is thus about 11% slower . Epsilon approximation , where the failure arcs are approximated with epsilon arcs took 1.17 seconds (1.3 msec per lattice ). The 1The error rate is a couple of points higher than in Lehr and Shafran (2011) since we discarded nonlexical words , which are absent in maximum likelihood estimated language model and are typically augmented to the unigram backoff state with an arbitrary cost , fine-tuned to optimize performance for a given task.
2If size became an issue , the first dimension of the ? T , T ?- weight can be represented by a single byte.
4 slightly slower speeds for the exact method using the failure LM , and ? T , T ? can be related to the overhead of computing the failure function at runtime , and determinization , respectively.
6 Conclusion
In this paper we have introduced a novel application of the lexicographic semiring , proved that it can be used to provide an exact encoding of language model topologies with failure arcs , and provided experimental results that demonstrate its efficiency . Since the ? T , T ?- lexicographic semiring is both left - and right-distributive , other optimizations such as minimization are possible . The particular ? T , T ?- lexicographic semiring we have used here is but one of many possible lexicographic encodings . We are currently exploring the use of a lexicographic semiring that involves different semirings in the various dimensions , for the integration of part-of-speech taggers into language models.
An implementation of the lexicographic semiring by the second author is already available as part of the OpenFst package ( Allauzen et al , 2007).
The methods described here are part of the NGram language-model-training toolkit , soon to be released at opengrm.org.
Acknowledgments
This research was supported in part by NSF Grant # IIS-0811745 and DARPA grant # HR0011-09-1-0041. Any opinions , findings , conclusions or recommendations expressed in this publication are those of the authors and do not necessarily reflect the views of the NSF or DARPA . We thank Maider Lehr for help in preparing the test data . We also thank the
ACL reviewers for valuable comments.
References
Cyril Allauzen , Mehryar Mohri , and Brian Roark . 2003.
Generalized algorithms for constructing statistical language models . In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics , pages 40?47.
Cyril Allauzen , Michael Riley , Johan Schalkwyk , Wojciech Skut , and Mehryar Mohri . 2007. OpenFst : A general and efficient weighted finite-state transducer library . In Proceedings of the Twelfth International Conference on Implementation and Application of Automata ( CIAA 2007), Lecture Notes in Computer Science , volume 4793, pages 11?23, Prague , Czech Republic . Springer.
Jonathan Golan . 1999. Semirings and their Applications.
Kluwer Academic Publishers , Dordrecht.
Werner Kuich and Arto Salomaa . 1986. Semirings , Automata , Languages . Number 5 in EATCS Monographs on Theoretical Computer Science . Springer-
Verlag , Berlin , Germany.
Maider Lehr and Izhak Shafran . 2011. Learning a discriminative weighted finite-state transducer for speech recognition . IEEE Transactions on Audio , Speech , and
Language Processing , July.
Mehryar Mohri , Fernando C . N . Pereira , and Michael Riley . 2002. Weighted finite-state transducers in speech recognition . Computer Speech and Language , 16(1):69?88.
Mehryar Mohri . 2002. Semiring framework and algorithms for shortest-distance problems . Journal of Automata , Languages and Combinatorics , 7(3):321?350.
Brian Roark and Richard Sproat . 2007. Computational Approaches to Morphology and Syntax . Oxford University Press , Oxford.
Brian Roark , Murat Saraclar , and Michael Collins . 2007.
Discriminative ngram language modeling . Computer
Speech and Language , 21(2):373?392.
5
