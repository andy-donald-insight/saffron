NOUNCLASSIFICATION FROMPREDICATE . ARGUMENTSTRUCTURES
Donald Hindle
AT&T Bell Laboratories
600 Mountain Avenue
Murray Hill , NJ 07974

A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject  , verb and object in a large text corpus is described  . The resulting quasi-semantic classification of nouns demonstrates the plausibility of the distributional hypothesis  , and has potential application to a variety of tasks  , including automatic indexing , resolving nominal compounds , and determining the scope of modification . 
1. INTRODUCTION
A variety of linguistic relations apply to sets of semantically similar words  . For example , modifiers select semantically similar nouns , selecfional restrictions are expressed in terms of the semantic class of objects  , and semantic type restricts the possibilities for noun compounding  . Therefore , it is useful to have a classification of words into semantically similar sets  . Standard approaches to classifying nouns , in terms of an " isa " hierarchy , have proven hard to apply to unrestricted language  . 
Isa hierarchies are expensive to acquire by hand for anything but highly restricted domains  , while attempts to automatically derive these hierarchies from existing dictionaries have been only partially successful  ( Chodorow , Byrd , and Heidom 1985) . 
This paper describes an approach to classifying English words according to the predicate-argument structures they show in a corpus of text  . The general idea is straightforward : in any natural language thereate restrictions on what words can appear together in the same construction  , and in particular , on what can he arguments of what predicates . For nouns , there is a restricted set of verbs that it appears as subject of or object of  . For example , wine may be drunk , produced , and sold but not pruned . Each noun may therefore he characterized according to the verbs that it occurs with  . Nouns may then he grouped according to the extent to which they appear in similar environments  . 
This basic idea of the distributional foundation of meaning is not new  . Hams ( 1968 ) makes this " distributional hypothesis " central to his linguistic theory  . His claim is that : " the meaning of entities , and the meaning of grammatical relations among them  , is related to the restriction of combinations of these entities relative to other entities  . " ( Harris 1968:12) . SparckJones (1986) takes a similar view . 
It is however by no means obvious that the distribution of words will directly provide a useful semantic classification  , at least in the absence of considerable human intervention  . The work that has been done based on Harris ' distributional hypothesis  ( most notably , the work of the associates of the Linguistic String Project  ( see for example , 
Hirschman , Grishman , and Sager 1975 ) ) unfortunately does not provide a direct answer , since the corpora used have been small ( tens of thousands of words rather than millions ) and the analysis has typically involved considerable intervention by the researchers  . The stumbling block to any automatic use of distributional patterns has been that no sufficiently robust syntactic analyzer has been available  . 
This paper reports an investigation of automatic distributional classification of words in English  , using a parser developed for extracting rammatical structures from unrestricted text  ( Hindle 1983 )  . We propose a particular measure of similarity that is a function of mutual information estimated from text  . 
On the basis of a six million word sample of Associated Press news stories  , a classification of nouns was developed according to the predicates they occur with  . This purely syntaxbased similarity measure shows remarkably plausible semantic relations  . 
268 2. ANALY ZING THE CORPUS
A 6 million word sample of Associated Press news stories was analyzed  , one sentence at a time , 

I/I
DNC PROT NS VS PRO
IIIIIII the land that t*s ustains us

NP i ?) ' ? CNQpDNPL
IIIII and many of the products we
S?Axvs 7 AYxi
PROTNS VPROT hiSV SDN
IIIIIII * use ?* are the result Figure 1 . Parser output for a fragment of sentence (1) . 
by a deterministic parser ( Fidditch ) of the sort originated by Marcus ( 1980 )  . Fidditch provides a single syntactic analysis -- a tree or sequence of trees -- for each sentence  ; Figure 1 shows part of the output for sentence ( 1 )  . 
(1) The clothes we wear , the food we eat , the air we brea the , the water we drink , the land that sustains us , and many of the products we use are the resulto fagric ultural research  .   ( March 221987 ) The parser aims to be noncommittal when it is unsure of an analysis  . For example , it is perfectly willing to parse an embedded clause and then leave it unattached  . If the object or subject of a clause is not found  , Fiddit ch leaves it empty , as in the last two clauses in Figure 1 . 
This noncommittal pproach simply reduces the effective size of the sample  . 
The aim of the parser is to produce an annotated surface structure  , building constituents as large as it can , and reconstructing the underlying clause structure when it can  . In sentence (1), six clauses are found . Their predicate-argument information may be coded as a table of  5-tuples   , consisting of verb , surface subject , surface object , underlying subject , underlying object , as shown in Table 1 . In the subject-verb-object table , the root form of the head of phrases is recorded , and the deep subject and object are used when available  .   ( Noun phrases of the formanl of n2 are coded as nl n2  ; an example is the first entry in Table 2) . 

Table 1 . Predicate-argument relations found in an AP news sentence  ( 1 )  . 
verb subject object surface deep surface deep wear we eat we brea the we drink we su stain O trace use we bel and l and 
O tracefood
Otrace air
Ot race water us result
The parser's analysis of sentence ( 1 ) is far from perfect : the object of wear is not found  , the object of use is not found , and the single element l and rather than the conjunction of clothes  , food , air , water , l and , products is taken to be the subject of be . Despite these errors , the analysis is succeeds in discovering a number of the correct predicate-argument relations  . The parsing errors that do occur seem to result , for the current purposes , in the omission of predicate-argument relations , rather than their misidentification . This makes the sample less effective than it might be  , but it is not in general misleading .   ( It may also skew the sample to the extent hat the parsing errors are consistent  . ) The analysis of the 6 million word 1987 AP sample yields 4789 verbs in 274613 clausal structures , and 267 zt2 head nouns . This table of predicate-argument relations is the basis of our similarity metric  . 
3. TYPICAL ARGUMENTS
For any of verb in the sample , we can ask what nouns it has as subjects or objects  . Table 2 shows the objects of the verb drink that occur ( more than once ) in the sample , in effect giving the answer to the question " what can you drink ? " 
Table 2. Objects of the verb drink.
OBJECT COUNT WEIGHT bunch beer 212.34 tea41 1.75
Pepsi211 . 75 champagne 411 . 75 liquid 2 10 . 53 beer 51 0 . 20 wine 29 . 34 water 77 . 65 anything 35 . 15 much 32 . 54 it 31 . 25 < SOMEAMOUNT > 21 . 2 2 This list of drinkable things is intuitively quite good  . The objects in Table 2 are ranked not by raw frequency , but by a cooccurrence score listed in the last column  . The idea is that , in ranking the importance of noun-verb associations  , we are interested not in the raw frequency of cooccurrence of a predicate and argument  , but in their frequency normalized by what we would expect  . More is to be learned from the fact that you can drink wine than from the fact that you c and rink it even though there are more clauses in our sample with  #as an object of drink than with wine  . To capture this intuition , we turn , following Church and Hanks (1989) , to " mutual information " ( see Fano 1961) . 
The mutual information of two events l ( x  y ) is defined as follows:
P ( x  y ) l ( x  y ) = log2 P ( x ) P ( y ) where P ( x  y ) is the joint probability of events x and y , and P ( x ) and P ( y ) axe the respective independent probabilities . When the joint probability P ( x  y ) is high relative to the product of the independent probabilities  , I is positive ; when the joint probability is relatively low , I is negative . We use the observed frequencies to derive a cooccurrence score Cobj  ( an estimate of mutual information ) defined as follows . 
270/(. v )

C ~, j(nv ) = log2/(n)/(v)
NN where f in v ) is the frequency of noun n occurring as object of verb v  , f ( n ) is the frequency of the noun n occurring as argument of any verb  , f(v ) is the frequency of the verb v , and N is the count of clauses in the sample . ( C , , , bi(nv ) is defined analogously . ) Calculating the cooccurrence weight for drink , shown in the third column of Table 2 , gives us a reasonable tanking of terms , with it near the bottom . 
Multiple Relationships
For any two nouns in the sample , we can ask what verb contexts they share . The distributional hypothesis is that nouns axe similar to the extent that they share contexts  . For example , Table 3 shows all the verbs which wine and beer can be objects of  , highlighting the three verbs they have in common . The verb drink is the key common factor . There are of course many other objects that can be sold  , but most of them are less alike than wine or beer because they can't also be drunk  . So for example , a car is an object that you can have and sell , like wine and beer , but you do not -- in this sample ( confirming what we know from the meanings of the words  ) -- typically drink a car . 
4. NOUNSIMILARITY
We propose the following metric of similarity , based on the mutual information of verbs and arguments  . Each noun has a set of verbs that it occurs with ( either as subject or object )  , and for each such relationship , there is a mutual information value . For each noun and verb pair , we get two mutual information values , for subject and object , 
Csubj(Vinj ) and Cobj(1Jinj )
We define the object similarity of two nouns with respect oa verb in terms of the minimum shared coocccurrence weights  , as in (2) . 
The subject similarity of two nouns , SIMs ~ j , is defined analogously . 
Now define the overall similarity of two nouns as the sum across all verbs of the object similarity and the subject similarity  , as in (3) . 
(2) Object similarity.
SIMobj(vinjnt ) = min(Cobj(vinj ) Cobj(vln , )) , ffConi(vinj ) > 0 and abs(m~x ( Cobj(vinj ) , Cobj(Vink ))) , if Cobj(vinj ) < 0
O , otherwise
Cobj(vi , ,) > 0 and Cobj(vin , ) < 0 (3) Noun similarity . 

SIM(ntn2) = ~'.

SIM ~ a , i(vinln2) + SIMobj(vin ln2)
The metric of similarity in ( 2 ) and ( 3 ) is but one of many that might be explored , but it has some useful properties . Unlike an inner product measure , it is guaranteed that a noun will be most similar to itself  . And unlike cosine distance , this metric is roughly proportional to the number of different verb contexts that are shared by two nouns  . 
Using the definition of similarity in (3) , we can begin to explore nouns that show the greatest similarity  . Table 4 shows the ten nouns most similar to boat , according to our similarity metric . The first column lists the noun which is similar to boat  . The second column in each table shows the number of instances that the noun appears in a predicate -argument pair  ( including verb environments not in the list in the fifth column  )  . The third column is the number of distinct verb environments  ( either subject or object ) that the noun occurs in which are shared with the target noun of the table  . 
Thus , boat is found in 79 verb environment . Of these , ship shares 25 common environments ( ship also occurs in many other unshared environments  )  . The fourth column is the measure of similarity of the noun with the target noun of the table  , SIM(nln2) , as defined above . 
The fifth column shows the common verb environments  , ordered by cooccurrence score , C(vinj ) , as defined above . An underscore before the verb indicates that it is a subject environment  ; a following underscore indicates an object environment  . In Table 4 , we see that boat is a subject of cruise , and object of sink . In the list for boat , in column five , cruise appears earlier in the list than carry because cruise has a higher cooccurrence score  . A-before a verb means that the cooccurrence score is negative--i  . e . the noun is less likely to occur in that argument context han expected  . 
For many nouns , encouragingly appropriate sets of semantically similar nouns are found  . 
Thus , of the ten nouns most similar to boat ( Table 4) , nineare words for vehicles ; the most Table 3 . Verbs taking wine and beer as objects . 
VERB wine beer count weight count weight drug 212 . 26 sitaround l 10 . 29 smell 11 0 . 07 contaminate 19 . 75 rest 29 . 56 drink 29 . 34 5 10 . 20 rescue 17 . 07 purchase 16 . 79 lift 16 . 72 prohibit 16 . 69 lovel 6 . 33 deliver 15 . 82 buy 35 . 44 name 15 . 42 keep 24 . 86 offer 14 . 13 begin 14 . 09 allow I3 . 90 be on 13 . 79 sell I 4 . 21 1 3 . 75' s22 . 84 make 11 . 27 have 10 . 84 2 1 . 38 similar noun is the near-synonymship . The ten nouns most similar to treaty ( agreement , plan , constitution , contract , proposal , accord , amendment , rule , law , legislation ) seem to make up a duster involving the notions of agreement and rule  . Table 5 shows the ten nouns most similar to legislator , again a fairly coherent set . 
Of course , not all nouns fall into such neat clusters : Table  6 shows a quite heterogeneous group of nouns similar to table  , though even here the most similar word ( floor ) is plausible . 
We need , in further work , to explore both automatic and supervised means of discriminating the semantically relevant associations from the spurious  . 

Table 4. Nouns similar to boat.
Noun ~ n ) verbs SIMboat 1537 937 0 . 16 ship 353 2579 . 02 plane 445 266 8 . 85 bus 104 20 64 . 49 jet 153 176 2 . 77 vessel 172 18 57 . 14 truck 146 21 56 . 71 car 4149_,452 . 22 helicopter 151 14 50 . 66 ferry 37 10 39 . 76 man 1396 3038 . 31
Verbs_cruise , keel_ , _plow , sink_ , drift_ , step of f_ , step from _ , dock_ , righ L , submerge , near , hoist , intercept , charter , stay on_ , buzz_ , stabilize_ , _siton , intercept , hijack_ , park_ , _ be from , rock , get off_ , board , miss_ , stay with _ , catch , yield- , bring in _ , seize_ , pull_ , grab , hit , exclude_ , weigh_ , _issue , demonstrate , _force , _cover , supply_ , _name , attack , damage_ , launch_ , _ provide , appear , carry , _goto , look aL , attack_ , _reach , _ be on , watch_ , use_ , return _ , _ask , destroy_ , fire , be on_ , describe_ , charge_ , include_ , be in _ , report_ , identify_ , expecL , cause , ' s , ' s , take , _make , " be_ , -say , " give_ , see , " b e , " have_ , " get_near , charter , hijack_ , get off_ , buzz_ , intercept , board_ , damage , sink_ , seize , _carry , attack_ , " have_ , _ be on , _ hit , destroy_ , watch_ , _goto , " give , ask , " be_ , be on_ , " say_ , identify , see_hijack_ , intercept_ , charter , board_ , get off , _near , _attack , _carry , seize_ , - have_ , _ be on , _ catch , destroy_ , _ hit , be on_ , damage_ , use_ , - be_ , _goto , _reach , " say_ , identify_ , _ provide , expect , cause- , see-step of f_ . , hijack_ , park_ , get off , board , catch , seize- , _carry , attack_ , _ be on , be on_ , charge_ , expect_ , " have , take , " say_ , _make , include_ , be in , " be charter , intercept , hijack_ , park_ , board , hit , seize- , _attack , _force , carry , use_ , describe_ , include , be on , "_ be , _make , -say_right- , dock , intercept , sink_ , seize , catch , _attack , _carry , attack_ , " have_ , describe_ , identify_ , use_ , report_ , " be_ , " say_ , expecL , " give_park_ , intercept- , stay with _ , _ be from , _ hit , seize , damage_ , _carry , teach , use_ , return _ , destroy_ , attack , " b e , be in , take , - have_ , - say_ , _make , include_ , see_step from _ , park_ , board , hit , _ catch , pull , carry , damage_ , destroy_ , watch_ , miss_ , return _ , " give_ , " b e , - be , be in _ , - have_ , - say_ , charge_ , _' s , identify_ , see , take , -get_hijack_ , park_ , board_ , bring in , catch , _attack , watch_ , use_ , return _ , fire_ , _ be on , include , make , -_bedock_ , sink_ , board- , pull_ , _carry , use_ , be on_ , cause , take , " say_hoist_ , bring in _ , stay with _ , _attack , grab , exclude , catch , charge_ , - have_ , identify_ , describe_ , " give , be from , appear_ , _goto , carry , _reach , _take , pull_ , hit , -get , ' s , attack_ , cause_ , _make , "_ be , see , cover , _name , _ask
Nounfin ) verbs SIM legislator 4535 165.85
Senate 36 61 140 . 19 commit ~ e6972039 . 97 organization 351 16 34 . 29 commission 389 17 34 . 28 legislature 86 12 34 . 12 delega ~132 133 3 . 65 law maker 176 143 2 . 78 panel 253 12 31 . 23
Congress 827 153 1.20 side 327 153 0.00
Table 6. Nouns similar to table.
Noun f(n ) verbs SIM table 6630 181 . 43 floor 946 30 . 01 farm 808 22 . 94 scene 135 10 20 . 85
America 156719 . 68 experience 1295 19 . 04 river 954 18 . 73 town 1956 18 . 68 side 3278 18 . 57 hospital 1907 18 . 10
House 4536 17.84
Verbs cajole , thump , _grasp , convince_ , inform_ , address , vote , _predict , _address , _withdraw , _adopt , _approve , criticize_ , _ criticize , represent , _reach , write , reject , _accuse , support_ , go to _ , _consider , _win , pay_ , allow_ , tell , hold , call __ , _kill , _call , give_ , _get , say , take , "__ be_vote , address_ , _approve , inform_ , _reject , go to _ , _consider , adopt , tell , - be , give__vote , _approve , go to _ , inform_ , _reject , tell , " b e , convince_ , _hold , address_ , _consider , _address , _adopt , call_ , criticize , allow_ , support_ , _accuse , give_ , _call adopt , inform_ , address , go to _ , _predict , support_ , _reject , represent_ , call , _approve , -_ be , allow , take , say_ , _hold , tell__reject , _vote , criticize_ , convince - , inform_ , allow , accuse , _address , _adopt , "_ be , _hold , _approve , give_ , go to _ , tell_ , _consider , pay_convince_ , approve , criticize_ , _vote , _address , _hold , _consider ,  "_ . be , call_ , give , say_ , _take-vote , inform_ , _approve , _adopt , allow_ , _reject , _consider , _reach , tell_ , give , " b e , call , say_-criticize , _approve , _vote , _predict , tell , reject , _accuse , "__ be , call_ , give , consider , _win , _get , _take_vote , approve , convince_ , tell , reject , _adopt , _ criticize ,  _ . consider , "__ be , _hold , give , _reach inform_ , _approve , _vote , tell_ , _consider , convince_ , goto , " b e , address_ , give_ , criticize_ , address , _reach , _adopt , _hold reach , _predict , criticize , withdraw , _consider , goto , hold , -_ be , _accuse , support_ , represent_ , tell_ , give_ , allow , take
Verb shide beneath_ , convolute_ , memorize_ , sitat , sit across_ , redo_ , structure_ , sitaround_ , fitter , _carry , lie on_ , go from _ , hold , wait_ , come to , return to , turn_ , approach_ , cover , be on- , share , publish_ , claim_ , mean_ , goto , raise_ , leave_ , " have_ , do , be litter , lie on- , cover , be on- , come to _ , go to __ carry , be on- , cover , return to _ , turn_ , goto . _ , leave_ , " have_approach_ , retum to _ , mean_ , goto , be on- , turn_ , come to _ , leave_ , do _ , be_go from _ , come to _ , return to _ , claim_ , go to _ , " have_ , do_structure_ , share_ , claim_ , publish_ , be_sit across_ , mean_ , be on- , leave_litter , , approach_ , go to _ , return to _ , come to _ , leave_lie on_ , be on- , go to _ , _hold , " have_ , cover , leave . _ , come to_g o from _ , come to _ , cover , return to _ , go to _ , leave_ , " have_return to_ , claim_ , come to _ , go to _ , cover_ , leave_We can define " reciprocally most similar " nouns or " reciprocal nearest neighbors "  ( RNN ) as two nouns which are each other's most similar noun  . This is a rather stringent definition ; under this definition , boat and ship do not qualify because , while ship is the most similar to boat , the word most similar to ship is not boat but plane  ( boat is second )  . For a sample of all the 319 nouns of frequency greater than 100 and less than 200  , we asked whether each has a reciprocally most similar noun in the sample  . For this sample , 36 had a reciprocal nearest neighbor . These are shown in Table 7 ( duplicates are shown only once )  . 
Table 7 . A sample of reciprocally nearest neighbors . 
RNN word counts bomb device ( 192 101 ) ruling-decision ( 192 761 ) street road ( 188 145 ) protest strike ( 187 254 ) list fie M ( 184 104 ) debt deficit ( 183 351 ) guerrillare bel ( 180 314 ) fear concern ( 176 355 ) higher lower ( 175 78 ) freedom right ( 164 609 ) battle fight ( 163 131 ) jet plane ( 153 445 ) shot bullet ( 152 35 ) truck car ( 146 414 ) researcher scientist ( 142 112 ) peace stability ( 133 64 ) property l and ( 132 119 ) star editor ( 131 85 ) trend pattern ( 126 58 ) quake earthquake ( 126 120 ) economist analyst ( 120 318 ) remark comment ( 115 385 ) data information ( 115 505 ) explosion blast ( 115 52 ) tie relation ( 114 251 ) protester demonstrator ( 110 99 ) college school ( 109 380 ) radioIRNA ( 107 18 )  2 3  ( 105 90 ) The list in Table 7 shows quite a good set of substitutable words , many of which axene at synonyms . Some are not synonyms but are 2-3 . Some we recognize as synonyms in news reporting style : explosion-blast  , bomb-device , tie-relation . And some are hard to interpret . Is the close relation between star and editor some reflection of news reporters ' world view ? Is list most like fie M because neither one has much meaning by itself ?  . 
5. DISCUSSION
Using a similarity metric derived from the distribution of subjects  , verbs and objects in a corpus of English text , we have shown the plausibility of deriving semantic relatedness from the distribution of syntactic forms  . This demonstration has depended on : 1 ) the availability of relatively large text corpora  ; 2) the existence of parsing technology that , despite a large error rate , allows us to find the relevant syntactic relations in unrestricted text  ; and 3 )   ( most important ) the fact that the lexical relations involved in the distribution of words in syntactic structures are an extremely strong linguistic constraint  . 
A number of issues will have to be confronted to further exploit these structurally-mediated lexical constraints  , including : Po/ysemy . The analysis presented here does not distinguish among related senses of the  ( orthographically ) same word . Thus , in the table of words similar to table , we find at least two distinct senses of table conflated  ; the table one can hide beneath is not the table that can be commuted or memorized  . Means of separating senses need to be developed . 
Empty words . Not all nouns are equally contentful . For example , section is a general word that can refer to sections of all sorts of things  . As a result , the ten words most similar to section ( school , building , exchange , book , house , ship , some , headquarter , industry . , office ) are a semantically diverse list of words . The reason is clear : section is semantically a rather empty word  , and the selectional restrictions on its cooccurence depend primarily on its complement  . You might read a section of a book but not , typically , a section of a house . It would be possible to predetermine a set of empty words in advance of analysis  , and thus avoid some of the problem presented by empty words  . 
But it is unlikely that the class is well-defined.
Rather , we expect hat nouns could be ranked , on the basis of their distribution , according to how empty they are ; this is a matter for further exploration . 
Sample size . The current sample is too small ; many words occur too infrequently to be adequately sampled  , and it is easy to think of usages that are not represented in the sample  . 
For example , it is quite expected to talk about brewing beer , but the pair of brew and beer does not appear in this sample  . Part of the reason for missing selectional pairs is surely the restricted nature of the AP news sublanguage  . 
Further analysis . The similarity metric proposed here , based on subject-verb-object relations , represents a considerable reduction in the information available in the subjec-verb-object table  . This reduction is useful in that it permits , for example , a clustering analysis of the nouns in the sample , and for some purposes ( such as demonstrating the plausibility of the distribution-based metric  ) such clustering is useful . However , it is worth noting that the particular information about  , for example , which nouns may be objects of a given verb , should not be discarded , and is in itself useful for analysis of text . 
In this study , we have looked only at the lexical relationship between a verb and the head nouns of its subject and object  . Obviously , there are many other relationships among words -- for example  , adjectival modification or the possibility of particular preposition ald juncts -- that can be extracted from a corpus and that contribute to our lexical knowledge  . It will be useful to extend the analysis presented here to other kinds of relationships  , including more complex kinds of verb complementation  , noun complementation , and modification both preceding and following the head noun  . But in expanding the number of different structural relations noted  , it may become less useful to compute a single -dimensional similarity score of the sort proposed in Section  , 1 . Rather , the various lexical relations revealed by parsing a corpus  , will be available to be combined in many different ways yet to he explored  . 

Chodorow , Martin S ., Roy J . Byrd , and George
E . Heidom .  1985 . Extracting semantic hierarchies from a large online dictionary  . 
Proceedings of the 23rd Annual Meeting of the ACL ,  299-304 . 
Church , Kenneth .  1988 . A stochastic parts program and noun phrase parser for unrestricted text  . Proceedings of the second
ACL Conference on Applied Natural
Language Processing.
Church , Kenneth and Patrick Hanks .  1989 . Word association or ms , mutual information and lexicography . Proceedings of the 23rd
Annual Meeting of the ACL , 76-83.
Fano , R . 1961. Transmission of Information.
Cambridge , Mass : MIT Press.
Harris , Zelig S .  1968 . Mathematical Structures of
Language . New York : Wiley.
Hindle , Donald . 1983. User manual for Fidditch.
Naval Research Laboratory Technical
Memorandum #7590-142.
Hirschman , Lynette .  1985 . Discovering sublanguage structures , in Grishman , Ralph and Richard Kittredge , eds . Analyzing Language in Restricted Domains ,  211-234 . 
Lawrence Erlbaum : Hillsdale , NJ.
Hirschman , Lynette , Ralph Grishman , and Naomi
Sager .  1975 . Grammatically-based automatic word class formation  . 
Information Processing and Management , 11, 39-57.
Marcus , Mitchell P . 1980. A Theory of Syntactic
Recognition for Natural Language . MIT

SparckJones , Karen . 1986. Synomyny and
Semantic Classification . Edinburgh
University Press.

