PARSINGHEAD-DRIVENPHRASEST RUCTUREG RAMMAR
Derek Proudlan and Carl Pollard
Hewlett-Packard Laboratories
1501 Page Mill Road
Palo Alto , CA . 94303, USA

The Head-driven Phrase Structure Grammar project ( HPSG ) is an English language database query system under development at Hewlett-Packard Laboratories  . 
Unlike other product-oriented efforts in the natural language understanding field  , the HPSG system was designed and implemented by linguists on the basis of recent theoretical developments  . But , unlike other implementations of linguistic theories  , this system is not a to y , as it deals with a variety of practical problems not covered in the theoretical literature  . We believe that this makes the HPSG system , nique in its combination of linguistic theory and practical application  . 
The HPSG system differs from its predecessor GPSG , reported on at the 1982 ACL meeting ( Gawron et al 119821 )  , in four significant respects : syntax , lexical representation , parsing , and semantics . The paper focuses on parsing issues , but also gives a synopsis of the underlying syntactic formalism  . 
1 Syntax
HPSG is a lexically based theory of phrase structure  , socalled because of the central role played by grammlttical heads and their associated complements  . ' Roughly speaking , heads are linguistic forms ( words and phrases ) tl , at exert syntactic and semantic restrictions on the phrases  , called complements , that characteristically combine with them to form larger phrases  . 
Verbs are the heads of verb phrm~es ( apd sentences )  , nouns are the heads of noun phra~es , and so forth . 
As in most current syntactic theories , categories are represented as complexes of feature specifications  . 
But the \[ IPSG treatment of lcxical subcategorization obviates the need in the theory of categories for the notion of bar-level  ( in the sense of Xbar theory , prevalent in much current linguistic research . \[ n addition , the augmentation of the system of categories with stack-valued features-features whose values ~ resequences of categories - unilies the theory of lexical subcatego-riz ~ tion with the theory of bi  , ~ ding phenomena . By binding pimnomena we mea a essentially no JL -clause-bounded delmndencies  ,   , ' such a . ~th ~ rse involving dislocated constituents , relative ~ Lnd interrogative pronouns , and reflexive and reciprocal pronouns\[12I . 
* iIPS Gularelinw lJ~i ? ~ ld?zt . ,~nsioll , , f th ~ clu~dy rel~tteu Gt~lmr~dilmd Ph? . 'tmeStructulm Grannarl TI . The detaa Jsuflily tllt ~ J/-y of HPSG ar~Nt for thin I i\[  . 
More precisely , the subcategorization of a head is encoded as the value of a stack-valued feature called ~ SUBCAT "  . For example , the SUBCAT value of the verb persuade is the sequence of three categories IVP  , NP , NPI , corresponding to the grammatical relations ( GR's ) : controlled complement , direct object , and subject respectively . We are adopting a modified version of Dowty's \[19821 terminology for GR's , where subject " LS last , direct object second-to-last , etc . For semantic reasons we call the GR following a controlled complement the controller  . 
One of the key differences between HPSG and its predeces or GPSG is the massive relocation of linguistic information from phrase structure rules into the lexicon  \[5\]  . This whole sale lexicalization of linguistic information in HPSG results in a drastic reduction in the number of phrase structure rules  . Since rules no longer handle subcategorization , their sole remaining function is to encode a small number of language-specific principles for projecting from \ [ exical entries h  , surface constituent order . 
The schematic nature of the grammar rules allows the system to parse a large fragment of English with only a small number of rules  ( the system currently uses sixteen )  , since each r1 , le can be used in many different situations . The constituents of each rule are sparsely annotated with features  , but are fleshed out when taken together with constituents looked for and constituents found  . 
For example the sentence The manager works can be parsed using the single rule RI below  . The rule is applied to build the noun phrase The manager by identifying the head H with the \[ exical element man-aqer and tile complement CI with the lexical element the  . The entire sentence is built by ideutifying the H with works and the  C1 with the noun phrase described above . Thus the single rule RI functions as both the S- * NP VP  , and NP~Det N rules of familiar contextfRe grammars  . 
R1.x->cihi(CONTROLINTRANS)\]a*
Figure I . A Grammar Rule.
167\] Feature Passing
The theory of HPSG embodies a number of substantive hypotheses about universal granunatical principles  , Such principles as the Head Feature Principle , the Binding Inheritance Principle , and the Control Agreement Principle , require that certain syntactic features specified on daughters in syntactic trees are inherited by the mothers  . Highly abstract phrase structure rules thus give rise to fully specified grammatical structures in a recursive process driven by syntactic information encoded on lexical heads  . Thus HPSG , unlike similar ~ unification-based " syntactic theories  , embodies a strong hypothesis about the flow of relevant information in the derivation of complex structures  . 

Another important difference between HPSG and other unification based syntactic theories concerns the form of the expressions which are actually unified  . 
In HPSG , the structures which get unified are ( with limited exceptions to be discussed below ) not general graph structures as in Lexical Functional Qrammar  \[1 I , or Functional Unification Granmar Il OI , but rather fi at atomic valued feature matrices , such as those ~ hown below . 
\[ ( CONTROL 0 INTRANS )   ( MAJNA )   ( AGR3RDSG )   ( PRDMINUS )   ( TOPMINUS ) \] \[ ( CONTROLO )   ( MAJHV )   ( INVPLUS ) \]
Figure 2. Two feature matrices.
In the implementation f\[\[PSG we have been able to use this restrictiou on the form of feature tnatrices to good advantage  . Since for any given version of the system the range of atomic features and feature values is fixed  , we are able to represent fi at feature matrices , such as theores above , as vectors of in tcKers , where each cell in the vector represents a feature  , and ~ he integer in each cell represents a disjunctioa of tile possible values for that feature  . 
CON MAJ AGR PRD IN VTOP ...

it T t to i 2It 13ItI
It\[t21713 It I3I
Figure 3: Two ~ ransduced feature matrices.
For example , if the possible values of the MAJ feature are N , V , A , and P then we can uuiquely represent any combination of these features with an integer in the raalge  0  . .15 . This is accomplished simply by a ~ ign-ing each possible value an index which is an integral power of  2 in this range and then adding up the indices so derived for each disjunction of values encountered  . 
Unification in such cases is thus reduced to the " logical and " of the integers in each cell of the vector representing the feature matrix  . In this way unification of these flat structures can be done in constant time  , and since = logical and " is generally a single machine instruction the overhead is very low  . 
NVAP\[t\[0\[t\[0\[=tO=(MAJNA)
It It 10 iol = t2 = ( MAJ gV)
Il mll mll mmllllll ~ lUn?fication
III0I0I0I = 8= ( MAJH)
Figure 4: Close up of the MAJ feature.
There are , however , certain cases when the values of features are not atomic  , but are instead themselves feature matrices . The unification of such structures could , in theory , involve arbitrary recursion on the general unification algorithm  , and it would seem that we had not progressed very far from the problem of unifying general graph structures  . Happily , the features for which this property of embedding holds  , constitute a small finite set ( basically tlte socalled " binding features " )  . Thus we are able to segregate such features from the rest  , and recurse only when such a " category valued ~ feature is present  . \[ n practice , therefore , the time performance of the general uailication algorithm is very good  , essentially the sanzea . s that of the lint structure unification algorithm described above  . 
2 Parsing
As in the earlier GPSG system , the primary job of the parser in the HPSG system is to produce a semantics for the input sentence  . This is done compositionally as the phrase structure is built  , and uses only locally available information . Thus every constituent which is built syntactically has a corresponding semantics built for it at the same time  , using only information available in the phrasal subtree which it immediately dominates  . This locality constraint in computing the semantics for constituents is an essential characteristic of HPSG  . For a more complete description of the semantic treatment used in the HPSG system see Creary and Pollard  \[2\]  . 
Head-driven Active Chart Parser
A crucial dilference between the HPSG system and its predecessor GPSG is the importance placed on the head constituent in HPSG  . \[ nHPSG it is the head constituent of a rule which carries the subcategorization information needed to build the other constituents of phrase structure of a sentence  , rather than left to right through the sentence string  . 
The parser itself is a variation of an active chart parser  \[4  , 9 , 8 , 13\] , modified to permit he construction of constituents head first  , instead of in left-to-right order . 
In order to successfully parse " head first " , an edge * must be augmented to include information about its span  ( i . e . its position in the string ) . This is necessary because heaA can appear as a middle constituent of a rule with other constituents  ( e . g . complements or adjuncts ) on either side . Thus it is not possible to record all the requisite boundary information simply by moving a dot through the rule  ( as in Earley )  , or by keeping track of just those constituents which remain to be built  ( as in Winograd )  . An example should make this clear . 
Suppose as before we are confronted with the task of parsing the sentence The manager works  , and again we have available the gramma rule R1 . Since we are parsing in a ~ head first " manner we must match the H constituent against some substring of the sentence  . 
But which substring ? In more conventional chart parsing algorithms which proceed left to right this is not a serious problem  , since we are always guaranteed to have an anchor to the left  . We simply try building the \[ eftmost constituent of the rule starting at the \[ eftmost position of the string  , and if this succeeds we try to build the next \[ eftmost constituents arting at one position to the right of wherever the previous constituent ended  . However in our case we cannot ausume any such anchoring to the left  , since as the example illustrates . 
the H is not always leftmost.
The solution we have adopted in the HPSG system is to annotate a chedge with information about the span of substring which it covers  . In the example below the inactivedge E1 is matched agains the head of rule R1  , and since they unify the new active edge E2 is created with its head constituent instantiated with the feature specifications which resulted from the unification  . This new edge E2 is annotated with the span of the inactive edge El  . Sometime later the inactive edge I , : 3 is matched against he " np " constituent of our active edge  E2  , resulting in the new active edge E . I . The span of E4 is obtained by combining the starting position of  E3 i . e . t ) with the finishing postion of E2(i . e . 
3) . The point is that edges ~ L reconstructed from the head out  , so that at any given tame in L helife cycle of an edge the spanning informatiun on the edge records the span of contiguous substring which it covers  . 
Note that in the transition from rule ill to edge  1~2 we have relabeled the constituent markers z , cl , ~nd h with the symbol s ~ , np , ~ utd VP respectively . 
This is done merely a . s ~ tmnemouic device to reflect the fact that once the head of the edge is found  , the subcategorization information on that head ( i . e . the values of the " SUHCAT " feature of the verb work  . s ) is Anedi\[e is , Iooe ~ yspea & ing ,   , -tninl Cantiation of a nile witll~nnle of tile \[ e ~ urml on conlltituent llm ~ dentore spm : if l ?  . 
propagated to the other elements of the edge , thereby restricting the types of constituents with which they can be satisfied  . Writing a constituent marker in uppercase indicates that an inactive edge has been found to instantiate it  , while a lower case ( not yet found ) constituent in boldface indicates that this is the next constituent which will try to be instantiated  . 
El . V < 3.3 >
RI . x -> ciha * g2.s < 3.3 > -> np VP a*
E 3. NP < I , 2 > "
E2.s < 3,3>-> np VP a*
E4.s < 1.3 > -> ~ PVPR*'
Figure 5: Combining edges and rules.
Using Semantics Restrictions
Parsing ~ head first " offers both practical and theoretical advantages  . As mentioned above , the categories of the grammatical relations subcategorized for by a particular head are encoded as the SUBCAT value of the head  . NowGR's are of two distinct types : those which are ~ saturated "  ( i . e . do not subcategorize for anything themselves ) , such as subject and objects , and those which subcategorize for a subject ( i . e . controlled complements ) . One of the language-universal grammatical principles  ( the Control Agreement Principle ) requires that the semantic controller of a controlled complement always be the next grammatical relation  ( in the order specified by the value of the SUBCAT feature of the head  ) after the controlled complement to combine with the head  . But since the HPSG parser always finds the head of a clause first  , the grammatical order of its complements , as well as their semantic roles , are always specified before the complements are found  . As a consequence , semantic processing ~ f constituents can be done on the fly as the constituents are found  , rather than waiting until an edge has been completed  . Thus semantic processing can be do . e extremely locally ( constituent-to-constituent in the edge , rather than merely node-to-node in the parse tree as in Montague semantics  )  , and therefore a parse path , an beab and one donsemantic grounds ( e . g . sort a lilt con-sistency ) in the rniddle of constructing an edge . lath is way semantics , as well as syntax , can be used to control the parsing process . 
Anaphora ill HPSG
Another example of how parsing ~ head first " pays oil is illustrated by the elegant technique this strategy makes possible for the binding of intr ~ entential a ~ taphors  . This method allows us to assimilate cases of bound anaphora to the same general binding method used iuthe HPSG system to handle other non -lexically-governe dependencies ~ u cha  . s~ap . ~, ~, ttt ~ ro ~, t . ive pronouns , and relative pronouns . Roughly , the unbound dependencies of each type on every constituent are en-?coded as values of a  , nappropriate stack-valued feature axe kept track of by two binding features  , REFL ( for reflexive pronouns ) and BPRO\['or personal pronouns available to serve as bound anaphors  . According to the Binding Inheritance Principle , all categories on binding-feature stacks which do not get bound under a particular node are inherited onto that node  . Just how binding is effected depends on the type of dependency  . 
In the case of bound anaphora , this is accomplished by merging the relevant agreement information  ( stored in the REFL or BPRO stack of the constituent containing the anaphor  ) with one of the later GR's subcategorized for by the head which governs that constituent  . 
This has the effect of forcing the node that ultimately unifies with that GR  ( if any ) to be the sought-after antecedent . The difference between reflexives and personal pronouns is this  . The binding feature REFL is not allowed to inheriton to nodes of certain types  ( those with CONTROL value \[ N' rRANS , thus forcing the reflexive pronoun to become locally bound  . In the case of nonreflexive pronouns , the class of possible antecedents is determined by n  , ,difying the subcategorization information on the hel  , ,l governing the pronoun so that all the subcategorized-fl ~ rGR's later in grammatical order than the pronoun are " contra-indexed " with the pronoun  ( and thereby prohibited front being its antecedent  )  . Binding then takes place precisely as with reflexives  , but somewhere higher in the tree . 
We illustrate this d~ttt tction v , ti,khI . ~ O examples . 
\[ n sentence SI below told subcategorizes for three constituents : the subject NP Pullum  , the direct object Gazdar , and the oblique object PP about himself . ' Thus either Pu Uumor f ; uzdura repo~ible antecedents of himself , but not Wasow . 
SI . Wasow was convinced that Pullum told
Gazdarabouthimself.
$2. Wasow persuaded Pullum to shave him.
\[ n sentence 52 shave subcategorizes for tile direct object NP him and an NP subject eventue  . tly tilled by the constituent Pullum via control . Since the subject position is contra-indexed with tile pronoun  , PuUum is blocked from serving a ~ the a , tecedent . The pro , mun is eventually bound by the NP Wanou J higher up in the tree  . 
Heuristics to Optiudze . "Joareh'\['heliPS ( ; system , based as it is upon a carefully developed hngui ~ tictheory  , has broad expressive power . In practice , how-ver , much of this power is often not necessary . To exploit this fact the IiPSC , system u . ~ cs heuristics to help r , ,duve the search space implicitly defined by the grammar  . These heuristics allow the parser to produce an optimally ordered a genda of edges to try ba  . sed on words used in tile sentence , and on constituents it has found so far . 
? The pN pOlltiOlltltreltt~eel~lttt ~tllF:isac:~setam'king  . 
One type of heuristic involves additional syntactic information which can be attached to rules to determine their likelihood  . Such a heuristic is based on the currently intended use for the rule to which it is attached  , and on the edges already available in the chart . 
An example of this type of heuristic is sketched below  . 
RI . x -> clha *
Heuristic-l : Are the features of cl + QUE ? Figure  6: A rule with an attached heuristic . 
Heuristic-I encodes the fact that rule RI , when used in its incarnation as the S--NP VP rule  , is primarily intended to handle declarative sentences rather than questions  . Thus if the answer to Heuristic-1 is " no " then this edge is given a higher ranking than if the answer is " yes "  . This heuristic , taken together with others , determines the rank of the edge instantiated from this rule  , which in turn determines the order in which edges will be tried  . The result in this case is that for a sentence such as  53 below , the system will prefer the reading for which an appropriate answer is "  . a character in a play by Shakespeare " , over the reading which has as a felicitous answer " Richard Burton "  . 
S3. Who is Hamlet ?
It should be empha~sized , however , that heuristics are not an essential part of the system  , as are the feature passing principles , but rather are used only for reasons of efficiency  . In theory all possible constituents permitted by the grammar will be found eventually with or without heuristics  . The heuristics siu , ply help a linguist tell the parser which readings are most likely  , and which parsing strategies are usually most fruitful  , thereby allowing the parser to construct the most likely reading first  . We believe that this clearly diifer euti-ares\[ IPSG from " adhoc " systems which do not make sharp the distinction between theoretical principle and heuristic guideline  , and that this distinction is an izn-portant one if the natural language understanding programs of today are to be of any use to the natural language programs and theories of the future  . 
ACKOWLED (4EME1 NTS
We would like to acknowledge the valuable as si -tance of Thomas Wasow ~ md Ivan Saghttile writing of this paper  . We would also like to thank Martin Kay and Stuart Shi  . e ~: rlottlke ~ rtte\[p\[u\[cuttutteut , aly on an earlier draft . 


Ill Bresnan , J . ( ed ). (1982)
The Mental Representation of Grammatical Relations  , The MIT Press , Cambridge , Mass . 
\[ z ) Creary , L . and C . Pollard ( 1985 ) ~A Computational Semantics for Natural Language  "  , Proceedings of the gSrd Annual Meeting of the Association for Computational Linguistics  . 
\[31 Dowry , D . R .   ( 1982 ) "Grammatical Relations and Montague Grammar " , In P . Jacobson and G . K . Pullum ( eds . ), The Nature of Syntactic Representation D . Reidel Publishing
Co ., Dordrecht , Holland.
L41 Earley , J .   ( 1970 ) " An efficient contextfree parsing algorithm " , 
CACM 6:8, 1970.
\[5t Fliekinger , D . , C . Pollard . T . Wasow ( 1985 ) " Structure-Sharing in Lexical Representation " , Proceedings of the 23rd Annual Meetin ! l of the Association for Computational Linguistics  . 
i61 Gawron , 3 . et al ( 1982 ) " Processing English with a Generalized Phrase Structure Grammar "  , ACL Proceedings 20 . 
it ! Gazdar , G . et al ( in pres , s)
Generalized Phrase Structure (; rammar,
Blackwell and Harvard University Press.
Is ! Kaplan , R .  ( 197: . ! ~ A General Syl ! tlxt : l , ic Processor " , la Rustin ( ed . ) Natural Langua~te Proeessiny . Algorithmics Press,

Kay , M . \[t973) " The MIND System ", luRusl , in ( ed . ) Natural Language Processiur . i . Algorithmics Press , N . Y . 
it 01 Kay , M .   ( forthcoming ) " Parsing in Functiotml Uailicatiou Grammar " . 
iLll Pollard . C . (198, 1)
Generalized ContextFree (; rammur ~, Ile , ad(:r . m-mar . s , and Natural L , mtl U . V e,I't n . D . Dissertation,

Pollard , C .   ( forthcotnitlg ) " A Semantic Approlu : hto Ilhuling in ; tMovms-trata\[Theory " , To appe , ~ rinLin!luistic ~ and

131 Winograd , T . ( tg ~ O)
Language as a Uo~nitivel ' rocess.
Addi ~ on-W ~ lcy , lteadiag , Marts.

