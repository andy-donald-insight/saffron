Efficient Integrated Tagging of Word Constructs
Andrew Bredenkamp
Frederik Fouvry
Dept . Language and Linguistics
University of Essex
Wivenhoe Park

Essex CO 43 SQ
United Kingdom
Thierry Declerck

University of Stuttgart
D-70174 Stuttgart

thierry@ims,uni-stuttgart , de
and rewb , fouvry ~ essex.ac.uk
Bradley Music
Center for Sprogteknologi
N jals gade 80
DK-2300 Copenhagen S

music ~ cst , ku . dk

We describe a robus text-handling com-
ponent , which can deal with freetext in
a wide range of formats and can suc-
cessfully identify a wide range of phe-
nomena , including chemical formulae,
dates , numbers and proper nouns . The
set of regular expressions used to cap-
ture numbers in written form (" sech-
sund zwanzig ") in German is given as
an example . Proper noun " candidates "
are identified by means of regular ex-
pressions , these being then rejected or
accepted on the basis of runtime in-
teraction with the user . This tagging
component is integrated in a large scale
grammar development environment , and
provides direct input to the grammat-
ical analysis component of the system
by means of " lift " rules which convert
tagged text into partialinguistic struc-

1 Motivation
1.1 The problem : messy details
Messy details are text constructs which do not lend themselves well to treatment by traditional techniques for linguistic analysis  , when cetheir ' messines s ' . Typical examples are numbers , codes or other ( sequences of ) wordforms which can occur in many variations ( often in finite )  , making impossible a comprehensive tratment by traditional means  . 
There are various types of phenomena classified as messy details which can be subclassified according to a twhat level of generality as regards text structure they occur  , viz . general format level , sentence level and word level phenomena . 
General . format level phenomena occur over sentence boundaries  , example being headers , meta-comments and tables . Phenomena classified as sentence level occur within a single sentence  , but cannot be considered word constructs of a fixed nature  . These are more ' linguistic ' than the usual messy details  , but are considered messy details since they lend themselves to partial analysis via a similar type of preprocessing  . Examples of these are the use of parentheses and commas which can be used within practical implementations as a basis for segmentation during preprocessing  . 
Word level phenomena are usually the most frequently occurring messy details  , including such things as dates , document references of various sorts , codes , numbers and proper nouns . For any realistic application these types of construct must be processed efficiently  , the alternative being coding them individually in some lexicon and/or implementing sets of grammar rules for parsing them syntactically  . 
This problem area was given priority in the EU -sponsored LSG RAM project  ( LRE61-029 ) which aimed to integrate an approach to mess y details into a largescale grammar implementation  . The coverage of the grammars developed was based on corpus analyses within each language group of the project  , these revealing a large number of messy details of the types mentioned  . What was called for then was an efficient means of identifying word-level messy details  ( or word constructs ) such that they could be processed in a general way  , avoiding additional grammar rules and the need for an infinite number of lexical entries  . 
2 The basic approach 2 . 1 Identif ication using regular express ions The types of word construct of interest here lend themselves well to identification by matching regular expressions over each input sentence  ( considered as a record )  , tagging them as specific instances of general phenomena  ( e . g . dates , numbers , etc . ) . 
awk is a programming language specifically designed for in this type of string manipulation provisions for treating text in the form of records  . 
aukreads input record by record , matching user-defined regular expressions and executing corresponding actions according to whether a match has been retold  . 
The regular expressions can be stored invariables and reused to build more complex expressions  . This is important , as some of the phenomena we were attempting to match were complex  ( see below ) and occurred in avm'iety of formats . 
The auk-implemented tagger developed for this project  , tag\[t , can be used as a standalone tagger for SGML texts  . It has been integrated within the text handling procedures of the ALEP system after sentence recognition and before word recognition  . When a pattern matches against the input , the matched string is replaced with a general tag of the relevantype  ( e . g . DATE , NUMBER ) . Subsequent agging and morphological parsing then skip these tags  , and further processing ( i . e . syntactic analysis ) is based on the tag value , not the original input string . 
2.2 Sample case : currency patterns in
German tag it has been integrated into the German LS-GRAM grmnmar for the identification of word constructs occurring in the mini-corpus taken as the departure point for the work of the German group  , consisting of an article on economics ( taken from the weekly newspaper " DieZeit " )  . As usual when using ' real world ' texts , mm~ymessy details were found , including dates and numbers used within t ) ercentages and , as would be expected from the text type , within amounts of currency . 
These occur both with and without numerals , e.g.
"16 , 7 Millionen Dollar " , "Sechs und zwanzig Mil-lim'den D-Mark " . The text examples are especially problematic given the German method of expressing the ones -digit before the tens-digit  , e . g . 
" Sechs und zwmmig " is literally " six-and-twenty " .
In order to deal with this phenomenon , regular expression patterns describing the currency m nounts were defnedinaw k  . First , patterns for cardinals were specified , e . g ~ Umlauted characters and " if ' are matched by the system  , though they are not shown here . 
Note that regular expressions are specified as strings and must be quoted using pairs of double quotes  . Variables are not evaluated when they occur in quotes  , so quoting is ended , and then restarted after the variable name , whence the proliferation of double quotes within the complex patterns  . 
Some auk syntax : "=" is the assignment operator , parentheses are used for grouping , "1" is the disjunction operator , "?" indicates optionality of the preceding expression  , "+" means one or more instances of the two_to_nlne : "  ( \[ Zz \] . ei I\[Dd\]rei I\[Vv\]ierI"\\[Ff\ ] unfI\[Ss\]echsI\[Ss\]iebenI\[Aa\] chti\[Nn \] eun  ) " one_to_nine = " ( lee\]in\["two_to_nine " ) " card=" ( " one to_nine " ) " number="\[09\]+ (  ,  \[09\]+ ) ?" range=" ( " number " i " card " ) " The actual pattern used in the implementation is more complex and goes up to  999  , but the example shows the principle . Given this set of variables , the pattern assigned to card can mat d , the text version of all cardinal numbers from \] to  999  , e . g . " Drei " , " Neunzehn " , " Z weiund zwanzig " , " A chthundertFf inflmdvierzig " , etc . The value assigned to range can match number , optionally with a comma as decimal point , e . g .  "99,09" . The following patterns are also needed : amount =  "  ( Millionenl Milliarden ) " currency = " ( Mark ID-Mark I Dollar ) " carmeasure=" (   ( " amount " ( " currency " )  ? ) I "\" ( " currency " )   ) " measure = I ,   ( " range " " cur measure " ) " The last two patterns describe define measure being the succession of a cardinal number  ( as a digit or a string ) followed by curmeasure , being the concatenation of amount and currency . 
But both of them , amount and currency , are defined as being optional . So that inputs like "30 , 6 Mill\[arden Dollar " , " Z weiund zwanzig Dollar " or " D reiund vier zig Mill\[arden Dollar " are automatically recognized  . But the definition of ' measure ' disallows the tagging of " Z weiun dz wanzig " asa'measure ' expression  . The tag provided for this string will be the same as for any other cardinals  . 
tag it applies these patterns to each record within the input  , assigning the appropriate tag information in case a match is found  . Further processing is described below . 
3 Extens ion fo r p roper nouns : in teract ive tagging Proper nouns present another problem that falls undermessy details  . A small extract from the corpus used for tim English grammar showed a wide range of possible proper noun configurations : " James Sledz "  , " Racketeer Influenced and Cor-rupt Organizations  "  , " Sam A . Call ", " Mr . Yasuda ", " Mr . Genji Yasuda ", .   .   . 
Regular expressions can catch several of those cases  , but it is difficult to get certainty , e . g . " Then Yasuda .   .   . " vs " Genii Yasuda ": one can never besure that an English word is not a name in another language  . Since this is a preprocessing treatment , here is no disambiguating information present , and fully automatic tagging cannot be preceding expression  , square bracket surround alternative characters ( possible specified as a range , e . g . 

1029 done , unless the program can have access to either some lookup facility and/or caniater ~ ct with a human user  . 
3.1 Patterns for proper nouns
For financial texts , the domain of our reference corpus , the proper nouns are company or institution names and person names  . Product and company names can be very unconventional  . Therefore the regular expressions need to be rather generous  . The interaction with the user and the dictionaries will provide a way to tune the effect of these expressions  . 
We defined the proper noun regular expression to be nearly anything  , preceded by a capital . Person names can contain initials , and they might be modified by titles (" Mr " ,   .   .   . ) or functions , business names can be modified by some standard terminology  ( like " Ltd . ") . Lowercase words are allowed if they are not longer than three chm ' acters  ( for nmnes containing " and " etc . ) . 
3.2 Interacting with the user
Tagging proper nouns presents a special problem , since , unlike the case of numbers and dates , there is a great deal of uncertainty involved as to whether something is a proper noun or not  . Therefore a natural extension to tag it was the implementation of an interactive capability for confirming certain tag types such as proper nouns  . 2 If a proper noun is found , then the tagger first does some lookup to limit the number of interactions during the tagging  . We used the two following heuristics : 1 . Hasit already been tagged as a proper noun ?
If so , do it again.
2 . Hasit already been offered as a proper noun , but was it rejected ? If so , and if it occurs at the beginning of a sentence , reject it again . 
Those two checks are kept exclusively disjunctive . If a word occurs both as a proper noun and as a " non-proper noun "  , the user will be asked if he or she wants it to be tagged  . This allows one to use different name dictionaries for different exts  . 
If the program itself is certain that a proper noun is found  , then it tags it and goes on to a next match . Otherwise it asks the user what to do with the match that was found  . There are two possible answers to this question :   1  . The user accepts the match as a proper noun . The program tags it , stores it for future use , and proceeds . 
2The graphical interface to the interactive tool has been implemented in Tel/Tk  . 
When the match is not entirely a proper noun , the matching string can be edited . This consists of removing the words before and/or after the first proper noun in the match  .   3 The remaining substring of the match is tagged as a proper noun and stored  . The words before the first word are skipped ( and also stored )  ; everything that comes after the tagged proper noun is resubmitted  . 
2. The user rejects the match that is offered.
The program stores it ( as a " non-proper noun " ) and proceeds . 
4 Integration with linguistic analysis
The ALEP platform ( Alshawi et al ,  1991 ) provides the user with a TextHandling ( TH ) component which allows a " preprocessing " of input  . 
An ASCII text will first go through a processing chain consisting in a SGML-based tagging of the elements of the input  . The default setup of the system defines the following processing chain : the text is first converted to an EDIF  ( Eurotra Document Interchange Format ) format . Then three recognition processes are provided : paragraph recognition  , sentence recognition and word recognition . The output from those processes consist of the input decorated with tags for the recognized elements : ' p ' for paragraphs  , ' S ' for sentences , ' W ' for words ( in case of morphological nalys is , the tag'/4' is provided for morphemes ) and ' PT ' for punctuation signs . Some specialized features are also provided for the tagged words  , allowing to characterize them more precisely , so for exmnple ' ACR0' for acronyms and so on . 
So the single input " John sees Mary . " after being processed by the TH component will take the form : < P > < S > < W > John </ W > < W > sees </ W > < W > Mary </ W > < PT >  . </PT><IS><IP><P > and </ P>mark the beginning and the respective ending of the recognized paragraph structure  . The other tags must be interpreted analogously . 
In the default case , it this this kind of information which is the input to the TH-LS component  ( Text-Handling to Linguistic Structure ) of the system . Within this component , one specifies so called ' tsAs ' ( text structure to linguistic structure ) rules , which transforn l the TH output into 3To extend the matches , the user would need to change the regular expressions  . 
1030 partialinguistic structure ( in ALEP terminology , this conversion is called lifting ) . The syntax of these lift rules is the following : ts_is_rule  ( < id > , < tag_name > , \[< features > f , < tag content > ) where : < ld > is a Linguistic Description ( LD )  ; < tag_name > is the name of an SGML tag ( e . g . ' S ' , ' W ') ; < features > is a list of feature-valued scrip- -tions of the tag's features  ; < tag content > is tile atomic content of tile string within the tag  ( optional in the lift rule )  . 
This kind of mapping rule allows a flow of information between text structures and linguistic structures  . So if the input is one already having PoS information  ( as the result of a corpus tagging )  , timTH-LS is the appropriate place to assure the flow of information  . This allows a considerable improvement of parse time  , since some information is already instantiated before the parse starts  . 
The TIt component of the ALEP platform also foresees the integration of user-defined tags  . The tag < USR > is used if the text is tagged by a user-defined tagger  , as is done when processing messy details . 
When tag it matches a pattern against im input , the matched string is replaced with an appropriate USR tag  . Thus " l ) reiund vierzig Milliardenl ) ollm '" is matched by the pattern measure ( see above )  , and is replaced by the SGML markup < USR VAL = "Drei und vierzig Milliar den Dollar " LEVEL = MTYPE = MEhSURE>Drei und vierzig Milliarden_Dollar</USR > Note that tile matched sequence is copied into the attribute VAL and that in the data content spaces are replaced by underscores  . For some pattern types , a generalized representation of the matched sequence is computed and stored in an attribute CONY  . For instance , when the pattern for dates matches the input " March  15  ,  1995" , CONV is assigned a standardized version , i . e . CONV="95/03/15" . 
This version with USR tags inserted is then processed by the set of lift rules  . The \[ bllowing en-erallif trule does the conversion for all USR tags : ts_is rule  ( 
Id : sign => sign:string => STRING , synsem=>synsem:syn=>syn:constype=> morphol:lemma=>VAL valuo  , lu => TYPE value , ' USR ' , \[' TYPE '=> TYPE value , ' VAL '=> VAL value \] , 
STRING).
Here we ( : an seetile mapping of inforlnation between the user-defined tlS/t tag  ( the attributes of which are listed in timlast line of this rule  ) and the linguistic description ( ' ld'--' linguistic description ' , a structured type within tile Typed Feature System  )  , using the rule-internal variable TYPE value : the value of the attribute TYPE is assigned to the lexical unit  ( ' lu ' ) value of the linguistic description . After applying this rule to the result of matching " D reiund vierzig Dollar "  , the ld is the following : id : sign = > sign : string =>' D reiund vierzig_Dollar '  , synsem=>synsem:syn=>syn:constype=> morphol:lemma=>'Drei und vierzigDollar'lu = >' M ~ ASURE ' Although the original input sequence is available as the value of the feature lemma  , further processing is based solely on the lu value ' MEASURE'  , thus making it possible to have a single lexical entry for handling all sequences matched by the pattern measure shown above  . The definition of such generic entries in the lexicon keeps the lexicon smaller by dealing with what otherwise couhl only be coded with an infinite number of entries  . 
In addition , treating such word constrncts as a siregle unit gives a significant improvement in parsing runtirne  , since only the string ' MEASURE ' is used as a basis for further processing  , instead of the original sequence of three words . Finally , runt in misalso improved and development eased by the fact that no gramma rules need be defined for parsing such sequences  . 
5 Conclusion
The implementation described here handles a variety of word-level messy details efficiently  , speeding up over all processing time and simplifying the grammars and lexica  . General format level and sentence lvel phenomena cn be handled in a similar way  . Within our project , reimplementation using a more powerful tool per1 is taking place , allowing filrther extensions to the flmctionality . 
We maintain that user-interaction combined with some table lookup is the only viable approach to the robus tagging of freetexts  . The fact that an interactive tagging tool can be so easily inte~grated into the linguistic processing system is of obvious and considerable benefit  . 

Alshawi H . , Arnold \]) . J . , Backofen R . ., Carter 1) . M . , Lindop J . , Netter K . , Pulrnar ~ S . , Tsuji , I . , Uszkoreit 11 .  1991 . l '\] urotra ET6/h Rule ~ brmalism and Virtual Machine Design Study ( final report )  . CEC 1031,
