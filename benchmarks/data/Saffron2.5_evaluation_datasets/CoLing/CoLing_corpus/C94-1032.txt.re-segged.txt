A Stochastic Japanese Morphological Analyzer Using a 
Forward-DP Backward-A * NBest Search Algorithm
Masa . aki NAG ATA
NTT Network Information Systems l ~, ~ bor~t torics
1-2356 Take , Yokosuka-Shi , Kanagaw ~ t , 238-03 Japan
( tel)4-81-468-59-2796
( fax)+81-468-59-3428
( email ) nagata@nttnly.ntt.jl )

We present a novel method for segmenting the input sentence into words and assigning parts of speech to the words  . It consists of a statistical language model and an efficient wo-pa~qs Nbest search algorithm  . The algorithm does not require delimiters between words  . 
Thus it is suitable for written Japanese . q'he proposed Japanese morphological naly zer achieved  95  . l % recall and 94 . 6% precision for open text when it was trained and tested on the ATI'?Corpus  . 
1 Introduction
In recent years , we have seen a fair number of l ) al ) ers reporting accuracies of more than 95% for English part of speech tagging with statistical language modeling techniques  \[24  ,  10 ,  11\] . On the other hand , there are few works on stochastic Japanese morphological nalysis  \[9  ,  12 ,  14\] , and they don't seem to have convinced the Japanese NLP community that the statistically -based teclmiques are superior to conventional rule-based technique such as  \[16  ,  17\] . 
We show in this paper that we can buihla stochastic Japanese morphological naly zer that offers approximately  95% accuracy on a statistical language modeling technique and an efficient two-pass Nbest search strategy  . 
We used tile simple tri-POS model as the tagging model for Japanese  . Probability estimates were obtained after training on the ATIl  ) ialogue Database\[5\] , whose word segmentation ad part of speech tag assignment were laboriously performed by hand  . 
We propose a novel search strategy forgetting the Nbest morphological nalys is hypotheses for the input sentence  . It consists of the forward dynamic programming search and the backward A * search  . The proposed algorithm amalgamates and extends three wellknown algorithms in different fields : the Minimum Connective-Cost Method  \[7\] for Japanese morphological analysis , Extended Viterbi Algorithm for character recognition  \[6\]  , and " l ~' ee-Trellis NBest Search for speech recognition  \[15\]  . 
We also propose a novel method for handling unknown words uniformly within the statistical pproach  . 
Using character trigram smstim word model , it generates the Nbest word hypotheses that match the leftmost substrings starting at a given position in the input sentence  . 
Moreover , we propose a novel method for evaluating the performance of morphological analyzers  . Unlike English , Japanese does not place spaces between words . It is difficult , even for native Japanese , to place word boundaries consistently because of the agglutinative nature of the language  . Thus , there were no standard performance metrics . We applied bracketing accuracy measures \[1\] , which is originally used for English parsers , to Japanese morphological naly zers . We also slightly extended the original definition to describe the accuracy of tile Nbest candidates  . 
In the following sections , we first describe the techniques used in the proposed morphological naly zer  , we then explain the cw duation metrics and show the system's performance by experimental results  . 
2 Tagging Model 2 . 1 Tr i - POS Mode l and Re la t ive F re - quency Training We used the tri-POS  ( or triclass , tritag , tri-Ggram etc . ) model ~ L stile tagging model for Japanese . Consider a word segmentation f the input sentence W = wl  w2  .   .   . w , ~ and a sequence of tags T = t its .   . , t ,, of the same length . The morphological analysis tmsk cau I ) e formally defined , ~ finding a set of word segmen-t at . ion and parts of speech ~ ssignment that maximize the joint probability of word sequence arm tag sequence P  ( W ,  7') . In the tri-POS model , the joint probability is approximated by the product of parts of speech trigram probabilities P  ( tilti_2 , ti_l ) and word output probabilities for given part of speech P  ( wl \] ll ) : r ( w , : r ) =\] ~ r(tdt , _o . , t,_x)r'(w,lt4(1) i=1 symbols as follows . 
P ( W,T ) = P ( ql#)P ( wtltt)P ( t, . l# , tl)P ( w21 t ~) ~ IP ( tilti_2 , ti_l)P(willi)P(#\[t , ,_l , ? , , )   ( 2 )   i=3 where "#" indicates the sentence boundary marker . If we have some tagged text available , we can estimate the probabilities P ( tdti_2 , ti_l ) and P ( wiltl ) by computing the relative frequencies of the corresponding events on this data :  . 
N(ti_2, ti1,tl)
P ( tifti-2'ti-t ) = f(qltl-2'ti-x)-iV(ti_ .   . , , ti_,)(3)
P ( wilti ) = f(wilt,)--N(w , t ) ('1)
N(t ) where f indicates the relative frequency , N(w , t ) is t ! , e number of times a given word w appears with tag l  , aid N(li_2 , ti-l , tl ) is the number of times that sequer ~ cel ~ i _2t i_lli appears in the text . It is inevitable to s~irer from sparse-data problem in the part of speech tag trigram probability I  . To handle open text , trigram Frol : - ability is smoothed by interpolated estimation  , wi ~ ich simply interpolates trlgram , bigram , unigram , and ze-rogram relative frequencies\[8\] , P ( tilq_ ,   , q_l ) = q af ( tilt , _2 , q_ ,  )
A-q2f ( tdti_l ) + q tf ( ti ) + qoV ( 5 ) where f indicates the relative . frequency and V is a uniform probability that each tag will occur  . The nonnegative weights q is a tisf y q3 + q~+ q1  +  q0  =  1  , and they are adjusted so as to make the observed at a most probable after the adjustment by using EM algorithm ~-  . 
2.2 Order Reduction and Recursive

In order to understand the search algorithm described in the next section  , we will introduce the second order HMM and extended Viterbi algorithm  \[6\]  . Considering the combined state sequence U = ltl'tt2  . ., ttn , where ul = tl and ui = ti-tli , we have
P ( uilui_l ) = P ( tilti _=, ti_l ) (6)
Substituting Equation (6) into Equation ( l) , we have l We used 120 part of speedl tags . In the ATR Corpus , 26 parts of speech , 13 conjugation types , and 7 conjugation forms are defined . Out of 26,5 parts of speech ave conjugation . Since we used a list of part of speech , conjugation type , and conjugation form as a tag , there are 119 tags in the A'IT ? Corpus . We added the sentence boundary marker to them . 
a To handle open text , word output probahility P ( loilti ) must also be smoothed . Tiffs problem is discussed in a later section * Ls the unknown word problem  . 
Equation model.
' ill I ... Wi we have
P ( W , r ) = 1\] . P ( mlui-a ) P ( wilti )   ( 7 ) i = 1 ( 7 ) have the same form as the first order Conshler the partial word sequence HI /= and the partial tag sequence T i = t l  .   .   . tl,F(w ~, ~) = ?)( w,_,,~-,)P (~ d, . -~) p(wdtO(8)Equation (8) suggests that , to find the maxlmmn P ( I , Vi , 7\]) for each ul , we need only to : remember the maximum P ( W ?_I ,  7\]_1) , extend each of these probabilities to everyul by computing Eqnation  ( 8 )  , and select them ; uxin mmP ( ~/ Vi , Ti ) for each ui . ' thus , by increasing i by 1 t on , selecting the u . t t l a t maximize P ( W . ,7\]~) , and backtracing the sequence leading to the nmx in mm probability  , we can get the optimal tag seqnence . 
3 Search Strategy
The search algorithm consists of a forward dynamic programming search and a backward A * search  . First , a linear time dynamic programming is used for recording the scores of all partial paths in a table  3  . A backward A * algorithm based tree search is then used to extend the partial paths  . Partial paths extended in the backward tree search are ranked by their corresponding fill path scores  , which are cmnputed by adding the scores of backward partial path scores to the cot responding best possihle scores of the remaining paths which are prerecorded in the forward search  . Since the score of the incomplete portion of a path is exactly known  , the backward search is admissible . That is , the top N candidates are exact . 
3.1 The Forward DP Search
Table 1 shows the two data structures used in our algorithm  . The st , ' t , cture parses to restile information of a word and the best partial path up to the word  . 
Parse . start and parse . end are the indices of tile start and end positions of the word in the sentence  . 
Parse . posistile part of speech tag , which is a list of part of speech , conjugation type , and conjugation form in our system for Japanese . Parse . nth-order-~tate is a list of the last two parts of speech tags including that of the current word  . This slot corresponds to the combined state in the second order IIMM  . 
Parse . prob-so-far is the score of the best partial path from the beginning of the sentence to the word  . 
Parse . prev ? ousi the pointer to the ( best ) previous parse structure as in conventional Viterbi decoding  , which is not necessary if we use the backward N best search  . 
~lnfact , we use two tables , pa~se-\] . ist and path-~ap . The reason is described later . 

The structure word represents the word information in the dictionary including its lexical form  , part of speech tag , and word output probability given tt , epart of speech . 
Tableh Data structures for the Nbest algorithm start endpeanth-order-state prob-ao-far previous parse strttctul'e"tim beginning pasition of the word the end position of the word part of speech tag of the word a list of the la -' ~ t two parts  (  , fspeech the b , ~t partial path score from the start a pointer to previous parses trllettll'e word structure form \ ] lexical f  ,  . '- n of the word l ) Oa\[part of speech tag of the word prob_word outlmt probability Before explaining tim forward search  , we will define some flmctions and tables used in the algorithm  . In the forward search , we use a table called parse-list , whose key is the end position of the parse structure  , and wlm , sevalue is a list of parse structures that have the best partial path scores for each combined state at the end position  . Function register-to-parse-list registers a parse structure against the parse -list and maintains the best par-tim parses  . Function get-parse-list returns a list of parse structnres at the specified position  . We also use the fime tionlelt most-substr ings which returns a list of word structures in the dictionary whose lexical form matches the substring starting at the  . specified position in the input sentence . 
function ~ or ward-paae ( string ) begin initial-ate pO ; It Pods special symbols at both ends . 
for iff il to length ( string ) do for each parse in get-parse-list ( i ) do for each word ill leftmost-subatrings ( atring , i ) , I (7 poa-ngrma :- append(parse . nth-order-stato , list(word . poa ) ) if ( trane prob ( poe-ngrtm ) > O ) then new-parse : ' . make-parse O ; new-parse . mtart : ~ i ; new-parse . end : - i + length(word . form ); hey-pares , poe:-word . pea ; new-parae . nth-order-mtate : ~ rest(pos-ngram ); naw-pare e . preb-ee-far:-parae . prob-so-far * transprob(pos-ngram)*word . prob ; new-parse . previous := paras ; regieter-parse-to-parae-ltst ( new-parse )   ; register-pare e-to-path-map ( new-parse )   ; end if elld end endf in nl-etQp () ; i/R and lant rtmaition to thoe~d symbol . 

Figure h The forward DP search algorithm Figure 1 shows the central part of the forward dynamic programming search algorithm  . It starts from the beg , string of timinlmt sentence , and proceeds char-attar by character . At each point in tim sentence , it looks up the combination of the best partial parses ending at the point and word hypotheses tarting at that point  . If tim connection of a partial parse and a word llypothesis is allowed by the tagging model  , a new continuation parse is made and registered in the parse-list  . The partial path score for the new con-titular , on parse is the product of the best partial path score up to the poi  , g , the trigram probability of the last three parts of speech tags and the word output probability for LIfe part of speech  4  . 
3.2 The Backward A * Search
The backward search uses a table called path-map , whose key is the end position of tile parse structure  , and whose value is a list of parse structures that have the best partial path scores for each distinct combin ~ ties of the start position and the combined state  . The dilference 1 ) etween parse-list and path-map is that path -map is classi/ied by tim start position of the last word in addition to tim combined state  . 
This distinction is crucial for the proposed N best algorithm  , l " or timt brward search to tind a parse that maximizes Equation  ( 1 )  , it is the parts of speech sequence that matters . For the backward Nbest search , how ( wet , we wantN most likely word segmentation and part of speech sequence  . Parse-list may shadowless probable candidates that have the same part of speech sc : qnence for the best scoring candidate  , but differ in tim segment a L , on of the last word . As shown in Figure 1 , path-map is made during the forward search by the function register-parse-to-path-map  , which registers a parse structure to path-map and maintains the best partial parses in the table's criteria  . 
Now we describe the central part of timbackward A * search algorithm  . But we assume that the readers know the A * algorithm  , and expht in only the way we applied the algorithm to the problem  . 
We consider a parse structure , ~ q a state in A * search . Two slates are e ( platif their parse structures have the same start position  , end position , and combined state . The backward search starts at the end of the input  , sentence , and backtracks to the beginning of the sentence using t impath-map  . 
Initial states are obtained by looking up the entries of tim sentencend position of the path -map  . The successor states are obtained by first , looking u1 ) timentries of the path-map at the start position of the current parse  , then c be cldng whether they satisfy the constraint of the combined state transition in the second order IIMM  , aim whether the transition is allowed by the tagging model  . The combined state transition constraint means that t impart of speech sequence in the parse  . nth-order-state of the current parse , ignor-4In Figure 1 , function transprob returns the probability of given trlgral n  . Functions initial-step and final-s teptreat\[betl'aliSlt\[onsI % L slt lill ~ llce\]  , Ollldlll'ieg , ignoring the first element . 
The state transition cost of the backward search is the product of the part of speech trigram probability and the word output probability  . Tiles core estimate of the remaining portion of a path is obtained from the parse  . prob-so-~ars lot in the parse structure . 
The backward search generates the Nbest hypotheses sequentially and there is no need to preset N  . The complexity of the backward search is significantly less than that of the forward search  . 
4 Word Model
To handle open text , we have to cope with unknown words . Since Japanese do not put spaces between words , we have to identify unknown words at first . To do this , we can look at the spelling ( character sequence ) that may constitute a word , or look at the context o identify words that are acceptable in this context  . 
Once word hypotheses for unknown words are generated  , the proposed Nbest algorithm will find tile most likely word segmentation ad part of speech assignment taking into account he entire sentence  . Therefore , we can formalize the unknown word problem as ( letermin-ing the span of an unknown word , assigning its part of speech , and estimating its probability given its part of speech  . 
Let us call a computational model that determines the probability of any word hypothesis given its lexical form and its part of speech the " word model "  . The word model must account for morphology and wordfor-marion to estimate the part of speech and tile probability of a word hypothesis  . Fortile first approxinmtion , we used the character trigram of each part of sl ) eec has the word model . 
Let C =  cic  ~ . . , c , ~ denote the sequence of n characters that constitute word zv whose part of speechist  . 
We approximate the probability of the word given part of speech P  ( wlt ) by tile trigram probabilities , p( , , , I z ) = P , ( C)--f' , (~ , l# ,  #)~' , (~1# , <) u
IXP , (~, lc+-=,~-~)r,(#1c . ._l , . . , ) i=3 ( 9 ) where special symbol "#" indicates ttle word boundary marker  . Character trigram probabilities are estimated from the training corpus by computing relative frequency of character bigram and trigram that appeared in words tagged as t  . 
Pt(cilci-2 , q-i ) = f , ( c ~ l ~ -= ,  ? , -~) = Nt(ci_2 , Ci_l , ci )
N,(c~_ . ~ , i ) ( lO ) where Nt(ci_2 , ci_~ , ci ) is tile total number of times character trigram ci_2ci_~el appears in words tagged as t in the training corpus  . Note that the character trigram probabilities reflect the frequency of word tokens in tile training corpus  . Since there are more than 3 , 000 characters in Japanese , trigram probabilities are smoothed by interpolated estimation to cope with the sparse -data problem  . 
It is ideal to make this character trigram model for all open clmss categories  , l lowever , the amount of training data is too small for low frequency categories if we divide it by part of speech tags  . Therefore , we made trigram models only for tile 4 most frequent parts of speech that are open categories and have no conju ~ gation  . They are common noun , proper noun , sahenno/ln 5~ and nunleral . 
> ( estimate-paxt-of-spoech ~) ; Hiyako Hotel(C ~\] 2 . 762 191564 1723623E-7)(~f ~/~6 . 3406095003694205E-9)(~l~\]5 , 840424519473811E-19)(~(~5 . 7364195413101E-29 ) ) >  ( estimate-part-of-speech ~ I994 )   ( (~ ( )~ 1 , 8053860295767367 E-6) (~ Mo . s1224s6sls404~zE-17)(~-Jdrl:~"~%~2 . 28 868 4007 2465 24E-17)(~, ~7 . so s~s3~aso211e-20)) ; proper noun ; common noun ; sa_hannoun ; numeral ; numeral ; proper noun ; common noun ; sahennoun Figure 2: Nbest Tags for Unknown Words Figure 2 show two examples of part of speech estimation for unknown words  . Each trigram model returns a probability if the input string is a word belonging to the category  . In both examples , the correct category has the largest probability . 
>  ( get-leftmost-substriags-uith-word-model (   ( i ~ 4 ) - ~ M2 . 519457'597358691E-7)(~~;~f:~"~j~2 . 3449215070 189967E-8) (~~ ~ tlfj/~l~i\]7 . 02439907471337451-9)(~\]i,~,'12 . 375650975098567 E-9) (', l ~ J ~"4 . )'~; ' ~$ il 5 . 706 S ' (4990251415E-IO ) ( t ~ . ~' ~ j ~ c j ~,\] 4 . 735628004876359E-13)(~,~~$~18 . 928942348 1071831-14) ( i ~ )'~, ~ il 7 . 266613344265452E-14)(~1~,~\[~i~6 . 866d 9949613207E-16)(, l~b'RlJlIiG,~I2 . 45302390s251351sE-17))
Figure 3: NBestWordlIylm theses
Figure 3 shows the Nbest word hypotheses generated by using tile character trigram models  . A word hypothesis is a list of word boundary , part of speech assignment , and word probability that matchestile leftmost substrings starting at a given position in tile input sentence  . In the forward search , to handle unknown words , word hypotheses are generated at every position in addition to the ones generated by the function leftmost-subs ~  ; rings , which are the words foundilltile dictionary , l lowever , ill our system , we limited then tunl ) er of word hyl ) otheses generated at each position to 10 , for efficiency reasons . 
a A nountlmt can be used a ~ saverb when it is followed by a for lna  , \] verb"s~tr~t " , We applied the performance measures for English parsers  \[1\] to Japanese morphological analyzers . The basic idea is that morphological nalys is for a sentence can be thought of as a set of labeled brackets  , where a bracket corresponds to word segmentation and its la-  . 
bel corresponds to part of speech . We then compare the brackets contained in the system's output to the brackets contained in the standard analysis  . For the Nbest candidate , we will make the union of t\] , ebrackets contained in each candidate , and compare then r to the brackets in the standard . 
For comparison , we court , the number of I ) rackcts in the standard data ( Std )  , the number of brackets in the system output ( Sys )  , and then unlber of matching brackets ( M ) . We then calculate then leasurcs of recall ( = M/Std ) and precision ( = M/Sys )  . We also connt the number of crossings , which is tile mmt ber of c , ' mes where a bracketed sequence from the standard data overlaps a bracketed sequence from tile system output  , but neither sequence is completely coutained in the other  . 
We defined two equaiity criteria of brackets for counting tim number of matching brackets  . Two brackets are unlabeled-bracket-equal if the boundaries of the two brackets are tile same  . Two brackets are labeled-bracket . equal if the labels of the brackets ark the same in addition to unlabeled-I  ) racket-equal . In comparing the consistency of the word segmentations of two brack-clings  , wllich we call structure-consistency , we count the measures ( recall , precision , crossings ) by unlabeled-bracket-equal . In comparing the consistency of part of speech assignment in addition to word segmentation  , which we call label-consistency , we couut them by labeled-bracket-equal . 
-31 . 90894138309038 -38  . S9433~3fi658235 fi~b/tRllDll~iil-ill!lll , Til~lt-J-/lJ/ltl Dil , l . l~ko I if d~)-43 ,   I0367483fi46801 Figure 4: NBest Morphological Analysis hypotheses For example  , Figure 4 shows a sample of N-hestanal-ysls hypotheses , where the first candidate is the correct analysis a  . For the second candhlate , since there are ! )   ) rackets in tim correct data ( Std = 9 )  , 11 brackets in the second candidate ( Sys=ll ) , and 8n latciiing brackets ( M = 8) , tile recall and precision with respect to label consistency are  8/9 and 8/11  , respectively . For the top 6Probabilities m ' einliiltura \] log b ~ se . 
two candidates , since tliere ; ire 12 distinct brackets in tile systems otll . litlt and 9Inatehing brackets , tile recall and precision with respectohal ) el consistency are 9/9 aud 9/12 , respeetiw qy . For the third candidate , since the correct data and the third candidate differ in just one part of Sl  ) eech tag , the recall and precision wittlrespect o structure consistency are  9/9 and 9/9  , respectiw > ly . 
6 Experiment
Table 2: The aillount of training and test data ~_ ~ trahling texts closed test open  10 o 0 Sentences / -1~5  "  10i0   13899 Words ' 149059   13176  \[
Characters_2 67,122\[9422 ~98997
We used the NI'I~Dialogue Databaae\[5\] to train and test the proposed morphological nalys is method  . It is a corpus of approxium tely 800 , 000 words whose word segmentatio , l and part of speech tag assigmnent were laboriously performed by hand  . In tiffs experilneut , we only used one fourth of the A'F t ~ . Corl ) us , a portion of the keyl ) oard dialogues in the conference registration domain  . First , we selected 1 , 000 test sentences for all open test , arid used I . he others for training . Tile corpus was divided into 90% R ) r training and 10% for testing . We then selected 1 , 0 00 sentences from tile traiu-ing set and used them for a closed test  . The number of sentences , words , and characters for each test set and training texts are show niu ' Pable  2  . 
The training texts contained 6580 word types and 6945 tag trigram types . There were 247 unknown word types and 213 unknown tag trigram types intimopentest senteuces  . Thus , both part of speech tri-gralrll ) robabilities all d word output probabilities must be snioothed to handle open texts  . 
Table 3: Perccld . ;ige of words correctly segmented and tagged : raw parto\['speech bigram audtrigrmnI  2 I 98'l % I 8   9   .  7 ' ~ ~ \[90 . 7% \[ 0 . 007\[I : ~\[ os . , ~: ~\[8a .  ~  . s ~\] 84 . a %\ [ o . m2\]I'~9a . '2%I7s . ~/ oI 79 . 6% Io . o15 Ik5I<~l''i''i?I>in'/~%Ir(~ . o ~ Io . o~s_lFirst , as a I ) reliminary experiment , we compared tile perforn ) ances of part of speech bigram and trigram . 
Table 3 shows the percentages of words correctly segmented and tagged  , tested on the closed test sentences . 
The trigram model achiew ; d 97 . 5% recall and 97 . 8% precision flu " the top candidate , while tile bigram model achiew . ' d 96 . 2% recall and 96 . 6% precision . Although both tagging models sllow very high l ) er formane e , tile 20 , 5 trigram model outperformed tile bigram model in every metric  . 
We then tested the proposed system , which uses smoothed part of speech trigram with word model  , on the open test sentences . Table 4 show stile percentages of words correctly segmented and tagged  . In Table 4 , label consistency 2 represents the accuracy of segmentation and tagging ignoring the difference in conjugation form  . 
For open texts , tile morphological naly zer achieved 95 . 1% recall and 94 . 6% precision for the top candidate , and 97 . 8% recall and 73 . 2% precision for the 5best candidates . This performance is very encouraging , and is comparable to the state-of-the-art stochastic tagger for English  \[24  ,  10 ,  11\] . 
Since the segmentation accuracy of the proposed system is relatively high  ( 97 . 7% recall and 97 . 2% precision for the top candidate ) compared to the morphological analysis accuracy , it is likely that we can improve the part of speech assignment accuracy by refining the statistically-based tagging model  . We find a fair number of tagging errors happened in conjugation forms  . 
We assume that this is caused by the fact that the Japanese tagset used in tile ATR  . Corpus is not detailed enough to capture the complicated Japanese verb morphology  . 
100$580 q
Hogpholoqlcal Analylts Accuracy for N-I lest Sentences riw tr tqram  ( closed ce : ~ t ) - a- . - - raw bit/tam , a ~ t , ? ~ a ~ othedtrt ~ ram with I open text ) - o ,   .   . 
I moothed trl qram wi word a lot lell opeste ~ t  )  -~  . . . .
rawm with word moO ellapentext ~ . -irawt rrlra without wordnloc ~ ellopes text  )  - . 12-::- . 


II , I234
Rink
Figure 5: Tile percentage of sentences correctly segmented and tagged  . 
Figure 5 show stile percentage of sentences ( not words ) correctly segmented and tagged . For open texts , the sentence accuracy of the raw part of speech trigram without word model is  62  . 7% for the top candidate and 70 . 4% for the top 5 , while that of smoothed trigram with word model is  66  . 9% for the top and 80 . 3% for the top 5 . We can see that , by smoothing tile part of sllecch trigram and by adding word model to handle unknown words  , the accuracy and robustness of the morphological analyzer is significantly improved  . I lowever , tile sentence accuracy for closed texts is still significantly better that that for ol  ) entexts . It is clear that more research as to be done on the smoothing problem  . 
7 Discussion
Morphological analysis is an important practical problem with potential a pl  ) lication in many areas including kana-to-kanji conversion  7  , speech recognition , character recognition , speech synthesis , text revision support , information retrieval , and machine translation . 
Most conventional Japanese morphological naly zers use rule-based heuristic searches  . They usually use a connectivity rnatrix ( part-of-sl ) eech-pair grammar )   , as the language model . To rank the morphological-ysis hypotheses , they usually use heuristic such as Longest Match Method or Least Bunsetsu's Number 
Method\[16\].
There are some statistically-based approaches to Japanese morphological nalysis  . The tagging models previously used are either part of speech I  ) igram\[9 , 14\] or Character-based IIMM\[12\] . 
Both hem'istic-based and statistically-based approaches use t  . he Minimum Connective-Cost Method\[7\] , which is a linear time dynamic programming algorithm that finds the morphological hypothesis that hastile minimal connective cost  ( i . e . bigram-ba ~ sed cost ) as derived by certain criteria . 
q'o handle unknown words , most Japanese morphological analyzers u , ~e character type heuristics\[17\] , which is " a string of the same character type is likely to constitute a word "  . There is one stochastic approach that uses bigram of word formation unit  \[13\]  . t lowever , it does not learn probabilities from training texts  , but learns them fi'om machine readable dictionaries  , and the model is not incorporated in working morphological analyzers  , as fitr as the author knows . 
The unique features of the proposed Japanese morphological analyzer is that it can find tile exact N most likely hyl  ) otheses using part of speech trigram , and it can handle unlmown words using character trigram  . 
The algoril . hm can naturally be extended to handle any higher order Markov models  . Moreover , it cannat-nrally be extended to handle lattice -style input that is often used as t  . he output of speech recognition and character ecognition systems  , by extending the function ( leftmost-subatring a ) so as to return a list of words in the dictionary that matches the substrings in tile input lattice stm ' ting at the specified p  ( xqition . 
For future wot'k , we have to study the most effective way of generating word hypotheses that can hand h  . ? un : known words . Currently , we are limiting the number of word hypotheses to reduce ambiguity attile cost of accuracy  . We have also to study tile word model for open categories thai  , have conjugation , because the training 7Kana -to-kanji conversion is a pop ~ alar JI Lpanese input method on computer using ASCII keyboard  . Phonetic tran Herip-tion by Roams ( ASCII ) characters are input and converted fir , st to the Japanese syllabary hiragana which is then converted to orthographic trm~scrlption ncluding Chinese character kanjl  . 

Table 4: The percentage of words correctly segmented and tagged : smoothed trigram with word model smoothed trigram with word model  ( open text ) lal ) el consistency recall precision crossings 95 . 1% 94 . 6% 0 . 013 96 . 5% 88 . 0% 0 . 023 97 . 3% 82 . 1% 0 . 031 97 . 6% 77 . 4% 0 . 0'16 97 . 8% 73 . 2% 0 . 0 61 label consistency 2 recall precision crossings 95  . 9% 95 . 4% ( J . 013 97 . 0% 90 . 3% 0 . 023 97 . 6% 85 . 1% 0 . 031 97 . 9% 80 . 7% 0 . 046 98 . 1% 77 . 1% 0 . 060 structure consistency recall precision 97 . 7% 97 . 2% 98 . 2% 94 . 4% 98 . 5% 91 . 7% 98 . 7% 89 . 6% 98 . 8% 87 . 9% crossings 0 . 013 0 . 022 0 . 029 0 . 044 0 . 0 56 data gets too small to make trigrams if we divide it by tags  . We will probably have to tie some parameters to solve the insufficient data problem  . 
Moreover , we have to study the method to adap the system to a new domain  . To develop an m~supervised learning method , like the forwardbackward algorithm for IIMM , is a nurgent goal , since we can't always expect the availability of manually segmented and tagged data  . We can think of an EM algorithm by replacing maximization with summation in the extended Viterbi algorithm  , but we don't know how to handle unknown words in this algorithm  . 
8 Conclusion
We have developed a stochastic Japanese morphological analyzer  . It uses a statistical tagging model and an efficient two-passearch algorithm tollnd the N best morphological nalys is hypotheses for the input sentence  . Its word segmentation adtagging accuracy is approxl matcly  95%  , which is comparable to the star . e-of-the-art stochastic tagger for English . 

References\[14\]\[1\]Black , E . et al : " A Procedure for Quantit . a-tively Comparing the Syntactic Coverage of English Grammars "  , I)AIH'A Speechan ( IN alm ' a \] Language Workshop , pp . 306-311, Morgan Kauf-\[15\]mann , 1991 . 
\[2\] Charniak , E . , Ilendrickson , C . , Jacol ) son , N . , and Perkowitz , M . : " Equations for Part-of ~ Speech Tagging " , AAAI93 , I ) 1) . 784-789, 1993 . \[16\]\[3\]Church , K . : " A Stochastic Part of Speech Tagger and Noun Phrase Parser for English "  , ANLP-88 , pp . 136-143, 1988 . 
\[4\] Cutting , D . , Kupiec , J . , Pederseu , J . , and Sibnn , P . : " A Practical Part-of-Speech Tagger " , ANLP-\[17\]92 , pp . 133-140, 1992 . 
\[5\]E hara , T . , Ogura , K . and Morimoto , T . :" h'rl ~ Dialogue Database , "1CSLP-90, pp . 1093-1096, 1990 . 
\[6\]IIe,Y . : " Extended Viterbi Algorithm for Second Order Ilidden Markov Process "  , ICI'R-88 , pp . 718-720, 1988 . 
Ilisamitsu , T . and Nitta , Y . : " Morphological Analysis by Minimum Connetiv c -Cost Method "  , '\[' echnical Report S/GNLC 90-8 , IEICE , pp . 17-24,1990 ( in Japanese) . 
Jelinek , F . : " Self-organized language modeling for speech recognition "  , IBM Report ,  1985  ( Reprinted in Readings in Speech Recognition , 1) i ) . 450-506) . 
Matsunobu , E . , lIitaka , T . , and Yoshida , S . : " Syn-t . actic Analysis by Stochastic P , UNSETSUG rammar " , Technical Rel ) ort SIGNL 56-3 , IPSJ , 1986 ( in Japanese) . 
Meriahlo , 1t . : " Tagging Text with a Probabilistic
Moder ', ICASSP-9I , pp . 809-812, 1991.
Meteer , M . W . , Schwartz , R . and Weischedel , R . : " I'OST : Using l ) robal ) ilities in Language Processing ' , lJCAI-9t , pp . 960-965, 1991 . 
Murakaini , J . and Sagayama , S . : " llidden Markov Model applied to Morphological Analysis "  , 45th National Meeting of the IPSJ , Vol . 3, pp . 161-162,1992 ( in Japanese) . 
Nagai , I1 . and llital?a , T . : " Japanese Word For-marion Model and Its Evahmtion "  , Trans IPSJ , Vol . 34, No . 9, pp . 1944-1955, 1993 ( in Japanese) . 
Sakai , S . : " Morphological Category l ~' ; igram : A Single Language Model for botl , Spokenl , anguage and Text " , ISS1)-93 , I ) 1) . 87-90, 1993 . 
Soong , F . K . amllluang E . : " A Tree-Trellis Based Fast Search for Finding the NBest Sentence llypotheses in Continuous Speech Recognition "  , ICASSP-9I , pp . 705-708, 1991 . 
Yoshinmra , K ,llitaka , T . , and Yoshida , S . : " Morphological Analysis of Non-marked-off Japanese Sentences hy the Leastll UNSETSU's Number Method "  , ' t'rans . I 1'$3, Vol . 24, No . l , pp . 40-46, 19811 ( in Japanese) . 
Yoshimura , K ., Takeuchi , M ., Tsuda , K.
and Shudo , K . : " Morphological Analysis of Japanese Sent . ences Containing Unknown Words ", Trans . IPSJ , Vol . 30, No . 3, pp . 294-301, 1989 ( in


