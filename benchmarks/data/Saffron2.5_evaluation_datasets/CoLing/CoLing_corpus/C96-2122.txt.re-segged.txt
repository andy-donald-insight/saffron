An Earley-type recognizer for dependency grammar
Vincenzo Lombardo and Leonardo Lesmo
Dipartiment odiln formatic and Centrodi Scicnza Cognitiva 
Universith di Torino
c . so Svizzcra 185, 10149 Torino , Italy
email : vincenzo , lesmo@di.unito.it

The paper is a first attempt to fill a gap in the dependency literature  , by providing a mathematical result on the complexity of recognition with a dependency grammar  . The paper describes an improved Earley-type recognizer with a complexity O  ( IGl2n3 )  . The improvement is due to a precompilation f the dependency rules into parse tables  , that determine the conditions of applicability of two primary actions  , predict and scan , used in recognition . 
1 Introduction
Dependency and constituency frameworks define different syntactic structures  . Dependency grammars describe the structure of a sentence in terms of binary head-modifier  ( also called dependency ) relations on the words of the sentence . A dependency relation is an asymmetric relation between a word callex lhead  ( governor , parent ) , and a word called modifier ( dependent , daughter ) . A word in the sentence can play the role of the head in several dependency relations  , i . e . it can have several modifiers ; but each word can play the role of the modifier exactly once  . 
One special word does not play the role of the modifier in any relation  , and it is named the root . The set of the dependency relations that can be defined on a sentence form a tree  , called the dependency tree ( fig . la ) . 
Although born in the same years , dependency syntax ( Tesniere 1959 ) and constituency , or phrase structure , syntax ( Chomsky 1956) ( see fig . lb ), have had different impacts . The mainstream of formalisms consists ahnost exclusively of constituency approaches  , but some of the original insights of the dependency tradition have found a role in the constituency formalisms : in particular  , the concept of head of a phrase and the use of grammatical relations  . 
The identification of the head within a phrase has been a major point of all the recent frameworks in linguistics : the Xbar theory  ( Jackendoff 1977 )  , defines phrases as projections of ( pre ) terminal symbols , i . e . word categories ; in GPSG ( Gazdar et al1985) and HPSG ( Pollard , Sag 1987) , each phrase structure rule identifies ahead and a related subcategorization within its righthand side  ; in HG ( Pollard 1984 ) the head is involved in the socalled head -wrapping operations  , which allow the formalism to go beyond the contextfree power  ( Joshi et al 1991 )  . 
Grmmnatical relations are the primitive entities of relational grammar  ( Perhnutter 1983 )   ( classified as a dependency-based theory in ( Mercuk 1988 ) ): cooked
SUBJ cheftish
T ~ r / , , the a(a)
S/NP\/Vk
DN VNP
II"DNII the dlef cooked a fish
Figure 1. A dependency tree ( a ) and ap.s.
tree ( b ) for the sentence " The chef cooked a fish " . The leftward or rightward orientation of the arrows in the dependency tree represents the order constraints : the modifiers that precede the head stand on its left  , the modifiers that follow the head stand on its right  . 
subject , object , x complement . . . . label the dependency relations when the head is a verb  . Grainmatical relations gained much popularity within the unification formalisms in early  1980%  . FUG ( Kay 1979) and LFG ( Kaplan , Bresnan 1982 ) exhibit mechanisms for producing a relational ( or functional ) structure of the sentence , based on the merging of feature representations . 
All the reconstituency formalisms acknowledge the importance of the lexicon  , and reduce the amount of information brought by the phrasal categories  . The " lexicalization " of contextfree grmn mars ( Schabes , Waters 1993 ) points out many similarities between the two paradigms  ( Rainbow , Joshi 1992) . Dependency syntax is an extremely lexicalized framework  , because the phrase structure component is totally absent  . Like the other lexicalized frameworks , the dependency approach does not produce spurious grammars  , and this facility is of a practical interest , especially in writing realistic grammars . For instance , there are no heavily ambiguous , infinitely ambiguous or cyclic dependency grammars  ( such as S~SS ; S  ~ a ; S --* ~ ; see ( Tomita 1985) , pp .  72-73) . 

Dependency syntax is attractive because of the immediate mapping of dependency structures on the predicate-argmnents sructure  ( accessible by the semantic interpreter )  , and because of the treatment of free-word order constructs  ( Sgall et al 1986 )   ( Mel'cuk 1988 )   ( Hudson 1990 )  . A number of parsers have been developed for some dependency frameworks  ( Fraser 1989 )   ( Covington 1990 )   ( Kwon , Yoon 1991) ( Sleator , Temperley 1993) ( Hahn et al1994) ( Lai , Huang 1995): however , no result of algorithmic efficiency has been published as far as we know  . The theoretical worst-case analysis of O ( n3 ) descends from the ( weak ) equivalence between projective dependency grammars  ( are stricted of dependency grammars ) and contextfree grammars ( Gaifman 1965 )  , and not from an actual parsing algorithm , This paper is a first attempt of illagap in the literature between the linguistic merits of the dependency approach  ( widely debated ) and the mathematical properties of such formalisms  ( quite negleted )  . We describe an improved Earley-type recognizer for a projective dependency formalism  . As a starting point we have adopted a restricted dependency formalism with contextfree power  , that , for the sake of clearness , is described in the notation introduced by Gaifman  ( 1965 )  . The dependency grammar is translated into a set of parse tables that determine the conditions of applicability of the primary parser operations  . Then the recognition algorithm consults the parse tables to build the sets of items as in Earley's algorithm for contextfree grammars  . 
2 A dependency formalism
In this section we introduce a dependency formalism  . 
We express the dependency relations in terms of rules that are very similar to their constituency counterpart  , i . e . contextfree grammars . The formalism has been adapted from ( Gaiflnan 1965 )  . Less constrained dependency formalisms exist in the literature  ( Mel'cuk 1988 )   ( Fraser , Hudson 1992) , but no mathematical studies on their expressive power exist  . 
A dependency grammar is a quintuple < S , C , W , L , 
T > , where
W is a finite set of symbols ( vocabulary of words of a natural language )  , C is a set of syntactic ategories ( preterminals , in constituency terms ) , S is a nonempty set of root categories ( C_S ) , L is a set of category assignment rules of the form X : x  , where XCC , x@W , and

YIY 2... Yi1Yi+l . . . Ym
Figure 2-A dependency rule : X is the governor , and Y1 . . . . . Y m are the dependent of X in the given order ( X is in  #position )  . 
T is a set of dependency rules of the form X ( Y1Y2 . . . Yi1#Yi + l . . . Ym ), where XGC , Y1GC, . . . . Ym@C , and  #is a special symbol that does not belong to C  . ( see fig .  2) . 
The modifier symbols Y j can take the form Y j *: as usual  , this means that an indefinite number of Yj's ( zero or more ) may appear in an application of the rule 1 . In the sample grammar below , this extension allows for several prepositional modifiers under a single verbal or nominal head without introducing intermediate symbols  ; the predicate-arguments structure is immediately represented by a one-level  ( flat ) dependency structure . 
Let x = ala2 . . . a p ~ W * be a sentence . A dependency tree of x is a tree such that : 1 ) the nodes are the symbol sai ~ W ( l < i < p )  ; 2) a node a k , j has left daughters ak , 1  . . . . . ak , j-1 occurring in this order and right daughters ak , j + l ,   . . . . ak , q in this order if and only if there exist the roles Ak  , l : ak , 1  . . . . . Akj : akj . . . . . Ak , q : ak , qin L and the rule Ak , j(Ak , 1 . . . A k j-I #A k j + l . . . Ak , q ) in T . 
We say that ak , 1 . . . . . akj-1, ak,j+l . . . . . ak , q directly depend on a k , j , or equivalently that a k , j directly governs ak ,  1  . . . . . ak,j . 1, akj + l . . . . . . ak , q . akj and ak,h(h=1 .   .   .   .   . j-l,j + l .   .   .   .   . q ) are said to be in a dependency relation , where ak , j is the head and ak , h is the modifier , if there exists a sequence of nodes ai , ai+l .   .   .   .   . aj-l , aj such that a k directly depends on ak-1 for each k such that i + l ~ k-~j , then we say that ai depends ' on aj ;  3 ) it satisfies the condition of projectivity with respect to the order in x  , that is , if ai depends directly on a jandak intervenes between them  ( i < k < j or j < k < i )  , then either a k depends on ai or a k depends on aj  ( see fig .  3) ;  4 ) the root is a unique symbol as such that As : as EL and As ~ S  . 
The condition of projectivity limits the expressive power of the formalism to be equivalent othe contextfree power  . Intuitively , this principle states ajai
Aiki ' akii iii ajai
Figure 3. The condition of projectivity.
1 The use of the Kleene star is a notational change with respect o Gaifman : however  , it is not uncommon to allow the symbols on the right hand side of a rule to be regular expressions in order to augment the perspicuity of the syntactic representation  , but not the expressive power of the grammar ( a similar extension appears in the contextfree part of the LFG formalism  ( Kaplan , Bresnan 1982)) . 
7 24 that a dependent is never separated from its governor by anything other than another dependent  , ogether with its subtree , or by a dependent of its own . 
As an example , consider the grammar
GI = < iV , iV , N , P , A , D , I , saw , a , tall , old , man , in , the , park , with , telescope , N : I , V : saw , D : a , A : tall , A : old , N : man , P : in , D : the , 
N : park , P : with , N : telescope
TI > , where T1 is the following set of dependency rules : t . V ( N#P *); 2 . V ( N#NP *); 3 . N(A*#P *); 4 . N(DA*#P *); 5 . P ( #N ); 6 . A(#); 7 . D (#) . 
For instance , the two rules for the root category V ( erb ) specify that a verb ( V ) can dominate one or two nouns and some prepositions  ( * )  . 
3 Recognition with a dependency grammar ~\[ he recognizer is an improved Earley-type algorithm  , where the predictive component has been compiled in a set of parse tables  . We use two primary actions : predict , that corresponds to the topdown guessing of a category  , and scan , that corresponds to the scanning of the current input word  . In the subsection 3 . 1 we describe the data structures and the algorithms for translating the dependency rules into the parse tables : the dependency rules for a category are first translated into a transition graph  , and then the transition graph is mapped onto a parse table  . In the subsection 3 . 2 we present he Earley-type recognizer , that equals the most efficient recognizers for context-tree grmn mar  . 
3, 1 Transition graphs and parse tables
A transition graph is a pair ( V , E ) , where V is a set of vertices called states , and E is a set of directed edges labelled with a syntactic ategory or the symbol #  . 
Given a grammar G = < S , C , W , L , T > , a state of the transition graph for a category Cat ~ C is a ' set of dotteds lxings of the Ibrm "  .  13" , where Cat ( c~13) CT  #* and et , \[~ E(CU ); an edge is a triple < si , s j , Y > , where si , sjCV and YGCU# . A state that contains the dotted string " . " is called final ; a final state signals that the recognition of one or more dependency rules has been completed  . The following algorithm constructs the transition graph for the category Cat : l Sull  . c . ti~graph((' . at , G ): initializations0:=O ; each rule in G of theiorm Cat ( a ) do so := s0U star (   . a )
V := is 0;
E := ~ ; expansion mll cat take a non-marked states from V  ; marks ; each eatcgory YGCU#do
S ': : ~; each dotted string r = . Y\[5 in sdoi\[Y is starred s':=s'Ustar( . Y\[~)e,l~s':=s'u . F ; end for each dotted string ;
V := VU is ';
E := EU < s , s ' , Y > each category until all states in V are marked  ; graph := < V , E> . 
star ( dotted-string ) : set-of strings := dotted-string ; rsW~a~t take a non-marked dotted string ds from set-of-strings  ; markds ; if d s has the form " . Y\[V ' and Y is starred then set-of-strings := set-of-strings U "  . f ~" all dotted strings in set-of-strings are marked star := set-of-strings  . 
The initial set of states consists of a single state so  , that contains all the possible strings " . a ", such that Cat ( c0 is a dependency rule . Each string is prefixed with a dot . The marked states are the states that were expanded in a previous step  . The expansion of a states takes into account each symbol Y that il mnediately follows a dot  ( YCCU# )  . Y is a possible continuation to a new states ' , that contains the dotted string " . \[3", where " . Y\[5" is a dotted string in s . s ' is added to the set of states , and a new edge from s to s ' labelled with Y is added to the set of edges  . A dotted string of the form . Y'13 is treated as a pair of dotted strings . Y '13, . \[3 , so to allow a number of iterations ( one or more Y's follow ) or no iteration ( the first symbol in \[~ follows ) in the next step . The function " star " takes into accounthese cases  ; the repeat loop accounts for the case when the first symbol of  13 is starred too . 
The transition graphs obtained for the five categories of  G1 are in fig .  4 . Conventionally , we indicate the nonfinal states as h and the final states as Sk  , where h and k are integers . 
The total number of states of all the transition graphs for a grammar G is at most O  ( IGI )  , where IGI is the sum of the lengths of the dependency rules  . The length of a dependency rule Cat ( c0 is the length of ( ~ . 
Starting from the transition graph for a category Cat  , we can build the parse table for Cat , i . e . PTC at . 
VI'Cat is an array hxk , where h is the number of states of the transition graph and k is then mn be r of syntactic ategories in C  . Each row is identified by a pair < Cat , State > , where State is the label of a state of the corresponding transition graph  ; each column is associated with a syntactic category  . In order to improve the topdown algorithm we introduce the concept of " first " of a category  . The first of the category Cat is the set of categories that appear as leftmost node of a subtree headed by Cat  . The first of a category X is computed by a simple procedure that we omit here  . The function parse_table computes the parse tables of the various categories  . E ( t-graph cat ) returns the set of the edges of the graph t- graph cat  . 
The contents of the entries in the parse tables are sets  ( possibly empty ) of predict and scan . The initialization step consists in setting all entries of the table to the empty set  . 
725 0 1 $2 $3  ( a ) category VP0$1@'-<D ( d ) category A01$20$1 ( c ) category P ( e ) category D " cat < cat , state > ' ~< V , 0> < V , I > < V , $2> < V , $3 > < N , 0> Fig .   4 -The transition graphs obtained for the grammar G  1  . 
VNAp < scan , S2 > < predict(N ), l >
D < predict(N ) , l > < predict(N ) , $3> < predict(P ) , $3> < predict(N ) , $3> < predict(P ) , $3> < scan , S2> < predict(D ) , l > < N , I > < s can , S2> < N , $2> < P , O > < predict(N ) , $2> < P , I > < A , 0> < D , 0> < predict(N ) , l > < predict(N ) , $3> < predict(A) , l > < predict(A ) , l > < predict(N ) , $2> < scan , S1> < predict(P ) , $2> < scan , l > < predict(N ) , $2> < scan , S 1 > Figure 5 -The parse tables for the grammar G 1 although this does not happen for our simple grammar G  1  . 
32 A dependency recognizer
The dependency recognizer exhibits the same data structures of Earley's recognizer  ( Earley 1970 )  , but improves the performance of that algorithm because of the precompilation of the predictive component into the parse tables  . 
In order to recognize a sentence of n words , n + l sets Si of items are built . An item is a quadruple < Category , State , Position , Depcat > where the first two elements ( Category and State ) correspond to a row of the parse table PT Category  , the third element ( Position ) gives the index i of the set Si where the recognition of a substructure began  , and the fourth one ( Depcat ) is used to reques the completion of a substructure headed by Dep cat  , parse-table ( Cat , t-graph cat ): initialize PTC at ; for each edge < s , s ' , Y > in E ( t-graph cat ) do if YEC then for each category Z ~ first ( Y ) 0 saPTCat ( < Cat , s > , Z ) := PTCat(<Cat , s > , Z ) U
I ~ Cat(<Cat ,  ~  , Z ) U < predict(Y ) , s '> else if Y =  #then PTCat(<Cat , s > , Cat ) := PTCat(<Cat , s > , Cat ) U < s can , s '> end if : end for parse-table := PTCat . 
The parse tables for the grammar G 1 are reported in fig .  5 . The firsts are : first(V ) = first(N ) = N , AD ; first(P ) = P ; first(A ) = A ; first(D ) = D . Note that an entry of a table can contain more than one action  , slructure headed by Category ( Depcat="_"means that the item is not waiting for any completion  )  . 
Sentence : w0w1 . . . W n . 1 initialization each root category V do
INSERT < V ,  0 ,  0 , > into SO~a~d for body l~i fix/m0 ~ n do each item P = < Cat , State , j , > in Si do completer : if final ( State ) then . f . 0 Zeachitem < Cat ', k,j ', Cat > in Sid ~
INSERT < Cat ' , k , j ' , > intoSi " predictor : if < predict(Cat') , State '> Cl~l'Cat(<Cat , State > ? lnput cat ) ? r- ,  )  , INSERT < Cat , 0 ,  ~ , > tntcSi . 
INSERT < Cat , State " j , Cat '> il \] to Sim difs caIltIt ; F . " if < scan , State ' > ClrFCat(<Cat , State >?
Input cat ) th ~
INSERT < Cat , State ' , j _> into Si + le , nd ~ each item tern fination j ~ <  V , $ k , O , > is in Snlh ~ acceptg\]~reject'File external loop of the algorithm cycles on the sets Si  ( 0 < i < n )  ; tile inner loop cycles on the items of the set Si of the form < Cat  , State , j ,  >  . At each step of the inner hoop , the action(s ) given by the entry " < Cat , State > xlnput cat " in the parse table PTCat is ( are ) executed ( where lnput cat is one of the categories of the current word  )  . Like in Earley's parser there are three phases : completer  , predictor and scanner . 
completer : When an item is in a final state ( of the form $ h )  , the algorithm looks for the items which represent he beginning of tile input portion just analyzed : they are the l:i  ) ur-elementiems contained in the set referred by j  . These items are inserted into Si after having set to " null " the fourth element  ( _ )  . 
predictor : " < predict(Cat ') , State '> " corresponds to a prediction of the category Cat ' as a modifier of the category Cat and to the transition to State '  , in case a substructure headed by Cat ' is actually found  . This is modeled by introducing two new items in the set : a  ) < Cat ' ,  0 , i ,  >  , which represents the initial state of the transition graph of the category Cat ' which will span a portion of the input starting at i  . In F , arley's terms , this item corresponds to all the dotted rules of the form Cat '  (  . cz ) . 
b ) < Cat , State ' , j , Cat ' > , which represents the arc of the transition graph of the category Cat  , entering the state State ' and labelled Cat ' . In Earley's terms , this item corresponds to a dotted rule of the form Cat  ( ~ z . Cat ' l ~) . The items including a non-null Dep cat are just passive receptors waiting to be re-activated later when  ( and ii ) the recognition of the hypothesized substructure has successfully completed  . 
scanner : "< scan , State '> " results in inserting a new item < Cat , State ' , i , __> into the set Si + l . 
Let us trace the recognition of the sentence " I saw at allold man in the park with a telescope  "  . The first set SO(fig . 6) includes three items : the first one , < V ,  0 ,  0 ,  >  , is produced by the initialization ; the next two , < V ,  1 , 0 , N > arid < N ,  0 ,  0 , _> are produced by the predictor ( a N-headed subtree beginning in position 0 must be recognized and , in case such a recognition occurs , the governing V can pass to state 1) . 
In S1 the first item < N ,  $2 , 0 , > is produced by tirescanner : it is the result of advancing on the input string according to the item < N  ,  0 ,  0 , > in SO with an input noun " I " ( the entry in the parse table PTN < N , 0 > xN contains < scan , S2>) . The next item , < V ,  1 , 0 , _> is produced by applying the completer to the item in S  0 < V ,  1 , 0 , N > . 
$2 contains the item < V ,  $2 , 0 ,  _> , obtained by the scanner , that advances on the verb " saw " . The other four items are the result of a double application of the predictor  , which , in a sense , builds a " chain " that consists of a noun governed by the root verb and of a determiner governed by that noun  ; this is the only way , according to the grammar , to accomodate an incoming determiner when a verb is under analysis  . 
The subsequent steps can easily be traced by the reader  . The input sentence is accepted because of the appearance in the last set of the item < V  ,  $3 , 0 ,  >  , encoding that a structure headed by a verb ( i . e . a root category ) , ending in a final state ($3) , and covering all the words from the beginning of the sentence has been successfully recognized  . 
The space complexity of the recognizer is O ( IGIn 2 )  . Each item is a quadruple < Cat , State , Position , Depcat >: Dep cat is a constant of the grammar ; the pairs of Cat and State are bounded by O(IGI ) ; SO\[1\]<N , O , 2_> < N ,  1 ,  2 , _><V ,  $3 , O , P > < N ,  1 ,  7 , > < V , O , O , _ > < N ,  1 , 2 , D > < A , O ,  4 , _ > < N , $2 , 2 , P > Slola \] S 12 < V ,  1 , O , N > < D , O ,  2 , _ > < N ,  1 ,  2 , A > S9\[with\]<P ,  1 ,  9 , _ > < N ,  $2 ,  10 , _ > < N , O , O , _>$7 ltlw\]<N ,  $2 ,  7 , > < N , O ,  10 , > < P ,  $2 ,  9 , >$3\[tall\]S5#nan\]<P ,  1 ,  6 , _>< P ,  $2 , 6 , > < P ,  $2 , 9 , N > < N ,  $2 , 7 , > S1\[saw\]<D , $1 , 2 , _ > < A , $1 , 4 , > < N , O ,  7 , > < N , $2 , 2 , _><D , O ,  10 , _ > < N , $2 , 2 , > < N , $2 , 0 , > < N ,  1 , 2 , _ > < N ,  1 , 2 , > < P , $2 , 6 , N > < V , $3 , 0 , _ > < N , I , IO , D > < V , $3 , 0 , _><V ,  1 , O , > < A , O ,  3 , _><D , O ,  7 , > < P , O ,  9 , _>< P ,  $2 , 6 , _ > < N ,  1 ,  2 , A > $6l in \] < N ,  1 ,  7 , D > < N ,  $2 , 7 , P > S11\[telescope\]$2\[a\]<N ,  $2 , 2 , > < N ,  $2 , 2 , P > < D ,  $1 , 10 , _><V ,  $2 , O , _>S4\[old\]<V ,  $3 ,  2 , _>$8\[park\]<V ,  $3 , O , P > < N ,  1 ,  10 , _><V ,  $3 , O , N > < A ,  $1 , 3 , _>< P , O ,  6 , > < D ,  $1 , 7 , > Figure 6 . Sets of items generated in recognizing " Isawa tallold man in the park with a telescope "  . 

Position is bounded by O(n ) . The number of such quadruples in a set of items is bounded by O  ( IGIn ) and there are n sets of items . 
The time complexity of the recognizer is O ( IGI2 n3 )  . The phases canner and predictor execute at most O ( IGI ) actions per item ; the items are at most O ( IGIn 2 ) and the cost of these two phases for the whole algorithm is O  ( IGl2n2 )  . The phase completer executes at most one action per pair of items  . The variables of such a pair of items are the two states  ( O ( IGI2 ) ) , the two sets that contain them ( O(n2)) , and the two positions ( O(n2)) . But the pairs considered are not all the possible pairs : one of the sets has the index which is the same of one of the positions  , and the complexity of the completer is O ( IGI2 n3 )  . The phase completer prevails on the other two phases and the total complexity of the algorithm is O  ( IGI2 n3 )  . 
Even if the O-analysis is equivalent to Earley's , the phase of precompilation i to the parse tables allows to save a lot of computation time needed by the predictor  . All the possible predictions are precomputed in the transition to a new state  . A similar device is presented in ( Schabes 1990 ) for contextfree grammars . 
4 Conclusion
The paper has described a recognition algorithm for dependency grammar  . The dependency formalism is translated into parse tables  , that determine the conditions of applicability of tile parser actions  . The recognizer is an improved Earley-type algorithm  , whose performances are comparable to the best recognizers for the contextfree grammars  , the formalism which is equivalent to the dependency formalism described in this paper  . The algorithm has been implemented in Common Lisp and runs under the Unix operating system  . The next step in our research will be to relax the condition of projectivity in order to improve the expressive power and to deal with phenomena that go beyond the contextfree power  . These changes imply the restructuring of some parts of the recognizer  , with a plausible increment of the complexity . 

Chomsky N . , Three models for the description of language , IRE Transactions on Information Theory , 
IT-2, 1956, 113-124.
Covington M.A ., Parsing Discontinuous
Constituents in Dependency Grammar,
Computational Linguistics 16, 1990, 234-236.
Covington M . A . , An Empirically Motivated Reinterpretation f Dependency Grammar  , Res . Rep . 
AI-1994-01, Univ . of Georgia ( also on Comp Ling
Server ), 1994.
Earley J . , An Efficient Context-free Parsing Algorithm . Comm . of the ACM 13, 1970, 94-102 . 
Fraser N . M . , Parsing and Dependency Grammar , UCL Working Papers in Linguistics ,  1989 ,  296-319 . 
Fraser N . M . , Hudson R . A . , Inheritance in Word Grammar , Computational Linguistics 18 ,  1992 ,  133-158 . 
Gaifman H . , Dependency Systems and Phrase Structure Systems , Information and Control 8 ,  1965 ,  304-337 . 
Gazdar G ., Klein E ., Pullum G ., SagI.,
Generalized Phrase Structure Grammars , Basil
Blackwell , Oxford , 1985.
Graham S . L . , Harrison M . A . , Ruzzo W . L . , An improved ContextFree Recognizer , ACM Trans . on Programming Languages and Systems2 ,  1980 ,  415-462 . 
Hahn U . , Schacht S . , Broker N . , Concurrent , Object-Oriented Natural Language Parsing : The Parse Talk Model  , CLIF Report 9/94 , Albert-Ludwigs - Universitat , Freiburg , Germany . 
Hudson R ., English Word Grammar , Basil
Blackwell , Oxford , 1990.
Jackendoff R ., Xbar Syntax : A Study of Phrase
Structure , MIT Press , 1977.
Jacobs P . S . , Rau L . F . , Innovations in Text Interpretation , Artificial Intelligence Journal 63/1-2 ,  1993 ,  143-191 . 
Joshi A . K . , Vijay-Shanker K . , Weir D . , The Convergence of Mildly Context-sensitive grammatical formalisms  , in Sells P . , Shieber S . , Wasow T . ( eds . ), Foundational Issues in Natural
Language Processing , MIT Press , 1991.
Kaplan R . , Bresnan J . , Lexical-Functional Grammar : A Formal System for Grammatical Representation  , i Bresnan J . ( ed . ) , The mental representation fgrammatical relations , MIT Press ,  1982 . 
Kay M . , Functional Grammar , Proc . 5th Meeting of the Berkeley Linguistic Society ,  1979 ,  142-158 . 
Kwon H ., Yoon A ., Unification-Based
Dependency Parsing of Governor-Final Languages,
Proc . IWPT 91, 1991, 182-192.
LaiB . Y . T . , Huang C . , Dependency Grammar and the Parsing of Chinese Sentences  , Unpublished
Manuscript on Comp Ling Server , 1995.
Mel'cuk I ., Dependency Syntax : Theory and
Practice , SUNY Press , Albany , 1988.
Perlmutter 1983 , Studies in Relational Grammar 1 , Univ . of Chicago Press , Chicago , 1983 . 
Pollard C . J . , Generalized Phrase Structure Grammars , Head Grammars , and Natural Language , 
Ph.D . Thesis , Stanford Univ ., 1984.
Pollard CJ . , Sag I . , An Information Based Syntax and Semantics , vo1 . 1, Fundamentals , CSLI Lecture
Note 13. CSLI , Stanford , 1987.
Rambow O ., Joshi A ., A Formal Look at
Dependency Grammars and Phrase Structure Grammars , with Special Consideration of Word-Order Phenomena  , Proc . of the Int . Workshop on the
Meaning-Text Theory , Darmstadt , 1992.
Schabes Y . , Polynomial Time and Space Shift-Reduce Parsing of Arbitrary ContextFree Grammars  , Proc . ACL 90, Pittsburgh ( PA ), 1990, 106-113 . 
Schabes Y ., Waters R . C ., Lexicalized Context-
Free Grammars , Proc . ACL 93, 121-129.
Sgall P . , Haijcova E . , Panevova J . , The Meaning of Sentence in its Semantic and Pragmatic Aspects  , 
D . Reidel Publ . Co ., Dordrecht , 1986.
Sleator D . D . , Temperley D . , Parsing English with a Link Grammar , Proc . of l WPT 93,1993, . 277-291 . 
Tesniere L ., Elements de Syntaxe Structurale,
Kliensieck , Paris , 1959.
Tomita M ., Efficient Parsing for Natural
Language , Kluwer Acad . Publ ., 1985.

