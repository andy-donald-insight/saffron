NEURAL NETWORK APPROACH TO WORD CATEGORY PREDICTION 
FORENGLISH TEXTS
Masami NA KA MURA , Katsuteru MARUYAM Af , Takes hiKAWABATAf ? , Kiyohiro SHIKANOtit
ATR Interpreting Telephony Research Laboratories
Seika-chou , Souraku-gun , Kyoto 619-02, JAPAN
email masami@atr-la.atr.co.jp

Word category prediction is used to implement an accurate word recognition system  . Traditional statistical approaches require considerable training data to estimate the probabilities of word sequences  , and many parameters to memorize probabilities . To solve this problem , NET gram , which is the neural network for word category prediction  , is proposed . Training result show that the perfornmnce of tim NET gram is comparable to that of the statistical model  ; although the NET gram requires fewer parameters than the ~  ; tatistic at model . Also the NET gramper forms effectively ? or unknown data  , i . e . , the NET gram interpolates sparse training data . Results of analyzing the hidden layer show that the word categories are classified into linguistically ti ~ ignificant groups  . The results of applying the NET gram to HMM English word recognition show that the NET gram improves the word recognition rate fi'om  81  . 0% to 86 . 9% . 
1. Introduction
For the realization of an interpreting telephony system  , an accurate word recognition system is necessary . Because it is difficult to recognize English words using only their acoustical characteristics  , an accurate word recognition system needs certain linguistic information  . Errors in word recognition results for sentences uttered in isolation include the tbllowing types of errors recoverable using linguistic in fi  ) rmation . 
( a ) Local syntax errors.
(b ) Global syntax errors.
(c ) Semantics and context errors.
Many errors arise with one-syllable words such as ( I , by ) and ( the , be ) . More than half of these errors can be recovered by use of local syntax rules  . The Trigram language model is an extremely rough approximation falanguage  , but it is a practical and useful model from the J Research and Development Department  , NITSUF , O CorporatAo ~ l1N'ff FBasic Research\[ , aboratm'ies"J"l"tN'UI't tumanIterface\[ , aboratories viewpoint of entropy . At the very least , the trigram model is useful as a preprocessor for a linguistic processor which will be able to deal with syntax  , semantics and context . 
Text Mr . Hawk sly said yesterday he would
Category NP NP VBD NR PP SMD
Category 5\] 51 79 55 66 46

Bigram : 1/""~'-~"" prediction\]/:'' . . . . . . . . . . . . .  ~ . 
Fig . 1 Word Category Prediction
Using Brown Corpus Text Data
The trigram model using the appearance probabilities of the following Word was efficiently applied to improve word recognition results  111\]\[121  . However , the traditional statistical pproach requires considerable training samples to estimate the probabilities of word sequence and considerable memory capacity to process these probabilities  . Additionally , it is difficult to predict unseen data which never appeared in tile training data  . 
Neural networks are interesting devices which can learn general characteristics or rules from limited sample data  . Neural networks are particularly useful in pattern recognition  . In symbol processing , NET talk\[3\] , which produces phonemes from English text , has been used successfully . No waneural network is being applied to word category prediction  \[4\]  . 
This paper describes the NET gram , which is a neural network for word category prediction i text  . The NET gramis constructed by a trained Bigram network with two hidden layers  , so that each bidden layer can learn the coarse -coded features of the input or output word category  . Also , the NET gram can easily be expanded from Bigran ) to Ngram network without exponentially increasing the number of parameters  . TireNET gram is tested by ti ' ain in ~ experiments with the Brown Corpus English Text Databasei  213   \[51  . The NET gram is applied to IIMM English word recognition resulting in an improvement of its recognition performance  . 
2 . Word Category Prediction Neural Net ( NET gram ) The basic Bigram network in the NET gram is a 4-layer feedforward network , as shown in Fig . 2, which has 2 hidden layers . Because this network is trained for the next word category as the output for an input word category  , hidden layers are expected to learn some l inguistic structure from the relationship between one word category and the next in the text  . The Trigram network in the NET gram has a structure such that  , as the number of next word category \[ .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .  1
L ___(!
IiO 0.....0', ......
I .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . J present word category
Output Layer 89 units
Hidden layer 216 units
Hidden layer 116 units
Input Layer 89 units
Fig . 2 NET gram(Basic Bigram Network ):, oo ? .  , ,~?  .   .  ? , , , ,
Nth word " I Output !
IUnit < s9),, i
IHL2"Io , ?"~ lu'it ~06) I"I . ~ iI .   .   . LU'it < lmI'1IfiputI' . " Input .   .   .   . i
Units ( 89 ) IUn its ( 89 ) : N-3 ) thWord " ( N-2 ) th Word ( N-1 ) th Word ! 4gram Trigram Bigram network Fig . 3 NET gram ( Trigram , 4gram Network ) grams increases , every new input block produced is fully connected to the lower hidden layer of one basic Bigram network  . The link weight is set at wt'as shown in Fig . 3 . 
When expanding from Trigram network to 4gram network , one lower hidden layer block is added and the first and second input blocks are fully connected to one lower hidden layer block  , and the second and third input blocks are fully connected to the other lower hidden layer block  . 
3. How to Train NE Tgram
I low to train a NE'P gram , e . g . a Trigram network , is shown in Fig . 4 . As input data , word categories in the Brown Corpus text\[5\] are given , in order , from the first word in the sentence to the last , In one input block , only one unit corresponding to the word category number is turned ON  ( 1 )  ; The others a returned OFF (0) . As output data , only one unit corresponding to the next word category number is trained by ON  ( 1 )  . The others are trained by OFF (0) . The training algorithm is the Back -Propagation  algorithm\[6l   , which uses the gradient descent to change link -weights in order to reduce the difference between the network output vectors and the desired output vectors  . 
First , the basic Bigram network is trained . Next , the Trigram networks are trained with the llnk weight values trained by the basic Bigram network as initial values  . 
This task is a many-to-many mapping problem . Thus , it . 
is difficult to train because the updating direction of the link weight vector easily fluctuates  . In a two-sentence . ~ ko . o ? ? ?010 iO .   .   .   .   .   .  0  . . . . O ! oo,pot ~_1 .   .   .   .   .   .   .   .  5~  .   .   .   .   . J ! 9_~Layer
Hidden Layers
L _ ! .   .   .   .   .   .   .   . _s 3 .   .   .   .   . 8_9_2 L_t .   .   .   .   .   .   .   .   .   .   . 79___8_9_2 Layer ~ .   .   . ~?, o , , , ' li_ . .d Fig . 4t to w to Train NET gram ( Trigram Model ) that the output value soft be basic Bigram network converge on the next occurrence probability distribution  . 
t to wever , for many training data , considerable time is required for training . Therefore , in order to increase training speed , we use the next werd category occurrence probability distribution calculated for  1  , 024 sentences ( about 24 , 000 words ) as output training data in the basic Bigram network  . Of course , in Trigram and 4gram training , we use the next one word category as output training data  . 
4 . Training llcs nlts 4 . 1 . Basic Bigram Network Word category prediction results show that NET grmn  ( the basic Bigram network ) is comparal ) le to the statistical
Bigram model.
Next , we consider whether the hidden layer has obtained some linguistic structure  . We Calculated the similarity of every two lower hidden layer  ( HIA ) output vectors for 89 word categories and clustered them . 
Similarity S is calculated by ( M(Ci ), M(Cj ))
S(ci , cj ) = (4.1)
I 1M ( Ci ) it IIM ( Qi ) I1 where MfOi ) is the lower hidden layer ( ILL1 ) output vector of the input word category CL ( M ( Ci )  , M(C ~\])) is the immr product of M(Ci ) and M(Cj) . It M(Ci ) II is the norm of M(Ci ) . 
The clustering result is shown in Fig . 5 . Clustering by the threshold of similarity , 0 . 985 , the word categories are classified into linguistically significant groups  , which are the HAVE ' , verb group , BE verb group , subjective pronoun group , group whose categories should be before a noun , and others . Therefore the NET gram can learn linguistic structure naturally  . 
4.2. Trigram Network
Word category prediction results are shown in Fig . 6 . 
The NET gram ( Trigram network ) is comparable to the statistical Trigram model for test data  . 
Furthermore , the NET gramper forms effectively for unseen data whicilnever appeared in the training data  , although the statistical Trigram cannot predict the next word category for unseen data  . That is to say , NET grams interpolates parse training data in the same way deleted interpolation  \[71 does . 

GORY "3"g3qV  ~  37 HVD 40   EIV7   16 BEDZ 20 BER 21 BEZ 19 BEN 14 B E 17BEG   38 I-IVG ~ ffPPS--86 WPS 67 PPSS "2gD-r---45 JJT 58 OD 42 JJ 48 NN $ 61 PP $ 52 NP $ 13 AT 78 VB 80 VBG 06   ,   1 lAP 22 CC 09 ABN 10 ABX 75 RP 81 VBN 89 DUM 23 CD 43 JJR 47 NN noun , single ) 55 NR lome , west 79 VBD ( verb , past ) 32 VSZ ( verb , - s , -es)~2 DTS these 55 PPO me , him , it 19 NNS(noun , plural ) t0 RB ( adverb ) ~0 NNS$men's1 NPP ATR , Tom ~ thersothers EXAMPLE Threshold of Simi larity  ( part of speech )  1 . 000 0 . 995 0 . 990 0 . 985 0 . 9 80 had-- -1 J : has:\----~-- T_Y2-L _~_L_--
WaS .   .   .   .   .   .   .   . /! are .   .   .   .   .   .   . ~i ' eenQ ?) ibei . ~  .   .   .   .   .   .   . ~ jh ~ .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 7 fie , Jt .   .   .   .   .   .   .   .   .   .   . ~ whe , whicb .   .   .   .   .   .   .   .   .   .   .   .   .   . ~I ;
I , we , the v .   .   .   .   .   .   .   . ~; biggest .   .   .   . ~/ IIFFl ; first , 2nd .   .   .   . Jill I\[i ( adjective) .   .   .   .   .   .   .   . 1I Ittidog's - - - - - -
ATR's .   .   .   .   .   .   . 1III ~ I ! a the .   .   .   .   .   .   . ~_II ;!( verb , base ~-- t\]\[;(~er U .  ~ . g)-2_~iii \] many , next .   .   .   .  ~ ' , i and , or --~!! half , all ~- I ~ both-II about , of f~--i(verb , - ed ) ~_ ~ i ( dummy ) one , 2I--t -' , comp . adj . )
Prediction Rate 0.8-..........
NET gram for test data
Statistical Model for test data 0 . 6 0  . 4  . , . . . , , , '""'" data as Trigram data . 

I _
Fig . 5 Clustering Result of iliA Output Vectors of NET gram  ( Bigram ) l 1 l__J 0  \]  2   3   4   5 Number of Prediction Candidate Categories Fig . 6 NET gram ( Trigran 0 Prediction Rates 4 . 3 . Differences between the statistical model and the
NET gram
We discuss differences between two approaches , the conventional statistical model and the NET graln  . The conventional statistical model is based on the table-lookup  . 
In the case of the Trigram n lode l , next appearance probabilities are computed frmn the histogram  , counting the next word category for the two word categories in the training sentences  . The probabilities are put in an 89*89*89 size table . Thus , the 89 appearance probabilities of the next word category are obtained from the  89*89*89 size table using the argument of 89*89 symbol permutation . 
B89.s9 - --> R89B ; binary space
R ; real space
In order to get 89 prediction values for the next word category , the trained NET gram procedure is as follows : First  , encode from an 89*89 symbol permutation to a 16-dimensional nalogue code .   ( h'om the input layer to the hidden layer 1 ) Second , transform the 16-dimensional nalogue code to a 16-dimensiotial nalogue code of the next word's 89 prediction values .   ( from hidden layer 1 to hidden layer 2 ) Finally , decode the 16-dimensional nalogue code to 89 prediction values of the next word's 89 prediction values . (fl'om hidden layer2 to output layer )
B89"89__>R16.__>R16___>R89
The values of each space are output values of the NET gram units of each layer  . These mappings are uniquely determined by link -weight values of the NET gram  . That is to say , each layer unit value is computed by summing lower-connected unit values multiplied by link -weights and passing through the nonlinear function  ( sigmoid function )  . 
These two approaches need the following memory area  ( number of parameters )  . 
' Statistical model 89?89 X89=704 , 969  ( max number of table elements " NET gram ( 89+89 ) X 16+ 16X 16+ 16X 89+ 121 = 5 , 193 ( number of link-weights ) (121 ; offset parameters ) Thus , the parameters of the statistical model ar ( ~89?89 X89 probabilities . In practice , there are ninny 0 values in 89 X 89 X 89 probabilities and the size of the table can be reduced using a particular technique  , t lowever , this depends on the kind of task and the number of ' training data  . On the other band , the NET gram can produce 89?89?89 prediction values using link-weight values memorized as parameters  . 
Next , concerning ' the data representation , the statistical model does not use input data structures because it is based on the table -lookup which get probabilities directly by symbol series input  . On the other hand , the NET gram extracts a feature related to the distance between word categories from symbol series input into  16-dimensional analogue code . 16-dimensional analogue codes are described in 4 . 1 as the feature of the NETgram hidden layer in the Bigram nmdel  . Thus , the NET gram interpolates sparse training data in the process of bigram and trigram training  . 
From the viewpoint of data coding , The NET gram compresses data from 89-dimensional binary space into 16-dimensional real space . 
4 . 4 .   4gram Network 4gram prediction rates of the NET gram trained by 2  , 0 48 are not much higher than the trigram prediction rates of that trained by  1  , 024 sentences . The statistical model experiment result show that more than  6  , 0 00 sentences are necessary as training data in order for the  4gram prediction rates to equal the trigram prediction rates of the NET gram trained by  1  , 024 sentences . Futhermore , the trigram prediction rates of the statistical model increase a stim training sentences increase  , up to a max of 16 , 000 training sentences . The NET gram compensates for the sparse 4gram data through the interpolation effect . 
However , it is clear that the 4gram prediction NET gram needs far more than 16  , 0 00 training sentences in order to better the performance of the trigram prediction  . Training for so many sentences was not possible because of the limited database and considerable computing required  . 
5 . Applying the NE Tgram to Speech Recognition The algorithm for applying the NET gram to speech recognition is shown in Fig  . 7 . HMM refers to the ltidden Markov Model which is a technique for speech recognition \[ l\]  \[8\]   [9\]  . 

Acoustic lin___quistic
Keyboard Conversation Speech Data Brown Colpus Toxt Data  , * Train in q ~ , ~ Test "** ~* Training ~ . 
Data " ~ ,, Data ? ~ ~ Data * "' l '""
Recoqnito
Training ~ j
Fig . 7 Improven mnt of 11 MM English Word Recognition
Using the NET gram 5 . 1 . Formulation l . ,et w ~ show a word just after wi_ 1 and just before wi+I . 
Let Cishow one of the word categories to which the word wi belongs  . The same we , ' d belonging to a different category is regarded as a different word  . The trigram probability of w i is calculated using the following approximations  . 
t ' ( wi/wi . 2 wi ! ) ~- ~ P ( wi/Ci-2Ci1 ) := P ( Ci/Ci-2Ci4 ) XP ( wi/Ci . 2Ci 1 ) /P ( Ci/Ci2Ci1 ) := P ( Ci/Ci-2Ci1 ) P ( wi ) /P ( C i )    ( 5 . 1 ) Word trigram probabilities are approximated using category trigram probabilities as follows : P  ( wi/wi . 2 wi . 1) ~ P ( wi/Ci-2Ci1) (5 . 2 ) The probability of w ~ is denoted by the preceding two-word sequence  , wi . 2, Wi . l , and is approximated by their preceding two -category sequence  . 
P ( wi/Ci2Ci1 ) /P ( Ci/Ci2Ci-l ) = I ' ( wi ) /P ( C i )   ( 5 . 3 ) The probability ratio of wi and Ci given by Ci2 Cij is nearly equal to the total probability ratio of wi and Ci  . 
Toeah . ' ulate the above probability , the trigram probability of word category , P ( Ci/Ci2Ci1) , and word occurrence probability , P ( wi)/P ( Ci) , are required . The word probability , P ( wi)/P ( Ci) , is prestored in tim dietio , mry of word wi for each wm'd category . 
To avoid the mult it ) lication of probabilities % t belog likelihood , ST i , is defined as : STi = Iol IP ( Ci/Ci-2Ci-l ) ? log ( P ( wi ) /P ( C  ~ ) )  ( 5 , 4) ' rhe . first term is retrieved from the trigram of word categories and the second term is retrieved from the word dictionary  . 
The maximum likelihood of a word , SW , is given by the sum of word likelihood values of an word sequence  . The jth word candidate in the ith word of a sentence is denoted by wij  . The likelihood of " wij , SW i , i , is defined as the sum of two types of likelihood which are the loglikelihood of the ItMM output probability  , SHia , and the trigram likelihood , ST ij . Thus , the likelihood of wij is described as follows : SW i  . j = (1-o ~) . SH ij + ~ o . S'l'ij ( 55 ) where a ~ is the weighting parameter to adjust the scaling of two kinds of likelihood  . 
The maximum sentence likelihood values , G , are denoted by the following equations :
Go , ; = SWod(i = O ) (5.6)
Gij--max(SWij-~Gi . 1, k)(iv = 0) (5 . 7) k When the lengther a sentence is N , the maximum value of GN . ij is regarded as the maximum likelihood of the word sequence  . The back-tracing of wij gives the optimal word sequence  . 
In this paper , the best-ten candidates in the tlMM word recognition results are used  . As the same word belonging to a different category is regarded as a different word  , there are ten or more word candidates . 
5 . 2 . English Word Recognition Results The exper iment task is to translate keyboard conversations which include  377 English sentences ( 2 , 834 words ) uttered word by word by one matenative speaker . 
The sentences are composed of 542 different words . HMM phone models are trained using 190 sentences ( 1 , 487 words ) without phone labels . 
The trigram models , the NET gram and the statistical model , are trained using using 512 and 1 , 024 sentences of the Brown Corpus Textl ) at a base . One sentence is about 24 words long . 
English word recognition results for 18'7 sentences ( l , 347 words ) of keyboa , ' deonvers at im~suingHMM and the trigram models are shown in Table  1  . The recognition rate in the experiment using only fl MM is  81  . 0% Using the 5 or 6% . The number of recognition errors decreases using
Nl , \] Tgram.
Table 1 tl MM English Word Recognition Rates using NET grana or Statistical model  ( % ) 
Model Training NET gram Statistical
Sentences Model !
I5 12 86.3 85.5
Trigram 1,024 86.98 5.4 ? iiii
The results of analyzing the hidden layer after training showed that the word categories  ; were classified into some linguistically significant groups  , that is to say , the NlgTgram learns a linguistic structure . 
Next , the NET gram was applied to ttMM English word recognition  , and it was shown that the NET gram can effectively correct word recognition errors in text  . The word recognition rate using tlMM is 81 . 0% . The NET gram trained by 1 , 0 24 sentences improves the word recognition rate to 86  . 9% . The NET gramper forms better than tim statistical trigram model when data is in *  ; ufficient o estimate the correct probabilities of a word sequence  . 
Comparing the NET gram and the statistical trigram model  , the performance of the NET gram is higher than that of the statistical trigram in the case of training data consisting of  512 and 1  , 024 sentences . Furthermore , the statistical trigram model cannot learn word sequences which do not appear as a trigram in the training datm Thus  , the prediction value o . c that word sequence is zero . The NET gram does not make such fatal mistakes . 
Additional results for 4 , 096 and 30 , 000 sentences show that recognition rates are 86 . 9% and 87 . 2% using the NET gram , and 86 . 6% and 87,7% using the statistical model . 
It is confirmed that the NET gramper forms better than the statistical trigram nm delwhen data is insufficient o estimate the correct probabilities T berefore  , even if training data were insufficient to estimate accurate trigram probabilities  , the NET grampe , ' forms effectively . That is to say , the NET gram interpolates sparse trigram training data using bigram training memory  . 
6. Conclusion
In this paper we have presented the NET gram , a neural network for Ngram word category prediction in text  . The NET gram can easily be expanded from Bigram to Ngram network without exponentially increasing the number of parameters  . 
The training results showed that the Trigram word category prediction ability of the NET gram was comparable to that of the statistical Trigram model although the NgTgram requires fewer parameters than the statistical model  . We also confirmed that NET grams performed effectively for unknown data which never appeared in the training data  , that is to say , NET grams interpolates parse training data naturally  . 

The authors would like to express their gratitude to Dr  . 
Akira Kurematsu , president of ATR Interpreting Telephony Research Laboratories  , which made this research possible , for his encouragement ad support . We are also indebted to the members of the Speech Processing Department a ATR  , for their hell ) in the various tages of this research . 
References\[11I,' . Jetinek , " Continuous Speech Recognition by Statistical Methods "  , Proceedings of the iI'~EI'\] , Vol . 64, No . 4 (1976 . 4) 12 1K . Shikano , " hn proven mnt of Word Recognition Result by Trigram Model "  , \[CASSP 87 ,  29 . 2(1987 . 4)\[al T . J . Sejnowski , C . R . Rosenberg , " NET talk , A Parallel Network that l , earns to ReadAloud " , Teeh . Report , The Johnsllopkins University EESS-86-01 ( 1986 ) \[41 M . Nakamura , K . Shikano , " A Study of English Word Category Prediction Based on Neural Networks "  , ICASSP 89 , SI3 . 10(1989 . 5) 151 13 town University , " Brown Corpus " , Tech : Report , 
Brown University (1967) 16 lD . E . Rumelhart , G . E . tllnton , R . d . Williams , " Parallel Distributed Processing ", M . I . T . Press (1986)\[7\]F . Jelinek , R . Mercer , " Interpolated Estimation of Marker Source Parameters from Sparse Data "  , Pattern Recognition i Practice , ed . E , S . Gelsem and L . N . Kanal,
Northttoll and (1980)\[81 S . E . Levinson , L . R . Rabiner , M . M . Sondhi , " An Introduction to the Application of the Theory of Probabilistie Functions of a Marker Process to Automatic Speech Recognition "  , Bell Syst . Teeh . J . 62(4), pp 1035-1074 (1983)\[91T . 1Ianazawa , G . Kawabata , K . Shikano , " Study of Separate Vector Quantization for HMM Phoneme 
Recognition ", ASJ2-P-8(198 8.10) 2't.8
