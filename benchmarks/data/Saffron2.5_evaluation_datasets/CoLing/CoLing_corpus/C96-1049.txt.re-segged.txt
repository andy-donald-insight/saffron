Lean Formalisms ~ Linguistic Theory ~ and Appl ications  . 
Grammar Development in ALEP.
Paul Schmidt & Axel Theofilidis
& Sibylle Rieder

Martin-Luther-Str .14
D-66111 Saarbrficken
paul , axel , sibylle@iai.uni-sb.de

This paper describes results achieved in a project which addresses the issue of how the gap between unification-based grammars as a scientific concept and realworld applications can be narrowed own  1  . Application-oriented grammar development has to take into account the following parameters : Efficiency : The project chose a socalled ' lean ' formalism  , a term-encodable language providing efficient term unification  , ALEP . Coverage : The project adopted a corpus-based approach  . 
Completeness : All modules needed from text handling to semantics must be there  . The paper reports on a text handling component , Two Level morphology , word structure , phrase structure , semantics and the interfaces between these components  . Mainstream approach : The approach claims to be mainstream  , very much indebted to HPSG , thus based on the currently most prominent and recent linguistic theory  . The relation ( and tension ) between these parameters are described in this paper  . 
1 Introduction
Applications on the basis of unification-based grammars  ( UG ) so far are rather rare to say the least . 
Though the advantages of UGs are obvious in that propertie such as monotonicity  , declarativity , per-spicouity are important for maintaining and easily extending grammars  , their popularity ( despite 15 years of history ) is still restricted to the academia . 
This paper reports of a project , LS-GRAM , which tried to make a step further in bringing UGs closer to applications  . 
Application-oriented grammar development has to take into account the following parameters : ? Efficiency : A major problem of UGs is  ( lacking ) efficiency . For the LS-GRAM project this 1This project is LS-GRAM , sponsored by the Commission of the European Union under LRE  61029  . 
Thierry Declerek
IMS , University of Stuttgart
Azenbergstr . 12
D-70174 Stuttgart thierry@ims . uni-stuttgart . deled to the decision to use a socalled ' lean ' for-nrMism  , ALEP , providing efficient term unification . ' Leanness ' means that computationally expensive formal constructs are sacrificed to gain efficiency  . Though this is at the cost of expressiveness , it is claimed that by ' leanness '' hnguistic felicity ' does not suffer  . 
? Coverage : Most grammar development projects are not based on an investigation of real texts  , but start from ' the linguists " textbook ' . This is different here in that a corpus-based approach to granlmar development has been adopted which is the implementation of the sinlple principle that if a grain nlar is supposed to cover reM texts  , that the coverage of these texts has to be determined first  . The was a corpus investigation in the in the beginning  , in the course of which tools have been used and developed which allow for automatic and semiautomatic determination of linguistic phenomena  . 
? Conlpleteness : All modules needed from text handling to semantics had to ve developed  . 
This is why this paper does not focus on one single topic  , but tries to represent heulaj or achievements of the whole of the system  . The paper reports on a text handling component , Two Level morphology , word structure , phrase structure , semantics and and very importanly the interaction of these components  . 
? MM nstream approach : None-the-less , the approach we adopted clainls to be mainstream , very much indebted to HPSG , thus based on the currently most prominent and recent linguistic theory  . 
The relation ( and tension ) between these parameters is the topic of this paper  . 
First , we will show , how a corpus-investigation estabfished the basis for the coverage  , second , how various phenomena deternlined by corpus -investigation are treated in text handhng  ( TH )  , third , how the linguistic modules , Two Level Morphology ( TLM ) , word and phrase structure , the lexicons look hke . The last section is devoted to the are given which prove that the system is not so far from real applications  2   2 Corpus-Investigation with the

The project started with a corpus investigation . It consisted of 150 newspaper articles from the German'DieZEIT' . They are descriptive texts from the domain of economy  . They were investigated automatically by the ( nonstatistical ) ' MPRO'tagger . 
'MPRO'provides the attachment of rich linguistic information to the words  . In addition , 'MPRO'provides a builtin facility for resolving categorial ambiguities on the basis of homograph reductions and a facility for handling unknown words which are written on a file  . Encoding the missing stems ( which were very few ) ensured complete tagging of the corpus . 
'MPRO ' also provides a facility for searching syntactic structures in corpora  . A detailed analysis on the internal structure of main clauses  , subordinate clauses , verbal clusters , clausal to poi(e . g . 
structure of Vorfeld and Nachfeld ) , NPs , PPs , APs , CARDPs , coordinate structures , occurrence of ex-pletives , pronominals and negation occurring in the corpus was made which then guided grammar development  . 
Another major result of the corpus investigation was that most sentences cout Mnsocalled'messy details '  , brackets , figures , dates , proper names , appositions . Most sentences contain compounds . 
In gener M , most of the known linguistic phenomena occur in all known variations  . Description has to be done in great & tail ( all frames , all syntactic realizations of frames ) .   ( Long distant ) discontinuities popular in theoretical linguistics did not play a role  . In order to give a ' general tlavour ' of the corpus-investigation enoteworthy result should be reported :  25% of the number of words occur in NPs of the structure \[ DET  ( A )   ( A ) N\] . But ' A ' and ' N ' are of a complex and unexpected nature : ? A :  ( name ) -er : Dortmund-er , Schweiz-er ? ( figure ) - A : 5-j/?hrig , ffinfj ~ hrig , 68-prozentig?N : Ex-DDR-Monopolisten ( Hyphenated compounding , including names and abbreviations ) . 
The corpus-investigation guided the grantmard e -velopnrent  . A . o . it showed the necessity to devlop a TH component and separate outspecific phenomena from the treatment in the grannnar  .   ( This was also necessary from an efficiency point of view  )  . 
2It should be mentioned that we are referring to the German grammar built in the LS-GRAM project  . For other language similar system exist . 
3 Text Handling
The ALEP platform provides a TH component which allows " preprocessing " of inputs  , it converts a number of formats among them ' Latex '  . 
Then the ASCII text first goes through SGML-based tagging : Convertion to an EDIF  ( Eurotra Document Interchange Format ) format , then paragraph recognition , sentence recognition and word recognition . The output of these processes consists in the tagging of the recognized elements : ' P ' for paragraphs  , ' S ' for sentences , ' W ' for words ( in case of morphological analysis , the tag'M'is provided for morphemes ) and ' PT ' for punctuation signs as exelnplified in  ( 1 )  . 
(1 ) < P><S><W>Jolm</W><W>sleeps</W><PT > . </ PT></s > </ P > In the default case , this is the information which is input to the TH -LS component  ( Text-Handling to Linguistic Structure ) component . ALEP provides a facility ( tsls-rules ) which allows the grammar writer to identify information which is to flow from the TtI to the linguistic processes  . We will show how this facility can be used for all efficient and consistent treatnlent of all kinds of ' messy details '  . 
The TH component of the ALEP platform also foresees the integration of user-defined tags  . The tag ( USR ) is used if the text is tagged by a user-defined tagger  . An example of an integration of a user-defined tagger between the sentence recognition level and the word recognition level of the TH tool is given below  . 
The tagger for ' messy details ' has been integrated into the German grammar and has been adapted for the following patterns :  1 ' Quantities ' ( a cardinal number followed by an amount and a currency name  ( e . g .  "16 , 7 Mil-lionen Dollar ")) 2Percentages (12% , 12 Prozent , zwSlf Prozent ) 3 Dates (20 . Januar 1996 ) 4 Acronyms and abbreviations ( ' Dasa ' , ' CDU ' , ' GmbH ' , etc . ) . 
5 Prepositional contractions ( zum , zur , am etc . ) ? Appositions : Pr of . Dr . Robin Cooper We will examplify the technique for ' quantities '  . 
Recursive patterns are described in the program -ruing language ' awk '  . (2) defines cardinal numbers ( in letters ) . 
287  ( 2 ) two-to-ten=" ( (\[ Zz\]wei ) l ( \[Dd\]rei ) l ( \[Vv\]ier ) I ( \[Ff\]Inf ) I ( \[Ss\]echs )  \]  ( \[Ss\]leben )  \]  ( \[ Aa\]cht ) l ( \[ Nn \] ean ) I ( \[ Zz \] ehn )   ) " eleven-to-nineteen = .   . . . 
twenty-to-ninety etc.
card = " ( (\[ Ee\]inl"two-to-ten " )   ( und " twenty-to-ninety " ) ? I " . . . .  ) On the basis of these variables other variables can be defined such as in  ( 3 )  . 
(3 ) range = " ( " number " l " card " ) " amount=" ( " Millionen " I " Milliax den " ) currency = " ( Mark " , " DM " , " Dollar " ) " curmeasure = " ( " amount "? ?" currency "? ) " quantity=" ( " range . . . . curmeasure " ) " The following inputs are automatically recognized as ' quantities ': " Z wei und z wanzig Dollar " " Sechs und zwanzig Milliarden "" Drei und vierzig Milliar den DM " This treatment of regular expressions also means a significant improvement of efficiency because there is only one string whereas the original input consisted of five items  ( " vierzigbisffinfzig Milharden Dollar " ) : " vierzig_bis_fuenfzig_Milliarden_Dollar " . 
(4 ) gives an exaln ple for information flow from TH to linguistic structure :  ( 4 ) id:spec=>spec:lu=>TYPE , sign=>sign:string=>string:first=>\[ STRINGIREST\]  , rest => REST , synsem=>synsem:syn=>SYN=>syn:constype= > morphol:lemma=>VAL  , rain=>yes , ' USR ' , \[' TYPE '=> TYPE , ' VAL '=> VAL\] , STRING ) . 
The feature ' TYPE ' bears the variable TYPE ( in our case : " quantities " )  . The feature ' VAL ' represents the original input  ( e . g . : " ffinfzig Milliar den Dollar " ) and the variable STRING represents the output string of the tagged input  ( in this case : " fuenfzig_Milharden_Dollar " )  . This value is co-shared with the value of the " string " feature of the lexicon entry  . The definition of such generic entries in the lexicon allows to keep the lexicon smaller but also to deal with a potentially infinite number of words  . 
These strategies were extended to the other phenomena  . The TH component represents a preprocessing component which covers a substantial part of what occurs in real texts  . 
4 The Linguistic Modules 4 . 1 Two Level Morphology ( TLM ) The TLM component deals with most major morphographemic variations occurring in German  , such a sumlautung , ss-fl alternation , schwa-instability . 
We will introduce this component by way of ex -emplification  .   ( 5 ) represents the treatment of ' e'-'i' umlautung as occurring in German verbs like ' gebe '  , ' gibst ' , referring to ( Trostg0) . 
An ALEPTL rule comes as a four to five place PROLOG term  , the first argument being the rule name , the second a TL description , the third ( represented by the anonymous variable ) a specifier feature structure , the fourthatyped feature structure constraining the application of the rule and a fifth allowing for linking variables to predeflned character sets  . 
(5) tlm_rule(umlautE_no , \[\]\[ el\[\]<=>\[\]\['E'\]\[\] , syn:constype=>stem:phenol=>pho:umlaut = > no  )  . 
tim_rule(umlautE_i_yes , \[3\[i\]\[C\]<=>\[\]\['E'\]\[\] , syn:constype=>stem:phenol=>pho:umlaut = >   ( yes , i ) , \[C in consonants\]) . 
In order to understand the treatment one has to bear in mind that there exists the lexical entry in  ( 6 ) to which the TL rules above map to . 
,6 , \[  , \]\] syn\]consphenol\[umlautnone&iThe morphologically relevant information is encoded in ' cons '  . It contains two features , ' lemma'which encodes the abstract nlorpheme with a capital'E'  ( this is the basis for a treatment according to ( ( Trost90 ) ) and the feature ' phenol ' which encodes phonologically relevant information  , in this case whether ulnlautung is available or not  . 
The values for ' phenol ' is a boolean conjunction from two sets : sl = none  , no , yes and s2 = e , i \] . 
The treatment consists of mapping a surface'e' to a lexical'E'in case the constraint which is expressed It says that for the first rule'e ' is nrapped on ' E ' if the feature ' umlaut ' has a value which is ' no '  . This applies to (6) . This handles cases such as ' geb-e' . 
The second rule maps'i''E'if'umlaut'has the value ' yes&i '  . This also holds in cases like ' gib-st ' . 
One would expect according to ( Trost 90 ) that only two values ' no ' and ' yes ' are used . The change has been done for merely esthetic reasons  . The ' 2nd perssing ' morpheme'st'e . g . requires an ' umlaut = yes'stenr , if the stem is capable of ulnlautung , at all which is the case for ' gibst ' . In case the stem cannot have umlautung ( as for ' kommst ' ) ' st ' also attaches . This makes that u on-unflautung stems have to be left unspecified for umlautung  , as otherwise'st ' could not attach . ' st ' can now be encoded fox ' ' uml aut = no ' . 
4 . 2 Lexiconf , exical information is distributed over three lexicons : ? ATL lexicon which contains information relevant for seglnentation  , cxd usively . 
? A syntax lexicon.
? A semantic lexicon.
The distribution of information over three lexicons has a sinrple reason  , namely avoiding lexical an rbi-guities at places where they cannot be resolved or where they have in rpact one ificiency  . So , e . g . the verbal suffix ' t'ha slots of interpretations :  '3rd petssing ' , 2nd perspl ' , preterite and more . These ambiguities are NOT introduced in the TLM lexicon as the only effect would be that a great nunlber of scg-mentations would go into syntactic analysis  . Only then these ambiguities could be resolved on the basis of morphotactic intorlnation  . A similar situation holds on syntactic level . There is no point in nmltiplying syntactic entries by their semantic ambiguities and make all of these entries available for analysis  . It would result in a desasteribr efficiency . 
Semantic reading distinctions thus are put into the  ( semantic ) refinement lexicon . We would like to introduce lexical information for the preposition ' in ' by way of il lust ration  . 
TL-Entry for ' in ': (7) ' string\[inl_\] , , , od , , , , , , , ,ut , , ojjjj The nrorphological information is encoded in the ' cons ' feature  . 
Analysls - Entries for ' in ':
Prepositions \] nay occur in ' strongly bound PPs where they are function ale lenrents  ( semantically empty , but syntactically relevant ) . This is encoded in (8) . APP headed by a functor cannot be an adjunct ( rood = none )  . The head-dtr chosen by ' in ' is an NP acc . The mother of such a construction also comes out as NP acc which is encoded in ' projects '  . 
The major reason for such a treatment lies in the fact that it allows for a unified treatment of all functional elements like inflectional affixes  , com-plen rentizers , auxiliaries , infinitival zu , functional prepositions etc . .) . 

Ic~t/..\[selects NP acc\[.\[roodoo et
LsuDca ~ func\[projects NP acc ( 9 ) is the entry for ' in ' as a head of a PP subcate -gorizing for an NP acc  . 
(9)\['' y'c't'SubctLsubt\[c ?' ps<-cc>\]
Semantic entries for qrlh
Prepositions need ( semantically ) different entries depending on whether the . pheads a PP which is a conlplentent or & it adjunct  . 
qn ' as complement : lsubj < >
The content of a PP is a relation alp so a.
' in ' as Adjunct :
I-quos11 synlcat " " m ? dl ' " lc?nt / .   . \[ pso . ,\[211//V ?- c?n ~ Lrest4z\]\]//
Lr-lm ? aA/
L subcat I comp ~< . . I sem \[ cont \[4\] >/"" c ? ~' trd-c ? = t\[r~s trrrexi ~\] ' z " < \[  , ,~ gi \[4\]\] t J > r_l ) SO aIt composes the restriction list with the restriction list of the modified item  . Quants and the pso a are copied . 
4 . 3 Word Structure and Phrase Structure ( PS ) Both the word structure and the phrase structure conlponent are based on the sames nlall set of binary schenlata closely related to HPSG  . In the sys-tenl described here they exist as nlacros and they are spelt out in category -specific word and phrase structure rules  . ( Efficiency is the major reason , as underspecified syntax rules are very inefficient  )  . 
Such a schemaise . g . the following head-comp-schema . 
Mother:synsem\[Ic the'?LJl\]\]syn , subcat\[compls\[5\]

Lsem\[3\]
Head-dtr:ksem\[3\]
Comp-dtr:\[synsem\[J\]\]subcat\[compls < \[41115\] >\ [ subj \[2\] He adinformation is propagated from head-dtr to mother  , so is semantic information . ' subact'infor-marion is structured slightly differently as in HPSG to allow for one innovation wrt HPSG which is our treatment of functional elements  . 

SYN\[\]ICONSTR funct_att\]
L /
SEM ~\]

HEADIBASE ?\[--....? oj\]
Functor : lF " EADII
The functor macro is highly general . It shows the functor treatment applied in the German grammar  , namely that the functor selects its head-dtr , combines itself with the head-dtr and projects the mother  . More specifically : The functor-dtr , indicated by the value ' funct ' of the attribute ' subcat'shares the value of the attribute ' selects ' with the ' synsem ' value of the head -dtr and its ' projects ' value with the ' syn ' attribute of the mother  . The ' head'value is shared between head-dtr and mother  , the ' base'value in addition between head-dtr and functor  . The subcategorization is shared between head-dtr and mother  . 
The difference to the head-comp schema is that head information comes from the functor  , also the semantics . ' subcat'is inherited from the head-dtr . 
The powerful mechanism comes in by the subcat -feature of the functor which allows for a very detailed specification of the information to be projected  . 
The PS component covers all categories , especially all clausal categories ( main clauses , subordinate clauses , relatives ) , NPs , APs , ADVPs , PPs . 
5 ' E f f i c iency ' and Per fo rmance In this section we would like to address the topic of efficiency  . A number of points contributing specifically to efficiency should be summarized here  . 
? ALEP is designed to support efficiency as far as the formalism  ( ' lean ' approach ) is concerned . 
Formal constructs known to be computationally expensive are not available  3  . 
? Refinement ( mentioned already ) is a monotonic appfication of phrase structure rules and lexical entries to further featurize  ( flesh-out with features ) a finguis6 c structure , established in analysis . 
If Q1 is the finguistic output structure of the analysis  , then Q ~ is the output structure of ' r e -finenlent'if  Q1 subsumes Q2  , i . e . every local tree in Q = and every lexical entry in Q~is subsumed by a corresponding local tree and a corresponding lexical entry in  Q1  . 
Any nondeterministic backtracking algorithm ( depth-first ) is badly effected by ambiguities as it has to redore peatedly large amounts of work  . In terms of lingware development this means that lexical ambiguities have to be avoided for analysis  . As on the other hand lexicalist theories result in an abundance of lexical ambiguities  , ' refinement ' is a relief . Optimal distribution of inforlnation over analysis and refinement results in a gain of efficiency by several factors of magnitude  . 
, , Head selections : ALEP allows for user-defined parsing head declarations as " the most  appro-3It should have been shown in the preciou sections that felicitous descriptions are possible anyway  . 
2 90 priate choice of head relations is grammar dependent "  ( Alshtl )  , p . 318 . On the basis of the user-defined head relations the reflexive transitive closure overhead relations is calculated  . It has to be made sure that the derived relations are as compact as possible  . Optimal choice of head relations pays off in a gain in efficiency by several factors of magnitude  . 
? Keys : Keys are values of attributes within linguistic descriptions defined by path declarations  . Keys allow for indexation and efficient retrieval of rules and lexical entries  . This be conresextremely relevant for larger-scale resources  . A key declaration which the grammar developer may do identifies the atomic value which is to serve as a key  . Optimal keys again result in a substantial gain in efficiency  . 
? Last not least tuning the grammars with a view oil efficiency has contributed to tile current performance of the system  . 
In the following we would like to give some actual figures which may illustrate performance  . These figures are not meant to be an exact measurement as exact measurenrents are not available  , in order to give an indication it may be said that ALL the phenomena which increase in determinism in a grammar of German are covered : All forms of the articles  ( ' die ' , ' der ') and homomorphous relative pronouns , all readings of verbs ( all frames , all syntactic realizations of complements ) , semantic readings , prepositions and honu ) lnorphous prefixes , PPs as nominal adjuncts , aspread jectival complements , as adjuncts to adverbs , as VP adjuncts , valent nouns ( with optional complementation ) , all readings of Gerlnan'sein ' , coordination , N-~N combinations , relatives , Nachfeld . 
One result of the corpus investigation was that 95% of the sentences in the corpus have between 5 and 40 words . The grammar is able to parse sentences with up to  40 words in 120 sees . The following are corpus examples containing time -consmning parse problems  . 
Input : Inden Wochen vor Weihnaehtenkonnte der stolze Vorsitzen de der zu Daimler-Benz gehoeren den Deutsche Aerospace A Gein 
Jahresergebnis , dasalle Erwartungenue bertraf , verkuenden . 
(Comment : In the weeks before X-mas the proud head of the Deutsche Aerospace AG which belongs to Daimler-Benz could announce an annual statement of accounts which exceeds all expectations  . ) total R Word SegR Lift An a Refine sol:i34 .  450 0 . 380 34 . 070 0 . 000 Input : Dieser Erfolgueber raschtinz wei

(Comment : This success is surprising in two respects  , ) total R Wprd SegR Lift An a Refine sol:11 . 910 0  . 130 1  . 780 0  . 0 00 For industrial purposes this may still be too slow  , but we think that the figures show that the system is not so far away front reality  . 
6 Conclusions
This paper was about the following aspects of lingware development : eLinguistic felicity and leanness  . 
? Leanness and efficiency.
? Methods for largescale grammar development.
?' I lolistic ' approach.
We can summarize briefly : ? ALEP provides all modules and tools from text handling to discourse processing  ( the latter not introduced here )  . The lingware createdises : pecially interesting ill that it provides an integrated design for all the modules  . 
? The formalism forling w are development is lean , but it provides sufficient means to support mainstream felicitous linguistic descriptions  . 
? Efficiency is good compared to other unification -based systems  . 
It is not yet ready for immediate commercial applications  , but it is neither very far away . 
? The corpus-based approach to grammar development is the only realistic way to get closer to a coverage that is interesting from all application point of view  . 
ALEP is a promising platform for development of largescale application-oriented grammars  . 

Iliyan Alshawi , Arnold DJ , Backofen R , Carter DM , Lindop J , Netter K , Pulman SG , Tsujii J and Uszkoreit H ,  (1991) , Eurotra ET6/I : Rule Formal-i , ~m and Virtual Machine Design Study ( Final Report )  , CEC 1991 . 
H . Trost : The application of two-level nror phology to nonconcatenative Grman morphology  . Procecd-ing . s of COLING90, Helsinki , 1990, vol . 2, 371-376 . 

