Learning Part-of-Speech Guessing Rules from Lexicon : 
Extension to Non-Concatenative Operations *
Andrei Mikheev
HCRC Language Technology Group
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW , Scotland , UK
: Andrei . Mikheev@ed.ac.uk

One of the problems in part-of-speech tagging of real-word texts is that of unknown to the lexicon words  . In ( Mikheev ,  1996) , a technique for fully unsupervised statistical acquisition of rules which guess possible parts -of-speech for unknown words was proposed  . 
One of the oversimplification assumed by this learning technique was the acquisition of morphological rules which obey only simple coneatenative rgularities of the main word with an affix  . In this paper we extend this technique to the nonconcatenative cases of suffixation and assess the gain in the performance  . 
1 Introduction
Part-of-speech ( pos ) taggers are programs which assign a single postag to a word-token  , provided that , it is known what parts-of-speech t is word can take on in principle  . In order to do that taggers are supplied with a lexicon that lists possible l'os-tags for words which were seen at the training phase  . Naturally , when tagging real-word texts , one can expect to encounter words which were not seen at the training phase and hence not included into the lexicon  . This is where word-Posguessers take their place -they employ the analysis of word features  , e . g . word leading and trailing characters to figure out its possible pos categories  . 
Currently , most of the taggers are supplied with a word -guessing component for dealing with unknown words  . The most popular guessing strategy is so called " ending guessing " when a possible set  , of postags for a word is guessed solely on the basis of its trailing characters  . An example of such guesser is the guesser supplied with the Xerox tagger  ( Kupiec ,  1992) . A similar approach was taken gome of the research reported here was funded as part of EPSRC project  IED4/1/5808 " Integrated Language Database " . 
in ( Weischedel et al ,  1993 ) where an unknown word was guessed given the probabilities for an unknown word to be of a particular pos  , its capitalisation feature and its ending . In ( Brill ,  1995 ) a system of rules which uses both ending-guessing and more morphologically motivated rules is described  . Best of these methods were reported to achieve 82-85% of tagging accuracy on unknown words , e . g . ( Brill , 1995; Weischedel et al , 1993) . 
In ( Mikheev , 1996) a cascading word-Posguesser is described . It applies first morphological prefix and suffix guessing rules and then ending-guessing rules  . This guesser is reported to achieve higher guessing accuracy than quoted before which in average was about by  89% better than that of the Xerox guesser and by 67% better than that of Brill's guesser , reaching 8792% tagging accuracy on unknown words . 
There are two kinds of word-guessing rules employed by the cascading uesser : morphological rules and ending guessing rules  . Morphological word-guessing rules describe how one word can be guessed given that another word is known  . In English , as in many other languages , morphological word formation is realised by affixation : prefixation and suffixation  , so there are two kinds of morphological rules : suffix rules  ( A ' ~ ) -rules which are applied to the tail of a word , and prefix rules ( AP ) -- rnles which are applied to the beginning of a word  . For example , the prefix rule:
AP:\[u ,   ( VBD VBN )   ( JJ ) l says that if segmenting the prefix " un " from an unknown word results in a word which is found in the lexicon as a past verb and participle  ( VBD VBN )  , we conclude that the unknown word is an adjective  ( JJ )  . This rule works , for instance , for words\[developed-+undeveloped\] . An example of a suffix rule is:
A ~ :\[ ed(NNVB ) ( JJVBDVBN)\]
This rule says that if by stripping the suffix " ed " from an unknown word we produce a word with the pos-class noun/verb  ( NN VB )  , the unknown word is of the class adjective/past -verb/participle  ( JJ VBD VBN )  . This rule works , for instance , for Unlike morphological guessing rules , ending-guessing rules do not require the main form of an unknown word to be listed in the lexicon  . These rules guess a pos-c . lass for a word . just Oil the basis of its ending characters and without looking up it ' ~ st  ; erain the lexicon . For example , an ending-guessing rule
Ae :\[ ing --- ( a a NN VBG ) \] says that if a word ends with " ing " it ; can be an adjective , a noun or a gerund . Unlike a morphological rule , this rule does no task to ( : hock whether the snb string preceeding the " ing " -ending is a word with a particular postag  . 
Not surt ) risingly , morphoh ) gical guessing rules are more accurate than ending -guessing rules lint their lexical coverage is more restricted  , i . e . dmy are able to coverless unknown words . Sine (' . the yal-(~ . illore , accurate , in the cascading uesser they we real ) plied before the ending-guessing rules and improved the pre  , cision of the guessings by about 5? . /0 . This , actually , resulted in about 2% higher accuracy of tagging on unknown words . 
Although in general the performance of the cascading guesser was detected to be only  6% worse than a general-language lexicon lookup , one of the over-simt ) lifications a sumed at the extraction of i ; hemort ) hological rules was that they obey only simI ) lecon ( : a tenativergularities : book ? ~ book--ed ; take--+take-l-n ; play-4 play qoing . 
No atteml ) tS were made to model non-concaten advecases which are quite e o in mon in 
English , as for instance : try- , tries ; reduce-+reducing ; advise-~advisable . 
So we though that the incorporation of a set of guessing rule  , s which call capture morphok ) gical word dependencies with letter alterations houldext  ; end the lexieal coverage of tile morphoh ) gical rules and hence might contribute to the overall guessing accuracy  . 
In the rest of the paper first , we will I ) riefly outline the unsupervised statistical learning technique proposed in  ( Mikheev ,  1996) , then we propose a modification which will allow for the incorporation of the learning of nonconcatenative mor-t  ) hological rules , and finally , wc will ew fluate and assess the contribution of the nonconcatenative sutfix morphological rules to the overall tagging av  , curaey on unknown words using the cascading guesser  . 
2 The Learning Paradigm
The major topic in the development of worth pos guessers is the strategy which is to be used f  ( )r dm acquisition of the guessing rules . 
Brill ( Brill ,  1995 ) outlines a transformationbased learner which learns guessing rules from a pretagged training corpus  . A statistical-based suffix learn exis presented in  ( Schmid ,  1994) . Fromal ) re-tagged training corpus it constructs the suffix tree where every sutfix is associated with its information measure  . 
The learning technique employed in the induction of tile rules of the cascading guesser  ( Mikheev ,  1996 ) does not require specially prepared training data and employs fullytm super-vised statistical learning from the lexicon supplied with the tagger and word-ti'e queneies obtained from a raw corpus  . The learning is implemented as a two-staged process with f e  . edback . First , set-dng certain parameters a set of guessing rules is acquired  , th ( mit is evaluated and the results of evaluation are used for re  . -acquisition of a bette . r tuued rule set . As it has been already said , this learning technktuet ) roved to be very successful , but did not attempt at the acquisition of word -guessing rules which do not obey simple concatenations of a main word with some prefix  . Ilere we present a ne , xte , nsion to accommodate such cases . 
2.1 Rule Extraction Phase
In the initial learning technique ( Mikheev ,  1996 ) which ac ( : ounted only tbrsit nl ) leconcate native regularities aguessing rule was seen as a triph '  , :
A = ( S,I,H , ) where
S is the affix itself ;
I is the l'o s-elass of words which should be looked llI  ) in the lexicon as main forms ; R is the pos-elass which is assigned to unknown words if the rule is satisfied  . 
II ere we extend this structure to handle cases of the mutation in the last n letters of tile main word  ( words of /- class )  , as , for instance , in the case of try-?tries , wtlen the letter " y " is changed to " i " before the suffix  . To accommodate such alterations we included an additional mutation dement  ( M ) into tile rule structure . This element keeps the , segment o be added to the main word . So the application of a guessing rule can be described as : unknown word-  5' + M:I-H , i . e . fl'om an unknown word we strip the affix S , add then lutative segment M , lookup tile produced string in the lexicon and if it is of  ( : lass I we conclude that the unknown word is of class 
H , . For examt ) le:thesut tix rule A ~ :\[ S~-~iedI ~--- ( NN , VB ) R ~ -- ~ ( JJ VBD VBN ) M = y \] or in shortlied ( NN VB )   ( JJ VBD VBN ) y \] says that if there is an unknown word which ends with "  led "  , we should strip this ending and append to the remaining part the string " y "  . If then we find this word in the lexicon as ( NN VB )   ( noun/verb )  , we conclude that the guessed word is of category ( JZ VBD VBN )   ( adjective , past verb or participle ) . This rule , for example , will work for word pairs like , pecify-specified or deny-denied . 
Next , we modified the V operator which was rules . We augmented this operator with the index n which specifies the length of the mutativending of the main word  . Thus when the index n is 0 the result of the application of the V0 operator will be a morphological rule without alterations  . 
The V1 operator will extract the rules with the alterations in the last letter of tile main word  , as in the example above . The V operator is applied to a pair of words from the lexicon  . First it segments the last n characters of the shorter word and stores this in the M element of the rule  . Then it tries to segment an affix by subtracting the shorter word without the mutative ending from the longer word  . If the subtraction results in an nonempty string it creates a morphological rule by storing the pos-class of the shorter word as the /- class  , the pos-class of the longer word as the R- ( : lass and the segmented affix itself . For example : \[ booked ( JJ VBD VBN ) \] Vo\[book ( NN VB ) \] -- ) As : \[ ed ( NN VB )   ( JZ VBD VBN )   . . . . \] \[ advisable ( JJ VBD VBN ) \]~1\[advise ( NN VB ) \]--~ A ~ :\[ able ( NN VB )   ( JJ VBD VBN ) " e "\] The V operator is applied to all possible lexicon-entry pairs and if a rule produced by such an application has already been extracted from another pair  , its frequency count ( f ) is incremented . Thus sets of morphological guessing rules together with their calculated frequencies are produced  . Next , from these sets of guessing rules we need to cut out infrequent rules which might bias the further learning process  . To do that we eliminate all the rules with the frequency fless than a certain threshold  0:  . Such filtering reduces the rule sets more than tenfold and does not leave clearly coincidental cases among the rules  . 
2.2 Rule Scoring Phase
Of course , not all acquired rules are equally good as plausible guesses about word-classes  . So , for ew~ry acquired rule we need to estimate whether it is an effective rule which is worth retaining in the final ruleset  . To perform such estimation we take one-by-on each rule from the rule sets produced at the rule extraction phase  , take each word-token from the corpus and guess its POS set using the rule if the rule is applicable to the word  . 
For example , if a guessing rule strips a pro'titular suffix and a current word from the corpus does not have such suffix we classify these word and rule as incompatible and the rule as not applicable to that word  . If the rule is applicable to the word we perform lookup in the lexicon and then compare the result of the guess with the information listed in the lexicon  . If the guessed pos-set is the same as the Pos-set stated in the lexicon  , we count it as success , otherwise it is failure . Then for each rule : usually we set this threshold quite low:  24  . 
we calculate its score as explained in ( Mikheev , 1996) using the scoring function as follows: . ~/ t1scorei=/3i-1 . 65* V ' ~ , t ~+ log ( ISd ) ) where /3 is the proportion of all positive outcomes ( x ) of the rule application to the total number of compatible to the rule words  ( n )  , and IS l is the length of the affix . We also smooth/3 so as not to have zeros in positive or negative outcome  probabilities:/3 = n + l " Setting the threshold Os at a certain levellets only the rules whose score is higher than the threshold to be included into the final rule sets  . 
The method for setting up the threshold is based on empirical evaluations of the rulesets and is described in Section  2  . 3 . 
2.3 Setting the Threshold
The task of assigning a set of pos tags to a particular word is actually quite similar to the task of document categorisation where a document should be assigned with a set of descriptors which represent its contents  . The performance of such assignment can be measured in : recall-the percentage of pos tags which the guesser assigned correctly to a word  ; precision - the percentage of POS tags tile guesser assigned correctly over the total number of postags it assigned to the word  ; coverage-tile proportion of words which the guesser was able to classify  , but not necessarily correctly . 
There are two types of test data in use at this stage  . First , we measure the performance of a guessing ruleset against the actual lexicon : every word from the lexicon  , except for closed-class words and words shorter than five characters  , is guessed by the rulesets and the results are compared with the information the word has in the lexicon  . In the second experiment we measure the performance of the guessing rule sets against the training corpus  . For every word we measure its metrics exactly as in the previous experiment  . Then we multiply these measures by the corpus frequency of this particular word and average them  . Thus the most fi'equent words have the greatest influence on the final measures  . 
To extract he best-scoring rule sets for each acquired set of rules we produce several final rule sets setting the threshold  0  , at different values . 
For each produced rule set we record tile three metrics  ( precision , recall and coverage ) and choose the sets with the best aggregate measures  . 
3 Learning Experiment
One of the most important issues in the induction of guessing rulesets is the choice of right data for training  . In our approach , guessing rules are ex-
Strategy-Suffix ( S6o)
SutIix with alt . ( Aso)

Aso+S(~o
Ending ( E75)
Srm+E75$6 o+As0+E75


Precision Recall Coverage 0 . 920476 0 . 959087 0 . 373851 0 . 964433 0 . 97194 0 . 193404 0 . 925782 0 . 959568 0 . 4495 0 . 928376 0 . 959457 0 . 4495 0 . 666328 0 . 94023 0 . 97741 0 . 728449 0 . 941157 0 . 9789471 0 . 739347 0 . 941548 0 . 979181 0 . 740538 0 . 941497 0 . 979181

Precision Recall Coverage 0 . 978246 0 . 973537 0 . 29785 0 . 996292 0 . 991106 0 . 187478 0 . 981375 0 . 977098 0 . 370538 0 . 981844 0 . 977165 0 . 370538 0 . 755653 0 . 951342 0 . 958852 0 . 798186 0 . 947714 0 . 961047 0 . 805789 0 . 948022 0 . 961047 0 . 805965 0 . 948051 0 . 961047 Tablel . : Results of the cascading application of the rule sets over the training lexicon and training corpus  .   As0 -suffixes with alteration scored over 80 points ,   $60 -suffixes without alteration scored over 60 points , ET~-ending-guessing ruleset scored over 75 points . 
tracted from the lexicon and the actual corpus fre ~ qnencies of word-usage then allow for discrinfina-tion between rules which are no longer productive  ( but haw'~left their imprint on the basic lexicon  ) and rules that are productive in real-life texts . 
Thus the major factorill the learning process is the lexicon-it should be as general as possible  ( list all possible Poss for a word ) and as large as possible , since guessing rules are mean to capture general anguage regularities  . The corresponding corpus should include most of the words fi'om the lexicon and be large enough to obtain reliable estimates of word-frequency distribution  . 
We performed a rule-induction experiment using the lexicon and word-frequencies derived from the Brown Corpus  ( Prancis & Kucera ,  1982) . 
There are a number of reasons tbr choosing tile Brown Corpus data for training  . The most important ones are that the Brown Corpus provides a model of general multidomain language use  , so general language regularities carl be induced h'omit  ;  , and second , many taggers come with data trained on the . Brown Corpus which is usefll l for comparison and evaluation  . This , however , hynomeans restricts the described technique to that or any other tagset  , lexicon or corpus . Moreover , despite the fact that tile training is performed on a particular lexicon and a particular corpus  , the obtained guessing rules suppose to be domain and corpus independent and the only training -dependent  , feature is the tagset in use . 
Using the technique described above and the lexicon derived fror a the Brown Corpus we extracted prefix morphological rules  ( no alterations )  , suffix morphological rules without alterations and ending guessing rules  , exactly as it was done in ( Mikheev ,  1996) . Then we extracted suffix morphological rules with alterations ill the last letter  ( V1 )  , which was a new ruleset for the cascading guesser  . Quite interestingly apart frolntile expected suffix rules with alterations as : \[ S=led  1=   ( NN , VB ) R = ( JJ VBD VBN ) M = y\]which can handle pairs like deny-+ denied  , this ruleset was populated with " second-order " ules which describe dependencies between secondary for nls of words  . For instance , the rule\[S=i on I = ( NNS VBZ ) R = ( NN )   M=8\] says if by deleting the suffix " ion " from a word and adding " s " to the end of the result of this deletion we produce a word which is listed in the lexicon as a plural noun and  3rd form of a verb ( NNS VBZ ) the unknown word is a noun ( NN )  . 
This rule , for instance , is applicable to word pairs : affects -+ affection  , asserts -+ assertion , etc . 
Table 1 present some results of a comparative study of the cascading application of the new ruleset agains the standard rule sets of the cascading guesser  . Tim first part of Table 1 show stile best obtained scores for the standard suffix rules  ( S ) and suffix rules with ~ flterations in the last letter  ( A )  . Wtmn we applied the two suffix rule sets cascadingly their joint lexical coverage increased by about  78%   ( from 37% to 45% on the lexicon and fl'om 30% to 37% on the corpus ) while precision and recall remained attiles anle high level  . 
This was quite an encouraging result which , a ('- tually , agreed with our prediction . Then we measured whether suffix rules with alterations  ( A ) add any improve ulent if they are used in conjunction with the ending-guessing rules  . Like in the previous experiment we measured the precision  , recall and coverage both on tim lexicon and on tile corpus  . The second part of Table 1 shows that simple concatenative suffix rules ( $60 ) improved the precision of the guessing when they were applied before the ending-guessing rules  ( E 75 ) by about 5% . Then we cascadingly applied the suffix rules with alterations  ( As 0 ) whict l caused further improvement in precision by about  1%  . 
After obtaining the optimal rulesets we performed tile same experiments on a word-sample which was not included into the training lexicon and corpus  . We gathered about three thousand words from tile lexicon dev cloped for tile Wall 
ScoreScore
Lexicon Guessing strategy
Pull standard : P+S+E ) hfll with new : P+A+S+E
Small standard : P+S+E
Small with new : P+A+S+E
Total Unkn . Total words words mistag.



292 33 292 33 95 . 1% 95 . 1% 94 . 44% 94 . 79% 90 . 5% 90 . 5% 86 . 05% 87 . 0 0% Table 2: Results of tagging a text using the standard Prefix+Suffix + Ending cascadinguesser and the guesser with the additional rule set of suffixes -with-Alterations  . For each of these cascading uessers two tagging experiments were performed : the tagger was equipped with the flfll Brown Corpus lexicon and with the small lexicon of closed-class and short words  ( 5 , 465 entries ) . 
Street Journal corpus 2 and collected frequencies of these words in this corpus  . At this test-sample evaluation we obtained similar metrics apart from the  ( : overage which dropped by about 7% for both kinds of sutfix rules . This , actually , did not come as a surprise , since many main tbrms required by the suffix rules were missing in the lexicon  . 
4 Evaluation
The direct performance measures of the rulesets gave us the grounds for the comparison and selection of the best performing uessing rule sets  . 
The task of unknown word guessing is , however , a subtask of the overall part-of-speech tagging process  . Thus we are mostly interested in how the advantage of one ruleset over another will affect the tagging performance  . So , we performed an independent evaluation of the lint  ) act of the word guessing sets on tagging accuracy . In this evaluation we used the cascading application of prefix rules  , suffix rules and ending-guessing rules as described in  ( Mikheev ,  1996) . We measured whether the addition of the suffix rules with alterations increases the accuracy of tagging in comparison with the standard rule sets  . In this experiment we used a tagger which was a c ++ reimplementation of the LISP implemented HMM Xerox tagger described in  ( Kupiec , 1992) trained on the Brown Corpus . For words which failed to be guessed by tile guessing rules we applied the standard method of classifying them as common nouns  ( NN ) if they are not capitalised inside a sentence and proper nouns  ( NP ) otherwise . 
In the evaluation of tagging accuracy on unknown words we payed attention to two metrics  . 
First we measure the accuracy of tagging solely on unknown words : Unkown Seore = Correctly Ta  , q , qcdUnkownWords
Total Unknown Words
This metric gives us the exact measure of how the tagger has done when equipped with different guessing rule sets  . In this case , however , we do not account for the known words which were mistagged because of the unknown ones  . Toputa 9these words were not listed in the training lexicon perspective on that aspect we measure the overall tagging performance : 
Tota IS core = C ? rrectly Tagged W ? rds
TotaI Words
To perform such evaluation we tagged several texts of different origins  , except ones from the Brown Corpus . These texts were not seen at the training phase which means that neither the tagger nor the guesser had been trained on these texts and they naturally had words unknown to the lexicon  . For each text we performed two tagging experiments  . Intile first experiment we tagged the text with the fullfledged Brown Corpus lexicon and hence had only those unknown words which naturally occur in this text  . In the second experiment we tagged the same text with the lexicon which contained only closed -classa and short  4 words . This small lexicon contained only 5 , 456 entries out of 53 , 015 entries of the original Brown Corpus lexicon . All other words were considered as unknown and had to be guessed by the guesser  . 
In both experiments we ineasured tagging accuracy when tagging with the guesser equipped with the standard Prefix+Suffix + Ending rule sets and with the additional rule set of suffixes with alterations in the last letter  . 
Table 2 present some results of a typical example of such experiments  . There we tagged a text of 5,970 words . This text was detected to have 347 unknown to the Brown Corpus lexicon words and as it can be seen the additional ruleset did not cause any improvement to the tagging accuracy  . Then we tagged tile same text using the small lexicon  . Out of 5 , 970 words of the text ,  2 , 215 were unknown to the small lexicon . Here we noticed that the additional ruleset improved tile tagging accuracy on unknown words for about  1%: there were 21 more word-tokens tagged correctly because of the additional ruleset  . Among these words were : " classified " , " applied " , " tries " , " tried " , " merging " , " subjective " , etc . 
a articles , prepositions , conjunctions , etc.
4 shorter than 5 characters
The target ; of the research reI ) orted in this pa-1 ) er was to incorporate the learning of morl ) holog-ical word-t'osguessing rules which ( lonotol ) eysim I ) le ( : on catenations of main words with affixes into the learning paradigm proposed in  ( Mikheev ,  1996) . ~ l . k ) do that we extended the data stru ( :-tures and the algorithlns for the guessing-rule ap-1  ) li ( : ation to handle the mutations in the last n letters of the main words  . Thus siml ) leconcate native rules naturally became as ul ) set of the mu-tative rules they can 1 ) e seen as mutative rules with the zero in utation , i . e . when the M element of the rule is empty . Simple . con(:atenative rules , however , are not necessarily regular morphological rules and quite often they capture other nonlinear morphological dependen  ( : ies . For instance , consonant doubling is naturally cal ) tured by the affixes themselves and obey siml ) leconcatenations , as , fl ) rexaln I ) le , describes the suffix rule A ~':\[ S=gang l = ( NN VII )  1~: =  ( JJ NN VB (   ; ) M~---""\] This rule . for examl)le , will workfl ) r word pairs like t , ~ g-tagging OF dig-digging . Not ( ) that here we don't speei\[y the prerequisites for the stem-word to have one syllable and end with the same consonant as in the beginifing of the affix  . Our task here is not to provide at ) recise morphological deserii ) tion of English 1 ) ut rather to SUl ) t ) ort computationally effective pos-guessings , by elll-1) loying some , morphological information . So , in-st ; cad of using a prol ) er morphological t ) ro (: essor , we adopted an engineering at ) preach which is argued tbrin ( Mikheev & Liubushkina ,  1995) . There is , of course , i lothing wrong with morphological processors per se  , but it is hardly feasit ) le to retrain them fully automatically for new tagsets or to induce  , new rules . Our shallow Ix ~ ( ' hnique on the contrary allows to in ( hlce such rules completely automat ; ically and ensure that these rules will have enough discriminative fatures for robust guessings  . In fact , we abandoned the notion of morpheme attd are dealing with word segments regardless of whether they are  , " proper " morphemes or nol; . So , for example , in the rule above " ging " is ( : onsidered as a suffix which illi ) rincil ) le is not right : the suffix is " ing " and " g " is the dubbed  ( : onsonant . Clearly , such nuan ( : es are impossible to lem ' n autolnati ( : ally without specially l ) repared training data , which is denied by the technique in use . On the other hand it is not clear that this finegrained information will contribute to the task of morphological guessing  . The simplicity of the l ) rol ) osed shallow morphology , however , ensures flflly automatic acquisition of such rules and the emi  ) iri ( : alevahlation presenl ; ed in section 2 . 3  ( ' ( ) ntirmed that they are just right for the task : 1  ) recision ; rod recall of such rules were measure dilither a ilge of  96-99%  . 
The other aim of the research tel ) erred here was to assess whether nou-concatenative morphological rules will improve the overall performance of the cascading uesser  . As it was measured in ( Mikheev ,  1996 ) simple concatenative prefix and sutlix morphological rules iIn proved the overall i  ) recision of the cascading uesser 1 ) y about 5% , which resulted in 2% higher a ( :curacy of tagging on ml known words . The additional rule set of stirk fix rules with one  , letter mutation caused so ille flirt , her improvement . The precision of the guessing increased by al ) out1% and the tagging ac-cura ( : y on a very large set of unknown words increased l  ) yat ) out 1% . in ( : on chlsion we ( : t in say that although the ending-guessing rules , which aren mehs impler than morphological rules , can handle words with affixes longer than two chara  ( > ters almost equally well , in the fi'a mework of postagging even afl'action of per  ( : ent is an importantimi ) rovement . Therefore the ( : ontribution of the morphological rules is w flual ) le and ne ( : essary for I ; hero bust t'os-tagging of realworld texts . 

E . Brill 1995 . ' l'ransformation-l ) ased error-driven learning and Natural Languaget ) roeessing : a case study in part-ot~speee h tagging  . In Computational Linguistics 21 (4) W . Fran(:is and 1I . Kucera 1982 . Frequency Analysis of English Usage . Houghton Mitflin,

3 . Kupiec 1992 . l/ , obust Part-of-Speech Tagging Using al Iidden Markov Model  . in Uomputer , qpeech and Language A . Mikhe <; v and L . Liubushkina 1995 . Russian m <) r phology : An engineering approach . In Nat-u'y nl Language Engineering , 1(3) A . Mikheev 1996 . Unsupervised Lem:ning of \? ord-C~tegory Guessing Rules  . Inl ' rocecdings of the . ? ~ thAnr ~ at al Meeting of the As . ~ocicttion for Computational Linguistics ( AUL-96) , Santa
Cruz , USA.
\[ t . Schmid 1994 . Part of Speech Tagging with Neural Networks . In P~vcecdinqs of the l Sth In-ter'national Cot @fence on Cornputatior ~  , al Linguistics ( COLING-9~) , Kyoto , Japan . 
R . Weischedel , M . Meteer , R . Schwartz,
L . Ramshaw and J . Pahnucci 1993 . Coping with ambiguity and unknown words throught ) rol ) at ) ilistic models . In Computational Lin- . quistic , % w ) l19/2
