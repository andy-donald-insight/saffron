MODULARITYINACONNECTIONIST MODEL
MORPHOLOGY ACQUISITION
Michael Gasser
Departments of Computer Science and Linguistics
Indiana University

Abstract
This paper describes a modular connection , st model of the acquisition of receptive inflectional morphology  . The model takes inputs in the form of phones one at a time and outputs the associated roots and infections  . In its simplest version , the network consists of separate simple recurrent subnetworks for root and inflection identification  ; both networks take the phone sequence as inputs . 
It is shown that the performance of the two separate modular networks is superior to a single network responsible for both root and inflection identification  . 
In a more elaborate version of the model , the network learns to use separate hidden-layer modules to solve the separate tasks of root and inilection identification  . 

For many natural languages , the complexity of bound morphology makes it a potentially challenging problem for a learning system  , wl , ether human or machine . A language learner must acquire both the ability to map polymorpheml c words onto the sets of semantic elements they tel  ) resent and to map meanings on topoly morphemic words  . 
Unlike previous work on connection , st morphology ( e . g . , MacWhinney ~5 Leinbaeh (1991) , Plunker , & Marehman ( 1991 ) and Rumelhart & MeClell and ( 1986 ) ) , the focus of this paper is receptive nmr-phology , which represents the more fundamental , or at least the earlier , process , one which productive nmr phology presumably buihls on  . 
The task of learning receptive morphology is viewed here  , as follows . The learner is " trained " on pairs of forms , consisting of sequcnces of phones , and " meanings " , consisting of sets of roots and inflections . I will refer to the task as root and inflection identification  . Generalization is tested by presenting the learner with words consisting of novel combinations of familiar morphemes  . If the rule in question has been acquired , the learner is able to identify the root and inflections in the test word  . 
Of interest is whether a model is capable of acquiring rules of all of the types known for natural languages  . This paper describes a psychologically motivated connection  , stmodel ( Modular Connec-tion , stNetwork for the Acquisition of Morphology , MCNAM ) which approaches this level of performance . The emphasis here is on the role of modularity at the level of root and inflection in the model  . I show how this sort of modularity improves performance  ( lramatically and consider how a network might learn to use modules it is provided with  . 
Asel ) a rate paper ( Gasser ,  1994 ) looks in detail at the model's performance for particular categories of morI  ) hology , in particular , template morphology and reduplication . 
The paper is organized as folh ) ws . I first provide a brief overview of the categories of morphological rules found in the werh l's languages  . I then present a simple version of the model and discuss simulations which demonstrate hat it generalizes for most kinds of morphoh  ) gical rules . I then describe a version of the model augmented with modularity at the level of root and inflection which generalizes significantly better and show why this appears to be the case  . Finally , I describe some tentative attempts to develop a model which is provided with modules and learns how to use them to solve the morphology identification tasks it is faced with  . 
CATEGORIESOF
MORPHOLOGICAL PROCESSES
I will be discussing morphology in terms of the traditional categories of " root " and " intlection " and morphological processes in terms of " rules "  , though it should I ) e emphasized that a language learner does not have direct access to these notions  , and it is an open question whether they need to be an explicit part of the system which the learner develops  , letah ) nethedevice which the learner starts out with . I will not make a distinct , m , between inflectional and deriwttional morl ) hoh ) gy ( using " inth , c-tion " for both ) and will not consider compmmding . 
AJ Iixation , revolves the addition of the inflection to the root  ( or st , , , , , ) , either I , efore (' ~ , , , ' e fixatio , O , after ( su/fizatlon ) , within ( infixation ) , or both before and after ( circun ~ Ji : r . ation ) the root . A further type of morphological rule , which I will refer to asmutation , consists in modification to the root segments themselves  . A third type of rule , familiar in Semitic languages , is known as template morphology . tIere a word ( erstem ) consists of a root and a pattern of segments which are intercalated  1  ) ct ween the root segments in a way which is specified within the pattern  . A fourth type , the rarest of all , consists in the deletion of one or , nor (; segments . A fifth type , like aflixation , involves the addition of something to the root form  . But the form of what is added in this case is a copy  , or a systematically cess , reduplication , is it , one way the most cmn plex type of morphology ( though it may not necessarily be the most difficult for a child to learn  ) because it seems to require a variable . It is not handled by the model discussed in this paper  . G ; Lsser ( 1994 ) discusses modification of the model which is required to accommodater duplication  . 
THEMODEL
The al ) l ) roach to hmguage acquisition exemplilied in this paper differs from traditional symbolical  ) -proaches in that the focus is on specifying tile sort of cognitive architecture and the sort of general pro-ce  . ssing and learning mc chani . ~ ms which h ; we the capacity to learn some ; ~speet of language , rather than the innate knowledge which this might require  . 
If successflfl , such ~ t model would provide a simpler account of the acquisition of morphology thav one which begins with symbolic knowledge and constraints  . Connectionlst models are . interesting in this regard because of their power fi  , lsul ) - symbolie learning algorithms . But in the past , there has been relatively little interest in investigating the effect on tile language acquisitio  , t capacity of structuring networks in particular ways  . The concern in this l ) a per will I ) e with what is gained 1 ) y adding modularity to a network . 
Giventile I ) i usic l ) rol ) lemof what it means to learn receptive morphology  , I will begin witl , one of the siml ) lest networks that could have that capacity and then augmen the device as necessary  . 
In this paper , two versions of the model are described . Version 1 successfidly learns simple exam-pies of all of tile morl  ) hological rules except reduplication and circumfixation  , but its l ) erformance is far from the level that might be exl ) ected frm na human language learner . Version 2 ( MCNAM proper ) incorporates a form of builtin modularity which separates portions of tile network resl  ) onsi-l ) le . for tile i ( lentificatim t of the root and the inflections ; this improves then ct work ' s1 ) erformance signiticantly on all of the rule types except reduplication  , which cav not be learned even by a network outfitted with this form of modularity  . 
Word recognition is an incremental process.
Words are often recognized hmg before they finish ; hearer seem to be continuously coml ) ariug the contents of at linguistic short term memory with the phonological representations ill their mental lexicons  ( Marsk , n-Wilson & Tyler ,  1980) . Thustile task at hand requires a short term memory of some sort  . There are several ways of representing shortterm memory in cmmectionist networks  ( Port ,  1990) , in particular , through the use of time-delay connections out of input units and through the use of recurrent time -delay cmmections on some of the network units  . The most ttexible apl ) roach makes use of recurrent connections on hidden units  , though the argument sill favor of this opthm are beyond the scope of this l  ) a per . The model to be described here is a network of this type  , a version of the simple recurrent network due to Elman  ( 1990 )  . 
Version 1
The Version 1 network is shown in Figure 1 . Each box represents a layer of connectionist processing units and each arrow a cmn plete set of weighted connections between two layers  . The network operates as follows . A sequence of l ) hanes is presented to the input layer one at a time  . Tl , at is , each tick of the network's chick represents the presentation of ~ tsingle phone  . Each l ) hone unit represents allhonetic fi ~ ature , and each word consists of a sequence of i ) hones l ) reeede ( l by a boundary " phone " made . up of 0 . 0 actiwtt lons . 
"1 root ~ ) inflection : :======================= ======= I"i l~ure  1: Network for Acquisition of Morphology ( Version 1 ) An input phone 1 ) atternsm , ds actiwt tion totile network's hidden layer . The 1 , idden layer also receives activation from the pattern that a pl  ) eared there on the l ) revious times tel )  . Thus each hidden unit is joined by a time-deh W connection to each other hidden unit  . It is the previous hidde , > layer pattern which represents the system'short -term memory  . Because the hi ( ldcn layer has access to this previous state , which in turn del ) ended on its state al . the time step before that , there is no absolute limit to the . length of the context stare (1 in the sho , ' t-term memory . At the 1) eginnlng of each word sequence , the . hidden layer is reinitialized to a pattern consisting of  0  . 0 activations . 
l :' in ally the output re , its are activated l ) y the hidden layer . There are three output layers . Onerep-rese . nt simply a copy of the current input l ) hone . 
Training the network to auto-associate its current input aids in learning the root and inflection identification task because it forces the network to learn to distinguish the individual phones at the hidden layer  , a prerequisite to using the shortterm memory effectively  . The second layer of output uv its rel ) resents the root " , neavlng " . For each root there is a single outlmt unit . Thus while there is no real semantics , the association between the in l ) ut phone sequence and the " meaning " is at least anarl  ) it rary inflection " meaning " . Again there is a unit for each separate inflection  . 
For each input phone , the network receives it target consisting of the correct phone  , root , and inflection outputs for the current word . The phone target is identicM to the input phone . The root and inflection targets , which are constant hroughout the presentation of a word  , are the patterns associated with the root and inflection for the input word  . 
The network is trained using the backpropagation learning algorithm  ( Rumelhart , II inton , & Williams ,  1986) , which adjusts the weights on all of the network's connections in such a way as to minimize the error  , that is , the difference between the network's outputs and the targets  . For each morphological rule , a separate network is trained on a subset of the possible combinations of root and inflection  . At various points during training , the network is tested on unfamiliar words , that is , novel combinations of roots and inflections . The performance of the network is the percentage of the test roots and inflections for which its output is correct at the end of each word sequence when it has enough information to identify both root and inflection  . A"correct " output is one which is ch > serto the appropriate target than to any of the others  . 
In all of the experiments reported on here , the stimuli presented to the network consisted of words in an artificial language  . The phoneme inventory of the language was made up  19 phones ( 24 h ) r the mutation rule , which nasalizes vowels ) . For each morphological rule , there were 30 roots , 15 each of CVC and CVC VC patterns of phones . Each word consisted of two morphemes , a root and a single " tense " inflection , marking the " l ) resent ' ' or " past " . Examples of each rule: ( 1 ) suffix : present-vibuni , pmst-vibuna ; (2) prefix : present-ivibun , past-avibun ; (3) infix : prescnt-vik bun , past-vinbun ; (4) circumfix : 1) rescnt-ivibuni , pmst-avibuna ; (5) mutation : prcsent-vibun , past-vib . Sn ; (6) deletion : prescnt-vibun , l ) ast-vibu ; (7) template : present-vaban , past-vbaan . 
For each morphological rule there were 60 ( 30 roots x2 inflections ) dilferent words . From these 40 were selected randomly as training words , and the remaining 20 were set a . side as test words . For each rule , ten separate networks , with different random initial weights , were trained for 150 epochs ( repetitions of all training patterns )  . Every 95 epochs , the performance of the network on the test patterns was assessed  . 
Figure 2 shows the performance of the Version I network on each rule  (  , as well as perfor , nance on Versiou 2 , to be described below ) . Note that chance perforrnance for the roots was . 033 and for the . iu-fiections . 5 sluce there were 30 roots and '2 . inflections . There are several things to notice in these results  . Except for root identification for the eircum-fix rule  , the network performs well above cllance . 
I Iowever , the results are still disappointing in many cases  . In particular , note the poor performance our oot identification for the prefix rule and inflection identification for the sufHx rule  . The 1 ) chavior is much poorer than we might expect from a child learning these relatively simple rules  . 
The problem , it turns out , is interference between the two tasks which the network is faced with  . On the one hand , it must pay attention to infornmtion which is rel cw tn to root identification  , on the other , to information relevant o inflection identification  . 
This means making use of the network'short-term memory in very different ways  . Consider the prefixing case , fl ) rexample . Here for inflection identification , the network need only pay " attention to the first phone and then remeln berit until the end of the sequence is reached  , ignoring all of the phones which appear in between  . For root identification , however , the network does best if it ignores the initial phone in the sequence antithen pays careful attention to each of the following phones  . 
hleally the network's lfidden layer would divide into modules  , one dedicated to root identification , the other to inflection identificatlon . This could happen if some of the recurrent hidden -unit weights and some of the weights on hidden-to -output connections went to  0  . tIowcver , ordinary backpropagation tends to implement sharing among hidden-layer units : each hidden -layer unit participates to some extent in activating all output units  . When the rearc conflicting output tasks , as in this ease , there are two sorts of possible consequences : either performance on both t~sks is mediocre  , or the simpler task comes to dominate the hidden layer  , yielding good perform a acconth at task and poor performance on the other  . In the Version 1 results shown in Figure 2 , we see both sorts of outcomes . 
What is apparently needed is modularity at the hidden-layer level  . One sort of modularity is hardwired into the network's architecture in Version  2 of the model , described in the next section . 
Version 2 \]\] ccause root and inflection i < lentitication make con-tlicting demands on the network'short -term memory  , it is predicted that performance will improve with scparate hid < lenlayers for the two tasks  . Various degrees of modolarity are possible in connectionist networks  ; the form implem cllted in Version 2 of the model is total modularity , coml ) let ely separate networks h > r the two tasks . This is shown in Figure 3 . There are now two hidden-layer mo<l-ules , each with recurrent connections only to milts within the same module and with connections to one of the two output identification layers of units  . 
(Both hidden layers connect o the autoassociative phone output layer  . ) The same stimuli were used in training and test -i - ~  0   . 6  .   .   .   .   . 
o.
z 0.4 - i im 0.2 . . . . . .
suffix PrefixInfix circumfix Delete Mutate Template ~ ~ Root  , V . 1 ~ N , , , ", ~ Root , V . 2 Chance .   .   .   . 
IE~Inflection , V . 1 ~ x ~' - ~ Inflection , V . 2 Chance .   .   .   .   .   .   .   . 
Figure 2: Performance on Test Words I " ollowing Training ( Network Versions 1 and 2 ) \[\[ t , root I inflection I ,  . !'\ [ phonei .   . ! Z Z Z Z Z Z Z Z Z Z Z Z . t : ~ Figure 3: Network for Acquisitiou of Merl > hology ( Version 2 ) ing the Version 2 network as the Version 1 network . 
Each Version 2 network had the same number of total hidden units as each Version  1 network ,  30 . 
Each hidden-layer module contained 15 units . Note that this means there are fewer connections in the Version  2 than the Version 1 networks , hwestiga-tions with networks with hidden layers of different sizes indicate that  , if anything , this should faw ) r the Version 1 networks . 
Figure 2 comp ~ tres results from the two versions following  150 epo ( ' hs of training . For all of the rule typcs , modularity improves pcr for nlance for both root and inilection identification  . Olwiously , hidden-layer modularity results indiminished inter-feren <: c between tile two output tasks  . Performance . 
is still far from perfect for some of the rule types  , but further iml > rovcment is l > ossible with optiniza-tion of the learning parameters  . 
TO WARDS ADAPTIVE

It is important o1 ) c clear on tile nature of the modularity being prol  ) osed here . As discussed al ) ove , I have ( lefit Le ( l the task of word recognition in such ~ tway that there is a builtin distinction between lexical : tadgrammatical " meanings " because these are localize diu separate ~  ) utl ) ut layers . T i t ( . ' modular architecture of Figure 3 extends this dist in ( : tiou into the domai of phonology . That is , the shape of word six rel ) resente ( liuternally ( on the hidden layer ) in terms of two distinct patterns , one for the root and one for the inflection , and the network " knows " this even before it is trained  , though of course it does not know how the root and intlec-tiens will  1  ) erealized in the language . 
A fit rt her concern arises when we consider what hapl  ) enswh cx ~ more than one grammatical c~t tegory is represented in tile words Acing recognized  , for example , aspect in addition to tense on verbs . Assuming the hidden-layer modules are almrt of tile innate makeul  ) of tile learning device , this n teans that it fixed number of given modtdes must be divided up among the separate outl  ) ut " tasks " which would have the capacity to figure out for itself how to distril  ) ute the modules it starts with among the various output tasks  ; I return to this possibility below . But it is , also informative to investigate what sort of a sharing arrangement achieves the best performance  . For example , given two modules and three output tasks , root identification and the identification of two separate inflections  , which of the three possible ways of sharing the modules achieves the best performance ? Two sets of experiments were conducted to investigate the optim M use of fixed modules by a network  , one designed to determine the best way of distr ibuting raodnles among output tasks when the number of modules does not match the number of output tasks and oned csign e < l to determine whether a network could assign the modules to the tasks itself  . In both sets of experiments , the stimuli were words composed of a stem a n < l two affixes  , either two suffixes , two prefixes , or one prefix and one suffix .   ( All of these possibilities occur in natu : ral languages  . ) The roots were the same ones used in the afl\ ] xation and deletion experiments already reported  . In the two-suffix ease , the first suffix was / a/or / i / , the second suffix/s/or / k / . Thus the h > ur forms for the root migon were migonik  , migo-nis , migonak , and migon as . In the two-prefix case t it (': l , retix cs were/s/or/k/alid/a/or/i /   . In the prefix---sufflx case , the prefix ,   , ' as/u/or / e / and the suffix lalor Ill . ' there , , , erein all case ~ two hidden-layer modules . The size of the nlodules was sltch that the root identilieation task had potentially  20 units and each of the inilection identification tasks potentially  3 units at its disposal ; the sum of the units in the two modules was always  26  . 
The results are only summarized here . The con-tiguration in which a single nm ( tule is shared by the two affix-identification tasks is consistently superior for pet brmance on root identification but only superior for affix identification in the two-sufflx case  . 
For the l ) refix-sullix case , the configuration i which one module is shared by root identification and suffix identification is clearly inferior to the other two configurations for performance on sn flix identifica:tion  . For the two-prefl x ciLs c , the configurations make little diff crcnce for performance on identification of either of the prefixes  . Note that the results for file two-prefix and two -suffix cases agree with those for the single -prefix and single-suffix cases respectively  ( Figure 2 )  . 
What the results for root identification make clear is that  , even though the affix identification tasks arc easily learned with only  3 units , when they are provided with more units ( 23 in these experi-mcnts )  , they will tend to " distribute " themseh , es over the available units . If this were not the case , performance on the competing , and more diffic nlt , task , root identification , wouhl be no better when it has 20 units to itself than when it shares 23 units with one of the other two tasks . 
We conclude that the division of labor into separate root and inflection identification modules works best  , I ) rimarily because it reduces interference with root identification  , but also for the two-suffix ease , and to a lesser extent for the prefix-suffix case  , because it improves performance on affix identification  . If one distribution of the awtil-able modules is more efficient than the others  , we wouhllike the network to be able to find this dis-tribution on its own  . Otherwise it wouhl have to be wired into the system from the start  , and this wouhl require knowing that the different inflection tasks belong to the same category  . Somc form of adaptive use of the awtilable modules seems cM led for  . 
Given a system with a fixed set of modules but no wired-in constraints on how they are used to solve the wtrious output t~sks  , can a network organize itself in such a way that it uses the modules efficiently ? There has been considerable interest in the last few years in architectures which are endowed with modularity and learn to use the modularity to solve tasks which call for it  . The architecture described by Jaeobs , Jordan , & 13arto (1991) is an example . In this approach there are connections from each modular hidden layer to all of the output units  . In addition there are one or more gating networks whose function is to modulate the input to the on tpnt units from the hidden-layer modules  . 
In the version of the architecture which is appropriate for domain such as the current one  , there is a single gating unit responsible for the set of connections from each hidden nmdule to each output task gronl  )  . The outl ) uts of the modules are weighted by the outl ) uts of the corresponding gating units to give the output of the entire system  . The whole network is trained using backl ) ropagation . For each of the niodules , the error is weighted by the vahle of the gating input as it is l > assed back to the modules  . 
Thus each niodule adjusl ; sits weights in such a way that the difference , between the system's output and the desired target is min in lized  , and the extent to which an io < ht le's weights are change < l < leiden < Is on its contribution to the outl  ) ut . For the gating networks , the error function implcments coml ) etition among the modules for each output task group  . 
For our purposes , two further augmentations are required . First , we are dealing with recurrent networks , so we permit each of the modular hidden layers to see its own previous values in ad  ( lition to the current input , but not the l ) revious values of the hidden layers of the other modules  . Second , we are interested not only in competition among the modules for the output groups  , but also incoml ) e-tition among the outpnt groups for the modules . 
In particular , we would like to preven the network from assigning a single module to all output tasks  . 

To achieve this , the error function is modified so that error is mi  , fimized , all else l ) eing equal , when the total of the outputs of all gating units dedicated to a single module is neither close to  0  . 0 nor close to the total number of output groups . 
Figure 4 shows the arct fitccture for the situation in wl fich there is only one intlection to be l carne  ( l .   ( The autoassociative phone output layer is not shown  . ) The connections ending in circles symbolize the emnpc fition between sets of gating units which is built into the error function for the network  . Note that the gating units have no input connections  . These units have only to learn a bias , which , once tile system is stable , h . ' ads to a relatively constant outlmt . The ~ ussumption is that , since we are dealing with a spatial crosstalkl ) roblem , the way in which l ) articular modules are assigned to particular tausks shonld not wny with the in l  ) ut to then ctwm'k . 
~- 0~ .   .   .   hidden2 /~--~~ hiddent "6 o ' E Figure 4: Adal ) tive Modular Architecture fro " Morphology Acquisition An initial experiment demonstrated that the adaptive modular network consistently assign e  ( 1 separate modules to the output tasks when the , ' e were two modules and two tasks ( identification of the root and a single intlection  )  . 
Next a set of experiments tested wh c the . r the adaptive modular architecture wonhl assign two modules to three tasks  ( root and two intlections ) in the most efficient way for the two-suffix , two-prefix , and prefix-suffix cases . Recall that tile most efficient patteru of connectivity in all cases was the one in which one of the two modules was sl  , ared by the two affix identification tasks . 
Adaptive mmlular networks with two modules of 15 units each were trained on tile two-sufflx , two-prefix , and prefix-suffix tasks described in the last section  . Following 120 epochs , the outputs of the six gating units fl ) r the different modules were ex-anfined to determine how the modules were shared  . 
The results were completely negative ; the three possible ways of assigning the modules to the three identilication tasks occurred with approximately equal frequency  . The prolfiem was that the inflection identilication tasks were sonm cheasier than the root identilicatlon task that they claimed the two modules for thems clvesearly on  , while neither module was strongly prefc , ' red by the root task . 
q_'hus as often as not , the two inflection sended up assigned to dil\[ ' erent modules  . To compensate for this , then , is it reasonable to give root identification some sort of advantage over i  , dlection identiti-cation ? It is wellknown that children begin to acquire lexlcal morphemes before they acquire grammatical morphemes  . Among the reasons for this isll robably the more abstract nature of the ln can-ings of the grammatical morphemes  . In terms of the network's tasks , this relative difficulty wouhl translate into an inability to know what the inllee-tion targets would I  ) efl > r particular in lmt patterns . 
Thus we couh \[ m <> delit by ( lelayi , lg training on the inltection identification task . 
The exl ) eriment with the adaptive modular networks was repeated  , this tix new it l , the fl ) llowing training regimen . Entire words ( consisting of root and two at \[ ixes ) were i ) resented throughout training , lint for the first 80 epochs , the network saw targets for only the root identification task  . That is , the connections into the output units for the two inilcctions were not altered during this plume  . I -' of lowing the 80 the poch , by which time the network was well on its way to learning the roots  , training on the inllections was introduced . This procedure was followed for the . two-sulfix , two q ) retix , and prelix-sul\[ix tasks ; 20 sel ) : tratenet works were trained for each type . For the two-sutlix task , in all cases the network organized itself in the p  , ' cdicted way . That is , for all 20 networks one of the mod-nh . 's was associated mainly with the two intlectio , , output units and the other associatcd with the root output units  . In the preil x-suflix case , however , the results were more equivocal . Only 12 out of 20 of the networks organized themselves in such a way that tile two intlect i  ( m tasks were shared by one module , while in the 8 other cases , one module w~s shared by the root and pretix identitication t~sks  . 
Finally , in the two-pretlx case , all of the networks organized themselves ill Sllchav  , 'ay that the root and the first pretix shared a module rather than in the apllarently more eillcient contlguration  . 
The ditt'erence is not surprising when we consider the nature of the advantage of the configuratio it shared  1  ) 3 , one module . For all three types of affixes , roots are identified better with this configuration  . But this will have little effect on the way the network organizes itself be canse  , following the 80th epoch when competition among tile three output tasks is introduced  , one or the other of tile modules will already be firmly linked to the root output layer  . At this point , the outcome will depend mainly on the competition between tlle two inflection identification t  , ' ~ sks for the two modules , the one already claimed for root identification and the one which is still unused  . Thus we can expect this training regiment o settle on tl  , ebest configuration only when it makes a significant ditference for inflection  , as opposed to root , identification . Since this difference was greater fortile two -suflix words than for the prefix-sufl\]x words and virtually nonexistent for the two-prefix words  , there is the . greatest preference in the two-suffix case for tile configuration in which the two inflection tasks are shared by a single module  . It is also of interest hat fortile prefix-suffix cruse  , tile network never chose to share one module between the root and the suffix  ; this is easily the least efficient of the three configurations from the perspeet lve of inflection klentlficatlon  . 
Thus we are left with only a partial so httion to tlle problem of how the modular architecture might arise in the first place  . For circumstances in which the different sorts of modularity impinge on inflection identification  , the adaptive api ) roach can find the right configuration . When it is performance on root identification that makes the difference  , however , this api ) roach has nothing to offer . Future work will also have to address what happens when there are more than two modules and /or more than two intloint lections in a word  . 

Early work applying connectim fist networks to high -level cognitive tasks often seemed based on the assumption that a single network wouhll  ) e al ) le to handle a wide range of phenomena . Increasingly , however , the emphasis is moving in the direction of special-l  ) urpose modules for subtasks which maye on tlict with each other if handled by the same hardware  ( aacobs et al ,  1991) . These apl ) roaches bringe on nectionist models somewhat more in line with tile symbolic models which they seek to replace  . In this paper I have shown how tile ability of simple recurrent networks to extract " structure in time "  ( Ehnan ,  1990 ) is enhanced by builtin modularity which I ) ermits the recurrent hidden-unit connections to develop in ways which are suitable for the root and inflection identification tasks  . Not ( . , that this modularity does not amount o endowing the network with the distlnctiml  1  ) etween root and affix because both modules take the entire sequence of phones as input  , and the modularity is the same when tile rule being learned is one for which there are  11o affixes at all ( mutation , for examph !) . 
Modular approaches , whether symbolic or connectionist , inevitably raise fllrther questions , however . The modularity in the prewired version of MCNAM , which is reminiscent of the traditional separation of lexical and grammatical knowledge in linguistic models  , assumes that the division of " semantic " outlmt units into lexical and grammatical categories has already l  ) een made . The adaptive version partially addresse stills shortcoming  , lint it is only etfective in cases where modularity  1  ) cue-fits inflection identification . Furthermore , it is still based on the assumption that the output is divided initially into groups rel  ) resenting separate competing tasks . I am currently experimenting with related a ( lal ) tive approaches , as well asine thods involving weigl , t decay and weight pruning , which treat each output unit as a separate task . 
References
El , nan , J .  (1990) . Fiudi , , g structure in time . Cognitive Science , 14, 179-211 . 
Gasser , M .  (199,1) . Acquiring receptive morphology : a counectionist model  . Annual Meeting of the Association for Computational Linguistics  ,  32 . 
Jaeobs , R . A . , Jordan , M . I . , & Barto , A . G .  (1991) . 
Task decomposition through competition in at modular conneetionist architecture : the what and where vision tasks  . Cognitive Science , 15, 219-250 . 
MacWhinney , B . ~Leinbach , J .  (1991) . hnplemen-tatkms are not conceptualization : revising the verb learning model  . Cognition , 40, 121-157 . 
Marslen-Wilson , W . D . & Tyler , L . K .  (1980) . The temporal structure of Sl ) oken language understanding . Cognition , 8, 1--71 . 
Plunkett , K . & Marchman , V .  (1991) . U-shaped learning and frequency effects in at multilayered  1  ) erc ( ' ptron:implications for chihl language acquisition  . Cognition , 38, 1-60 . 
Port , R .  (1990) . Representation and recognition of teml ) or al1 ) atterns . Connection Science , 2, 151-176 . 
Rumelhart , D . E . & McCMland , a . L . (198G ) . On learning the past tense of English verbs . Iu McClelhmd, . l . L . ~ Rumelhart , D . E . ( Eds . ), Parallel Distributed Processing , Voh Lme2, pp . 
216-271. MIT Press , Cambridge , MA.
Rumelhart , D . E . , II inton , G . , , ~ Williams , R . 
(1986) . Learning internal representations by error propagation  . In Rmnelhart , D . E . & Me-Clell and , a . L . ( Eds . ), Parallel Distributed Processing , Volume 1, pp .  318-304 . MIT Press,
Cambridge , MA.

