Automatic Optimization of Dialogue Management
Diane J . Litman , Michael S . Kearns , Satinder Singh , Marilyn A . Walker
AT&T Labs-Research
180 Park Avenue
Florham Park , NJ 07932 USA
diane , mkearns , baveja , walker@research . art . corn

Designing the dialogue strategy of a spoken dialogue system involves many nontrivial choices  . This paper I ) resents are inforcement learning approach for automatically optimizing a dialogue strategy that addresses the technical challenges in applying reinforcement learning to a working dialogue system with huln an users  . \? e then show that our approach measurably improves performance in an experimental system  . 
1 Introduction
Recent advances in spoken language understanding have made it  1  ) ossible to develop dialogue systems tbr many applications  . The role of the dialogue manager in such systems is to interact in a natur Mw~y to hel  1   ) the user complete the tasks that the system is designed to support  . Tyl)ically , an expert designs a dialogue manager by hand , and has to make m~nynon trivial design choices that can seriously impact system performance  . This paper applies reint brce-ment learning ( RL ) to automatically learn design choices that optimize system pertbrnmnee for a cho-se LL pertbrmance measure  ( Levin et al , 2000; Walker et al ,  1998) . 
Consider the spoken dialogue system named NJ-Fun , wl fich we built to help users find fun places to go in New Jersey  . A sample dialogue with NJ-Fun is shown in Figure  1  , with system utterances labeled Si and user utterances labeled Ui  . When NJF ungreets the user , it can provide an open greeting such as utterance  S1  , expecting that the user will take the initiative . Alternatively , it can provide a directive greeting in which the system takes the initiative  , as in " Welcome to NJF un . Please say an activity name or say ' list activities ' for a list of activities I know about  . " After tile user's response U1 , NJF unmust decide whether it should explicitly confirm its understanding  , as in utterances $2 and $3 . NJF uncan also simply continue on with the dialogue  , as when it does not explicitly confirm that the user wants to find out M  ) outwineries . In NJFun , as shown in more detail below , decisions about initiative and confirmation strategies alone result in a SI : Welcome to N  . llSm . How may I help you ? UI : I'd like to find mn wine tasting in Lambert ville in the morning  .   ( ASR output : I'd like to find out wirterics the in the Lambert vill c in the morn in  . q .   )   $2: Did you say you are interested in Lambertville ?
U2: Yes.
$3: Did you say you want to go in the morning ?
U3: Yes.
$ 4: I found a winery near l ~ ambert ville that is open in the morning  , it is \[ . . . \] Please givelne feedback by saying ' good ' , ~ so-so ' , or ' bad ' . 
U4: Good.
Figure 1: An example dialogue with NJFun.
search space of 242 potential globnldialogue strategies . Furthermore , the performance of a dialogue strategy depends on many other factors  , such as the user population , the robustness of the automatic speech recognizer  ( ASR )  , and task difficulty ( Kamm et al , 1998; Dan Mi and Gerbino ,  1995) . 
In the main , previous research has treated the specification of the dialogue managements rategy as an iterative design problem : several versions of a system are created  , dialogue corpora are collected with human users interacting with different versions of tile system  , a number of evaluation metrics are collected ibr each dialogue  , and the different versions are statistically compared  ( Danieli and Gerbino , 1995; Sanderman et al ,  1998) . Due to the costs of experimentation , only a few global strategies are typically explored in any one experiment  . 
However , recent work has suggested that dialogue strategy can be designed using tile formalism of Markov decision processes  ( MDPs ) and the algorithms of RL ( Biermann and Long , 1996; Levin et al . , 2000; Walker et nl . , 1998; Singh et al , 1999) . 
More specifically , the MDP formalism suggests a method for optimizing dialogue strategies from sample dialogue data  . The main advantage of this approach is the 1 ) otential tbr computing an optilnal dialogue strategy within a much larger search space  , using a relatively small n m n ber of training dialogues  . 
This paper presents an application of RL to the the NJF nn system  , and exl ) erimentally demonstrates the utility of the ~ l ) proach . Section 2 exl ) lahls how we apply RL to dialogue systems , then Se('tion3 describes t . he NJF unsystem in detail . Section 4 deescribes how NJF unoptimizes its dialogue strategy from experimentally obtained dialogue data  . Section 5 reports results from testing the learned strategy demonstrating that our al  ) l ) roach improves task coml ) letion rates ( our chosen measure for 1 ) erfor-mance optimization )  . Aconll ) all iOll paper provides only anal ) brevi~tted system and dialogue manager description  , but includes additional results not presented here  ( Singh et al ,  2000) , such as analysis establishing the veracity of the MDP we learn  , and comparisons of our learned strategy to strategies handpicked by dialoguexperts  . 
2 Reinforcement Learning for

Due to space limitations , we 1 ) resent only a 1 ) rief overview of howdi~dogue strategy optimization can be viewed as an llL  1  ) roblem ; for more details , see Singhctal . (\]999), Walkerel ; a . 1 . (\]998), Levin et al (2000) . A dialogue strategy is a map l ) ing h'o mase to t ! states ( which summarize the entire dialogue so far ) to a set of actions ( such as the system's utter-mines and database queries  )  . There are nmltil ) l ( ~ reasonable action choices in each state ; tyl ) ically these choices are made by the system designer  . Our RL-I ) ased at ) l ) roach is to build a system that explores these choices in a systematic way through experiments with rel  ) resentative human us ( ! rs . A scalari ) erf()rmane ellleasllre , called are wal'd , ist h(m ( ; al-eulated for each Cxl ) erimental di Mogue .   ( We discuss various choices for this reward measure later  , but in our experiments only terminal dialogue states ha  , renon zerorewi-l , rds , slid the reward lneasul ' ( sarc quantities directly obtMnable from the experimental setup  , such as user satisfaction or task coml ) letion .   ) This experimental data is used to construct an MDP which models the users ' intera  ( : tion with the system . 
Thel ) roblem of learning the best dialogue strategy from data is thus reduced to computing the optimal policy tbr choosing actions in an MDP-that is  , the system's goal is to take actions so as to maximize expected reward  . The comput ~ t tion of the ol ) timal policy given the MDP can be done etficiently using stan & trdRL algorithms  . 
How do we build the desired MDP from sample dialogues ? Following Singh et al  ( 1999 )  , we can view a dialogue as a trajectory in the chosen state space determined by the system actions and user resl  ) onses : S1-~al , rl'5'2--a ~ , rs83"-~aa , ra "'" Heresi-% , , ,  . ~ si + l indicates that at the i the x change , the system was in state si , executed action ai , received reward ri , and then the state changed to si+~ . Dialogue sequences obtained froln training data can be used to empirically estimate the transition probabilities P  (  . s " la ' , a )   ( denoting the probability of ammsition to states  '  , given that the system was in state . s and took ; ration a) , and the reward function R ( . s , ( t ) . The estilnated transition 1 ) tel ) a bil-ities and rewi~rdflmction constitute an MDP model of the nser population's interaction with the system  . 
Given this MDP , the exl ) ected cumnlative reward ( or Q-value ) Q ( s , a ) of taking action a from states can be calculated in terms of the Q-w dues of successor states via the following recursive quation : Q  (  . %=,) + r(,,'l , ) n , a : , : Q(,,",,') . 
t ; !
These Q-values can be estimated to within a desired threshold using the standard RL value iteration algorithm  ( Sutton ,  1 . 991 . ) , which iteratively updates the estimate of Q(s , a ) based on the current Q-vahms of neighboring states  . Once value iteration is con > pleted , the optima \] diah ) gue strategy ( according to our estimated model ) is obtained by selecting the action with the maximum Q-value at each dia  . logue state . 
While this apl ) roach is theoretically appealing , the cost of obtaining sample human dialogues makes it crucial to limit the size of the state space  , to minimize data sparsity problems , while retaining enough information in the state to learn an accurate model  . 
Our approad ~ is to work directly in a minimal but carefully designed stat  ; espace ( Singh et al ,  1999) . 
The contribution of this paper is to eml ) irically vMi ( tate a practical methodology for using IlL to build a dialogue system that ol  ) timizes its behavior from dialogue data . Our methodology involves 1 ) representing a dialogue strategy as a map l ) i l ~ g fron leach state in the chosen state space S to a set of dialogue actions  ,  2 ) deploying an initial trah > ing system that generates exploratory training data with respect oS  ,  3 ) e on strncting an MDP model from the obtained training data  ,  4 ) using value iteration to learn the optimal dialogue strategy in the learned MDP  , and 4 ) redeploying the system using the learned state/ ~ mtion real  ) ping . The next section details the use of this methodology to design the 
NJF unsystem.
3 The NJFun System
NJF nn is a realtime spoken dialogue system that provides users with intbrmation about things to do in New Jersey  . NJF unisbuilt using a general purpose 1 ) latt ' or nlt br spoken dialogue systems ( Levin et al ,  1 . 999) , with support tbr modules tbratt to-rustic speech recognition  ( ASI/ . ), spoken language Greets Welcome to NJI qm . Please say an activity name or say ' list activities ' for a list of activities I know about  . 
Greet U\Vel come to NdP un . How may I help you ? ReAskl SI know about amusement parks  , a quariums , cruises , historic sites , museums , parks , theaters , wineries , and zoos . Please say an activity name from this list . 
ReAskl MP lease tell me the activity type . You can also tell me the location and time . 
A sk2S Please say the name of the townor city that you are interested in  . 
Ask 2 UP lease give me more information.
R eAsk2S Pleaset elime the name of the townor city that you are interested in  . 
R eAsk2M Please tell me the location that you are interested in  . You call also tell me the time . 
Figure 2: Sample initiative strategy choices.
understanding , text-to-speech ( TTS ) , database access , and dialogue management . NJF nn uses a speech recognizer with stochastic language and understanding models trained from example user utterances  , and a TTS system based on concatenative diphone synthesis  . Its database is populated from the nj . online web page to contain information about activities  . NJF unindexes this database using three attributes : activity type  , location , and time of day ( which can assume values morning , afternoon , or evening ) . 
hffornmlly , the NJF undialogue manager sequentially queries the user regarding the activity  , location and time attributes , respectively . NJF unfirst asks the user for the current attribute  ( and 1 ) ossibly the other attributes , depending on the initiative ) . 
If the current attribute's value is not obtained , N . J-Funasks for the attrilm te ( and possibly the later attributes ) again . If NJFun still does not obtain a value , NJF unmoves onto the next attribute(s) . 
Whenever NJF unsuccessihlly obtains a value , it can confirm the vMue , or move on to the next at-tribute(s ) . When NJF unhasfinished acquiring attributes , it queries the database ( using a wildcard for each unobtained attribute value  )  . The length of NJF undialogues ranges from 1 to 12 user utterances before the database query . Although the NJF undialogues are fairly short ( since NJF unonly asks for an attribute twice )  , the information access part of the dialogue is similar to more complex tasks  . 
As discussed above , our methodology for using RL to optimize dialogue strategy requires that all poten-timactions tbr each state be specified  . Note that at some states it is easy for a human to make the correct action choice  . We made obvious dialogue strategy choices in advance  , and used lear Ifing only to optimize the difficult choices  ( Walker et al ,  1998) . 
Ill NJ Fun , we restricted the action choices to 1 ) the type of initiative to use when asking or reasking for an attribute  , and 2 ) whether to confirm an attribute value once obtained  . The optimal actions may vary with dialogue state , and are subject o active debate in the literature . 
Tile examples in Figure 2 show that NJF uncan ask the user about the first 2 attributes I using three types of initiative , based on the combination of tile wording of the system prompt  ( open versus directive )  , and the type of grammar NJF unuses during ASR ( restrictiv cversus nonrestrictive )  . If NJF unuses an open question with m ~ unrestricted grammar  , it is using v . scr initiative ( e . g . , Greet\[l ) . If NJ-Fun instead uses a directive prompt with a restricted grammar  , the system is using syst cm initiative ( e . g . , Greet S) . If NJF unuses a directive question with a nonrestrictive granmlar  , it is using mizcd initiative , because it allows the user to take the initiative by supplying extraint brnlation  ( e . g . , ReAskl M) . 
NJFun can also vary the strategy used to confirm each attribute  . If NJF unasks the user to explicitly verify an attribute  , it is using czplicit confirmation ( e . g . , ExpConf2 for the location , exemplified by $2 in Figure 1) . If NJFun does not generate any COlt-firnmtion prompt  , it is using no confirmation ( the
No Confaction).
Solely tbr the purposes of controlling its operation  ( as opposed to the le~trning , which we discuss in a moment ) , NJN minternally maintains an opcratio'ns vector of  14 variables .   2 variables track whether the system has greeted the user  , and which attribute the system is currently attempting to obtain  . For each of the 3 attributes ,   4 variables track whether the system has obtained the attribute's value  , the systent's confidence in the value ( if obtained )  , the number of times the system has asked the user about the attribute  , and the type of ASR grammar most recently used to ask for the attribute  . 
The formal state space Smaintained by NJF unfor tile purposes of learning is nmch silnt  ) ler than the operations vector , due to the data sparsity concerns already discussed  . The dialogue state space $ contains only 7 variables , as summarized in Fig-sire 3 . S is computed from the operations vector using a hand-designed algorithm  . The " greet " variable 1 " Greet " is equivalent to asking for the first attribute  . NJ-Funal ways uses system initiative for the third attribute  , because at that point the user can only provide the time of  ( lay . 
504 greet attrconfvaltimesgramhist\]0 , 1 1 , 2 , 3 , 4 0 , 1 , 2 , 3 , 4 0 , 1 0 , 1 , 2 0 , 1 0 , 1 \]
Figure 3: State features and vahles.
tracks whether tile system has greeted tile user or not  ( no = 0 , yes = l) . " Attr ~: specifies which attrihute NJF unis ( ' urrently ~ tttelnpting to obtain or verify ( activity = l , location = 2 , time = a , done with at-tributes = 4) . " Conf " tel ) resents the confidence that NaFun has after obtaining aw due for an attribute  . 
The values 0 ,  1 , and 2 represent the lowest , middle and highest ASR confidence vahms ? The wdues  3 and 4 are set when ASR hears " yes " or " no " after a confirmation question  . " Val " tracks whether NaFun has obtained a value  , for tile attribute ( no = 0 , yes = l) . 
" Times " tracks the number of times that N , lFun has ask e(1 the user ~ d ) outhe attribute .  " ( 4 ram " tracks the type of grammar mostree ( mtly used to obtain the attribute ( 0= non-restrictive , 1= restrictive ) . Finally , " hist " ( history ) represents whether Nalflm had troullle understanding the user ill the earlier p~tr t of the conversation  ( bad = 0 , good = l) . We omit the full detinition , but as a , nex ~ unl > le , when N . lFun is working on the secon ( 1 attribute ( location )  , the history variable is set to 0 if NJF undoes not have an activity , has an activity but has no confidence in the value  , or needed two queries to obtain the activity . 
As mentioned above , the goal is to design a small state space that makes enough critical dist in  ( ' tions to suPi ) or t learning . The use of 6" redu ( : esthemmfl ) er of states to only 62 , and SUl ) l ) or ts the constru ( ' tion of mt MI ) P model that is not sparse with respect o , g , even using limite (1trMning ( btta . : ~ Tit ( . ' states p~t ( ; e that we utilize here , although minimal , allows us to make initiative decisions based on the success of earlier ex  ( :ha . nges , and confirmation de (: isions based on ASR . confidence scores and graln mars . 
' . Phestate/~t ( ' tiol:real ) pingr ( -`l ) resenting NaFun's initial dialogue strategy EIC ( Explor : ttory for Initiative and Confirmation ) is in Figure 4 . Only the exploratory portion of the strategy is shown  , namely those states for which NaFun has an action choice  . 
~ klreach such state , we list tile two choices of actions available .   ( The action choices in bold fime are the ones eventually identified as el  ) ritual 1 ) y the learning process , an (1 are discussed in detail later . ) The EIC strategy chooses random , ly between these two ac-21" or each utterm me , the ASH . outfmt includes 11 o , only the recognized string , but also a Ilasso (: ia . ted acoustic (: on Jld ( mce score , iBased on data obtain tMdm'ing system deveJolmmnt  , we defined a mapl ) ing from raw confidence , values into 3 approximately equally potmlated p~rtitions . 
3 62 refers to those states that can actually occur in a dialogue  . \[<) r example ,   greet=0 is only possible in the initial dialogue state "0   1   0   0   0   0   0"  . Thus , all other states beginning with 0(e . g . "0I00I00") will never occur . 
g100 (  0 0 1 1 0  (  1 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 \] 0 1 1 2 1 0 0 0 1 1 2 1 0 1 0 1 1 4 0 0 0 0 \] 1 4  (  1 0 0
St ~ tte Action Choices acvt git 1   2   0   0   0   0   0   1   2   0   0   0   0   1   1   2   (  0 1  (  0 1 2 0  (  1 0 1 1 2 0 1 0  (  0 1 2 0 \] 0 0 1 1 2 0 1 0 1 0 1 2 0 1 0 1 1 1 2 1 1 0  ( 1 0 1 2 1 \] 0 0 1 1 2 1 1 0 1 0 \] 2 1 1 0 \] 1 1 2 2 1 0 0 0 1 2 2 \] 0 0 1 1 2 2 \] 0 1 0 \] 2 2 1 0 1 1 1 2 4 0 0  ( 1 0 1 2 4 0 0 0 1 1 2 4 0 1 0 0 1 2 4 0 1 0 1 1 3  ( ) 1  ( )  ( ) 0 \] 3 0 1 0 0 1 1 3  ( 1 1 0 1 0 1 3 \[ )  1 0 \] \] 1 .  3 1 1 0 0 0 1 3 1 1 0 0 1 1 3 1 1 .  0 1 0 1 3 1 1 0 \] 1 1 3 2 1 0 0 0 1 3 2 1 0 0 1 1 3 2 1 0 \] 0 \] 3 2 1 0 1 1


No Conf , Exp Confl
No Conf , Exp Colffl
No Cont , Exp Confl
No Conf , Exp Confl

No Conf , Exp Confl






No Conf , Exp Cont'2
No Conf , Exp ConP 2


No Coiff , Exp Coa F2
No Conf , Exp Conf2
No Conf , Exp Cont2


No Conf , Exp Conf2
No Conf , Exp Conf2
No Conf , ExpCon\[2




No Conf , ExpCon\[3
No Conf , Exl ) Conf3
No Conf , Exp Conf3

No Conf , Exp Cont ~
No Conf , Exp Conf3
No Conf , Exp Conf3

No Conf , Exp Coni ~ J
No Conf , Exp Conf3
No Colff , Exp Conf3
No Conf , Exp Conf3
Figure 4: Exploratory portion of EIC strategy.
tions in the indicated state , to maximiz exploration and minimize data sparseness when constructing our model  . Since there are 42 states with 2 choices each , there is n search space of 242 potential global dialogue strategies ; the goal of RL is to identify an apparently optimal strategy fl ' om this large search space  . Note that due to the randomization of the EIC strategy  , the prompts are designed to ensure the coherence of all possible action sequences  . 
Figure 5 illustrates how the dialogue strategy in Figure 4 generates the di Mogue in Figure 1  . Each row indicates the state that NJF unis in , the ac-gacvt gh 0100000 Greet U $1   0   1121000 No Conf 0   1221001   ExpConf2   $2   0   1   3   2   1   0   0   1   ExpConf3   $3   0   1400000 Tell $4   1 Figure 5: Generating the dialogue in Figure 1  . 
tion executed in this state , the corresponding turn in Figure 1 , and the reward received . The initial state represents that NaFun will first attemp to obtain attribute  1  . NJF unexecutes Greet U ( although as shown in Figure 4 , Greet S is also possible ) , generating the first utterance in Figure 1 . Alter the user's response , the next state represents that NJ-Fun has now greeted the user and obtained the activity value with high confidence  , by using a nonrestrictive grmnmar . NJF nn then chooses the No-Conf strategy , so it does not attempt to confirm the activity , which causes the state to change but no prompt to be generated  . The third state represents that NJF unis now working on the second attribute  ( location )  , that it already has this vahle with high confidence  ( location was obtained with activity after the user's first utterance  )  , and that the dialogue history is good . 4 This time NaFun chooses the ExpConf2 strategy , and confirms the attribute with the second NJF un utterance  , and the state changes again . The processing of time is similar to that of location  , which leads NJF unto the final state , where it performs the action " Tell " ( corresponding to querying the database , presenting the results to the user , and asking the user to provide a reward ) . Note that in NJF un , the reward is always 0 except at the terminal state , as shown in the last column of Figure 5 . 
4 Experimentally Optimizing a

We collected experimental dialogues for both training and testing our system  . To obtain training dialogues , we implemented NJF unusing the EIC dialogue strategy described in Section  3  . We used these dialogues to build an empirical MDP  , and then computed the optimal dialogue strategy in this MDP  ( as described in Section 2 )  . In this section we describe our experimental design and the learned dialogue strategy  . In the next section we present results from testing our learned strategy and show that it improves task completion rates  , the performance measure we chose to optimize . 
Experimental subjects were employees not associ-a , ted with the NJF unproject . There were 54   sub-4Recall that only the current attribute's features are ill the state  , lIowever , the operations vector contains information regarding previous attributes  . 
jects for training and 21 for testing . Subjects were distributed sotile training and testing pools were balanced for gender  , English as a first language , and expertise with spoken dialogue systems . 
During both training and testing , subjects carried out freeform conversations with NJF unto complete six application tasks  . For examl ) le , the task executed by the user in Figure 1 was : " You feel thirsty and want to do some wine tasting in the morning  . 
Are there any wineries ( ; lose by your house in Lam-bert ville ? " Subjects read task descriptions on a web page  , then called NJF un from their office phone . 
At the end of the task , NJF unasked for feedback on their experience ( e . g . , utterance $4 in Figure 1) . 
Users the nhung up the phone and filled out a user survey  ( Singh et al , 2000) on the web . 
The training phase of the experiment resulted in 311 complete dialogues ( not all subjects completed all tasks )  , for which NJF unlogged the sequence of states and the corresponding executed actions  . 
The number of samples perst~tte for the initi ~ fl ask choices are:  0   1   0   0   0   0   0   GreetS=IS5   GreetU=156   1   2   0   0   0   0   0   Ask2S=93   Ask2U=72   1   2   0   0   0   0   1   Ask2S=36   Ask2U=48 Such data illustrates that the random action choice strategy led to a fairly balanced action distribution per state  . Similarly , the small state space , and the fact that we only allowed 2 action choices per state , prevented a data sparseness problem . The first state in Figure 4 , the initial state for every dialogue , was the most frequently visited state ( with 311 visits )  . 
Only 8 states that occur near the end of a dialogue were visited less tlmn  10 times . 
The logged data was then used to construct he empirical MDP  . As we have mentioned , the measure we chose to opt infize is a binary reward flmction based on the strongest possible measure of task completion  , called Strong Comp , that takes on value 1 if NJF unqueries the database using exactly the attribute specified in the task description  , and 0 otherwise . Then we eoml ) uted the optimal dialogue strategy in this MDP using RL  ( cf . Section 2) . The action choices constituting the learned strategy are in boldface in Figure  4  . Note that no choice was fixed for several states , in ealfing that the Q-values were identical after value iteration  . Thus , even when using the learned strategy , NJF unstill sometimes chooses randomly between certain action pairs  . 
Intuitively , the learned strategy says that the optimal use of initiative is to begin with user initiative  , then back off to either mixed or system initiative when reasking for an attribute  . Note , however , that the specific baekoff method differs with attribute  ( e . g . , system initiative for attribute 1 , but gcner Mly mixed initiative for attribute 2) . With respect to confirmation , the optimal strategy is to however , the point where contirl nation becomes unnecessary difl'ers across attributes  ( e . g . , confidence level 2 for attribute 1 , but sometimes lower levels for attributes 2 and 3 )  , and alsodt ! txmds on other features of the state besides confidence  ( e . g . , grammar and history ) . This use ( if ASP , (: ontidence . by the dialogue strategy is more Sol ) hisli ( ' ated than previous al ) proaches , e . g . ( Niimi and Kot ) ayashi , 1996; Lit\]nan and Pan ,  2000) . N . lI , ' un ( ' an learn such line-grained distinctions lecause the el  ) ritual strategy is based on a e onll ) arisoi ) of 24 ~ lossible exlh ) ratory strategies . Both the initiative and confirmation re-suits sugge  . sl that the begim fing of the dialogue was the most problenm tie for N  . lli'un . Figure I ix an example dialogue using the Ol ) tilnal strategy . 
5 Experimentally Evaluating the

For the testing i )\] tase , NJFun was reiln plemented to use the learned strategy  . 2: ttestsul)je ( ; Is then performed the same 6 tasks used during training , re-sulling in 124 complete test dialogues . () he of our main resull ; s is that task completion its measured by Strong Com  11 increased front 52cX in training 1o   64% in testing ( p < . 06 ) ) There is also a signilicant in~twaction ( ! II ' ( ~c . t between strategy nnd task ( p < . 01) for Strong-Colnl ) . \]' revious work has suggested l ; hat novic ( ~ users l ) erform ( : Oml ) arably to eXl ) erts after only 2 tasks ( Kammetill . , \] 9!18) . Sill('e Ollr \] oarlltdsl . rat-egy was based on 6 tasks with each user , one ( ? xpla-nation of the interaction eft'cot is that the learn c  . d strategy is slightly optimized for expert users . ~lb explore this hyi ) othesis , we divided our corpus into diah ) gues with " novice " ( tasks \] and 2 ) and " expert " ( tasks 36 ) users . We fOltlt d that the learned strategy did in fact lc ' a  ( l to a large an ( 1 significant improvement in Strong Compt br (  ; Xl ) erts ( EIC = . d 6, learned == . 69, 11< . 001) , and a non-signilieant degradation for novices (1 , 31C= . 66, learned = . 55, 11< . 3) . 
An apparent limitation of these results is that EIC may not  1  ) e the best baseline strategy t br coral ) arisentoour learned strategy . A more standard alternative would be comparison to the very best hand-designed fixed strategy  . However , there is no it greement in the literature , nor amongs the authors , its to what the 1 ) est hand-designed strategy might have been . There is agreement , however , that the best strategy ix sensitive to lnally unknown and unmodeled factors : the a The  ( ' . xlm rimental design ( lescribed above Colmists of 2 factors : the within-ingroul ) f a ( : torsl~ntefly aim thel ) etween-groui ) sfacl ; or task . \, Ve 11812, ~1, l,WO-~,g~l,yD . llO . ly , qiS of variance ( ANOVA ) to comtmte wlmt lmrmain and int ( ! raction ( ! flk ! cts of strategy are statistically signitica nt  ( t ) < . 05) or indicative of a statistical trend ( p < . 101 . Maineffe . cts of strategy are task-in(lel ) endent , while interaction eIt ' ( ! cts involving strat ( % y are task-dependent . 

Strong Comp\VcakComp



EIC(n=:31110 . 52 1 . 75 2 . 50 0 . 18 1 . 3 . 38 v__l~eatned p(n = 124) 0 . 64 2 . 19  . 02 2 . 67  . 04 0 . 11  . d213 . 29  . 86 Table 1: Mainett'ects of dialogue strategy . 
user1) olmlation , the specitics of the , task , the 1) ar-ticular ASR used , etc . Furthernlore , \] P , IC was ( : are-fully designed so that the random choices it makes never results in tmun natural dialogue  . Finally , a companion paper ( Singh et al ,  2000 ) shows that the 1 ) erforntanee of the learned strategy is better tha llseveral " stmtdard " fixed strategies  ( such as always use system-initiative and no -confirmation  )  . 
Although many types of measures have been used to evaluate dialogue systems  ( e . g . , task success , dialogue quality , ettit : ie ney , usability ( l ) anieli and Gerbino , 1995; Kamm et al ,  11998)) , we optimized only t brone task success measure , Strong Conll ) . 
I lowever , we also examined the 1 ) erl ' or nmnee of the learned strategy using other ewduation measures  ( which t ) ossibly could have llo011 used its our reward function )  . Weak Comp is a relaxed version of task comt ) letion that gives partial credit : if all attribute values are either correct or wihh : ards  , the value is the sum of the correct munl ) er of attrilmt es . () tlmrwise , at least one attribute is wrong ( e . g . , the user says " Lanfl ) ertvilh f ' but the system hears " Morristown " )  , and the w due is -1 . ASR is a dialogue quality lllea-sure that it l ) l ) roxinmtes Sl ) eech recognition act : uracy for t l , edatM ) ase query , a . n d is computed 1:)3 , adding 1 for each correct attribute value alt d . 5 for every wiht ca . rd . Thus , if the task ix to go wine tasting near Lambert ville in the morning  , and the systenl queries the database for an activity in New Jersey in the morning  , Strong Comp = 0 , \ VeakComp = l , and ASR=2 . In addition to the objective measures discussed a , bove , we also CO mlmted two subjective usability measures  . Feedback is obtained front the dialogue ( e . g . $4 in Figure 5) , by mapping good , so-so , bad to 1 ,  0 , m~d-1 , respectively . User satisfaction ( User Sat , ranging front 020 ) is obtained by summing the answers of the webbased user survey  . 
Table I summarizes the diflhrence in performance of NJFunt brour original reward flmction and the above alternative valuation measures  , from trail > ing ( EIC ) to test ( learned strategy for Strong Comp )  . 
For WeakComp , the average reward increased from 1 . 75 to 2 . 19 ( p < 0 . 02) , while tbrASll the average reward increased from 2  . 5 to 2 . 67 ( p < 0 . 04) . Again , the seiml ) rovements occur even though the learned strategy was not optilnized for these measures  . 
The last two rows of the table show that for the cantly differ for the EIC and learned strategies  . Interestingly , the distributions of the subjective measures move to the middle from training to testing  , i . e . , test users reply to the survey using less extreme answers than training users  . Explaining the subjec-tire results is an area for future work  . 
6 Discussion
This paper presents a practical methodology for applying RL to optimizing dialogue strategies in spoken dialogue systems  , and shows empirically that the method improves performance over the EIC strategy in NJF un  . A companion paper ( Singh et al ,  2000 ) shows that the learned strategy is not only better than EIC  , but also better than other fixed choices proposed in the literature  . Our results demonstrate that the application of RL allows one to empirically optimize a system's dialogue strategy by searching through a much larger search space than can be explored with more tradition all nethods  ( i . e . empirically testing several versions of a systent )  . 
RL has been appled to dialogue systems in previous work  , but our approach ditlhrs from previous work in several respects  . Biermann and Long ( 1996 ) did not test RL in an implemented system , and the experiments of Levinet 31 . (2000) utilized a simulated user model . Walker et al ( 1998 ) 's methodology is similar to that used here , in testing RL with an imt ) lelnented system with human users . However that work only explored strategy choices at  13 states in the dialogue , which conceivably could have been explored with more traditional methods  ( ~ ts compared to the 42 choice states explored here )  . 
We also note that our learned strategy made dialogue decisions based on ASR confidence in conjunction with other features  , midal to varied initiative and confirmation decisions at a finer grain than previous work  ; as such , our learned strategy is not ; a standard strategy investigated in the dialogue sys-teln literature  . For example , we would not have predicted the complex and interesting backoff strategy with respect o initiative when reasking for an attribute  . 
To see how our method scales , we a real ) plying RL to dialogue systems for eustolner care and t br travel planning  , which are more complex task-oriented domains . As fllture work , we wish to understand the aforementioned results on the subjective reward measures  , explore the potential difference between optimizing t brex pert users and novices  , automate the choice of state space for dialogue systems  , il we s-tigate the use of a learned reward function  ( Walker et al ,  1998) , and explore the use of more informative nonterminal rewards  . 

The authors thank Fan Jiang for his substantial effort in implenmnting NJF un  , Wieland Eckert , Esther Levin , Roberto Pieraccini , and Mazin R . a hinl for their technical help , Julia Hirsehberg for her comments on a draft of this paper  , and David McAllester , I~i chard Sutton , Esther Levin and Roberto Pieraccini for hell ) tiff conversations . 

A . W . Biermann and P . M . Long .  1996 . The composition of messages in sl ) eeeh-graphies interactive systems . In Proe . of the International Symposium on Spoken Dialogue  , pages 97100 . 
M . Danieli and E . Gerbino .  1995 . Metrics for evaluating dialogue strategies in a spoken language system  . In P~vc . of the AAAI Spring Symposium on Empirical Methods in Discourse Interpretation and Generation  , pages 3439 . 
C . Kamm , D . Litman , and M . A . Walker .  1998 . From novice to expert : The effect of tutorials on user exl  ) er-tise with spoken dialogue systems . In P~vc . of the International Conference on Spolccn Language P~vccss-in  . q , ICSLP98 . 
E . Levin , R . Pieraccini , W . Eekere , G . DiFabbrizio , and S . Narayanan .  1999 . Spoken language dialogue : lh'om theory to practice  . In Pwc . IEEE Workshop on Automatic Speech R . ecognition and Understanding , AS-
RUU 99.
E . Levin , R . Pieraccini , and W . Eckert .  2000 . A stochastic model of humanmachine interaction for learning dialog strategies  . IEEE TTn nsactions on Speech and
Audio Processing , 8(1):11-23.
D . J . Litman and S . Pan .  2000 . Predicting and adapting to poor Sl ) eech recognition in a spoken dialogue sys-tern . In Proc . of the Scv ( ' ntccnth National Confl:rcn ccon Artificial In tcllig cncc  , AAAI-2000 . 
Y . Niimi and Y . Kobayashi .  1996 . A dialog control strategy based on the reliability of speech recognition  . In Proc . of the International Symposium on Spoken Dia -loguc  , pages 157--160 . 
A . Sanderman , J . Sturm , E . denOs , L . Boves , and A . Cremers .  1998 . Evaluation of the dutch traintime table inibrmation system developed in the arise project  . In Interactive Voice Technology for Tclccom -munications Applications  , IVT2'A , pages 91-96 . 
S . Singh , M . S . Kearns , D . J . Litman , and M . A . \ Valker . 
1999 . Reinforcement learning for spoken dialogue systems  . In Proc . NIPS 99 . 
S . B . Singh , M . S . Kearns , D . J . Litman , and M . A . Walker .  2000 . Empirical evaluation of a rein-forccment learning spoken dialogue system  . In Proc . 
of thcScv cnt ccnth National Conference on Artificial 
Intelligence , AAAI-2000.
R . S . Sutton .  1991 . Plamfing by incremental dynamic programming . In Proc . Ninth Confcwztccon Machine
Learning , pages 353-357.
M . A . Walker , J . C . Promer , and S . Narayanan .  1998 . 
Learning optimal dialogue strategies : A ease study of a Sl  ) oken dialogue agent bremail . In P~vc . of the 36th Annual Meeting of the Association of Computational Linguistics  , COLING//ACL98 , pages 13451352 . 

