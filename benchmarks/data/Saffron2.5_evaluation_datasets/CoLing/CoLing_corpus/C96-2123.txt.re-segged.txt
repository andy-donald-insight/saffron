On the Structural Complexity of Natural Language Sentences 
Dekang Lin *
Artificial Int cllig cnccI , a , bovatory
Ma.ss ~ cclmsettsInsdtttt(; of:l~cttnology
lm7 (57, 545 Technology Square
( ~ atnbridgc , Massac.husetts , USA , 02\]239
E-maihlind ck ~* ai.mit.edu

The objective of this pal ) eristo\[brmal-ize the intuitional ) outl , he comph ; xity of syntactic structures . We propose a definition of structm : alCOml ) h'xity such that sentences ranked by our definition as more COml  ) h ; xaregen ( ; rally more diI'-ficultl brhumans to process . We justify the definition by showing how it is a hleto account for several seemingly unrelated phenomena in natural anguages  . 
1 Introduction
Intuitive \] y , certain syntactic structures arcu Lore difficult for htn nans to process thau others  . For example , compare the following to sentences : (1) a . ' Fhe cat that the dog that them an bought chased died  . 
b . The man bought the dog that chased the cat that died  . 
It is ohvious that sentence ( la . ) is much mor ( ' difficult to understarl d than ( 1b )  . Since the two sentences are of the same length an ( l involve the same set of semantic relationships , the ditliculty in rm-derstan ( ling ( 1a ) can only be attributed to its syntactic structure . 
'\[' he objecl ; ive of this pal ) er is to fortnalize the intuition a . bout the complexity of syutactics tru (> tures . We propose a detinition of structural colnI ) h~xil ; y(SC ) such thai ; sentences ranked by our definition as more complex are generally more difficult for humans to process than otherwise similar sentences  , hi other words , suppose a pair of sentences A arid B consist of the same set of words and have essentially the same meaning  , then sentence A is more difficult to process than sentence  1~ if SC ( A ) >SC ( B )  . For example , the proposed detinition of structural complexity correctly pre -* Onlea  , reDorathe University of Manitoba . , Win-nipeg , M~mitoba , (\] ~ tnmla . Thisrt, . se , Lrchas1)een supported by NS li ', II . (\] ltcsearch (', rantOG1) 121338 . The author is very gr~teful to the reviewers who pointed o at several mistakes in the draft  . 
dicts that ( la ) ix much more difficult to process than ( lb )  . 
' I'll ( : notiou of structural complexity proposed in this l  ) apc ' roilers explanations \[' or a set of seem -iugly unrelated phenomena : ? We will showdlat the definition of structural comph:xity explains why aI  ) utch sentence involving cross-serial dependencies is liglrdy easier to underst~md than a corresponding cenl  , er-embedded German sentence . 
? We will also show that extrapositions , uchasheavy-NP shift and PP extractions are motivated by reducing syntactic omplexity  . The extraposition of an element is only warranted when the  . structural CO ml ) lexity of the sen-
I.en (: e is reduced as a result.
? NPn to difiers of a head tend to be closer to the head than its PP modifiers  , which in turn tend to be closer than its CP ( clausal ) modi-tiers . In Generalized Phrase Strcuture Grammar ( (~ VS ( ~ )   ( Gazd ~ u " ctal .   ,  1985) , these linear order constraints are stated explicitly in the gral Illnar  . The notion of structured complexity provides an explanatory account  . 
' l ' here are several reasons why the notion of strHCtltl:a\]COml  ) lexity ix tlseful . Firstly , in natural language generation , a generator should get > era . rethes imphest sentence that conveys the intended meanings  . Structural complexity can be used to choose l ; he syntactic strnctures with l ; he lowest structural complexity so that the resulting sentence is easier to understand than other alternatives  . 
Secondly , structural complexity is also needed in assessing the readability of dommtents  . \[ t is well known that the length of a sentence is not  , are lit~ble indicator of its readability . Yet , the readability of texts has up to no wheen measured by tlJelengths of sentences and familiarities of th  ( : words in the documents . Using structural complexity instead of sentence length allows the read-~fl  ) ility of documents to be measured tnorc accurately . 
Finally , we propose , in Section 4 , that extrapo-sitions ~ rernotiw~ted by reduction of structural only allowed if the structural complexity of tile sentence is reduced as a result  . This constraint is nsefnl both in parsing sentences with extrapo-sitions and in deciding where to use extraposition during generation  . 
The notion of structural complexity is defined in Section  2  . We then justify the definition of structural complexity by demonstrating in Sections  3  ,  4 , and 5 that sentences with lower structural complexity are easier to understand than otherwise similar sentences with higher structural complexity  . 
2 Structural Complexity
The definition of structural complexity presumes the notion of dependency relationships between words in a sentence  . In dependency grammars ( Hudson , 1984; Mel'Suk ,  1987) , a dependency relationship is a primitive relationship between two words  , called the head and the modifier . In constituency grammars that contain the Xbar theory as a component  , dependency relationships between words are implicitly specified in Xbar structures  . The modifiers of a word w are the head words of the specifier  , complements , and adjuncts of w . For example , Figure 1 is the Xbar structure of (2) . The word " will " has two modifiers : the head word  ( ) fits NP specifier ( " K inf ' ) and the head word of its VP complement ( " bring " )  . The dependency relationships in timXbar structure in l ! ' igure  1 are shown in Figure 2  . Each directed link in Fignre 2 represents a dependency relationship with the direction going from the head to the modifier  . 
(2) Kim will bring the wine in the evening.

Kim will v bring
DET l'theNwine
PP in P//~
DETI'theN
Figure 1: Xbar structures of ( 2 ) evening Kim will bring the wine in the evening Figure  2: t ) ependency structure of ( 2 ) In order to recognize the structure of a sentence  , a parser must establish the dependency links between the words in the sentence  . Structural complexity measures howeasy or di\[\ [ icnlti is to establish these dependency links  . The definition of structural complexity is based on the assumption that the shorter dependency links are easier to es--tablish than longer ones  , where the length of a dependency link is one plns then mn be r of words between the head and the moditier  . I : or e . xample , the lengths of tile links in Figure 2 are shown by the numbers attached to the dependency links  . 
Definition 2 . 1  ( Struetural Complexity ) The slructural complexity of a dependency struc -lure is the total length of the dependency links in the structure  . 
For example the structural complexity of the dependency structure in Figure  2 is 11  . 
\[ n the next three sections , we will show that the definition of structural comph '  . xity does i , -deed retlect the difficulty in processing a sentence  . 
We will present examples in which sentences with lower structural complexities are easier to process than similar sentences with higher structural com-ph  ; xities . 
3 Center embedding
The difficulty in processing center embeddings en -ten  ( ' es:such as ( 13 )  , hgs been explained by its requirement on the size of tile stack in a pars b  , r . 
This explanation presumes that the human parser uses a pushdown stack to store the partially built constituents  . ' l'he notion of structural complexity provides an explanation of the difficulty of processing center embedding that makes much weaker commitment to the parsing model  . Figure 3 shows the lengths of the dependency links in a center-embedding sentence  ( la ) and a non-center-embedding sentence ( lb ) with similar semantics . The structural complexity of the center-embedding sentence is  30  , which is much higher than the structur M complexity  ( =112 ) of the non-center-embedding sentence . 
The presumption that human sentence processor uses a push downsl  ; ack is challenged by the contrast between cross -serial dependencies in Dutch  ( e . g . , Figure 4a ) and center-embedding sentences in German ( e . g . , Figm : e4b . ) Since the cross serial dependencies are much more ditficnlt to handle with pushdown stacks 
I"F7L rill1~1r-?l?-Iu\]
The cat that the dog that them an bought chased died Theman bought the dog that chased the cat that died Figure  3:   ( \] ent , er-l , \] lnlmdding vs . Not-(, elt . e . l-(hireth ; m their English couvd , erpm'l ; s"(p .  249) . 
' I'his is also consistent with the sl . ru(;1;ural(:()lll-Iflexity account , since the structur3l comtJexity of I : igurc 4c is 9  , whic . his signili('nntly lowert : hmiitsDutcha , ml (; c;rman connterl)iU'l , s(l"igure43 an(l4l )) i4 Extrapositions l , \]xl , ra , l ) osil:i on I'e\[~l'Sl , O i , helllOVetllell\[ , Of ~ Illeleinenl ;\[' roillil , s , ov'nm li ) ositio \] lt , oa I ) os ii , iotl3t , orl  le3 rI ihe end of l , tlese nl , ellC (' , l'\]xa\[I\]l ) l(tso\[extr~t . 
1) osition in I ", nglishin , :; lu(h<l!hnlw . ( hling Sentences .   .   .   .   .   .   . I hmvy-NP shift Deman ne hebben Hans depaar den lerenvoeren Theman have Hans the horsest each feeda  . 1) utch : cross serial dependency , shuctum l complexity = 13 Die Maenner haben Hans die Pferdefuettern gelehrt Themen have Hans the horses feedt each b  . Gennan : center embedding , structural complcxily=14 Thementaught Hans to feed the horses c . English : right branching , slmclural complexity=9I " igure 4: crosss( ; ri31 dependency vs . ceni . er (3) a .   . Iocsent\]12 (; I zook he found in I ) a . ris/ohispalb . Joesent : ohispal
I , he book lie found in I ) 3 r is
F , xtral)oS(~ . d relative (4) a . Am3nI ; h31; no one knew slood ' uph . Anr , mslood " uptihal \] I10 Olleklle W
I ~ P -( , , xtral ) osition (5) a . I Feaida . dcscril ) tion of llockncy'slal , csi,l > ic ( . ur(~!lcslcrdayb . 1 reada descril ) t , ionycsler day of IIo(kI(yshitestF , icl , urelgxl ; ra('l ; ion from AP (6) a . . \[ low cerl , a in that the , Me~s will wi , arc : qo'u'?h . II owcertaiua ' rc'!lO~ttlmt the Mets will wi  . ? Me ( ' hanislll for constraining exlira posiii iOll iSIll'-gently  , ee(h'xli , both parsing 3ndg~encr3l . ioll . '1'0 et ), lm(l(ling .   .   .   .   .   .   .   .   .   .   . vs . right . -i~ra . nchillg\[ihcI ) CStl of the ; ulthor's I ~ no whe(lge , noue of ( . I rathannested dependeucies , the hypothesis th3thu-m3n parsex uses a , pushdownsta , ' k would predict th 3 t the I ) utch sentence sttch as Figure 43 shot lld be much lilt ) reditli ( : ult ( IC ) underst3nd than the correslmnding (  . lerl lt3II set lLeil('es with IleSl ; ec\[(lepende~ncies ( Figure 4b ) . I to wever , da . t a from psycholinguistic exper in lenl ; suggest ( ; h3t the , ya , reinfact slightly easier to proce . ss than the corresponding ( ; erm3nsenl ; ent-cs with nested de , pen-dencics ( B3 chet31 . , 1986) . This obser w ~ , tion can be 3 ( ' counted\[or l ) ystructur 31 complexity , since thesl ; ru('tur31 comt)l cxity of the I ) ut , ch sentence ( Figure 4&) is 13 , which is slightly lower tha , nLhc structur 31 complexity ( =14 ) of the correspollding ( ~ ernl3IIs enl . (: tlceI:'igurc 411 . It was 31 soel ) served in ( Bachetel . , 1! ) 86 ) that " For someone with ( weu 3 limited competence in English 3nd either of the other langu3ges   , the p3t terns in l ) ut ch and Ger-m3n seem to be more difficult to process 3nd pro-I ) road coverage parse . rs or ~ ener3 l , orsh3\[IdIesex-~P\[~\[~Pt )   ( $1 ~ I ( )\] ~ 1 ~ \] ~ I I~rincil4e . dfashio h . The reason (' OF this iS that exlii'a , i)osil ; iOllS3Ill ) eart O be dependent upon ccrt3 in as t ) ects of ( : on t cx ts thit3 renot cN ) l ; ured by usual synt~wtic fe3t ; ures . For exam-t > le , compare the following 1) 3 Jr of sentences (7) a , I ( , alked wi(;ha , m3n yesh : rda , ! l with a must 3cheb . * ll , alked with a , ma , none year and fo'~ur'monihsago wiiJia tnjhs bw  . hq The syuta , cticstruct , , , ' es of (7a ) 3 , , , I(7B ) are th , , s3me , which is shown in I : igure 5 , except I ih a , \[ it ) he3dwa:bial phrase Advl ) is " yesterday " in ( 7a ) a , ml " one year a ,  . dfour months 3go " in (71)) . Although the two adw ; rt ) i31 phra , sesm'c two different stri . gs , ILiheya , reidentical in their syntactic (~/ . lltll ' ~ sIYet . , extr31) osition is good in (73) but b3d in (7b ) . 
We propose ~ th 3 t the lm rpos cof extr 31 ) osition is to m3ke   3 sentence easier to mlderst3nd   . There-'ore , ext ,   r3posil&m is only allowed when the structural comph~xity of l  ; heS(Hll ; ellCe is reduced 3s are-still ;  . Note 1 , 131 , reduction of structura Jc()mt Jexity is not l , he only const , r3 into n cx t r 3 position . '\[' here
NP ~ ~__ l PP
VPP with mustache talked to a man
Figure 5: Parse tree of ( 7a ) and ( 7b ) are also syntactic constrain such as Right Roof Condition  ( Ross ,  1967 ) or Complement Principle ( Rochemont and Culicover ,  1990) . 
When a phrase is extraposed , the set of dependency relationships remains the same  . However , the lengths of some of the dependency links will change  . The structural complexity of the sentence may change as a result  . Figure 6 illustrates how extrapositions at lhct the lengths of dependency links is  ,  (3) ,  (4) ,  (5) , and (6) . Only the dependency links whose lengths are changed are shown there  . In all cases , structural complexity is reduced by the extraposition  . 
Consider the difference between (7a ) and (7b).
In (7a ) , the extra position of \[ pp with a mustache\] increases the length of the dependency link between " man " and " with " by  1  , but reduces the length of the dependency between " talked " and " yesterday " by  3  . Therefore , the structural complexity is reduced by 2 as a result of the extrapo-sit , on . In contrast , in ( Tb ) , the extra position of \[ pp with a mustache\] increase stile length of the dependency link between " man " and " with " by  6 and reduces the length of the dependency link between " talk " and " ago " by  3  . Thus the structural complexity is increased when \[ Pe with a mustache \] is extraposed  . 
The hypothesis that extraposition must reduce the structural complexity also explains why inheavy -NP shift  , the extraposed NP must be heavy , i . e . , consisting of many words . When the complement Nil ) of a verb is ' shifted ' to the right across an adjunct modifier of the verb  , the length of the dependency link from the verb totile head of the NP is increased by length the adjunct modifier  . On the other hand , the length of the dependency link fi'om the verb to the adjunct modifier is reduced by the length of the NP  . The retbre , the structural complexity of the sentence can only be reduced as a result of the extraposition when the NP is longer than the adjunct modifier  , Joesent the book he found in Paristo his pal Joes ent to his palt he book he found in Paris  ( a ) Heavy-NP shift , SC reduction = (7+2)-(5+1) = 3
Aman that no one knews tood up
A mans tood up that no one knew ( b ) Exuaposcd relative clause , SC reduction = (5 + l)-(3 + l ) = 2
I7~
I read a description of Hockney's latest picture yesterday I read a description yester day of Hockney's latest picture  ( c ) PP-extraposition , SC reduction = ( 7+l ) - ( 4+2 ) = 2 How certain that the Mats will w in are you How certain are you that the Mats will w in  ( d ) Extraction fl'om AP , SC reduction = ( 6+ 1  ) - ( 3+ 1 ) = 3 Figure 6: Extraposition must reduce structural complexity 5 Linear Precedence In most languages , the NP modifiers of a word tend to be < ; loser to the word than it , s P Prood , -tiers , which , in turn , tend to be closer to the word than its CP ( clansal ) modifiers . In GPSG ( Gazdar et al ,  1985) , these lineal : order constraints are stated explicitly as the linear precedence rules  . In this section , we show thai ; the linear precedence rules in GPSG can be derived fl ' om the assumption that the linear order among different types of modifying phrases  , such as NP , PP , and CP , should minimize the structural complexity so that the sentence is as easy to process as possible  . 
Suppose a word w has n modifiers XP:I,XP~, . . . , XP ,   ; the number of words in XPi is li ; mM the head word of XP i is w i , which is the pi ' th word in XP i . Without loss of generality , let us assume that w precedes its modifiers . \[f the order of the modifiers is XP1, XPu, . . . , XP , ; , then the length of the dependency link between w and the head of XPi is  ( Pi+2j-:tllj ) and the total length of dependency links within tile maximal projection -  , ~ \- , i-tlj \]) ~ , ~i = l(Pi-1-L , j = I /= : ( . :- l ) lt + ( , , . -U ) l + .   .   . + l . . . .  , + Among all \]) ert\]lltt ; al ; i of lsOfXPI , Nit 2 ,   . . . , Xl ; ' . . . .
t ; hcal > ovesumist it < ' + minitnal when 11  <_  12  <_  . . . < In . Inol , her words , the total h ' , ngt ; h of dCl ) cnden <: y links is minilnal when tit <'+ modifiers with I'ewer words are <: loser to the h  (  ; a d . ( Icnerally spealdng , PPs contain more wor<ls than NPs and Cl ) scon : rain more words than l . ) Ps . Therefor (' . , Nt ) mod-i\[iers shouhl b <; closer 1 , ot , he\]mad word t , han1 ) 1> moditiers and t ) l ) modifiers shoul < l be closer Lot ; h("head word t . han CI ) mo(li\[i(wsifl ; hcsl ; ru < : l , ural comph ' . xity of timma . x in mlpl : ojec@tu of the . \] mad word w is to be minimized . 
6 Discussion
We used the total length of the dci > endency links in the definition of structural complex il  , y . The examples l tresented in the previous sect , ions are also consistent w : it hadefinition l ; hat uses the in axi-mum length of structural links . The reasot L we choose to use the stnrt is that the definition naturally incorporate the length into consi<l cration  . 
' I'hc arguments presented in previouss ( ; <:l ; ions arc preliminary . Our riga , rework in chldc backing up th ( ' ~ hypothesis with (  ; mph'ical cvidc . n < : e and in-vest ; igate the application of struct tn'al compl cxil , y in handling extraposition in parsing and genera -lion  . 
7 Conclusion
We have proposed a notion of stru < ' tural cotuph ' , x-ity+A senten <' e with higher st , ructural cOral > It + x-ity is more dif\[icultI , oprocess than a similar set > tences with lower structural complexity  . Structural coruph '+ xity is needed in both l > arsing a  . nd general ; ion . \[ t can also be used to a . sscsstl , ereadability of < locument ; s . W < : supl > or tl , hcd('\[init ; ion of stru <: t , ural (' . omph ~ xity wil . hasel , of se <> mingly un-relate < phenomena : the contrast hcl  , w cencenter -- embedding and right-branching sentences  ,  (' . xt;ra-i > ositions , and the linear order among modifying l > hrases . \[ in all of these cases , sent cnc ( ; s with lower structural complexity arc easier to mtd crst and  . 
li < : lmr < lludson .  1984 . Word Crammar . Basill ~ lackw (; lll ) ublishers\[fruit ; cal . , Oxfor(1, England . 
Igor A . Mcl'Suk .  1987 . l ) cp c . ndency . synlax : theory and practice . Sl;al ; cUniv <; rsityo\['New York
Press , Albany.
Michael S . ILochcmont and P ('~ t(',rW . (\] ulicov cr . 
1990 . l ', ' nglish I " ocu , ~ Constructions a'n . d the Theory of Grammar . Cambl fidge Studies in Linguistics . Cambridge Univ crsit , y Press . 
J . I ( , ( ) s s .  1967 . Constraints on . variables in synla, . 
I'h.I ). thesis , M.\[.'I'., Canal>ridge , MA.

E . Bach , C . lrown , an <\[ W . Marslen-Wilson.
1986 . (5ossed and nestedd (; pemlencies in (, e~-ma . n and I ) utch : A psycholinguistic , stmdy . Language and Cognilivel > rocess cs , 1(4):249262 . 
"((+ Gerald Gtzlat , Ewan Klein , G ' , of lclyl'ulhm:h and Ivan Sag .  1985 . ( ~ cneralizcd Ph . r'aisc , %' lruc-ture Grammar . Basil Blackwell Publisher Lt , (\[,
Oxibrd , UK.

