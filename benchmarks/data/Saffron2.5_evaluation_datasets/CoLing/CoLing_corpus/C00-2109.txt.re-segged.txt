Backward Beam Search Algorithm
for Dependency Analysis of Japanese
Satoshi Sekine
Computer Science Department
New York University
715 Broadway , 7th floor
New York , NY 10003, USA
sekine@cs , nyu.edu
Kiyotaka Uchimoto Hitoshi Isahara
Communications Research Laboratory
588-2 I waoka , I waoka-cho , Nishi-ku,
Kobe , Hyogo , 651-2492, Japan
\[ uchimoto , isahara \]@ crl.go.jp
Abstract
Backward beam search tbr dependency analysis of Japanese is proposed  . As dependencies normally g of l'om left to right in Japanese  , it is effective to analyze sentences backwards ( from right to left )  . The analysis is based on a statistical method and employs a bemn search strategy  . 
Based on experiments varying the bemn search width  , we found that the accuracy is not sen-sitiv c to the bemn width and even the analysis with a beam width of  1 gets ahnos the stone dependency accuracy as the best accuracy using a wider bemn width  . This suggested a deterministic algorithm for backwards Japanese dependency analysis  , although still the bemn search is eit bctive as the Nbest sentence accuracy is quite high  . The time of analysis is observed to be quadratic in the sentence lngth  . 
1 Introduction
Dependency analysis is regarded as one of the standard methods of Japanese syntactic analysis  . The Japanese dependency structure is usually represented by the relationship between phrasal units called ' bunsetsu '  . A bunsetsunsu-ally contains one or more content words  , like a noun , verb or adjective , and zero or more function words , like a postposition ( case marker ) or verb/nouns ul ~\[ ix . The relation between two bunsetsu has a direction front a dependent to its head  . Figure 1 shows examples of 1 ) unsets u and dependencies . Each bunsetsu is separated by " I "" The first segment " KARE-HA " consists of two words  , KARE ( He ) and HA ( subject case marker) . The numbers in the " head " lines how the head ID of the corresponding bunsetsus  . 
Note that the last segment does not have a head , and it is the head bunsetsu of the sentence . The task of the Japanese dependency analysis is to find the head ID for each bunsetsu  . 
The analysis proposed in this paper has two conceptual steps  . In the first step , dependency likelihoods are calculated for all possible pairs of bunsetsus  . In the second step , an optimal dependency set for the entire sentence is retrieved  . 
In this paper , we will mainly discuss the second step , a method fbr finding an optimal dependency set . In practice , the method proposed in this paper should be able to be combined with any systems which calculate dependency likelihoods  . 
It is said that Japanese dependencies have the t bllowing  characteristics1:   ( 1 ) Dependencies are directed from left to right ( 2 ) Dependencies don't cross ( 3 ) Each seglnent excep the rightmost one has only one head  ( 4 ) In many cases , the left ; context is not necessary to determine a dependency The analysis method proposed in this paper assumed these characteristics and is designed to utilize them  . Based on these assumptions , we can analyze a sentence backwards ( from right to left ) in an efficient manner . There are two merits to this approach . Assume that we are analyzing the M-th segment of a sentence of length N and analysis has already been done for the  ( M + 1 ) -th to Nth segments ( M < N )  . 
The first merit is that the head of the dependency of the M-th segment is one of the  seg-1Of course , there are several exceptions ( S . Shirai ,  1998) , but the frequencies of such exceptions are negligible compared to the current precision of the system  . 
We believe those exceptions have to be treated when the problems we are facing at the moment are solved  . Assumption (4) has not been discussed very much , but our investigation with humans showed that it is true in more titan  90?  . /0 of the cases . 

IDi 234 56
KARE-HA\[FUTATABIIPAI-W0\[TSUKURI , IKANOJO-NII0KUTTA . 
( He-subj ) ( again ) ( pie-obj ) ( made , ) ( to her ) ( present )
Head 64 46 6-
Translation : He made a pie again and presented it to her  . 
Figure 1: Exmnt ) lea JaI ) an ese sentence ,  1 ) unsetsus and det ) endenciesments between M+1 and N ( because of assumption 1 )  , which are already analyzed . Because of this , we don't have to kce1 ) a huge lnll n-1 ) er of possible analyses , i . e . we can avoid something like active edges in a chart parser  , or making parallel stacks in GLR parsing , as we can make a decision at this time . Also , we can use the beam search mechanism , 1) y keet ) ing only a certain n m n l ) er of . analysis candidates at (', ach segment . The width of the 1) (; amsearch can 1)c , easily tuned and the memory size of the i ) ro- ( : essisl ) rot ) ortional to the 1 ) roduct of the in l ) ut sentence length and tile bor on search width . 
The other merit is that the poss it ) le heads of tiled ( ~l ) en ( lency cant ) enarrowed down 1 ) c-cause of the ~ ssuml ) tion of noncrossing det ) en- ( lencies ( assumption 2 )  . For exani 1) le , if the Kthse glll ( ; nl ; dCl ) ends on the L-tll segnient ( A4<\]~<~L ) , then the \] ~ J-th segillent ( : ~ l~n't depend on any segments between 1~ and L . 
According to our experilnent , this reduced the numl ) er of heads to consider to less than 50 ( X~ . 
The te(:hnique of backw~trd analysis of , lal ) an ese sentences has 1 ) een used in rule-based methods , for example ( Fujita ,  1988) . However , there are several difficulties with rule-based methods  . First the rules are created by hmnans , so it is difficult to have wide coverage and keel  ) consistency of the rules . Also , it is difficult to incorporate a scoring scheme in  rule-1  ) ased methods . Many such met ; hods used hem'isties to make deterministic decisions  ( and backtracking if it ; fails in a sear (: hing ) rather l ; han using a scoring scheme . However , the com-1 ) ination of the backward analysis and the statistical method has very strong advantages  , one of which is the 1) em nsearch . 
2 Statistic framework
We . coinlined tile backward beam search strategy with a statistical dependency analysis  . ' rile det ~ filof our statistic framework is described ill  ( Uehimoto et al ,  1999) . There have been a lot of prol ) OS~fls for statistical analysis , in ninny languages , in particular in English and Japanese ( Magerman , 1995) ( Sekine and Grishman , 1995) ( Collins , 1997) ( I/atnal ) arkhi , 1997) ( K . Shirai et . al , 1998) ( Fujio and Matsnlnoto , 1998) ( Itarunoct . al , 1997) ( Ehara , 1998) . One of the most advance ( t systems in English is l ) ro-posed 1 ) y I at na parkhi . It , uses the Maximum Entropy ( ME ) model and both of the accuracy and the speed of the system arc among the best ret  ) or tcd to date . Our system uses the ME model , too . in the ME model , we define a set el!\]2~ , at lll'eS which arc thought to l ) euscflflindel ) ealden ( : y analysis , and it : learns the weights of the R~atures fl'om training data  . Our t~nt ttresin (: lude part-of-st ) eech , inflections , lexical items , the existence of a contain or bra ( : ket1 ) etween the segments , and the distmme between the segments . Also , confl ) inations of those features are used as additional f  e  , atures . The system eal- ( : ulates the probabilities of dependencies based on the model  , which is trained using a training corpus . The probability of an entire sentence is derived from the  1  ) roduct of tile probal ) ilities of all the dependencies in the sentence . We choose the analysis with the highest probafl ) ility to be the analysis of the sentence . Although the accuracy of the analyzer is not the main issue of the t  ) al ) er , as any types of models which used e-1 ) endency 1 ) rol ) al ) ilities can beiml ) lelnented by our method , the 1)ert brmancert ) orted in ( Uchi-lnoto et al ,  1999 ) is one of the best results reported by statistic ~flly based systems  . 
7553 Algorithm
In this section , the analysis algorithm will be described . First the algorithm will be illustrated using an example  , then the algorithm will be formally described . The main characteristics of the algorithm are the backward analysis and the beam search  . 
The sentence " KARE-HAFUTATABIPAI-W\[I
TSUKURI , KANOJ0-NI0KUTTA .   ( He made a pie again and presented it to her ) " is used as an input . We assume the POS tagging and segmentation analysis have been done correctly before starting the process  . The border of each segment is shown by "1" . In the figures , the head of the dependency for each segment is represented by the segment number shown at the top of each segment  . 

ID 1234 56
RARE-HA\[FUTATABI\[PAI-WO\[TSUKURI , \[KANOJO-NIIOKUTTA . 
( He-subj ) ( again ) ( pie-obj ) ( made , ) ( to her ) ( present ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
Algorithm 1 . Analyzenp to the second segment from the end The last segment has no dependency  , so we don't have to analyze it . The second segment fl ' om the end always depends on the last segment  . So the result up to the sec-end segment from the end looks like the following  . 
< Up to the second segment from the end >
ID 1234 56
KARE-HAI FUTATABI\[PAI-WOIT SUKURI , IKANOJO-NIIOK FITA . 
( He-subj ) ( again ) ( pie-obj ) ( made , ) ( to her ) ( present )
C and 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
. The third segment from the end
This segment (" TSUKURI , ") has two dependency candidates . One is the 5th segment ( " KANOJ0-NI " ) and the other is the 6th segment ( "0KUTTA " )  . Now , we use the probabilities calculated using the ME model in order to assign probabilities to the two candidates  ( C and l and C and 2 in the following figure )  . Let's assume the probabilities 0 . 1 and 0 . 9 respectively as an example . At the tail of each analysis , the total probability ( the product of the probabilities of all dependencies  ) is shown . The candidates are sorted by the total probability  . 

< Up to the third segment from the end >
ID 1234 56
KARE-HAI FUT A TABIIPAI-WOIT SUKURI , IKANOJO-HIIO KUITA . 
( He-subj ) ( again ) ( pie-obj ) ( made , ) ( to her ) ( present )
C and l60-(0.9)
C and 256-(0 . I ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
The t burth segment from the end
For each of the two candidates created at the previous tage  , the dependencies of the f burth segment from the end  ( " PAI-W0" ) will be analyzed . For C and l , the segment can't have a dependency to the fifth segment  ( " KANOJ0-1gI " )  , because of the noncrossing assmnption . So the probabilities of the dependencies only to the fourth  ( Candi-1 ) and the sixth ( Candi-2 ) segments are calculated . In the example , these probabilities are assmned to be 0 . 6 and 0 . 4 . 
A similar analysis is conducted for C and 2 ( here probabilities are assumed to be 0 . 5, 0 . 1 and 0 . 4) and three candidates are created ( Cand2-1 , C and 2-2 and C and 2-3) . 
< Up to the fourth segment from the end >
ID 1234 56
RARE-HAI FUT A TABIIPAI-WOIT SUKURI , IKANOJO-NIIOKUTTA . 
( He-subj ) ( again ) ( pie-obj ) ( made , ) ( to her ) ( present )
C~dt-i 466 -(0.64)
Candl-2666-(0.30)
Cand2-1456-(0.05)
Cand2-2656-(0.04)
Caud2-3556-(0 . 01)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
A stile analysis proceeds , a large number ( almost L ! ) of candidates will he created . 
However , by linfiting the number of candidates at each stage  , the total nmn ber of candidates can be reduced . This is the beam search , one of the characteristics of the algorithm . By observing the analyses in the example , we can e~sily imagine that this beam search may not cause a serious problem in performance  , because the candidates with low probabilities may be incorrect anyway  . For instance , when we set the beam search width = 3 , then Canal2-2 and Cand2-3 in the figure will be discarded at this stage , and hence won't be used in the following analyses  . The relationship of the beamsearch width and the accuracy oh-served in our experiments will be reported in the next section  . 
756. Up to the , first segment
The analyses are conducted in the , same way up to the first segment . For example , the result of tile analysis t br the entire sell -tence will be shown below  . ( Appropriate , probabilities are used . ) 4 . 2 Beam search w idth and accuracy In this subsection  , the relationship between the beam width and the accuracy is discussed  . In principle , the wider the beam search width , the more analyses can be retained and the better the accuracy cml be expected  . However , the re- . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . sultis somewhat different froan tile expectation . 
< Up to the first segment >
ID 1234 56
KARE-IIA\[FUTATABI\[PAl-W0T SUKURI , \[KANOJ0-NI\[0KUTTA . 
( Ile-subj ) ( again ) ( pie-obj ) ( made , ) ( to her ) ( present )
Candl 64460-(0.ii )
Cand 244 666 -(0.09)
C and 364656-(0 . 05)  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
Now , the formal algorithm is described induc-tive Jy in Figure  3  . The order of the analysis is quadraticill the length of the sentence  . 
4 Experiments
In this section , experiments and evaluations will be reported . We use the Kyoto University Corpus ( version 2 )   ( Kurohashiel . el ,  1)97) , a hand created Japanese corpus with POS tags , bunsetsu segments and dependency information . 
The sentences in the articles from January 1 , 1994 to January 8 ,  1994 (7 , 960 sentences ) a . reused t ' or tim training of the ME model , and the sente , nccs in the art Mes of Janum'y 9 ,  1994: (1 , 246 sentences ) are used for the ewduation . 
These id ; encesill the articles of Jalluary 10 , 1994 are kept for future evaluations . 
4.1 Basic Result
The evahlation result of our systenlis shown ill Table  1  . The experiment uses the correctly seg-ment e ( 1 and 1 ) art-oSsl ) eet'h tagger1 sentences of the Kyoto University corpus . The bealn search width is sol ; to 1 , in other words , the systeln runs deterministically . Here , ' dependency accuracy '
Table 1: l B valuation
Dependency accuracy
Sentence accuracy
Average analysis time 87 . 14% (9814/11263) 40 . 60% 0503/1239) 0 . 0 3 sec is the percentage of correctly analyzed dependencies out of all dependencies  . ' Sentence accuracy ' is the i ) ercentage of the sentences in which all the dependencies are analyzed correctly  . 
Table 2 shows the dependency accuracy and sentence accuracy for bemn widths  1 through 20  . The difference is very small , but the best Table 2: Relationship between beam width and accuracy
Bemn width Dependency Sentence
Accuracy Accuracy 87 . 14 87 . 16 87 . 20 87 . 1 . 5 87 . 14 87 . 16 87 . 20 87 . 20 86 . 21 86 . 21 40 . 60 40 . 76 40 . 76 40 . 68 40 . 60 40 . 60 40 . 60 40 . 60 40 . 60 40 . 6 0 accuracy is obtained when the beain width is 11   ( f br the dependency accuracy )  , and 2 and 3 ( t br the sentence accuracy ) . This proves that there are cases where the analysis with the highest product of probabilities is not correct  , but the analysis decide ( 1 at each stage is correct . This is a very interesting result of our experiment  , and it is related to assuln p tion 4 regarding Japanese dependency , lnentioned earlier . 
This suggests that when we analyze a . Japanese sentence backwards , we can do it deterministically without great loss of accuracy  . 
Table 3 shows where the mlalys is with bemn width 1 appears among the analyses with beal n width 200  . It shows that most deterministic analyses appear a stile best analysis in the nondeterministic analyses  . Also , mnong the deter-aninistic analyses which are correct  ( 503 Sell-tences )  , 498 sentences (99 . 0% ) have the same mmlys is at the best rank in the 200-beam-width analyses . ( Followed by 3 sentences at the see- . 
end , 1 sentence ach at the third and fifth rank . ) It means that in most of the cases , them mlysis



Length of the input sentence in segments
The beam search width
Candidate list ; C for each segment keeps the top W partial analyses from that segment to the last segment  . 
< Initial Operation >
The second segment from the end depends on the last segment  . 
This analysis is stored in C\[Length-l\].
< Inductive Operation >
Assume the analysis up to the ( M + l ) -th segment has been finished . 
For each candidate ~ c ' in C\[M+i \] , do the following operation . 
Compute the possible dependencies of the M-th segment compatible with ' c'  . For each dependency , create a new candidate Cd~by adding the dependency to ' c'  . Calculate the probability of ' d' . 
If C\[M\]has fewer than W entries , add ~ d ~ to C\[M\] ; else if the probability of Cd ~> the probability of the least probable entry of C\[M\]  , replace this entry by ' d' ; else ignore'd ' When the operation finishes for all candidates in C\[M+i \]  , proceed to the analysis of the ( M-l)-th segment . 
Repeat the operation until the first segment is analyzed  . 
The best analysis for the sentence is the best candidate in 

Figure 2: Formal Algorithln with the highest probability at each stage also has the highest probability as a whole  . This is related to assumption 4 . The best analysis with the left context and the best analysis without tile left context are the same  95% of the time in general , and 99% of the time if the analysis is correct . These numbers are much higher than our human experinmnt mentioned in the earlier footnote  ( note that the number here is the percentage in terms of sentences  , and the number in the footnote is the percentage in terms of segnmnts  . ) It means that we may get good accuracy even without left contexts in analyzing 
Japanese dependencies.
4.3 NBest accuracy
As we can generate Nbest results , we measured Nbest sentence accuracy . Figure 3 shows the Nbest accuracy . Nbest accuracy is the percentage of tile sentences which have the correct analysis among it stop N analyses  . By setting a large beam width , we can observe Nbest accuracy . The table shows the Nbest accuracy when the beam width is set  , to 20 . When we set N = 20,78 . 5% of the sentences have the correct analysis in the top  20 analyses . If we have \] 5" e<luc , ny 1175 (%) ( . ,5 . 8)
Rank 11
Frequen(:y1(%)
Table 3: The rank of the deterministic analysis 2   3   4:   5   6   7   8   20   11   8   4   2   1   2   ( 1 . 6) (0 . 9) (0 . 6) (0 . 3) ( I ) . 2 ) (0  . 1 ) (0  . 2) 12j , 51 ( i 17 180 o10 JJ (0 . 1) ( . 1) (0 . 1) (0 . 1) 91003 (02) 1920 and more 08 (0 . 6)
Sent ; once Accuracy . 53% #, 40.60%
IIIIIIIIII-~TE05101 1.520

Figure 3: Nbest sentenc ( ~ Accuracy an ideals ysl ; ( ml for finding th(~COl'lCCi ; mmlysis a , lnOllg ? th(;ln ~ which maS ,  11 . % OSCllltl , lll ; icO1"COll-l , ( ; x ; in for lllt ~ I ; io \]\] ~ we can have a v(T ya (:( ; Hr~d ; e analyzer . 
\ ~ TCC allllltl , l(( ; two interesting observations trom the result . The ac(:uracy of the 1--best mmlys is about 40% , which is more tlm . nhalf of t , heaccura (: y of 20-1) est analysis . This shows that although the system is not 1 ) erfb , ct , the computation of the 1 ) rolml ) ilities is t ) rol ) ably good in order l ; of ind the correct mmlys is at the top rank . 
The other point is that the accm'aey is saturated at m'om M  80%  . Iml ) rove mel , to ver 80% seelns very dit\[icult even if we use a very large bemn width W  . ( lf we set ; W to the number of all possible combinations , which means almost L ! for sentence length L , we (21 MgC ; 100 (~0N best accm'aey , lint this is not worthe on sidel'-ing . ) This suggests tlmt wc h~we missed something important  . In part ; icular , from our investigation of the result , we believe that ( : oordinate structure is one of the most important factors to iml  ) rove the accuracy . This remains one area of fllturc work . 
4.4 Speed of the analysis
Based on the f'(n'nml algorithm , the analysis tinle can be estimated as t ) rot ) or l ; ional to the square , of the inl ) ut sentence length . Figure 4: shows the relations hiI ) between the analysis time and the sentence length when w c set the beam width to  1  . We use a Sun Ultra10 machine and the process size is about 8M byte . 
We can see that the actual analyzing time al-
Analysis time ( see . ) 0 . 3 0 . 2*/"0 ~ ~, r , ,, 0 10 20 30 40
Sentence length\]?igure 4:\] ~ . elations hi1 ) between sentence length and mmlyzing time most follows the quadratic urve  . The ~ verage amfly sistime is 0 . 0 3 second and the ~ werage sentence lengl : his 10 segments . The analysis time for the longest sentence ( 41 segments ) is 0 . 29 second . W \ ; have no tot ) l ; imized the In'ogram in terms of speed aim there is room to shrink /  ; he processize . 
759 5 Conclusion
In this paper , we proposed a statistical Jtt panese dependency analysis method which processes a sentence backwards  . As dependencies normally go from left to right in Japanese  , it is eflhctive to analyze sentences backwards ( from right to left )  . In this paper , we proposed a Japanese dependency analysis which combines a backward analysis and a statistical method  . It can naturally incorporate a beam search strategy  , an effective way of limiting the search space in the backwm'd analysis  . We observed that the best perf brmances were achieved when the width is very small  . Actually ,   95% of the analyses obtained with bemn width = l were the stone as the best analyses with beam  width=20  . The analysis time was proportional to the square of the sentence length  ( nmn ber of segments )  , as was predicted from the algorithm . The average analysis time was 0 . 03 second ( average sentence length was 10 . 0 bunsetsus ) and it took 0 . 29 sec-end to analyze the longest sentence , which has 41 segments . This method can be ~ tp plied to various languages which haw ~ the stone or similar characteristics of dependencies  , for example
Koran , Turkish etc.

Adam Berger and Harry Printz . 1998:"A
Comparison of Criteria for Maximum Entropy / Mininmm Divergence Feature Selection "  . Proceedings of the EMNLP-98   97-106 Michael Collins . 1997: " Three Generative , Lexicalized Models for Statistical Parsing " . 
Proceedings of the ACL 971623
Terumasa Ehara . 1998: " CM culation of
Japanese dependency likelihood based on
Maximmn Entropy model " . Proceedings of the ANLP , Japan 382-385 Masakazn t51jio and Yuuji Matsumoto .   1998 : " Japanese Dependency Structure Analysis based on Lexicalized Statistics "  . Proceedings of the EMNLP-98   87-96 Katsuhiko Fujita .   1988 : " A Trial of deterministic dependency analysis  "  . Proceedings of the Japanese Artificial Intelligence Annual  meet-in9   399-402 Masahiko Haruno and Satoshi Shirai and Yoshifumi Ooyama  .   1998 : " Using Decision Trees to Construct a Practical Parser "  . Proceeding sqf the the COLING/A CL-98   505-511 Sadao Kurohashi and Makoto Nagao . 1994: " KN Parser: , J ~ tpanese Dependency/Case Structure Analyzer " . Proceedings of The International Workshop on Sharable Natural 
Language Resources 4855
Sadao Kurohashi and Makoto Nagao . 1997: " Kyoto University text corpus project " . Proceedings of the ANLP , Japan 115-118 David Magerman .   1995 : " Statistical Decision-Tree Models for Parsiug  "  . Proceedings of the
ACL-95276-283
Adwait I /, at na parkhi .   1997 : " A Linear Observed Time Statistical Parser Based on Maximum Entropy Models "  . Proceedings o . f

Satoshi Sekine and Ralph Grishman .   1995 : " A Corpusbased Probabilistic Grammar with Only Two Nonterminals "  . Proceedings of the
IWPT-95216-223
Satoshi Shirai . 1998: " Heuristics and its lira - itation " . Jowrnalo \ [ the ANLP , Japan Vol . 5
No . l , 12
Kiyoaki Shirai , Kentaro Inui , Takenobu ' lbku-naga and Hozunli Tanaka .   1998 : " An Empirical Evaluation on Statistical Parsing of Japanese Sentences Using Lexical Association Statistics "  . P'roceedings of EMNLP-988086 Kiyotaka Uchimoto , Satoshi Sekine , Hitoshi
Isahara . 1999: " Jat ) anese Dependency
Structm'e Analysis Based on Maximum Entropy Models  "  . P~vceedingso\[the EACL99 pp 196-203
