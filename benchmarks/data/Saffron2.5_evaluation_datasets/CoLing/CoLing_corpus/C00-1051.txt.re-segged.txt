Committee-based Decision Making
in Probabilistic Partial Parsing
INUI Takashi * and INUI Kentaro * t
?Department of Artificial Intelligence , Kyushu Institute of Technology
? PRESTO , Japan Science and Technology Corporation t_in u i  , inui@pluto . ai . kyutech , ac . jp
Abstract
This paper explores two direction sibr the next step beyond the state of the art of statistical parsing : probabilistic partial parsing and committee-based decision making  . Probabilistic partial parsing is a probabilistic extension of the existing notion of partial parsing ~ which enables finegrained arbitrary choice on the tradeoff between accuracy and coverage  . Committee-based decision making is to combine the outputs from different systems to make a better decision  . While varions committee-based techniques for NLP have recently been investigated  , they would need to be fln'ther extended so as to be applicable to probabilistic partial parsing  . Aiming at this coupling , this paper gives a general fl'amework to committee-based decision making  , which consists of a set of weighting flmctions and a combination function  , and discusses how it can be coupled with probabilistic partial parsing  . Our ext ) eriments have so far been producing promising results  . 
1 Introduction
There have been a number of attempts to use statistical techniques to improve parsing performance  . While this goal has been achieved to a certain degree given the increasing availability of large treebanks  , the remaining roomt br the improvement appears to be getting saturated as long as only statistical techniques are taken into account  . This paper explores two directions t br the next step beyond the state of the art of statistical parsing : probabilistic partial parsing and committee-based decision making  . 
Probabilistic partial parsing is a probabilistic extension of the existing notion of partial parsing  ( e . g . ( Jensen et al ,  1993 ) ) where a parser selects as its output only a part of the parse tree that are probabilistically highly reliable  . This decision making scheme enables a finegrained arbitrary choice on the tradeoff between accuracy and coverage  . Such trade-oil is important since there are various applications that require reasonably high accuracy even sacrificing coverage  . A typical example is the t ) araI ) hras-ing task embedded in summarization , sentence simplification ( e . g . ( Carroll et al , 1998)), etc . 
Enabling such tradeoff " choice will make state-o f the-art parsers of wider application  . Partial parsing has also been proven usefll libr bootstrapping leanfing  . 
One may suspect hat the realization of partial parsing is a trivial matter in probabilistic parsing just because a probabilistic parser inherently has the notion of " reliability " and thus has the tradeoff : ' between accuracy and coverage  . However , there has so far been surprisingly little research focusing on this matter and ahnost no work that evaluates statistical parsers according to their coverage-accuracy  ( or recall precision ) curves . Taking the significance of partial parsing into account  , the ref i ) re in this paper , we evaluate parsing perfbrmance according tO coverage-accuracy cn rves  . 
Committee-based decision making is to con > bine the outputs from several difl'erent systems  ( e . g . parsers ) to make a better decision . Recently , there have been various attempts to at ) -ply committee-based techniques to NLP tasks such as POS tagging  ( Halteren et al , 1998; Brill et al ,  1998) , parsing ( Henderson and Brill ,  1999) , word sense disambiguation ( Pedersen ,  2000) , machine translation ( lh'ederking and Nirenburg ,  1994) , and speech recognition ( Fiscus ,  1997) . Those works empirically demonstrated that combining different systems often achieved significant improvelnents over the previous best system  . 
In order to couple those committee-based ever , Olle would still need to make a fllrther extension  . A inling at this coupling , ill this t ) at ) er , we consider a general framework of (: ommil , tee-based decision making that consists of ~ set of weighting flmctions mida combination flmc - tion  , and ( lis ( ' us show that Kalne work en al ) les the coupling with t ) robal ) ilistic t ) artial t ) m:sing . 
To denions tr ~ te ho wit works , were t ) or t the results of our t ) arsing exl ) eriments on a Japanese treebank . 
2 Probabilistic partial parsing 2 . 1 Dependency probability In this t ) at ) er , we consider the task of ( le ( : id-ing the det ) endency structure of a Jat )  ; mese input sentence . Note that , while we restrictore : discussion to analysis of Jat  ) an eses enl ; ( ; nc( ; s in this t ) ~ l)er , what we present l ) elow should also t ) estrnightfi?rwardly ? xt ) plical ) h ~ to more wide-ranged tasks such as English det ) endency analysis just like the t ) roblem setting considered t ) y
Collins (1996).
Givell ; min l ) ut sentence , s as a sequence , of B'unset , su-t ) hrases ( BPs ) J , lq b2 .   .   . lh ~ , our task is to i(tent , i \ [ y their inter-BP del ) endency struc-t ,   , en = l , j ) l , : = ' , , , , where ( tenot ; es that bi(let)on(Ison(or modities ) bj . 
Let us consider a dependency p ' roba , bility(I)P ): P('r(bi , bj ) l . s ') , at ) rol ) al ) ility l ; lu ~ t ' r(bi , b : j ) hohts in a Given senl:ences : Vi . EjP (','(51, t,j)l . 4= a . 
2.2 Estimation of DPs
Some of the state-of:the-art 1) rol ) at ) il is ( ; icbm-guage i node ls such as the l ) ottomut ) models P ( l~ , l . , . ) propos , ,d by Collins ( 1: ) 96 ) and Fujio et al ( 1998 ) directly estimate DP st br : ~ given in t ) ut , whereas other models su ( ' hasPCFO-t ) ased tel ) down generation mod (  ; lsP ( H , , , s ) do not , ( Charnink , 1997; Collins , 1997; Shir ~ fiet ~ rl . , 1998) . If the latter type of mod ( , ' ls were totally exchlded fronlany committee , our commit ; tee-based framework would not work well in I ) rac-lice . Fortm : ately , how(:ver , event br such a model , one can still estimate l ) l?s in the following way if the rood (  ; 1 provides the nbest del ) en-1A bunsct suphrase ( BP ) is a chunk of words ( -on-sist ; ing of a content word ( noun , verl ) , adjective , etc . ) accoml ) mfied by sonic flmctional word ( s )   ( i ) arti ( : le , mlx-iliary , etc . ) . A . l ai ) anes ( ' sent c'nce can 1 ) canalyzed as a sequence of BPs , which constitutes an inter-BP de I ) en-dency structure dency structure candidates cout ) led with prot ) -abilistic scores . 
Let Ri be the ith best del ) endency st ; ruct ; ure(i = 1 ,   .   .   .   , ' n ) of ; ~ given input , s ' according to a given model , and h ; t ~ H l ) e a set ; of H , i . Then , , . , u , l , ecsl ; ima ; ed by the following ai ) l ) l " O Xilnation equation : . /) F?7 ~ HP (', ( b,z , (1) where P ' R . u is the probal ) i lit ; y mass of H , E7 ~ Lr , and prn . is the probability mass of R~~H that suppori  ; s'r(bi , bj ) . Tile approximation error c is given 1) y c < l ; r~--1% , where l ) , p , , is 1 ; t2( ; --l~p ~' prol ) a bilil ; y mass of all the dependency structure candidates for s  ( see ( Peele , 1993) for the l ? roof ) . This means that the al ) t ) roximation error is negligil ) leif P ' R , , is sut\[iciently close to 1) , R , which holds for a reasonably small mlmt ) er'ninlnOSt cases in practical statistical parsing  . 
2.3 Coverage-accuracy curves
We then conside , r the task of selecting dependency relations whose estimated probability is higher I : han a  ( : e:i ; ainl ; hreshoh to-(0 < a < 1) . 
When ( risset1; obe higher ( closer to 1 . 0) , t ; he accuracy is cxt ) ected to become higher , while the coverage is ext)ecl ; ed to become lowe , : , and vi (: eversm Here , (; over ~ geC*anda , (; ctlra(;yA are defined as follows :  #of the . decided relations
C  #of nilther e , lations in I ; \] let ; est so , i2 ) /~ #of the COl'rectly decided relati?n~3~vJ A  #of the decided relations Moving the threshohl cr from  1  . 0 down toward 0 . 0 , one ( : an obtain a coverage-a ( :cura ( : y ( : urve ( CA curve )  . In 1) rol ) al ) ilistict ) artial parsing , weew flunte the t ) erforman ( ' e of a model ~ m cording to its CA curve . A few examt ) les are shown in Figure 1 , which were obtained in our ext ) erim ( mt ( see Section 4 )  . Ot ) viously , Figure 1 shows that model A outt)erformed the or , her two . To summarize a CAcIlrve , we use the ll-t ) oint average of accuracy ( ll-t ) oint at :- curacy , hereafl ; er ) , where the eleven points m'eC = 0 . 5, 0  . 55 ,  .   .   .  , 1 . 0 . The accuracy of total parsing correst ) onds to the accuracy of the t ) oint in a CA curve where C = 1 . 0 . We call it total ~ ccuracy to distinguish it from l\]-l  ) oint at : el > racy . Not ; (' . that two models with equal achieve-
A0 . 95 0 . 9 0 . 85 0  . 8 0 0  . 2 0  . 4 0  . 6 0  . 81 coverage
Figure 1: CA curves meuts in total accuracy may be different in ll-point accuracy  . In fact , we found such cases in our experiments reported below  . Plotting CA curves enable us to make a more finegrained perfbrmance valuation of a model  . 
3 Committee-based probabilistic partial parsing We consider a general scheme of comnfittee-based probabilistic partial parsing as illustrated in Figure  2  . Here we assume that each connnit-tee member M ~ ( k = 1 ,   .   .   .   , m ) provides a DP matrix PM~(r ( bi , bj ) ls ) ( bi , bjEs ) tbreach input 8 . Those matrices are called in lmt matrices , and a regive :: to the committee as its input . 
A committee consists of a set of weighting functions and a combination flmction  . The role assigned to weighting flmctions is to standardize input matrices  . The weighting function associated with model Mk transforms an input matrix given by MI ~ to a weight matrix WaG-The majority flmction then combines all the given weight matrices to produce an output matrix O  , which represents the final decision of the con > mittee  . One can consider various options for both flmctions  . 
3.1 Weighting functions
We have so far considered the following three options  . 
Simple The simplest option is to do nothing : ~ a ~ = PA ~  (  ,  . ( b  ~ , bj ) l ~) (4) ijoM k where wij is the ( i , j ) element of I/VM k . 
Normal Abare DP may not be a precise estimation of the actual accuracy  . One can see this by plotting probability-accuracy curves  ( PA curves ) as shown in Figure 3 . Figure 3 shows that model A tends to overestimate DPs , while/Olllll\[ttct~based decision nlakilll ~ ii " i - -= "  , , models it lpl lt i weight matrices i .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . : matrices " tVl : : " ~ Vt ~ igltiJIg\]"un ?1\]Oll 
CF:Ct3 mhinatlanI : mlcl\[.n
Figure 2: Committee-based probabilistic partial parsing 0  , 9 ; , ~ 0 , 8 = 0 . 7 0 . 6
C0 . 5 0,5 0 . 6 0 . 7 0 . 8 0 . 9 dependency probability
Figure 3: PA curves model C tends to underestimate DPs . This lneans that if A and C give different answers with the same DP  , C's answer is more likely to be correct . Thus , it is : sotnecessarily a good strategy to simply use give::bare DPs in weighted majority  . To avoid this problem , we consider the tbllowing weighting flmction : w ~Jk=@lkAM ~  ( PMk ( ' , '( bi , b : i ) l . s ) )  ( 5 ) where AMk ( P ) is the function that returns the expected accuracy of Mk's vote with its depen-Mk dency probability p  , and oz i is a normalization factor . Such a function can be trained by plotting a PA curve fbr training data  . Note that training data should be shared by all the committee members  . In practice , t br training a PA curve , some smoothing technique should be applied to avoid overfitting  . 
Class The standardization process in the above option Normal can also be seen as an effort for reducing the averaged crossentropy of the model on test  , data . Since PA curves tend to defi ~ , r not only between different models but also between different problem classes  , if one incorporate some problem classification into  ( 5 )  , the averaged crossentropy is expected'w ~=/~' ; ' ~ AM . %(i)M~(r(b~ , bj)l , s )) (6) where AMkcl , i ( P ) is the PA curve of model Mk only t br the problems of class Cb ~ in training data  , and flMk is a normalization factor . Fori probleln classification , syntactie/lexieal features of bi may be useful . 
3.2 Combining functions
For combination flmctions , we have so far considered only simple weighted voting  , which averages the given weight matrices : 1; I , 
Mk1 v -" Mk ?= --2_ , ~' J'U (7) ? iJ'm , h = l where o . i . f / ~ :_ is the ( i , j ) element of O . 
Note that the committee-based partial parsing frmnework t  ) resented here can be see , n as a generalization of the 1 ) reviously proposed voting-based techniques in the following respects :  ( a ) A committee a ( : ( : epts probabilistically para-meterized votes as its in tmt  . 
(d ) A committee ac(:el ) tsmultil ) levoting ( i . e . it ; allow a comn fitteemenf l ) er to vote not only to the 1 ) est-scored cal Mi ( late trot also to all other potential candidates )  . 
((:) A .   ( : ommittee 1 ) rovides a metal st br standard-izing original votes . 
( b ) A committee outl ) uts a 1 ) rot ) a bilisti ( " distribution representing a tinal decision , which constitutes a CA curve . 
For examt ) le , none of simple voting techniques for word class tagging t  ) roposed 1 ) yvan Halteren et al ( 1998 ) does not accepts multiple voting . Henderson and Brill ( 1999 ) examined constituent voting and naive Bayes classifi  ( : a-lion for parsing , ol ) taining positive results i breach . Simple constituent voting , however , does not accept parametric votes . While Naive Bayes seems to partly accept l ) arametric multit ) levoting , it ; does not consider either sl ; and ardization or coverage/accuracy tradeoff . 
4 Experiments 4.1 Settings
We conducted eXl ) erinmnts using the tbllow-ingtive statistical parsers : Table  1: The total/ll-t ) oint accuracy achieved 1 ) y each individual model total 11-point
A 0.8974 0.9607
B 0.8551 0.9281
C 0.8586 0.9291
D 0.8470 0.9266
E 0 . 7885 0 . 8567 ? KANA ( Ehara ,  1998 ) : a bottom-up model based ollmaxinmmentropy estimation  , Since dependency score matrices given by KANA have no probabilistic semantics  , we normalized them t breach row using a certain function manually tuned for this parser  . 
? CI \] AGAKE ( Fujio et al ,  1998 ) : an extension of the bottom-up model proposed by
Collins ( Collins , 1996).
? Kanaymna's parser ( Kanayama et al , 1999): al)o , tom-up model coupled with an HPSG . 
? Shirai's parser ( Shirai et al ,  1998 ) : a topdown model incorporating lexical collocation statistics  . Equation (1) was used t brestimating DPs . 
? Peach Pie Parser ( Uchilnoto et al ,  1999 ) : a bottom-up model based on maximum entropy estimation  . 
Note that these models were developed fl fily independently of e a  ( ' hother , and have sigl fifi-Calltly different ( : haracters ( I i ) ra comparison of their performance , see % tble1) . In what Jbl-lows , these models are referred to anonymously . 
For the source of the training/test set , we used the Kyoto corpus ( ver . 2 . 0) ( Kurohashi et al . , 1 . 997) , which is a collection of Japanese newspaper articles mmotated in terms of word boundaries  , POS tags , BP boundaries , and inter-BP dependency relations . The corpus originally contained 19,956 sentences . To make the training/test sets ~ we tirst removed all the sentences that were rejected by any of the above five parsers  ( 3 , 146 sentences ) . For the remaining 16 , 810 sentences , we next checked the consistency of the BP boundaries given by the parsers since they had slightly different criteriat brBP segmentation fl'om each other  . In this process , we tried to recover as many inconsistent boundaries as possible  . For example , we tbund there were quite a few cases where a parser recoglfized a certain word sequence msa single BP  , where a some other parser recognized the same sequence as two BPs  . In such\[A SimpleD Normal\[\]Class 0 . 96 0 . 955 0 . 95
A : 0.9607\\ na?o ~ ~00.975\[eeg
Figure 4: ll-point accuracy : A included 0 . 96 ', \[ . ~ Normalm Class 0 . 95 0 . 94 0 . 93 0 . 9 2   9291 f 5 g a0 Figure 5: l 1-point accuracy : B/C included a case , we regarded that sequence as a single BP under a certain condition  . Anaresult; , we obtained 13 , 9 90 sentences that can be accepted by all the parsers with all the BP boundaries consistent  2 We used thin set tbr training and evaluation . 
For cloned tests , we used 11 , 192 sentences (66 , 536 BPsa ) for both training and tests . 
For open tests , we conducted fivefold crossvalidation on the whole sentence set  . 
2 In the BP concatenation process described here , quite a few trivial dependency relations between eigl  , -boring BPs were removed from the test set . This made our test set slightly more difficult tlmn what it should have  1  ) cert . 
3This is the total nmnber of BPs excluding the rightmost two BPs for each sentence  . Since , in Jal ) anese , a BP ahvays depends on a BP following it , the rightmost BP of a sentence does not ( lei ) (tnd on any other BP , and the second rightmost BP ahvays depends on the rightmost BP  . Therefore , they were not seen as subjects of evahmtion . 

D Simple\[21 Normalm Class 0 . 965 0 . 97 0 . 96 0 . 9 55 Figure 6: ll-point accuracy : + KNP For the classification of problems  , we manually established the following twelve ( : lasses , each of which is defined in terms of a certain nlol : phological pattern of depending BPs :  1  . . nonfinal BP wit , ha case marker "' wa ( topic ) " 2 . nominal BP with a case marker " no ( POS ) "3 . nominal BP with a case marker " ga(NOM ) " 4 . nominal BP with a case marker % ( ACC ) "5 . nonlinal BP with a case marker " hi(DAT ) " 6 . nominal BP with a case marker " de ( LOC/ .   .   .   ) " nominal BP ( residue ) adnominal verbal BP verbal BP ( residue ) adverb adjective residue 4 . 2 Resu l ts and d iscuss ion Table 1 shown the total/ll-point accuracy of each individual model  . The performance of each model widely ranged from 0  . 96 down to 0 . 86 inll-point accuracy . Remember that A is the optimal model , and there are two second-best models , B and C , which are closely comparable . 
In what t bllows , we use these achievements ms the baseline for evaluating the error reduction achieved by organizing a committee  . 
The pertbrmanee of various committees is shown in Figure  4 and 5  . Our primary interest here is whether the weighting functions presented above effectively contribute to error reduction  . According to those two figures , although the contribution of the flmction N or -malweren or very visible  , the flmction Class consistently improved the accuracy  . These results can be a good evidence t br the important role of weighting flmctions in combining parsers  . 






35 20.96 0.94 0.92 t ~ tII,\[iJ
Figure 7: Single voting vs . Multiple voting While we manually tmill : the 1 ) roblem class it i ( 'a-l ; ion in our ext ) erimen ;  , autom ; ~ I ; ic(: lassitication te . chniques will also 1) e obviously worth considering . 
Wel ; hene . on ( tucted another exl)e , rime . nI ; to examine the , et\['e(-l ; sofmuli ; it ) levoting . One (: ansl ; raighi ; forwardly sinnlate a single-voting com-nlil ; tee by ret ) lacing wij in equal ; ion (7) with w ~ . i given by :, wi . i ( if ' j=m'xk'wit~)=_0(o ; he . wise ) ( S ) The resull ; s are show l in Figure 7 , which corot ) are sl ; he original multi-voting committees and l ; hesinmlai ; e(t single-voi:ing (: olm nil ; l ; ees . 
Clearly , in our se;tings , multil ) levoting signif-i can l ; lyoul ; pertbrmed single vol ; ing1) art i (: ul ~ rly when t ; he size of a (' ommii ; tee is small . 
The nexl ; issues are whel ; her ~(: Omlnil ; te , (' , always oul ; perform its indivi(tmdmemt ) ers , mtd if not ; , what should be ( -onsidered in organizing a commii ; i ; e e . Figure 4 and 5 show ; hal ; COlllllil ; -teesnol ; ilmlu(lingt ; heot ) timal model A achieved extensive imt ) rovemenl ; s , whereas the merit of organizing COlmnitl ; ees including A is not very visible . This can bet ) arl , lyat tril ml ; ed to the fa . ct that the corot ) el ; once of the , individual mem-l)ers widely diversed , and A sign it i (: md ; ly OUt l ) er-forms the ol:her models . 
Given l , he good error reduct ; i on achieved by commit , tees containing comt ) ar~blememl ) erssue h~sBC , B Da , ndB@I ) , however , it should t ) e reasonable 1 ; oe Xl ) ect thai ; a (: omlnil , l , e , e including A would achieve a significant imt ) rovement ; if a nol ; her nearly ol ) t ; ilnal model was also in corl ) o -0 . 8 v 0,70 . f i 0 . 5 0 . 6 1,7 ) . g0 . 9 dependency probability
Figure 8: PA curves : + KNPrated . To empirically prove this as smnpl ; ion , we , conduct ; edanot ; her experiment , where we add another parser KNP ( Kurohashiel ; al . , 1 !)94:) 1 ; o each commil ; ;ee that apt)ears in Figure 4 . KNI ? is much closer to l node lAinl ; ol ; alaccuracy t ; hant ; he other models (0 . 8725 intol ; al accuracy) . However , il ; does not provide . DP rea-l ; rices since it is designed in a rule-l ) ased fashion the current ; version of KNP1) rovides only the t)esl ; -t ) referrext parse t ; reefore a (: hinl ) Ul ; sentence without ~ my scoring annotation . Wel ; huslet KNP1 ; os imply vol ; e i t s l ; ol ; alaeem : aey . Tim results art ; shown in lqgure 6 . This time all l ; he commil ; tees achieved significant improvemenl ; s , wil ; h;hem ~ ximume , rrorre(hu :; ionrate upl;o'3 ~% . 
As suggested 1) y ; he . re , suits of t ; hisexl ) erimenl ; with KNP , our scheme Mlows a rule-based 110 11-t ) ~ r ; m , el : ricp ~ rse . rt ; opb ~ yinae ommil ; l ; e . epreserving it ; s ~ d ) i l i t : y t ; ooui ; t ) ul ; t ) aralnel ; rieI ) Pma-( ; ri(:es . To1) ush (; he ~ u'gumen(;fl , r l ; her , SU l ) pose ; ~1) lausil ) lesil ; ual ; i on where we have ; mOl ) l ; imall ) ut non-1 ) a rame trie rule-based parser and several suboptimal si  ; atistical parsers . Insu('h ~ case , our commil ; tee A ) ased scheme may t ) eable l ; o organize a commi , tee that can 1 ) rovidel ) Plnatri ( : es while preserving the original to l ; alaccuracy of the rule-b~sed parser . To set this , we conducted another small experiment , where , we combined KNP with each of C and D , 1) oth of whi (: hareless compe . tent than KNP . The resulting (: ommil ; l ; ees successflfly t ) rovided reasonal ) le PA curves as shown in Figure 8 , while even further lint ) roving the original ; ol ; alat : curacy of KNP (0 . 8725 to 0 . 8868 tbrCF and 0 . 8860 for DF ) . Furthermore , t ; he CO mlnittees also gained the 11-point accuracy over C and D ( 0 . 9291 to These . results suggest hat our committee-based scheme does work even if the most competent member of a committee is rule-based and thus nonparametric  . 
5 Conclusion
This paper presented a general committee-based frmnework that can be coupled with probabilistic partial parsing  . In this framework , a committee accepts parametric multiple votes , and then standardizes them , and finally provides a probabilistic distribution  . We presented a general method for producing probabilistic multiple votes  ( i . e . DP matrices) , which allows most of the existing probabilistic models for parsing to join a committee  . Our experiments revealed that ( a ) if more than two comparably competent models are available  , it is likely to be worthwhile to combine them ,   ( b ) both mult it ) levoting and votest and ardization effectively work in committee-based partial parsing  ,   ( c ) our scheme also allows a nonparametric rule-based parser to make a good contribution  . 
While our experiments have so far been producing promising results  , there seems to be much room left for investigation and improvement  . 

We would like to express our special thanks to all the creators of the parsers used here for enabling ~ fll of this research by providing us their systems  . We would also like to thank the reviewers tbr their suggestive comments  . 

Brill , E . and J . Wu . Classifier Combination for ha-proved Lexical Disambiguation  . In Proc . of the 17th COLING , pp . 191-195, 1998 . 
Carroll , J . , G . Minnen , Y . Cmming , S . Devlin and J . Tait . Practical Simplification of English Newspaper Text to Assist Aphasic Readers  . In Prvc . of AAAI98 Workshop on Integrating Artificial Intelligence and Assistive Technology  , 1998 . 
Charniak , E . Statistical parsing with a contextfree grammar and word statistics  . In Prvc . of the
AAAI , pp . 598603, 1997.
Collins , M . J . A new statistical parser based on bi-grmn lexical dependencies  . In Proc . of the 3~th
ACL , pp . 184-191, 1996.
Collins , M . J . Three generative , lexicalised models for statistical parsing . In Proc . of the 35th ACL , pp . 16-23, 1997 . 
Ehara , T . Estinlating the consistency of Japanese dependency relations based on the maximamen -trot  ) y modeling . Ill Proc . of the/~th Annual Meeting of The Association of Natural Language Processing  , 1) t ) . 382-385, 1998 . (In Japanese ) Fiscus , J . G . A postprocessing system to yield reduced word error rates : Recognizer output voting error reduction  ( ROVER )  . In EuroSpccch , 1997 . 
Fk'ederking , R . and S . Nirenburg . Three heads are better t it a none . In Proc . of the dth ANLP , 1994 . 
Fujio , M . and Y . Matsmnoto . Japmmse dependency structure analysis based on lexicalized statistics  . 
In Proc . of the 3rd EMNLP , t)I ).87-96, 1998.
Henderson , J . C . and E . Brill . Exploiting Diversity in Natural Language Processing : Combining Parsers  . In Proc . of the 1999 Joint SIGDAT Con-fcrcnccon EMNLP and I/LC , pt ) . 187--194 . 
Jensen , K . , G . E . Heidorn , and S . D . Richardson , editors , natural anguage processing : The PLNLP AppTvach . Kluwer Academic Publishers , 1993 . 
Kanayama , H . , K . Torisawa , Y . Mitsuisi , and J . Tsujii . Statistical Dependency Analysis with an HPSG -based Japanese Grainmar  . In Proc . of the NLPRS , pp . 138-143, 1999 . 
Kurohashi , S . and M . Nagao . Buildinga Jat ) an eseparsed corpus while lint ) roving tile parsing system . 
In Proc . of NLPRS , pp . 151-156, 1997.
Kurohashi , S . and M . Nagao . KNParser : Japanese Dependency/Case Structure Analyzer  . in Proc . of Th . ehtt cr national Worksh . op on Sharablc Natural
Lang'aagcRcso'arccs , pp . 48-55, 1994.
Poole , D . Average-case analysis of a search algorithm fl ) restimating prior and 1 ) ost crior probabilities in Bayesia networks with extreme  1  ) rot ) a bil-ities , thci3th LICAL pp . 606 612, 1993 . 
Pedersen , T . A Simple AI ) l ) roach to Building Ensembles of Naive Bayesian Classifiers for Word Sense Dismnbiguation In Proc  . of the NAACL , pp . 63-69, 2000 . 
Shirai , K . , K . hmi , T . Tokunaga and H . Tanaka An empirical evaluation on statistical 1 ) arsing of Japanese sentences using a lexical association statistics  , thc3rd EMNLP , pp . 80-87, 1998 . 
Uchimoto , K . , S . Sekine , and H . Isahara . Japanese dependency structure analysis based on maximum entopy models  . In Proc . of thc 13th EACL , pp . 196-203, 1999 . 
van Halteren , H . , J . Zavrel , and W . Daelemans . hn-t ) roving datadriven word class tagging 1 ) y system combination . In Proc . of the 17th COLING , 1998 . 

