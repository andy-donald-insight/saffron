ABL : Alignment-Based Learning
Menno van Zaanen
School of Computer Studies
University of Leeds
LS29 JTL(~eds

menno@scs , leeds , ac.uk
Abstract
This \] ) al ) er intro du ( : esanewty l ) e of grammar learning algorithm , i i l s t ) i r e d l ) y s l ; ringed it distance ( Wagner and F is (: her ,  1974) . The algorithm takes a ( : ortms of tlatS ( ~lltell ( : es as input and returns a ( : ortms of lat ) elle d , l)ra(:ket(~(1sen-~ten(:(~s . The method works on 1 ) airs of unstru ( :-tllr ( ? ( l Sell tell C ( ~ , s that have one or more words in (: on unon . W\]lc , ll two senten ( ' es are ( tivided into parts that are the same in 1 ) oths ( mten ( : es and parl ; stha ; are ( litl'erent , this intbrmation is used to lind l ) arl ; s that are interchangeal ) le . The set ) arts are taken as t ) ossil ) le ( : on stituents of the same tyl ) e . After this alignment learning stel ) , the selection learning stcl ) sel ( ~ ( ' ts the most l ) rot ) -at ) le constituents from all 1 ) ossit ) le ( : onstituents . 
Thism ( ; thod was used to t ) ootstrat ) structure ( m the ATIS ( : or tms ( Mar ( : us et al , 1f ) 93 ) and on the OVIS ~ ( : or t ) us ( Bommma et ~ d . , 1997) . 
While the results are en (: om : aging ( weol ) ; ained Ul ) to 89 . 25% noncrossing l ) ra (: kets precision ) , this 1 ) at ) er will 1 ) oint out some of the shortcomings of our at ) l ) roa ( : h and will suggest 1 ) ossible solul ; ions . 
1 Introduction
Unsupervised learning of syntactic structure is one of the hardest  1  ) rol ) lems in NLP . Although people are adept at learning grammatical structure  , it is ditficult to model this 1 ) recess and therefore it is hard to make a eomtmter learns trll Ctllre  . 
We do not claim that the algorithm described here models the hmnanl  ) rocess of language learning . Instead , the algorithm should , given unstructured sentences , find the best structure . 
This means that the algorithm should assign 1Opcnbam " Vcr voer hfformatie Systeeln ( OVIS ) stands for Pul ) licTranst ) orthfformation System . 
sl ; ru (' ture to sentences whi (: h are similar to the , ~; tru(:ture peot ) le would give to sentences , lint not ne (: essarily in the same lille or Sl ) ; ~( ; el'e-strictions . 
The algorithm (: on sists of two t ) hases . The tirst t ) hase is a constituent generator , whi ( :\] l generate sam ( )tiw~ted set of possible constituents 1 ) y aligning sentenc ( : s . These ( : on di ) hase re-stri ( : tstll is set l ) y selecting the best constituents from the set . 
The rest of this t ) aper is organized as ibl-lows . Firstly , we will start t ) y describing l ) revi-ous work in machine learning of languages tru ( :- ture and then we will give a descrit ) tion of the ABL algorithm . Next , some results of al ) t ) lying the ABL algorithm to different corpora will 1  ) e given , followed 1 ) y a discussion of the algorithm alia flltllre resear  (  ; h . 
2 Previous Work
I , e ; wning met l , o ( lscant ) e grouped into suitor-vised and unsut ) ervised nmthods . Sul ) ervised methods are initial seal with structured input  ( i . e . stru ( : ture ( \] sent ( m ( : es for grannnar learning methods )  , while mlsut ) ervised methods learn l ) y using mlstru ( : ture data only . 
In 1) ractice , SU l ) ervised methods outpert brmmls ut ) ervised methods , since they can adapt their output based on the structure dexami  ) les in the initial satont ) hase whereas unSUl ) ervised l nethod semmet . However , it is worthwhile to investigate ml supcrvised graml nar learning methods  , since " the costs of annotation are prohibitively time and ext  ) ertise intensive , and the resulting corpora may 1 ) e too suscet ) tible to re-stri ( : tion to a particular domain , apt ) lication , or genre " . ( Kehler and Stolcke , 1 . 999 ) There have 1 ) een several approaches to the unsupervised learning of syntactic structures  . We will give a short overview here . 

Memory based learifing ( MBL ) keeps track of possible contexts and assigns word types based on that information  ( Daelemans ,  1995) . Redington et al ( 1998 ) present a method that bootstrap syntactic categories using distributional information and Magerman and Marcus  ( 1990 ) describe a method that finds constituent boundaries using mutual information values of the part of speech ngrams within a sentence  . 
Algorithms that use the minimmn description length  ( MDL ) principle build grammars that describe the input sentences using the minimal nunfl  ) er of bits . This idea stems from int brnm-tion theory . Examples of these systems can be found in ( Grf in wald , 1994) and ( de Marcken ,  1996) . 
The system by Wolff ( 1982 ) pertbrms a heuristic search while creating and In erging symbols directed by an evaluation function  . 
Chen (1 . 995) presents a Bayesian grammar induction method , which is t bllowed by a post-pass using the insideoutside algorithm  ( Baker , 1979; Lari and Young ,  1990) . 
Most work described here cmmot learn complex structure such as recursion  , while other systems only use limited context to find constituents  . However , the two phases in ABL are closely related to some previous work  . 
T im alignment learning phase is et lb . ctively a compression technique comparat ) let o MDL or Bayesian grammar induction methods . ABL remembers all possible constituents , building a search space . The selection h ; arning phase searches this space , directed by a probabilistic evaluation function . 
3 Algorithm
We will describe an algorithm that learns structure using a corpus of plain  ( ml structured ) sentences . It does not need a structured training set to initialize  , all structural information is gathered from the unstructured sentences  . 
The output of the algorithm is a labelled , bracketed version of the inlmt corpus . Although the algorithm does not generate a ( context-fl'ee ) grammar , it is trivial to deduce one from the structured corpus  . 
The algorithm builds on Harris's idea ( 1951 ) that states that constituents of the same type can be replaced by each oth  , er . Consider the sen-
Wh , at is a family fare
Wh , a tist h , e payload of an African Swallow
Wh , at is & family fare ) x
Wh , at is ( the payload of an African Swallow ) x Figure 1: Example bootstrapping structure
For each sentence sl in the corpus:
For every other sentences 2 in the corpus :
Aligns ~ to s2
Find the identical and distinct parts between s ~ and  s2 Assign nonterminals to the constituents ( i . e . distinct parts of s ~ and s2 ) Figure 2: Alignment learning algorithm fences as shown in figure  1  . 2 The constituents a . family fare and the payload of an African Swal -low both have the same syntactic type  ( they are both NPs )  , so they can be replaced by each other . This means that when the constituent in the first sentence is replaced by the constituent in the second sentence  , the result is a wflid sentence in the language ; it is the second sentence . 
The main goal of the algorithm is to establish that a family  . f are and the payload of art , African Swallow are constituents and have the same type  . This is done by reversing Harris's idea : ' i1" ( a group o . f ) words car- , , be ; replaced by each other , they are constituents and h . aveth , esame type . So the algorithm now has to find groups of words that can be replaced by each other and after replacements ill generate valid sentences  . 
The algorithm consists of two steps : 1 . Alignment Leanfing 2 . Selection Learning 3 . 1 A l ignment Learning The model learns by comparing all sentences in the intmt corpus to each other in pairs  . An overview of the algorithm can betbund in figure  2  . 
Aligning sentences results in " linking " identical words in the sentences  . Adjacent linked words are then grouped . This process reveals 2All sentences in the examlfles can be fbund in the
ATIS corlms.
962.f , ' o , , , . Sa , , . F , ' a , . ci ., ' co(to Dallas).
./'rout ( Dallas to ) San Francisco 02 ( Sa ,, l . o ) Dallas02
O , Da Uas  #o  Sa , , . J ';' a, . cisco ) 2?\[;1"0~II, . fF()Ii't(SanFrancisco ), to(Dallas ) 2(Dalla . gj to(Sa , , . 
Figure 3: Ambiguous alignments 1 ; t1( ; groul ) S of identical words ,  1 ) ut it also llIlC ( )v-ers the groups of distinct wor ( ls in the sentences . 
In figure 1 What is is the identical part of the sentences and a fam  , i l y J ' a ~ v , and the payload of an A . /ricau , Swallow are the distinct l ) arts . The distinct parts are interchangeable , so they are ( teter milm d to 1 ) e constituents o17 the same I ; yl)e . 
We will now Cxl ) la in the stel ) s in the align-men learning i ) hase in more de , tail . 
3 . 1 . 1EditDistanceq\[bfind the identi ( : alword grouI ) S in ; he sentences , we use the edit ; distan ( : e algorithm by Wagner and Fischer ( 197 d : )  , which finds the minimum nmnl ) er of edit operations ( insertion , ( lelei ; ion and sul)st ii ; ul ; ion ) l ; o change one sen-te , nce into the other , ld ( mti ( : alwor ( ts in the sen-t ( races can 1 ) et ' ( mndat\] ) \] a (  ; esW\]l(~ , l'elie edit operation was al ) plied . 
The insl ; antia , tiol ~ of the algoril ; hm that fin(isl ; lelongest COlllllOllSlll ) S((\]ll(ll ( ; ( ~ , ill two Sell-tences sometimes " links " words that are  , too far apart , in figure 3 when : 1) esides the o(:cm'-rences of . from ,, theocem : rences of San4"au , ci . scoor Dallas are linked , this results in unintended constituents . We wouktr ; d ; her have the l nodel linking to , resulting in as l ; 1"u( ; I ; llre with the 110 1111 phrases groul ) ed with the same type corre ( :tly . 
Linking San Francisco or Dallas results i ~ l const ituents that vary widely in size  . This stems from the large distance between the linked urords in the tirsi  ; sentence midint h( ; s(:cond sentence . This type of alignlnent cant ) eruled out by biasing the cost fimction using distances between words  . 
3.1.2 Grouping
An edit distance algorithm links identical words in two sentences  . When adjacent wor ( ls are linked in l ) oth sentences , they can l ) e grouped . 
Agroul ) like this is a part of a senten ( : e that can also betb mM in the other sentence . (In figure 1,
What is is a group like this.)
The rest of the sentences can also be grouped.
The words in these grout ) sarm words that are distinct in the two sentences  . When all of these groups fl : om sentence , one would 1 ) erelflaced by the respective groups of sentence two  , sentence two is generated . ( a family f are and th , cpayload of an African Swallowart : of this type of group in figure  1  . ) Each pair of these distinct groups consists of possilfle constil  ; uents Of the same type . : ~ As can be , seen in tigure 3 , it is possible that empty groups can lm learned . 
a.l . a Existing Constituents
At seine 1 ) o in t it may be t ) ossible that the model lem'ns a co11stituent that was already stored . 
This may hal ) l ) en when a new sentence is compared to a senlaen (  ; e in the partially structured corpus . In this case , , no new tyl)e , is intro(hu:ed~lint the , consti ; ucnl ; in l ; he new sentence gel ; sl ; he same type of the constituent in the sentence in the partially structm:ed corpus  . 
It may event ) e the case that a partially si ; ruc-tured sentence is compared to another partial lysl  ; r t l c t l l r e (1 s e l l ; elR , e . This occm:s whel ~ as (: n-fence that ( ; onl ; ains some sl ; ructure , which was learner 11 ) yCOlnl ) aring to a sentel me in the par-t ; \] ally structure ( l ( ; Ol ; pllS ~ is ( ; Olllt ) ar(~ , (\] 1 ; o all-other ( t ) art ; ially stru(:ture(t ) sente , n(:e . When the (' omparison of these twose , nl ; ence , syields a constituent hai : was a h : e a ( lyt ) resent in both senten ( : es , the tyl)es of these constitueld ; S are merged . All constituents of these types are ut)-dated , so the , y have the same tyl)e . 
By merging tyl)es of constituents we make t ; he assuml ) tion that co\]l stil ; uents in a ( : ertain context can only have one tyl ) e . In section 5 . 2 we discuss the , imt)li (: atiolls of this as smnpl ; ion and propose an alternative at ) t ) roach . 
3.2 Selection Learning
The first step in the algorithm may at some point generate COll stituents that overlap with other constituents  , hi figure 4 Give me all flights . from Dallas to Boston receives two over-lal ) ping structures . One constituent is learned 3Since the alger Inn does not know any ( linguist ; c ) llal IICS for the types , the algerfinn chooses natural numbers to denote different types  . 
963  ( Book Delta 128 ) flwn Dallas to Boston ? ' Givem ? ( all . fligh , ts ) ' f , ' om Dallasto Boston )
Giveme ( help on classes ) l ? igure 4: Overlapping constituents by comparing against Book Delta  128 firm Dallas to Boston and the other ( overlapl ) ing ) constituent is tbund by aligning with Giveme help on classes  . 
The solution to this problem has to do with selecting the correct constituents  ( or at least the better constituents ) out of the possible con-stitnents . Selecting constituents can be done in several dittbrent ways  . 
ABL:incrAs sume that the first constituent learned is the correct one  . This means that when a new constituent overlaps with older constituents  , it can 1) e ignored ( i . e . they are not stored in the cortms ) . 
ABL:leaf The model corot ) rites the probability of a constituent counting then mn be r of times the particular words of the constituent have occurred in the learned text as a constituent  , normalized by the total number of constituents . 
Ple , f(c ) =\] c'CC:yield(c')=yicld(c)l
ICI where C is the entire set : of constituents.
ABL:braneh In addition to the words of the sentence delimited by the constituent  , the model computes the probability based on the part of the sentence delimited by the words of the constituent and its nonterminal  ( i . e . 
a normalised probability of ABL : leaf).
Pb , . ~ na ,   , ( cl root(c ) = r ) = ec : y/el(l ( , -') -- yield(c)A ; "1
Ic " c : , ' oot(c ") =
The first method is nonprobabilistic and may be applied every time a constituent is found that overlaps with a known constituent  ( i . e . while learning ) . 
The two other methods are probabilistic . The model computes the probability of the constituents and then uses that probability to select constituents with the highest probability  . These methods are ~ pplied afl ; er the aligmnent learning phase , since more specific informatioil ( in the form of 1 ) etter counts ) can be found at that time . 
In section 4 we willew fluate all three methods on the ATIS and OVIS corpus  . 
3.2.1 Viterbi
Since more than just two constituents can overlap , all possible combinations of overlapping constitueni  ; should be considered when com-Imting the best combination of constituents  , which is the product of the probabilities of the separate constituents as in SCFGs  ( cf . ( Booth , 1969)) . A Viterbi style algorithm optimization ( 1967 ) is used to etficiently select the best combination of constituents  . 
When conll ) uting the t ) r ( )t ) ability of a combination of constituents , multiplying the separate probabilities of the constituents biases towards a lown nmber of constituents  . The retbre , we comt mte the probability of a set of constituents using a normalized version  , the geometric mean 4 , rather than its product . ( Caraballo and Charniak ,  1998 )   4 Results The three different ABL algorithms m~d two  1  ) aseline systems have been tested on the ATIS and OVIS corpora  . 
The ATIS corlms ti ' om the P ( ; nn Treebank consists of 716 sentences containing 11  , 777 (: on-stituents . The larger OVIS corpus is a Dutch corpus containing sentences on travel intbrma-tion  . It consists of exactly 10,000 sentences . We have removed all sentences containing only one word  , resulting in a corpus of 6 , 797 sentences and 48 , 562 constituents . 
The sentences of the corpora a restript ) ed of their structures . These plain sentences are used in the learning algorithms and the resulting structure is compared to the structure of the original corpus  . 
All ABL methods are testentimes . Th ( , ABL:incr method is applied to random orders of the input corpus  . The probabilistic ABL methods select constituents at random when different combinations of constituents have the same probability  . The results in table 1 show the 4The geometric mean of a set of constituents ^ .   .   . A = VFI = , P ( d




NCBP 32 . 6O 82 . 70 83 . 24 (1 . 17) 81 . 42 (0 . 11) 8,  . 31 (0 . 01)
AT1SOVIS
NCBIZCSNCB\]) NCBRZCS 76 . 82 92 . 91 87 . 21 (0 . 67) 86 . 27 (0 . 06) 89 . 31 (0 . 01) 1 . J238 . 83 18 . 56 (2 . 32) 21 . 63 (0 . 5O ) 29 . 75 (0 . 00) 51 . 23 75 . 85 88 . 71 (0 . 79) 85 . 32 (0 . 02) 89 . 2 . 5 (0 . oo ) 73 . 17 86 . 66 84 . 36 (1 . \]0) 79 . 96 (0 . 03) 8 > o4 (0 . 0)) 25 . 22 48 . 08 30 . 87 (0 . 09) 42 . 20 (0 . 01) laJ)h , 1: Results of I ; he mean ; rod standard deviations ( between bra(:k-ets ) . 
The two base , line syst cn is , left and right , ontyt ) uiM left:midright brnnching trees respectively . 
Three , metrics hnve been compnl ; cd . NCBP stmlds for Non-(\] rossing Bra . (: kets Precision , which denotes the percentage , of learned (: on-stituents th ~ , t do not overlai ) with any con-sl ; it ; uent ; s in I ; hem'igi'n , al (: or pus . NCIH ~ is the Non-Crossing Bracketsll . e (: all mid shows ; het )( ; rt'ent ~ geof constituents in the original colt ) us thai ;   ( 1o not overlap with : my constituents in the learned  ( : of t ) us . Finnlly , Z(LS'strumsti)l'Zero-(Jrossing Sentences a , ndr (' , l ) reseuts heper-c(ml ; age of sentence , s that ( t (1 not have m ~ y over-lnt ) l ) ing const ii ; uenl ; s . 
4 . 1 Evaluation ' l-'t mincr mode t1 ) erfi ) rm s ( tuii : e well ( : on si ( hwing the t ' ~ m that it ;  (: ; mnotre(:ov(wt'roln in corre(:t(:()nstituents , with at ) re(:isiona , ndre(:~dl of()V ( ~ l'8 t )% . The order of these nl ; en (: eshow ( we , r is quite iml ) or bmt , since tiesl ; ml ( tard deviation of the inc'r model is quite high ( est ) e~ ( : i alty with the ZCS , reaching 3 . 22% on the OV!S (: or pus ) . 
We expected the prot ) nl ) ilistic nmtho ( ts to i ) er form t ) o , l ; ter , trotthelc((fmodet performs slightly worse . The , ZCS , however , is somewhat better , re , suiting in 21 . 63% on the AT1S corpus . Furthermore , d ; he standard deviations of the le , : fmodel(&lidOf the branch , model ) are c\]ose to 0% . The st;~tisti(:al methods generate more precise , results . 
Ttmbra'n , ch , modet dearly out l ) er for nlallo ~ , her models . Using more Sl ) e ( : itic statistics generate better results . 
Although the resull ; s of the NFIS (: or pusm MO VlS cor Ims differ , the , conclusions that (: ml ) ereached are similm : . 
ATIS and OVIS corpora 4 . 2 ABL Compared to Other Methods It ; is difficult to corot ) are the results of the ABL model ag~dnst other lnethods  , since , often d if thrent corpora or m (' , tricsm : e used . The methods describe , d by Pereira and Schabcs (1 . 9( . )2) comes reasonably close to ours . The unsupervised nmthodle ~ rnstructure on plain sentences from the ATIS corlms resulting in  37  . 35% precision , while the " un . supcrvised ABL signili ( : mltly outperforms this method , reaching 85 . 31% l ) re-cision . Only theirs ' uperviscd version results in n slightly higher pre  ( ' ision of 90 . 36 % . 
The syste , nlth ; d ; simt ) lybuihts right branch-ins structures results in 82  . 70% precision mid 92 . 91% tee M1 on the ATIS cortms , where ABL got 85 . 31% and 89 . 31% . This wa , sexpected , sin(:e English is a right ) rmmhing language ; a left branching sysl ; Clllt ) ( ~ rff ) l . ' ltle(lllllChwoFsc(32 . 60% pre(:ision and 76 . 82% rccnll ) . C(m-versely , right branching wouht not do very well on ~ , l ~ q ) mmse , corpus(~left1) r~m(:hinglan-gua . ge ) . Sin(:eA\]31 , does not have a 1 ) ref ( ~ renc ( ~ fi ) r direction built ; in , we exi)ect ABL to t ) ertbrm similarly on nJ a , t ) anese (: or pus . 
5 Discussion and Future Extensions 5 . 1 Recurson All ABL methods des ( ' ribed here can lem:n recursive structures and have been fom td when ~ t  ) plying ABI , to the NIl ? IS and OVIS (: or lms . 
As (: ml be sc(m in figure 5 , the learned recursive structure , is similm : to the , original structure . Some structure hast ) een removed to make it easier to see where the recurson occurs  . 
Roughly , recursive structures arc built in two steps . First , the algorithm generates the structure with difl ' cro  , nt nonterminals . Then , the two nonq ; ermim ds are merged as described in so , el ; ion 3 . 1 . 3 . The merging of the nonterminals m~y occur anywhere in the cortms  , sin(:eall merged non-terndnals are ut ) dated . 
965 learned original learned original
Please ez plain the ( field FLTDAY in the ( table ) is ) is Please explain ( the . field FLTDAY in ( the table ) NP ) NpEx plain classes QW and ( QX and ( Y ) a2 ) ~' eExplain classes ( ( QW ) N p and ( QX ) NI , and ( Y ) NP ) NP Fignre 5: Recursive structures learned in the ATIS corpus
Show me the ( morning ) x flights
Show me the ( nonstop)x fli . qhts
Figure 6: Wrong syntactic type 5 . 2 Wrong Syntactic Type In section 3 . 1 . 3 we made the assumt ) tion that a constituent in a certain context can only have one type  . This assumption introduce some problems . 
The sentence John likes visiting relatives illustrate such a problem  . The constituent visiting relatives can be a noun phrase or n verb phrase  . 
Another prol ) lemisil hlstrated in figure 6.
When applying the ABL learning algorithm to these sentences  , it will determine that morning and nons to pare of the same type  . Unt brtu-nately , morning is a noun , while nonstop is an adverb ) A fixture extension will not only look at the type of the constituents  , lint also at the context ; of the constituents . I i 5 the example , the constituent morningnlay also take the t ) lace of a subject position in other sentences ~1 ) ut the constituent nonstop never will . This intbrnm-tion can be used to determine when to merge constituent types  , efl'ectively loosening the as-sunlption . 
5.3 Weakening Exact Match
When the ABL algorithms try to learn with two conlpletely distinct sentences  , nothing can be learned . If we weaken the exact match between words in the alignment step of the algorithm  , it is possible to learn structur even with distinct sentences  . 
Instead of linking exactly matching words , the algorithm should match words that are equivalent  . An obvious way of implementing this is by making use of cquivalence classes  .   ( See 5Harris's implication does hold in these sentences . 
nonstop can also be replaced by for example cheap ( another adverb ) and morning can be replaced by even in . q ( another noun ) . 
for example ( Redington et al , 1998) . ) The idea 1 ) ehind equivalence classes is that words which are closely related are grouped together  . 
A big advantage of equivalence classes is that they can be learned in an unsupervised way  , so the resulting algorithm remains nnsui ) ervised . 
Words that are in the same equivalence class are . said to be sufficiently equivalent , so the aligmnent algoritlnn may assunm they are sin > ilar and may thus link them  . Now sentences that do not have words in common , but do have words in the same equivalence class in common  , can be used to learn structure . 
When using equivalence classes , more constituents are learned and more terminals in con-stitnents may l  ) e seen as similar ( according to the equivalence classes )  . This results in a much richer struct m'ed corlms . 
5.4 Alternative Statistics
At the moment we have tested two difl brent ways of computing the probal  ) ility of a constituent : ABL:leaf which computes the t  ) rot ) -ability of the occurrence of the terminals in a constituent  , and ABL:b' , ' anch which coml ) utes the probability of the occurrence of ; 11( ; terminals together with the root nonterminal in a  ( -onstitueut , based on the learned corpus . 
Of course , other models can bcimt ) lemented.
One interesting possibility takes a DOP-like approach  ( Bod ,  1998) , which also takes into account the inner structure of the constituents  . 
6 Conclusion
W c have introduced a new grammar learning algorithm based  055 c ( )mparing and aligning plain sentences ; neither prelabelled or bracketed sentences , nor pretagged sentences arc used . It uses distinctions between sentences to find possible constituents and afterward selects the most probable ones  . The output of the algorithm is a structured version of the corpus  . 
Byl;aking entire sentences into account , the context used by the model is not limited by window size  , instead arbitrarily large contexts are learn recursion  . 
~\[' ln'eeditl'erent instances of the algorithm have l  ) eenal ) t ) lied to two corpora of differ-eat size , the ATIS corpus ( 716 sentences ) and the OVIS corpus ( 6 , 797 sentences ) , generating promising results . Althought ; he OVIS corpus is almost tent ; imes the size of the ATIS corpus , these corpora describe a small conceptual domain . We plan to ~ l ) t ) ly the ~ flgori ~ hms to larger domain corpora in the near fllture  . 
References
J . K . Barker .  1979 . Trainabh ; grammars for speech recognition . In J . J . Wolf and 1) . H . 
Klatt , editors , Speech , Communication Papers for the Ninety-seventh Meetin  . q of the Acoustical Society of America , pages 547-550 . 
R , ens Bod .  1998 . Beyond Grammar An_F , zpcric ncc-Bascd Th , eory of Language . Stan-
Jbrd , CA : CSLI Publications.
R . . Bonnema , R . Bod , and R, . Scha .  1997 . A DOP model for semanticiil ; ertn'el ; ation . In Proceedings of the Association for Compu ~ tational Ling'aistics/Eurw pean Ch  , apter of th , cAssociation for Computational Linguistics , Madrid , p ~ ges 159 167 . Sommer set , NJ : As-soci ~ tiont br Compul ; ational Linguistics . 
T . Booth .  1969 . Probal ) ilistic representation fformed languages . In Co ' ~@' , rcn cell , cco'rdo / "1959" lEnth , Annual Symposium on , 5'witcl~ , in . q and Automata Theory , pages 748:1 . 
Sharon A . Caraballo and Eugene Charniak.
1998 . New figures of merit for bestfirst prol )- abilist ; ie chart parsing . Computational Linguistics , 24(2):275298 . 
Stanley F . Chert .  1995 . Bayesian gralm nar induction for language modeling  . \]: ll Proceedings of the Association J ' or Computational 
Linguistics , pages 228235.
Walter Daelemmls .  1995 . Memory-based lexical acquisition and 1) rocessing . In P . Stefh ; ns , editor , Mach , in c Translation and the Lexicon , vohm m 898 of Lecture Notes in Artificial Intelligence , pages 8598 . Berlin : Springer Verlag . 
Carl G . de Marcken .  1996 . Unsupervised Lan-g'aage Acquisition . I ) h . D . thesis , Departnmnt of Electrical Engineering mid Comtmter Science  , Massachusetts Institute of Technology , 
Cambridge , MA , sep.
Peter Oriin wald .  1994 . Anfinim mnde , scription lengl ; happroach to grammar inference . In G . Scheler , S . Wernter , and E . R , iloif , editors , Connectionist , Statistical and S!pnbolic Approaches to Learning for Natural Language  , vohnne 1004 of Lecture Notes in dl ~ pages 203-216  . Berlin : Springer Verlag . 
Zellig Harris .  1951 . Methods in Structural Linguistics . Chicago , IL : University of Chicago

Andrew Kehler and Andreas Stoleke . 1999.
Preface . In A . Kehler and A , Stolcke , editors , Unsuper'viscd Learning in Natural Language Processing  . Association for Comlmta-tional Linguistics . Proceedings of the workshop . 
K . Lari and S . J . Young .  1990 . The estimation of stochastic on text-free grammars using the insideoutside ~ dgorithm  . Computer
Speech and Language , 4:3556.
\]) . Magerman and M . Marcus .  1990 . Parsing natural language using mutual in tbrma -tion statistics  . In Pwcecdin . qso , fth , e National Con . fcrcnce on Artificial Intelli . qence , p~ges984989 . Cambridge , MA : MIT Press . 
M . Marcus , B . Santorini , and M . Marcinkiewicz . 
1993 . Building a large annotated corpus of english : the Penntr  (  , ebank . Computational
Linguistics , 19(2):31.3330.
F . Pereira and Y . Schgd)e , s .  1992 . Inside-outside reestimation fl : ompm:tially t ) racketed corpora . Inl'r occ c dings of th , cAssociation for Computational Lin . quistics , pages : 128-135,
Newark , Debm~are.
Martin Redington , Nick Chater , and Steven Finch .  1998 . Distrilmtional information : A power flfl cue for acquiring synt  ; actic categories . Cwnitiv c Science , 22(4):4:25469 . 
A . Viterbi . t967 . Errorbmmdsfor convoh > tiona \] codes and an asymptotically ol  ) timum decoding algorithm . Institute of Electrical and Electronics Engineers Transactions on 
Information Th , cory , 13:260269.
Robert A . Wagner and Michael J . Fischer.
1974: . The string-to-string correction problem . Journal of th , e Association for Computing Machinery ,  21(1):168-173 , jmLJ . G . Wollf .  1982 . Lmlguage acquisition , data compression and generalization . Langv , agc ~
Communication , 2:57-89.

