Hopfield Models as Nondeterministic Finite State Machines 
Marc F . J . Drossacrs
Computer Science Department,
University of Twente,
P . O Box 217, 7500AEEnschede,
The Netherlands,
email : mdrssrs@cs.utwente.nl.

T be use of neural networks for integrated linguistic analysis may be profitable  . This paper presents the first results of our research on that subject : a Hopfield model for syntactical analysis  . We construct a neural network as an implementation fabounded pushdown automaton  , which can accept contextfree languages with limited center-embedding  . The net-work's behavior can be predicted a priori  , so the presented theory can be tested . The operation of the network as an implementation f the acceptor is provably correct  . Furthermore we found a solution to the problem of spurious states in Hopfield models : we use them as dynamically constructed representations of sets of states of the implemented acceptor  . The socalled neural-network a ceep to r we propose , is fast but large . 
1 Introduction
Neural networks may be well suited for integrated linguistic analysis  , as Waltz and Pollack\[10\] indicate . 
An integrated linguistic analysis is a parallel composition of several analyses  , such as syntactical , semantical , and pragmatic analysis . When integrated , these analyses constrain each other interactively  , and may thus suppress a combinatoric explosion of sentence structure and meaning representations  . 
This paper presents the first results of our research into the use of neural networks for integrated linguistic analysis : a Hopfield model for syntactical nalys is  . 
Syntactical analysis in the context of integration with other analyses boils down to the decision whether a sentence is an element of a language  . A parse tree is superfluous here as an intermediary representation  , since it will not be finished before the complete integrated analysis is  . This fact allows us to deal with the problem of a restricted length of the sentences a neural par  .   .   .   . n h a n d l e , s e e e . g .  \[51 , \[7\] , a problem that could not be elegantly solved , see e . g .  \[6\],\[3\] . 
In this paper we propose a formal model that recognizes syntactically correct sentences  ( section 2 )  , atlop field model on to which we want to map this for-mar model  ( section 3 )  , the parameters that makes the network operate as intended  ( section 4 )  , and a way to map the formal model onto the Hopfield model  , including a correctness result for the latter ( section 5 )  . The theoretically predicted behavior of the so ~ obtained network has been verified  , and a simple example provides the taste of it ( section 6 )  . We alas consider complexity aspects of the model ( section 7 )  . 
Section 8 consists of concluding remarks.
2 A Bounded Push-Down Automaton
Although it is not ant . ~tablished fact , it is as-stoned here that natural languages are contextfree  , and consequently that sentences in a natural language can be recognized  , by a pushdown att to ma-t on ( PDA ) . i lowever , we are not interested in modeling the competence of natural anguage users  , but in modeling their performance . The human performance in natural language use is also characterized by a very limited degree of center-embedding  . In terms of PDAs this means that there is n bound on the  , number of items on the stack of a PI ) A for a natural anguage . A bounded pushdown automaton M = ( Q , Y~ , I' , 6 , q s , Zo , F ) is a PDA that has an upper limit kE ~ on the number of items on its stack  , i . e . H~<k for every instantaneous description ( ID )   ( q , w , a ) of M . The set of stack states of this PDA is delined to be:QST=-aI  ( qo , w , Zs)P*~t(q , e , ?~) . 
QsT is finite : IQsT\[<_(IFI ) ~ , therefore we may define a nondeterministic finite ~state aeceptor  ( NDA ) M ~ that has QST ms its set of states . 
The class of PDAs of which we would like to map bounded versions onto NDAs is constrained  , among others to the class of v-free PDAs . By this constraint we anticipate the situation that grammars are stored in a neural network by self-organization  . In that sit-nation a neural network will store e -productions only if examples of applications of e -productions are repeatedly presented to it  . This requires e to have a representation i the machine  , in which case it fails to accommodate its definition  . 
Another restriction we would like to introduce is to grammars in  2-standard form with a minimal number of quadratic productions : productions of the form ACrEs DE  COL~G-92  , NANTES , 2328 AOOT1992 II3 PROC . OFCOLING-92, NA~rEs , AIJG .  2328 ,   1992 A ~ bCD where b is a terminal and C and D are variables  . Such a grammar can be seen as a minimal extension of a right-linear grammar  . Within such grammars , quadratic productions provide for the center -embedding  . Since such grammars have a minimal number of quadratic productions  , acceptance by a PDA defined for such grammars requires a minimal use of  ( stack ) memory , and titus generates a mini-realQs'r . To maintain this minimal use of memory a restriction to one-state PDAs that accept by empty stack is also required : when a PDA is mapped onto an NDA  , the information concerning its states is h)st , unless it was stored on the stack . 
Ane-free , one-state PDA that simulates a contextfree grammar in  2-standard form with a minimal number of quadratic productions  ( and that accepts by empty stack ) satisfies all our criteria . For every such PDA we can define an NDA , for which we can prove \[4\] that it accepts the same language ms the
PDA does.
Definition 2.1
Let M = (* , Z , I' , 6 , * , Zo , ~) be an e-free bounded PDA with hound k . An NDA defined for M is an NDAM '= ( Q' , Y2' ,  6' , Q'o , f ") such that :
Q '= QST , as defined above;
E ' = E ; 6': QsTxE~2osT is defined by : ~'(~ l , ~) = ~ I(* , ~ , ~)  ~ M( , , w , ~)
Q~:z0; and
F ' = el(empty stack).
Theorem 2.2 ( correctness of the NDA )
Let M be an e . free one-state PDA with bound k , if M ' is an NDA a . ~ defined in definition 2 . 1 , then M accepts a string by empty stack tf and only if M ' accepts it by accepting state  . 
In as far as a natural language is contextfree , we claim that there is an instance of our a eeep tor that recognizes it  . 
3 An Input-Driven Sequencing
Hopfield Model
In this section a noiseless Hopfield model is proposed that is tailored to implement NDAs on  . The model is based on the associative memory described by Buhmann et al  \[2\] and the theory of delayed synap~s from \[8\]  . We chose the Itop field model because of its analytical transparency  , and its capability of sequence traversing , which agrees well with the sequential nature of language use a tap be no menologica\[level  . The Hopfield model proposed is a memory for temporal transitions extended with externa-input synapses  . Figure 1 shows the architecture involved . 
Ill this network only those neurons are active upon which a combined local field operates that transcends the threshold  . The activity generated by such a lo~calfield is the active overlap of the temporal image of past activity provided by so called temporal synapses  , and the image of input external activity provided by socalled input synapses  . By the temporal synapses , this activity will later generate another ( subthresh-old ) temporal image , so network activity may be considered a transition mechanism that brings the network from one temporal image to another  . Active overlaps are unique with high probability if the activity patterns are chosen at random and represent lown rean network activity  . This uniqueness make stile selectivity of the network very plausible : if an external activity pattern is presented that does not match the current temporal image  , then there will no the activity of any significance  ; tile input is not recognized . 
When an NDA is mapped onto this network , pairs of NDA-state q and input-symbol x , such that 6(q , x)y ? ~ , are mapped onto activity patterns . Temporal relations in the network then serve to implement NDA transitions  . Note that single NDA transitions arc mapped onto single network transitions  . 
This results in complex representations of the NDA states and the input symbols  . An NDA state is rapresented by all activity patterns that represent a pair containing that state  , and input patterns are represented by a componentwise OR overall activity patterns containing that input symbol  . A consequence is that mixed temporal images , the subthresh-old analogue of mixture states , are a very natural phenomenon in this network , because tile temporal image of an active overlap comprises at least all activity patterns representing a successor state  . But this is not all . Also the network will act as if it implements the deterministic equivalent of the NDA  , i . e . it will trace all paths through state space the input allows for  , concurrently . The representations of the states of this deterministic finite-state automaton  ( FSA ) are dynamically constructed along the way ; they are mixed temporal images . The concept of a " dynamically constructed representation " is borrowed from Touretzky  \[9\]  , who , by the way , argued that they could not exist in the current generation of neural networks  , such m sl top field models . 
A time cycle of the network can be described as follows:  1~ The network is allowed to evolve into a stable activity pattern that is the active overlap of a temporal image of past activity  , and the input image of external input for a pe ~ ' iodtr  ( = relaxation time )  , when an external activity pattern is presented to the network  ;  2 . After sometime the network reaches a state of stable activity and starts to construct a new temporal image  . It is allowed to do this for a period t , (= active time ) ' ,  3 . Then the input is removed , and the network evolves towards in activity . This takes again about a period t ~ ; Acids DECOLING-92 , NANTES , 2328 AOt ~ T1 992 114 PRO(: . OFCOL\]NG-92, NANTI~S , AUG .  2328 .  1992 4 . Not before a period ta ( = delay time ) has passed , a new input arrives . The new temporal image is forwarded by tiles low synapses during a period ta+iv  , starting when tdends . The slow synapses have forgotten the old temporal image while the network was in its td  . 
The synapses modeled in the network collect the incoming activity over a period ~+ tr  , and emit the time average over again a period ta + trafter having waited a period ta+t ~  . In the network this is modeled by computing a time average over prior neuronal activity  , and then multiply it by the synop-ticefficacy . Tile time average ranges over a period ( 2t ~+ la+3 1 ~ ) /N - ( l , +I ~)/ N . The first argument is the total time span in the network  , covering two active periods and an intervening delay time  , including their transition times . The second argument is tile current period of network is activity  , activity that cannot directly interfere with the network's dy-na\[lliCS  . 
More formally the network can be described as follows:  5'i   ( 5 0 , 1 , where i = 1 .   .   .   . , N , l1 if h~(t ) > U . ')' i(t+l ): 0 if hi(t ) < U , h~(t):h~(t ) + hl(t) , 
Nj = l the temporal transition term , tPu all-a ) Nj z ~= l ""- a )(~?~'- a) , with J .  = 0 , ~( t ) ~ rO-0L ~ Sj(t-t ') w(t ') d t ' , where 7~- if O < t < r w(t ) = 0 otherwise , h~(t ) = x~(s , '( t)-a ) , the external input term , The Si are neuronal variables (5" , ' . is a neuron in another network ) , hi is the total input on Si , U is a threshold value which is equal for all Si , Jij is the synaptic efficacy of the synapse connecting Si to Si  , and A is the relative magnitude of the synapses . 
The average at time ~ is expressed by ~/(~) , where r =-(2t . +ta+3tr)/N and ~-~( ta+t ~)/ N . The function w ( t ) determines over which period activity is averaged  . The input synapses are nonzero only in case i = j  . These synapses carry a negative ground signal-A ' a  , which is equivalent to an extra threshold generated by the input synapses  . The activity patterns ~'(~"~(~\], ~ .   .   .   .   . ~ N )) are statist , tally independent , and satisfy the same probability distribution as the patterns in the model of Buhmann et al  .  \[2\]:  .   .   .   .   .   . -(1-a ) b(~) , where l if x : ~0 ~( x ) ~ 0 otherwise . 
If a ?? the pattern is biased . For N-~co , I/N ~ N=I~'--*a . The updating p .   .   .   . is a Monte
Carlo random walk.
DII ~ I
Figure 1: 7' he model for N = 3 . Usually HopJield models consist of very many neurons  . The arcedar rows denote temporal synapses . The straiyhtal T ' ows denote input synapses . 
4 Estimation of Parameters
A number of system parameters need to be related in order to make the model work correctly  . 
Timing ; is fairly importantill this network . Tile time the network is active ( to ) should not exceed tile delay time t ~ . If it does then ta + lr > ta + tr , and since no average is computed over a period ta +tr back in time  , not the fldl time average of the previous activity need to be computed  , consequently we choose ta < ta . 
The choice for a transition time tr depends on tile probability with which one requires the network to get in the next stable activity state  . This subject will be dealt with in section 7 . 
In the estimates of At and the storage capacity below  , an expression of the temporal transition term in terms of the overlap parameter moisused  , which will be introduced here first . The overlap parameter rnP ( t ) measures the overlap of a network state S~_ ( $1 , $2 ,  . .  .   , SN ) rattime t with stored pattern ~ P , and is delined by : N1~(~'- . ) s ', ( t ) . mo(t ) = ~- N=Im"Ei-a,I-a\] . The expression for the temporal transition term is: 
Nh ~( t ) = ~ slj ~ j (1).
j = l
A caq is OECOLING-92 , NANTES , 2328 ao ~ r1992115PROC . Of COLING-92, N^N Tgs , AUG .  2328 ,   1992 Assuming that N --* cowhile premains fixed this is  , after expansion of the J q and ignoring in finite simal terms  , approximated by:
Atp N h : ( t ) - a ( 1- a ) NZ ( ~+ l-a ) Z ( ~ f-a )   ; j ( t ) t ~= lj = ltI?A ' ~ , - . tf ? + , = (1-a ) , = z'-~l'"-a)t~l"(t ) , where r-O ~ o ? ? m " ( t ) ~ ~ m " ( t-t ' ) w ( t' ) d t ' . 
If the temporal image is ~? ~ then h ~ is about ( Nco ) : h  ~ ( t )  = ~ , ( ~+ l_a)(r_O)w(t ) dt . 

If a number of patterns in a mixture state have the same successor  , that pattern may be activated . 
To prevent his A ~ will be chosen such that the slow synapses do not induce activity in the network autonomously  , not even if all the neurons in the network are active  . On average , the activity in the network is given by the parameter a  . The total activity in a network is a quantity x such that z =  1/a   , so what we require is that x h~<U , i . e . that : a ~( ~+ ~- . )( r -0) w(t ) d t < U . 

The interesting case is ( i "+1 = 1 . Since the integral is at most O/ ( r-O ) which is the strongest condition on the left side  , the left expression can be written as ) d ( 1- a ) / a . It was earlier demanded that only a combined local field can transcend the threshold  , which implies that external input . ~ e(1-a ) < U , so we can take A~<A~a safely . This is small because a is small . 
Next a value for the threshold that optimizes to r -age capacity is estimated by signal-to-noise ratio analysis  , following\[1\] and\[2\] , for N , p - -* o o . Temporal effects are neglected because they effect signal and noise equally  . It is also assumed that external input is present  , so that the critical noise effects can be studied  . In this model the external input synapses do not add noise  , they do not contain any information apart from the magnitude of the incoming signal  . 
Now suppose the system is in state S = ~ . The signal is that part of the input that excites the current pattern : s = A '  ( ~ , ~- a ) . 
The noise is the part of the input that excites other patterns  . It can be seen as a random variable with zero mean and it is estimated by its variance : ~ tv ' ~- awherea = p/N  . We want that given the right input hi > U , if both the temporal and the external input excite Si  , and that hi < U if the temporal input does not excite Si  . This gives signal-to-noise ratios :
P t =(, V + A t ) (1-a)-U , and
U+,Va-,~'(1-a)
PO = , vvCg ~
Recall is optimal in case Po = Pl which is true for a threshold : 
Uor , , = At(1-a ) +, V(?-a).
Substituted in either P0 or Pt it results in Pore = lT his result is the same as obtained by Buhmann et al  \[2\]  , and they found a storage capacity ctc . ~-( ainu ) -1 where ac = prna , :/ N . The storage capacity is large for small a , so a will be chosen a << 0 . 5 . A last remark concerns an initial input for the network  . In case there has been no input for the network for some time  , it does not contain a temporal image any more , and consequently has to be restarted . 
This can be done by preceding a sequence by an extra strong first input  , a kind of warning signal of magnitude e . g . A "+ At . This input creates a memory of previous activity and serves as a starting point for the temporal sequences stored in the network  . 
5 Neural-Network Acceptors
In this section it is shown how NDAs from definition  2  . 1 can be mapped onto networks as described in sections  3 and 4  . Such networks can only be used for cyclic recognition runs  . Where " cyclic " indicates that both the initial and the accepting state of an NDA are mapped onto the same network state  . If this were not done , the accepting state is not assigned an activity vector  , since no transition departs from it , see definition 5 . 2 below . Cyclic recognition in its turn can only be correctly done for grammars that generate end-of -sentence markers  . Any grammar can be extended to do this . 
An NDA is related to a network by a parameter list . 
Definition 5.1
Let M = ( Q ,  , 6, Qo , F ) be an NDA . A parameter list defined . for an NDAM is a list of the form ( a , A  ~ , At , N , p , prnaz , ta , td , tr , U ) , where : 1 . a6\[0,1\]c~t ; 2 . t~<td , with ta , td6~I ; 3 . A ~ 6 ~+ where ~+= xxE  Ax > 0; 4 . 0 < At <, Va ; 5 . p = ~ q . q'eQIx6~Clq'e~f(q , x)Ix
IY 6~, 16 ( q ', y ) #0 I ; 6 . Pmax > ~ P ; 7 . N > (- alna ) p . . . . . ; Ac-t ~ DECOLING-92 , NANTES , 2328 AO6"r1992116 PROC . OFCOLING-92, NANTES , AUG .  2328 .  1992 8 . G : see section 7; 9 . U = M(1-a ) + M(?-a) . 
Note that the rear can infinite number of parameter lists for each NDA  . 
The mapping of an NDA onto a network starts by mapping basic entities of the NDA onto activity patterns  . Such a pattern is called a code . 
Definition 5.2
Let M = ( Q , E , ti , Qo , F ) hean NDA , let t " : ( a , A e , M , N , p , pma , ,ta , ta , tr , U ) be a parameter list defined according to definition  5  . 1 . The coding\[unc-tionc is given by : c : QxE ~ 0 , 1N , such that for qEQ , and xE:E:1 . if 6(q , x ):/:0: c(q , x ) = ~ , where ~ i is chosen at random from 0 , 1 with probability distribution r' ( ~ ) = a6 ( ~- I )  +  ( t-a ) ~ ( ~ ,  )  , a . d2 . undefined otherwise . 
The set of codes is then partitioned : into sets of activity patterns corresponding to NDA states  , and into sets of patterns corresponding to input symbols  . 
Definition 5.3
Let M = ( Q , E , 6 , Qs , I ") I rean NDA , let P = ( a , M , At , N , p , PmaJ : , In , td , tr , U ) be a parameter list defined according to definition  5  . 1 . 
The setP q of activity patteT~s for qEQ is:
G = c(%z ) lxe ~ .
The set P :: of activity patterns for " xEE is:
P ,= c(q , x ) IqEQ.
Then a network transition is defined ms a matrix operator specified by the network'storage prescription  , and related to NDA transitions using the previously defined partition of the set of codes  . 
Definition 5.4
Let M = ( Q , ~ , 6 , Qo , F ) b can NDA , let P = ( a , M , At , N , p , Pma * , ta , td , G , U ) be a parameter list defned according to definition  5  . 1 . and let a bean N-dimensional vector , with each component a . The set Tr of network transitions is:
Att
Tr = d(?,q,x)\[d(q ', q,x ) = a(1-a)N
E(c(q ' , y)-a)(c(q , x)--a ) TAc(q ' , y)ePu , qtE~(q , x ) , where each jt is an NxN matrix . 
This suffices to define a neural onetwork acceptor.
Definition 5.5
Let M = ( Q , E , 6 , Qo , I ") be an NDA , let P = ( a , A * , A * , N , p , Pmax , to , td , tr , U ) be a parameter list defined according to definition  5  . 1 . A neural-network acceptor ( NNA ) defined for an NDAM that takes its parameters from P is a quadruple H =  ( T , f , U , S ) , where : 1 . tim topology T , a list of neurons per layer , is : ( N ); 2 . the activation fimction f is given by : Si = 1 if E~J/j . ~ j + A ' ( S~-a ) > U ( if EjJej ~ J+A' ( ' d~-a ) <- U'3 . the update procedure is a Monte Carlo random walk . 
4 . the synaptic coefficients are given by : ' It = : Ejl  ( q ,  . q . ~) CTr , and j e . : A~I where I is the identity matrix . 
In order to construct activity patterns that can servexs external input x for the network a componentwise OR operation is performed over the set P ~ as defined in definition  5  . 3 . 
Definition 5 . 6  7he OR operation over a set t ? of activity patterns is specified by:  1  . o a(~v,U ) = OR (~/', ~); ~ . oit(?, .   .   . , W ') : ~ of t . ( K ', ( ) lt(~q .   .   .   .   . ~")); ~ udaOR(P ~) = OR(~~ .   .   .   .   . C ') if ? e~=W .   .   .   .   .  ~" , At last a formal definition can be given of a temporal image a  . s the set of all activity patterns for which there is a network input that makes such an activity pattern the network's next qua  . st-stable state . 
Definition 5.7
Let M = ( Q , E , 6  , Qc , F ) I ) e an NDA , let P = ( a , M , M , N , p , p ,   .   ,   ,   , t , , ta , it , U ) be a parameter list defined according to definition  5  . 1 , and let H = ( T , f , U , S ) be an NNA defined according to definition 5 . 5 that takes its parameters from P . A temporal image is a set : c(q , x ) \[ input OIt ( P  ~ ) for H implies m ? ( q ' ~ ) = 1--a , a set Pq , is a temporal image of a quasi-stable stale S = c  ( q , x ) of H if and only if J~q , , q , ,:) is a transition of H . 
Now that we have a neural-uetwork acceptor , we triay also Wa hl to use it to judge the legality of strings against a given grammar with it  . 
ACTE's DECOLl NG-92 , NANTES , 2328 AO~I'1992117I'ROC . OVCOLING-92, NANTES , AUCl .  2328, 1992
Definition 5.8
Let M = ( Q , E , 6 , Qo , F ) be an NDA , let P--(a , M , At , N , p , Pm~x , is , Gt , G , U ) be a parameter list defined according to definition  5  . 1 , let H = ( T , f , U , S ) be an NNA defined according to definition 5 . 5 that takes its parameters from P , and let ai EE , qEQ 0 , and q'EF . H is said to accept a string w = al """ an if and only if Hevolves through a series of temporal images that ends at Pq  , if started at P q , if OR(P ~) ,   .   .   .   , OR(P a , ) appears as external input for the network . 
Next the correctness of an NNA is to be proven.
Since an NNA is essentially a stochastic machine , this raises some extra problems . What we propose is to let various network parameters approach values for which the NNA has only exact properties  , and prove that the uetwork is correct in the limit  . Those " various network parameters " are defined below  . 
Definition 5.9
Let M : ( Q , E , 6 , Qo , F ) hean NDA , let P = ( a , M , M , N , p , P , na , , , ta , t , t~t ,  . , U ) be a parameter list defined according to definition  5  . 1 . A list of large parameters i a parameter list P such that :  1  . a = _cl/N where cl < < N is a constant ; 2 . ~ ~ c~N/cl , where c2 is a small constant ; 3 . 0 < M < A ~ a , i . e . 0 < M < c2; 4 . Pm,~"---alna)-lN ; 5 . N ~ oo ; 6 . t ~ lN ~ oo . 
The following lemma states that for neural-network acceptors that take their parameters from a list of large parameters both the probability that the network reaches the next stable state within relaxation time  , and the probability that only the patterns that are temporally related to the previous activity pattern will become active  , tend to unity . Essentially it means that such networks operate exactly as prescribed by the synapses  . Such networks are intrinsically correct . 
Lemma 5.10 intrinsical correctness
Let M = ( Q , ~2 , 8 , Qo , F ) be an NDA , let P = ( a , A ' , ~ t , N , p , pm , :: , ta , ta , tr , U ) be a parameter list defined according to definitions  5  . 1 and 5 . 9 , and let H = ( T , f , U , S ) be an NNA defined according to definition 5 . 5 that takes its parameters from P , then H is such that : 1 . for all neurons Si in H , P ( SI is selected ) ~1 during network evolution ; and 2 . /or all act wity patterns c ~ U pu , y ~ Qu Z , if l ~7?" , then P ( ~ , ~ = ~=1) ~ O , where i = l ,   .   .   . , N . 
Then the correctness of an NNA follows.
Theorem 5.11 ( correctness of the NNA )
Let M = ( Q , Z , 6 , Q0 , F ) be an NDA , let P = ( a , A ' , At , N , p , p , , o ~ , t , , , ta , G , U ) be a parameter list defined according to definitions  5  . 1 and 5 . 9 , let H = (71 , f , U , S ) be an NNA defined according to definition 5 . 5 that takes its parameters from P , and let wEE + , then the probability that M accepts a string w if and only if H accepts w  , tends to unity . 
The proof of the theorem is given in \[4\].
6 Simulation Results
As an ' example we constructed au NNA that accepts the language generated by a grammar with productions : 
S ' ~ the BE ,
B~naanSV\[womanSV\[babySV\[mau VJ woman VI bahy V  , 
S ~ the B ,
E ~ t ,
V ~ saw I cried I comforted.
It takes its parameters from the llst : (0 . 05,1 . 5,0 . 07,800,64,6 . 68N,5,5,5,1 . 46) . It was tested with tile sentence " . tile baby the woman comforted cried . " The preceding fulls top is a first input that a waken stile network  . The graph below shows the time evolution of the network  . 
c(BS , wmm)cS~VR . ~ by ) ? Cave)c(SVVE , ~) c ~ VVE , ~) c(VVV r~coma)c(VVVR~d)~(VVV~w) , ~ vVl~xaD/~~vvE , ~ w)c(S' . ~)0i2~d~67

Plot of the system dynamics.
7 Complexity Aspects
If a neural-network acceptor h ~ . stoprocess a sequence of n input patterns , it ( worst ease ) first has to construct its initial temporal image  , when a wakened by an initial input ( that is not considered a part of the sequence )  , and then has to build n further temporal images . The time required to process a string of AC'rE . SDECOLING-92 , NaNTEs , 2328 A or ~ r19921 I8P gOC . OFCOLING-92, NA WrES . AUG .  2328 ,   1992 length u as a function of the length of the input sequence is thus  ( T--O ) (n + 1 )  . The constantral ~ depends ot it, . which is chosen to let the network satisfy a certain probability that it reaches the next state in relaxation  . This probability is given by (1- . ~/) o where B = tr/N . The time complexity of the neural-network acceptor is O  ( n )  . 
The upper limit on the number p of stored temporal relations between single activity patterns is \[ Q  1:~ xI ~3   \[2  . The number of neurons in a network is then cxQ \] e?\]E  17  , where e depends on the storage capacity and the chosen  ( low ) probability that selection errors occur . The randomly chosen activity patterns overlap , so if a large number of patterns is active they may constitute  , by overlap , other unselected activity patterns that will create tlreirown causal consequences  . This is called a selection er = rot . The probability that this can happen can be estinrated by l ' ~  ,  . , ,, ~( n)-~ .  1 - 1'( , '? ,  = 0) , where the latter is : (- l-2np'1 . -2,,i ) ~ In this express i .   .   .   . P ----( .   .   .   .   . ,)/ v wh .   .   .   .  -_- (~ ) ,   , : is then mnber of activity pattern stored in the network  , and m is the number of patterns that were supposed to be present in the mixture state  . The probability q = 1-p , and , 1 :_~  ( aN ) is the number of patterns that can bcconstructed fi ' om the aet iw ~ neurons in the mix  . S , , is the mnnber of wrongly selected activity patterns for a given n  . l ) , rror ( n ) decreases with increa~ing N if the other parameters remain tixed  . 
The space complexity of the network , exprc&sed as then nmber of neurons , and as a function of the number of NI ) A states is O ( \[Q17 )  . This is large because Q=Qs'r <1 FI ~ for some PDAM . tlowever things conld have been worse . Not using mixed temporal images to represent FSA states would necessitate the use of a mnnber of temporal images of order  2 IQ'I ~ , So compared to a more conventional use of lloptield models  , this approach yields a redaction of the space complexity of the network  . 
8 Conclusions
We proposed an receptor for all contextfree languages with limited center-embedding  , and a suitable variant of the Ilop field mode \] . The formal model was implemented on the llop tield model  , and a correct = uess theorem for the latter was given  . Simulation re-stilts provided initial corroboration of onr theory  . The obtamed neural-networke ceptor is fast but large  . 
Continuation of this research in the nearfnture consists of the design of an adaptive variant of this mode\[  , one that learns a grammar from examples in an unsupervised fashion  . 
Acknowledgements 1 amvery grateful for the indispensable support of Mannes Poel and Anton Nijholt in making this paper  . 
Special thanks go out to Bertllelthu is who wrote tile very beautiful simulation program with which the theory was corroborated  . 
References\[11) . J . Amit . Modcltag Brain FunctiOn . Cambridge
University Press , (~ ambridge , 1989.
\[2\] J . iluhmann , R . Divko , and K . Schulten . Associative memory with high information content . 
Physical Review A , 39(5):2689-2692, 1989.
\[3\]E . Charniak and E . Santos . A connections t contextfree parser which is not contextfree  , but then it is not really connections teither . In Pro-eeedinys 91h Ann . Couf . o \]' the Cognitive Science
Society , pages 7077, 1987.
\[4\] M . F . J . l ) rossaers . Neural-network ace eptors . 
"l ~ chuical Report Memor and alnformatica 92-36,
University of Twente , Enschede , 1992.
\[5\] M . A . Fanty . Context = free parsing in connection-st networks . Technical Report TR174 , University of Rocbester , Rochester , NY ,  1985 . 
\[6\] 11 . Nakagaw and T . Mort . A parser based on connections t node l . In Proceeding so J:COLING'88 , pages 454--458 ,  1988 . 
\[7\] B . Sehnan and G . llirst . Parsing as an energy minimization problem , in E . Davis , editor , Genetic Algorithms and Simulated Annealing , Research Notes in Artzficial ntelligence , pages 141-154 , Los Altos , Calif . , 1987 . Morgan Kaufinan

\[8\]II . Sompoiins ky and 1 . Kanter . Temporala . s . so-elation in asymmetric neural networks . Physical Review Letters , 57(22):2861-2864, 1986 . 
\[9\]I ) . S . Touretzky . Connections in and compositional semantics . Technical Report CMU-CS-89-147 , Carnegie Mellon University , Pittsburgh ,  1989 . 
\[1(\]\]I) . L . Waltz and J . B . Pollack . Massively parallel parsing : A strongly interactive model of natural language interpretation  . Cognitive Science , pages 51-74, 1985 . 
ACTESDECOL 1NG-92 , NAN ri ! s , 2328 A of n "1992119I>ROC . OFCOLING-92, NANIT ~ S , AUG .  2328, 1992
