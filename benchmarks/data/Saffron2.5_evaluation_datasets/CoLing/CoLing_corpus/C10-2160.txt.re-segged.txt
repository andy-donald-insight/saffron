Coling 2010: Poster Volume , pages 1399?1407,
Beijing , August 2010
Jointly Identifying Entities and Extracting Relations in Encyclopedia
Text via A Graphical Model Approach?
Xiaofeng YU Wai LAM
Information Systems Laboratory
Department of Systems Engineering & Engineering Management
The Chinese University of Hong Kong
{xfyu,wlam}@se.cuhk.edu.hk
Abstract
In this paper , we investigate the problem of entity identification and relation extraction from encyclopedia articles , and we propose a joint discriminative probabilistic model with arbitrary graphical structure to optimize all relevant subtasks simultaneously . This modeling offers a natural formalism for exploiting rich dependencies and interactions between relevant subtasks to capture mutual benefits , as well as a great flexibility to incorporate a large collection of arbitrary , overlapping and nonindependent features . We show the parameter estimation algorithm of this model . Moreover , we propose a new inference method , namely collective iterative classification ( CIC ), to find the most likely assignments for both entities and relations.
We evaluate our model on realworld data from Wikipedia for this task , and compare with current state-of-the-art pipeline and joint models , demonstrating the effectiveness and feasibility of our approach.
1 Introduction
We investigate a compound information extraction ( IE ) problem from encyclopedia articles , which consists of two subtasks ? recognizing structured information about entities and extracting the relationships between entities . The most common approach to this problem is a pipeline architecture : attempting to perform different subtasks , namely , named entity recognition and relation extraction between recognized entities in several separate , and independent stages . Such kind of design is widely adopted in NLP.
?The work described in this paper is substantially supported by grants from the Research Grant Council of the Hong Kong Special Administrative Region , China ( Project No : CUHK4128/07) and the Direct Grant of the Faculty of Engineering , CUHK ( Project Codes : 2050442 and 2050476). This work is also affiliated with the Microsoft-CUHK Joint Laboratory for Human-centric Computing and
Interface Technologies.
The most common and simplest approach to performing compound NLP tasks is the 1best pipeline architecture , which only takes the 1best hypothesis of each stage and pass it to the next one . Although it is comparatively easy to build and efficient to run , this pipeline approach is highly ineffective and suffers from serious problems such as error propagation ( Finkel et al , 2006; Yu , 2007; Yu et al , 2008). It is not surprising that , the end-to-end performance will be restricted and upper-bounded.
Usually , one can pass Nbest lists between different stages in pipeline architectures , and this often gives useful improvements ( Hollingshead and Roark , 2007). However , effectively making use of Nbest lists often requires lots of engineering and human effort ( Toutanova , 2005). On the other hand , one can record the complete distribution at each stage in a pipeline , to compute or approximate the complete distribution at the next stage.
Doing this is generally infeasible , and this solution is rarely adopted in practice.
One promising way to tackle the problem of error propagation is to explore joint learning which integrates evidences from multiple sources and captures mutual benefits across multiple components of a pipeline for all relevant subtasks simultaneously ( e.g ., ( Toutanova et al , 2005), ( Poon and Domingos , 2007), ( Singh et al , 2009)). Joint learning aims to handle multiple hypotheses and uncertainty information and predict many variables at once such that subtasks can aid each other to boost the performance , and thus usually leads to complex model structure . However , it is typically intractable to run a joint model and they sometimes can hurt the performance , since they Due to these difficulties , research on building joint approaches is still in the beginning stage.
A significant amount of recent work has shown the power of discriminatively-trained probabilistic graphical models for NLP tasks ( Lafferty et al , 2001; Sutton and McCallum , 2007; Wainwright and Jordan , 2008). The superiority of graphical model is its ability to represent a large number of random variables as a family of probability distributions that factorize according to an underlying graph , and capture complex dependencies between variables . And this progress has begun to make the joint learning approach possible.
In this paper we study and formally define the joint problem of entity identification and relation extraction from encyclopedia text , and we propose a joint paradigm in a single coherent framework to perform both subtasks simultaneously . This framework is based on undirected probabilistic graphical models with arbitrary graphical structure . We show how the parameters in this model can be estimated efficiently . More importantly , we propose a new inference method ? collective iterative classification ( CIC ), to find the maximum a posteriori ( MAP ) assignments for both entities and relations . We perform extensive experiments on realworld data from Wikipedia for this task , and substantial gains are obtained over state-of-the-art probabilistic pipeline and joint models , illustrating the promise of our approach.
2 Problem Formulation 2.1 Problem Description This problem involves identifying entities and discovering semantic relationships between entity pairs from English encyclopedic articles . The basic document is an article , which mainly defines and describes an entity ( known as principal entity ). This document mentions some other entities as secondary entities related to the principal entity . Clearly , our task consists of two subtasks ? first , for entity identification , we need to recognize the secondary entities ( both the boundaries and types of them ) in the document 1. Second , 1Since the topic/title of an article usually defines a principal entity ( e.g ., a famous person ) and it is easy to identify , in after all the secondary entities are identified , our goal for relation extraction is to predict what relation , if any , each secondary entity has to the principal entity . We assume that there is no relationship between any two secondary entities in one document.
As an illustrative example , Figure 1 shows the task of entity identification and relationship extraction from encyclopedic documents . Here , Abraham Lincoln is the principal entity . Our task consists of assigning a set of predefined entity types ( e.g ., PER , DATE , YEAR , and ORG ) to segmentations in encyclopedic documents and assigning a set of predefined relations ( e.g ., birth day , birth year , and member of ) for each identified secondary entity to the principal entity.
2.2 Problem Formulation
Let x be an observation sequence of tokens in encyclopedic text and x = { x1, ? ? ? , xN }. Let sp be the principal entity ( we assume that it is known or can be easily recognized ), and let s = { s1, ? ? ? , sL } be a segmentation assignment of observation sequence x . Each segment si is a triple si = {? i , ? i , yi }, where ? i is a start position , ? i is an end position , and yi is the label assigned to all tokens of this segment . The segment si satisfies 0 ? ? i < ? i ? | x | and ? i+1 = ? i + 1. Let rpn be the relation assignment between principal entity sp and secondary entity candidate sn from the segmentation s , and r be the set of relation assignments for sequence x.
Let y = { r , s } be the pair of segmentation s and segment relations r for an observation sequence x . A valid assignment y must satisfy the condition that the assignments of the segments and the assignments of the relations of segments are maximized simultaneously . We now formally define this joint optimization problem as follows : Definition 1 ( Joint Optimization of Entity Identification and Relation Extraction ): Given an observation sequence x , the goal of joint optimization of entity identification and relation extraction is to find the assignment y ? = { r ?, s ?} that has the maximum a posteriori ( MAP ) probability y ? = argmax y
P ( y|x ), (1) this paper we only focus on secondary entity identification.
1400                !"# $# %&  '  ' '  Figure 1: An example of entity identification and relation extraction excerpted from our dataset . The secondary entities are in pink color and labeled . The semantic relation of each secondary entity to the principal entity Abraham Lincoln ( in green color and we assume that it is known or can be easily recognized ) is also shown.
where r ? and s ? denote the most likely relation assignment and segmentation assignment , respectively.
Note that this problem is usually very challenging and offers new opportunities for information extraction , since complex dependencies between segmentations and relations should be exploited.
3 Our Proposed Model 3.1 Preliminaries
Conditional random fields ( CRFs ) ( Lafferty et al , 2001) are undirected graphical models trained to maximize the conditional probability of the desired outputs given the corresponding inputs . Let G be a factor graph ( Kschischang et al , 2001) defining a probability distribution over a set of output variables o conditioned on observation sequences x . C = {? c(oc , xc )} is a set of factors in G , then the probability distribution over G can be written as
P ( o|x ) = 1Z(x ) ? c?C ? c(oc , xc ) (2) where ? c is a potential function and Z(x ) =? o ? c?C ? c(oc , xc ) is a normalization factor.
We assume the potentials factorize according to a set of features { fk(oc , xc )} as ? c(oc , xc ) = exp ( ? k ? kfk(oc , xc )) so that the family of distributions is an exponential family . The model parameters are a set of real-valued weights ? = {? k }, one weight for each feature . Practical models rely extensively on parameter tying to use the same parameters for several factors.
However , the traditional fashion of CRFs can only deal with single task , they lack the capability to represent more complex interaction between multiple subtasks . In the following we will describe our joint model in detail for this problem.
3.2 A Joint Model for Entity Identification and Relation Extraction Following the notations in Section 2.2, let L and M be the number of segments and number of relations for sequence x , respectively . We define a joint conditional distribution for segmentation s in observation sequence x and segment relation r in undirected , probabilistic graphical models . The nature of our modeling enables us to partition the factors C of G into three groups { CS , CR , CO}={{?S }, {? R }, {? O }}, namely the segmentation potential ? S , the relation potential ? R , and the segmentation-relation joint potential ? O , and each potential is a clique template whose parameters are tied . The potential function ? S(i , s , x ) models segmentation s in x , the potential function ? R(rpm , rpn , r ) ( m 6= n ) represent dependencies ( e.g ., long-distance dependencies , relation transitivity , etc ) between any two relations in the relation set r , where rpm is the relation assignment between the principal entity sp and the secondary entity candidate sm from s , and similarly for rpn . And the joint potential ? O(sp , sj , r ) captures rich and complex interactions between segmentation s for secondary entity identification and relation r between each secondary entity candidate sj to the principal entity sp . According to the celebrated Hammersley-Clifford theorem ( Besag , 1974), the joint conditional distribution P ( y|x ) = P ({ r , s}|x ) is factorized as a product of potential functions over cliques in the graph G as the form of an exponential family:
P ( y|x ) = 1Z(x ) (?
CS ? S(i , s , x )) (?
CR ? R(rpm , rpn , r ) )(?
CO ? O(sp , sj , r ) ) (3) ?
CS ?
S(i , s , x)?CR ? R(rpm , rpn , r)?CO ? O(sp , sj , r ) is the normalizationfactor of the joint model.
We assume the potential functions ? S , ? R and ? O factorize according to a set of features and a corresponding set of real-valued weights . More specifically , ? S(i , s , x ) = exp ( ?| s | i=1 ? K k=1 ? kgk(i , s , x )). To effectively capture properties of segmentation , we relax the first-order Markov assumption to semi-Markov such that each segment feature function gk (?) depends on the current segment si , the previous segment si?1, and the whole observation sequence x , that is , gk(i , s , x ) = gk(si?1, si , x ) = gk(yi?1, yi , ? i , ? i , x ). And transitions within a segment can be non-Markovian.
Similarly , the potential ? R(rpm , rpn , r ) = exp ( ? M m,n ? W w=1 ? wqw(rpm , rpn , r )) and ? O(sp , sj , r ) = exp(?Lj=1 ? T t=1 ? tht(sp , sj , r )), where W and T are number of feature functions , qw (?) and ht (?) are feature functions , ? w and ? t are corresponding weights for them . The potential ? R(rpm , rpn , r ) allows long-range dependency representation between different relations rpm and rpn . For example , if the same secondary entity is mentioned more than once in an observation sequence , all mentions probably have the same relation to the principal entity . Using potential ? R(rpm , rpn , r ), evidences for the same entity segments to the principal entity are shared among all their occurrences within the document . The joint factor ? O(sp , sj , r ) exploits tight dependencies between segmentations and relations . For example , if a segment is labeled as a location and the principal entity is person , the semantic relation between them can be birth place or visited , but cannot be employment . Such dependencies are essential and modeling them often leads to improved performance . In summary , the probability distribution of the joint model can be rewritten as:
P ( y|x ) = 1Z(x ) exp { | s |? i=1
K ? k=1 ? kgk(i , s , x ) +
M ? m,n
W ? w=1 ? wqw(rpm , rpn , r ) +
L ? j=1
T ? t=1 ? tht(sp , sj , r ) } (4)                                      Figure 2: Graphical representation of the probabilistic joint model . The gray nodes represent sequence tokens { x1, ? ? ? , xN }. Each ellipse represents a segment consisting of several consecutive sequence tokens . The pink nodes represent segmentation assignment { s1, ? ? ? , sL } of sequence.
The yellow nodes represent relation assignment { rp1, ? ? ? , rpL } between the principal entity sp ( in green color ) and secondary entity segments.
As illustrated in Figure 2, our model consists of three substructures : a semi-Markov chain on the segmentations s conditioned on the observation sequences x , represented by ? S ; potential ? R measuring dependencies between different relations rpm and rpn ; and a fully-connected graph on the principal entity sp and each segment sj for their relations , represented by ? O.
While several special cases of CRFs are of particular interest , and we emphasize on the differences and advantages of our model against others . Linear-chain CRFs ( Lafferty et al , 2001) can only perform single sequence labeling , they lack the ability to capture long-distance dependency and represent complex interactions between multiple subtasks . Skip-chain CRFs ( Sutton and Mccallum , 2004) introduce skip edges to model long-distance dependencies to handle the label consistency problem in single sequence labeling and extraction . 2D CRFs ( Zhu et al , 2005) are two-dimensional conditional random fields incorporating the two-dimensional neighborhood dependencies in Web pages , and the graphical representation of this model is a 2D grid . Hierarchical CRFs ( Liao et al , 2007) are a class of CRFs with hierarchical tree structure . Our probabilistic model for joint entity identification and relation extraction has distinct graphical structure from 2D and hierarchical CRFs . And this modeling has sev-ical models by using semi-Markov chains for efficient segmentation and labeling , by representing long-range dependencies between relations , and by capturing rich and complex interactions between relevant subtasks to exploit mutual benefits.
4 Learning the Parameters
Given independent and identically distributed ( IID ) training data D = { xi , yi}Ni=1, where xi is the ith sequence instance , yi = { ri , si } is the corresponding segmentation and relation assignments . The objective of learning is to estimate ? = {? k , ? w , ? t } which is the vector of model?s parameters . Under the IID assumption , we ignore the summation operator ? Ni=1 in the loglikelihood during the following derivations . To reduce overfitting , we use regularization and a common choice is a spherical Gaussian prior with zero mean and covariance ?2I . Then the regularized loglikelihood function L for the data is
L = log [ ?( r , s , x )]? log [ Z(x )] ?
K ? k=1 ?2k 2?2? ?
W ? w=1 ?2w 2?2? ?
T ? t=1 ?2t 2?2? (5) where ?( r , s , x ) = exp{?|s|i=1 ? K k=1 ? kgk(i , s , x ) + ? M m,n ? W w=1 ? wqw(rpm , rpn , r )+ ? L j=1 ? T t=1 ? tht(sp , sj , r )}, Z(x ) = ? y ? ?( r , s , x ), and 1/2?2?, 1/2?2?, 1/2?2? are regularization parameters.
Taking derivatives of the function L over the parameter ? k yields : ? L ?? k = | s |? i=1 gk(i , s , x )? | s |? i=1 gk(i , s , x)P ( y|x ) ?
K ? k=1 ? k ?2? (6)
Similarly , the partial derivatives of the loglikelihood with respect to parameters ? w and nut are as follows : ? L ?? w =
M ? m,n qw(rpm , rpn , r)?
M ? m,n qw(rpm , rpn , r ) ? P ( y|x)?
W ? w=1 ? w ?2? (7) ? L ?? t =
L ? j=1 ht(sp , sj , r)?
L ? j=1 ht(sp , sj , r)P ( y|x ) ?
T ? t=1 ? t ?2? (8)
The function L is concave , and can be efficiently maximized by standard techniques such as stochastic gradient and limited memory quasi-Newton ( LBFGS ) algorithms . The parameters ? k ? w and ? t are optimized iteratively until converge.
5 Finding the Most Likely Assignments
The objective of inference is to find y ? = { r ?, s ?} = argmax{r,s } P ( r , s|x ) such that both s ? and r ? are optimized simultaneously . Unfortunately , exact inference to this problem is generally prohibitive , since it requires enumerating all possible segmentation and corresponding relation assignments . Consequently , approximate inference becomes an alternative.
We propose a new algorithm : collective iterative classification ( CIC ) to perform approximate inference to find the maximum a posteriori ( MAP ) segmentation and relation assignments of our model in an iterative fashion . The basic idea of CIC is to decode every target hidden variable based on the assigning labels of its sampled variables , where the labels might be dynamically updated throughout the iterative process . Collective classification refers to the classification of relational objects described as nodes in a graphical structure , as in our model.
The CIC algorithm performs inference in two steps , as shown in Algorithm 1. The first step , bootstrapping , predicts an initial labeling assignment for a unlabeled sequence xi , given the trained model P ( y|x ). The second step is the iterative classification process which reestimates the labeling assignment of xi several times , picking them in a sample set S based on initial assignment for xi . Here we exploit the sampling technique ( Andrieu et al , 2003).
The advantages of sampling are summarized as follows . Sampling stochastically enables us to generate a wide range of inference situations , and the samples are likely to be in high probability areas , increasing our chances of finding the max-performance . The CIC algorithm may converge if none of the labeling assignments change during an iteration or a given number of iterations is reached.
Noticeably , this inference algorithm is also used to efficiently compute the marginal probability P ( y|x ) during parameter estimation ( the normalization constant Z(x ) can also be calculated via approximation techniques ). As can be seen , this algorithm is simple to design , efficient and scales well w.r.t . the size of data.
6 Experiments 6.1 Data
Our data comes from Wikipedia2, the world?s largest free online encyclopedia . This dataset consists of 1127 paragraphs from 441 pages from the online encyclopedia Wikipedia . We labeled 7740 entities into 8 categories , yielding 1243 person , 1085 location , 875 organization , 641 date , 1495 year , 38 time , 59 number , and 2304miscellaneous names . This dataset alo contains 4701 relation instances and 53 labeled relation types . The 10 most frequent relation types are job title , visited , birth place , associate , birth year , member of , birth day , opus , death year , and death day . Note that this compound IE task involving entity identification and relation extraction is very challenging , and modeling tight interactions between entities and their relations is highly attractive.
6.2 Feature Set
Accurate entities enable features that are naturally expected to be useful to boost relation extraction.
And a wide range of rich , overlapping features can be exploited in our model . These features include contextual features , part-of-speech ( POS ) tags , morphological features , entity-level dictionary features , clue word features . Feature conjunctions are also used . In leveraging relation extraction to improve entity identification , we use a combination of syntactic , entity , keyword , semantic , and Wikipedia characteristic features . More importantly , our model can incorporate multiple mention features qw (?), which are used to collect 2http://www.wikipedia.org / Algorithm 1: Collective Iterative Classification Inference Input : A unlabeled sequence xi and a trained model P ( y|x ) Output : The set of predicted assignment yi = { ri , si } // Bootstrapping foreach yi ? Y do yi ? argmaxyi P ( yi|xi ); end // Iterative Classification repeat Generate a sample set S based on initial label assignment yi for sequence xi ; foreach si ? S doAssign new label assignment to sample si ; end until all labels have stabilized or a threshold number of iterations have elapsed ; return yi = { ri , si } evidences from other occurrences of the same secondary entities for consistent segmentation and relation labeling to the principal entity . The features ht (?) capture deep dependencies between segmentations and relations , and they are natural and useful to enhance the performance.
6.3 Methodology
We perform fourfold crossvalidation on this dataset , and take the average performance . For performance evaluation , we use the standard measures of Precision ( P ), Recall ( R ), and Fmeasure ( the harmonic mean of P and R : 2PRP+R ) for bothentity identification and relation extraction . We conduct holdout methodology for parameter tuning and optimization of our model . We compare our approach with a series of linear-chain CRFs : CRF+CRF and a joint model DCRF ( Sutton et al ., 2007): dynamic probabilistic models combined with factored approach to multiple sequence labeling . CRF+CRF perform entity identification and relation extraction separately . Relation extraction is viewed as a sequence labeling problem in the second CRF . All these models exploit standard parameter learning and inference algorithms tion.
Entities CRF+CRF DCRF Our modelP R F1 P R F1 P R F1 person 75.33 83.22 79.08 75.96 83.82 79.70 82.91 84.26 83.58 location 77.03 69.45 73.04 77.68 70.13 73.71 82.94 80.52 81.71 organization 53.78 47.76 50.59 54.55 46.98 50.48 61.63 62.61 62.12 date 98.54 97.53 98.03 97.98 95.22 96.58 98.90 96.24 97.55 year 97.14 99.10 98.11 98.12 99.09 98.60 97.36 99.55 98.44 time 60.00 20.33 30.37 50.00 25.33 33.63 100.0 25.00 40.00 number 98.88 60.33 74.94 100.0 66.00 79.52 100.0 65.52 79.17 miscellaneous 77.42 80.56 78.96 79.81 83.14 81.44 82.69 85.16 83.91 Overall 89.55 88.70 89.12 90.98 90.37 90.67 93.35 93.37 93.36 in our experiments . To avoid overfitting , penalization techniques on likelihood are performed.
We also use the same set of features for all these models.
6.4 Experimental Results
Table 1 shows the performance of entity identification and Table 2 shows the overall performance of relation extraction 3, respectively . Our model substantially outperforms all baseline models on the overall Fmeasure for entity identification , resulting in an relative error reduction of up to 38.97% and 28.83% compared to CRF+CRF and DCRF , respectively . For relation extraction , the improvements on the Fmeasure over CRF+CRF and DCRF are 4.68% and 3.75%. McNemar?s paired tests show that all improvements of our model over baseline models are statistically significant . These results demonstrate the merits of our approach by capturing tight interactions between entities and relations to explore mutual benefits . The pipeline model CRF+CRF performs entity identification and relation extraction independently , and suffers from problems such as error accumulation . For example , CRF+CRF cannot extract the member of relation between the secondary entity Republican and the principal entity George W . Bush , since the organization name Republican is incorrectly labeled as a miscellaneous . By modeling interactions between two subtasks , enhanced performance is achieved , as illustrated by DCRF . Unfortunately , training a DCRF model with unobserved nodes ( hidden variables ) makes this approach difficult to opti-3Due to space limitation , we only present the overall performance and omit the performance for 53 relation types.
Table 2: Comparative performance of our model , CRF+CRF , and DCRF models for relation extraction.
Model Precision Recall Fmeasure
CRF+CRF 70.40 57.85 63.51
DCRF 69.30 60.22 64.44
Our model 72.57 64.30 68.19 mize , as we will show below.
The efficiency of different models is summarized in Table 3. Compared to the pipeline model CRF+CRF , the learning time of our model is only a small constant factor slower . Notably , our model is over orders of magnitude ( approximately 15.7 times ) faster than the joint model DCRF . The DCRF model uses loopy belief propagation ( LBP ) for approximate learning and inference . When the graph has large treewidth as in our case , the LBP algorithm in DCRF is inefficient , and is slow to converge . Using LBFGS and the CIC approximate inference algorithms , both learning and decoding can be carried out efficiently.
Table 3: Efficiency comparison of different models on learning time ( sec .) and inference time ( sec.).
Model Learning time Inference time
CRF+CRF 2822.55 6.20
DCRF 105993.00 127.50
Our model 6733.69 62.75
Table 4 compares our CIC inference with two state-of-the-art inference approaches : Gibbs sampling ( GS ) ( Geman and Geman , 1984) and the iterative classification algorithm ( ICA ) ( Neville and Jensen , 2000) for our model . The CIC inference is shown empirically to help improve classi-ference algorithms for our model on entity identification and relation extraction.
Entity Precision Recall Fmeasure
GS 92.45 92.15 92.30
ICA 92.19 91.98 92.08
CIC 93.35 93.37 93.36
Relation Precision Recall Fmeasure
GS 71.22 63.29 67.02
ICA 71.58 63.68 67.40
CIC 72.57 64.30 68.19 fication accuracy and robustness over these two algorithms . When probability distributions are very complex or even unknown , the GS algorithm cannot be applied . ICA iteratively infers the states of variables given the current predicted labeling assignments of neighboring variables as observed information . Prediction errors on labels may then propagate during the iterations and the algorithm will then have difficulties to generalize correctly.
We mention some recently published results related to Wikipedia datasets ( Note that it is difficult to compare with them strictly , since these results can be based on different experimental settings).
Culotta et al (2006) used a data set with a 70/30 split for training/testing and Nguyen et al (2007) used 5930 articles for training and 45 for testing , to perform relatione extraction from Wikipedia.
And the obtained Fmeasures were 67.91 and 37.76, respectively . Yu et al (2009) proposed an integrated approach incorporating probabilistic graphical models with first-order logic to perform relation extraction from encyclopedia articles , with a Fmeasure of 65.66. All these systems assume that the golden-standard entities are already known and they only perform relation extraction . However , such assumption is not valid in practice . Notably , our approach deals with a fairly more challenging problem involving both entity identification and relation extraction , and it is more applicable to realworld IE tasks.
7 Related Work
A number of previous researchers have taken steps toward joint models in NLP and information extraction , and we mention some recently proposed , closely related approaches here . Roth and Yih (2007) considered multiple constraints between variables from tasks such as named entities and relations , and developed a integer linear programming formulation to seek an optimal global assignment to these variables . Zhang and Clark (2008) employed the generalized perceptron algorithm to train a statistical model for joint segmentation and POS tagging , and applied multiple-beam search algorithm for fast decoding.
Toutanova et al (2008) presented a model capturing the linguistic intuition that a semantic argument frame is a joint structure , with strong dependencies among the arguments . Finkel and Manning (2009) proposed a discriminative feature-based constituency parser for joint named entity recognition and parsing . And Dahlmeier et al (2009) proposed a joint model for word sense disambiguation of prepositions and semantic role labeling of prepositional phrases . However , most of the mentioned approaches are task-specific ( e.g ., ( Toutanova et al , 2008) for semantic role labeling , and ( Finkel and Manning , 2009) for parsing and NER ), and they can hardly be applicable to other NLP tasks . Since we capture rich and complex dependencies between subtasks via potential functions in probabilistic graphical models , our approach is general and can be easily applied to a variety of NLP and IE tasks.
8 Conclusion and Future Work
In this paper , we investigate the compound IE task of identifying entities and extracting relations between entities in encyclopedia text . And we propose a unified framework based on undirected , conditionally-trained probabilistic graphical models to perform all relevant subtasks jointly . More importantly , we propose a new algorithm : CIC , to enable approximate inference to find the MAP assignments for both segmentations and relations.
As we shown , our modeling offers several advantages over previous models and provides a natural formalism for this compound task . Experimental study exhibits that our model significantly outperforms state-of-the-art models while also running much faster than the joint models . In addition , the superiority of the CIC algorithm is also discussed and compared . We plan to improve the scalability of our approach and apply it to other realworld problems in the future.
1406
References
Christophe Andrieu , Nando de Freitas , Arnaud Doucet , and Michael I . Jordan . An introduction toMCMC for machine learning . Machine Learning , 50(1):5?43, 2003.
Julian Besag . Spatial interaction and the statistical analysis of lattice systems . Journal of the Royal Statistical Society , 36:192?236, 1974.
Aron Culotta , Andrew McCallum , and Jonathan Betz . Integrating probabilistic extraction models and data mining to discover relations and patterns in text . In Proceedings of HLT/NAACL-06, pages 296?303, New York , 2006.
Daniel Dahlmeier , Hwee Tou Ng , and Tanja Schultz . Joint learning of preposition senses and semantic roles of prepositional phrases . In Proceedings of EMNLP-09, pages 450?458, Singapore , 2009.
Jenny Rose Finkel and Christopher D . Manning . Joint parsing and named entity recognition . In Proceedings of HLT/NAACL-09, pages 326?334, Boulder , Colorado , 2009.
Jenny Rose Finkel , Christopher D . Manning , and Andrew Y.
Ng . Solving the problem of cascading errors : Approximate Bayesian inference for linguistic annotation pipelines . In Proceedings of EMNLP06, pages 618?626,
Sydney , Australia , 2006.
Stuart Geman and Donald Geman . Stochastic relaxation , Gibbs distributions , and the Bayesian restoration of images . IEEE Transitions on Pattern Analysis and Machine
Intelligence , 6:721?741, 1984.
Kristy Hollingshead and Brian Roark . Pipeline iteration . In Proceedings of ACL07, pages 952?959, Prague , Czech
Republic , 2007.
Frank R . Kschischang , Brendan J . Frey , and Hans-Andrea Loeliger . Factor graphs and the sum-product algorithm.
IEEE Transactions on Information Theory , 47:498?519, 2001.
John Lafferty , Andrew McCallum , and Fernando Pereira.
Conditional random fields : Probabilistic models for segmenting and labeling sequence data . In Proceedings of
ICML01, pages 282?289, 2001.
Lin Liao , Dieter Fox , and Henry Kautz . Extracting places and activities from GPS traces using hierarchical conditional random fields . International Journal of Robotics
Research , 26:119?134, 2007.
Jennifer Neville and David Jensen . Iterative classification in relational data . In Proceedings of the AAAI-2000 Workshop on Learning Statistical Models from Relational
Data , pages 42?49, 2000.
Dat P . T . Nguyen , Yutaka Matsuo , and Mitsuru Ishizuka . Relation extraction from Wikipedia using subtree mining . In Proceedings of AAAI-07, pages 1414?1420, Vancouver,
British Columbia , Canada , 2007.
Hoifung Poon and Pedro Domingos . Joint inference in information extraction . In Proceedings of AAAI-07, pages 913?918, Vancouver , British Columbia , Canada , 2007.
Dan Roth and Wentau Yih . Global inference for entity and relation identification via a linear programming formulation . In Lise Getoor and Ben Taskar , editors , Introduction to Statistical Relational Learning . MIT Press , 2007.
Sameer Singh , Karl Schultz , and Andrew Mccallum . Bidirectional joint inference for entity resolution and segmentation using imperatively-defined factor graphs . In Proceedings of ECML/PKDD-09, pages 414?429, Bled,
Slovenia , 2009.
Charles Sutton and Andrew Mccallum . Collective segmentation and labeling of distant entities in information extraction . In Proceedings of ICML Workshop on Statistical Relational Learning and Its Connections to Other Fields , 2004.
Charles Sutton and Andrew McCallum . An introduction to conditional random fields for relational learning . In Lise Getoor and Ben Taskar , editors , Introduction to Statistical
Relational Learning . MIT Press , 2007.
Charles Sutton , Andrew McCallum , and Khashayar Rohanimanesh . Dynamic conditional random fields : Factorized probabilistic models for labeling and segmenting sequence data . Journal of Machine Learning Research , 8:693?723, 2007.
Kristina Toutanova , Aria Haghighi , and Christopher D . Manning . Joint learning improves semantic role labeling . In Proceedings of ACL05, pages 589?596, 2005.
Kristina Toutanova , Aria Haghighi , and Christopher D . Manning . A global joint model for semantic role labeling.
Computational Linguistics , 34:161?191, 2008.
Kristina Toutanova . Effective statistical models for syntactic and semantic disambiguation . PhD thesis , Stanford University , 2005.
Martin J . Wainwright and Michael I . Jordan . Graphical models , exponential families , and variational inference . Foundations and Trends in Machine Learning , 1:1?305, 2008.
Xiaofeng Yu , Wai Lam , and Shing-Kit Chan . A framework based on graphical models with logic for chinese named entity recognition . In Proceedings of the Third International Joint Conference on Natural Language Processing ( IJCNLP-08), pages 335?342, Hyderabad , India , 2008.
Xiaofeng Yu , Wai Lam , and Bo Chen . An integrated discriminative probabilistic approach to information extraction . In Proceedings of CIKM-09, pages 325?334, Hong
Kong , China , 2009.
Xiaofeng Yu . Chinese named entity recognition with cascaded hybrid model . In Proceedings of HLT/NAACL-07, pages 197?200, Rochester , New York , 2007.
Yue Zhang and Stephen Clark . Joint word segmentation and POS tagging using a single perceptron . In Proceedings of ACL08, pages 888?896, Ohio , USA , 2008.
Jun Zhu , Zaiqing Nie , Ji-Rong Wen , Bo Zhang , and Wei-Ying Ma . 2D conditional random fields for Web information extraction . In Proceedings of ICML05, pages 1044? 1051, Bonn , Germany , 2005.
1407
