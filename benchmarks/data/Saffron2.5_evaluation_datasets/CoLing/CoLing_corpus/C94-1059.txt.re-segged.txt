ENGLISHGENERATION FROMINTERLINGUA
BYEX AMPI~E-I~ASI~I ) METHOI)

This paper describes the experiment of the English generation from interlinguaby the example-based method  . The generator is implemented by using English Word Dictionary  , and Concept Dictionary developed in EDR . How to construct examples lind how to define the similarities are main problems  . The results of experiments are shown . 
1. Introduction
Eiji Komatsu * , Jin Cui ** , lliroshi Yasuhara ** ( * ) Oki Electricht dustry Co . Ltd . Meltimedia I . aboratory 11-22 , Shibaura 4-Chome , Minato-ku , Tokyo 108 Japan email : komatsu@okilab . oki . co . jp ( ** ) Japan Electronicl ) ictionary Research Institutel . td . (EDR)6th Laboratory Mita-Kokusai-Bldg .  4-28 , Miral-Chome , Minato-kv , Tokyo 108 Japan email : sai@ed rrr . edr . co . jp , yasuhara@edr6r . edr . co . jp to mean example data of the example-based method  . And the terms " interlingua " and " syntactic tree " are used to mean sets  , elements m~d fragments of elentents . 
2. Inputanti Output
The generator t , anslates an interling t , a to a syntactic tree . Fig . 2 . 1 shows a sample of input interling nae and Fig . 2 . 2, a sample of output syntactic trees . Both samples correspond to the same sentence " My brother will take the medicine "  . 
This paper describes the generator that is originally implemented to correct and evah  , ateEnglish Word Dictionary and Concept Dictionary being developed in EDR  ( El ) R , 1993) . To evaluate Concept Dictionary , as the first strategy , interlingua method was introduced . As the number o1' concepts is very large and they are elements of complex hierarchy  , it is difficult to make roles and on the other hand the example-based method was expected to be more effective than the rule-based method  . So , as the second strategy , the example-bused method was also introduced . 
The example-based method is usually used in MT by the transfer method  ( Nagao , 1984; Sato , 1991; Stnnita ,  1992) , though one by Sadler ( 1989 ) is by the interlingua method . In this generator , the example-based method coexists with the interlingua method because of above reasons  , but the combination of the example-based method and tim interlingua method is not int portant  , because l ' roman other point of view , the generation from interlingua is recognized as a translation from one hm-guage i  . e . interlinguato another i . e . English and the generation from interlingua can be seen similar as translations in above MT systems  . So in this experiment , how to apply the example-based method to various natural hm-guage processing and lbr which parts the method are suitable are the main interests  . For this purpose , the generator is designed to execute the generation with maximum usage of the example -based method  . 
In this experiment , he coverage of the generation is not complete , that is , some elements t , chas articles and conjunctions are not generated . 
Below , section 2 describes the input and ot , tput of the generator , section 3 , examples used in this system , section 4 , the similarities used to retrieve xamples and to select words  , section 5 , the generation algorithm , section 6 , the experiments for verb selections and section 7 , the conclusion . 
The examples , similarities and the generation algorithm are decided a priori then modilied in response to the output of the generator  . 
To avoid confusions , the word " example " is used only ( * ) This work has been done when the author wits in EDR  . 
" My brother will take tile medicine . " ( non-statement ) . ~ e086 ) - - - - - - - - liD --2 dc30I )   ( no > statement ) modify agent future 3bf 0 lx )   ( non-statement ) 
Fig . 2 . 1 Input lnterlingua lnterlinguae consist of concepts  , conceptual relations and attributes . Each concepts are classified as " statements " or " non-statements "  . Concepts are represented by concept identification umbers  ( To distinguish concepts easily by men , concept illustrations are also given ) . Interpretations of codes relating to interlinguae in this paper are shown in Table  2  . 1 . In the table , as for concept identification umbers , concept illustrations are showed a sin-terp , ' etations of codes . 
"My brother will take the medicine . " brother ( ENl ) - M-my ( EPl)
M(sj ) tak(EVE ; EVED ; ICV9 ; EVDO0) - - S--wi II(EAV ) " ~ S'- . e(EEV )


Fig . 2.2 Output Syntactic ' Free





Table 2.1 Codes for Interlinguae
Code Interpretation ( 3bfOd2 ) to drink something ( 3be086 ) brothers ( Oe 351f ) sisters ( 2dc301 ) c#I ( 2dc304 ) c#1 ~ ( 3bf 0D ) a substance used on or in the body to treat a disease  ( 3bdbf6 ) a drilled liquor named wtf is key ( 3bd862 ) adrug or agen that reduces fever ( 3cee4f ) to obtain a thing which one wante ~ l ( 3ceae3 ) to become a certain condition ( 0 f de 5 f ) to accept others ' opinions and wishes ii . .
(0c98dc ) tbo first part of the day , from the time when thesunagentrises , usually tm til the time when the midday mealts eaten Subject hat brings about a voluntary action  . 
Conscious attd automated ntities are suclt subjects  . 
" Animal seat " ( eat ) -- agent ~ ( animals ) Object affected by an action or change " Eat food  . "( eat)--object ~ ( food)
Time at which an event begins " Work until as c time "  ( wake up ) ~ time ~ G in time ) object time modifier O the relations hlps past Viewpoint is in the past present Viewpoint is in the present future Viewpoint is in the future end The end of an action or event already Already occurred 
Table 2.2 hfformatlon Code
Part-of Speech ENI

EVE " ~ i ; v


Grammatical EVST Mhffonnation I~V II





Surface Relation : M(sj )




Codes for Syntactic Trees lnteq ~ retatlon
Commonll Olli1
Personal pronoun

Anxu ! iary verb
Verbeliding


Uninflected part

Past tense
Past participle
Partially irregular inflections (%" follws)
Takes a direct object
Takes a direct object ( the direct object is to-infinltiv ( subject relation direct object relation adjective modification obligatory prepositional phrase relations between content words and functional words Syntactic trees consist of words  , part-of-speeches , grammatic ~ dinformation and syntactic relations . 
The interpretations of codes relating to syntactic trees used in this paper are shown Table  2  . 2 . 
3. Examples
An example should be a pair of an interlinguand a syntactic tree  . For the flexibility of usage of examples , interlinguae and syntactic treesinex : unples are divided into smaller parts that are small enough to use flexibly but have enough information for generations  . 
Fig . 3 . 1 shows the common form of interlinguae and syntactic trees in examples  ( referred as " basic unit " , below ) . An example is a pair of fragments in this form made from an interling u and a syntactic tree  . 
tip ( near totile root of I he tree lower n ~ lt : structure of an  interlingm0 lower arc lower node upptrn ( ~ leUpl~rarc ~" attribute
Fig . 3.1 Basic U if its
Fig . 3 . 2 shows the linguistic resources used by the generator  . As the results of trying to execute as many processes as possible by the example-based method  , it became necessary for the generator to use two different kinds of examples  ( referred as " BasicExampleSet " and " Example Sett br Attribute "  , below ) . 
1/~~. qtWordDictionar ~
IEnglish Generator_EDR Concept Dictionary \]/  \ 
Examples ~_~ I~I
Fig . 3.2 Linguistic Resources
Fig . 3.3 shows examples in the Basic Example Set.
Circlod nodes are " central nodes " . Basic Example Set is supposod to be used for selecting content words for concepts  . Functional words except prepositions and grmn -matical information for inflections are removed  , since they are unnecessary for this purpose . In Fig . 3 . 2 , example ( A ) and ( 13 ) have 11 oup per node and Example ( C ) and ( D ) have no lower node . Examples in this set are accessed by concepts in the central nodes of interlinguae  ; Example ( A ) and ( B ) are accessed by ( 3bf0d2 ) and ( C )  , by (3bf0f9) and ( D)by (0c98dc) . When several examples with the same key exist , by the simih ' u ' ity defined below , only one example is finally accepted . 
Fig . 3 . 4 shows examples in the Example Set for Attributes  . This example set is supposed to be used for deciding inflection  ( i . e . selecting the word whose inflection corresponds to the attributes  ) and adding functional words for attributes . Content words in lower nodes a retion of the center word  , but the lower nodes rarely don't . 
Functional words in lower nodes are added to the outputs  . 
Concepts and spellings of words are also removed , since they can be decided by Basic Example Set and unnecessary here  . Examples are accessed by combinations of attributes in interlinguae  , some grammatical information of the upper node , those of central nodes and the surface relation of the upper arc  ; in Fig . 3 . 4 , Example ( a ) is accessed by ( past ,  - , EVE ; EVED ,  -) , Example ( b ) by ( end , already ,  - , EVE ; EVEN ,  -) , Example ( c ) by ( present ,  - , EVE ; EVSTM ; ECV9 ,  -) , Example ( d ) by ( present ,   ,  - , EVE ; EVI l ,  -) , Example(e ) by ( future ,  - , EVE ; FNSTM ; ECV9 , -) and Example (1) by (- , EVE ; EVDO 0 , EN1 , M(do )) . Example ( a) , ( b ) , ( c ) , ( d ) and ( e ) have no upper node . Since examples in this set don't include concepts  , examples are accesse deterministically and the similarity is not used  . 
4. Similarities
There are two major similarities in the example -based method  . One is for the source language and used for selecting examples  . A notber is for the target language and used for creating outputs  . In this generator , the lbrmer is the similarity between interlinguae  ( in tile form of basic t , nits ) and the latter is the similarity between words . In the generator , the similarity is used only for Basic Ex- . 
ample Set.
Example ( A ) : Brother takes the medicine in the morning . 
(31~086 bro0~er(l!NI) . . , 1 agent f -' M ( s j ) object ( 3bf0f9 ) mcdic in c ( EN1 )  \] , Rellinguu Syntactic WlecF . xampl'e(B ): Sister drinks the whiskey . 
( Oe 35 lf ) st ~ r(I';N1) agent ~( .   .   .   .   .   .   .   .   .   . ) ~, lk ( .   .   .   .   .   .   .   .   .   .   .   .   . RI~M(nj ) " ~' J ?' :' . . . .~ . "" M ( . , , ) (31 + dbf6) ~ hinkcy(lNI)
Interlingua Synt~tic " l'l . oo
Example ( C ) : Brothers takes medicine in tile morning , 3bf ljd2"~hjeet--II~@tak(EVE ; EVDO0)-M(do)q ~ . - ~ ( non-slalemenl )
Illterlingua Synt~lic Tree
Example ( D ) : Brothers takes medicine in the morning . 
( . icAl-statement ) SNI Kiu ( El ' R ) Inlerlinguz Syntactic Tree
Fig . 3.3 Examples in Basic Example Set
Example(a ) :* ( EVE ; EVED ; EVDO0)(~statemerit)~~past
Example ( b ): lmve * ( EVE ; EVEN ; EVDO0) state merit ) ~_(I~VE ; EVEN ; ECV9~S--have ( EAV ) end already Example ( c )  : * ( EVE ; EVSTM ; ECV9 ; EVDO0) estatement ) Q(r' . ' W ;; EVSTM ; ECV g ) ~-- S--e(EI?V ) present
Example(d ) :*( FNE ; FVI~;;F . VI)O0)"; littel rlC-l~tt)Q(I , ; VIZ ; f2VIt ) . ~ pvese-tatExample(e ): will*(I:VE ; IiVSTM ; ECV9 ; EVDO0) eQ-~*~"~(stalctne , ,t ) ~---~( F . VI?;F . VS'I'M;ECV9)~ . ~, S--wil I(EAV)c(\]2 . EV ) future Example(D :* ( IVI,\];F . VI)O0)*(EN1)1 . )* ( r~vJ:4EVDO0)l'tl tuleFig . 3 . 4 Examples in Example Set for Attributes The simihu'ity between interlingt  , ae is defined its follows ; SiI(ILI , IL2) = ( Sc(Clcent , C2cent ) ? KcentIE , Rc(('li , C2i ) ? K ( slel ( i ) ) X ( k01 um ( R lf'H~2 ) II ) iG illf/R2
ILI , IL 2: int cr lingua e
Clcent ,   C2cent : concepts in central nodes K cent : weight of simihuity between central nodes Cli  , C2i : concepts in lower nodes with arc ik ( x ) : weight of similarity between concepts in lower nodes  , xistim number of elements in t be interjunction srel  ( i ) : surface relation which corresponds to the concept relation iR  1  , R2: set of concept t , al relations each for IL l ,  11 . 2nt un ( S ) : the number of elements of set SIt is always assured in adwmce by tile generator that  1  ) tile word in t be upper node of tile input is already selected  ( if there is imupper node )  ; 2) arcs of imerling t , a which corm-spond to obligatory relations of tile syntactic tree in the ex-  ; nnple , exist in the interjunction fP . 1 and R2 ; 3) upper arcs are same ( if already decided ) ;  4 ) part-of-speeches of words in upper nodes are same  . l : , xamples that don't satisfy these The similarity between concepts used in the above similarity is defined as follows  ; 
Sc(Cl , C2 ) = the ~ lumber of common ancesters the number of ancesters of CI + the number of ancesters of  C2 Ilere , ancestors until three layers above are used . ( Cut ;  1993 ) It is difficult to find the most similar interlingua in an example set to the input interlingua  , because to find it , it is necessary to calculate all similarities between interlinguae in the ex  , -unpleset and the input . To avoid this , in this generator , some constraints are given for access keys i . e . central nodes . For " statements " in interlingua , central nodes of examples should be same with that of the input and for " non-statements " in interlingua  , central nodes of examples can be tiles , ' u-neconcepts or sister concepts in the concept hierarchy  . By this constraints , the search of examples can be executed fast . 
The similarity between words is defined as follow ; k(0 < k < 1) if p~t-of-tspe . ee handl graln matic nt in for nl ~ tlon . ~ w(~*t1 , W2) ~ tJitlrG ~ irt ~ 0 if six = ling , part-of-speech mildgrninmtic mlin for n lati ~ mare 
Lalldltf , arelt k , 1: some numbers 5 . Generation Algorithm The generator generates fragments of a syntactic tree and tiredly combines them into a syntactic tree  . 
The generation algorithm is as follows;
Step 1 : Sets the current central node at the root node of the input interlingua  . 
Step 21 : Cuts the basic unit for the current central node  . 
Step 22 : Extracts candidate English words for concepts of the central node and lower nodes of the current basic unit  , from English Word Dictionary . 
Step 31 : Retrieves an example from Basic Example Set . 
Step 32: Selects the same word ( neglecting inflection ) from the candidate word lists and checks if there is an example in Example Set for Attributes  , whose attributes and words in the central node coincide with attributes in the current basic unit and the selected word  . 
Step 33: If the word selection succeeded , accepts the example . Generate supper arc ( if exists ) , lower arc ( only for obligatory relations ) central nodes , and functional words for the central node , saves the results and similarity and calculates the similarity of interlinguabetween the input and the example  . Prepositions are extracted from the basic example . 
Step 34 : Repeat Step 32 to Step 33 until there remains no basic examples . 
Step 35 : Selects one example that is accepted in Step 33 and the simih'u'ity is largest . 
Step 36: Puts the results.
Step 4 : Move the current central node in the input linterlingua in depth-first order  . 
Step 5 : Repeat Step 21 to Step 4 until the movement of the current central node ends or the word selection for a node fails  . 
2 dc 304 ( non-statement ) agent 3bf0d2 ( statement ) objoct p , t3 bd 862 ( non-statement )
Figure 5.1 Inputted lnterll ngua
Suppose the interlingua such as Fig . 5 . 1 is inputted and examples in Fig . 3 . 3 are used as Basic Example Set and Fig . 3 . 4 used as Example Set for Attributes . 
The list of candidate words for 3bf0d2 is as follows ; tak(EVE ; EVSTM ; ECV9 ; EVDO0) , took ( EVE ; EVED ; EVDO0) , taken(EVE ; EVEN ; EVDO0) , drink(I~VE;EVB ; EVDO0) , drank(EVE ; EVED ; EVDO0) , drunk(EVE ; EVEN ; EVDO0) . 
From Basic Example Set , Example ( A ) and ( B ) are re-trieve ( l , since central nodes are same . 
By Example ( A ) and Example ( a) , took ( EVE ; EVED ; EVDO 0 ) is selected and by Example ( B ) and Example ( a )  , drank(EVE;EVED ; EVDO0) is selected . 
As similmity between the input and Example ( A ) is larger than that between the inpvt and Example  ( B )  , " took " is selected . This is because similarity between 3bd 862 but ( 3 bf0 fg is 0 . 876535 and one between 3bd862 and 3bdbf6 is 0 . 
6 . Experiments for Verb Selections This chapter describes experiments o evaluat examples  , similarities and the generation algorithm . Experiments for verb selections are executed . 
The generator selects one word from candklate word list retrieved from EDR English Dictionary  . 
The experiments are ( lone by Jack-knife test method ( Sumita ;  1992)  ; 1) Specify a concept ;  2 ) Collect examples that include a word in candidate word list whose meaning is same with the specified concept  ; 3) Remove one example from example sets ;  4 ) Maketile input interlingua from the removed example  ;  5 ) Generate a sentence from this interlinguaby using remained examples  ;  6 ) Compare the original word and the generated word for the verb  ;  7 ) Repeat 3 )  - 6 ) by removing each example in turn . 
Below the results of three experiments ( Experiment 1 , 
Experiment 2, Experiment 3) me shown.
' Fable 6 . 1 shows specified concepts for experiments and candidate word lists for the concepts  . As for Experiment 1 and Experiment 2 , words that have no examples is omitted from candidate word lists  , since they won't never be selected . Fig . 6 . 1, Fig . 6 . 2 and Fig . 6 . 3 show examples and generated sentences for Experiment  1  , Experiment 2 and Experiment 3 each . Examples in Fig . 6 . 1, ' rod Fig . 6 . 2 are extracted from EDR English Corpus and examples in Fig  . 6 . 3 are extracted from a published printed English -Japanese dictionary  , though some modifications ( Tenses , aspects , Sentences in the lefthand sides of , arrows are original sentences and those in the right hand side are generated sentences  ( In generated sentences , only verbs are generated words and others are copied from origim d sentences  )  . Underlined words are words for the specified concepts  . For sentences with a circle at the head of left hand sides  , the generator select same words with those in the original sentences  . 
Sentences without circles include both right and wrong results  . 
In interlingua method , roughly speaking , all words corresponding to it concept are basically right its the generated word if it is grammatically consistent  . So the evaluation of tire experiments idelicate . 
The rates of coincides between original verbs and generoated verbs are  85%   ( Experiment 1 )  , 13% ( Experiment 2) and 16% ( Experiment 3) . Since some sentences without coincides can be also right  , the real rates of success are lager than above nt  , mbers . 
7. Conclusions
The English generation by the example-based meth + ? l is descrihed  . For experiments of verbs el . ' . + c tions , tile effective-hess of tile method is different for verbs to be generated  . ( In experiment 3 , for " confirn ?' and " endorse " the success rate is high  )  , It also depends on concepts and then unlber of candidate words  . 
Since examples are made automatically from large scale corpus and to make examples is easier than to make rules  , the effort to design the generator became little . By removing redtmdant basic units , the efficiency of examples is not serf

In this paper , only the experiments for verb selections are shown  . But the strategies that the generator uses should wiry in response to the categories of words to be generated  . For example , to generate prepositions the semantic is more important  , bnt to generate other functiomtl words the syntax is more important  . For verb selections , both are necessary . 
These strategies are also remained problems.
Table 6.1 Concepts and Word List
Expriments Specified Concept Candidate Word List E ? l ~ rimentI  ~3cee4f   ) ) acj?eve ( I~ , VI !) get ( EVl9tak\[E ) ( F . VF +) ( others are omitted)
Ext ~ riment 2 ( 3ceae3 ) ) get ( EVE ) grew ( EVE ) fall ( EVE )   ( others are omitted ) 
Experiment 3 ( Of dc5f ) ) accept ( EVE ) acknowledgie ( EVF . ) a ( ~ fit ( EVE ) allow ( EVE ) answer ( EVE ) appmve ( EVE ) confirm ( EVE ) endorse ( FVE ) grant ( EVF + ) receive )   ( EVE ) ratify ( EVE ) recognize l ( EVE ) respond ( EVE ) homologate ( EVF . ) ex . 01: lie had achieved a certain transquility . 
'-liehad , gina certain transquility.
ex . 02: Q ) You have ~ our keys.
--'- You have ~ our keys.
ex .03: (1) liequietly , got abroom.
-" lleqt , ietly ~ it broom.
ex .04: (.. ~ lleg ~ the menus.
-,-lie , tg ~. the menus.
ex . 05: ~) ln the storm 1 tookshelter under it lree . 
"In the storm I tookshelter under it tree.
ex .06: ( ) lle takes dangerous drugs.
--~ lle takes dangerous drugs.
ex .07: (. ~ The people look our old house.
The people took our old house.
Fig . 6 . 1l ~ xamples and Results of Exper in mnt1cx . 01: Diantondscome expensive . 
? ~+ Diamonds become xpensive.
ex . 02: You~rg LQ wolder.
, - You become older.
ex . 03: Athing was bc~conairlg increasingly sure . 
A thing was gct_tir ~ increasingly sure.
ex .04: lnvironment becomes individualized.
'- Enviromnent grows individualized.
ex . 05: Aman ~ o Many how.
--'- Aman becomes old any how.
ex .   06 : These letters became the center of my existence . 
" These letters went the center of my existett ce.
ex . 07: Almost unbearable my fantasies become.
Ah nost unbearable my fantasies go.
ex . 0g : Sonmthing bad ~ wrong.
++ Something had fallen wrong.
ex . 09: We had become good f , i ends during my stay at the hospital . 
-+ We bad ~ good friends during mystay at the hospital  . 
ex . 10: You're the kind to go_ violent.
-'- You ' retile kind to become violent.
ex . 11: ( ) tlere yes be came bright.
-" llere yes became bright.
ex . 12: Eventually it become a movie.
- ~ Fventually it ~ a movie.
ex . 13: After a while the signal became a buzz.
... After a while the signal wenlabuzz.
ex . 14: It was ~ glight.
-" It was becoming light.
ex . 15: I lefell silent , its yester day.
- ~ liewent silent , as yesterday.
ex . 16: After a few jokes his speech became serious . 
-+ After a few jokes his speech went serious.
ex . 17: You'llgg $. even fatter.
-'- You'll , rgre . weven fatter.
ex . 18: She became stout.
--" She ~ stout.
ex . 19: The fish has ~ bad.
+ The fish has become bad.
ex . 20: Q ) lle suddenly became we : tlthy.
+ lie suddenly became we althy.
ex . 21: She became impatient.
-" She went impatient.
ex . 22; (.) lie became apriest.
-" lie became a priest.
Fig . 6 . 2l + . xmnples and Results of F+xperiment 2 . 367 ex . 01: I ~ an invitation . 
-" I allow an invitation.
ex . 02:1~a noffer.
-" I . receive an offer.
ex .03: I acknowledge a defeat.
I acceota de feat.
ex .04: I acknowleclg ? his fight.
-+ I~his right.
ex .05: I acknowledge the truth of an argument.
-.-1 ~ the truth of an argument.
ex . 06: I admit a claim.
-" I allow a claim .
ex . 07: I admit defeat.
--" I acknowledge defeat.
ex . 08: I admit my guilt.
-~1 acknowledge my guilt.
ex . 09: I will admit no objection.
-" I will ~ no objection.
ex . 10: I allow a claim.
1 ~ a claim .
ex . 11: I allow your , argument.
I confirm your argument.
ex . 12: I answer his wish.
-"I receive his wish.
ex . 13: I~eabill.
-" I acceota billex . 14: I ap_prove a resolution . 
-, 1 confirm a resolution.
ex . 15:1, approve accounts.
-" I ~ accounts.
ex . 16: Q ) I confirm a treaty.
-"I confirm a treaty.
ex . 17: Q ) I confirm an appointment.
-"I confirm an appointment.
ex . 18:1. confirm a verbal promise.
- ~- I a_ ~ rove a verbal promise.
ex . 19: I confirmate legraphic order.
-~- I answer a telegraphic order.
ex . 20: I confirm possession to him.
-" I ~ , c knowledge possession to him.
ex . 21:1 confirm a functionary in his new office . 
-~1 ~ a functionary in his new office.
ex . 22: Q") I endorse his opinion.
I endorse his opinion.
ex . 23: OI endorse a policy.
-- ~ I endorse a policy.
ex . 24: I ~ a request.
- ~ I acknowledge a request.
ex . 25: The king granted the old wo man her wish.
-" The king answered the old wo man her wish.
ex . 26: Japan receive a treaty.
-" Japan ratifies a treaty.
ex . 27: QP arliament ratified the agreement.
--, Parliament ratified the agreement.
ex . 28: I receive a proposal.
--+ I ~ a proposal.
ex . 29: I receive an offer.
- ~ I accep K_an offer.
ex . 30: I receive a petition.
--" I answer a petition.
ex . 31: Q ) Priest receives his confession.
-" Priest receives his confession.
ex . 32: Priest receives his oath.
'- Priest ratifies his oath.
ex . 33: I recognize a claim as justified.
-- ~ I allow a claim as justified.
Fig . 6 . 3 Examples and Results of Experiment 3 ex .   34 : Japan recognizes the independence of a new state  . 
Japan acknowledges the independence of ...
ex .   35 : He ~ quickly to the appeal for subscriptions . 
-" He~quickly to the appeal for ...
Fig . 6 . 3 Examples and Results of Experiment 3 ( remainder ) 

Cui , J . , Komatsu , E . and Yasnhara , tl .  (1993) . A Calculation of Similarity between Words Using EDR Electronic Dictionary  . Reprint of ll ' SJ , Vol . 93, No . 1 ( in Japanese ) EDR (1993a ) . EDR Electronic Dictionary Specification
Guide . TR .04 l.
EDR (1993b ) . English Word Concept Dictionary . TR-026 Komatsu , E . , Cni , J . and Yasuhara , II .  (1993) . A Mono-lin-gual Corpus-Based Machine Translation of the Interlingua Method  . Fifth International Conference on Theoretical and
Methodological Issues
Nagao , M .  (1984) . A Framework of A Mechanical Translation between Japmlese and English by Analogy Principle  . 
Artificial and Human Intelligence ( A . Elithorn and R . 
Banerji , editors ) Elsevier Science Publishers , B . V . 
Sadler , V .  (1989) . Working with Analogical Semantics , Disambiguation Tect , niques in DLT , For is Publications , 
Dordrecht Holland.
Sato , S .  (1991) . Example-Based Translation Approach . 
Proc .   ( g ' International Workshop on Fundamental Research for the Future Generation of Natural Language Processing  , ATR Interpreting Telephony Research Laboratories , pp .  116 . 
Sumita , g . and Iida , 11 .  (1992) . Example-Based Transfer of Japanese Adnominal P articles into English  . IEICE TRANS . 
INF . & SYST ., VOL . E75-D , NO . 4
