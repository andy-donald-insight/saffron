Proceedings of the 23rd International Conference on Computational Linguistics ( Coling 2010), pages 680?688,
Beijing , August 2010
Filtered Ranking for Bootstrapping in Event Extraction
Shasha Liao Dept . of Computer Science New York University liaoss@cs.nyu.edu
Ralph Grishman Dept . of Computer Science New York University grishman@cs.nyu.edu Abstract Several researchers have proposed semisupervised learning methods for adapting event extraction systems to new event types . This paper investigates two kinds of bootstrapping methods used for event extraction : the document-centric and similarity-centric approaches , and proposes a filtered ranking method that combines the advantages of the two . We use a range of extraction tasks to compare the generality of this method to previous work . We analyze the results using two evaluation metrics and observe the effect of different training corpora . Experiments show that our new ranking method not only achieves higher performance on different evaluation metrics , but also is more stable across different bootstrapping corpora . 1 Introduction The goal of event extraction is to identify instances of a class of events in text , along with the arguments of the event ( the participants , place , and time ). In this paper we shall focus on the subproblem of identifying the events themselves . Event extraction systems from the early and mid 90s relied primarily on handcoded rules , which must be written anew for every task . Since then , supervised and semisupervised methods have been developed in order to build systems for new scenarios more easily . Supervised methods can perform quite well with enough training data , but annotating sufficient data may require months of labor.
Semisupervised methods aim to reduce the annotated data required , ideally to a small set of seeds . Most semisupervised event extractors seek to learn sets of patterns consisting of a predicate and some lexical or semantic constraints on its arguments . The semisupervised learning was based primarily on one of two assumptions : the document-centric approach , which assumes that relevant patterns should appear more frequently in relevant documents ( Riloff 1996; Yangarber et al 2000; Yangarber 2003; Surdeanu et al2006); and the similarity-centric approach , which assumes that relevant patterns should have lexically related terms ( Stevenson and Greenwood 2005, Greenwood and Stevenson 2006). An effective semisupervised extractor will have good performance over a range of extraction tasks and corpora . However , many of the learning procedures just cited have been tested on only one or two extraction tasks , so their generality is uncertain . To remedy this , we have tested learners based on both assumptions , targeting both a MUC ( Message Understanding Conference ) scenario and several ACE ( Automatic Content Extraction ) event types . We identify shortcomings of the prior bootstrapping methods , propose a more effective and stable ranking method , and consider the effect of different corpora and evaluation metrics . 2 Related Work The basic assumption of the document-centric approach is that documents containing a large number of patterns already identified as relevant to a particular IE scenario are likely to contain further relevant patterns . Riloff (1996) initiated this pattern matches a passage in a test document , a hiring event will be instantiated with the items matching the variables being the arguments of the event . So training an event extractor becomes primarily a task of acquiring these patterns . In a semisupervised setting , this involves ranking candidate patterns and accepting the top-ranked patterns at each iteration . Our goal was to create a more robust learner through improved pattern ranking . 3.1 Problems of Document-centric Bootstrapping Document-centric bootstrapping tries to find patterns with high frequency in relevant documents and low frequency in irrelevant documents . The assumption is that descriptions of the same event or the same type of event may occur multiple times in a document , and so a document containing a relevant pattern is more likely to contain more such patterns . This approach may end up extracting patterns for related events ; for example , start-position often comes with end-position events . This effect may be salutary if the extraction scenario includes these related events ( as in MUC6), but will pose a problem if the goal is to extract individual event types . Also , because an extra corpus for bootstrapping is needed , different corpora might perform quite differently ( see Figure 2). 3.2 Problems of Similarity-centric Bootstrapping Similarity-centric bootstrapping tries to find patterns with high lexical similarities . The most crucial issue is how to evaluate the similarity of two patterns , which is based on the similarity of two words . In this strategy , no extra corpus is needed , which eliminates the effort to find a good bootstrapping corpus , but a semantic dictionary that can provide word similarity is required . S&G used WordNet1 to provide word similarity information . However , in the similarity-centric approach , lexical polysemy can lead the bootstrapping down false paths . For example , for start-position ( hire ) events , ? name ? and ? charge ? are in the same Synset as appoint , but including these words is quite dangerous because they contain other common senses 1http://wordnet.princeton.edu / ?
Filter(p ) =
Yangarber(p ) Stevenson(p ) >= t 0 otherwise ?? ?? ?? where t is the threshold , which is initialized to 0.9 in our experiments . 4 System Description Our approach is similar to that for document-centric bootstrapping , but the ranking 2 We also tried using the product of the document relevance score and word similarity score , and found the results to be quite similar . Due to space limitations , we do not report these results here.
function is changed to incorporate lexical similarity information . For our experiments bootstrapping was terminated after a fixed number of iterations ; in practice , we would monitor performance on a heldout ( devtest ) sample and stop when it declines for k iterations . 4.1 Preprocessing Instead of limiting ourselves to surface syntactic relations , we want to get more general and meaningful patterns . To this end , we used semantic role labeling ( Gildea and Jurafsky , 2002) to generate the logical grammatical and predicate-argument representation automatically from a parse tree ( Meyers et al 2009). The output of the semantic labeling is the dependency representation of the text , where each sentence is a graph consisting of nodes ( corresponding to words ) and arcs . Each arc captures up to three relations between two words : (1) a SURFACE relation , the relation between a predicate and an argument in the parse tree of a sentence ; (2) a LOGIC1 ( grammatical logical ) relation which regularizes for lexical and syntactic phenomena like passive , relative clauses , and deleted subjects ; and (3) a LOGIC2 ( predicate-argument ) relation corresponding to relations in PropBank ( Palmer et al 2005) and NomBank In constructing extraction patterns from this graph , we take each dependency link along with its predicate-argument role ; if that role is null , we use its logical grammatical role , and finally , its surface role . For example , for the sentence : John is hit by Tom?s brother . we generate the patterns : < Arg1 hit John > < Arg0 hit brother > < Tpos brother Tom > where the first two represent LOGIC2 relations and the third a SURFACE relation . To reduce data sparseness , all inflected words are changed to their root form ( e.g . ? attackers???attacker ?), and all names are replaced by their ACE type ( person , organization , location , etc .), so the first pattern would become < Arg1 hit PERSON > 4.2 Document-based Ranking The document-centric method employs a ?
Re l i ( d ) =1? (1?Prec i ( p )) p?K ( d ) ? where K(d ) is the set of accepted patterns that match document d , and ?
Prec i+1 ( p ) = ? Re l i ( d ) d ? H ( p ) ? where H(p ) is the set of documents matching pattern p . Patterns are then ranked by ?
RankFun
Yangarber ( p ) =
Sup(p)
H(p ) * logSup(p ) where ( a generalization of Yangarber?s metric ), and the top-ranked candidates are added to the set of accepted patterns . 4.3 Pattern Similarity For two words , there are several ways to measure their similarity using WordNet , which can be roughly divided into two categories : distance-based , including Leacock and Chodorow (1998), Wu and Palmer (1994); and information content based , including Resnik (1995), Lin (1998), and Jiang and Conrath (1997). We follow S&G (2005)?s method and use the semantic similarity of concepts based on Information Content ( IC ). Every pattern consists of a predicate and a constraint (? argument ?) on its local syntactic context , and so the similarity of two patterns depends on the similarity of the predicates and the similarity of the arguments . We modified S&G?s structural similarity measure to reflect some differences in pattern structure : first , S&G only focus on patterns headed by verbs , while we include verbs , nouns and adjectives ; second , they only record the subject and object to a verb , while we record all argument relations ; third , our patterns only contain a predicate and a single constraint ( argument ), while their pattern might contain two arguments , subject and object . With two arguments , many more patterns are possible and the vector similarity calculation over all patterns in a large corpus becomes very time consuming . We do not limit ourselves to verb patterns because nouns and ( occasionally ) adjectives can also represent an event . For example , ? Stevenson?s promotion is a signal ?? expresses a start-position event . Moreover , in our pattern , we assume that the predicate is more important than constraint , because it is the root ( head ) of the pattern in the semantic graph structure , and place different weights on predicate and constraint . Finally , the similarity of two patterns p1 and p2 is computed as follows : where ?+?=1, f represents a predicate , r represent a role , and a represent an argument . In our experiment , ? is set to 0.6 and ? is set to 0.4. The role similarity is 1 for identical roles and for roles which generally correspond at the syntactic and predicate-argument level ( arg0 ? subj ; arg1 ? obj ); selected other role pairs are assigned a small positive similarity (0.1 or 0.2), and others 0. As with the document-centric method , bootstrapping begins by accepting a set of seed patterns . At each iteration , the procedure computes the similarity between all patterns in the training corpus and the currently accepted patterns and accepts the most similar pattern(s ). In S&G?s experiments the evaluation corpus also served as the training corpus . 5 Experiments There have been two types of event extraction tasks . One involved several ? elementary ? event types , such as ? attack ?, ? die ?, ? injure ? etc .; for example , the ACE 2005 evaluation3 used a set of 33 event types and subtypes . The other type involved a scenario ? a set of related events , like ? attacks and the damage , injury , and death they cause ?, or ? arrest , trial , sentencing etc .?. The 3See http://projects.ldc.upenn.edu/ace/docs/English-Events - Guidelines_v5.4.3.pdf for a description of this task.
683
MUC evaluations included two scenarios that have been the subject of considerable research on learning methods : terrorist incidents ( MUC-3/4) and executive succession ( MUC6). We conducted experiments on the MUC6 task to make a comparison to previous work . We also did experiments on ACE 2005 data , because it provides many distinct event types ; we conducted experiments on three disparate event types : attack , die , and start-position . Note that MUC6 identifies a scenario while ACE identifies specific event types , and types which are in the same MUC scenario might represent different ACE events . For example , the executive succession scenario ( MUC6) includes the start-position and end-position events in ACE . 5.1 Data Description There are four corpora used in the experiments : MUC6 corpora ? Bootstrapping : preselected data from the Reuters corpus ( Rose et al 2002) from 1996 and 1997, including 3000 related documents and 3000 randomly chosen unrelated documents ? Evaluation : MUC6 annotated data , including 200 documents ( official training and test ). We were guided by the MUC6 key file in annotating every document and sentence as relevant or irrelevant . ACE corpora ? Bootstrapping : untagged data from the Gigaword corpus from January 2006, including 14,171 English newswire articles from Agence France-Presse ( AFP ). ? Evaluation : ACE 2005 annotated ( training ) data , including 589 documents 5.2 Parameters used in Experiments In our bootstrapping process , we only extract patterns appearing more than 2 times in the corpus , and the similarity filter threshold is originally set to 0.9. If no patterns are found , it is reduced by 0.1 until new patterns are found . In each iteration , the top 3 patterns in the ranking function will be added to the seeds . For the similarity-centric method , only patterns appearing more than 2 times and in less than 30% of the documents will be extracted , which is the same as S&G?s approach.
5.3 MUC6 Experiments Our overall goal was to demonstrate that filtered ranking was in all cases competitive with and in at least some cases clearly superior to the earlier methods , over a range of extraction tasks and bootstrapping corpora . We began with the MUC6 task , where the efficacy of the earlier methods had already been demonstrated . < Arg0 resign Person > < Arg1 appoint Person > < Arg0 appoint Org_commercial > < Arg1 succeed Person > Table 1. Seeds for MUC6 evaluation For MUC6 evaluation , we follow S&G?s approach and assess extraction patterns by their ability to identify event-relevant sentences.4 The system treats a sentence as relevant if it matches an extraction pattern . Bootstrapping starts from four seeds which yield 80% precision and 24% recall for sentence filtering . To compare with previous work , we tested the filtered ranking method on two corpora : the first is the Reuters corpus used in S&G?s recreation of Yangarber?s experiment ( Filter1), to compare with their results for the document-centric method ; the second uses the test corpus as S&G did ( Filter2), to compare with their results for the similarity-centric method . We compare methods based on peak F score ; in practice , this would mean controlling the bootstrapping using a heldout test sample.
Figure 1. F score for different ranking methods on MUC6 evaluation Figure 1 showed that the filtered ranking 4 We also tried the document filtering evaluation introduced by Yangarber but , as S&G observed , this metric is too insensitive because over 50% of the documents in the MUC6 test set are relevant.
684 methods edge out both document and similarity-centric methods . Our scores are comparable to S&G?s , although they report somewhat better performance for similarity-centric than for document-centric (55 vs . 51) whereas document-centric did better for us . This difference may reflect differences in pattern generation ( discussed above ) and possibly differences in the specific corpora used . However , document-centric bootstrapping needs an extra corpus for bootstrapping ; S&G used a preselected corpus that contains approximately same number of relevant and irrelevant documents5. We wanted to check if such a corpus is essential for the document-centric method , and if the need for preselection can be reduced through filtered ranking . Thus , we set up another experiment to see if the document-centric method is stable or sensitive to different corpora . We used two additional corpora for MUC6 evaluation : one is a subset of the Wall Street Journal ( WSJ ) 1991 corpus , which contains 18,734 untagged documents ; the other is the Gigaword AFP corpus described in section 5.1. Both corpora are much larger than the Reuters corpus , and while we do not have precise information about relevant document density , the WSJ contains quite a few start-position events because it is primarily business news ; the Gigaword corpus ( AFP newswire ) has fewer start-position events because it contains a wider variety of news.
Figure 2. Document-centric and Filtered ranking results on different corpora for MUC6 Figure 2 showed that the document-centric method performs quite differently on different corpora , which indicates that a preselected corpus plays an important role in 5 The preselection of relevant and irrelevant documents is based on document metadata provided as part of the Reuters Corpus Volume I ( Rose et al , 2002).
document-centric ranking . It suggests that the percentage of relevant documents may be more important than the overall corpus size . The figure also shows that filtered ranking is much more stable across different corpora . Richer corpora still have better peak performance , but the difference is not quite as great ; also , peak performance on a given corpus is consistently better than the document-centric method . From the above experiments , we conclude that our filtering method is better in two aspects : first , bootstrapping on the same corpus performs better than either document or similarity-centric methods ; second , if we can not get a corpus with an assured high density of relevant documents , it is safer to use filtered ranking because it is more stable across different corpora . 5.4 ACE2005 Experiments The ACE2005 corpus includes annotations for 33 different event types and subtypes , offering us an opportunity to assess the generality of our methods across disparate event types . We selected 3 event types to report on here : ? Die : ? occurs whenever the life of a PERSON Entity ends . It can be accidental , intentional or self-inflicted .? This event appears 535 times in the corpus . ? Attack : ? is defined as a violent physical act causing harm or damage .? Attack events include a variety of subevents like ? person attack person ?, ? country invade country ?, and ? weapons attack locations ?. This event type appears 1120 times . ? Start-Position : ? occurs whenever a PERSON Entity begins working for ( or changes offices within ) an ORGANIZATION or GPE . This includes government officials starting their terms , whether elected or appointed ?. It appears 116 times in the corpus . We choose these three event types because they reflect the diversity of events ACE annotated : die events appear frequently in the ACE corpus and its definition is very clear ; attack events also appear frequently , but its definition is rather complicated and contains several different subevents ; start-position?s definition is clear , but it is relatively infrequent in the corpus . Based on the observations from the MUC6 corpus , we eschewed corpus preselection for methods , but still worse than the supervised method ; for start-position events , the semisupervised method beats the supervised method . The reason might be as follows : Die events appear frequently in ACE 2005, and most instances correspond to a small number of forms , so it is easy to find the correct patterns both from WordNet or related documents . As a result , filtered ranking provides no apparent benefit . Attack is a more complicated event including several subevents , which also have a lot of related events like die and injure . As a result , the document-centric method?s performance goes down much faster , because patterns for related event types get drawn in ; while the similarity-centric method performs worse than filtered ranking because some ambiguous words are introduced . For example , ? hit ? is an attack trigger , but words in the same Synset , such as ? reach ?, ? make ?, ? attain ?, ? gain ? are quite dangerous because most of the time , these words do not refer to an attack event . Start-position events do not appear frequently in ACE 2005, and supervised learning cannot achieve good performance because it can?t collect enough training samples . The similarity-centric and Filter2 methods , which also depend on the ACE 2005 corpus , do not perform well either . Filter1 performs quite well because the Gigaword AFP corpus is quite large and contains more relevant documents , although the percentage is very small . This confirms our assumption that filtered ranking can achieve reasonable performance on a large unselected corpus , which is especially useful when the event is rare in the evaluation corpus . < Arg1 kill Person > < Arg1 slay Person > < Arg1 death Person > Table 2. Seeds for Ace 2005 Die evaluation < Arg0 hire ORG > < Arg1 hire Person > < Arg1 appoint Person > < Arg0 appoint ORG > Table 3. Seeds for Ace 2005 Start-Position evaluation Figure 4. Performance on different ranking methods on ACE2005 word level evaluation 5.4.3 Word-level ACE Event Evaluation Word-level evaluation is different from sentence-level evaluation because patterns which appear around an event but do not predicate an event are penalized in this evaluation . For example , the pattern < Sbj chairman PERSON >, which arises from a phrase like ? PERSON was the chairman of COMPANY ?, appears much more in relevant start-position sentences than irrelevant sentences , and adding this pattern to the seeds will improve performance using the relevant-sentence metric . We would prefer a metric which discounted such patterns . As noted above , ACE event annotations contain triggers , which are more specific event locators than a sentence , and we use this as the basis for a more specific evaluation . Extracted patterns are used to identify event triggers instead of identifying relevant sentences . For every word w in the ACE corpus , we extract all the patterns whose predicate is w . If the event extraction patterns include one of these patterns , we tag w as a trigger . In word level evaluation , document-centric performs worse than the other methods . The reason is that some patterns appear often in the context of an event and are positive patterns for sentence level evaluation , but they do not actually predicate an event and are negative patterns in word level evaluation . In this situation , the document-centric method performs worse than the similarity-centric method , because it extracts many such patterns . For example , of the sentences which contain die events , 29% also contain attack events . Thus in word level evaluation , filtered ranking continues to outperform either document - or similarity-centric methods , and its advantage over document-centric methods is accentuated . 6 Conclusions In this paper , we propose a new ranking method in bootstrapping for event extraction and investigate the performance on different bootstrapping corpora with different ranking methods . This new method can block some irrelevant patterns coming from relevant documents , and , by preferring patterns from relevant documents , can eliminate some lexical ambiguity . Experiments show that this new ranking method performs better than previous ranking methods and is more stable across different corpora.
687
References D . Gildea and D . Jurafsky . 2002. Automatic Labeling of Semantic Roles . Computational Linguistics , 28:245?288. MA Greenwood , M . Stevenson . 2006. Improving semisupervised acquisition of relation extraction patterns . Proceedings of the Workshop on Information Extraction Beyond the Document , pages 29?35. Jay J . Jiang and David W . Conrath . 1997. Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy . In Proceedings of International Conference Research on Computational Linguistics ( ROCLING X ), Taiwan C . Leacock and M . Chodorow . 1998. Combining local context and WordNet similarity for word sense identification . In C . Fellbaum , editor , WordNet : An electronic lexical database , pages 265?283. MIT Press . D . Lin . 1998. An information-theoretic definition of similarity . In Proceedings of the International Conference on Machine Learning , Madison , August . A . Meyers , M . Kosaka , N . Xue , H . Ji , A . Sun , S . Liao and W . Xu . 2009. Automatic Recognition of Logical Relations for English , Chinese and Japanese in the GLARF Framework . In SEW-2009 ( Semantic Evaluations Workshop ) at NAACL HLT-2009 MUC . 1995. Proceedings of the Sixth Message Understanding Conference ( MUC6), San Mateo , CA . Morgan Kaufmann . Martha Palmer , Dan Gildea , and Paul Kingsbury , The Proposition Bank : A Corpus Annotated with Semantic Roles , Computational Linguistics , 31:1, 2005. Siddharth Patwardhan , Satanjeev Banerjee , and Ted Pedersen . 2003. Using measures of semantic relatedness for word sense disambiguation . In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics , Mexico City , Mexico . Patwardhan , S . and Riloff , E . 2007. Effective Information Extraction with Semantic Affinity Patterns and Relevant Regions . Proceedings of the 2007 Conference on Empirical Methods in Natural Language Processing ( EMNLP-07) T . Pedersen , S . Patwardhan , and J . Michelizzi . 2004. WordNet::Similarity - Measuring the Relatedness of Concepts . In Proceedings of the Nineteenth National Conference on Artificial Intelligence ( Intelligent Systems Demonstrations ), pages 1024-1025, San Jose , CA , July 2004. P . Resnik . 1995. Using information content to evaluate semantic similarity in a taxonomy . In Proceedings of the 14th International Joint Conference on Artificial Intelligence , pages 448?453, Montreal , August . Ellen Riloff . 1996. Automatically Generating Extraction Patterns from Untagged Text . In Proc . Thirteenth National Conference on Artificial Intelligence ( AAAI-96), 1996, pp . 1044-1049. T . Rose , M . Stevenson , and M . Whitehead . 2002. The Reuters Corpus Volume 1 - from Yesterday?s news to tomorrow?s language resources . In LREC02, pages 827?832, La Palmas , Spain . M . Stevenson and M . Greenwood . 2005. A Semantic Approach to IE Pattern Induction . Proceedings of ACL 2005. Mihai Surdeanu , Jordi Turmo , and Alicia Ageno . 2006. A Hybrid Approach for the Acquisition of Information Extraction Patterns . Proceedings of the EACL 2006 Workshop on Adaptive Text Extraction and Mining ( ATEM 2006) Z . Wu and M . Palmer . 1994. Verb semantics and lexical selection . In 32nd Annual Meeting of the Association for Computational Linguistics , pages 133?138, Las Cruces , New Mexico . Roman Yangarber ; Ralph Grishman ; Pasi Tapanainen ; Silja Huttunen . 2000. Automatic Acquisition of Domain Knowledge for Information Extraction . Proc . COLING 2000. Roman Yangarber . 2003. Counter-Training in Discovery of Semantic Patterns . Proceedings of ACL2003
