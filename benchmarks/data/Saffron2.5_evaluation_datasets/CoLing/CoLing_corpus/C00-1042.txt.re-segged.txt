Statistical Morphological Disambiguation for
Agglutinative Languages
Dilek Z . Hakkani-Tfir , Kemal Oflazer , GSkhan Tiir
\]) el ) a Itlncn(; of CO mlmt crEngin (', ; ring,
Bilkent University,
Ankara , 06533, TUll . KEY
hakkani , ko , tur ~ cs , bilkent , edu.tr

In this 1) aper , we present sta . tistical models for morphological disambiguation i Tm'kish  . Turkish presents an interesting problem for statistical  , nodcls since the potential tagset size is very large because of the productive  , derivational morl/hology . \ Vepropose to handle this by breaking Ul)1 ; 11( ; morhosyn-tactic tags into inflectional groups , each of which contains the inflectional features ti  ) reach ( internm-diate ) derived tbrm . Our statistical models score the probability of each morhosyntactic tag by considering statistics over the individual inflection groups in a trigram model  . Among the three models that we have deveh ) l ) ed and tested ,  ( ; 11( ; simplest model ignoring the lo ( : almort ) hota ( : ties within words l ) er-tbrms the best . Ollr ) (' . st ; rigram model 1) er for nl s with 93 . 95% accuracy onotirtest data getting all 1 ; 11 olllor hosyllta ( ; ti ( ; aild semantic fc . a tul'es correct . If we are just interested in syntactically relevant features all digilore a verys in all set of semantic features  , then (; tie accuracy increases to 95 . 07% . 
1 Introduction
Re ( : ent advances in ( : onltmterhar ( lware and avail-al ) ility of very large corpora have made (  ; t1(`-al ) pli-cation of s( ; a tistical techniques to natural language processing at  ) a sible and a very at ) pealing resem'ch area . Many useflll results have 1 ) cell obtained by apply ilig these techniques to English  ( and similar languages ) in parsing , word sense dismnbiguation , part of speech ( POS ) tagging , speech recognition , et ; c . However , languages like Turkish , Czech , Hungarian and Finnish , displ Wa substantially different behavior than English  . Unlike English , these languages have agglutinative or inflective morphology and relatively free constituent order  . Such languages have . received little previous attention in statistical processing  . 
In this lmper , wet ) resent our work on modeling Turkish using statistical methods  , and present re-suits on morphological disain biguation  . The methods developed here are certainly al ) plicable to other agglutinative languages , especially those involving productive derivational phenomena  . The Iml ) er is organized as follows : After a brief overview of related previous work  , we smnma . rize relevant aspects of Turkish and present details of various statistical models for nlorlfliological disanfl/iguation for Turkish  . We then present results and analyses fronlour experiments  . 
2 Related Work
There has been a large numl ) er of studies in tagging and mori ) hological disambiguation using various techniques . POS taggiug systems have used either a statistical or a rule-based approach  , hit he statistical api ) roach , a large corpus \] ms been used to train at ) rotm bilistic model wl fieh then has been used to tag new text  , assigning the most likely tag for a given word in a given context  ( e . g . , Church (1 . 988), Cuttingel ; al .  (1992)) . In the rule-based approach , a large mmfl > e . r of h and-craft ; ed linguisiic constraints are used to el infinate impossible tags or morphological t  ) arse . st bra given word in a given context ( Ka . rlsson et al , 1995) . Brill ( 1995a ) has presented a transfl ) rnmtioi > based lea . rning at ) l ) roach , whi ( : h induces disanlbiguation rules from tagged corpora  . 
Morphologi ( : ald is an lbiguation i inflecting or agglutinative languages with COlnl  ) lex morphology involves more than determining the major or minor Imrts-of-sl  ) cech of the lexie a . litems . Typically , roof phology marks a mlmber of inflectional or deriva-tioiml features and this involves ambiguity  . For instance , a given word nlay be chopl ) edup in difl'erent ways into mort ) heroes , a given mort ) heine mayinark different features depending on the morphotactics  , or lexicalized variants of derived words may interact with productively derived versions  ( see Ottazer and Tiir ( 1997 ) for the difl'erent kinds of morphological ambiguities in Turkish  . ) We assume that all syntactically relevant fcat ' urcs of wordforms have to be determined correctly for morphological disambigua  . -tion . 
In this context , there have l ) een some interesting previous studies for difl ' erent languages  . Levinger ctal .   ( 1995 ) have reported on an approach that learns morpholexical probabilities fi'omanml tagged e or lms mid have  . used the resulting infornlation in and Hla ( lk~i ( 1998 ) have used ntaxim unlentropy modeling approach for morphological dismnbigua-tion in Czech  . Ezeiza et al ( 1998 ) have combined stochastic and rule-based is ambiguation methods for Basque  . Megyesi ( 1 9991 has adapted Brill's POS tagger with extended lexical templates to Itungar-tan  . 
Previous ai ) proaches to morphological dismnbi-guation of Turkish text  ; had employed a constraint-based approach ( Otlazer and Kuru Sz , 1994; Oflazer and Tiir , 1996; Oflazer and Tiir ,  1997) . Although results obtained earlier in these at ) preaches were reasonable , the fact that tim constraint rules were hand crafted posed a rather serious impedimento the generality and improvement of these systems  . 
3 Turkish
Turkish is a flee constituent order language . Tlmorder of the constituents may clmnge freely according to tim discourse context and the syntactic role of the constituents i indicated by their case marking  . 
Turkish has agglutinative morphology with productive inflectional and derixmtional suflixations  . The number of wordforms one can derive from a Turkish root  ; formm W be in the millions ( ttankmner ,  19891 . 
Hence , the number of distinct word forms , i . e . , the vocabulary size , can be very large . For instance , Table 1 shows the size of the vocabulary for I and 10 million word corl ) or a of Turkish , collected from online new spaI ) ers . This large vocabulary is the reason
Corpus size Vocabulary size 1M words 106 , 547 10M words 41 . 7 , 7 75 Table 1: Vocabulary sizes for two Turkish corpora . 
for a serious data sparseness problem and also significantly increases the number of parameters to be estimated even for a bigram language model  . The size of the vocabulary also causes the perplexity to be large  ( although this is not an issue in morphological disambiguation  )  . Table 2 list stlm training and test set perplexities of trigram language models trained on  1 and 10 million word corpora for ri51rkish  . 
For each corpus , tile first cohm m is the perplexity for the data the language model is trained on  , and the second column is the pert ) lexity for previously unseen test data of 1 million words . Another major reason for the high perplexity of Turkish is the high percentage of out of vocabulary words  ( words in the test ; data which did not occur in the training data ) ; this results from the productivity of the word t brmation process  . 
Training Training Set Test Set (1M words )
Data Perplexity Perplexity 1M words 66 . 13 t4 49 . 81 10M words 94 . 08 1084 . 13 Table 2: The pert ) lexity of Turkish corpora using word-based trigram language models  . 
The issue of large vocabulary brought in by productive inflectional and derivational processes also makes tagset design an important issue  . 111 languages like English , then unlber of POS tags that can be assigned to the words in a text  ; is rather l infited ( less than 100 , though some researchers have used large tagsets to refine g  , : anularity , but they are still small compared to Turkish . ) But , such a finite tags et al ) proach for languages like Turkish may lead to an inevitable loss of information  . The reason for this is that the lnorphological features of intermediate derivations can contain markers for syntactic rela-tionshil  ) s . Thus , leaving out this information witl f in a fixed -tagset scheme may prevent crucial syntactic information fl ' om being represented  ( Oilazer et al . , 1999) . For examl ) le , it ; is not clear what POS tagsllould be assigned to the words a  . ~ lamlaqtwmak ( below ) , without losing any information , the category of the root ; ( Adjective ) , tile final category of the word as a whole ( Noun ) or one of the intermediate categories ( Verb )  . 1sa\[flam+laq+t , r+ma~:saglam+kdj^DB+yerb+Become^DB+gerb + Caus+Pos^DB+Noun+Inf +  A3sg+Pnon+lqom to ca'ass (  , s ' ometh , i . ng ) to becomes tron9/to strength , or , /fortify ( somcth , ing ) Ignoring the fact that the root ; word is an adjective may severany relations lfips with a  . n adverbial modifier modi ~ ying the root . Thus instead of a sim-I ) le POS tag , wc use the full r no ~ T flt , o Iogicala ' nahts cs of the words , rcprcs cntcd as a combination of \]' ca-tures ( including any dcrivational markers ) as their morphosyntactic tags . For instance in the exami ) leabove , we would use everything including the root ; form as the morphosyntactic tag . 
In order to alleviate the data sparseness probleln we break down the fl fll tags  . We represent each word as a sequence of inflectional groups  ( IGs hereafter )  , separated by " DBs denoting derivation boundaries  , as described by Oflazer (1999) . Thus a morphological parse would be represented in the following general tbrm:t The morphological features other than the POSs are : + Become : become verb  , + Cans : causative verb , + Pos : Positive polarity , + Inf:marker that derives an infinitive form fl ' om a verb  , +A asg:3sg number-person agreement , + Pnon : No possessive agreement , m~d + Nora : Nominative case . " DB's mark derivational boundaries . 

Full ~\ [ hgs(Noroots ) hfltectional GrouI)s

OO 9,129
O1) served 10 , 531 2 , 194% fl ) le 3: Numbers of q2tgs and iGsroot+IGi ~ DB+IG2 ~DB+---^DB+IG . 
where IGi denotes relevant inflectional feal ; urcs of the inflectional groul ) s , including the 1 ) art-ofsl ) eech for the root or any of the derived forms . 
For exaln lf le , the infinitive , tbtms(u . ~lamla . #' trmak given above would be ret ) resented with the adjective reading of the roots a . rf lamm M thet bllowing 4IGs : 1 . Adj 2 . Verb+Become 3 . Verb+Caus+Posd . Noun+Inf+A3sg+Pnon+NomTable 31) rovides a(' , oml ) arison of them nnl ) er dis-l , in ( : t full morl ) hosyntactic tags ( ignoring the root words in this case ) midIGs , generativ ( dy1) ossil ) lea . nd observed in a ( : or tms of 1M words ( considering a \]\[ ambiguities )  . One can see thai ; the ' nmn ber observed till tags ignoring the root words is very high  , significantly higher than quoted tbrCzech by Ita . ji5 and Itladk 5 (1998) . 
4 Statistical Morphological
Disambiguation
Morphoh)gica . 1 disambiguation is the prol ) lcun of tinding the . correspondings (; qucnce , of morl/hological parses ( including l ; heroot ) , 7' = t ~ ~= ll , 12 ,   .   .   .   , l ,   ,   , given a sequence of words 1? =' w ~' = 1 u1 , ' W2 ,   . . . ~' lUn . 
Our at ) proachix to model the ( list rilm tion of lil Or-phological I ) arscs give , n the words , using a hidden Markov model , and then to seek the variable 7' , I . hat maxilnizes . I'(TII'V)::/'TP(W )) = ~ . ~,, laxP ( T ) ? P ( W lT ) (2)

The term P ( W ) ix a constant for all choices of T , and can thus be ignored when choosing the most probable  7'  . \ VeC ~ lll further simplify the t ) roblem using the a SSUlnlil ; i on that words arc ind c'i ) endent of each other given their tags . In Tm'ldsh we can use the additional simplification that \]'  ( wilti ) = l since l , i ill cludes timroot fbrm and all morphosyntactic t~a-tures to uniquely determine the word f ' or m  . 2 Since 2' l'hat is , we assume that there is no morphological generation ambiguity  . This is a hnost always true . There are at b . wwordfin'ms like flelir kcne and horde , which have the mo , l , - ~/ soz '(' ,   ,   , ; It T ): P (* , ,~I *~) = 1 , w(;~u ~ , vri ,   , : 7b
P ( WIT ) = I\]P ( w ; It ~') = 1i--\]a , lld ~ / rgl ~ . , xP(SqW ) = arg?11Hix\])(T ) (3) 7' !1'
Now
P (~') = P ( t ,   , lt ~'-') xP ( t , -ilt ~'-2)x .   .   . 
xP(t~ltl ) ? l)(tl)
Simplifying fin % her with the trigram tag model , we get :
P ( T ) = P ( t , dt , ,_ . , . , Z , _,)?l'(t , ,_lIt ,,-:~, t ,, - . , ) x .   .   . 
v(l,:~l~,,~ . _ , ) ? e(t~lt .   , ) ? z '( ~ , ) = flP ( l , i l t i_e , ti_l ) (4) i=1 w in , to w , , d  ~ , ti , ,; P(z . , It-,,z . 0) = I"(z : 1), p(z . , I *, ,, 1, 1) =
P ( tuItl ) to simplif3, the notation.
If we consider morl ) hoh ) gi ( : alanalyses as a se- ( t11011 ( : ( 2 of root ; 111 ( l\]Gs , each parse t , ican \]) ercp-?res(;ntedas(Ii , IGi ,   ,   .   .   .   , IGi ,   ,   ,  )  , where ni is then u in be r() t " IG's in the , in , word . : ~ This rel ) resental . ioil changes the l ) ro ) lemas shown in Figure 1 wher ( ' , the , chain rule has been used to factor out the individual comt  ) oncnts . 
This f(irtlttll ~ ltioll still suffers from 1: , t1(! data spat'so-ll(SS1) robl clll . Toallo , via tcl;\]lis ~ w cill ~ ll , \[ cthQfolkiw-ing siml)lifying assumlitions : 1 . A root wor (1 del ) ends only onl ; he roots of the 1) revious words , all dix in det ) cndent of the inflectional and derivation a\] productions on the in : l '  ( nl ( n - ~ , IG ~_ . ,4, . . . , 1G ~_ . ,,,,  . ,), ( ri-a , I6'i_~,~, .   .   . ,/G ~_~,,,_,)) =
P (" ~ l "~- ~ , n - , )( ~')
The intention here is that this will be usefll l in tile disambigua ? ion of the root word when a given form hasmori  ) hological parses with dif-fiwent root words . So , t br instance , for disambiguating the surface , for madam with the following two parses : same morphological parses with the wordforms g clirk cn and here tic  , respectively but are i ) ronounced ( and writte . n ) sllghlly differently . These . m'e rarely seen in written te . xts , and can thus l)e . ignored . 
aln our training and W . st data , then mnbcr of 1Gs in a word form is on the average 1  . 6, the . refore , ni is usually 1 or 2 . We . 
have seen , occasionally , word tbrms with 5 or 6 inflectional groups . 

I ) ( ti\[t1-1)z
I ) ( tiIti-2, ti1)
P (( ri , IGi , l .   .   . IGi , n ~)\[( ri-2, IGi-2, 1 .   .   . IGi-'2, ni_2), ( ri-1,IGi-l,1 .   .   . IGi-i , , i-,))P ( ril(ri-2,\[Gi-2, 1 .   .   . IGi-~,,,_2), ( ri-1, IGi-La .   .   . IGi-1,, zi_,)) xP(IGij\[(ri-2,\[Gi-'e,1 . . . IGi-'e,ni_2), ( I'i-1, IOi-l , 1 . . . \[Gi_l,ni_,),I'i)x .   .   . XP ( IGi , m\[(ri-u , IGi-2, 1 . . . \[Gi-2,,~,_=),(ri-l,IG~-I,1 . . . \[Gi-l , , ,_~), ri , IGi , 1, . , . , i Gi , m-l ) Figure 1: Equation tbr morphological disambiguation ( a ) adam+Noun+A3sg+Pnon+Nom ( man )   ( b ) ada + Noun + A3sg+Plsg+Nom ( y isl and ) in the iloun phrase k ' trm ~ z ~ kazakhadam ( the man with a reds weater )  , only the roots ( along with the part-of speech of the root ) of the previous words will be used to select the right root  . 
Note that tile selection of the root has some impact on what the next IG in the word is  , but we assuine that IGs are determined by the syntactic context and not by the root  . 
2 . An interesting observation that we can make about q_hrkish is that when a word is consid-ere  ( l as a sequence of IGs , syntactic relations are between the last IG of a ( dependent ) word and with some ( including the last ) IG of the ( head ) word on the right ( with n finore X Cel ) - tions )   ( Oflazer ,  1999) . 
Based on these assumptions and the equation in Figure  1  , we define three models , all of which are based on word level trigrams : 1 . Model 1: The presence of IGs in a word only depends on the final IGs of the previous words  . 
This model ignores any morphotactical relation between an IG and any previous IG in the same word  . 
2 . Model 2: The presence of IGs in a word only depends on the final IGs of the previous words and the previous IG in tile same word  . In this model , we consider morphotactical relations and assume that an IG  ( except the first one ) in a wordform has some dependency on tile previous IG  . Given that on the average a word has about 1 . 6 IGs , IG bigrams should be sufficient . 
3 . Model 3: This is the same as Model 2 , except that the dependence with the previous IG in a word is assumed to be in delmndent of the dependence on the final IGs of the previous words  . 
This allows the formulation to separate the contributions of the morphotactics and syntax  . 
The equations for these models are shown in Figure  2  . We also have built a baseline model based on when tags are decomposed into inflectional groups  . 
tile standard definition of the tagging problem in Equation  2  . For the baseline , we have assumed that the part of the morphological nalys is after the root word is the tag in the conventional sense  ( and the assumption that P ( wi \] ti ) = 1 no longer holds )  . 
5 Experiments and Results
To evaluate our models , we frst trained our models and then tried to morphologically disambiguate our test data  . For statistical modeling we used SRILM-the SRI language modeling toolkit  ( Stolcke ,  1999) . 
Both the test data and training data were collected from the web resources of a Turkish daily newspN  ) er . The tokens were analyzed using the morphological analyzer  , developed by Oflazer (1994) . Them n biguity of the training data was then reduced fl'om  1  . 75 to 1 . 55 using a preprocessor , that disambiguates lexicalized and nonlexicalized collocations and removes certain obviously impossible parses  , and trigs to analyze unknown words with all unkllown word processor  . The training data consists of the unambiguous sequences  ( US ) consisting of about 650K tokens in a corpus of imillion tokens , and two sets of manually dismnbiguated corpora of  12  , 000 and 20 , 000 tokens . Tile idea of using unambiguous sequences is similar to Brill's work on unsupervised learning of disambiguation rules for POS tagging  ( 199517 )  . 
The test data consists of 2763 tokens ,  935  ( ~34?/0 ) of which have more than one morphological nalys is after preprocessing  . The ambiguity of the test data was reduced from 1 . 74 to 1 . 53 after prct ) rocessing . 
As our evaluation metric , we used accuracy defined as follows :  #of correct parses x  100 accuracy =  #o . f tokens The accuracy results are given in Table 4 . For all cases , our models pertbrmed better than baseline tag model  . As expected , the tag model suffered considerably from data sparseness  . Using all of our training data . , we achieved an accuracy of 93 . 95%, wl fich is 2 . 5 7% points better titantile tag model trained using the same amount of data  . Models 2 and 3 gave Model 1: This model assumes that unIG in ~ word depends on the last IGs of the two previous words  . 
P ( IGi , t:\[(ri-~,1Gi-2, ~ . . . Gi-'~,,~_ . 2), ( ri-1, IGi-l , ~, .   .   . , IGi-~,,,~_~), ri,IGi,~, .   .   . , IGi , t , -~)) I " l1(G ~, ~ . IIG ~-~, , , ,_~,-/Gi-l,ni_l)
Ther (; for e , l ) ( ti\[ti-~, ti-1)
II ik = . l(0)
Model 2: The model as smn ( ~s that in addition to th ( ~ dcl ) (mdonci (  ; s in Model 1 , an IG also ( lot ) ends on tim previous IG in the s~mm word . 
P(1Gi,~ . l(ri_ . ~, IGi-~,~ . . . IGi-~,~,i . , ), ( ri - ~

IGi_~,,, .   .   . , IGi1,, ~_ , ), ri , lGi,1, . . . ,lGi,~ . --1) = ?\[ I ~'( ~ c ~, ~ . l~(;~--~,,,,_~,~(;~-~,,,,_,,IG~,k . _ , ) (7) k=l Model 3: This is same as Model 2 , except them or t ) hot actic and syntactic dt'4) t ; ntlenci( ; sarc considered to bc independent . 
\])(\]Gi,kl(l"i-2,lGi-2,1 . . . .\[Gi-2,,,i . , ), ( ri-l , . ld/i- . i , 1, . . . ,\] Gi-l , ,, ~_1), ri , lGi , l, .   .   . , l-Gi , k-l ) =) ~\]) ( IGil . \[IGi . .2,,,~  . , , t ' (~ i_l , , , , . l(~ri , t, . -- l ) r . l?lwr(;forc,P ( ti\[ti-2, l . i-j ) = l )( ~' i \] ri_ . ~, ri_l)x\](1(,,,11 .  , . IG , _u), .   .   .   . , , , IOi-i , , , ,  , ) x

In order to simpli\[ytheuotation , w clmved ct lnc d the foll c , v:ing : ) , t ?
I(IGi , t . llGi,k_l = l'(1Gi , ~ . ll Gi . _,,, .  ,  . IGij , , , ,_,)xP ( IGi , ~, . )
I'(IGi , t.IIGi , , ~- I))
F (', ' ~ l','-,,','o ) = IX ',',) j~(, . :+ . (,,, . , ) = r , ( , . + . , ) l(m ,, ~ l ?(,_, .   .   .   . , , mo , , , o ) = J ( . rc ~, ~ , , ), , , = 17, , . I(IG2, t\[IGo . . . . . . ICt , , , , ) ( IG2 , 11 IG , , , , , ) 1 ( IGi , l\[IGi-2 ,   , ~i _ ~ , IGi-1 ,   , ~ i_l , IGi , o)P ( IGI , I~IIG-I . . . . 1, IGo,~o , IGx,k-1)
P ( IG2 , t\[IGo , ~ o , IG1 , ~1 , IG ' ~ , t-l ) = P ( IGi , lllGi_ . 2  ,   ,   , i _ ,   , IGi_l ,   ,   , i_~) = P ( IGI , ~IlGI , , , _I ) = I(IG2 , t\[IG1 , , ~ I , IG2 , t_I)P ( IG2 , ~\[ IGI ,   ,   . , IG2, o ) = P ( IG2, , IIG ~ . . . . ) P ( fG i , l l \ [ a i , o ) = I ; ' ( ~ r ( . T ~ i , 1) for k = 1, 2, . .  . , ' hi,1 = 1, 2, . . . , n  ~ , and i = 1, 2, . . . , ' n . 
Figure 2: Equutions for Modcls \], 2, and 3.

Training Data ~ Lag Model Model 1 Model 1 Model 2 Model 3   ( Baseline )   ( Bigram ) Unambiguou sequences ( US )  86  . 75% 88  . 21% 89  . 06% 87  . 01% 87 . 19% US+12,000 words 91 . 34% 93  . 52% 93  . 34% 92  . d3% 92 . 72% US+32,000 words 91 . 34% 93  . 95% 93  . 56% 92  . 87% 92 . 94% Table 4: Accuracy results for difli ; rent models . 
similar results , Model 2 suffered from data sparse-hess slightly more than Model  3  , as expected . 
Surprisingly , the bigram version of Model I(i . e . , Equation (7) , but with bigrams in root and IG models ) , also performs quite well . If we consider just the syntactically relevant morl  ) hological features and ignore any senlantic features that we markinul or phol-ogy  , the accuracy increases a bit flirt , her . These stem ti'om two properties of % lrkish : Most Turkish root words also have a proper noun reading  , when written with the first letter cai ) italized . 4 We (: ount it ; as an error if the tagger does not get the correct  1  ) roper noun marking , for a proper noun . But this is usua \] lyimpossil ) le especially at the begim fing of sentences where the tagger cannot exploit caI  ) italization and has to back off to a lower-order model  . In a hnost all of such cases , all syntactically relevant morl ) hosyn-tactic features except the proper noun marking are actually correct  . Another imi ) ortantease is the pronoun o , which hast ) oth personal proll ottll ( s/he ) and demonstrative 1 ) ronoun readings ( it )   ( in addition to a syntactically distinct determiner reading  ( that ) ) . 
Resolution of this is always by semantic cousi ( ler-atious . When we count as ( : orreetm~y errors involving such selnantic marker cases  , we get an accuracy of 95 . 07% with the best (', as ( ; ( cf .  93 . 91% of the Model 1) . This is slightly 1 ) etter than the precision figures that is reported earlier on morphological disambiguation of Turkish using constraint-based techniques  ( Oflazer and T/Jr ,  1997) . Our re-suits are slightly better than the results on Czech of Haji ~ and Hla  ( lkg ( 1998 )  . Megyesi (1999) reports a 95 . 53% accuracy on Hungarian ( a language whose features relevant othistask are very close to those of Turkish  )  , with just the POS tags 1) eing correct . In our model this corresponds to the root and the POS tag of the last IG  1  ) eing correct and the accuracy of our best model with this assumi  ) tion is 96 . 07% . 
When POS tags and subtags are considered , the reported accuracy for Hungarian is 91 . 94% while the corresl ) onding accuracy in our case is 95 . 07% . We . 
can also note that the results presented by Ezeiza et al  ( 1998 ) for Basque are better titanours . The main reason t br this is that they eml ) loya much more sot ) histicated ( comt ) ared to our t ) reprocessor ) d in fact , any word form is ai ) oten tial first name or a last na II 10 . 
constraint-grammar based system which im I ) rovest ) recision without reducing recall . Statistical techniques applied at ' ~ er this disaml ) iguation yield a better accuracy compared to starting from a more  am-1  ) iguous initial state . 
Since our models assmned that we have independent models for disambiguating the root words  , and the IOs , we ran experiments to see the contribution of the individual models  . Table 5 summm'izes the accuracy results of the individual models for the best case  ( Model 1 in Table 4 . )
Model Accuracy
IG Model 92.08%
Root Model 8 0.36%
Combined Model 93.95%
Table 5: The contril ) ution of the individual model sibr the 1 ) est case . 
There are quite a number of classes of words which are always ambiguous and the t  ) reprocessing that we have employed in creating the unambiguous sequences ca  . nnever resolve these cases . Tlms statistical models trained using only the unambiguous sequences as the training data do not handle these ambiguous cases at all  . This is why the accuracy results with only unaln biguou sequences are significantly lower  ( row 1 in Table 4 )  . The manually dismnl ) iguated training sets have such mn biguities resolved  , so those models perform much better . 
An analysis of the errors indicates the following : In  15% of the errors , the last IG of the word is incorrect ) ut the root and the rest of the IOs , if any , are correct . In 3% of the errors , the last IG of the word is correct but the either the root or SOlne of the previous IGs are incorrect  . In 82% of the errors , neither the last IG no rany of the previous IOs are correct  . Along a different dimension , in about 51% of the errors , the root and its part-of-speech are not determined correctly  , while in 84% of the errors , the root and the tirst IG combination is not correctly determined  . 
2906 Conclusions
W ( ; have 1 ) resented an ai ) l ) roacht ( ) slatisti ( : almodeling fl ) ragglutinativ ( : lmlguages , esi ) ( ; (: i ally those having l ) roducl ; ived ( ; rivational 1)\]: ( ulomena . ()urai)-l ) roa (' hessentia . lly involves l ) re . al : ingut ) the full m(/r-t ) hological ana . lys is a cross(l(~'rivational boundaries midl ; reai ; ing the (: Onll ) On(mt ; s ; Issul ) tags , and l ; helt determining the corre(:l ; se(tuenc (' , of tags viasl ; al ; is-ticall ; echniques . This , to our knowl(~ . (lge,isth(' . first detailed attempt in statistical modeling of agghtt in a-rive langua  . g('~s and (: ancerl ; aJnlyl ) (' . al ) plied to other such lmL guages likelt mlgari:m ; rodFimfish with 1 ) re-duetive derivational morl ) hology . 
7 Acknowledgments
We tlmnk Andreas Stoleke of SIHSTA Ilml ) tbrpro-vi ( lingus wil ; hthe\]anguag ( ; mod (' . lingt ; oolkit and ti ) rvery help flfl dis(:ussions on I ; his work . IAz Stl riberg of SRISTARL abs , and Bilge Say of Middle East Te ( : hnical University hf formal ; i(:s\]nstitul ; e , 1) rovided hell ) rid insights and (: ommenl ; s . 

F , ri (: Brill . 1995a . Trat : slbrmal . i()n-1) as(~derr()r-(h:iv(ml('m'ningmid ha . ruralangm ~ gcpro ( : ( : ssing : A case stu ( lyin 1 ) art-ofsl ) (~e ( : h lagging . Compul , a-tional Lin 9' u , istics ,  21 (4):543 566 , l ) ceein l)er . 
Eric Brill .  19951) . Unsupervised learning of disam-Mguation rule's for l  ) art of st ) (w~ ( : h (  ; agging . In l ) ~ v-eeedings of th , eThirdl , Vorksh , op on Very \] , a ~: ( I(:
Corpora , Carat ) ridge , MA , . hme.
K(umeth W . Church .  1 . 988 . As to (: hasti (: parts 1) ro-p ~ r/lllall(\] ; * llOllll phrase l ) ars(r for llll(sl ; li( ; \[ ; ((1t(:t . Inl'roeeediv , 9 s of I , h , e , 5' eemMCo ' , @ re'm:e on AppliedNat'm'alLang'ua9el)~vcessi'n9 , Austin , ~lZ'xas . 
1) ougCutting , Julim : Kupiec , Jan Pedersen , and l ) enelope Sibun .  1992 . A practic a limri ;- of-st ) eech tagger , llt : Proceeding so J " l , h , eThird Co'nfercnce on Applied Natural Language . l ) ~ vees . sin9, Trenl;o,

N . Ezeiza , I . Alegria , J . M . Arriola , R . Urizar , and I . Aduriz .  1998 . Coln bining stochastic and rule-I ) as edm ( ; l , hods for dismnl ) iguation i agglutina , rivebmgu ; ~ ges . In Proceedin9s of the 36 tl ' Annual Meeting of th , e Association for Computational Linguistics and 17 th International Co ~@ r-enecon Computational Linguistics  , pages 379384 , Montreal , Quebec , Canada , August . 
Jan Haji ~ and Barl ) or a Hladk S .  1998 . Tagging hfllective languages : Predictiol : of morphological categories fl  ) rarich , smtcCuredt , ~gseI ; . In Proceedings of COLING/ACL'98, pages 483-490,
Montreal , Canada , Augusl;.
Jorge Hankmner , 1989 . Lezieal Ile presentation and Process , chapter Mort ) hological Parsing and the
Lexicon . The MIT Press.
l , ?ed Karlsson , Atro Voutilainen , Juha \] teikkilii , and Arto Anl : tila .  1995 . Constraint Gramm , ar-A Language Inde . pcndentSystem for I > arsi'n 9Un re- . ~trieted Tezt . Mouton de Gruyter . 
Moshc I~oving ; cr , Uzzi Ornan , and Aton Itai .  1995 . 
Learning morpholexical probabilities fl'oman untagged corpus with an application to He-tire w  . Comp ' utational Lin 9uisties , 21(3):383404,

Be:l~Megyesi .  1999 . Iml ) roving Brill's POS tagger f(/ra . n agglutinative language . In Pascal (' , Fung and Joe Zhou , editors , \]) roeeed in 9 softh , e , \] oint , 5' IGDAT ' Confere . ' nee on E'm , pirieal Methods in Natural La'n9uage I ' rocess in 9 and Ve Ul Lmyle Co , 7 ~ ora , pages 275-284 , College \] ? ark , Maryland , 
USA , June.
Kema . 1 ( ) llazer and Ilker Km'uSz .  1994 . Tagging and morphological disambiguation f ~ l Strkish text  . Inl'roeeed in w of the 4u ' Applied Nal , ' m ' al Language Processing Co ~@ renee , t)ages 144149 . ACL , ( ) c-

I ie mal ( ) flazerm~dGSktmn' . l ' ir .  1996 . Coml ) in-ing hand-t:rafl ; (~ d rules a . ndunsul ) (' . rv is ( ~( ll ; arn-ing in (: on straint-t ) as (' . dmorphological disaml ) igua-ti(m . In 1, 3 ri (: Brill and Kemw . thChur(:h,editors,\]"lvceedin9 . softll . eA(fL-SIGDATCo'nil'fence on \], ) mpiriealMeth . odsi'nNal,'m'alLan . qv , ageP ~ veess-in 9 . 
t ( cmal Otlazer and GSklmnTilt .  1997 . Mor-1)h () logi(:aldisaml ) iguation l ) yvol ; ing (: onstra . in(;s . 
In Proeeeding . s of the 35 u'A'n'n'u al Meeting of the A , s'sociatio'n for Computational Li'nguistie . s(A(H,'OT/lC~ACL'97), Madrid , Sl/ain , July . 
\]( . cmal()tlaz(!r , 1) iM:Z . Hal:kani-Tiir , mid G6k hanT/Jr .  1999 . l ) (; signiora Turkish : ; reebank . Inl ' , v-e('ed in 9s of Workshop on Ling'uisl , ieally l'nt , e ~ T n'el , r ' , dColTmra , al , I ~ , ACL'99 , Bergen , Norway ,   . June . .
Kcmal Otlazer .  1994 . Two-level description of Turkish morphology . Literary and l , inguistie Co'reput-ing ,  9(2):137 148 . 
K( . ' m~d()t lazer .  1 . 999 . Del ) endency pa . rsing with an extended i in itestate al ) proach . It : Proceedings of the 371 , hAnn'ual Meeting of the Association for Co'm , irutational Linguistics , College Park , Maryland ,   . hme . 
Andreas Stolcke .  1999 . SRILM -- the S1H language mo(teling toolkit , http://www . speech , sri . corn/projects/srilm / . 

