Explaining away ambiguity : Learning verb selectional preference 
with Bayesian networks *
Massimiliano Ciaramita and Mark Johnson
Cognitive and l , inguistic Sci('.n(:es
Box\]978, Brown University
Pr()vi(h;n(:e , l,I02912, USA
mass?m ?: lJ_ano_ciaramita@broun . edumj@cs . broun , edu
Abstract
This t ) at ) er presents a Bayesian l node l for unsu-t ) ( ; rvised learning of v ( ; rbs ( ; le(-t ; ionalt ) refer ( ; nc('s . 
For each vert ) the model creates a 13 ~ Wo . si an n(~t work who search ii ; e(:lan'(~is(l(fl ; (wmin( ; d t ) . 5' the h' , xical hicr~m:hy of W () r(hmtmt d whos('~1) ar~mmt ; ( ; rs are (' ~ sl ; im ; ~l : (~ d from a list ; of v ( 'd'l ) -ol ) je ( ' t pairs\]'cramfromatort ) us . " lgxl ) laining away " , t~wellknownt)rop('xi;y of Baycsi~mnetworks , helps the moth ; 1  ( teal in a natural fashion with word sense aml ) iguity in tlw , training ( tat ~ L . () na , word sense disamt ) igu ; ~ tionLest our model t ) er formed l ) ctl ; c , r thanot ; h(' , rstal ; (~ of tim art systems for unSUl ) crvis ( ~d learning ( ) t7 seh'x:-tionM t ) r ( d'er ( mces . Coml ) utational (: Oml ) lcxityl ) rol ) lems , wn . ysofim proving tiffs ; tl ) l ) roa ( : hmM methods for iml ) h ' menting " ( ' xt ) laining away " in oth (  ; rgraphical frameworks are dis('ussed . 
1 Se lec t iona l p re ference and sense ambigu i tyR  ( ' . gularil ; i('~s of avcrt ) with rcsl ) e(:tot ; lw . semantic class of its m:guments(sul)j (' . cl : , ol ) j (' . (: l ; mM indirecto \]) je(:l ; ) arc called selectional prefer-enees ( S1 ) )  ( Katz and Fodor , 1964; Chomsky , 1965; Johnson-Laird ,  1983) . The verb pilot carries the information thal ; its ol ) je cl ; will likely 1) e some kind of veh , icle ; sut ) jects of timvert ) th , in t , : t(md to 1) e h , ' uman ;   ; rodsul ) jects of the verb barkl ; endl ; ol ) c , dogs . For the sake of simt ) licity we will locus on the verl ) -ot ) je ( : t relation all ; hough the techniques we will describe can be at ) t ) li (  ; d to other verb-argument pairs . 
* We wouhllike to ttmnk the Brown Lal ) or at ; ory for Linguistic Inibrmation Processing ; Thomas II of lnann ; Elie Bienenstoek ; 1) hilip Resnik , who provided us with training and test data ; and Daniel Oarcia for his hel ) with the SMILE lil ) rary of ( : lassest brBayesia networks that we used for our exl  ) eriments . This research was SUl ) l ) or ted 1 ) yNSF awards 97 20368 , 9870676 and 9812169 . 
. EN77TY ._, vomet\]tiug-"~,"-.."~,~.
._"" CtHHy
FOOl ) I , IQUll ) PIISI : h'ICAI . Olt J Ii CT . . . . . "? /' i "\, I,\\\ .   .   .   . ; -"\\, ICK . lbod/II . IM liNTIII'VI ; 'RA( ; I " WAT/:'RliqltidlANDolejeet":?/ 71<4 bel'i ' rat ~ e ( ' OI"H' . ' I ' ; drink carlhl and ISLAND ~ roll ( ' APE <' q ~ lFeI ; 'S1't?1:'S . ';0  . IAVA-I , I/'~VA-2IIAI , Ii . ~ hmdt .   .   .   .   .   .   .   .   .   .   .   .   .   .   . i e . v~rcs . ~ o . \] av ~ bali
Figm'e1:A1) or tion of Wordnet.
Models of the acquisition of SP arc impor- ; ant in their own right and h ; w (' , at ) plic ~ tt . ions in N ~ tural , anguage l ~ ro (: essing ( NIA') . The selcc-tional l ) rclhr ( ; nc( ; so f ~ L verb can b ( ; used t ; o i n t i ; rI ; hel ) ossil ) \] c meanings of an ml known re'gum ( mr of a known verb ; e . g . , it ; might be possibh ; to infer that x zzz is ~ kindot ! dog front the tbllow-ing sentence : " The : rzJ:z barked all night "  . In p~rsing ; ~ sentence seh , c tionall ) refe , rcn (: escan1)(; used to rank competing parses , providing a par-timnlt ' , asur ( ; of scmmlt ; icwell-for lnedness , in-v ( ' stigating SI ) might hell ) us to understand the structure of the mental lexicon  . 
Systems for ml supervised learning of SP usually combine statistical aim  knowledge-1  ) ased approaches . The knowledge base component is typic Mlya database that groups words into classes  . In the models w (' will see . the knowledge base is Wordnet ( Miller , 1990) . Wordnet groups nouns in to c , lasses of synonyms ret ) resenting concel)ts , called synsets , e . g . , car , ,,' ld,o,a,'utomobilc, .   .   . . A noun that lm-hmgs to sew:ral synsets is ambiguous  . A tran-is defined between synsets . A synset is a hyponym of another synset if the tbrmer has the latter as a broader concept  ; for example , BEV-ERAGE is a hyponym of LIQUID . Figure 1 depicts a portion of the hierarchy . 
The statistical component consists of predicate -argument pairs extracted from a corpus in which the semantic lass of the words is not indicated  . A trivial algorithm might get a list of words that occurred as objects of the verb and output the semantic classes the words belong to according to Wordnet  . 
For example , if the verb drink occurred with ' water and water ELIQUID  , then lodel would learn that drink selects tbr LIQUID  . As Resnik ( 1997 ) and abney and Light ( 1999 ) have found , the main problem these systems face is the presence of ambiguous words in the training data  . If the word java also occurred as an object of drink  , since java CBE VERAGE and java CISLAND , this model would learn that drink selects tbr both BEVERAGE and 

More complex models have been t ) roposed.
These models , though , deal with word sense ambiguity by applying an unselective strategy similar to the one above  ; i . e . , they assmne that anfl ) iguous words provide equal evidence t brall their senses  . These models choose as the con-eepts the verb selects tbr those that are in common among several words  ( e . g . , BEVERAGE above ) . This strategy works to the extent that these overlapping senses are also the concepts the verb selects tbr  . 
2 Prev ious approaches to learn ing se lect iona l preference  2  . 1 P~esnik's model Ours system is closely related to those proposed in  ( Resnik , 1997) and ( Atmey and Light ,  1999) . 
The fact ; that a predicate p selects for a class c , given a syntactic relation r , can be represented as a relation , selects ( p , r , c ); e . g . , that eat selects for FOOD in object position can be represented as selects  ( eat , object , FOOD ) . 
In ( Resnik ,  1997 ) selectional preference is quan-tiffed by comparing the prior distribution of a given class c appearing as an argument  , P ( c ) ~ and the conditional probability of ; lie same class given a predicate and a syntac-
COGNI 770 N1/4FOOD 7/4
ESSENCE 1/4 FLESH 1/4 FRUH " 1/2 BREAD I/2 DAIRY I/2 l ' Jlllidea ( O ) meal ( l ) apple ( I ) bagel ( l ) cheese ( l ) Figure 2: Simplified Wordnet . The numbers next to the synsets represent he values of freq  ( p , r , c ) estimated using (3) , then mnbersinl ) aren theses represent the values of frcq ( p , r , w ) . 
tic relation P ( c\[p , r ), e . g . , P(FOOD ) and J ( OODleat , object ) , The relativentropy between P ( c ) and P ( clp , r ) measure shownmch the predicate constrains its arguments : 
S(p , r ) = D(P ( clp , r)IIP(c )) (1)
Resnik defines the selectional association of a predicate for a particular class c to be the portion of the selectional preference strength due to that class : P  ( clP , ~') 1P ( clp , r ) log(2)A(p , ,  .   , c)-S(v ,   ,  '  ) P ( c ) Here the main problem is the estimation of P ( clp , r ) . Resnik suggests as a plausil ) leestimator /5 ( clp , r ) de = r freq(p , r , c)/freq(p , r ) .   13ut since the model is trained on data that are not sense-tagged  , there is no obvious way to estimate freq(p , r , c ) . Resnik suggests considering each observation of a word as evidence t breach of the classes  ; lieword belongs to , count(p , r , w)c ) aasses(w ) (3) ~ uc:e where count(p , r , w ) is then mn be r of times the word w occurred as an argument of p in relation r  , and classes ( w ) is the number of classes w belongs to . For exmnple , suppose the system is trained on ( eat , object ) pairs and the verb occurred once each with meat , apple , bagel , and cheese , and Wordnet is simplified as in Figure 2 . An ambiguous word like meat provides evidence also tbr classes that appear unrelated to those selected by the verb  . 
Resnik's assumption is that only the classese -lected by the verb will be associated with each /  / - "   1\8  \ \ -  . ~ S
COGNIllON () FOOl ) 1\7"- .  " - - -( - -  .   1   1 ' I'I'ideame at apph'bagel cheeae Figure 3: The HMM version of the siml ) le example . 
of the observed words , and hence will re ( : eive the highest values for l ' ( clp , r ) . Using (3) we fill ( t that the highesl ; fre(luen ( ; y is in t '~ t ( ; tas-social ; ed with FOOD : . f ' req(ea , I , objecl , , food ) I+~+7_1_21~~--717 an ( tI ' ( leOODie . , l ,) = 0 . 44 . 
H ( ) wever , some eviden (' eistom M also for COG-NITION : . fr(:q(cat , , obj , cl , , co9'~ , , it , io ' ~ , ,) ~ ? and
I'(COGNIT'ION ieat ) = 0.06.
2.2 Abney and Light's approach
At mey and Light (1999) pointed ou I ; l ; h ~ t t l ; he distril ) ution of se . llSeS of all anfl ) iguous word is no ; unifornl . '\[' tiey not ; iced also l : hai ; it is nol ; clear how the 1) rol ) ability l '((:\[ p , 7") is t ; o 1) e i n i ; er-t ) reteds in ( : e there is no exl ) lie its to ( : llasl , i(:geil-(' , rationnlodel involved . 
They\])rol)ose(tasyst ; enll;hat;ass()(' , ial ; esa Hidden Markov Model ( \[ IMM ) wii ; h0 , a(:hl)re(lic~te-re , lal ; ion 1) air(p , r ) . ' l ~' ansil ; ions between synsel ; states rel ) resent he hyt ) on ynly relation , and c , the empty word , is emitted with probal ) ility 1 ; transitions to ~ t tinal sta ; eenli ; a word w with 1) rot ) al ) i lity 0 < P (' m ) < I . 35 an-siton and emission t ) rol ) al ) ilities are estimated using the \]' DM ; dg or it tn no ni ; raining data I ; hat consist of the lieu is that o ( ; clirrext with the w ; rb . 
Abney and Ligh , ' smo(lele , stintates i'(clp , r ) ti ' om the model trained tbr(p , r ); the disl , ri-lint on P ( c )   ( : ml1 ) e calculat xxt from a model trained for all n ( mns in the ( : or pus . 
This model did not perti ) rm as well as expected . Allamt ) iguous word ill then lodel call be generated 1 ) y more than ones l ; ;~te sequence . 
At mey and Light dis (: overed that the EMal-goril ; hm tinds t ) a rame terw flues that associate some t ) rol ) ability mass with all the trmlsitions in the l nultil  ) le paths that lead to an ambiguous word . In other words , when there are several state sequences fi ) r the same word , \] DM does not select one of l ; hen : over the others . I Figure 3 shows the I ) arameters es l ; imated by EM for the same examt ) leas above . The transition to the COGNITION sl ; ate has \] ) een assign e ( tat ) rol ) a-1 ) lily of 1/8 because it is part of ~ t possible l ) at h tomeat . The IIMM nlodel does not solvet , hel ) roblent of the unselective distribution of the frequen  ( : y o f o c e u r r e n ( : e of a naml ) iguous word to all its senses . A1) ney and Light claimed that ; this is a serious l ) roblem , par ; i (: iilarly when l ; heaml ) iguous word is a ti'equent one , and cruised the model to learn the wrongs eleel ; ional pref-eren (' es . To (: or re(:i ; this undesiral ) leout come they introduced some smoothing and t ) alam:ing te , chniques . Howe , ver , even with these modiliea-tions their sysl ; em'sl ) er for nlance , wast ) elowfllal ; a (: hieved l ) yResnik's . 
3 Bayesian networks
A Bayesian network ( Pearl , 1988), or
Bayesian 1)elelnel ; work ( BBN) , e o n s i s i ; so fasol ; of variables and a sel ; of directed edges (: on-neel ; ing the w ~ riat)les . The variables and tile edges detine , a dire , (: te , dacyclic graph ( DAG ) where each w trial ) leisrei ) resented 1 ) y ~ node . 
Ea ( : hvmi~d ) leisasso ( : iated with a finite number of ( nmi ; u ; dlyex (' lusive ) sl ; ates . '1)) each wu:ial ) leA with \]); n'eni;s\]31, . . . , I7~ is ; t;l;;mll(' , (l ; tcondi-tio'n , al probability tabh ' , ( CPT ) l'(A\[131 ,   . . . , Hn ) . 
Given a BBN , Baye simlint ~ rence (: ~ m1) e used 1 ; oesi ; ilm~l ; emarginal and posterior proba-bilities given the evidence at h and ~ m d  (  ; It (' , in-fornlation six ) red in the CPTs , the prior prob-abilities , by means of B~yes'rule , P ( HIE ) = l'(H)P ( s~ln)P ( E) , where It stands fi ) rhyt ) othesis and
Et breviden (: e.
Baye , siannel ; works displaym lexl ; remely inter-eslfing t ) roi ) ert ; y called explaining away . Word sense mn biguity in the 1 ) recess of learning SP de , -tines a 1) rot ) lem that nlight , l ) esolved by a model that imt ) lements an explaining away strategy . 
Sul)t ) ose we ; ~ relearning the , selectional 1) refer-en (: e of drink , and the network ill Figure 4 is the ' As an mtt ; er of fi * cl ;  , for this HMM there are ( infinitely ) many i ) a rame l ; ervahles that nmxin fize the likelihood of t ; he training data ; i . e . , l ; hei ) arame , l;ers are , not ; idenl ; ifiable . The intuil ; ively correct solution is one of l ; hell l , \]) ILLSO are infinitely lilalty (); her , intuitively incor-re(:t ; ones . Thus il , is no surprisel ; hat the EM algorithm emmet ; lind the intuitively correct so hlt ; ion . 

IISIAND ?> BEVERAGE.\](11'(10 WllICF
Figure 4: A Bayesian network for word a in bigu-ity.
knowledge base . The verb occurred with java and water . This situation can be ret ) resented as a Bayesian network . The variables ISLAND and BEVERAGE represent concepts in a semantic hierarchy  . The w triables java and water stand for I ) ossible instantiations of the concet ) ts . 
All the w , riables are Boolean ; i . e . , they are associated with two states , true or false . Suppose the tbllowing CPTs define the priors associated with each node  . 2 The unconditional probabilities are P ( I = t , '~ , ,e ) = P ( B = t , '~ , ,~0 = 0 . 01 and P ( I = false ) = I " (13 = false ) = 0 . 99, and the
CPTs for the child nodes are
I =>) I1,131,~13~I,13-~I,~f3j=true 0 . 99 ( I . 99 0 . 99 0 . 01 j = false 0 . 01 0 . 01 0 . 01 0 . 99 w = true 0 . 99 0 . 99 0 . 01 ( I . 01 w false .  0 . 01 0 . 01 0 . 99 0 . 99 These vahms mean that the occu , ' rence of either concept is a priori unlikely . If either concept is true the word java is likely to occur  . Similarly , if BEVERAGE occurs it ; is likely to observe also the word water . As the posterior probabilities show , if java occurs , the belief ~ in both concepts increase : P ( II . j ) = P ( BIj ) = 0 . 3355 . However , ' water provides evidence for BEVERAGE only . 
Overall there is more evidence for the hypothesis that the concept being expressed is BEV-ERAGE and not ISLAND  . Bayesian networks implement his inference scheme ; if we compute the conditional probabilities given that both words occurred  , we obtain P ( BIj , w ) = 0 . 98 and P ( I\[j , w ) = 0 . 02 . The new evidence caused the " island " hyt ) othesis to be explained away ! 3 . 1 The relevance of priors Explaining away seems to depend on the specification of the prior prolmt  ) ilities . The priors 2I ,  13 , j and w abbreviate ISLAND , 1 lEVERAGE , java and water , respectively . 
(-) coaNmo,v)roe,\[0ii~\]L_m
Figm'e 5: A Bayesian network for the simple example . 
define the background knowledge awdlable to the model relative to the conditional probabilities of the events represented by the variables  , but also about the joint distributions of several events  . In the simple network above , we defined the probat ) ility that either concept is selected ( i . e . , that the correst ) onding variable is true ) to be extremely small . Intuitively , there are many concepts and the probability of observing any particular one is small  . This means that the joint probability of the two events is much higher in the case in which only one of them is true  ( 0 . 0099 ) than in the case in which they are both true ( 0 . 0001) . Therefore , via the priors , we introduced a bias according to which the hypothesis that one concept is selected will bet /wored over two cooccurring ones  . This is a general pattern of Bayesian networks ; the prior cause simpler explanations to be preferred over more complex ones  , and thereby the explaining away effect . 
4 A Bayesian network approach to learning select ional preference  4  . 1 Structure and parameters of the model The hierarchy of nouns in Wordnet defines a DAG  . Its mapping into a BBN is straightibr-ward . Each word or synset in Wordnet is a node in the network  . If A is a hyponym of B there is an are in the network from B to A  . All the variables are Boolean . A synset node is true if the verb selects for that class  . A word node is true if the word can appear as an argument of the verb  . The priors are defined t bllowing two intuitive principles  . First , it is unlikely that a verb a priori selects for troy particular synset  . 
Second , if a verb does select for a synset , say FOOD , then it ; is likely that it also selects tbr ; H ) t)Iy ( ; o words : it is likely l ; h~tawor(t ; q ) I ) ears as an m:gmnent of t , h ( ; verl ) if the vert ) seh ; (:l ; s for any of il ; spossible senses . Onl ; heother h~m(t , if ( , he verb does nol ; selecl ; for a synsel; , it ; is ' u , n likely that the words in sl ; anl ; b~l ; ing ( ; he synse( ; occur ~ sits ; ~rgumen~s . "Likely " and " unlikely " are given mml ( ; rical values l ; h  ~ l ; Sllill 1 lt ) 1 ; o1 . 
The following l ; at ) le defines l ; \] te scheme for the CPTs associated wil ; heach node in the nel ; work ; pi(X ) ( lcnot ; cs1 ; 12 (: il . h,t2ar(:n(,of(;h(:no(t(:X . 
F __\] . P ( X = xlI,I(X)V , . . . , VI ,,( X ) = t,",,(~)\]z . lalsc , unlihcly\[_Tl "( X = : , : lp , ( X ) a ,   .   .   . ,/>,,( X ) = . l ', , . Z . ~(:)\]: t; . fitls (: like . l ? / For(;h (; rool ; nod(:s , the l ; ;d)le r(:(hl(:es 1 ; o ( ; h(:uncondil ; ion ~ dt ) rol ) al/iliI ; y of ( ; h(:node . Now WO ,  ( ; UII ; (: Si ;  ( ; hemo ( Mon ( ; h(:siml)l(:ex~mq)\](:seen(:~rli(:r . W+isth(:set of wor(lsl ; ha(o(:-(:m'rc(t with I ; hev (: rl ) . ' l . '\] l(:\]2 o(l(:s(:orr(:st)on(l-ing ( ; o ; hew or ( lsinl/l/q ; w(:s(:l , 1; olr , , , c;rod(;h(:o(;hersl(:f(;mls(:l; . For 1; 12 (: l ) revious example W-I = mca/ , ~ apph ' . , bagel , ch,c~c' . sc ' . , mt (\] l ; h ( ; (: or rest ) on ( ting no ( t(:sm:csol ;  ( ; oI/rue , as ( t(:-t)i (: l;e(t in Figm'(;5 . Wil ; hlil , : ( dy;m(t ', . nhT ~: c' . lyr(:sl ) (:(: l;iv(:ly equalIx ) 0 . 9 . 0 mM 0 . 01 , tit(:i ) osl ; (:- riorl ) rol ) ; d)ilil ; i (: s area /)(/ i'\[m , a . , b , c . ) = 0 . 9899 mM . P ( Cl , m , , a , b , c ) = 0 . 0101 . Expla . ining a way works . Th ( ; t ) osl ; ( ; rior 1) rol ) ; fl)ilil ; y of COGNI-TION g ( ; l ; saslo wasi ( ; sprior , whcr (' , as l ; h ( ; 1) rol ) al ) ilil ; y of FOOD goesut ) to almost ;  1 . A13 ~ y( ; sim~n ( ; l ; work ~ q)t/roa(:\]~seems1 ; o ; l(:l ; u~dl . yimt ) lean cn ( ; he . conse . rva/ , ' ive , stral ; egyw(:l ; hough ( ;  ( ; o1) e( ; he corr ( ; (: I ; one . for unsupervisc(tl(;~mfing of sehx-t , ional resi ; ri(:tions . 
4.2 Computational issues in building
BBNs based on Wordnet
Th(:imt ) l(:m(:ni ; ~d ; ionfa BBN for ( ; h ( ; whole of W ( ) r(lnel ; fax : as (: oml ) ul ; al ; ional (: oml ) lexi ; ypro)-l(:ms ( ; ypi(:alofgraphi(:altoo(Ms . A(l(:ns(:ly(:ommcI ; ed\]IBN presents ( ; wo kinds of l ) rol ) l(:ms . 
The tirst is ( ; hest orage of the CPTs . The size of a CPT grow sext Ionenl ; i all y with then mn be r of pa rents of the node . 4 This prol ) lem can liea F , C , m , a , b and crespec ( ; ively stand for FOOD , COGNITION , meat , apph ' . , bagd and chces ( : 4 Some words in \ Vordnet have mor ( : than 20 senses . 
For ( , Xaml ) h: , line in \' Vordne(:isasso (: ia ( , ed with 25
EN 77TY ? j ? " ~.

I "0()1)I,IQUID\-..//
BEVERAGE('OFI"EI?drink
I , IAVA-I
I > IISY SIC'ALOB , II'2 CT1 AND t
ISIANI ) >/.//-
JAVA-2
X ./:/ j <, ra
Figure . 6: Th(; sulmel ; workford ' riuJ < .
solved 1) yot)i ; in fizing the r(:l ) r(:s( ; nl ; ~ti ; ionf thes (: ( ; ; d)l(:s . In our case most of l , h(:(:ntri(:sh~w(:l ; hes~tln('~v ; ~luos ~~ ttl(l~tCOlll\])a , (:( ; rel ) resenl ; al ; ion for l ; lmln ( ; & nl )( ; fOlll(\](ltlll ( ; h like l ; h(' , () hellS(Xlin(;h (' , noisy-OR too ( tel ( Pearl ,  1)88)) . 
Ah ; ~ r(l(:rlirol ) lemislmr forming in f(; rc\]me.
The gr ; q ) hi ( : alsl ; rlt('i ; llr(~ , of & BBN r ( ; t)resenl ; sl ; h(:(l(:t ) (: n(len(:yr(:lal ; ions among th (: rml ( lomw trial ) lcs of the , nel ; work , r \] ? he ~ d goril ; hlns use ( twil ; ltB\]IN susmdly l ) (M'orm inference t ) y ( ty-mmfi ( " t ) rogrmmning on the tri~mgul~d ; e ( t l n or ~ d g r ~ q ) h . Alow(n " 1) ( mn(to nl , hemmfl ) er of (: om-l ) ul ; al ; ionsl ; h ; ~( ; aren (: (:(: ssa . ryI ; ( ) mo(t(:Il ; h ( ; joint(lisl ; ritmi ; ionov(:rl ; h(:wn'bd ) h:s using su(:h ~ dgo-ril ; hms is 21"1 tIwh(:r(:'r ~ , ist;\]m size of (: h(;ma . x-imall ) ( mn(tarys(:l ; a (: (: or ( ling ( ; ot ; hc visil ; a , tions (: h(:(lul(: . 
4.3 Subnetworks and balancing
B( , (: mls(; of l ; h(:s ( ,  1 ) rol ) h:msw ( :  ( : ( told not t ) uihlasing l ( : BBN for Wor ( hmI ;  . In sl ; e ~ Mw (: simt/litie(t ( ; hesl ; rll('l ; ur (: of 1 ; 12 (: model by building a smallers ut mei ; work for each 1) re(ticate-argumenl ; pair . A sulm (: two rk cons is ( ; sof ( ; hemlion of the s(:ts of ml (: ( ; sl ; ors of the words in W + . Figure . 6 pro-vid ( : sml example of the union of these : % nces-tral sul  ) grat ) hs " of Wordne (  ; for ( ; he words java ~ m(ldrink(COml ) ~ Wei ( ; wil ; h Figure 1) . 
This siml ) liti ( : ation ( toes not at f e ( : t the ( : om-pul ; ~ tion of the ( tistril ) ui ; ions we are in l ; ( ; resl ; edin ; l ; h ; fl ; is , the marginals of the synset nodes . 
ABBN provi ( tesacoral ) act representation t br the join I ; disl ; rit ) ution over the set of variables senses . The size of its OPT is therefor (' .  2 2(~ . Six ) ringa(:a-1)Ie of tloa ( ; numbers tbrl ; his node alone requires around ( 2'- ) ~ ) 8=537 MBytes of memory . 
191 in the network . If N = X1, . . . , Xni8a Bayesian network with variables X1, . . . , Xn , its joint distribution P ( N ) is the product of all the conditional probabilities pecified in the network  , 
P ( N ) = IIP(XJlp , ,( Xj )) (4)
J where pa(X ) is the set of parents of X . ABBN generates a factorization of the joint distribution over its variables  . Consider a network of three nodes A , B ~ C with arcs fl ' omA to \] 5 and C . Its joint distribution can be characterized as P ( A , B , C ) = P ( A)P ( BIA)P ( CIA) . If there is no evidence for C the joint distribution is 
P(A , B , C ) = P ( A ) P ( BIA ) ~ P ( CIA ) c = P ( A ) P ( BIA ) = P ( A , B ) The node C gets marginalized out . Marginaliz-ing over a childless node is equivalent ore moving it with its connections from the network  . 
Therefore the subnetworks are equivalent to the whole network  ; i . e . , they have the same joint distribution . 
Our model comtmtes the value of P ( c\[p , r ) , lint we did not compute the prior P ( c ) for all n ( mns in the cortms . We assumed this to be a constant , equal to the ' u ' nlihcly w flue , for all classes . In a BBN the w dues of the marginals increase with their distance fl ' om the root nodes  . 
To avoid undesired bias ( see table of results ) we defined a balancing formula that adjusted the conditional probabilities of the CPTs in such a way that we got  ; all timmarginals to have approximately the same wdue  ) 5 Experiments and results a 5 . 1 Learn ing of se leet iona l pre ferences When trained on t  ) redicate-argument pairs extracted from a large corpus  , the San . Jose Mercury Corpus , the model gave very good results . 
The corpus contains about 1 . a million verb-object tokens . The obtained rankings of classes according to their posterior marginal probabilities were good  . Table 1 shows the top and the ' ~ More details can be found in an extended version of the paper : www  . cog . brown . edu/~massi/ . 
6For these experimc'nts we used values for the likely and unlikely  1  ) a rameters of 0 . 9 and 0 . 1 ~ respectively . 
Ranking Synset P(clp , r)1 VEI IIC LE 0 . 99952 VESSEL 0 . 98933 AIRCRAFT 0 . 99374 AIRPLANE 0 . 95005 SHIP 0 . 91 14 255 CONCEPT 0 . 1002256 LAW 0 . 1001257 PIIILOSOPIIY 0 . 1000258, IUI ?, ISPRUDENCE 0 . 1000 Table 1: Results tbr(maneuver , object ) . 
bottom of the list of synsets for the verb maneuver  . Tile model learned that maneuver " selects " for melnbers of the class VEtt lCLE and of other plausible classes  , hyponynls of I/E HI-CLE . It also learned that the verb does not select for direct  ; objects that are inembers of (: lasses , like CONCEPT or PItILOSO PltY . 
5.2 Word sense disambiguation test
A directew fluation measure for unsupervised learning of SP models does not exist  . Thesel nodels are instead evaluated on a word sense disambiguation test  ( WSD )  . The idea is that systems that learn SP produce word sense dis-amt  ) iguation as a side-effect . Java might be in-terl ) reted as the island or the beverage , but in a context like " the tourists flew to Java "the former seems more correct  , because fly could select for geographic locations but not for beverages  . 
A system trained on a predicate p should heable to disambiguate arguments of p if it has learned its selectional restrictions  . 
We tested our model using the test and training data developed by Resnik  ( see Resnik ,  1997) . The same test was used in ( Almey and Light ,  1999) . The training data consists of predicate-object ounts extracted fl ' oln  4/5 of the Brown corpus ( at ) out 1M words )  . The test set consists of predicate-object pairs from the remaining  1/5 of the corpus , which has been manually sense-annotated by Wordnet researchers  . The results are shown in Table 2 . 
The baseline algorithm chooses at random one of the multiple senses of an ambiguous word  . 
The " first sense " method always chooses the most frequent sense  ( such a system should be trained on sense-tagged data  )  . Our model per-
Baseline 28.5%
AbneymMLight(HMM smootlm(l ) 35.6%
Abney and Light ( It MM1) alan(:ed ) 42.3%
Resnik 44.3%
BBN ( without bM~mcing ) 45.6%
BBN ( with b Mancing ) 51.4(/(/
First Sense 82.5 (/0
Table 2: R , esults formed 1) etterth ; m the state of tlmartmo ( tels for ml SU l ) ervised le~n'ning of SP . It seems to define i ~ l ) ett (; resi ; ima , l ; or for \]) ( ell) , ' r ) . 
It is remnrkabh : fliat the model ~ mhi ( :ved this result making only a limi (  ; (: ( tuse of distributional informal ; ion . An ( mn is in 14f+ifitoe-('urredath ; astonce inth (: tra , ining set , 1) ut thesy si ; em does not know if i ( ; o(:(: urre(lonce or several times ; either i to c(:urred or it didn't . Then l ( )( te , l did not ; suffer to omu (: h frox n this limi -( ; ntion(htril ~ g ( ; his task . This is 1 ) rol ) ~d ) ly ( tucto the Sl ) arsencss of the (  ; r l t i n i n g ( t ; tl ; a for ( ; tie test . For each verb ( ; heaver agemmfl ) er ( ) fel)-\]eel ; tyl)es is 3 . 3 , for each of them tim : werng (: l xumber of ( ; ok(ms is 1 . 3; i . e . , most of the , words in the training data . only ( )( : (: urred once . Pin " this training set we~dsot ( : sted a version of (  ; hem ( ) delth atrail ( ; it word node ti ) reachel ) serve do l)je(:l ; token ; ~ n(( ; here , foreinl , ( ~ gral ; e(t the ( listri-lmtional informntion . ( ) n ( ; \] mWSI ) test it ; per-ti ) rmed exactly the stone its the simph ~ r version . 
When trained on the , San . lose Mercury Cort ) us the model l ) erfornle ( two rse on the WSI ) t : esl ;  (35 . 8%) . This is not too surprising considering ( ; he dift bxel mesbe A ; ween tim SJM and the \] ~ rown ( : or t ) or ~: the former , i ~ re(:ent newswire cortms ; the llg ; ter , & Xlolder , ) alml(:ed('or l ) uS . Allot ; heriln portmlt factor is the different relevance of distributional informntion  . The training ( tatn fl ' om the SJMC or lms arennl chri ( : her and noisier than the Brown data . Here the , fl'equen (: yin-tbrm ~ tion is probably crucb fl ; however , in this case we could notimt ) lement the siln l ) les ( ' hem ( ~  ; tbove . 
5.3 Conclusion
Explaining awayimt ) lements ; ~  ( : ognitively ~ tt-tractive and successful strategy  . A straighttbr-ward lint ) rove , men ( would lmt br theme ( tel to make fl fll use of the distrilmtional ild brmtL tion present in the training data  ; we only partially achieved this . B ~ yesian networks are usually confronted with a single present ~ L tion of evi-den  ( ' e . Theirexi ; ension to multil ) le evidence is not triviM . We believe the model can be extended in this direction  . Possibly the rem'esev-erM ways to do so ( muir ( hernial Saml ) ling , ded-i(:ated implementations , etc . ) . However , we believe that the most relevmit finding of this research mighl  ; t ) e( ; h~t ( ; % xpl nining it ww " is not only a 1) roper l ; y of Bayesian networks but of \]3 a yesim x inf e , rence in general and that it mighttmimt ) lemental ) leinother kinds of graphical models . We , observed that ( ; heprol ) erty seems to del ) end on the specification of the prior probabilities  . Wet'omld ( ; lm ( ;  ( ; he HMM model of ( Ab-heymid Ligh ( ;  , 1999) was ' unidentifiable ; that is , (; here are several so hli ; iol xst brthe\])~tra , xnel ; els of the lno(te , 1 , including the desired Olle . O Hr intuition is ( ; hat its houhtl ) e possible to imt ) lemelxt " exl ) laining a wlty " in a HMM with 1 ) tiers , so ( ; hit ( ; i l ; wouh\[1) rethr only ( ) lie or ~ tti ~ wsolu ( ; ions ( ) verl muiy . This model would have also the a ( t-wm t t t g c of t ) eing ( : Onll ) U ; ~d ; ionally silnt ) ler . 

S . Almeytold M . Light .  1999 . Hiding a serum > tichierarchy in ~ Mm'kov model . In P'm(:e , d-ing sq /' the Workshop o'nUns ' ~ q)ervi , scdLe , ,'r'n , -ing in Nat ' wra , 1 Lang , uagc Proeess in 9 , ACL . 
N . Chomsky . 1965. Aspcct . softh , e Theory of
Sy'nta , : r . MIT Press , Cambridge , MA.
P . N . Johnson-Laird .  1983 . Mental Models : 2b-'wards a Cognitive Sciev , ce of Lang'uage , P n : f i ' , r-c'nce , a ' n , dCon . sciousncss . ilm ' w ~ rdUniversity

J .   .  \]  . Kntz and . \] . A . Fodor .  1964 . The . structure of ; ~ semmiti (: theory . In , \] . , J . KaI ; zm MJ . A . l/od or , editors , The St ' ructure of Lan-g'uage . Prentice-Ilall . 
G . Miller . 299 ( I . Wordnet : An online lexical database , btternational Journal of Lexicography ,  3(4) . 
J . Pearl .  \] 988 . Probabilistic R , easo'n , ing in Intelligent , Systems : Net ' worl?s of Plausible l~l:fer-e'n , e e . Morgml Kimthmn . 
P . Resnik .  2997 . Sele(:tiona \] prefi:ren(:e and sens( ; disamt ) iguation . In Proceedings of th . e ANLP-97 Workshop : Taggin 9 Text with Lexical Semantics : Wll . y , What , and I to w ?
