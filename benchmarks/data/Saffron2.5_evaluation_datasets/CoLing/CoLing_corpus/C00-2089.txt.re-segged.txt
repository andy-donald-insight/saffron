Tagging and Chunking with Bigrams
Ferran Pla , Antonio Molina and Natividad Prieto
Universitat Politbcnica de Val 5ncia
Departament de Sistemes Inform\ticsi Computac  i6 
Camf de Veras/n
46(120 Val Sncia
fpla , amolina , nprieto@dsic.upv.es
Abstract
In this paper we present an integrated system for tagging and chunking texts from a certain language  . 
The approach is based on stochastic finite-state models that are learnt automatically  . This includes bigrmn models or tinite-state automata learnt using grammatical inference techniques  . As the models involved in our system are learnt automatically  , this is a very flexible and portable system . 
It lorder to show the viability of our approach we t  ) resent results for tagging mid chunking using bi -grain models on the Wall Street Journal corpus  . We have achieved an accuracy rate for tagging of 96  . 8%, and a precision ratet brNP chunks of 94 . 6% with a recall rate of 93 . 6% . 
1 Introduction
Part of Speech Tagging and Shallow Parsing are two wellknown problems in Natural Language Processing  . A Tagger can be considered as 2 translator that reads sentences from a certain language and outputs the corresponding sequences of part of speech  ( POS ) tags , taking into accounthe context in which each word of the sentence appears  . A Shallow Parser involves dividing sentences into nonoverlapping segments on the basis of very superticial analysis  . It ; includes discovering the main constituents of the sentences  ( NPs , VPs , PPs ,   . . . ) and their heads . 
Shallow Parsing usually identifies non-recnrsive constituents  , also called chunks ( Abney ,  1991 )   ( such as nonrecursive Noun Phrases or base NP , base VP , and so on ) . It can include deterlnining syntactical relationships such as subject verb  , verb-object , etc . , Shallow parsing wlfich always follows tlm tagging process  , is used as a fast and reliable pre-l ) rocessing phase for full or partial parsing . It can be used for hffbrmation Retrieval Systems , Information Extraction , Text Summarization and Bilingual Alignment . 
In addition , it is also used to solve coln putational linguistics tasks such as disambiguation t  ) roblems . 
1.1 POS Tagging Approaches
The different a I ) proaches for solving this problem can be classified into two main classes depending  oi1 tile tendencies followed for establishing tile Language Model  ( LM ) : tile linguistic apI ) roach , based oil handcoded linguistic rules and the learning ap-I  ) roach derived fi'om a corpora ( labelled or non-labelled )  . Other at ) proximations that use hybrid methods have also been proposed  ( Voutilaiuen and
Padr 6, 1997).
In timlinguistic apl ) roach , a nexI ) ertlinguist is needed to formalise the restrictions of the language  . 
This implies a very l figh cost and it is very dependent on each particular language  . We can lind an important contribution ( Voutilainen , : 1995) that uses Constraint Grammar tbrmalism . Supervised learning methods were proposed in ( Brill , 1995) to learn a set , of transforlnation rules that repair tim error committed by a probabilistic tagger  . The main a ( t-vantage of the linguistic approach is that the model is constructed from a linguistic I  ) oint of view and contains many and complex kinds of knowledge_  iI1 timlem'ning approach , tile most extended tbrmalism is based on n-grains or IIMM  . In tiffs case , the language i node l can be estimated from a labelled corpus  ( supervised methods )   ( Church , 1988) ( Weisehedeltal . , 1 . 993 ) or from a non-labelled corpus ( unsupervised methods )   ( Cutting et 21 . , 1992) . In the first ; case , the model is trained from the relative observed Dequencies  . In the second one , the model is learned using the Baunl-\?elch algorithm from an initial model which is estimated using labelled corpora  ( Merialdo ,  1994) . The advantages of the unsupervised approach are the facility to tmild language models  , the flexibility of choice of categories and the ease of apt  ) lication to other languages . 
We can find some other machine learning approaches that use more sophisticated LMs  , such as Decision Trees ( Mhrquez and Rodrfguez , 1998) ( Magerman ,  1996) , memory-based approaclms to learn special decision trees  ( Daelemans et al ,  1996) , maximmn entropy approaches that combine statistical information from different sources  ( Ratnaparkhi ,  1996) , finite state autonmt2 inferred using Grammatical Inference ( Pla and Prieto ,  1998) , etc . 
The comparison among differental ) t ) roaches is difficult due to then ml tiple factors that can bee on sid-the size of tilt vocabulary  , thK ambiguity , the diiti-culty of the tests ki , Kte . The best rKsults rel ) orted on the Wall Street , lore'hal(WSJ ) %' e(' . l ) ank ( \] ~'\ [ al ' CllSel al . , 1993) , using statistical language models , have a nae ( : uracy rack ) between 95% and 97% ( del ) Knding on the different factors mKntion o . dal ) ove ) . For the linguistical ) proachtim results arkl ) etter . For exmn-p\]e , in ( Voutilaineu , 1995) an accuracy of 99 . 7% is rel ) or ted , but cKrtain ambiguities illth Kou ; tnl ( ; remain unsolved . Some works have recently l ) eenpul ) -lished ( Brill and Wu , 1998) in which as el ; of taggers are combined in order to lint ) rove the . Jrl/erfornmn (: e . 
In some cases , these methods achieve an accuracy of 97 . 9% ( llalter Kn(31; al . , 1998) . 
1, 2 Shallow Parsing A1) t ) roaches
Since the early 90's~sK verall ; Kchni ( tues for carrying outs halh ) w parsing have been d ( 3velol ) ed . Tlms ( ~techniques can also bK classified into two main groups : basKd on hand-codKd linguistic rules and based on iKarning algorithms  . ThKsK approadmsll ~ wea conu nonchara ( : tcristi ( :: thKy take , l ; hese- ( lUKnCK of 1Kxi ( : altags 1 ) rot ) oscdt ) ya POS tagger as input , for both the h;arning and the (: bunking pro-

1 . 2 . 1 Techniques based on handcoded linguistiK rules These methods use a handwritten set of rules that ark defined llsing POS astKrn finals of timg I ' gtlll-mar  . Most of these works use t in it (! slate\]nel ; llo(lsfor(tel ; Kcl ; ing (: hunks or f()ra (: (: olni ) lishing el ; her linguisti(:l ; asks ( Ej Krhed ,  1988) , (:\ lm(~y ,  1996) , ( Ato Mokhtar and Chanod ,  :19!)7) . () ther works use ( tit'--ferell I ; ~ raltllllg d ; ical \] ' orlllalisll S ~ S/l ( ; has (: OllSl ; r ; /illl ; grmnmars ( Voutilainen ,  1993) , or (: oral ) in Kth (' . grammar rules with a set of heurist i ( : s ( Bourigault ,  :1992) . 
ThesK works usually use . a small test SKi that is lllall-ually evaluated , so the achieved results are not sig-ni\[icant . The regular KXln:cssions defined in ( Ejer-lied ,  1988 ) identified both nonrecursive clauses and nonrecursive NPs in English text  . The cxperim Kn-tation onl ; he Brown ( : or tmsachiKvK daprK ( : ision ratK of 87% ( for clauses ) and 97 . 8% ( for NPs ) . Ab-hey introduced the concept of chunk ( Almey ,  1991 ) m ) dl/resent Kd an increment all ) artial parser ( Abney ,  1996) . This pars Krid entities chunks l ) ase on the parts of Sl ) eKch , and it then chooses how to con > bine them tbr higher level analysis using lexical in -tbrmation  . ThK average 1 ) rK cision and recall rates for chunks were 87 . 9% and 87 . 1% , rest)ectivKly , on a tKst set of 1000s KntKneKS . An iimrenmntal archit Kcture of finite--state transducers for French is pres  ( mted in ( At-Mokhtar and Chanod ,  1 . 997) . Each transducer 1) ert ' oinguisti ( ; tasks u ( : hasid ( 3ntif~ying sKg-ments or syntactic strueturKs and dKtecting subjects and ol  ) jects . The system was ( 3wfluated on various corpora for subject and object detKction  . The precision rate varied between 9(,) . 2% and 92 . 6% . The recall rate varied between 97 . 8% and 82 . 6% . 
The NP2 bolllars Kr described in ( Voutilainen , 1993) identified nmximal-length noun phrases . 
NP tool gave a precision ral , e of 95-98% and a recall ratK of 98 . 5-100% . These results were criticised in ( Raulshaw and Marcus ,  1 . 995) due to some inconsistencies and a plmrenl ; mistakKs which appeared onthKsample given in ( Voutilainen ,  1993) . Bourigault dKvelopKd the LECTER parser fin " French using grmnmatical rules and soum hem ' istics  ( Bourigault ,  1992) . lit achieved a recall rate of 95% iden-tit ~ ying maxiln a length tern finological noun phrases  , but tie ( lid not givK aprK cision ratK , so it is difficult ; to Kvaluate the actual pK ribrmance of tile parsK r . 
1..2.2 LKarning Techniques
The seal ) lnoach csautoma . tica . lly (: onstruel ; a language model from a label lo . dalld brack K ted corpus . 
The lirst probabilistic approach was proposed in ( Church ,  1988) . This method learn ( ; a bigram model for detecting simph3 noun phrasKs on the Brown corpus . Civ ( ' na sequen ( 'e of parts of st ) (3eeh as inl ) ug , the Church program inserts the most prol ) able openings and Kndings of NPs , using a Viterbi qiko . dynamic programming algorithm . Church did not giVK precision and recall rates . He show Kd that 5 out of 24:3 NP were omitted , but in a very small test with a POS tagging a c ( : uraey of 99 . 5% . 
Transfornlation-based 1 Karning ( TBI , ) was USK din(\]~; unshawan(lMar(:us , 1995) to ( lc , t (' , (' t ba SK NP . 
In this work ( ' hunldng was consid Kre ( 1 as a tagging technique , so that each P ( )S could be tagged with I ( insidelms eNP )  , O(outside base Nl )) or B ( inside a base NP , but 1 ; 11 (3 pre (: eding word was illm lother bas KNP ) . This at ) preach rKsulted in a precision rate of 91 . 8% and arKcall rate of 92 . 3% . This iesult was automatically Kwlhlat ; edell , q . (; est set ; (200 , 000 words ) extracl ; Kd from the WS . \] Treebank . The main drawlmek to this approach are the high requiremKntst brtilne and space which ark needed to train ~ hesys-l  ; elll ; it needs to train 100 tKmplates of combinations of words . 
There are s ( ; v( ; ral works that use am (' mory-based h , arning algorithm . ThK seat ) proaeh Ks construct a classifier tbratask by storing as KI  ; of exmnples in inemory . Each ( ; xamI ) le is def in Kdl ) y a set of fhatures that havK to 1 ) c . learnt from a 1) racketed corpus . The MemoryBased Learning ( MBL ) algorithm ( l ) aele , -roans (3 tal . , 1999 ) takes into account lexical and POS information . It stores the following features : thK wordform mid POS tag of thK two words to the left  , the tbeus word and on K word to the right . This sys-tK machiKved a precision rate of 93 . 7'7 o and a recall rate of 94 . 0% on t\]l KWSJ Treebank . Howev Kr , when only POS information was used thel ) erformance decreased a . chiKving a precision rate of 90 . 3% mida Learning ( MBSL ) algorithm ( Argamon et al ,  1998 ) learns substrings or sequences of POS and brackets  . 
Precision and recall rates were 92 . 4% on the same data used in ( Ramshaw and Marcus ,  1995) . 
A simple approach is presented in ( Cardie and Pierce , 1998) called Treebank Apl ) roach ( TA) . This techt fique matches POS sequences from an initial noun phrase grammar which was extracted fl'om an annotated corpus  . The precision achieved for each rule is used to rank and prune the rules  , discarding those rules whose score is lower than a predefined threshold  . It uses a longest match heuristic to determine base NP  . Precision and recall on the WSJ Treebank was 89 . 4% and 90 . 0%, respectively . 
It is difficult to compare the differental ) proaches due fbr various reasons . Each one uses a different definition of base NP . Each one is evaluated on a different corpus or on different parts of the same cortms  . Some systems have even been evaluated by hand on a very small test set  . Table 1 summarize stile precision and recall rates for learning approaches that used at a extracted from the WSJ Treebank  . 
Method NP-Pl'ecision NP-Recall
TBL 91.8 92.3
MBSL 92.4 92.4
TA 89.4 90.9
MBL 93.7 94.0
MBL ( only POS ) 90.3 90.1
Tat ) le 1: Precision and recall rates tbr difl hrent NP parsers  . 
2 General Description of our
Integrated approach to Tagging and Chunking We propose an integrated system  ( Figure 1 ) that combines different knowledge sources ( lexical probabilities , LM for chunks and Contextual LMt br the sentences  ) in order to obtain the corresponding sequence of POS tags and the shallow parsing  ( \[ suWllC ~ W . ~/ c ~ su\]W . ~ lC ~ . . . \[ suW , lC , , su \]) from a certain input string (1'I:1 , I? . 2,  . . . , I/l:n ) . Our system is a transducer composed by two levels : the upper one represents the Contextual LM for tile sentences  , and the lower one modelize the chunks considered . The formalism that we have used in all levels are finite-state automata  . To be exact , we have used models of bigrmns which are smoothed using the backoff technique  ( Katz ,  1987 ) in order to achieve flfll coverage of the language  . The bigrams LMs ( bigram probabilities ) was obtained by means of the SLMTOOLKIT ( Clarks ond and Ronsenfeld , 
LEAIINING ~-\[-C,m , zxtual l . ~ I2"l "?' ~ . Chunks\]ll'e?ical Pmbabilities J
CIUNKIN(;~~
Figure 1: Overview of the System.
1997 ) from tile sequences of categories in the training set  . Then , they have been rei ) resented like finite-state automata . 
2.1 The learning phase.
The models have been estimated from labelled and bracketed corpora  . The training set is composed by sentences like : \[ suw  , /c , w . , / c . , su\]w ~/ c ~ . . . \[ su ~, ~:, ~/ c,~su\] . / . 
where W i are the words , Ci are part-of-speech tags and SU are tile chunks considered  . 
Tile models learnt are : ? Contextual LM : it is a smoothed bigram model learnt from tile sequences of part-of speech tags  ( C i ) and chunk descrit ) tors ( XU ) present in the training corpus ( see Figure 2a )  . 
? Models for the chunks : they are smoothed bigram models learnt fl ' om the sequences of part-of -speech tagse or rest  ) onding to each chunk of the training corpus ( see Figure 2b )  . 
? Lexical Probabilities : they are estilnated from the word frequencies  , tile tag frequencies and the word per tag frequencies  . A tag dictionary is used which is built from the full corpus which gives us the possible lexical categories  ( POS tags ) for each word ; this is equivalent o having an ideal morphological analyzer  . The probabilities for each possible tag are assigned from this information taking into account the obtained statistics  . Due to the fact that the word cannot have been seen at training  , or it has only been seen in some of the possible categories  , it is compulsory to apply a smoothing mechanism . In our case , if the word has not previously been seen ~ the same probability is assigned to all the categories given by the die-tionary  ; if it has been seen , but not in all the .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 
i,', zf+@,,--_...
Jiitii .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 
(c ) Integrated LMiI(<SU>\[((()x*
Figure 2: Integrated Language Model fin " Tagging and Chunking  . 
categories , the smoothing called " add one " is applied . Afterwards , are normalization process is carried out . 
Once the LMs have been learnt , a regular substitution of the lower model ( s ) into the upper one is made . In this way , we get a single Illtegrated LM which shows the possible concatenations of lexical tags and syntactical uuits  , with their own transition probabilities which also include the lexical probabilities ms well  ( see Figure 2c )  . Not (' , that the models in Figure 2 are not smoothed ) . 
2.2 The Decoding Process : Wagging and
Parsing
The tagging and shallow parsing process consists of finding out the sequence of states of maximum  1  ) rob-ability on the Integrated LM to ran input sentence  . 
Therefore , this sequence must be compatible with the contextual  , syntactical and lexical constraints . 
This process can be carried out by Dynamic Pro-gt ' ammiitg using the Viterbi algorithm  , which is conveniently modified to allow for ( ; ransitions between certain states of the autotnata without consmning any symbols  ( epsilonl ; ransitious ) . A portion of the Dynamic Progranmfing trellis for a generic sentence using the Integrated LM shown in Figure  2c can be seen in Figure 3  . The states of the automata that can be reached and that are compatible with the lexical constraints are marked with a black circle  ( i . e . , fl'om the state Ck it is possible to reach the state Ci if the transition is in the automat and the lexical probability P  ( Wi\[Ci ) is not null )  . Also , the transitions to initial and final states of the models for chunks  ( i . e . , fl'om Ci to < SU >) are allowed ; these states are marked in Figure 3 with a white circle and in this case no symbol is consumed  . Ill all these cases , the transitions to initial and final produce transitions to their successors  ( the dotted lines in Figure 3 ) where now symbols must be consumed . 
Once the Dynamic Programing trellis is built , we can obtain the maximum probability path for the input sentence  , and thus the best sequence of lexical tags and the best segmentation i chunks  . 

Cicj < Is > .   .   .   .   .   .   . \]\~"~\]',` % linalx\" . ' State < ~ u > .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . it >, .   .   .   .  ~>; ,  .   .   .   .   . 
Ci /""/"',': ~ tIL(:k ', s', . Jill , / t ? // c,, . . . . . . . . . . . .  . . . . . . . . 7~-o . .  .   . . . . . . .  //~  .   .   .   .   . 
</SU > "% 3~----t~--hlput : .   .   . Wll-2 Wll-IWn </ S > Output: .   .   .   Wn~2/Ci ISU Wnq/Cn SUI Wn/Ck </ s > Figure 3: Partial % ' ellis for Programming Decoding based oil tile Integrated LM  . 
3 Experimental Work
In this section we will describe a set of experiments that we carried out in order to demonstrate the capabilities of the proposed approach for tagging and shallow parsing  . The experiments were carried out in ( Marcus et lal .   ,  1993) , considering only the NP chunts ( lefine ~ l by ( Church ,  1988 ) and using tile models that we have presented above  . Nevertheless , the use of this apt ) roach on other corpora ( changing the reference language )  , other lexical tagsets or other kinds of chunks can be done in a direct way  . 
3.1 Corpus Description.
We used at ) ortion of the WSJ corpus (900 , 000 words ) , which was tagged according to the Penn Treebank tagset and bracketed with NP markers  , to train and test the system . 
The tagset contained 45 different tags . About 36 . 5% of the words in the cortms were mn biguous , with an ambiguity ratio of 2 . 44 tag/word over the ambiguous words , 1 . 52 overall . 
3.2 Experimental Results.
In order to train the models and to test the system  , we randomly divided the corpora into two parts : approximately  800  , 000 words for training aud 100 , 000 words t br testing . 
Both the bigram models for representing contextual information mid syntactic description of the NP chunk and the lexical probabilities were estimated from training sets of different sizes  . Due to the fact that we did not use a morphological nalyser for English  , we constructed a tag dictionary with the lexicon of the training set and the test set used  . This dictionary gave us tile possible lexical tags for each word fl ' om the corpus  . In no case , was the test used to estimate the lexical probabilities  . 


BIG-BIG\[\[100 200\[i(~\]1
LIiiii 300   400   500   60O  #Words x 1000 Figure 4: Accuracy Rate of Tagging on WSJ for increment M training sets  . 
In Figure 4 , we show the results of tagging on the test set in terms of the training set size using three at  ) proaches : the simplest ( LEX ) is a tagging process which does not take contextual information into account  , so the lexical tag associated to a word will
Recall ~ ,  + < , , +  , i , __i i 100 200 300 4o 0 500 6007 ( 3 0   800  #Words x 1000 Figure 5: NP-chunldng results on WSJ for incremental training sets  . 



BIG-BIG 96.8
Lex 94.3
BIG 96.9
IDEAL 100 ( assumed )

Precision IRecall 94 . 6 193 . 6 90 . 8 91 . 3 94 . 9 94 . 1 95 . 5 94 . 7 Table 2: Tagging and NP-Chunking results t'or dif-ferents taggers  ( training set of 800 , 000 words ) . 
be that which has a I ) peared more often in the training set . Tile second method corresponds to a tagger based on a bigram model  ( BIG )  . The third one uses the Integrated LM described in this pai  ) er ( BIG-BIG )  . The tagging accuracy for BIG and BIG-BIG was close  ,  96 . 9% and 96 . 8% respectively , whereas without the use of the language model ( LEX )  , tile tagging accuracy was 2 . 5 points lower . The trend in all the cases was that an increment in the size of the training set resulted in an increase in the tagging accuracy  . After 300 , 000 training words , the result be camestabilized . 
In Figure 5 , we show the precision (  #correct proposed NP/#proposed NP ) and recall (  #correct proposed NP/#NP in the reference ) rates for NP chunking . The results obtained using the Integrated LM were very satisfactory achieving a precision rate of  94  . 6% and a recall rate of 93 . 6% . The performance of the NP chunker improves as the training set size increases  . This is obviously due to the fact that tile model is better learnt when the size of the training set increases  , and the tagging error decreases as we have seen above  . 
The usual sequential 1 ) rocess for chunking a sentence can also be used . That is , first we tag the sentence and then we use the Integrated LM to carry out the chunking  . In this case , only tim contextual t ) robabilities are taken into account in the decoding suits that we obtained for tagging and tbr NP chunking  . The first row shows the result when the tagging and the chunking are done in a integrated way  . The following rows show the performmme of the sequential process using differentaggers : ? LEX : it takes into account only lexical proba-t  ) ilities . In this case , the tagging accuracy was 94 . 3% . 
? BIG : it is based on a bigram model that achieved an accuracy of  96  . 9% . 
? IDEAL : it siinulates a tagger with an accuracy rate of  100%  . To do this , we used the tagged sentences of the WSJ corlms directly  . 
These results confirm that precision and recall rates increase when the accuracy of the tagger is be N  ; er . The pert ' or mmme of 1; he , s e ( tuential process ( u : dng the BIG tagger ) is slightly 1letter than the pet ' formance of the integrated process ( BIG-BIG )  . 
We think that this is 1) robably b ( ; cause of the way we combined the I ) robabilities of t ; hedit thrent models . 
4 Conclusions and Future Work
In this 1) aper , we have t ) rcs cnt cda system to t " Tagging and Chunldng based on an Integrated Language Model that uses a homogeneous tbrmalism  ( finite-state machine ) to combine different knowledge sources : lexical , syntacti (: al and contextual i node ls . It is feasible l ) oth in terms of 1) er fl ) rmanc ( ; and also in terms of computational (: tliciency . 
All the models involv ( : d are learnt automatically fi'om data , so the system is very tlexibte and 1 ) or table and changes in the reference language . , lexical tags or other kinds of chunks can be made in a direct way  . 
The tagging accuracy (96 . 9% using BIG and 96 . 8% using BIG-BIG ) is higher tlm nother similar al Il ) roaches . This is because we have used the tag di ( ' tionary ( including the test set in it ) to restrict the possible tags for unknown words , this assmnp-lion obviously in ( : rease the rates of tagging ( we have not done a quantitative study of this factor  )  . 
As we have mentioned above , the comparison with other approaches iditficult duemnong other reasons to tim following ones : the definitions of base NP are not always the stone  , the sizes of the train and the test sets are difl ' erent and the knowledge sources used in the learning process are also different  . The precision for NP-chunking is similm ' to other statistical at  ) preachest ) resented in section 1 , tbr1) oth the integrated process (94 . 6%) and l ; tm sequential process using a tagger based on 1 ) igrams ( 94 . 9%) . The recall rate is slightly lower than for some apl  ) roaches using the integrated system ( 93 . 6%) and is similar for the sequential process (94 . 1%) . When we used the sequential system taking an error ti ' ee input  ( IDEAL )  , the performance of the system obviously increased  ( 95 . 5% precision and 94 . 7% recall ) . These results show the influence of tagging errors on the process  . 
Nevertheless , we are studying why the results lie-tween the integrated process and the sequential process are difl brent  . We are testing how the introduction of so In e adjustnmnt factors among the models tk  ) r w e , ighting the difl'erent1 ) robability distribution can lint ) rove the results . 
The models that we have used in this work , are ill-grams , but trigrams or any stochastic regular model can be used  . In this respect , we have worked on a more coml ) lexLMs , formalized as a . finite-state automata which is learnt using Grammatical Inference tectufiques  . Also , our ai ) l ) roach would benefit fl ' om the inclusion of lexical -contextual in % rmation into the LM  . 
5 Acknowledgments
This work has been partially supl ) or ted 1 ) y the Stmnish Iesem'ch Projct : tCICYT ( TIC97-0671-C02-


S . Abney .  1991 . Parsing by Chunks . R . Berwick , S . 
Almey and C . Tcnny(eds . ) Principle-based Parsing . Kluwer Acadenfic Publishers , Dordrecht . 
S . Almey .  1996 . Partial Parsing via Finit (' . -Sta~e Cascades . In Proceedings of the ES , S'LLI'96 Robust Parsinfl Workshop , l ? rague , Czech lelmblie . 
S . Argamon , I . Dagan , and Y . Krymolowski .  1 . 998 . 
A Memory based Approach to Learning Shallow Natural Language  , Patterns . Inl ~ roceedi'ng soft , h , ejoint 17th , International Conference on Computational Linguistics and  36th Annual Meeting of the Association for Computational Linguistics  , COLINGACL , pages 6773 , Montrdal , Canada . 
S . At-Mokhtar and , l . P . Chanod .  1997 . Incremental Finite State Parsing . In Proceedings of the 5th , Conference on Applied Natural Language Processing  , \ Vashington D . C . , USA . 
D . Bourigault .  1992 . Surface Grmnmatical Anal- , ) , sisfortim Extraction of ~ l ~ . ~ rminological Noml Phrases . In Proceedings of the 15th International Conference on Computational Linguistics  , pages 977-981 . 
Eric Brill and Jun Wu .  1998 . Classifier Combination for hnproved Lexical Disambiguation  . In Procccdings of the joint 17th , International Con-fcrcnccon Computational Linguistics and  36th Annual Meeting of thc Association for Computational Linguistics  , COLINGACL , pages 191-195 , 
Montrdal , Canada.
E . Brill .  1995 . Transibn nation-based Error-driven Learning and Natural Language Processing : Atational Linguistics  ,  21 (4) :543-565 . 
C . Car ( lie and D . Pierce .  1998 . Error-Driven Prun-ning of Treebank Grammars for Base Noun Phrase Identification  . In Proceedings of the joint 17th International Conference on Computational Linguistics and  36th Annual Meeting of the Association for Computational Linguistics  , COLINGACL , pages 218224 , Montrdal , Canada , August . 
K . W . Church .  1988 . A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text  . 
In Proceedings of the 1st Conference on Applied Natural Language Processing  , ANLP , pages 136143 . ACL . 
P . Clarksond and R . Ronsenfeld .  1997 . Statistical Language Modeling using the CMU -Cambridge Toolkit  . In Procccdinfls of Eurospccch , Rhodes,

D . Cutting , J . Kut)iec , J . Pederson , and P . Nil ) un . 
1992 . A Practical Part-of-speech Tagger . In Pfv-cccdings of the 3rd Confcrcnce of t Applied Natural Language Processing  , ANLP , pages 133140 . 

W . Daelelnans , J . Zavrel , P . Berck , and S . Gillis . 
1996 . MBT : A MeInory-Based Part of speech Tagger Generator  . In Proceedings of the / tth Workshop on Very Large Cmpora  , pages 14-27 , 
Copenhagen , Denmark.
W . Daelemans , S . Buchholz , and J . Veenstra .  1999 . 
MemoryBased Shallow Parsing . In Proceedings of EMNLP/VLC-99 , pages 239246 , University of
Maryla .nd , USA , June.
E . Ejerhed .  1988 . Finding Clauses in Unrestricted Text by Finitary and Stochastic Methods  . In Pro-cccdings of Second Confcrcnccon Applied Natural Language Processing  , pages 219-227 . ACL . 
H . van Halteren , J . Zavrel , and W . Daelemans .  1998 . 
Improving Data Driven Word class Tagging by System Combination  . In Proceedings of the joint 17th International Confcr'cnccoft Computational Linguistics and  36th Annual Mccting of the Association for Computational Linguistics  , COLINGACL , pages 491-497 , Montrdal , Canada , August . 
S . M . Katz .  1987 . Estimation of Probabilities from Sparse Data for tile Language Model Component of a Speech Recognizer  . IEEET ~ nnsactions on Acoustics , Speech and Signal Processing ,  35 . 
D . M . Magerman .  1996 . Learning Grammatical Structure Using Statistical Decision-Trees  . In Proceedings of the 3rd International Colloquium on GT nmmatical Inference  , ICGI , pages 121 . 
Springer-Verlag Lecture Notes Series in Artificial
Intelligence 1147.
M . P . Marcus , M . A . Marcinkiewicz , and B . Santorini .  1993 . Building a Large Annotated Cortms of English : Tile Penn Treebank  . Computational
Linguistics , 19(2).
Llu/s Mhrquez and Horacio Rod Hguez .  1998 . Part-of Speech T~gging Using Decision Trees . In C . 
Nddellee and C . Rouveirol , editor , LNAI 1398: Proceedings of th clOth European Conference on Machine Learning  , ECML'98 , pages 25-36 , 
Chemnitz , GermNly . Springer.
B . Merialdo .  1994 . Tagging English Text with a Probabilistic Model . Computational Linguistics , 20(2):155-171 . 
F . Pla and N . Prieto .  1998 . Using Grammatical Inference Methods tbrAutomatic Part of speech Tagging  . In Proceedings of 1st International Conference on Language Resources and Evaluation  , 
LREC , Granada , Spain.
L . Ramshaw and M . Marcus .  1995 . Text Chunking Using ~ lYansfbrmation-Based Learning  . In Pro-cccdings of third Workshop on Very Large Colpora  , pages 8294 , June . 
A . Ratnapm'khi .  1996 . A Maximum Entrol ) y Part of speech Tagger . In Proceedings of the 1st Con-fcrcnccon Empirical Methods in Natural Lan -guagc Processing  , EMNLP . 
Atro Voutilainen and Llufs Padrd .  1997 . Develol ) - in na Hybrid NP Parser . In Proceedings of the 5th Conference on Applied Natural Language Prvecss - ing  , ANLP , pages 8087 , Washington DC . ACL . 
Atro Voutilainen .  1993 . NP Tool , a Detector of English Noun Phrases . In Proceedings of the Workshop on Very Lafflc Corpora  . ACL , June . 
Atro Voutilainen .  1995 . A Syntax Based Part of speech Analyzer . In Prvcccdings of the 7th Conference of the European Ch , apt cr of the Association for Computational Linguistics  , EACL , Dut ) lin , h ' el and . 
R . Weischedel , R . Schwartz , J . Pahnueci , M . Meteer , and L . Ramshaw .  1993 . Coping with Ambiguity and Unknown\~or ( ls through Probabilistic Models . Computational Linguistics , 19(2):260-269 . 

