Proceedings of the 23rd International Conference on Computational Linguistics ( Coling 2010), pages 340?348,
Beijing , August 2010
Opinosis : A GraphBased Approach to Abstractive Summarization of
Highly Redundant Opinions
Kavita Ganesan and ChengXiang Zhai and Jiawei Han
Department of Computer Science
University of Illinois at Urbana-Champaign { kganes2,czhai,hanj}@cs.uiuc.edu
Abstract
We present a novel graphbased summarization framework ( Opinosis ) that generates concise abstractive summaries of highly redundant opinions . Evaluation results on summarizing user reviews show that Opinosis summaries have better agreement with human summaries compared to the baseline extractive method . The summaries are readable , reasonably wellformed and are informative enough to convey the major opinions.
1 Introduction
Summarization is critically needed to help users better digest the large amounts of opinions expressed on the web . Most existing work in Opinion Summarization focus on predicting sentiment orientation on an entity ( Pang et al , 2002) ( Pang and Lee , 2004) or attempt to generate aspect-based ratings for that entity ( Snyder and Barzilay , 2007) ( Lu et al , 2009)(Lerman et al , 2009)(Titov and Mcdonald , 2008). Such summaries are very informative , but it is still hard for a user to understand why an aspect received a particular rating , forcing a user to read many , often highly redundant sentences about each aspect . To help users further digest the opinions in each aspect , it is thus desirable to generate a concise textual summary of such redundant opinions.
Indeed , in many scenarios , we will face the problem of summarizing a large number of highly redundant opinions ; other examples include summarizing the ? tweets ? on Twitter or comments made about a blog or news article . Due to the subtle variations of redundant opinions , typical extractive methods are often inadequate for summarizing such opinions . Consider the following sentences : 1. The iPhone?s battery lasts long , only had to charge it once every few days.
2. iPhone?s battery is bulky but it is cheap..
3. iPhone?s battery is bulky but it lasts long ! With extractive summarization , no matter which single sentence of the three is chosen as a summary , the generated summary would be biased.
In such a case , an abstractive summary such as ? iPhone?s battery is cheap , lasts long but is bulky ? is a more complete summary , conveying all the necessary information . Extractive methods also tend to be verbose and this is especially problematic when the summaries need to be viewed on smaller screens like on a PDA . Thus , an informative and concise abstractive summary would be a better solution.
Unfortunately , abstractive summarization is known to be difficult . Existing work in abstractive summarization has been quite limited and can be categorized into two categories : (1) approaches using prior knowledge ( Radev and McKeown , 1998) ( Finley and Harabagiu , 2002) ( DeJong , 1982) and (2) approaches using Natural Language Generation ( NLG ) systems ( Saggion and Lapalme , 2002) ( Jing and McKeown , 2000). The first line of work requires considerable amount of manual effort to define schemas such as frames and templates that can be filled with the use of information extraction techniques . These systems were mainly used to summarize news articles . The second category of work uses deeper NLP analysis with special techniques for text regeneration . Both approaches either heavily rely on manual effort or are domain dependent.
In this paper , we propose a novel flexible summarization framework , Opinosis , that uses graphs to produce abstractive summaries of highly redundant opinions . In contrast with the previous work , Opinosis assumes no domain knowledge and uses shallow NLP , leveraging mostly the word order in the existing text and its inherent redundancies to generate informative abstractive summaries . The key idea of Opinosis is to first construct a textual graph that represents the text to be summarized . Then , three unique properties of this graph are used to explore and score various subpaths that help in generating candidate abstractive summaries.
Evaluation results on a set of user reviews show that Opinosis summaries have reasonable agreement with human summaries . Also , the gener-well-formed . Since Opinosis assumes no domain knowledge and is highly flexible , it can be potentially used to summarize any highly redundant content and could even be ported to other languages . ( All materials related to this work including the dataset and demo software can be found at http://timan.cs.uiuc.edu / downloads.html .) 2 Opinosis-Graph Our key idea is to use a graph data structure ( called Opinosis-Graph ) to represent natural language text and cast this abstractive summarization problem as one of finding appropriate paths in the graph.
Graphs have been commonly used for extractive summarization ( e.g ., LexRank ( Erkan and Radev , 2004) and TextRank ( Mihalcea and Tarau , 2004)), but in these works the graph is often undirected with sentences as nodes and similarity as edges.
Our graph data structure is different in that each node represents a word unit with directed edges representing the structure of sentences . Moreover , we also attach positional information to nodes as will be discussed later.
Algorithm 1 ( A1): OpinosisGraph(Z ) 1: Input : Topic related sentences to be summarized : Z = { zi}ni=12: Output : G = ( V,E ) 3: for i = 1 to n do4: w ? Tokenize(zi)5: sent size ? SizeOf(w ) 6: for j = 1 to sent size do 7: LABEL ? wj8: PID ? j 9: SID ? i10: ifExistsNode(G,LABEL ) then 11: vj ? GetExistingNode(G,LABEL)12: PRIvj ? PRIvj ? ( SID , PID)13: else14: vj ? CreateNewNode(G,LABEL)15: PRIvj ? ( SID , PID)16: end if 17: if notExistsEdge(vj?1 ? vj , G ) then18: AddEdge(vj?1 ? vj , G)19: end if20: end for21: end for Our graph representation is closer to that used by Barzilay and Lee ( Barzilay and Lee , 2003) for the task of paraphrasing , wherein each node in the graph represents a unique word . However , in their work , such a graph is used to identify regions of commonality and variability amongst similar sentences . Thus , the positional information is not required nor is it maintained . In contrast , we maintain positional information at each node as this is critical for the selection of candidate paths.
Algorithm A1 outlines the steps involved in building an Opinosis-Graph . We start with a set of sentences relevant to a specific topic , which can be obtained in different ways depending on the application . For example , they may be all sentences related to the battery life of the iPod Nano . We denote these sentences as Z = { zi}ni=1 where each ziis a sentence containing part-of-speech ( POS ) annotations . ( A1:4) Each zi ? Z is split into a set of word units , where each unit , wj consists of a word and its corresponding POS annotation ( e.g.
?service:nn ?, ? good:adj ?). ( A1:7-9) Each unique wj will form a node , vj , in the Opinosis-Graph , with wj being the label . Also , since we only have one node per unique word unit , each node keeps track of all sentences that it is a part of using a sentence identifier ( SID ) along with its position of occurrence in that sentence ( PID ). ( A1:10-16) Each node will thus carry a Positional Reference Information ( PRI ) which is a list of { SID:PID } pairs representing the node?s membership in a sentence.
(A1:17-19) The original structure of a sentence is recorded with the use of directed edges . Figure 1 shows a resulting Opinosis-Graph based on four sentences.
The Opinosis-Graph has some unique properties that are crucial in generating abstractive summaries . We highlight some of the core properties by drawing examples from Figure 1: Property 1. ( Redundancy Capture ). Highly redundant discussions are naturally captured by subgraphs.
Figure 1 shows that although the phrase ? great device ? was mentioned in different parts of sentences (1) and (3), this phrase forms a relatively heavy subpath in the resulting graph . This is a good indication of salience.
Property 2. ( Gapped Subsequence Capture ). Existing sentence structures introduce lexical links that facilitate the discovery of new sentences or reinforce existing ones.
The main point conveyed by sentences (2) and (3) in Figure 1 is that calls drop frequently . However , this is expressed in slightly different ways and is reflected in the resulting subgraph . Since sentence (2) introduces a lexical link between ? drop ? and ? frequently ?, the word ? too ? can be ignored for sentence (3) as the same amount of information is retained . This is analogous to capturing a repetitive gapped subsequence where similar sequences with minor variations are captured . With this , the subgraph calls drop frequently can be considered redundant.
Property 3. ( Collapsible Structures ). Nodes that resemble hubs are possibly collapsible.
In Figure 1 we see that the subgraph ? the iPhone is ?, is fairly heavy and the ? is ? node acts like a pho ne call s freq uen tly too {3:8} with dro p ipho ne is a my {2:1} pho ne {2:2 } call s {2:3 , 3: 6} freq uen tly {2:5 , 3: 9} with {2:6 } the dro p {2:4 , 3: 7} gre at {1:5 , 3: 1} {1:2 , 2: 8, 4 :2} {1:3 ,4:3 }{1 :4} .
{1:7 ,2: 9,3 :10 } {1:1 , 2: 7, 3 :5, 4:1 ,4:5 } wor th pric e {4:6 } { , } , {3:3} but {3:4}{1:7 ,2: 9,3 :10 } wor th {4:4 } no de labe l
SID : PID pa irs dev ice {1:6 , 3: 2}
Inp ut:
SID :1.
The iPh one is a gre at d evi ce.

SID :2.
My ph one ca lls dro p fr equ ent ly w ith the iPh one .
SID :3.
Gr eat de vice , bu t th e c alls dro p to o fr equ ent ly.
p , p q y
SID :4.
Th e iP hon eis wo rth the pri ce.
Figure 1: Sample Opinosis-Graph . Thick edges indicate salient paths.
?hub ? where it connects to various other nodes.
Such a structure is naturally captured by the Opinosis-Graph and is a good candidate for compression to generate a summary such as ? The iPhone is a great device and is worth the price?.
Also , certain word POS ( e.g . linking verbs like ? is ? and ? are ?) often carry hub-like properties that can be used in place of the outlink information.
3 Opinosis Summarization Framework
In this section , we describe a general framework for generating abstractive summaries using the Opinosis-Graph . We also describe our implementation of the components in this framework.
At a high level , we generate an abstractive summary by repeatedly searching the Opinosis graph for appropriate subgraphs that both encode a valid sentence ( thus meaningful sentences ) and have high redundancy scores ( thus representative of the major opinions ). The sentences encoded by these subgraphs would then form an abstractive summary.
Going strictly by the definition of true abstraction ( Radev et al , 2002), our problem formulation is still more extractive than abstractive because the generated summary can only contain words that occur in the text to be summarized ; our problem definition may be regarded as a word-level ( finer granularity ) extractive summarization.
However , compared to the conventional sentence-level extractive summarization , our formulation has flavors of abstractive summarization wherein we have elements of fusion ( combining extracted portions ) and compression ( squeezing out unimportant material from a sentence ). Hence , the sentences in the generated summary are generally not the same as any original sentence . Such a ? shallow ? abstractive summarization problem is more tractable , enabling us to develop a general solution to the problem . We now describe each component in such a summarization framework.
3.1 Valid Path
A valid path intuitively refers to a path that corresponds to a meaningful sentence.
Definition 1. ( Valid Start Node - VSN ). A node vq is a valid start node if it is a natural starting point of a sentence.
We use the positional information of a node to determine if it is a VSN . Specifically , we check if Average(PIDvq ) ? ? vsn , where ? vsn is a parameter to be empirically set . With this , we only qualify nodes that tend to occur early on in a sentence.
Definition 2. ( Valid End Node - VEN ). A node vs is a valid end point if it completes a sentence.
We use the natural ending points in the text to be summarized as hints to which node may be a valid end point of a path ( i.e ., a sentence ). Specifically , a node is a valid end node if (1) the node is a punctuation such as period and comma or (2) the node is any coordinating conjunction ( e.g ., ? but ? and ? yet?).
Definition 3. ( Valid Path ). A path W = { vq...vs } is valid if it is connected by a set of directed edges such that (1) vq is a VSN , (2) vs is a VEN , and (3) W satisfies a set of wellformedness POS constraints.
Since not every path starting with a VSN and ending at a VEN encodes a meaningful sentence , we further require a valid path to satisfy the following POS constraints ( expressed in regular-expression ) to ensure that a valid path encodes a wellformed sentence : 1. . ? (/ nn ) + . ? (/ vb ) + . ? (/ jj ) + .? 2. . ? (/ jj ) + . ? (/ to ) + . ? (/ vb ).? 3. . ? (/ rb ) ? . ? (/ jj ) + . ? (/ nn ) + .? 4. . ? (/ rb ) + . ? (/ in ) + . ? (/ nn ) + .? This also provides a way ( if needed ) for the application to generate only specific type of sentences like comparative sentences or strictly opinionated sentences . These rules are thus application specific.
3.2 Path Scoring
Intuitively , to generate an abstractive summary , we should select a valid path that can represent most of the redundant opinions well . We would thus favor a valid path with a high redundancy score.
Definition 4. ( Path Redundancy ). Let W = { vq...vs } be a path from an Opinosis-Graph . The path redundancy of W , r(q , s ), is the number of overlapping sentences covered by this path , i.e ., where ni = PRIvi and ?? is the intersection between two sets of SIDs such that the difference between the corresponding PIDs is no greater than ? gap , and ? gap > 0 is a parameter.
Path redundancies provide good indication of how many sentences discuss something similar at each point in the path . The ? gap parameter controls the maximum allowed gaps in discovering these redundancies . Thus , a common sentence X between nodes vq and vr , will be considered a valid intersect if ( PIDvrx ? PIDvqx ) ? ? gap.Based on path redundancy , we propose several ways to score a path for the purpose of selecting a good path to include in the summary : 1. Sbasic(W ) = 1|W | ? s k=i+1,i r(i , k ) 2. Swt len(W ) = 1|W | ? s k=i+1,i | vi , vk | ? r(i , k ) 3. Swt loglen(W ) = 1|W |( r(i , i + 1) +? s k=i+2,i+1 log2|vi , vk | ? r(i , k )) vi is the first node in the path being scored and vs is the last node . | vi , vk | is the length from node vi to vk . | W | is the length of the entire path being scored . The Sbasic scoring function scores a path purely based on the level of redundancy . One could also argue that high redundancy on a longer path is intuitively more valuable than high redundancy on a shorter path as the former would provide better coverage than the latter . This intuition is factored in by the Swt len and Swt loglen scoring functions where the level of redundancy is weighted by the path length . Swt loglen is similar to Swt len only that it scales down the path length so that it does not entirely dominate.
3.3 Collapsed paths
In some cases , paths in the Opinosis-Graph may be collapsible ( as explained in Section 2). In such a case , the collapse operation is performed and then the path scores are computed . We will now explain a few concepts related to collapsible structures . Let W ? = { vi...vk } be a path from the
Opinosis-Graph.
Definition 5. ( Collapsible Node ). Node vk is a candidate for collapse if its POS is a verb.
We only attempt to collapse nodes that are verbs due to the heavy usage of verbs in opinion text and the ease with which the structures can be combined to form a new sentence . However , as mentioned earlier other properties like the outlink information can be used to determine if a node is collapsible.
Definition 6. ( Collapsed Candidates , Anchor).
Let vk be a collapsible node . The collapsed candidates of vk ( denoted by CC = { cci}mi=1) are the Canchor CC Connectora . the sound quality is cc1 : really good and cc2 : clearb . the iphone is cc1 : great but cc2 : expensive Table 1: Example of anchors , collapsed candidates and suitable connectors remaining paths after vk in all the valid paths going through vi...vk . The prefix vi...vk is called the anchor , denoted as Canchor = { vi...vk }. Each path { vi...vn }, where vn is the last node in each cci ? CC , is an individually valid path.
Table 1 shows a simplistic example of anchors and corresponding collapsed candidates . Once the anchor and collapsed candidates have been identified , the task is then to combine all of these to form a new sentence.
Definition 7. ( Stitched Sentence ) A stitched sentence is one that combines Canchor and CC to form a combined , logical sentence.
We will now describe the stitching procedure that we use , by drawing examples from Table 1. Since we are dealing with verbs , Canchor can be combined with the corresponding CC with commas to separate each cci ? CC with one exception - the correct sentence connector has to be used for the last cci . For Canchora , the phrases really goodand clear can be connected by ? and ? due to the same sentiment orientation . For Canchorb , the collapsed candidate phrases are well connected by the word ? but ?. We use the existing Opinosis-Graph to determine the most appropriate connector . We do this by looking at all coordinating conjunction ( e.g . ? but ?, ? yet ?) nodes ( vcconj ) that are connected to the first node of the last collapsed candidate , ccm . This would be the node labeled ? clear ? for Canchora and ? expensive ? for Canchorb . We denotethese nodes as v0,ccm . The vcconj , with the highest path redundancy with v0,ccm , will be selectedas the connector.
Definition 8. ( Collapsed Path Score ) The final path score after the entire collapse operation is the average across path scores computed from vi to the last node in each cci ? CC.
The collapsed path score essentially involves computing the path scores of the individual sentences assuming that they are not collapsed and then averaging them.
3.4 Generation of summary
Once we can score all the valid paths as well as all the collapsed paths , the generation of an abstractive summary can be done in two steps : First , we rank all the paths ( including the collapsed paths ) in descending order of their scores . Second , we by using a similarity measure ( in our experiments , we used Jaccard ). We then take the top few remaining paths as the generated summary , with the number of paths to be chosen controlled by a parameter ? ss , which represents summary size.
Although conceptually we enumerate all the valid paths , in reality we can use a redundancy score threshold , ? r to prune many non-promising paths . This is reasonable because we are only interested in paths with high redundancy scores.
4 Summarization Algorithm
Algorithms A2 and A3 describe the steps involved in Opinosis Summarization . A2 is the starting point of the Opinosis Summarization and A3 is a subroutine where path finding takes place , invoked from within A2.
Algorithm 2 ( A2): OpinosisSummarization(Z ) 1: Input : Topic related sentences to be summarized : Z = { zi}ni=12: Output : O ={ Opinosis Summaries } 3: g ? OpinosisGraph(Z ) 4: node size ? SizeOf(g ) 5: for j = 1 to node size do 6: if V SN(vj ) then7: pathLen ? 1 8: score ? 09: cList ? CreateNewList () 10: Traverse(cList , vj , score , PRIvj , labelvj , pathLen)11: candidates ? { candidates ? cList } 12: end if13: end for14: C ? EliminateDuplicates(candidates ) 15: C ? SortByPathScore(C ) 16: for i = 1 to ? ss do17: O = { O ? PickNextBestCandidate(C )} 18: end for ( A2:3) Opinosis Summarization starts with the construction of the Opinosis-Graph , described in detail in Section 2. This is followed by the depth first traversal of this graph to locate valid paths that become candidate summaries . ( A2:6-12) To achieve this , each node vj in the Opinosis-Graph is examined to determine if it is a VSN and , if it is , path finding will start from this node by invoking subroutine A3. A3 takes the following as input : list - a list to hold candidate summaries ; vi - the node to continue traversal from ; score - the accumulated path score ; PRIoverlap - the intersect between PRIs of all nodes visited so far ( see Definition 4); sentence - the summary sentence formed so far ; len - the current path length . ( A2:7-10) Before invoking A3 from A2, the path length is set to ?1?, path score is set to ?0? and a new list is created to store candidate summaries generated from node vj . ( A2:11) All candidate summaries generated from vj will be stored in a common pool of candidate summaries.
Algorithm 3 ( A3): Traverse (...) 1: Input : list , vk ? V , score , PRIoverlap , sentence , len2: Output : A set of candidate summaries 3: redundancy ? SizeOf(PRIoverlap)4: if redundancy ? ? r then5: if V EN(vk ) then6: if V alidSentence(sentence ) then 7: finalScore ? scorelen8: AddCandidate(list , sentence , finalScore ) 9: end if10: end if 11: for vn ? Neighborsvk do12: PRInew ? PRIoverlap ?? PRIvn13: redundancy ? SizeOf(PRInew)14: newSent ? Concat(sentence , labelvn )15: L ? len + 116: newScore ? score + PathScore(redundancy , L ) 17: if Collapsible(vn ) then18: Canchor ? newSent19: tmp ? CreateNewList () 20: for vx ? Neighborsvn do21: Traverse(tmp , vx , 0, PRInew , labelvx , L)22: CC ? EliminateDuplicates(tmp ) 23: CCPathScore ? AveragePathScore(CC ) 24: finalScore ? newScore + CCPathScore 25: stitchedSent ? Stitch(Canchor , CC)26: AddCandidate(list , stitchedSent , finalScore ) 27: end for28: else29: Traverse(list , vn , newScore , PRInew , newSent , L)30: end if31: end for32: end if ( A3:3-4) Algorithm A3 starts with a check to ensure that the minimum path redundancy requirement is satisfied ( see definition 4). For the very first node sent from A2, the path redundancy is the size of the raw PRI . ( A3:5-10) If the redundancy requirement is satisfied , a few checks are done to determine if a valid path has been found . If it has , then the resulting sentence and its final score are added to the list of candidate summaries.
(A3:11-31) Traversal proceeds recursively through the exploration of all neighboring nodes of the current node , vk . ( A3:12-16) For every neighboring node , vn the PRI overlap information , path length , summary sentence and path score are updated before the next recursion . ( A3:29) If a vn is not collapsible , then a regular traversal takes place . ( A3:17-27) However , if vn is collapsible , the updated sentence in A3:14, will now serve as an anchor in A3:18. ( A3:21) A3 will then attempt to start a recursive traversal from all neighboring nodes of vn in order to find corresponding collapsed candidates . ( A3:22-26) After this , duplicates are eliminated from the collapsed candidates and the collapsed path score is computed . The resulting stitched sentence and its final score are then added to the original list of candidate summaries.
(A2:14-18) Once all paths have been explored removed and the remaining are sorted in descending order of their path scores . The best ? ss candidates are ? picked ? as final Opinosis summaries.
5 Experimental Setup
We evaluate this abstractive summarization task using reviews of hotels , cars and various prod-ucts1. Based on these reviews , 2 humans were asked to construct ? opinion seeking ? queries which would consist of an entity name and a topic of interest . Example of such queries are : Amazon Kin-dle:buttons , Holiday Inn , Chicago : staff , and so on . We compiled a set of 51 such queries . We create one review document per query by collecting all review sentences that contain the query words for the given entity . Each review document thus consists of a set of unordered , redundant review sentences related to the query . There are approximately 100 sentences per review document.
We use ROUGE ( Lin , 2004b ) to quantitatively assess the agreement of Opinosis summaries with human composed summaries . ROUGE is based on an ngram cooccurrence between machine summaries and human summaries and is a widely accepted standard for evaluation of summarization tasks . In our experiments , we use ROUGE1,
ROUGE2 and ROUGE-SU4 measures . ROUGE1 and ROUGE2 have been shown to have most correlation with human summaries ( Lin and Hovy , 2003) and higher order ROUGE-N scores ( N > 1) estimate the fluency of summaries.
We use multiple reference ( human ) summaries in our evaluation since it can achieve better correlation with human judgment ( LIN , 2004a ). We leverage Amazon?s Online Workforce2 to get 5 different human workers to summarize each review document . The workers were asked to be concise and were asked to summarize the major opinions in the review document presented to them . We manually reviewed each set of reference summaries and dropped summaries that had little or no correlation with the majority . This left us with around 4 reference summaries for each review document.
To allow performance comparison between humans , Opinosis and the baseline method , we implemented a Jackknifing procedure where , given K references , the ROUGE score is computed over K sets of K1 references . With this , average human performance is computed by treating each reference summary as a ? system ? summary , computing ROUGE scores over the remaining K1 reference 1Reviews collected from Tripadvisor , Amazon , Edmunds 2https://www.mturk.com summaries.
Due to the limited work in abstractive summarization , no natural baseline could be used for comparison . The existing work in this area is mostly domain dependent and requires too much manual effort ( explained in Section 1). The next best baseline is to use a state of the art extractive method . Thus , we use MEAD ( Radev et al , 2000) as our baseline . MEAD is an extractive summarizer based on cluster centroids . It uses a collection of the most important words from the whole cluster to select the best sentences for summarization.
By default , the scoring of sentences in MEAD is based on 3 parameters - minimum sentence length , centroid , and position in text . MEAD was ideal for our task because a good summary in our case would be one that could capture the most essential information . This is exactly what centroid-based summarization aims to achieve . Also , since the position in text parameter is irrelevant in our case , we could easily turn this off with MEAD.
We introduce a readability test to understand if Opinosis summaries are in fact readable . Suppose we have N sentences from a system-generated summary and M sentences from corresponding human summaries . We mix all these sentences and then ask a human assessor to pick at most N sentences that are least readable as the prediction of system summary.
readability(O ) = 1? # CorrectPickN
If the human assessor often picks out system generated summaries as being least readable , then the readability of system summaries is poor . If not , then the system generated summaries are no different from human summaries.
6 Results
The baseline method ( MEAD ) selects 2 most representative sentences as summaries . To give a fair comparison , we fix the Opinosis summary size , ? ss = 2. We also fix ? vsn = 15. The best Opinosis configuration with ? ss = 2 and ? vsn = 15 is called Opinosisbest (? gap = 4, ? r = 2, Swt loglen).
ROUGE scores reported are with the use of stemming and stopword removal.
Performance comparison between humans,
Opinosis and baseline . Table 2 shows the performance comparison between humans , Opinosisbest and the baseline method . First , we see that the baseline method has very high recall scores compared to Opinosis . This is because extractive methods that just ? select ? sentences tend to be much longer resulting in higher recall . However , these summaries tend to carry information that may not be significant and is clearly reflected by the poor
ROUGE1 ROUGE2 ROUGE-SU4 Avg # Words
Human 0.3184 0.1106 0.1293 17
Opinosis 0.2831 0.0853 0.0851 15
Baseline 0.4932 0.1058 0.2316 75
Precision
ROUGE1 ROUGE2 ROUGE-SU4 Avg # Words
Human 0.3434 0.1210 0.1596 17
Opinosis 0.4482 0.1416 0.2261 15
Baseline 0.0916 0.0184 0.0102 75
Fscore
ROUGE1 ROUGE2 ROUGE-SU4 Avg # Words
Human 0.3088 0.1069 0.1142 17
Opinosis 0.3271 0.0998 0.1027 15
Baseline 0.1515 0.0308 0.0189 75
Table 2: Performance comparison between Humans , Opinosisbest and Baseline.
0.110
E-SU4 0.3100.330
GE1 0.0900.1000.110
ROUG basi c 0.2900.3100.330
ROU basi c 00700.0800.0900.1000.110 basi c wt_l ogle n wt_l en 0.2700.2900.3100.330 basi c wt_l ogle n wtl en 0.0700.0800.0900.1000.110 5basi c wt_l ogle n wt_l en 0.2500.2700.2900.3100.330 5bas ic wt_l ogle n wt_l en ? gap ? gap 0.0700.0800.0900.1000.110 5basi c wt_l ogle n wt_l en 0.2500.2700.2900.3100.330 5bas ic wt_l ogle n wt_l en ? gap ? gap Figure 2: ROUGE scores ( fmeasure ) at different levels of ? gap , ? r = 2.
precision scores.
Next , we see that humans have reasonable agreement amongst themselves given that these are independently composed summaries . This agreement is especially clear with the ROUGE2 recall score where the recall is better than Opinosis but comparable to the baseline even though the summaries are much shorter . It is also clear that Opinosis is closer in performance to humans than to the baseline method . The recall scores of Opinosis summaries are slightly lower than that achieved by humans , while the precision scores are higher ( Wilcoxon test shows that the increase in precision is statistically more significant than the decrease in recall ). In terms of fscores , Opinosis has the best ROUGE1 score and its ROUGE2 and ROUGE-SU4 scores are comparable with human performance . The baseline method has the lowest fscores . The difference between the fscores of Opinosis and that of humans is statistically insignificant.
Comparison of scoring functions . Next , we look into the performance of the three scoring functions , Sbasic , Swt len and Swt loglen described in Section 3. Figure 2 shows ROUGE scores of these scoring methods at varying levels of ? gap . First , 0.30
ROU
GE-
ROU
GE-
SU4 0.200.250.30
ROU
GE-
R2
R3
R4
ROU
GE-
SU4 0.200.250.30
R2
R3
R4
ROU
GE-ic wt_ logl en wt_ bas ic 0.050.060.070.080.090.10
R2
R3
R4
ROU
GE-
SU4 bas ic wt_ logl en wt_ bas ic
Figure 3: ROUGE scores ( fmeasure ) at different levels of ? r averaged across ? gap ? [1, 5] 0.230.28
PRECISION precis ion-re call cu rve
ROUG
E-SU4 0.400.450.50
PRECISION precis ion-re call cu rve
ROUG
E1 x coll apse x dup elim x coll apse + dup elim
Opino sis ( base line ) + ll x coll apse x dup elim x collap se + dup elim
Opino sis ( base line ) 0.180.230.28 0 .06 0.07 0.08 0.09 0.1
RECA
LL precis ion-re call cu rve
ROUG
E-SU4 0.350.400.450.50 0 .22 0.24 0.26 0.28 0.3
RECA
LL precis ion-re call cu rve
ROUG
E1 x coll apse x dup elim x coll apse + dup elim
Opino sis ( base line ) + coll apse x dup elim x coll apse x dup elim x collap se + dup elim
Opino sis ( base line ) + col lapse x dup elim Figure 4: PrecisionRecall comparison with different Opinosis features turned off.
it can be observed that Swt basic which does not use path length information , performs the worst.
This is due to the effect of heavily favoring redundant paths over longer but reasonably redundant ones that can provide more coverage . We also see that Swt len and Swt loglen are similar in performance with Swt loglen marginally outperforming Swt len when ? gap > 2. Since Swt len uses the raw path length in its scoring function , it may be inflating the path scores of long but insignificant paths . Swt loglen scales down the path length , thus providing a reasonable tradeoff between redundancy and the length of the selected path . The three scoring functions are not influenced by different levels of ? r as shown in Figure 3.
Effect of gap setting (? gap ). Now , we will examine the effect of ? gap on the generated summaries . Based on Figure 2, we see that setting ? gap=1 yields in relatively low performance . This is because ? gap=1 implies immediate adjacency between the PIDs of two nodes and such strict adjacency enforcements prevent redundancies from being discovered . When ? gap is increased to 2, there is a big jump in performance , after which improvements are observed in smaller amounts . A very large gap setting could increase the possibility of generating ill-formed sentences , thus we recommend that ? gap is set between 25.
Effect of redundancy requirement (? r ) . Figure 3 shows the ROUGE scores at different levels of ? r . It is clear that when ? r > 2, the quality of summaries is negatively impacted . Since we only have about 100 sentences per review document , ? r > 2 severely restricts the number of paths that can be explored , yielding in lower ROUGE scores.
Since the scoring function can account for the level of redundancy , ? r should be set according to the size of the input data . For our dataset , ? r = 2 was ideal.
346 ? A bo ut foo da t H oli da y I nn , L on do n?
Human summaries : [1]
Foo d w as exc ell en t w ith a wid e r an ge of ch oic es an d g oo d s erv ice s.
[2]
The fo od is go od , th e s erv ice gr ea t . Ve ry go od se lec tio n o f fo od fo r b rea kfas t bu ffe t.
?W ha t is fr ee at
Be stw es ter nI nn , S an Fr an cis co ?
Human summaries : [1]
The re is fre e Wi
Fiin ter ne t a cc es s a va ila ble in al l th e r oo ms .. Fro m 56 p.m . th ere is fre e win e t as tin g a nd ap pe tize rs av ail ab le to all th e g ue sts .
[2]
Eve nin g w ine re ce pti on an d f ree co ffe e i n t he m orn ing . Fre e i nte rne t , f ree pa rkin g a nd fre em as sa ge
Opinosis abstractive summary:
The fo od w as e xce lle nt , g oo d a nd d eli cio us . Ver y g oo d s ele cti on of fo od .
Baseline extractive summary:
With in ard s o f le av ing th e h ote l a nd he ad ing to th e Tu be St ati on yo u h av e a nu mb er of fas t fo od ou tle ts , hi gh str ee tRe sta uta nts , P as try sh op s a nd su pe rm arke ts so ify ou did wis ht ol ive in yo ur ho tel roo m for the du rat ion of yo ur fre e m as sa ge .
Opinosis abstractive summary:
Free w ine re ce pti on in ev en ing . Fre e c off ee an d b isc ott i a nd w ine .
Baseline extractive summary:
The fre e w ine an d n ibb les se rve d b etw ee n 5 pm an d 6 pm w ere a lov ely to uc h . Th ere ' s f ree co ffe e tea sa tb rea kfas tti me wit hl ittl eb isc ott ia nd be st of all fro m 5t ill6 pm yo ug et af ree su pe rm arke ts , s o i f y ou di d w ish to liv e i n y ou r h ote l ro om fo r th e d ura tio n o f y ou r sta y , yo u c ou ld do .......
co ffe e , tea s a t b rea kfas t tim e w ith lit tle bi sc ott i a nd , b es t o f a ll , fro m 5 t ill 6 pm yo u g et a f ree win e ' t as tin g ' r ec ep tio n w hic h , as lo ng as yo u d on ' t ta ke ?? Figure 5: Sample results comparing Opinosis summaries with human and baseline summaries.
Effect of collapsed structures and duplicate elimination . So far , it has been assumed that all features used in Opinosis are required to generate reasonable summaries . To test this hypothesis , we use Opinosisbest as a baseline and then we turn off different features of Opinosis . We turn off the duplicate elimination feature , then the collapsible structure feature , and finally both . Figure 4 shows the resulting precisionrecall curve . From this graph , we see that without duplicate elimination and when collapsing is turned off , the precision is highest but recall is lowest . No collapsing implies shorter sentences and thus lower recall , which is clearly reflected in Figure 4. On top of this , if duplicates are allowed , the overall information coverage is low , further affecting the recall.
Notice that the presence of duplicates with the collapse feature turned on results in very high recall ( even higher than the baseline ). This is caused by the presence of similar phrases that were not eliminated from the collapsed candidates , resulting in long sentences that artificially boost recall . The Opinosis baseline which uses duplicate elimination and the collapsible structure feature , offers a reasonable tradeoff between precision and recall.
Readability of Summaries . To test the readability of Opinosis summaries , we conducted a readability test ( described in Section 5) using summaries generated from Opinosisbest . A human assessor picked the 2 least readable sentences from each of the 51 test sets ( based on 51 summaries ). Collectively , there were 565 sentences out of which 102 were Opinosis generated . Out of these , the human assessor picked only 34 of the sentences as being least readable , resulting in an average readability score of 0.67. This shows that more than 60% of the generated sentences are indistinguishable from human composed sentences . Of the 34 sentences with problems , 11 contained no information or were incomprehensible , 12 were incomplete possibly due to false positives when the sentence validity check was done , and 8 had conflicting information such as ? the hotel room is clean and dirty ?. This happens due to mixed feelings about the same topic and can be resolved using sentiment analysis . The remaining 3 sentences were found to contain poor grammar , possibly caused by the gaps allowed in finding redundant paths.
Sample Summaries . Finally , in Figure 5 we show two sample summaries on two different topics.
Notice that the Opinosis summaries are concise , fairly wellformed and have closer resemblance to human summaries than to the baseline summaries.
7 Conclusion
In this paper , we described a novel summarization framework ( Opinosis ) that uses textual graphs to generate abstractive summaries of highly redundant opinions . Evaluation results on a set of review documents show that Opinosis summaries have better agreement with human summaries compared to the baseline extractive method . The Opinosis summaries are concise , reasonably wellformed and communicate essential information.
Our readability test shows that more than 60% of the generated sentences are no different from human composed sentences.
Opinosis is a flexible framework in that many of its modules can be easily improved or replaced with other suitable implementation . Also , since Opinosis is domain independent and relies on minimal external resources , it can be used with any corpus containing high amounts of redundancies.
Our graph representation naturally ensures the coherence of a summary , but such a graph emphasizes too much on the surface order of words . As a result , it cannot group sentences at a deep semantic level . To address this limitation , we can use a similar idea to overlay parse trees and this would be a very interesting future research.
8 Acknowledgments
We thank the anonymous reviewers for their useful comments . This paper is based upon work supported in part by an IBM Faculty Award , an Alfred P . Sloan Research Fellowship , an AFOSR MURI Grant FA9550-08-1-0265, and by the National Science Foundation under grants IIS-0347933, IIS-0713581, IIS-0713571, and CNS-0834709.
347
References [ Barzilay and Lee2003] Barzilay , Regina and Lillian Lee . 2003. Learning to paraphrase : an unsupervised approach using multiple-sequence alignment.
In NAACL ?03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology , pages 16?23, Morristown , NJ , USA . Association for Computational Linguistics.
[DeJong1982] DeJong , Gerald F . 1982. An overview of the FRUMP system . In Lehnert , Wendy G . and Martin H . Ringle , editors , Strategies for Natural Language Processing , pages 149?176. Lawrence Erlbaum , Hillsdale , NJ.
[Erkan and Radev2004] Erkan , Gu?nes and Dragomir R.
Radev . 2004. Lexrank : graphbased lexical centrality as salience in text summarization . J . Artif . Int.
Res ., 22(1):457?479.
[Finley and Harabagiu2002] Finley , Sanda Harabagiu and Sanda M . Harabagiu . 2002. Generating single and multidocument summaries with gistexter . In Proceedings of the workshop on automatic summarization , pages 30?38.
[Jing and McKeown2000] Jing , Hongyan and Kathleen R . McKeown . 2000. Cut and paste based text summarization . In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference , pages 178?185, San Francisco , CA , USA . Morgan Kaufmann Publishers
Inc.
[Lerman et al2009] Lerman , Kevin , Sasha BlairGoldensohn , and Ryan Mcdonald . 2009. Sentiment summarization : Evaluating and learning user preferences . In 12th Conference of the European Chapter of the Association for Computational Linguistics ( EACL09).
[Lin and Hovy2003] Lin , Chin-Yew and Eduard Hovy.
2003. Automatic evaluation of summaries using ngram cooccurrence statistics . In Proc . HLTNAACL , page 8 pages.
[LIN2004a ] LIN , Chin-Yew . 2004a . Looking for a few good metrics : Rouge and its evaluation . proc . of the 4th NTCIR Workshops , 2004.
[Lin2004b ] Lin , Chin-Yew . 2004b . Rouge : a package for automatic evaluation of summaries . In Proceedings of the Workshop on Text Summarization Branches Out ( WAS 2004), Barcelona , Spain.
[Lu et al2009] Lu , Yue , ChengXiang Zhai , and Neel Sundaresan . 2009. Rated aspect summarization of short comments . In 18th International World Wide
Web Conference ( WWW2009), April.
[Mihalcea and Tarau2004] Mihalcea , R . and P . Tarau.
2004. TextRank : Bringing order into texts . In Proceedings of EMNLP-04and the 2004 Conference on Empirical Methods in Natural Language Processing,
July.
[Pang and Lee2004] Pang , Bo and Lillian Lee . 2004.
A sentimental education : Sentiment analysis using subjectivity summarization based on minimum cuts.
In Proceedings of the ACL , pages 271?278.
[Pang et al2002] Pang , Bo , Lillian Lee , and Shivakumar Vaithyanathan . 2002. Thumbs up ? Sentiment classification using machine learning techniques . In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing ( EMNLP ), pages 79?86.
[Radev and McKeown1998] Radev , DR and K . McKeown . 1998. Generating natural language summaries from multiple online sources . Computational Linguistics , 24(3):469?500.
[Radev et al2000] Radev , Dragomir , Hongyan Jing , and Malgorzata Budzikowska . 2000. Centroid-based summarization of multiple documents : Sentence extraction , utility-based evaluation , and user studies.
In In ANLP/NAACL Workshop on Summarization , pages 21?29.
[Radev et al2002] Radev , Dragomir R ., Eduard Hovy , and Kathleen McKeown . 2002. Introduction to the special issue on summarization.
[Saggion and Lapalme2002] Saggion , Horacio and Guy Lapalme . 2002. Generating indicative-informative summaries with sumum . Computational Linguistics , 28(4):497?526.
[Snyder and Barzilay2007] Snyder , Benjamin and Regina Barzilay . 2007. Multiple aspect ranking using the good grief algorithm . In In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics ( HLTNAACL , pages 300?307.
[Titov and Mcdonald2008] Titov , Ivan and Ryan Mcdonald . 2008. A joint model of text and aspect ratings for sentiment summarization . In Proceedings of ACL08: HLT , pages 308?316, Columbus , Ohio , June . Association for Computational Linguistics.
348
