Acquisition of Phrase-level Bilingual Correspondence 
using Dependency Structure
Kaoru Yamamoto and Yuji Matsumoto
Graduate School of Information Science,
Nara Institute of Science and Technology ,  8916-5 , Takaymna-cho , Ikoma-shi , Nara , Japan
Abstract
This paper describes a method to find phrase-level translation patterns from parallel corpora by applying dependency structure analysis  . We use statistical dependency parsers to determine dependency relations between base phrases in ase N  ; ence . Our method is tested with a business expression corpus containing  10000 English Japanese sentence pairs and achieved approximately  90 % accuracy in extracting bilingual correspondences  . The result shows that the use of dependency relation helps to acquire interesting translation patterns  . 
1 Introduction
Since the advent of statistical methods in Machine qh ' anslation  , the bilingual sentence align-merit ( Brown et al , 1991) or word alignment ( Dagan et al ,  1992 ) have been explored and achieved numerous success over the last decade  . 
IncoN ; rasl , , fewer resull ; s are reported in phrase-level correspondence . As word sequences are not translated literally a word for a word  , acquiring phrase qevel correspondence still remains an important problem to be exploited  . 
This paper proposes a method to extract phrase -level correspondence fi'om sentence-aligned parallel corpora using statistically probable dependency relations  , i . e . head-modifier relations in a sentence . 
The distinct characteristics of our approach is twofold  . First , our approach uses dependency relations rather than alignment  , cognate and/or position heuristics previously applied  ( Melamed ,  1995) . Our approach is based on the assumption that the word ordering and positions may not necessarily coincide between the two languages  , but the dependency structure between words will be preserved  . We believe that dependency relations offer richer linguistic clues  ( syntactic information ) and are effcctive for language pairs with different word ordering constraints  . 
Secondly , statistical dependency parsers are used to obtain candidate patterns  . Previous methods mostly use rule-based parsers for pre-processing  ( Matsumotoeal . , 1993), ( Kitamura and Matsumoto , 1995) . The progress in parsing technology are noteworthy  , and in particular , variou statistical dependency models have been proposed  ( Collins ,  1997) , , ( Ratnaparkhi ,  1997) , ( Charniak ,  2000) . It has an advantage over the rule-based counterpart in that it achieves wider coverage  , does not need to care for consistency in rule writing  , and is robus to domain changes . 
We conjecture that our approach improves coverage a  . nd robustness by use of sl ; a tistical dependency parsers . 
In this paper , we aim to b we stigate timefficacy of statistically probable dependency structure in finding phrase level bilingual correspondence  . Though our discussion will proceed for English Japanese phrasal correspondence  , the proposed approach is applicable to any pair of languages  . 
This paper is organised as follows : In the next section  , we present he overview of our approach . In Sections 3 and 4 , components are elaborated in detail . In Section 5~experiment and results are given . In Section 6 , we compare our approach with related works , and finally our findings are concluded in Section  7  . 
2 Overview of Our Approach
Our approach presupposes a sentence-aligned parallel corpora  . The task is divided into two steps : a monolingual step in which candidate patterns are generated by use of dependency relations  , and a bilingual step in which these candidate patterns fi'om each language are paired iii  ; ii
Candidate Generator
J Candidates
I Candidate Generator

E Candidates / ? Phrase Matching \]
JE Translation Patterns
Figure 1: flow of our approach with their translations . Figure 1 shows the flow of our method . 
Our primary aim is to investigate the effectiveness of dependency structures in the monolingual candidate generation step  . For this reason , the bilingual step borrows the weighted Dice coefficient and greedy determination from  ( Kitanmra and Matsumoto ,  1996) . 
In the following sections , we explain each step in detail . 
3 Dependency-Preserving Candidate
Patterns
Dependency grammar or related paradigm ( Hudson ,  1984 ) focuses on individual words and their relationships  . In this framework , every phrase is regarded as consisting of a governor and dependants  , where dependants may be optionally classified further  . The syntactically dominating word is selected as the governor  , with modifiers and complements acting as dependants  . Dependency structures are suitably depicted as a directed acyclic graph  ( DAG )  , where arrows direct from dependants to governors . 
We use a maximum likelihood model proposed in ( Fujio and Matsumoto ,  1998 ) where the dependency probability between segments are determined based on its cooccurrence and distance  . It has constraints that ( a ) dependencies do not cross , ( b ) e e ; ch segment has at least one governor I . Furthermore , the model has an 1except for the ' root ' segment . For Japanese , the ' root ' segment is the rightmost segment . For English , option to allow multiple dependencies whose probabilities are above certain confidence  . It is usefl fi for cases where phrasal dependencies cannot be determined correctly using only syntactic information  . It has an effect of improving recall by sacrificing precision and may contain more partially correct results useful for our candidate pattern generation  . 
We apply the following notions as units of segments : For English  ,   ( a ) a preposition or conjunction is grouped into the succeeding base NPs  2  ,   ( b ) auxiliary verbs are grouped into the succeeding main verb  . For Japanese , one ( or a sequence of ) content word ( s ) optionally followed by function words3 . 
Having chunked into suitable segments , sen-tcnccs are parsed to obtain dependency relations  . We have setup the following three models : 1 . best-one model : uses only the most likely ( statistically best ) dependency rela-tio~m . At most one dependency is allowed for each segment  . 
2 . ambiguous model : uses dependency relations above the certain confidence score  0  . 54  . Multiple dependencies may be considered for each segment  . 
3 . adjacent model : uses only adjacency relations between segments  . A segment is adjacent to the previous segment . 
In tile ambiguous model , we expect that n lore likely dependency relations will appear frequently given in a large corpus  , thereby increasing the correlation score . Hence , ambiguity at parsing phase will hopefully resolved in the following bilingual pairing phase  . As for the adjacent model , only chunking and its adjacency are used . 
Finally , dependency relations between segments is used to generate candidate patterns  . 
the segment that contain stimma hlverb is regarded as the ' root ' segment  . 
2aba~seNPor'minimal'NP is non-reeursive NP , i . e . 
none of its child constituents are NPs.
' ~ often referred , ~ sabunsetsu.
4statistically-not-the-best dependencies m'e also in-eluded if prob  ( kth--ranked dependency ) p  ~ ob ( (g + 1 ) th ~ ged dependency ) -> O . SO ) size i ) size 2 ) size 3 ) \[ saw\]\[agirl\]\[in lhepark\]1 , saw , gM , parkl_saw , girl_saw , in-park_sawl girl saw(T ) , l_in-park saw ( T ) \[ I\]\[saw\]\[agMl\[inl the park\]s izei  ) size 2 ) size 3 ) I , saw , girl , park saw_l , girl saw , in-park_girl girls aw_l(L ) , in-park girl saw ( L )
Ill size l ) size 2) size 3)
Figure 2: best-one model\[saw\]\[agM\]\[inl the park lI  , saw , gM , parkI__saw , gM saw , in q ~ ark saw , in-park_girlI_gil'lsaw(T ) , ljn-parks aw(T ) , in-park , girl saw ( L )
Figure 3: anfl . figuous model
In this paper , dependency size of a candidate pattern designates the nuln ber of segments con-netted through dependency relations  . Figures 2 ,  3 , and 4 illustrate xamples of English can ( li- ( late patterns of dependency size 71 , 2 and 3 for the propose dependency models . 
In a del ) endency-connected candidate pattern , function words of the governor segment is dropped  . This is to cope with data sparseness in generated candidate patterns  . Moreover , two types of DACs can be generated from patterns of size  3  , and we use DAO-type tags ( ' I2 and ' T' ) to distinguish their types . W ( ' also note that candidate patterns do not necessarily for low the word ordering of original sentences  . 
The algorithn ~ is as follows:
Input : a corpus , the inininmm occurrence threshold in a corpus fm in and the dependency sized w  . 
For each sentence illa corpus , process tlm following : 1 . Part-of-Speech Tagging 2 . Chunking : Rules are written as regular expressions defined over POS word sequences  . 
3 . Dependency Analysis 4 . Candidate Pattern Generation : Candidate patterns are generated and stored with their sentence ID  . Dependency-connected patterns of less than or equal to the sized w are extracted  . 
Figure 4: adjacent model
Output : a hash table that maps from candi- ( late patterns appearing at least the minimum occurrence f'm into their sentence IDs found in the corpus  . 
4 Phrase-level Correspondence
Acquisition
Pairing of candidate patterns is a confl ) inatorial problem and we take tile following tactics to reduce the seard~space  . First , our algorithm works in a greedy manlmr . This nmans that a translation pair deternfined in the early stage of the algorithm will imverbe consktered again  . 
Secondly , filtering process is incorporated.
Figure 5 illustrates filtering for a sentence pair " l saw a girl in the park /*\]~ : ~  . /L ~, DJ cO(J/"~-~~\]-" . A set of candidate patterns derived fi'oln English is depicted on tile left  , while thai ; from Japanese is depicted on the right . Once a pair " I_girl_saw(T ) /& . .~'~ _~ k ( T ) " is de-termied as a translation pair , then the algo-rithm assumes that " gI~_~'~~~\ ]:--   ( T ) " will not be paired with candidate patterns related to " L girl ~ qaw  ( T ) "  ( cancelled by diagonalines in Figure 5 ) for tile sentence pair . The operation effectively discards the found pairs and causes recalculation of correlation scores in the proceeding iterations  . 
As mentioned in Section 2 , our correlation score is calculated by the weighted Dice Coefficient defined as:  2f   . ipj ) = + . 5 where . \[j and . reare the number of occurrences in Japanese and English corpora respectively and fej is the number of cooccurrences  . 
'.\['. he algorithm is as follows:
Input : hash tables of candidate patterns for each language  , the initial threshold of frequency . fc ~, rr and the final threshold of fi'equency fm in . 
935/\[I\[.\["i,iI
L .'\[ i
Figure 5: Filtering : word correspondence = Repeat the following until fcurr reaches fm in  . 
1 . For each pair of English candidate pe and Japanese candidate pj appearing at least f ~  ,  . r times , identify the most likely correspondences according to the correlation scores  . 
? For an English pattern Pc , obtain the correspondence and idates et pa = Pjl , Pj2 ,   . . . , Pin  su & that sim(pe , pjk ) > log2fmir ~ for all k . Similarly , obtain the correspondence andidate set PE for an Japanese pattern pj ? Register  ( P e , Pj ) as a translation pair if pj = arglnaxPjkEPJ sim  ( Pc , Pjk ) and Pc = argmaxPekCPEs in 1(pj , p &) . The correlation score of ( P e , Pj ) is the highest among PJ for Pe and PE for pj . 
2 . Filter out the cooccurrence positions for Pc , Pj , and related candidate patterns . 
3 . Lower the threshold of frequency if no more pairs are found with fcurr  . 
5 Experiment and Result 5 . 1 Exper imenta l Set t ing We use a business expression corpus  ( Takubo and Hashimoto ,  1995 ) containing 10000 sentences pairs which are prealigned . 
NLP tools are summarised in Table 1.
Parameter setting are as follows : dependency size d ~ is set to  3  . Initially , fc ~, . ~ and fmin are set to 100 and 2 respectively . A stile algorithm proceeds , f , u ~ is adjusted to half of its previous value if it is greater than  10  . Otherwise f , ~ r , " is(I , gI~)(saw , g\]5_)(girl , ~"~ k ) ( park , g  ~ , N ) preprocessing tool
POS(E ) ChaSen 2.096% precision
POS(J ) ChaSen2 . 097% precision chunking ( E ) SNP lexl . 0 rule-based cN mking ( J ) Unit rule-based dependency ( E ) edeptrial system dependency ( J ) j dep 85 87% precision
Table 1: NLP tools decremented by i . If the number of registered translation pairs is less than  10  , then fcurr is lowered in the next iteration . All parameters are empirically chosen . 
5.2 Result
Our approach is evaluated by the metrics defined below : count  ( pl , ) prccisicm-count(px )
E p , ( h ; ngth ( pt ) * cofreq ( pt )   ) coverage = ~ ploccur ( Pl ) Precision measures the correctness of extracted translation pairs  , while coverage measure stile proportion of correct ranslation pairs in the parallel corpora  . Let X be a pattern . 
count ( X ) give stile mml be r of X returned , occur ( X ) gives the mlmber of occurrences of X in each corpus  , length ( X ) gives the dependency size of X and c of rcq ( X ) gives the number of cooccurrences in the parallel corpora  . . Px nmans extracted patterns , and of which correct patterns are designated as pt-p ~ means the candidate patterns generated from each side of parallel corpora  . Coverage is calculated for English 10   6   9   4   8   13   7   10   6   19   5   29   4   67   3   150   2   414   ( * 2264 total 725 ( * totM 989 correct extracted c/e 66 100 . 00 7 100 . 00 7 85 . 71 4 100 . 00 13 100 . 00 13 76 . 92 20 95 . 0029 t . 00 . 00 72 93 . 05 164 91 . 46 461 89 . 80 474 55 . 69796 - - 1269 - - precision 1 . 00 . 00 95 . 00 95 . 83 92 . 30 97 . 29 92 . 00 92 . 85 94 . 94 94 . 15 92 . 83 91 . 08 77 . 93) 91 . 08 77 . 93)
Tal ) le 2: Precision : best one model th correct 25   6   12   7   10   6   9   4   8   13   7   11   6   18   5   29   4   68   3   118   2   432   ( * 2256 total 712 ( * total 968 extracted 1 . 3 1 . 524
C/e100 . 00 100 . 00 85 . 71 100 . 00 100 . 00 84 . 61 94 . 73 100 . 00 93 . 15 93 . 65 91 . .50 33 . 72 precision 10 0 . 00 i0 0 . 00 95 . 00 95 . 83 97 . 29 94 . 00 94 . 20 95 . 91 94 . 73 94 . 27 93 . 07 63 . 51) 93 . 07 63 . 51)
Table 3: Precision : aml ) iguous model and Japanese separately and then thiern man is taken  . 
Precision for each model is summarised in Tables 2 ,  3 , and 4 , while coverage is shown in Table 5 . To examine the characteristics of each model , we expand correspondence andidate sets PE and Pa so that patterns  '5 with tile correlation score > l og2   2   ( > 1 ) are also considered . These are marked by asterisks "*" in Tables . 
Random samples of correct and near-correct translation pairs are shown in Table  6  , Table 7 respectively . Extracted translation pairs are matched against he original corpora to restore their word ordering  . This restoration is donenmnually this time , but can be automated with little modification i our algorithm  . 
5 1 . ( L patterns where f~j = f ( , = fj = fi , ~ i , , = 2  ( *2 total ( * totMth 25   6   12   7   10   6   9   4   8   13   7   10   6   18   5   29   4   68   3   114   2   419 correct extracted c/o precision 100  . 00 100 . 00 100 . 00 100 . 00 85 . 71 95 . 00 100 . 00 95 . 83 100 . 00 97 . 29 84 . 61 92 . 00 94 . 73 92 . 75 100 . 00 94 . 89 93 . 15 94 . 15 93 . 65 92 . 59 86 . 57 88 . 86 56 . 45 76 . 27)
Table 4: Precision : ad . jacent model 88 . 86 76 . 27) model English Japanese coverage
I ) est-one 18 . 16 % 18 . 43 % 18 . 29% best one *19 . 12 % 19 . 59 % 19 . 13% ~ mt ) iguous 18 . 63 % 18 . 82 % 18 . 72% ambiguous * 19 . 57 % 19 . 95 % 19 . 76% ~ d . ia('ent17 . 74 % 18 . 03 % 17 . 88% adjacent * 18 . 69 % 19 . 20 % 18 . 94 %
Table 5: Coverage 5.3 Discussion
As we see from Tal)le 2 and 3 , the t ) est one model adfieves 1 ) etter precision than the adjacent model . Upon inspecting the results , nearly the same translation patterns are extracted for higher thresholds  . This is because our dependency parsers use the distance feature in determining dependency  . Consequently , nearer segments are likely to 1 ) edependency-related . Experiment data shows that tile exact overlaps are found in  9348 out of 14705   ( 63 . 55% ) candi- ( late patterns for English and 6625 out of 11566 ( 57 . 27%) for Japanese . 
However , the difference appears when the threshold reaches  3 and pattern such as " no the sitate to contact / ~ ) ~ ,  < ~*~ , ~" which is not found in the adjacent model are extracted  . 
Moreover , the l ) est-~one model is l ) ettm " in terms of coverage . These results support that the dependency relations appear useful clues than just being linearly ordered  . 
Comparing the 1 ) est one model with the am-t ) iguous model , the aml ) iguous model achieves a higher precision except for  *2  . This indicates thank+you consultations + include apply + for_the_position thank+you+in_advance not + hesitate+to_contact be+en closed+a_copy be_writ ing + to Jet+know applications + include upcoming_borard+of_director_s "_ meeting will J ~ ave +to_cancel have + high_hope business + is_expanded we + have_learned+t ~ om_your_fax leaving+in+about _ten_daysget+you+in_close_business_relations hi pwe + are_inquiring+regarding pay+special_at tcn tion 
Japanese ~ bUD ~' n~- 9 ~_t ~: bo ~+ ~ 6-~_~ ~~-~?~-_a ' , 5_~+i ~_t ~<+ t ~: k ~ ~ lc_+Jt/l . f,~_t-,5,~_1_0_H_f ~+, N~score 4 . 7037 2 . 3219 2 . 2157 1 . 6000 1 . 6000 1 . 0566 1 . 0566 1 . 0000 1 . 0000 1 . 0000 1 . 0000 1 . 0000 1 . 0000 1 . 0000 1 . 0000 1 . 0000 1 . 0 000 Table 6: random smnples of correct ranslation patterns in best-one model  . "+" indicates a segment-sepm'ator ~ md "2' indicates a morpheme-sepm'ator . 
~English ( have_been_pleased ) + to_serve+as_thier_main_banker\[be_held\]-t - aL hotel_new_ohtanias sets_position +  ( in_good_shape )   ( have_been_placed ) + into_our_file ( put ) + one_month Jimit\[passed\]+on_past_uesday
Japanese ~ . l , k + ~_ a )  +  7   7   4 : bl_OH_cO+iNliJ ~ Table 7: r and mn samples of near-correct translation patterns where score is  1  . 000 . Segments obc deleted to become correct patterns are embraced by  "0"  . Segment so be added are embraced by " ~" that the accuracy of dependency parsers currently achieves are insufficient  , and therefore , better to expand the possibilities of candidate patterns by allowing redundant dependency relations  . As the dependency parsers improve , tlmbest ~ one model will outperform the ambiguous model  . However , as the result of *2 shows , candidates from redundant dependency relations are mostly exl  ; racted at the low threshohl . The overall trend reveals that redundant relations act as noise at low thresholds  , but help to scale up the the correlation score at higher thresholds  . 
As shown in Table 6 , a domain-specific disambiguation sample ( " Thank you Fb U ; b~~9" vs . " Thank you in advance / ~: b-z " E ~3N ~ b 3= W ? 9-"  ) is found . As for long-distance dependency-related ranslation patterns  , " ~ i "- case ( nominative ) and verb patterns ( consulta-t ; ions include / ~ , ~ t:--I , :1; . ~~~) are extracted 6 . 
6A typical Japanese sentence follows S-O-V s ~ructure : Other types of long-distance translation patterns such as "~ d "- case  ( accusative ) and verb patterns ( be held at X/X-d~g@ . 9-  ; 5  ) are not extracted even candidate patterns fi'om each corpus are generated  . 
Generally speaking , acquiring long-distance translation patterns is a hard problem  . We still require fllrther investigation examining under what circumstance the dependency relations are really effective  . So far , we use relatively " clean " business expression corpora which is a collection of standard usage  . However , in the realworld setting , more repetitions and variations will be observed . Adjuncts can be placed in less constrained way and the adjacent model cannot deal withif they are apart  . In such cases , awdl-ablilty of robust dependency parsers become s-sential  , dependency relations plays a key role in finding the long-distance translation patterns  . 
while tile English counterpart follows S-V-Os ~rucfiure  . 
9386 Related Works
Smadjaeta1 . (1996) finds rigid and flexible collocations . They first identify candidate collo-eLtions in English ~ and subsequently  , find the corresponding lq'ench collocations by gradually expanding the candidate word sequences  . Ki-tamura et a1 . (1996 ) enmnerates word sequences of arbitrary length ( n-grmn of content words ) that appear more than them in in mln threshold from English and Japanese and attempts to t lnd the correspomlence based on the prepared candidate lists  . 
Difference from Smadja et a1 . (t996 ) is that our method is hi-direction alnd difference from Ki Lamura et al  ( 1996 ) is that we use dependency relations whidl leads to " structured " phrasal correspondence as opposed to " flat " adjacent correspondence  . 
On the other hand , Matsumoto et a 1 . (1993), Ki Lamura et a 1 . (1995) and Meyer set a 1 . (1996) use . dependency structure for struct m'al matching , of sentences to acquire translation rules . 
Their method semplW grammar-based parsers and only work for declarative sentences  . Their objectives are complete in atching of dependency trees of two languages  . 
:\[ nstea(t , our method uses statistical dependency parsers and are not restricted to simple sentences for input  . Fnrthermore , we are concerned with partial matdfing of dependency tree so that the overall robustness and coverage will be improved  . 
7 Conclusion
In this paper , we propose a method to find phrase-level bilingual correspondence using dependency structure from parallel corpora  . We have conducted a preliminary experiment with 10000 business sentence pairs of English and Japanese and achieved approximately  90% precision . 
Though a fuller investigation still requires , our finding shows that the dependency relations serve as usefulinguistic lues in the task of phrase-level Mlingual correspondence acquisition  . 
References
P . \]? . Brown , J . C . Lai , and R, . L . Mercer .  1991 . 
.Aligning sentences in parallel corpora . In ACL-29:   29th Annual Mceting of the Association . for Computational Linguistics , pages 169-176 . 
E . Charniak .  2000 . A maximum-entropy-inspired parser . In NAA CL-2000:   1st Meeting of the North American Cltapter of the Association for Computational Ling v  , islics , pages 132139 . 
M . J . Collins .  1997 . Three generative , lexicalised models for statistical parsing . In ACL97:   35th Annual Meeting of the Association for Computational Linguistic % pages  1623  . 
I . Dagan , K . Church , and W . Gale .  1992 . R . c ) -bust bilingual word alignment for machine aided translation  . In PTve . of life Workshop on Very Large Co77) or a , pages 18 . 
M . Fujio and Y . Matsumoto .  1998 . Japanese dependency structure analysis based on Icyicalized statistics  . In Pros . of 3rd Conf . on Empcricat Mcthods in Natural Language Processing  , pages 8896 . 
R .. Hudson . 1984. Word Grammar . Blackwell.
M . Kitamura and Y . Matsumoto .  1995 . A machine translation system based on translation rules acquired from parallel corpora  . In Pros . of Recent Advances in Natural Lan-nguage P ~ vccssing  , pages 27-44 . 
M . Kitamura and Y . Matsumoto .  1996 . Aul , o-matic extraction of word sequence correspondences in parallel corpora  . In Pros . / ~ ttt Workshop on Very Large Corpora , pages 7987 . 
Y . Matsumoto , H . Ishimoto , and T . Utsuro.
1993. Structural matching of parallel texts.
In ACL93:   31st Annual Mcetin 9of the Association for Computational Linguistics , pages 2330 . 
I . D . Melamed .  1995 . Automatic evaluation and uniform filter cascades for inducing nbest translation lexicons  . In Pros . of 3rd Workshop on Very Large Cmpora , pages 184-198 . 
A . I/atnaparkhi .  1997 . A linear observed time statistical parser based on maximum entropy models  . In Proc of 2nd Conf . on Empsri-eal Methods in Natural Language P ~ vcessing  , pages 110 . 
K . Takubo and M . Hashimoto .  1995 . A Dictionary of English Bussiness Letter " Expressions  . 
Nihon Keizai Shimbun , Inc.

