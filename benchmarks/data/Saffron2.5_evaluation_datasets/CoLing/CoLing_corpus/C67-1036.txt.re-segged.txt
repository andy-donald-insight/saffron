Research Group for
Quantitative Linguistics

Stockholm 40

KVALPM 339
June 1911967
The Entropy of Recursive Markov Processes

BENNYBRODDA
The work reported in this paper has been sponsored by Humanistiska for sknings r~det  , Tekniska for sknings r ~ det and RiksbankensJub i leums-fond  , Stockholm , Sweden .  ' . 

THE~ENTROPYOFRECURSIVE MARKOV PROCESSES

BENNYBRODDA
KVAL , Fack , Stockholm 40, Sweden

The aim of this communication is to obtain an explicit formula for calculat-ing the entropy of a source which behaves in accordance with the rules of an arbitrary Phrase St ructure Grammar  , in which relative probabilities are a ttached to the rules in the grammar  . With this aim in mind we introduce analte ~ rnative definition of the concept of a PSG as a set of self-embedded  ( re-Cursive ) FiniteState Grammars ; when the probabilities are taken into account in such a grammar we call it a Recurs ive Markov Process  . 
1 . In the first section we give a more detai led definition of what kind of Mar-kov P rocesses we are going to generalize later on  ( in sec .  3) , and we also outline the concept of entropy in an ordinary Markov source  . More details " of information may be foup d ~ e  . g .   , in Khinchins " Mathematical Foundations of Information Theory "  , N . Y . ~1957 ~ or " Information Theory " by R . Ash , N . Y .  , 1965 . 
A Markov Grammar is defined as a Markov Sourcew it h the following proper-ties : Assume that there are n +  1 states , say SO , S1 ,   .   .   . , Sn , in the source . SO is defined as the initial state and S is defined as the final state and theo the rn states are called intermediates tates  . Weshall , of course , also have a transi-tion matrix , M = ( Pij) , containing the , transition probabilities of the source . 
a ) A transition from state Si to state Sk is always accompanied by a produc-tion of a  ( nonzero ) letter aik from a given finite a lphabet  . Transition to different states from one given state always produced iff e rent letters  . 
b ) From the " initial state ,   S0~ director indirect transitions should be possible to any other state in the source  . From no state is a transition to SO al lowed  . 
c ) From any state , director indirect transitions to the f inal state S should n be possible  . From Snnotransition is allowed to anyo the r state  ( Sn is an " absorbing state " )   . 
The work reported in this paper has been sponsored by Humanistisk a for sk-nings r ~ det  , Tekniska for sknings r ~ det and RiksbankensJub i leums fond  , Stock-holm , Swederi . 
A ( grammatical ) sente'nce should now be defined as the ( left-to-right ) conca-tenation of the letters produced by the source  , when passing from the initial state to the final state  . 
The length of a sentence is defined as the number of letters in the sentence  . 
To simplify matters without dropping much of generality we also require that d  ) The greatest common divisor for all the possible lengths of sentences is = l  ( i . e .   , the source becomes an aperiodic source , if it is short-circuited by identify ing the final and initial states  )   . ~- With the properties a-d above , the source obtained by identifying the fina l and initial states is an indecomposab le  , ergodic Markov process ( cf . Feller , " Probability Theory and Its Applicat ions "  , ch . 15, N . Y . s1950) . 
In the transition matrix M for a Markov grammar of our type all elements in the fir st column are zero  , and in the last row all elements are zero ex -cept the last one which is =  1  . For a given Markov grammar we define the uncer tainty or entropy  , Hi , for each state S i , i = 0 ,  1  .   .   .   . , n , as : nHi = ~ lPijl ? gPij ; i = 1, Z .   .   .   .   . n . 

We also define the entropy , Hor H(M ) , for the grammar as n = 1 (1) . = x . H . 
11i = 0 where x = ( x0, xz , .   .   .   , Xn_l ) is defined as the stationary distribut ion - ~ the source obtained when SO and Sn are identified  ; thus x is defined as the ( unique ) solution to the set of simultaneous equat ions  ( z ) xM 1 = x x0 + Xl +''"+ Xn-1  =  1 where M 1 is formed by shifting the last and firs t columns and then omitting the last row and column  . The mean sentence length .  ~ , of the set of grammat-ical sentences can now be easily calculated as  ( 3 ) = 1/x0 ( cf . Feller , op . t i t . ) 2 . Embedded Grammars We now assume that we have two Markov grammars  , M and M1 , with states SO , S1 .   .   .   . , Sn , and To , TI, .   .   .   , Tm , respectively , where SO and sn , TO and Tm are the corresponding initial and final states  . Now consider two states Si and Sk in the grammar M  ; assume that the corresponding transition probability is = Pik " We now transform the grammar  , M 1 , into a new one , M \] , by embedding the grammar M 2 in M 1 between the states Si and Sk , an operation which is performed by identify ing the states TO and T with them states S i and Sk respectively  . Or , to be more precise , assume that in the grammar M 1 the transitions to the states Tj , j  ~ l , has the probabilities q0 j " Then , in the grammar M' , transitions to a state T . from the state S . will 3   1 take place with the probability = . Pikq0j . A return to the state Sk in the " main " grammar from an intermediate state Tj in M  1 takes place with the probability qjm " With the conditions above fulfilled  , we propose that the entropy for the . composed grammar be calculated according to the formula:  ( 4 ) H ( M ' ) = H ( M ) + xipik " ~ I"H ( M ) 1 + xiPik ( ~1 - 1 ) where H ( M ) is the entropy of the grammar M when there is an ordinary connection  ( with probability Pik ) between the states Si and Sk , and where x . is M1 alone .   ( It is quite natural that this number appears as a weight in the formula  , since if one is producing a sentence accord ing to the grammar M and arrives at the state Si and from there " dives " into the grammar  M1  , then ~1 is the expected waiting time for emerging again in the main grammar M  .   ) The factor xiP ik may be interpreted as the combined probability of ever arriving at  . Si and there choosing the path over to M1 ( you may , of course , choose quite another path from Si ) . 
The proof of formula ( 4 ) is very'straightforward , once the premises accord-ing to the above have been given  , and we omit it here , as it does not give much extra insight to the theory  . T He formula may be extended to the case when there are : more than one subgrammar embedded in the grammar M'  , by adding similar terms as the one standing , to the right in the numerator and the denominator  . The important thing here is that the factors of the type x  . p . ~depend only on the probability matrix for the grammar M and a rede-  1   1 pendent of the subgrammars involved . 
3 . Recursive or Self-embedded Sources ~- .   . . . .
It is now quite natural to allow a grammar to have itself as a subgrammar or to allow a grammar M  1" to contain a grammar M ~ . which , in its turn , con-tains M1 , and so on . The grammars thus obtained cannot , however B be re-written as an ordinary Markov grammar  . The relation between an ordinary Markov grammar and a recursive one is ~ exactly simi larto the relation between Finite state Languages and Phrase Structure Languages  . 
To be more precise , assume that we have a set of Markov grammars M ~ Ml  .   .   .   .   . M ~ where MI 0 is called the main grammar and in the sense that the process always starts at the initial state in M ~ and ceases when it reaches the final state in M  0  . Each of the grammars may contain any number of the others  ( and itself ) as subgrammars . The only restriction is that from any state in any one of the grammars there should exist a path which ends up at the final state of MO  . 

If we interpret a source of our kind as a Phrase Structure Language  , the re-writing rules are all of the fol lowing kind:  ( 5 ) Si -* Aik + Sko . r_rSn- , #; where the S's are all nonterminal symbols .   ( They stand for the names of the states in the sources - M  ~  , l ~ iI .   .   .   .   . M ~ and where SO is assumed to be the initial symbol/the Chomsky an S/andSn is the terminating state which produces the sentence delimiter #  . The symbols Aik are either terminal symbols/l etters from a finite alphabet/or non- terminal symbols equal to the name of the in i tial state in one of the grammars M ~  , N i ~ .   .   .   .   . M ~/ one may grammar / . ) : i : s tands as an abbrev ia t ion fo r an arb i t ra ry sentence of that We assoc ia te each grammar M ! with the grammar M  . , j = 0, 12 .   .   .   .   , N , by 3   3 just considering it as a non-recursive one  , thaf is , we consider all the sym-bols Aikasterm in a l symbols  ( even if they are : ' not )   . The grammars thus ob-tained are ordinarily Markov grammars according to our definit ion  , and the entropies Hj = H ( Mj ) are easily computed according to formula ( 1 )  , as are the stationary distributions / formula  ( 2  )  /  . The follwoing theorem shows how the entrop ies H ! for the fully recursive grammars M ! are connected with the 
J3 numbers H..


The entropy H ! for a set of recursive Markov grammar Mj  , j = 0 ,  1 , 
J can be calculated according to the formula .   .   . , N , (6) k j = 0, 1 .   .   .   . , N . 
Here the factors Y j k are dependent only of the probability matrix of the ? grammar and the numbers ~ k defined as the mean sentence length of the sentences of the grammar M ~  , k = 0 ,  1 ,   .   .   .   . N , and computable accord-ing to lemma below . 
H ~ is the entropy for the grammar.
The theorem above is a direct application for the grammar of formula  ( 4 )  , sec .  2 . 
The coefficients Yjk in formula (6) can , more precisely , be calculated as a sum of terms of the type x iPim with the indices  ( i , m ) are where the gram-!"xi and are the components the sta-mar M~appears in the grammar  M3~ Pimtionary distribution and probabili ty matrix for the grammar M  . o , JAs sume now that we have a Markov grammar of our type  , but for which each transition will take a certain amount of time  . A very natural question is then : " What is the expected time to produce a sentence in that language ? " The answer is in the following lemma  . 

Let Mbea_M Markov grammar with states Si , i = O , S are the initial and final states respectively  , n1 .   .   .   .   , n , where SO and Assume that each transition Si  -  . Sk will take Ylk time units . 
Denote the expected time for arrival at S given that the grammar is in staten Si by ti  , i = 0 , I ,   . . . ~ n  ~ ( thus to is the expected time for producing a sen -tence  )  . The times tI will then fulfill the following set of simultaneously linear equations :  ( 7 ) ti = ~ Pik ( tik+tk ) k Formula ( 7 ) is itself a proof of the lemrn a . 
With more convenient notations we can write ( 7 ) as ( E-P ) t = Pt where E is the unit matrix , P is the probability matrix ( with P = 0 ) and nnPt is the vector with components Pi ( t ) = ~ Pimtim'i = 0 ,  1  .   .   .   . , n . 

The application of ~ helemma for computing the numbers ~ k in formula  ( 6 ) is now the following . 
The transition times of the lemma are , of course , the expected time ( or " lengths " as we have called it earlier ) for passing via a subgrammar of the grammar under consideration  . Thus the number tiki-~\] itself the unknownen -titles ~ k " 
J equations of type ( 7 ) for determining the vectors t of 1 emma . The first component of this vector , i . e . j the number tO , is then equal to the expected length ,  ~ , of the sentences of that g ~ ammar . ( Unfortunately , we have to compute extra the expected time for going from any state of the sub-gram-mars to the corresponding final state  .   ) The total number of unknowns involved when computing the entropy of our grammar  ( i . e .   , the entropy H ~ ) is equal to ( the total number of states in all our subgrammars  ) plus ( the number of subgrammars )   . 
This is also the number of equatior ~ , _for we have n+1 e~uations from formula ( 6 ) and then ( n + 1 ) sets of equations of the type ( 7 )  . We assert that all these simultaneous equat ionsa~e solvable  , if the grammar fulfills the conditions we earlier stated for the grammar  , i . e .   , that from ' each state in any sub-grammar ex is ts at least one path to the final state of that grammar  . 
