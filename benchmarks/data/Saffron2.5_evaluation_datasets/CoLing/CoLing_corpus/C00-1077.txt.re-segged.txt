Identifying Terms by their Family and Friends
Diana Maynard
Dept . of Computer Science
University of Sheffield
Regent Court , 211 Portobello St
Sheffield , $14 DP , UK
d.maynard0 dcs , shef.ac.uk
Sophia Ananiadou
Computer Science , School of Sciences
University of Saltbrd , Newton Building
Saltbrd , M54 WT , U.K.
s.anania dou@salford.ac.uk

Multiword terms are traditionally identified using statistical techniques or  , more recently , using hybrid techniques combining statistics with shallow linguistic information  . Al ) proaches to word sense disambiguation and machine translation have taken advantage of contextual information in a more mean-ing flflway  , but terminology has rarely followed suit . 
We present an approach to term recognition which identifies salient parts of the context and measures their strength of association to relevant candidate terms  . The resulting list of ranked terms is shown to improve on that produced by traditional methods  , in terms of precision and distribution , while the information acquired in the process can also be used for a variety of other applications  , such as disambiguation , lexical tuning and term clustering . 
1 Introduction
Although statistical approaches to automatic term recognition  , e . g . ( Bourigault , 1992; Daille et al , 1994; Enguehard and Pantera , 1994; 3usteson and Katz , 1995; Lauriston ,  1996) , have achieved relative success over the years , the addition of suitable linguistic information has the potential to enhance results still further  , particularly in the case of small corpora or very specialise domains  , where statistical information may not be so accurate  . One of the main reasons for the current lack of diversity in approaches to term recognition lies in the difficulty of extracting suitable semantic information from speeialised corpora  , particularly in view of the lack of appropriate linguistic resources  . The increasing development of electronic lexieal resources  , coupled with new methods for automatically creating and finetuning them from corpora  , has begun to pave the way for a more dominant appearance of natural language processing techniques in the field of terminology  . 
The TRUCKS approach to term recognition ( Ter-m Recognition Using Combined Knowledge Sources  ) focuses on identifying relevant contextual information from a variety of sources  , in order to enhance traditional statistical techniques of term recognition  . 
Although contextual information has been previously used  , e . g . in general language ( Grefenstette ,  1994 ) mid in the NCV alue method for term recognition ( Frantzi , 1998; Frantzi and Ananiadou ,  1999) , only shallow syntactic information is used in these cases  . The TRUCKS approach identifies different ; elements of the context which are combined to form the Information Weight  , a measure of how strongly related the context is to a candidate term  . The hffbrmation Weight is then combined with the statistical information about a candidate term and its context  , acquired using the NCValue method , to form the SNC-Value . Section 2 describes the NC Value method . Section 3 discusses the importance of contextual information and explains how this is acquired  . Sections 4 and 5 describe the hffbrmation Weight and the SNC-VMue respectively  . We finish with an evaluation of the method and draw some conclusions about the work and its fllture  . 
2 The NC Value method
The NCV alue method uses a combination of linguistic and statistical information  . Terms are first extracted from a corpus using the CV alue method  ( Frantzi and Ananiadou ,  1999) , a measure based on frequency of occurrence and term length  . This is defined formally as : is not nested C -Value  ( a ) = Zo . q~l(~ll~('n , ) ~ b~T ~ f ( b ) ) a is nested where a is the candidate string , f(a ) is its frequency in the corpus , eT , is the set of candidate terms that contain a , P(T a ) is the number of these candidate terms . 
Two different cases apply : one for terms that are found as nested  , and one for terms that are not . If a candidate string is not found as nested , its termhood is calculated from its total frequency and length  . If it is found as nested , termhood is calculated from its total frequency , length , frequency as a nested string , in . 
The NCV alue metho ( 1 build so il this by incorl ) o-rating contextual information in the form of a context factor for each candidate term  . A context word can be any noun , adjective or verb apI ) earing within a fixed-size window of tim candidate term  . Each context word is assigned a weight , based on how frequently it appears with a ca lldidate term  . Ttm seweights m'et it an SUlls lned for all colltext words relative to a candidate term  . The Contextl " actor is combined with the CV alue to form tlm NC Value : NC valu c  ( a )  = 0 . 8* Cvalue(a ) + 0 . 2* Cl , '( a ) (1) where aistile candidate term , Cvahte ( a ) is the Cvalue fin't lm candidate term , CF ( a ) is the context factor t br the candidate term . 
3 Contextual Information : a Term's
Social Life
Just as a person's social life can provide valuable cluesal  ) out their i ) ersonality , so we can gather much information about the nature of a term by investigating the coral  ) any it keeps . We acquire this knowledge by cxtra:ting three differentypes of contextual information :  1  . syntactic ; 2 . terminologic ~ fl ; 3 . semantic . 
3.1 Syntactic knowledge
Syntactic knowledge is based on words in the context which occur immediately t  ) efore or afl ; era can-did at cterm , wt fich we call boundary words . Following " barrier word " al ) proaches to term recoglfition ( Bourigault , 1992; Nelson et al ,  1995) , where par-titular syntactic ategories are used to delimiter a > didate terms  , we develop this idea fllrther by weighting boundary words according to tlmir category  . The weight for each category , shown in Table 1 , is all ) -cate ( 1 according to its relative likelihood of occurring with a term as opposed to a non-term  . A verb , therefore , occurring immediately before or after a candidate  , term , is statistically a better indicator of a term than an adjective is  . By " a better indicator " , we mean that a candidate term occurring with it is more likely to be valid  . Each candidate term is assigned a syntactic weight  , calculated by summing the category weights tbr the context boms dary words occurring with it  . 
Category Weight
Verb 1.2
Prep 1.1
Noun 0.9
Adj 0.7
Table 1: We . ights for categories of boundary words3 . 2 Termino log ica l knowledge Ternfinological knowledge concerns the terminological sta  . tus of context words . A context word whicl l is also a term ( whicll we call a contexter m ) is likely to 1 ) e a better indicator than one wl fich is not . 
The terminological status is determined by applying the NCV alueat  ) proach to the corlms , and considering tile top third of the list ; of ranked results as valid terms . A contexter m ( CT ) weight is then produced f in " each candidate term  , based on its total frequency of occurrence with all relewmt context terms  . The CT weight is formally described as follows : where a is the candidate term  ,  7' , is the set : of contexterms of a , disa word from Ta , f a ( d ) is the frequency of d as a context term of a . 
3.3 Semantic knowledge
Semantic knowledge is obtained about contexterms using the UMLS Metathesaurus and Semantic Network  ( NLM ,  1997) . The former provides a semantic tag for each term , such as Acquired Abnormality . 
The latte , r provides a hierarchy of semantic types , from wl fich we compute the similarity between a candidate term and the context I  ; erms it occurs with . 
An example of part of tim network is shown in Figure \]  . 
Similarity is measured because we believe that a contexterm which is semantically similar to a candidate term is more likely to be significant hanone wl fie his less similar  . We use timmethod for semantic distance described in  ( M~\ynard and Ananiadou , 1999a ) , wt fich is based on calculating the vertical position and horizontal distance betwee nodes in a hierarchy  . Two weights are cMculated : ? position a hmeasured by the combine distance from root to each node ? commonality : measured by the number of shared common ancestors multiplied by them unber of words  ( usuMly two )  . 
Similarity between the nodes is calculated by dividing tim commom flity weight by the  1  ) ositional weight to t ) roduce a figure between 0 and 1 , I being the ease \[' rM
ENTII'?\['r Ail
PIIYSICM , () IHECr /, /\[ TAIII

ITAIItl rrAtl 21
PI , ANTI " UN(;US

ALGA\['rlq
EVI,:NT\[TA2I
CONCEI~I'UAI,~N'I'I'I'Y

ANATOMII2ALSTIIUCTURI , ://
ITAI 211\[TAI 221
EMIIRYONICANA'I'OM\[IUA1,
STllUC'I'UItEAIINOILMAL rI'Y
Figure 1: Fragment of the Semantic Network where tile two nodes are identical  , and 0 being the case where there is no common ancestor . This is formally defined as follows : sim(w , .   .   . w , , )- com(w , . . . w , , ) (3) pOS(~Ul . . . Wn ) where corn(w1 . . . w , ~) is the commonality weight of words 1 .   .   . npos('wl . . . w , ~) is the positional weight of words l . . . n . 
Let us take an example from the UMLS . The similarity between a term t ) elonging to the semantic category Plant and one belonging to the category Fungus would be calculated as follows :-? Plant has the semantic odeTAlll and Fungush as the semantic ode TAl  l2  . 
? The commonality weight is the number of nodes in common  , multiplied by the number of terms we are considering  . TAlll and TAll2 have 4 nodes in common(T , TA , TA1 and TAll ) . So the weight will be 4*2 = 8 . 
? The positional weight is the total height of each of the terms  ( where tile root node has a height of 1 )  . TA lll has a height of 5(T , TA , TA1 , TAll and TAll1) , and TAl12 also has a height of 5(T , TA , TA1 , TAll and TAll2) . The weight will therefore be 5+5 = 10 . 
? The similarity weightistile comlnonality weight divided by the positional weight  , i . e . 

4 The Information Weight
The three individual weights described above are calculated for all relevant context words or context terms  . The total weights for the context are then combined according to the following equation : IW  ( a )  = ~  . syria(b ) + ~ f,(d) . sim , ( d ) (4) be C . ( l ~ 7 ~ where a is the candidate term,
Ca is the set of context words of a , b is a word from C , , f , ( b ) is tlm frequency of b as a context word of a , syn ~ ( b ) is the syntactic weight of b as a context word of a  , 
T . is the set of context terms of a , d is a word fl ' om T . , fi , ( d ) is the frequency of d as a contexterm of a , sims ( d ) is the similarity weight of d as a context term of a  . 
This basically means that the Infornlation Weight is composed of the total terminological weight  , 511 151-tiplied by tile total semantic weight , and then added to the total syntactic weight of all the context words or contexterms related to the candidate term  . 
5 The SNC-Value
Tile Information Weight gives a score for each candidate term based on the ilnt  ) ortance of the contextual intbrmation surrounding it  . To obtain the final SNC-Value ranking , the Information Weight is combined with the statistical information obtained using the NC - Vahmnmthod  , as expressed formally below : SlVCV , a . ,c ( a ) = NC Val ~ u ~ ( a ) + IW ( a )   ( 5 ) where a is the candidate term
NC Value ( a ) is the NC Value of a
IW is the Inqmrtance Weight of a
For details of the NCV alue , see ( l:5'antzi and Ananiadou ,  1999) . 
An example of the final result is shown in Table 2 . This corot ) are stile top 20 results from the SNC-Value list with the top 20 from the NCV alue list . 
The terms in italics are those which were considered as not valid  . We shall discuss the results in more detail in the next section  , but we cannote here three points . Firstly , the weights for the SNC-Value are substantially greater than those for the NC-Vahm  . 
This , in itself , is not important , since it , is the position in the list , i . e . the relative weight , rather than the absolute weight , which is important . Secondly , we can see that there are more valid terms in the SNC-Value results than in the NC Value results  . Itl ) owlllall ~ S_lllelllbralle\]n alignant_melanoma hyaline_fibrous_tissue planes_of_sectiontral  ) ecular Jnesh work keratinous_del ) risl ) ruch~s_inenll ) r&lieplane_of_section = m clanoma_of_choroid lymphocy tie An filtration ciliary_processes cellular Aibrous_tissues quamous_ct  ) i the liumo I ) tic_nerve_headl ) Ul ) illary_border ( : or lmal_el ) it helium selerald nw~siongranulation_tissue stratified_squamous_epi the liumocular ~structures  94996  . 2 90109 . 4 71 . 615 . 1 51486 . 8 46928 . 9 39054 . 5 36510 . 8 31 . 335 . 9 31017 . 4 28010 . 1 27445 . 5 26143 . 6 pla'ne_@sectiondencelnel ; ~ s_ill(~ . llll ) r ~ / iEebasal_cell_carcinomastump_of_optic_nerve1 ) asal_cell_l ) at ) illoma planc_of_section = rn clano , na_of_ch , or oid pla'ncs_@scctionmalignant_melanoma optic_nerveAmad ciliary q  ) rocesses 1 ) ruth's_membranekeratinous_eystellipse_of_skin wcdgc_of_lid_ma ~ yins caT "_tT ' ack conImctive_tissue vertical_plane carcinoma_of_lid excision_biopsy  1752  . 71 1 . 345 . 76 1 . 268 . 21 993 . 15 616 . 614 506 . 517 497 . 673 453 . 716 448 . 591 422 . 211 421 . 204 413 . 027 392 . 944 267 . 636 211 . 41 . 4 228 . 217 167 . 053 167 . 0 15 Table 2: Top 20 results for the SNC-VaIue and NCValue in hard to make flu : ther judgements based on this list alone  , 1) ecause we cmmots ~3 ; wlm theron ( ; ter-\]u is 1) etter than another , if tiE ( ; two terms are both valid . Thirdly , we can nee that more of the top 20 terms are valid t in ' tim SNC-Vahm than for the NC Value:  17   ( 851 X , ) as ot)t ) osed to 10 (50%) . 
6 Evaluation
The SNC-Value method wan initially t ( ; sted on a e or-l ) US of 800 , 000 eyet ) at hoh ) gyre I ) or tn , which had 1 ) eentagged with the Brillt ) art-of-nl ) eeeh tagger ( Brill ,  1992) . The ca . ndidate terms we , ' e first extracted using the NC Value method ( lhant zi ,  1998) , and the SNC-Value was then (: alculated . To exvdu-ate the results , we examined the p( . ' r formanee of the similarity weight alone , and the overall 1) erformance of the system . 
6.1 Evaluation methods
The main evaluation i ) rocedure was carried out with resl ) ec to a manual assessment of timlist of terms l ) y2 domainexI ) erts . There are , however , 1) roblems associated with such an evaluation . Firstly , there ix no golds t and m : d of evaluation , and secondly , manual evaluation is both fallil ) le and sul ) jective . To avoid this 1) rol ) lem , we measure the 1 ) erformance of the system ill relative term n rather than in absolute terms  , by measuring the improveln ( mt over the results of tile NCV aluease omt ) ared with mmmale vahlation . Although we could have used the list of terms 1 ) rovided in the UMLS , instead of a manu ~ ally evahlated list , we found that there was a huge discrei ) an ( : y1 ) etween this lint and the lint validated by the manual experts  ( only 20% of the terms they judged valid were fOtlEl ( 1 ill the UMLS )  . There are also further limitations to the UMLS , such as the fact that it is only nl ) e ( : if ictomedicine in general , 1) ut not to e yet ) athology , and the fact that it ; is organised ill nll chaway that only the preferred terms  , and not lexical variants , m'e actively and (: on nistent-ly1) r(~sent . 
We first evaluate the similarity weight individually  , since this is the main 1 ) rinciple on which the SNC-\S flue method relies . We the new duate the SNC-Va Iue as a whole t ) y comparing it with the NCV alue , so I ; hat we canew fluate the impact of tile addition of the deel  ) erforms of linguistic information in corl ) orated in : hehn I ) ortance Weight . 
6.2 Similarity Weight
One of the 1 ) roblems with our method of calculating similarity is that it relies on a  1  ) re-existing lexi- ( : al resource , which Eneans it is 1 ) rone to errors and omissions . Bearing in mind its innate inadequacies , we can nevertheless evaluate the expected theoretical performance of tilt measure by concerning ourselves only with what is covered by the thesaurus  . This means that we assume CO ml ) leteness ( although we know that this in not the case ) and evahtate it accordingly , ignoring anything which may be in issing . 
The semantic weightix based on the premise that tile more similar a context term is to the candidate term it occurs with  , the better an indicator that context term is . So the higher the total semantic weight top set 76%   24% middle set 56%   44% bottom set 49%   51% Table 3: Semantic weights of terms and nonterms for the candidate term  , the higher the ranking of the term and the better the chance that the candidate term is a valid one  . To test the performmme of the semantic weight , we sorted the terms in descending order of their semantic weights and divided the list into  3  , such that the top third contained the terms with the highest semantic weights  , and the bottom third contained those with the lowest  . We then compared how many valid and non-valid terms  ( according to the manual evaluation ) were contained in each section of the list ,  . 
Tile results , depicted in Table 3 , can be interpreted as follows . In the top third of the list ; , 76% were terms and 24% were nonterms , whilst in the middle third , 56% were terms and 44% were nonterms , and so on . This means that most of the valid terms are contained in the top third of tile list mid the fewest valid terms are contained in the bottom third of the list  . Also , the proportion of terms to nonterms in tile top of tile list is such that there are more terms than nonterms  , whereas in the bottom of the list ; there are more nonterms than tern is . This therefore demonstrates two things : ? more of ' the terms with the highest semantic weights are valid  , and fewer of those with the lowest semmitic weights are valid  ; ? more valid terms have high semantic weights than nonterms  , midmore nonterms have lower semantic weights than valid terms  . 
We also tested the similarity measure to see whether adding sosne statistical information would improve its results  , and regulate any discrepancies in tile uniformity of the hierarchy  . The methods which intuitively seem most plausible are based on information content  , e . g . ( Resnik , 1995; Smeaton and Quigley , 1996) . The informatios l content of an-ode is related to its probability of occurrence in the corpus  . Tiles no refi'equently it appears , the snore likely it is to be important in terms of conveying information  , and therefore the higher weighting it should receive  . We performed experiments to cosn-pare two such methods with our similarity measure  . 
The first considers the probability of the MSCA of the two terms  ( the lowest node which is an ancestor of both )  , whilst the second considers the probability of the nodes of the terms being colnpared  . However , the tindings showed a negligible difference between the three methods  , so we conchlde that there is no
SNC-Value NC-Vahm
Section Valid Precision Valid Precision 1   163   64%   160   62%   2   84 a a % 98   38%   3   89   35%   69   27%   4   89   35%   78   30%   5   76   30%   87   34%   6   57   22%   78   30%   7   66   26%   92   36%   8   75   29%   100   39%   9   70   27%   42   16%   10   59   23%   68   27% Table 4: Precision of SNC-Vahle and NCV alue advantage to begained by adding statistical int ' or-mation  , fbr this particular corpus . It ; is possible that with a larger corlms or different hierarchy  , this might slot be the case . 
6 . 3 Overall Evaluation of the SNC-Value We first ; compare the precision rates for the SNC-Value and the NCValue  ( Table 4 )  , by dividing tile ranked lists into 10 equal sections . Each section contains 250 terms , marked as valid or invalid by the manual experts . In the top section , the precision is higher for the SNC-Value , and in the bottom section , it is lower . This indicates that the precision span is greater fl ~ r the SNC-Value  , and therefore that the ranking is improved . The distribution of valid terms is also better for the SNC-Value  , since of the valid terms , more appear at the top of the list than at the bottom  . 
Looking at Figure 2 , we can see that the SNC-Value graph is smoother than that of the NC-Vahle  . 
We can compare the graphs niore accurately using a method we call comparative upward trend  . Be-cruise there is no one ideal graph , we instead measure how much each graph deviates from a monotonic line downwards  . This is calculated by dividing the total rise in precision percentage by the length of the graph  . A graph with a lower upward trend will therefore be better than a graph with a higher upward trend  . If we compare the upward trends of the two graphs  , we find that the trend for the SNC-Value is 0 . 9, whereas the trend for the NC Value is 2 . 7 . This again shows that the SNC-Valuer miking is better thmithe NCV alue ranking  , since it is more consistent . 
Table 5 shows a more precise investigation of the top portion of the list  ,   ( where it is to be expected that tern is are most likely to be wflid  , and which is therefore the inostimi ) ortant part of the list ) We see that the precision is most iml ) roved here , both in terms of accuracy and in terms of distribution of weights  . At the I ) ottom of the top section , the
U 71,111 .... NC-Vah , c\\
T ~ TTI
I34~678910
Scctionollist
Figure 2: Precision of SNC-Value and NC-Vatue

Section Valid IP recision 1   21   184%   2   19   176%   3  ~"  '68% ii 4:   16   164%   5   1  . 8 172% 6 12 148% 7 13 152% 8 : 7 : 68/ ) 9\]3I52% 10\]4i 56%\] NC Value
Valid Precision z 19   76%   23   92%   21   84%   13   52%   13   52%   19   76%   18   72%   14   56%   10   40%   8   32% Table 5: Precision of SNC-\S due and NC-Vahm for top 250 terms precision is much higher for the SNC-Value . This is important because ideally , all the terms in this part of the list should be valid  , 7 Conclusions In this paper , we have described a method for multiword term extraction which improves on traditional statistical at  ) proaches by incorporating more specific contextual information  . It focuses particularly on measuring the strength of association  ( in semantic terms ) l ) etween a candidate term and its context . 
Evahlation show simi ) rovement over the NC-Vahm approach , although the percentages are small . This is largely l ) ecmlse we have used a very small corpus for testing  . 
The contextuM information acquired can also be used for ammlber of other related tasks  , such as disambiguation and clustering . At present , the semantic information is acquired from a 1 ) re-existing domain-slmcitic thesaurus , but therem : c1 ) ossibili-tics for creating such a thesaurus automatically  , or entrancing an existing one , using the contextual information we acquire ( Ushioda , 1996; Mayna Mand
Anmfiadou , 1999b).
There is much scope t br filr the r extensions of this research  . Firstly , it ; could be extended to other ( lo-mains and larger corpora , in order to see the true benefit of such a . napl ) roach . Secondly , the thesaurus could be tailored to the corpus , as we have men-tion cd . An incremental approach might be possible , whereby the similarity measure is combined with statistical intbrmation to tune an existing ontology  . 
Also , the UMLS is not designed as a linguistic resource  , but as an information resource . Some kind of integration of the two types of resource would be use fifl so that  , for example , lexical variation could be more easily handled . 

D . Bourigault .  1992 . Surface grammatical analysis fortile extraction of terminological noun phrases  . In Proc . of l~th International Co ~@ rcnccon Computational Linguistics  ( COL\[NG )  , pages 977-981 , Nantes , b Yance . 
Eric Brill .  1992 . A simple rule-based part of speech tagger . In Pwc . of 3rd Confc~vnce of Applied Natural Language Processing . 
B . l ) aille , E . Gaussicr , and J . M . Lang 5 .  1994 . Towards automatic extraction of monolingual and t ) ilingual terminology . In Proc . of iSth International Conference on Computational Linguistics  ( COLIN (  ; ) , pages 515-521 . 
Chantal Enguehard and Lmu'ent Pantera . 1994.
Autoumtic naturala (: quisition of a terminology.
Journal of Quantitative Linguistics , 2(1):27-32.
K . T . li'r ; mtzi and S . Ananiadou .  1 . 999 . The C-Value/NC-Vahm domain independent method ~ br multiword term extraction  . Journal of Natural
Language PT vccssing , 6(3):1.45179.
K . T . Frantzi .  1 . 998 . Automatic Recognition of MultiWord Terms . Ph . D . thesis , Manchester
Metropolitan University , England.
G . Grefenstette .  1994 . E : rplorations in Automatic Thesaurus Discovcry . Kluwer Aca(temic Publishers . 
J . S . Just cson and S . M . Katz .  1995 . Technical terminology : some linguistic properties and an algorithm for identification in text  . Natural Language
Engineering , 1:9-27.
Andy Lauriston .  1996 . Automatic term recognition : performance of lin9uistic and statistical learning techniques . Ph . D . thesis , UMIST , Manchester,

D . G . Maynard and S . Anmfiadou . 1999a . hlentify-ing contextual information t brterm extraction  . In iTvc , of 5th International Congress on 7~rminol-212-221 , Innsbruck , Austria . 
D . G . Maynard and S . Anmfiadou . 1999b . A linguistic ~ I ) proach to context clustering . In Proc . of Nat-n~nl Language Proecss in flPacific\]~im Symposium  ( NLPRS )  , pages 346-351 , Beijing , China . 
S . J . Nelson , N . E . Olson , L . Fuller , M . S . Turtle , W . G . 
Cole , and D . D . Sherertz .  1995 . Identifying concepts in medical knowledge . In Proc . of 8th World Congress on Medical Informatics ( MEDINFO )  , 1) ~ ges33 36 . 
NLM , 1997 . UMLSK ? wwlcdgcS our ccs . National Library of Medicine , U . S . Dept . of Health and Human Services , 8th edition , January . 
P . Resnik .  1995 . Disambiguating noun groupings with respect o WordNet senses  . In Proc . of 3rd
Workshop on Very Large Corpora . MIT.
A . Smeaton and I . Quigley .  1996 . Experiments on using semantic distances between words in image caption retrieval  . In Proc . of 19 t . hhtter nationa IC on ferc'nc con Research and Development i ~  . I'n-formation Retrieval , Zurich , Switzerland . 
Akira Ushioda .  1996 . I Iierarchical clustering of words . In Proc . of 16th I'nt crnational ConfcT ~ cnccon Computational Linguistics  ( COLING )  , pages 11591162 . 

