A Hybrid Japanese Parser with Hand-crafted Grammar and 
Statistics
Hiroshi Kanayama1, Kentaro Torisawa1:*,
Yutaka Mitsuishi * a , nd Jun'ichi Tsujiit *
\[ Tokyo Research Lal ) oratory , IBM Jal ) an , Ltd.
1623-1d Shimo-tsuruma , Yamato-shi , Kanagawa 2428 502 , Jal ) an
:!: Del ) artment of Inlbrnmtion Science , Graduate School of Science , University of 2bkyo
7-a-1Hongo , Bunkyoku , Tokyo 113-0033, .\] al ) an
? hfformation and Human Behavior , PII . I~3S'\] . 'O , Jal ) an Scie , tce and Technology Corporation
Kawaguehilion - the 4-1-8 , Kawaguchi-shi , Saitmnaa 32-0012 , Japan
, CCL , UMIST , U.K.
kanayama , torisawa , mitsuisi , tsujii@is , s . u-tokyo , ac . jp

Thisl ) al ) er ( leseril ) es a hybrid t ) arsing method for Jal ) an ese which uses both a handcrafted grammar and a statistical  1  ; e(:hniqlte . The keyfeat m'e of our syst ( nn is that in order to estimate likelihood for a parse tree  , the systeln Ilses information taken from Mternative  1  ) artial parse trees generated by the grammar . This utilization of alternative trees enables us to construct a new statistical model called ' l 'it  ) let/Quadrul ) let Model . We show that this model can capture a certain tendency in  . \] a palm seyntactic structures and this point COlltril  ) lltest oil nl ) rove tllell to f l ) al'sill ~ acctlracy oil a shallow level . We rel ) or t that , with an under-Sl ) ecified HPSG-1 ) ased grammar and ~ t maximum entropy estilnation , our parser achieved high a (: curacy : 88 . 6% accuracy in del ) endency analysis of the EDR annotated eorlms , and that it , out I ) erformed oth ( u ' lm rely statistical l ) arsing methods on the same corpus . This result suggests that 1 ) rol ) ertreatlnent of handcrafted gra , mnml'Sca , n contribute to l ) arsiltg accuracy on a shallow level . 
1 Introduction
There have been many attempts to combine handcrafted high-level gramnmrs  , such as FB-UfAG , HPSG and LFG , and statistical disambiguation techniques tool ) tain precise linguistic struc , tures ( Schabes , 1992; Almey , 1996; Carrollel , al . , 1998) . 
One evident advantage of this a pl ) roaehover lmrely statistical parsing techniques is that grammars can provide precises mnantie representations  . However , considering that remarkable parsing accuracy in a shallow level has been achieved by purely statistical techniques  ( e . g . Ratnal ) arkhi (1997)) , it may be thought more reasonable to use high -level gramnmrs just tin '  1  ) osti ) rocessing which nmps results of shallow syntactical analyses onto dee  1  ) analyses . 
'l . ' his work was conducted while the first~mth or was a graduate student at Univ  . of Tokyo . 
Figure 1: A tree M with a nonhead aughter NH and a mad aughter H  . 
In this work we prol ) ose that hand-crafl , ed high-level grammars ( : all be useful in shallow-level analyses and statistical models  . In our fl'a mework , grammars are used to obtain precise features for probability estimation  , which are difficult to obtain without a grannnar , and we show that such features contribute to high parsing accuracy on a shallow level  . 
In this lmper , the most preferable parse trees are chosen with a statistical model  . In our method , the likelihood value L ( M ) of a ( partial ) tree M in Figure 1 is detined as in ( 1 ) : L ( M ) dor L ( NH ) xL ( H ) xP ( ' n ~ h )   ( 1 ) where NH is M's nonhead daughter ( whose lexical head is n )  , H is the head daughter ( whose lexical headish ) , and / ) ( n -~ h ) is the probability of nt ) eing related to h . For a . single lexical iten iW,L(W ) is defined as 1 . 0 . 
In most models already proposed , the probability P ( n~h ) is calculated with the conditional probability ( 2 ) : -  , h)d?dP ( TI?' , , ,  % ,   ( 2 ) where T indicates that the dependency is true ; (1) ~ and q ~ h are attributes of ' n and h , respectively . And An , h , the distance between the two words , is widely used , because this attribute is believed to strongly affect whether those two words are going to be relate & In contrast  , in the statistical model proposed in this paper , P ( n-~It ) depends not only on the at-trilm tes of the tree M  , but also on alternative trees
MiMz
M1nh , 1""" ? zhi ~, h,l
Figure 2: Pro'tiM trees whose nonhead aughter's lexical head is n  . 
Mlr
Figure 3: ~ h ' anst brmation fl'om a tree to a dependency . 
l ' and r ' denote the bunsets , ~sl and r belong to , respectively . 
in the parse forest generated by the grmmnar . More precisely , when P(n--+h ) is calculated , we consider partial trees whose nonhead aughter's lexical head is n  , as displayed in Figure 2 . Here alternative possible hk(k = 1,- .   .   , l ) are taken into consideration , and ordered according to their distance to n . We call such set of hk modification candidates , and all modification candidates are placed together in the conditional part of the probability as in  ( 3 )  . Now assume h = hi . 
P ( iI % , , %2 ,   ,  % ,   ,   ,  % , )  ( 3 ) where " i " indicates the ith candidate m nong the modification candidates  . Equation ( 3 ) shows two important properties of our model . One point lies in the new distance metric .   ( 3 ) is the probability that n chooses the ith candidate as the modifiee among the modification candidates which are ordered according to their distance to n  . Thus , we no longer require the distance metric A ~ , h , instead we use the relative position among the modification candidates  , which works as an attribute of the modification . The other point is the use of the attributes of the alternative parse trees  , that is , attributes of the modifier and all its modification candidates are considered simultaneously  . We show that these technique sophisticate our model  , by providing linguistic examples in Section 3 . 2 . 
In practice , however , treating all candidates in otfeasible because of data sparseness  . We therefore apply a strategy of restricting the modification candidates to at most three  . The strategy and its justification are discussed in Section  3  . 1 . 
Applying the strategy to the equation (3) , we obtain equations (4) and (5):
P ( Ithi ) de = fP ( iI(i = 1 , 2) (4) hi ) der P ( iI % ,   ,  %= ,  % , )( i =* ,  2 , t ) (5) When there are only two candidates , equation (4) is used ; otherwise , equation (5) is used . Our statistical model is called the ~ Hplet /Quadrut  ) let Modal , which was named after then mnbcr of constituents in the conditional parts of the equations  . 
We report that our parsing framework achieved high accuracy  ( 88 . 6% ) in dependency analysis of Japanese with a combination of an underspecified HPSG-based Japanese grammar  , SLUNG ( Mitsu-ishi et al ,  1998 ) and the maximum entropy method ( Berger et al ,  1996) . Moreover , the resulting parse trees generated by our hybrid parser are legitimate trees in terms of given handcrafted grammars  , and we are expecting that we can enjoy advantages provided by high-level gramnmr formalisms  , such as construction of semantic structures . 
In the above explanation , we used the notion of lexical heads for the estimation of probabilities of trees for the sake of simplicity  . But , in the present implementation , we use bunscts , Ls instead of lexical heads , and a relation on a tree is converted to a bunsetsu-dependency as shown in Figure  3  . Abun-sctsu is a basic syntactic unit in Japanese  . It consists of a content word and some flmctional morphemes such as a particle  . 
In Section 2 , we describe some existing statistical parsers , and the Japanese grannnar which we adopted . Section 3 describes our statistical method and its adwm tages in detail  . We report ext ) erimental results in Section 4 . 
2 Background
In this section , we describe several models for Japanese dependency analysis and works on statistical approaches with gramlnars  . Next , we introduce SLUNG , the HPSG-based Japanese grammar which is used in our hybrid parser  . 
2 . 1 P rev ious Dependency Analysis Mode ls of Japanese Several statistical models for Japanese dependency analysis which do not utilize all and -crafted granl-mar have been proposed  . We evaluate the accuracy of bunsetsu-dependencies as they do  , thus here we introduce then l for comparison . All models introduced below are based on the likelihood value of the dependency between two bunsetsus  . But they differ from each other in the attributes or outputs which are considered when a likelihood value is calculated  . 
There are some models which calculate the likelihood values of a dependency between bunsetsui and j as in  ( 6 )  , such as a decision tree model ( Haruno et al . , 1998), a maximum entropy model ( Uchimoto et al . , 1999) , a model based on distance and lexical information  ( Pujio and Matsumoto ,  1998) . Attributes ( I ) i and ~ I , j consist of a part-of-speech ( POS ) , a lexical item , presence of a comma , and so on . AndAi , jj . 
p(i-~j)d,j ~' Crl,I,i , %, a ~, j ) ((0
However , these l nodels Niltor eftect contextual information because attributes of the surrounding bunsets  , t ts are not considered . 
Uchimoto et al ( 2000 ) proposed a model using posterior context ;  . The model utilizes not only attributes about bunscts ~ si  , j but also attributes about all bunsets ~> ( including j ) wl ficht bllow bunsetsui . That is , instead of learning two output values " T ( true ) " or ': F ( false ) " for the del ) endency between two bunsets ~ zs , three output values are used * brleanfing : the b~m  . setsuiis " bynd ( dependent on a bunsct subey on dj ) "  , " d pnd ( del ) endent on the b~t sets ~ t3 ) " or " btwn ( dependent on ab ' unscts~t between i and j ) " . The 1 ) robability is calculated by multiplying probabilities for all bunscts  , ~ ls which tbl-low b~trtscts ui as in (7) . ' l ' hey report that this kind of contextual information improves accuracy  . However , the model has to assume , the independency of all the random variables , which may cause some er-rors . 
P ( i --, j ) " ? ZH ~'( by . dI ?' ~ ,  % ,  & , k ) i < k < jxP ( d  pndI (1) i , i l ) 5 , A i d ) x H l ' ( b t w , , \[ ( I , i , q ? k , A < k ) (7 ) k > j The difference between our model and these previous models are discussed in Section  3  . 
2 . 2 Stat is t ica l Approaches w i th a grmnnmr There have been nlally l  ) rOl ) os alst br statistical t'rame works particularly designed tbr  1  ) arsers with handcrafted grmn mars ( Schal ) es , 1992; Briscoe and Carroll , 1993; Abney , 1996; Inui et al ,  1!)97) . The main issue in tintiffs type of research is how to assign likelihoods to a single linguistic structure generated by a gramlnar  . Some of tlmm ( Briscoe and Carroll , 1!)93; hmi et al , 1997) treat information on contexts , but the contextual intbrmation is de . rived only fl'om a structure to wl fich the parser is trying to assign a likelihood value  . Then , timma jor difference be . -tween their method and ours is that we consider the attributes of alternative linguistic structures generated by the grammar in order to detern fine the likelihood for linguistic structures  . 
2.3 SLUNG : Japanese Grammar
The Japanese grammar which we adopted , SLUNG ( Mitsuishi et al ,  1998) , is an HPSG-based underspecified grammar . It consists of 8 rule schemata ,   48 lexical templates for POSs and 105 lexical entries for functional words . As can be seen fl ' om these figures , the granmmr does not contain detailed lexk : al information that needs intensive labor for development  . However , it is precise in the sense that it acl fieves 83 . 7% dependency accuracy with a silnple heuristics 2 for the El ) I almotated corl ) us , and it can produce at least one parse tree for 98 . 4% sentences in the EDR annotated corpus . We use the grammar for generating parse tree forests  , and our'l ~' iplet / Quadruplet Model is used tbr picking Ul  ) a single treefl'oma forest . 
3 The Hybrid Parsing Method
This section describest improcedure of parsing with the ~ l " riplet / Quadrul  ) let Model . Our hybrid 1 ) arsing method proceeds as t bllows : ? A t ; the beginning , dependency structures are obtained from trees generated by SLUNG  . For each bunscts u , modification candidates are enumerated , and if there are four or more candidates , tlmy are restricted to three . The lmuristic used in this process is described in Section  3  . 1 . 
? Then , with the ~' il ) let / Quadruplef ; Mode . l and max in n n n entropy estimation , prol ) abilities of the del ) endencies are calculated . Secti(m3 . 2 discusses the characteristics and advantages of the model  . 
? Finally , the most preferable trees for the whole sentence are selected  . 
3 . 1 Rest r i c t ion o f Modi f i ca t ion Cand idates Kanayama et al  ( 1999 ) report that when modification candidates are emnnerated according to SLUNG  ,  98 . 6% of the correct modifie . es are in one of the following three 1 ) ositions among the candidates : 1 ; 11( ; nearest one from the modifier , the second nearest one , and the . farthest one . 
As a consequence , we can siml ) lil\[yI ; 11( ; problem by considering only these three candidates and discarding timother candidates  , with only 1 . 4% potential errors . We therefore assume that the . number of modification candidates ix always three or less  . 
This idea is s in filar to that of Sekine ( 2000 ) 's study , which restricts the candidates to five , i ) ut in his case , without a granmmr . 
3 . 2 The Tr ip le t / Quadrup le t Mode l The ' Diplel  , / Quadruplet Model calculates the likelihood of the dependency between bunsetsui and bunsct sucn  ; P ( i - - , cn ) with the formulas (8) and (9) , where c , ~ denotes the nth candidate among b , m-sctsui's candidates ; ( I , i denotes some attributes of i ; and ~ I ~? , ~ denotes attributes of c , ~( including attributes between i and cn ) . 
P ( i - ~ c , ddJP ( nI ?, ~, % . ,, %~) ( . n = 1 , 2) (8) P (~- ~ c , ~)  , ,~ r p ( , ~I ?' i ,  % , ,  , I , ~ ,   , I %) ( , n = 1 ,  2  ,  /  )   ( 9  )   2This heuristics is a Japanese version of a left -association rule : see  ( Mitsuishi et M . , 1998) for detail . 

As (8) and (9) suggest , the model considers attributes of the modifier bunsetsu and attributes of all modification candidate simultaneously in the conditional parts of the probabilities  . Moreover , what is calculated is not tile probability of " whether the dependency is correct  ( T , see Formula (6)) " , but the probability of " which of tile given candidates i chosen a stilen lodifiee  ( n = 1 ,  2 , or 1) " . These characteristics imply the f bllowing two advantages  . 
Advantage 1 A new distance metric . The correct modifiee can be chosen by considering relative position among grannnatically licensed candidates  , instead of the absolute distance between bunsets ~as  . 
Advantage 22)' eating alternative trees . The candidates are taken into consideration simultaneously  . But because then lodifica?ion candidates are restricted to at most three  , we considerably avoid data sparseness 1) rot ) lems . 
Below we discuss these advantages in order . These advantages clarify the differences fl'om previous models described in Section  2  . 1 , and are empMcally confirmed through the experiments in Section  4  . 
3 . 2  . 1 Advantage 1 : A new distance metric As discussed in Section  2  . 1 , the distance metric Ai , j used in previous statistical methods was obtained simply by counting intervening words or b ' unscts  , t ~ l ) etween i and j . On the other hand , we use the relative position among the modification candidates as the distance metric  . Tile following examples illustrate a difference between those two types of melric  . 
The correct modifiee of kare-gaishashir'u-no-wo in both  ( 10 a ) ~ u ~ d ( lOb )  . 
(10) a . kare-ga hashiru-no-womirako to he-SUBJmm see fact  ( the fact that I saw him run ) b . kare-ga yukkuri hashiru-no-womirako to he-SUB . slowly run see fact ( the fact that I saw him run slowly ) In previous models , (10a ) and (10b ) would yield , P . ( kare-o , ~--* t ~ ashir ' u-no-wo ) = P ( TI kar , ~- ga , h , ~shiru-no-~vo , A1)\])b(kare-ga--+hashi ,  . tt-?zo-wo ) =\])( ~ l'll~a~ve-ga , hashi , 'u- , zo-wo , A2) respectively , where A1 = 1 and A2 = 2 . Then , the two probabilities above do not have the same value in general  . 
Our grammar does not allow the dependency " kare- . qa--~yukkurYtbr(10b ) . The modification candidates of karc-ga are hashiru-no-wo and mita  , hence ( 8 ) gives the probabilities between k are-ga and hashiru-no-wo as follows  , in both examples . 
\] ~ ( kare-ga - ~ hashiru-no-wo ) = Pb(karc- , qa--~hashiru-no-wo ) = P ( llkare-ga , hashiru-no-wo , mita ) Thus , P ( kare-ga--+hashiru-no-wo ) has the same value for both examples . Our interl ) retation of this difl'er-enee is sumnlarized as follows  . The word yukk ' ur is an adverb modifying the verb h  , ash &' u . Our linguistic intuition tells us that the presence of such adverb should not affect the strength t br the dependency between k are-ga and hashiru-no-wo  . According to this intuition , the existence of the adverb should be considered as a noise  . Our model allows us to ignore such a noise in learning from annotated corpus  , while previous n lodels are atfected by such noisy elements  . 
3 . 2  . 2 Advantage 2 : Treating alternative trees or contextual information 
Consider the following examples.
(11) a . Ta~v-noka waii musume
NP Adj NP
Taro-POgS 1) retty daughter(~\[h . ro's pretty daughter ) b . Taro-no yuu jin-no musumc
NP NP NP
Taro-POSS friend-POSS daughter ( Taro's fl'i end's daughter ) Contrary to tim previous examl ) les , TaTv-noill ( 11 ) n to difies different n lodification candidates . In example (11a ) , " ~ hr'o-no--+musume " is the correct dependency while " Taro-no-~musume " is not correct in  ( 11 l ) ) . This difference is caused t ) y the b'u'a-setsu between Taro-no and musume , kawaii ( Adj ) in ( lla ) and y , u ~ lfin-no ( NP ) in ( llb ) . Actually , the grann nar allows Taro-no to depend on either of these types of words  . Thus , in our model ,  /' , (' late-no-- , musume ) l't , ( ~1aro-7~o-- . m ~* sume ) = P (21 Then , P ( varo-no-+musume ) has different values for the two examples , hitheannotated corpus , l ' (21 ~ laro-no , kawaii , musume ) tends to have a high value since kawaii is an adjective  . However , since yuu jin-no is an NP , P(2\[Taro-no , yuujin-no , musume ) tends to have a low value . 
Now consider previous models.
Pb(Taro- , ~ o--+m**s~mz ~) = P ( TIT in'o-no , musume , 2) Then , contrary to our model , P ( Taro-no--~musumc ) lms exactly the samew due for both examples . The outconle is determined by = P ( TI Taro-no , kawaii , 1) In text corpora , P ( TITaro-no , yu  ~ , jin-no , 1) tends to be high , and consequently , P ( TITaro-no , musume , 2) is very small . These values will make the correct prediction for  ( 111 ) ) as yuujin-no will be favored over musume . However , for (11a ) , these models are likely to incorrectly favor kawaii over musume  . This is likely to be snlaller than P ( T\]:late-no , t  ~ , ,waii ,  1) . 
4 Experiments and Discussion \] . ' his section reports a series of parsing experiments with our mode  , l and gives some discussion . 
4.1 Environlnents
We used the EDR , lal ) aneseCorl ) us(El ) R ,  1996 ) for training and evaluation of 1 ) arsing accuracy . The EI ) R Corpusixa , Japanese treebank which consists of 208 , 1 . 57 sentences from newspapers and magazines . We . used 192 , 778 sentences for training ,  (1 , 744 for pro-analysis ( as reported in Section 3 . 1), and 3, 372 t br testing 3 . 
With tril ) lets constitute ( \] of a modifice and two modification e and ida . te . sextract c(lti ' onl the learning corl ) uSl ; hc Triplet Model is (' . on structed . \ Vith the quadruplets constituted of a moditiee and three candidates  , the Quadruplet Model is constructed . 
' ? hese ~ i nodels arc estimated by the Choicc Maker Maxinmm Entropy Estimator  ( Borthwick ,  19!)9) . 
The features fin'the estimation are listed in Ta -/  ) le1 . The values partially folk ) wother researches e . g . Uchimotoel ; al .  (\]999) , and JUMAN's outputs are used for POS classification  . Mainly the head of the b'unsc . tsu ( the rightmost morl ) heln cinab ' unscts'u , except for whose major POS is " peculiar " , " auxiliary verb " , " particle " , " suffix " or " copula " ) and type of the b'ltnscts'u ( the rightmost morphenminab'wnsel . s'lt except br whose major P ( )S is " l ) eculiar " ) are used as th cat . tributes . \ ~; es how the meaning of some f (' atures below . 
POSJUMAN's min or \] ? ( ) S ( for both " head " and ': type " )  . 
particle , adverb Frequent words : 26 lmrticles and 69 adverbs . 
head lex2 . (14 lexical forms regardless of their POS . 
type lex 70 suffixes or auxiliary verbs.
inflection 6 types of inttcction : " normal " , " a ( lver-l ) ial " , " adnominal " , " tc-fornf ' , " ta-tbrm " , and " others " . 
The cohmm % aria ( ion " in Tal)h ;   1 denotes them nnbcr of possible values t br the feature  . " Valid features " indicates then mn be r of features which al  ) -peared three times or more in the training corlms . 
4.2 Results
Wil ; hour model and the features described above , the accuracy shown in Tal)lc2 is achieved . We oval-uate the following two tyl ) eS  of accuracy : 35 , 263 SOld , enccs were rOll loved 1 ) eCml Set he order of the words in the annotal ; ion ditl'ered front that in the original
SOllt Oll CeS.
\[ Ii \ , , re , it , . . . . . . . <, ~
Ill Peal , ure type Val'iation'l'rip . \] Q , md , IIIoadI'OS?~fnmdifier 24 , 12 6 , t2 Type P (   ) So fillodil'ier 340 ( l ! ) D3l ' artich , of l/led(tier 27 , t77:4 Adverb , fl ' in ( ~difior 701311035 Tylml ( X ( if iiltl difiol " 71   110   225 d Inflect i ( nl ( )" i . lodifi(n " ; 12187\ Vh ( ! thl!rlxl ( iditi ( !r has a CO lllllllt*24~-8 lIeadI'OS , ffltH~dil'ioo247 ( I 158   9 TypeI'OS of lnqMi ' ie e 34   96   231   10 lh , ad\]?~x ~ , 1 in i ) difieo2D 5 llG , I259711 lhtrt ; iclo\[d " I nodilie(~279220 . 1  12 ' l'ypoIoxoillodii/~o 71   210   454   13 \] nlh~l:t . it HI of In ( . difle (, 62, 15:11 . l  ~ . V hoth or I nodil ' ioc ~ has it GI ) Illl IIII 2   8   18   15 \ V hethorn lodifio e has % rod ' 2   8   18 IG ~ V hol , h ~ rinodl fioelilts " / o " 261717:\]/ ( if ( ~ ( )1\[111~ii ~ b ( !tlg ( ~ ( ! lll ; l' ; ? lIHtlt . utlt . uos , 1163618 #i ) f " ilia " l)tl ~ , ~v ( ! ( * lIt ~ V ( ) bltYl~t:~gllH 3   12   27   19   2 X 8   816   1187   2727   20   2 x 7 x 14   13G   38  ( 87 112 13X 107 90 56 , t(1513 , 10:222x 91156 1213311 ) 8 23 3X 117 296 1816 372 . 12X 11i 118 102 52 . 194'252 X12241 , t 1483   351   '1   26   2 x 3 x 7 x 8   132192   1331   3O58   27   1 X 2 X 6 X 8 X 13   7  (   ) 5024  ( i ( i051 , t 710 It'r''':''~I-I '224:':~ I """~" J Table 1: Usedf'eaturcs : l , k , ,atures from 8 to 27 are related to the nm(litiee , thus they are considered for each candidate , li ' eatures from 19 to 27 are combination fea-\[ ; IlI'CS . 
III-COVCI'a ~ O
S(~ . lltOlICCS\]htnset . su accuracy 88 . oo X , (23078/2 ( i062) Sentence accu , ' acy/16 . )0% (1560/3326) Alllhm . set . su accuracy 88 . 33% (2335()/26436) sentences , qentcncc ~ accuracy 46 . 35% (1563/3372) Tal)h ; 2: lcsulls of parsing with the Tril ) let/Quadrul ) let

Bunsetsu accuracy The percentage of bu . n , ~ cts ' us whose rood ( rice is correctly identified . The dc-nonfinator includes all b'unsets'us except for the last bun  , ~ cts ' u of a sentence . 
Sentence accuracy The percentage of sentences whose detmndencies art '  . perfectly correct . 
" h > coverage sentences " is the accuracy for the sentences flw which SLUNG could generate parse trees  . We give the accuracy for " All sentences " too , by 1 ) art ( all y1 ) arsing sentences which SLUNG fail to parse . The coverage of SLUNG is al ) out 99% , thus high accuracy is achieved even for " All sentences "  . 
Moreover , we conducted a series of experiments in order to evaluate the COld  ; ribution of each characteristic in our parsing model  . The parsing schemes used are the four in Figure 3 . Major differences among them are ( I ) whether a graln lnar is used ,   ( II ) whether modification candidates are restricted to three  , and ( III ) whether a previous pair model with Formula ( 6 ) or the ' lS ' iplet / Quadrulflet Model with Formula ( 8 )  , (9) was used . 
W/O Grammar Model This model does not use a grammar  . Likelihood values for d cpenden-4I 5
W/O Grammar
W/O Restriction
Pair ~ IMplet / Quadruplet
IGRP
P\]~Un8gts~L accuracy 86 . 70% (22594/26062) + - P 87 . 37% (22770/26062) ++ P 87 . 67% (22849/26062) + + T 88 . 55% ( 23078/26062 ) Table 3: Bunsetsu accuracies for four models . Cohmm " G " indicates whether the grmmn aris used  , " R " indicates whether the modification candidates are restricted to three  , and " F " denotes the formula ; " P " is the pair tbrmula (6) , and " T " is the % ' iplet / Quadruplet formula ( s )  ,  (9) . 
cies are calculated for all bunscts iLs that follow a modifier bunsctsu  . Formula (6) is used , and as a distance metric Ai , j , them nnl ) er of bun-scts ~ ls between the modifier and tile modifiee  4 are combined with all features . In general lines , this model corresponds to models such as ( Fu-lie and Matsumoto , 1998; Haruno et al ,  1998;
Uchimoto et al , 1999).
W/O Restriction Model Modification candidates are restricted by SLUNG  . Tim remaining is the same as the W/O Grannnar Model  . 
Pair Model Modification candidates are restricted to three  , in the way described in Section 3 . 1 . 
The remaining is the same as W/O Grannnar

Triplet/Quadruplet Model T iffs is the model proposed in the paper  . Modification candidates are restricted to tln ' ee , and Fornmla (8) or (9) are used . 
From the result shown in Table 3 , we can say our method contributes to the improvement of our parser  , because of the following reasons : ? The %' iplet /Quadruplet Model outperforms the Pair Model by  0  . 9% . Both of them restricts modification candidates to three  , l ) nttim accuracy got higher when all candidates are considered simultaneously  . It is because of the two adwmtages described in Section a  . 2 . 
? TILe Pair Model outperforms the W/O Restriction Model by  0  . 3% . Thus the restriction of modification candidates does slot reduce tile accuracy  . 
? TILeW/O Restriction Model outperform stile
W/O Grammar Model by 0 . 7% . This means that the use of a grammar as a preprocessor works well to pick up possible modifice  . 
We found that many structure similar to the ones described iLL Section  3  . 2 appeared in the EDR4 Threevahms : "1" , " from 2 to 5" , "6 or more " are distinguished . 
In-coverage \] 3 unsct . su accuracy 87 . 08% (8299/9530) sentences Sentence accuracy 44 . 70%  ( 493/\]103 ) Table 4: Accuracy tbrKyoto University Corpus corpus . Our Tl'iplet/Quadruplet model could treat these structures precisely as we intended  . T lf is is the main factor that contributed to the improvement of the overall parsing accuracy  . 
Based on tim above experiments , we can say that our approach to use the grammar as a preprocessor before the calculating of the probability is appropriate for the improvement of parsing accuracy  . 
4 . 3 Comparison to other models 4 . 3 . 1 Mode ls us ing the EDR corpus There are several works which use the EDR corpus for evaluation  . The decision tree model ( Haruno et al . , 1998) achieves around 85% , the integrated model of lexical/syntactic information  ( Slfirai et al , 1998) achieves around 86% , and the lexicalized statistical model ( Ft0io and Matsumoto , 1999) achieves 86 . 8% in bunsets'u accuracy . Our model outperforms all of them by 2 or 3% . 
4 . 3 . 2 Models using the Kyoto corpus Slfirai et al ( 1998 ) used the Kyoto University text corpus ( Kurohashi and Nagao , 1997) for evaluation and achieved around 86% . Ucl fimoto et al ( 2000 ) also used the Kyoto corlms , and their accuracy was 87 . 9% . For comparison , we applied our method to the same 1 , 246 sentences that Ucl fimoto et al (2000) used . The result is shown in Table 4 . 
Our result is worse than theirs . The reason is thought ol ) e as follows : ? g ~ reusetim EDR corpus for training  . Although we used around 24 times the amount of training data that Uchimoto et al used  , our training data lead to ca'ors in tile analysis of the Kyoto Corpus  , because of differences in tilemmotation schenms adopted  . 
? Uchimoto et al used the correct morphological analyses  , but we used JUMAN . Solnetimes this may cause errors . 
? The grammar SLUNG was designed fortile EDR corpus  , and some types of structures in the Kyoto Corpus are not allowed  . 
Clearly , our parser should be improved to overcome these problems and compared with other works directly  . 
4.4 Discussion and I~lture Work
TILe following are some observations about the speed of our parser  . Existing statistical parsers are quite etficient compared to grammar-based systems  . Particularly , our system used an HPSG-1) as edgrmmnar , vances in HPSG1) arsing ( ~ Ibrisawa et al ,  2000 ) enabled us to obtain a unique parse tree with our sys-gemin  0  . 5 sec . in average t br sentences in the EDR corpus . 
Future workshall extend SLUNG so that senmn tie represent at k ms are produced  . Carrollel ; al .  (1 . 998) discussed i ; he1) recisiol ~ of arguments i ; ruetures . V ~ T e1 ) elieve that the focus of ore ' study will shift ; from a shallow level to such a deeper level for ( ) Ill ' tinal aim , realization of intelligent natural anguage processing systems  . 
5 Conclusion \? e1) resenl ; edahyl ) rid1) arsing scheme l ; hat uses a handcrafted grammar and a statist . teal technique . 
As other hybrid pa . rsing ntethods , l ; hest . al ; isi ; ical technique is used for 1 licking u1 ) the most l ) re , ferable , lmrs (; ire (; fl'oml ; he parsefol "(; sI ; gent ' . rai ; e , dI ) yt ; h(~grammar . The difference fl ' om other works is that the precise contexi  ; ualinformation needed to estimate ; he likelihood of a parse ,  1 ; ree is obtained fl ' on tadternative 1 ) arse trees generated 1 ) 5' the grammar , and that such contextual information from alternative I  ; rees enables Its to e on sl ; ruel ; our new statistical model called the ' l ? iplet /Quadruplet model  . We have shown that these poinl ; s contributed to sul ) sl ; an-tialill l proven lenl ; of parsing a cell ra (: yill , lal ) an e~sedc-1) en(lency analysis , through a serie , so fext ) (~ riments using an IiPSG-based . lalmnese grammar SLUNC , and the , maxinmm entropy method . 

St ; even Abney .  1996 . Sl ; ochasti (: aH ; ribut(',-vahm grannnars . The Computation and \]\] anguage E-
Print Archive , October.
Adam L . Berger , Stephen A . Della Pietra , and Vincent . J . Della Pietra .  \]996 . Aitia xilnuln entropy approach to naturalanguag ( ' ~ processing . Compu-tatio'n , al Li'n . gui . stics , 22(1 . )::/9 71 . 
Andrew Borthwiek .  199 . () . Choiee maker maximmn entropy estimator . Choiee Maker'lbch . , Inc . Email borthwic ~ cs . nyu . edu for information . 
~\[ l'~d Briseoe and John Carroll .  1993 . Generalized 1 ) robabilistic LR parsing of natural anguage ( col 1 ) Ol ' ~ t ) with unifieation-I ) as edgram nmrs . Computational Linguistics , 19(1):25-50 . 
Jolm Carroll , Guido Minnen , and ~ lL'd Briscoe .  1998 , Can sube a tegorisation probabilities help a statistical parser ? In Proc  . of th , e5th , ACL/SIGDAT Workshop on Very Lawe Corpora , pages 118126 . 
EDR .  1996 . EDR ( Japan Electronic Dictionary Research Institute , Ltd . ) dictionary version 1 . 5 technical guide, . Second edition is awdlable via http://www . i ij net , or . jp/edr/E_TG . html . 
Masakazu Fujio and Yuji Matsumot ; o . 1998.
,htl)anes <' , < lel ) endeney structure analysis 1 ) ased on lexicalized statistics . In PTvc . of the 3rd Cm@r-ence on Empirical Methods in Natural Language 
Procc.ss in 9, pages 8896.
Masakazu FI0io and Ymtji Ma?sumoto .  1999 . Sta-tistieal syntactic analysis based on cooccurrence probability of words  . In P ' roc . of 5th workshop of Nat'u~nl Language Processing , pages 7178 . ( in
Jal ) altese).
Masahiko Haruno , Satoshi Shirai , and gosh if lmfi Ooyama .  1998 . Using decision trees to construct a . practical parser . In Prec . COLINGACL'98, pages 505-511 . 
Kentarohmi , Virach Sornlert bunw mich , Hozumi Tanaka , and Takenobu 9bkmmga .  1997 . Anewl > robal ) ilistic LR language l node lt ' ( 31' statistical parsing . ' l'echnical Ieport TR974)005, Dept . of Coml ) uter Science , Tokyo Institute of ' l behnology . 
lliroshi Kanayanm , Kentaro'l . brisawa , Yutaka Mit-suishi , and Jun'i (: hi Tsujii .  1999 . Statistical de-1) e , ndency analysis with an HPSG-1 ) ased Jal ) an ese grannnar . In P ' roc . 5th NLPRb ', pages 138-143 . 
Sadao I ( urohashiatt ( l Makoto Nagao .  1 . 997 . Kyoto University text corpus in ' oje cKIn Prec . of 3rd Ann'ual M(~cti ~ N of Nat , u , raILanguagei ) roccssi'ng , l ) ages 115118 . ( in Japanese) . 
Yutaka Mii ; suishi , Kentaro Torisawa , and Jun'ichi Tsujii .  1 . (/98 . HPSG-sts de undersl)e(:ified . Japanese grammar with wide coverage . In P'mc . COLING-
ACL'98, 1) ages 876880, Augusl;.
Adwaitllatnalmrkhi .  1997 . A linear obse , rvedtinl (; statistical lm . rserbased Oll maximum entropy models . In P'mc . th . cEmpirical Mt~thods in Nat'u-'ral\])a , 'n , guag(:\])' roce , ssi'n , 9 Co ~@ rence . 
Yves Sehabes .  \]992 . Stoclmsti (: lexicalize , d tree-adjohfing granmwms . In P'mc . 1 dth COLINO , pages d26 432 . 
S~toshi Sekim, .  2000 . Japanese dependency analysis using a detern finistic  , tinite state , transdue (; r . In Prec . COLING 2000 . ( this proceedings ) . 
Kiyoaki Shirai , Kentarohuff , Takenolm Tokunaga , and Hozumi Tanaka .  1998 . A framework of in l ; e-g rating , syntactic and lexical statistics in si ; atisti-cal1)arsing . .lo,urnal ofNat'ural Langua9c l)~vccss - int . \], 5(3) . ( in Japanese) . 
Kentaro ' Ibrisawa , Kenji Nishida , Yusuke Miyao , and Jun'ichi Tsujii .  2000 . An HPSG parser with CFG filtering . Jounal of Nat'mal Language E'n , gi-nccrin . q . ( toal ) pear ) . 
Kiyotaka Uchimoto , Satoshi Sekine , and Hitoshi1sa-hara .  19!19 . Japanese dependency structure analysis based on maximum entropy models  . In P ' roc . 
13th EACL , pages 196-203.
Kiyotaka Uchimoto , Masaki Mural ; a , Satoshi Sekine , and Ititoshi is a hara .  2000 . \]) el ) endeney model using posterior context . In Prec . of Sixth , in tcrna-lionel Workshop on Parsing 7' cch , no lo9ics . 

