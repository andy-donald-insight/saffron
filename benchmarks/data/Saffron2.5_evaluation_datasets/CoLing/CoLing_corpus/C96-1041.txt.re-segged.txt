Marker random field based
English Part-Of-Speech tagging system
Sung-Young Jung , Young C . Park , Key-Sun Choi and Young whan Kim *
Comput ; er Science \]) el ) a.rtment
Korea Advanced\[nstitul , e of Science and q % chnology
Taejon , Korea
* Multimedia Re.search Ibaboratories
Korea ~\ [> lecom
chopin , ycpark , kschoi(ci!csone.kaist.ac.kr

Probabilistic models have been widely used for natural language processing  . 
Part-of-speech tagging , which assigns the most likely tag to each word in a given sentence  , is one . of tire problems which can be solved by statistic M approach  . Many researchers haw ~ tried to solve the problem by hidden Marker model  ( HMM )  , which is wellknown as one of the statistical models  . But it has many difficulties : integrating heterogeneous information  , coping with data sparseness prohlem , and adapting to new environments . In this paper , we propose a Markov radom field ( MRF ) model based approach to the tagging problem . 
The MRF provides the base frame to combine various statistical information with maximum entropy  ( ME ) method . 
As Gibbs distribution can be used to describe a posteriori probability of tagging  , we use it in ma . ximuma posteriori ( MAP ) estimation of optimizing process . Besides , several tagging models are developed to show the effect of adding information  . Experimental results show that the performance of the tagger gets improved as we add more statistical information  , and that MtF-based tagging model is better than ttMM based tagging model in data sparseness problem  . 
1 Introduction
Part-of-speech tagging is to assign the correct tag to each word in the context of the sentence  . '\[' here are three main approaches in tagging problem : rule-based approach  ( Klein and Simmons 1% 3 ; Brodda 1982 ; Paulussen and Martin 1992 ; Brill et al 1990) , statistical approach ( Church : 1988 ; Merialdo 1994 ; Foster 1991 ; Weischedel et al 1993 ; Kupiec 1992 ) and connectionist approach ( Benello et al 1989 ; Nakanmra et al 1989) . In these approaches , tatistical approach as the following advantages : ? a theoretical framework is provided ? automatic learning facility is provided ? the probabilities provide a straightforward way to disambiguate Many information sources must be combined to solve tagging problem with statistical approach  . 
It is a significant assumption that tire correct tag can generally be chosen from I  . he local context . 
Not only local sequences of words and tags are needed to solve tagging problem  , but syntax , semantic , and morphological level information is also required in general  . Usually information sources such as t ) igram , trigram and migra . m are used in the tagging systems which are based on statistical method  . Traditionally , linear interpolation an ( tits variants have been used to combine the information sources  , ) tit these are shown to be seriously deficient . 
ME ( Maximum Entropy ) estimation method provides the facility to combine several information sources  . Each in R ) rmation source gives rise to a set of constraints , to be imposed on the con > bined estimate . The function with the highest entropy within the constraints is the ME solution  . 
Given consistent statistical evidence , a unique ME solution is guaranteed to exist and an iteratiw ~" algorithm is provided  . 
MRF ( Marker random field ) model is based on ME method and it ; has the facility to combine many inlbrmation sources through feature flmc-tions  . MRF model has the following adwmtages : robustness  , adaptability , parallelism and the facility of combining informatiort sources  . MRF-based tagging model inherits these advantages  . 
In this paper , we will present one of the statistical models , namely MRF-based tagging systern . 
We will show that several information sources including unigram  , bigram and trigram , can be combined in MRF-based tagging model . Experimental results show that the MRF-based tagger has very good performance specially when training data size is small  . 
Section 2 describes the tagging problem , Section " l describe statistical model already known calin \[' or utal  . ion . Sect . i on 5 provides MlF-I ) ased tagging model at td secl . ion 9 sho , . w's the expcri-t : twnl ; alrcsult . s . Sc . ct . iou 10 (: Opal ) arcs M\[I : wilh I\]MM . Fit(ally we conclude in scct , iott l . 
2 The Problem of Taggil + g
When scnt . ct ~ c(!i,'V = wt,w2, . , . , u , , , ~ is given , t . lwrc exist ( ' or resl > onding ( . ags 7' =/ i,*2, .   .   . , t , of the same hmgl . h . W>calllhepair(W,T ) an align-lltOtll; . ~' Vo ::-; a . y that . wor(liv:hasl ) ('(' tt assigned l . wl . agtiillt . hisaliguut cnl . . V'v'o suppose l . hat , asel . 
, :) fI , ags is giv ~' n . ' l ' aggitt g is assign iltg ('() r r ('('1 . lag:s(!quetlCO"1'--It,t2, . . . , tnI ' or given words c : q t l cn ce
H/~-lt ; t,11:2, ..., IU++.
3 Probabilistic
Formulation ( IIMM )
I :% usassunmt , we wanti . oI , : t to v , , the n~ , : :) st;likc'lytagseqtt < mcc~b(PV ) , given a parlict tlar wot:dSe(lUtm ccI'V . Timl . agging prol)h-nt is do lim'd as liuding t . l wtt to sl , likely t . ?tgs("tltlClt('(!"1'q~(w ) :- . ,': z , t  ~ , tx/'('/'llI , :)( t ) z'(W l ' , ")/'('/') =  . . , ' qmax(2)" , ' P(w ) = + , rffnF3xI'(VVI'l')l'(73 (: It/wh . creP (7') is 1, heap riorii wo hal ) i lit . y of a tags c quom : o ~' r , t , ( WI'I')ist , h . ccot ~ dil . ioual I , ' olmhilil , y of words c,,tu(m(-c . H / , given I . It , : ~ . '-; cqucucoo1"tags 7' , at , d /'( W ) isl , hotm cotMitiotle di ) roba . I ) i lit , yot:word scqmmccW . ' l'hct > rol > abilit . yI ' ( W ) in ( 2 ) isrc'tnovc'dI ) ccause it lt~ts no ell'cot , on 0(W ) . (: on-scquent . ly , it . issuilic icnti . of ind the tagSC ( ltmnce7'which sal . isiies (3) . 
Wc can rcw vit , (> the prolmld lily of 0 a chsc qttcnc('asa . prodltct , of 1, he cot Mil . iona . lprot > altilit . i < ~ s of each word or faggiven all of the lm ' viottst  , ags . 
t,(wv /') v(~r ),, :'( . , , , : It+, . . . . ~,, ,,,_ ~,  . . . , , , t ) = lli = l ' ( li\[li_I, .   .   . ,/1) Typically , otton m k cst . wo silllplifying as smttp ( . ionsI . o(:llt . dowuo , l , lt <: n t tnd > er of F , rc , l > al: , ilil . i e s 1 . , . ) Ive <+ st . inml . ed . I , ' irst . , rat . her I . h ; tn ; ts suttling lt ' id clwnds on all IH'c viotmwords and all i  ) rc viottsta . gs , on o ; - t s s t t t n e s w + d , :' F , cn , : Is(: , nly , : mli . Sc , : : oml , rath ( , ri : hauaSS ( lining the tag tidel tends on the t ' ulls ( > quc:nccofl ) rc viousl , ags , w (' can assume l . hal . 
local COltl ; oxl , is sullicicnt . This locality assume disrcfer cdt , oas ; tMm'kovimlcl ) endencer lSStllll ) liioIl . 
Using 1 . lwseass(trutH,ion,w(>al ) lwoxhtml,cl,touqua . l . ionl , ol , ttcR ) llowing z'(WlV')~II'A~z'( . ,~le , ) (4) / , (~/') ~ tl % , z , ( , ~lz+_t ) (: , ) Accordingly , O(i'V ) is(h'rivedI)3 , applying ( , I ) ,   . td ( , ~) to (: ~) . 
, / , ( w): .   .   . , , + p + . : tI'L , ;'(, . +lz ; ) z'(z , : lt,_ . )(( i ) We can geleach l ) robabilit , yva . htefront the t . aggcd cor Itus which isi , rq+arcdforl . raining by usiu:4 (7) aml (8) . 
z'( . ,zlt , ) - ( : (+< , t , )(T ) cT(t ~)/'(/ , :1t~_ , ) c'(z ~) ( st where (7( t :) , C(ti , Ig ) is t it ("\[' reqt tency obt . a in cd fronll rain hlg dal . a . 
\:it ; or l > i algorilhnt ( l:orncy 73 ) is the one goner-ally used to liw . lt . het , a . gSO < luencc which safislies (6) aim I . tt is algoril . hntgttaranl . ccs the opl , it ttalso-hition to I , he I ) r(+b hmt . 
This model has several prot > l(+t ns . First , , so(no wot'ds or \[ , ag~s < Xltl(~ll(W' , '-; 1/13 . yitot , O(HHIt ' illl . ra . ili-htgdal . a or Hlay occur with very low \[ reqttetlcy ; ii('vt'rlh('lcs , % t , llc words or lagsoqtt(~ltC(~sc ; / t t ; +\])- l ) earillt . agging l > r occs s , lit this case , it , usually causes V (! l'y bad result , t . oCOlll ) ttt . c(6) , because the l wol ) al ) ility has zerow due or very low value . 
' l'h is problontisc . alh ' d dataslmr,sc . cssi > rol)h+tn . 
To avoid thi+ql ~ roldet n , sm , ) ot , hing of itd ' or tttat . i , : mtlJtts/,I :, cttscd . Sntool , hingproc('ssisahnost('s-scntia , \] intl MMtm catts eIIMM hassevet'cd:at , ; tsparseness prol>hmt . 
4 combining information sources 4 . 1 linear in l ; (' , \] rlmlatiou Various Mttdso\['in lol'ntal , ionsf : . ttr(;(~s and differ-out knowledge sources must , he Colnl ) in cdl . OS ( ) IV (" thel , a . gging prol > l(+m . The gemwal method used it ( Il MM is lineariul , erl ~ olatiot L which is l , hew ( gght , edsutnmal , ion of all prol ) al filil : y in for n lat , ion , % o11 rc(:s . 

P . . . . . . .  ,, . , , , , ( , ,'7 , )-: ~ A ~/' ~ I , , , I / , )(!))/=1wlt~t'c0 < AiNI mid3:Ai=1 . 
This hie ( hodcnnI ) eused I ) ol , has a way of con > Itining M to wh : dg ( " sources and snloot , hingin for nm-t , iotIs out ' . ::cs . 
I1MM based l . agging modd times unigram , t>i-gl'atlla , lldt . rigt'alttin\[ortn ; d . iott . These in for ( mr ( ion sources are linearly cot nl > ined by weighl . cdSlllliliift-l . ion . 

P ( zgIt ~ - ~ , tg_2) = A1P ( tiIti_l , li-2) + A2P ( tiIti-1) ( lo ) where A1 + A2 = 1 . Ti reparameterll and A2 can be estimated by forwardbackward algorithm ( Deroua 86 + )   ( Charniak 93+ ) (tIUANG90+ )  . 
Linear interpolation is so advantageous because it reconciles the different information sources in a straightforward and simple-minded way  . But such simpliticy is also the source of its weaknesses : ? Linearly interpolated information is generally inconsistent with their information sources because information sources are heterogeneous for each other in general  . 
? Linear interpolation does not make optimal combination of information sources  . 
? Linear interpolation has overestimation problem because it adjusts the model on the training data only and has no policy for untrained data  . This problem occur seriously when the size of the training data is not large enough  . 
4.2 ME ( maxlmum entropy ) principle
There is very powerful estimation method which combines information sonrces objectively  . 
ME ( maximum entropy ) principle ( , laynes 57 ) provides the method to combine information sources consistently and the ability to overcome overestimation problem by maximizing entropy of the domain with which the training data do not provide information  . 
Let us describe ME principle briefly . For given x , the quantity x is capable of assuming the discrete w duesxi  , ( i = 1 ,  2 ,   . . . , n ) . We are not given the corresponding probabilities pi  ; all we knowistire expectation value of the function f  ,  . ( x ), ( r = 1, 2, . . . , m ):; qE\[fr(x)\]=Epi(xi)f , .   ( xi )   ( 1 l ) i = 1 On the basis of this information , how can we determine the probability value of the function pi  ( x ) ? At first glance , the problem seems in sol-uble because the given information is insufficient to determine the probabilities pi  ( x )  . 
We call the function f , . ( xi ) a constraint function or fealure . Given consistent constraints , a unique ME solut on is guaranteed to exist and to be of the form : where the Ar's are some nn known constants to be found  . This formula is derived by maximizing the entropy of the probability distribution Pi as satisfying all the constraint given  , qb search thel ,  . ' s that make pi(x ) satisfy all tile constraints , an external observation :
OOOW\[_2Wi1WiWi+lWi+2o oo
MRF : . co ~ ~ eee < L . V <? L ~/',, IL/<dJ < L . v Figure 1: MRFT is defined for the neighborhood system with distance  2 iterative algorithm , " Generalized Iterative Scaling " ( GIS) , exists , which is guaranteed to converge to the solution ( l ) arroch72+ )  . 
(12) is similar to Gibbs distribution , which is the primary probability distribution of M\[F model  . MRF model uses ME principle in combining information sources and parameter estimation  . 
We will describe MRFF model and its parameter estimation method later  . 
5 MRF-based tagging model 5 . 1 MRF in tagging Neighborhood of given random variable is defined by the set of random variables that directly atfect the given random variable  . Let N ( i ) denote a set of random variables which are neighbors of ith random variable  . Let's define the neighborhood system with distance L in tagging fbr words W = wl  ,   . . . , w,~, and tags T = h, . . . , t , ~ . 
N(i ) = i-L , .   .   . , i-1, i + l, .   .   .   , i + L(13) This neighborhood system has on ( ; dimensional relation and describes the one dimenstional structure of sentence  . Fig .   1 showes MP~FT which is defined for the neighborhood system with distance  2  . The arrows represent that the random variable ti is affected by the neighbor sti-  2  , ti-1 , ti+t , ~ j + ' ~ . 
It also showes that ti , ti-t and ti , ti + l have the neighborhood relation connected by bigram  , and that ti , ti-l , ti-2 and ti , ti+l , t i+2 have ttm neighborhood relation connected by trigram  . 
A clique is defined as the set of random variables that all of the pairs of random variables are neighborhood in it  . Let's define the clique as the tag sequence with size L in tagging problem  . 
G = ti-L , ti-(t,-1), . . . , ti (14)
A clique concept is used to define clique fimction that evaluates current state of random variables inclique  . 
The definition of MRF is presented as following.
Definil ~ ion of MIF : Random ' variable T is Markov random field if T'salisfies the following two properties  . 

Positivity : t ) (' F ) > O,W'(15)
Locality:
S'(t~I% , Vj , j ? i ) = P ( t ~ I % , Vj , j ~ iV(j )) We assume tha . te very prob ; d > lity value el tagse-(lue nee is larger l , hmlzerobee ; rose ungraluiu at , ic ; dSellt , ellCeS (; fill , tl ) pem " in htlllHilll~tligll&geliS~ge , including meaningless sequence of characters . St ) the positivity of ' MRt !' is satislied . This :+ tSStllnp-tion results in the robustness midada  . ptability of the i node l , evel i though unti:a ~ ined event so colir . 
The locality of MRF is consistent with the as-Sll iliptioiiO\[I  ; a . g giilg t ) roble in in that the tag of given word ca , it be deterinined by the local context . ( Tonsequenl , ly , the random variable 7' is MRF for neighborhood systen lN ( i ) its 7' satis-ties the positivity and the locality . 
5 . 2 A Poster io r i Prot iat i i l l ty A posteriori probat  ) ility is needed to sea . rcb for the Jr lost , likely tag sequence . MII , F provides the i ; heoretical bi~cliground about the probal ) ility of the system ( Bes~tg 74 )   ( ( leiJfla , ii 84+) . 
H~mniersley- ( \] lifl brd thcore in: 7'he probability dish'ib'ulio'nI' ( 7' ) is ( Tibbs dish'ibulion if and only if ' random wzriable  7' is , , llarkov random field for givcn ncigborhood syslc '/n N  ( i )   , e . ", '~, , uCr)
P (7') - Z (17)
Z = Ee-~"~'~l(m ) (:18) where "\[' HI is l ; elill Jel'i ~ tllre ~ is nor lnalizing COIl-Sl ; ~ tllt , called partition ftlll CLioi la Ald U ( '\[' ) iSet lergy fimct ; ion . The a priori probal ) i lit ; yP ( 7' ) of tag sequence 7 ~ is Gibbs distribution because the ran-dora variable  7' of tagging is MRF . 
It can be proved that a posteriori probability i ) (TiW ) for given word sequen (  ; eW is also Gil ) bs distribution ( Chun 93) . (7 on sequent/y , aI ) osteriori probability of 7' forgiwm Wist , u ( . rlw ) 1'(' VlW ) = --/~-~( i , qZWellSe(9)i , O (' . m'ryOlltMAPestiui ; dion in the tagging model . The energy function U(TJW ) is of this form . 
u@'lW ) = ~ w~(; t'lW ) (20) c where V , , is clique function wii ; h the property that Vc depends only oil those r and oui variable  , inclique e . This lllelLllSt ; hatellergy funcl , ioli ( Urill be obliained \[ rOll leach clique funtion which split sl  , \[ ie set of ralld Olll viu ' iables to slibs cLs . 
6 Clique function design
The more state of random variables are near to Lliesolution  , theniore the system becomes table , and energy function has lower vahie . Energyflmc-i , ionrepre , sents the degree of unstability of currents tntc of raiid on ivl  . triables in MRF . It is similar to the I ) e haviouro\['molecular particles in ther cM world  . 
('~lique function is proportional to energy fun ( :- tion , and it represents the unstability of current state of randonivaria  . bles in clique or it has high value when the state of MRF is bad  , low value when the st ; ~te of MI Fisnero : to solution . Clique fimction contributes to reduce the comi ) utation of evahmtion function of entire MRF by clique concept that separates random v ~ triables to the subsets  . 
(llique function V/(TJW ) is described by the . few . 
tures that represent the constraint or information sources of givcuprol  ) h ; m domain . 
~5 (: z ' lvv ) := ~ a , . j ; ~.(' clm ) ('2J )
F 6.1 MRF Model 1 ( Basic model)
The basic information sources which arc used m statistical tagging model are unigram  , l ) i graninnd trigrain . MII " n lodelIlises unigrmn , higranri and t , rigral n . We write the \[ ea ? ure furiction o\[unigraln ; iSj\~ , , . :,, .   . . . . = ( t - ~'( t~I , < ) )  ( ~ ) and the feature f ' illtCtiollO\[II-grall l , in chiding bigram , trigram ~ sfli ,  - : ,  ?  . . . . . = where ( t-Z' ( t ~ IJ ) )  ( m3 ) ioN ( i ) t ' ( tilti_j , t~_j+x ,  . . . ,t~_t ) , iI'i>jP ( tiIJ ) = t " (6: Iti+l , h + ~ ,   . . . , zi+j ) , it " i < j The clique filnction of the model 1 is ttt ~ de as follows . 
/01 ' lw ) -- A , ?/; , ,,~, . , , , ~+ x ~ . f ,,-:,, . , m(~4) 6 . 2 Model 2 ( Morphological inforntation in dnded ) Morphok ) gical evel information helps tagger to determine the tag of the word  , more . especially of the unknown word . The suffix of a word gives very useful information about the tag of the word in F  , nglish . The ( : li ( ltte function of model 2 is de-lined as f . ~,\]: i ~,= (\[- t'(gi\] . suffix ( wi ) ) )   ( 25 ) We used the statistical distribution of the sixty sll\[lixes thzttareIlK  ) st frequently used ill English . 

We can expand the clique flnlction of the model 1 easily by just adding Stlficix in for ui~-ttionto the clique function of then to del  2  . 
'~,~ . (7' IW ) = A ~ J ; , , , o , ' . . . . . + A e . f,+_<, . a,,,+Aa . f . ~ff~ . 
(2 5) 6 . 3 Model 3 ( error correction ) There exist error prone words in every ta . gging sys-tern . We adjust error prone words 1) y collecting the . error results and adding more inforniation of the words  . The feature function of Model 3 is for adjusting errors in word level . 
= (1-(2r)f#vo , . 2=(;1-P ( lil'wi_2 , ti_l ) )  ( 28 ) Y Veused the probat ) ility distribution of five hun-tired error proiie words ill Model  2 in oMer to reduce the tlt lllber 0t  '  paF31ileters  . 
7 Optimization
The process of selecting the best tag sequence is called ms optimization process  . We use MAP ( Maximum Al ) osteriori ) estiniation method . The tag sequence 7' is selected to niaximize the a posteriori probM ) ility of tagging ( 19 ) by MAP . 
Simulated annealing is used to searcti the optimal tag sequence as Gibbs distribution provides simulated anne Ming facility with teliiperatur  ( + arideile Fgy ( ' Oll Cept . go change the tag candidate of one word selected to tninilnize the energyt " i in c tionink th step froniT  ( k ) to j , ( k + i ) , a . n(ll'(+'t)e ; /t this process until there is tlO change . Thet ( ?llll ) (?l?a-ture 7'm is started in high vahle and lower to zero as tile above process is doing  . Then the final tagseqtle llce is the solution . Sininlat , ed annealing is US0 flil in the prol ) leni which has very hugo search sl ) ace , and it is the approxiniation of MAPest . i-fllatioll((\]elll&iq84-t--) . 
There is another algorithm called Viterbi algo -rithtn to lindol  ) timal solution . Viterbi algoritll mguarantees optinial so hltion \] tilt  , it canilot bcused in the problel n which has very huge search space  . 
SOitiS/iscd in thel ) rol ) leni which hasSlliallsea , rch space 3 , 11(1 Ilsedill I1 MM . MRF model Callil Seboth Viterbi algoril , hni and siinulated a nealing , but it is not \] ( nowtl IOI lSes in itllated all ne , aling illf IMM . 
8 parameter estimation
The weighting parameter A in tile clique \[' unction  ( 19 ) Call be estiinated frOlil training data by MIg principle  (  . \] ayiles 57) . 
Lettl S descrit ) eMEprincil ) le and IIS algorithn i briefly . For given x = ( Xl, .   .   .   , ; Frt ) , the corr ( ?-Sl ) onding probal ) ilitiest ) i ( xi ) is nodklloWll . All we know is the expectation value of the flmction 
J ;+( x ), ( r = 1, 2, . .  . , m ): ? t\];;\[J ; . (,)\] = pg ( . <) J ; . (  . +:) (2 . <)) i=1 ( riven consist(mr constraints , we can find the prot ) ability distribution p ~ that n iakes the entrol ) y--~Pi Int ) i w l h l e l l l a x i l l l l l l i l \] ) yll Sillg Largl : angi all niultipliers in then sual way  , and obtaiu the result : pi(a?i ) = cXt ) (-- ~ J ,  . J; . ( xi )) (30) 7" This forniula is alniosts iinilar 1 . o Gibbs distribution (17), also J\] . correspoilds to the feature of clique function in MI  , F (20) (2 l) . Using this fact , we can use M1! ; in paranieter estimation in MI i . F+We can derive ( 31 ) to be used in pgLralneteres-tiniation fi'om training data  . 
0-o-A-t , + z--(31) ir'l ? o solve the solution of it , a numerical analysis mt ; thod ( - ~ IS ( (\] enerlaized Iterative Scaling ) was suggested ( l ) arroch 72+ )  . Pietraused hisowual-gorithm IIS ( hnt ) roved lterative Scaling ) based on GIS to induce the features and parameters of random field automatically  ( l ) ietra95 )   . Following is
IIS algorithn lllS ( In ) provcdlterative Scaling ) t Initial data A r of ' ol'ellce distribution 1 )  , all initial model q0 and fl ) , fl . . . . , fn . 
? Output q . alldA by MI'\]estiiiiation
Algorithm ( 0 ) Set qC0 ) = q s )   ( 1 ) Pereachi ( ihd X i , t , i l ( ; l l n i q u e s o h l t h ) nel " q(X . ) fi (7,) c ~, lk ) ~, . f , . ( T ) =-~ IS(T )); (7/,)
T 7' (33) (2) k + -- k + l , set q~+l with new Ai ( 3 ) l\['qt ~ Ot ins converged , set q , = q(~') and tertilhiate . Otherwise go to step ( i ) where q ( k ) is the distribution of their iode link th step , all dit , corresponds to the posteriori pro > ability of the tagging model  ( \[ . ( J ) . A , tilesohltion()f'(:/:t )(: all beol)i , ained 1) y Newtonniei . hod(( , ' tlr-t is 89+) , Olie Oflillll(~rica \] analysisnietilod . 
The I'ef ? Tell C ( " distribution \] ~ is thel ) rol ) ability distril ) ution which is obtaiued directly frOll lill'aiD -illg data  .  \] ) corresponds to tile posterior ( listri-button t ' ( TIW ) ill the IA/g~illgiItod ( ti . ~ ? Votl S(t the 11 MM 96 . 111 MH , F ( l ) !) 6 . 2MI , F(2)9 ( i . 5MIF (3) . 97 . I Table 1: Measuring l , hc . a (' cur ; ~ cy of IIM Maud
MR , Fnioch ; Is.
posterior t ) rol ) alfility of l : , hc words sequence o\[win-low size n ( CSl > ecially 3 in this lliodel ) I ) y colillting the entry Oil training data . Trail iiilg data llle w is tagged ( : or l ) ustmrc . 
9 Experiments
The Ili; , I , illol > jcctiv cof ' this experinicnts is to ( ' Olii-l ) a , rettio MIi !' l , a , gg hignlod cl withllic IIMM tagging nl ( )( hJ . \?Cconsl , lulci(xla , ~,'111"/ . aggcrmid a IIMM tagger usiilgs a . lneinl ' or ll ; tliionont . hcsailic (? ll Vil : onlllelll; . 
li , islle(:es Sa , l : y t , odo smoothing tiroccss for datasl ) arse iicssl > roblelit which is scw ; l : ci\[l\]1MM , while MRF hastll ('\ [ acility of sniooi , hing in it , self like neura , l-nel ,   . IvVcil SCdline ; trinl , erl > ol at , ionine / , hod(l ) , ~rot , a . S6+)(jclin & Sg ) and assigning & C , lUel , cy1 for uil knowll word ( Weisctig :/+ ) for sliioothiligin IIMM . 
! t"V'(?llS(?(Il , \[ lC\]lrowII ( ; or lillSilllicn liTl'ceBank , dcscril)cdin(Ma . rc , s93+) with , l~dilli'rent , tags . 
A set of t ~00 , 000 words is colhx ' to d for (' a ch parl , Oflirowlt('Orl ) llSali(\[llSe(I ; t , "; t . l ' aiilillg(h ~ l ; a , which is used to 1) uih l1 , twn iodels . And a set of 30 , 000 words (' , ( ) l'l ) llS is used as i , c's t d a . C ~, which is used to t , esl , the qua . lil ; y of Ltic models . 
' l'alJe1 shows the 3~CC lll ; &( ; yo \[" each L~g gill gniod cl . '1' t,~averagea(:(:ura . (: yoftll('IIMM-hascdl , a , gg(;riSSiinilari , ot Ila , t of MIF (1) l , a , ggerI)c (: aiiS(~l , heyi is (1\[ , he Salil Ohil ' or nial , h)n . 
ld < % 7 sliows dial , l , he error tale as \[ , iicsize o\[\] , r ; IJ nhigdnla , is illcreased . MIF ( I ) has lower error rate L ha , n that of il MM when l , lie size of training data is slnMl . '\] ' hccrrol:l : ~ tl , e of Mt , F ( 2 ) is decreased CSlmcially wllelltlw size of the trdn-ing dab~tisStlllJl  , l)c : (; a . ll SClu or phologic vdia\['ornia . -donhelps I , t , ~ process of lllkiio wii words . Filially , MIl " (3) show it nliroveinent as the size of l ; rain-ing(hfl , ; ~ grows I ) ul ; COll Vel'g ( isl ; ol , i le\]in litOllsOl\]l(?poinl , s . 
The see ? pcrilnen ~ show thai , Mtt , Fhas I ) el , l , cra , d(lal)l ; abilil , y with snl MII , raillilig data than IIMM does , audl , h ; fl : MIF tagger has bss datasl)arse
IleSS probhmithan IIMM l : aggor.
<; 04 O
Ht4i , le20 MRF(LI~I ! . "\[2) i0 do 041 .   .   .   . L000/<: .   .   .   .   .   .  13  .   .   .   .   .   .   .   .   .  ;  .   .   .   .   .   .   .  %  .   .   .   . t . . . . . 
Ji < ~ ttLaIIll\[IIWL ~\[ Il " igurc :7: Errorral , c of each model \[ orgiwm size of t , raining word 10 Comparison of MRF with

We (: ~\[ iide rivel , llcsi in ldified cquatioil of IIMM only wilh bigra  . m : l > ( v'tw ) = (35) is consid(m~dasl , he Inull , ilflied probal > ilticso\[atheh)calcwml , s . Theiio arer the probabilily vahm of local cv ( mt is to zero , t , hc , ~ or (' it , a\[Ii;cl , sI , h('l ) robahilil : y of the ( ml , ire even l ,  . This prol)erty strictly reflccl ; s on the cwmt . '-; which does not occur in l , rain hig(lat , ~L\[:Jui , lid prohibits even the cvcul . 
that does llot OCCllrinl ; rainiug datl halthougt l the
C Vell t is legal.
MII " canhesinil ) lific dt ) ylhesunnlml:ion of clique\['un (  ; tion as (3\]) . 
I - ~% : v , + v , + .   .  +< , , (3(J ) /'('/' lW )-= 2 !' vl I , 1" uses rvalual , io , funclion I ) ysu unuali <) . , while IIMM do (' sI ) yumltil)lic ~ tion . F , venifacliqm~flmcl , ionwd . e is very bad , o ( , hcr cliqn c function ca , nconll ) ensate dequa ~ elylm ca , use the clique functions are coime (' l , ed by summatiou . 
' l ' here is no critical point of postcrioril ) robal ) lil , y in MRIi ' , while IIMM hascril , icalpoi , 1 , in zero value . This property results in the rolmstness mid the ada  , ptability o\[l ; tl ( ~llio do laudnia kcsMt Fuiod clstronger in data Sl  ) arscncss probhml . 
11 Conclusion
We prol ) oscdaMl :\[ l !'- based tagging l node l . Informations our ccs for tagging aro combiim dhy M F  , 1) rincil ) le which is , so di , MIF as theoretical background . AI1I ) ara ~ liiclxu's used in the iiio ( \] e \] are eslJmated from tra , ining data a . ul , omatica , lly . As ; t result , , our MRl!'-l)ascd tagging nlodcl has bet , -tertmr for nla , nccLha , nt\[MM tagging nio(h'l , CS l ) C cially when the size of " the training dal , a is Slii all . 
Vv ' ehaw ~ sooiil , hattimi ) or\['oriilali(:oOfl , he MHI "- I ) as cd tagging nio ( hJ can be ' i in prow ' d by adding in l ' or in a , ii on I , otimniod ct . 


Besag , J . " Spatial interaction and the statistical an Mysis of lattice systems  ( with discusstion )  , " J . Royal Statist . , % c . , series B , vol . 36, pp .  192-326, 1974 . 
Besag , J . " On the Statistical Analysis of Dirty Pictures " , J . Royal Statist . Soc . , vol . B 48,1986 . 
Brill , E . " A Simple Rule-Based Part of Speech Tagger " , In Proceedings of the 3rd Co@on Applied Natural Language Processing , pages 153-155 , April ,  1992 . 
Charniak , E . , C . lten dricks on , N . Jacobson and M . Perkowitz , " Equations for Part-of Speech ' Fagging , " Proc . of Nat'l Conf . on Artificial\[ntelligence ( AAAI-86), pp . 784-789, 11993 . 
In , G . Chun , " Rangehnage Segmentation Using Multiple Markov Random Fiehls "  , Ph . D . thesis,
KAIST , KOtEA , 1993.
Church , K . W . , " A Stochastic PAI~ ; rS Program and Noun Phrase Parser for Unrestricted Text  , " , Proceedings of Applied Natural Language Processing  , Austin , Texas , pp .  136143, 1988 . 
Darroch , J . N . and D . Ratcliff , " Generalized Iterative Scaling for Long-Linear Models  . ." , The Annals of Mathematical Statistics , Volume 43 , pages 1470-1480 ,  1972 . 
Derouault , A . M . and B . Merialdo , " Natural Language Modeling for Phoneme-to-Text Transcription  , " , \[ EEETr . on Pattern Anaysis and Machine Intelligence , vol . PAMI-8, no . 6, Nov . 

Curtis , F . G . and Patric O . Wheatley , " Applied Numerical Analysis " , for the dition , ADDISON
WESLEY , 1989.
Forney , G . D . , " The Viterbi Algorithm ", Proc . of the IEEE , vo\[ . 61, pp . 268-278, Mar .  1973 . 
Gamble , E . B . , Geiger D . and Possio T . , " integration of Vision Modules and labeling of Surface Discontinuities "  , IEEE Transactions on systems , man and cybernetics , vol . 19, no .  6,
November/deeemver 1989.
Geman , S . and Geman D . , " Stochastic Relaxation , Gibbs Distributions , and the Bayesian Restoration of Images " , IEEE transactions on pattern analysis and machine intelligence  , 
VoI . PAMI-6, NO . 6, NOVEMBER 1984.
Geiger , D . and Girosi F . , " ParalM and Deterministic Algorithms from MRF's : Surface Reconstruction "  , \[EEE Transactions on pattern analysis and machine intelligence  , VOL 13 , NO .  5,
MAY 1991.
IIUANF , X . D . , Y . At HKI and M . A . JACK , " Hidden Markov Models for Speech Recognition " ,  1990 . 
Jaynes , E . T . , Information Theory and StatisticM Mechanics , Physics Reviews l06 , pages 620-630 ,  1957 . 
Jelinek , F .   , " Self-Organized Language Modeling for Speech Recognition  . "  , in Readings in Speech Recognition , Alex Waibel and Kai-l , 'u
Lee ( Editors ). Morgan Kaufinann , 1989.
Kupiec , , l . , Robust Part-of-Speeh Tagging Using a llidden Markov Model  , Computer Speech and
Language , 1992.
Marcus , M . P . , Beatrice Santorini and Mary Ann Marcinkiewiez , " Building a large annotated corpus of English : the Penn Treebank "  , Computational Linguistics , Vol . 19, No . 2, pp 313-330,
June , 1993.
Merialdo , B . , " Tagging English Text with a Probabilistic Model "  , Computational Linguistics , 
Volume 20, no 2, June 1994.
Nakamura , M ., K . Maruyama , T . Kawanata and
K . Shikano , " Neural Network Approach of Word Category Prediction for English rl ~ XtS  , " Int'l Confon Computational Linguislics ( Coling-90 )  , pp .  213-218, 1990 . 
Pietra , S . D . , V . D . Pietra and J . Lafferty , " Inducing features of random fields " , Carnegie Mellon University , Technical report CMU-CS-95-144 , 
MAY , 1995.
Rosenfeld , R ., " Adaptive Statistical language
Modeling : A Maximum Entropy Approach " , Carnegie Mellon Uniw ~ rsity , technical report
CMU-CS-94-138, April 19, 1994.
Weischedel , 1., R . Scewartz , a . Ralmucci , M.
Meteer , and L . P ~ awshaw . " Coping with Ambiguity and Unknown Words through Probabilistic Models "  , Computational Linguistics ,  19(2):359-382 ,  1993 . 
Zhang , J . and J . W . Modestino , " A Markov Ran-dora Field model-based approach to image interpretation "  , Visual Communications and image Processing IV , Vol 1199 ,  1989 . 

