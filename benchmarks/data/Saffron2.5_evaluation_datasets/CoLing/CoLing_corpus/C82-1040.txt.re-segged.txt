COLING82, J . Horeck ) (? d,)
North-Holland Publishing Company
? Academia , 1982
A Parser Which Learns the Application Order
of Rewriting Rules
Makoto Nagao , Junichi Nakamura
Department of Electrical Engineering
Kyoto University
Sakyo , Kyoto

0. Introduction
The efficiency of syntactic analy-
s is by using a set of rewriting rules
is greatly influenced by the order or
the arrangement of the rules for the
application . There are some trials
which subdivide the set of rules into
subsets and specify the sequence of
rule applications , thus avoiding the
useless rule applicagions\[i \]. But the
subdivision of the ruleset and the
specification of the sequence of rule
applications are not so easy for the
establishment of the most efficient a-
nalysis system.
We have developed a rewriting rule
system which can manipulate arbitrary
list of trees . The control mechanism of
this system can adjust the weight of
the rewriting rules , and can analyze
the most plausible sentential structure
f t , thus realizing the fast syntac-
t analysis . The system learns ( so to
s\[) the weight of importance of the
reutiting rules during the analysis of ~
input sentences.
I . Objectives of the Parser
We designed a new syntactic analy-
s is system ( a parser ) with the follow-
ing objectives.
( i ) The function of rewriting rules
must be powerful enough to handle a
list of trees and to express trans-
formational rules,
(2) All the possible sentential struc-
tures must be obtained for an input
sentence in the sequence that the
most plausible one is analyzed

(3) The analysis must be efficiente-
nough for practical applications.
(4) The syntactic parser must have a
learning mechanism as to the ap-
plication sequences of rewriting
rules to obtain the efficiency of

2. Method of Analysis
The input data for this parser is
assumed as a word sequence which is the output of a morphological analysis  . The output from this parser is a tree structure . The analysis is controlled by the bestfirst graph -searching technique about the rule applications  . 
2.1. Description of Rewriting Rules
The rewriting rules transform a list of trees into a list of trees  . An example of the rewriting rule in this parser is shown in Fig  .  1 . It shows that if there is a symbol sequence composed of a tree not-V  ( erb )  , a tree
N ( oun)P ( hrase ), a tree
C(ase-particle) , and a tree not-A ( dverbial ) -P ( article ) in this order , this is transformed into a tree

NP CY/~~---XNP CY
NPC(not V ) ( not AP)
NP : Noun Phrase C : Case-particle
V : VerbAP : Adverbial-

Fig . 1An example of the rewriting rule
The right side of rewriting rules is a matching pattern which is to be found in the given input symbol string  . 
Table 1 shows the function symbols to describe the matching patterns  . By using these function symbols , it is possible to specify the repetition of pattern elements  , to assign data to a variable , and so on . T t is also possible to check the input data by using user-defined functions  . These functions enable us to describe complex syntactic functions  , semantic relations , and many heuristic checks . 
From ?#(% Ffnal . . . an (%  #at xl ., . xn
Table IF unction symbols of the matching patterns
I Function
Match an arbitrary tree ?
Hatch any number of arbitrary trees.
) Evaluate function = fn whose arguments are a
Corresponding tree , al , ?.., and an . When the
Value is not NIL , matching succeed.
) Match any number o4 lists of matching patterns x l ? . . xn . Trees are assigned to variable at . 
(% Axl . . . xn ) Matching succeeds if all xl , . . . , xn  are matched to a tree . 
(% Oxl . . . xn\]Matching succeeds if one (% Nx ) of xl , . ? . , xn  is matched to a tree . 
Matching succeeds if x is not matched to a tree.
Table 2 Function symbols of the creation patterns
Form Function at If at is a variable , then its value , otherwise at itsel ~ o(*F f n x l .   . xn ) The value Of the function : fn whose arguments are xl  , ? -- txn . 
(* Satx ) The value of a generation
Of x assigned to the variable at ?
The left side of rewriting rules is a creation pattern of new syntactic structures  . Table 2 shows the function symbols for structure creation  . User-defined functions can also be used to check certain relations in this creation pattern  . We can generate an arbitrary tree structure by this re-writing rule system  . 
NP-Ct NP . -CJ NP-Ck VF . F-ci ~ -- ~ csP-ciNF-CJNF-CkVE
NPC ~ NP-Ck VP NPC : Noun Phrase + Case-partlcle
NP-Ct MP-C~~VP : Verb Phrase
S : Sentence
NP-Ck VP
Fig .   2 An example of generating three left sides from a right side 
As shown in Fig .  2 , we can specify arbitrary numbers of structures in the left side for the same right side in a rewriting rule  . 
Each rewriting rule has a weight basic score ) and a function ( fittedness function )  . The basic score is a static weighting measure which ~ e-flects the importance of a rule co ~ -pared to the other rule G of the same category  . The basic score is adjusted by a learning process which will be explained in section  3  . The ' fittedness function gives a dynamic weighting measure which expresses the fittedness of the rule application to a sentential structure  . The function is a user-defined one which can use the data in both the right side and the left sides of the rewriting rules  . 
The basic score and the fittedness function are used for the sequence control of rule applications in the bestfirst graph-searching  , which is the essential strategy to get the most plausible structural analysis first  . 
2.2. Flow of Analysis
Fig .   3 shows an intermediate structure in the cOurse of a sentence analysis  . 
( NPC
Datiset v PTENSE)


ShtesurU(specify )
Fig .   3 The structure of a sentence during the analysis LOT  ) 
This structure is represented by a list of trees . We call this structure as a LOT List of Trees ) . An analysis step is an application of a rewriting rule to a LOT as shown in Fig  .  4 . which changes the content of the LOT . 
( ~ cNPC

N(NF-C
NFC

NvTENSE )
NP <--- N
VTENSE ) ~ N~-C<---NFC
VTENSE
Fig . 4 Progress of an analysis
PARSERLEARNINGORDER OFREWRITING RULES 255
To obtain the result of an analysis one by one in the order of plausibility  , we use the bestfirst graph-search lng technique . If we regard a LOT as a node in a search graph , the new LOT created by the application of a rewriting rule to an old LOT is a sis-teen ode  . When several rules are applicable to a LOT or the rule has several left sides  , the same number of sister nodes are created from one moth-er node  . The progress of analysis can be represented by an expansion tree  ( in general , by a graph ) as shown in Fig . 

I(AZI /%) LOT-node / ~ ~- . . . Application of ( A/k )   ( ZiA ) rewriting rule / ~\ . . .  / "%'~ . . . = expans ? on (~)
Fig . 5 Search tree
This tree can be regarded as a search tree . We expand the node which has the highest evaluation value  ( the score assigned to the LOT first . The expansion is the application of a rewriting rule to a LOT  . The evaluation value is obtained by the summation of the following four values :  ( 1 ) the evaluation value of the mother node . 
( 2 the basic score which is attached to the applied rule  . 
(3 ) he value obtained from the tittedness function which ~ is attached to the applied rule  . 
(4) the score of the sentential pattern ( SP . which will be explained in section 2 . 5), if it matches to the

Analysis is executed by principle of the bestfirst graph-sear'ching technique as follows =  ( i ) Find the LOT which has the highest evaluation value  . 
(2) Apply rewriting rules to the selected LOT.
(3) If a rule is applicable , create new nodes ( LOTs ) . 
(4 ) Assign the new evaluation values to the new LOTs by the above method  . 
(The initial LOT value is the summation of the scores attached to words  . ) (5) Go to (1) . 
2.3. Application of Rewriting Rules.
The detail of the rule application sequence to a LOT which is selected by the bestfirst graph -searching technique is the following order =  ( 1 ) From left elements of the LOT . 
2) FrOm the rule which has the longest right side.
(3) From the rule whose basic score is the largest . 
( A B  I ( X  ~ Z ( 2 )   ( X ' Y ' )   ( 3 )   ( x - )   ( 4 )   ( xYz )  5 )   ( X ' Y ' )   ( 6 )   ( X " )   ( 7 )   ( X ' Y ' )   ( 8 )   ( X " )   ( 9 )   ( x - ) 
CD ) <--- LOT application order of rewriting rules 
Fig .   6 An example of the application order of rewriting rules 
Fig .   6 shows a simple example of the rule application when rewriting rules have  ( xYZ )  , ( XY ) , and ( X") as their right side , and ( X") , and the selected LOT is ABCD ) . First ( AB
C ) is matched with ( XYZ) . If the matching is not successful , ( AB ) is matched with X wyW ) . Tf the matching is not successful , A ) is matched with ( X") . If the matching is not successful again , BCD ) is matched with ( XYZ) , and so on . 
To speed up the rule applications , matching patterns which are right sides of rewriting rules are reconstructed in a tree structure such as shown in Fig  . 

original reconstructed rewriting rules rewriting rules r l  ( ABC ) A->B->Cr2 ( ABD ) --> I~D r3 ( AE ) r4 ( FG ) F -> G
Fig . 7 Reconstruction of rewriting rules In Fig . 7, if the first element of the
LOT does not match with A , we do not need to test the rules rl-r3 . So the rule r4 alone is tested for the application . By this reconstruction , the number of rules which are to be applied to a LOT is decreased qrately  . 
256 M . NAGAO and J . NAKAMURA 2.4. Pruning Rule
This parser is essentially a bottom-up parser , and there are cases that unnecessary expansions are executed  . To minimize such unnecessery expansions , we introduced a mechanism of pruning such unnecessary nodes by certain pruning ~ ules  . For example , in the analysis of Japanese svntenc ~ there must be ~ ome verb phrase =  ( %~ ) to the right of a noun phrase ( ME , so % ~ euse the pruning rule shown in Fig .  8 . 
It ~ , atches with LOT , if LOT consists of sc~etrees , a tree N , NP or NPC ~ and trees which are not V , V-DA or VP in this order . 
(#  ( %0  ( ? N # )   ( ? NP # )   ( ? N~-C ~ ) )  ( % #NIL ( % N ( %0  ( ? V # )   ( ? V-DA # )   ( ? ~ # ) ) ) )  )   ( There must be V ,  ~ . DA or VP in ~ h ~ ~ igh ~: of N , NP or NP-Co ) ? igo 8 An example of the pruning ru ! e
The p~un!ng rules are described by matching patterns just the same as the right side of re ~ rit ! ng rules ? They are matched with the whole LOT at the time that a LOT is created  . If a pruning rule matches with the LOT , the node is pruned . 
2 . 5 . Sententlal Pattern sentent lal pattern ( SP expresses the global structure of a sentence . 
Fig . 9 shows examples of SPo ( I )   ( S-OBJiNPV-DATENSE ) :- i ( 2 )   ( NP-CkS-OBJj NPV-DATENSE )  : +1  ( a ) Sentential patterns ( i )   ( S-OBJiM PV-DATENSE ) 
NP-CkNP-CmVEITENSE(2) ( NP-CkS-OB . _JjNPV-DATENSE )
NP-Cm VPITENSE ( b ) Corresponding LOTs ( NE-CkT ~ P-CmVPITENSEV . -DA"TENSE ) ( c ) Original LOT Fig . 9 Examples of Sententlal Pattern E(sp)
The top two lines are the LOTs which are intermediate structures from an input sentence :   ( NP-CkNP-CmVPIT ~ SENPV-DATENSE ) 
JSEU PDTE-pEogram Ha Source-program-ll brary Wo Shuselsuru  ( modify ) 
Dataset-utillty Dears(Is).
(JSEUPDTE program is a Dataset uti l ity which modifies source program libraries  . ) Each element of sentential pattern is a grm ~matl cal category name  , not a tree structure . The elements of a sentential pattern are compared with the sequence of grammatical category names in a node  . SP(1)~p~esents that NP-Ck(JSEUPDTE-progr ~ , I-H ~ ) is related to VP1 ( the first embedded verb , Shuselsuru ( mcdify ) . SP ( 2 ) represents that NP-CkIs related to V-DA ( main verb DA ( is ) )o
The ~ a , :, ~ er assigns ~ P-sco~s and
SP-rule to a sentential Fattern . SP-score is a number such as shown in Fig . 
~ . ~ his $ ~ presses the plausibility of the styl ~s of sentence s ? in this exmn-p  )  . C:SP ( i ) is assigned the numerical v ? ~ ue : - I ~ and SP ( 2 ) is essggned the value :+! ~ as the SP-sco ~: e o The set ~ ovaiue ~ mean that ~ whenth ~ main verb is 
V - ~ A , th ~ first NPC ha ? tendency to be related to the main verb rather than to the first embedded verb  . This SP-score is added to the evaluation value explained in section  2  . 2 . Therefore ~ analysis ( 1 ) takes precedence over a nal- , s is (2) in hhis case . 
( NPC ~ C VP TENSE ) : SP (   ( rule-i2 ) ' rule --21 ) : SP-rule ( rule-3i )   ( ru ! e-4! )   ) 
Fig .   i0 An exmaple of SP-rule ~!' I go i0 shows an example of SP-rule o The sentential pattern whose SP-score is positive has at least one correct analysls  . And a sequence of rule appll cat lon ~ to the sentent lalstructure is guaranteed  . S~-rules represent this sequence . However , it is not evident whether the sentent lal pattern whose SP-score is negative has correct analyses  , because it has at least one incorrect analysis . So we do not attach any SP-rule to it . 
SP-rule in Fig . I0 shows that we can get a correct analysis , if we apply rule-i-rule-4 to the LOT . Fig . 11 shows this process of rule applications . The sequential rule application of these four rules Isequivalent to a PARSERLEARNING ORDEROF REWRITING RULES  257 rewriting rule shown in Fig .  12 . But the rewriting rules Of the form shown in Fig .   10 are much better because the semantic check functions can be easily introduced to the simpler rules such as those in Fig  .   i0 rather than to such complex rules as those in Fig  .  12 . 
NP CNP CVP TENSE)
I .... NPC <--- NPC(rule-I)
NP CNP CVP TENSE . . . . S-NUCL <--- NPC NPC VP ( rule2
S-NUCLTENSE ) . . . . S-OBJ <--- S-NUCL TENSE ( rule-3
S-OBJ).... S<---S-OBJ ( rule-4)
S
Pig . 11 An example of the SP-rule application
Si
S-OBJ <- NP CNP CVP TENSE
S-NUCLTENSE ~ NI~.CVP

NPC
Fig . 12 An example of the equivalent rewriting rule
Each LOT is compared to sentential patterns from the first element of the LOT  . The LOT is regarded as matched if the first part of the LOT matches a The parser changes the scores of sentential pattern  , rewriting rules and SP-scores in the following way ~  3  . Supervised Learning of Basic Scores ,   ( i ) Increase the scores of the rewrit-SP-scores and SP-rulesing rules and SP-scores on the path 
I from the root node to the success-
To increase the efficiency of the fulnode , and those on the pathes analysis , the parser controls basic which flow into the successful scores attached to rewriting rules  , pathes . 
SP-scores and SP-rules . It is not easy ( 2 ) Decrease the scores of the rewrit-for rule writers to assign scores to ing rules and SP -scores on the rewriting rules and to sentential f ir starcs of the pathes which flow patterns  , and also to write SP-rules out the successful pathes  . 
for a sentential pattern . We tried to SP-rules are gathered for each adjust these scores and to get SP-rules sential pattern on the successful by the supervised learning in which the pathes by using the information in the user teaches the correctness of an a-search graph  . 
nalys is to the parser.
Fig . 13 shows an example of a 4 . Result of Seme Experiments search graph when a sentence is analyzed  . Each node of the search graph The sample sentences to be and-corresponds to a LOT  . Each arclyzed are taken fro ~ a computer manual corresponds to a rule application  . We in Japanese . About 150 sentences are can regard the LOTs on the path from used for the experiments  . Conjunction the root node to the successful node as structures of noun phrases are useful structures  , and the rewriting eliminated from these sentences  . Among rules on the path as useful rules for the future analysis of similar sentential structure  . On the other hand , other LOTs and rewriting rules in the search graph are regarded as useless to the future us ~ e  ; But ~ e nodes and arcs\[i \] in Fig . 13 are not the direct reason of the failure . The direct cause for the failure comes from the nodes and ~ cs\[ii \] in Pig  .  13 . 
acte ~ d~s1//ii/ .  - '~--: /  . / ~ failed ~ t~d~s ~ failed ~ z ~ iting Rule
SP : Sententlal Pattern
Fig .   13 Relation ~ t ~ en the state of the expa ~ ion and failure or success of the analysis  258 M . NAGAO and J . NAKAM ~ ltA150 sentences ,   20 sentences are used for the supervised learning . These are selected ran & : mly . The rewrfting rules are created from the gra lmar proposed by Okutsu  \[2\]  . The number of rewriting rules is 54 . The re~rlting rules in this experiment do not have the semantic check functions for s~pll clty  . 
They are prepared to get the syntactic structures for a sentence  . 
4.1. Experiment I-Learning of Basic
Scores of Rewriting Rules.
To see the efficiency improvement of the ana lys is from the contribution o ? basic scores  , SP-scOres and SP-rules are not used . The initial order of the rewriting rules is determined by random numbers  . The initial basic scores are set the same value I for all rules  . We adjusted basic scores 4 times , every time after 20 sentences for learning are analysed . We corpared the CPU-tlmes of the 2nd , 3rd and 4th analyses to the
CPU-tlme of the let analysis . The result is shown in Table 3 . 
Table 3 Effect of basic scores 12nd/let   3rd/Ist   4th/Ist max .  99 . 37~ 102 . 10% 108 . 78% averaq ~94 . 62% 96,75~ 96 . 47% mln .  87 . 69% 87?88% 89 . 49%  ( The values are the ratio o?th ( 2 PO-time per word . ) Table 3 tells us that the basic scores of rewriting rule ~ are not ~ ouseful for the improvement of the efficiency o ? analysis  . The learned order o?re-writing rules does not have a slgnlf ~- cant tendency  . The reason Is that the structure of natural languages is recursive and the relative order of rules are more important to the anslysls than the over all ordering  , so that the basic scores cannot express the relative order  . 
4.2. Experiment 2-The Effec ~ of
SP-sonres and SP-rules
The learning of the SP~scores and
SP-rules are done by enalys~ng these ~ of sample sentences once  ( 2 0 Sentences selected amon 9   153 sentences r ~ n ~ ly . 
Then the analysis of these to ~3~mpAe sentences ( 153 sentences ) isd ~ ewit / ~ and without using SP-soo ~ e ~ ~ S ~ -rules  . The result of the experiment is in Table 4 . 
Table 4Effect of SP-scores and
SP-rule e o Y he same SP not the same SP nu ~ Lber sentences ~  42   111 max .   26 . 06~ 108 . 63% average\[19 . 23 t 67 . 36% min .   1 . 03% 9 . 46%  ( The values are the ratio of th ~ analysis time with SP-score e and S~rules to the analysis time with oul them  . )
About 200 sentent lal patterns are extracted frc , n the 20 sample sentences for learning . SP-zules are very useful for the sentences which have the same sentent lal patterns  , because the ze-writing rules and their application sequence in the analysis of the senten-tlal pattern can be obtained from SP-rules which are defined from the past analysis  , and no more trial search is necessary .  27 . 5% o ? sample sentences have the same ssntentlal patterns as the sentences foe learning  . This n ~ ans that s ( ~ e documents l ~ k e a computer manual contain very similars e ~ ences  . 
Sentent lalp atterns and SP-rules are useful ? or the analysis o ? such documents  . 
5. Conclusion
The experiments to examine the effect of lea~nlng are performed  . The results of ~ he experiment shows that
SP-rules a ~ very useful . Th~s ~ eans that ~ hlsp ~ Eser can learn the s~yle of the sentences an ~ can increase the  ef-?1c~ncy of & nalyels when the senten-tlal structure so ? the texts in the partlcular field are ~ estric ted  . 
This parser is implemente ~ ~ LZSP on ~ ACOM M-2O0 in Com~uter Cen~eEo ?
Kyoto University.
Reference ~\[ I\]Boltet , C . , Aut ~ tlc ~ rc~uct ~ nof
CF an ~ CS-a~ly~using . A General sclen ~ If lque ~ u ~ I ~ de
Gr ~; ~ eble,I % 79 o\[2 ~ Okut~u , ~ . ~ Sei~el ~ Ipp ~ o ~ un~o-
