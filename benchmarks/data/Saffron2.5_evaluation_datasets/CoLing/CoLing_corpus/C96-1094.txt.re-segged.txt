Translating into Free Word Order Languages
Beryl Hoffman
Centre for Cognitive Science
University of Edinburgh
2 Buccleuch Place
Edinburgh , EH8 9LW , U.K.
hoffman ~ cogsci , ed.ac.uk
Abstract
In this paper , I discuss machine translation of English text into a relatively " free " word order language  , specifically
Turkish . I present algorithms that use contextual information to determine what the topic and the focus of each sentence should be  , in order to generate the contextually appropriate word orders in the target language  . 
1 Introduction
Language such as Catalan , Czech , Finnish , German , Hindi , Hungarian , Japanese , Polish , Russian , Turkish , etc . have much freer word order than English . For example , all six permutations of a transitive sentence are grammatical in Turkish  ( although SOV is the most common )  . When we translate an English text into a " free " word order language  , we are faced with a choice between many different word orders that are all syntactically grammatical but are not all felicitous or contextually appropriate  . In this paper , I discuss machine translation ( MT ) of English text into 2hrk-ish and concentrate on how to generate the appropriate word order in the target language based on contextual information  . 
The most comprehensive project of this type is presented in  ( Stys/Zemke , 1995) for MT into Polish . They use the referential form and repeated mention of items in the English text in order to predict the salience of discourse entities and order the Polish sentence according to this salience ranking  . They also rely on statistical data , choosing the most frequently used word orders . I argue for a more generative approach : a particular information structure  ( IS ) can be determined from the contextual information and then can be used to generate the felicitous word order  . This paper concentrates on how to determine the IS from contextual information using centering  , old vs . new information , and contrastiveness . ( Hajifiov?/et al , 1993; Steinberger ,  1994 ) present approaches that determine the IS by using cues such as word order  , definiteness , and complement semantic types ( e . g . 
temporal adjuncts vs arguments ) in the som:cc language , English . I believe that we cannot rely upon cues in the source language in order to determine the IS of the translated text  . Instead , I use contextual informati < ) ni the target language to determine the IS of sentences in the target language  . 
In section 2 , Idiscuss the Information Structure , and specifically th < ~ topic and the focus in naturally occurring Turkish data  . Then , in section 3 , I present algorithms for determining the topic and the focus  , and show that we can generate contextually appropriate word orders in '\[~ / rkish using these algorithms in a simple MT implementation  . 
2 In fo rmat ion St ructure \] n the Information Structure  ( IS ) that I use for Turkish , a sentence is first divided into a topic and a comment  . The topic is them aiu element that the sentence is about  , and the comment is the information conveyed about this to I  ) ic . 
Within the comment , we tind the focus , the most information-bearing const . it nent in the senten (: e , and the ground , the rest of the sentence . The focus is the new or important information in the sentence and receives prosodic prominence in speech  . 
In Turkish , the pragmatic fimction of topic is assigned to the sentence-initial position mM the focus to the immediately preverb M position  , following ( Erguvanh ,  11984) . The rest of the sentence forms the ground . 
In ( Iloffman , 1995; Iloffman , 1995b ) , I show that the information structure components of topic and focus can be suecess fiflly used in generating the context-appropriatens werto database queries  . Determining the topic and focns is fairly easy in the context of a simple question  , however it is much more complica . ted in a text . In the fol-
Cb = Subject 14 (47%)
Cb = Object 6 (20%)
Cb = Subj or Obj ?6 (20%)
Cb = Subj or Other Obj ?0 (0%)
No Cb 4(1.3%)
TOTAL 3O
Tim Cb in OSV sento , n co.s.
Cb = Subject 4 (13%)
C'b = Object : t6 (53%)
Cb = Sub , i or Ob . i ? -6 (2()%) -
Cb = Sul ) jor Other () b.i'?2U %)
No Cb2 (7%)
TO'rnl , 30
Figure 1: The Cbit , SOVa , nd OSV Sentences.
lowing sections , I will describe the characteristics of topic , focus , and ground components of the 1S in naturally occurring texts analyzed in ( lloffman , 1995b ) and allude to possible algorithms for determining them  . The algorithms will then be spelled out in section  3  . 
An example text from the cortms1 is shown below . The noncanonical OSV word order in ( 1 ) bis contextually appropriate because the object t  ) ro-noun is a discourse-old topic that links these . n-tence to the previous context , and the sul ) jeet , " your father " , is a discourse-new focus that is being contrasted with other relatives  . Discourse-old entities are those that were previously mentioned in the discourse while discourse-newentil  , ics are those that were not ( Prince ,  1992) . 
O ) a .

Budefter idegok say ( liraben.
This note bk-acc too much like-l)st-lSI.
'As for this notebook , I like it very much.'
Bunudababan mlver di ? ( OSV )
This-Ace too father-2S Quest give-Past ?' Did your FATHER , give this to you ?' ( CHILDES lba . cha ) Many people have suggested that " free " word order languages order information from old to new information  . However , the Old-to-New ordering prim : iple is a generalization to which exceptions can be found  .   1 believe that the order in which speakers place old vs  . new items in a sentence reflects the information structures that are awdlable to the speakers  . The ordering is actually tile ' Ibpic followed by the Focus  . Tileqbpic tends to be discourse-old inlbrmation and the focus disconrse-new  . However , it is possible to have a disconrse-NEW topic and a discourse-OLD focus  , as wc will see in the following sections , which explains the exceptions to the Old-To-New ordering principle  . 
1The data was collected fi'om transcribed conversations  , contemporary novels , and adult speed l from the CHILDES corpus . 
2.1 Topic
Although humans can intuitively determine whal , the tol)ic of a sentence is , the traditional delinition ( what tim sentence is about ) is too vague to be implemented in a CO ml ml , ational system , l propose heuristics based on familiar it , y and salience to determine discourse-old seal ; antetopics , ~tt~?l heuris ~ ties based on grammatical reb ~ tions Ibr discourse-newt  . opics . Speakers can shill ; Loa new topic at the start , of a new discourse sag/ileal . , ; tsiH (2) a . Or they can continue ta . lking about Lh(~sam ( , ( liscours (> o\[(Itot ) it , asiu (2) 1) . 
(2) a .\[ Mary\]mwent to lhe I , ook store.
b . \ [ She\] . / . I ) ought a new book on linguistics . 
A discourse-old topic often serves 1 . oliuk the sentence to the previous context l ) yevoking a familiar and sMient discourse entity . ( ~ enteriug Theory (( ~ rosz / et al ,  1 ) 95 ) provides a measure of saliency based on the obserwrtions t  ; hat salient discourse entities are often mentioned rel  ) ea . 1; edly within a discourse segment and are of t . an r(mlized as pronouns . ( rl~lran , 1995) provides a .   ( : OUlpre-hensive study of null and overt subjects in Turkish using Centering Theory  , and \[ inw~stigate the interaction between word order and  ( ' , catering in
Turkish in ( Iloffman , 1996).
In the Centering Algoritl . n , each nt , l , era . nce in a discom:se is associated with a ranked list of discourse entities called the forward -lookiugeent  . ers(Cf list ; ) that contains every ( lis ( : ours ( ~ entity that is reMized in thai ; utteraltce . The C f list is usually ranked according to a hierarchy of granmmtica \] relal  , ions , e . g . subjects are a SS llllled to \] ) elllore salient than objects . The backward looking center ( Cb ) is the most salient member of t , he C flist that links theer a ' rent utterance to the i we vious utterance  . The Cb of an utterance is delined as the highest ranke  ( lelement of the previous utterance's Cf list that also occurs iu the curren  (  , utterance . 
If there is a pronoun in the sentence , it ialikely to be the ( Jb . As we . will see , the ( ~ , b has much in common with a sentence-tol)ic . 



D-New , Hearer-Old
S-in its ov , osv 55 (85%) 8 (13%) i(2%)
IPV Post-Vsov , os_vovs , svoo_43 ( 67% )  56  ( 93% )  10 06% )  4  ( 7% )  1  ( 2% ) 0* D-New , Hearer-New 010 (15%) 0
TOTAL 64 64 60
Figure 2: Given/New Status in Different Sentence Positions The Cb analyses of the canonical SOV and the noncanonical OSV word orders in  251rkish are summarized in Figure 1   ( forthcoming study in ( Hoffman ,  1996)) . As expected , the subject is often the Cb in the SOV sentences . However , in the OSV sentences , the object , not the subject , is most often the Cb of the utterance . A comparison of the 20 discourses in the first two rows 2 of the tables in Figure 1 using the chi-square test shows that the association between sentence-position and Cb is statistically significant  ( X2=10 . 10, p < 0 . 001) . a Thus , the Cb , when it is not dropped , is often placed in the sentence initial topic position in Turkish regardless of whether it is the subject or the object of the sentence  . The intditive reason for this is that speakers want to form a coherent discourse by immediately linking each sentence to the previous ones by placing the Cb and discourse-old topic in the sentence-initial position  . 
There are also situations where no Cb or discourse -old topic can be found  . Then , a discourse-new topic can be placed in the sentence-initial position to start a new discourse segment  . Discourse-new topics are often subjects or situation-setting adverbs  ( e . g . yesterday , in the morning , in the garden ) in 3Mrkish . 
2.2 Focus
The term focus has been used with many different meanings  . Focusing is often associated with new information  , but it is wellknown that old information , for example pronouns , can be focused as well . I think part of the confusion lies in the distinction between contrastive and presentational  2The centering analysis is inconclusive in some cases because the subject and the object in the sentence are realized with the same referential form  ( e . g . 
both as overt pronouns or as full NPs).
Z Alternatively , using the canonical SOV sentences as the expected frequencies  , the observed frequencies for the noncanonical OSV sentences significantly diverge from the expected frequencies  ( X2 = 8 . 8, p < 0 . 005) . 
focus . Focusing discourse-new information is often called presentational or informational focus as shown in  ( 3 ) a . Broad/wide focus ( focus projection ) is also possible where the rightmost element in the phrase is accented  , but the whole phrase is in focus . However , we can also use focusing in order to contrast one item with another  , and in this case the focus can be discourse-old or discourse-new  , e . g . (3) b . 
(3) a . What did Mary do this summer ?
She\[wandered around TURKEY \] F.
b.It wasn't\[ME\],.,-It was\[HF,R\]e.
( VM lduvf ,  1992 ) defines fbcns as the most information-bearing constituent  , and this definition encompasses both contrastive and presentational focusing  . I use this definition of focus as well . However , as will see , we still need two different algorithms in order to determine which items are in focus in the target sentence in MT  . We must check to see if they are discourse-new information as well as checking if they are being contrasted with another item in the discourse model  . 
In Turkish , items that are presentationally or contrastively focused are placed in the immediately preverb M  ( IPV ) position and receive the primary accent of the phrase  . 4 As seen in Figure 2 , brand-new discours entities are found in the , ,IPV position , but never in other positions in the sentence in my Turkish corpus  . The distribution of brand-new ( the starred line of the table ) versus discourse-old information ( the rest of the table 5 ) is statistically significant , ( X2=10 . 847, p < . 001) . 
This supports the association of discourse-new\[ b-cus with the IPV position  . 
4Some languages uch as Greek and Russian treat presentation alnd contrastive focus differently in word order  . 
5 lnferrables refer to entities that the hearer can easily accmnmodate based on entities already in the dis-  . 
course model or the situation . Hearer-old entities are wellknown to the speaker and hearer but not necessarily mentioned in the prior discourse  ( Prince ,  1992) . 
They both behave like discourse-oM entities.

However , as can be seen in Figure 2 , most of the focused subjects in the OSV sentences in my corpus were actually discourse-old information  . Discourse-old entities that occur in the IPV position are contrastively focused  . In ( Rooth , 1985)'s alternative-set heory , a contrastively focused item is interpreted by constructing a set of alternatives from which the focused item must be distinguished  . Generalizing from his work , we can determine whether an entity should be con -trastively focused by seeing if we can construct an alternative set from the discourse model  . 
2.3 Ground
Those items that do not play a role in IS of the sentence as the topic or the focus form the ground of the sentence  . In Turkish , discourse-old information that is not the topic or focus can be  ( 4 ) a . dropped , b . postposed to the right of the verb , c . or placed unstressed between the topic and the focus  . 
Postposing plays a backgrounding fnnction in Turkish  , and it is very common . Often , speakers will drop only those items that are very salient  ( e . g . mentioned just in the previous sentence ) and postpose the rest of the discourse-old items , lIow-ever , the conditions for dropping arguments can be very complex  . ( Turan ,  1995 ) shows that there are semantic considerations ; for instance , generic objects are often dropped , but specific objects are often realized as overt pronouns and fronted  . 
Thus , the conditions governing dropping and post-posing are areas that require more research  . 
3 The Implementation
In order to simplify the MT implementation , I concentrate on translating short and simple English texts into Turkish  , using an interlingua representation where concepts in the semantic representation map onto at most one word in the English or Turkish lexicons  . The translation proceeds sentence by sentence ( leaving aside questions of aggregation , etc . ) , but contextual information is used during the incremental generation of the target text  . These simplifications allow me to test out the algorithms for determining the topic and the focus presented in this section  . 
In the implementation , first , an English sentence is parsed with a Combinatory Categorial Grammar  , CCG , ( Steedman ,  1985) . The semantic representation is then sent to the sentence planner for Turkish  . The Sentence Planner uses the algorithms in the following subsections to determine the topic  , focus , and ground from the given semantic representation ~md the discourse model  . 
Then , the sentence planner sends the semantic representation and the information strncture it has determined to the sentence realization component for Turkish  . This component consists of a head-driven bottom up generation algorithm that uses the semantic as well as the information strnc-ture features given by the planner to choose an appropriate head in the lexicon  . The grammar used for the generation of 3hlrkish is a lexicalist formalism caltiset-CCG ( Hoffman , 1995; Iloff-man , 1995b ) , an extension of CCGs . Multiset-CCG was developed in order to capture formal and descriptive properties of " free " and restricted word order in simple and complex sentences  ( with discontinuous constituents and long distance dependencies  )  . Mnltise t-CCG captures the context-dependent meaning of word order in ' Fnrkish by compositionally deriving the predicate-argument structure and the information strnctm'e of a sentence in parallel  . 
The following sections describe the algorithms used by the sentence plauner to determine the IS of the ' l Slrkish sentence  , given the semantic representation of a parsed English sentence  . 
3.1 The Topic Algorithm
As each sentence is translated , we update the discourse model , and keep track of the forward looking centers list  ( Cf list ) of the last processed sentence . This is simply a list of all the discourse enities realized in that sentence ranked according to the the ta-role hierarchy found in the semantic representation  . Thus , the C f list for there I ) re-sentation give ( Pat , Chris , book ) is the ranked list \[ Pat , Chris , book\] , where the subject is assmned to be more salient than the objects  . 
Given the semantic representation for the sentence  , the discourse model of the text processe ( lso far , and the ranked C\[lists of the current and previous sentences in the discourse  , the following algorithm determines the topic of ( ; he sentence . First , the algorithm tries to choose the most salient discourse-old entity as the sentence topic f If there is no discourse-old entity realized in the sentence  , then a situation-setting adverb o , the subject is chosen as the discourse-new topic . 
l . Compare the current Cf list with the previous sentence'sC flist  ; and choose the firs ( item that is a member of both of the ranked lists  ( the Cb )  . 
6 ( Stys/Zemke ,  1995 ) use the saliency ranking to order the whole sentence in Polish  . tIowever , \[ I ) clieve that there is a distinct notion of topic and fo  ( : as in

559 2 . If 1 fails : Choose the first item in the current sentence's Cf list that is discourse-old  ( i . e . is already in the discourse model ) . 
3 . If 2 fails : If there is a situation-setting adverb in the semantic representation  ( i . e . a predicate modifying the main event , in representation ) , choose it as the discourse-new topic . 
4 . If 3 fails : choose the first item in the Cf list ( i . e . the subject ) as the discourse-new topic . 
Note that the determination of the sentence topic is distinct from the question of how to realize the salient Cb/topic  ( e . g . as a dropped or overt pronoun or full NP ) . In the MT domain , this can be determined by the referential form in the source text  . This trick can also be used for accommodating inferrable or hearer-old entities that behave as if they are discourse-old even though they are literally discourse-new  . If an item that is not ; in the discourse model is nonetheless realized as a definite NP in the source text  , the speaker is treating the entity as discourse -old  . This is very similar to ( Stys/Zemke ,  1995 ) ' sMT system which uses the referential form in the source text to predict the topicality of a phrase in the target text  . 
3.2 The Focus Algorithm
Given the rest of the semantic representation for the sentence and the discourse model of the text processed so far  , the following algorithm determines the focus of the sentence  . The first step is to determine presentational focusing of discourse-new information  . Note that the focus , unlike the topic , can contain more than one element ; this allows broad focus as well as narrow focusing  . If there is no discourse-new information , the second step in the algorithm allows contrastive focusing of discourse-old information  . In order to construct the alternative sets , a small knowledge base is used to determine the semantic type  ( agent , object , or event ) of the entities in the discourse model . 
1. If there are any discourse-new entities ( i.e.
not in the discourse model ) in the sentence , put their semantic representations into focus ,  2 . Else for each discoursentity realized in the sentence  ,   ( a ) Lookup its semantic type in the KB and construct an alternative set that consists of all objects of that type in the discourse model  ,   ( b ) If the constructed alternative set is not empty , put the discoursentity'seman-tic representation i to the focus  . 
Once the topic and focus are determined , the remainder of the semantic representation is assigned as the gronnd  . For now , items in the ground are either generated in between the topic and the focus or postposed behind the verb as backgrounded information  . Further research is needed to disa . m-biguate the use of the two possible word orders . 
Furthere search is also needed on the exact role of verbs in the IS  . Verbs can be in the focus or the ground in Turkish  ; this cannot be seen in the word order , but it is distinguished by sentential stress for narrow focus readings  . The algorithm above works for verbs since I place events that are realized as verbs in the sentence into the discourse model as well  . l to wever , verbs are usually not in focus unless they are surprising or contrastive or in a discourse -initiM context  . Thus , the algorithm needs to be extended to a ( : comnaodate discourse-new verbs that are nonetheless expected in some way into the ground component  . In addition , verbs often participate in broad focus readings , and fllrther research is needed to account for the observation that broad focus readings are only available in canonical word orders  . 
3.3 Examples
The English text in ( 5 ) is translated using the word orders in ( 6 ) following the Mgorithrns given above . In (6) , the numbers following T and F indicate the step in the respective algorithm which determined the topic or focus for that sentence  . Note that the inappropriate word orders ( indicated by # ) cannot be generated by the algorithm . 
(5) a . Pat will meet Christoday.
b . There is at Mk at four.
c . Chrisis giving the talk.
d . Pat cannot come.
(6) a .

Bugiin Pat Chris'lebulu ~ a cak . ( AdvSOV )
Today Pat Chris-with meet-flit . ( T : 3, F ~ I ) D 6 rt de birko nu ~ mavat . ( AdvSV , #SAdv V )
Four-Lotone talk exist . ( T : 3, F : I ) c . Konu ~ mayl Chrisw ' . riyor . ( OSV , #SOV )
Talk-Ace Chrisgive-Prog . ( T:I,F:2) d.
Patgele miyecek . ( SV,@VS)
Patcome-Neg-Fu; . (' F : 2 , F : I for the verb ) The algorithms can also utilize long distance scrambling in  3~rkish   , i . e . constructions where an element of an embedded clause has been ex-order to play a role in the IS of the matrix clause  . 
For example the b sentence in the following text is translated using long distances crambling because " the talk " is the Cb of the utterance and therefore the best sentence topic  , even though it is the argument of an embedded clause  . 
(7) a . There is a talk at four.
b . Pat thinks that Chris will give the talk.
(8) a . D6rt de birk on u~mavar.(AdvSV )
Four-Lotone talk exist.

Konu+mayhPat\[Chris'ineiverecegini\]Taik -AcciPat\[Chris-genci givc-ger-as-a<:c\] samy or  . (O281\[S2V2\]V1) think-Prog . ( T:I , F : I )   4 Conclusions In the machine translation task from Fnglish into a " free " word order language  , it is crucial to choose the contextnally appropriate word order in the target language  . In this paper , I discussed how to determine the appropriate word order using contextual information in translating into Turkish  . I presented algorithms for deterndning the topic and the focus of the sentence  . These algorithms are sensitive to whether the information is old or new in the discourse model  ( incrementally constructed from the translated text  )  ; whether they refer to salient entities ( using Centering Theory )  ; and whether they can be contrasted with other entities in the discourse model  . Once the imformation structure for a semantic representation is constructed using these algorithms  , the sentence with the contextually appropriate word order is generated in the target language using Multiset CCG  , a grammar which integrates syntax and information structure  . 

Eser Emine Erguvanh .  1984 . Thel , ' uuction of Word Order in Turkish Grammar . University of California Press . 
Barbara Grosz and Aravind K . Joshi and Scott Weinstein .  1995 . Centering : A Framework for Modelling the Local Coherence of Discourse  . 
Computational Linguistics.
Haji`ov & Eva , Petr Sgall , and lianaSkounm , low t1 1993 . Identifying Topi ( : and Focus1 ) yan Auto=marie Procedure . l ' r occ c dings of the , % , : th Coat-ference of the Eurol wan Chapter of the As  . soci-ali(m for Computational Linguistic . < Berylt Iott ' man .  1995 . Integrating FrecWord O > der Syntax and Information Structure  . t ' roeced-ings of the European Assoeiation for Com  . puta-tiou , I Linguistics ( I '; ACL ) . 
Beryl Hoffman .  1995 . 7 he Computational Anah , l-sisof the Syntax and Inte ~ Tnvtatimt of "\[ i' , ' ee '; Word Order in Turki . ~ h . Ph . I ) . dissertation . 
1 RCSq > ch Report 95-17 . l ) ept , of ( ~ on , puter and Information Science . \ [ Miversil ; y of I ' eJm syl-vania . 
Beryllloffman , to appear 1996 . Word Order , in-fbrmation Structure , and Centering in Turkish . 
Centering in Discourse . eds . F , llcnI'rin<:e , Aravind . loshi , and Marilyn Walker . Oxford ( hal-versify I ) ress . 
Ellen F . Prince . The ZPG Letter : Subjects , l ) ef-initeness and Information Status . Discourse descro ~ tion : diw'rse analyses of a\] ) rodraising t , e . v t , ed s . Thonrl)son , S . and Mann , W . 
Philadelphia : , lohn Beujamins ILV . pl ), 2( ,) 5325 .  1992 . 
Matsl . ooth .  1985 . Association with l , ' o-cus . Ph . D . Dissertation . lJ niversity of Mas-sachusel ; t,s . Amherst . 
Mark Steedman .  1985 . Dependencies mid<:o ordi-nation in the grammar of l  ) uteh and Englislr , 
Language , 61:523568.
lalf Steinberger .  1994 . Tr<mting Free Word Order in Ma <: hine Translation . Coling , Kyol , o , Jal 0nl\] . 
Malgorzata E . Stys and Stefan S . Zemke .  \] 995 . In : corporating l ) is course Aspects in English-l > olish MT : Towards Robust Implementation  . Ic cc nl
Advanees in NLI'.
)rnit Turan .  1995 . Null vs . O w'~r t , 5'ubjer:ls i ~ 7lrkish Discourse : g Centering An ~ dysis . Uni:versil , y of Pennsylvania , Linguistics l > h . l ), dissertation . 
Fmric Va . llduvL 1992 . Thel'n formational Corn . po-rtent . New York : Garla , d . 

