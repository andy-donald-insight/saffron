Context-Based Spelling Correction for Japanese OCR 
Masaaki NAG ATA
NTT Information and Communication Systems I , aboratorics
1-2356 Take , Yokosuka-Shi , Kanag ~ wa , 238-03 Japan
naga % aent tnly , isl.ntt.jp
Abstract
We present a novel spelling correction
method \[' or those languages that have
node limiter between words , such ~ rs
, lap ; mese , (.', hinese ,, ~ ndThM.It con-
sists of anal ) proximate word match-
ing method and an Nbest word seg
mental on Mgorithm using a statistical
la . nguage model . For OCR errors , the
proposed word-based correction method
outperf . or nrs the conventional charact m '-
b ` ased correction method . When the
bmselme charactere cognition accuracy
is 90%, it achieves 96.0% character
recognition accuracy and 96.3% word
segmentation accuracy , while the cilar-acter recognition accuracy of cilaracter-b  , ased correction is 1 Introduction Automatic spelling correction research dates t  ) ack in the 1960s . ~ lb day , there are some excellent academic ~ nd commercial spellcheckers available \[' or English  ( Kukich ,  1992) . However , for those languages that have a different morphology and writing system from English  , spelling correction remMns one of the signill cant unsolved researcil problems in computational linguistics  . 
'\[' heb , asic strategy for English spelling correction is sitnple : Word boundaries are defined by white space characters  . If the tokenized string is not found in the dictionary  , it , is either a nonword or an unknown word . For a . nonword , correction candidates axe generated t ) y approxi-nm . tely matching the string with the dictionary , using context independent word dismice measures such  , as edit distance ( Wagner and l , ' is cher , 1974; Kernighan et M . , 19 q0) . 
It is impossible to apply these " isolated word error correction " techniques to Japanese in two re`asons : First  , in noisy texts , word tokenization is difficult because there are node limiters between words  . Second , context-independent word distance measures ~ re useless because the average word length is very short  ( < 2 )  , and the chnra . cterset is huge (> 3000) . There are a large number of one edit distaalce height  ) ors for a , lapanese word . 
In English spelling correction , " word bound a . ryproblem ", such as splits ( forgot-~ . lot gol ) a . n drun-ons ( inform --+ in . lbrm ) , mad " short word problem ' ( ot-~on , or , of , at , it , to , etc . ) are also known to I ) every dil Iicult . Context inf ofmaton , such as word Ngram , is used to supplement he underlying context -independent co > reelont br these problematic examples  ( GMe and ( ~ hurch , 1990; Mayseta J . , 1991) . To the contra . ry , Japanese spelling correction must be essentially context-dependent  , because Japanese sentence is , as it were , a . run on sequence of short words , possibly including some typos , something like ( lfor- . qolol info'mn you--~I forgot to inibrtn you ) . 
In this pa . per , we present a novel ~ t ) proach for spelling correction , which is suite . hie for those l~n-guages that have no delimiter between words  , such f~saN ) anese . It consists of two stages : First , MI substrings in the input sentence are hypothesized ms words  , and those words that approximately matched with the substrings axe retrieved from the dictionary ms well  , as those that exactly matched , l , ased on the statistic M language model , the Nd ) est word sequences are then selected as correction ca  , ndidates from all combinations of exactly and approximately matched words  . Figure 1 illustrates this ~ p proach . Out of the list of characte recognition candidates for the input sentence " ~  b~R~7-~  , ~Y2~g ) k  ~ 79o " which means " to hill out the necessary items in the application form  . " , the system searches the eombi-n , ~ tion of exactly matched words ( solid boxes ) and a pl ) roximately matched words ( dashed boxes )   1 The major contribution of this paper is its solutions of the word boundary problem and short word problem in Japanese spelling coffee  . 
tion . lly introducing a statistical model of word tOCR output tm  , ds to be very noisy , est ) e(:ially for handwriting . To (: omt ) ensate for this bd , avior , OC'Rs usuMly output at , ordered list of tit (! bestNelutra (: ters . 
The list of the ( : ~ u Mid~tes for an int ) ut string is called et l ~* ra ( : term ~ mix . 
8 06 input sentence character matrix 171 i ~ dH\[:?~K , i  ~ , - ~ d tfJlitt41Dp\]X . - ~ t7~-t7j~h . ~(7b ,, 7 forward search\[\]T_~K-\[\]
A5 ~ J3 exactly matched word i .   .   .   .   . I approximately matched word .   .   .   .   .   . ? I " igure 1: l ' ossihle(1 oinl ) hia , l , ioliSorl : , Xal Cl , lya , ndAtf l ) roxhua , l , ely~l\]n , l , ched Words Imigl ; ha , i idsl/ell hig , l , lie proposed sysLei\[la , Cctira , t , elyphi , cesword bound a , rics in noisy LexLs1 , 1u/ , Lilicludeli Oll-words n , nd t ll lkl/O Wll words . Ily using t , he cla , ra , cl , or I ) ased CC~l\]l ; 0xl ; lilode\] , i l ; a , c(:ura , 1; elyselecl , s (: or r0?1 , iOll(umdid ~ l . es\['orshorL words\['i'Ol\[iL he h~rgentl l l l l  ) cro\[' . ~ p proxini ; ~ l ; elyill ~ l , L(tiletIworcts wiLhL heslmlcedit , disl ; n , nco . 
The goldo\[our project , isl , oiniplenienl , a , iIh'li , ~ r ~ ci . ive word correcl , or for ~ lia , ndwri Li , m ~ FAXOCI , sysl , eili . ldT ~ a , reespecia , lly in l ; eresl ; ed in 1; exl , s t , lia , l , include a , ddresses , IHI , II ICS , ~ l , lld\[iiessa , ~ es , such as order fOrll lS , ques Lionnn , ires , a , n d t , ch~gi'ig ) h . 
2 Noisy Channel Model for
Charac Ler Recognition
Firs L , we ( \] lrniula , l , ei ; hc , spell in T ; c(~rrcct ; iono\['O(2I~<~i ' rors in I , he noisy cha , nnclpa , r ; ~ div ; ln . I , el , (< ropresenL1 , tie in puLs L rhlga , ncl , \ rct > resenl , l ; tie ( ) ( Jlt , oul ; Imt , s t , r h i g . l " hid hig Lliefllos Ltm ~ lmblesf , i ' higC < given iJleO ( ill , oul , ptll , <\" a , nioulil , sI ; oma , xiniizili ~; u , el ; , , ,   , ~ , io , , r ( x IC ) / - ' ( ( ; ) ,  ?: =  , H  ~ , , ,   , ~n(c'lX ) = , ~ ,  . ~, IID , X\])(X\[~ . ))\])( . ;)(i ) (3' lieca , uselia , yes ' rules l ; ; d , esl ; ha , L , #'(~' l . V ) = s'(xl(: . ')#'(~: . ') ( . ~) /'( X ) /'( C ) is ca , tied the hmgu ; Lge model . II , isc . nipuled\[l'Oiltl,ra , iil higCoI'\]) CiS . I , et , uscMIP ( XI ( . <) l , lteO (', ll , niodel . ILca , it ) ocont tmLedl ' ronit ; ii ca , priori likelihoodo , gLilnn , Les for individua , Icila , ra , cl ; ers , is p(xIC ) = l-i/'":'I < , ) (: ~) where ' n is l , hesl , rhlv ; leu ~> l , h , t ' (: l : ilcT ) is c;i , IN'dLicCOlifUsionlun , l , rixo\[ch;i , ra , (tl , or < ~ . ILi , g t , i';i , hicd tishlgl , liehipu La , n doul , 1) ul , sl ; rhigso\['Lhe (   )   ( ill The coii\['tiSiOll\[lialrixish iglly depention Lelil : iie chn  , r;l , ct , err0 co~ni Liona , l ~ or it , iuna , ndLhequa , I iLy of the hlput , docunion l ,   . IL is a , lld ) or inl ; ensive 1; aM ~ Lo prepa , rea , con\[tiSiOl/niaJ ; rixlbrca , ohcha , ra , cl , errecognil Jonsys , ellt , sillce . Ia , p < ~ liose ha . slilore1; hail3,0()()clai','l,cl ; ers . ' l ' here\[7) re , we used a , shnpD()(\]11 , model wlie rel , lie confusion ni ~ t , rix is a > p proximai , edby t , tie correcl ; cha , r;~cl , erdist , ribul ; ionovm " t , her & nk of Ltieca , ndicl & Lcs . We asst ttl let ; ll ~; l , her a , nkerder disl , i ; ibul , ion of l , io correc Lclia , ra , cLore is a , geonl 0 Lric disf , ribu Lion whosopa , ra , niet , erisl ; hea:CCtll '; % cyOI " Lhefir M ; ca , n did a , Le . 
l , ei , c ~ betha7-i , hcla , ra , cl , er in L he in puLs l , r in ~4 , : l:G\])el , iejt , hca , ndida , t , e\[or (: ~ , ; uid p1) o Lifeprol > idfilii , yl , ha , i , l , helh'sl ; ca , ndida , Leiscorrect ,  . ' l'lieCOltf'tiSiOllpt'olml>ilil~y\['( ; vU\[r:i ) is a , ppro?inla , t , od as , r ( ~: , ~ l < , ,) ~ P (: , : , ~ i , ~ <:< , , . , . <:<: c ) . ~ ~ , (lp ) ~'( , 1) I'klua , l : ion(d ) a , hust , o a , Pl~roxhna , i : el , hea , ccura , cy of t , hc firs Lca , ndida . im , a , ndtJlel , endency tJmt , tj . -relia . bilit ; y of L he ca . ndhla . l ; ech'cramsesal , ' uptly as its ranlcincr(m . ~ es . For exa . mple , ill the recognition ~ ccura . cy of t , helirsi ; candida . t . ep is 0 . 75 , we will assign i , he prob ; dfilii ~ y of Lllo Iirst , , secmid ~ ; rod i , hird cn , n did a . i , csl , o0 . 75, 0 . \]9, a,i/d(I . 05, respect . ively , regli , i ' d loss ~ lI'L hehipu La , i id Ollt ; pul , cha , r~cl;ers . 
(-) 11 o ~ . ) J ' L he I : lelietil , so \[ us in < ga , siliiplc ( ) ( Itn lodel is L ha , LL hespelling correct , ions . yM , enibe coiiles hi~4hlyimh3)endenl , of l , lie underlying ; ( )( i1 cha , rax q , crisl , ics . Obviously , a , more sophislfic a LedO(\]11 , niodol would in iprove OI'FOF Col'rect ; ioli /) el " retina , liCe , hut , eVelil , hisshnlHeO(II/ilit > d<q worksfa , h'iywdlinoureXllerinient , s 72()11 ('( if ( , lit ! I ) i ' ax : l , iciilr (' , a , Stlll ~' ( Jril Sill , ~Lhc~tXJlll CIL-rh:di ~ lrilml , hJlii ~ ilia , i , well ~ cdl , hccuid ' H , HO , nlal , ri ? for ilnl ) h!i ii cnl , in ~ the O(;Rsilnlila , l , Ol- . \'\" cfcclh , i , ~ ilnfllir I , t)ii ~ t!I , hcsliillccon\[llblonlilaA , rixbtJL hftJl't!lr()ll ' ~ Clicr&l , ioli illl d error corrc , d , lon . 
7073 Word Segmentation Algorithm 3 . 1 Statistical Language Model For the language model in Equation  ( 1 )  , we used the part of speech trigram n lode l ( POS trigranlor 2nd-order HMM )  . It is used , as tagging mode\[in English ( Church , 1988; Cutting et al ,  1992 ) and morphological nalys is n lodel ( word segmentation and tagging ) in Japanese ( Nagata ,  1994) . 
Let the input character sequence be (/ = c\]c . e .   .   . c . . . . We approxinlate P ( C)by P(W ,  7') , the joint prol > ability of ' word sequence W = wlw2  . . . ' u ), ~ and part of speech sequence '\[' = tlt . e .   . , t , , . P ( W , T ) is then approximated t > y the product of parts of speech trigram probabilities P  ( ti\]ti-'2 , i-l ) and word output probabilities for given part of speech P  ( wiltl )   , i = 1P ( tilti- , e , ti - ~ ) and /-' ( w ~ lti ) are estimated \[> y computing the relative frequencies of the corresponding events in training corpus a  3  . 2 Forward-DP Backward-A * Algorithm \[/ sing the language model  ( 5 )  ,   . Japanese morp\[lo-logical analysis can be detined  , as finding tile set of word segmentation and parts of speech  ( 1~/ ,  7'' ) that maximizes the joint probability of word sequence and tag sequence P  ( W ,  7') . 
( V ?, ~') =, ~, - g , ,,~? P ( w,'J')(~)
W  ~ T
This maxinfization search can be efficiently implemented t > y using the forward-DP backward-A * algorithm  ( Nagata ,  1994) . It is a natural extension of the Viteri > i algorithm  ( Church ,  1< , ) 88; Cutting et al ,  1992 ) for those languages that do not have delimiters between words  , and it can generate Nbest morphological nalys is hypotheses  , like treetrellis search ( Soong and l\[uang ,  1991) . 
The algorithm consists of a forward dynamic programming search and a backward A * search  . 
The fbrward search starts from tile beginning of the input sentence  , and proceeds character by character . At each point in tintile sentence , it looks up the combination of the best partial parses ending at the point and word hypotheses starting at the point  . If the connection between a partial parse and a word hypothesis i allowed by the language model  , that is , the corresponding part of speech trigram probability is positive  , a new continuation parse is made and registered in the best partial path table  .  \[ , ' or example , at point 4 in Figure 1 , tile final word of the partial parses ending at 4 are gab ~ ( ' application ' )  ,   . ~(' prospect ') , SAs a word segme ot al , ionnmdel , the advantage of the POS trigram model is that it can be trained using a smaller <: or pus  , than the word bigram mode . 1 . 
and ~(' inclusive ') , while tile word hypotheses starting at 4 arem ? ~ ( ' form ' )  , ~(' s~ne ') , Y \] (' moon ') , and Fq('circle') . 
In tile backward A * search , we consider a partial parse recorded in the best partial path tat>lc`as a state in A * seareiL ' Filebackward search starts attile end of the input sentence  , and backtracks totile beginning of the sentence . Since the probabilities of the best possible remaining paths are exactly known by the forward search  , the backward search is admissible . 
We made two extensions to tile original fbrward-DP backward-A * algorithm to handle OCR outputs  . First , it retrieves all words in tile dictionary that match the strings which consist of a combination of the characters in the matrix  . Second , the path probability is changed to the product of the language model probability and the OCR model probability  , so as to get the most likely character sequence , according to Equation (1) . 
4 Word Model for Non-Words and
Unknown Words
The identification of non:words and unknown words is a key to implement Japanese spelling cot -rector  , because word identilication error severely at Dets the segmentation of neighboring words  . 
We take tile following approach for this word boundary problem  . We first tly po the size all sub : strings in the input sentence as words  , and assign a reasonable nonzero probal > ility .  \[ , ' or example , at point 7 in Figure 1 , other than the exactly and approximately matched words starting at  7 such , as ,  . g , ~(' necessary ') ,  ~ , ' ~(' necessarily ') , and alZ('pond') , wetly pothesize the sut > strings ~ , , ~ , ~ ,  ~ , @~ ,   ,   . g ,@ ~, . . . as words . We then locate the most likely word boundaries using the forward-I  ) P backward-A * algorithm , taking into account the entire sentence . 
We use a statistical word model to assign a probat > ility to each subsring  ( Nagata ,  1996) . It is defined a stile joint probability of tile character sequence if it is an unknown word  . Without loss of generality , we can write,
P ( ~ I < ~ z >) = p(c ~ . . . , : ~ I < ~ z >) = r(k)P ( , : , .   .   . , ; ~ lk ) (7) where <' . 1 ? ? ? <'+ is the character sequence of length k that constitutes word wi  . We call P ( k ) the word length model , and P ( cl ? . . ck \] k ) the spelling nmdel . 
We assume that word length probability P ( k ) obeys a Poisson distribution whose parameter is the average word length A  ,  ( . ~ __ \]  ) k This means that we think word length is the in terval between hidden word boundary markers  , which are randomly placed where tile average interval equal stile average word length  . Although key role in making tile word segmentation algorithm rot > ust  . 
Weal ) proximate the spelling probability given word length P  ( el . . . ck\]k ) > ytile word-t ) a ~ ed character trigram model , regardless of word length . 
Since there are more than 3 , 000 characters in Japanese , tile amount of training data would be too small if we divided them by word length  . 
@:~- . . "~) -- P(c ~ I # , #) P ( c = I # , q ) k z=3 where "#" indicates the word t > oundary marker . 
Note that tile word-I > , % sed character trigram model is different from tile sentence-b~Lsed character trigram model  . ' l ' hetbrmer is estimated from tile corpus which is segmented into words  . Ita , ssigns large probabilities to character sequences that appear within a word  , and small probat > ilities to those that appear across word boundaries  . 
5 Approximate Match for
Correction Candidates
As described t > elBre , we hypothesize all sul > strings in the input sentence  , as words , and retrieve ap : proximately matched words from the dictionary as correction candidates  . For a word hypoth- . 
esis , correction candidates are generated based on tile minimmn edit distance technique  ( Wag-netantil !' is cher ,  1974) . Edit distance is defined as then tiniulum number of editing operations  ( insertions , deletions , and substitutions ) required to transform one string into another . If tile target is OCIL output , we can restrict tile type of errors to substitutions only  . Thus , the similarity of two words can be computed as c / n  , where cistile nund ) er of matched characters and nistile length of the misspelled  ( and dictionary ) word . 
For longer words ( . _>3 characters ) , it is rea:s on able to generate correction candidates t > y retrieving all words in the dictionary with similarity above a certain threshold  ( et a >_0 . 5) . For exam-pie , at point 0 in Figure 1 , g + b ~ ( ' application ' ) is retrieved by approximately ntatching the string IttL ~  ; 9-with the dictionary ( c/n = 3/4 = 0 . 75) . 
I lowever , tbr short words (1 or 2 character word ) , this strategy is unrealistic because there area large numt > cr of words with one edit dis-lance  . Since the total nund ) er of one character words and two <: haracter words anlounts to luore than  80% of the total word tokens in Japanese , we cannot neglec these short words . 
It is natural to resort to context-dependent word correction methods to overcome tile short word prol > lem  . In English , ((-; ale and (\] hurch ,  199 ( t ) achieved good spelling check performance using word bigran Ls  , l lowever , in , lapanese , we cannot use word bigram to rank correction candidates  , because we have to rank them betbre we pert brm word segnmntation  . 
The ref bre , we used character context instead of word context  . For a short word , correction candidates with the same edit distance are ranked by tile joint probability of tile previous and tile following two characters in the context  . This probwbility is computed using the sentence -based character trigram model  . For 2 character words , for example , we first retrieve a set of words in the dictionary that match exactly one character with the one in the input string  . We then compute the 6 grant probability Ibrall candidate words . siSi + l , and rank them according to the prot > ability . 
P ( c,_2, ci-l , . sl , si + . t,ci+:~,ci+a):P ( . s'ilci-~, cl-t)P ( si + l\]ci4, . ' ~ i)P ( ci += lsl , . si+l)P(ci+al . si+t,ci+ . 2) (10) For example , at point 12 in Figure 1 , there are many two character words whose first character is ~ g  , such~s-gEil~('mention') , ~E~4$(' article') ,  ~0 . 
.~ (' journalist '), gg . zX . (' entry '), g0,,&~,(' commen > oration '), etc . By using character contexts , tile system select sgg ) k . anti~t\]fti ; ~ as approximately matched word hypotheses . 
6 Experiments 6 . 1 Language Data and OCR S imulator We used tile NI ' R Dialogue Database  ( Ehara et el . , 1990 ) to train and test tile spelling correction method  . It is a corpus of approximately 800 , 0 00 words whose word segmentation antipartok ' speech tagging were laboriously performed by hmu \[  . In this experiment , we used onel burth of tile ATR , Corpus , a portion of tile keyboard dialogues in the conference registration domain  . ' l'a-ble1 shows then mn ber of sentences , words , and characters for training antitest data . The test data is not included in the training data  . That is , open data were tested in the experiment . 
Tat>leit : The Amount of ' l?aining and '\[> st Data 
Training set Test set
Senten<:es 10945 lO0
Words 1500 391 134
C , haracters 2688 302097
For the spelling correction experiment , we used an OC , R simulator because it is very difficult to obtain a large amount of test data with arbitrary recognition accuracies  . The OCR , simulator takes an input string antigenerates a character matrix using a conflmion matrix for Japanese hand writing OCI  , , developed in our laboratory . The parameters of the OCRs in mlator are tile recognition accuracy of the lirst candidate  ( lirst c and k late correct rate )  , antitile percentage of tile correct the . r-candidate included rate ) . 
In general , the accuracy of current Japanese hand writing OCR is around  90%  . It is lower than that of printed characters ( around 98% ) due to the wide variability in handwriting . When the input comes from FAX , it degrades another 10% to 15% , because tile resolution of most FAX machines is 200dpi   , while that of scanners is 400d pi . The re-\['ore , we made \[ burtest sets of ' character matrices whose first candidate correct rates and correct candidate included rates were  ( 70% ,  90%) ,  (80% ,  95%) ,  (90% ,  98%) , and (95% ,  98%) , respectively . 
The average numt > er of candidate sibra character w~s  8  . 9 in these character matrices 46 . 2 Character Recognition Accuracy First , we compared the proposed word-based spelling correct or using the POS trigram model  ( POSe ) with tile conventional character I ) msed spelling e or reet or using tile character trigram model  ( Char 3 )  . Table 2 show stile character recognition accuracies after error correction \[' or various b~seline OCR accuracies  . We also changed the condition of the approximate word match  . In Tat)le 2 , Matel , Match 2 , and Match 3 represent that tilt approximate mM ; chf br substrings whose lengths were more than or equal to one  , two , and three characters , respectively . 
In gener M , tile approximate match for short words improves character recognition accuracy by about one percent  . When the lirst candidate correct rate is low ( 70% and 80% )  , tile word based correct or significantly outper IbrnL ~ tile character-based correct or  . This is because , by approximate word matching , tile word-based correct or can correct words even if the correct  , characters are not present in the matrix . When the first candidate correct rate is high ( 90% and 95% )  , the word-I>~sed correct or still outperl ` ormstile character based eorrector  , although the dit Drenee is small . 
This is because most correct characters a real ready included in them a  . trix . 
Table 2: Comparison of Character Recognition Accuracy ( Character Trigram vs . POS trigra . m)
OCR ( thou'3 70% (90%) 74 . 4% 80% (9 a %) 8 ~ . 0% ~),~% (98%) !)5 . o%
M~m:hl 84.6% ~) 2..5% 96.0%, ~) 6.~%

Match 2 Mateh 38 a . 9% 83 . 1% 92 . 0% 90 . 6% 95 . 9% 95 . 6% 96 . 7% 95 . 9% ~ The par~m/eters ~ rre scected considering the filet that the corre  . ct candidate included r ~ t t c increases a . s the tirst c and i(hm~correctrate in crc~Lscs , a . nd that NOllle correct characters ~ relev ( : r\[ ) reselltill tile I llg--trixewm if the first candidate correct  , : ~ Lt(~is high . 
6.3 Word Segmentation and Word
Correction Accuracy
First , we deline the performance mea , sures of Japanese word segmentation and word correction  . 
We will think of ' tile output of tile spelling e or-rector ~ a set of  2-tuples   , word segmentation and orthography . We then compare tile tuples contained in the system's output to tiletuptes contained in the standard analysis  . For tile Nbest candidate , we will make the union of tile tuples contained in each candidate  , in other words , we will make a word lattice from Nbest candidates  , and compare them totile tuples in the standard . 
For comparison , we count tile number of tuples in tile standard ( Std )  , the number of tuples in the system output ( Sys ) , and tile number of matching tuples ( M ) . We ' then calculate recall ( M/Std ) and precision ( M/S ys ) as accuracy measures . 
We define two degrees of equality among tuples for counting the number of matching tuples  . For word segmentation accuracy , two tuples are equal if they have tile same word segmentation regardless of orthography  . For word correction accuracy , two tuples are equal if they have the same word segmentation and orthography  . 
Table 5 shows the words segmentation accuracy and word correction accuracy  . The word segmenration accuracy of tile spelling e or rector is sig-nitieantly high  , even if the input is very noisy . 
For example , when the accuracy of the baseline OCI . is 80%, since tile a . verage numlmr of characters and words in the test sentences are  20  . 1 and 11 . 3, there are 4 . 0 (=20 . 1'(1-0 . 80)) chm'ac-tee errors in the sentence , in average . Ilowever , 94 . 5% word segmentation recall means that there are only  0  . 62 (=11 . 3'(1-0 . 945 ) ) word segmenta tions that are not found in the first candidate  . 
Moreover , we t > el the word correction accuracy in Table 3 is satisfactory \[' or an interactive spelling corrector  . For example , when the accuracy of the b~seline OCI is 90% , there are 2 . 0 (=20 . 1"(1 0 . 90)) cha . racter errors in the test sentence , l lowever ,  92 . 8% reca . ll for the first candidate and 95 . 6% recall for tile top 5 candidates means that there are only 0  . 81 (11 . 3"0-0 . 928 ) ) words that are not found in the lirst candidate , and if you exa . mine the top5 candidates , this w due is reduced to 0 . 50 (~1 . 3'(1-0 . 9S @) . That is , about half of the errors in the lirst candidate are corrected by simply selecting tile alternatives in the word lattice  . 
7 Discussion
Previous worksoil Japanese OCR error correction arel  ) ased on either the character trigram model or tile part of speech t  ) igram model . Their targets are printed characters , not handwritten characters . 
That is , they assutne the underlying OCI . 's accuracy is over 90% . Moreover , their treatment of unknown words and short words is rather adhoe  . 
810' l'a , ble 3: Word Segmenta . tion Accura , cya , ndWord(Jorrection Accuracy for Noisy Texts
O(:117 o % (9o %) 8o % (9~%) 9o % (:) 8%) 95% (98%)
Wor(l Segt nent ; ~ tion
R(x:M1(llest-5) l)re<:ision(l\]est-5) 89 . o % (9e . 1%) ~ . a % (752%) 94 . 5% (97 . 4%) 90 . 5% (81 . 7%) 96 . a % (97 . 9%) 9a . (~% (85 . s %) 97 . 3% (98 . 6%) !\]4 . 8% (86 . 8%)
Wet((-' , or e(:th)n1c(:all(l\]est-5) Prc . (: ision ( lt:st-577 . 1% (82 . 4%) 71 . a % (58 . 2% 87 . 9% (92 . 6%) 84 . 2% (67 . 2% !\]2 . 8% (95 . 6%) 90 . 1% (72 . 1% 94 . a % (!\]7 . 0%) !1 . 8% (74 . 0% (' l'Mmo and Nishino , 1989) used 1) ~ u ' t of speech bigra , ma , nd best\[irsl , sea+rob for ( ) C , I , correction . 
They used heuristic templal ; es\[Lrttnkllownwords . 
(11; oa , nd Ma , rtty , ' tma ,, 1 . ()92) used pa , rt of speech I ) igraan a , nd\]lea , In search illorder to get , niultiple c , ' mdida J , es in their int ; eracl ; ive0(-:11 , correcter r , The proposed Ja , paa\]es espelling correction meLh . od uses pa , rt of speech trigra , m ; rodNbest sea , reh , This (: oml > in a , l , ionisl , heoretica , llya , ndpra , ctica , llyiilore ; l , CCtlr ; l , Le (;\ [ liLII previous reel , hods . 
In addition , t > y using sl , a , t;istiea , I wordn to del , a , n d c c ) l l t e X t ; I > a , sedn , l)lm)xin\]a , l , cword\[na , l , ch , i l , t ) ecomes robusten ottgh ; otm ~ dlevery noisy texts , such a , stheottl , puto\['FAXO(111 , systet ns . 
To improve the word correction a , ccuraey , more powerful hmgua , ge models , stte has word bigram , are required . ( Jelinek , 1 . ( . ) 85) pointed out that " I )() S(pa , rt of speech ) el assilie a , tion is too crude a , nd not necess a , rily suited 1 , ola + ng tutge modeling " . 
I lowever , il ; is 1; c ) o expensive to prep a , rea , la , rgem , ~ nua , lly segmented (: or t ~ , t t s ( ) f e ; t ch l , a , rget do Ilia , illL()(:O\[llpute the word1)igra , m . ' l ' her < q ' or e , we a , rethinking o\['ran , kinga , set\["orga , tfized word segme ni ; aJ , i on method I ) y gener Mizing thel " or wm'd Ibtek wa , rda , lg or it lml \[' or those hmgua , gest ha , tha , veno delimiter between words ( Na , ga J , a , , 199(i ) , 8 Conclusion Weh ; tve present ; ed~spelling e or recl , ionmet , hodtbr noisy , la , pa , nese texts . Wea , recurrently I > uilding a , n inter a , ctive Ja , pa , nesespelling corrector j spcll , where words are the I ) msic object : ma . 
nipuhtt , ed1) yt , he user in ope\]'~l ; ions such as repla . ee , a , ceept , and edit . It is something like the Ja , pa , nese countert ) a , rt of IJ nix's spelling correcter is pell , with a , user interf~tce similar to kan ( t-lo-ka ' njZ converter , a , popu\[a , rJa , pa , nese in pul , method ~ A(:<: ording to Fig . 6 ill(~\]'a , k ; to and NM tim ) ,  1989) , they achieved it i ) Olll , 95 (~1(: ha , ra < : tcrI'C ~ : Og , tlil , io II&(:CII-r ; t(:y when , IraI ) ms (' . l l n r . ~L(:cllr,~l(:yiS9\[% forill ; tga . -7 , in es~tndint , ro(\]ll(:lA)ryt(!xl , 1) ooks of scien (: (! and t , e(:ll-no logyd mu ; tiu . According to TM ) Ic . I in (11 , o ~ tnd Ma , ruya , tn~t ,  1992) , they it chicv cd 94 . 61%, < ha , la , < . : tcrI't ? (: O , ~ , ll\[LiOlla+c(:tu'+t<:y when , it ) a , selinc ~ tc (: ur + ~< : y is 87 . 46%\[m " pal , el LSh , uhx:tri <: c . gilme . rlng , dora ; tin . We~t(:hit:vcd 9 fi . 0% c\]l . + trltci , crJ'e(:og . il , ion + ~(: c . ra(:y , when the I ) + ~ ell . ca+(:c . r + ~ cyi ~ 90% in thucunf (' . rcn < : croy < is tr~tLion doma . i . . It is very ( l if l ic,lt to c, . ) nlt)a+v . ! our rusu \] ts with thu previous rcsUlll\[ , ~I > (: ca , ust ! t , \] l(' , expuri-merit <: on ditio . sa , rc(:Oml ) h:Lt:ly dill'rxenL . 
for the AS(3lkeyt > oaa'd.
e\['erel\]ces
Kt ! rltl(!thW .  ( . ~, lll r(:h .  1988 . AS to <: ha . sti(:P+u't ~ Pro-g ,, ' am+u . lNo . n Phra . seP ~ H's, . : I " for IJ lll'(!s , rlcIA ' dq'ext , , \] n lJrocc(:dng . ~( ffANI,tLSb;,t ) ~ tges 136-143 . 
I ) oug (' , utting , Julhul Kut)ie(: , J ~ ulPudersen ~ , + tn(lIJcllelot)cSibun .  1992 . At ) llaj cth:+tl\[)a , nlt-o\[-Sl ) eu(:h~l'a+gger , Int ) roc('+:ding . ~(ffAN 1,1)-92, I)a+gcs 13; 1-140 . 
q ' erumasa , lCl ; u % Kunl , a Ju Ogura , Tsuyoshl Mori-mol , u .  19!0 . ATII)ia ,\] oguc\[)atal > asu . 1, lq'o('ccd-i . , g . ~ of ICSI , P , \[m ; q , es1 (193-109 ( J . 
Wi\]li:-llnA . (\] a \] l . ~a + ndI < r . n . et ll W . ( ~ lill r(:h .  199(\] . Pooll'~sthna , tcs of C , on tcxt are Worse . th;LnNora : . litPro-ccc(liT*ys of I ) AHPAA ' atur , ILan(j'u , gcm*dSt ) etch
Workshop , 1) + tgcs 28a-287.
M~trkD . I < crnigha , n Kenneth W . Chur(:h , and Willi ~ un A . ( ~ th ! .  1990 . A Spelling Correction Pro-gr ~ un Based o . a , Noi . sy(-lh . ; ut nel Model . Inl'roccc(l-in(jsof(701 , 1N(;-90 , l)~t gus2 (-15-210 . 
K~u'r . nKuki <: h .  111!12 . ' l'c cll niqurs For Aut , omnth : a . lly Col'ree Ling Words in Text . A(:M(/omlmlin9,%+r-O(:g . '? ~ VoI . L ) 4, No . 4, I ) ~ t ~, (: s;\]77-4119 . 
Nol ) uyms . \[ to ; ? lldl\[iroshiM : ? rilya , lllat .  19!12 . A , Method of \]) u . te ( : tingaim ( hJr recLing l?t ' rors in the tesults of Jitl ) a+ne: . + eOCR . In Tra . nsacliono\]In \] otto+ilionPro-f'??S , 'fi~L ~ ( J ~ ' OC + dll I of , /f + pfllZ , Vc ) l . ;\], ~, No . 5, J ) a . gC ? SJJ4-670 ( in J+q ) + tnese) . 
I"re(h!rh:kJe . line k + 1985 . Self-org+uli/,(!dl , a, . :e , u + ~ gcMo(h ! ling , \[ o17 St ) et !(: hRtx:ognition . IBM t , . :l)Od, . 
I ", rh:M + tys,I"rcdJ . J ) + uncratlt , aJMl/obt:rtI, . Mcr-(:er .  1991 . C , tmtcxtl + Ls (!( lSpelling (', or n't ! ctio . . / n-Jormalion l)ro(:cssing ~',; M,nagcmcnl , V . I . 27, Nt ) . 5,
I ) ~ g(!s517-522.
Masa ~ tki N + tgati,~t . 1!!t 4 . AStO(:Ua , M , i (: , \] ; ~\[ ) ttlcSCM or t ) hoh ) gl calAmdyzer Using itl " or wa , r ( l-l ) l ~ lack w ~ r ( 1-/t*NBestSe+tr ( : h Algorithm . Int ? occ(:d-inys of ( JOI , 1N (7-9~, l ) a # . e , s201-207 . 
Ma , , , + t+tki Nitgi LL+L 1996 . alui , on l ~ t t i < : l ~ X l , l';-:t(:lion of New Words from J~tl ) + tneseq'exl , su , , , ingGc , uraJiz cdI " or wa . r(1-1 b t c k w ~ u ' d ~ q e ~ t r < : h . q ' o att ) l )( , . it rill l ) r . ' wccd-my . ~ of EMNI , I' . 
I"r ~ nkK . , qt ) on Z~tndl'\]n,e--l " oulgllu~tng .  1991 . A Ti'cc-Trellislbk , ~ udl"+tstSuar<:h for I " indi , g the Nlusl Senten(:e . llyl ) otheses h!(hmtinuousSpeechIe(:og-nil , ion . l . /) roccc<ling*of1(7ASSILOI,I)a,g:sT05:7(IS . 
" l'el , suya , k~m+~l ~ ( lFunlihi LoNiMfino .  1989 . lml ) h>m ( , . nt ; tl , ionatit ( IEwdui ~ tlon of Post-processiug for Ja . l)aJl(!St!I)O(:lltncnL\] . (! a , ( l, . : u's . lit ~' nns . (: li,n*o/hffo,'m++li out'l'occ . ' t , ? i ? + fj , qo (: icly of Japeln , Vol . 30, No . lI , l ) a , ges13 !) 41401 (1 . , \]; Ltla . llt : Se ) . 
iol ) erl , A . W ; ~ z , u r + ul ( IM iclut(!l . I . F is (: her .  1974 . q'he~l , rlng-l , tr . , qtring C , or rr . cl , ionProbh!m . In , h ) mrud of thc , 4UM , Vol . 21, No . I , t ) a + Zes 168-173 . 

