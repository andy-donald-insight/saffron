Estimation of Stochastic Attribute-Value Grammars using an 
Informative Sample
Miles Osborne
osborne ~ let . rug . nl
Rijksuniversiteit Groningen , The Netherlands *
Abstract
We argue that some of the computational complexity associated with estimation of stochastic attribute-value grammars can be reduced by training upon an informative subset of the full training set  . Results using the parsed Wall Street Journal corpus show that in some circumstances  , it is possible to obtain better estimation results using an informative sample than when training upon all the available material  . Further experimentation demonstrates that with unlexicalised models  , a Gaussian prior can reduce overfitting . However , when models are lexi-ealised and contain overlapping features  , overfitting does not seem to be a problem , and a Gmlssian prior makes minimal difference to performance  . Our approach is applicable for situal ; ions when there are an infeasibly largemnnber of parses in the training set  , or else for when recovery of these parses fl'om a packed representation is itself comi  ) utationally expensive . 
1 Introduction
Abney showed that attribute-value grammars cannot be modelled adequately using statistical techniques which assume that statistical dependencies are accidental  ( Ablmy ,  1997) . Instead of using a model class that assumed independence  , Abney suggested using Random Fields Models ( RFMs ) t brattribute-value grmn mars . RFMs deal with the graphical structure of a parse  . Because they do not make independence assumptions about the stochastic generation process that might have produced some parse  , they are able to model correctly dependencies that exist within parses  . 
When estimating standardly-formulated RFMs , it is necessary to sum over all parses licensed by the grammar  . For many broadcoverage natural language grammars  , this might involve summing over an exponential number of parses  . This would make the taskeom tm tationally intractable  . Almey , following the lead of Lafferty et al suggested a Monte * Current address : osborne@eogsei  . ed . ae . uk , University of Edinburgh , Division of Informaties , 2 Bueeleuch Place , EII8 9LW , Scotland . 
Carlo simulation as a way of reducing the computational burden associated with RFM estimation  ( Lafferty et al ,  1997) . However , Johnsonctal considered the form of sampling used in this sinmlation  ( Metropolis-Hastings ) intractable ( Johnson et M . , 1999) . Instead , they proposed an Mternative strategy that redefined the estimation task  . It was argued that this redefinition made estimation eomtmtation-Mly simple enough that a Monte Carlo simulation was unnecessary  . They presented results obtained using a small unlexicalised model trained on a modest corlms  . 
Unfortunately , Johnson et al assumed it was possible to retrieve all parses licensed by a grmnmar when parsing a given training set  . For us , this was not the case . In our experiments with a manually written broad coverage Definite Clause Grammar  ( DCG )   ( Briscoe and Carroll ,  1996) , we were only able to recover M1 parses for Wall Street . Journal sentences that were at most 13 tokens long within acceptable time and space bounds on comtmtation  . When we used an incremental Minilnum Description Length  ( MDL ) based learner to extend the coverage of our mmmally written gralnular  ( froul roughly 6 ( )~ to around 90% of the parsed Wall Street . Jouriml ), the situation became worse . Sentence ambiguity considerably increased . We were then only able to recover all parses for Wall Street Journal sentences that were at most  6 tokens long ( Osborne ,  1999) . 
We can however , and usually in polynomial time , recover up to 30 parses for sentences up to 30 tokens long when we use a probabilistic unpacking mechanism  ( Carroll and Briscoe ,  1992) . ( Longer sentences than 30 tokens can be parsed , but then mn be r of parses we can recover for them drops of frapidly  )  . 1 However ,   30 is far less tlmn the maximum number l " v Vema dean attemp to determine the maximum number of parses our grammar might assign to sentences  . On a 450MIIz Ultra Spare 80 with 2 G'b of real memory , with a limit of at most 1000 parses per sentence , and allowing no more than 100 CPU seconds per sentence , we found that sentence ambiguity increased exponentially with respect to sentence lngth  . Sentences with 30 tokens had an estimated average of 866 parses ( standardeviation 290 . 4) . Without the limit of 1000 parses per sentence , it seems likely that this average would in crc , as e . 
586 of parses per sentence o111' grammar mighl , assign to Wall Stl " ec't Journal sent ; enees . Any training set we have a ( : eess to will therefore bele eess arily limit e ( linsize . 
We therefore need an estimation strategy that takes seriously the issue of extracting the  1  ) esl , per-refinance fl'om a limited size training Met  . A limited size tra . ining sol ; means one ereate ( l ) y retrieving at most nt ) arses per Ment ( mee . Although we ( : annot recover all t ) ossil ) lei ) arses ~ we ( lo lave a choice as to which llarses estimation should  1  ) ebased Ul ) On . 
Ourai ) proach to the prol ) lem of making IFM estimation feasible , ibrour highly amt ) iguous I ) CG is to seek ol\] ; an ivformativ csamt ) le and train ui ) on that . We ( lo not redefine the estimation task in a non-sl ; al~tlardw ; ~ y ,  1101'  ( lowellSea~o \] lte Carlosiln-ulation . 
We ( : all as alul ) lc informative if it 1 ) oth leads to the select ; ion of a 111ollol that does not mlder titor overfit , and also is typical of t'utm'e samples , l ) esl ) it c()lie's intuitions , an infornmtive saml ) le might be a prol ) er subset of the fifll training set . This means that estinlation using the int ' or nmtiv  (  ; sample might yield 1 ) ette results than estimation using all of the l ; rainhlg Met ;  . 
The ): (; st of this 1) a per is as t bllows , l , ' irstly we introduce RFMs . Then we show how theyn lay bees l ; imated and how an infbrmative saml ) le might 1 ) e identified . Nexl; , we give details of the , a (; tribute-vahlegramnlar we use , all ( tshow \] lOW we ~ oat ) otlt modelling it . We then i ) resent two sets of experi-mel ) ts . The first set is small scale , and art ! de . signed to show the existent ; e of ; minti ) rmative sample . The second ski of CX l ) erill l ( ; llI , Sal'e larger in scale , an ( 1 build upon the COml ) utational savilg Sweareal ) le to achieve using a probabilistic Unl ) acking strategy . 
They show how large me ( Ms ( two orders of magnitude larger than those reported by Johnson ctal  ) can 1 ) e estimated using the l ) arsed Wall Street . lour-hale or t ) us . Overlitting is shown to take place . They also show how this overfitting can be ( partially ) reduced by using a Gaussian prior . Finally , we end with SOllle COlllll lellt SOil Ollr WOl . k . 
2 Random Field Models
Here we show how attribute-wf lue grammars may be modelle  ( 1 using RFMs . Although our commentary is in terms of RFMs and grammars  , it should t ) eol ) -vious that RFM technology can be applied to other estimation see  . narios . 
Let G be an attribute-value grammar , D the set of sentences within the string-set defined llyL  ( G ) and ~ the union of the set of parses assign e ( 1 to each sentence in D by the gram nm rG . A Random Field Model , M , consist of two coral ) orients : a set of features , F and a set ; of ' wei . qhts , A . 
l ? eatures are the basle building blocks of RFMs.
They enable the system designer to spccit ) ~ ; liekey asl ) ects of what it ; takes to ditferentiate one 1 ) arse from a 11 other parse . Each feature is at'l metion from a 1 ) arse to an integer . Her ( . ' , the integer value associated with a feature is interpreted as then mn-ber of times a feature ' matches '  ( is ' active ' ) with a parse . Note feature should not be confllsed with features as found in feature-value t  ) undles ( these will be called at l ; ril ) utes instead ) . \] Peatures are usually manually selected by the sysl  ; e in designer . 
The other component of a RFM , A , is a set of weights , hffornmlly , weights tell it show ti ; atures are to be used when n lodellillg parses . For exanlple , an active feature with a large weight might indicate that some parse had a higtl prolmlfility  . Each weight A i is associated with a that m'e f i . Weights arc ' real-valued nm nl ) ersan ( l ~ : H'O autonmtically deternfined 113: an estimation process ( for example using hnproved Itera-tire Scaling ( Lafl L'rty et al ,  1997)) . One of the nice l ) roI ) erties of Rl . i ' Ms is that 1111o likelihood fiuw ? ion of a RFM is strictly concave . This means 1; hather e ~/ t " e . 11 oh ) callllillillla ~ and sowc can be , l ) esure that sealing will result in estinmtion of a  11  . 1,'54 that is glol ) ally ot ) timal . 
The ( unnormalised ) total weight of ai ) arse : c , '( J(:r ) , is a flulction of the . kfea Lures that are ' active ' on a 1) arse : k . ,/,( . ;) = ( . )) (1) i = l The prol ) ability of a parse , P ( xIM ) , is simply the result of norm ~ dising the total weight associated with that parse : 
J '(:,, IM ) = 12) z--(a ) yGf ~
The in l ; er pretation of this I ) robability depends upon the apt ) lication of tile RFM . Here , we use parse prol ) -abilities to rettect preferences for parses . 
When using RFMs for parse selection , we sin > ply select the parse that ma . ximises ~/; (:1:) . In these circumstances , there is 11 one ed to nornlalise ( compute Z ) . Also , when comtmting , /~(: c ) for comi ) eting parses , there is no builtin bias towards shorter ( or longer ) derivations , and so no need to normalise with respect oder iw ~ tion length /  2The reason there is no need to normalisc with respect to derivation length is that features can have positive  o1" negative weights . The weight of a parse will ttl crc for cnot always monotonically increase with respect to there  , tuber of active ti ~ atm'cs . 
5 87   3 RFM Estimation and Selection of the Info rmative Sample We now sketch how RFMs may be estimated and then outline how we seek out an inform a  . tive smnple . 
We use hnproved Iterative Scaling ( IIS ) to estimate RFMs . In outline , the IIS algorithm is as follows : 1 . Start with a reference distribution H , , a set of features F and a set of weights A . Let M be the RFM defined using F and A . 
2 . Initialise all weights to zero . This makes tile initial model uniform . 
3. Compute the expectation of each feature w.r.t

4 . For each feature f i ( a ) Find a weight ~ ; that equates the expectation of fiw . r . t /?, and the expectation of fiw . r . tM . 
( b ) I leplace the old value of k i with 21.
5. If the model has converged to /?, output M.
6. Otherwise , go to step 4
Tile keystep here is 4a , computing the expectations ? of features w . r . t the RFM . This involves calculating the probability of a parse  , which , as we saw fronlequation 2 , requires a summation over all parses in ft . 
We seek out an informative sample ~ l ( f hC ~ ) as follows : I . Pick out from ~ a sample of size n . 
2 . Estimate a model using that smnple and evaluate it  . 
3 . If the model just estimated shows signs of overfitting  ( with respectoan unseen heldout dataset )  , halt and output the i node l . 
4. Otherwise , increasen and go back to step 1.
Our approach is motivated by tile following ( partially related ) observations : ? Because we use a non-Imrm netric model class and select an instance of it in terlns of some sample  ( section 5 gives details )  , a stochastic complexity argument tells us that an overly simple model  ( resulting from a small sample ) is likely to underfit . Likewise , an overly complex model ( resulting from a large sample ) is likely to overfit . An informative sam I ) le will therefore relate to a model that does not under or overfit  . 
? On average , an informative sample will be % yp-ical ' of future samples  . For many reaMife situations , this set is likely to be small relative to the size of the full training set  . 
We incorporate the first observation through our search mechanism  . Because we start with small sam-pies and gradually increase their size  , we remain within the donmin of etliciently recoverable samples  . 
The second observation is ( largely ) incorporated in the way we pick samples . The experimental section of this paper goes into the relevant details  . 
Note our approach is heuristic : we cmmot afford to evahm teall  21~1 possible training sets . The actual size of the informative sample fit will depend both tile Ill  ) On the model class used and the maximum sentence length we can de ~  , l with . We would expect : richer , lexicalised models to exhibit overfitting with slnaller samples than would be the case with unlexicalised models  . We would expect hesize of an informative sample to increase as the maxil num sentence length increased  . 
There are similarities between our approach and with estimation using MDL  ( Rissanen ,  1989) . However , our implementation does not explicitly attempt to minimise codelengths  . Also , there are similarities with importance sampling approaches to RFM estimation  ( such as ( Chen and ll , osenfeld , 1999a )) . 
However , such attempts do not mi if infise under or overfitting  . 
4 The Grammar
The grammar we model with I/andom Fields ,   ( called the Ta9 Sequence Grammar ( Briseoe and Carroll ,  1996) , or TSG for short ) was developed with regard to coverage , and when compiled consists of 455 Definite Clause Grammar ( DCG ) rules . It does not parse sequences of words directly , but instead assigns derivations to sequences of part-of-speech tags  ( using the CLAWS2 tagset . The grammar is relatively shallow , ( for exmnple , it does not fltlly analyse unbounde dependencies ) but it does make an attelnp to deal with coilunou constructions  , uchas dates or names , commonly found in corpora , but of little , theoretical interest . Furthermore , it integrates into the syntax a text gramma . r , grouping utterances into units that reduce the overall ambiguity  . 
5 Modelling the Grammar
Modelling the TSG with respecto the parsed Wall Street  . \] our nal consists of two steps : creation of a feature set and definition of the reference distribution  . 
Our feature set is created by parsing sentences in the training set  ( ~ br )  , and using earl , parse to ill-stantiate templates . Each template defines a family of features . At present , the templates we use are somewhat ad hoc . However , they are motivated by the observations that linguistically-stipulated units  ( DCG rules ) are informative , trod that n inny DCG apl ) lications in preferred parses can be predicted using lexical information  . 



A1/aI ) pi : unimpededunimI ) eded PP/t ) I : by\[
P1/iml:by by N1/n:trafli:
Itrafl \]:
Figure 1: TSG Parse Fragumnt
The first template creates features that count ; lienuml ) eroftinms a . DUG instantiation is i ) resent within , 21 arse . a For examtle , Sul ) ps ~ we 1 ) arset the Wall Street Journal AP : 1 unimpeded 1y t : ralicA parse tree generated by TSGn fight lie as shown in figure  1  . Here , to s : ~ veonSlace , w c have labdled each interior node in the parse tree with TSG rule names  , and not attribut (;-valu(~bun(lies . Furthermore , we have mmota . t ( ' xl each node with the lmad w ( rd of timlh rase in question . Withinollrgl ; aill-mar , heads arc ( usually ) ext ) lM tly market . This 1110;/ , 118 W(~dol\]ot\]l ; ~v ~ to Ill & k ( ~ ; lily gllosso swl cllidentit\[ying the head of a  . local tree . With head in-foi'mtd ; io \] b we are alo/e to lexicalise models . \ Vehaa ; esuppressed taggillg information . 
For ' . xamp\]e , a \] hature(h ' J in ( ; d using this t ( ; nlplat ; might(:O1111t tho , nuinber () f times th (! we saw:


A1/at/1) 111 a1) arse . Such features r ( ~ coi'd sore (  2 ( if the context of the rule a . tp\]i (: ation , i that rule altIication8 that differ ii1 terms of how attributes are bound will 1e modelled by ( litlhren that ures . 
Our se ' ond total late creates features that al '' ~ partially lexicalised  . I ~ breach lo : altree ( of depth one ) that has a \] ? P daughter , we create a feature that counts the lmmber of times that h  ) caltree , de (: orated with the head-wo M of the I'l' , was seen in a . parse . 
Ancxmnple of such ; 1 lexicMised feature would 1e :


PI)/til:l)y3Note , all (111" fo . al;/ll'esSlll)i)r(?ss ; tllyt!l'nlillals thgtl , al)iem ' in a hcaI1 , F e (! . Lexical informa I ; ioll is in : luded when we decide to lexicalise features  . 
These feat m'cs are designed to model PP attachments that can be resolved using the head of the 

The thh'd midtinM template creates featuros that are again partiMly lexicalised  . This time , we create local trees of detth one that are , decorated with the head word . For example , here is one such feature :
AP/al:mfimpeded


Note the second and third templates result in features that overlap with features resulting fl ' omat  ) -i\]icati ( ms of the first template . 
We create the reference distribution 1~ ( an association of t ) rlal ) i \] ities with TSG parses of sentences , such that the trobabilities reflect 1a . rsei ) references ) using the following process : 1 . Extra ; t some samile f ~ T ( using the al ) l ) roach mentioned in sc ( : tion 3 )  . 
2 . For each sentence in tim sample , for each l ) arse of that sent ; encc' , : Olnl ) ute the ' ( list a . ncc ' between the TSG 1mse and the WSJ refereuce parse . \]1\] our at ) t ) roach , dista . nceiscM : lfla . tc ( 1 in tcl7111s of a weighted Slltll of crossing rates , recall and 1 recision . Min in lising it maximises our definition of parse plausibility  . 4 However , there is nothing inherently crucial about this decision  . 
Auyoth c'r objective flmction ( tha J ; can l ) cret ) - r(~sent(' . (l as an CXlOncntial distribution ) couh\]1 ) e used instead . 
3 . Normalise the distan('es , uch that for some , sen-tcn(:e , tim sum of tim distances of all rt ~ cov-O , l . ' od ~\[? SGt ) al " S(~S\]\['(/1" that soii ; (! il CO , is a COllSt?tilta . cross all sent o . nces . Nornmlising in this manner ensures that each sentence is cquil  ) roballe 0"emcmber that \] FM probabilities are in terms of I a . rselir ' . fl~r' . nces , and not probability of oc-:ll rrHI eeill 8111 ~( ; or l ) llS ) . 
4 . Map then or inalised distances into 1robabili-ties   . If d(p ) is the normalised listance of TSGl / ; ~l " Sep , then associate with parse 1 ) the refer- ( race probability given by the maximum likelihood estimator : rl  ( 1 , )  ( 4 ) Our approach therefore give startial cl'e ( lit ( a 11 oil-zero reference l ) robability ) to a . ll parses in ~ z .  /2 , is thcreibr ( ; not as discontimlous as the equivalent dis-trit ) ution used by Johnson at al . We therefl ) redo not need to use simulated annea . ling o1' other numerically intensive techniques to cstiinate models  . 
4Ore'distanc(~mo . l ; ric is the same one used Iyllektom ( ltek to en ,  19 . 97) Here we present wo sets of experiments . The first set demonstrate he existence of an informative sample  . It also shows some of the characteristics of three smnpling strategies  . The second set of experiments is larger in scale , and show RFMs ( both lexicalised and unlexicalised ) estimated using sentences up to 30 tokens long . Also , the effects of a Gaussian prior are demonstrated as a way of  ( partially ) dealing with overfitting . 
6.1 Testing the Various Sampling

In order to see how various sizes of sample related to estimation accuracy and whether we could achieve similar levels of perform m~ce without recovering all possible parses  , we ran the following experiments . 
We used a model consisting of features that were defined using all three templates  . We also threw away all features that occurred less than two times in the training set  . We randomly split ; the Wall Street Journal into disjoint training , heldout and testing sets . All sentences in the training and heldout sets were at most  14 tokens long . Sentences ill the test-tugset , were at most 30 tokens long . There were 676 sentences in the training set ,   98 sentences in the heldout set and 441 sentences in tile testing set . 
Sentences in the heldout set had on average 12 . 6 parses , whilst sentences in the testing-set had on average  60  . 6 parses per sentence . 
The heldout set was used to decide which model performed best  . Actual performmme of the models should be judged with rest  ) ec to the testing set . 
Evaluation was interIns of exact match : t breach sentence in the test set  , we awarded ourselves at ) oint if the RFM ranked highes the same parse that was ranked highest using the reference probabilities  . 
Whenevahmting with respect to the heldout set , we recovered all parses for sentences in the heldout set  . When evaluating with respecto the testing-set , we recovered at most 100 parses per sentence . 
For each run , we ran IIS for the same number of iterations (20) . In each case , we evaluated the RFM after each other iteration and recorded the best classification pertbrmance  . This step was designed to avoid overfitting distorting our results  . 
Figure 2 shows the results we obtained with possible ways of picking ' typical'samples  . The first column shows the max in mm number of parses per sentences that we retrieved in each sample  . 
The second column shows the size of the sample ( in parses )  . 
The other cohmms give classification accuracy results  ( a percentage ) with respect o the testing set . 
In parentheses , we give performance with respect ; to the heldout set . 
The column marked R and shows the performance
Maxparses Size 1   6626   2   12331   3   17026   5   24878   10   39581   100   119694   1000   246686 oo 267400 
R and SCFG Ref 25 . 2 (51 . 7) 23 . 3 (59 . 0) 23 . 4 (50 . 0) 37 . 9 (63 . 0) 40 . 4 (60 . 3) 40 . 4 (60 . 0) 43 . 2 (65 . 5) 43 . 7 (63 . 8) 43 . 7 (63 . 8) 43 . 7 (70 . 2) 45 . 8 (69 . 5) 45 . 8 (69 . 5) 47 . 4 (72 . 0) 47 . 0 (70 . 0) 46 . 9 (70 . 0) 45 . 0 (68 . 7) 45 . 0 (68 . 0) 45 . 0 (68 . 0) 44 . 4 (67 . 4) 43 . 0 (67 . 0) 43 . 0 (67 . 0) 43 . 0 (66 . 0) 43 . 0 (66 . 0) 43 . 0 (66 . 0 ) Figure 2: Results with various sampling strategies of runs that used a sample that contained parses which were randomly and uniformly selected out of the set  , of all possible parses . The classification accuracy results for this sampler are averaged over  10 runs . 
The column marked SCFG shows the results obtained when using a saln ple that contained  1  ) arses that were retrieved using the probabilistic unI  ) acking strategy . This did not involve retrieving all possible parses for each sentence in the training set  ,  . Since there is no random component , he results arc fl'om a single run . Here , parses were ranked using a stochastic context freeback bone approximation of TSG  . Parameters were estimated using simple counting . 
FinMly , the eo hunnmarked Ref shows the result sol ) tained when US illg a sample that contained the over all nbest parses per sentence  , as defined in terms of the reference distril ) ution . 
As a baseline , an lodel containing randomly assigned weights produced a classification accuracy of  45% on the heldout sentences . These results were averaged over 10 runs . 
As can be seen , increasing the sainple size produces better results  ( for ca&smnl ) ling strategy )  . 
Around a smnple size of 40k parses , overfitting starts to manifest , and per Ibrmance bottoms-out . One of these is therefore our inforinative sample  . Note that the best smnple ( 40k parses ) is less than 20% of the total possible training set . 
The ditference between the various samplers is marginal  , with a slight preference for R and . However the fact that SUFG sampling seems to do ahnost as well as R and sampling  , and fllrthermore does not require unpacking all parses  , makes it the sampling strategy of choice . 
SCFG sampling is biased in the sense that the sample produced using it will tend to concentrate around those parses that are all close to the best  , parses . R and smnpling is unbiased , and , a part h'om the practical problems of having to recover all parses  , n fight in some circumstances be better than SCFG sampling  . At the time of writing this paper , it was unclear whether we could combine SCFG with R and sampling-sample parses from the flfll distribu-for i  ) robabilistic unt ) acking to be efficient , it nmst\]:ely upon some nonuniform distribution  . Unpack-ing randomly and uniformly would probably result in a large loss in computationale iiciency  . 
6.2 Larger Scale Evaluation
Here we show results using a largers alnl ) le and testing set . We also show the effects of lexicalisation , overtitting , and overfitting avoidance using a Gaussian prior . Strictly speaking this section could have been omitted fl ' om the paper  . However , if one views estimation using an informatives a mi ) leasover fit-ling avoi ( lance , then estimation using a Gaussianl ) rior Call be seen as another , complementary take on the problem . 
The experimental setup was as follows . We rall-domly split the Wall St , reel : Journal corpus into a training set and a testing set  . Both sets contained sentence . st ; hat were at most 30 tokens hmg . When creating the set of parses used to estimate I i  . FMs , we used the SCFG approach , and retained the top 25 parses per sentence . Within the training set ( arising Dora 16 , 200 sentences ) , there were 405 , 020 parses . 
The testing set consisted of 466 sentences , with an average of 60 . 6 parses per sentence . 
Whenevahm till g , we retrieved at lllOSt 100 lmrscs per sentence in the testing set and scored them using our reference distribution  . Aslmfore , we awarded ourselves ai ) oin l ; if the most probable testing parse ( in terms of the I/ . MF ) coincided with the most t ) rol ) -able parse ( in terms of the reference distribution )  . In all eases , we ran IIS tbr 100 iterations . 
For the tirst experiment , we used just the first teln p \] at (' . ( features that rc'la . t( ; d to DC ( I in sl ; antia-tions ) to create model l ; the second experiment uso . d the first and second teml ) lat ( ~ s ( additional t'eat m'o . s relating to PP attachment ) o create model 2 . The linal experiment used all three templat ( ' ~ s ( additional fea , tllres that were head-lexicalised ) to create model 3 . 
The three mo(lels contained 39 , 230 ,  65 , 568 and 278 , 127 featm : es respectively , As a baseline , a model containing randomly assigned weights achieved a  22% classification accuracy . These results were averaged over 10 runs . Figure 3 shows the classification accuracy using models 1  , 2 and 3 . 
As can 1) e seen , the larger scale exl ) erimental results were better than those achieved using the smaller samples  ( mentioned in section 6 . 1) . There a-Sell for this was because we used longer sent c  , 11ces . 
The . informative sainple derivable Kern such a training set was likely to be larger  ( more representative of 5Oo > ,  ~ , 48 o < ~'\ .  , / ' , - - - " '  .   .   .  '"  .   .   .   .   . . model1 .   .   .   .   .   .   .   . 
"ff ~" ~ ~ L model 2......
/  .   .   .   . , ' ~ model 3 .   .   .   .  _ ,, ,,,,-  .   .   .   .   .   .   .   .   .   .  ~_ \ /  .   . ~ \ 10 20 30 40 50 60 70 80 90 100

Figure 3: Classification Accuracy tbr Three Models
Estinmted using Basic IIS 56 -- ~ rr\]l 1 l 1   7  ~:  4a   model2   . . . . . 
0 10 20 30 40 50 60 70 80 90 1O 0

Figure . l : Classification Accuracy for . \[ hre ( . Models Estinmted using a Gmlssian Prior and IIS the population  ) than the informative sample deriv-al ) led from a training set using shorter , less syntat'-tically ( Xmll ) lexsenten (: es . With the unle . xicalised model , we see (: lear signs of overfitting . Model 2 overfits even more so . For reasons that are unclear , we see that the larger model 3 does not ai ) pem : to exhibit overtitting . 
We next used the Gaussian Prior method of Chen and Rosenfeld to reduce overfitting  ( Chen and Rosenfeld , 1999b ) . This involved integrating a Gaussian prior ( with a zero mean ) into Ills and searching for the model that maximised the  , product of the likelihood and prior prolmbilities  . For the experiments reported here , we used a single wlri-ante over the entire model ( better results might be achievable if multiple variances were used  , i ) erhaps with one variance per teln l ) late type ) . The a et llal value of the variance wast ' cmnd by trial-and-error  . 
I to we ver , optimisation using a heldout set is easy to achieve  ,  . 

We repeated the largescal experiment , but this time using a Gaussian prior . Figure 4 shows the classification accuracy of the models when using a 
Gmlssian Prior.
When we used a Gaussian prior , we fotm d that all models showed signs of imt ) roven mnt ( all be it with varying degrees ) : performance ither increased , or else did not decrease with respect to the munber of iterations  , Still , model 2 continued to underperform . Model 3 seemed most resisten to the prior . 
It the retbre appears that a Gaussian prior is most useful for unlexicalised models  , and that for models built from complex , overlapping features , other forms of smoothing must be used instead . 
7 Comments
We argued that RFM estimation tbrbroad coverage attribute-valued grammars could be made eompu -tationally tractable by training upon an inforlna -tive sample  . Our small-scal experiments suggested that using those parses that could be etliciently unpacked  ( SCFG sampling ) was a hnost as effective as sampling from all possible parses  ( R ~ and salnplill g )  . 
Also , we saw that models should not be both built and also estimated using all possible parses  . Better results can be obtained when models m'e built and trained using an intbrmatives an@e  . 
Given the relations h iI ) between sample size and model complexity , we see that when there is a danger of overfitting  , one should build models on the basis of all informative set  . I to we ver , this leaves open the possil ) ility of training such a model upon as u-1 ) erset of the , informative set ; . Although we ha . renottested this scenario , we believe that this would lead to t ) ettere sultst tlant hose achieved here . 
The larger scale experiments showed that IFMs can be estimated using relatively long sentences  . 
They also showed that a simple Gaussian prior could reduce the etfects of overfitting  . However , they also showed that excessive overfitting probably required an alternative smoothing approach  . 
The smaller and larger experiments can be both viewed as  ( complementary ) ways of dealing with overfitting . We conjecture that of the two approaches , the informative smnpleal ) proach is preferable as it deals with overfitting directly : overfitting results fi ' omfitting to complex a model with too little data  . 
Our ongoing research will concentrate upon stronger ways of dealing with overfitting in lexicalised RFMs  . One line we are pursuing is to combine a compression-based prior with an exponential model  . This blends MDL with Maximum Entropy . 
We are also looking at alternative template sets.
For example , we would probably benefit fi'om using templates that capture more of the syntactic on text of a rule instantiation  . 

We would like to tliank Rob Malouf , Domfla Nie Gearailt and timanonymous reviewers for comments  . This work was supported by tile TMR Project Lcar'nin9 Computational Grammars . 

St , even P . At mey .  1997 . Stochastic Attribute Value Grmm nm:s . Computational Linguistics ,  23(4):597- 618 , December . 
Miles Osborne 19?9 . DCG induction using MDL and Parsed Corpora . In James Cussens , editor , Lcarn in 9Langua9cinLogic , pages 63-71 , Bled , 
Slovenia , June.
Ted Briscoe and John Carroll .  1 . 996 . Autolnatic Extraction of Subcategorization from Corpora  . 
In Proceedings of the 5th Conference on Applied
NLP , p~ges 356-363, Washington , DC.
John Carroll and Ted Briscoe .  1992 . Probabilistic Normalisation and Unpacking of Paclmd Parse Forests for Unification-lmse  , dGrmnmars . Ill Pro-cccdi , n9s of the AAAI Fall Symposi'u , monP ~ vb-abilistic AppTvach , es to Natural Language , pages 33-38 , Cambridge , MA . 
Stanley Chen and Honald losenfeld . 1999a . Efficient Sampling and Feature Selection in Whole Sentence Maxinm in Entrol  ) y Language Models . In
ICASSP'99.
Stanley F . Chen and Ronald Rosenfeld . 1999b.
A Gaussian Prior for Smoothing Maxinmm \]211-tropy Models . Technical Rel ) or tCMU-CS-99-108,
Carnegie Mellon University.
Eirik Hektoen .  1997 . Probabilistic Parse Select ; ion Based on Semantic Cooet : llrl ' ellees . \] illPl'og(' . cd-ing so J " th , e5th l'ntc , r ' national Wo~wkh , op on Parsing Tcch , no lo . qics , Cambridge , Massach'usctts , 1) ages 113122 . 
Marl < Johnson , Stuart Geman , Stephen Cannon , Zhiyi Chi , and Stephan Riezler .  1999 . Esl , in mtors for Stochastic " Unification-based " ( ~rammars . In 37th Annual Meeting of the ACL , J . Latferty , S . Della Pietra , and V . Della Pietra . 
1997 . Inducing Features of Random Fields . 1EEE Transactions on Pattern Analysis and Mach , inc
Int clligcncc , 19(4):380393, April.
Jorma Rissanen .  1989 . Stochastic Complezity in Statisticali ' nquiry . Series in Computer Science-
Volmne15. World Scientific.

