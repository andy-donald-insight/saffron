Significance tests for the evaluation of ranking methods 
Stefan Evert
Institut fu?r maschinelle Sprachverarbeitung
Universita?t Stuttgart
Azenbergstr . 12, 70174 Stuttgart , Germany


This paper presents a statistical model that interprets the evaluation of ranking methods as a random experiment  . This model predicts the variability of evaluation results  , so that appropriate significance tests for the results can be derived  . The paper concludes with an empirical validation of the model on a collocation extraction task  . 
1 Introduction
Many tools in the area of natural language processing involve the application of ranking methods to sets of candidates  , in order to select the most useful items from an all too often overwhelming list  . 
Examples of such tools range from syntactic parsers  ( where alternative analyses are ranked by their plausibility  ) to the extraction of collocations from text corpora  ( where a ranking according to the scores assigned by a lexical association measure is the essential component of an extraction ? pipeline ?  )  . 
To this end , a scoring function g is applied to the candidate set  , which assigns a real number g ( x ) ? R to every candidate x . 1 Conventionally , higher scores are assigned to candidates that the scoring function considers more ? useful ?  . Candidates can then be selected in one of two ways :  ( i ) by comparison with a predefined threshold ? ? R ( i . e . x is accepted iff g(x )? ?) , resulting in a ?- acceptance set ;   ( ii ) by ranking the entire candidate set according to the score sg  ( x ) and selecting then highest-scoring candidates , resulting in an nbest list ( where n is either determined by practical constraints or interactively by manual inspection  )  . Note that an nbest list can also be interpreted as a ?- acceptance set with a suitably chosen cutoff threshold ? g  ( n )   ( determined from the scores of all candidates )  . 
Ranking methods usually involve various heuristics and statistical guesses  , so that an empirical eval-1Some systems may directly produce a sorted candidate list without assigning explicit scores  . However , unless this operation is ( implicitly ) based on an underlying scoring function , the result will in most cases be a partial ordering  ( where some pairs of candidates are in comparable ) or lead to inconsistencies . 
uation of their performance is necessary . Even when there is a solid theoretical foundation  , its predictions may not be borne out in practice . Often , the main goal of an evaluation experiment is the comparison of different ranking methods  ( i . e . scoring functions ) in order to determine the most useful one . 
A widely-used evaluation strategy classifies the candidates accepted by a ranking method into ? good ? ones  ( true positives , TP ) and ? bad ? ones ( false positives , FP ) . This is sometimes achieved by comparison of the relevant ?- acceptance sets or nbest lists with a gold standard  , but for certain applications ( such as collocation extraction )  , manual inspection of the candidates leads to more clear cut and meaningful results  . When TPs and FPs have been identified , the precision ? of a ?- acceptance set or an nbest list can be computed as the proportion of TPs among the accepted candidates  . The most useful ranking method is the one that achieves the highest precision  , usually comparing nbest lists of a given size n . If the full candidate set has been annotated , it is also possible to determine the recall R as the number of accepted TPs divided by the total number of TPs in the candidate set  . While the evaluation of extraction tools ( e . g . in information retrieval ) usually requires that both precision and recall are high  , ranking methods often put greater weight on high precision  , possibly at the price of missing a considerable number of TPs  . Moreover , when nbest lists of the same size are compared , precision and recall are fully equivalent . 2 For these reasons , I will concentrate on the precision ? here . 
As an example , consider the identification of collocations from text corpora  . Following the methodology described by Evert and Krenn  ( 2001 )  , German PP-verb combinations were extracted from a chunk-parsed version of the Frankfurter Rundschau Corpus  . 3 A cooccurrence frequency threshold of 2Namely , ? = nTP?R/n , where nTP stands for the total number of TPs in the candidate set  . 
3The Frankfurter Rundschau Corpus is a German newspaper corpus  , comprising ca . 40 million words of text . It is part of the ECI Multilingual Corpus 1 distributed by ELSNET . For this f?30 was applied , resulting in a candidate set of 5   102 PP-verb pairs . The candidates were then ranked according to the scores assigned by four association measures : the loglikelihood ratio  G2   ( Dunning ,  1993) , Pearson?s chi-squared statistic X2 ( Manning and Schu?tze ,  1999 ,  169?172) , the tscore statistict ( Church et al ,  1991) , and mere cooccurrence frequency f . 4 TPs were identified according to the definition of Krenn  ( 2000 )  . The graphs in Figure 1 show the precision achieved by these measures , for n ranging from 100 to 2000 ( lists with n < 100 were omitted because the graphs become highly unstable for small n  )  . The baseline precision of 11 . 0 9% corresponds to a random selection of n candidates . 
0 500 1000 1500 2000n ? best list precision ( % ) baseline = 11 . 09%

Figure 1: Evaluation example : candidates for German PP -verb collocations are ranked by four different association measures  . 
From Figure 1 , we can see that G2 and t are the most useful ranking methods , t being marginally better for n ? 800 and G2 for n ? 1   500  . Both measures are by far superior to frequency -based ranking  . The evaluation results also confirm the argument of Dunning  ( 1993 )  , who suggested G2 as a more robust alternative to X2  . Such results cannot be taken at face value , though , as they may simply be due to chance . When two equally useful ranking methods are compared  , method A might just happen to perform better in a particular experiment  , with B taking the lead in a repetition of the experi-experiment  , the corpus was annotated with the partial parser
YAC ( Kermes , 2003).
4See Evert ( 2004 ) for detailed information about these association measures  , as well as many further alternatives . 
ment under similar conditions . The causes of such random variation include the source material from which the candidates are extracted  ( what if a slightly different source had been used  ?  )  , noise introduced by automatic preprocessing and extraction tools  , and the uncertainty of human annotators manifested in varying degrees of interannotator agreement  . 
Most researchers understand the necessity of testing whether their results are statistically significant  , but it is fairly unclear which tests are appropriate  . 
For instance , Krenn ( 2000 ) applies the standard ?2-test to her comparative evaluation of collocation extraction methods  . She is aware , though , that this test assumes independent samples and is hardly suitable for different ranking methods applied to the same candidate set : Krenn and Evert  ( 2001 ) suggest several alternative tests for related samples  . A wide range of exact and asymptotic tests as well as computationally intensive randomisation tests  ( Yeh ,  2000 ) are available and add to the confusion about an appropriate choice  . 
The aim of this paper is to formulate a statistical model that interprets the evaluation of ranking methods as a random experiment  . This model defines the degree to which evaluation results are affected by random variation  , allowing us to derive appropriate significance tests  . After formalising the evaluation procedure in Section  2  , I recast the procedure as a random experiment and make the underlying assumptions explicit  ( Section 3 . 1) . On the basis of this model , I develop significance tests for the precision of a single ranking method  ( Section 3 . 2 ) and for the comparison of two ranking methods ( Section 3 . 3) . The paper concludes with an empirical validation of the statistical model in Section  4  . 
2 A formal account of ranking methods and their evaluation In this section I present a formalisation of rankings and their evaluation  , giving ?- acceptance sets a geometrical interpretation that is essential for the formulation of a statistical model in Section  3  . 
The scores computed by a ranking method are based on certain features of the candidates  . Each candidate can therefore be represented by its feature vector x ? ?  , where ? is an abstract feature space . 
For all practical purposes , ? can be equated with a subset of the ( possibly highdimensional ) real Euclidean space Rm . The complete set of candidates corresponds to a discrete subset C ?? of the feature space  . 5 A ranking method is represented by 5More precisely , C is a multiset because there may be multiple candidates with identical feature vectors  . In order to simplify notation I assume that C is a proper subset of ?  , which a real-valued function g : ? ? R on the feature space  , called a scoring function ( SF ) . In the following , I assume that there are no candidates with equal scores  , and hence noties in the rankings . 6 The ?- acceptance set for a SF g contains all candidates x ? C with g  ( x )  ? ? . In a geomet-rical interpretation , this condition is equivalent to x ? Ag (?) ? ? , where
Ag ( ? ) := x??g ( x ) ?? is called the ?- acceptance region of g . The ?- acceptance set of g is then given by the intersection Ag  ( ? ) ? C =: Cg ( ? )  . The selection of an nbest list is based on the ?- acceptance region Ag  ( ?g ( n ) ) for a suitably chosen nbest threshold ? g ( n )  . 7 As an example , consider the collocation extraction task introduced in Section  1  . The feature vector x associated with a collocation candidate represents the cooccurrence frequency information for this candidate : x =  ( O11 , O12 , O21 , O22) , where Oij are the cell counts of a 2  ?  2 contingency table ( Evert ,  2004) . Therefore , we have a four-dimensional feature space ? ? R4 , and each association measure defines a SF g : ?? R  . The selection of collocation candidates is usually made in the form of an nbest list  , but may also be based on a predefined threshold ? . 8 For an evaluation in terms of precision and recall  , the candidates in the set C are classified into true positives C + and false positives C ?  . The precision corresponding to an acceptance region A is then given by ? A := C + ? A/C ? A  , (1) i . e . the proportion of TPs among the accepted candidates  . The precision achieved by a SF g with threshold ? is ? Cg  ( ? )  . Note that the numerator in Eq . (1) reduces to n for an nbest list ( i . e . ? = ? g(n )), yielding the nbest precision ? g,n . Figure 1 shows graphs of ? g , n for 100 ? n ? 2000 , for the SFs g1 = G2 , g2 = t , g3 = X2 , and g4 = f . 
can be enforced by adding a small amount of random jitter to the feature vectors of candidates  . 
6 Under very general conditions , random jittering ( cf . Footnote 5 ) ensures that no two candidates have equal scores . This procedure is ( almost ) equivalent to breaking ties in the rankings randomly  . 
7Since I assume that there are no ties in the rankings  , ? g ( n ) can always be determined in such a way that the acceptance set contains exactly n candidates  . 
8 For instance , Church et al (1991) use a threshold of ? = 1 . 6 5 for the tscore measure corresponding to a nominal significance level of ? =  . 05 . This threshold is obtained from the limiting distribution of the t statistic  . 
3 Significance tests for evaluation results 3 . 1 Evaluation as a random experiment When an evaluation experiment is repeated  , the results will not be exactly the same . There are many causes for such variation , including different source material used by the second experiment  , changes in the tool settings , changes in the evaluation criteria , or the different intuitions of human annotators . Statistical significance tests are designed to account for a small fraction of this variation that is due to random effects  , assuming that all parameters that may have a systematic influence on the evaluation results are kept constant  . Thus , they provide a lower limit for the variation that has to be expected in an actual repetition of the experiment  . Only when results are significant can we expect them to be reproducible  , but even then a second experiment may draw a different picture  . 
In particular , the influence of qualitatively different source material or different evaluation criteria can never be predicted by statistical means alone  . 
In the example of the collocation extraction task , randomness is mainly introduced by the selection of a source corpus  , e . g . the choice of one particular newspaper rather than another  . Disagreement between human annotators and uncertainty about the interpretation of annotation guidelines may also lead to an element of randomness in the evaluation  . 
However , even significant results cannot be generalised to a different type of collocation  ( such as adjective-noun instead of PP-verb )  , different evaluation criteria , a different domain or text type , or even a source corpus of different size , as the results of Krenn and Evert (2001) show . 
A first step in the search for an appropriate significance test is to formulate a  ( plausible ) model for random variation in the evaluation results  . Because of the inherent randomness , every repetition of an evaluation experiment under similar conditions will lead to different candidate sets C + and C ?  . Some elements will be entirely new candidates , sometimes the same candidate appears with a different feature vector  ( and thus represented by a different point x ? ? )  , and sometimes a candidate that was annotated as a TP in one experiment may be annotated as a FP in the next  . In order to encapsulate all three kinds of variation  , let us assume that C + and C ? are randomly selected from a large set of hypothetical possibilities  ( where each candidate corresponds to many different possibilities with different feature vectors  , some of which may be TPs and some FPs ) . 
For any acceptance region A , both the number of TPs in A , TA := C + ? A , and the number of FPs in A , FA := C ? ? A , are thus random variables . 
We do not know their precise distributions , but it is reasonable to assume that ( i ) TA and FA are always independent and ( ii ) TA and TB ( as well as FA and FB ) are independent for any two disjoint regions A and B  . Note that TA and TB cannot be independent for A ? B  6= ? because they include the same number of TPs from the region A ? B  . The total number of candidates in the region A is also a random variable NA := TA+FA  , and the same follows for the precision ? A , which can now be written as ? A = TA/NA . 9Following the standard approach , we may now assume that ? A approximately follows a normal distribution with mean piA and variance  ?2A   , i . e . 
? A?N(piA , ?2A) . The mean piA can be interpreted as the average precision of the acceptance region A  ( obtained by averaging over many repetitions of the evaluation experiment  )  . However , there are two problems with this assumption . First , while ? A is an unbiased estimator for pia , the variance ?2A cannot be estimated from a single experiment . 10 Second , ? A is a discrete variable because both TA and NA are nonnegative integers  . When the number of candidates NA is small ( as in Section 3 . 3) , approximating the distribution of ? A by a continuous normal distribution will not be valid  . 
It is reasonable to assume that the distribution of NA does not depend on the average precision piA  . In this case , NA is called an ancillary statistic and can be eliminated without loss of information by conditioning on its observed value  ( see Lehmann ( 1991 , 542 ff ) for a formal definition of ancillary statistics and the merits of conditional inference  )  . Instead of probabilities P ( ? A ) we will now consider the conditional probabilities P  ( ? ANA )  . Because NA is fixed to the observed value , ? A is proportional to TA and the conditional probabilities are equivalent to P  ( TANA )  . When we choose one of the NA candidates at random  , the probability that it is a TP ( averaged over many repetitions of the experiment ) 9In the definition of the nbest precision ? g , n , i . e . for A = Cg(?g(n )) , the number of candidates in A is constant : NA = n  . At first sight , this may seem to be inconsistent with the interpretation of NA as a random variable  . However , one has to keep in mind that ? g(n ) , which is determined from the candidate set C , is itself a random variable . Consequently , A is not a fixed acceptance region and its variation counter-balances that of NA  . 
10 Sometimes , crossvalidation is used to estimate the variability of evaluation results  . While this method is appropriate e . g . for machine learning and classification tasks , it is not useful for the evaluation of ranking methods  . Since the crossvalidation would have to be based on random samples from a single candidate set  , it would not be able to tell us anything about random variation between different candidate sets  . 
should be equal to the average precision piA . Consequently , P ( TANA ) should follow a binomial distribution with success probability piA  , i . e . 
P ( TA = kNA ) = (
NAk ) ?( piA)k?(1 ? piA)
NA ? k(2) for k = 0, .   .   . , NA . We can now make inferences about the average precision piA based on this binomial distribution  . 1 1 As a second step in our search for an appropriate significance test  , it is essential to understand exactly what question this test should address : What does it mean for an evaluation result  ( or result difference ) to be significant ? In fact , two different questions can be asked : A : If we repeat an evaluation experiment under the same conditions  , to what extent will the observed precision values vary ? This question is addressed in Section  3  . 2 . 
B : If we repeat an evaluation experiment under the same conditions  , will method A again perform better than method B ? This question is addressed in Section  3  . 3 . 
3.2 The stability of evaluation results
Question A can be rephrased in the following way : How much does the observed precision value for an acceptance region A differ from the true average precision piA ? In other words  , our goal here is to make inferences about piA , for a given SF g and threshold ? . From Eq .  (2) , we obtain a binomial confidence interval for the true value piA  , given the observed values of TA and NA ( Lehmann ,  1991 , 89 ff ) . Using the customary 95% confidence level , piA should be contained in the estimated interval in all but one out of twenty repetitions of the experiment  . Binomial confidence intervals can easily be computed with standard software packages such as R  ( R Development Core Team ,  2003) . As an example , assume that an observed precision of ? A = 40% is based on TA = 200 TPs out of NA = 500 accepted candidates . Precision graphs as those in Figure 1 display ? A as a maximum-likelihood estimate for piA  , but its true value may range from 35 . 7% to 44 . 4% ( with 95% confidence) . 1 2   11Note that some of the assumptions leading to Eq . (2) are far from self-evident . As an example ,   ( 2 ) tacitly assumes that the success probability is equal topi A regardless of the particular value of NA on which the distribution is conditioned  , which need not be the case . Therefore , an empirical validation is necessary ( see Section 4 )  . 
1 2This confidence interval was computed with the R command binom  . test(200,500) . 
Figure 2 shows binomial confidence intervals for the association measures  G2 and X2 as shaded regions around the precision graphs . It is obvious that a repetition of the evaluation experiment may lead to quite different precision values  , especially for n < 1000 . In other words , there is a considerable amount of uncertainty in the evaluation results for each individual measure  . However , we can be confident that both ranking methods offer a substantial improvement over the baseline  . 
0 500 1000 1500 2000n ? best list precision ( % ) baseline = 11 . 09%

Figure 2: Precision graphs for the G2 and X2 measures with 95% confidence intervals . 
For an evaluation based on nbest lists ( as in the collocation extraction example )  , it has to be noted that the confidence intervals are estimates for the average precision piA of a fixed ?- acceptance region  ( with ? = ? g ( n ) computed from the observed candidate set )  . While this region contains exactly NA = n candidates in the current evaluation  , NA may be different from n when the experiment is repeated  . Consequently , piA is not necessarily identical to the average precision of nbest lists  . 
3.3 The comparison of ranking methods
Question B can be rephrased in the following way : Does the SF  g1 on average achieve higher precision than the SF g2?   ( This question is normally asked when g1 performed better than g2 in the evaluation . ) In other words , our goal is to test whether piA > piB for given acceptance regions A of  g1 and B of g2  . 
The confidence intervals obtained for two SF g1 and g2 will often overlap ( cf . Figure 2 , where the confidence intervals of G2 and X2 overlap for all list sizes n )  , suggesting that there is no significant difference between the two ranking methods  . Both observed precision values are consistent with an average precision piA = piB in the region of overlap  , so that the observed differences may be due to random variation in opposite directions  . However , this conclusion is premature because the two rankings are not independent  . Therefore , the observed precision values of g1 and g2 will tend to vary in the same direction , the degree of correlation being determined by the amount of overlap between the two rankings  . Given acceptance regions A := Ag 1 ( ?1 ) and B := Ag2 ( ?2 )  , both SF make the same decision for any candidates in the intersection A ? B  ( both SF accept ) and in the ? complement ? ?\ ( A ? B )   ( both SF reject )  . Therefore , the performance of g1 and g2 can only differ in the regions D1 := A\B ( g1 accepts , but g2 rejects ) and B\A ( vice versa ) . 
Correspondingly , the counts TA and TB are correlated because they include the same number of TPs from the region A ? B  ( namely , the set C+?A?B ) , Indisputably ,   g1 is a better ranking method than g2 iff piD1  >  piD2 and vice versa . 1 3 Our goal is thus to test the null hypothesis H0  :  piD1  =  piD2 on the basis of the binomial distributions P ( TD1  ND1 ) and P ( TD2  ND2 )  . I assume that these distributions are independent because  D1  ?  D2  = ?  ( cf . 
Section 3 . 1) . The number of candidates in the difference regions  , ND1 and ND2 , may be small , especially for acceptance regions with large overlap  ( this was one of the reasons for using conditional inference rather than a normal approximation in Section  3  . 1) . Therefore , it is advisable to use Fisher?s exact test ( Agresti ,  1990 ,  60?66 ) instead of an asymptotic test that relies on large-sample approximations  . The data for Fisher?s test consist of a 2?   2 contingency table with columns ( TD1 , FD1) and ( TD2 , FD2) . Note that a two-sided test is called for because there is no a priori reason to assume that  g1 is better than g2   ( or vice versa )  . Although the implementation of a two-sided Fisher ? stest is not trivial  , it is available in software packages such as R . 
Figure 3 shows the same precision graphs as Figure 2  . Significant differences between the G2 and X2 measures according to Fisher?s test ( at a 95% confidence level ) are marked by grey triangles . 
1 3Note that piD1  >  piD2 does not necessarily entail piA > piB if NA and NB are vastly different and piA?B?piDi  . In this case , the winner will always be the SF that accepts the smaller number of candidates  ( because the additional candidates only serve to lower the precision achieved in A?B  )  . 
This example shows that it is ? unfair ? to compare acceptance sets of  ( substantially ) different sizes just in terms of their overall precision  . Evaluation should therefore either be based on nbest lists or needs to take recall into account  . 
Contrary to what the confidence intervals in Figure  2 suggested , the observed differences turn out to be significant for all nbest lists up to n =  1250   ( marked by a thin vertical line )  . 
0 500 1000 1500 2000n ? best list precision ( % ) baseline = 11 . 09%

Figure 3: Significant differences between the G2 and X2 measures at 95% confidence level . 
4 Empirical validation
In order to validate the statistical model and the significance tests proposed in Section  3  , it is necessary to simulate the repetition of an evaluation experiment  . Following the arguments of Section 3 . 1 , the conditions should be the same for all repetitions so that the amount of purely random variation can be measured  . To achieve this , I divided the Frank-furter Rundschau Corpus into 80 contiguous , nonoverlapping parts , each one containing approx . 500k words . Candidates for PP-verb collocations were extracted as described in Section  1  , with a frequency threshold of f ? 4 . The 80 samples of candidate sets were ranked using the association measures  G2  , X2 and tas scoring functions , and true positives were manually identified according to the criteria of  ( Krenn ,  2000) . 1 4 The true average precision piA of an acceptance set A was estimated by averaging over all  80 samples . 
Both the confidence intervals of Section 3 . 2 and the significance tests of Section 3 . 3 are based on the assumption that P ( TANA ) follows a binomial distribution as given by Eq .  (2) . Unfortunately , it 14I would like to thank Brigitte Krenn for making her annotation database of PP-verb collocations  ( Krenn , 2000) available , and for the manual annotation of 1   913 candidates that were not covered by the existing database  . 
is impossible to test the conditional distribution directly  , which would require that NA is the same for all samples  . Therefore , I use the following approach based on the unconditional distribution P  ( ? A )  . If NA is sufficiently large , P ( ? ANA ) can be approximated by a normal distribution with mean ? = piA and variance  ?2 = piA ( 1 ? piA ) /NA ( from Eq .  (2)) . 
Since ? does not depend on NA and the standard deviation ? is proportional to  ( NA ) ?1/2 , it is valid to make the approximation
P ( ? ANA ) ? P ( ? A )   ( 3 ) as long as NA is relatively stable . Eq . (3) allows us to pool the data from all samples , predicting that
P (? A ) ? N (? , ?2) (4) with ? = piA and ?2 = piA(1 ? piA)/N . Here , N stands for the average number of TPs in A . 
These predictions were tested for the measures g1  =  G2 and g2 = t , with cutoff thresholds ?1 = 32 . 5 and ?2 = 2 . 09  ( chosen so that N = 100 candidates are accepted on average )  . Figure 4 compares the empirical distribution of ? A with the expected distribution according to Eq  .  (4) . These histograms show that the theoretical model agrees quite well with the empirical results  , although there is a little more variation than expected  . 1 5 The empirical standard deviation is between 20% and 40% larger than expected , with s = 0 . 057 vs .  ? = 0 . 044 for G2 and s = 0 . 066 vs .  ? = 0 . 047 for t . These findings suggest that the model proposed in Section  3  . 1 may indeed represent a lower bound on the true amount of random variation  . 
Further evidence for this conclusion comes from a validation of the confidence intervals defined in Section  3  . 2 . For a 95% confidence interval , the true proportion piA should fall within the confidence interval in all but  4 of the 80 samples . For G2 ( with ? = 32 . 5) and X2 ( with ? = 239 . 0) , piA was outside the confidence interval in 9 cases each ( three of them very close to the boundary )  , while the confidence interval for t(with ? = 2 . 09) failed in 12 cases , which is significantly more than can be explained by chance  ( p < . 001, binomial test) . 
5 Conclusion
In the past , various statistical tests have been used to assess the significance of results obtained in the evaluation of ranking methods  . There is much confusion about their validity , though , mainly due to 15The agreement is confirmed by the Kolmogorov test of goodness-of-fit  , which does not reject the theoretical model ( 4 ) in either case . 
Histogram for G2 precision number of samples 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 620 observed expected
Histogram for t precision number of samples 0 . 0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6  20 observed expected Figure 4: Distribution of the observed precision ? A for ?- acceptance regions of the association measures  G2   ( left panel ) and t ( right panel )  . The solid lines indicate the expected distribution according to Eq  .  (2) . 
the fact that assumptions behind the application of a test are seldom made explicit  . This paper is an attempt to remedy the situation by interpreting the evaluation procedure as a random experiment  . The model assumptions , motivated by intuitive arguments , are stated explicitly and are open for discussion  . Empirical validation on a collocation extraction task has confirmed the usefulness of the model  , indicating that it represents a lower bound on the variability of evaluation results  . On the basis of this model , I have developed appropriate significance tests for the evaluation of ranking methods  . These tests are implemented in the UCS toolkit , which was used to produce the graphs in this paper and can be downloaded from http://www  . collocations . de/ . 

Alan Agresti . 1990. Categorical Data Analysis.
John Wiley & Sons , New York.
Kenneth Church , William Gale , Patrick Hanks , and Donald Hindle .  1991 . Using statistics in lexical analysis . In Lexical Acquisition : Using Online Resources to Build a Lexicon  , pages 115?164 . 
Lawrence Erlbaum.
Ted Dunning .  1993 . Accurate methods for the statistics of surprise and coincidence  . Computational Linguistics , 19(1):61?74 . 
Stefan Evert and Brigitte Krenn .  2001 . Methods for the qualitative evaluation of lexical association measures  . In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics  , pages 188?195 , Toulouse , France . 
Stefan Evert .  2004 . An online repository of association measures . http://www . collocations . de/AM/ . 
Hannah Kermes .  2003 . Offline ( and Online ) Text Analysis for Computational Lexicography . Ph . D . 
thesis , IMS , University of Stuttgart . Arbeit spa-piere des Instituts fu?r Maschinelle Sprachverarbeitung  ( AIMS )  , volume 9 , number 3 . 
Brigitte Krenn and Stefan Evert .  2001 . Can we do better than frequency ? a case study on extracting pp-verb collocations  . In Proceedings of the ACL Workshop on Collocations  , pages 39?46 , 
Toulouse , France , July.
Brigitte Krenn .  2000 . The Usual Suspects : Data-Oriented Models for the Identification and Representation of Lexical Collocations  . , volume 7 of Saarbru?cken Dissertations in Computational Linguistics and Language Technology  . DFKI & Uni-versita?t des Saarlandes , Saarbru?cken , Germany . 
E . L . Lehmann .  1991 . Testing Statistical Hypotheses . Wadsworth , 2nd edition . 
Christopher D . Manning and Hinrich Schu?tze.
1999 . Foundations of Statistical Natural Language Processing  . MIT Press , Cambridge , MA . 
R Development Core Team , 2003 . R : A language and environment for statistical computing  . R Foundation for Statistical Computing , Vienna , Austria . ISBN3-900051-00-3 . See also http://www . r-project . org/ . 
Alexander Yeh .  2000 . More accurate tests for the statistical significance of result differences  . In Proceedings of the 18th International Conference on Computational Linguistics  ( COLING 2000 )  , 
Saarbru?cken , Germany.
