An NP-Cluster Based Approach to Coreference Resolution 
Xiaofeng Yang ?? Jian Su ? Guodong Zhou ? Chew Lim Tan ? 
?Institute for Infocomm Research
21 Heng Mui Keng Terrace,
Singapore , 119613


?Department of Computer Science
National University of Singapore,
Singapore , 117543


Traditionally , coreference resolution is done by mining the reference relationships between NP pairs  . However , an individual NP usually lacks adequate description information of its referred entity  . In this paper , we propose a supervised learning-based approach which does coreference resolution by exploring the relationships between NPs and coreferential clusters  . Compared with individual NPs , coreferential clusters could provide richer information of the entities for better rules learning and reference determination  . The evaluation done on MEDLINE dataset shows that our approach outperforms the baseline NPNP based approach in both recall and precision  . 
1 Introduction
Coreference resolution is the process of linking as a  cluster1 multiple expressions which refer to the same entities in a document  . In recent years , supervised machine learning approaches have been applied to this problem and achieved considerable success  ( e . g . Aone and Bennett (1995) ; McCarthy and Lehnert (1995) ; Soon et al . (2001); Ng and Cardie (2002b )) . The main idea of most supervised learning approaches is to recast this task as a binary classification problem  . Specifically , a classifier is learned and then used to determine whether or not two NPs in a document are coreferring  . Clusters are formed by linking coreferential NP pairs according to a certain selection strategy  . In this way , the identification of coreferential clusters in text is reduced to the identification of coreferential NP pairs  . 
One problem of such reduction , however , is that the individual NP usually lacks adequate descriptive information of its referred entity  . Consequently , it is often difficult to judge whether or not two NPs are talking about the  1In this paper the term ? cluster ? can be interchangeably used as ? chain ?  , while the former better emphasizes the equivalence property of coreference relationship  . 
same entity simply from the properties of the pair alone  . As an example , consider the pair of a non-pronoun and its pronominal antecedent candidate  . The pronoun itself gives few clues for the reference determination  . Using such NP pairs would have a negative influence for rules learning and subsequent resolution  . So far , several efforts ( Harabagiu et al , 2001; Ng and Cardie , 2002a ; Ng and Cardie , 2002b ) have attempted to address this problem by discarding the ? hard ? pairs and select only those confident ones from the NP-pair pool  . Nevertheless , this eliminating strategy still cannot guarantee that the NPs in ? confident ? pairs bear necessary description information of their referents  . 
In this paper , we present a supervised learning-based approach to coreference resolution  . Rather than attempting to mine the reference relationships between NP pairs  , our approach does resolution by determining the links of NPs to the existing coreferential clusters  . In our approach , a classifier is trained on the instances formed by an NP and one of its possible antecedent clusters  , and then applied during resolution to select the proper cluster for an encountered NP to be linked  . As a coreferential cluster offers richer information to describe an entity than a single NP in the cluster  , we could expect that such an NP-Cluster framework would enhance the resolution capability of the system  . Our experiments were done on the the MEDLINE data set  . Compared with the baseline approach based on NPNP framework  , our approach yields a recall improvement by 4 . 6%, with still a precision gain by 1 . 3% . These results indicate that the NP-Clusterbased approach is effective for the coreference resolution task  . 
The remainder of this paper is organized as follows  . Section 2 introduces as the baseline the NPNP based approach  , while Section 3 presents in details our NP-Cluster based approach  . Section 4 reports and discusses the experimental results . Section 5 describes related research work . 
Finally , conclusion is given in Section 6.
2 Baseline : the NPNP based approach 2 . 1 Framework description We built a baseline coreference resolution system  , which adopts the common NPNP based learning framework as employed in  ( Soon et al ,  2001) . 
Each instance in this approach takes the form of iNP j  , NP i , which is associated with a feature vector consisting of  18 features ( f1?f18 ) as described in Table 2 . Most of the features come from Soon et al ( 2001 ) ?s system . Inspired by the work of ( Strube et al , 2002) and ( Yang et al ,  2004) , we use two features , StrSim1(f17) and StrSim2(f18) , to measure the string-matching degree of NPj and NP i  . Given the following similarity function : Str Simlarity  ( Str1 , Str2) = 100 ? Str1 ? Str2Str1
S trSim1 and StrSim2 are computed using Str Similarity ( SNP j , SNP i ) and Str Similarity ( SNPi , SNP j ) , respectively . Here SNP is the token list of NP , which is obtained by applying word stemming , stopword removal and acronym expansion to the original string as described in Yang et al  ( 2004 ) ?s work . 
During training , for each anaphor NP j in a given text , a positive instance is generated by pairing NP j with its closest antecedent  . A set of negative instances is also formed by NPj and each NP occurring between NPj and NP i  . 
When the training instances are ready , a classifier is learned by C5 . 0 algorithm ( Quinlan , 1993) . During resolution , each encountered noun phrase , NP j , is paired in turn with each preceding noun phrase  , NP i . For each pair , a testing instance is created as during training , and then presented to the decision tree , which returns a confidence value ( CF ) 2 indicating the likelihood that NPi is coreferential to NP j  . In our study , two antecedent selection strategies , Most Recent First ( MRF ) and Best First ( BF ) , are tried to link NP j to its a proper antecedent with CF above a threshold  ( 0 . 5) . MRF ( Soon et al ,  2001 ) selects the candidate closest to the anaphor , while BF ( Aone and Bennett ,  1995 ; Ng 2The confidence value is obtained by using the smoothed ratio  p+1t+2   , where p is the number of positive instances and t is the total number of instances contained in the corresponding leaf node  . 
and Cardie , 2002b ) selects the candidate with the maximal CF . 
2.2 Limitation of the approach
Nevertheless , the problem of the NPNP based approach is that the individual NP usually lacks adequate description information about its referred entity  . Consequently , it is often difficult to determine whether or not two NPs refer to the same entity simply from the properties of the pair  . See the the text segment in Table 1 , for example , [1Amutant of [2KBF1/p50]] , unable to bind to DNA but able to form homo-or [3 heterodimers ] , has been constructed . 
[ 4 This protein ] reduces or abolishes the DNA binding activity of wild-type proteins of  [5 the same family ( [6KBF1/p50] , c - and v rel )] . 
[ 7 This mutant ] also functions in vivo as a transacting dominant negative regulator :  .   .   . 
Table 1: An Example from the dataset
In the above text , [1Amutant of KBF1/p50] ,   [4 This protein ] and [7 This mutant ] are annotated in the same coreferential cluster  . According to the above framework , NP7 and its closest antecedent , NP 4 , will form a positive instance . 
Nevertheless , such an instance is not informative in that NP4 bears little information related to the entity and thus provides few clues to explain its coreference relationship with  NP7  . 
In fact , this relationship would be clear if [1 A mutant of KBF1/p50]  , the antecedent of NP4 , is taken into consideration . NP1 gives a detailed description of the entity . By comparing the string of NP7 with this description , it is apparent that NP7 belongs to the cluster of NP1  , and thus should be coreferential to NP4 . This suggests that we use the coreferential cluster  , instead of its single element , to resolve an NP correctly . In our study , we propose an approach which adopts an NP-Cluster based framework to do resolution  . The details of the approach are given in the next section  . 
3 The NP-Cluster based approach
Similar to the baseline approach , our approach also recasts coreference resolution as a binary classification problem  . The difference , however , is that our approach aims to learn a classifier which would select the most preferred cluster  , instead of the most preferred antecedent , for an encountered NP in text . We will give the framework of the approach , including the instance rep-Features describing the relationships between NPj and NP i  1  . DefNp11 if NPj is a definite NP ; else 02 . Demo NP 11 if NP j starts with a demonstrative ; else 03 . Indef NP 11 if NP j is an indefinite NP ; else 04 . Pron11 if NPj is a pronoun ; else05 . Proper NP 11 if NP j is a proper NP ; else 06 . Def NP21 if NP i is a definite NP ; else 07 . Demo NP21 if NPi starts with a demonstrative ; else08 . Indef NP21 if NP i is an indefinite NP ; else 09 . Pron21 if NP i is a pronoun ; else 010 . Proper NP21 if NP i is a proper NP ; else 011 . Appositive 1 if NP i and NP j are in an appositive structure ; else 012 . Name Alias 1 if NP i and NP j are in an alias of the other ; else 013 . Gender Agree 1 if NP i and NP j agree in gender ; else 014 . NumA gree1 if NP i and NPj agree in number ; else 015 . Semantic Agree 1 if NP i and NP j agree in semantic class ; else 016 . Head StrMatch 1 if NP i and NPj contain the same head string ; else 017 . StrSim 1 The string similarity of NPj against NP i 18  . StrSim 2 The string similarity of NPi against NPj Features describing the relationships between NPj and cluster Ck  19  . Cluster NumA gree 1 if Ck and NPj agree in number ; else 020 . ClusterGenAgree1 if Ck and NPj agree in gender ; else 021 . Cluster Sem Agree 1 if Ck and NPj agree in semantic class ; else 022 . Cluster Length The number of elements contained in Ck  23  . Cluster Str Sim The string similarity of NP j against Ck  24  . Cluster StrLNPSim The string similarity of NPj against the longest NP in Ck Table  2: The features in our coreference resolution system  ( Features 1  ?  18 are also used in the baseline system using NPNP based approach  ) resentation , the training and the resolution procedures , in the following subsections . 
3.1 Instance representation
An instance in our approach is composed of three elements like below : iNP j  , Ck , NP i where NP j , like the definition in the baseline , is the noun phrase under consideration , while Ck is an existing coreferential cluster . Each cluster could be referred by a reference noun phrase NP i  , a certain element of the cluster . A cluster would probably contain more than one reference NPs and thus may have multiple associated instances  . 
For a training instance , the label is positive if NPj is annotated as belonging to Ck  , or negative if otherwise . 
In our system , each instance is represented as a set of 24 features as shown in Table 2  . The features are supposed to capture the properties of NPj and Ck as well as their relationships  . In the table we divide the features into two groups  , one describing NPj and NPi and the other describing NPj and Ck  . For the former group , we just use the same features set as in the baseline system  , while for the latter , we introduce 6 more features :
Cluster Num Agree , Cluster Gen Agree and Cluster SemAgree : These three features mark the compatibility of NPj and Ck in number  , gender and semantic agreement , respectively . If NPj mismatches the agreement with any element in Ck  , the corresponding feature is set to 0 . 
Cluster Length : The number of NPs in the cluster Ck  . This feature reflects the global salience of an entity in the sense that the more frequently an entity is mentioned  , the more important it would probably be in text . 
Cluster StrSim : This feature marks the string similarity between NPj and Ck  . Suppose SNP j is the token set of NP j , we compute the feature value using the similarity function 
Str Similarity ( SNPj , SCk ), where
SCk = ?


Cluster StrLNP Sim : It marks the string matching degree of NPj and the noun phrase in Ck with the most number of tokens  . The intuition here is that the NP with the longest string would probably be arricher description information of the referent than other elements in the cluster  . The feature is calculated using the similarity function Str Similarity  ( SNP j , SNP k ) , where
NP k = argmax NP i?C kSNP i 3 . 2 Training procedure Given an annotated training document  , we process the noun phrases from beginning to end . 
For each anaphoric noun phrase NP j , we consider its preceding coreferential clusters from right to  left3  . For each cluster , we create only one instance by taking the last NP in the cluster as the reference NP  . The process will not terminate until the cluster to which NPj belongs is found  . 
To make it clear , consider the example in Table 1 again . For the noun phrase [7 This mu-tant] , the annotated preceding coreferential clusters are: 
C1: ..., NP2, NP6
C2:...,NP5
C3: NP1, NP4
C4: ..., NP3
Thus three training instances are generated : i NP7  , C1 , NP6 iNP7 , C2 , NP5 iNP7 , C3 , NP4 Among them , the first two instances are labelled as negative while the last one is positive  . 
After the training instances are ready , we use C5 . 0 learning algorithm to learn a decision tree classifier as in the baseline approach  . 
3.3 Resolution procedure
The resolution procedure is the counterpart of the training procedure  . Given a testing document , for each encountered noun phrase , NP j , we create a set of instances by pairing NPj with each cluster found previously  . The instances are presented to the learned decision tree to judge the likelihood that NP j is linked to a cluster  . 
The resolution algorithm is given in Figure 1.
As described in the algorithm , for each cluster under consideration , we create multiple instances by using every NP in the cluster as the reference NP  . The confidence value of the cluster 3We define the position of a cluster as the position of the last NP in the cluster  . 
algorithm RESOLVE ( a testing document d )
ClusterSet = ? ; // supposed has N markable NPs ; for j = 1 to N for each cluster in ClusterSet CF cluster = maxNPi ? cluster CFi  ( NP j , cluster , NP i ) select a proper cluster , Best Cluster , according to a ceter in cluster selection strategy  ; if BestCluster != NULL
Best Cluster = BestCluster ? NPj ; else // create a new cluster
NewCluster = NPj;
ClusterSet = ClusterSet ? New Cluster;
Figure 1: The clusters identification algorithm is the maximal confidence value of its instances  . 
Similar to the baseline system , two cluster selection strategies , i . e . MRF and BF , could be applied to link NPj to a proper cluster . For MRF strategy , NP j is linked to the closest cluster with confidence value above  0  . 5 , while for BF , it is linked to the cluster with the maximal confidence value  ( above 0 . 5) . 
3.4 Comparison of NPNP and
NP-Cluster based approaches
As noted above , the idea of the NP-Cluster based approach is different from the NPNP based approach  . However , due to the fact that in our approach a cluster is processed based on its reference NPs  , the framework of our approach could be reduced to the NPNP based framework if the cluster-related features were removed  . From this point of view , this approach could be considered as an extension of the baseline approach by applying additional cluster features as the properties of NP i  . These features provide richer description information of the entity  , and thus make the coreference relationship between two NPs more apparent  . In this way , both rules learning and coreference determination capabilities of the original approach could be enhanced  . 
4 Evaluation 4.1 Data collection
Our coreference resolution system is a component of our information extraction system in biomedical domain  . For this purpose , an annotated coreference corpus have been built 4 , which 4The annotation scheme and samples are available in http://nlp  . i2r . a-star . edu . sg/resources/GENIA-coreference
MRFBF
Experiments R P F R P F
Baseline 80.2 77.4 78.8 80.3 77.5 78.9
AllAnte 84.4 70.2 76.6 85.7 71.4 77.9
Our Approach 84 . 4 78 . 2 81 . 2 84 . 9 78 . 8 81 . 7 Table 3: The performance of different coreference resolution systems consists of totally  228 MEDLINE abstracts selected from the GENIA data set  . The average length of the documents in collection is  244 words . One characteristic of the bio-literature is that pronouns only occupy about  3% among all the NPs . This ratio is quite low compared to that in newswire domain  ( e . g . above 10% for
MUC dataset).
A pipeline of NLP components is applied to preprocess an input raw text  . Among them , NE recognition , part-of-speech tagging and text chunking adopt the same HMM based engine with error-driven learning capability  ( Zhou and Su ,  2002) . The NE recognition component trained on GENIA ( Shen et al ,  2003 ) can recognize up to 23 common biomedical entity types with an overall performance of  66  . 1 Fmeasure ( P=66 . 5% R = 65 . 7%) . In addition , to remove the apparent non-anaphors ( e . g . , embedded proper nouns ) in advance , a heuristic-based nonanaphoricity identification module is applied  , which successfully removes 50 . 0% non-anaphors with a precision of 83 . 5% for our dataset . 
4.2 Experiments and discussions
Our experiments were done on first 100 documents from the annotated corpus , among them 70 for training and the other 30 for testing . 
Throughout these experiments , default learning parameters were applied in the C5  . 0 algorithm . 
The recall and precision were calculated automatically according to the scoring scheme proposed by Vilain et al  ( 1995 )  . 
In Table 3 we compared the performance of different coreference resolution systems  . The first line summarizes the results of the baseline system using traditional NPNP based approach as described in Section  2  . Using BF strategy , Baseline obtains 80 . 3% recall and 77 . 5% precision . These results are better than the work by Castano et al  ( 2002 ) and Yang et al ( 2004 )  , which were also tested on the MEDLINE dataset and reported a Fmeasure of about  74% and 69%  , respectively . 
In the experiments , we evaluated another NPNP based system , All Ante . It adopts a similar learning framework as Baseline except that during training it generates the positive instances by paring an NP with all its antecedents instead of only the closest one  . The system attempts to use such an instance selection strategy to incorporate the information from coreferential clusters  . 
But the results are nevertheless disappointing : although this strategy boosts the recall by  5  . 4% , the precision drops considerably by above 6% at the same time . The overall Fmeasure is even lower than the baseline systems  . 
The last line of Table 3 demonstrates the results of our NP-Clusterbased approach  . For BF strategy , the system achieves 84 . 9% recall and 78 . 8% precision . As opposed to the baseline system , the recall rises by 4 . 6% while the precision still gains slightly by 1 . 3% . Overall , we observe the increase of Fmeasure by 2 . 8% . 
The results in Table 3 also indicate that the BF strategy is superior to the MRF strategy  . 
A similar finding was also reported by Ng and
Cardie (2002b ) in the MUC dataset.
To gain insight into the difference in the performance between our NP-Cluster based system and the NPNP based system  , we compared the decision trees generated in the two systems in Figure  2  . In both trees , the string-similarity features occur on the top portion  , which supports the arguments by ( Strube et al , 2002) and ( Yang et al ,  2004 ) that string-matching is a crucial factor for NP coreference resolution  . As shown in the figure , the feature StrSim 1 in left tree is completely replaced by the Cluster Str Sim and Cluster StrLNPSim in the right tree  , which means that matching the tokens with a cluster is more reliable than with a single NP  . Moreover , the cluster length will also be checked when the NP under consideration has low similarity against a cluster  . These evidences prove that the information from clusters is quite important for the coreference resolution on the dataset  . 
The decision treevisualizes the importance of the features for a dataset  . However , the tree is learned from the documents where coreferential clusters are correctly annotated  . During resolu-
Head Match = 0:: . . . NameAlias=1:1(22/1):NameAlias=0::: . . . Appositive=0: 0 ( 13095/265 ) : Appositive=1: 1 ( 15/4 ) 
Head Match = 1:: . . . Str Sim_1 > 71:: . . . Demo NP_1 = 0:1 (615/29): Demo NP_1 = 1::: . . . NumA gree=0:0(5): NumA gree=1:1 (26)
StrSim_1 <= 71::... Demo NP_2 = 1:1(12/2)
Demo NP_2 = 0::... Str Sim_2 <= 77:0(144/17)
Str Sim_2>77::... Str Sim_1 <= 33:0 (42/11)
Str Sim_1 > 33:1(38/11)
Head Match = 1:: . . . Cluster_StrSim > 66:1 ( 663/36 ) : Cluster_Str Sim <= 66::: . . . StrSim_2 <= 85:0(140/14): StrSim_2>85:: . . . Cluster_StrLNPSim > 50:1 ( 16/1 ) : Cluster_StrLNPSim <= 50::: . . . Cluster_Length <= 5:0 ( 59/17 ) : Cluster_Length > 5: 1 ( 4 ) 
HeadMatch=0::...NameAlias=1:1(22/1)
NameAlias=0::... Appositive=1:1(15/4)
Appositive=0:: ... Str Sim_2<=54::..
Str Sim_2 > 54::..
Figure 2: The resulting decision trees for the NPNP and NP -Cluster based approaches 
Features RP Ff 1?218 0 . 3 77 . 5 78 . 9 f1?21, f22 84 . 1 74 . 4 79 . 0 f1?21, f23 84 . 7 78 . 8 81 . 6 f1?21, f24 84 . 3 78 . 0 81 . 0 f1?21, f23, f22 84 . 9 78 . 6 81 . 6 f1?21, f23, f24 84 . 9 78 . 9 81 . 8 f1?21, f23, f24, f22 84 . 9 78 . 8 81 . 7 Table 4: Performance using combined features ( fi refers to the i ( th ) feature listed in Table 2 ) tion , unfortunately , the found clusters are usually not completely correct  , and as a result the features important in training data may not be also helpful for testing data  . Therefore , in the experiments we were concerned about which features really matter for the real coreference resolution  . For this purpose , we tested our system using different features and evaluated their performance in Table  4  . Here we just considered feature Cluster Length ( f22 )  , Cluster Str Sim ( f23 ) and Cluster StrLNPSim ( f24 )  , as Figure 2 has indicated that among the cluster-related features only these three are possibly effective for resolution  . Throughout the experiment , the BestFirst strategy was applied . 
As illustrated in the table , we could observe that : 1 . Without the three features , the system is equivalent to the baseline system in terms of the same recall and precision  . 
2 . Cluster Str Sim ( f23 ) is the most effective as it contributes most to the system performance  . Simply using this feature boosts the Fmeasure by 2  . 7% . 
3 . Cluster StrLNP Sim ( f24 ) is also effective by improving the Fmeasure by 2 . 1% alone . 
When combined with f23 , it leads to the best Fmeasure . 
4 . Cluster Length ( f22) only brings 0 . 1% Fmeasure improvement . It could barely increase , or even worse , reduces the Fmeasure when used together with the the other two features  . 
5 Related work
To our knowledge , our work is the first supervised-learning based attempt to do coreference resolution by exploring the relationship between an NP and coreferential clusters  . In the heuristic salience-based algorithm for pronoun resolution  , Lappin and Leass ( 1994 ) introduce a procedure for identifying anaphorically linked NP as a cluster for which a global salience value is computed as the sum of the salience values of its elements  . Cardie and Wagstaff ( 1999 ) have proposed an unsupervised approach which also incorporates cluster information into consideration  . Their approach uses hard constraints to preclude the link of an NP to a cluster mismatching the number  , gender or semantic agreements , while our approach takes these agreements together with other features  ( e . g . cluster-length , string-matching degree , etc ) as preference factors for cluster selection . Besides , the idea of clustering can be seen in the research of cross-document coreference  , where NPs with high context similarity would be chained together based on certain clustering methods  ( Bagga and Biermann ,  1998;
Gooi and Allan , 2004).
6 Conclusion
In this paper we have proposed a supervised learning-based approach to coreference resolution  . Rather than mining the coreferential relationship between NP pairs as in conventional approaches  , our approach does resolution by exploring the relationships between an NP and the coreferential clusters  . Compared to individual NPs , coreferential clusters provide more information for rules learning and reference determination  . In the paper , we first introduced the conventional NPNP based approach and analyzed its limitation  . Then we described in details the framework of our NP-Cluster based approach  , including the instance representation , training and resolution procedures . We evaluated our approach in the biomedical domain  , and the experimental results showed that our approach outperforms the NPNP based approach in both recall  ( 4 . 6%) and precision (1 . 3%) . 
While our approach achieves better performance , there is still room for further improvement . For example , the approach just resolves an NP using the cluster information available so far  . Nevertheless , the text after the NP would probably give important supplementary information of the clusters  . The ignorance of such information may affect the correct resolution of the NP  . In the future work , we plan to work out more robust clustering algorithm to link an NP to a globally best cluster  . 

C . Aone and S . W . Bennett .  1995 . Evaluating automated and manual acquistion of anaphora resolution strategies  . In Proceedings of the 33rd Annual Meeting of the Association for Compuational Linguistics  , pages 122?129 . 
A . Bagga and A . Biermann .  1998 . Entity-based cross document coreferencing using the vector space model  . In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics the  17th International Conference on Computational Linguistics  , pages 79?85 . 
C . Cardie and K . Wagstaff .  1999 . Noun phrase coreference as clustering . In Proceedings of the Joint Conference on Empirical Methods in 
NLP and Very Large Corpora.
J . Castano , J . Zhang , and J . Pustejovsky .  2002 . 
Anaphora resolution in biomedical literature.
In International Symposium on Reference Resolution  , Alicante , Spain . 
C . Gooi and J . Allan .  2004 . Cross-document coreference on a large scale corpus  . In Proceedings of 2004 Human Language Technology conference/North American chapter of the Association for Computational Linguistics annual meeting  . 
S . Harabagiu , R . Bunescu , and S . Maiorano.
2001 . Text knowledge mining for coreference resolution . In Proceedings of the 2nd Annual Meeting of the North America Chapter of the Association for Compuational Linguistics  , pages 55?62 . 
S . Lappin and H . Leass .  1994 . An algorithm for pronominal anaphora resolution . Computational Linguistics , 20(4):525?561 . 
J . McCarthy and Q . Lehnert .  1995 . Using decision trees for coreference resolution . In Proceedings of the 14th International Conference on Artificial Intelligences  , pages 1050?1055 . 
V . Ng and C . Cardie . 2002a . Combining sample selection and error-driven pruning for machine learning of coreference rules  . In Proceedings of the conference on Empirical Methods in Natural Language Processing  , pages 55?62 , 

V . Ng and C . Cardie . 2002b . Improving machine learning approaches to coreference resolution  . In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics  , pages 104?111 , Philadelphia . 
J . R . Quinlan .  1993 . C4 . 5: Programs for machine learning . Morgan Kaufmann Publishers,
San Francisco , CA.
D . Shen , J . Zhang , G . Zhou , J . Su , and
C . Tan .  2003 . Effective adaptation of hidden markov model-based named-entity recognizer for biomedical domain  . In Proceedings of ACL03 Workshop on Natural Language Processing in Biomedicine  , Japan . 
W . Soon , H . Ng , and D . Lim .  2001 . A machine learning approach to coreference resolution of noun phrases  . Computational Linguistics , 27(4):521?544 . 
M . Strube , S . Rapp , and C . Muller .  2002 . The influence of minimum edit distance on reference resolution  . In Proceedings of the Conference on Empirical Methods in Natural Language Processing  , pages 312?319 , Philadelphia . 
M . Vilain , J . Burger , J . Aberdeen , D . Connolly , and L . Hirschman .  1995 . A modeltheoretic coreference scoring scheme . In Proceedings of the Sixth Message understanding Conference  ( MUC6 )  , pages 45?52 , San Francisco , CA . 
Morgan Kaufmann Publishers.
X . Yang , G . Zhou , J . Su , and C . Tan .  2004 . Improving noun phrase coreference resolution by matching strings  . In Proceedings of the 1st International Joint Conference on Natural Language Processing  , Hainan . 
G . Zhou and J . Su .  2002 . Named Entity recognition using a HMM-based chunk tagger  . In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics  , Philadelphia . 
