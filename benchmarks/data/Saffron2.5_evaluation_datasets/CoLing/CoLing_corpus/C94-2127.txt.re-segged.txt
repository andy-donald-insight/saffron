Bottom-Up Earley Deduction
Greg or Erbach *
University of the Saarland
Computational Linguistics
D-66041 Saarbrficken , Germany

Abstract
We propose a bottom-up variant of Earley deduction  . Bottom-up deduction is preferable to topdown deduction because it allows incremen-tai processing  ( even for head-driven grammars )  , it is datadriven , o subsumption check is needed , and preference values attached to lexical items can be used to guide bestfirst search  . We discuss the scanning step for bottom-up Earley deduction and indexing schemes that help avoid useless deduction steps  . 
1 Introduction
Recently , there has been a lot of interest in Earley deduction  \[10\] with applications to parsing and generation \[13  ,  6 ,  7 ,  3\] . 
Earley deduction is a very attractive framwork for natural anguage processing because it has the following properties and applications  . 
? Memoization and reuse of partial results ? Incremental processing by addition of new items ? Hypothetical reasoning by keeping track of dependencies between items ? Best-first search by means of an agenda * This work was supported by the Deutsche Forschuugs-gemeins chaft through the project  N3 " Bidirektionale Linguistische Deduktion ( BiLD ) " in the Sonderforschungs be-reich 314 Kilnstliche Intelligenz--Wissens basierte Sy-sterne and by the Commission of the European Communities through the project  LRE-61-061 " Reusable Gramma-tical Resources . "I would like to thank Gfinter Neumann , Christer Samuels sonad Mats Wirdn for comments on this paper  . 
Like Earley's algorithm , all of these approaches operate topdown ( backward chaining )  . The interest has naturally focussed on topdown methods because they are at least to a certain degree goal-directed  . 
In this paper , we present a bottom-up variant of Earley deduction  , which we find advantageous for the following reasons : Inerementality : Portions of an input string can be analysed as soon as they are produced  ( or generated as soon as the what-to-say component has decided to verbalize them  )  , even for grammars where one cannot assume that the left-corner has been predicted before it is scanned  . 
Data-Drlven Processing : Top-down algorithms are not well suited for processing grammatical theories like Categoriai Grammar or nesG that would only allow very general predictions because they make use of general schemata instead of construction-specific rules  . For these grammars datadriven bottom-up rocessing is more appropriate  . The same is true for large-coverage rule-based grammars which lead to the creation of very many predictions  . 
Subsumption Checking : Since the bottom-up algorithm does not have a prediction step  , there is no need for the costly operation of subsumption checking  ) Search Strategy : In the case where lexicalent -ries \] lave been associated with preference  in-1Subsumption checking may still be needed to filter out spurious ambiguities  . 
796 formation , this information can be exploited to guide the heuristic search  . 
2 Bottom-up Earley Deduction
Earley deduction \[10\] is based on grammars encoded as definite clauses . The instantiation ( prediction ) rule of topdown Earley deduction is not needed in bottom-up Earley deduction  , because there is no prediction . There is only one inference rule , namely the reduction rule (1) 3In (1) , X , G and Gt are literals , ~ is a ( possibly empty ) sequence of literals , and a is the most general unifier of G and G ' . The leftmost literal in the . lmdy of a non : unit clause is M ways the selected literal  . 
X ~( ;' A ~ (. ~ l(:__~(.x+-~)(~)
In 1) riuciple , this rule can be applied to any pair of unit clanses and non : unit clauses of the program to derive any consequences of the pro:gram  . In order to reduce this search space and achieve a more goal-directed behaviour  , the rule is not applied to any pair of clauses , but clauses are on \] y selected if they can contribute to a proof of the goal  . The set of selected clauses is ( ; ailed the chart .   3 The selection of clauses is guided by a scanning step  ( section 2 . 1) an ( lindexing of clauses ( section 2 . 2) . 
2.1 Scanning
The purpose of the scanning step , whic : h corresponds to lexical lookup in chart parsers  , is to look up base cases of recursive definitions to serve as a starting point for bottom-up rocessing  . The scanning step selects clauses that can appear as leaves in the proof treel bragivengoal C  . 
Consider the following simple definition of an HPSG  , with the recursive definition of the predicate sign/I  . 42 This rule is called combine by Earley , and is also referred to as the flmdamental rule in the literature on chart parsing  . 
a Thcchart differs from the state of \[10\] in that clauses in the chart arc indexed ( cf . section 2 . 2) . 
4 We use feature terms in dcfinitc clauses in addition to Prolog terms  , f : X denotes a feature structure where X is the value of h:ature f  , and X ~ Y denotes the conjunction sign ( X ) <-phrasal_sign ( X )  . 
sign(X ) <- lexical sign(X).
phrasal sign ( X ~ dtrs: ( he add tr:RD&comp_dtr:CD ) sign ( RD )  , sign(CD ) , principles ( X , HD , CD) . 
) <- principles(X , HD , CD ) <- constituent_order_principle(X , HD , CD ) , head_feature principle(X , RD ) , constituent order principle ( phon : X_Ph , phon : HD_P h , phon:CD_Ph ) <- sequence_union(CD_Ph , HD_Ph , X_Ph) . 
The predicate sign/1 is defined recursi-vely , and the base case is the predicate lexical_s ign  /1   . But , clearly it is not restrictive enough to find only the predicate name of the base case for a given goal  . The base cases must also be instantiated in order to find those that are useful for proving a given goal  . In the case of parsing , the lookup of base cases ( lexical items ) will depend on the words that are present in the input string  . This is implied by the first goal of the predicate principles  /3   , the constituent order principle , which determi-nesi how the l'nON value of a constituent is con-strutted from the eflON values of its daughters  . 
in general , we assume that the constituent order principle makes use of a linear and nonerasing oI  ) eratkmt or combining strings J If this is the case  , then M1 the words contained in the PnON value of the goal can have their lexical items selected as unit clauses to start bottom-up processing  . 
l % r generation , an analogous condition on logical forms has been proposed by Shieber  \[13\] as the " semantic monotonicity condition , " which requi-res that the : logical form of every base case must subsume some portion of the goal's logical form  . 
Base case lookup must be defined specifically tbr different grammatical theories and directions of processing by the predicate  lookup/2   , whose first argument is the goal and whose second argument is the selected base case  . The following of the feature terms X and Y . 
r ' There is an obvions connection to the Linear ContextFree Rewriting Systems  ( LCFRS )  \[15 ,  16\] . 
7 97 clause defines the lookup relation for parsing with 

7 . lookup (+ Goal , -BaseCase ) lookup(phon:PhonList , lexical_sign ( phon:\[Word\]~synsem:X )   ) <-member ( Word , Phon List ) , lexicon ( Word , X) . 
Note that the base case clauses can become further instantiated in this step  . If concatenation ( of difference lists ) is used as the operation on strings , then each base case clause can be instantiated with the string that follows it  . This avoids combination of items that are not adjacent in the input string  . 
lookup(phon:PhonList , lexical_sign ( phon:\[Word\[Suf\]-Sufsynsem:Synsem )   ) <- append ( _ , \[ Word IS uf \] , Phon List ) , lexicon ( Word , Synsem ) . 
In bottom-up Earley deduction , the first step towards proving a goal is perform lookup for the goal  , and to add all the resulting ( unit ) clauses to the chart . Also , all non-unit clauses of the program , which can appear as internal nodes in the proof tree of the goal  , are added to the chart . 
The scanning step achieves a certain degree of goal-directedness for bottom-up algorithms bec -ause only those clauses which can appear as lea -vesin the proof tree of the goal are added to the chart  . 
2.2 Indexing
An item in normal contextfree chart parsing can be regarded as a pair  ( R , S ) consisting of a dotted rule R and the substring S that the item covers  ( a pair of starting and ending position )  . The fundamental rule of chart parsing makes use of these string positions to ensure that only adjacent substrings are combined and that the result is the concatenation f the substrings  . 
In grammar formalisms like DCG or IIPSG , the complex nonterminals have an argument or a feature  ( PtION ) that represent she covered substring explicitly . The combination of the substrings is explicit in the rules of the grammar  . As a consequence , Earley deduction does not need to make use of string positions for its clauses  , as Pereira and Warren\[10\]point out . 
Moreover , the use of string positions known from chart parsing is too inflexible because itel = lows only concatenation of adjacent contiguous substrings  . In linguistic theory , the interest has shifted from phrase structure rules that combine adjacent and contiguous constituents o?principle-based approaches to grammar that state general wellformedness conditions instead of describing particular constructions  ( e . g . IIPSG ) ? operations on strings that go beyond conca -tenation  ( headwrapping\[11\] , tree adjoining\[15\] , sequence uuion\[12\]) . 
The string positions known from chart parsing are also inadequate for generation  , as pointed out by Shieber \[13\] in whose generator all items go from position 0 to 0 so that any item can be combined with any item . 
I to we ver , the string positions are useful as an indexing of the items so that it can be easily detec-ted whether their combination can contribute to a proof of the goal  . This is especially important for a bottom-up algorithm which is not goal-directed like topdown processing  . Without indexing , there are too many combinations of items which are useless for a proof of the goal  , in fact there may be infinitely many items so that termination problems can arise  . 
For example , in an order-monotonic grammar formalism that uses sequence union as the operation for combining strings  , a combination of items would be useless which results in a sign in which the words are not in the same order as in the input string  \[14\]  . 
We generalize the indexing scheme from chart parsing in order to allow different operations for the combination of strings  . Indexing improves efficiency by detecting combinations that would fall anyway and by avoiding combinations of items that are useless for a proof of the goal  . 
We define an item as a pair of a clause Cl and an indexIdx  , written as ( Cl ~ Idx . 

Below , we give some examples of possible indexing schemes  . Other indexing schemes can be used if they are needed  . 
1 . Non-reuse of Items : This is useful for LCFRS , where no word of the input string can be used twice in a proof  , or for generation where no part of the goal logical form should be verbalized twice in a derivation  . 
2 . Non-adjacent combination : This indexing scheme is useful for order-monotonic grammars  . 
3 . Non-directional adjacent combination : This indexing is used if only adjacent constituents can be combined  , but the order of comhination is not prescrihcd ( e . g . nondirectional basic categorial grammars ) . 
4 . Directional adjacent combination : This is used tbr grammars with a " context-flee backbone  . " 5 . Free combination : Allows an item to be used several times in a proof  , to rexample for the non-unit clauses of the program  , which would be represented as items of the form < X ~ - -  ( 11 A .   .   . hGn , fret ;) . 
The following table summarizes the properties of these live coml  ) ination schemes . Index 1 ( 11 ) is the index associated with the non-unit clause , Index 2 (12) is associated with the unit clause , and I1*12istit ( ; result of coml ) ining tile indices . 
IIndex1Index 2 Result\[Nolc 1211, 12\[1 . XYXUYIX rqY -- q)2 . X Y X ( . ) YI3 . X+Y Y + ZX+ZI
Y + ZX+YX+ZI .   .   .   .   .   .   .  + 4 . X-YY-ZX-ZI 5 . l--~--' free ~ .   .   .   . ~--+ In case 2 (" nonadjacent combinatiou ") , the indices X and Y consist of a set of string positions  , and tile operation ( : ) is the union of these string positions , provided that no two string positions fi'omX and Y do overlap  . 
In (2) , the reduction rule is augmented to handle indices  . X , Y denotes the combination of the indices X and Y . 
( X+-GAfl , ll ) ( c'+-,12/+-a ), n . 12>(2) With the use of indices , the lookup relation becomes a relation between goals and items  . The following specification of the lookup relation provides indexing according to string positions as in a chart parser  ( usable for combination schemes2 ,  3 , and 4) . 
lookup(phon:PhonList , item ( lexical sign(phon:\[Word\]&synsem:X ) , 
Begin-End )) <- nth_member(Word , Begin , End , Phon List ) , lexicon ( Word , X ) . 
nthmember(X , O , l,\[X\[_\]).
nth member ( X , NI , N2 , \[_lR \]) <- nth member(X , NO , Ni , R) , 
N2 is N1+1.
2.3 Goal Types
In constraint-based grammars there are some predicates that are not adequately dealt with by bottom-up Earley deduction  , for example the llead Feature Principle and the Subcategorization Principle of nr sG  . The Head Feature Principle just unifies two variables  , so that it can be executed at compile time and need not be called as a goal at runtime  . The Subcategorization Principle involves an operation on lists  (   appond/3 or dolo t+/3 in different formalizations ) that does not need bottom-up processing , but can better be evaluated by topdown resolution if its arguments are sulficiently instantiated  . Creating and managing items for these proofs is too much of a computational overhead  , arid , moreover , a proof may not terminate in the bottom-up case because infinitely many consequences may be derived fi'om the base case of a recursively defined relation  . 
In order to ( teal with such goals , we associate the goals in the body of a clause with goal types  . 
The goals that are relevant for bottom-up Earley deduction are called wailing goals because they wait until they are activated by a unit clause that unifies with the goal fiWhenever at unit clause is  6The other goM types arc topdown goals ( topdown the first waiting goal of the resulting clause are proved according to their goal type  , and then a new clause is added whose selected goal is the first waiting goal  . 
In the following inference rule for clauses with mixed goal types  , E is a ( possibly empty ) sequence of goals without any waiting goals , and 9 t is a ( possibly empty ) sequence of goals starting with a waiting goal . ( r is the most general unifier of G and G ~ , and the substitution v is the solution which results from proving the sequence of goals ~  , ( X+--GA ~ A ~ , I 1)(a '~- , x2)(ra(X~f ~) , 11*12) ( a ) 2 . 4 Correctness and Completeness In order to show the correctness of the system  , we must show that the scanning step only adds consequences of the program to the chart  , and that any items derived by the inference rule are consequences of the program clauses  . The former is easy to show because all clauses added by the scanning step are instances of program clauses  , and the inference rule performs a resolution step whose correctness i wellknown in logic programming  . The other goal types are also proved by resolution  . 
There are two potential sources of incomple-teness in the algorithm  . One is that the scanning step may not add all the program clauses to timchart that are needed for proving a goal  , and the other is that the indexing may preventhe derivation of a clause that is needed to prove the goal  . 
In order to avoid incompleteness , the scanning step must add all program clauses that are needed for a proof of the goal to the chart  , and the combination of indices may only fail for inference steps which are useless for a proof of the goal  . That depth-first search ) , x-corner goals ( which combine bottom-up and topdown processing like left-corner or head-corner algorithms  )  , Prolog goals ( which are directly executed by Prolog for efficiency or side-effects  )  , and chart goals which create a new , independent chart for the proof of the goal . 
DSrre\[3\] proposes a system with two goal types , namely trigger goals , which lead to the creation of items and othcr goals which don't  . 
the lookup relation and the indexing schemes a -tisfy this property must be shown for particular grammar formalisms  . 
In order to keep the search space small ( and finite to ensure termination ) the scanning step should ( ideally ) add only those items that are needed for proving the goal to the chart  , and the indexing should be chosen in such a way that it excludes derived items that are useless for a proof of the goal  . 
3 Best First Search
For practical NL applications , it is desirable to have a bestfirst search strategy  , which follows the most promising paths in the search space first  , and : finds preferred solutions before the less preferred ones  . 
There are often situations where the criteria to guide the search are available only for the base cases  , for example ? weighted word hypotheses from a speech recognizer ? readings for ambigous words with probabilities  , possibly assigned by a stochastic tagger ( el .  \[2\] ) hypotheses for correction of string errors which should be delayed  \[5\] Goals and clauses are associated with preference values that are intended to model the degree of confidence that a particular solution is the ~ correct ' one  . Unit clauses are associated with a numerical preference value  , and non-unit clauses with a formula that determines how its preference value is computed fi ' om the preference va-lues of the goals in the body of the clause  . Prefe-rence values can ( but need not ) be interpreted as probabilities .   7 The preference values are the basis forgiving priorities to items  , l'b run it clauses , the priority is identified with the preference value  , tibr non-unit clauses , where the preference formula may contain uninstantiated variables  , the priority is the value of the formula with the free variables instantiated to the highest possible preference value  ( in case 7For further details and example see \[4\] and \[5\]  . 
800 of an interpretation as probabilities : 1) , so that the priority is equal to the maximal possible preference valne for the clause  , s The implementation of bestfirst search does not combine new itelns with the chart immedia-tely  , but makes use of an agenda\[8\] , on which new items are ordered in order of descending priority  . 
The following is the algorithm for bottom-up bestfirstF  , arley deduction . 
procedure prove ( Goal ) :- initialize-agenda ( Goal ) -consume-agenda-for any item G , I ) - return mgu ( Goal , G ) assolution if it exists procedure initialize -agenda  ( Goal ) :- for every unit clause UC in lookup ( Goal , UC)-create the index I for UC-add item ( UC , I ) to agenda-for every non-unit program clause H+ -Body-add item  ( H " , --13 ody . free ) to agenda procedure add item 1 to agenda-compute the priority of I-agenda := agenda  12 I procedure consume-agenda-while agenda is not empty-remove item I with highest priority from a genda-add item I to chart procedure add item  ( C , It ) to chart-chart := chart O(C , I 1 ) - if 6' is a unit clause-for all items ( H~--GAEA ~ , 12)-if I=12-kI1 exists and r , = mgu(C , G ) exists and goals ~ are provable with solution r then add item  ( ra ( H ~ - ~ )  ,  1 ) to agenda-if C = H~-GAEA ~ is a non-unit clause-for all items  ( G ' ~- , I2)-if I = I1-kI2 exists and ~ r = mgu(G , G ' ) exists and goals ~ are provable with solution r then add item  ( ra ( lt+-~2 )  , I ) to agenda The algorithm is parametrized with respect o the relation  lookup/2 and the choice of the inde-xing scheme , which are specific for difi'erent grammatical theories and directions of processing  . 
S There are also other methods forms signing priorities to items  . 
4 Implementation
The bottom-up Earley deduction algorithm described here has been implemented in Quintus Prolog as part of the GeLD system  . GeLD ( Gene-raJized Linguistic Deduction ) is an extension of Prolog which provides typed feature descriptions and preference values as additions to the expressi-vity of the language  , and partial evaluation , topdown , head-driven , and bottom-up Barley deduction as processing strategies  . Tests of the system with small grammars have shown promising re-salts  , and a medium-scale HPSG for German is pre-sently being implemented in GeLD  . The lookup relation and the choice of an indexing scheme must be specified by the user of the system  . 
5 Conclusion and thlture Work
We have proposed bottom-up Earley deduction as a useful alternative to the topdown methods which require subsumption checking and restriction to avoid prediction loops  . 
The proposed method should be improved in two directions  . The first is that the lookup predi- ( : ate should not have to be specified by the user , but automatically inferred from the program . 
The second problem is that all non-unit clauses of tile program are added to the chart  . The addition of non-unit clauses should be made dependent on the goal and the base cases in order to go from a purely bottora-up algorithm to a directed algorithm that combines the advantages of topdown and bottom-up rocessing  . It has been repeatedly noted\[8 ,  17 ,   1\] that directed methods are more efficient than pure topdown or bottom-up methods  . However , it is not clear how well the directed methods are applicable to grammars which do not depend on concatenation and have no unique ' left cornet " which should be connected to the start symbol  . 
It remains to I ) ese eit how bottom-up Barley deduction compares with  ( and can be combined with ) the improved topdown Barley deduction of l ) hrre\[3\] , Johnson\[7\] mudNeumann\[91 , and to head-driven methods with wellformed substring tables  \[1\]  , and which methods are best suited for which kinds of problems  ( e . g . parsing , generation , noisy input , incremental processing etc . ) . 

References \[1\] Gosse Bouma and Gertjan van Noord . Head-driven parsing for lexicalist grammars : Experimental results  . In EACL93, pages 71-80,
Utrecht , NL , 1993.
\[2\] Chris Brew . Adding preferences to CUF.
In Jochen D6rre , editor , DYANA-2 Deli-verable RI . 2 . A : Computational Aspects of Constraint-Based Linguistic Descrotion I  , pages 57-69 . Esprit Basic Research Project 6852, 1993 . 
\[3\] Jo chen D6 rre . Generalizing Earley deduction for constraint -based grammars  . In Jochen D6rre , editor , DYANA-2 Deliverable RI . 2 . A : Computational Aspects of Constraint-Based Linguistic Description I  , pages 23-41 . Es-prit Basic Research Project 6852, 1993 . 
\[4\] Greg or Erbach . Using preference values in ty-ped feature structures to exploit non-absolute constraints for disambiguation  . In Harald Trost , editor , Feature Formalisms and Linguistic Ambiguity . E1 Jis-Horwood , 1993 . 
Gregor Erbach . Towards a theory of degrees of grammaticality . In Carlos Martfn-Vide , editor , Current Issues in Mathematical Linguistics . North-Holland , Amsterdam , to appear . Also published as CLAUS Report 34,
Universit?t des Saarlandes , 1993.
\[6\] Dale Douglas Gerdemann . Parsing and Ge-neration of Unification Grammars . PhD thesis , University of Illinois at Urbana-Champaign ,  1991 . Cognitive Science technical report CS-91-06 ( Language Series )  . 
\[7\] Mark Johnson . Memoization i constraint logic programming . Department of Cognitive Science , Brown University . Presented at the 1st International Conference on Constraint Programming  , Newport , Rhode Island ; to appear in the proceedings ,  1993 . 
\[8\] Martin Kay . Algorithm schemat and data structures in syntactic processing  . Technical Report CSL-80-12, XEROXPARC , Palo
Alto , CA , 1980.
\[9\] Giinter Neumann . A Uniform Tabular Algorithm for Natural Language Parsing and Generation and its Use within Performance-based Methods  . PhD thesis , University
Saarbrficken . forthcoming.
\[10\] Fernando C . N . Pereira and David H . D . Warren . Parsing as deduction . In ACL Proceedings , 21st Annual Meeting , pages 137-144 ,  1983 . 
\[11\] Carl Pollard . Generalized ContextFree
Grammars , Head Grammars , and Natural
Language . PhD thesis , Stanford , 1984.
\[12\] Mike Reape . A theory of word order and discontinuous constituency in West Germanic  . 
In E . Engdahl and M . Reape , editors , Parametric Variation in Germanic and Romance : Preliminary Investigations  , pages 2540 . ~ S-PRIT Basic Research Action 3175 DY ANA , Deliverable RI . I . A , 1990 . 
\[13\]StuartM . Shieber . A uniform architecture for parsing and generation  . In Proceedings of the 12th International Conference on Computational Linguistics  ( COLING )  , Budapest ,  1988 . 
\[14\] Gertjan van Noor d . Reversibility in Natural Language Processing . PhD thesis , Rijksuni-versiteit Utrecht , NL ,  1993 . 
\[15\]K . Vijay-Shanker , David J . Weir , and Ara-vind K . Joshi . Characterizing structural descriptions produced by various grammatical formalisms  . In 25th Annual Meeting , pages 104-111 , Stanford , CA ,  1987 . Association for
Computational Linguistics.
\[16\]David J . Weir . Characteri-zing Mildly Context-Sensitive Grammar Formalisms  . PhD thesis , Department of Computer and Information Science , University of
Pennsylvania , 1988.
\[17\] Mats Wir6n . A comparison of rule-invocation strategies in contextfree chart parsing  . In ACL Proceedings , Third European Conference , pages 226-235 ,  1987 . 

