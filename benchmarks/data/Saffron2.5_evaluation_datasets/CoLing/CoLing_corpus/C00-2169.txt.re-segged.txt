Processing Self Corrections in a speech to speech system 
J SrgSpilker , Martin Klarner , Gfinther G6rz
University of Erlangen-Nuremberg-Computer Science Institute  , 
IMMD8-Artificial Intelligence,
AmWeich selgarten9 , 91058 Erlangen-Tennenlohe , Germany
spilker , klarner , goerz ~ immd 8, informatik . uni-erlangen , de

Speech repairs occur often in spontaneous spoken dialogues  . The ability to detect and correct those repairs is necessary for any spoken language system  . We present a framework to detect and correct speech repairs where all tel-evant levels of information  , i . e . , acoustics , lexis , syntax and semantics can be integrated . The basic idea is to reduce the search space for repairs as soon as possible by cascading filters that involve more and more features  . At first an acoustic module generates hypotheses about the existence of a repair  . Second a stochastic model suggests a correction for every hypothesis  . Well scored corrections are inserted as new paths in the word lattice  . Finally a lattice parser decides on accepting the repair  . 
1 Introduction
Spontaneou speech is disfluent . In contrast to read speech the sentences aren't perfectly planned before they are uttered  . Speakers often modify their plans while they speak  . This results in pauses , word repetitions or changes , word fragments and restarts . Current ml to-rustic speech understanding systems perform very well in small domains with restricted speech but have great difficulties to deal with such disfluencies  . A system that copes with these self corrections ( = repairs ) must recognize the spoken words and identify the repair to get the intended meaning of an utterance  . To characterize a repair it is commonly segmented into the following four parts  ( el . fig . i ) : ? reparandum : the " wrong " part of the utterance ? interruption point  ( IP ) : marker at the end of the reparandum ? editing term : special phrases  , which indicate a repair like " well " , " I mean " or filled pauses such as " uhln ' ~ , " uh "? reparans : the correction of the reparandum on Thursdayl cannot ? no I can meet " a hafter on c//\-/""Rct  ) ar and mn Interruption-Editing Rc parans point Term
Figure 1: Example of a self repair
Only if reparandum and editing term are known , the utterance can be analyzed in the rightway . It remains an open question whether the two terms should be deleted before a semantic analysis as suggested sometimes in the literature  1  . If both terms are marked it is a straightforward preprocessing step to delete reparandum and editing term  . In the Verbmobil 2 corpus , a corpus dealing with appointment scheduling a . n d tr ~ vel planning , nearly 21% of all turns contain at least one repair . As a consequence a speech understanding system thai  ; cannot handle repairs will lose perforlnance on these turns  . 
Even if repairs are defined by syntactic and semantic wellformedness  ( Levelt ,  1983 ) we observe that most of them are local phenomena . .
At this point we have to differentiate between restarts and other repairs a  ( modification repairs )  . Modification repairs have a strong correspondence between reparandum and reparans  ,   1In most cases are paraudum could be deleted without any loss of information  . But , for exmnple , if it introduces an object which is referred to later  , a deletion is not appropriate . 
> l?his work is part of the VERB MOBIL project and was funded by the German Federal Ministry for Research and Technology  ( BMBF ) in the framework of the Verbmobil Project under Grant BMBF  01 IV 701   V0  . The responsibility for the contents of this study lies with the authors  . 
SOften a third kind of repair is defined : " abridged repairs "  . These repairs consist solely of an editing term and are not repairs in our sense  . 
1116 whereas restarts a . reless structured . In our believe there is nonted for a . complete syntacticam@s is to detect ~ md correct most modification repairs  . Thus , in wh ~ tt follows , we will concen-tra . te on this ldnd of repa . ir . 
There are two major arguments to process repairs before t  ) arsing . Primarily spontaneous speech is not always syntactically wellformed even in the absence of sell'corrections  . Second ( Meta -) rules increase the pa . rsers'search space . This is perhaps acceptable for transliterated speech but not for speech recognizers output like l ~ ttices because they represent millions of possible spoken utterances  . \[ n addition , systems whk ; ha . renot based on a . deep syntactic and semantic amdysise . g . statistical dialog act prediction -- require a repa  . ir processing step to resolve contr~dictions like the one in tit  .  1 . 
We propose all algorithm for word latticesth , ~t divides repa . ir detection a . nd correction in three steps ( of . fig . 2) l " irst , ~ r trigger indicates potential 1Ps . Second , as l ; ochasl , ic model tries to lind an appropria . te repair h ) reach IP by guessing 1 , hemosl ; l ) robable segmentation , q b accomplish this , repair processing is seen as a statistical machine translation problem where the repa  . randum is a transl~tion of the reparans . 
For every repair found , apa . threp resenting the spcaker . ' intended word sequence is inserted into the la . ttice . In the last step , a lattice parser selects the best pa . th . 
tlll ' llllll'S day I ? iIIlllt ) lIlO\[CIIIIlllCel " ahtillert ) llCgpeec\]lI't'cOgtllZCiwllnlIoSllyICOll i  i1 il Oll+l'htus day 1 C ; lllllOIlit )  \]  t'311 IIIL'L~I'liIhalter 111112 local word based scoped clectiol l of lattice editing  1o represent rest llt lcl ) t t l ' i l l / d t l l l \] ~ . c \] ) alans ! f 1 ?- il J\]?wlliii\[o sayicollill , ii on "\[\] lttlSdllyt'lllllK~l t'atI1 litter I/"lib''tll'\[Cl"tt ) ll CtJ seleclion by 1 linguistic all aly slss 011 " l'htll'g day\]C/Illnlcel " till:tile rolle Figure  2: An architecture for repa . ir processing 2 Repair qh'iggers Because it is impossible for ; trea . l time speech system to check for every word whether it can be part of a repair  , we use triggers which indicate the potential existence of a repa  . ir . These triggers nlllst be immediately detectable for every word in the lattice  . Currently weart using two different ldnd of triggers4:  \] . 

Acoustic/prosodic cuts : Spe~kers mark the 117 in many cases by prosodic signals like 1  ) a uses , hesitations , etc . A prosodic classi-tier 5 determines for every word the proba-bility of an IP following  . If it is above a cer-t~dnl ; hreshold , the trigger becomes active . 
For a detailed description of the acoustic aspects e e  ( Batlinere Lal . , 1998) . 
Word Dagments are a very strong repair indicator . Unfortunately , no speech recognizer is able to detect word fl : agmtnts to date  . But there are some interesting approaches to detect words which are not in the recognizers vocabulary  ( Klakow et al ,  1999) . A word fi'agment is normally an unknown word and we hope that it can bt distinguished from unfra  . gmented unknown words by the prosodic classifier . So , currently this is a hypol ; hetical trigger . We will elaborate on it in the evaluation section  ( cf . sect . 5) to show the impact of this trigger . 
If a trigger is active , a . sea , rch for an acceptable segmentation into rel ) a randum , editing term a . nd reparansis initia . ted . 
3 Scope Detection
As mentioned in the introduction reDfir segmentation is based mainly on a stochastic trans-la  . tion modtl , l~el ' or e we explain it in detail we give a short introduction to statistical machine translation  ?  . The fundalnent a Jidea . is the assumption that a given sentence S in a source language  ( e . g . English ) can be translated in any ^ sentence 5/' in al ; ~ rgel ; I , ~nguage ( e . g . German ) . 
To every pair (5' ,  ~/' ) a probability is assigned which reflects the likelihood that at ra  . nsl ~ tor whose esS will produce \]' as the translation  . 
Thesta . tistical machine translation problem is 4 Other trigger scal , be added as well . ( Stolckectal . , 1999 ) for example integrate prosodic cues and an extended language model in a speech recognizer to detect 

S The classifier is developed by tile speech group of the  IMM1  )  5 . Special thanks to Anton Batliner , Richard
Iluber and Volker Warnke.
~ A more detailed introduction is given by ( Brownel , al . , 1990)
I 117 formul ; ~tedas:5~' = argmaXTI' ( TlS ) This is reformulated by Bayes ' law for a better search space reduction  , but we are only interested in the conditional probability P  ( TIS )  . For further processing steps we have to introduce the concept of alignment  ( Brown et al ,  1990) . 
Let S be the word sequence S1, S2 .   .   .   . 5, l ~ SI and T = ~, T 2 .   .   . Tm~77\] ~ . We can link a word in T to a word in S . This reflects the assumption that the word in T is translated from the word in S  . \]? or example , if S is " On Thursday " and T is " Aml ) onners tag " " Am " can be linked to " On " but also to " Thursday "  . 
If each word in T is linked to exactly one word in  , S ' these links can be described by a vector a ~ ' ~= al  .   .   . a , ~ with ai EO . . . l . If the word 51 ~ . 
is linked to Si then aj = i . If it is not connected to any word in S then aj =   0  . Such a vector is called an alignment a . P ( T \] , 5 , ) can now be expressed by ' ( TIS ) = al , 5 , )  ( 2 ) a is alignment Without any further assumptions we can infer the t bllowing :  )  1 *  (  -45 )   , H \]) ( ajl(t'-l'rj-l'?'"''5 , ) ~
J - -' T i i -', m ,, 5, ) (3)
Now we return to self corrections . How can this framework help to detect the segments of a repair ? Assulne we have a lattice l  ) ~th where the repar and n . ( m )) a , d the reparans(S ) are given , then ( RS , \]D ) can be seen as a . translation pair and P(RD\]R , 5 , ) can be expressed exactly the same way as in equation  ( 2 )  . Hence we have a method to score ( ITS , P~D ) pairs . . But the triggers only indicate the interruption point  , not the complete segmentation . Let us first look at editing terms . We assume them to be a closed list of short phrases  . Thus if an entry of the editing term list ; is found after an 1P , the corresponding words are skipped . Any subsequence of words befbre/after the IP conld be the reparanduln/reparans  . Because turns ca . nh ~ we an arbitrary length it is impossible to compute P  ( I-~D\]IL5 , ) for every ( RS , H . D ) pair . Bug this is not necessary at all , if repairs are considered as local phenomena . We restrict our search to a window of four words before and after the IP  . A corpus analysis showed that 98% of all repairs are within this window . Now we only have to compute probabilities for 42 difl'erent pairs . If the probability of a ( RS , RD ) pair is above a certain threshold , the segmentation is accepted as a repair . 
3.1 Parameter Estimation
The conditional probabilities in equation ( 3 ) cannot be estimated reliably fi'o many corpus of realistic size  , because there are too many p ~> rameters . For example both P in the product depend on the complete reparans R  , 5 ,  . Therefore we simplify the probabilities by assuming that m depends only onl  , a . i only on j,m and l and finally RDj on 1L5,, . j . So equation (3) becomes
P ( ZD , siZeS):\]-I(4) j = l
These probabilffies can be directly trained fi'or n anlannally annotated corl  ) ns , where all repairs are labeled with begin , end , liP and editing term and for each l'epar and nnl the words are linked to the corresponding words in the respective reparalls  . All distributions are smoothed by a simple backoff method  ( Katz ,  1987 ) to avoid zero probabilities with the exception that the word replacement probability P  ( I~I ) jIILS , j ) is smoothed in a more sophisticated way . 
3.2 Smoothing
Even it " we reduce the number of parameters for the word replacement probability by the simplifications mentioned above there are a lot of parameters left  . With a vocabulary size of 2500 words , 25002 paralneters have to be estimated for P ( I~DjllL5 , ~j ) . The corpus 7 contains 3200 repairs fi'om which we extra . ct about 5000 word links . So most of the possible word links never occur in the corpus  . Some of the ln are more likely to occur in a repair than others  . For example , the replacement of " Thursday " by "\] M-clay '" is supposed to be more likely than by " eat'ing  "  , even if both replacements are not in the training corpus  . Of course , this is related to 7~110006urns with ~240000 wordsmanticanomaly . We makense of it by a . dding two additional knowledge sources to our model  . 
Minimal syntactic information is given by part-of speech  ( POS ) tags and POS sequences , se-mmltic information is given by semantic word classes  . I lence the input is not merely a sequence of words but a sequence of triples  . Each triple has three slots ( word , POS tag , semantic class ) . In the next section we will describe how we ol ) ta in these two information pieces \[ brevery word in the lattice  . With this additional inform a . tion , P ( RDjI1LS' , ~ j ) probability could 1) esmoothed by line a . r interpolation of word , POS and semantic la . ss replacement \]) robabilities . 
= n , , l'(Word(l . Dj ) ll 4 ro . rd(n,S' . .j ) ) +/3 , + with a '+\ [3+7=1 . 
l'Vord(IM):i ) is the not ~ tiont br1 ; 11 (: selector of the word slot of the triple a , t position j . 
4 Integration with Lattice

Weca , llllOW del ; e(;ta , nd correct a , repa , ir , given a sentence a . nnotated with I )() Stag ; san(Iseman-1; ic classes , l ~ tll , how ca . nwe('onsl ; rucl , such a . 
sequence , from a wor ( lla . tl ; ic (< ? Integrating then to delinal attice algoril ; hm requires three steps : ? mapping the word la ? tice to a  . tag lattice ? triggering IPs and extra . cting the possible rel ) ar ; mdum/reparansl ) : ~ irs ? intr < ) ducing new paths to represent tile plausible repa . rans The tag lattice constrnction is adapted from ( Samuelsson ,  11997) . For every word edge and every denoted POS tag a corresponding tag edge is crea  , ted and tim resulting prol)ability is determined . \[ I'a tag edge already exists , tile probabilities of both edges are merged . 
The original words are stored together with their unique semantic lass in a associated list  . 
Paths through the taggraph a . rescored by a IX ) S-trigram . If a trigger is active , all paths through the word before timll'need to be tested whether an acceptable rel  ) air segmentation exists . Since the scope model takes at most \[' our words for reparanduma  . ndrel ) a . ra . ns in account it is sufficient to expand only partial paths  . 
l) ; a ch of these partial paths is then processed by the scope model  . To reduce these ~ rch space , paths with a low score can be pruned . 
Repair processing is integrated into the Verbmobil system as a  . filter process between speech recognition a . nd syntactic analysis . This enforces are p ~ fir representation that ca . n be into-grated into a lattice . It is not possible to ln a . rk only the words with some additional information  , because a rel ) air is a phenomenon that ( le-pends on a path . Imagine that the system has detected ~ repair on ~ certain path in the btttice and marked all words by their top  , fir function . 
Then a search process ( e . g . the parser ) selects a different D~th which shares only the words of the repa  . randum . But these words are no reparandum for this path . A solution is to introduce a new path in the . lattice where reI ) aranduma . nd editing terms a . redeleted . As we said betbre , we do not want l ; o delete these segments , so they are stored in a special slot of 1 ; 11 of irst word of the reparans . The original path can now 1 ) ere construct if necessary . 
To ensure that these new I ) at hs are coml ) ~> ra . ble to other paths we score the reparandum the same wa  . y the l ) arser does , and add the re-suiting w due to the \[ irst wor ( l of the reparaits . 
As a result , l > oth the original path a . nd the . one wil , h the repair get thesa . mescore excel ) tone word tra . nsition . The ( proba . blybad ) transition in l , he original path from the last wordo\["therei ) a r and t n n to the first word of 1 ; here pa . rans is rel ) laeed by a . ( proba . bly go o(t ) transition From the repa . ran(hnn~sonset to there l > arans . \ Ve take the lattice in fig . 2 to give an example . 
The SCO l ) emo ( M hasma . rked " lca . n not " as the reparandum , " no " as an editing term , and " lca . n " as the rel ) arans . We sumtip the acoustic scores of "1" , " can " and " no " . Then we add the maximnm language model scores for the tra  . n-sition to "1" , to " can " given " I " , and to " no " given ' T ' and " can " . This score is ~ ( I ( le ( 1 as an offset to the acoustic score of the second  "1"  . 
5 Results and Further Work
Due to the different trigger situations we performed two tests : One where we use only acoustic triggers and ~ mother where the existence of a perfect word fr~gment detector is as-sume  ( 1 . The input were unsegmented translit-era . ted utterance to exclude intluences a word 1   1   19 recognizer . We restrict the processing time on a SUN/ULTIA 300MIIZ to 10 seconds . The parser was simulated by a word trigram . Training and testing were done on two separated parts of the German part of the Verb mobil corpus  ( 12558 turns training/1737 turns test )  . 
Detection Correct scope
Recall Precision Recall Precision
Test 149% 70% 47% 70%
Test 271% 85% 62% 83%
A direct comparison to other groups is rather difficult due to very different corpora  , evaluation conditions and goals . ( Nakatani and Hirschberg , 1 . 993 ) suggest a acoustic/prosodic detector to identify IPs but don't discuss the problem of finding the correct segmentation idepth  . Also their results are obtained on a corpus where every utterance contains at least one repair  . ( Shriberg ,  1994 ) also addresses the acoustic aspects of repairs . Parsing approaches like in ( Bear et al , 1992; Itindle , 1983; Core and Schubert ,  1999 ) must be proved to work with lattices rather than transliterated text  . An algorithm which is inherently capable of lattice processing is prot  ) osed by Heeman ( Hem-nan ,  1997) . He redefines the word recognition problem to identify the best sequence of words  , corresponding POS tags and special rel ) air tags . 
He reports a recall rate of 81% and a precision of 83% for detection and 78%/80% tbr correction . The test settings are nearly the same as test 2 . Unibrtunately , nothing is said about the processing time of his module  . 
We have presented an approach to score potential reparandum/reparans pairs with a relative simple scope model  . Our results show that repair processing with statistical methods and without deep syntactic knowledge is a promising approach at least for modification repairs  . 
Within this fi'alne work more sophisticated scope models can be evaluated  . A system integration as a filter process is described  . Mapping the word lattice to a POS tag lattice is not optimal  , because word inlbrmation is lost in the search t br partial paths  . We plan to implement a combined combined POS/word tagger  . 

A . Batliner , R . Kompe , A . Kiettling , M . Mast,
H . Niemann , and F, . NSth .  1998 . M = syntax + prosody : A syntactic-prosodic labelling schema for large spontaneous speech databases  . Epeech Communication , 25:193-222 . 
J . Bear , J . Dowding , and E . Shriberg . 1992.
Integrating multiple knowledge sources \[" or detection and correction of repairs ill human computer dialogs  . In Proc . ACL , pages 5663 , University of Delaware , Newark , 

P . F . Brown , J . Cocke , S . A . Della Pietra , V . J . 
Della Pietr ~, F . Jelinek , J . D . Lafferty , R . L . 
Mercer , and P . S . Rooss in .  1990 . Asta . tisti-cal approach to machine translation . Computational Linguistics , 16(2):79-85, June . 
M . G . Core and K . Schubert .  1999 . Speech repairs : A parsing perspective . Satellite meeting ICP IIS 99 . 
P . A . I-Iceman .  1997 . Speech Repairs , Intonation Boundaries and Discourse Markers : Modeling Epeak crs ' Utterances in  , 5' pokcn Dialog . Ph . l ) . thesis , University of Rochester . 
D . Hindle .  1983 . Deterministic parsing of syntactic nontluencies . In Proc . ACL , MIT,
Cambridge , Massachusetts.
S . M . Katz .  1987 . Estimation of probabilities from sparse data for tile language model con > ponent of a speech recognizer  . 7)' ansaction on Acoustics ,   , 5' pcech and , 5' ignal 1) rocessing , 
ASSl'-35, March.
ill ). Klakow , GRose , and X . Aubert . 1999.
OOV-Detection in Large Vocabulary System Using Automatically Defined Word-Fragments as Fillers  . In EUR . OSPEECII'99, volume 1, pages 4:9-52, Budapest . 
W . Levelt .  1983 . Monitoring and self-repair in speech . Cognition , 14:41-104 . 
C . Naka . tani and a . tlirschberg .  1993 . A speech-tirst model for repair detection and correction  . In P , vc . ACL , Ohio State University,
Cohmbus , Ohio.
C . Samuelsson .  1997 . A left-to-right tagger for word graphs . In Proc . of the 5th Inter'national workshop on Parsing technologies , pages 171-178 , Bosten , Massachusetts . 
E . E . Shriberg .  1994 . Preliminaries to a Theory of Epeech Disflucn cics . Ph . D . thesis , University of California . 
A . Stolcke , E . Shriberg , D . Hakkani-Tur , and G . Tur .  1999 . Modeling the prosody of hidden events for improved word recognition  . In EUROS'PEECII'99 , volume 1 , pages 307-310 , Budapest . 

