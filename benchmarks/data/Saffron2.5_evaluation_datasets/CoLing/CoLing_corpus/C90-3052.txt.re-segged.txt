Typed Unification Grammars
Martin C . Emele , Dhni Zajac
Project Polygloss *
University of Stuttgart
IMS ~ CL/Ifl ~ AIS , Keplerstrage 17,
D-7000 Stuttgart1, Federal Republic of Germany
emele , zajac@is.informatik.uni-stutgart.dbp.de

We introduce TFS , a computer formalism in the class of logic ibrmaiisms which integrates a powerful type system  . Its basic data structures are typed feature structures  . 
The type system encourages an object-oriented approach to linguistic description by providing a multiple inheritance mechanism and an inference mechanism which allows the specitication of relations between levels o\[linguistic description defined as classes of objects  . We illustrate this alc-proach starting from a very simple DCG  , and show how to make use of the typing system to enforce general constraints and mod-ularize linguistic descriptions  , and how further abstraction leads to at l PSG-Iike grammar  . 
1 Introduction
Various proposals have been made for the integration of type intbrmation in unification -based grammar formali  , nnsto enforce constraints described in a hierarchical way where types are partially ordered with a subtype relation  . Authors describe these extensions as " inheritance grammars "  , " inheritance networks " , ': Ii ; ' ature sorts " , " typedt ~ ature structures " ,  . . . \[1, 3, 5, 13, 17, 15, 9, 11, 7, 8\] . 
These formalisms exhibit , to various degrees , one or several of the following properties , characteristic of the socalled object-oriented paradigm : a high level of abstraction  , a capacity of inference , modularity and distributed control . Abstraction and modularity are needed when the linguist wants to describe a hierarchy of concepts  ( like a lexical hierarchy or the hierarchy of phrasal categories  )  , and to describe linguistic data at different levels  ( e . g . morphology , syntax , semantics ) . At first glance it seems rather natural to develop separate modules for different linguistic levels  , and to describe separately their interactions ; however , great difficulties are encountered when these modules have to be integrated  . Usually , there are two choices . Either everything is described in a single place using a deeply intricate data structure  , like packing both syntactic and semantic equations in CF rules in some LFG extensions  ( e . g .  \[10\]) ; the price is a loss in understmt dability and general ~ ity  . Or descriptions are kept separate and the pro -eessing is done accordingly : first  , a morphological phase , then a syntactic analysis , and then a semantic analysis , without any communication between these different steps  \[4\]  . The price is that interdependent constraints between these levels are lost  , resulting in inadequate linguistic description or very complex control strategies at the implementation level  . 
In this paper , we argue that typed unification grammars give the linguist a formal framework which has the desirable properties  . We will give an introduction to such a formalism , called ' IF , _(~IypedI"ea-tureStructure) , which integrates disjunctions , conjunctions and conditional expressions of typed feature structures  . This introduction will start from a very simple DCG  , and will show how one can write a DCG-like grammar in TFS  , making use of the typing system to enforce general constraints valid for classes of objects and to modularize linguistic descriptions  . 
We then show that further abstraction leads to a I -\[ PSG-like grammar  . It is not our goal to give here a formal account of the formalism  ( the interested reader should refer to \[2\] where a very cleart brmal semantics on which TFS is based is given  )  , and we will use an informal approach wherever possible  . 
2 Typed t~ature s t ructures and unif ication Tlle basic data structure of tile language is a typed featm'estructure : a feature structure  ( FS in the following ) with which a type can be associated . Corn-pared to untyped FSs ( as presented in \[16\] for example )  , the TFS system offers the possibility to name complex FSs  , and to associate constraints with these names , thus defining a type . 
We write feature names in small caps letters ( v ,  ~ , u ) , type symbols in uppercase letters ( A , B ) , and we use symbols inside a box\[~ , called tags , for denoting shared values . For cxarnple , the typed FS , written in a linear form A\[F :\[\[\] B\[H:C\] , a : ~\]\] , is an FS of type A with two features f . " and e , v having as a value tile typed FSB\[H : A \] and G having the same shared value aSF  . 
In the system , one can specify type definitions which can , as a first approximation , be seen as a kind of template definition like in e  . g . PATR-II . There is , however , a major difference . The system uses a type inference mechanism to derive new types dynamically during computation whereas templates in PATR-II are expanded statically at compile time  . 
A type that encodes agreement features can be written : AGR =\[ num:NUM  , gender : GEN\] and types NtJM and GEN being themselves defined as NUM=SING VPLUR  ( where the symbol " y " denotes the logical OR ) and GEN:MASC VFEMVNEU . The types NUM , SG, . . . do not have definitions : they are called atomic types  . AGR , NUM and GEN are called complex types . 
From a set of type definitions , one can extract , a partial order on type symbols . For example , from the * Researdl reported in this paper is partly supported by the German Ministry of Research and Technology  ( BMFT , Bun-desministerf fir Forschung und Technologie ) , under grant No . 08B 311 63 . The views and conclusions contained herein are those of the authors and should not be interpreted as representing official policies  . 
293 set of definitions above , we can derive the following partial order on type symbols  ( Fig . l ) where T represents the greatest element ( no information ) and 3_the smallest element ( inconsistent information , leading to failure in unification ) . This partial order is in turn used to derive a lattice of type syml  ) ols , which is the nexl . ended to typed FSs or ( lere ( 1by ( typed ) sub-stunption , forming a lattice on which the interpreter works ( see a formal account in \[2\] )  . 
/1"~GEN
NUM:\/1~
SIN (; PLUR MASC FEMNEU
For exain ple , the FS flAGR\[nnm:NUM\]subsumes the FS f9 AGR\[num:PLUR , gen ( ler:FEM \] because f2 has more specifc information than fl:no gender is specified in fl  , and the number value of f2 PLUR is more specific than the number value of f l  , NUM . 
Typed unification proceeds as ordinary unification for FSs  , recnrsively corot ) thing substructures at the same paths . When two ( typed ) FSs are unified , frst the type symbols are unified , and if this unification succeeds , the FSs are unified . Unification of two types X and Y is defined .   . ( .   .   .   . as the set of ) most general type ( s ) which is smaller than both x and Y : ~ t is the greatest lower bound  ( GLB ) of these two symbols in the lattice of type syml ) ols . If taso types are directly comparable , the smallest iv the result of the unification : HUMM PLUR = PLUR  . This extension is consistent with the definition of the unifier of two FSs~s the GLB of these structures  ( see , for exam-pie ,  \[16\]) . 
3 Feature types as data types and feature types as relations  3  . 1 The L IST type as a data type A list of words will be defined in a LISP-like fashion as either the END of a list or a CONS with two attributes first and rest:LIST = ENDVCONS\[vms'r:WORD  , nEsT:LIST\] . 
WORD denotes the set of word forms , and the list . of words " John likes Mary " will be encoded as

CONS\[Fm . s'r:CONS\[FroST:LIKESL,-,:ST:LR . < T:ONSV " ST:AR*l\]t , , sr : j j which is a wellformed list . with respect to the LIST definition .   ( We shall use in the following a more concise syntax for lists : END will be written as  0  ; CONS\[FIRsT:WORD , nEs'r:klST\]will be written as ( WORD . LIST ) ; lists will be written using the usual abbreviation for lists : the list of words " John likes Mary " will then be written as  ( JOHNLIKES MARY )  . 
3.2 The APPEND type as a relation
One can also understand feature types as relations much like those in PROLOG  . Let us recall the classical PROLOG definition of append : append  (  \[\]  , L , L) . 
append (\[ XlL1\] , L2 , \[XlL3\]):-append(L1 , L2 , L3) . 
in I > ROLOG , the arguments of a term are identified by their positions in the term  , and the presence ( feature vahles ) are not identified by their position but by a label  , the feature , and the absence of an attrilm te-value pair will denote any kind of value for this attribute  ( type T )  . Using the TFS syntax , where the symbol ':-' after an FS introduces a condition  , a definition for append can be as follows: APPEND = iF:  0  , iS :\[ i ~ LIST , W :\[~\]\] ViF:(\[XJ?gi \]) ,   . :\[\] ~ L,ST,w : (~ . 1i~1)\]:--APPEND\[F : ~ . t-~,13:\[~\], W:\[~\] . 
Note that the tagging syntax allows to specification of identity between structures and a partial instance of the structure  . This possibility ( together with the fact . that typingiven forced by the system ) allows the writing of a typed version of append , in contrast to the untyped PROLOG version . 
3.3 Type checking as deduction
Contrary to PROLOG , there is no distinction in TFS between toplevel types  ( which could be interpreted as predicates ) and inner types ( which could be interpreted as arguments ) : they are all typed FSs , and the same deduction mechanism applies for the toplevel structure ~ swell as for all substructures  . A ( typed ) FS is consistent with respect to a set of type definitions if it unifies with the definition of its type  , and if each of its substructures i also consistent  . 
Conditions like in the definition of append above introduce additional constraints which are erased after having been successfidly evaluated  . When a type iv defined as a disjunction , a structure has to be consistent with at , least , one element of the disjunction ( but all possibilities are explored , creating as many possible solutions as there are disjuncts  )  . When a type is defined as a conjunction ( using the AND operator noted " A " )  , a structure has to be consistent with every single element of the conjunction  . The order used for type checking ( roughly topdown ) guarantees that the solution the system finds is the GLB of the set of definitions augmented by the initial structure  \[2\]  . 
For example , the ( typed ) FSAGR\[num:PLUR\]is consistent with regard to the set of definitions above  ( Sect . l ) . The interpreter will apply the definition of AGR at the root of the FS:AGR\[num:PLUR\]Mnum :NUM  , gender:GEN \] =
AGR\[nunl:PLUR , geuder:GEN
AGR\[num : MASC\] is an inconsistent ( typed ) FS:AGR\[num:MASCJI1\[uum:NUM , gender:GEN\]=I because the types MAS Cail ( \] NUM have only J_ , the bottom of the lattice , ~sacommons nl ) type representing inconsistent information . Note that this type checking process may introduce new type symbols also used for checking  , thus defining a type inheritance mechanism . 
A full evaluation of APPEND\[w : ( AB\]produces a set of three FSs : iF : 0 , ": ID(AB ) , w : ~\] vF :< ~ A .  (>),  . : ~( B ), w : < t ~ . r ~ >\] ve : (~ A . ( m@, . : r~0,~:(@?(m . ~>)\] 4 Type dunification grammars 4 . 1 DCGs In this section , we describe how one can ( but should not ) write grammars using this formalism . To make comparisons easier , we will start from the small example of DCG presented in \[ Pereir and Warren  80\] and show how this grammar ( Fig . 2) can be written in TFS . 
294 sentenee(s(NP , VP )) --+ noun_phrase(Num , NP ) , verb_phrmse(Num , VV) . 
noun_phrase tNum , np(Det , Noun )) -+ determiner(Nnm , Det ) , noun(Nnm , Nonn) . 
noun_phrase ( singular , np(Name))--~name(Name).
verb . .phrase(Num , vp(TV , NP )) ~ trans_verb(Num , TV) , noun_phrasc(N1 , NP) . 
determiner(Num , det(W )) --~\[ W\] , is_determiner(W , Num) . 
noun(Num , n(l~oot )) --+\[ W\] , is_noun(W , Num , Root) . 
name(name(W )) --*\[ W\], is_name(W).
trans_verb(Num , tv(Root )) ~\[ W\] , is_trans(W , Num , Root) . 
( Figure 2) is_determiner(all , pural).
is_noun(man , singular , mais J ~ oun(men , phlral , man ) . 

is_trans(likes , singular , ike ) is_trans(like , plural , like ) . 
In a specification like this , there are three different kinds of information mixed together  . Take for example the rule " noun_phrase ( Num , np(Det , Noun )) determiner ( Num , Det ) , noun(Num , Noun ) " . In " this rule we find : 1 . a specification of a set of w cll-formed substrings using the CF skeleton : noun_phrase-- ~determiner  , noun ; 2 . a specification of wellformed ( partial ) syntactic structures : the structure np ( Det , Noun ) is wellformed if Det and Noun area wellformed structure and if its agreement value  ( variable Num ) is the same for the Det , the Noun , and the noun_phrase ; 3 . a specification of a relation between wellformed ( partial ) syntactic structures and wellformed substrings by augmenting the CF skeleton with annotations representing those structures  . 
4.2 ATFS specification
All this information mixed together can be separated out and specified in a more modular way  . 
1 . The set of wellformed strings of words is defined as in Sect  . 2 . 1, where WORD = all Vmen . . . 
2 . The set of wellformed partial syntactic structures  , i . e . every syntactic on straint like agree-\[Rentor subcategorisation  , should be expressed in this part of the specification  . 
PI4RASAL_CATEGORY=SVNP VVP.
S=\[NP:NP\[AaR:\[~NUM\] , vP:VP\[*oR:\[g3\]\] . 
\[DET:DET\[AGR:Z\]NUM\]I\[NAME : PN\]
NP = Vo , .: N\[,oR:~\]/VL*aR:SG'
LAOR:\[\]~\]J
VP=\[V:TV\[AoR:\[~INUM\] , NP:NP:AGR:\[1~\]\] . 

LEXICAL_CATEGORY : DET VN VP NVV .
DET : ALL VEVERY VAV THE.
ALL:\[WORD:all , AOR : PL\].
N:MANV WOMAN.
MAN=\[WORD:man , AGR:SG\]V\[WORD:men , *GR:Pq . 
PN=JOHN VMARY .
MARY=\[WORD:Mary\].
V :: IV VTV .
TV----LIKE VLOVE.
L , KE=\[WORD : , ike , ,  , on : SG\]V\[WORD : , ike , hoR : PL\] . 
The relation between strings and structures should be stated independently of wellformedness conditions on syntactic structures  . 
It is expressed here in CF manner by using the APPEND relation on strings  . ( However , we do not advocate the exclusive use of CF-like relations  ; more complex ones can be specified to gain expressive power  , e . g . by incorporating linear precedence rules ) . 
SENTENCE ==
NOUN_PHRASE\[sTm\[Na:~LIST , C-STR:\[n~\]VERB_PHRASE\[sTRINO : ~ , C-STm\[~J
APPEND\[P : ~, .:~, w:~\]
NOUN_PHRASE = -\[ STmNO : ~ . ___~ , C-STR:NP\[DI~T:\[~\] ,   , OUN:IK\]\]:--
DETERMINER\[STmNQ : ~, C-STm~\]\]
NOUN\[sTmNO : ~, c-s~.rm\[Ell
APPEND\[F : ~ B : ~, w : ~\]
V\[STRINO : ~, C-STR:PN\[NAME : ~\]\]:--
NAME rs'rmNG : ~, C-STm~\]
VERB-PHRASE :\[ STRING : ~ , C-STR:VP\[v:\[~TV , NP:~\]:--TRANS_VERB\[sTRINO : ~ , C-ST , R:\[~1\]NOUN_PHRASE\[sTmNO : ~ , C-STR : ~\] , 
APPEND\[~:~B : ~, w:~\]
LEXICAL-RULE=\[STRING : (\[~\]) , C-STR:\[WORD:\[~\]\] . 
DETER MINER --= LEXICAL_RULE\[c-sTmDET\].
NOUN=LEXl CAL.RULE\[c-STR:N\].
NAME = LEXICAL-RULE\[C-STmPN\].
TRANS_VERB = LEXICAL-RULE\[C-STR:TV\].
4.3 Parsing and generation
Both parsing and generation i the system amount to type inference  . Either ( 1 ) for parsing or ( 2 ) generation yield the same result ( 3 )  . 
(1  ) SENTENCE\[sTRING : ( Mary likes all men ) \]  ( 2 ) SENTENCE
NP:NP\[NAME:MARY\]
Iv : LIKE
ALL , o:MANIJ (3) SENTENCE"sTR , NG : ( lEMony\[\]like , \[\] a , Rime . )
C-STR:S~:NP\[ . . . E:MARY\[woRD:I~\], . oR : aSG\]vP:VP
Iv:UKE\[woRD:\[\],AGR:~\]
NP:NP\[DET:ALL\[WORD:\[~ , AOR:\[~PL\]\]NOUN:MAN\[woRD:\[~ , AGR:\[b \]\]/
L^o ~:\[\] J
AGR:\[\]
This shows that the formalism has the same power as PI~OLOG to synthesize unspecified arguments  , and the same evaluation mechanism can be used for both generation and parsing  , depending on the input . 
295 4.4 From DCG to HPSG
In the following , we explain how one can generalize the principles used for describing a DCG grammar in TFS to write an HPSG-like grammar  . HPSG linguistic objects of all kinds , be they syntactic , phrase-structural , or semantic , are modeled by feature structures\[14\] . In addition , HPSG relies heavily on the notion of type . Hence , TFS is perfectly suited for an implementation f HPSG  . The grammar itself is purely declarative in the sense that it characterizes what constraints should hold on linguistic objects independently of the order in which these constraints are actually applied  . 
We first generalize the description of linguistic structures : instead of defining explicit types for sentences  , noun phrases , etc . , we define a generic constituent structure for any kind of phrase  . According to the specification of ItPSG linguistic objects  , we define SIGNs as being either of type PHRASAL_SIGN or of type LEXICAL-SIGN  \[15\]  . ASIGN has a phonological value , represented as a list of words , and syntactic and semantic information ( omitted for this comparison )  . The subtypes PHttASAL . SIGN and LEXICAL-SIGN inherit all the attributes and type restrictions of SIGN  . 
(4 ) SIGN -= ( PHRASAL_SIGNVLEXICAL-SIGN ) APHON : LIST_OF-STRINGS
ISYN : CATEGORY
LSEM:SEMANTIC_OBJECT
PHRASAL_SIGNs ( 5 ) differ from LEXICAL_SIGNs ( 6 ) by having an additional dtrs ( ' daughters " ) attribute that gives information about the ( lexical or phrasal ) signs which are their immediate constituents . This attribute encodes the kind of information about constituency conventionally described as constituent structures  . In addition , the various daughters are distinguished according to what kinds of information they contribute to the sign as a whole  . Thus , daughters are classified as heads and complements as in the standard Xbar theory  . In order to be a wellformed object of type PHRASAL-SIGN  , a linguistic object has too bey some general principles such as the " Head Feature Principle " and the " Subcategorization Feature Principle "  . 
(5) phrasal-sign----(HEAD_FPASUBCAT-FPA . . . A(CH_CO_FPVHC* . CO-FP . . . )) A
LCOMP-DTI~S : LIST_OF_SIGNSJ ( 6 ) lexical_sign----VERB VPNOUN VNOUN VDET V . 
General principles The " Head Feature Principle " ensures that the head features of the head daughter always be shared with their phrasal projections  . It generalizes the passing of agreement information from e  . g . a verb to the VP for all kind of constituent and for all information related to agreement and subcatcg orisation  . 
\[s . , . N :\[ . . E . ,, o:I-~-al\]\](7') HEAD_FP . --- LD~rp's:\[ . EA~o " rp . :\[ sv . :\[ HEAD:Iii ~\]\]\] In the DCG example , subcategorization was expressed by introducing different kinds of lexical categories like transitive verb  ( TV ) vs . intransitive verbs IV ) . In HPSG , subcategorization is expressed by us-ngalist of signs  . This list specifies the number and kind of signs that the head subcategorizes for the formation of a complete sign  . Subcategorization information is described in lexical entries  . The " Subcat Feature Principle " ensures that in any phrasal sign  , the subcat list of the head daughter is the concatenation of the list of complement daughters and the subcat list of them other  .   ( The order of the elements in the complements lit does not reflect the surface order but rather the more abstract " obliqueness hierarchy "  ( \[14\]Chap . 7)) . 
(8) SUBCAT-FP . ----?
Grammar rules Just as we have generalized the notion of constituency  , we are also able to generalize the relations between phonological representations and their desired constituent structure representations  . The specialized CF-like relations for a sentence , a noun phrase , and so on in the DCG example can be replaced by two more general rules which specify constituent structure configurations according to the Xbar theory  . 
The " Complement Head Constituent Order Feature Principle "  ( 9 ) simply states that a " saturated phrasal sign " ( i . e . with\[syn:\[subcat:0\]\] ) is the combination of an unsaturated phrasal head with one phrasal complement  ( e . g . S--+NP VP ) . 
(9) CH-CO-FP----
SYI',I:SU~BCAT:01 PHRASAL . SIGN\[PHON " hJh_~ . a~
DTRS:\[HEAD-DTR : : . \]\[ oo MP-DT . ~:( S,GN\[P . oN : l ? omp-pho-\]\] ) :- APPEND The " Head Complements Constituent Order Feature Principle "  ( 13 ) states that an " unsaturated phrasal sign " is the combination of a lexical head and any number of complements  ( e . g . VP--*VXP*) . The relation ORDER_COMPL is used for specify in l , ,?-the ordering of the phonological values of all complements  . 
The phonological value of the whole phrase can then be specified as the concatenation f the head phonology value with the complement phonology value  . 
(13) HC*-CO-FP-~\[F:igeaa-pnonl "
APPENDB : I comp-phonl\[w.~
ORDER-COMPLI ~: oM : S : ~ hon , \] Lw:\[comp-pnonlj(11) SIGNL , KD rRSMAN , I\]\]J1COMP-DTF~S:\[COMP-D'r . s : ( ALL )) ' (12)
PItRASAL-SI(;N"PHON:(l-~"Mary" .  \[2\](  . \[~\]" likes " .  \[~\] ( " all " " men " ) ) ) IIEAD-DTn : PHRASAL-SIGNItNAD-DTR:LEX lCAL -SIGN//ctNAD:\[DTRS:SYN:SUBCAT:!L\[  :  ) TRS:LCOMP-D'!"itS:\[~PHRASAL . : SIGN\[PltON:\[~\] . . . \]) COMP-O'rRs : ( I~PHRASAL_SIGN to n1\]y . : F . Ex :
L sucA:UM\]
Lexical entries
AI , L = DET\[sYNIIINAD:\[LEX:"aII" , NUM : pl\]\]\] , MAN=:NOUN\[~YN:IIn ~ A ) :\[ bEX:"man " , NUM:sg\]V\]\] . 
\[Lr ~ X:"me.", NUM:pl\]
MARY = PNOUN\[SYN:I) , BAD : ( Lt ~ X : " mary " , NUM:sg\]\]\] . 
LIKE = TRANSA(3RD-SGISYN : ha , ~ Ao:\[u~x :" likes"\]\]\]V ) . 
3RD-SG:~\[sYN:rHI';AD:\[PFmSON:3,ug:sg\]\]\].
~ RANS=\[SYN:SVr ~ CAT: ( ISYN:\[m':AD:ICASI'-':acc\]\]\] ) \]  . 
, 5 Parsing and generation
Either (10) for parsing or (11) generation , tile evaluation yields I , he same fully specified sign (12) . 
6 Conelus ion ' I'he main characteristics of the formalism wepre--s  (  . nted are (1 , type inheritance which provides a clean way of itetining classes and subclasses of ob-  . 
jects , and ( 2 ) an evaluation mechanism based on typed unitication which provides a very powerful and semantically  ( : lear means of specifying and cornput-irlg relations between classes of objects  . 
' l The possibility of defining types as ( conditional ) ex = pressions of typed FSs encourages a very different approach to grammar specification than integrated CF based approaches like DCG or LFG : the grammar writer has to deline the set of linguistic objects relevant for the problem  , define the possible relations between these objects  , and specify explicitly the constraints between objects and relations  . 
The TFS system has been implemented in Common-Lisp and has been tested on Symbolics  , TIExplorer , VAX and Allegro Common-Lisp . Sample grammars have been developed (\[6\] ,  \[18\] ) in order to demonstrate the feasibility of the approach  . 
Acknowledgments The current system is based ol ~ a previous implementation carried out by the authors at ATR  , Kyoto , as a part of ' a visiting research program . We would like to thank Dr . Akira Kure-matsu , president of ATIL Interpreting Telephony Research Laboratories for making our stay possible  , and Mr . Teruaki Aizawa , head of the Natural Language Understanding Department for his constant support  . We owe many clarifications to Son-dra Ahlen with whom we had many lively discussions  . This paper has benefited from rnany comments fi ' omour collegues at the IMS of the University of 

Yeferences \[1\] Ilassan Ait-Kaci : A Lattice Theoretic Approach 1o Computation Based on a Calculus of Partially Ordered Type Structures  , Ph . D . Thesis,
University of Pennsylvania .   1983   \[2\] Itassan Ai't-Kaci : " An Algebraic Semantics Approach to the effective Ie solution of Type Equations  . " in : Theoretical Compuler Science , Vol . 
45, p .  293-351 . 1986\[3\] tlassanAi't-Kaci , Patrick Lincoln : LIFE : a natural language for natural anguage  , MCC Technical Report ACA-ST-074~88 . 
\[ d\]D . J . Arnold , S . Krauwer , M . Rosner , L . desTorn bes , G . B . Varile : " The < C , A> , T framework in Eurotra : a theoretically committed notation for MT "  , llth International Conference on Computational Linguistics  ( COLING-86 )  , 
Bonn . 1986.
\[5\] lfdl~neBestougeff , G ~ rardLigozat : " Parame-terized abstract objects for linguistic information processing "  , 2nd European ACL Conference , Geneva .  1985 . 
\[6\] Martin C . Emele : " A Typed Feature Structure Unification -based Approach to Generation " in : Proceedings of the WGNLC of the \[ ECE  1988  , ( Japan : Oiso University ) 1989 . 
\[7\] Martin Emele , R~miZajae:"RETIF : A Rewriting System for Typed Feature Structures "  , ( Kyoto ) 1989 , \[ ATR Technical Report TR-I-0071\]   \[8\] Martin Emele , ~ miZajac : " Multiple Inheritance in RETIF " , ( Kyoto ) 1989 , \[ ATR Technical Report TR-I-0114\] \[ lO\] \[11\]   \[12\]   \[13\] Roger Evans , Gerald Gazdar : " Inference in DATR " , in : 4th European ACL Conference , 
Manchester . 1989.
Jens E . Fenstad , Per-Kristian Halvorsen , Tore Langholm , Johan van Benthem : Situation , language , and logic ,  1987 , ( Dordrecht : Reidel ) Marc Moens , Jo Calder , Ewan Klein , Mike Reape , ttenk Zeev at : " Expressing generalizations in unification-based formalisms "  , in : 4th European ACL Conference ,  1989 , ( Manchester ) Fernando C . N . Pereira , David H . D . Warren : " Definite Clause Grammars for Language Analysis - A Survey of the Formalism and a Comparison with Augmented Transition Networks "  , in : Artificial Intelligence 13: 231-278 .  1988 . 
Harry H . Porter : " Incorporating Inheritance and Feature Structures into a Logic Grammar Formalism "  , in : 25th Annual Meeting of the
ACL ,  1987 , ( Stanford )\[14\] Carl Pol Lard , Ivan A . Sag : Information-based Syntax and Semantics . CSLI , Lectures Notes Number 13 , Chicago University Press ,   1987   \[15\] Carl Pollard : " Sorts in unification-based grammar and what they mean "  , To appear in M . 
Pinkal and B . Gregor ( eds . ), Unification in natural language analysis , 1988 . 
\[16\]StuartM . Shieber : An Introduction to Unification-based Approaches to Grammar  , CSLI , Lecture Notes Number 4 , Chicago University Press ,  1986 . 
\ [17\] GertSmolka : A feature logic with subsorts , LILOG report 33 , IBM Deutschland , Stuttgart ,  1987 . 
\ [18\]   R4mi Zajac : " A Transfer Model Using a Typed Feature Structure Rewriting System with Inheritance  . " , in : Proceedings of the 27th Annual Meeting of the A CL-89   ( Vancouver , Canada ) 1989 . 

