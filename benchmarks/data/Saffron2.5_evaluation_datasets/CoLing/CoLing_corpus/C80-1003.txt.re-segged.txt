ASYNTAXPARSERBASED ONTHECASEDEPENDENCY
GRAMMARANDITSEFFICIENCY
Toru Hitaka and Sho Yoshida
Department of Electronics , Kyushu University , Fukuoka , Japan
SUMMARY
Augumented transition network
grammars ( ATNGs ) or augumented context-
free grammars are generally used in
natural language processing systems.
The advantages of ATNGs may be summa-
rized as i ) efficiency of represent a -
tion , 2) perspicuity , 3) generative
power , and the disadvantage of ATNGs is
that it is difficult to get an effi-
cient parsing algorithm becuase of the
flexibility of their complicated
additional functions.
In this paper , the syntax of
Japanese sentences , based on case
dependency relations are stated first,
and then we give an bottom-up and
breadth-first parsing algoritbx n which
parses input sentence using time O(n3)
and memory space O(n2), where n is the
length of input sentence . Moreover,
it is shown that this parser requires
time O(n2), whenever each B-phrase in
input sentence is unambiguous in its
grammatical structure . Therefore , the
efficiency of this parser is nearly
equal to the Earley's parser which is
the most efficient parsing method for
general contextfree grammars.
1. FUNDAMENTALS OFJAPANESESENTENCE
The Japanese sentence is ordinarily
written in kana ( phonetic ) letters and
kanji ( ideographic ) characters without
leaving a space between words . From
the viewpoint of machine processing,
however , it is necessary to express
clearly the units composing the
sentence in such a way as to leave a
space between every word as in English.
We have no standard way of spacing the
units though the need for this has
been demanded for a long time.
We give some examples in Figurei.
The first sentence in the figure is
of ordinary written form.
The second indicates a way of
spacing ( i.e . putting a space between
every word).
The third indicates another way of
spacing ( i.e . putting a space between
every B-phrase).
Nowadays , many other spacing methods
have been tried in several institutes
in Japan.
In this paper , input sentences are
given in colloquial style in which a
spacing symbol is placed between two
successive B-phrases.
In Japanese sentences , BUNSETSUs ( B-
phrase ) are the minimal morphological
units of case dependency , and the syntax
of Japanese sentences consists of ( i )
the syntax of B-phrase as a string of
words , and (2) the syntax of a sentence
as a string of B-phrases.
A B-phrase usually pronounced
without pausing consists of two parts
--main part \ [ or equally an independent part in the conventional school grammatical term \] and an annexpart which is postpositioned  . We denote the connection of two parts in a B -phrase by a dot if necessary  . Amain part , which is a conceptual word \ [ or equally an independent word \]  ( e . g . noun , verb , adjective or adverb ) provides mainly the information of the concept . On the other hand , an ann expart , a possibly null string of suffix words ( e . g . auxiliary verbs or particles ) provides the information concerning the kakari uke relation and/or the supplementary information  ( e . g . the speaker's attitude towards the contents of the sentence  , tense , etc . )
A word w has it's spelling W , part of speech H and inflexion K . We call ( W , H , K ) the word structure of w . 
Suppose that a string b of length n be a B-phrase . Then , there exist an independent word w 0 and suffix words
Wl , wz , . . . , w ~, and b = w I . ? ? w ~
Cont(Hk , Kk , Hk+1) (0= k < i ) . . . ( i)
Termi ( H ? , KZ ) ? ? ? (2) where ( Wi , Hi , Ki ) is the word structure of wi(0~i~?) , Cont(Hk , Kk , Hk+1 ) means a word whose part of speech and inflexion are Hk  , Kk respectively can be followed by a word whose part of speech is Hk+lin  -15--B-phrases and Termi ( HQ , Kz ) means a word whose part of speech ~ nd inflexion are H ?  , KZ respectively can be a rightmost subword of B -phrases  . 
( i ) , (2) are called the rules of B-phrase structure , and ( W0 , H0 , K0) ( Wi , HI , K ~) " ' ' ( W z , H~ , K ~) ? .   . (3) is called B-phrase structure of b . If (3) satisfies the condition ( i ), w0 wlw ? .   . wZ is called to be a left partial 2

The kakariuke relation is the dependency relation between two B-phrases in a sentence  . A B-phrase has the syntactic functions of governor and dependent  . The function of governor is mainly represented by the independent word of B-phrase  . The function of dependent is mainly represented by the string of particles which is the rightmost substring of B-phrase and by the word in front of it  ( rightmost non-particle word )  . 
Every particle has the syntactic and partially semantic dependent function with its own degree of power  . The particle whose power of dependent function is strongest of all particles appearing in the string of particles is called the representative particle  . 
Therefore , the syntactic function of dependent of a B-phrase is mainly represented by the representative particle and by the rightmost non-particle word  . 
Let ( W0 , H0 , K0) , ( Wi , Hi , Ki ) , ( W  ~ , H~ , K ~) be the word structures of independent J word , rightmost non-particle word and representative particle of a B-phrase  , respectively . Then , < W ^, H ^>_, < W . .,Hi , u u ~ &
Hj > d are called the inrormatlon or governor and the information of dependent of the B-phrase respectively  , and the pair (< W0 , H0>~ , <Wi , Hi , Hj > d ) is called dependency ~ informati 6n of the

There are many types of dependency relation such as agent  , patient , instrument , location , time , etc . Let C be the set of all types of dependency relation  . The set of all possible dependency relations from a B-phrase bl to a B-phrase b  2 is founded on the information of dependent of bI and the information of governor of b  2  . Therefore , there is a function 6 which computes the set of all possible dependency relations ~  ( a , 8 ) between a B-phrase of dependency information ~ and another 
B-phrase of dependency information 8.
The function ~ is realized by the dependency dictionary retrieved with the key of two dependency informations  . 
The order of B-phrase is relatively free in a simple sentence  , except for one constraint that the predicative
B-phrase governing the whole sentence must be in the sentence's final position  . Japanese is a postpositional in this sense . 
The pattern of the dependency relations in a sentence has some structural property which is called the rules of dependency structure  , and the dependency relations in a sentence are called the dependency structure of a sentence  . The dependency structure of a sentence is shown in figure  2  , where arrows indicate dependency relations of various types  . The rules of dependency structure consist of following three conditions  . 
iEach B-phrase except one at the sentence final is a dependent of exactly one B-phrase appearing after it  . 
ii A dependency relation between any two B-phrases does not cross with another dependency relations in a sentence  . 
ii iNo two dependency relations depending on the same governor are the same  . 
Let N be the number of B-phrases in a input sentence  , and all B-phrases are numbered descendingly from right to left  ( see figure 2 )  . We shall fix an input sentence , throughout this chapter . 
Let DI ( i ) be the set of all dependency informations of ith B-phrase  . 
Definition : A dependency file DF of a sentence is a finite set of  5-tuples  . 
( i , j , ai , ej , c ) 6 DF . . . . _~  N=i>j=l , a i E DI ( i ), cde . ~ j6DI(j ) and cE~(ai , aj) . 
Definition : If a subset of DF satisfies following conditions i  ) to 5 )  , it is called a dependency structure from the Z-th B-phrase to the mth B-phrase  ( N ~ Z > m ~ i ) and denoted by DS ( ? , m ) or DS'(i , m) . 
i ) If ( i , J , ei , ~ j , c ) 6 DS (?, m ), then ? ~ i > jAm.
2) For arbitrary i(ZAi>m ) , there exists unique j , ai , ej , c such that ( i , j , ai , ej , c)~DS(Z , m) . 
( Uniqueness of Dependent ) 3) If ( i , j , a~ , a~ , c ) 6 DS (? , m ) and ,   , ~ o(j , k , ~j , ~k , C ) % DS(Z , m ) , then ~ = ~ ~ , OJ"(Uniqueness of B-phrase structure) , 4) If ( i , J , ~i , ~j , c ) ~ DS (? , m ) , ( i , j , ~ f , ~ j ,   , c ) EDS (? , m ) and i > i '> j , then j , hj . 
(Nest Structure of Dependency ) 16?5) If ( i , j , @ i , ~ , c ) eDS (? , m ) ( i ' , j , ai ,   , ~ j , c ') ~ D ~(? , m ) and ~ i ' , then c~c' . 
( Inhibition of Duplication of a Case )
The set of all dependency structur ~ from ith B -phrase to mth B-phrase is denoted by ~  ( ~ , m ) . Any DS(N , i ) ~ ~( N , i ) is called a dependency structure of the input sentence  . The dependency information of jth B-phrase is unique in 
DS(i , m ), since 2) and 3) hold . Let
JDiDS(Z , m ) and jGDS(Z , m ) be the dependency information of the jth B-phrase in D ~?  , m ) and ~ set of all the dependency relations that the jth B-phrase governs in DS  ( ? , m ) , respectively . 
def~i , ~, C ) JGDS(i , m)u__ . cI(i,J ~ Ds(~m)
Definition : If the kth B-phrase ( i ~ k ~_m ) in DS ( ? , m ) has the following property , k ( the kth B-phrase ) is called a joint of DS (? , m):
For any ( i , j , ai , ~ j , c ) ~ DS(~ , m ) , k ~ i or J ~ k . 
Let j ~( = ?) > j , > Jl > "'"> j (= m ) be u , the descendlng sequence of ale the joints of DS(i , m ) ( see figure &) . 
Then , the Jk-th B-phrase is called the kth joint of DS ( ? , m ) . There is a dependency relation from kth joint ( dependent ) to k+i-th joint ( governor ) in DS ( ? , m ) . Let J . DS(?,m ) be a set of all the joints of DS(?,m) . DS (? , m/i , j ) a subset of DS(Z , m ) , is defined as follows:
DS(? , m/i , j ) -~( p , q , av , ~o , c)I(p , q , ap , aq , C ) ~ DS(~ , my , i ~ p > q~j . 
Lemmai . For any positive integer ? , i , j , m(N ~ ? ~ i > j ~ m) , the following propositions hold . 
( i ) DS(i , m/i , j ) 6 ~( i , j ) , if j is a joint of DS(i , m) . 
( ii ) DS(I , j ) UDS(j , m ) ~ ~( ? , m ) , if and only if JD iDS(Z , j ) = JDiDS(j , m ) . 
( iii ) ( Z+i , j , ~ ,  ~ , c)uDS(~ , ~) 6~( Z+i , m ) if and only if ( i + l , j , e , 8 , c)
EDF , 8=JDiDS(Z,m ), jEJ . DS(?,m ) and c~jGDS(Z,m) . 
( iv ) If ( jk , Jk+1 , ak , ek+1 , c)~DS(j , m ) ( k = 0 , 1 , 2 , - . .), then Jk is the kth joint of DS(J0,m) . 
Syntax analysis of a Japanese sentence is defined as giving B-phrase structures and dependency structure of the sentence  . 
2. THE PARSING ALGORITHM
ANDITSEFFICIENCY
In this chapter , we shall give a parsing method which will parse an input sentence using time O  ( n  ~ ) and space O ( n  ~ )  , where n is the length of input sentence . Moreover , if the dependency information of each B-phrase is unambiguous  , the time variation is quadratic . 
The essence of the parsing algorithm is the const ruction of B-phrase parse list BL and dependency parse list DL which are constructed essentially by a " dynamic programming " method  . The parsing algorithm consists of four minor algorithms that are the construction of BL  , the obtaining of B-phrase structure , the construction of DL and the obtaining of dependency structure  . 
13-PHRASE PARSELIST
Let b be a string of n length and b ( i ) denote the ith character from the left end of it . 
b = b(1) (2) . . . b(n).
The B-phrase parse list of b consists of n minor lists BL  ( 1 )  , BL(2) ,  ?  . . , BL(n) . 
\[\] Form of items in BL(j)(i , WS , DI ) where , IL_i<j~n , WS is a word structure and DI is a dependency information  . 
\[\] Semantics ( i , WS , DI)EBL(j ) of ( i , WS , DI)EBL(j ) , if and only if there exists a sequence of words w o  , wl ,   . . .   , w ? satisfying following two conditions : i ) b ( 1 ) b ( 2 )   . . . b(i ) = w0wI . . ? w~_ I , b(i+l)b(i+2) . . . b(j ) = w ?, and
WS is the word structure of w?.
2) The string of word w_wI . . . wZ is ? Daleft parclal B-phrase of dependency information DI  . 
ALGORITHMFORTHECONSTRUCTION OFBL
Input . An input string b = b(1)(2) ? ?. b(n).
Output . The B-phrase parse list
BL(1), BL(2), ..., BL(n).
Method . Stepi : Find all the independent word which are the leftmost subwords of b  , using independent word dictionary and for each independent word w = b  ( 1 ) b ( 2 )  '' . b(j ) , add (0 , ( W , H , K) , a ) to BL(j ) where , ( W , H , K ) is the word structure of w and ~= (< W , H > K , < W , H , -> d ) . Then , set the contro I word i to 1 and repeat Step 2 until ~= n ?
Step 2: Obtain all the suffix words which are the leftmost subwords of B  ( i+l ) B ( i+2 )   . . . b ( n ) and for each suffix word w = b ( i + l ) b ( i + 2 )   . . . b(k ) of word structure ( W' , H ' , K ') , and for each item ( j , < W , H , K> , a ) #BL(i ) , add ( i , (W' , H' , K ') , ( W' , H ') oe ) to BL(k)if--17
C(H,K,K') . ( W' , H' ) 0 a is a dependency information defined as follows . 
i If H ' is a auxiliary verb ? then ( W' , H ') o ~ def (< ~> g , <W , ,H , ,_>d ) where ?< a > g is the information of governor or a . 
iiLet < W " , H" , H "'> be the information of dependent of ~ . When H ' is a particle , ( W ,, H ,) oa def .   .   .   . (< a > g , <W" , H" , H '> d ) if the power of dependency function of H'is stronger than that of H "'  , and else(W , ,H , ) o ~ def ~ . 
There exists upper limit in the length of words and there exists upper limit in the number of dependency informations of all left partial B -phrase of a  ( 1 ) a ( 2 )   . . . a(i ) . Therefore , there exists upper limit for the necessary size of memory space of BL  ( i ) and the theorem 1 follows . 
Theoremi.
Algorithm for the construction of
BL requires O(n ) memory space and
O(n ) elementary operations.
We shall now describe how to find a
B-phrase structure of specified dependency information from BL  . The method is given as follows . 
ALGORITHMFOROBT AINING AB-PHRASE
STRUCTUREOFANINPUTSTRING
Input . The specified dependency information ~ and BL . 
Output . A B-phrase structure of dependency information a or the error signal " error "  . 
Method . STEPi : Search any item ( i , (W , H , K) , a ) in BL(n ) such as Termi(H , H ) . If there is no such item , then emit " error " and halt . Otherwise , output the word structure ( W , H , K ) , set the register R to ( i , (W , H , K) , a ) and repeat the step 2 until i = 0 . 
STEp2:LetRbe(i , ( W,H,K),e).
Search any item(i ', ( W ', H ', K'), a ') in
BL(i ) such as C(H ', K ', H ) and ( W,H)o ~= a.
There exist at least one element which satisfies above conditions  . " Output the word structure ( W ', H ', K ') and
R?(i ', ( W ', H ', K'), a').
It is easy to know theorem 2 holds.
Theorem 2.
A B-phrase structure of specified dependency information is output by the above algorithm  , if and only if the input string has at least one B-phrase structure of specified dependency information and it takes constant memory space and O  ( n ) elementary operations to operate the above algorithm  . 
The set of all the dependency informations DI of input string b is obtained from BL  ( n )  , since DI = aI(i , ( W , H , K ) , a ) ? SL(n ) , C(H , K ) . 
DEPENDENCY PARSELISTDL
Lets beain put sentence of NB-phrases . The set of all the dependency informations DI ( i ) of the ith
B-phrase is obtained by operating the algorithm of construction of BL on the string of the ith B -phrase  . 
The dependency parse list DL of s consists of N-i minor lists DL  ( 2 )  , 
DL(3), ''-, DL(N).
\[\] of items in Form DL(i).
(ai , J , aj , ~ , P ) I(ai ? J , aj ,   , P ) where , N ~ i > j ~ l , aieDI(i ) , ajEDI(j ) , ce ~ , P ~ and $ is a specially introduced symbol . 
I ~ Semantics of ( ai , J , aj , c , P ) 6DL(i).
(ai , J , aj , c , P ) ~DL\]i ) ? if and only if there is a dependency structure DS  ( i , i ) of s , where ( i , J , ai , a~ , c)~DS(i , i ) , jGDS(i , l ) < P . 
~ Semantics of ( ai ? j ? ~, S,P ) 6DL(i).
(ai?J , ? j , $ , P ) eDn(1) , if and only if there is a dependency structure
DS(i , i ) of s , where ai = iDiDS(ii ) a . : JDiDS(i , i ) , ? rJj is a joint of DS(i , i ) except
O-thor1stjoint,jGDS(i , i ) = P.
ALGORITHMFORTHECONSTRUCTION OFDL
Input . The sequence of the sets of all dependency informations DI  ( 1 )   , 
DI(2), ''" ? DI(N).
Output . Dependency list DL(2),
DL(3), ''", DL(N).
Method . STEP1 ( Construction of
DL ( 2 ) )~ For each a e DI ( 2 ) ? a16 DI ( 1 ) and cE ~ such that ~ e6 ( c~2 , c~i ) , add ( a2 , l , al , c , c ) to DL ( 2 ) ? set i to 2 and repeat the STEP 2 and the STEP 3 until i = N . 
STEP2(Registration of items of the form ( ai+l , j , aA , c , P )): For any ( ai , J , aj?c , P ) ~ DL(i ) and ~ i + 16DI(i + l) , compute 6 ( ai+I , ~i ) and add every ( ai+l , i , ai , c' , c ') to DL(i+i ) such that c'66 ( ei+1 , ~ i ) . And , for any ( c~i , J , aj , A , P ) 6DL(i ) where A~~~'$ and ai+1 ? DI(i + l) , compute ~( c~i+1 , aA ) and add every ( ai+l , j , c~j , c' , PUc'~toDL(i+i ) such that c'6~(ai+1 , aj ) and c'P . GotoStep 3 . 

STEP3 ( Registration of items of the form ( ai+1 , j , ej , $ , P )): For any ( ai+1 , j , al , c , P ) ~ DL(i + i ) and ( al , k , e k , A , P ')  #DL~j) , add ( ei+1 , k , ak , $ , ~') to
DL(i + i ). Then , set i to i +; and go to STEP 2.
Theorem 3.
If there exist no ambiguity in the dependency information of B-phrases of input sentence  , then the step 3 in the above algorithm can be replaced to the following step  3'  . 
STEP 3 ' : For each ( ~ K i + ! , j , ~A , A , P ) 6DL(Ki + ~) , add ( ~ i + ~ , j , aj , ~ , P ) ? o
DL(i + i ) , where de----~maxkI(ai+lk~k , C , P ) Ki + l ,   , 
DL(i + l).
Then , set i to i + l and go to STEP 2.
The efficiency of each step of above algorithm is as follows  . 
The memory size of DL(i ) is O(N).
The step i , the step 2 and the step 3 take constant , O(N ) and O(N ~) elementary operations , respectively . 
The step 3' takes O ( N ) elementary operations since it takes O ( N ) elementary operations to compute Ki+~ . 
Therefore , the theorem 4 holds.
Theorem 4.
The algorithm for the construction of DL requires O  ( N  ~ ) memory space and O ( N  ~ ) elementary operations . Moreover , if there exist no ambiguity in the dependency information of each B-phrases  , the algorithm requires O ( N  ~ ) elementary operations by replacing the step 3 with the step 3' 
We shall now describe how to find a dependency structure of input sentence from DL  . To begin with , we shall explain items of partial dependency structure list PDSL  . 
Form of items in PDSL(i , j , a~ , a~ , P #) where , N hi~j ~ i a #~ DI(i ) ~ # , ~% DI(j)U ~ , P ~ a subset of C or  #O and  #is specially introduced symbol  . 
~ Semantics of ( i , j , ~#, e#.p#). ~ ij-
The item ( i , j , a~ , e~ , P#) % PDSL means to be a dependence S-structure
DS(i , j ) ~ ~( i , j ) such that following conditions i ) , 2) and 3) hold . 
i ) If a ~= ~ i (%#) , then iDiDS(i , j ) = ei?2)Ife#=aj(~#! , ~then JDiDS(i , j)=aj . 
3) If P~=P (~#). then JGDS(i , j ) = P.
Therefore , ( N , i , # , # , # ) means to be a dependency structure of the input sentence  . 
ALGORITHMFOROBT AINING ADEPENDENCY
STRUCTURE FROMDL
Input . DL.
Output . A dependency structure of input sentence or the signal " error "  . 
Method . STEPi:If DL(N ) is empty , emit the message " error " , else , initialize PDSL to(N , i , # , # , #) and repeat step 2 until PDSL becomes empty . 
STEP2: Take an item freely out of
PDSL and delete it from PDSL . According to the form of the item , execute i ) or 2) or 3) . 
i ) If the item is ( N , i , # , # , #) of the form and ( aN , J , ej , c , P ) ~ DL(N) , then output ( N , J , eN , ej , c ) , add ( N-i , j ,  # , aj , P/c ) to PDSLi ~ N-i~j and add ( j , l , aj , # , #) to PDSL if j~i . 
2) If the item is ( i , l , ei , # , #) of the form and ( j , ~i , ej , c , P ) EDL(i ) , then output ( i , J , ei , e~ , c ) , add ( i-l , j ,  # , aj , P/c ) to PDSLi ~ ii@j and add(j , l , a  ~ , # , #) to PDSL if j@13) a If the item is ( i , j , ~ , e  ~ , P ) of the form , where ~#= ~= or # , and a(ai , j ,  ~  , c , P ) EDL(i ) , then ? output ( i , J , ~ i , e  ~ , c ) and add ( i-l , j , # , ~j , P/c ) to
PDSL if i i % j . When there is not such item in DL(i ) , search a pair of items ( ei , k , ak , C , P ') EDL(i ) and ( ak , J , ej , A , P ) ~ DL(k) , then output ( i , k , ei , ak , cy , add ( i-l , k , # , ~k , P'/c ) to PDSL if ii@k and add ( k , j , ~k , ej , P ) to PDSL . 
PDSL needs O(N ) memory space and
STEP i , STEP2 take constant , O(N ) elementary operations , respectively . 
Theorem 5.
A-igorithm for obtaining a dependency structure from DL requires O  ( N ) memory space and O ( N2 ) elementary operations . 
PARSING ALGORITHM
Input . A Japanese sentence in colloquial style.
Output . A dependency structure DS(N , i ) of the input sentence and a B-phrase structure of the jth B-phrase  , whose dependency information is JDiDS(N , i ) , for every j(j = l , 2 ,  "'"  , N ) . 
Method . STEPi : Construct NB-phrase parse lists of all B-phrases of the input sentence and get the sets of dependency informations DI  ( 1 )  , DI(2) ,  ?  . ", DI(N ) . 
STEP 2: Construct dependency parse list DPL from DI ( 1 )  , DI(2) ,   . . . , DI(N ) . 
STEP3: Obtain a dependency structure DS(N , i ) of the input sentence from DL . 
STEP 4: Obtain a B-phrase structure of the jth B-phrase , whose dependency information is JDiDS(N , i ) , for every j(j = l , 2 ,   . . . , N ) and stop . 

Let n ~ be the length of j th B-phrase (~= i , 2 ,  ?--  , N ) , and N , n denote the number of B-phrases and the length of input sentence  , respectively . Then , n1 + n2 + . . . + nN = n
NLn
By theorem i , theorem 2 , theorem 4 and theorem 5 , next theorem holds . 
Theorem 6.
The parsing algorithm requires
O ( n2 ) memory space and O ( n3 ) elementary operations . Moreover , if the dependency information of each B-phrase is unambiguous  , it requires O(n2) elementary operations . 
3. CONCLUSION
Syntax of Japanese sentences is stated and a efficient parsing algorithm is given  . A Japanese sentence in colloquial style is parsed bY the parsing algorithm  , using time O(n ~) and memory space O(n2) , where n is the length of input sentence . Moreover , it is parsed using time O ( n2 ) whenever dependency information of every B-phrase is unambiguous  . 
REFERENCES i . Aho , Ullman : " The Theory of
Parsing , Translation , and Compiling " , Prentice Hallvol .  1 (1975) . 
2. Woods : " Transition Network
Grammars for Natural Language
Analysis ", Communication of the
ACM , 13(1970).
3. Pratt : " LINGOL - - A Progress
Report ", Proc . IJCAI4 (1975).
(~)(~ s ) (2) 0-)
J0 = 5 ) Jl ( =4 ) J2 ( =3 ) J 3 ( = i ) :main part a:agent--: ann expart p: patient 
J0, Jl , J  ~, J3: the sequence of joint
Figure 2. Dependency Structure
Example : T aroread the composition written by Hanako  . 
Figurei . Ways of Spacing
