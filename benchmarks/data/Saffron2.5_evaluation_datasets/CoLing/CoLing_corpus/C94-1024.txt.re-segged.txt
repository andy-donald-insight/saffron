Disambiguation of SuperPint , of Speech ( or Supertags )
Ahnost Parsing
Aravindi (. Joshi and B . Sriniw ~ s
Depa . rtment of Computer and Information Science
University o\[Pennsylvanh ~
I > hiladelphia , PA 19104, USA
joshi , stint ~ linc.cis.upeiln.edu
Abstract : In a lexicalized grammar Ibrnlal-is ni such as Lexic Mized Tree-Adjoining  ( h ' ~ unmar ( I3'AG )  , each lexicM item is associated with at least one elementary structure  ( supertag ) that localizes syntactic a . nd semantic dependencies . 
Thus a parser for a lexicalized grammar must search a large set of supertags to choose the right ones to combine for the parse of the sentence  . We present techniques I'or dlsambiguating supertags using local inlorlnlttions ~ Lchas lexicM preference and local lexicN dependencies  . Tim similarity between LTAG and l ) ependency grammars i exploited in the dependency niodel of snpertag dis-a  . mbiguation . The performance results for vari-otis models of supert  ; tg disambigu ~ t tk ) n such as unigram ; trigram and dependency-based models are presented . 
1 In t roduct ion l>art-of-spee<:h disanll >iguation techni<lues  ( taggers ) are often used to eliminat < , ( or sul>sl;an-tlally reduce ) the lm . rt-of-spee , <; hanil > i guity prior to parsing . The ta . g gel's are all local hithesense that they use inform ~ tion front a limited context in deciding which tag  ( s ) to choose for each word . 
As is well known , these taggers are quil ; e , successful . 
In a lexicalized grammar such its the I , exic M-ized " Dee-Adjoining Grammar (13~AG ) , each lexical item is associated with at least one elementary structure  ( tree )  . The elementary structures of I'PAG localize dependencies  , including long distance dependencies , by requiring that M1 and only the dependent elements be present within the sautestruct nre  . As a result of this localization , a Ioxic Mitemmay be ( a . nd , in general , a , 1-most alwa , ysis ) associa , ted with more . than one elementary structure . ~ Vewill c Ml the seele . -mentary structure supertags , in order to distinguish theml ' rom the standard part-of-speech tags  . Note that even when a word has a unique standard part-of-speech  , say a verb ( V ) , there will usually lie more than one superta . g associated with this word . Since when the parse is complete , there is only one supertag for each word ( assuming there is no global ambiguity )  , an L'\['AC , parser ( SchMms , 1988) nee . ds to search a large space o1" supertags to select the right one l breach word before combining them for the parse of a  . sentence . It is this 1) roblem of supertag dis ; unbigua-tion that we address in I , his paper . 
Since l , ' l . ' A (', s are lexlcalized , we are . presented with a novel opportunil ; y to elimill ; tte or substantially reduce the supertag assign n mnt ambiguity by using local information such a  . slocal lexical dependencies , prior to parsing . As in standard lmrt-of slieech disambiguatio oii , we can use local statistical iufort natiot ~ in the termo\[n-gt'anl models based Oilthedistri l  ) ution of stiper l ; agshia I , ' I'A ( Iliarsed corpus . Moreover , since the sli-l)erta . gselicode depemde , ncy hfl ' or nlal ; k)n ~ we can also use in form a . tion about the distribution of distances between a  , given superi ; ag and its dependent superl ; ags . 
Note that its illsta , ndard part-of-speech disaun-biguation , superl ; agdisambiguation could have been done by a parser  , l loweve G carrying out part-of-speech disaml ) igua . tion prior to pamsing lnMces the jobo1' the . parser much easier and therefore , speeds it np . Stl per tag disalnl ) igua-tiona . s proposed in this paper reduces the work of the parser even further  . Afters n per tag dis-ain biguation , we have effectively completed the 1 , 54 parse * rod the p~u'serneed'only ' ( : omhine the indi-vi ( hlM structures ; hence the term -- ahnost parsing . 
This method can a . lso be used goi ) ~v ' sesenten ( : e fragments in cases where the sn per tag sequence after the disambiguation may not combine into : L single structure  . 
Thema . intom of this paper is to presentte . chniques for dis~unbiguating Sul)erta . gs , and to(wahu ~ tetheirpe\]'formm~cea . nd their impa . ctonI;I'AG parsing . Although presented with resl)ect to Ill'A (' , , these techniques are a l)plica . bD to lexicalized gl ' aitltlla . rsill gener M . Section 2I ) rovi(h , ~ ~ mintroduction to l , exi(' . ~ dized'\['reeAdjoiningGr ~ mlmaa's . The objective of supertag(l is a . m-1 ) iguation is illustrated through an example in Section  3  . Section 4 l ) rielly deseril ) es the system used to collect the data , needed for Sul)ert ~ tg disambiguation . Various methods and their i ) er-formance results for superta . g(tisambigua . tion are discussed in ( let ~ dl in Section 5 . 
Lexicalized'l S'ee Adjoining
Grammars l , exicalized ' . l Yee Adjoining ( \] r ~ mmla ~ r ( I : t'AC ) is ~ lexicMized tree rewriting grammar lbrm~dism . 
The primary structures of ILFAG ~ u'eI'~LI , ;MI , ; N-TALLY'PII . FI , IS . l ' ~ wh elementary tree has a lexi-ea . litem(a , nchor ) on its fi'ontier and l ) rovides a , n extended ( lomain of/ocMity over which the au- ( ' hor specifies syntactic a . nd semantic ( pre(lica . te argument ) constra . ints , l'~lementary trees a . reoft woldnds : INITIAl , TRI , H , : Sa~rt d AUXILIARY TI/I , ~ , : s . Examples of initial trees (~ ts)~ul(Ia . uxil-ia , rytrees ( fls ) are shown in I , ' igure I . Nodes ( m the frontier of initiM trees are , ma , rke(Ia , ssul ) stil , u-tion sites1) ya'J ' , while exa . ctly one node on the fl ' on tier of an a . uxili ~ rytree . whose la , h(qm:~tches the halel of the root of the tree , isula . rkedas ; ~ footnode1) y ~ L ',' . The other nodes on the frontier of an mlxiliary tree ~ u'e marked as sul  ) stit , u-tion sites , lfl!A(l\['actorse cursion\['rom thesta , te-ment of the syntactic dependencies , l !' , lementary trees ( initiM and ~ mxiliary ) are the domain for specifying dependencies . Recursion in specilied via the auxili ~ u ' y trees . \];' Jementa . rytrees ~ L recom-l ) ined by the Substitution and Adjunetion op -eri ~ tions  . Substitution inserts element ; u ' ytre . esa , t the substitutio nodes of other element a . rytrees . 
Adjunction inserts : mxili ~ rytrees into elementary trees at the node whose la  . belin the same as therout lM ) el of tile auxilia , rytree . An ~ Lnexam-pie , the (: Oml ) onent trees (~ s , me , n ,  . % n . 4 , fls , ( ~ s , n's ) , shown in Figure lc~m be combined to form the sentence Johns awaman with lhetelescopel as follows :\[  . 



n's substitutes at the NP0 node . in (~.
n , : ~ sul ) stitutes a . t the \]) etl ) node in e ~ 4 , there s~llt of which is sul ) stituted : ~1: the NP l node in r ~ 2 . 
~: ~ substitutes : LI , the I ) et l ) node in ~( ; , the result of which is sul ) sl , it uted a , L the NI ) node hl 138 . 
The result of step (3) a . bove a . djoins to the VP node of the result of step (2) . Tim re-surfing pa , rsetree . is shown in Figure 2 (: t ) . 
The process of coral ) thing timelement m ' y trees resulting in the I ) a . rse of the seutel ~ ce is rel ) re . -sented by the deriwltion tree , shown in li'ig-ure2(I )) . The nodes of the deriwttion tree are the tree names that a  . reanchored by the ~ Lp pro-pria . te lexical item . The c . omposition opera . lionisindica . ted by the nature of the a . rcs-(h~shedllne . for sul ) stitutiou : uLd bold line . for a . (ljunction , while the ~ ul(h'ess of the operation is h~dica . ted as part of the node label . The deriw Ltiontreeea , na . lsoI ) eiuter pre . ted,~s : ~ dependency gra . l ) h with unhd ) eleda . rcs I ) etweell words o\["the . sentence as shown in Figure 2((') . 
We will ca . ll the element a . rystructures as so-ei:tted with e a . ch lexi (: a . I item a . ssuperl ) a . rts-of-speech ( super I'OS ) or supertngs . 
3 Example of Supertagging
Ana . result o\['locafization htI:I'A(I,a . lexica . I item may I ) e as so ch ~ . l . ed with more tha . none SU l)ert ~ g . 
The eXaml)le hiI , ' igure 3ilhlstr ; Ltes the iniLia . lseto\["supertags assigned to each word of the sentence  , \] olz , * sawammz wilh l helcle scope . The ord c'r of the superta , gsfore ; Lch lexica . litem in the ex-aml ) le is not signili (: ant . Figure 3M so shows the \[ in a , lSUl)e . r tagse ( ll nm ce a . ssigned by the supe . rt ~ Lg-ger , which picl ( s the best supertag sequence ll Sil'lgsta . tistica . linforlna , tion(descril)e . d in Sectionel ) ~ dmugh Mividual superta . gsaim the h'dependencies on other supertags . The chosen SU l ) ert ~ Lgsnre ( : omblt led to derive a . \[) axse . , asexl ) h) . inedill ~ The parse with t it (: PP ~ t l . Lached to the NP has not \]) (! ell . ' ~ lOW ll . 

NP l ) et P $ N

Johns ,/ N
NP ~ VP v NF A
I s ~ w
I ) et PNP nP
NI " I'I'
I ) l ) etPlNnx/X
I'NI~II~IIll ; HI~llh
OiLOi2(~3(~', If ~ l
Anl ~ lven/Nvnlsn ~
John\[i0:'80'9
Ribvl ' n ~* n ~ A/\
VNI'IL

Johlluw

I ) lhe(/'5
DeIPNPvlt1)etP,&;&I1llt~NI ) ll~l,
PNI~1i~I
IIIIIIIII with the
Oil0(~'11f~8(X'I2
DeIPr1~s,
I ' PN "
I ) I ) elP\]+I % 1/k
I\[lIll\[Illxlllh
I ) el Pr
I ) DelPp
I the

I ) eIP$N
Itelescope?.V ( ;


Itelescope/XnN ; * telescope#2 , - ~ r , qa # , , #~ # , ~ fir

Pigm'eI:\],'lementary trees of LTAG
NV l'pl ,
I ~ l.l,.vnl'P ~ e
I/~Imw\[hipN + llhlkll'N
II 1 IIIm ~ IIt d ? ~. p +
II ..

P : u'seTree , , 2lsa ~ q~l~l\[ . hdmJ (11 ILSIwIIM (21 a4 l numl (2 , 21 ~' ~ le ~ r-lW \] (221n Sial(I) . 5 lieli ) (11) l ) eriw ~ . tion Treel ? igure 2: Struct m ' es of LTAG saw . lldm MIhhum
IItelesclqk'a
Ithe(,.)l)eI > endellcy(,~a,ph
Sentence : , lohns~t x v a man with . 1; 1 lotelescope . 
initia , l Supertag set : (~1(I '8( ~ . * ~ Oim(viiflsOiJ2(~r ~
O i r ~ a : , f l , , f l s f l , ; f i r
F ' in MAs signment : Oi8Oi'2rv:~(v , if 18 Oiso : ( ; Pigure 3: Supertag Assignment for , loires awama ' . , will z Iltc/eles (: Ol~e Without the superta . gger , the\[)au'ser wouhlha , vetoprocess combinatioi , so\[tlteentireseto\['tree+s(28) ; with it the parser must only i ) rocesses con > binations of 7 trees . 
4 Data Collection
The ( t + ~ t :+ re(luired for disant bigtt a . tine ; superta . gs ( discussed in Section 5 ) luwe I ) eencolle <: l:edI ) yl ) a , rsing the Wa . llStreet , Iottrna + . l'2 , ll~IVl-tnautud and ATIS:ori ) ot':t using the wide+coverax eI%-glish gr:-tmma , r being ( l(welopeda . spart of the X'F A(I systen i (1) or ; meL . a \] . , 1994) . The pa . rsesgene . ra , ted by the system for these sentett ( : es Irom the corpora , ; tl '( . ' llot subject e < lto:tny1 , : i u < l or lil-tering or selection . All the deriw ~ tion structures are used in the collection  o1' the sta+tistics . 
4.1. About XTAG
X ' IJAC , ' is a , large ongoing proje(:t to develop a , wide-c . over a gegra . mma . rf , . ) rl " , nglish , l ) ased ( ) nt it , :;\];\['AG form+dism . Ita . lso serves + Lsa . tl\]'\['A (' , grammlar develolm teut system and consists of a . 
predictive left-to-right i ) a . rser , au X-wht ( low interface , ~ rnorphologica da + na . lyzera , nda . part-of-speec \] l tagger . The wide -(: over a . ,e . ;e English gram mar of the XTA ( \] system contains : tl 7 , 000 in-tleete di Let ns in them or l ) hology (213 , 0 (1(\] of these ~ Lre nouns ~ tnd 46 , 500 are verbs ) a . nd 37,000 ( mtries in t it (', syntactic lexicon . The syntactic lex+i (: on a , ssocia , teswords with the trees i ; ha . i ; they anchor . ' Fhere ; u'e385 trees it I a . ll , in a , gra . tn ln+L r which is compose . d of 40 ( li\[l'erent sul)c~d , egoriza + tionI ' rantes . lC , au : h word iuthesy t , tactic lexi ('+ m , on the + w ( , ,rage , depending on the st~utda . t'dlm . rts-of speech of the word , is an a . nchorl ' or a . bouttod 0 element ; try trees . 
Models , Experiments and
Results
The SUl :) ert ~ Lg statistics which h + a . vebeent ts , :' d in the prellndna . ry experiments descrihed I ) elow h ~ we been to ilet:ted from the XTAG parsed corpora  . The deriw U : ionstructur('s resulting rromi ) a . rsed corpor ~( W~dlStreet . Journa L1 , for the + ~ xperiments descril ) ed here ) serve as tr~h~ing da . ta . 
for these experiments.
'2 Sentences of hmgth < 15 words 5 . : 1Unigram model Onemet . hodo\['disanil , igua , tingi . h,:'sup(n'l , a . e ; s assigned to e ; tch word iX to or ( ler the Stll ) el't : - i , gs by the lexic ~ dl ) rel'erence tluLt the word ha . s (+ or the lll . 
Thel'r ( Xluency with which a . certaiu supertag is associated with a + word is a  , ( lire(:tinca . sure of ii : ~ lexica . ilU'(d'eren(:eR+rtha . 1, SU l ) ( wta . g . Associating fre(luench > s with the superta . gsa . n d t t s i n , e ; the ln to ; ISSO <' i ; Lt(+; t . l ) a . rli(:ula . r Sttl ) erta . g with a . word is clearly the simplest in ca . its ',) 1'(l is a . utl ) i + vguatin K
Sll\])('t't:-t.gs.Thus,
S,~I,ot . L ; ~ . ~, ; ( , , , ~) = ta :) a . , ' p;n , a ?, , : t , ip ; t';, . m(t\] . . . 1"" J . 
5.1. \] Exl)eriments and Results
Owing to sl ) a . rs+'lmsso\['(la . ta . , we ha . w+I ) a . (: ke(l-c~ffI'rom word/supertagpa , it's to i ) a . rt-o\[-sl ) eech/st , l)erta . gpa . irs , i . e . , collected the unigram I're(ll , eucies or superl . a . / ~; sas ; so (' . i ~ ted with the pa . rt-or-speech : ~+ s signed to words instea . dor the words themselves . Ta . blelillusl . r ; ttesthena + tureo\['tlm statistics used , with n . rews a . ml ) le entries . 
\[\]Tiu'(~)iCTt~e(~-I~-(SUlwr tag , u , ligram i ) rol ) al ) i litv)
IN
L-v .. (.>,
II ) (( ~. ~, 0, 9 (13)
T : d)le1:,qa . nll ) h , ent , ries of uuigra . mda . ta . I)a+se'l'al)h'2:
Model'l'opnSUl)erta . gs % Success "- . , i = -: ~ .   .   .   . 2~-cX7-+ . ; TY-~+--F : , ~II('+~; t , ll + ~\ [' r < mt the\[luigt'a . nl+qUl )(, rt:t . piTimw()r < lsu . refirsta , ,~; , ~dgu( , ( Istauda , rdi > arL > of speech us in t , ~ ~ couventioual ta , gger(Churdl , l !) gg ) . Then these to \[' Sul)er tags a . sso(qated with ca . oh word is retrieved rroln XTAC,'s synta . ctic(lata . bn . s e . ' l ' hese sul)erta . gsa . reordered ha . sed . :) n their uni , <,; ra . mrr < ~( lUeUCy , a . n ( I the top n Sul)erta . gsa . rea . ssocia . ted with th (, word . ' r ~ L ble2 suntm ; > rizes the successl) , rcenti~g ~ e o n a , heldout test set or 100 Wall Street , lottrna . lSell telH'A ~8 ~ . : IS11 iSvaried , lra , sentence p;u'ses using then sllper ta . gssele(:tedform L(:hwor(I then the a . ssigument is cou-si(lereda , success . 
The unigt'a . tn superta . ggertha+t selects Ix ) l ) three Sul ) ertags hasl ) een interl'aced wiLhX'\]'A (  :  . This(D , <~)( N , ~ s ) ( N , , ~ , ) ( V , o , 2)
Direction of

Supertag(-)(-,+)(-,+)\]) ependent
Supertag(Y3('gg
Table 3: I ) epen deney Data
Ordinal position Prob-1C ) . .()99 - I C) . 300 1  (   L374 speeds the runtime of the parser by 87% on the average , whenever the snper tagger succeeds . 
5.2 n-gram model
In a unigram model a word is always associated with the supertag that is most preferred by the word  , irrespective of the context in which the word appears  . An Mternate method that is sensitive to context is the ngram model  . The ngram model takes into accounthee on textua J dependency probabilities between supertags within a window of n words in associating supertags with words  . Thus the most prob~t ble supertag sequence for a N word sentence is given by 
Y ' = argmax rPr(T ~, 5' ~, ..., TN )*
Pr(I'VI,I'V2,...,WNIT~/&,...,7~)
To compute this using only local information , we approximate , taking the I ) robM ) ility of a word to depend only on its supertag
Pr(W1,W2, . .  . , WNIT , , T2, .   .   .   , 7 ~) l-IY_- , Pr(l + ~ , ' I~1~) and also use an ngram ( trigram , in this case ) approximation P " OL -'& ,  .   .   . , TN)~F\[~,P"('/~I"L-~,'/t~-I ) 5 . 2 . 1 Exper iments and Resu l ts A trigram model has been used to model the contextual dei  ) endencies in supertag sequences . 
Again , due to sparseness of ( hint , the particular words have been ignored and the training of the trigram model has been done on the part-of-speech/supertag pair  . The model has been tested on the same set of held out sentences as in the unigram experiment  . The percentage success is 68%, i . e . ,  68% of the words of the test corpus were assigned the corrects ui  ) ertag . 
5 . 3 Dependency model hithengram model lot ( lis~unbiguating supertags , dependencies t ) etween supertags that appear beyond then word windowea  , n not be incorporated into the mode . 1 . This limitation can be overcome , if no a priori bound is set on the size o\["the window but instead a prol  ) ability distril ) u-tion of the distane e . so \[' the < lel ) endent supertags for each supertagism a . intained . A supertag is dependent on another supert ~ gi\[' the former sul  ) -stitutes or adjoins into tit (  . ' latter " ~ . 
5.3.1 Experiments and l /, esults
Table 3 shows the data required for the dependency model of supertag disambigua  . tion . Ideally each entry would be in ( lexed by a ( word , su-i)ertag ) pair I ) ut , due to si ) arsenesso\['(lata , we have backed-off to a . (I )() S , supertag ) pa . ir , l'3 a(:h entry contains the following information . 
? POS and Supertag p~dr.
IJs to l '+ aml- , representing the ( lirectiol l of the ( h , peIM(mtsuperta , gs with resl ) e(:t to the indexed supe . r tag .   ( Size of this listii Micates the total number of dependelt tSUl  ) e , 'ta . gs required . ) ? l ) ependent supertag . 
Signed numl ) erre present hig the direction a . nd the ordinal position of the l ) a . rticul ; u ' dependent SU l)e . rtag mentioned in the entry from the position ( ff the indexed su\[ ) ertag . 
a We are computing dependencies between words with respecto supertags associated with the words  , although the complete structure of the supcr tags inotused  . It is of interest oCOml ) ~ U : e our work with some other dependency-based appro~ches as described by  , for example , Sle ~ tor ( Sleator and Teml ) erley ,  1990) , l\[indle(llindle ,  \] 993) , Milward ( Milward ,  1!)!)2) . 
158 ? A probal ) ility of occn rrence of such : t ( lepen-dency . The sum probability over all the dependent supertags at all ordinal positions in the same direction is one  . 
For example , the fourth entry in the T : d)le ; I reads that the tree (~2 , a . nehored 1) yaverl ) ( V ) , has a left and a right dependent (- , +) and the tirst word to the left (-1) , with the 1; ree . (~ s , is dependent on the current word . The strength of this association is rel ) resented by the i ) robal ) ility 0 . 300 . 
The dependency model of ( lisaunl ) iguation works as follows . Stil)l ) ose(~'2i Sa , llleiillie . r of tile ' set of super(ass associa . te(l with : t word a . t posities n in the senten (: e . The : d < e ; or it hulproceeds to slttisfy the depende . ncyreq ' < lh'e . ment of < t , 2I ) y pield ng up the dependency entries for e : t < : h ( >\[ the directions . It picks a , del ) en < lency dai , at entry ( the fourth entry , say ) from the ( hmd ) : tse that is indexed by a 2 all ( I proceeds to sol ; i1l ) at pa . tll with tile first word to tile left that has the  ( lepe . ndent supertag ( (~8) a . saineml ) er (!\[' its set o\["sul)er La . gs . 
If the first word to the left th ~ t tha , s ( h ~ as a clneu > ber of its set of super ( ass is a . tl ) ositioum , t , 111!1ia . II arc is setup 1) etwee . nc ~, 2 and (~ s . Also , the arc is verified not tokite-string-tangle/i with auly other i ~ l '  ( : s in the path up to e ~2 . Thei ) ; ttll prol ) M ) ility up to a 2 is incremented by log0 . 300 to reflect the success of them a , tch . Thel ) atth probad ) ility uI ) to ( Is incorporates then nigra ! n probability of ( vs . 
On the other hand , if no word is found 1 , \[ llti;\]la . sa8as ; ~member of its set of supertags then the entry is ignored  . The a\]gorit\]inlmltkes agreedy elloice t ) y selecting the path wit\]/theill ; i . xil/lllIll path probabilii , y to extend to there imdniug direction sill ti le  ( \] elmll ( lellcy list . AS llCl'l , Ss\[ulSllper(asseqllen ( ; e is one which ; ~ l , SS it ~ llSitSllp(!l't ; I . g to ( . ' it chl ) osition such that e au : h supertag \] H is all ( ) fits dependents an ( 1ma?hnizes the accunlula . i . ed path l ) rob ~ d ) ility . It is to lie noted tll at l , tile algorithm when pairing l , he head it ll ( lits del ) endent is not really parsing since it does so evell without looking attimstrllctll reo ~" the striilg ~ l  ) etween the head and the del ) endent . 
The implementation and testing of this Ill ( )( l ( , I of slipertag(lis~mbiguation is under way . Ta . 1 ) led shows preliminary results on the same held out test set of  100 Wall Street Jollrlla \] sei it elices thai : was used in the unigram and triRrain models  . 
The table shows two nieast lres of eva . hlal , ioil . Ill 4' l'wo arcs(a , c ) and ( b , d ) kite-string-tangle , if . . < b < c<dorb<a<d<c , the first , the dependency link measnre , the test seilteRces were indel ) endently ha . n ( l tagged with dependency links an ( ltlien were used tOn la . tch1 , he the lhlks output I ) y the del ) endency n lode l . 
The c:o huni+show tit ( ; total n unllJerel'clel ) en- ( lency liuks hit helilm d tagged set , the nuiriber of nm . t ched links output by this model and the i ) el'cellta . ~ e(-Ol Teetlless . The second lll Oa Slll'e ~ Sll-f)erta . gs , shows the tot: . 1 null ) her of cori'ects u-l)ertag , sassiD led to the words hit he COl'l ) USt ) y this model . 
C,'it( . , . io ,, I ,, U_@,'~\[_(, o , ', ', . c . t_1 < . o , . , . , ~<?_1 SUlierl'lgs\] 915_  ~__  707   77   26%  __'  "2  '_ ~' ' _  .   .   .   .   .   .   .   .   . ~__~'"'~' l'id)le . ' l : Results el " l ) epeudency nlo ( le 6Conclusion Lexica \ [ ized grammars : i , ssociate with each word richers gructll re ~ ; ( trees ill case ()\[' l'l'A (' , s and c~t-egories hi case o1'(Joml ) hl~tI , or yCa , l , egoria J (' , I'\[LI\]t--Ill ; ll'S((\](~(~S . ) ) OVeI ' which tile wor ( I specille syn-t : t ( : gic : lidS ( qll ; i . lltiC coll strathlts . I lence every word is as so (' ia . ted with ~ tuluch la . rger set of lllOl'eCOlll\[ ) \] exstl'll ( ' tlll'es\[ , hailill the ca , sew here the words : . reassociated with sta , nda . rdi ) a . rts-olZsl ) eech , l lowever , these more complex description salk ) w more comple-~coustraints to be imposed a . n d w , ' if ied locally on the coutexts in which these words a ? pea  . r . This fea . ture of lexicalized grammars can be taken a , dvantage of , to further reduce the ( lisalnl ) iguatioii task of the I ) arser , as slmwll in SUlmri . agd is a . ml)igua . i . ion . 
Ileu (: esui ) el ' Da , g~(lisai , nll ) igua ( , ioli(;a , l/Imuse(I : t ~'; a . g ; enera . I i ) re-i ) a . rsing ( : olnl ) one uto\['lexicalized ~ rl ' all ) Illal'pai'sels . 
The d ( , gree of distiuct , ionl ) et we ( mSUlml '( . a . g dis-aml ) igua . tiona . n(Ii/arsing va . ries , depen(ling on the . lexicalized g ; ranima . rbe(us(:onsi(M'ed . l , ' or both I/I'A (' , an ( IC'CG , supertag disaml ) igui ~ tion serves as a , preq ) arser filteri ; tutteffectively we . eds Oil ( iila , l)l ) rol)ria , tee IelIl (': llta , rystl'il('tures(tre . es or categories ) given l the c ( m text of the sentence . It also in ( liea . test he dopen den ( ' ies alnoi ~ g the elementary stru ( ' tlu'es but not timspe ( ' if icel ) era . ties to lie used l , ocoral ) ( he the strllctul/es or timit ( I-dress a . t which the el ) era . ties is to be l ) erformed " a . llah liost parse " , lifc'ases where 1 , 1l(;SUl ) ertagsequelice\[Tirthe~iW . ~lihil ) uts trilig c : l , llilot lie most parse " may m dee ( i be the best one can do . 
In case of LTAG , even though no exl ) licit substitutions or adjunctions are shown , the dependencies among LTAG trees uniquely identify the combining operation between the trees and the node at which the operation  ( : an be performed is almost always uniques . Thus supertag disambiguation is almost parsing lbrUI'-AGs  . In contrast , the dependencies among the CCG categories do not result indirectly identifying the combining operations between the categories since two categories can often be cornI  ) in edin more than one way . Hencefor CCG fiu'ther processing needs to be performed to obtain the complete parse of the sentence  , although without any supertag ambiguities . 
The supertag disaml ) iguation , dependency model in particular , is even closer top ~ wsing in dependency grammar formalism  , l ) ependency parsers establish relationships among words  , unlike the phrase-structure parsers which construct a phrase-structure tree spanning the words of the input  . Since LTAGs are lexicalized and each elementary tree is associated will  , a . tleast one lexical item , the supertag disaml ) iguation for EPAG can therefore be viewed as establishing the relationship a among words as dependency parsers do  . Then the elementary stru ( > tures that the related words anchor are combined to reconstruc the phrase-structure tree similar to the result of phrase-structure parsers  . Th , s the interplay of both dependency , ~nd phrase-structure grammars can be seen in U\[ ' AGs  . Rambow and Joshi(R , ambow and Joshi , 1993) discuss in greater detail the use of LTAC , in reh~ting dei ) endency analyses to phrase-structure analyses and I  ) rOl ) O Seadei ) endency-I ) as edl ) arser for a , phrase-structure based grammar . 
In summary , we have presented a new technique that performs the disambiguation of supertags using local int brmation such as lexi  ( ' alpreference and local lexical dependencies . This technique , like part-of-speech disambigua . tlon , ro . -duces the disambiguation task that needs to be S in some cases  , the dependency information between an auxiliary and an elementary tree may be insufficient o uniquely identify the address of adjunction  , if the auxiliary tree can adjo into more than one node in the elementary tree  , since the specific attachments are not shown . 
6 The relation a labels between two words it , L'I'AG is associated with the address of the operation between the trees that the words anchor  . 
done 1) y the parser . After the disa . nd ) iguation , we have effectively comi ) leted the parse of the sentence ~ md the parser needs % nly ' to coml  ) lete the ~ djunction and substitutions . This method can also serve to parse Sell tetlce\[ ' ra~lfleutsill cases where the supertag sequence after the disambiguation may not contbine to form a single structure  . We have implemented this technique of disambiguation using the ngram models using the prol  ) ability data collected from LTAGI ) arsed corpus . The similarity between lilAC and l ) e-pendency grammars is exploited in the ( lepen-dency mo ( M of supertag disambigm ~ tion . The per\['ormancer sults of these models have been presented  . 

Church , K .  (1988) . A Stochastic Pari ; sI ) rogram and Noun Phrase Parser for Unrestri (' i ; edTex LIn 2~ld Applied Natural Language Processing ConJ'cre~tcc 1988  . 
Doran , C . , l'\]gedi,D . , l\[ockey , B . A . and Srinivas , B . 
(1994) . XTAG7'ec&/calReport . I ) ep~rtrnento\['Computera . mlhdbrmation Sciences , University or lhmnsylwmia , l ) hihuh ! lld li~t , PA . In In ' ogress Ilindle , D .  (1993) . I'rediction of Lexic ~ dized Tree Fragments in ' I ' ex ~  ; ARPA Workshop on \[\[ um~ml , anguage Technology , March 11993 . 
Milward , D .  (1992) . Dynamics , Dependency Grammars and Incremental\[nterpreta . tion . In Proceedings of Ihe 14 th International Confe'ccn ccon Coml rutalional Linguistics  ( COLINC'92 )  , Nantes , 
France , August.
1 Lambow , O . and Joshi , A . K .  (1993) .  1 ) epen- ( h'ncy Parsing for I ) hrase-Structure ( \] rammars . 
Man'usc'cipt,U , liversil , y of I ' ( mnsylv:mia.
.qh,ator , I ) . a . n(1'l'elnp,wh'y,I ) .  (1991) . Parsing I ' ; nglish wil . halaid(h'muinar . 7 ~ chnical'report CMU-C', q'-91-196, Deparl . ment of Compul , er Science , Carnegie Mellon Uld versity ,  1991 . 
Schabes , Y . , AIMII 6A . and Joshi , A . K . ( t988) . Pro's-ing strat ; egies with ' lexicalized'grammars : Appli-c ; d , i onto tree adjoin in grammars . In P' . ,'oceedinys of the 12 th Inlernalional CoT@:rence on Comp ' ula-lional Linguislics  ( COLINC'88 )  , BudN ) est , Ilun-gary , August . 
Schal)es , Y . (\]9 q()) . Malhcmalical and Comp . ula-lional Aspects of Lczicalizcd Grammars . Ph . I ) . I , he-sis , University of Pe , msylva . nia , l'hiladelphia , PA , August . Ava , ilal ) leastechnic Mrel ) ort ( MS-CIS-90-48 , LINCLA\]~ , I79) from the l ) elmrl , menl ; of(\]om-lmter and htI'ormationS (: icnce . 

