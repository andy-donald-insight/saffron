A Formalism for Universal Segmentation of Text
Julien Quint
GETA-CLIPS-IMAG , BP 53 , F-38041 Grenoble Cedex 9 , France
Xerox Research Centre Europe ,  6 , chemin de Maupertuis , F-38240 Meylan , France
email : julien , quin'c@iraag.fr

Sumo is a formalism for universal segmentation of text  . Its purpose is to provide a franlework for the creation of segmentation applications  . It is called " universal " a stile formalism itself is independent of the language of the documents to process and independent of the levels of segmentation  ( e . g . words , sentences , paragraphs , nlor phemes . . . ) considered by the target application . This framework relies on a layered structure representing the possible segmentations of the document  . This structure and the tools to manipulate it are described  , followed by detailed examples highlighting some features of Sumo  . 
Introduction
Tokenization , or word segmentation , is a fundamental task of a hnost all NLP systems . In languages that use word separators in their writing  , tokenization seenls easy : every sequence of characters between two whitespaces or punctuation marks is a word  . This works reasonably well , but exceptions are handled in a cumber some way . On the other hand , there are languages that do not use word separators  . A much n lore complicated processing is needed , closer to morphological nalysis or part-of-speech tagging  . Tokenizers designed for those languages are generally very tied to a given system and language  . 
I to we ver , the gap become smaller when we look at sentence segmentation : a simplistic approach would not be sufficient because of the ambiguity of punctuation signs  . And if we consider the segmentation of a document into higher-level units such as paragraphs  , ections , and so on , we can notice that language becomes less relevant . 
These observations lead to the definition of our formalism for segmentation  ( ot just tokenization ) that consider stile process independently fl : om the language  . By describing a segmentation systenl formally , a clean distinction can be made between tile processing itself and tile linguistic data it uses  . This entails the ability to develop a truly multilingual system by using a common segmentation egine ~ br the various languages of the system  ; conversely , one can imagine evaluating several segmentation , eth-ods by using the same set of data with different strategies  . 
Sumo is the name of the proposed formal-is nl , evolving from initial work by ( Quint , 1999; Quint ,  2000) . Some theoretical works from the literature also support this approach :  ( Guo ,  1997 ) shows that sonle segmentation techniques can be generalized to any language  , regardless of their writing systenl . The sentence segmenter of ( Pahner and Hearst , 1997) and the issues raised by ( Habert et al ,  1 . 998) prove that even in l~n-glish or French , segmentation is not so trivial . 
Lastly , ( A~t-Mokhtar ,  1997 ) handles all kinds of presyntactic processing in one step  , arguing that there are strong interactions between segnlent a-tion and morphology  . 
1 The Framework for Segmentation 1 . 1 Overv iew The framework revolves around the document representation chosen for Sulno  , which is a layered structure , each layer being a view of the document at a given level of seglnentation  . 
These layers are introduced by the author of the segmentation application as needed and are not imposed by Sulno  . The example in section 3 . 1 uses a two-layer structure ( figure 4 ) corresponding to two levels of segmentation , characters and words . To extend this to a sentence seglnenter , a third level for sentences i added . 
These levels of segmentation can have a lin-can be introduced a  . swell when needed . It is also interesting to note that severalayers can belong to the same level  . In the example of section 3 . 3 , the result structure can have an indefinite number of levels  , and all levels are of the same kind . 
We ( : all item the segmentation unito\['a doc-untent at a given segmentation level  ( e . g . items of the word level are words ) . The document is then represented at every segmentation level in  1  ; erms of its items ; I ) ecause segmentation is usually ambiguous , item . qraph . ~ are used to \[' actorize all the possible segm c 'l  , ta . tions . Ambiguity issues are furthel ' addressed in section  2  . 3 . 
The main processing i ) aradigms of Sumo are ident/icatio'n and h ' ansJbrmation  ,  . With ideutifi-cal ; ion , new item graphs are built by identif ' ying items fi'om a source graph using a segmentation resource  , q ' hese graphs are 1; hen modified l ) y translbrula . tion processes . Section 2 gives the details al ) out both identificatio ~ l and t\]'a . nsf ofmation . 
1.:2 Item Graphs.
' l'lleiten:lgral ) hs are directed acyclic gral ) hs ; they are similar to the word graphs of ( Amtru 1 ) et al , 11996) or the string graphs of ( C'olmer-auer ,  1970) . They are actually rel ) resente ( I 1 ) ymeans of finite-sta . teautomata ( see section 2 . \]) . 
I Horder to facilitate their manilm lation , two a ( 1-ditio ~ talprol ) erties are on forced : the semJtomata ahva yslm . veasing le start-state and finite-slate , and no dangling arcs ( this is verified by pruning the automatafter modifications  )  . The exam-pies of section 3 show va . rio ~ lsitelngraphs . 
An item is an arc in the automato ~ l . An arc is a complex structure containing a label ( generally the surface/brm of the item )  , named attributes and relations . Attributes are llsed to hold information on the item  , like part of speech tags ( see section 3 . 2) . These attributes can also be viewed as annotations in the same sense as the annotation graphs of  ( Birdel ; 3 l . , 2000) . 
1.3 Relations
Relations are links between levels . Items from a given graph are linked to items of the graph from which they were identified  . We call the first graph the Iowcr graph and the gral  ) h that was the source \[ br the identification the upper graph  . Relations exist between a path in the upper graph and either a path or a subgraph in the lower graph  . 
Figure i illustrates the first kind of relation , called path relation . This example in French is a relation between the two characters of the word " du " which is really a contraction of " dele "  . 
Figure 1: A path relation
Figure 2 illustrates the other ldnd of relation called subgraph relation  . In this example the sentence ABCI ) EI , G .   ( we can imagine that A through G are Chinese characters  ) is related to several possible segmentations . 
ABCD~E"-()"9--.
ABCDE~FG .  ( ) ,<) -,<) ~() ,-() ,-() , , > ~ /7 ~ . BC DEF G~'1"l ? . : ~ mCDZFG % - O
FigElre 2: A graph relation
The interested reader may refer to ( Pla . nas , 1998) for a conq ) arable 8 trllctul ; e ( multiple layers of a document and relations ) used intra . ns-lation memory . 
2 Processing a Document 2 . 1 Descr ip t ion of a Docmnent The core of the document representation is the item graph  , which is represented by a finite-state automaton . Since regular expressions define finite-state automata  , they can be used to describe an item graph . I to we ver , our expressions are extended because the items are more complex than simple symbols  ; new operators are introduced : ? attributes are introduced by an@sign  ; ? path relations are delimited by and ; ? tile inlbrmation concerning a given item are parenthesized using \[ and \]  . 

As an exemple , the relation of figure 1 is described by the following expression :\[ de ledu \]  2  . 2 Ident i f icat ion Identification is the process of identifying new items froln a source graph  . Using the source graph and a segmentation resource  , new items are built to form a new graph . A segmentation resource , or simply resource , describes the vocabulary of the language , by defining a mapping between the source and the target level of segmentation  . A resource is represented by a finite-state transducer in Sumo  ; identification is performed by applying the transducer to the source automaton to produce the target automaton  , like in regular finite-state calculus . 
Resources can be compiled by regular expressions or indentification rules  . In the former case , one can use the usual operations of finite-state calculus to compile the resource : union  , intersection , composition , etc ) A benefit of the use of Sumostructures to represent resources i that new resources can be built easily from the document that is being processed  . ( Quint ,  1999 ) shows how to extract proper nouns from a text in order to extend the lexicon used by these g -reenter to provide more acurate results  . 
In the latter case , rules are specified a . s shown in section 3 . 3 . The lefthand side of a rule describes a suh path in the source graph  , while the right hand side describes the associated subpath in the target graph  . A path relation is created between the two sequences of items  . In an iden-tific ~ tion rule , one can introduce variables ( for : callback ) , and even calls to transformation functions ( see next section )  . Naturally , these possibilities cannot be expressed by a strict finite-state structure  , even with our extended formalism ; hence , calculus with the resulting structures is limited . 
A special kind of identification is the automatic segmentation that takes place at the entry point of the process  . A character graph can be created automatically by segmenting an input text document  , knowing its encoding . This text document can be in raw form or XML format  . 
Another possibility for input is to use a graph 1The semanl , ics of these operations is broadened to accomodate the more complex nature of the items  . 
of items that was created previously , either by Sumo , or converted to the tbrmat recognized by ~1_11\]10  . 
2.3 Transformation
Ambiguity is a central issue when talking about segmentation  . Tile absence or ambiguity of word separators can lead to multiple segmentations  , and more than one of them can have a meaning . As ( Sproat et al , 1996) testify , several native Chinese speakers do not always agree on one unique tokenization for a given sentence  . 
Th~nks to the use of item graphs , Sumo can handle ambiguity efficiently . Why try to fully disambiguate a tokenization when there is no agreement on a single best solution ? Moreover :  , segmentation is usually just a basic step of processing in an NLP system  , and some decisions may need more information than what a set-reenter is able to provide  . An uninformed choice at this stage can affect the next stages in a negative way  . Transformations are a way to modify the item graphs so that the " good " paths  ( segmentations ) can be kept and the " bad " ones discarded . We can also of course provide flll disambiguation  ( see section 3 . 1 for instance ) by means of transformations . 
In Sumotransformations are handled by transformation  5mctions that manipulate the objects of the tbrmalism : graphs  , nodes , items , paths ( a special kind of graph ) , etc . These functions are written using an imperative language illustrated in section  3  . 1 . A transformation can either be a pl ) lied directly to a graph or attached to a graph relation  . In the latter case , the original graph is not modified , and its transformed counterpart is only accessible through the relation  . 
Transformation functions allow to control the flow of the process  , using looping and condition-sis . An important implication is that a same resource can be applied iteratively  ; as shown by ( Roche ,  1994: ) this feature allows to implement segmentation models much more powerful than simple regular languages  ( see section 3 . 3 for an example ) . Another consequence is that a Sumo application consists of one big transformation function returning the completed Sumo structure as a result  . 
658 3 Examples of Use 3 . 1 Maximum tokenization Some cla . ssic heuristics for tokenization a . reclassified 1) 3 ,   ( Gi % 1997 ) under the collective monil<er of mare \] mum tokenization  . Thiss Betion describes how to iml ) lement a . " maxiln nm tokenizer " tha . t tokenizes raw text doculner its in alA\]- given language and cha  . racter encoding ( e . g . ea < (! l\]glishin .   .   .   .   . , French in Iso-Latin-l , Chinese ill
Big 5 or GB).
8.1.1 Coml nonset-up
Our tokenizer is built with two levels : the input level is the characterlevel  , automatically segmented using the encoding intbrmation  . The token level is built from these cha , racters , first by ~ liexllaustive identification of the to l < en s  , then by re ( hieing the U Hlnbero \] "1 ) > ~ tl is totile one coil-side re ( 1 tlle best 1 ) y the Ma . xil\]\]Ul\]\]]bkenization heuristic . 
The system works ill three stel ) S , with complete codeshow nill figure 3 . First , the characterlevel is created 1) 3 , automatic segnleutation ( lines ; 15 , input levei being the special gi ' aph that is automatically created from a  . ra , w file throng hstdiu ) . The second step is to create the word grapli1 ) y ident if ' ying words D ' oln chata . c to Pllsi i ig a diction a . ry . Aresour ( : e called ABg dic is created from a transducer file  ( lines 68 )  , then the gra , ph words is created by identifying it , en is from the SOllr Ce level characters ll Sing the re-so II rCO ABC dic  ( lines 912 )  . The third step is the disalnl ) igua , tion of ' the woM levelt ) yal)l ) lying a , Ma , xinii n ~ Toke \] iization lmuristic ( line 13) . 
i characters : input level 2 encoding : < ASCII , UTF8 , Big 5 . . . >3 type:raw ; 4 from : std in ; 56 ABC dic : resource 7 file : ' CABC dic . sumo '' ;   8   9 words : graph <- identify i0 source : characters ; il resource : ABC dic ; 1213 words <- ft(words . start-node ) ; l Pigure 3: Maximuln'lkkenizer in Sumol qgure 4 illustrates the situatiori for the ill-put string " ABCI  ) I~FG " where A through Gaxe characters and A , A B , B , B C , 13Cl )\]'; le , C , CI ) ,  13 , 1) E , E , F , I"C and ( 3 are words folm d in the resource ABC dic . The situation shown is after line 12 and before line 13  . 
ABC ~-< DEFG (3 (3 ~ . -<3 - ~- - ) (2 )  .  -<3 ~ -  . <~
Z/,<=-,>,TA'?M\-//"BCDEF/
Figllre 4: lP xhaustive tokenization of the string
ABA ) LIG
We will see in the next three subsections l ; he different heuristics and their implementations in 
Slll\]\]O.
3 . 1 . 2 Forward Maxhnum Token izat ion l%rward maxilnlltn ' lbkenization consists of scanning tile string from left to right and selecting the token of maxinulm lerigth any time an ambiglfity occurs  . On the exaln ple of figured , tile resl , lt tokeliization of the in I ) ~lt string would 1 ) eAI ~/ CD/I'\]/IeG . 
lqgn re5 shows at'lmction called ft that 1 ) uilds a path recursively by traversing tile token graph  , al ) l ) ending the longest item to the pa . that each node . ftta . kesa , node as input and retlirils a . 
path ( line 1) . If tile node is final , the enll ) tyl ) atll is retm'ned(lines 23) , otherwise the array of items of tlle nodes In . items ) is sea . rched and the longest item store ( \] in longest ( lines 410 )  . 
The returned pa , th consists of this longest item prepended to the longest path starting from the destination ode of this item  ( line 11 )  . 
a . :t . a Backward Maxinmm Tokenlzation l ~ a . ckward Maximum Tokenizationistile same as librward Maximum'lbkenization except that the string is scanned fi'om right to left  , instead of left to right . On the example of figure 4 , the tokenization of the input string would yield A/I~C/I  ) E/1 , ' Cunder Backward Maximum Tokenization . 
A function b t can be written . It is very sim-ila . r to f t , except that it works backward by looking at incoming arcs of ' the considered node  . 
bt is cM led on the final state of tile graph and

I i if final ( n ) return ( ) ; else longest : item <- n . items\[l \]; for each it in n . items\[2 . .\]  if it . length > longest . length longest <- it ; return ( longest  #f t ( longest . dest));
Figure 5: The ft function stops when at the initial node . Another implementation of this function is to applyft on the reversed graph and then reversing the path obtained  . 
3.1.4 Shortest Tokenization
Shortest Tokenization is concerned with minimizing  (  ; he overall number of tokens in the text . 
On the example of figure 4 , the tokenization of the input string would yield A/B CI  ) I~ , I :/ G under shortest tokenization . 
Figure 6 shows afnnction called st that finds the shortest path in the graph  . This function is adapted from an algorithm for : single-source shortest paths discovery in a DAG given by  ( Cormen et al ,  1990) . It calls another function , t_sort , returning a list of the nodes of the graph in topological order  . The initializations are done in lines 26 , the core of the algorithm is in the loop of lines  714 that computes the shortest path to every node , storing for each node its " predecessor " . Lines 1 . 5-20 then build the path , which is returned in line 21 . 
3.1.5 Combination of Maximum
Tokenization techniques
One of the features of Sumo is to allow the comparison of different segmentation strategies using the same set of data  . As we have . just seen , the three strategies described above can indeed be compared efficiently by modifying only part of the third step of the processing  . Letting the system run three times on the same set of input documents can then give three different sets of results to be compared by the author of the system  ( against each other " and against a reference tokenization  , for instance ) . 
if unctions t ( g : graph ) -> path 2d : list <- ( ) ; // distances 3p : list <-() ; // predecessors 4 for each n in ( g . nodes ) 5 d\[n\]=integer . max ; // ~ C infinite ~6 ? for each n : node int_sort ( g . nodes ) 8 for each it in n . items 9 if ( d\[it . dest\]>din\]+i ) then i0d\[it . dest\] = din \] + i ; ii p\[it . dest\]=n ; 12131415 n<-g . end ; // end state 16 sp : path <-( n ) ; // path 17 while ( n != g . start ) 18n = pin \] ; 19 s p = ( n #s p ) ; 2O21 returns p ;  22 
Figrlre 6: the st function
And yet a different setup for our " maximum tokenizer " would be to select not  . just the optimal pa . thac cording to one of the heuristics , but the paths selected by the three of them , as shown in figure 7 . Combining the three paths into a graph is perfbrmed by changing line  13 in figure 3 to : word s <- f t ( words . start-node ) Ibt(words . end-node )\] st(words . start-node);
ABCDE
Figure 7: Three maximum tokenizations 3 . 2 Statistical Tokenlzation and Part of
Speech Tagging
This example shows a more complicated tokenization system  , using the same sort of setup as the one from section  3  . 1 , with a disaln bigua-tion process using statistics ( namely , a bigram model ) . Our reference for this model is the Chasen Japanese tokenizer and part of speech '  . l'h is example is a high-level description of how to implemen ~ a simila  . r system with Sumo . 
The setup for this example adds a new level to the pre  . vious example : the " bigra . m level . " The word level is still built by identification using dictionaries  , then the bigral n level is built by computing a . connectivity cost between each pair of tokens . This is the level that will be used for disambigu ~ tion r selection of the best solutions  . 
3.2.1 Exhaustive Segmentation
All possible segment artiOns ~ rederived from the characterlevel to create the word level  . Timre , ~ onrce used\['or this is a dictionary of the la . n-gua , ge that maps the surface form of the words ( in terms of their characters ) to their baseform , part of speech , and a . cost ( Chasellal so a . ddsl ) ronunciation , co1\jugation type , and semantic information ) . All this inlbrmation is stored in the item as attril  ) utes , the base for mheing used as the label for the item  . I , ' igure 8 sllows the identifica J ; ion of lille word " ca . ts " which is identified as " cat " , with category " noun " ( i . e . @ CAT = N ) and with some cost k (@ COST = k) . 
cats ( ), , <) ~ , .  /~ ,*( ) ~ .  )  . . . . , \ /  . . . . . 
cat@CAT=N@COST=k
Figure 8: Identification of the wor<l " cats " 3 . 2  . 2 S ta t i s t i ca l D isambiguat ion The disambiguation method relies on a bigran l model : each pair of successive items has a " connectivity cost "  . In the bigram level , tim " cost " attribute of an item W will be the connectivity cost of W and a following item X  . Note that if a same W can be followed by severa J items X  , Y , etc . with different connectivity costs for e~chp ~ tir , then W will be replicated with a . different " cost " attribute , l : igure 9 shows a word W followed by either X or Y , with two different connectivity costs h and U . 
The implementation f this technique in Sume is straightibrward  . Assume there is a fllll C tion f that , given two items , computes their connectivity cost ( depending on both of their category , i ) l dividual cost , etc . ) mid returns the first item / f 0~! coopt=w ( ) Y'"~O
Figure 9: Connectivity costs for W with its modified cost . We write the following rule a . nda , p ply it to the word graph to creat ; e the bigram graph : _\[$ w l = . e .  \ ] _ \[$~2 = @ . \]-> eval(f($wl , $2)) T if fsr , lle can be read as : for any word $ wl with any attribute  ( "  . " matches any label , " O . " a . nyset of attributes ) followed by any word $ w2 with any attribute ( "_ " being a context separator )  , create the item returned by the fimction f($ul , $ u2) . 
I)isambiguaJ ; ion is then be perforlned by selecting the pa . th with optimal cost in this graph ; but we ca , n also select a . ll paths with a cost colresl ) on clillg to a certain threshold or the nbest t ) a . ths , etc . Note also that this model is easily extensible to any kind of ngrams  . A new fllnction f($wl .   .   .   .   . Swn ) must be provided to corn-pule the connectivity costs of this sequence of items  , and the above rule m , lst be modified to take a larger context into accom~t  . 
3.3 A Forlnal Exmnple
This last examl ~ h'~is more formal and serw~s as an il hlstra  . tion of some powerful features of ' Sumo . ( Cohnerauer ,  1970 ) has a similar exam-pie implemented using Q systems  . In both cases the goaJ is to transform an input string of tiltl brma ~'% "~ c'  , n > 0 into a single item , S'(assuming theft the input a , lphal ) et does not contain , S ') , meaning tha . t the input string is a word of this laaguage . 
These tuphere is once again to start with a lower level automatically created fl ' om the input  , then to build intermediate lvels until ~ final level containing oMy the item S is produced  ( at which point the input is recognized )  , or until the process C all no longer carry on ( at which point the input is rejected )  . 
.I hc building of intermediary levels is handled by the identifica  . tioll rule below :  #S ? a\[$A = a*\]b\[$B = b* \] c\[$c=c*\]#->SSASB$CformS?aa*bb*cc  *  , storing alla's but the first one in the varia . bleSA , all b's but the first one in $ B and M1 c's but the first one in $ C . The first triplet abc ( with a possible S in front ) is then absorbed by , 5' , and the remaining a ' s , b's and c's are rewritten after , 5' . 
Figure 1 . 0 illustrates the first application of this rule to the input sequence aabbcc  , creating the first intermediate lvel ; subsequent applications of this rule will yield the only item  , 5' . 
, .. ~_) a abbcc

We have described the main features of Sumo , a dedicated formalism\[br segmentation f text . A document is represented by item graphs at d if ferent levels of segmentation  , which a . llows multiple segmentations of the same document a  . t the same time . Three detailed ex~mples illustrated the features of Sumo discussed here  . For the sake of simplicity some aspects could not be evoked in this paper  , they include : management of the segmentation resources  , ef\[iciency of the systems written in Sumo , larger a . pplica-tions , evaluation of segmentation systems . 
Sumo is currently being prototyped by the author.

Sala . hAPr-MOKHTAR , "Dutexte ASCII autexte lemmatis5 : laprf syntax en une seule 6tape  " , in Proceedings of TALN-97 , pages 60-69 , Grenoble , France , June ,  1997 . 
Jan W . AMTRUP , Henrik HE , N ~ and U we
JEST , l/V hat's in a Word Graph . Evaht-ation and Enhancement of Word Lattices , Verbmobil report 186 , Universit ' ~ ttltamburg , http://www . dfki . de/,l)ecember , 1997 . 
Steven BraD , David DAY , John GARO FOI , O,
John It ENDERSON , Christophe LAPRUN and
Mark\[,mERMAN , " ATI , AS : A Flexible and
Extensible Architecture for Linguistic Anne-tation  "  , in Proceedings of LRI~C2000 , Athens , 
Greece , May , 2000.
Ala . in ( JOLMEI~AUER , Lc , ~  , ~yst~1) ~ , c ~" (2ott " t t l t formal is ' m . epour analyseret synth . dtiser desphrases s~trordinateur ' , Publication internenum fro 43 , Universit 6 de Montr 6a . 1, 1970 . 
Thomas H . COaM ~ . TN , Charles E . I , I ; ISERSON and Ronald L . R w l ~ , s'r , Introduction to Algorithms , MIT Press , Cambridge , Massachussets ,  1990 . 
Jin Guo , " Critical Tokenization and its Properties " , in Computational Linguistics ,  23(4) , pages 569-596 , December ,  1997 . 
B.HABERT , G.ADI)A,M . ADDA-I)I~CI < EI~.,P.
BOULAl ) EMARI';; UIL , S . I?I ~, II/AI/I , O . \];' Ell-RET , G . \]LI , ouza , ndP . PAI OUBI , ; K , " Towards Tokenization Evaluation " , in Proceedings of
LREC-98, pages 4:27-431, 1998.
Yuji MAT SUMOTO , A kira\[(\]TAUC lII , Tatsuo YAMAS ItlT Ael ; Yoshitaka HIRA NO , Japanese Morphological Analysis System Cha , 5' cnversion 2 . 0Man'aal , Technical Report NAIST-IS-TR 99009 , Nara Institute of Science and Technology , Nara . , April , 11999 . 
David 1) . PALMERa . nd Ma . rti A . It EAIS'G"Adaptative Multilingual Sentence 13oundary Disambigua . tion " , in Computational Ling'uis-lies ,  23(2) , pages 241-267 , June ,  11997 . 
h;mmanuel P1,ANAS , 7'\['2 LA .   , 5~tr'uct'urcs et al gorithmcs pour la 7'aduction Fond & surla Mdmoirc , Th 6 sed'lnformatique , Universit 6 . 1o sephl , ' ourier , Grenoble , 1998 . 
.Julien QUIN % " Towm'ds a fbrmalism for language -independent text segnmntatio Jl "  , in Procccd in 9s of NLPRS'99 , pages 404-408 , Beijing , November ,  1999 . 
Julien QUr N % " Universal Segmentation of Text with the Sumol  , brnmlism " , im Proceedings of NLP 2000 , pages 1 . 6-26, Pa . tras , Greece , June , 2000 . 
Emmanuel ROCIIE , " Two Parsing Algorithms by Means of Finite State Tl : ansducers "  , in Proceedings of COLING-9/~ , pages 431-435 ,  1994 . 
Richard SPI ~, oArp , Chilin SIIIII , William Ga Ll . ; and Nancy CIIANG , " A Stochastic Finite State Word-Segmentation Algorithm for Chinese "  , in Computational Linguistics 22(3) , pages 377-404 ,  1996 . 

