EXPERIMENTS WITH APOWER FULPARSER

Martin KAY
The R and Corporation
1700 Main Street
SANTA-MONICA-California 90406-U.S.A.
EXPERIMENTS WITH APOWER FULPARSER
This paper describes a sophisticated syntactic -analysis program for the IBM  7040/44 computer and discusses some of the problems which it brings to light  . Basically the program is a nondeterministic device which applies unrestricted rewriting rules to a family of symbol strings and delivers as output all the strings that can be derived from members of the initial family by means of the rules provided  . A subsidiary mechanism deals with the relation of dominance  , in the sense common in linguistics . This makes it possible for rules to refer to complete or partial syntactic structures  , or P-markers , so that the program can be used at least to some extent for transformational analysis  . 
A program of this kind , which is intended for analy-sing natural languages  , must be capable of operating on a family of strings as a single unit because of the grammatical ambiguity of words  . Take , for example , the famous sentence " Time flies like a narrow . " These five words are not , themselves , the primary data on which a parsing program can be expected to operate  . Instead , each word is replaced by one or more symbols representing the grammatical categories to which it belongs  . The assignments for this example might be somewhat as follows: 
Word Grammatical category
Time flies like a narrow
Noun , verb , adjective
Plural noun , 3rd person verb
Singular noun , preposition , verb
In definite article
Singular noun , adjective.
Taking one category symbol for each word , it is possible to form 30 different strings , preserving the order of the original sentence . These 30 strings constitute the family on which the program would operate if set to analyze this sentence  . 
The program is said to perform as a nondeterministic device because whenever two mutually incompatible rules are applicable to the same string neither is given any priority  ; both are applied , and the resulting strings developed independently  . Given the string " ABC " and the rules i
AB--~XY
BC -' ~ Z the program will therefore produce two new strings : 
X Y C
AZ
The program contains no mechanism for guarding against sequences of rules which do not terminate  . 
If the grammar contains the following rules
AB"~BA
BA--~AB and the string to be parsed contains either " ABy ' or " BA  , " then the program will continue substituting these substrings for one another until the space available for intermediate results is exhausted  . This may hot seem to present any particularly severe problem because a pair of rules such as these would never appear in any properly constructed grammar  . But , as we shall shortly see , entirely plausible grammars can be constructed for which this problem does arise  . 
i . THEFORMOFRULES
In order to get a general idea of the capabilities of the program  , it will be useful first to consider the notation used for presenting rules to it and the way this is interpreted by the machine  . In what follows , we shall assume that the reader is familiar with the terminology and usual conventions of phrase -structure and transformational grammar  . An example of the simplest kind of rewrite rule is 
VPRSG = PRESSG VERB
The Y ' equals " sign is used in place of the more familiar arrow to separate the left and righthand sides of the rule  . The symbols on which the rules operate are words consisting of between one and six alphabetic characters  . The above rule will replace the symbol " VPRSG " by a string of three symbols " PRESSG VERB " whenever it occurs  . The following rule will invert the order of the symbols " VERB " and " ING " 
VERBING = ING VERB
The simplest way to represent a contextfree phrase structure rule is as in the ' following example : 
NP AUX VP = S
Notice that the normal order of the left and right hand sides of the rule is reversed because the recognition process consists in rewriting strings as sin-gle symbols  ; the rules must therefore take the form of reductions rather than productions  . 
The program will accept phrase structure rules in the form we have shown  , but , in applying them , it will not keep a record of the total sentence structure to which they contribute  . In other words , it will cause a new string to be constructed , but will not relate this string in any way to the string which was rewritten  . One way to cause this relationship to be preserved is to write the rule in the following form : 
NP . IAUX . 2 VP .3 = S(I23)
The number following the symbols on the left , hand side of the rule function very much like the numbers frequently associated with structural indices in transformational rules  . When the \] eft-hand side of the rule is found to match a particular substring  , ~ the number associated with a given symbol in the rule becomes a pointer to  , or a temporary name for , that symbol . With this interpretation , the left hand side of the above rule can be read somewhat as follows " Find an NP and calliti  ; Find an AUX following this and call it 2 ; Find a VP following this and call it 3 . " The numbers in parentheses after a symbol on the right hand side of a rule are pointers to items -identified by the lefthand side  , and which the new symbol must dominate . In the example , the symbol " S " is to dominate all the symbols mentioned on the lefthand side  . 
A pointer may refer to a single symbol , as we : have shown , or to a string of symbols . The following rule is equivalent to the one just described : 
NP . IAUX . IVP . I = S(1)
Furthermore , the string to which a pointer refers need not be continuous  . Consider the following example
NP . IAUX VP . I--S(1) written as " S " , but the " S " will dominate only " NP " and " VP . " There will be no evidence of the intervening " AUX " in the ~ nalP-marker which will contain the following phrase : 
S/\ .
l ~ VP
Consider now the following pairs of rules :
A . IB . 2C . ID . 2 = P (1) Q(2)
P . lQ . I = s(1)
If these rules are applied to the string " ABCD " the following P-marker will be formed : /\/\ o Notice that the first rule in the pair not only reorders the symbols in the P-marker but forms two phrases simultaneously  . 
A different way of using pointer numbers on the right hand side can be illustrated by comparing the effects of the following two rules : N  . ISG . 1V . 2 SO . 3=NOUN(l)V (2) SG(3)
N . ISG . IV . 2 SG .2 = NOUN(l)2
What is required , we assume , is a context sensitive phrase structure rule which will rewrite " NSG " as " NOUN " in the environment before " VSG "  . The first rule achieves this effect but also introduces a new " J ~" dominating the old one  , and a new " SG " . The second rule does what it really wanted : It constructs phrase labeled " NOUN " as required  , and leaves the symbols referred to by pointer number  2 unchanged . 
The context sensitive rule just considered is pre I sumably intended to insure that singular verbs have only singular subjects  . A second rule in which " SG " is replaced by " PL " would be required for plura ~ verbs  . But , ~ nce agreements of this kind may well have to be specified in other parts of the grammar  , the situation might better be described by the following three rules : 
SG . I=NUM (1)
PL . I = NUM(!)
N . INUM . 2V .32 = NOUN(I2) 32
The first two rules introduce a node labeled " NUM '! into the structure above the singular and p lural morphemes  . The third rule checks for agreement and forms the subject noun phrase  . Pointer number 2 is associated with the symbol " NUM " in the second place on the lefthand side  , and occurs by itself in the fourth place . This means that the fourth symbol matched by the rule must be " NUM  , " and also that it must dominate exactly the same subtree as the second  . 
In the example we are assuming that " NUM " governs a single node which will be labeled either " SG " or " PL " and the rule will ensure that whichever of these is dominated by the first occurrence of " NUM " will also be dominated by the second occurrence  . Notice that noun and verb phrases could be formed simultaneously by the following rule : N  . INUM . 2V . 32 = NOUN ( I 2 ) VERB ( 3 2 ) The symbols " ANY " and " NULL " are treated in a special way by this program and should not occur in strings to be analyzed  . The use of the symbol " NULL " is illustrated in the rule : This will cause the symbol " PPH " to be deleted from any string in which occurs  . The program is nondeterministic in its treatment of rules of this kind  , as elsewhere , so that it z will consider analyses in which the symbol is deleted  , as well as any which can be made by retaining it . The symbol " NULL " is used only on the right hand sides of rules  . 
The symbol " ANY " is used only on the lefthand sides of rules and has the property that the word implies  , namely that it will match any symbol in a string . The use of this special symbol is illustrated in the following rule : This will form a verb phrase from a verb and a noun phrase  , With one intervening word or phrase , whose grmmnatical category is irrelevant . 
Elements on the lefthand sides of rules can be specified as optional by writing a dollar sign to the left or right of the symbol as in the following rules : 
DET . IADJ $. INOUN . I = NP (1)
VERB .1 SANY .1 NP . I = VP (1)
The first of these forms a noun phrase from a determiner and a noun  , with or without an intervening adjective . The second is a new version of a rule already considered  . A verb phrase is formed from a verb and a noun phrase  , with or without an intervening word or phrase of some other type  . 
Elements can also be specified as repeatable by writing an asterisk against the symbol  , as in the following example :
VERB.i*NP.i = VP(1)
This says that a verb phrase may consist of a verb followed by one or more noun phrases  . It is often convenient to be able to specify that a given element may occur zero or more times  . This is done in the obvious way by combining the dollar sign and the asterisk as in the following rule: 
SDET . I*$ADJ . IN . I*PP $. I=NP (1)
According to this , a noun may constitute a noun phrase by itself . However the noun may be preceeded by a determiner and any number of adjectives  , and followed by a prepositional phrase , and all of these will be embraced by the new noun phrase that is form-ed  . Notice that the asterisk and the dollar sign can be placed before or after the symbol they refer to  . 
The combination is often useful with symbol " ANY " in rules of the following kinds N  . INUM . 2*$ ANY . 3V . 42 = NOUN ( I 2 ) 3 VERB ( 4 2 ) This is similar to an earlier example . It combines the number morpheneme with a subject noun and with a any number of other symbols to intervene  . The symbol " ANY " with an asterisk and a dollar s~gncorL responds in this system to the so called variables in the familiar notation of transformational grammar  .   . , . ?
Consider now the following rule:
SCONJ .1 NP(S ). I=NP (1)
This will form a noun phrase from a subordinating conjunction followed by a nou ~ phrase  , provided that this dominates only the " ~ ymbol " S  . " Any symbol on the left hand side of the rule may be followed by an expression in parentheses specifying the string of characters that this symbol must directly dominate  . This expression is constructed exactly like the left hand sides of rules  . In particular , it may contain symbols followed by expressions in parentheses  . The following rule will serve as an illustration of this  , and of another new feature : NP ($ DET . I $* ANY . IADJ ( PRPRT . 2)$* ANY . 3N . 4$ PP . 5 ) 134 WHDE F4 BEADJ ( (2 ) )  5 This rule calls for a noun phrase consisting of a noun  , a preceding adjective which dominates a ~ re- sent participle and  , optionally , a number of other elements . This noun phrase is replaced by the determiner from the original noun phrase  , if there is one , the elements preceding the noun except for the present participle  , the noun itself , the symbol ' ~ H , " the symbol " DEF ~" another Copy of the noun , the symbol f ~ E ~ " the symbol " ADJ " dominating exactly those elements originally dominated by ' ~ RPRT " and  , finally , any following prepositional phrases the original noun phrase may have contained  . 
The number "2" in double parentheses following " ADJ " on the right hand side of this rule specifies that this symbol is to dominate  , not the present participle itself , but the elements , if any , that it dominates . This device turns out to have wide utility . 
Double parentheses can also be used following a symbol on the lefthand side of a rule  , but with a different interpretation . We have seen how single parentheses are used to specify the strin~in~ne-diately dominated by a given symbol  . DouSle'parantheses enclose a string which must be a pro-per analysis of the subtree dominated by the given symbol  . A string is said to be a proper analysis of a subtree if each terminal symbol of the  . sub-tree is dominated by some member of the string  . As usual , a symbol is taken to dominate itself . As an example of this , consider the following rule : ART . IS((ARTN . 2ANY *)) . I2=DET ( 1 )   2 This rule applies to a string consisting of an article  , a sentence , and a noun . The sentence must be analysable , at some level , as an article followed by a noun , followed by at least one other word or phrase . The noun in the embeded sentence , and the subtree it dominates , must be exactly matched by the noun corresponding to the last element on the lefthand side of the rule  . The initial article and the embeded sentence will be collected as a phrase under the symbol " DET " and the final noun will be left unchanged  . 
The principal facilities available for writing rules have now been exemplified  . Another kind of rule is also available which has a lefthand sidelike those already described but no equal sign or right hand side  . However it will be in the best interests of clarity to defer an explanation of how these rules are interpreted  . 
The user of the program may write rules in exactly the form we have described or may add informat ion to control the order in which the rules are applied  . 
This additional information takes the form of an expression written before the rule and separated from it by a comma  . This expression , in its turn , takes one of the following forms : nI , nl/n2 , nl/n2/n3 , nl//n3 , n I in an integer which orde ~ this rule relative to the others  . Since the same integer can be assigned to more than one rule  , the ordering is partial . Rules to which no number is explicitly assigned are given the number  0 by the program . 
n2 and nx , when present , are interpreted as fol-Jlows : Egery symbol in the substring matched by the lefthand side of the rule must have been produced by a rule with number i  , where ng ) i ~ n q . 
For these purposes the symbols in the 5riginal family of strings offerred for analysis are treated as though they had been produced by a rule with number O  . 
2. PHRASE-STRUCTUREGRAMMAR
It will be clear from what has been said already that this program is an exceedingly powerful device capable of operating on strings and trees in a wide variety of ways  . It would clearly been tirely adequate for analyzing sentences with a contextfree phrase -structure grammar  . ~ut this problem has been solved before , and much more simply . We have seen how the notation can be used to write context-sensitive rules  , and we should therefore expect the program to be able to analyze sentences with a context -sensitive grammar  . However in the design of parsing algorithms , as elsewhere , context-sensitive grammars turn out to be surprisingly more complicated than contextfree grammars  . 
The problem that context-sensitive grammars pose for this program can be shown  . ~ with a simple example . I Consider the following in grammar :
E(S ) (2)
B -,- F/E J (4)
D_,.~G/A-(5) lB/___.E (6)
This grammar , though trivial , is well behaved in all important ways . The language generated , though regular and unambigious , is infinite . 
ii am indebted for this example , as for other ideas to onumerous to document individually  , to
Susumu Kuno of Harvard University.
Furthermore , every rule is useful for some derivation . Since the language generated is unambigious , the grammar is necessarily cycle-free , in other words , it produces no derivation in which the same line occurs more than once  . Suppose , however , that the gr~nmar is used for analysis and is presented With the string " A DE "-- not a sentence of the language  . The attempt to analyze this string using rules of the grammar re-suits in a rewriting operation that begins as follows and continues indefinitely : 
ADE
ABE ( by rule 3)
ADE ( by rule 6)
ABE ( by rule 3) etc.
It would clearly be possible , in principal , to equip the program with a procedure for detecting cycles of this sort  , but the time required by such a procedure , and the complexity that it would introduce into the program as a whole  , are sufficient to rule it out of all p ractical con-sideration  . It might be argued that the strings which have to be analyzed in practical situat ions come from real texts and can be assumed to be sentences  . The problem of distinguishing sentences from nonsentences is of academic interest  . 
But , in natural languages , the assignment of words to grammatical categories is notoriously ambigious and for this problem to arise it is enough for suitably ambigious words to come together in the sentence  . A sentence which would be accepted by the above gram ~ nar  , but which would also give rise to cycles in the analysis  , might consist of words with the following grammatical categories : 
Word Grau ~ atical Category
IA2 B3C , Ei0
The program , as it stands , contains no mechanism which automatically guards against cycles  . However , if the user knows where they are likely to occur or discovers them as a result of his experience with the program  , he can include some special rules in his grammar which will prevent them from occurring  . These rules , which we have already eluded to , are formally similiar to all others except that they contain no equals sign and no righthand side  . When a P-marker is found to contain a string which matches the lefthand side of one of these rules  , the program arranges that , then ceforward , no other rules hall be allowed to apply to the whole string  . The cycle in this latest example could not occur if the grammar contained the rule : 
ABE3. TRANSFOR MATION ALGRAMMAR
We now come to the main concern of this paper which is to discuss the extent to which the program we have been describing can be made to function as a transformational analyzer  . The main purpose of the examples that have been given is to show the great power of the program as a processor of symbol strings  . The notion of dominance is provided for , but only in a rudimentary way . It certainly could not be claimed that the program is a tree processor in any really workable sense  . But grammatical transformations are operations on trees and our investigation therefore must take the form of showing that these operations can frequently  , if not always , be mimicked by string rewriting rules . 
We shall take it that a transformational grammar consists of a contextfree or context-sensitive phrase-structure component and a set of transformations ordered in some way  . To begin with , very little will be lost if we assume that the transformational rules are simply ordered  . 
Gonsider now the first transformation in the list.
In general , this may be expected to introduce phrases into the P-markers to which it applies which could not have been generated by the phrase-structure component  . Let us now write some additional phrase-structure rules capable of generating these new phrases  . Let us insert these rules into the grammar immediately following the first transformational rule and establish the convention that  , when they ii are used in the analysis of the string  , their output will be used only as input to the first transformation  . Now treat the second transformational rule in the same way  . It also can be expected to create new kinds of phrase and phrase-structure rules can be written which would recognize these  . 
It may be that some of the phrases formed by the second rule could also be ~ formed by the first  , and in this case , it may be possible to move the appropriate rule from its position after the first transformation to a position after the second and to mark it as providing input only for these two rules  . 
Notice that the rules we are proposing to construct will not constitute what has sometimes been called a surface ~ rammar  . The phrases they describe certainly do not belong to the base structure and many of them may not be capable of surviving unchanged into the surface structure  . In general these rules describe phrases which can only have transititory existence somewhere in the generative process  . Notice also that in order to describe these phrases adequately it may sometimes be necessary to extend the notion of phrase structure grammar somewhat  . Consider for example the following transformation : 
X-A-B-Yi 234
Adjoin 2 as right daughter of 3
If we make the usual ass~ption that a rule is applied repeatedly until no proper analyses of the P-marker remain which can be matched by its structural index  , then this transformation , and many others , may produce phrases of indefinitely many types . Let us suppose that , before this transformation is applied for the first time  , all possible phrases that can be dominated by the symbol " B " are describable by contextfree phrase structure rules of the following form 
B - - ~ ?2 I ! ~ kture gramma ~ needed to describe all the phrases that can exsist after the operation of this transformation must co~tain the following rules  , or more accurately rule schemata
If .".

Where the asterisk indicates one or more repetitions of the symbol " A "  . If the left and right hand sides of these rules are reversed and they are presented to the program in the proper notation  , then the transformation itself can be represented by the following pair of rules : 
B(*$ANY . I*A .2) = 2B+I+B
B+B .1 + B = i
Since there are no facilities for specifying ~ dom -inance relations among elements on the righthand sides of these rules  , it is necessary to resort to subterfuge . The phrase dominated by the symbol " B" is reproduced in the output of this rule with copies of the symbol " A " removed from the right hand end and the remainder bounded by the symbols " B +" and ' tFB "  . These symbols serve to delimit a part of the string which can only figure in the complete analysis of the sentence if it constitutes a phrase of type " B '  . The second rule removes these boundry symbols from the phrase of type ' B " and  , since no pointer is assigned to them , they will leave no trace in the final P-marker .   , ~/ Another , and perhaps more economical , way to write recognition rules corresponding to this transformation involves conflating the additional phrase-structure rules with the reverse of the transformational rule itself to give rules of the following kind :=  . i * A . 2 = 2B + i + B(i ~ i < n ) 1
B+B . I+B = iter adjunct ! on that we are providing for here is more general than that often allowed by transformational grammarians  . It is common to require that if some element ! is adjoined as a daughter of an " other element b then b must have no daughters before the transformatiol l takes place  . 
Sister adjunct ! on can be treated in an analogous manner  . Consider the . following transformation :
X-A-B-Yi 234
Adjoin 2 as right sister of 4.
The phrases exsisting before this transformation is carried out  , and which have " B" as a constitutent , can be thought of as being described by a set of rules as follows : aI ~ B ~ I - - - ~ B  ~2   a2 ! ! a - - - ~ B ~ nn Here the a . are nonterminal symbols and the u . are 1?1 ? strings , possibly null . The grammar which descrlbes the phrases existing after the operation of this transformation must contain  , in addition , the following rules : al ~ B ~ iA * a2T-~B   ~2 A*!!ta----~B~A*nn~The reverse transformation itself can now be represented by a set of rules as follows: 
B . I ~.. iA *.2 ffi2B+I+Bl
Notice that the strings referred to by the symbols " X " and " Y " in both of the above examples are unchanged by the transformation and are therefore Experience shows that it is in fact rarely nec -essary to write separate rules for each =  . . In most cases , a transformation of this kind l could be " handled in the program with a rule of the following form : 
B . IANY . IA *.2 = 2B+i+B
This is one of a large number of cases in which it has been found that the analysis rules can be made more permissive than the original grammar suggests without introducing spurious structures and without seriously increasing the amount of time or space used by the program  . 
While it is possible that transformational analysis can be done in an interesting way with a program of this sort there seems to be little hope of finding an algorithm for writing analysis rules corresponding to a given transformational grammar  . 
The following rule also involves sister adjunction but poses much more serious problems than the previous example : 
X-A-Y-B-Zi 2345
Adjo in 2 a ~ right sister of 4
The problem here is that a variable " Y " intervenes between " A " and " B "  . On the face of it , the analysis rule corresponding to this transformation would have to be somewhat as follows : *$ ANY  . IB . 2* A . 3 --  3   1   2 And in principal the program could carry out a rule of thf skind  . However the first symbol on the lefthand side of this rule will match any string whatsoever  , so that , if the rule can be applied ~ tall , it can be applied in a prodigious number of ways . But , with real grammars , it usually turns out that quite a lot can be said about the pa~t of the sentence covered by the variable " Y " so that analysis rules = an be written which are sufficient lY specific to be practiable  . L~Deletions are notoriously troublesome in grammars of any kind because they can so easily give rise to cycles and undecidable problems  . Transformational gran~arians require that lexical items should only be deleted from a P-marker if there is some other copy of the same item which remains  . 
This condition insures what they call the recover -ability of the transformation  . However , it is very important to realize that recoverability  , in this sense is a very weak condition . The requirement is that , knowing that an item has been deleted from a certain position in the P-marker  , it should be possible to tell what that item was . But there is no requirement that a P-marker should contain evidence that it was derived by means of a deletion transformation or of the places in it where deletions might have taken place  . 
Deletions are more easy to cope with in certain situations than others  . Consider for example the following transformation : 
X-A-B-A-Yi 2345
Delete 4.
The recoverability requirement is satisfied because of the identity of the second and fourth elements in the structural index  . The corresponding rule for the program might be as follows:  23/22  , A . IB . 2 =  1   2   1 It is necessary to provide ordering information with a rule of this kind because it would otherwise be capable of operating on its own output and cycling indefinately  . But presumably this transformation can be carried out any number of times and the same therefore should be true of the corresponding analysis rule  . Once again , experience shows that the grammarian almost invariably knows more about the environment in which a deletion takes place than is stated in the rule  , and if this information is used carefully , analysis rules can be written which do not lead to cycles  . 
In principle the situation is even worse in rules of the following kind: 
I 23 45
Delete 4
Here the third element is a variable which can cover any number of nodes in the P-marker  . In analysis we are therefore not only without information about how many times the rule may have been applied but we know nothing about where to insert new copies of the symbol " A "  , except that they must be to the right of the existing copy  . 
The other commonly used elementary transformations  ( substitutions and Chomsky-adjunction ) do not present special problems . The main outstanding difficulty comes from the fact that trans-formational rules are ordered  . We have already said that the theory of transformational grammar is in the state of continual change and this is particularly true of the part that concerns the ordering of rules  . For this reason we have assumed that the rules are simply ordered in the hope that other possibilities will not be notably more difficult to deal with  . We shall also make the assumption that transformational rules are all obligatory  . 
Consider now the following grammar
Phrase structure i . S--~A(D ) BC2 . C - - ~ DE
Transformations i . A-B-X 1   2   3   0   2+1   3 and suppose that the program is required to analyze the string " ADBE "  . Since , in generation , the list of transformations is read from top to bottom it is reasonable to suppose that in analysis it should be read from bottom to top  . 

We may take it that the analysis rule corresponding to the second transformation is somewhat as follows: 
D . IB . 2 = 21
This , together with the two phrase-structure rules , is sufficient to give a complete analysis of the string with this underlying P-marker : But if this is an underlying P-marker  , the second transformational rule could not possibly be used to produce a derived structure from it because the first transformation  , which according to our assumption is obligator Y , can be applied to it giving the following result : It is in fact not sufficient to scan the list of transformations from bottom to top because this procedure does not make allowance for the fact that the transformations are obligatory  . To regard transformations as optional which were intended to be obligatory is J in general to associate spurious base structures to some sentences  . The solution for the present gr ~ , maris to use the following set of analysis rules :
I/0, BD2/1, D . IB . 2=2113/2, AB4/3, B'IA .  2 = 2 1
D .1 F.1 = c(1)
A . I$D . IB . IC . I = S(1)
The first and third rules contain , in effect , the structural indices from the second and first transformations respectively  . The first rule says that no string is acceptable as a sentence which contains " BD " as a substring because to this it  2  . The second rule reverses the effect of transformation  2  . The third rule , excludes any P-marker existing at this stage with a proper analysis containing " AB " as a substring  . This is the structural index of transformation i which therefore should have been applied to any P -marker containing it  . The fourth rule reverses the effect of transformation i and the remaining rules are the phrase-structure component of the grammar  . Once again it turns out that what may be necessary in theory is only rarely needed in practice  . Experience with t , his program is , so far , very limited but no cases have so far been found in which incorrect analysis have resulted from omitting rules such as those numbered one and three above  . 

It requires skill to write rules for analyzing natural sentences with the program described in this paper  . A program can only properly be call-ed a transformational parser if it can work directly with the unedited rules of the transformation-al grammar  . But no algorithm is known , nor is it likely that one will shortly be found , which will produce from a transformational grammar a set of corresponding rules of the kind required by this program  . It is not d~fficult to construct a transformational grammar for which no exactly corresponding set of analysis rules can be written  . However , other programs have been written which , though they are still in many ways imperfect , can more reasonably be called transformational parsers  . What then are the advantages of the present program ? The current version of the program is written in ALGOL and with very little regard for efficiency  . 
But the basic algorithm is inherently a very great deal more efficient than any of its competitors  . 
The various interpretations of an ambiguous sentence  , or a sentence which seems likely to be ambiguous in the early stages of analysis  , are all worked on simultaneously . At no stage can the program besa~d to be developing one interpretation of a sentence rather than another  . If two interpretations differ only in seme small part of the P-marker  , then only one complete P-marker part . Work done on the unambiguous portion is done only once for both interpretations  . 
The program , though undoubtably very powerful , seems naive from the point of view of modern linguistic theory  . The program embodies very little of what we know or believe to be true about the structure of natural languages  . It might well be said that a computer program for analyzing natural languages is only interesting to the extent that it makes a claim about the basic form of those languages  . But the program described here is intended as a tool and not as a linguistic hypothesis  . There is much to be learned about natural language from ruminating on the form of universal generative grammar and trading counterexample for example  . But there is also much to be learned from studying text as it actually occurs  . The small amount of work that has so far been done with this program has been sufficient to suggest strongly that a set of rules derived algorithmically from a transformational grammar is unlikely to be the most effective or the most revealing analytic device  . 

