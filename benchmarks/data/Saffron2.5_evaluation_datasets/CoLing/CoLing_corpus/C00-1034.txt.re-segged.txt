Theory Refinement and Natural Language Learning
Hervd Ddjean *
Seminar fiir S1) rach wissenschaft
Universit St Tiibingen
dejean ~ sfs.nphil,uni-l ; uebingen , de

This 1 ) aI ) erl ) resents a learning system for identifying synta ( : tie structures . This system relies on the use of backgromld knowledge an  ( 1 default wflues in order to buihl utl an initial grammar and the use of theory retiim lnent in order to iml  ) rove this granunar . 
This contbilmtion provides a good machine learning fl : amework for Natural Lmlguage Learning  . We illustrate this 1 ) oint with the 1 ) resentation of ALLiS , a learl fiug system which generates a regular ext ) res-siongrmnmar of nonrecursive phrases fi'om bracketed corpora  . 
1 Introduction
Apl ) lying Machine lx ~' arning t(' . chniques to Natural Language Processing is a booming domain of ru-s  ( :~ar ( ' h . One of the reasons is the .   ( levelopment of cor-1 ) or a with morl ) ho-synta ( : ti ( : and syntact i ( : mmota-tion ( Marcus et al ,  1993) , ( Sampson ,  1995) . One recent l ) opular sul ) task is the learning of non-re ( : ursive Nouns Phrases ( NP )   ( \] amshaw and Mart : us ,  1995) , ( ~\[' jong Kim Sangan (1 V censtra ,  1999) , ( Mufioz et al . , 1999) , ( Group ,  1998) , ( Cardie and Pierce ,  1999) , ( Buchholzel ; al . , 1999) . 
When other learning tc chui ( lues ( symbol i ( : or sta-tistical ) are widely used in Natural Language Bern'h-int , theory retlnelnent ( Aliecker and Schmid , 199G ) , ( Mooney ,  1993 ) seems to be ignored ( excel ) t ( Brunk and Pazzmfi ,  1995)) . Theory refinement consists of iml ) roving an existing knowledge base so that it ; better at : cords with data . No work using theory refinement apI ) lied to tile grammar learning paradigm seems to have been develol  ) ed . We would like to point out in this article the adequacy between theory refinement and Natural Language Learning  . 
To illustrate this claim , we present ALLiS ( Architecture for Learning Linguistic Structures )  , a learning system which uses theory refinement in order to learn non-reeursive noun phrases  ( also called base norm t ) hrases ) and nonrecursive verbal 1 ) hrases . We will show that this technique comliine ( 1 with the * This research is flmded be the TM ll . network L('m'ning Coml ) Ut ; ~ tional ( lrmn mars www . leg-www . uia . ac . be/lcg/use of default vahms provides a good architecture to learn natural anguage structures  . 
This article is organised as follows : Section 2 gives all overview of theory refinement . Section 3 CX l ) I ains the advantage of combining default vahles and theory refinement to build a learning system  . Section d des (: ril ) est h(' . genc'ral characteristics ( if ALLiS , and Sc(:tion5 explains the learning algorithm . The evaluation of ALLiS is described Section 6 . The examples which illustrate this arti ( : le corresl ) ond to English NPs . 
2 Theory Refinement
We 1 ) resent here al ) riefill tro ( hlction to theory re-finement . For a more ( tetailed presentation , we refer the reader to ( A becker and Schmid ,  1996) , ( Brunk , 1996) or ( Ourston and Mooney ,  1990) . ( Mooney , 1993) detines it ; as : Theory retinement systems develot ) edin Machine Learning automatically modify a Knowle  ( lgeBase to render it ; consiste . nt with a set of (: lassifie (1 training examples . 
This technique thus ( : on sists of trot ) roving a given Knowledge Base ( here a grammar ) on tilel/as is of examt Iles ( here a treebank )  . Someiml ) O Se to mod if ' y the initial knowledge 1 ) as e as little as possible . Applied in conjmmtion with existing learning techniques  ( Explanation-Based Learning , Inductive Logicl ) rogramming ) , TR seems to achieve ' better results than these techniques used alone  ( Mooney ,  1997) . Theory retinement is mainly used ( and has its origin ) in Knowledge Based Systems ( KBS )   ( Craw and Sleeman ,  1990) . It consists of two main steps : 1 . . Build a more or less correct grammar on the basis of background knowled  . qe . 
2 . Refine this grmnmar using training examt ) les: ( a ) Identify the revision 1 ) oints ( b ) Correct them The first ; step consists of acquiring an initial gram-nmr ( or more generally a knowledge base )  . In this work , the initial grain mar is automatically induce ( 1 step ( the refinement ) compares the prediction of the initial graln mar with the training corpus in order to firstly identify the revision point  . s , i . e . points that are not correctly described by the grammar  , and secondly , to correct these revision points . The error identification and refinement operations are explained Section  5  . 3 . 
The main difference between a TR , system and other symbolic learning systems is that a TR system must be able to revise existing rules given totile system as background knowledge  . ( A system such as TBL ( Brill ,  1993 ) cannot be considered us TR since it only acquires new rules  )  . In the case of other techniques , new rules are learned in order to improve the general efficiency of the system  ( selection of the " best rule " according to a preference function  ) and not in order to correct a specific rule . 
3 Theory Refinement , Default values and Natural Language L ~ arning This section explains how default values combined with theory refinement can provide a good in a chine learning framework for NLP  . 
3.1 The Use of Default Values
The use of default values is not new in NLP ( Brill ,  1993) , ( Vergne and Giguet ,  1998) . We can observe that often ( but not necessarily ) in a language , an element belongs to a predominant class ( Vergne and Giguet ,  1998) . Some systems such as stochastic models use this property implicitly  . Some others use it explicitly . For instance , the general principle of the Transformation-Based Learning  ( Brill ,  1993 ) is to assign to each element its most frequent category  , and then to learn transformation rules which correct its initial categorisation  . A second example is : the parser described ill ( Vergne and Giguet ,  1998) . 
They first assign to each granunatical word a default category  ( defaultag )  , and then might modify it thanks to local contexts and grammatical relation assignment  ( in order to deal with constraints due to long distance relations which cannot be expressed by local contexts  )  . 
The main work is done by the lexicon and by default values  ( even if further operations are ol ) viously necessary ) These approaches are thus different for the disambiguation often used in tagging  . The default rules are not numerous ( one per tag )  , easy to automatically generate but they nevertheless produce a satisfactory starting level  . 
3 . 2 The Combination of Default Values with TR The idea on which ALLiS relies is tile following : a first  ; " naive gramnmr " isl miltup using default values  , and then TR is used in order to provide a " more realistic graln mar "  . Tiffs initial grammar assigns to each element its default category  ( the algorithm is explained ill Section 5 . 2) . Tile rules learned are categorisation rules : assign a category to anclement  ( a tag or a word )  . Since an element is automatically assigned to its default category  , the system has not to learn the categorisation rules for its category  , and just learns categorisation rules which correspond to cases in which the element does not belong to its default category  . This minimised the number of rules that have to be learned  . Suppose the element c can belong to several categories  ( a frequent case )  . Tilefrst rule learned is tile " default " rule : assign  ( e , dc ) , where dc is the defimlt category of c . Then ALLiS just learns rules for cases where c does no  1  ) elong to its default category . The numerous rules concerning the default category are replaced by the simple default rule  . 
4 ALLiS
The goal of ALLiSx is to automatically build a regular expression grammar from a bracketed and tagged corpus  9  . In this training data , only the structures we want to learn are marked at their boundaries by square brackets a  . The following sentence shows an example of tile training corpus for the NP structure  ( only base NPs occur inside brackets )  . 
In/IN\[early/JJ trading/NN \] in/IN\[
Itong/NNP Kong/NNP\]\[Monday/NNP\] , / , \[ gold/NN\]was/VBD quoted/VBN at/IN\[$/$ 366  . 50/CD\]\[an/DTounce/NN\] . / . 
ALl , iS uses an internal formalism in order to represent hegrm mna rules  . In order to parse a text , a module converts its formalism into a regular expression grammar which can be used by a parser using such representation  ( two modules exist : one for the CASS parser ( Almey , 1996) and one for XFST ( Karttunen et al ,  1997)) . 
Following the principle of theory refinement , he learning task is composed of two steps . The first step is the generation of the initial grammar  . This grammar is generated ti'om examples and baek grom x d knowledge  ( Section 5 )  . This initial grammar provides an incomplete and /or incorrect analysis of the data  . The second step is the refinement of this grammar  ( Section 5 . 3) . During this step , the validity of the gramma rules is checked and the rules are improved  ( refined ) if necessary . This improvement corresponds to find contexts in which elements which i http://www  . sfb 441 , uni-tuebing en , de /' dejean/chunker , html . 
2'\]'he WSJ corpus ( Marcus et al , 1993).
a ( Mufioz et al ,  1999 ) showed that this representation tends to provide better results than the representation used in  ( Ramshaw and Marcus ,  1995 ) where each word is tagged with a tag I ( inside )  , O ( outskte ) , or B ( breaker ) . 
230 are considered to be meinbers ( f the structure do hoe1elong to this stru:ture ( and re ( :itr ) c ~ dly )  . 
We give here a simple exami ) leto illustrate tit (  . * learning process . The first step ( initial grammar generation ) categorises the tag JJ ( adje:tive ) as belonging by default to the NP strlleture if it  ; oec : irs before a noun . Tit ; second stet ) ( refinemellt ) tinds ou I ; that some adjectives do not oley to these rules4 . 
Tim refiimment is triggered in order tmodify the default rule so that these ex'el  ) tions cante: ( rre ( : tly processed . 
Thus , the learning algorithm simlly consists of eategorising the elements of the eorlms  ( tags and words ) in to st celtic categories , and this eategorisa-lion allows the extraction of the stru : i  ; ures we want to learn . These ( : a tegories are exi ) lained in the next section . 
5 The Learning System 5 . 1 The Background Knowledge . 
\] norder to ease the learning , the system uses 1) aek-ground knowledge . This knowledge , 1 rovide , sa for-real and general ( les ( ' ription of the S\[ ; lll : tllr ) s that ALLiS can learn . WeSUl ) l ) ose that the stru:tures are ( : Oml ( se ( lo fammle us with ( t ) timal left ; an (1 right ad . \] un:ts . We here give in t'rmal(et in it in s , h(' , for-inal/disl ; rilmtimal ones are given in Section 5 . 2 . 
Tit (; nucleus is the head of the structure . . We authorise the presence of several nuclei in the  . stone strll ; l ; ure . 
All the other e . lements in the st ; ruet ; nre ( except the linker ) me ( : onsidered as adjuncts . They are independence relation with the , head of tit!st:rue-tm'e . Tit (; adjuncts are (' , hara (: l ; eris( ; I 1yl ; heir \]) osi-tion(left/right)re , lative to the nu (: leus . 
A linker is a Sl)e:ialelenle , nt ; which 1 uihls a nen-lo:entri:st ; ruetlne with t ; w ( elentenl ; s . It usually ; ) rreston(tst;oc(or(lilla;i)n 5 . 
A neh Bment ( micleus or adjunct ) might possess the break 1 ) rI ) erty . This llotiol tisiu ; rodtl ce(( ; ISill ( laxnshaw and Marcus ,  19\[)5) , ( Mufioz et al ,  1999 ) ) in order to deal with sequences where adjacent m ~ eleicompose several structures  ( Section 5 . 2 . 4) . 
This 1 attern can 1 ) e seen as a variant of the X-l ) artemplate ( I lead , Spec and COral )) , whieh was already used in a learning sysl ; ent(Berwiek ,  1 . 985 )   ( alth ( mgh Coral ) is not useflfl for the nm-reeursive structures )  . 
The possible ditferent categories of air element are summm'ised in Figure  5  . 1 . 
The tbllowing sentence shows an exmnlle of categorisation  ( elements which do not all ) ear in the structure ( NP ) are tagged O ) : 4For examlle : \[ the/l ) T 7/el ) %/ NN ( mchmarl:/NN is-sue/NN\]due/aa\[October/NNP l . q , qg/Cl)\] . 
5Only linkers occurring between the two coordinated eh!-I l l  ! l l \ [  , Sal ' , ~ i ) l'o c(! , qs~(\] , 
CAT /%
OUTIN
Al/r NU+/-B+/-B
Figure 1: The different categories.
Ill 0 earl YAB + trading NU in 0 Iong NUKOng NUM on day NuB + , 0 goldNu was0 quoted0 at0$A3 66 . 50 NU all AB+Ollllee Nu -0 Thestru ( :ture is fornmlly defined as : S+A*\[A*NUb_A *\] +* l  , b+1 , 1 > r , b-At , b+\]A*k*A **" l , bq1 , 1-NUb+r , b-Ar , b + 2 File symlol , : or resl ) onds to the Kleene star . For the sake of legitility , we do not introduce linkers in this expression . But each symbol X(NU , A ) cml1)ede , tined 1y the rule ; X --~- XIX 1X where l is the list ; of linkers . The symbols B + and \] Y-indicate whether the element has the  . breaker i ) rot ) erty or n(t . 
Since lit ( ; : or pus does not contain information about these distrilmtional categories  , ALLiS has to figure the mut . This : a tegorisation relies on the distritmtional behaviour of the ele  . nlents , and e an be automati:ally achieved . 
5.2 The Initial Categorisation
The general ideat : a teg ( rise elements is t ( uses le-:iti: ( : ont : xts which 1 ) ointuts ( nie of the distrilm-l ; iona \] t)r(perti:s of lhe( ; ategory . ~\] Plle ; al ; eg()r is af , ioll is a sequential 1) ro ; ess . First ; themmM have to be found out . 14) reachtag ( f the:(r l ) uS , we ai ) lly the fim ' timJ'm~(described bekw) . This flmctions e\]e ( ' tsafist of elements w\]fich are eateg ( rised as mmIei . The fun : tionft ) is alp\]ied to this list in 0> der to tigure outmmlei which a , ' ebreakers . Then the a(ljmt cts are , found out , and the function fb is also apl\]iedl ; ( them to figure outbreakers . 
5.2.1 Categorisation of Nuclei
The ( ; Oll text used to find out the nuclei relies ott this simlle following ol  ) servation : A structure requires at least ; menu cleusr . Thus , the elements tha . t ; occur alone in a structure are assimilated to nucleus  , since a structure requires a nucleus . For example , the tags PRP ( pronouns ) and NNP ( proi ) ernora:s ) mayeom psea . lone a structure ,   ( respectively 99% 6 The regular exl ) ression lbrmalism does not allow such rule , and then , this recursion is simulated . 
7 The partial structures ( structures without mt cleus ) tel > re-sent2 % of all the structures it : the , training corl ) us , and I , h(m introduce little noise . 
231 and 48% of these tags appear alone in NP ) but the tag JJ appears alone only 0 . 009% . We deduce that PRP and NNP belong to the nuclei and not JJ  . But this criterion does not allow tile identification of all the nuclei  . Some often appear with adjuncts ( an English noun ( NN ) oftens occurs with a detern finer or an adjective and thus appears alone only  13%  )  . The single use of this characteristic provides a continuum of values where the automatic setup of a threshold between adjuncts and nuclei is probleinatic and depends on the structure  . To solve this problem , the idea is to decompose the categorisation of nuclei in two steps  . First we identify characteristic adjuncts . 
The adjuncts cannot appear alone since they depend on a nucleus  . The function fchar is built so that it provides a very small value for adjuncts  . If the value of an element is lower than a given threshold  ( 0 char = 0 . 05) , then it is categorised as a characteristic adjunct  . 
= ~ cx ~ cP corresi ) onds to the number of occurrences of the pattern P in the corpus C  . For exmnple the number of occurrences in the training corpus of the pattern \[ JJ\] is  99: and the number of occurrences of the pattern JJ ( including the pattern \[ JJ\] ) is 11097 . 
Solobar(JJ ) = 0 . 009 , value being low enough to consider JJ as a characteristic adjunct  . The list pro-vi ( le ( 1 byfchar for English NP is : A char = DT , P1 H~$ , POS ,   . l . l , . I . IR , JJS , " " , " " These elements can correspond to left ; or right adjuncts . All the adjuncts are not identified , buttiffs list allows the identification of the nuclei as explained in the next paragrapl L The second step consists of introducing these elements into a new pattern used by the funetiol l fro  ,  . 
This pattern matches element surrounded by these characteristic adjuncts  . It ; thus matches nuclei which often appear with adjuncts  . Since a sequence of adjuncts ( as an adjunct alone ) cannot alone compose a complete structure , X only matches elements which correspond to a nucleus  . 
= EcAche , .* xAl
The flmction fnu is a good discrimination function between nuclei and adjuncts and provides very low values for adjuncts and very high values for nuclei  ( table 1 )  . 
5 . 2  . 2 Categor i sa t ion o f Ad juncts Once the nuclei are identified  , we can easily find out tlm adjuncts . They correspond to all the other ele-inents which appear in the context : There are two 
SAt least , in the training corpus.
xFreq(x ) fml(X ) int cleus
POS 175 90.00 no
PRP $1876 0.01 no
JJ 11097 0.02 no
DT 1809 70.03 no
RB 91 90.06 no
NNP 1104 60.7 4 yes
NN 21240 0.87 yes
NNPS 164 0.93 yes
NNS 777 40.95 yes
WP 52 70.97 yes
PRP 3800 0.99 yes
Table 1: Detection of some nuclei of the English NP ( nouns and pronouns )  . 
kinds of adjuncts : the left and the right adjuncts  . 
The contexts used are :\[_NU\]f brtile left , adjuncts\[AI*NU_\] for the right adjuncts If an element appears attile position of the underscore  , it is categorised as adjunct . Once tile left adjuncts are found out , they can be used for the categorisation of the right adjuncts  . They thus appear in the context as optional dements  ( this is help fifl to capture circumpositions )  . 
Since tilt ; Adjective Phrase occurring inside an NP is not marked in the Upenn treebank  , we introduce the class of all ' inner of adjunct . The contexts used to find out the a cljuncts of the left adjunct are : \[_A  1 NU \] for the left adjuncts of A 1 \[ al * A 1 _NU \] for the right adjuncts of A 1 The contexts are similar for adjuncts of right a ( 1-juncts . 
5.2.3 The Linkers
By definition , a linker commcts two elements , and appears between them . The contexts used to find linkers are : \[ NU_ NU \] linker of nuclei\[A_ANU \] linker of left adjuncts\[NUA_A\]linker of right adjuncts Elements which occur in these contexts trot which have already been categorised as nucleus or adjuncts are deleted from the list  . 
5.2.4 The Break Property
A sequence of several nuclei ( and the adjuncts which depend on them ) can 1 ) elong to amfique structure or compose several ad . iacent structures . An element is a breaker if its presence introduces a break into a sequence of adjacent nuclei  . For example , the pres-once of the tag DT in the sequence NNDT JJNN introduces a break before the tag DT  , although the single structure in the training corlms  . 
.  .   . \[ the/DT coming/VB ? week/NN\]\[the/l ) T foreign/JJ exchange/NN mar-ket/NN\] .   .   . 
The tag DT introduces a 1) reak on its lefl; , but some tags can introduce a break on thoir right or on their left  , and right . For instance , the tag WDT ( NU by defmflt ) introduces a bre , akon its M't and on its right . In other words , this tag cannot belong to the same structure as the preceding adjacent nucleus and to the same structure as the following adjacent 

.  .   . \[ raih'oads/NNS and/CC trucking/NN companies/NNS \]\[ that/WDT \] tw  , -ga , UW ~ Diu/ll N\[19S0/CD\] . . . 
. . . i , l/l:N\[which/WDT\]\[peop >/ ~ , ' NS\]generally/I/\]larc/V'l TP .   .   . 
lit order to detect wl fichtag has thet ) re , ak1) rot ) erl ; y , we build up two fimctions ' f b h ; fl ; and f bright " . l't,i , ; n(x ) = ~ . Nux " W\])?f b , . i ~ O , t(x)-~'x\]\[N ,  )~- , c'XUNU:'nuch' . i ? wb(;wb:corpuswiLhoutl)ra(:kcl , s These funcl ; ions are used to coniput (; l ; he , break 1) rop-erl ; y for nuclei , but ; also for adjull cts(\]\]i . hi . q case , the pattern X is conll ) leted ) ya(hling the , elemeut NU to the left ; or 1 ; otheright of X ( tim 1 ) ot ( mtial adjunct ) according 1 ; o the kind of adjull (: t ( left or right adjunct )) . The table 2 shows some vahles for some tags . Anolon)e Ill ; Call\])ea left brcakem(DT ) , a right breaker ( no example fi ) r English NI ? at the tag level )  , or both ( PRP ) . The break property is generally well-markc'd and the thre~shold is easy to set up  ( 0 . 66 in practice ) . 
TAG fbh;\[I ; fbright_1) 310 . 97 ( yes ) O . O0(no)pR > 0 . 97 ( yes ) O . OS(yes)
POS 0 . 95 ( yes ) 0 . O0(no)l'lU'$0 . 94 ( yes ) 0 . 00(no)
J , l 0.44 ( no)O . O0(no)
NN 0.04 ( no)O.\].l.(no)
NNS 0.03( , , o ) 0.14 ( no)
Table 2: Breaker determin a . tion . Values of the flmc-tions ot'b left and fb- right for so ille elements  . 
In the refinement step ( Se(:tion 5 . 3), the breaker property can be extended to words . Thus , the word yesterday is considered as a right breaker  , although its tag(NN ) is not . 
5 . 3 The Refinement Step 5 . 3 . 1 The not ion o f re l iab i l i ty Tile preceding function sident  , i(y the category of D , II elenmnt when it occurs in the str'ltctltT'e . \] lilt , fill ele-lnent can occur in the structure as well as out  ; of the structure . For example , the tag VBG is only considered as adjunct when it occurs in the structure  . Nevertheless , it mainly occurs out of the structure ( 84% of its occurrences )  . If an element mainly 9 occurs out of the st , ructure , it is considered as non-reliable and its default category is OUT  . For each dement occurring inside the structure , its rdiable is tested . The initial grammar corresponds to the grammar which only contains the reliable elements  . Its precision and its recall area romM 86% . 
It ow is determined the reliability of an element ? This notion of reliabh  ; element is contextual and depends on th ( ! category of the element . 
For the uuclei , the context is eml)ty . W'ejust com-tmtel ; heratio ) el ; ween l ; hc number of occurrences in the strll ( ; tur ( over th ( llllllber of occtlr relices occurring outside of the sl  , ructure . 
For the adjmicts , the context in ( -ludes an adjacent nuc:leus ( on the right for left adjun ( tts or on the h ~ f t for right a ( ljun ( -l ; s ) . For instance , the tag . lJ is categorise ( lasleft a@mct for the English NP . It ap-1) ears 9617 times 1) e forealm (: leus and 9 , 189 times in the stru(-ture . It is thus (: on si(lered as relial ) le , and its default category is left adjmmt . In the case where the tag . \] J occurs without mmleuson its righl ; ( apr ( , dicative us ( , ,) , it is not considered as adjunct mM this kind of occn rr  ( mcesix not used to deter-niinether clial fility of the element  . Onl ; he coutrary , the tag VIIG appears 468 times before ammle us , but , in this context , it occurs only 138 times in the structure . This is not enough (29%) to consider t . he element as a relial ) leleft ; adjunct , and thus il ; sde-5mltca ( ; egoi'y is OUT . For the adjunct of adjunct , the conl ; ext includes a ( tjm/ctandim cleus . 
5. a . 2 Detection of errors
Once the inil ; i Mglmn marix built ufh its errors have I ; o1)e corrected . The detection of the errors corresponds to a ln is categorisation fatag  . Anaut ; omatic error done by the initial grammar is to wrongly analyse the structures coml  ) osed with non-reliable elements ( false negative examples )  . Each time that a non-reliable elenmnt occurs in the structure corm-spends to an error  . For instance , the initial grammar cmi not correctly recognise the following sequence as an NP  , the default category of the tag VBG being
OUT ( outside the structure): .   .   . \[ the/1) Teoming/VBG week/NN\] .   .   . 
~') ' l ' he threshold used is of 50%.

The second kind of errors corresponds to sequences wrongly recognised as structures  ( false positive exmnples )  . This kind of error is generated by reliable elements which exceptionally do not occur ill the structure  . In the following example , or de ~ NN occurs outside of the structure , although the default category of the tag NN is NU  ( nucleus )  , and thus the initial grammare cognises an NP . 
.. . in/IN order/NN to/TO1,ay/VB ...
5.3.3 Correction
In both kinds of errors , the stone technique is used to correct them . I~r this purpose , ALLiS disposes of two operators , thee on textualisation ad the lexicalisation . 
The contcxtualisation consists of finding out contexts in order to fix the errors  . The idea is to add constraints for recategorising non-reliable lements as reliable m  . Thet ) resence of some specific elements can completely change the behaviour of a tag  . The table 3 shows the list of contexts where the tag VBN is not categorised as OUT but as leftAdjmmt  . 
PRP $ VBGNU
IN\[VBGNU
DT VBG NU aJ VBG NU
POS VBG NU
VBG\[VBGNU
Table 3: Some contexts where the non-reliable le-ment VBN becomes reliable  . 
For each tag occurring in the , structure , all tile possible contexts 11 are generated . For the non-reliable tags ( first kind of error ) , we evaluate the reliability of them contextually , and we delete the contexts in which the tag is still non-reliable  ( tile list of contexts can be empty , and in this case the error cannot be fixed ) . For the reliable tags ( second kind of error ) , we keel ) the contexts in which the tag is categorised OUT . 
The lcxicalisation consists of introducing lexical information : the word level  . Some words can have a specific behaviour which does not appear at the Part-Of-Speech  ( POS ) level . For instance , the word yesterday is a left breaker , behaviour which cannot be figured out at the POS level  ( Table 4 )  . The introduction of the lexicalisation improves the result by  2%   ( Section 6 )  . 
The lexicalisation and the contextualisation can be combined when both separately a  . renot power flf lenough to fix the error . For example , the word about tagged RB ( default category of RB:OUT ) followed l?q'he same techifique is used in ( Sima ' an ,  1997) . 
H The contexts depend on the category of the tag , but are just composed of one element . 
word(context ) default cat . new cat.
about/RB(_Cl )) OUTA1 , B+order ( IN_TO ) NUOU Tyester day/NN NUB-NUB + operating/VBG OUT  A1  , B_last / . JJA1 , B-A1 , B+Table 4: Some specific lexical behaviours . 
by the tag CD is recategorised as left : adjunct and left breaker  ( Table 4 )  . 
6 Evaluation
We now show some results and give some comparisons with other works  ( Table 5 )  . The results are , quites in filar to other approaches . Two rates are measured : precision an recall . 
~, ~ Nu ~ nbcr of correct proposed patt c'rns
Number of correct patterns pzNumbero . fcorrect proposed patterns
Nu ' mber of proposed patt crns
Tile training data . are comt ) osed of the sections 1518 of tile Wall Street Journal Corpus ( Marcusel ; al . , 1993) , and we use the section 20 for the test corpus 12 . The data is tagged with the Brill tagger . The works generating syinbolic rules like ALLiS are  ( Rainshaw and Marcus ,  1 . 995 )   ( Transformation-Based learning ) and ( Cardie and Pierce ,  1 . 998) ( error-driven pruning of treebank grammars ) . AL-LiS provides better results than them . ( Argamon et al , 1 . 998) use a MemoryBased Shallow Learning system , ( Tjong Kim Sang and Veenstra ,  1999 ) the MemoryBased Learning nm tho ( l and ( Mufiozel ; at . , 1999) uses a network of linear functions . The latter work seems to integrate better lexical intbrmation since ALLiS gets better results with POS only  . 
POS only with words
NP MPRZ 99 90.3/9 0.99 2.40/93.10
ALLiS9t .0/91 .292 .56/92 .36
TV 999 2.50/92.25
ADK98 91.6/9 1.6
RM95 90.7/90.5 92.3/91.8
CP98 91.1/90.7
VPALLiS 91.39/90.52 92.15/91.95
Table 5: Results for NP and VP structures ( preci-sion/recall )  . 
The main errors done by ALLiS are due to errors of tagging  ( the corpus is tagged with the Brill tagger ) or errors in the bracketing of the training cortms  . Then , the second type of errors concerns 12This dataset ; is avMlable viaft p://ftp , c is . upenn , edu/pub/chunker / . 
234 tim(:o()rdin ; dx:(1sl , ru (: tures . ' l'h ( ; s(~twotyl )( ; s(l'l'Ol'S correspond to 51% of the , over all errors . We (: antin(l1 ; 11 (: sam ( ; l ; yl)olop ; y in other works ( \] anl shaw : rodMarcus ,  1995) , ( Cardi (: and Pierc (: ,  1998) . \ Ve did some tries in order to manually imt ) r ( ) v ( ' , th(;final , grammar : I ) utl ; lm only tyt /( ; ( ) f errors whi (: h can 1) emm nmll yiml ) r()v ( ; (1(:(m (: ( wnstimt)r()t)h ; m of the ( tuo-la I ; iontamks ( th ( ~ inll ) rov(mw . nlisal ) ( ) ul()l'0 . 2% i ~ ll ) re(:isionmid recall ) . 
Error types ~ j /- % tagging/bracketing errors 57   28  . 5% coordination d 52 2 . 5% germM1 57 . 5% adv(wl ) 136 . 5% ai ) l ) ositiv(:s136 . 5% ( lUOi ; al ; ionl ; lrks~1) ml (: tualion\]05% past ; 1) art Ml ) le 9: 1 . 5% that ( IN ) fi3%Tal)h' . 6: Typology of the 200 tirst errors . 

An(h'easAb(;ck(;r and Klaus Schmid .  :1996 . From tlmory l ' ( :filmm ( mt1okl ) lIlliIl ( (~llll lC ( ~:  ; /I ) ( ) sition : - d ; a , l ; (: ln ( ; lll; . \]11U6'Al'96, Budat )(> l ;,\] hmgary . 
Steven Almey .  1996 . \]) artial tmrsing via tinil ; (> stal:e(:as(:ades . Inl ) roccedings of the E?'SLLI'95 ll , obust l ) arsing Workshop . 
Shlom () Argmnon , Mo \]) agan , and Yuval Krymolowski .  1998 . Am ( :mory-1 ) as ( ~ ( 1 at ) l ) r ( )a ( : h1 ( ) learning s\]mllow natural \[ an~t~uag ( ! i ) al . l([rns . \]1l
COL1NG'98, Montrdal.
Robert C . 13 erwick .  1985 . Th , c acquisition of syntactic k ' now lcdg (' . . M\[T press , Cmnbri(lg(' . .
Eric Brill .  1993 . A Corpv , s-BascdAp\]nvach1 , o Language Learning . Ph . l ) . thesis , 1) el ) art nm nl ; of Computer and hfformation Science , University of \]' en nsylvania . 
Clifl'ord Alan Brunk and Michael Pazzmii . 1995.
A lexically based semantics bias for theory rex'i- : don  . In Morganl ( aufl'man , e(litor , 7' welf lhl nl , cr-national Co't@rc'nc (: on Mach . in eLearning , pages 81-89 . 
Cliflbrd Alto 1 Brtmk .  1996 . An invest , igation of Knowlc@ch ~ , tensiv c Appwach , cs to Concept Learning and Theory Refinement . Ph . \]) . thesis,
University of Caliibrnia , irvine.
Sabine B , u : h holz , \] or n Veenstxa , and \\ qdter 1) aele-marts .  1999 . Cascaded grammatical relal ; ion as-tdgmnenl; . Inl'T vcccdings of l ~ MNL1)/VLC-99 , pages I ) P-2392 . "16, University of Maryland , USA . 
Claire , Car(liemtd1) avid Pierce .  1998 . Error-driven pruning ( ) ftr ( ; ellank gl'ml mlars for basen(nlnphras ( ; identiti ( ; ation . Inl'T vccedings of th , e . IT th , International Co'l@rc , ncconComp ' , ,tational Lin-g'aistics ( COLING-A CL '98) . 
Claire , Cardie and David Pierce .  1999 . The . rol ( ~ of lexicalization an ( pruning for base noun 1 ) n ' ase grammars . \] nl ) ~ vccc : dings of I , he Sizl , c . cnth , Na-l , ional Confl:renc (: on Artificial Intelligence . 
Susan Craw and I) . Sleenmn .  1990 . Automating the r(~lineln(!nl ; of l ~ now ledge-based syst ; cn ls . \]nPro-cecdings of the ?'( MI'90Cm@~v' , ,cc : pages 167 172 . 
The XTACl . esear (: h Group .  1998 . A lexicalized tree adjoining grammar fbrenglish . Technical Reporti \] CS 98-18 , University of Pemlsylvania . 
Lauri Karl ; I ; un(~ll , ~l ' am  ~ isGal , and Andrd K (! Illp ( ~ ,  . 
1997 . Xeroxtinite-state tool . Technical report , Xerox \] . es(!archCentr(~Eurol)e , Grenobl(,, . 
Mitchell Marcus , Bdatric (' Sanl ; orilfi , and Marc Ami Marcinkie , wicz .  1993 . \] hill ( ling a large annotated corpus of english : the I ) e1111 tree l ) ank . Comp'nta-tional Linguistics , 19(2):31330 . 
Raymond J . Mooney .  1( . )93 . In ( hlction over the l111-exl ) lained : Using overly-general domain theories ( o aid ( : on ( : (  ; p(learning . Mach , i ',, r ~ Lc:arni ', , g , 10:79 . 
llas , mond . \] . Mooney .  1997 . Induc ( ; ivelogic programming form d ; urallanguag ( ; processing . In Sizl , hlnt cvnal , i on all nd ' u , ct , ive Logic : Programming Wovl , : shop , pages 20522 d , Sl;o(:ldlohn , Swedcn . 
Mar(:i ; t Mufioz , Vasill ' unyal ( anol % ) 2'1 11i/()th , and l ) avZimak .  1999 . A learning approach to shallow lmrsing , liltt'~vceedings of EMNLI)-WVLC'99 . 
l ) irk ( ) m's t(m and \] aymond Mooney .  1990 . Changing the rules : A(:(mq ) rt ~ hensiv ( , , al)prtmcli to th(' . or yl'(tfillOlll(tlll . . \] ll l'roccc:dings of t/w ls'igM , National Co'l@re'm:r , o'nArl , ~ ific:ial\[nl , elligr:nce , 1) ag(!s815820 . 
l ~ anceA . llmnshaw and Mitchell P . Marcus .  1995 . 
Text ; chunking using trmlst brmation-based larn-ing . In A67 , Third Worksh , op on Very Lmyle Car-pora , pages 8294 . 
Geotti'ey Sampson .  1995 . English , for the Comp ' ut cr . 
The SUSANNEU orpus and Analytic , 5'ch , cme.
Oxford : Clarend on Press.
Khalil Sima'an .  19)7 . Exl ) lanal ; ion-based learning of ( lal ; a oriented parsing . Ill T . Mark Ellison , editor , Uompul , ational Nal , ural Lwng ' ung cLearning ( CoNLL ) , ACL/EACL-9~ , Madrid . 
Erik Tjong Kim Sang and . Jonl Veenstra . 1999.
Rel ) resenting text chunks . In Proceedings of EACL'99 , Association for Computational Linguistics , Bergen . 
Jacques Vergn(~mid Enunan : le , l Giguet .  1998 . Re-gardsthdoriqu (', surle " tagging " . In proceedings of 7'aitemc'nt Autom cztiquc des Langucs Na-tu~vlh ' . s(TALNI998), 1) aris . 

