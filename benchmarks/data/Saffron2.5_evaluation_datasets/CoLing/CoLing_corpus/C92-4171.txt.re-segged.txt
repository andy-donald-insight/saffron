I ) YNAMICS , DEPENI ) ENCYGRAMMARAND INCREMENT ALINTERPRETATION *
DAVID MILWARI)
Centre for Cognitive Science , University of Edinburgh
2 , lhn : cleuch Place , Edinburgh , ElI89LW , Scotland
davidmC ~ cogsci.ed,ac.uk
Abstract
The paper describes two equiwtlent grammatical for -malisnLs  . The first is a lexicalised version of dependency grammar  , and this can bensed to provide tree-structured analyses of sentences  ( though somewhat t latter than those usually provided by phra  . sestructure grammars ) . The second is a new for rnalism , ' Dynamic Dependency Gramniar ' , which uses axioms and deduction rules to provide analyses of sentences in terms of transitioos between states  . 
A reformulation of dependency grammar usiug state transitions is of interest on several grounds  . 
Firstly , it can be used to show that incremental interpretation is possible without requiring notions of overlapping  , or flexible constituency ( as ill some versions of categorial grammar )  , and without destroying at rasm parent link between syntax and semantics  . Secondly , the reformulation provides a level of description which can act as an intermediate stage between the original grammar and a parsing algorithm  . Thirdly , it is possible to extend the relbrnmlated grammars with further axioii~s and deduction rules to provide coverage of syntactic on structions such as coortlination which are tlitficult to encode lexically  . 
1 Dynamics
Dynantics can roughly be described ms the study ( ff systems which consist of a set of states ( cognitive , physical etc . ) and a family of binary lmnsil~on re-lalionships between states  , corresponding to actions which can be performed to change from one state to another  ( van Benthem ,  1990) . 
This paper introduces a notion of dynamicy ram..
Slat , where each word illa sentence is treated a ~ san action which ha  . s the potential to produce a change in state , and each state encodes ( in some form ) the syntactic or semantic dependencies of the words which lave been absorbed so far  . There is no requirement for tile number of states to he finite  . ( ln fact , since dependency grammar allows centrem bedding of arbitrary depth  , the corresponding dynamic grammar provides an unlimited number of states  )  . 
Dynamic grammars are specified using very simple logics  , and a sentence is accepted , ~sgrammatical if and only if there is some proof that it perforn~s a transition between some suitable initial and final * This resen  . rchw ~ . snUplmrted by ml SERC research fellowship . 
states , It is worth noting at this early stage that dynamic grammars are not lexicalised rehashes of Augmented Transition Networks  ( Woods ,  1973) . A'I'Ns use a finite number of states combined with a recursion mechanism  , and actea ' ~ entially in the same wayms a top down parser  . They are not particularly suited to increment , 'd interpretation . 
To get an idea of how logics ( instead of the more usual algebra . s ) can be used to specify dynamic sys-ten Ls in general  , it is worth considering a reformulation of the bllowing finite state machine  ( FSM ) : h0 l2:1 This accepts . ' ~ grammatical nystring which maps from the initial state  ,  0 , to the final state , 3 ( i . c . 
strings of the form : nb * eb ) . The FSM cart be reformulated using a logic where the notation  , 
State OStr State l is used to state that the string  , Str , perforn matransition from State OtoStatel . The axioms ( or atomic proofs ) in tile logic are provided by the transitions peribrnted by the individual letters  . Thus the following are mssumed , t 0 " a " t 1 " b " I 2 " b " 3   1 " c " 2 The transitions given by the single letter strings are put together using a deduction rule  , Sequencing , 2 which states that , provided there is a proof that String , takes us from some state , So , to a state St and a proof that String b takes us from St to  $2  , then there is a proof that the concatenation of the string stakes us from  S0 to $2  . The rule puts to a ether strings of letters if the final state reached by tile first string matches an initial state for tilese contl string  . For example , the rule may be instantiated as : A string is grammatical ccording to the logic if and only if it is possible to construct a proof of the statement  0 Str 3 using the axioms and the Sequencing Rule . For example , the string " abbcb " performs the transitions , l " a " is a ~ trin ~ coxudsting of the single letter  , a . 
2Notation : capital ettet ~ will be used to denote variables throughout this paper  . ' a ' will be used to denote on catena ~ tion . For example , if String a = " kl " madString b = " at ilt " , then StrlngaeStrlngb = ukltlllll . 
ACRESDEC ( ) LING . 92, NAN-rES . 2328 AOt'rr 1992l095l ) Roe , ov COLING-92 , NA ~ I'ESAUG .  2328 , 1992 and has the following proof , amongst others : 1 " b " l 1 " b " 1   1 " bb " 1   1 " c " 2   0 " a " 1   1 " bbc " 2   0 " abbc " 2   2 " b " 3   0 " abbcb " 3 Each leaf of the tree is an axiom , and the subproofs are put together using instantiations of the Sequencing Rule  . 
2 Lex iea l i sed Dependency Grammar " lYaditional dependency grammar is not concerned with constituent structure  , but with links between individual words . For example , an analysis of the sentence John thought Mary showed Ben~oSue might be represented as follows: 
John thought Mary showed Bento Sue
The word thought is the head of the whole sentence  , and it has two dependents , John and showed , showed is the head of the embedded sentence , with three dependents , Mary , Benand to . A dependency graph is said to respect adjacency if no word is separated from its head except by its own dependents  , or by another dependent of the same head and its dependents  ( i . e . 
there are no crossed links ) . Adjacency is a reasonably standard restriction , and has been proposed as a universal principle e . g . by Hudson (1988) . 
Given adjacency , it is possible to extract bracketed strings ( midhence a notion of constituent structure ) by grouping together each head with its dependents  ( and the dependents of its dependents )  . For example , the sentence above gets the bracketing : \[ John thought\[Mary showed Ben\[toSue\]\]\]A noun phrase can be thought of as a noun plus all its dependents  , a sentence as a verb plus all its dependents . 
In this paper we will assume adjacency , and , for simplicity , that dependents are fixed in their order relative to the head and to each other  . Dependency grammars adopting these assumptions were formalised by Gaifman  ( Hays ,  1964) . Lexicalisation is relatively trivial given this formalisation  , and the work on embedde dependency grammar within categorial grammar  ( Barry and Picketing ,  1990) . 
Lexiealised Dependency Grammar ( LDG ) treats each head as a flmction . For example , the headlhought is treated as a function with two arguments  , a noun phrase and a sentence . Constituents are formed by combining functions with all their arguments  . The example above gets the following bracketing and tree structure :\[ John thought\[Mary showed Ben\[toSue\]\]\]s  .   . . . . . I ~'-~ llnp ) //
L ~ ls ~ J , il npl\
Lr(np , pp)
PP1 np 10

The tree structure is particularly flat , with all arguments of a function appearing at tile same level  ( this contrasts with standard phrase structure analyses where the subject of showed would appear at a different level from its objects  )  . 
Lexical categories are feature structures with three main features  , a base type ( the type of the constituent formed by combining the lexical item with its arguments  )  , a list of arguments which must appear to the left  , and a list of arguments which must appear to the right  . The arguments at the top of the lists must appear closes to the functor  . For example , showed has the lexical entry,
I1 showed : llnp)
L = (- p , pplJ and can combine with an np on its left and with an np and then appon its right to form a sentence  . 
When left and right argument lists are empty , categories are said to be saturated , and may be written as their base type i . e . \[X )\] is identical to X . 

A requirement inherited from dependency grammar is for arguments to be saturated categories  , a LDGs will be specified more formally in Section  4  . 
It is worthout lining the differences between the categories in LDG and those in a directed categorial grammar  . Firstly , in LDG there is no specification of whether arguments to the right or to the left of the functor should be combined with first  . Thus , the category , I(Y ) , maps to both ( X\Y)/Z and
LrlZ ~ J(X/Z)\Y , 4See on dly , arguments in LDG must be saturated , so there can be no functions of functions .   5 a Independency granunarit is not ponaible to specify that a head require a dependent with only some  , but not all of its dependent a . 
4So called ' Steed rn~n ' otation.
5An extended form of LDG which allows ungaturated argu-ACRESDE  COLING-92  . NANTES , 2328 AOITI "19921096 PROC . OFCOLING-92, NANTES , AUO .  2328 ,   1992   3 Dynamic Dependency Grammar Lexicali ~ d Dependency Grammar can be reformulated as the dynamic grammar  , Dynamic Dependency Grammar ( DDG) . Each state in DDG encodes the syntactic context , and is labelled by the typc of the string absorbed so far  . For example , a possible set of transitions for the string of words " Sue saw 
Ben " is as follows : ' Ii 1 SSue\]J'et lL~lslj ~ l ~ , up ,   , L~-Inplj The state after absorbing " Suesaw " is of type sentence mi ~ ing a noun phrase  , and that after absorb-lug " Suesaw Ben " is of type sentence  . 
States are labelled by complex categories which arc similar to tile lexical categories of LDG  , but without the restriction that arguments must be saturated  ( for example , the state after absorbing " Sue " has an unsaturated argument on its right list  )  . A string of words , Str , is grammatical provided tile following statement can be proven : i  0 Strs\[r ( s The initial state is labelled with an identity typei  . e . 
a sentence missing a sentence . This cat , be thought of as a context in whic ~ a sentence is expected  , or as a suitable type for a string of words of length zero  . 
The final state is just of type sentence.
DDG is specified nsing a logic consisting of a set of axioms and a deduction rule  . The logic is similar , but more general , than that used in Axiomatic Grammar ( Milward ,  1990 ) f The deduction rule is again called Sequencing . The rule is identical inform to the Se-quencing Rule used in the reformulation of the FSM  , though here it puts together strings of words rather tiian strings of letters  . The rule is as follows , ~
CoString ~ C,~CtString,C ~
CoString . oString bC ~ and is restricted to nonempty strings  , smenks h&s been developed , and this also e2tn be reformulated as a dynamic ~ , m*mma " ( Milward ,  1992) . 
6Axiomatic Grmnm~r is a particular dynamic grammar designed for English  , which take ~ relations lfips between states tma primary phenomenon  , to be justified solely by linguistic data ( rather thmt by an existing formalism such as dependency granmlar  )  . 
Z Here'o'concatenates strings of words e.g.
" John " o " ~ leep a "= " John sleeps " .
SThis re~trictlon is not actually necessary as far as the equivalence between LDGs and DDGais concerned  . However its inclusion makes it trivial to show certain fontud properties of DDGs  , such a . ~termination of proofs . 
The set of axioms is infinite since we need to consider transitions between an arbitrary number of categories  .   9 The set can be described using just two axiom schemata  , Prediction and Application . Prediction is given below , but is best understood by considering various instantiations  ) ? i0iOIt ( IL , L ') , It ' rR , ( IIX ) , L ' , R'L~OJL~o

Prediction is usually used when the category of tile word encomitere does not match the category expected by the current state  . Consider the following instantiation : i 0 "Sue " s where Sue : np\[rls ) r ( l l n p )   ) 

The current state expects as eutence and encounters a noun phrase with \] exical entry Sue : rip  . The resulting state expects a sentence missing a noun phrase on its left e  . g . a verb phrase . 
Application gets its name from its similarity to function application  ( though it actually plays the role of both application and composition  )  . The schema is ~ follows : I'\[1I1I: , li0s " W "10 where W : r(XIL) . R '\[ rlt . R'JkrR\]rOJAn example instantiation is when a noun phrase i ~ both expected and encountered e  . g . \[ sll0"Ben"s where Ben:np
Lrlup , j
Given a word and a particular current state , the only nondeterminism in forming are snlting state is due to lexical ambiguity or from a choice between using Prediction or Application  ( Prediction is possible whenever Application is )  . Non-determinism is gen-?An infiniten mnber of distinguishable stat ~ is required to deal with centrem bedding  . 
X ? L,L ~, R dad R ' are lists of categories . '#' COhCatermtes lists e . g . ( k , l ? ( , non = ( k , l,m,n) . 
AC1ES DECOTING-92, Nnl,rn ~ . s , 2 . 3 ~ 28Ao ~'1992 1097P ~ to c . OFCOL1NG-92, N^NaXS , AUO .  2328 ,   1992 erally kept low due to states being labelled by types as opposed to explicit tree structures  . This is easiest to illustrate using a verb final language  . Consider a pseudoEnglish where the strings , " Ben Suesaw " and " Ben Suesleeps believes " are acceptable sentences  , and have the following LDG analyses : ll(np , Lr ? np s/\l(np)
Lr(IJ
Despite the differences in the LDG tree structures  , the initial fragment of each sentence , " BenSue " , can . . be treated identically by the corresponding DDG . 
The proof of the transition performed by the string " BenSue " involves two applications of Prediction put together using the Sequencing Rule  . The transitions are as follows : 11 ) ~ Z'~ns~ts
Lr(s ) jrll(np )) r(l(np,np))
Lr(lLL  ~
The transitions for the two sentences diverge when we consider the words saw and sleeps  . In the former case , Application is used , in the latter , Prediction then Application . 
Efficient parsing algorithms can be based upon DDGs due to this relative lack of nondeterminism in choosing between states  . H The simplest algorithm is merely to nondeterministically apply Prediction and Application to the initial category  . Derivations of algorithms from more complex logics  , and the use of normalised proofs and equivalence classes of proofs are described in Milward  ( 1991 )  . 
4 LDGs--+DDGs
An LDG can he specified more formally as follows :   1  . A finite set of base types To, . . " In ( such as s , up , and pp ) ll Determlnism can also be increased by restricting the axioms according to the properties of a particular lexicon  . For example , there is no point predicting categories missing two noun phrases to the left when pansing English  . 
2 . An infinite set of lexical categories of the form  , XL1 where X is a base type , and L and ItarerR\]lists of base types . When L and R are empty , a category is identical to its base type , X3 . A finite lexicon , L , which assigns lexical categories to words 4 . A distinguished base type , To . A string is gram-matieal iff it has the category , To 5 . A combination rule stating that,
F\]if W has category , 1(Ti , .., TI)
Jr (7, + a . . . . . Ti+i ) and String1 has category T1 , String2 has category 7~ etc . 
then the string formed by concatenating
String1, . . , String ~, " W ' , Stringi+l , . .  , String/+j has category X corresponding DDG is as follows:  .   .   .   . 
L~R j where X is a base type , and L and Yt are lists of categories2 . Two axiom schemata , Application and Prediction 3 . The lexicon , L(as above ) 4 . One deduction rule , Sequencing I\]5 . A distinguished pair of categories , ll ), 7~
L ~: ( nlJ where To is as above . A string , Str , is grammatical iff it is possible to prove :\[ 1'7  ( ~? ) \]Str7~r ( To ) j A proof that any DDG is strongly equivalent to its corresponding LDG is given by Milward  ( 1992 )  . The proof is split into a soundness proof ( that a DDG accepts the same strings of words and assigns them corresponding  analysesl2  )  , and a completeness proof ( that a DDG accepts whatever strings are accepted by the corresponding LDG  )  . 
The 1.
5 Incremental Interpretation
It is possible to augment each state with a semantic type and a semantic value  . Adopting a ' standard ' A-calculu semantics ( c . f . Dowty et al 1981 ) we obtain the following transitions for the string " Sue saw ":  12For this purpose , it is convenient to treat an analysis in a DDG as the traasition ~ performed by each word  . Each analysis is a label for all equivalence Class of proofs  . 
Acr ~ DECOLING-92 , NANTES , 2328 AOI~'T19921098 PROC . OFCOLING-92 . NA ~ CrEs , AUG .  2328 . 1992I'I1Is\]iOSue10~SL~(s)j ~( il ,, v)
Lr(IJl~t(e--~t)-~t
AQ . QAP . P ( sue ') . . . . . Is\]I0) L~-Iup ) je--~t

The semantic types can generally be extracted fronl the syntactic types  . The base typess and apmap to the semantic types lande  , stmt ding for trutb-value and enl , ity respectively . Categories with argmnents map to corresponding flmctional types  . 
Provided a close mapping between syntactic and semantic types is assumed  , the addition of semantic values to the axiom schemata is relatively trivial  , as is the addition of semantic vahtes to the lexicon  . For example , the semantic value given to the verb saw is AYAX . saw'(X,Y ), which has type e ~( e-~t ) . 
It is worth contrasting the approach taken here with two otlleral  ) proaches to incremental interpretation . Timfirst is that of Pnlman (19851 . Pulman's approach separates syntactic and senmn tie analysis  , driving semantic ombinations off the actions of a parser for a phrase structnre grammar  . The approach was important ill showing that hierarchical syntactic analysis and word by word incremental interpretation are not incompatihle  . The second approach is that of Ades and Steedman ( 1 9821 who incorporate conlposition rules directly into a catego-rim granlmar  . This allows a certain amount of incremental interpretation dim totile possibility of forming constitnents for some initial substrings  , flow-ew : r , the incorporation of composition i to the grammar itself does haw : sonicun wanted side effects when n fixed with a use of functions of fimctions  . For exam-pie , if the two types , N/N and N/N are composed to give tile type N/N , then this can be modified by an adjectival modifier of type  ( N/N ) / ( N/N )  . Thus , the phrase the very old green car " can get the bracketing  , \[ the\[very\[old green \]\] car\] . Althoughtile Application schema used in DDGs does compose functions together  , DI ) Gs have identical strong generative capacity to the LDGs they are based upon  ( the coverage of the grammars i identical , and tile analyses are illaone-to-one correspondence  )  . 136 Applications So far , l ) ynamicl ) epen deney Grammars can be seen solely as a way to provide incremental parsing and ill-terpretation for Lexicalised l  ) ependency Grammars . 
As such , they are not of particular linguistic significance  , l to wever , it is possible to use DDGs as subsets of n to re expressive dynamic gramnrars  , where extra axioms and deduction rules are used to provide coverage of syntactic phenomena which are difficult to la'I'his is also true for dyn ~anic refomlulations of extended versions of LDG which allow function n of functions  . 
encode lexically ( e . g . coordination , topicalisation and extrapcsition ) . For example , tile following deduction rule ( again restricted to nonempty strings )  , 
CoString , C ,, C~C,
CoString , , " and '~ C ~ provides all account of the syntax of nonconstituent coordination  ( Milward ,  1991) . The sentences John gave Mary a book and Peter a paper " and Suesold and Peter thinks Bert bought a painting are accepted since " Mary a book " and " Peterapaper " performtile same transitions between syntactic states  , ms do " Suesold " and " Peter thinks Ben botl ght " Thegr  , ' mtmars described in this paper have been impleme uted in Prolog  . A dynamic gramnlar based upon the extended version of l  , I ) Gs is being developed to provide incremental interpretation for the natural hmguage interface to a grapllics package  . 
References
Ades , A . and Steedman , M . (19721 On the Order of Words . Linguistics and Philosophy 4, 517-558 . 
Aho , A . and Ulhnan , J . (19721 The Theory of Parsing , Traaslatton and Compiling , Volume 1: Parsing . 
Prentice-Ilallnc , New Jersey.
Barry , G . and Pickering , M .   ( 19901l ) et ) endcncy and Constituency ill Categorial Gramnlar . Ill Barry , G . 
and Morrill , G . ( eds ), Studies in UategoTial Grammar . Centre for Cognitive Science , University of E(I-inburgh . 
van Benthem , 3 . (19901 General ) ynamics . ITL1 report , Anlsterdaln ( to appear in Theoretical Linguistics )  . 
I ) owty , I) . R . , Wall , R . F . and Peters , S . (19811 ht-troduction to Montague Semantics .  1) . l~idel , Dordrecht . 
Hays , II . G .   ( 1 9641 Dependency Theory : A Formalism and Some Observations  . Language 40, 511-525 . 
lhlds on , IL ( 1988 ) Coordination and Gramnlatical Relations . Journal of Linguistics 24, 303-342 . 
Milward , D . (19901 Coordination in an Axiomatic Grammar . hi Coling-90,llelsmki , vol 3, 207-212 . 
Milward , I) . (1991) Axiomatic Grammar , Non-Constituent Coordination , and lnerenmntal Interpretation . PhD thesis , University of Cambridge . 
Milward , 11 . (1992) Dynamic Grammars . Technical Report , Centre for Cognitive Science , University of
Edinburgh . In preparation.
Puhuan , S . (1985) A Parser Ttl at Doesn't . In 2nd
European ACL , Geneva.
Woods , W .   ( 1973 ) An Experimental Parsing System for Transition Network Grammars  . In Rustin , R . ( ed . ), Natural Language Processing , Algorithmics
Press , New York.
Aorl ! s DECOLl NG-92 , NANTES , 2328 AOUq'1992I099PROC . OFCOLING-92, NAN'rI ~ . S , Auo .  2328, 1992
