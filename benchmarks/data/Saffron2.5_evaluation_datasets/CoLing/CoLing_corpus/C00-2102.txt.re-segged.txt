Named Entity Chunking Techniques
in Supervised Learning for Japanese Named Entity Recognition 
Manabu Sassano
Fujitsu Laboratories , Ltd.
4-4-1, Kamiko danaka , Nakahara-ku,
Kawasaki 211-8588, Japan
sassano (@ i lab . fujitsu.(:o.jI )
Takehito Utsuro
l ) el ) artment of Intbrl nation
and Computer Sciences,
Toyohashi University of Technology
lcnl)aku-cho,~l ~) yo hashi 441-8580, Jat ) an
utsm'og )) ics , rut.ac.jp
Abstract
This 1 ) a perfocuses on the issue of named entity chunking in Japanese named entity recognition  . 
We apply the SU l ) ervised decision list lean > ing method to Japanese named entity recognition  . We also investigate and in ( :ori ) orate several named-entity noun phrase chunking tech  . -niques and experimentally evaluate and con>t ) are their l ) er for nlane e , ill addition , wet ) rot ) ose a method for incorporating richer ( : on textua \] il flbrmation as well as I ) atterns of constituent morphenms within a named entity  , which h ~ ve not 1 ) een considered ill previous research , and show that the t ) roi ) osed method out t ) erfi ) rms the set ) revious ai ) proa ( 'hes . 
1 Introduction
It is widely a . greed that named entity recognition is an imt ) or t ; antstet ) ti ) r various al ) pli- ( : ations of natural language 1 ) ro ( : ( ' . ssing such as in tbn nation retrieval , macl fine translation , in-t brmation extraction and natural language understanding  . In tile English language , the task of named entity recognition is one of the tasks of the Message Understanding Confer -once  ( MUC )   ( e . .g . , MUC7 (19!)8)) and has be . on studied intensively . In the . al ) anese language ~ several recent conferences , uchas MET ( Multilingual Entity Task , MET-I ( Maiorano , 1996) and MET2 ( MUC ,  1998 ) ) and IREX ( Information letriew ~ l and Extraction Exercise ) Workshop ( IREX Committee ,  1999) , focused on named entity recognition ms one of their contest tasks  , thus promoting research on Jat ) an ese named entity recognition . 
In Japanese named entity recognition , it is quite common to apply morphological analysis as at  ) reprocessing step and to segment the sentence string into a sequence of mor-i  ) henles . Then , handcrafted t ) atternm~t ching rules and/or statistical named entity recognizer are apt  ) lied to recognize named entities . It is of l ; en the case that named entities to be recognized have different segmentation boundaries from those of morpheums obtained by the morphological analysis  . For example , in our analysis of the \] Ill , F , X workshop's training corpus of llallled entities , about half of the mtmed entities have segmentation boundaries that al'ed if-ferellt\]' rein the result of morphological naly sist  ) ya . \] al ) an esel nor phological nalyzer BI~EAK-FAST ( Sassano et al , 1997) ( section 2) . Thus , in . Japanese named entity recognition : among the most difficult problems is how to recognize such named entities that have segmentation boundary mismatch agains the morphemes ot  ) tained l ) y morphological nalysis . Furthermore , in almost 90% of ( : ases of those segmentation t ) oulld-ary mismatches , named entities to l ) e recognized can t ) e ( te conq ) osed into several mort ) heroes as their constituents . This means that the 1 ) roblem of recognizing named entities in those cases can be solved by incorporating techniques of base noun phrase chunking  ( Ramshaw and Marcus ,  1995) . 
In this paper , we tb cus on the issue of named entity chunking in Japanese name  . dentity recognition . First , we take a supervised learning approach rather than a handcrafted rule based approach  , because the tbnner is nlore promising than the latter with respect othe amomlt of human labor if requires  , as well as its a daI ) t-ability to a new domain or a new definition of named entities  . In general , creating training data tbr supervised learning is somewhat easier than creating pattern matching rules by hand  . 
Next , we apply Yarowsky's method tbr supervised decision list learning I  ( Yarowsky ,  1994 ) to 1VVe choose tile decision list learning method as the
NEType








Total frequency (%)
Training 3676 (19 . 7) 3840 (20 . 6) 5463 (29 . 2) 747 (4 . 0) 3567 (19 . 1) 502 (2 . 7) 390 (2 . 1) 492 (2 . 6) 361 (23 . 9) 338 (22 . 4) 413 (27 . 4) 48 (3 . 2) 260 (17 . 2) 54 (3 . 5) 15 (1 . 0) 21 (1 . 4 ) we incorporate several noun phrase chunking techniques  ( sections 3 and 4 ) and experimentally evaluate their performance on the IREX  , workshop's training and test data ( section 5) . 
As one of those noun phrase chunking techniques , we propose a method for incorporating richer contextual information as well as patterns of constituent morphemes within a named entity  , compared with those considered intire previous research  ( Sekine et al , 1998; Borthwick ,  1999) , and show that the proposed method out-perl brms these approaches  . 
2 Japanese Named Entity
Recognition 2 . 1 Task of the IREX Workshop The task of named entity recognition of the IREX workshop is to recognize ight named entity types in Table  1   ( IREX Conmfittee ,  1999) . 
The organizer of the IREX workshop provided 1 , 174 newspaper articles which include 18 , 677 named entities astire training data . In the formal run ( general domain ) of the workshop , the participating systems were requested to recognize  1  , 5 10 nanm d entities included in the heldout 71 newspaper articles . 
2.2 Segmentation Boundaries of
Morphemes and Named Entities
In the work presented here , we compare the segmentation boundaries of named entities intire IREX workshop's training corpus with those of supervised learning technique mainly because it is easy to implement and quite straightibrward to extend a supervised lem'ning version to a milfimally supervised version  ( Collins and Singer , 1999; Cucerzan and Yarowsky ,  1999) . We also reported in ( Utsuro and Sassano ,  2000 ) the experimental results of a minimally supervised version of Japanese named entity recognition  . 
Table 2: Statistics of Boundary Match vs . Mis-lnatch of Morphemes ( M ) attd Named Entities ( NE ) Match/Misnmtch II freq . of NE Tags (%) 1M to 1NE 10480 (56 . 1) n(>2)Ms to 1NEn = 2n = 3n > 44557 (24 . 4) 1658 (8 . 9) 717 596 o(5 . 1) (38 . 4) other boundary mismatch 1022 (5 . 5)
Total J\[ 18677 morphemes which were obtained through morphological analysis by a Japanese morphological attalyzer BREAKFAST  ( Sassano et al ,  1997) .   2 Detailed statistics of the comparison are provided in ' Fable  2  . Nearly half of the named entities have bmmdary mismatches agains them or I  ) hemes and also almost 90% of the named entities with boundary mismatches can be tie-composed into more than one morpheme  . Fig--ure 1 shows some examples of such cases , a 3Chunking and Tagging Named
Entities
In this section , we formalize the problem of named entity chunking in Japanese named entity recognition  . We describe ~ tnovel technique as well as those proposed in the previous works on nanted entity recognition  . The novel technique incorporates richer contextual information as well as p~tterns of constituent morphemes within ~ named entity  , compared with the techniques proposed in previous research on named entity recognition and base noun phrase chunking  . 
3.1 Task Definition
First , we will provide out " definition of the task of Japanese named entity chunking  . Suppose ' ~ The set of part-of-speech tags of ll U . ~ AKFAST consists of about 300 tags . mmAK FaST achieves 99 . 6% part-of-speech accuracy against newspaper a ticles  . 
a In most cases of the " other boundary mismatch " in Table  2  , one or more named entities have to be recognized as a part of a correctly analyzed morpheme and those cases are not caused by errors of morphological analysis  . One frequent example of this type is a Japanese verbal noun " hou-bei  ( visiting United States ) " which consists of two characters " hou ( visit in . q ) " and " bet ( United States ) " , where " bet ( United States ) " has to be recognized as < LOCATION > . \ Vebelieve that 1 ) ouudary mismatches of this type can be easily solved by employ-ink a supervised learning technique such as the decision list learning method  . 
706' Dfl ) le 3: Exmoding Schemes of Named Entity Chunldng States
Named Entity Tag
Mort ) heine Sequence
Illside / ( ) utside Encoding
Stmt/End Encoding < 0RG > < LOC > < L0C > - .   . MIMM\]M 0   0RG_I   0   L0C_I LOC_ILOC_ILOC_B 0   0   0RG_U   0 LOC_S L0C_C LOC_EL OC_U 0 VMoi-l ) hemes to 1NamedEntity\]<ORGANIZATION > . . . . Roshia gun - . .
( S < , s,~i , . 0(a , ', , . j ) < PERSON > . . . . Murayama IbIni ichi shushOUprimc \]"" ( last nmne )   ( first name )   ( minister " \[3 Morphemes to1 NamedEntity\]<TIME>gozenku . ii "? ( AM )   ( niuc )   ( o?1ock ) < ARTIFACT > hokubei . jiyuu-1) oue kikyoutei--?
Norfl ~ ( America ) ( fleetrade . )  ( treaty ) Figure 1: Ex~mq ) les of B ( mndary Mismatch of
Morl ) hemes midNamed Entities that a sequen ( ' e of m or l ) heme sigiven as 1 ) e-low:
Left ; l , ight(Context ) ( NamedEntity ) ( Context ;)? .   . ~I '_' ~ .   .   . ~ IL ~, i ~' ~ .   .   . M /' .   .   . ~, l/,''~~1~" .   .   . ~, f /" .   .   . 
t ( Current Position )
Then , given tht~t the current t ) osition is at the morpheme M .   N1': the task oftanned elltity e hlll kill g is to assign a  , C\]luukillg state ( to ) e described in Section 3 . 2 ) as well ~ rsanmned entity type to them or l ) helne MiNE attim current position , considering the patterns of surrounding morl ) hemes . Note that in the SU l ) ervised learning phase we can use the ( : lmnkingiuibn nation on which morphemes constitute angune  ( lentity , and whi(-hmorphemes are in the lefl ; /right contexts of t it ( ; named entity . 
3.2 Encoding Schemes of Named
Entity Chunking States
In this t ) at ) er , we evalu ~ te the following two s ( ' hemes of encoding ctmnking states of nalned entities  . EX alnples of these encodings ( : hemes are shown in Table 3 . 
3.2.1 Inside/Outside Encoding
The Inside/Outside scheme of encoding chunking states of base noun phrases was studied in Ibmlshaw and Marcus  ( 1995 )  . This scheme distinguishes the tbllowing three states :  0 the word at the current position is outside any base holm phrase  . I the word at the current position is inside some base holm phrase  . B the word at the current position marks the beginning of ~ base no mlt  ) hrase that immediately foplows another base noun phrase  . We extend this scheme to named entity chunking by further distinguishing each of the states I and B into eight named entity types  . 4 Thus , this scheme distinguishes 2x8+1 = 17 states . 
3.2.2 Start/End Encoding
The Start/End scheme of encoding clmnking states of nmned entities was employed in Sekine e  , tal . (1998) and Borthwick (1999) . This scheme distinguishes the , following four states for each named entity type : Sthelll OlTt  ) \] lell leat the ( : urrel d ; position nmrks the l ) eg in ldng of al Ul . in(xt ( ; lltity consisting of more than one mor-1)\]mme . Cl ; heln Orl ) heme ~ I ; the cm'r ( mt ) osi-tionmarks the middle of ammmd entity ( : on sist-ing of more tlm none lilOrt ) hellle . E - - the illOft ) heme , at the current position ram:ks the ending of an ~mmd entity consisting of more than one morl  ) heme . U-the morpheme at the current t ) osition is a named entity consisting of only one , mort ) heine . The scheme ; dso considers one ad ( li-tional state for the position outside any named entity:  0 t ; hemort ) heine at the current position is outside any named entity  . Thus , in our setting , this scheme distinguishes 4x8+1=33 states . 
a . 3 Preced ing / Subsequent Morphemes as Contextua l Clues In this l  ) a per , weew fluate the following twoll lodels of considering preceding/subsequent  4\Ve allow the , state : c_B for a named entity tyt ) ex only when the , morl ) hcmeatt , he current 1 ) osition marks the 1 ) egimdng of a named entity of the type a " that immediately follows an mned entity of the same type x  . 
7 07 morphemes as contextual cluestonamed entity clmnking/tagging  . Here we provide a basic outline of these models , and the details of how to incorporate them into the decision list learning framework will be described in Section  4  . 2 . 2 . 
3. a . 13gram Model
In this paper , we refer to the model used in Sekine et al ( 1998 ) and Borthwick ( 1999 ) as a 3gram model . Suppose that the current position is at the morpheme  M0  , as illustrated below . Then , when assigning a chunking state as well as a named entity type to the morpheme  M0  , the 3gram model considers the preceding single morpheme M1 as well as the subsequent single morpheme M1 as the contextual clue . 
Left Current Right ( Context )   ( Position )   ( Context )  ?  .   . M0M , .   .   .   ( 1 ) The major disadvantage of the 3gram model is that in the training phase it does not take into account whether or not the l  ) re-ceding/subsequent morphemes constitute one named entity together with the mort  ) heine at the current position . 
a . a . 2 Variable Length Model
In order to overcome this disadvantage of the 3gram model , we propose a novel model , namely the " Variable Length Model " , which incorporates richer contextual intbrmation as well as patterns of constituent morl  ) hemes within a named entity . In principle , as part of the training phase this model considers which of the pre-ceding/subsequent morphenms constitute one named entity together with the morpheme at the current position  . It also considers several morphemes in the lefl ; /right contexts of the named entity . Here we restrict this model to explicitly considering the cases of named entities of the length up to three morphenms and only implicitly considering those longer than three morphemes  . We also restrict it to considering two morphemes in both left and right contexts of the named entity  . 
Left ( Context ) . . . ML2MI_'Ill,ight(NamedEntity)(Context)
M # .   .   .   . . . Mm ( <3  )  1"  ( 2 )   ( Current Position ) 4 Supervised Learning for Japanese
Named Entity Recognition
This section describes how to applytile decision list learning method to chunking/tagging named entities  . 
4.1 Decision List Learning
A decision list ( Rivest , 1987; Yarowsky , 1994) is a sorted list of decision rules , each of which decides the w flue of a decision D given some evidence E  . Each decision rule in a decision list is sorted in descending order with respect os ome preference value  , and rules with higher preference values are applied first when applying the decision list to some new test  ; data . 
First , the random variable D representing a decision w , ries over several possible values , and the random w ~ riable E representing some evidence varies over  '1' and '0'   ( where '1' denotes the presence of the corresponding piece of evidence  , '0' its absence ) . Then , given some training data in which the correct value of the decision D is annotated to each instance  , the conditional probabilities P ( D = xIE = 1 ) of observing the decision D = x under the condition of the presence of the evidence E  ( E = 1 ) are calculated and the decision list is constructed by the tbllowing procedure  . 
1 . For each piece of evidence , we calculate the Iw of likelihood ratio of the largest  ; conditional probability of the decision D = : rl ( given the presence of that piece of evidence ) to the second largest conditional probability of the decision D =  x2: 
IE = I ) l ? g2P ( D = x2IE = I)
Then ~ a decision list is constructed with pieces of evidence sorted in descending order with respect to their log of likelihood ratios  , where the decision of the rule at each line is D = xl with the largest conditional probability  ) ' ~ Yarowsky ( 1994 ) discusse several techniques for avoiding the problems which arise when an observed count is  0  . lq-om among those techniques , we employ tlm simple stram , i . e . , adding a small constant c ~ (0 . 1 < < 0 . 25) to the numerator and denominator . With this inodification , more frcquent evidence is preferred when several evidence candidates exist with the same % default  '  , where the log of likelihood ratio is calculated D <  ) m the ratio of the largest ; marginal ) robability of the decision D = xt to the second largest margin all  ) rol ) at ) ility of the decision D =  x2:
P ( D=:/11) log~p(D = x'2)
The ' default'decision of this final line is D = Xl with the largest lnarginal probability  . 
4.2 Decision List Learning for
Chunking/Tagging Named Entities 4 . 2 . 1 Decision For each of the two schemes of enco ( li1~g chunking states of nalned entities descrit ) ed in Section 3 . 2 , as the l ) ossible values of the < teei-sion D , we consider exactly the same categories of chunking states as those described in Section  3  . 2 . 
4.2.2 Evidence
The evidence E used in the decision list learning is a combination of the tbatures of preced-ing /subsequent in or phemes as well as the morpheme at  ; the current position . The following describes how to form the evidence Efi  ) r1 ) oth the a-gramn lodel and varial ) lelength model . 
3-, gram Model
The evidence Eret ) resents a tut)le ( F-l , F0 , F1) , where F1 and F1 denote the features of immediately t ) receding/subsequent morphemes M_~ and M1 , respectively ,   F0 the featm:e of the morpheme 54o at the current position ( see Fonnuta ( 1 ) in Section 3 . 3 . 1) . The definition of the possible values of those t batures F_l  , F0 , and 1'~ are given below , where Mi denotes the roof 1)\] mnm itself ( i . e . , including its lexicM tbrm as well as part-of-sl ) eech )  , C , i the character type ( i . e . , JaI)anese ( hiragana or katakana ) , Chinese ( kanji ) , numbers , English alphabets , symbols , and all possible combinations of these ) of Mi , 
Ti the part-of-st ) eech of Mi :
F_1::m_\]~//--1I(C-1 , T-l ) IT-t In ull ml smoothed conditional probability P  ( D = x\[E = 1 )  . 
Yarowsky's training Mgoritl , malsod it fcr somewhat in his use of the ratio *' ( ~ D = , d *~- j ) ' which is equivalent in the case of binary classifications  , and also by the interpolation between the global probal filities  ( used here ) and tl , eresidual prol ) abilities further conditional on higher-ranked patterns failing to match in the list  . 
17'1::--~\]~/-/1I(C ,, V ;) IT*Inul\]
F0::-M0I(C0, T0) lT0
As the evidence E , we consider each possible coml ) ination of the values of those three f'ea-lures . 
Variable Length Model
The evidence Erel > resents a tuple ( FL , FNu , FIt ) , where FL and Fl ~ denote the features of the morphemes  ML_2ML1 and Mff'M ~~ in the left/right contexts of the current named entity  , respectively , FNE the features of the morphemes MN~"""MNE " "" MNEm  ( _<3 ) constituting the current named entity ( see Formula ( 2 ) in Section 3 . 3 . 2) . The definition of the possible values of those features  1 L , FNI , : , and FI~arc given below , where FNI ~ denotes the feature of the j-th constituent morpheme M  . NJ ~ within the current nalne (1 entity , and ak/lNI ~ is them or l ) hemeat the cm'ren~i ) osition:
FL::=M*_'2 M~~\[M ~ I null
Fu ::= M\]~M~IM ~ I null
FNEFNEFNEIFNE r . N I'2   7z~NE FNE : := i i+1   i+2  \[ *  i1 * i * i+1  \]  I~NI'A~NI'21pNE   17NE~NI  , 2 ?  , FNE , (3) ~ NICMN ~ c(Cm , : T~VJ  ~ ,   , NI , : As the evidence E , we consider each poss it ) le ( : oml ) ination of the w fiues of those threefba-tures , except that the tbllowing three restrictions are applied  . 
1 . In the cases where the current named entity consists of up to three mort  ) heroes , as the possible values of the feature FNIi in the definition  ( 3 )  , we consider only those which are consistent with the requirement that each nlort  ) heme MNE is a constituent of the cun'ent named entity  . For exain ple , suppose that the cun'ent named entity consists of three morphemes  , where the current position is at the middle of those constituent morphemes as below : 
Left Right ( Context )   ( Named Entity )   ( Context ) I . L M1N~'M N + ~ M~u I~t ~?" M1 Mi-'-_/l //_ 2/~//_   1   1"   ( 4 )   ( Current Position ) Then , as the possible values of the feature FN\] , ; , we consider only the tbllowing ibm ': rN . : :=\[ F . gU . g consists of more than three morphemes , only the three constituent morphemes are regarded as within the current named entity and the rest are treated as if they were outside the named entity  . For exam-pie , suppose that the current named entity consists of four morphemes as below: 
Left Right ( Context )   ( Named Entity )   ( Context ) 
LL $ ( Current Position)
I it this case , the fourth constitnent morpheme M N1c is treated as if it were in the right context of the current named entity as below : 
Left Right ( Context )   ( Named Entity )   ( Context )  ' . ,~ 1 . ~ r . , vJ , :~, Nu ~, . ,,' . r,_,CM~ZMfft(Curren~Position ) 3 . As the evidence E , among the possible combination of the values of three t'ea-tures / ~  , , ENId , and F/t , we only accept those in which the positions of the morphemes are continuous  , and reject those discontim mus combinations . For example , in the case of Formula (4:) above , as the evidence E , we accel ) t the combination ( M q , M ' M y , ull ) , while wer(iect(ML1 , M ~ EM ~1': ,  1 , ull) . 
4.3 Procedures for Training and

Next we will briefly describe the entire processes of learning the decision list tbretmnk-ing /tagging named entities as well as applying it to chunking/tagging unseen named entities  . 
4.3.1 Training
In the training phase , at the positions where the corresponding morpheme is a constitnent of a named entity  , as described in Section 4 . 2 , each allowable combination of features is considered as the evidence E  . On the other hand , at the positions where the corresponding morpheme is outside any named entity  , the way the combination of t ~ at ; ures i considered is difl brent in the variable length model  , in that the exception \] . in the previous section is no longer applied . 
The retbre , all the possible w flues of the feature FNB in Definition  ( 3 ) are accepted . Finally , the frequency of each decision D and evidence E is counted and the decision list is learned as described in Section  4  . 1 . 
4.3.2 Testing
When applying the decision list to chunk-ing /tagging nn see named entities  , first , at each morpheme position , the combination of features is considered as in the case of the non-entity position in the training phase  . Then , the decision list is consulted and all the decisions of the rules with a log of likelihood ratio above a certain threshold are recorded  . Finally , as in the case of previous research ( Sekine et al , 1998; Berth-wick ,  1999) , the most appropriate sequence of the decisions that are consistent throughout the whole sequence is searched for  . By consistency of the decisions , we mean requirements such as that the decision representing the beginning of some named entity type has to be followed by that representing the middle of the same entity type  ( in the case of Start/End encoding )  . Also , in our case , the appropriateness of the sequence of the decisions is measured by the stun of the log of likelihood ratios of  1  ; t1( ; corresponding decision rules . 
5 Experimental Evaluation
We experimentally evaluate the performance of the supervised learning t brJapanesenalned entity recognition on the IREX workshop's training and test data  . We compare the re-suits of the confl ) inations of the two encoding schemes of named entity chunking states  ( the Inside/Outside and the Start/Endencoding schemes  ) and the two at ) preaches to contextual feature design ( the 3gram and the Variable Length models )  . For each of those combinations , we search t bran optintal threshold of the log of likelihood ratio in the decision list  . The performance of each combination measured by Fmeasure  ( f l = 1 ) is given in Table 4 . 
In this ew duation , we exclude the named entities with " other boundary mismatch " in 
Tat)le 2 . We also classify the system output according to the number of con-stitnent lnorphemes of each named entity and evaluate the peribnnance tbr each subset of the system output  . For each sub-

Length ' l ' al ) le 4: Ewduation \] esults Measured by F-measm'e ( f l = 1 )  ~ , Mori ) lm lnesto1 NamedEntity- . , > J_ll , , , = I , , . =21, . .=311 . ,>_21 . ,, . >_3 1,,>4 inside/Outside 72 . 9 75 . 9 79 . 7 51 . 4 69 . 4 42 . 5 29 . 2 Start/End 72 . 7 76 . 6 79 . 6 43 . 7 68 . 1 37 . 8 29 . 6 inside/Outsidel J 74 . 3 77 . 6 80 . 0 55 . 5 70 . 9 49 . 9 41 . 0 Start/Endt\[72 . 1 77 . 0 75 . 6 51 . 5 67 . 2 48 . 6 43 . 6 set , we compare , ' the performmme of the fore'combinations of 3-grmn   , Vm'iableLength ? Inside/Outside , S ; ~ n't/E IldmM show the highest mrt brmance with boldfaced font  . 
Several remarkable points of these re , suits of 1 ) erfbrmance omparison can be stated as below : ? Among the four coml  ) inations , the Variable Length Model with hlside / ( )utside Ent:od-ing 1 ) erfi ) rm sbest into t ~ f l ( n > 1 ) as well as in the recognition of named entities consisting of more thmlonem or l  ) heme ( ' , , --2 ,  3 , n > 2 ,  3) . 
? in there , (:ognil ; ionofilsAll ( ; delll ; ities consisting of more than two mOl " l ) henles(~ , = 3: ? t~3 ,  4 ) ~the Vm'ial ) le Lellgth Modell ) er for lllSsignific ; mtly t ) etter thml the 3-rill " t ( . ~~ l ' all lmo(le\] .   . tn\] , ' result (: letu'lySUpl ) or ts the ( ; l ~ i in that our model i \] x g of the Vm'i-nl ) le Length Model has an adva , ntngein the recognition () flong named entities . 
" Ill general , the Inside/Outsiden ( : oding scheme l ) erfol'lns slightly t ) etl ; erth ; m the Sta\]'t/l'3nd encodings (: henm ,   ( Well though the tbrmer distinguislms ( : onsider a ) lyti~wersl ; atest h ; m the latter . 
6 Conclusion
In this 1) ~ per , weal ) plied the supervise deci-si ( mlist learning method to , \] at ) an esemmmd entity recognition , in towl fich we , incorporated several n ( mn phrase chunking teel miques ~ md experimentally evaluated their pertbrmance  . We , showed that a novel technique that we proposed out  , performed those using previously considered ( ; otd ; extual fe~tu \]: es . 
7 Acknowledgments
This research was c~rried out while the authors we revisiting scholars at l  ) epartment of Computer Science , Johns Hopkins University . 
The ~ mthors would like to thank Prof David Yarowsky of Johns Hopkins University for in -valual  ) lesut ) porl ; sto this research . 

A . Borthwick .  1999 . AJaI ) mmse named entity recognizer constructed by a non -speaker of Japanese  . 
In Proc . of the II~EX Workshop , pages 187193.
54 . Collinsa . ndY . Singer .  1999 . Unsupervised models of named entity classification  . In P . roc . of the 1999 Joint SIGDAT Cm@rcn ccon Empirical Mcth , ods in Natural LanguagcP ~ vccssing and
Very Large Corpora , pages 100110.
S . Cucerzmt and D . Yarowsky .  1999 . l ~ anguage inde-1 ) endent named entity recognition combining mor-1 ) hological mid contextua levide ime , in Proc . of th , c 1999 Joi'nt SIGDAT Cm@rence on Empirical Methods in Natural Language  PTvccssin9 mid
Very Large Corpora , pages 9099.
IREX Committee , editor .  1!)99 . P ~ v cecdings of the \] REX Workshop . ( in Japanese) . 
S . Maiorano .  1996 . The multilingual entity task ( MET ): Jalmne , s e , results . In ISvc . of TIPSTEI ~, PIH ) U1/, AMPHA , 5'1'; 11, pa . ges 44945\] . 
MUC .  1998 . l ) ' r occc ding so J " l , h , e , 7th Message Unde'r-standing ConJ?rence , ( MUC7) . 
L . lalns haw and M . Ma . rcus .  1995 . Text chunking using trmls form a . tion-based \] mning . In P , roc . of th , c3rd Work , vh , oponl/cry Larg (: Corpora ~ Im . ges83-94 . 
ILL . Rive , st . 1987, Le , arning decision lists . Machine
Learning , 2:229246.
M . Sassano , Y . Saito , and K . Matsui . 1997.
,J~l)alle , semorphological mmlyzer for NLP apl ) li-(:atioils . Ill Proc . of thc , 3rd gn ' ttual Meeting of th , cAssociation for Natural Language Processing , 1) a . ges 441-444 . ( in Jal ) anese) . 
S . Sekine , lL Grishman , and H . Shinnou . 1998.
A decision tree method t brt inding and ( ' lassit ~- ing names in Jat ) almse texts . In Proc . of the 6th Workshop on Very La~ycCtnpora , pages 148-152 . 
T . Utsuromid M . Sassano .  2000 . Minimally supervised , \] almnese name de , ntity recognition : I e-source , sandevahmtion . In Proc . of thc 2nd International Confcrcnccon Lanquaqc Resources and 
Evahtation , pages 1229-1236.
1) . Yarowsky .  1994 . Decision lists for lexical mnbi-guity resolution : Al  ) t ) lication to accent rest or a . -tion in Spanish and French . In Proc . of the 32 rid Annual Mecl , ing of ACL , 1) ages 88-95 . 

