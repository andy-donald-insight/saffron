CLAWS 4: THETAGGING OF THEBRITISH NATION ALCORPUS
Geoffrey Leech , Roger Garside and Michael Bryant
UCREL , Lancaster University , UK
1 INTRODUCTION
The main purpose of this paper is to describe the CLAWS4 general-purpose grammatical tagger , used for the tagging of the 100-million-word British National Corpus , of which c . 7 0 million words have been tagged at the time of writing  ( April 1994 ) ) We will emphasise the goals of ( a ) gener~d-purpose adaptability ,   ( b ) incorporation of linguistic knowledge to improve quality  , and consistency , and ( c ) accuracy , measured consistently and in a linguistically informed way  . 
The British National Corpus ( BNC ) consists of c . 1 00 million words of English written texts and spoken transcriptions  , sampled from a comprehensive range of text types . The BNC includes 10 million words of spoken h'm guage , c . 45% of which is impromptu conversation ( see Crowdy , forthcoming ) . It also includes , an immense variety of written texts , including unpublished materials . The gr , ' un-matical tagging of the corpus has therefore required the ' super-robustness ' of a tagger which can adapt well to virtually all kinds of text  . The tagger also has had to be versatile in dealing with differentag sets  ( sets of grammatical category labels -- see 3 below ) and accepting text invaried input formats . For the purposes of the BNC , l , he tagger has been requircd both to accept and to output ext in a corpus-oriented TEl-confonnant markup definition known as CDIF  ( Corpus Document Interchange Format )  , but within this format many variant for naats ( affecting , for example , segmentation into words and sentences ) can be readily accepted . In addition , CLAWS al-X The BNC is the result of a collaboration  , supported by the Science mid Engineering Research Council  ( SERC Grant No . GR/F99847) , and the UK Dep , ' u'tment of Trade and Industry , between Oxford University Press ( lead p~u't-ner )  , Longman Group Ltd . , ChambersHarrap , Oxford University Computer Services , the British Library and Lancaster University . We thank Elizabeth Eyes , Nick Smith , mid Andrew Wilson for their v , ' duable help in the preparation f this paper . 
lows variable output formats : for the current ag -ger  , these include ( a ) avertically-presented format suitable for manual editing  , and ( b ) a more compact horizontally-presented format often more suitable for end-users  . Alternative output formats are also glowed with ( c ) so called ' portmanteau tags ' , i . e . combinations of two alternative tags , where the tagger calculates there is insufficient evidence for safe dis  , ' unbiguation , and ( d ) with simplified ' plaintext ' malk-up for the human reader  .   ( See Tables I and 2 for examples of output formats . ) CLAWS 4 , the BNC tagger ,   2 incorporates many features of adaptability such as the above  . It " also incorporates many refinements of linguistic analysis which have built up over  14 years : particularly in the construction and content of the idiom-tagging component  ( see 2 below )  . At the same time , there are still many improvements to be made : the claim that ' you can put together a tagger from scratch in a couple of months '  ( recently hear data research conference ) is , in our view , absurdly optimistic . 
2 THEDESIGNOF THEGRAM-
MATICAL TAGGER ( CLAWS4)
The CLAWS4 tagger is a success or f the CLAWS 1 tagger described in outline in Marshall ( 1983 )  , and more fully in Garsid et al (1987) , and has the same basic architecture . The system ( if we include input , and output procedures ) has five major sections : ( a ) segnnentation of text into word and sentence units  ( b ) initial ( noncontextual ) part-of-speech assignment \[ using a lexicon , word-ending list , and various sets of rules for tagging unknown items \]  2CLAWS4 has been written by Roger Garside , with CLAWS adjunct software written by Michael Bryant  . 
622  ( c ) rule-driven contextual part-of-speech assignment ( d ) probabilistie tag disambiguation \ [ Markov process\]\[  ( c' ) second pass of ( c ) \]  ( e ) output in intermediate form . 
The intermediate form of text output is the form suitable for postediting  ( see 1 above ; also Table 1) , which can then be converted into other formats according to particular output needs  , as already noted . 
The preprocessing section ( a ) is not trivial , since , in any large and varied corpus , there is a need to handle unusual text structures  ( such as those of many popular and technical magazines  )  , less usual graphic features ( e . g . non-roman alphabetic characters , mathematical symbols ) , and features of conversation transcriptions : e . g . false starts , incomplete words and utterances , unusual expletives , un-planned repetitions , and ( sometimes multiple ) overlapping speech . 
Sections ( b ) and ( d ) apply essentially a Hidden Markov Model ( HMM ) to the assignme ut and disambiguation of tags . But file intervening section ( c ) has become increasingly important as CLAWS4 tuLs developed the need for versatility across a range of text types  . This task of rule-driven contextual part-of-speech assignment began in  1981 as an ' idiom-tagging'program for dealing , in the main , with parts of speech extending over more than one orthographic word  ( e . g . complex prepositions such as according to and complex conjunctions such as so that  )  . In the more fully developed form it now has , this section utilise several different idiom lexicons dealing  , for example , with ( i ) general idioms such as as much as ( which is on one analysis a single coordinator , and on another analysis , a word sequence ) ,   ( it ) complex munes such as Dodge City and Mrs Charlotte Green  ( where the capitaletter alone would not be enough to show that Dodgemad Green are proper nouns  )  ,   ( iii ) foreign expressions such as an nush or ribilis . 
These idiom lexicons ( with over 3000 entries in all ) can match on both tags and word-tokens , employing a regular expression formal is nlat the level both of the individual item and of the sequence of items  . Recognition of unspecified words with initial capitals is also incorporated  . Conceptually , each entry has two parts : ( a ) a regular-expression-based'template ' specifying a set of conditions on sequences of word-tag pairs  , and ( b ) a set of tag as-sigmnents or substitutions to be performed on any sequence matching the set of conditions in  ( a )  . Examples of entries from each of tile above kinds of idiom lexicon entry are:  ( i ) did/do \] does , (\[ XXXO/AVO/ORD\])2 , \[VVB\]VV I(it ) Monte/Mount/Mt NP 0 , (\[ WIC\])2 NP0 , \[ WIC\]
NP0(iii ) adAV021AJ021, hoe AV022AJ022
Exphmatory note: ( a ) Symbolic formalism : Let " IT be any tag , and let ww be any word . 
Let n , m be arbitrary integers . Then : wwTT represents a word and its associated tag  , separates a word from its predecessor\[TT\] represents : malready assigned tag\[WICI represents an unspecified word with a Word Initial Capilal " I"I'/'I'T means'either  '71" or TT ' ; ww/ww means ' either
WW or WW ' ww'13'TT represenls an unresolved ambiguity between " lq' and TI"TT * represents a tag wilh * marking the location of unspecified ch:u'acters  ( \[ TTI ) n represent she number of words ( up to n ) which may optionally intervene at a give q point in the template TTnm represents the ' dit to tag ' attached to ~ morthographic word to indicate it is part of a complex sequence  ( e . g . so that is tagged so CJS21, that CJS22) . The variable n indicates the number of orthographic words in the sequence  , and m indicates that the current word is in tile ttzth position in that sequence  . 
( b ) Ex ~ unples of word lags ( in the C5' basic'tagset ) :
AJ0 adjective ( tmm : uked)
AV0 adverb ( unmarked)
CJS subordinating conjunction
NP0 proper noun
ORD ordinal number
VVB finite base form of lexical verb
VVI infinitive of lexical verb
X X 0 negative mmker : not or n't
Rule ( i ) ensures that following a finite form of do and ( optionally ) up to two adverbs , negators or ordinals , a base form of the verb is tagged as an infinitive  . 
Rule ( ii ) ensures that in complex name such as Monte Alegre  , Mount Pleasant , Mount Palomar Obser-vatory , Mt Rush more National Memorial , all the words with word-initi , ' d caps are tagged as proper nouns . 
Rule ( i i l ) ensures that the Latin expression ad hoc is tagged as a single word  , either an adjective or an adverb . 
We have also now moved to a more complex , two-pass application of these idiom list entries . It is possible , on the first pass , to specify ambiguous output of an idiom assignment  ( as is necessary , e . g . , for as much as , mentioned earlier ) , so that this can then be input to the probabilistic disambiguation process  ( d )  . On the second pass , however , after probabilistic disambiguation , the idiom entry is deterministic in both its input and output conditions  , replacing one or more tags by others . In effect , this last kind of idiom application is used to correct a tagging error arising from earlier procedures  . For exanlple , a not uncommon result from Sections ( a ) - ( d ) is that the base form of the verb ( e . g . carry ) is wrongly tagged as a finite presentense form , rather than an infinitive . This can be retrospectively corrected by replacing VVB  ( = finite baseform ) by VVI ( = infinitive ) inappropriate circumstances . 
While the HMM-type process employed in Sections ( b ) and ( d ) affirms our faithin probabilistic methods , the growing importance of the contextual part-of -speech assigx waent in  ( c ) and ( c' ) demonstrates the extent o which it is important to transcend the limitations of the orthographic word  , as the basic unit of grammatical tagging , and also to selectively adopt nonprobabilistic solutions  . The term ' idiom-tagging ' originally used was never particularly appropriate for these sections  , which now handle more generally the interdependence between grammaticalnd lexical processing which NLP systems ultimately have to cope with  , and are also able to incorporate parsing information beyond the range of the one-step Markov process  ( based on tag bigram frequences ) employed in ( d )  .   3 Perhaps the term ' phraseological component ' would be more appropriate here  . The need to combine probabilistic 3We have experimented with a two-step Markov process model  ( using tag trigrams )  , and found little benefit over the one-step model ( using tag bigrams )  . 
and nonprobabilistic methods in tagging has been widely noted  ( see , e . g . , Voutilainen et al 1992:14) . 
3EXTENDING ADAPTABILITY :
SPOKENDATAAND TAGSETS
The tagging of 10 million words of spoken data ( including c . 4 . 6 million words of conversation ) presents particular challenges to the versatility of the system : renderings of spoken pronunciations such  , as ' avin ' ( for having ) cause difficulties , as dounplanned repetition such as I et , mean , Imean , 1 meantogo . Our solution to the latter pn~blem has been to recognize such repetitions by a special procedure  , and to disregard , in most cases , the repeated occurrences of tile same word or phrase for the purposes of tagging  . It has become clear that the CLAWS4 resources ( lexicon , idiomllsts , and tag transition matrix ) , developed for written English , need to be adapted if certain frequent and rather consistent errors in the tagging of spoken data are to be avoided  ( words such as I , well , and right are often wrongly tagged , because their distribution in conversation differs mazkedly from that in written texts  )  . We have moved in this direction by allowing CLAWS4 to ' slot in ' different resources according to the text type being processed  , by e . g . providing a separate supplementary lexicon and id lomlist for the spoken material  . Eventually , probabilistic analysis of the tagged BNC will provide the necessary information for adapting data structures at runtime to the special demands of particular types of data  , but there is much work to be done before this potential benefit of having tagged a large corpus is realised  . 
The BNC tagging takes place within the context of a larger project  , in which a major task ( undertaken by OUCS at Oxford ) is to encode the texts in a TEI-conformant markup  ( CDIF )  . Two tagsets have been employed : one , more detailed than the other , is used for tagging a 2-million-word Core Corpus ( a nepitome of the whole BNC )  , which is being postedited for maximum accuracy . Thus tagsets , like text formats and resources , are among the features which are task-definable in  CLAWS4  . In general , the system has been revised to allow many adaptive decisions to be made at runtime  , and to render it suitable for non-specialist researchers to use  . 
6244 ERRORRATES AND WHAT
THEY MEAN
Cun~ntly , judged in terms of major categories , 4 the system has an error rate of approximately 1 . 5%, and leaves c . 3 . 3%  , ' unbiguitics unresolved ( asportman-teau tags ) in the output . However , it is all too easy to quote error rates , without giving enough information to enable them to be properly assessed  . ~ We believe that any evaluation of the accuracy of auto-mattegrammatical tagging should take account of a number of factors  , some of which arc extremely difficul to measure :   4  . 1 Consistency It is necessary to measure tagging practice against some standard of what is an appropriate tag for a given word in a given context  . For example , is hor-rifying in a horrifying adventure , or washing in a washing machine an adjective , a norm , or a verb participle ? Only if this is specified independently  , by an annotation scheme , c : m we feel confident in judging where the tagger is ' correct ' or ' incorrect '  . 
For the tagging of the LOB Corpus by the earliest version of CLAWS  , the : umotation scheme was published in some detail  ( Johzmssou et al 1986 )  . We are working on a similar annotation scheme document  ( at present a growing inhouse document ) for the tagging of the BNC . 
4"lqqe rror rate and ambiguity rate are less favourable if we take account of errors and ambiguities which occtu " within major categories  . E . g . the porlmanteau tag NP0-NN1 records contidently that a word is a noun , but not whether it is a proper or common oun . If such cases are added to the count , then the estimated error raterises lo 1 . 78%, and the estimated ambiguity ale to 4 . 60% . 
5One reasonable attempt to evaluate competing accuracy of different aggcrs is that in Voutilaincn ct al  ( 1992:11-13 )  , where it is argued , on the basis of tlte tagging of sample written texls  , that the performance of the llcl sinki constraint grammar p : u'serENGCG is superior to that of CLAWS  1   ( the e:u-liest version of CLAWS , completed in 1983) , which is in turn is somewhat superior to Church's Parts tagger  . While recognizing that he accuracy of the tlels in ki system is impressive  , we note also that he method of evaluation ( in terms of ' precision ' and ' recall ' ) employed by Voutilainenctal innot easy to comp ~ u'e with the method employed here  . Further , a strict attempt at measuring compmability would have to take fuller account of the ' consistency ' and ' qu ' dily ' criteria we mention  , and of the need it ) compare across a broaderrt mge of texts , spoke ~ and written . This issuec , -umot be taken further in this paper , but will hopefully betile bzts is of future research  . 
4.2 Size of Tagset
It might be supposed that tagging with a finer -grained tagset which contains more tags is more likely to produc error than tagging with a smaller and cruder tagset  . Illtile BNC project , we have used a tagset of 58 tags ( the C5 tags ct ) for the whole cor ~ pus ,   , ' rodin addition we have used a larger tagset of 138 tags ( the C6 tagset ) 6 for the Core Corpus of 2 million words . The evidence so far is that this makes little difference to the error rate  . llut size of tagset must , in the absence of more conclusive evidence , remain a factor to be considered . 
4.3 Discriminative Value of Tags
Tile difficulty of grammatical tagging is directly related to the nuin ber of words for which a given tag distinction is made  . This measure may be called ' discriminative alue  '  . For example , in the C5 tagset , one tag ( VDI ) is used for the inlinitive of just one verb-- to do--where : ts ~ mother tag  ( VVI ) is used for the infinitive of all lexical verbs . On lheother hand , VDB is used for linite baseforms of to do ( including tile present tense , imperative , and subjunctive ) , whereas VVB is used of finite t ) ase forms of all lexic : d verbs . It is clc , ' u " the tags VDI and VDB have a low discriminative alue  , whereas VVI and VVB have a high one--since there are thousands of lexieal verbs in Englisb  . It is ~ d so clear that a tagset of the lowest possible discriminative value--one which assigned a single tag to each word and a single word to each tag--would b cutterly value less  . 
4.4 Linguislic Qualily
This is a very elusive , but crucial concept , tIow far are tile tags in a particular tagset valuable  , by criteria either of linguistic the t~ry /description  , r of usefulness in NLP ? For example , tile tag V D I , mentioned ill C . above , appears trivial , but it can be argued that this is nevertheless a useful category tbr English grammar  , where the verb do ( unlike its equivalent in most other Europe , ' m languages ) has a vet-yspecial ftmction , e . g . informing questions and negatives . On the other band , if we had decided to assign a special tag to the ved ~ become  , this would have been more questionable . Linguistic quality is ,   6The tagset figures exclude punctuation tags and port -manteau Rigs  . 
62 . 5 on the face of it , determined only in a judgemental manner . Arguably , in the long term , it can be determined only by the contribution a particular tag distinction makes to Success in particular applications  , such as speech recognition or machine-aided translation  . At present , this issue of linguistic quality is the Achilles ' heel of grammatical tagging evaluation  , and we must note that without judgement on linguistic quality  , evaluation in terms of b . and c . is insecurely anchored , It seems reasonable , therefore , to lump criteria b . - d . together as ' quality criteria ' , and to say that evaluation of tagging accuracy must be undertaken in conjunction with  ( i ) consistency \ [ How far has the armotation scheme been consistently applied ?\]  , and ( ii ) quality of tagging\[How good is the annotation scheme ?\]  .   7 Error rates are useful interim indications of success  , but they have to be corroborated by checking , if only impressionistically , in terms of qualitative criteria . Our work , since 1980 , has been based on the assumption that qualitative criteria count  , and that it is worth building ' consensual ' linguistic knowledge into the data structures used by the tagger  , to make sure that the tagger's decisions are fully informed by qualitative considerations  . 

Crowdy , S . ( forthcoming ) . The BNC Spoken Corpus . In G . Leech , G . Myers and J . Thomas ( Ed . ) . 
Spoken English on Computer . London : Long-mall.
Garside , R . , G . Leech , and G . Szunpson ( Eds ) . 
(1987) . The Computational Analysis of English : A Corpus -based Approach  . London : Longman . 
Johansson , S . , E . Atwell , R . Garside ~ mdG . Leech . 
(1986) . The Tagged LOB Corpus : User's Manual . Bergen : Norwegian Computing Centre for the Humanities  . 
Marshall , I .  (1983) . Choice of grammatical word class without global syntactic analysis : tagging words in the LOB Corpus  . Computers and the
Humanities , 17, 139-50.
V An example of a consistency issue is : How consistently is Time\[the name of a mag~ine\] tagged in the corpus ? Is it tagged always  NP0   (  , as a proper noun ) , or always NN1 ( as a common oun ) , or sometimes NP0 and sometimes NNI ? An example of a quality issue is : Is it worth distinguishing between proper norms and common nouns  , anyway ? Voutilainen , A . , J . Heikkil ~ i and A . Anttila . 
(1992) . Constraint Grammar of English : A Petformance-O riented lntroduction  . University of Helsinki : Department of General Linguistics  . 

Table 1: Tagg cd Spoken D~mt fiom the BNC-Vertictt l Format  0000203   070 I 03 PNP 0000203   080 can 03  \[  VM0/100\]   NNI%/0   0000203   090 just 03  \[  AV0/100\]   AJ0%/0   0000203   i00 take 98 WI 0000204   010 note 03  \[  NNI/99\] VVB/I 0000204   020 of 03 P R F 0000204   030 any 03  \[  DT0/100\]   AV0%/0   0000204   040 other 03  \[  AJ0/99\] NNI@/I 0000204   050 er 03 UNC 0000204   060 person a \] .   03   AJ0   0000204   070 pension 03  \[  NNI/100\]   VVB@/0   0000204   071   ,  03  ,   0000204   080 not 03   XX0   0000204   090 personal 03   AJ0   0000204   i00 pension 03  \[  NNI/100\]   WB@/0   0000204   i01   ,  03  ,   0000204   ii0 any 03  \[  DT0/97\]   AV0%/3   0000204   120 erm 03 UNC 0000205   010 other 03  \[  AJ0/98\]   NNI@/2   0000205   020 insurance 03 NNI 0000205   030 you > 03 PNP 0000205   031 ' re < 03 VIIB 0000205   040 got 98 VVN 0000205   041   ,  03  ,   0000205   050 just 03  \[  AV0/100\]   AJ0%/0   0000205   060 put 03  \ [  WB/66\  ]  VVD@/22   VVN@/13   0000205   070 it 03 PNP 0000205   080 on 03  \[  AVP@/62\]   PRP/38   0000205   090 there 03  \[  AV0/100\]   EX0/0   0000205   i00 and 97 CJC 0000205   i01   ,  96  ,   0000205   ii0 and 96 CJC 0000205   120 that > 97   DT0   0000205   121 ' s < 97  \[  VBZ/100\]   VilZ@/0   0000206   001 < unclear > 01 NULL 0000206   002 </ u > 01 NULL 0000207   001  *  '22  ; 2679 ; u01 NULL 00000207002 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 

Table 2: Tagged Spoken Data fi'om tile BNC -- Horizontal Format < s  c:"0000201   002"   n:00061> < ptr t:Pl3>   That&DT0  ; 's & VBZ ; what & DTQ ; < ptrt = Pl 4> I&P NP ; was & VBD ; told & VVN ; to & T00 ; bring & VVI ; and & CJC ; that & DT0 ; 's & VBZ ; what & DTQ ; 
I&P NP ; have & VHB ; brought & VVN; . & PUN ; </ U > < U id=D0027   who=W0000> < S C='0000203   002"   n=00062> Yeah & ITJ ;  , & PUN ; I&P NP ;' m&V BB ; , & PUN ; I&P NP ; ' ve & VHB ; got & VVN ; another & DT0 ; form & NNI ;  , & PUN ; I&P NP ; can & VM 0 ; just & AV 0 ; take & WI ; note & NN l ; of & PRF ; any & DT0 ; other & AJ0 ; er & UNC ; personal & AJ0 ; pension & NN l ;  , & PUN ; not & XX0; personal & AJ0; pension & NN l; , & PUN ; any & DT0 ; e~m & UNC ; other & AJ0 ; insurance & NN l ; you & PNP ; ' ve & VHB ; got & VVN ;  , & PUN ; just & AV 0 ; put & VVB ; it & PNP ; on & AVP-PRP ; there & AV 0 ; and & CJC ;  , & PUN ; and & CJC ; that & DT0 ; 's & VBZ ; < unclear > </ u > </ u >
