Feature Vector Quality and Distributional Similarity 
Maayan Geffet
School of Computer Science and Engineering,
Hebrew University
Givat Ram Campus,
Jerusalem , Israel , 91904

Ido Dagan
Department of Computer Science,
Bar-Ilan University
Ramat-Gan , Israel , 52900




We suggest a new goal and evaluation criterion for word similarity measures  . The new criterion-meaning-entailing substitutability-fits the needs of semantic -oriented NLP applications and can be evaluated directly  ( independent of an application ) at a good level of human agreement . Motivated by this semantic criterion we analyze the empirical quality of distributional word feature vectors and its impact on word similarity results  , proposing an objective measure for evaluating feature vector quality  . Finally , a novel feature weighting and selection function is presented  , which yields superior feature vectors and better word similarity performance  . 
1 Introduction Distributional Similarity has been an active research area for more than a decade  ( Hindle ,  1990) , ( Ruge ,  1992) , ( Grefenstette ,  1994) , ( Lee ,  1997) , ( Lin ,  1998) , ( Dagan et al ,  1999) , ( Weeds and Weir ,  2003) . Inspired by Harris distributional hypothesis ( Harris ,  1968) , similarity measures compare a pair of weighted feature vectors that characterize two words  . Features typically correspond to other words that cooccur with the characterized word in the same context  . It is then assumed that different words that occur within similar contexts are semantically similar  . 
As it turns out , distributional similarity captures a somewhat loose notion of semantic similarity  ( see Table 1 )  . By construction , if two words are distributionally similar then the occurrence of one word in some contexts indicates that the other word is also likely to occur in such contexts  . But it does not ensure that the meaning of the first word is preserved when replacing it with the other one in the given context  . For example , words of similar semantic types , such as company ? government , tend to come up as distributionally similar , even though they are not substitutable in a meaning preserving sense  . 
On the other hand , many semantic-oriented applications , such as Question Answering , Paraphrasing and Information Extraction , do need to recognize which words may substitute each other in a meaning preserving manner  . For example , a question about company may be answered by a sentence about firm  , but not about government . Such applications usually utilize reliable taxonomies or ontologies like WordNet  , but cannot rely on the ? loose ? type of output of distributional similarity measures  . 
In recent work Dagan and Glickman ( 2004 ) observe that applications usually do not require a strict meaning preserving criterion between text expressions  , but rather need to recognize that the meaning of one expression entails the other  . Entailment modeling is thus proposed in their work as a generic  ( application-independent ) framework for practical semantic inference . We suggest adopting such ( directional ) entailment criterion at the lexical level for judging whether one word can be substituted by another one  . For example , certain questions about companies might be answered by sentences about automakers  , since the meaning of automaker entails the meaning of company  ( though not vice versa )  . In this paper we adapt this new criterion , termed meaning entailing substitutability , as a direct evaluation criterion for the " correctness " of the output of word similarity measures  ( as opposed to indirect evaluations through WSD or distance in WordNet  )  . 
Our eventual research goal is improving word similarity measures to predict better the more deli-cate meaning entailment relationship between words  . As a first step it was necessary to analyze the typical behavior of current similarity measures and categorize their errors  ( Section 3 )  . Our main observation is that the quality of similarity scores nation region state * world island province * city territory area * town republicafrican_country*north*economy * neighbor * member * party * government * company * industry king domeuropean_countryplace colony Table  1: The 20 topmost similar words of country ( and their ranks ) in the similarity list by L in 98 , followed by the next 4 words in the similarity list that are judged as correct  . Incorrect similarities , under the substitutability criterion , are marked with ?*? . 
is often hurt by improper feature weights , which yield rather noisy feature vectors . We quantify this problem by a new measure for feature vector quality  , which is independent of any particular vector similarity measure  . 
To improve feature vector quality a novel feature weighting function is introduced  , called relative feature focus ( RFF )   ( Section 4 )  . While having a simple ( though nonstandard ) definition , this function yields improved performance relative to the two suggested evaluation criteria ? for vector quality and for word similarity  . The underlying idea is that a good characteristic feature for a word w should characterize also multiple words that are highly similar to w  . In other words , such feature should have a substantial " focus " within the close semantic vicinity of w  . 
Applying RFF weighting achieved about 10% improvement in predicting meaning entailing substitutability  ( Section 5 )  . Further analysis shows that RFF also leads to " cleaner " characteristic feature vectors  , which may be useful for additional feature-based tasks like clustering  . 
2 Background and Definitions In the distributional similarity scheme each word w is represented by a feature vector  , where an entry in the vector corresponds to a feature f  . Each feature represents another word ( or term ) with which w cooccurs , and possibly specifies also the syntactic relation between the two words  . The value of each entry is determined by some weight function weight  ( w , f ) , which quantifies the degree of statistical association between the feature and the corresponding word  . 
Typical feature weighting functions include the logarithm of the frequency of word-feature cooccurrence  ( Ruge ,  1992) , and the conditional probability of the feature given the word  ( within prob-abilistic-based measures )   ( Pereira et al ,  1993) , ( Lee ,  1997) , ( Dagan et al ,  1999) . Probably the most widely used association weight function is  ( pointwise ) Mutual Information ( MI )   ( Church et al . , 1990) , ( Hindle ,  1990) , ( Lin ,  1998) , ( Dagan ,  2000) , defined by : )()() , ( log ) , (  2 fPwPfwPfwMI = A known weakness of MI is its tendency to assign high weights for are features  . Yet , similarity measures that utilize MI showed good performance  . In particular , a common practice is to filter out features by minimal frequency and weight thresholds  . A word's vector is then constructed from the remaining features  , which we call here active features . 
Once feature vectors have been constructed , the similarity between two words is defined by some vector similarity metric  . Different metrics have been used in the above cited papers  , such as Weighted Jaccard ( Dagan ,  2000) , cosine ( Ruge ,  1992) , various information theoretic measures ( Lee ,  1997) , and others . We picked the widely cited and competitive ( e . g . ( Weeds and Weir ,  2003 ) ) measure of Lin ( 1998 ) as a representative case , and utilized it for our analysis and as a starting point for improvement  . 
2 . 1 Lin's ( ?98 ) Similarity Measure Lin's similarity measure between two words  , w and v , is defined as follows: , ) , () , ( ) , () , ( ) , (  ) ( ) (  ) ( ) ( ? ? ? ?? ?? + + = fvweightfwweight f v weight f w weight v wsim v F f w F f v F f where F  ( w ) and F ( v ) are the active features of the two words and the weight function is defined as MI  . A feature is defined as a pair < term , syntac-


Ranks Country-




















Table 3: The top10 common features for the word pairs country-state and country-economy  , along with their corresponding ranks in the sorted feature lists of the two words  . 
Feature Weight Commercial-bank , gen ? Destination , pcomp ? Air space , pcomp ? L and locked , mod ? Trade_balance , gen ? Sovereignty , pcomp ? Ambition , nn?Bourse , gen ? Politician , gen?Border , pcomp ? 8 . 08 7 . 9 7 7 . 83 7 . 79 7 . 78 7 . 78 7 . 77 7 . 72 7 . 54 7 . 53 Table 2: The top 10 ranking features for country . 
tic_relation > . For example , given the word ? company ? the feature < earnings_report  , gen ? > ( genitive ) corresponds to the phrase ? company ? searnings report ?  , and < profit , pcomp ? > ( prepositional complement ) corresponds to ? the profit of the company ? . The syntactic relations are generated by the Minipar dependency parser  ( Lin ,  1993) . The arrows indicate the direction of the syntactic dependency : a downward arrow indicates that the feature is the parent of the target word  , and the upward arrow stands for the opposite . 
In our implementation we filtered out features with overall frequency lower than  10 in the corpus and with MI weights lower than 4  .   ( In the tuning experiments the filtered version showed  10% improvement in precision over no feature filtering  . ) From now on we refer to this implementation as

3 Empirical Analysis of Lin 98 and
Vector Quality Measure To gain better understanding of distributional similarity we first analyzed the empirical behavior of  Lin98  , as a representative case for state of the art ( see Section 5 . 1 for corpus details) . 
As mentioned in the Introduction , distributional similarity may not correspond very tightly to meaning entailing substitutability  . Under this judgment criterion two main types of errors occur :  ( 1 ) word pairs that are of similar semantic types , but are not substitutable , like firm and government ; and ( 2 ) word pairs that are of different semantic types , like firm and contract , which might ( or might not ) be related only at a topical level . Table 1 shows the topmost similar words for the target word country according to  Lin98   . The two error types are easily recognized , e . g . world and city for the first type , and economy for the second . 
A deeper look at the word feature vectors reveals typical reasons for such errors  . In many cases , high ranking features in a word vector , when sorting the features by their weight , do not seem very characteristic for the word meaning  . This is demonstrated in Table 2 , which shows the top10 features in the vector of country . As can be seen , some of the top features are either too specific ( land locked , air space ) , and so are less reliable , or too general ( destination , ambition ) , and hence not indicative and may cooccur with many different types of words  . On the other hand , more characteristic features , like population and governor , occur further down the list , at positions 461 and 832 . 
Overall , features that characterize well the word meaning are scattered across the ranked list  , while many non-indicative features receive high weights  . 
This may yield high similarity scores for less similar word pairs  , while missing other correct similarities . 
An objective indication of the problematic feature ranking is revealed by examining the common features that contribute mostly to the similarity score of a pair of similar words  . We look at the common features of the two words and sort them by the sum of their weights in the two word vectors  ( which is the enumerator of Lin's sim formula in Section  2  . 1) . Table 3 shows the top10 common features for a pair of substitutable words  ( country-state ) and non-substitutable words ( country-economy )  . In both cases the common features are scattered across each feature vector  , making it difficult to distinguish between similar and non-similar word pairs  . 
We suggest that the desired behavior of feature ranking is that the common features of truly similar words will be concentrated at the top ranks of their vectors  . The common features for non-similar words are expected to be scattered all across each of the vectors  . More formally , given a pair of similar words ( judged as substitutable ) w and v we define the top joint feature rank criterion for evaluating feature vector quality :  ]  , ) , () , ([ ) , , (   ) ) ( ) ( (? + =? ??? fvrankfwrank n nvwranktop vFwFntopf where rank  ( w , f ) is the feature?s position in the sorted vector of the word w  , and n is the number of top joint features to consider  ( top n )  , when sorted by the sum of their weights in the two word vectors  . We thus expect that a good weighting function would yield  ( on average ) a low top-rank score for truly similar words . 
4 Relative Feature Focus ( RFF ) Motivated by the observations above we propose a new feature weight function  , called relative feature focus ( RFF ) . The basic idea is to promote features which characterize many words that are highly similar to w  . These features are considered as having a strong " focus " around w's meaning  . 
Features which do not characterize sufficiently many words that are sufficiently similar to w are demoted  . Even if such features happen to have a strong direct association with w they are not considered reliable  , as they do not have sufficient statistical mass in w's semantic vicinity  . 
4 . 1 RFF Definition RFF is defined as follows . First , a standard word similarity measure sim is computed to obtain initial approximation of the similarity space  ( L in 98 was used in this work )  . Then , we define the word set of a feature f , denoted by WS(f ) , as the set of words for which f is an active feature  . The semantic neighborhood of w , denoted by N(w ) , is defined as the set of all words v which are considered sufficiently similar to w  , satisfying sim(w , v ) > s where s is a threshold (0 . 04 in our experiments ) . RFF is then defined by : ???=) , () , ( ) ( )( v w sim f w R F F w N f WS v . 
That is , we identify all words v that are in the semantic neighborhood of w and are also characterized by f and sum their similarities to w  . 
Notice that RFF is a sum of word similarity values rather than being a direct function of word -feature association values  ( which is the more common approach )  . It thus does not depend on the exact cooccurrence level between w and f  . Instead , it depends on a more global assessment of the association between f and the semantic vicinity of w  . 
Unlike the entropy measure , used in ( Grefenstette ,  1994) , our " focused " global view is relative to each individual word w and is not a global independent function of the feature  . 
We notice that summing the above similarity values captures simultaneously a desired balance between feature specificity and generality  , addressing the observations in Section 3 . Some features might characterize just a single word that is very similar to w  . But then the sum of similarities will include a single element  , yielding a relatively low weight . 1 General features may characterize more words within N  ( f )  , but then on average the similarity with wover multiple words is likely to become lower  , contributing smaller values to the sum . A reliable feature has to characterize multiple words  ( not too specific ) that are highly similar to w ( not too general )  . 
4.2 Re-computing Similarities
Once RFF weights have been computed they are sufficiently accurate to allow for aggressive feature reduction  . In our experiments it sufficed to use only the top  100 features for each word in order to obtain optimal results  , since the most informative features now have the highest weights  . Similarity between words is then recomputed over the reduced vectors using Lin's sim function  ( in Section 2 . 1) , with RFF replacing MI as the new weight function . 
an average value , which might become too high by chance when computed over just a single element  ( or very few elements )  . 
 #Words Judge 1 (%) Judge 2 (%) Total (%) Top 1063 . 4 / 54 . 1 62 . 6 / 53 . 4 63 . 0 / 53 . 7 Top 2057 . 0 / 48 . 3 56 . 4 / 45 . 8 56 . 8 / 47 . 0 Top 30 55 . 3 / 45 . 1 53 . 3 / 43 . 4 54 . 2 / 44 . 2 Top 4053 . 5 / 44 . 6 51 . 6 / 42 . 0 52 . 6 / 43 . 3 Table 4: Precision values for TopN similar words by the RFF/  Lin98 methods . 
5 Evaluation 5 . 1 Experimental Setting The performance of the RFF -based similarity measure was evaluated for a sample of nouns and compared with that of  Lin98  . The experiment was conducted using an 18 million tokens subset of the Reuters RCV1 corpus , 2 parsed by Lin?s Minipar dependency parser ( Lin ,  1993) . We considered first an evaluation based on WordNet data as a gold standard  , as in ( Lin , 1998; Weeds and Weir ,  2003) . 
However , we found that many word pairs from the Reuters Corpus that are clearly substitutable are not linked appropriately in WordNet  . 
We therefore conducted a manual evaluation based on the judgments of two human subjects  . 
The judgment criterion follows common evaluations of paraphrase acquisition  ( Lin and Pantel ,  2001) , ( Barzilay and McKeown ,  2001) , and corresponds to the meaning-entailing substitutability criterion discussed in Section  1  . Two words are judged as substitutable ( correct similarity ) if there are some contexts in which one of the words can be substituted by the other  , such that the meaning of the original word can be inferred from the new one  . 
Typically substitutability corresponds to certain ontological relations  . Synonyms are substitutable in both directions . For example , worker and employee entail each other's meanings , as in the context ? high salaried worker/employee  ?  . Hyponyms typically entail their hypernyms . For example , dogentails animal , as in ? I have a dog ? which entails ? I have an animal ?  ( but not vice versa )  . In some cases part-whole and member-set relations satisfy the meaning-entailing substitutability criterion  . For example , a discussion of division entails in many contexts the meaning of company  . Similarly , the plural form of employee ( s ) often entails the meaning of staff . On the other hand , non-synonymous words that share a common hypernym  ( cohyponyms ) like company and government , or country and city , are not substitutable since they always refer to different meanings  ( such as different entities )  . 
Our test set included a sample of 30 randomly selected nouns whose corpus frequency is above guage  , 1996-08-20 to 1997-08-19 . 
500 . For each noun we computed the top 40 most similar words by both similarity measures , yielding a total set of about 1600 ( different ) suggested word similarity pairs . Two independent assessors were assigned , each judging half of the test set (800 pairs ) . The output pairs from both methods were mixed so the assessor could not relate a pair with the method that suggest edit  . 
5 . 2 Similarity Results The evaluation results are displayed in Table  4  . 
As can be seen RFF outperformed Lin98 by 910 percentage points of precision at all TopN levels  , by both judges . Overall , RFF extracted 111 ( 21% ) more correct similarity pairs than L in 98 . The overall relative recall3 of RFF is quite high ( 89% )  , exceeding L in 98 by 16 (73%) . These figures indicate that our method covers most of the correct similarities found by  Lin98  , while identifying many additional correct pairs . 
We note that the obtained precision values for both judges are very close at all table rows  . To further assess human agreement level for this task the first author of this paper judged two samples of  100 word pairs each , which were selected randomly from the two test sets of the original judges  . 
The proportions of matching decisions between the author's judgments and the original ones were  91  . 3% ( with Judge 1) and 88 . 9% ( with Judge 2) . 
The corresponding Kappa values are 0 . 83 (? very good agreement ?) and 0 . 75 (? good agreement ?) . 
As for feature reduction , vector sizes were reduced on average to about one third of their original size in the  Lin98 method ( recall that standard feature reduction , tuned for the corpus , was already applied to the Lin98 vectors ) . 
similarities found by each method relative to the joint set of similarities that were extracted by both methods  . 
Feature Weight
Industry , gen ?
Airport , gen ?
Neighboring , mod ?
Law , gen ?
Economy , gen ?
Population , gen ?
City , gen ?
Impoverished , mod ?
Governor , pcomp ?
Parliament , gen?1 . 21 1 . 16 1 . 06 1 . 04 1 . 02 1 . 02 0 . 93 0 . 92 0 . 92 0 . 91 Table 5: Top 10 features of country by RFF . 
5 . 3 Empirical Observations for RFF We now demonstrate the typical behavior of RFF relative to the observations and motivations of Section  3   ( through the same example )  . 
Table 5 shows the top 10 features of country.
We observe ( subjectively ) that the list now contains quite indicative and reliable features  , where too specific ( anecdotal ) and too general features were demoted ( compare with Table 2 )  . 
More objectively , Table 6 shows that most of the top10 common features for country-state are now ranked highly for both words  . On the other hand , there are only two common features for the incorrect pair country-economy  , both with quite low ranks ( compare with Table 3) . Overall , given the set of all the correct ( judged as substitutable ) word similarities produced by both methods , the average top joint feature rank of the top10 common features by RFF is 21  , satisfying the desired behavior which was suggested in Section  3  . The same figure is much larger for the Lin98 vectors , which have an average top joint feature rank of 105  . 
Consequently , Table 7 shows a substantial improvement in the similarity list for country  , where most incorrect words , like economy and company , disappeared . Instead , additional correct similarities , like king domandland , were promoted ( compare with Table 1) . Some semantically related but non-substitutable words  , like ? world ? and ? city ? , still remain in the list , but somewhat demoted . In this case all errors correspond to quite close semantic relatedness  , being geographic concepts . 
The remaining errors are mostly of the first type discussed in Section  3 ? pairs of words that are ontologically or thematically related but are not substitutable  . Typical examples are cohyponyms ( country-city ) or agent-patient and agent-action pairs ( industry ? product , worker ? job ) . Usually , such word pairs also have highly ranked common features since they naturally appear with similar characteristic features  . It may therefore be difficult to filter out such non-substitutable similarities solely by the standard distributional similarity scheme  , suggesting that additional mechanisms are required  . 
6 Conclusions and Future Work This paper proposed the following contributions :  1  . Considering meaning entailing substitutability as a target goal and evaluation criterion for word similarity  . This criterion is useful for many seman-tic -oriented NLP applications  , and can be evaluated directly by human subjects . 
2 . A thorough empirical error analysis of state of the art performance was conducted  . The main observation was deficient quality of the feature vectors which reduces the quality of similarity measures  . 
3 . Inspired by the qualitative observations we identified a new qualitative condition for feature vector evaluation ? topjoint feature rank  . Thus , feature vector quality can be measured independently of the final similarity output  . 
4 . Finally , we presented a novel feature weighting function , relative feature focus . This measure was designed based on error analysis insights and im- 


Ranks Country-














Table 6: RFF weighting: Top10 common features for country-state and country -economy along with their corresponding ranks in the two  ( sorted ) feature vectors . 
proves performance over all the above criteria.
We intend to further investigate the contribution of our measure to word sense disambiguation and to evaluate its performance for clustering methods  . 
Error analysis suggests that it might be difficult to improve similarity output further within the common distributional similarity schemes  . We need to seek additional criteria and data types  , such as identifying evidence for non-similarity , or analyzing more carefully disjoint features . 
Further research is suggested to extend the learning framework towards richer notions of ontology generation  . We would like to distinguish between different ontological relationships that correspond to the substitutability criterion  , such as identifying the entailment direction , which was ignored till now . Towards these goals we plan to investigate combining unsupervised distributional similarity with supervised methods for learning ontological relationships  , and with paraphrase acquisition methods . 

Barzilay , Regina and Kathleen McKeown . 2001.
Extracting Paraphrases from a Parallel Corpus.
In Proc . of ACL/EACL , 2001.
Church , Kenneth W . and Hanks Patrick . 1990.
Word association norms , mutual information , and Lexicography . Computational Linguistics , 16(1), pp .  22?29 . 
Dagan , Ido .  2000 . Contextual Word Similarity , in Rob Dale , Hermann Moisland Harold Somers ( Eds . ) , Handbook of Natural Language Processing , Marcel Dekker Inc ,  2000 , Chapter 19 , pp . 

Dagan , Ido and Oren Glickman .  2004 . Probabilistic Textual Entailment : Generic Applied Modeling of Language Variability  . PASCAL Workshop on Text Understanding and Mining . 
Dagan , Ido , Lillian Lee and Fernando Pereira.
1999 . Similarity-based models of cooccurrence probabilities  . Machine Learning , 1999, Vol . 
34(1-3), special issue on Natural Language
Learning , pp . 4369.
Grefenstette , Gregory .  1994 . Exploration in Automatic Thesaurus Discovery . Kluwer Academic

Harris , Zelig S .  1968 . Mathematical structures of language . Wiley , 1968 . 
Hindle , D .  1990 . Noun classification from predi-cate-argument structures  . In Proc . of ACL , pp . 

Lee , Lillian .  1997 . Similarity-Based Approaches to Natural Language Processing  . Ph . D . thesis , Harvard University , Cambridge , MA . 
Lin , Dekang .  1993 . Principle-Based Parsing without Overgeneration . In Proc . of ACL93, pages 112-120, Columbus , Ohio , 1993 . 
Lin , Dekang .  1998 . Automatic Retrieval and Clustering of Similar Words  . In Proc . of COLING?ACL98 , Montreal , Canada , August ,  1998 . 
Lin , Dekang and Patrick Pantel .  2001 . Discovery of Inference Rules for Question Answering  . 
Natural Language Engineering 7(4), pp .  343-360, 2001 . 
Pereira , Fernando , Tishby Naftali , and Lee Lillian . 
1993 . Distributional clustering of English words . In Proc . of ACL93, pp .  183?190 . 
Ruge , Gerda .  1992 . Experiments on linguisti-cally-based term associations  . Information Processing & Management , 28(3), pp .  317?332 . 
Weeds , Julie and David Weir .  2003 . A General Framework for Distributional Similarity  . In
Proc . of EMNLP03. Spain.
nation state island region area * neighbor colony * port republicafrican_country province * city * townking domeuropean_country zonel and place Table  7:   Top20 most similar words for country and their ranks in the similarity list by the RFF-based measure  . Incorrect similarities ( non-substitutable ) are marked with ?*? . 
