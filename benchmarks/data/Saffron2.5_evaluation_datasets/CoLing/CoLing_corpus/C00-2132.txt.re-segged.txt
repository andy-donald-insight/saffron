?1") " AM et ; hod for Ac ( : eleratilg CFG-lmsing / ) y\[Js\]ng\] ) ependency

Hideo\?atm Jalm
IBM lh's carch,' . l . ' okyolh ~' sc ' arch Lal ) oral : or y
\]623-1d Shimotsuruma , Ymnato , \]( anagawa 2428502 , Jalmn
wat an abt ~((0 trl . ilml . (: o.ji )
Abstract
'\] . 'hi . qlmlmrd ( , scrib( ; san algorithnl for accc'lerat-ingl ; h ( ; CF(~'qm . rsing t ) ro(:t ; ss by using ( lel )( ; nd( ; ncy ( or modifier-nlodifie ( ; relationship ) infornmtion given by , for in si , an (: e , d (' . llcnd('alcycstimal , ionl ) rogramsSll(:hassl ; o(:\]ulsl ; i(:1) arsers ~ llSCl' ;  , qJltdic ; d;ioninaninl ; ( , ra(;tiv (' , al)t)li(:al;il)n , mM\]inguisl ; icmmotal ; i(nm ; t(hh:(1 in as our (:(' l ; ( . ' xl ; . This is a ml ; l ; hod for (' . n-\]mn(:ingexi , % ing grmnmard/as (' , dCF(\]-l ) arsing sys-Wan by using dc'tmnden ( ; y informal ; ion . 
_1. Introduction
Th (' parsing sysl ; O . lilis ; i , key co111 t ) o11 (111;\]' or 11 at-tual language , ai)i/lication such as machine trans-lal ; ion , informal ; i on rel ; rJ(wal , l ; cxt , '-; unllnariz ; ll ; ion , and its l )( ; r for nlml ( : ( ; (\]) roct ; ssintt ; speed and act : u-racy ) is very in q ) or l ; antol ; h(!success of l ; lms (' ap-pli , ::ations . 
Timumm\]CF ( Ltmrsin / ~ algorithlns\[3, 6\]k(, . (! pall in term (' . dJat ( ; l ) ossibiliti ( ~s which may or may not t ) ctls ( xlill timtinal pm:ser (  ;  , qults . Tlmr ( ~ for (!, we usually reduce 1; hi ; s ( ; illl ; ( . ' l : lllc dial ; o , l ) ossibiliti('s which are unlikely to t )( ; used astinal results in the nlid-die () fl ; he process l ) y using s ( ; vt ; rall ) rmlingt('x:h-ni(luCS . One good information , sour (: (' ~ ~ for pruning is d ( ; t)end( ; n (' y information ) c . tw(' . cn words . It has nol ; l ) ( ; ( mso easyl ; ogel ; such d('l ) (' n(h ' . ncy information until a . few years a . go , but , th(;sil;uat ; ion has ret : ( ml ; ly chang ( , ,d . 
Recent intensive studies on statistical all l ) roach\[7 ,  1 , 2\]a ( lvanccxl statistical parsing systems , and wc can gel , relatively correct dct ) en ( h'ncy information using these systems , leurth c'r , if we SUl ) t ) os can interactive NLP system , then the reaa ( , sore ( , types of user intera (: tions which can b ( ; considered to determil m1 ; 11( ; modifice c ; mdid at c . I 11 addition , recent studies on the linguistic in fi ) rnmtion mmo- (  ; a . l;ion\[10,4,12,1 . 3\] provid ( ; tools l/y which a user can ( ; asily annotate linguistic int brnmtion ( si/ecial XML markup tags ) into source texts , and we can OX\[)(X ; I ; ; 0 , q tX ) a , ll increase of tho 111 1111) (11 of l ; exi ; swil ; h linguistic information . This linguistic in for-nlai ; ion usually includes depend tmc , y infornml ; ion . 
For instmmc , the following example shows m ~& llllO-( ; al ; ion (' xaml/h'by Linguistic Annol ; at ionlmnguag ( ; described in\[12 ,  13\] , and the id and rood at l ; l'i ) ll ; c , q insid et al : w ( , hmmifl ; spc . ci\[\[yword dependencies . 
IIe(lal:wid="1") saw(/lal:w)aman(lal:w , nod="1") with (/ lal:w ) at c'l (' , scolm . 
in this (; x anll ) h' , the word " with " modifi ( ; sl ; he word As shown in l ; hc , above ( ~ xample , % we can now get depc . n d c n c y i n l b r n m t i o n m o r e e a s i l y th a n a t i ; wyears ago . This paper describes an algorithn i for accel crnting CFG-lmrsing systems by using su  ( : hd (  ; pcnd( ; ncy ( or modifier-moditi ( ; er(~lationship ) information . Th ( ; prot ) oscd algorithm does not assume all words are given dctmndency int ' ormation ~ ratht  ; ritworks in case such that some of words are partia  . lly given dep(;ndt',ncyinfl ) rnm . tion . 
2Ol ) timizing Algorithm Using Dependency Infornmtion We use a  . nornml CFG lmrsing sysi ; (' m with one '( ; xl ; ( ; nsi on that for ( m . t'hru\]c(here,mus the . o11 crighl ; -h ; mdsid ('( or\]ITS ) t('rmImark ( , dasah ( , a d , and th (' . informati(m ( if a head term is trmlsJhrr (' . ( lLol ; hc . 
lc . ft-hm Mside ( or HIS ) tenn . In this lmtmr , a CI eG rule is ( hmol ; ed as follows : x-~~q . . .  ~  .   .   .   .  ~ . ; ,  ( , , , >0) Intim above , notation , X is dmleft-lmndside ( or LHS ) term , mMI5-are right hand side ( or lllH $) terms , m MaRHS term followed by an asterisk '*' is a head term  . Thel ; ypical usage of the head is that the LHSt ( ; nn shares many features of the head term in the RHS  . For instmme , a matching word of the the LHS tcnn becomes the same as the one of the head term in the RHS  . 
For each rule , an arc is constructed over a word segment in a . n input sentence . Anaa ' cisd (' a lot ( , dusing terms of its base rule as follows : Ix-~ ~ q  . . . E - .  ~1+,*  . . .  5 ,  \ ]  1A term expresses a nonterminal symbol in IAIS , an (1~' ~ non-terminld or a terminal symbol in l/ . IIS . 

The LHS term of an arcnmans the LHS term of the base rule of the arc  , and RHS terms of an arc means RHS terms of the base rule of the arc  . 
In the above notation , a single dot indicates thai ; RHS terms located to the left of a dot are inactive  , that is , they already match the LHS term of some other arcs  . Three dots are used to ret ) resent zero or any number of terms . An arc whose RHS terms are all in active is called an inactive arc  , otherwise it is called an active arc . An arc covers a segment of input words ; the start point of an arc is the index of the first word in the covering segment  , and the endpoint of an arc is 1 plus the index of the last word in the covering segment  . 
Basically , a standard CFG parsing algorithm such as \[3 , 6\] consists of the following three operations . 
Initialization : For each word , arcs are generated froln rules such that the leftmost RHS term matches it  . 
Operation A : For each in active arc A , an arc is generated fl'om A and a rule R such that the leftmost RHS term of R  , natch cs the LHS term of A . 
Operation B : For each in active arc A , an arc is generated from A and another active arc B such that the leftinost active RHS term of B matches the LHS term of A and the end t  ) oint of B is the stone a stile start point of A . 
We assume that some dependency information 1 ) etween words are given , and such det ) endency information is denoted as follows : w . ~ w , The first of the above examples represents that a word I/Vu modifies another word I ~  ( ~ , at tdW ~ , precedes 14~j , while the second one represents that a word Rq , modifies another word H~j and W , , precedes 1/1~/ . 
Given this kind of dependency information , the following conditions are imposed on Operation A and Operation B  . 
Conditions for Operation A :
Condition A1 ( when the leftmost RHS term of a rule is a head term  ) :
Given an inactive arcArc1 denoted by \[ A ~ . . . \] and a rule which has two or more RHS terms and the leftmost RHS term is a head denoted by X-+ A*B  . . . , Operation A is executed only if there is dependency information  144  , lYb where 1 , 14 ~ is a word matching the
LHS term A of Arci and l Vb is a word located anywhere to the right of the end 
I ) oint of Arc1.
X -> A * B ...
WaWD-..........?--"
Figure 1: Condition A1
Figure 1 shows the above condition . In this figure , a thick arc ret ) resents an inactive arc , a line represents a matching to be tried in this ot  ) eration , a dotted line represents a matching betweeu a term in an arc and a word  , and a dotted arrow represents dependency infbrmation  . In this case , this type of rule implies that a word matching the LHS term of the arc to be matched with the leftmosterm of the rule must be modified by any word which is located after the end t  ) oint of the arc , since the head term is the left ; mosterm of the rule . Therefore , if the A1 condition does not hold , Operation A is not required to be executed . 
Condition A2 ( when the leftmost RHS term of a rule is not a head term  ) :
Given an inactive arcArc1 denoted by \[ A--+ . . . \] and a rule which has two or more RHS terins and the leftmost IHS term is not a head denoted by X --+ A  . . . D * . . . , Operation A is executed only if there is a dependency information  14~  ~  1?~ where 1/1~ is a word
In atching the LHS term A of Arc1 and
Wv is a word located anywhere after the endpoint of  Arc1  . 
Figure 2 shows the above condition . In this case , this type of rule ilnplies that a word matching tile LHS term of the arc to be matched with the leftmost term of the rule Inustinodify any word which is located after the endpoint of the arc  , since the headter in is not tile leftmosterm of the rule  . 
Conditions for Operation B:
Condition B1 ( when the leftmost active RHS term of an active arc is the head term  ) :
Given an active arcA reade noted by \[ X--+A o . . . A  ~ . B .   .   .   . \] and an in-active arcArc1 denoted by \[ B-+ . . . \]
WaWb
Figure 2: Condition A2 such that the endpoint of Area is the santo as the start point of Arcr  , Ol ) era-tionB is executed only if , for each l , lz , ~  ( 0 < i < n ) which is a word matching the RItStermA~of Are A  , there is dependency information I'V ai=>I'V b , where W i , is a word matching the LHS term B of Arc ,  . 

, ::-, V/\
Wao . . . Wa , , Wb ""-- .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . -" l?igure 3: Condition B1 Figure 3 shows the above condition . Ill this figure , ~dotted thick arc represellts an active m : c . hith is case , this type of active arc implies that words matching in active terms before ' the head term of the active art : must modify a  . wordmatdfing tile
LHS term of the inactive arc.
Condition B2 ( when the head term is on the left side of the leftinost active RHS term of an active arc  ) :
Given an active mcA reA denoted by \[ X ~ . . . A * . . . B . . . \] and all in active arcArc1 denoted by \[ B--+ . . . \] such that the endpoint of Area is the same as the start point of  Arc1  , Operation B is executed only if there is de-t ) endency information W , , ~ Wb where 144~ is a word matching the RHS term
A of AreA , and Wv is a word matching the LIt Sterln B of Arc1  . 

IX->,"l "

WaWb
Figure 4: Condition B2
Figure 4 show stile above condition . In this case , this type of active arc implies that a word l natching the LIIS term of the inactive arc nmst modi ~ a word matching the head tcrin of the active arc  . 
Condition B3 ( when the head term is on the right side of the left l no stactive RItS term of an active arc  ) :
Given an actiw ; arcA reade noted by \[ X ~ A . B . . . C * . . . \] and an inactive arcArc1 denoted by \[/3-+ . . . \] such that the endpoint of Area is the same as the start point of Arc  , , Operation B is executed only if there is dependency in fofnmtion Wb ~  14~ where Wf , is a word matching the . LIt Sterm B of Arcl , and l'14: is a word on the right side of the , endpoint of Arci . 
Wb , . I , " Wc
Figure 5: Condition B3
Figure 5 shows the above condition . In this case , this type of active arc implies that a word matching the  . LHS term of tile in active arc must modify a word after the endpoint of tile in active arc  . 
The dependency information is not necessarily given to all words  . If there is any source word except for the root word of a sentence such that there then a set of such del  ) endeney inibrmation is called partial , otherwise , it is called total . If the given de-1 ) endency informatiol l is partial , the A\]condition cannot be used , since , even if there is node t ) en-dency information targeting I . V , , , we eanllot know if such del ) endency information does not really (  , xist , or if such delmndency inlbrmation is llotSul ) plied . 
For other conditions , we check them only when all source words for dependency checking have dependency information  . On the other hand , if the given dependency information ix total , all conditions are checked . 
3 Experiment
We have imt ) lemented the 1 ) reposed algorithm into an existing English CFG -parser we have developed for a machine translation t  ) roduct \[8 ,  9 , 11\]e , and conducted an experinmnt to know the effectiveness of this algorithm  . 
We selected 280 test sentences rmx domly from a sentence set created by  . \]EIDA : ~ forew fluating translation systen L and made the correct dei  ) en-- ( lency relation data for these selected test sentences  . 
We collected the number of inactive arcs , the num-b (; r of active arcs , and the t ) rocessing time for cases such that C modifiee candidates  ( one of which is the correct modifiee ) are given to a word . 4 If C := I then it ; corresponds to the best case for a parser such that only one correct modifiee is given fin ' each word  , while if C is 3 or 4 then it ; corresponds to the approximation of using a statistical modifiee esti-ination program for getting candidate modifiees  . 
The graphs in Figure 6 indicate the reduction ratios of active arcs , in active arcs , and 1 ) recessing time for using conditions for total dependency information and conditions t br partial del  ) endeney information . The de'nominators for calculating these r&tios are the number so far  ( : s and the processing time ( seconds ) in case of the parser without this algorithm . In these graphs , C = X indicates that X is the maxiln unlnuln ber of moditle e candidates given to a word  . 
From these gratlhs , we can so (; that the more words in a sentence , the better the 1) erformance . 
In a real domain , most sentences consist of more than ten words . Therefore , looking at values for around 10 in the X axis , we can see that in active arcs are reduced by about  40% and 25%  , active arcs 2This parser is used in a Web page translation software called " lnternet King of  3t'anslation " released from IBM . laI ) an . 
a . lal ) all Electronic Industry \] ) ev cloi ) ment Association 4Modifiee candidates are selected randomly except for the correct  oi1o  . 
are reduced by about 65% and 35% , and t ) rocessing time is reduced by about ~15% and 15% , for the ideal case ( C1 ) and more practical cases ( C = 3 or 4 )  , respectively , in the (: as (' . of total del ) endency information . Please note that , since the 1 ) arser in which this algorithmix impleumnted has already several pruning mechanisms  , we can expect more reduction ( or pertbrmance gain ) for generic CFG pars ( ' , rs . 
4 Discussion
As a study for accelerating the parsing tu'ocess using dependency information  ,   Imaichi\[5\] reported an algoritln n for Japanese language . The conditions introduced by hnaichiare described by using the notation in this paper as ~ bllows: 
Condition MI:
Given an active arcA reade noted by \[ X-~A .  13 . \] and an inactive arcArc1 denoted by \[ B-+ . . . \] such that the endpoint of Area is the same as the start point of Arcj  , Operation B is executed only if there is dependency in fl  ) rmat . ionI < 1 ~ , -=> lYb where 1'15~ is a word matching the RIIS term A of AreA , and lVt , is a word mat (: hing the I~HS term , r3 of mrcl . 
Con(lition M2:
Given an in active ar (: Arc1 denoted 1) y\[A -+ . . . \] and a rule denoted by X--~A . . . , Operatioil A is execllted only if there is no det  ) endency i if l ' ormation Wt . => l'-,~where1 . ' I ~ ix a word matching the LHS term A of Arc1 and lYt . is a word loca . ted before the start point of
A ~ r c l .
The condition M1 correspouds to B1 . Since hnaie hi's algorithln considers only . Japmmsein which all words other than the last ; word modifies one of the succeeding words , it does not deal with cases usually seen in Eurot ) e all languages where a word modities one of the preceding words  . Therefore , it is not applicable to any language other than Jat  ) an ese in general . Fnrthcr , since a CFG rule is restricted to be in Chomsky normal form  , hnaichi's algorithm is limited in terms of at ) plicability . 
Since the algorithm proposed in this pal ) er does not have any restrictions on the dependency direction and the CFG rule format  , it can be applicable to any CFG-parsers illany languages  . 

Reductiof Ratioo\[In active Arcs for Total Dependency  Inf0  . 
6085 o'~\[40
OqJ 456-!89 101 112
NUt'rl01"WOIL~;

Reduction I~atio of Active Arcs for lo Lal Dependency Info  . 
80 ' ~0 J , 50ft-=40o~:20\[J\]JG=1
I ~0=2\[\] O~-3
I ~0:-4
Nl.lm of ' lil ' ~ l ' Orl\[~
O0\[\]\]JO = l . ~  0--2  \ [ \ ]  0=3  \[ \ ]  0=4 for Total Dependency Info . 
~.5 O'~40~0~30,\]3 ~ clO45678g 101112
Nurl ' l of " ~ l'l , l ' Orlj 4, .   1:  \ [ \ ]  0=1  \ [ \ ]  O--2  \ [ \ ]  0--3  \ [ \ ]  O=4 
Reduction Raito of In active . Arcs\[or Partial Dependency In \[ o . 
60 ~5 o
E 10456 189 1011
NLIF r'IE lf'l,ll,?Oi"l.~-:(t,)t21O=2

F _1   0=4 for-Partial Dependency Info 5O   ~40 oo " ~ 20 " Uj 4   5   6   7   8 g 10   11 
NU ~'\[\], : If ), ll , tClrl ~: ~:( d)~O = l

O = 4 oail O
Reduction Ratio of lime for Partial Dependency Info  . 

Num , : , fW0rd~.
(,;)(f)~\]0 = 1
UO=2 ~ G=36 = 4
Figure . 6: leducl ; ion ratioso\[inaci . ivearcs , acl ~ ivearcs , a . ml processing Lime We developed an algorithm for accelerating the performance of the CFG t  ) arsing process if we are given dependency information  . From an experiment , we can show the effectiveness of this algorithm . 
By using this algorithm , we can enhance xist-ing grammar-based parsers using dependency information given by stochastic parsers  , interactive systems , and texts created by linguistic annotation systems  . 
References\[1\]M . Collins . A new statistical parser based on bigram lexical dependencies  . In Proc . of 3~fl ~
AC ?, pages 184-191, 1996.
\[2\] M . Collins . Three generative , lexicalized models fbr statistical parsing . In P~vc . of 35th ACL , pages 1623, 1997 . 
\[3\] J . Earley . An efficient contextfree parsing algorithm . In Readings in Natural Language Pro-cess in 9 . Morgan Kauflnan , 1969 . 
\[4\]K . Hashida , K . Nagao , et al , Progress and Prospect of Global Document Annotation  . ( in Japanese ) In P~vc . of ~ th Annual Meeting of the Association of Natural Language Processing  , pp .  618-621, 1998 . 
\[5\]O . Imaichi , Y . Matsumoto , and M . Fujio . An integrated parsing method using stochastic information and grammatical constraints  . Journal of Natural Language Prvcessing ,  5(3):67-83 ,  1998 . 
\[6\] M . Kay . Algorithm schemat and data structure in syntactic processing  . Technical Report;
CSL-80-12, Xerox PARC , 1980.
\[7\] D . M . Magerman . Statistical decision-tree models fi ) r parsing . In Prvc . of 33rd ACL , pages 276-283, 1995 . 
\[8\]K . Takeda . Pattern-based contextfree gram-inars for machine translation  . In Proc . of 3~th
ACL , pages 144-151, 1996.
\[9\]K . Takeda . Pattern-based machine translation . 
In Proc . of 16th Coling , volume 2, pages 11551158, 1996 . 
\[10\] Text Encoding Initiative ( http://www . uic . edu:80/orgs/tei/)\[11\]H . Watanabe and K . Takeda . A pattern-based machine translation system extended by example-based processing  . In Proc . of 17th Coling ( Coling-ACL'98) , volume 2 , pages 1369-1373 ,  1998 . 
\[12\]H . Watanabe , Linguistic Annotation Language-The Markup Language for Assisting NLP Programs-  . IBM Research Report
I/T0334, 1999.
\[13\]H . Watanabe , K . Nagao , et al , Linguistic Annotation System for Improving the Performance of Natural Language Processing Programs  . In Proc . of 6th Annual Meeting of The Association for NLP ( in Japanese )  , pp .  171-174, 2000 . 

