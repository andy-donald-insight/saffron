Statistical and Linguistic Strategies in the
Computer Grading : of Essays
Ellis B . Page
University of Connecticut
Storrs , Conn , , U.S.A.
Essay tests are used in the schools and colleges of all nations  , and in major testing programs of national and eveh international size  . Potentially , such essay tests are an important applied field for computational linguistics  , and . should eventually provide focus for much work . Yet in the past , little direct attention has been paid to such grading  , although there are ways to begin investigation which would not necessarily require much linguistic knowledge beyond that now available  . 
Beginning in December of 1964 , Project Essay Grade ( PEG ) , at the University of Connecticut , has investigated the c0mpute~lanalysis and evaluation of student writing . In February ,  1965 , the project was given pilot funding by the College Entrance Examination Board of New York City  , and in June ,  1966 , the United 3tares Office of Education gave it much larger ~ upport . 
Zhrough this period of preliminary investigation , icer-tain problems have become much better understood  ( Daigon , 1966; Page ,  1966 ,  1967) . ~his paper discusses these problems , relates certain major findings to date , and outlines apparently promising avenues for future work by linguists  , computer scientists , psychologists , and educators . 

It ' is useful to conceptualize the field of essay grading in two dimensions  , as represented in Figure 1;
Figure 1 liossible Dimensiens of Essay Grading


Rating ~( A)I(A)


Master I08) II(B )

Any serious effort to grade essays must obviously face problems of " content " as in Column I  , and of " style " as in Column II . Yet it is obvious that these columns are not mutually exclusive  . Similarly , the rows are not mutually exclusive either ; but their general meaning must be mastered to understand the work to date and the ~ problems of the field  . The first row refers to the simulation of the human judgment  , without great concern about the way this judgment was produced  . The second row refers to the accurate , deep , " true " analysis of the essay . 
We have coined two terms to describe this difference  . Since the top row is concerned with approximation  , we speak of the computer-variables employed as ~ . 
Since the bottom row is concerned with the true intrinsic " variables of interest  , we speak of such variabl-~a strins . Atrin , then , is a variable of intrinsic interest to the human judge  , for example , " aptness of word choice " . Usually a trin is not directly measurable by present computer strategies  . And a prox is any variable measured by the computer  , as an approximation ( or correlate ) of some trin , for example , proportion of uncommon words used by a student ( where common words are discovered by a list lookup procedure in computer memory  )  . 
In the early par ? of our investigations , we concentrated on the right column and top row of Figure i  , looking for actuarial strategies , seeking out those proxes which would be of most immediate use in the simulation of the final human product  , the ratings of stylistic factors . 
For the first attempts , we evolved a general research design , which we have more or less followed to date : ( i ) Samples of essays were judged by a number of independent experts  . For our first trial 272 essays , written by students in Grades 8 to 12 in an American high school , and judged by at least four independent teachers . These judgments of overall quality formed the trins  . 
(2 ) Hypotheses were generated about the variables which might be associated with these judgments  . If measurable by computer , and feasible to program within the logistics of the study  , these computer variables became the proxes of the study  . 
(3 ) Computer routines were written to measure these proxes in the essays  . These were written in FJORTRANIV , for the IBM 7040 computer , and are highly modular and mnemonic programs , fairly well documented and available to computational linguists interested in using them or adapting them  . 
(4) Essays were prepared for computer input . In the present stage of data processing , this means that they were typed by clerical workers on an ordinary key-punch  . They were punched into cards , and these cards served as input for the next stage  . 
(5) The essays were passed through the computer , under the control of the program which collected data about the proxes  . The output was as appears in Figure 2 . 
PEG-L4 Output
OwTHINKIHATFPEOPLE WOULD LlYEHO\[HEIR lANK  8i  ) UKEVERY QNEN QQLOIE 8E0IOZI 19SI 106 . 8826676 O0i ! 00000200, k00ZO ~ .   .   .   .   . 1CAP(N , , % TncLvw . ~ Ak , l ~ Luz ? ~ uS ? r . = u = , ~ k .   . lu ? u ? & uu . uu . v ? . u
GODFOR\[HAT0~ )   10E   1   E1 S 1 ot 46z   E1   44t   0 tZ 1   0   0   0  ?  0   0   0   0 Z 0   0  ~ )   102 IELSIZ 149T   t365   371   768S   0   4   26 El 0   3   0   0   0   8   0   0   32   2 Zk 0 t " ~ i1021-   -1~-17 I "= . ST *= k . & * u . lu * ~ , .  0  . C , . G .  &*  . & oJ , .  &  . ' ~ k ~ IOZ 2O .  9  . hl * 3 . E .  06 . tO0 .  100 . O .  3 . O .  404 .  109 . t3*
INLIFEiREFREE * ? THE
O ~ EmS ~ , ~ . u , vex .   . oac ~ M ,  ?  ; "lu=~i?uvz*=u ~ cqu ~ uu ~ ~ uuuuu zvu ~ uu ? Figure  2 shows a piece of output from PEG-IA . Line A shows the way a sentence from the student essay is rewritten in  12-character double-precision computer " words " and stored in memory  . Line B shows the summary of data for that sentence just analyzed  . The first number is the essay identification . The other numbers of Line B are some counts from that sentence  . Line C shows a summary of these counts , across sentences , for this whole essay . And Line D are these measures transformed in a number of simple ways  , and ready for input into the final analysis . 
(6 ) These scores were then analyzed for their multivariate relationship to the human ratings  , were weighted appropriately , and were used to maximize the prediction of the expert human ratings  . This was all done by use of standard multiple -regression programs  . 
The first analyses produced results as shown in Tablei  . Here it is possible to read the list of proxes ( Col . A ) , " and their correlation , after transformation , with the human judgments of overall quality ( Col . B ) . 
Col . C shows their contribution to the total multiple regression  , and Col . D indicates the test-retest relia-bility of the proxes themselves  , as discovered from two essays written by the same students  , with about a month between writings . 
Table 1"
Variables Used in Project Essay Grade IA for a Criterion of Overall Quality 
A . B .
Proxes Corr . with
Criterion 1 . Title present . 04 2 . Av . sentence lngth . 04 3 . Number of paragraphs . 06 4 . Subject-verb openings - . 16 5 . Length of essay in words . 32 6 . Numbero ~" parentheses . 04 7 . Number of apostrophes- . 23 8 . Number of commas . 34 9 . Number of periods -- . 05 10 . Number of underlined words . 01 11 . Number of dashes . 22 12 . No . colons . 02 13 . No . se/nicolons . 08 14 . No . quotation marks . 11 15 . No . exclamation marks- . 05 16 . No . question marks-?i417 . No . prepositions . 25 18 . No . connective words . 18 19 . No . spelling errors -- . 21  . 
20 . No . relative pronouns . 11 21 . No . subordinating conjs .  -- . 12 22 . No . common words on Dale-- . 48 23 . No . sents , endpunc . pres .  -- . 01 24 . No . declar , sents , type A . 12 25 . No . declar , sents , type B . 02 26 . No . hyphens . 18 27 . No . slashes- . 07 28 . Aver . word length in Itrs .   . 51 29 . Stan . dev . of word length . 53 30 . Stan . dev . of sent . length - . 07
C . O.
Betawts . Test-Ret . Rel.
( Two essays ) . 09  . 05 -- . 13  . 63 --  . I 1 . 42 -- . O i . 20  . 32  . 55 -- . 01  . 21 - -  . 06 ?42  . O9 . 6I -- . 05  . 57  . 00  . 22 ? 10  . 44 - -  . 03  . 29  . 06  . 32  . 04  . 27  . 09  . 20  . Ol . 29  . lO . 27 - -  . 02  . 24 -- . 13  . 23  . 11  . 17  . 06  . 18 - -  . 07  . 65 - -  . 08  . 14  . 14  . 34  . 02  . 09  . 07  . 20 - -  . 02 - - ~  . 02  . 12  . 62  . 30  . 61  . 03  . 48* N~r of students jud~d was 272 . Multiple R ~ ainst human criterion ( ~ urju das ) w  ~ , 71 ~ rbmhEs say C and Essay D ( Dd ~ a shown ~ R) . ~ rati ~ ~ r Multiple R we ~ highly significant . 

The overall accuracy of this beginning strategy was startling  . The proxes achieved a multiple-correlation of . 71 for the first set of essays analyzed and , by chance , achieved the identical coefficient for the second Set  . 
Furthermore , the betaweightings from one set of essays did well in predicting the human judgments for the second set of essays written by the same youngsters  . All in all , the computer did a respectable , " human-expert " jobing rading essays , as is visible in Table 2 . 
Table 2
Which One is the Computer ? " Below is the intercorre Mtion matr ~ generated by the crossvalidation of P ? o\[ 

ABCDE
A 51 51 44 57
B 51 53 56 61
C 51 53 48 49
D 44 56 48 59
E ? 576 1.49'59
Here we see the results of a crossvalidation.
These are correlations between judgments of 138 essays done by five " judges , " four of them human and one of them the computer . The computer judgments were the grades given by the regression weightings based on  138 other essays by other students . This crossvalidation , then , is very conservative . Yet , from a practical point of view , the five judges are indistinguishable from one another  . 
However useful such an overall rating m&ght be , we of course still wished greater detail in our analysis  . 
We therefore broadened the analysis --- ~ ive traits believed important in essays  , adapted partly from those of Paul Diederich . They may be summarized as : ideas , organization , s t __ ~ , mechanics , and creativity . We had a part fcular interest in creativity , since some ? critics from the beginning have believed that the computer must founder on this kind of measure  . "YOU might gra~emechanics all right , " someone will say , " but what about originality ? What about the fellow who is really different ? The machine can 'th and lehim ~" Therefore  , in 1966 we called together a group of 32 highly qualified English teachers from the schools of Connecticut to see how they would handle creativity and these other traits  . Each of 256 essays we srated on a five-point scale on each of these five important traits  , by eight such expert judges , each acting independently of any other judge . The teacher ratings were then analyzed , and it was found that the essay and the trait contributed significant variances  , as did the trait-by-essay interaction ,   ( perhaps the clearest demon-stration ' of the ipsative profile  )  . To investigate each of these five traitratings , the same 30 proxes were again employed , with the results to be seen in Table 3 . 
Table 3
Computer Simulation of Human Judgments
For Five Essay Traits (30 predictors , 256 cases )
A.B.C.D.E.
Hum .-Gp . Mult . Shrunk . Corr.
Traits Reliab . R Mult . R(At--~t-6~.)
I . Ideasor Content .75 .72 .68 .78
II . Organization .75 .62 .55 .64
III . Style .79 .73 .69 .77
IV . Mechanics ~85.6 9.6 4.69
V . Creativity . .72 .71 .66 .78

Coi . B represents the reliability of the human judgments of each trait  , based upon the sum of eight in de , pendent ratings , August 1966 . 
Col . C represents the multiple-regression coefficients found in predicting the pooled human ratings with  30 independent proxes found in the essays by the computer program of PEG-IA  . 
Col . D presents these same coefficients , shrunkent o eliminate capitalization on chance from the number of predictor variables  ( cf . McNemar , 1962, p . 184 ~ Col . E presents these coefficients , both shrunken and corrected for the unreliabil ity of the human groups  ( cf . McNemar , 1962, p .  153 . ) In our rapidly growing knowledge , Table 3 may temporarily say the most to us about the computer analysis of important essay traits  . Column A of course gives the titles of the five traits  ( more complete descriptions of the rating instructions may be supplied on request  )  . Column B shows the rather low reliability of the group of eight human judges  , computed by analysis of variance . 
Here in Column B " creativity " is less reliably judged by these experts than are the other traits  , even when eight judgments are pooled . And mechanics may be the most reliably graded of these five traits  . 
Surely , then , humans seemed to have a harder time with creat iv it ~ with mechanics  . 
What of the computer ? Column C shows the \[ raw multiple correlations of the proxes with these rather unreliable group judgments  . These were the coefficients produced by the standard regression program run by Dieter Paulus and myself  . Column D simply shows the same coefficients after the necessary shrinking to ? avoid the Capitalization on chance which is inherent with multiple predictors  . Finally , in order for a fair comparison to be made among the traits  , the criterion's unreliability should be taken into account  , as in Column E . Here such difficult variables as creativity and organization no longer seem to suffer  ; the computer's difficulty is apparently in the criter-i on itself  , and is therefore attributable to human limitations  , rather than to machine or program limitations . Column E , then , exhibits what might be the expectable crossvalidation from a similar set of essays  , if predicting a perfectly reliable set of human judgments  . 
Current and Projected Problems
Of course , all this is a temporary reading taken in the middle of the research stream  . Our investigators have also gone on With j other strategies  . Donald Marcotte ( 1967 ) has developed a phrase analyzer , and has discovered that cliches , as usually listed , were largely irrelevant to the judgment of such essays  . Dieter Paulus ( 1967 a ) has studied the Curvilinearity of proxes , and concluded that much elaborate statistical optimization may be a ? waste of time  ; and that the most major improvements should probably be made in other ways  . He also has studied feedback to the student writer  , using an online time-sharing console ( Paulus , 1967b ) , as has also Michael Zieky . Another researcher , Jack H . 
Hiller (1967) , has investigated quasi-psychological dimensions ( including opinionation and vagueness ) as predictors of the human judgments . Using techniques familiar from automatic content analysis  ( cf . Stone et al 1966) , he constructed lists of words and ph-ases to ~-6 -?- fine the variables of psychological interest  , and found these negatively correlated , a she predicted , with writing quality . And , in May ,  1967 , a sizeable improvement was made in the statistical accuracy  , increasing the multiple-regression coefficient from about  . 71 to about . 77 , and improving the variance accounted for by around  20%  . In other words , the newest programs apparently do better than the indiy idual  , expert English teacher . 
The early strategies , then , have provided fertile ground for statistical investigation of essay grading  , especially in the actuarial simulation of rating of style  . But what of the deeper dimensions of stylistic analysis  , and what of subject-matter ? content , as in essay questions in history , philosophy , or science ? been under more intensive study in recent months  , with the advice and help of Susumu Kuno (1964) , Stanley Petrick ( Keyser and Petrick ,  1967) , John Olney ( Olney and Londe , 1966; also see Harris , 1952) and others . 
(Of course these workers are not resppnsible for errors or misconceptions in the present paper  . ) Anticipated f~ture strategies are currently summarized in Table  4  . 
This table is based partly on work already accomplished in Project Essay Grade  , partly on suggested minor adaptations of systems already working for others  , and partly on projected programs which are not yet apparently operative in any system  , but which do not seem impossibly difficult at the efficiency desired  . 
Table 4
Project Essay Grade
Hypothetical Complete Essay Graderi.




INPUT and PUNCH . H and written or type written or other raw response of the writer is converted for computer input  . 
SNT ORG . Creates arrays of words and sentences as found in prose  . This is just as performed in

DICT . Assignment of available syntactic roles to each word  . This is currently done by many programs , but needs an expanded dictionary , and ambiguity resolver . ' At the same time , the semantic information will be stored in the work space for reference of other parts of program  . 
Availability of the tape-written Random House Dictionary  ( Unabridged ) has been promised . 
PARS . A modified Kuno ( 1964 ) program seems most promising , and is currently being programmed for both the 7094 and the 360 by workers at IBM . 
Alterations will be hecessary to accep ? well n formed substrings  . : REFER . This is intended to identify and encode the most likely referents of pronouns and other anaphoric expressions  . ( Cf . Olney and Londe , 1966) . This process must employ both syntactic features and semantic information from DICT  . 

Table 4 ( Continued ) 6 . KERNEL and STRUC . From the rewritten string output of (5) , KERNEL would establish a set of elementary proposit fons  , and STRUC would encode the relationships among these elements  . This step would retain all the information of an essay in simplest possible units  , yet would retain additional information about emphasis  , subordination , causal relation , etc . , among these units . 


7  . EQUIV . The elementary units would be augmented b . y the semantic information in DICT . To each word would be assigned a cluster of permissible synonyms  , with weightings of semantic distance . 
This permits an analysis of redundance and emphasis in the essay  , and permits a comparison of the content of the student essay with that of the key or master essay  . 
8 . STYLE . Descriptions of the surface structure characteristics of the essay ~ parts of speech  , organization of themes , types and varieties of sentence structure , grammatical de Pths , tightness of reference , etc ~ information about grammatical errors and strengths  . 
CONTNT . Comparison of the agreement of student and master essay  , through measure of kernel hits and struch its , these weighted by semantic distance of ~ language chosen  . 

SCOR . Multivariate prediction of appropriate pro-file for the immediate purpose  . 
The limitations of space will permit only a few comments on this table  , which may be seen as representing a hypothetical , ideal essay grader . For large grading systems , overestablished substantive content , it would be possible , for the key or master ~ , to edit by hand the output fro--m-ce--~ainro -utln~s  ( especlally REFER and STRUC )  . Of course , four of the most important routines listed in Table  4 are far from perfected in any existing programs . Ideally , they would assume better solutions to certain major  , stubborn problems in computational linguistics . 

Indeed , the steps in this hypothetical essay grader are close to the heart of the most persistent and trouble some problems in linguistics  . Is it necessary that sentences be syntactically analyzed before mapping into deep structure ? What is the proper role of semantics in such deep structure ? How can the outside knowledge of the reader be incorporated into the machine analysis ?   ( For some discussion of this problem , see Quillian ,  1966) . In general , how may we incorporate some of the intuitive richness which the literate hu ~ lanbrings to his reading ? It is not expected that workers in essay grading will suddenly resolve all such questions  . They may be recognized as those which so trouble linguists as to contribute to the recent official pessimism  , in the United States , about the future of mechanical translation . After 15 years of effort , mechanical translation is still regarded as disappointing in quality  , and virtually no sustained output of any machine program would be ordinarily mistaken for the work of a professional human translator  . 
On the other hand , the earliest attempts at essay grading by computer have  , in a very limited way , leaped ahead of machine translation . And if the expert human ratings of high school essays may be regarded as an acceptable goal  , then the machine program appears to have reached such a goal already  . For that matter , improved performance , even superior to that of the individual human expert  , appears to be immediately practi-cable as well . 
The explanation of this advantage , of course , is that the-problem of essay grading as attacked in the current work is much easier than the problem of machine translation  . In translation , every nuance of the input string should be accounted for in the output string  . 
In essay grading , only-a certain portion of the input text needs to be accounted for  , and the output . does not depend on the existence of any large , language-generating system . High quality machine translation apparently demands a fair portion of the total language -manipulating capability of the human  , but essay grading may use only a fraction of it , and may process language in ways quite different from that of the human being  . For example , our present programs have to date largely ignored order and sequence in the essays  , although to the human th--~rder of words is , of course , of crucial and unceasing importance . 
Since essay grading can work with such fractional information  , then , why pursue the deeper analysis of Table 4? Clearly , the purpose is not entirely the same as it would be for the usual linguist  . At any discrete i0 time in research , what is sought is not necessarily the perfect humanoid behavior  , but rather those portions of that behavior which , given any current state of the art , will contribute optimally to efficient and practicable improvements in output  . Indeed , regardless of the eventual perfection of deep linguistic behavior  , for any specific application to essay grading , at any one moment , large portions of such available behavior may be irrelevant  , just as it seems that ordinary human language processing does not usually call for our fulll inguistic effort  . 
Yet we regard it as eventually important to be ~ ble to perform these various kinds of advanced machine analysis when required  . Therefore , the eventual uses of the ideal essay analyzer may require analytic capability as deep as may be imagined  . Writing out suitable comments for the student , for example , will in some cases ~ taxany system which may be foreseen  . 
Even approximate solutions to these problems , however , though unsatisfactory for certain scientific purposes  , could make important contributions to the educational description and evaluation of essays  . For such evaluation is itself probabilistic , limited by imperfect asymptotes of writer consistency and rater agreement  . 
And such evaluation therefore does not require , to be practicable and satisfactory , the same deterministic perfection which has continued to elude and frustrate researchers ~ mechanical translation  . ~ There is a fund , a mental difference in goals , which must be realized . As has been demonstrated here , the output from much cruder statistical programs has already reached a quality not too remote from usefulness  . The more advanced strategies currently seem , at least to the present workers , bright with promise , Daigon , Arthur . Computer Grading of English Composition . 
The English Journal , January , 1966, 46-52.
Harris , Z.S . Discourse analysis.

Language , 1952,
Hiller , Jack H . , Page , E . B . , and Marcotte , D . R . A Computer Search for Traits of Opinionation , Vague-ness , and Specificity-Distinction in Student Essays . 
Paper read at the Annual Meeting of the American Psychological Association  , Washington , D . C . ,
September 2, 1967.
Keyser , S . J . , and Petrick , S . R . Syntactic Analysis , 1966 . ( In press in a forthcoming book . ~) Kuno , Susumu . Some characteristics of the Multiple-Path Syntactic Analyzer  . Language Data Processing , Cambridge : Harvard Computation Laboratory ,  1964 . 
C 6,18.
Marcotte , Donald . The . Computer Analysis of Clich ~ Behavior in Student Writing  . Paper read at the Annual Meeting of the American Educational Research Association  , New York , February 18 ,  1967 . 
McNemar , Quinn ~ Psychological Statistics , 3rd ed . New
York : Wiley , 1962.
Olney , John and Londe , D . A research plan for investigating English discourse structure with particular attention to anaphoric relationships  . TechMemomm-(L)-3256 . Santa Monica , California " System Development Corporation . November 22, 1966 . 17 p . 
Page , Ellis B . The Imminence of Grading Essays by Computer . Phi Delta Kappan , January , 1966, 238-243 . 
Page , Ellis B . Grading Essays by Computer : Progress Report . Proceedings of the 1966 In vitational Conference on Testing Problems . Princeton , N . J . : Educational Testing Service , 1967 . Pp .  87-100 . 
Paulus , Dieter . Problems of Nonlinearity in Grading Essays . Paper read at b the Annual Meeting of the American Educational  R4search Association , New
York , February '16, 1967 a.
Paulus , Dieter . Feedback in Project Essay Grade . Paper ? read at the Annual ' Meeting of the American Psychological Association  , Washington , D . C . , September 2, 1967b . 
Quillian , M . Ross . Semantic Memory . Cambridge , Mass . :
Bolt Beranek and Newman , 1966.

References ( Continued )
Stone , Philip J . , Dunphey , Dexter C . , Smith , Marshall S . , and Ogilvie , Daniel M . The General Inquirer : A Computer Approach to Content Analysis  . Cambridge:
M.I.T . Press , 1966. Pp . 651.
Woods , William A . Semantics for . a Question-Answering System . Paper read at the Annual Meeting of the Association for Machine Translation and Computational Linguistics  . Atlantic City , N . J . April 21,
