Japanese Dependency Analysis
using a Deterministic Finite State Transducer
Satoshi Sekine
Computer Science Department
New York University
715 Broadway , 7th floor
New York , NY 10003, USA
Abstract
A detern finistic finite state transducer is a fast device fbranalyzing strings  . It takes O ( n ) time to analyze a string of length n . In this 1) al)er , an application of this technique to Japanese dependency analysis will be described  . We achieved the speed at ; a small cost in accuracy . It takes about 0 . 1 . 7n fillisecond to analyze one sentence ( average length is 10 bunsetsu , 1) ased on Pen-t:tahiti650 MH zPC , Linux ) and we actually observed the analysis time to be proportional to the sentence length  . Thb accuracy is about ;   81% even though very little lexical information is used  . This is about 17% and 9% better than the default and a simple system , respectively . 
We believe the gap between our pertbrmm:ce and the best current  1  ) erforlnm : ce on the stone task , about 7% , can be filled by introducing lexical or sen : antic infornmtion  . 
1 Introduction
Syntactic analysis or parsing based on traditional methods  , like Chm't parsing or the GLR parsing algorithm , takes cubic or greater tin : e in the sentence length to analyze natural language sentences  . For Japmmse , Sekine et al ( Sekine et al ,  2000 ) proposed a Japanese dependency analyzer which mmlyze sentences in time quadratic in the sentence length using a backward search algorithm  . Recently , amnnber of research efforts using FiniteState Transducers  ( FST ) have been reported . Roche built an English syntactic mmlyzer by finding a fixed point in a nondeterministic FST  ( Roche ,  1994) . But it still can'tan Myze a sentence in time linear in the sentence length  . 
In this paper , we will propose a Japanese dependency analyzer using a Deterministic Fi-nile State Transducer  ( DFST )  . The Japmtese dependency structure is usually represented by relationships between phrasal units called % un-sets : :'  . Almn sets usually contains one or more content words  , like a noun , verb or adjective , and zero or more time : i on words , lil : ea post position ( case marker ) or verb/noun suffix . A dependency between two bunsetsu has a direction fl ' oma dependent to its head  . Figure 1 shows examples of lmnsets u and dependencies . 
Each lmnsets u is separated by "1" . The frst seglnent " KANOJO-HA " consists of two words  , KANOJO ( She ) and HA ( subject case ,   , narl:er ) . 
Then mnbers in the " head " lines how the head ID of corresponding bunscts us  . Note that the last segment does not have a head , and it is the headlmn setsu of the sentence . The task of the dependency analysis is to find the head ID for each lnmsets u  . 
2 Backward beam search algorithm l ? irst , we wouhllil : e to describe the backwm'd beam search algoril : lm:t br  , \] at ) m : esed pendency analysis proposed by Sekincet sd  . ( Sekine et al , 2000) . Their experin:ents suggested the method proposed in this paper  . 
The following characteristics are known ibr Japanese dependency  . Sekine et al assumed these characteristics in order to design the algorithm  1   ( 1 ) l ) ependencies are directed from left to right ( 2 ) Dependencies don't cross . 
(3 ) Each bunsetsu except the rightmost one has only one head  ( 4 ) Left context is not necessary to detern fine a dependency  . 
1We know that there are exceptions ( Shirai ,  1998) , but the frequencies of such exceptions are very stuN1  . 
Characteristic (4) is not well recognized , but based on our experiments with humans , it is true more than 90% of the time . 

ID 1234 56
K ANOJ0-HA IKARE-GAIT SUKUTTAI PAI-W0 IYOROKON DEIUKE TO TTA . 
(She-subj )   ( he-subj )   ( made )   ( pie-obj )   ( with pleasure )   ( received ) 
Head 63 46 6-
Translation : She received the piemade by him with pleasure  . 
Figure h Example of a Japanese sentence , bunsetsus and dependencies Sekine et al proposed a backward beam search algorithm  ( analyze a sentence froln the tail to the head , or right to left ) . Backward search has two merits . Let's assume we have analyzed up to the/14 + l-st bunsetsu in a sentence of length N ( 0 < M < N )  . Now we are deciding on the head of the M-th bunsetsu  . The first merit is that the head of the dependency of the  /1J-th bunsetsu is one of the bunsetsus between t14  +  1 and N , which are already analyzed . Becmlse of this , we don't have tokeel ) a huge number of possible analyses , i . e . we can avoid something like active edges in a chart parser  , or making parallel stacks in GLR parsing , as we can make a decision at this time . Also , wc can use the bemn search mechanism , t ) y keeping a certain n m n ber of candidates of analyses at each t  ) un-setsu . The width of the beam search can be e ~ L sily tuned and the memory size of the process is proportional to the product of the input sentence length and the beam search width  . The other merit is that the 1 ) ossible heads of the dependency can be narrowed down because of the assmnption of noncrossing dependencies  . For example , if the Kth bunsetsu depends on the Lth bunsetsn ( /1J < K < L )  , then the 21//-th bunsetsu can't depend on any bunsetsus between I ( and L . According to our experiment , this reduced then mnber of heads to consider to less than  50%  . 
Uchimoto et al implemented a Japanese dependency analyzer based on this algorithm in combination with the Maxinmm Entropy learning method  ( Uchimoto et al ,  2000) . The analyzer demonstrated a high accuracy . Table 1 shows the relationship between the beam width and the accuracy of the system  . Froln the table , we can see that the accuracy is not sensitive to the beam width  , and even when the width is
Beamwidth Dependency Sentence
Accuracy Accuracy \] 87 . 14 87 . 16 87 . 15 87 . 20 86 . 21 40 . 60 40 . 76 40 . 60 40 . 60 40 . 60
Table hBemn width and accuracy 1 , which Ineans that at each stage , the dependency is deterministieally decided , the accuracy is almost the same as the best accuracy  . This means that the left , context of a dependency is not necessary to decide the dependency  , which is closely related to characteristic (4) . This result gives a strong motivation tbrst ~ rting the research reported in this paper  . 
3 Idea
Untbrtunately , the fact that the beam width can be 1 by itself cannot lead to the analysis in time proportional to the input length  . Theoretically , it takes time quadratic in the length and this is observed experimentally as well  ( see Figure 3 in ( Sekine et al ,  2000)) . This is due to the process of finding the head among the candidate bun-sets ns on its right  . The time to find the head is proportional to the number of candidates  , which is roughly proportional to the number of bunsetsus on its right  . The key idea of getting linear speed is to make this time a constant  . 
Table 2 shows the number of candidate bunsetsus and the relative location of the head among the candidates  ( the number of candidate bunsetsus from the bml set subeing analyzed  )  . The data is derived fl ' om 1st to 8th of e and .  1 2 3 4 5 6 7 8 9 10 11 12 13 1 . 4  10232   3292   6774   2244   3692   1294   1843   614   848   278   34O   110   137   33   51   11   17   5   5   1   3   ( )  ( ) \] 888 660 1114 398 331 . 29 155 1176 133 i3 3125 2648 347 , 7 8   57   68   121   28   17   19   21   19   2/t   46   10   7   3   10   3   6   8   22   2   0   2   2   /1   2  / ,  210 1 2 0 0 0 0 22 13
I .   0   0   1   0   0   0   0   0   0   0  \]  0   0   0   0   0   0   0   0   0   0   2 rl ' able 2: Nmnber of candidate , and location of head January part of the Kyoto COl'pus  , wlfich is a hand created cort ) llStagged . with POS , tmn sets u and dependency relationships ( luro hashi , Nagao ,  1997) . It is the same 1 ) ortion used a stile training data of the system described late  , r . ' J ~ henmnbc , r of cm Mi dates is shown vertically and the location of the head is shown horizontally  . 
l ; brexaln ple , 2244 in the fom ' throw and these ( : Olid ( -ohmn ~ means that there are 2244 bunsetsu which have 4 head C all didates and the 2nd fl ' om the left is the head of the bunsetsu . 
We can observe that the . data . is very biased.
The head location is very lilnited .  98 . 7% of instances are covered 1 ) y the first 4 candidates and the last candidate , combined . In the table , the data which are not covered by the criteria are shown in italics  . From this obserw ~ tion , we come to the key idea . We restric the head candidate locations to consider  , midwere member all patterns of categories at the limited locations  . For each remembered lmtter n , we also remember where the . head ( answer ) is . This strategy will give us a constantime select ; ion of the head . For example , assmne , at ~ certain lmn sets u in the training data , there are five Call-didates . Then we . will remember the categories of the current lmn sets u  , s t\y " A " , and five candidates , " BC1) EF " , as wall as the head location , for example "2 nd " . At the analysis phase , if we encounter the same sittmtion , i . e . the same category bml sets u " A " to be mmly zed , and the same categories of five candidates in the same order  "13 CDEF " , then we can just return tile remenfl ) ered head location , "2 nd " . This process can be done in constant tilne and eventually  , the analysis of a sentence can 1 ) ed one in time 1 ) ro-portional to the sentence length . 
For the iml ) lementation , we used a DFST in which each state represents the patterll of the candidates  , the input of an edge is the category of the . l ) llnset sll1)eing analyzed and the output of a . nedge . is the location of hea(t . 
4: Implementation
We still have several 1 ) rol ) lelns whi ( 'h have to be solved in order to i in t ) lement the idea . As-SUlning the number of candidates to be 5 , the t ) roblelns are to ( l ) define the c~tegories of head bunsetsu candidates  ,   ( 2 ) limit then unlber of patterns ( the lmmber of states in DFST ) to a mallage able range , because the Colnbilmtion of five categories could t  ) ehuge ( 3 ) define the categories of intmt bunsetsus ,   ( 4 ) deal with unseen events ( sparseness l ) rob-lem )  . 
hit his section , wel ) resent how we implemented the , system , wl fich at the stonetinle we implemented the systeln in a very small size  ; 1200 lines of C progrmn , 188 KB data file and less than 1MB processing size . 
Structure of DFST
For the categories of head bunsetsu candidates , we used JUMAN's POS categories as the basis and optimized them using heldout data  . 
JUMAN has 42 parts-of-speech ( POS ) including them in or level POSs , and we used the POS of the head word of a candidate bunsetsu  . We also used the information of whether the bunsetsu has a colnma or not  . Then uln ber of categories 1 ) ecomes 18 after tuning it using the heldout data . 
The input of all edge is the information about the  1  ) unset sucurrently being analyzed . We used the inforination of the tail of the bunsetsu  ( mostly function words )  , and it becomes 40 categories according to the stone tuning process . 
The output of an edge is the information of which candidate is the head  . It is simply an un > ber from 1 to 5 . The training data contains examples which represent he same state and input  , but different output . This is due to the rough definition of the categories or inherent impossibility of to disambiguating the dependency relationship fi'om the given infbrmation only  . In such cases , we pick the one which is the most frequent as the answer in that situation  . We will discuss the problems caused by this strategy  . 
Data size
Based on the design described above , the number of states ( or number of patterns ) is 1 , 889 , 568 (18~) and the number of edges is 75 , 582 , 720 as each state has 40 outgoing edges . 
If we implement it as is , we may need several hundred megabytes of data . In order to keep it fast , all the data should be in memory , and the current memory requirement is too large to implement  . 
To solve the problem , two ideas are employed.
First , the states are represented by the combination of the candidate categories  , i . e . states can be located by the 5 candidate categories . 
So , once it transfers from one state to another , the new state can be identified from the previous state  , input bunsetsu and the output of the edge . Using this operation , the new state does not need to be remembered for each edge and this leads to a large data reduction  . Second , we introduced the default dependency relationship . 
In the Japanese dependency relationship , 64% of bunsetsu debend on the next bunsetsu . So if this is the output , we don't record the edge as it is the default . In other words , if there is no edge in ~ brmation for a particular input at a particular state  , the outtmt is the next bunsetsu ( or 1) . This gave unexpectedly a large benefit . 
For unseen events , it is reasonable to guess that the bunsetsu depends the next bunsetsu  . Because of the default , we don't have to keep such information . Actually the majority ( more than 99% ) of states and edges are unseen events , so the default strategy helps a lot to reduce the data size  . 
By the combination of the two ideas , there could be a state whidl has absolutely no information  . If this kind of state is reached in the DFST , the output for any input is the next bunsetsu and the next state can be calculated froln the information you have  . In fact , we have a lot of states with absolutely no information  . Before implementing the supplemen-tation , explained in the next section , the number of recorded states is only 1 , 006 and there are 1 , 554 edges ( among 1 , 889 , 568 possible states and 75 , 582 , 720 possible edges ) . After implementing the supplementation , we still have only 10 , 457 states and 31 . , 316 edges . The data sizes ( file sizes ) are about 15KB and 188KB , respectively . 
Sparseness problem
Tile amount of training data is about 8 , 000 sentences . As the average number of bunsetsu in a sentence is  10  , there are about 72 , 000 data points in the training data . This number is very much smaller than then mnber of possible states  ( 1 , 889 , 568) , and it seems obvious that we will have a sparseness problem  2  . 
In order to supplement the unseen events , we use the system developed by Udfimoto et . al ( Uchimoto et al , 2000) . A large corpus is parsed by the analyzer , and the results 2However , we can make the system by using the default strategy mM surprisingly the accuracy of the system is not so bad  . This will be reported in the Experiment section tice  , we parsed two years of lmwspaper articles (2 , 536 , 229 sentences of Mainichi Shinbun 94 and 95 , excluding Jalmary 110 ,  95 , as these are used in the Kyoto corpus ) . 
5 Experiment
In this section , we will report the experilnent.
~?\ r ( , used the Kyotocorl ) us ( ve . rsion 2) . The training is done using January 18 ,  95 (7 , 960 sentences ) , the test is done using Jmmary 9 ,  95 (1 , 246 sentences ) mid the parameter tuning is done using Jmmary 10 ,  95 (1 , 519 sentences ; heldout data) . The input sentences to the sys-rein a remorl ) hologically analyzed and bunsetsunre detected correctly  . 
Dependency Accuracy
Table 3 shows the accuracy of the systems.
The ' dependency accuracy ' metals the percentage of the tmn sets us whose head is correctly mmlyzed  . The bml sets us are all but the last bunsetsu of the sentence  , as the last 1) unsetsu has no head . The ' defaultn lethod ' is the sys-rein in which the all bunsetsus are supt  ) osed to dei ) end on the next bunsetsu . ~ t'he'baselinereel ; hod'is a quite siml ) le rule-based sys-toni which was created by hand an  ( thas a \] ) out 30 rules . ' l'he details of the system are rel ) orted in ( Uchimoto et al ,  1999) . The ' Uchimoto's system ' is tit( ; system rel ) orted in ( Uchimotoctal . , 2000) . They used the same training data m~d test ( lata . The del ) endency accuracies of
System Det ) en(tency

Our system ( with supp .) 81.231%
Our systenl ( without sut ) p .) 77.9 72%
Default method ( i 4.14:%
Baseline method 72.57%
Uchimoto ' system 87.93%
Table 3: Dependency Accuracy our systems are 81% with supl ) lenlentation ad 78% without supt ) lementation . The result is about 17% and 9% better than tile default mid the baseline n lethods respectively  . Compared with Uchimoto ' system , we are about 7% behind . But as Uchimoto ' system used about 40 , ()00 features including lexical features , and they also introduced combined features ( up to 5 )  , it is natural that our system , which uses only 18 categories and only combinations of two cat -cgories  , has less accuracy a . 
Analysis speed
The main objective of the system is speed.
Table 4 shows the analysis speed on three diffcrent platforms  . On the fastest machine , it analyzes a sentence in 0 . 17 millisecond . Tat ) le 5 shows a comlmrisou of the analysis speed of three difl'erent systems on SPARC-I  . Our system runs about 100 times faster than Uchimoto's system and KNP ( Kurohashi , Nagao ,  1994) . 
Platform Analysis time ( millise c./sent.)
Pentiuln III 650 MHz 0.17
Sml Enterprise , 400MHz 0.29
Ultra SPARC-I,\]70 MHz 1.)3
Table 4: Analysis Speed
System Analysis time ( millise c./sent.)
Our system 1.03
Uchimoto ' system 170
KNP 86
Tal)le 5: Coml ) arison of Analysis Speed
Figure 2 show stile relationship between tile sentence length and tile analysis tinle  . We used tiles lowest nlachine ( Ultra SPARC-I , 170 MHz ) in order to minimize obserw ~ tional errors . We cml clearly observe that the anMy sistinle is proportional to the sentence length  , as was predicted by the algorithm . 
The speed of tile training is slightly slower l : hant hat of tile analysis  . The training on the smaller training data ( about 8000 sentences ) t ; akes M ) out 10 seconds on Ultra SPAR . C-I . 
a However , our system uses context information offly ( ~ , bunsctsus , which is not used in Uchimoto's system . 

Analysis time ( millisec .)1 I05I'0152'0
Sentence length ( Num . of bunsetsu )
Figure 2: Analysis time and Sentence length 6 Discussion 6  . 1 The restriction People may strongly argue that the main I  ) rol > lenl of the system is the restriction of the head location  . We eonlple tely agree . We restrict the candidate to five bunsetsus , as we describe dear-lie : ' , and we thcore tically ignored 1 . 3% of accuracy . Obviously , this restriction can be loosened by widelfing the range of the candidates  . For example , 1 . 3% will be reduced to 0 . 5% if we take 6 instead of 5 candidates , hnplen lentation of this change is easy . However , the . problen : lies with the sparseness . As shown in Table 2 , there are fewer examples with a large number of bml-setsu candidates  . For exam I ) le , there are only 4 instances which haw ~ 14 candidates . It may be impossible to accumulate nough training ex-mnples of these kinds  , even if we use a lot of untagged text fbr the supplementation  . In such cases , we believe , we should have other kinds of remedies . One of them is a generalization of a large number candidates to a smaller number candidates by deleting nniniportant bunsetsus  . 
This remains for fl : ture research.
We can easily imagine that other systems may not analyze accurately the cases where the correct head is far fl ' om the bunsetsu  . For example , Uchimoto ' system achieves an accuracy of 41% in the cases shown in italics in Table 2  , which is much lower than 88% , the system's ow ~ rall accuracy . So , relative to the Uchimoto's system , the restriction caused a loss of accuracy of only  0  . 5% (1 . 3% x 41%) instead of 1 . 3% . 
6.2 Accuracy
There are several things to do in order to achieve better accuracy  . One of the major things is to use the information which has not been used  , but is known to be useful to decide dependency relationships  . Because the accuracy of the system against the training data is only  81  . 653% ( without supplementation ) , it , is clear that we miss some important information , We believe the lexical relationships in verb frmne element preference  ( VFEP ) is one of the most important types of information  . Analyzing the data , we can find evidence that such information is crucial  . For exmnplc , there are 236 exmnples in the training corpus where there are 4 head candidates and they are bunsetsus whose heads are noun  , verb , noun and verb , and the current bunsetsu ends with a kaku-j oshi  , a major particle . Out of the 236 exami ) les , then uln ber of cases where the first , second , third and last candidate is the head are 60 ,  142 , 3 and 31 , respectively . The answer is widely spread , and as the current system takes the most fl'e -quent head as the answer  , 94 eases out of 236 (40%) are ignored . This is due to the level of categorization which uses only POS information  . Looking at the example sentences in this ease , we can observe that the VFEP could solve the problenl  . 
It is not straightf brward to add such lexieal information to the current franlework  . If such information is incorporated into the state in ibr-marion  , then mn ber of states will become enormous . We can alternatively take an approach which was taken by augmented CFG  . In this approach , the lexical infbrmation will be referred to only when needed  . Such a process m ~\ y slow down the analyzer , but since the nuin ber of in-vocation of the process needed in a : ~ entence may be proportional to the sentence length  , we believe the entire process may still operate intiine proportional to the sentence length  . 
7 Conclusion
We proposed a Japmiese dependency analysis using a Deterministic Finite State Transducer  ( DFST )  . The system analyzes a sentence with fairly good accuracy of about  81% in 0  . 17 millise cond on average . It can beat ) plied to a wide range of NLP applications , including realtime Machine Translation , Information Ex-a number of efl brts to improve the accuracy of Japanese dependency analysis  ( Haruno et . al , 1997) ( Fujio , Matsumoto , 1998) ( Kanaymna et . al , 2000) . In particular , it is interesting to see that Kanwmna's method uses a similar idea  , limiting the head candidates , in order to improve the accuracy . We are plmming to incorporate the fruit of these works to improve our accuracy  . We believe our resem'ch is complementary to these research  . We also believe the method proposed in this l ) a perl n ~\ y be applic > ble to other languages which share the chaiac-teristics explained earlier  , for example , Korean , ~lhrkish and others . 
8 Acknowledgment
Thee Xl ) eriments in this paper were conducted using the Kyoto corpus  . We really appreciate their effort all d their kindness in making it pub-lit : domain  . We thank l ) r . Kuroluu ~ hi and other people who worked in the project  . Mr . Uchimoto helped us to prepare the d~l ; a and the experiment , mid we would like to thank him . 

Masakazu Fujio , Yuuji Matsumoto . 1998: " JaI)an(;seDct mndencyStrtlctll '( ~ , Analysis base ( lonl , cxicalizcd Statistics " , P ' rocccding . s " o . fTh,i'rdCo ~:/'(' . rcnce on EMNLP1) t ) 87-9 ( iI:liroshi Kanayama , Kcntaro '\] brisawa , Yutaka Mitsuishi ,   . Jml'ichi Tsujii . 2000:'" A hybrid , lalmnese Parser with Hand-crafl ; edGrmnmar and Statistics " , Proceedings o /" COLING-O0 , this pr ' oceedings Masahiko Haruno , Satoshi Shirai , Yoshiflmfi Ooymna .   1998 : " Using Decision ~ l ~' ees to Construct a Practical Parser "  , Procccding . so \[
COLING-ACL-08 pp 505-511
Sadao Kurohashi , Makoto Nagao .   1994 : " KNParser : Japanese Dependency/Case Structure Analyzer "  , Pwcccdin . qs of the Wor'k-shop on Sharable Natural Language l  ~  . c , sourc cspp48-55 Sadao Kurohashi , Makoto Nagao .  1 . 997: " Kyoto University text corpus project " , Proceedings of the ANLP-9Z Japan pp115-118 Emmmmel R . oche .   1994 : " Two Parsing Algorithms by Means of Finite State Transducers "  , Proceedings of COLING-9/jpp 431-435 , Satoshi Sekine , Kiyotaka Ucllimoto , Hitoshi Isahara .   2000 : " Statistical Dependency Analysis using Backwm ' dBeamSearch "  , Proceedings of COLING-O0 , this proceedings Satoshi Shirai . 19!18: " Heuristics and its limitation " ,   , Jou Tvtal of the ANLP , Japan Vol . 5
No.l , t ) 1) 1-2
Kiyotaka Uchimoto , Satoshi Sekine , Hitoshi Isahara . 1999:" Jal ) anese Dependency Struc-tllre Analysis Based on Maximum Entropy Models "  ,   , \] our ' nal of Information Processing Society of Japan Vol  . 40, No . 9 , pp 3397-3407 Kiyotaka Uchimoto , Masaki Mm'ata , Satoshi Sekine , Hitoshi Isahara .   2000 : " Dependency Model Using l?osterior Context " , Pr ' oceedings of the IIYPT-O0
