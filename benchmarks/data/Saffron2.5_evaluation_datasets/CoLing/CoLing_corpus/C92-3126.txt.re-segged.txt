A COMPUTATION ALMODE LOFL ANGUAGE
DATAORIENTED PARSING
RENSBOlt *
Department of Computational IJnguistics
University of Amsterdmn
Spuistra at 134
1012 VII Amsterdam
The Netherlands



1 ) at a Oriented Parsing ( IX ) P ) is a model where no abstract rules , but language xt ~ riences in the ti3ru ~ of all , ' malyzed COlpUS , constitute the basis for langnage processing . Analyzing a new input means that the system attempts to find tile most probable way to reconstruct the input out of frugments that alr " c ~yexistill the corpus  . Disambiguation occurs as a side-effect . 
DOP can be implemented by using colivelllional parsing strategies  . 

This paper to mmlizes the model for natural I mlgnage introduced m\[Sclm  199o  \] . Since that article is written in Dutch , we will translate Some parts of it more or less literally in this introduction  . According to Scba , the current radition of language processing systems is based on linguistically motivated competence models of natural Iml guages  . " llte problems that these systems lullia to , suggest file necessity of a more perfommnce oriented model of language processing  , that takes into account the statistical properties of real language use  . 
qllerefore Schaproposes a system ritat makes use of an annotated corpus  . An Myzing a new input means that the system attempts to find the most probable way to reconstruct the input out of fragments that already exist in the corpus  . 
The problems with competence grammars that are mentioned in Scha'saiticle  , include the explosion of ambiguities , the fact tilat It unmn judge meats on grammaticality are not stable  , that competence granunars do not account for language h ~ alge  , all d that no existing rule-based grammar gives a descriptively ' adequate characterization of an actual language  . According to Scha , tile deve h , pment of a fornmlg natunar furnaturallatl guage gets more difficult  , astire gram margets larger . 
When the number of phenotnena one has already take a into account gets larger  , the number of iarer actions that must be considered when  , metries to introduce all account of a new pllenomenon grows accordingly  . 
As totile problem of , ' mt biguity , it has turned out that as soon as a formal gratm narclmracterizes a nontrivial part of a natural anguage  , almost every input sentence of reasonable length get smlre\]manageably large number of different structural analyses  ( and * The author wishes to thank his colleagues at the Department of Computational Linguistics of the I laiversity of Amsterdam for many fruitful discussions  , and , in particular , Remko Scha , Martin van den Berg , Kwee Tjoel , iong and Frodenk Somsen for valuable comments on earlier w ~' rsions of this paper  . 
semantical interpretations ) . I " lids is problenmtic since most of these interpretations ~ renot perceived as lVossible by a hunmn language user  , while there are no systematic reasons 111 exclude tile l nonsyut actic or sematltic grounds . Often it is just an tatter of relative implausibility : tile only reason why a certain i are rpmtarion of a sentence is not perceived  , is that a anther interprctatilm is much more plausible  . 
Competence and Performance ' talelhn riations of the current language procossing systerus are not suprising : riley are the direct consequence of rile fact that these systems implement Chart\]sky ' snotion of a coutpetence grmn mar  . The formal griln uuars that constitute the subject -nmtter of theoretieal linguistics  , aim at characterizing the clnnpetencc of tile langnage user  . But the preferences language users have m the case of ambiguou sentences  , are paradigm instances of perfonatm cephenomena . 
In order to build effective lauguage processing systems wenms tint plement perform a nec-grammars  , rather than competence gratumars , qlaese performance granmuush ou M not only contain information on the structural possibilities of file general I~mgnage system  , but also on details of actual language use in a language conmmnity  , and oftile language experiences of an individual  , which cause this individual to have certain expectations on what kinds of u Uerances are going to occur  , and whats lractures and interpretations these utterances are going to have  . 
Thercisal lalter native linguistic tradition tluath as always focused on the concrete details of actual language use : file statistical tradition  . In this approach , syntactic structure is usually ignored ; only'superficial'stalistical properties of a large coqms are described : file probability that a certain word is followed by a certain other word  , the probability that a certain sequence of two words is followed by a ce~ml word  , etc . ( Markov-cludns , see e . g . \[Bahl 1983\]) . This approach bus perforumd succesfully ill certain practical tasks  , such , as selecting the most probable sentence from the outputs of a speech recognition coruptment  . It will be clear that this approach is not suitable form mly other tasks  , because no uotion of syntactic struct me is used . Aud there are statistical dependencies within the sentences of a corpus  , that cam extend over all arbitrarily long sequence of words  ; this is ignored by file Markov-approach . The challenge is now to develop a theory of language process lag that does justice totile statistie M  , as well as totile structural aspects of langange . 
1 In \[ Martin 19791 it is reported that their t~ser generated 455 different lx uses for tile sentence " lAst the sales of products produced in  1973 with the products produced in 1972"  . 
ACRESDE COLING-92 , NANTES , 2328 no ( tr 1992855 PROC . OVCOLING-92 . NAN rES , AUG .  2328 ,   1992 The Synthesis of Syntax and Statistics The idea that a synthesis between syntactic and statistical approaches could be useful has incidentally been proposed before  , but has not been worked out very well so far . The only technical elaboration of this idea that exists at the moment  , the notion of a probabilisd cgtam nmr , is of a rather simplistic nature . A probabilistic grammar is simply a juxtaposition of the most fundamental syntactic notion and the most fundamental statistical notion : it is an " old-fashioned " contextfree grammar  , that describes syntactic structures by means of a set of abstract rewrite rules that are now provided with probabilities that correspond to the application-probabilities of the rules  ( see e . g . \[Jeliuek1990\]) . 
As long as a probabilistic grammar only assigns probabilities to individual rewrite rules  , the grammar cannot account for all statistical properties of a language corpus  . It is , for instance , not possible to indicate how the probability of syntactic structures or lexical items depends on their syntactic flexical context  . 
As a consequence of this , it is not possible to recognize frequent phrases and figures of speech as such-a disappointing property  , for one would prefer that such phrases and figures of speech would get a high priority in the ranking of the possible syntactic analyses of a sentence  . Some improvements can be made by applying the Markov-approach to rewrite rules  , as is found in the work of \[ Salomaa 1969\] and \[ Magerman 1991\]  . 
Nevertheless , any approach which ties probabilities to rewrite rules will never be able to acconunodate all statistical dependencies  . Optimal statistical estimations can only be achieved if tile statistics are applied to different kinds of units than rewrite rules  . It is interesting to note that also in the field of theoretical linguistic stile necessity to use other kinds of structural units has been put forward  . The cleare starticulation of this idea is found in the work of \[ Fillmore  1988\]  . 
From a linguistic point of view that emphasizes the syntactic complexities caused by idiomatic and semi-idiomatic expressions  , Fillmore et al . arrive at the proposal to describe language not by means of a set of rewrite rules  , but by meaus of a set of constructions . A construction is a tree-strncture : a fragment of a constituent-structure that can comprise more than one level  . This tree is labeled with syntactic , semantic and pragnmticategories and feature -values  . 
Lexical items can be specified as part of a construction  . 
Constructions can be idiomatic in nature : the meaning of a larger constituent can be specified without being constructed front the meanings of its subconstituents  . 
Fillmore'side as still show the influence of the tradition of formal grammars : the constructions are schemata  , and the combinatorics of putting the constructions together looks very much like a contextfree gramnmr  . But the way in which Filhnore generalizes the notion of grmnmar resolves the problems we found in the current statistical grammars : if a constrnction-granunar is combined with statistical notions it is perhaps possible to represent all statistical information  . This is one of the central ideas behind our approach  . 
A New Approach : Data Oriented

The starting-point of our approach is the idea indicated above  , that when a human language user analyzes sentences  , there is a strong preference for the recognition of sentences  , constituents and patterns that occurred before in the experience of the language user  . There is a statistical component in language processing that prefers more frequent structures and interpretations to less frequently perceived alternatives  . 
The information we ideally would like to use in order to model the language performance uf a natural language user  , comprises therefore an enumeration fall lexical items and syntactic/semantic structures ever experieaced by the language user  , with their frequency of occurrence . In practice this means : a very large corpus of sentences with their syntactic analyses and semantic interpretatious  . Every senteace comprises a large number of constructions : not only the whole sentence and all its constituents  , but also the patterns that can be abstracted from the analyzed sentence by introducing ' free variables ' for lexical elements or complex constituents  . 
Parsing then does not happen by applying grammatical rule storite input sentence  , but by constructing an optinml analogy between the input sentence and as many corpus sentences  , as possible . 
Sometimes the system shall need to abstract a way from most of the properties of the trees in the corpus  , and sometimes a part of tile input is found literally in the corpus  , and can be treated as one unit in the parsing process  . Thus the system tries to combine constructions from the corpus so as to reconstruct the input sentence as ' well ' as possible  . ~lltep referred parse out of all parses of the input sentence is obtained by maximizing file conditional probability of a parse given the sentence  . 
Finally , the preferred parse is added to the corpus , bringing it into a new'state ' . 
To illustrate the basic idea , consider the following extremely simple exmnple . Assume that the whole corpus consists of only the following two trees : 

AM:'VP
AWaV NP . ~ . ppw'i'/x-ilil coting'92 P Propec~d Prin Judy
III in Nantescoling'90
Then the input sentence who opened cohn g '92 in Nantes in July can be analyzed as an S by combining the following constructions from file corpus :++ 
NP VP Pr Pr PP
V1 ? PP who coling ~2PPrAA!j
VNPP Prilos 1 II opened in July
ACRESDECOLING-92 ? NANTES , 2328 AOL'r1992856I ) ROC . (71: COLING-92, NANTES . AUG .  2328, 1992
The Model
In order to come to fomml definitions of p , ' u'se and prefetted parse we tirst specify some basic notions  . 

We distinguish between file set of lexicall , lbels L and the ~ t of nonlexical labels IV . Lexical labels represent words . Nonlexical l ' , fl~els represents yi~tactic and/or semantic mid/or i  ) ragnlalie infonnatiou , depending on file kind of corpns being used . We write J ~ for l , ul ~

Given a set of hlbcls ~ , a string is allu-tuple of elements of ~: ( LI ,  . . . , Ln ) ~ ~ u . All input string is mln quple of elements of L : ( l , t ,  . _, Ln)~I , n . A Collckt tellatio \ [ l ~ Gill big defined OI ( sllil ( gSUS usual : (  ; l ,  . . . ,b),~(c, . . . , ll ) = ( a , . . . ,b,c, . . . ,d) . 

Given a set of labels J ~ , the set of trees is defined a stiles n ml lest set Treesucl ~ that  ( 1 ) if I , ~  , then ( l , ,O)~Tree (2) i f L6"~ , tl ,  . .,,tneTi'ce . , then ( l ,, ( ll, . . . ,tn ) )eT~ee For a set of trees 77cc over a ~ t of labels ~ , we define a function root . ~/ i-ee-9~mid atuuction le ; tves:~l?ee ~ Ln by for n_>O , root (( L , (tl ,  . . . , tn ))) = I , rot n > O , le , ~ ves((L , ( tt, . . . , l~t ))) ~ l ?,' lves(tl )* . . . ~le~lves(tn ) to rn--O , leaves((L , O )) = ( L)

A corpus C is a multiset of trees , ill file ~ nse that ally tree can occur zero , nile or more times . ' Filelt ~ tves of every tree in a corpus is ml element of Ln : it consfimtes the string of wo  ( ds of which that tree is the amdys is that seemed most appropriate for understanding tiles triug ill the context in which it was uttered  . 

Ill order to define the Constowtions of a tree , we need two additional notions : Subffees and l ~tttems  , 
Snbtrees((L,(tl, . . . , t ~))) = n\[(L,(tl , . .,t ~)) u (~ Snhtrees?ll )) i=~
Pattems((L , ( t1, . . . , In ))) = ( L,O)1 ty(l,(ul , . . . , no . )) / Vi ~ 11 , ,l : nid~attenls ( ti ) l Constructions ( T ) = t / 3 be Subtrecs ( 1 ) : teP , 'tttenls ( u ) We Slullluse tile lbllowing notation for a constnlction of a tree in a corpus : tee=tier ~ nc  ( )" tc (  . imstmctionsO0 . 
Example : consider tree T . qhetrees T1 and T2m ~ conslnletions of T , while '\['3 is not . 
TSTI "
TT VP PP VP PP
Iv,I/xop~wwwiN ~ Ju~/viapo
T3~N,~vppp/p
Composition
If t and u are trees , such Ilmt tilele\[tmost non-lcxic ; llleMoft is equal to the motofn , then to u is the tree that results from substituting this leafint by tree u  . Thei ) mtial function o:'l~eexTree-47ivc is called ~ mlposJtion . 
We will write ( to U ) ov ; Ls touov , and ill general ( . .(( tloQ)o(~)o . .)otn as tl~t2o(~o . . . otn . 
Exmuple : vt ~ vp Np vp
TV Pppt ~ a:l VP PP
NP rtr0heI

Tree 7' is a par ~ of inputs lrings with respect oC , iff leaves (7) = s and therem'e constructions tl , - . , tne(~, such that 1'=t to . . . otn . A tuple ( f l , . . . , tn ) of such constructions is aid to generate par ~ To fs  . Note that different tuples of constructions Gm generate the  . , vante parse . The set of par~s of s will ( respect to C,
P,'use(s,C ), is given by
I ',' use(S,C ) = (1e Tive/l caves(T ) = sA3tl, . . . , t . eC"?-tlo .   . : tn\]"File set of tuples of C ( nlstructions that generate a parse 7 ; " lbples(F , C ) , is given by lupl cs('L(O =\[( tl ,  . . . , t~p/tl , . . . , tn ~" CA tlo . . . otn = T

All input string can have several parses and every such parse can be generated by ~ veral different c  ( )mbinations ( ) f COllstruclious lrOlll tile corpus . What we are interested in , is , given an input strings , tile probability that arbiffury conlbinations of COllSIxuctions fro  ( (I tile colpus generate a celtain prose 25 of s . Thus we are interested illtile colldJt kmal prolXlbility of apm's e  1  ) given s , with as probability space tile set of constructions of O'ees in the corpus  . 
l , et ' / ~ be a parse of iupet strings , and supl ~ ) setimt 15 can exhaustively be generated by k tuples of constructions :  1iqges  ( 15 , C ) = (( tll ,  . .,thn ), ( t21, . .,12n2) .   .   .   .   . ( tkh . .,tknt ) . The ll 7) occursill "( tll, . . . , tlnl ) or ( t21, . . . , ten2) or . . . . or ( Ikl,, . , tknk ) occur , aud ( thl, . . . , tlmt ) (~ culsiff thl and th2 and . . . .
ACrl ! sol . : COLING-92 . NAN rEs . 2328 AOt ~: f1992857 l ) mlc . OFCOI , ING-92, NANTES , AU? .  2328 . I992 and t/mh Occur(hall , k\]) . Thus the probability of T i is given by P ( Ti ) = P (   ( tllr % . r3t lm ) u . . . . ~ ( tglc3 . . . r ~ tknvJk ~ pIn shortened form : P ( Ti ) = P ( u ( eltl xl ) p = lq = lT ile event stpq are no __L mutually exclusive  , since conslructions can overlap , and can include other constructions . The formula for timjoint probability of events E i is given by : nn 
P ( ,'3 Ei ) = 11l'O SilEi_l . , . h'l ) i = li = l Tile formula for the probability of combination of events Ei  ( that are not independent ) is given by ( see e . g . \[ Harris 1966\] ) : kP ( L/E i ) = XP ( E i ) - Xl ' ( ldi1~L'i2 ) + XP ( h'it , ' ~ Ei2 ~ Ei3) i = lii1 < i1 < i2 . ~ i3- . . . . +/- P ( E I~E2~ . . . c ~ lSk ) We will use Bayes ' decomposition formula to derive the conditional probability of  "1  ) given s . Let 7/~ and Tj be parses of s ; the conditional probability of Ti given s , is illen given by:
P ( Ti)P ( sFI"i)P ( r)P ( srl ~)
V (7) ts) .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 
P ( s)z~jP(Tj)P ( slTj)
Since P ( slTj ) is 1 for all j , we may write

P ( Tils ) . . . . . . . . .
~ p < rj )
A parse 1 ) of s with n m x in ml conditional probability P ( Tils ) is called a preferred parse of s . 

Several different implementations of DOP are possible  . 
In \[ Scholtes 1992\] a neural net implementation fDOP is proposed , ltere we will show that conventional rule-based parsing strategies can be applied tnDOP  , by converting constructions into rules . A construction Can be seen as a production rule , where the left hand-side of the rule is constituted by the root of rite construction and the right hand side is constituted by the leaves of the construction  . The only exmt condition is that of every such rule its corresponding construction should be remembered in order to generate a parse tree for the input string  ( by composing the constructions that correspond to the rulesilmt are applied  )  . For a construction t , the corresponding production rule is given by root  ( t ) ~ leaves ( OIn order to calculate the pteterred parse of an input string by maximizing the conditional probability  , all parses with all possible tuples of constructions must be generated  , which becomes highly inefficient . Often we are not interested in all parses of an aln biguous input string  , neither in their exact probabilities , but only in which parse is the preferred parse . Thus we would like to have a strategy fllat estimates file top of the probability hierarchy of parses  . " llaiscall be achieved by using Monte Carlo techniques  ( see e . g . \[ Hammersley 1964\] ) : we estimate the preferred parse by taking random samples frotn the space of possibilities  . This will give us a more effective approach dian exhaustively calculating the probabilities  . 

Although DOP has not yet been tested thoroughly 2 , we call already predicts on ic of its capabilities  . In DOP , the probability of a parse depends on all tuples of coustructious that generate that parse  . ~ lhemore different ways in which a parse can be generated  , the lligher its probability . This implies that a parse which can ( also ) be generated by relatively large constructions is favoured over a parse which can only be generated by relatively small constructions  . This means that prepositiot mlplu ' as e attxichments arid figures of speech can be processed adequately by I  ) OP . 
As 1o the problem of hmguage acquisition , this ntight seem problematic for DOP : with all " already analyzed corpus  , only adult language behaviour can be simulated . The problem of language acquisition is it tour perspective the problem of the acquisition of an initial corpus  , in which nonlinguistic input and pragmatics should playna important role  . 
An additional remark should be devoted here to formal granlmars and disambiguation  . Much work has been done to extend rule-based granunars with selectional restrictions such that the explosion of ambiguities is constrabled considerably  , l lowever , to represent semantic and pragmatic on straints i a very expensive task  . No one has ever succeeded in doing so except in relatively small grammars  . Furthermore , a basic question renmins as to whether it is possible to formally et l code all of die syntactic  , semantic alld pragmatic infomlation needed for disambiguation  . In DOP , the additional infornmtion that one can draw from a corpus of hand-marked structural annotations i that one can by pass the necessity for modelling world knowledge  , since this will autonmtically enter into the disarn biguation of structures by Imnd  . Extracting constructions from these structures , and combining them in the most probable way , taking into account all possible statistical dependencies between them  , preserves this world knowledge in the best possible way  . 
In conclusion , it may be interesting to note that our idea of using past lallguage xperiences instead of rules  , has much in corm nonwithStich's ideas about language  ( \[Stich1971\] )  . luStich's view , judgements of gralnmaticality are not determined by applying a precompiled set of gratmuar rules  , but rather have the character of a perceptual judgement on the question to what extent rite judged sent once ' lotiks like ' the sentences the language user has in his head as examples of granlmaticality  . The cot ) cretelanguag experiences of file past of a language user determine how a new utterance is processed  ; there is no evidence for file assumption that past language experiences are generalized into a consistent heory that defines the  2 Corpora that will be used to lust DOP , mcudetile Tosca Corpus , built at the University of Nijmugen , and possibly the Penn Trcebm~k , built at the Umversity of Pennsylvania . 
AcrEsDr:COLING-92 . NANTES . 2328 AOt ', q "19928 S8PROC . OFCOLING-92, NANTES , Aunt .  2328 .   1992 grammaticality and the structure of new utterances univocally  . 
References\[Bahl 1983\]: Bahl , L . , Jelinek , F . and Mercer , R . , ' A Maximum Likelihood Approach to Continuous Speech Recognition '  , in :/ EEE Transactions on Pattern Analysis and Machine Intelligence  , Vol . PAMI-5, No . 2 . 
\[ Fillmore 1988\] Fillmore , C . , Kay , P . midO ' Connor , M . , ' Regularity and idiomaticity in grammatical constructions : the ease of let alne '  , L , ' mguage 64 , p . 

\[ Hanmlersley1964\]: Hauunersley , J . M . and tlandscomb , D . C . , Monte C~lo Methods , Chapnum and Hall , London . 
\[ Hams 1966\]: Harris , 11, lbeory of Probability,
Addison-Wesley , Reading ( Mass).
\[Jelinek1990\]: Jelinek , F . , l . afferty , J . D . and Mercer , R . I , . , B~ic Methods of Probabilistic Context Free Granmuws  , York towntleights : IBMRC 16374 (#72684) . 
\[ Magerman 1991\]: Magemmn , D . and Marcus , M . , ' Pearl : A Probabilistic Chart P~u'ser' , in : Proceedings of the European Chapter of the ACL'91  , Berlin . 
\[ Martin 1979\]: M , ' min , W . A . , Preliminary analysis of a bre . adth-tirst parsing algorithin : Theoretical , and experimental results ( Technical Report No . TR-261) . 
MIT LCS.
\[Salomaa1969\]: Salomaa , A . , ' Probabilistic and weighted grmnmars ' , in : lnfom Jation and control 15 , p . 
529-544,\[Scha1990\]: Scha , R . , ' Language Theory and Language Technology ; Competence and Perfomumce ' ( in Dutch) , in : Q . A . M . de Kort & G . L . J . I , eerdam ( eds . ) , Computertoepassingen in de Ncerlandistiek , Almere : L and elijke Vereniging van Neerlandici . ( LVVNjaarboek )\ [ Scholtes1992\]: Scholtes , J . C . and Bloembergen , S . , ' The Design of a Neural Data-Oriented Parsing 0DOP   ) System ' , Proceedings of the Intonational Joint Conference on Neural Networks  1992  , Baltimore . 
\[Stich1971\]: Stich , S . P . , ' What every speaker knows ' , in : Philosophical Review 80 , p . 476-496 . 
ACRESDE COLING-92 , NANTES , 2328 AOt~l "1992859 PROC . OFCOLING-92, NANTES , AUG .  2328, 1992
