Three New Probabilistic Models
for Dependency Parsing : An Exploration *
Jason M . Eisner
CIS Department , University of Pelm sylvaifia . 
200S . 33 rdSt . , Philadelphia , PA 19104-6"89, USA
jeisner@linc , c is . upenn , edu

Alter presenting a novel O ( na ) parsing algorithm for dependency grammar , we develop three contrasting ways to stochasticize it  . We propose ( a ) a lexical at finity mode \] where words struggle to modify each other  ,   ( b ) a sense tagging model where words tluc-tuate randomly in their selectional preferences  , and ( e ) a . generative model where the speaker fleshes ( ) tit each word's syntactic and concep . ual structure without regard to the implications : for the hearer  . W ( ! also give preliminary empirical results from evaluating the three models ' p  ; Lrsing performance on annotated Wall Street Journal trMning text  ( derived fi ' om the Penn Treebank )  . in these results , the generative model performs significantly better than the others  , and does about equally well at assigning pa . rt-of-speech tags . 
1 Introduction
In recent years , the statistical parsing community has begun to reachout  ; for syntactic formalisms that recognize the individuality of words  , l , in k grammars ( Sleator and ' Pemperley ,  1991 ) and lexicalized tree-adjoining ranunars ( Schabes , 1992) have now received stochastic treatments . Other researchers , notwishing to abandon context-flee grammar ( CI " G ) but disillusioned with its lexica \] blind spot , have tried to re-parameterize stochastic CI"G in context-sensitive ways  ( Black et al ,  1992 ) or have augmented the formalism with lexical head words  ( Magerman , 1995; Collins ,  11996) . 
In this paper , we 1 ) resent a \[ lexible l ) robat ) ilistic parser that simultaneously assigns both part-of-sl  ) eech tags and a barebones dependency structure ( illustrate . dinl ! ' igure 1) . The choice ot ' a simple syntactic structure is deliberate : we would like to ask some basic questions about where h'x-ical relationships al  ) pear and how best , to exploit * This materia . l is based upon work supported under a National Science I % undation Graduate Fellowship  , and has benefited greatly from discussions with Mike Collins  , Dan M (: lame(l , Mitch Marcus and Adwait Ratnaparkhi . 
( a ) Tileman in the coiner taugh this dachsht , ldIO playgolfI '; OS
DT NN IN DT NN VB DP P . P $ NNTOVH NN/?man N ~ . . phty ~ j J - y , . % ( b ) The ill __~/ . ~ dach shund It ) golf .   ) fCO fllelhis file Figure 1: ( a ) Abare-l > ones dependen ( -yparse . \]'\] a <: h word points to a single t ) arent , the word it modities ; the head of the sentence points to the EOS ( end-of:sentence ) ma . rk . Crossing links and cycles arc not allowed .   ( b ) Constituent structure and sub ( : a tegoriza-tion may be highlighted by displaying the same dependencies as a lexical tree  . 
them . It is us cflfl to look into thes0 basic questions before trying to tine-tmm the performance of systems whose behavior is harder to understand  . 1 The main contribution of ' the work is to I ) ro-pose three dist in ( ' t , lexieal is thy l ) otheses abou ( .  ( , he probability space underlyings eHl\]ence structure  . 
We il\]ustrate how each hypothesis is ( : xl ) ressed in a depemteney framework , and how each can be used to guide our parser toward its favored solution  . Finally , we point to experimental resul ( ; s that compare the three hypotheses ' parsing performance on sentences fi : om the Wall  , b ' treeldour-hal . \]' he parser is trained on an annol , ated corpus ; no handwritten grammar is required . 
2 Probabilistic Dependencies
It cannot be emphasized too strongly that a gram -marital rcprcsentalion  ( de4 ) endency parses , tagse-quen(-es , phrase-structure trees ) does not entail any particular probability model . In principle , one couht model the distribution of dependency l ) arses l ( )ur novel parsing algorithm a/so rescues depen dency from certain criticisins : " l  ) ependency granl-mars .   .   . are not lexic M , and ( as far ~ as we know ) lacl ( a parsing algorithm of efficiency compara . ble to link grammars . "( LMfertyet ; LI . , 1992, p . 3) choice of l ; heright model is not a prior i ( A ) vious . 
One way to huild al ) robabilistie grammar is to specify what sequences of moves  ( such as shift an ( /reduce ) a parser is likely to make . It is reasonable to expect a given move to be correct about as often on test data  . as on training data . This is tire philosophy behind stochastic CF ( I ( aeline keta1 . 1992) , " history-based " phrase-structure parsing ( I-~lack et al ,  1992) , + m ( I others . 
I Iowever , i ) rol ) ability models derived from parsers sotnetimes focus on i  , lci(lental prope . r ties of the data . This ut W be the case for ( l , all i ' . rt yet M . , 1992)'s model for link grammar , l\['we were to adapt their top- ( h ) wn stochastic parsing str~tegy to the rather similar case of depen  ( lency grammar , we would find their elementary probabilities tabulating only non-intuitive aspects of the parse structure : Pr  ( wordj is the rightmost pre-kchihl of wordi\] i is a right-sl  ) in est , rid , descendant of one of the left children of a token of word k  , or else i is the parent of k , and il ) re (; edesjt ) recerlesk ) . : eW hile it is dearly necessary to decide whether j is a child of i  , conditioning that ( Iccision as a lrove may not reduce its test entropy as mne has at no relinguistically perspienous condition woul  ( / . 
We believe it is ffttil,\['u\[tode . sign prol > al ) ility models in del ) en ( let rt ly of t it ( ' pa . rser . In this see-lion , we will outline the three + lexicalist , linguistically per spicuous , qualitatiw ~ ly different models that we have ( leveloped a , nd tested . 
2 . 1 Mode l A : Bigram lex iea l a f f in i t ies Ngram tatters like  ( Church ,  1988;  . leline k 1985 ; Kupiec 1992 ; Merialdo 1990 ) take the following view of \] row ~/ , tagged sent ctrce enters the worh l . 
I"irst , ase . ( tuenee of tags is g('nexate . dae cordittg to a Markovl ) rocess , with t . h ( ' random choice of e~ch tag conditioned ou the previous two tags  . Second , a word is choseu conditional on each tag . 
Since our sentences have links as well as tags and words  , suppose that afl ; er the words are in-serte(l , each senl ; ence passes through a third step that looks at each pair of words and ran  ( lotnly decides whether to link them . For the resulting sentences to resemble realtort ) era , the . probability that word j gets linked to word i should b  ( ' le:~:i- ( : ally scnsiliv c : it should depend on the ( tag , word ) pairs at both i and j . 
' Fhe probability of drawing a given parsed sen- ( once froln the + population may then be expressed  2This correspouds to l , Mi'ert yel , al . 's centrals t ~ t tis-tk:(p .  4) , l'r(m+-IL , le , l , r ) , in the case where i's pa . rent is to the leftel i . i , j , k correspond to L , W , R respectively . Owing to the particular re ( :ursiw~strategy the p~trscruses to bre+tkup thes  ( !\[tl , ( ? n (: e , the statistic would be measured ~ ttldutilized only under the condition  ( lescribed above . 
( a ) Ihe\[nice of I hcsRu:k1%11
I ) TNNINI ) 1'NNVIII ) ( b ) tile price uf . the stock R'II\]YI"NN INI)TNNV iii)t , ' igure 3: (++) Th ( ,   , : or rect parse . (b ) A cotnmon cr , or if the model ignores arity . 
as (1) in \[ , ' igure 2 , where the random wMable LijG 0 , 1 is tiff word i is the parent of word j . 
Expression (1) assigns a probability to e . very possible tag-a . nd-link-annotated string , and these l ) robabilities unl to one . Many or the annotated strings exhibit violations such as crossing links and multiple parents which  , if they w creallowed , wouhlle talthe words express their lexical prefe  . r-ences independently and situttlta . ne:ously . WeSiAl ) -ulate that the model discards fl ' om the popula + tiontiny illegal structures that it generates  ; they do not appear in either training or test data  . Therefore , the parser described below \[ inds the likeliest legal structure : it maximizes the lexical preferences of  ( l ) within the few hard liuguistic coush ' ainls it nlrosed by the del  ) endency formalism . 
In practice , solrre generalization or " coarsen-lug " of the condition M probabilities in  ( 1 ) heaps to avoid tile e . ll ~ ets of undertrMning . For exal H-ph ' . , we folk)w standard prn(-tice ( Church ,  1988 ) in ngram tagging hy using ( 3 ) to al ) proxitllate the lit'st term in ( 2 )  . I ) ecision sal ) out how much coars-enittgt , olieare+o1'great pra(-t , ieal interest , but they ( lel ) et M on the training corpus an ( ltnayl ) eoln it-ted from a e on c <' . t ) tuM discussion of ' the model . 
' Fhe model in ( I ) can be improved ; it does not ( : aptrl r ( " the fact that words have arities . For ex-+Unl ) h . ', lh . e price of lh . csleek fell ( l " igure 3a ) will tyl > i cally 1 ) enl is analyzed under this model . Since stocks often fall, . sleek has a greater affinity f <> rfl : llthanl br @llen <: estock  ( as w <' . llasprice ) willen < ltt\[ ) t > oint itt g to the verl > . /' ( ell(lqgure 31>) , result , hit in a double subject for JNI and \[ eavitlg of childless  . 
' l'o Cal)i . nrew or daril , iesan ( lothe+rstil > cal , < , gr)riza-lionI ' aets , we must recognize that the . chihh:ert of a word like J~ll are not in ( le 4) en de . nt of each other . 
' Filesohttioniston lodi/'y(t ) slightly , further conditioning l , lj on the number and/or type of children of i that alreadys it between i and j  . This means that in I , he parse of Figure 3b , the link price -+ \]?~11 will be sensitive to the fact that fell already has aok  ) set chihl tagged as a noun ( NN )   . Specifically , tire price --+ fell link will now be strongly disfavored in Figure  '3b   , since verbs rarely Lalw ~ two NN del ) endents to the left . ByCOllt ; rast , price --> fell is unobjectionable inl ! ' igure 3a , rendering that parse more probable . ( This change ( ; an be rellected in the conceptual model , by stating that tirel , ij decisions are Hla ( leill increasing order of link length li--Jl and are no longer indepen  ( lent . ) 2 . 2 Model B : Seleetionali ) references In a legal dependency l ) axse , every word except for the head of the setrtence ( tile EOS mark ) hasI-\[Itom ( i+1 )  , twom(i+2)) . I \] I two , . d(i ) , two , ' dO )) (' e)l<i < nl <_i , j < nl'v(tword(i)\]t word(i+1) , tword(i+2)) ~ l' , ' ( tag(i ) Itag(i+1) , tag(i+2)) . P , '( word ( i ) Itag (/)) ( a ) Pr(words , tags , links ) c~Pr(words , tags , preferences ) = /' r(words , tags ) . Pr(preferences\]words , t~gs ) (4)\]- Il', . ( twom(i ) It wod(i+1), to , ' d(i+2)) . HI two, . d ( i ) ) 1 < i<n t < i<n / 1 +# r ight - k ids ( i ) ' ~ Pv ( words , t + gs , links ) = II1-\[P ,  . (two, . d(kid+(i )) It , gj+dd+_ , ( i )) , t + o , 'd ( i ) ) l < i<n \ c =- ( \]-k  #left+kids ( i )   )   ,   eT~0 kid ~ q_ 1 if c < 0 Figure 2: tligh-level views of model A ( formuhrs I3 )  ; model l : l ( for inul ; t4) ; and model C(lbrmula ,  5) . If i and j are tokens , then tword(i ) represents he pair ( tag(i ) , word(i )) , and L , jC0 , 1i ~ ~ ill " i is the p~m : nt of j . 
exactly one parent . Rather than having the model select a subset of the  ~2 possible links , as in model A , and then discard the result unless each word has exactly one parent  , we might restrict the model to picking out one parent per word to begin with  . Model B generates a sequence of tagged words , then specifies a parent or more precisely , a type of parent for each word j . 
Of course model A also ends up selecting a parent t breach word  , but its calculation plays careful politics with the set of other words that happen to appear : in the senter l  (  ; C : word j considers both the benefit of selecting i as a parent  , and the costs of spurning all the other possible parents /'  . Model B takes an appro ; ~ chat the opposite extreme , and simply has each word blindly describe its ideal parent  . For example , price in Figure 3 might insist ( with some probability ) that it " depend on a verb to my right . " To capture arity , words probabilistically specify their ideal children as well : fell is highly likely to want only one noun to its left  . 
The form and coarseness of such specifications ia parameter of the model  . 
When a word stochastically chooses one set of requirements on its parents and children  , it is choosing what a link grammarian would call a dis-juuct  ( set of selectional preferences ) for the word . 
We may thus imagine generating a Markov sequence of tagged words as before  , and then independently " sense tagging " each word with a disjunct  , a Choosing all the disjuncts does not quite specify a parse  , l lowever , if the disjuncts are sufficiently specific , it specifies at most one parse . Some sentences generated in this way are illegal because their disjuncts cannot be simultaneously satisfied  ; as in model A , these sentences are said to be removed fi'om the population  , and the probabilities renormalized . A likely parse is therefore one that allows a likely and consistent aln our implementation  , the distribution over possible disjuncts is given by a pair of Markov processes  , as in model C . 
set of sells (' , tags ; its probability in the population is given in (4) . 
2.3 Model C : Recursive generation
The final model we prol ) ose is a generation model , as opposed l ; o the comprehension mo ( l-elsA and B ( and to other comprehension mod c , ls such as ( l , afferty et al , 1992; Magerman , 1995; Collins ,  1996)) . r \]' he contrast recalls a no hl debate over spoken language  , as to whether its properties are driven by hearers'acoustic needs  ( coml ) rehen-sion ) or speakers ' articulatory needs ( generation )  . 
Models A and B suggest that spe~kers produce text in such a way that the grammatical relations can be easily decoded by a listener  , given words ' preferences to associate with each other and tags ' preferences to follow each other  . But model C says that speakers ' primary goal is to flesh out the syntactic and conceptual structure \[' or each word they utter  , surrounding it with arguments , modifiers , and flmction words as appropriate . According to model C , speaker should no the sitate to add extra preposition M phrases to a noun  , even if this lengthen some links that are ordinarily short  , or leads to tagging or attachment mzJ ) iguities . 
The generation process is straightforward . Each time a word i is added , it generates a Markov sequence of ( tag , word ) pairs to serve , as its left children , and an separate sequence of ( tag , word ) pairs as its right children . Each Markov process , whose probabilities depend on the word i and its tag  , begins in a speciM STAIT state ; the symbols it generates are added asi's children  , from closest to farthest , until it re~ches the STOP state , q'he process recurses for each child so generated  . This is a sort of lexicalized contextfree model . 
Suppose that the Markov process , when gemcrating a child , remembers just the tag of the child's most recently generated sister  , if any . Then the probability of drawing a given parse f i ' om the population is  ( 5 )  , where kid(i , c ) denotes the cth-closestright child of wordi , and where kid(i , O ) = START and kid(i ,  1 + # , ' ight-kids(i )) = STOP . 
342  ( a )   ( b ) dach shundov cr there can really phty dachs hund ow : r there can really play I  , ' igure 4: Spans \]) ~ u ' ticipa , ting , in the (: or ru(:l . i ) a , rsc of 7' h , at dachs / * undo ' + wr there c(+uv calhlph+g golf ~ . ( st ) has one pa , rcnt , lcs scnd wor(I ; its sul ) sl ) + tn(b ) lists two . 
( c < 0 in ( h ' xesl('ft children , )' Fhismay bcthoughto\["asa , non-linca . rl ; rigrrmt model , where each t ; agg (' d wo M is genera , l , ed1) as cd on the l ) a . r('nl , 1, ~ gg (: d wor(l and ; tsistx ' r tag . ' l'he links in the parse serve Lopicko , tt ; t , her (' Jev ; mtt , rit:;t+a , n~s , and a . rc'chosen1;og('t ; l , rigrams t , lml , ot ) l , imiz(~t , hcglohMt , a , gging . ' l't t ; t l ; the liuks also ha . t)l)ent;o;ulnot,;:d,(' . 
useful set nant ; icrela , tions is , from this t > crsl ) ective , quil . ea (- cidcn , a , l . 
Note that the revised v (' , rsiol ~ of ulo(h:tAuses prol ) a , bilit , ics / " @ ink to chihl I child , I ) arenl , , closer-('hihh:en ) , where n . )( le\](; uses l'v(link1 , o child \] parent , , eloscr-chil(h'en ) . ' l ' his is I ) c(:;, . t ~ se model Aassunw . s1, lu~l,I,h(' . (: hild was i ) reviously gencrat , edI ) yalin(;a , r l ) r o c (' s s , amlallt ; hal , is nec-ess+u'yist , oli . k1, oit, . Model ( ~ a , cl , ually g ( , n(;ral , est , he chihl in the processo \[' liuking to il ,  . 
3 Bottom-\[ ) i ) Dependency Parsing luth is sec . tA on we sket (: hour dependel . ' yl)m'sing ; dg ; oril , hnl:~noveldytmJni (' . -l ) rogr ; m Jndngm (' . l , hod1 , o as set nh lel , hemosl , l > rol ) a , ble+i ) a . rseFrom the bet,-tomUl ) . The algori@m++(l(Is one link at al , ime , nmking il ; easy to multiply oul , the hie(lois'l ) rol mhility l'a(:t , or s . It , also enforces I , hc special direcLion ; dil , y requiremenl~s of dependency gra . nnnar , 1; hel ) rohibitions on cycles mMnlultiple par('nl , s . 4'\['\] 10 liic . t \] to dllse dissimilart ; otie CKY met . hod of cont . exl , -fr('el ) ~ rsing , which combines a JIM ys (: s of shorl , er substrings into analys<:s of progressively longer ones  . Multiplea . na . lyses It ; wel , hcs ~ tnm signature if t ; hey are indistinguishal > lei , their M ) ility to (; Otlll ) ill (? wit , hother analyses ; if so , the parser disca , rds all but , the higlmsl , -scoring one . 
CI , : Yt ' cquit ', ; s ( )( ?, . : t ~ ~) t . i , , , , '+ utdO(,, . :' . ~) sp + . ' . ,; , where n is the lenglih of 1 , hes(mtcn(:cand , s is a , nUpl ) (; rboui Monsignal ; ures 1) ersubsl ; ring . 
Let us consider dependency parsing in t ; his framework . () he mighl ; guess that each substa'ing ; mMys is shot tld bct+lcxicM tree ; + tagged he ; ul-word plus a Jl Icxical sulfl ; rees depend c'nt , uponi / ,  . ( Seel " igure 111 . ) l lowew , r , if a . o:/tst , i l , cnts?11 , Mmled depend ( reties a , repossible , a . ndamin or va , ria , ntha . ndlesthesit nplcr(:~tse of linkgra . tnltl ;- u ', hi-deed , abstra . ctly , the a . lgorithm rescmbies ; tc\](, . a amr , bottom-up vcrsiou of the topdown link gr~tmm~tr pa  , rser develol ) ed independently by ( l , Ml : ' crtyetaJ . , 1992) . 
.  .   .   .   .   . ~ fz_ .   .   .   .   .   .   .  ~ ~  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . + _~ . ._ . _  .   .   .   .   .   .   . , % i . y -- . . . .< ? ? ? ? ? ? ? ~ o d I a ( loll sfl!Slm , ,) Ji , b(_right subspan ) ' , I"igll'e5'Theass , : , mbly of a span c from two sm : LII crspaus ( a a , ndb ) ~ tmlacove . ring link . Only . is miuimal . 
probabilistic behavior depends on iL ~ he . adword ( ; helc xic MisL hypoiJms is tit and ilt'erent ; lyhc ~: uhxla . na . lyses need dilt'eren I ; sigmrtures . Therea . real . 
lca+sl,koft,hcscfora,s/ibst;rhl,~Ofle . .e;IJI k , when ce Ge houn(t , ,~ :: t : = ~( u ) , giving :; il , illmCOml ) lex-it , y of t l ( , s ) . (( ~ ollins , 19 . %) usest , hist ~(,' . "-') a , lgoril , lml(lireclJy(t , ogel , h('r wil , hl ) runiug ;) . 
\'\% I)r Ol)OSea , ua Jl , ermtl , ivea , I)l)roa . (' hl , ha , I , I ) reserves the OOP ) hound , hls~ca(t of analyzing sul ) st , ri . gsasl cxicalt , rees that , will be linked t , ogo J , her in ( , ola , rg cr h'~xi ca , Il , rees , t , licI ) arsc , r will an a , lyzeI , hc'lna , suon-const , itm' . nt , sl ) a:n . st ; haJ , will he coucat ; cm~t , ed into larger spans . AS l ) a , n cousis l ; sel ' > : ~; . t . i . :e , l <; words ; l , ; ~ gsI ' or a , ll these words cx (: el ) l , possibly the last ; ; t list , of all del . ' mle . cy\]i , ksmuong the words in l , hcSl ) an ; and l ) erha , l)Ss ( ) lue other in l ' or nml , ic , n carried a , long in t ; lu , siren's sig-na J , mc . No cych's , n , ull , iph'l ) a , rcnts , or (' , ' ossi , tgliul . : s are M lowed in the Sl ) a . u , and each Jut , re ' halword of ' l , he Sl > ml must ha , vc ~ Ira . reiniutheq ); m+Twosl > a , nsat <> illustra J , edinI " igured , ' l ' hese di-a , gra . ntsa , rcI , yl)ica , l : a , Sl ) a , nel " a(Icp end ct . : yl ) a + rsc may consist , of oil , hera I ) a + rcn ( , lessend word and some o\['its de s(:cn < hmt , so noneside(l"igtu'c4a ) , or two parent , less cnd words , with a . llt , heright & " s('(mda , nLsoF ( me and all l ; hcM'I , dc scen(hml , s of I , I i ( ~ el , her ( lq , e ; urc4 b ) . '1' tl ( . ' im , uil Aonis I , haJ , L\]le . i l l l , (' A ' halpart ; of a , span is gra , nmmticallyiuert : excel ) l , Iortit (' , cml words dachsh , u ~ tdmid play , l ; hcstruclure o1'e a , ch span is irrelew ml , I , ot , \]1(; Sl > Cm'sal ) ility t , o cotn bin ciuful , ure , sosl ) a , ns with different inter-1mlstrucl , tu'eca , n colnlmte to bct ; hcI ) est , -scoring span wil , ha , lm , rticula , rsignal ; urc . 
117 sl ) an a , ctM sont , he saanc word il ;\[ ha , l , s t , al'l , sspan b , t , h(;nlawI ) a , rs(ertriesl ; oc(>ml > in eI , hcl , wo spans I ) y cove , red-(-(mvatcnation(l"igur(;5) . 
The I , wo Col ) ies of word i arc id c . nt , i\[ied , a , fl , er which a M'l , wa Morright waMcove \]\[' ing link is ol ) l ; i on Mlyadded I ) ct , wceut , h('c . d wor ( tsoft , h0 .  , . > . vsf ) a , n . Anytlep cudcncy parse ca , nI ) c builtIll :) hyeovered-coit ca , tena , i ; ion . When the l ) a , rsercovcrcd-('O \] lCaJ , enat , cs(~trodb , it , ol ) l , a insup to IJ tr ce newSlmUS(M't , wa , rd , right , war(I , and no coveritlg \] in k ) . 
The <' o ', , ered-(:oncaJ , cnal , ion of (+ a . ndb , I ' ornfing (' , is 1) arr cdunh ; ssit , tricots terra , in simple test ; s : ? . must , I ) e minimal ( not , itself expressihle++saconca Lenal , ion of narrowers paus ) . This prcven Lsus from assend > ling cinumltiple ways  . 
? Since tim overlapping word will bcint ; ertta , ltoc , it ; Illll 81\[ , ha , ve?g parenl ; incx a , (; L\]y oile of a told b . 

HPr(tword(i ) It word(i+1), tword(i+2)) . HPr(i has peels that j satisfies It word ( i ) , tword(j )) (6) k <_i < gk < i , j < g with i , j linked HPr(LijItW?rd(i)'t word(j ) , tag'(next-closes t-kid(i ))) . HPr(LiJItW ? rd(i)'t word(j), . . . ) (7) k<_i,j < g with i,j linked k<i<(,(j<kor ~ . < j ) ? c must not be given a covering link if either the leftmost word of a or the rightmost word of b has a parent  .   ( Violating this condition leads to either multiple parents or link cycles  . ) Any sufficiently wide span whose left endword has a parent is a legal parse  , rooted at the EOS mark ( Figure 1) . Note that a span's signature must specify whether its end words have parents  . 
4 Bottom-Up Probabilities
Is this one parser really compatible with all three probability models ? Yes  , but for each model , we must provide a way to keep tr~tck of probabilities as we parse  . Bear in mind that models A , B , and C do not themselve specify probabilities for all spans  ; intrinsically they give only probabilities for sentences  . 
Model C . Define each span's score to be the product of all probabilities of links within the span  .   ( The link to i from its eth child is associated with the probability Pr  (   .   .   . ) in (5) . ) When spans a and b are combined and one more link is added  , it is easy to compute the resulting span's score : score  ( a )  , score(b ) . /?r ( covering link ) ) When a span constitutes a parse of the whole input sentence  , its score as just computed proves to be the parse probability  , conditional on the tree root EOS , under model C . The highest-probability parse can therefore be built by dynamic programming  , where we build and retain the highest-scoring span of each signature  . 
Model B . Taking the Markov process to generate ( tag , word ) pairs from right to left , we let ( 6 ) define the score of a span from word k to word ( ? . 
The first product encodes the Markovian probability that the  ( tag , word ) pairs k through g-1 are as claimed by the span , conditional on the appearance of specific ( tag , word ) pairs at g ,  ~+1 . ~ Again , scores can be easily updated when spans combine , and the probability of a complete parse P , divided by the total probability of all parses that succeed in satisfying lexical preferences  , is just P's score . 
Model A . Finally , model A is scored the same as model B , except for the second factor in (6) , S The third factor depends on , e . g . , kid(i , c-1) , which we recover fl ' om the span signature . Also , matters are complicated slightly by the probabilities associated with the generation of STOP  . 
6Different k-g spans have scores conditioned on different hypotheses about tag  ( g ) and tag ( g+1 )  ; their signatures are correspondingly different . Under model B , ak- . g span may not combine with an 6-~n span whose tags violate its assumptions about g and g +  1  . 
11 AI~1cIc'T-xI ~, ~ o1~1. oI
Non-punt 88.9 89.8 89.6 89.'1 89.87 7. J
Nouns 90.1 89.8 90.2 90.4 90.0 S(;.2
I , exverbs 74 . 6 75 . 9 7 . "/ . 3 75 . 8 73 . 3 67 . 5 ' Fable t : Results of preliminary experiments : Per-centage of tokens correctly tagged by each model  . 
which is replaced by the less obvious expression in  ( 7 )  . As usual , scores can be constructed from the bottom up ( thought word ( j ) in the second factor of ( 7 ) is not available to the algorithm , j being outside the span , so we back off to word(j )) . 
5 Empirical Comparison
We have undertaken a careful study to compare these models ' success at generalizing from training data to test data  . Full results on a moderate corpus of 25 , 000+ tagged , dependency-annotated Wall Street Journal sentences  , discussed in ( Eisner ,  1996) , were not complete hi ; press time . However , Tables 1   2 show pilot results for a small set of data drawn from that corpus  .   ( The full resnlts show substantially better performance  , e . g . ,  93% correct tags and 87% correct parents fbr model C , but appear qualitatively similar . ) The pilot experiment was conducted on a subset of  4772 of the sentences comprising 93  , a ~0 words and punctuation marks . The corpus was derived by semiautomatic means from the Penn Treebank  ; only sentences without conjunction were available  ( mean length = 20 , max=68) . A randomly selected set of 400 sentences was set aside for testing all models ; the rest were used to estimate the model parameters  . In the pilot ( unlike the full experiment ) , the parser was instructed to " backoil "' from all probabilities with denominators <  10  . For this reason , the models were insensitive to most lexical distinctions  . 
In addition to models A , B , and C , described above , the pilot experiment evaluated two other models for comparison  . Model C ' was a version of model C that ignored lexical dependencies between parents and children  , considering only dependencies between a parent's tag and a child's tag  . This model is similar to the modelnsed by stochastic CFG  . Model X did the same ngram tagging as models A and B  ( ~ . = 2 for the preliminary experiment , rather than n = 3) , but did not assign any links . 
Tables 1   -2 show the percentage of raw tokens that were correctly tagged by each model  , as well as the proportion that were correctly attached to 

NOLIn 817~1 verbs\[At ~--(' Cr-\[L~5 .   . , ~ r8 . 1S ~ , , a . ~ 47 . 3 ~ lr~sArr . ~I'~1~~-L40: , < ~ A_-~~_'l'~d)le2:\] . csults of preliininary (, Xl ) crimcnts : Per . 
contage of tokens corrc0Lly attached Lotheir par-onl ; s by each model . 
their parents . Per tagging , baseline per\[ol:lnance Wa , SI/leaSlli'ed by assigniug each word ill the test set its most frequent tag  ( i \[' any ) \[' roiii the train-lugset . Th ci in usually low I ) a seliuet ) crJ ' orillance I : esults\['l ' OlllkL conil ) iuation of ; tsHia Jll > i lot Lr ; ~ ill-ing set and ; tInil(lly(~xten(e(It~gset . 7\V col ) served that hit hit heka . ining set , detei:lniners n-lost colrinlonly pointed t . o the following ; word , so as a parsing baseline , we linked every test dct crnihler to the following word  ; likewise , w c linked every test prc positior , to the preceding word , and so () 11 , The l ' J atterll S in the preliuli/lary data ~ ti'estriking  , with w : rbs showing up as all a Fea el ( lil\[iculty , alld with SOll le\]t lod c is cl < ; arly farillg bctter I , \[I ; tll other . The si in plc stand\['astestuiodel , the l'(~cur--siw ~ , generation uiodel (7 , did easily i . hebcsl . , job of <' i-q ) turing the dependency s/ . ructur c('l'able2) . 
It m is attach cd t . hc fe west words , both overall aud in each categol : y . This sugg csts that sut ) eategjorization 1 ) rcferc\[lccs the only I ' ~ Lctor ( ' on sidered by model ( JI ) lay a substantial role in I ; hesti:uc-lure of Trcebanks cntcn(-cs . ( lnd ccd , tii ( ; erl ; or sill model I ~ , wliichpe:l : for Hled worst across the bO~Lr(l , were very frequently arity erl : or s , where ttie desire of a chihl to ~ Ltta ( : hLO a1 ) articular parent over - . 
calnether chi(:i ; ail( ; e of tile \[) are i it to a (: (- el ) tuiore children . ) A good deal of the l , arsi0_gSll(;(' , ess of inoclel ( 7 seems to h~vearisen from its k/iowle ( lgc , of individ--tiff . words , as we cxpe(:ted . This is show fi by the vastly in l ~ riorl ) Cl ; for n i a H (' e o \[' I ; lc control , model ( f t . Onl ; heot\]ier hand , I ) oth ( 7 an ( l ( J ' were conl-petitiv c with t\[10   oth0r UlOdCiSi ~ l ; tagging . This shows that at ~ Lg can 1 ) e predicted ~ d ) out as well\['rolri Lhetags of its putative p ; Lrel , t ; rods il ) \] in < g as it ( ' anfi X ) ill the\[~agsO\["string-a ( lja ( : cnt words , eVell when there is (' onsider al)le / ; l : O Filld cter in in -- ing the parent and s\[bling . 
6 Conclusions
I ~ arc-bories dependency grammar which requires 1lO Ihik labels > no ~ ralfliiiai ' , and It Ofll~St Olirlder standi Saclean tcstbcd for studying the lexical a\[lini Lies of words  . Wc believe filial ; this iSall ill , per , all , line of iliv cstigative research > olle that is likely to produce both useful parsing tools and signilic aut insights ~ t boilt language niodeling  . 
7 Wellsed distinctive t ~ tgs for a , uxi\[ia , ryverbs ;- I , ll(I for words being used as noun modifiers ( e . g . , participles ), bec<x use they ha . vevery ditferent subca . tcg or iz ~> lionfra . mes . 
As a lirst step in the study of lexic Ma@n-ity , we asked whether there was a " natural " way to stochasticize such ~ siint  ) leform Misma . s dependency , hif ~ ct , w c have now exhibited three promising types of l node l for this simple problem  . 
Further , we have develol ) cd a novel parsing algo-rithm to compare the sc hyt  ) otheses , with results tim , so far favor the spe;tker-oriented model C , eveu in written , edited Wall Slrcet dournal I ~ cxt . 
To our knowledge , the relative merits of speaker or icn / , cdV(~l'SilShcarer-orienl , ed probed ) ills , it syn-l . iL ? in o(h ; Is ii aveu oL been investigated l ) e for e . 
ll , ef el ' ellces
Ezra Bla . (: k , Fred , lelinck , et a . 1 .  1992 . Towards history-ba , sedgram nl~u:s:using richermod ( ,  . ls\[brprobabilistici,~trsing . \ [ u Fifth I ) AI ~, FA Worksh . opou ?' pecch and Natural Language , Arden (7 on fcrcn(:cCeutcr , l l n r r i m ~ m , New York , Febrl , u'y . 
\['(enne . thW . (3mr(:h .  1988 . A stochastic parts pro-gi:nt lla , nd noun l ) hra , se parser for unrestri(:tcd text . 
In /' roe , of the 2rid ( ; on f . on Applied Natural Lan-g'uagel Jroccssing ,  136 148 , Austin , TX . Asso(:i ~ Lti ( , n for (' ~ omput ~ L timmll , inguistics , Morristowu , N . I . 
Mi(:ha . el . / . (', ollins .  1996 . A new statistical parser based on bigr ~ unlexi ( : ~ fl del ) cn deucies , h , l ~ r occ . cd-iT tfJS of t it ( ; 24th ACL , S ~ l , nt  ~ , , (~171'Z , (\] A , July . 
Ja . sol!1'3 isner .  199(; . An empirical ( : omp ~ H'isonf prob-~dfilityn lodcls for dependeucy gl : a  , lnnlaJ : . Teehnic ; dI Leport IRCS 9611 , University of Pennsylva Jtilt . 
I !' red . felinck .  1985 . M~rkovsour (: e modeliug of text gener ~ Ltiou . In . I . Skwirzinski , editor , hnpact of IS"o-tossing7 ~ chniquesou ( ; ommunication , /) or drc (: ht , l " red Jelinek ,   , lohn 1) . l , Mferty , aml Robert1, Mercer . 
I . ?)92 . I\]~si (: niethodselprob ~ dfilistic context-fre( ,  . 
~ INI , I"I'IILI 7S . lit ? ' pccchtlccoqnition and U ~ zd crstand-ing : l ? ecent Advances  , Trends , and Applications . 
.I . Knpie . c .  1!392 . Iobustl ) arD of-speech ta . gging using a . hidden Ma , rkov model . (7 omput cr ? ' pccch . rid
Language , 6.
.\]ohu t , ~ Lfferty , I ) ~ ufielSle ~ ttor , ~ uidI)~vy'\['cmperley . 
1992 .   ( ~ ramm~Ltica Jtrigr ~ mm : A prob ~ bilistic model of link gr~mnnarIn  15"oc   . of the AAAI Conf . ont ) robabilistic Approaches to Natural Language , Oct . 
l ); wid M~tgerul~n . 1 l !) 95 . St ~ ttisti(:~d decision-tree models for p~u'sing , in Proceedings of the 33rd An -' nual Meeting of the ACL , l  ~ ost on , MA . 
Igor A , Mel'(:uk .  1988 . l)cpcndcncySyntax : 7? worg and l'racticc . St ~ te University of New York Press . 
IL Meria . hlo .  1990 . Tagging text with ; L probabilistic model , lul ~ roccc dinw of the IBM Natural Language . 
17'L , Paris , Fra.nce , pp . 161-172.
Yves S(:ha . bes . L992 . Stochastic lexi(:alized tree-~t djoining gra . mmars , litl'r occc dings of C()lHNG'-92, Na . nl . es,I )') ' auce, . lnly . 
I ) n niel Sleator and DaxyT cmperl cy .  1991 . Pro:sing I " , nglish with ~ tI , iuk(h: , ~ mm~mTe (: hnicifl report CMU . -('S-91-196 . ( iSDept . , C~m , egic Melk ) ntluiv . 

