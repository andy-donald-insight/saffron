Coling 2010: Poster Volume , pages 436?444,
Beijing , August 2010

ABSTRACT
Reranking for Information Retrieval
aims to elevate relevant feedbacks and
depress negative ones in initial retrieval result list . Compared to relevance feed-back-based reranking method widely adopted in the literature , this paper proposes a new method to well use three features in known negative feedbacks to identify and depress unknown negative feedbacks . The features include : 1) the minor ( lower-weighted ) terms in negative feedbacks ; 2) hierarchical distance ( HD ) among feedbacks in a hierarchical clustering tree ; 3) obstinateness strength of negative feedbacks . We evaluate the method on the TDT4 corpus , which is made up of news topics and their relevant stories . And experimental results show that our new scheme substantially outperforms its counterparts.
1. INTRODUCTION
When we start out an information retrieval journey on a search engine , the first step is to enter a query in the search box . The query seems to be the most direct reflection of our information needs . However , it is short and often out of standardized syntax and terminology , resulting in a large number of negative feedbacks . Some researches focus on exploring longterm query logs to acquire query intent . This may be helpful for obtaining information relevant to specific interests but not to daily realtime query intents . Especially it is extremely difficult to determine whether the interests and which of them should be involved into certain queries . Therefore , given a query , it is important to ? locally ? ascertain its intent by using the realtime feedbacks.
Intuitively it is feasible to expand the query using the most relevant feedbacks ( Chum et al , 2007). Unfortunately search engines just offer ? farraginous ? feedbacks ( viz . pseudo-feedback ) which may involve a great number of negative feedbacks . And these negative feedbacks never honestly lag behind relevant ones in the retrieval results , sometimes far ahead because of their great literal similarity to query . These noisy feedbacks often mislead the process of learning query intent.
For so long , there had no effective approaches to confirm the relevance of feedbacks until the usage of the web clickthrough data ( Joachims et al ., 2003). Although the data are sometimes incredible due to different backgrounds and habits of searchers , they are still the most effective way to specify relevant feedbacks . This arouses recent researches about learning to rank based on supervised or semisupervised machine learning methods , where the clickthrough data , as the direct reflection of query intent , offer reliable training data to learning the ranking functions.
Although the learning methods achieve substantial improvements in ranking , it can be found that lots of ? obstinate ? negative feedbacks still permeate retrieval results . Thus an interesting question is why the relevant feedbacks are able to describe what we really need , but weakly repel what we do not need . This may attribute to the inherent characteristics of pseudo-feedback , i.e.
their high literal similarity to queries . Thus no matter whether query expansion or learning to rank , they may fall in the predicament that ? favoring ? relevant feedbacks may result in ? favoring ? negative ones , and that ? hurting ? negative feedbacks may result in ? hurting ? relevant ones.
However , there are indeed some subtle differences between relevant and negative feedbacks , e.g . the minor terms ( viz . low-weighted terms in texts ). Although these terms are often ignored in Negative Feedback : The Forsaken Nature Available for Reranking Yu Hong , Qingqing Cai , Song Hua , Jianmin Yao , Qiaoming Zhu School of Computer Science and Technology , Soochow University jyao@suda.edu.cn on mining relevant feedbacks that have the same topic or kernel , they are useful in distinguishing relevant feedbacks from negative ones . As a result , these minor terms provides an opportunity to differentiate the true query intent from its counterpart intents ( called ? opposite intents ? thereafter in this paper ). And the ? opposite intents ? are adopted to depress negative feedbacks without ? hurting ? the ranks of relevant feedbacks.
In addition , hierarchical clustering tree is helpful to establish the natural similarity correlation among information . So this paper adopts the hierarchical distance among feedbacks in the tree to enhance the ? opposite intents ? based division of relevant and negative feedbacks . Finally , an obstinateness factor is also computed to deal with some obstinate negative feedbacks in the top list of retrieval result list . In fact , Teevan ( Teevan et al , 2008) observed that most searchers tend to browse only a few feedbacks in the first one or two result pages . So our method focuses on improving the precision of highly ranked retrieval results.
The rest of the paper is organized as follows.
Section 2 reviews the related work . Section 3 describes our new irrelevance feedback-based reranking scheme and the HD measure . Section 4 introduces the experimental settings while Section 5 reports experimental results . Finally , Section 6 draws the conclusion and indicates future work.
2. RELATED WORK
Our work is motivated by information search behaviors , such as eyetracking and click through ( Joachims , 2003). Thereinto , the clickthrough behavior is most widely used for acquiring query intent . Up to present , several interesting features , such as click frequency and hit time on click graph ( Craswell et al , 2007), have been extracted from clickthrough data to improve search results . However , although effective on query learning , they fail to avoid the thorny problem that even when the typed query and the clickthrough data are the same , their intents may not be the same for different searchers.
A considerable number of studies have explored pseudo-feedback to learn query intent , thus refining page ranking . However , most of them focus on the relevant feedbacks . It is until recently that negative ones begin to receive some attention . Zhang ( Zhang et al , 2009) utilize the irrelevance distribution to estimate the true relevance model . Their work gives the evidence that negative feedbacks are useful in the ranking process . However , their work focuses on generating a better description of query intent to attract relevant information , but ignoring that negative feedbacks have the independent effect on repelling their own kind . That is , if we have a king , we will not refuse a queen . In contrast , Wang ( Wang et al , 2008) benefit from the independent effect from the negative feedbacks . Their method represents the opposite of query intent by using negative feedbacks and adopts that to discount the relevance of each pseudo-feedback to a query.
However , their work just gives a hybrid representation of opposite intent which may overlap much with the relevance model . Although another work ( Wang et al , 2007) of them filters query terms from the opposite intent , such filtering makes little effect because of the sparsity of the query terms in pseudo-feedback.
Other related work includes query expansion , term extraction and text clustering . In fact , query expansion techniques are often the chief beneficiary of clickthrough data ( Chum et al , 2007).
However , the query expansion techniques via clicked feedbacks fail to effectively repel negative ones . This impels us to focus on unclicked feedbacks . Cao ( Cao et al , 2008) report the effectiveness of selecting good expansion terms for pseudo-feedback . Their work gives us a hint about the shortcomings of the one-sided usage of high-weighted terms . Lee ( Lee et al , 2008) adopt a cluster-based resampling method to emphasize the core topic of a query . Their repeatedly feeding process reveals the hierarchical relevance of pseudo-feedback.
3. RERANKING SCHEME 3.1 Reranking Scheme The reranking scheme , as shown in Figure 1, consists of three components : acquiring negative feedbacks , measuring irrelevance feedbacks and reranking pseudo-feedback.
Given a query and its search engine results , we start off the reranking process after a trigger point . The point may occur at the time when searchers click on ? next page ? or any hyperlink.
437
All feedbacks before the point are assumed to have been seen by searchers . Thus the unclicked feedbacks before the point will be treated as the known negative feedbacks because they attract no attention of searchers . This may be questioned because searchers often skip some hyperlinks that have the same contents as before , even if the links are relevant to their interests . However , such skip normally reflects the true searching intent because novel relevant feedbacks always have more attractions after all.

Figure 1. Reranking scheme
Another crucial step after the trigger point is to generate the opposite intent by using the known negative feedbacks . But now we temporarily leave the issue to Section 3.2 and assume that we have obtained a good representation of the opposite intent , and meanwhile that of query intent has been composed of the highly weighted terms in the known relevant feedbacks and query terms.
Thus , given an unseen pseudo-feedback , we can calculate its overall ranking score predisposed to the opposite intent as follows : scoreIscoreOscoreR ___ ??= ? (1) where the O_score is the relevance score to the opposite intent , I_score is that to the query intent and ? is a weighting factor . On the basis , we rerank the unseen feedbacks in ascending order.
That is , the feedback with the largest score appears at the bottom of the ranked list.
It is worthwhile to emphasize that although the overall ranking score , i.e . R_score , looks similar to Wang ( Wang et al , 2008) who adopts the inversely discounted value ( i.e . the relevance score is calculated as - scoreI _ scoreO _?? ) to rerank feedbacks in descending order , they are actually quite different because our overall ranking score as shown in Equation (1) is designed to depress negative feedbacks , thereby achieving the similar effect to filtering.
3.2 Representing Opposite Intent
It is necessary for the representation of opposite intent to obey two basic rules : 1) the opposite intent should be much different from the query intent ; and 2) it should reflect the independent effect of negative feedbacks.
Given a query , it seems easy to represent its opposite intent by using a vector of high-weighted terms of negative feedbacks.
However , the vector is actually a ? close relative ? of query intent because the terms often have much overlap with that of relevant feedbacks.
And the overlapping terms are exactly the source of the highly ranked negative feedbacks . Thus we should throw off the overlapping terms and focus on the rest instead.
In this paper , we propose two simple facilities in representing opposite intent . One is a vector of the weighted terms ( except query terms ) occurring in the known negative feedbacks , named as )( qO ? , while another further filters out the high-weighted terms occurring in the known relevant feedbacks , named as . Although )( rqO ?? )( qO ? filters out query terms , the terms are so sparse that they contribute little to opposite intent learning . Thus , we will not explore further in this paper ( Our preliminary experiments confirm our reasoning ). In contrast , not only differs from the representation of query intent due to its exclusion of query terms but also emphasize the low-weighted terms occurring in negative feedbacks due to exclusion of high-weighted terms occurring in the known relevant feedbacks.
)( qO ? )( rqO ?? 3.3 Employing Opposite Intent Another key issue in our reranking scheme is how to measure the relevance of all the feedbacks to the opposite intent , i.e . O_score , thereby the ranking score R_score . For simplicity , we only consider Boolean measures in employing opposite intent to calculate the ranking score
R_score.
Assume that given a query , there are known relevant feedbacks and
N
N known negative ones . First , we adopt query expansion to acquire the representation of query intent . This is done by pouring all terms of the relevant feedbacks and query terms into a bag of words , where all the occurring weights of each term are
N terms to represent the query intent as )( rqI ++ .
Then , we use the N negative feedbacks to represent the n-dimensional opposite intents . For any unseen pseudo-feedback u , we also represent it using an n-dimensional vector which contains its n top-weighted terms . In all the representation processes , the TFIDF weighting is adopted.
)( rqO ?? )( uV
Thus , for an unseen pseudo-feedback u , the relevance scores to the query intent and the opposite intent can be measured as : (2) } )( ),( {)(_ } )( ),( {)(_ rqOuVBuscoreO rqIuVBuscoreI ??= ++= where indicates Boolean calculation : },{ ?? B (3) ?? ? ? ?= ?=?
Yxif
Yxif
Yxb
XxYxbYXB i i i ii ,0 ,1 },{ },,{},{ In particular , we simply set the factor ? , as mentioned in Equation (1), to 1 so as to balance the effect of query intent and its opposite intent on the overall ranking score . The intuition is that if an unseen pseudo-feedback has more overlapping terms with )( rqO ?? than , it will has higher probability of being depressed as an negative feedback.
)( rqI ++
Two alternatives to the above Boolean measure are to employ the widely-adopted VSM cosine measure and Kullback-Liebler ( KL ) divergence ( Thollard et al , 2000). However , such term-weighting alternatives will seriously eliminate the effect of low-weighted terms , which is core of our negative feedback-based reranking scheme.
3.4 Hierarchical Distance ( HD ) Measure
The proposed method in Section 3.3 ignores two key issues . First , given a query , although search engine has thrown away most opposite intents , it is unavoidable that the pseudo-feedback still involves more than one opposite intent . However , the representation has the difficulty in highlighting all the opposite intents because the feature fusion of the representation smoothes the independent characteristics of each opposite intent . Second , given several opposite intents , they have different levels of effects on the negative score .
And the effects cannot be measured by the unilateral score.
)( rqO ?? )(_ uscoreO
Figure 2. Weighted distance calculation
To solve the issues , we propose a hierarchical distance based negative measure , abbr . HD , which measures the distances among feedbacks in a hierarchical clustering tree , and involves them into hierarchical division of relevance score.
Given two random leaves u and v in the tree , their HD score is calculated as : ),( ),( ),(_ vuW vurel vuscoreHD = (4) where ),( ?? rel indicates textual similarity , ),( ?? W indicates the weighted distance in the tree , which is calculated as : ? ? = mi i vuwvuW ),(),( (5) where m is the total number of the edges between two leaves , indicates the weight of the ith edge . In this paper , we adopt CLUTO to generate the hierarchical binary tree , and simply let each equal 1. Thus the ),( ?? iw ),( ?? iw ),( ?? W becomes to be the number of edges m , for example , the equals 5 in Figure 2. ),( kjW On the basis , given an unseen feedback u , we can acquire its modified reranking score scoreR _ ? by following steps . First , we regard each known negative feedback as an opposite intent , following the two generative rules ( mentioned in section 3.2) to generate its n-dimensional representation . Additionally we represent both the known relevant feedbacks and the unseen feedback u as n-dimensional term vectors . Second , we cluster these feedbacks to generate a hierarchical binary tree and calculate the HD score for each pair of )( rqO ?? ),( ? u , where ? denotes a leaf in the tree except u.
Thus the modified ranking score is calculated as : ? ? ? ? ?=?
Ni Nj ji vuscoreHDIvuscoreHDIscoreR ),(_),(__ (6) where iv indicates the ith known negative feedback in the leaves , N is the total number of back , is the total number of v
N v . Besides , we still adopt Boolean value to measure the textual similarity in both clustering process and ranking score calculation , thus the HD score in the formula (6) can be calculated as follows : ),( ?? rel ),( )(_ ),(_ ),( } )( ),( { ),(_ vuW uscoreO vuscoreHD vuW vVuVB vuscoreHD = = (7) 3.5 Obstinateness Factor Additionally we involve an interesting feature , i.e . the obstinate degree , into our reranking scheme . The degree is represented by the rank of negative feedbacks in the original retrieval results . That is , the more ? topping the list ? an negative feedback is , the more obstinate it is.
Therefore we propose a hypothesis that if a feedback is close to the obstinate feedback , it should be obstinate too . Thus given an unseen feedback u , its relevance to an opposite intent in
HD can be modified as : )(_)1()(_ uscoreO rnk uscoreO ?+=? ? (8) where indicates the rank of the opposite intent in original retrieval results ( Note : in HD , every known negative feedback is an opposite intent ), rnk ? is a smoothing factor . Because ascending order is used in our reranking process , by the weighting coefficient , i.e . )/1( rnk ?+ , the feedback close to the obstinate opposite intents will be further depressed . But the coefficient is not commonly used . In HD , we firstly ascertain the feedback closest to u , and if the feedback is known to be negative , set to maxv , we will use the Equation (8) to punish the pair of ( u , maxv ) alone , otherwise without any punishment.
4. EXPERIMENTAL SETTING 4.1 Data Set
We evaluate our methods with two TDT collections : TDT 2002 and TDT 2003. There are 3,085 stories in the TDT 2002 collection are manually labeled as relevant to 40 news topics , 30,736 ones irrelevant to any of the topics . And 3,083 news stories in the TDT 2003 collection are labeled as relevant to another 40 news topics , 15833 ones irrelevant to them . In our evaluation , we adopt TDT 2002 as training set , and TDT 2003 as test set . Besides , only English stories are used , both Mandarin and Arabic ones are replaced by their machine-translated versions ( i.e.
mttkn2 released by LDC).
Corpus good fair poor
TDT 2002 26 7 7
TDT 2003 22 10 8
Table 1. Number of queries referring to different types of feedbacks ( Search engine : Lucene 2.3.2) In our experiments , we realize a simple search engine based on Lucene 2.3.2 which applies document length to relevance measure on the basis of traditional literal term matching . To emulate the real retrieval process , we extract the title from the interpretation of news topic and regard it as a query , and then we run the search engine on the TDT sets and acquire the first 1000 pseudo-feedback for each query . All feedbacks will be used as the input of our reranking process , where the handcrafted relevant stories default to the clicked feedbacks . By the search engine , we mainly obtain three types of pseudo-feedback : ? good ?, ? fair ? and ? poor ?, where ? good ? denotes that more than 5 clicked ( viz . relevant ) feedbacks are in the top 10, ? fair ? denotes more than 2 but less than 5, ? poor ? denotes less than 2. Table 1 shows the number of queries referring to different types of feedbacks.
4.2 Evaluation Measure
We use three evaluation measures in experiments , P@n , NDCG@n and MAP . Thereinto , P@n denotes the precision of top n feedbacks . On the basis , NDCG takes into account the influence of position to precision . NDCG at position n is calculated as : n n i ur n Z iNDCG
Z nNDCG i ?= +?=?= 1 )( )1log ( where i is the position in the result list , Zn is a normalizing factor and chosen so that for the perfect list DCG at each position equals one , and r(ui ) equals 1 when ui is relevant feedback , else 0.
While MAP additionally takes into account recall , calculated as : ? ?= = ?= mi kj ijii jpurRmMAP 1 1 ))@()((11 (10) where m is the total number of queries , so MAP gives the average measure of precision and recall feedbacks relevant to query i , and k is the number of pseudo-feedback to the query . Here k is indicated to be 1000, thus Map can give the average measure for all positions of result list.
4.3 Systems
We conduct experiments using four main systems , in which the search engine based on Lucene 2.3.2, regarded as the basic retrieval system , provides the pseudo-feedback for the following three reranking systems.
Exp-sys : Query is expanded by the first N known relevant feedbacks and represented by an n-dimensional vector which consists of n distinct terms . The standard TFIDF-weighted cosine metric is used to measure the relevance of the unseen pseudo-feedback to query . And the rele-vance-based descending order is in use.
Wng-sys : A system realizes the work of Wang ( Wang et al , 2008), where the known relevant feedbacks are used to represent query intent , and the negative feedbacks are used to generate opposite intent . Thus , the relevance score of a feedback is calculated as I_scorewng - O_score?w ? wng , and the relevance-based descending order is used in reranking.
Our-sys : A system is approximately similar to Wng-sys except that the relevance is measured by O_scoreour - ?? I_scoreour and the pseudo-feedback is reranked in ascending order.
Additionally both Wng-sys and Our-sys have three versions . We show them in Table 2, where ? I ? corresponds to the generation rule of query intent , ? O ? to that of opposite intent , Rel . means relevance measure , u is an unseen feedback , v is a known relevant feedback , v is a known negative feedback.
5. RESULTS 5.1 Main Training Result
We evaluate the systems mainly in two circumstances : when both and N N equal 1 and when they equal 5. In the first case , we assume that retrieval capability is measured under given few known feedbacks ; in the second , we emulate the first page turning after several feedbacks have been clicked by searchers . Besides , the approximately optimal value of n for the Exp-sys , which is trained to be 50, is adopted as the global value for all other systems . The training results are shown in Figure 3, where the Exp-sys never gains much performance improvement when n is greater than 50. In fairness to effects of ? I ? and ? O ? on relevance measure , we also make n equal 50. In addition , all the discount factors ( viz .? , ? w2 and ? w3) initially equal 1, and the smoothing factor ? is trained to be 0.5.
Table 2. All versions of both Wngs and Ours Figure 3. Parameter training of Exp-sys
For each query we rerank all the pseudo-feedback , including that defined as known , so P@20 and NDCG@20 are in use to avoid overfitting ( such as P@10 and NDCG@10 given both and N N equal 5 ).
We show the main training results in Table 3, where our methods achieve much better performances than the reranking methods based on relevant feedback learning when N = N =5.
Thereinto , our basic system , i.e . Our-sys1, at least achieves approximate 5% improvement on P@20, 3% on NDCG@20 and 1% on MAP than the optimal wng-sys ( viz . wng-sys1). And obvi-?I ? n-dimensional vector for each v , Number of v in use is N ? O ? None
Wng-sys1
Rel . NvuscoreR
N i w /)), cos((_ ? I?
Number of v in use is N , all v combine into a n-dimensional bag of words bw2 ? O ? Number of v in use is N , all v combine into a n-dimensional words bag 2wb
Wng-sys2
Rel . ), cos(),cos(_ 2222 wwww bubuscoreR ??= ? ? I ? ? O ? Similar generation rules to Wng-sys2 except that query terms are removed from bag of words and 3wb 3wb Wng-sys3 Rel . ), cos(),cos(_ 3333 wwww bubuscoreR ??= ? ? I ? )( rqI ++ in section 3.3 ? O ? )( rqO ?? in section 3.2 Our-sys1
Rel . scoreIscoreOscoreR ___ ??= ? ? I ? ? O?
The same generation rules to Our-sys1
Our-sys2
Rel.
HD algorithm : ? ? ? ? ?=?
Ni Nj ji vuscoreHDIvuscoreHDIscoreR ),(_),(__ ? I ? ? O?
The same generation rules to Our-sys1
Our-sys3
Rel.
HD algorithm + obstinateness factor : )(_)1()(_ uscoreO rnk uscoreO ?+=? ? contributed by the HD measure which even increases the P@20 of Our-sys1 by 8.5%, NDCG@20 by 13% and MAP by 9%. But it is slightly disappointing that the obstinateness factor only has little effectiveness on performance improvement , although Our-sys3 nearly wins the best retrieval results . This may stem from ? soft ? punishment on obstinateness , that is , for an unseen feedback , only the obstinate companion closest to the feedback is punished in relevance measure.
Table 3. Main training results
It is undeniable that all the reranking systems work worse than the basic search engine when the known feedbacks are rare , such as = N N =1.
This motivates an additional test on the higher values of both and N N ( = N N =9), as shown in Table 4. Thus it can be found that most of the reranking systems achieve much better performance than the basic search engine . An important reason for this is that more key terms can be involved into representations of both query intent and its opposite intent . So it seems that more manual intervention is always reliable.
However in practice , seldom searchers are willing to use an unresponsive search engine that can only offer relatively satisfactory feedbacks after lots of clickthrough and page turning . And in fact at least two pages ( if one page includes 10 pseudo-feedback ) need to be turned in the training corpus when both and N N equal 9. So we just regard the improvements benefiting from high clickthrough rate as an ideal status , and still adopt the practical numerical value of and
N
N , i.e . = N N =5, to run following test.
5.2 Constraint from Query
A surprising result is that Exp-sys always achieves the worst MAP value , even worse than the basic search engine even if high value of N is in use , such as the performance when N equal 9 in Table 4. It seems to be difficult to question the reasonability of the system because it always selects the most key terms to represent query intent by query expansion . But an obvious difference between Exp-sys and other reranking systems could explain the result . That is the query terms consistently involved in query representation by Exp-sys.
Table 4. Effects of and N N on reranking performance ( when = N N =9, n = n =50) In fact , Wng-sys1 never overly favor the query terms because they are not always the main body of an independent feedback , and our systems even remove the query terms from the opposite intent directly . Conversely Exp-sys continuously enhances the weights of query terms which result in overfitting and bias . The visible evidence for this is shown in Figure 4, where Exp-sys achieves better Precision and NDCG than the basic search engine at the top of result list but worse at the subsequent parts . The results illustrate that too much emphasis placed on query terms in query expansion is only of benefit to elevating the originally high-ranked relevant feedback but powerless to pull the straggler out of the bottom of result list.

Figure 4. MAP comparison ( basic vs Exp ) 5.3 Positive Discount Loss Obviously Wang ( Wang et al , 2008) has noticed the negative effects of query terms on reranking.
Therefore his work ( reproduced by Wng-sys1, 2, 3 in this paper ) avoids arbitrarily enhancing the terms in query representation , even removes them as Wng-sys3. This indeed contributes to the - Our-sys1 Our-sys2 Exp-sys Wng-sys1 Basic P@20 0.6603 0.8141 0.63125 0.7051 0.6588 NDCG@20 0.7614 0.8587 0.8080 0.7797 0.6944
MAP 0.6583 0.7928 0.5955 0.7010 0.6440 systems N = N P@20 NDCG@20 MAP Factor
Basic - 0.6588 0.6944 0.6440 -1 0.4388 0.4887 0.3683 - Exp-sys 5 0.5613 0.6365 0.5259 -1 0.5653 0.6184 0.5253 - Wng-sys1 5 0.6564 0.7361 0.6506 -1 0.5436 0.6473 0.4970 2w ? =1Wng-sys2 5 0.5910 0.7214 0.5642 2w ? =1 1 0.5436 0.6162 0.4970 3w ? =1Wng-sys3 5 0.5910 0.6720 0.5642 3w ? =1 1 0.5628 0.6358 0.4812 ? =1 Our-sys1 5 0.7031 0.7640 0.6603 ? =1 1 0.6474 0.6761 0.5967 ? =1 Our-sys2 5 0.7885 0.8381 0.7499 ? =1 1 0.6026 0.6749 0.5272 ? =0.5Our-sys3 5 0.7897 0.8388 0.7464 ? =0.5 the better performances of Wng-sys1, 2, 3 shown in Table 3, although Wng-sys3 has no further improvement than Wng-sys2 because of the sparsity of query terms . On the basis , the work regards the terms in negative feedbacks as noises and reduces their effects on relevance measure as much as possible . This should be a reasonable scheme , but interestingly it does not work well in our experiments . For example , although Wng-sys2 and Wng-sys3 eliminate the relevance score calculated by using the terms in negative feedbacks , they perform worse than Wng-sys1 which never make any discount.
systems ?? =0.5 ?? =1 ?? =2
Our-sys1 0.4751 0.6603 0.6901
Wng-sys2 0.6030 0.5642 0.4739
Wng-sys3 0.6084 0.5642 0.4739
Table 5. Effects on MAP Additionally when we increase the discount factor 2w ? and 3w ? , as shown in Table 5, the performances ( MAP ) of Wng-sys2 and Wng-sys3 further decrease . This illustrates that the high-weighted terms of high-ranked negative feedbacks are actually not noises . Otherwise why do the feedbacks have high textual similarity to query and even to their neighbor relevant feedbacks ? Thus it actually hurts real relevance to discount the effect of the terms.
Conversely Our-sys1 can achieve further improvement when the discount factor ? increases , as shown in Table 5. It is because the discount contributes to highlighting minor terms of negative feedbacks , and these terms always have little overlap with the kernel of relevant feedbacks . Additionally the minor terms are used to generate the main body of opposite intent in our systems , thus the discount can effectively separate opposite intent from positive query representation . Thereby we can use relatively pure representation of opposite intent to detect and repel subsequent negative feedbacks.
5.4 Availability of Minor Terms
Intuitively we can involve more terms into query representation to alleviate the positive discount loss . But it does not work in practice . For example , Wng-sys2 shown in Figure 5 has no obvious improvement no matter how many terms are included in query representation . Conversely Our-sys1 can achieve much more improvement when it involves more terms into the opposite intent . For example , when the number of terms increases to 150, Our-sys1 has approximately 5% better MAP than Wng-sys2, shown in Figure 5.

Figure 5. Effects on MAP in modifying the dimensionality n ( when N = N =5, ? =1) This result illustrates that minor terms are available for repelling negative feedbacks , but too weak to recall relevant feedbacks . In fact , the minor terms are just the low-weighted terms in text . Current text representation techniques often ignore them because of their marginality . However minor terms can reflect fine distinctions among feedbacks , even if they have the same topic . And the distinctions are of great importance when we determine why searchers say ? Yes ? to some feedbacks but ? No ? to others.
Table 6. Main test results 5.5 Test Result We run all systems on test corpus , i.e . TDT2003, but only report four main systems : Wng-sys1, Our-sys1, Our-sys2 and Our-sys3. Other systems are omitted because of their poor performances.
The test results are shown in Table 6 which includes not only global performances for all test queries but also local ones on three distinct types of queries , i.e . ? good ?, ? fair ? and ? poor ?. Thereinto , Our-sys2 achieves the best performance around all types of queries . So it is believable systems metric good fair poor global Factor
P@20 0.7682 0.5450 0.2643 0.6205
NDCG@20 0.8260 0.6437 0.4073 0.7041Wng-sys1
MAP 0.6634 0.4541 0.9549 0.6620 -
P@20 0.8273 0.5700 0.2643 0.6603
NDCG@20 0.8679 0.6620 0.4017 0.7314Our-sys1
MAP 0.6740 0.4573 0.9184 0.6623 ? =2, ? =0.5
P@20 0.8523 0.7600 0.2714 0.7244
NDCG@20 0.8937 0.8199 0.4180 0.7894Our-sys2
MAP 0.7148 0.6313 0.9897 0.7427 ? =2, ? =0.5
P@20 0.8523 0.7600 0.2714 0.7244
NDCG@20 0.8937 0.8200 0.4180 0.7894Our-sys3
MAP 0.7145 0.6292 0.9897 0.7420 ? =2, ? =0.5 ways plays an active role in distinguishing negative feedbacks from relevant ones . But it is surprising that Our-sys3 achieves little worse performance than Our-sys2. This illustrates poor robustness of obstinateness factor.
Interestingly , the four systems all achieve very high MAP scores but low P@20 and NDCG@20 for ? poor ? queries . This is because the queries have inherently sparse relevant feedbacks : less than 6? averagely . Thus the highest p@20 is only approximate 0.3, i.e . 6/20. And the low NDCG@20 is in the same way . Besides , all MAP scores for ? fair ? queries are the worst . We find that this type of query involves more macroscopic features which results in more kernels of negative feedbacks . Although we can solve the issue by increasing the dimensionality of opposite intent , it undoubtedly impairs the efficiency of reranking.
6. CONCLUSION
This paper proposes a new reranking scheme to well explore the opposite intent . In particular , a hierarchical distance-based ( HD ) measure is proposed to differentiate the opposite intent from the true query intent so as to repel negative feedbacks . Experiments show substantial outperformance of our methods.
Although our scheme has been proven effective in most cases , it fails on macroscopic queries.
In fact , the key difficulty of this issue lies in how to ascertain the focal query intent given various kernels in pseudo-feedback . Fortunately , clickthrough data provide some useful information for learning real query intent . Although it seems feasible to generate focal intent representation by using overlapping terms in clicked feedbacks , such representation is just a reproduction of macroscopic query since the overlapping terms can only reflect common topic instead of focal intent . Therefore , it is important to segment clicked feedbacks into different blocks , and ascertain the block of greatest interest to searchers.
References
Allan , J ., Lavrenko , V ., and Nallapati , R . 2002.
UMass at TDT 2002, Topic Detection and
Tracking : Workshop.
Craswell , N ., and Szummer , M . Random walks on the click graph . 2007. In Proceedings of the Conference on Research and Development in Information Retrieval . SIGIR '30. ACM Press,
New York , NY , 239-246.
Cao , G . H ., Nie , J . Y ., and Gao , J . F . 2008. Stephen Robertson . Selecting Good Expansion Terms for Pseudo-Relevance Feedback . In Proceedings of the Conference on Research and Development in Information Retrieval . SIGIR '31. ACM Press,
New York , NY , 243-250.
Chum , O ., Philbin , J ., Sivic , J ., and Zisserman , A.
2007. Automatic query expansion with a generative feature model for object retrieval . In Proceedings of the 11th International Conference on Computer Vision , Rio de Janeiro , Brazil , 1?8.
Joachims , T ., Granka , L ., and Pan , B . 2003. Accurately Interpreting Clickthrough Data as Implicit Feedback . In Proceedings of the Conference on Research and Development in Information Retrieval . SIGIR '28. New York , NY , 154-161.
Lee , K . S ., Croft , W . B ., and Allan , J . 2008 A Clus-ter-Based Resampling Method for Pseudo-Relevance Feedback . In Proceedings of the Conference on Research and Development in Information Retrieval . SIGIR '31. ACM Press,
New York , NY , 235-242.
Thollard , F ., Dupont , P ., and Higuera , L.2000.
Probabilistic DFA Inference Using Kull-back-Leibler Divergence and Minimality . In Proceedings of the 17th Int'l Conf on Machine Learning . San Francisco : Morgan Kaufmann , 975-982.
Teevan , J . T ., Dumais , S . T ., and Liebling , D . J.
2008. To Personalize or Not to Personalize : Modeling Queries with Variation in User Intent.
In Proceedings of the Conference on Research and Development in Information Retrieval.
SIGIR '31. New York , NY , 163-170.
Wang , X . H ., Fang , H ., and Zhai , C . X . 2008. A Study of Methods for Negative Relevance Feedback . In Proceedings of the Conference on Research and Development in Information Retrieval . SIGIR '31. ACM Press , New York , NY , 219-226.
Wang , X . H ., Fang , H ., and Zhai , C . X . 2007. Improve retrieval accuracy for difficult queries using negative feedback . In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management . ACM press , New York , NY , USA , 991-994.
Zhang , P ., Hou , Y . X ., and Song , D . 2009. Approximating True Relevance Distribution from a Mixture Model based on Irrelevance Data . In Proceedings of the Conference on Research and Development in Information Retrieval . SIGIR '31. ACM Press , New York , NY , 107-114.
444
