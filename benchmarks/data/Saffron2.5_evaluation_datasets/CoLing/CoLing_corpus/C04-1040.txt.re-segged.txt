A Deterministic Word Dependency Analyzer
Enhanced With Preference Learning
Hideki Isozaki and Hideto Kazawa and Tsutomu Hirao
NTT Communication Science Laboratories
NTT Corporation
24 Hikaridai , Seikacho , Sourakugun , Kyoto , 619-0237 Japan


Word dependency is important in parsing technology  . Some applications such as Information Extraction from biological documents benefit from word dependency analysis even without phrase labels  . Therefore , we expect an accurate dependency analyzer trainable without using phrase labels is useful  . Although such an English word dependency analyzer was proposed by Yamada and Matsumoto  , its accuracy is lower than state-of-the-art phrase structure parsers because of the lack of topdown information given by phrase labels  . This paper shows that the dependency analyzer can be improved by introducing a Root-Node Finder and a Prepositional-Phrase Attachment Resolver  . 
Experimental results show that these modules based on Preference Learning give better scores than Collins ? Model  3 parser for these subproblems . We expect this method is also applicable to phrase structure parsers  . 
1 Introduction 1.1 Dependency Analysis
Word dependency is important in parsing technology  . Figure 1 shows a word dependency tree . Eisner ( 1996 ) proposed probabilistic models of dependency parsing  . Collins ( 1999 ) used dependency analysis for phrase structure parsing  . It is also studied by other researchers ( Sleator and Temperley , 1991; Hockenmaier and Steedman ,  2002) . However , statistical dependency analysis of English sentences without phrase labels is not studied very much while phrase structure parsing is intensively studied  . Recent studies show that Information Extraction ( IE ) and Question Answering ( QA ) benefit from word dependency analysis without phrase labels  . ( Suzuki et al , 2003; Sudo et al , 2003) Recently , Yamada and Matsumoto ( 2003 ) proposed a trainable English word dependency analyzer based on Support Vector Machines  ( SVM )  . 
They did not use phrase labels by considering annotation of documents in expert domains  . SVM ( Vapnik , 1995) has shown good performance in dif-
He agir latele scope with saw
He saw a girl with a telescope.
Figure 1: A word dependency treeferent tasks of Natural Language Processing  ( Kudo and Matsumoto , 2001; Isozaki and Kazawa ,  2002) . 
Most machine learning methods do not work well when the number of given features  ( dimensionality ) is large , but SVM is relatively robust . In Natural Language Processing , we use tens of thousands of words as features . Therefore , SVM often gives good performance . 
However , the accuracy of Yamada?s analyzer is lower than state-of-the-art phrase structure parsers such as Charniak?s Maximum-Entropy-Inspired Parser  ( MEIP )   ( Charniak , 2000) and Collins ? Model 3 parser . One reason is the lack of topdown information that is available in phrase structure parsers  . 
In this paper , we show that the accuracy of the word dependency parser can be improved by adding a base NP chunker  , a Root-Node Finder , and a Prepositional Phrase ( PP ) Attachment Resolver . We introduce the base NP chunker because base NPs are important components of a sentence and can be easily annotated  . Since most words are contained in a base NP or are adjacent to a base NP  , we expect that the introduction of base NPs will improve accuracy  . 
We introduce the Root-Node Finder because Ya-mada ?s root accuracy is not very good  . Each sentence has a root node ( word ) that does not modify any other words and is modified by all other words directly or indirectly  . Here , the root accuracy is defined as follows . 
Root Accuracy ( RA ) =  #correct root nodes/#sentences ( = 2 , 416 ) We think that the root node is also useful for dependency analysis because it gives global information to each word in the sentence  . 
Root node finding can be solved by various machine learning methods  . If we use classifiers , however , two or more words in a sentence can be classified as root nodes  , and sometimes none of the words in a sentence is classified as a root node  . Practically , this problem is solved by getting a kind of confidence measure from the classifier  . As for SVM , f ( x ) defined below is used as a confidence measure . 
However , f ( x ) is not necessarily a good confidence measure . 
Therefore , we use Preference Learning proposed by Herbrich et al  ( 1998 ) and extended by Joachims ( 2002 )  . In this framework , a learning system is trained with samples such as ? A is preferable to B ? and ? C is preferable to D  . ? Then , the system generalizes the preference relation , and determines whether ? X is preferable to Y ? for unseen X and Y  . This framework seems better than SVM to select best things  . 
On the other hand , it is wellknown that attachment ambiguity of PP is a major problem in parsing  . 
Therefore , we introduce a PP-Attachment Resolver.
The next sentence has two interpretations.
He saw a girl with a telescope.
1) The preposition ? with ? modifies ? saw . ? That is , he has the telescope . 2) ? With ? modifies ? girl . ? That is , she has the telescope . 
Suppose 1) is the correct interpretation . Then , ? with modifies saw ? is preferred to ? with modifies girl  . ? Therefore , we can use Preference
Learning again.
Theoretically , it is possible to build a new Dependency Analyzer by fully exploiting Preference Learning  , but we do not because its training takes too long . 
1.2 SVM and Preference Learning
Preference Learning is a simple modification of SVM  . Each training example for SVM is a pair ( yi , xi ) , where x i is a vector , yi=+1 means that xi is a positive example , and yi = ?1 means that x i is a negative example . SVM classifies a given test vector x by using a decision function f  ( x ) = wf ?? ( x ) + b = ?` i y i ? iK ( x , xi ) + b , where ? i and b are constants and ` is the number of training examples  . K(xi , xj )  = ? ( xi )  ? ? ( xj ) is a predefined kernel function .  ? ( x ) is a function that maps a vector x into a higher dimensional space  . 
Training of SVM corresponds to the following quadratic maximization  ( Cristianini and Shawe-
Taylor , 2000)
W (?) = ?` i=1 ? i ? ?` i , j = 1 ? i ? j y i y j K(x i , xj ) , where 0 ? ? i ? C and ? ` i=1 ? i y i = 0 . C is a soft margin parameter that penalizes misclassification  . 
On the other hand , each training example for Preference Learning is given by a triplet  ( yi , xi . 1, xi . 2), where xi . 1 and xi . 2 are vectors . We use xi . ? to represent the pair ( xi . 1, xi . 2) . yi=+1 means that xi . 1 is preferable to xi . 2 . We can regard their difference ?( xi . 1)??(xi . 2) as a positive example and ?( xi . 2) ??( xi . 1) as a negative example . 
Symmetrically , yi = ?1 means that xi . 2 is preferable to xi . 1 . 
Preference of a vector x is given by g ( x ) = wg ?? ( x ) = ?` i y i ? i ( K ( xi . 1, x ) ? K(xi . 2, x )) . 
If g(x ) > g(x ?) holds , x is preferable to x ? . Since Preference Learning uses the difference ? ( xi . 1)??(xi . 2) instead of SVM?s?(xi ) , it corresponds to the following maximization . 
W (?) = ?` i=1 ? i ? ?` i,j=1 ? i?j yiyjK(xi . ?, xj . ?) where 0 ? ? i ? C and K(xi . ?, xj . ?) = K(xi . 1, xj . 1) ? K(xi . 1, xj . 2) ? K(xi . 2, x j . 1) + K(xi . 2, x j . 2) . The above linear constraint ?` i=1 ? i yi = 0 for SVM is not applied to Preference Learning because SVM requires this constraint for the optimal b  , but there is no bing(x ) . 
Although SVMlight ( Joachims ,  1999 ) provides an implementation of Preference Learning  , we use our own implementation because the current SVMlight implementation does not support nonlinear kernels and our implementation is more efficient  . 
Herbrich?s Support Vector Ordinal Regression ( Herbrich et al , 2000) is based on Preference Learning , but it solves an ordered multiclass problem . 
Preference Learning does not assume any classes.
2 Methodology
Instead of building a word dependency corpus from scratch  , we use the standard dataset for comparison . 
Dependency Analyzer ? PP-Attachment Resolver ?
Root-Node Finder ?
Base NP Chunker ? ( POS Tagger ) ? = SVM , ? = Preference Learning
Figure 2: Module layers in the system
That is , we use Penn Treebank?s Wall Street Journal data ( Marcus et al ,  1993) . Sections 02 through 21 are used as training data ( about 40 , 000 sentences ) and section 23 is used as test data ( 2 , 416 sentences ) . 
We converted them to word dependency data by using Collins ? head rules  ( Collins ,  1999) . 
The proposed method uses the following procedures.
? A base NP chunker : We implemented an
SVM-based base NP chunker , which is a simplified version of Kudo?s method ( Kudo and Matsumoto ,  2001) . We use the ? one vs . all others ? backward parsing method based on an ?   IOB2? chunking scheme . By the chunking , each word is tagged as ? B : Beginning of a base NP  , ? I : Other elements of a base NP . 
? O : Otherwise.
Please see Kudo?s paper for more details.
? A Root-Node Finder ( RNF ) : We will describe this later . 
? A Dependency Analyzer : It works just like Ya -mada?s Dependency Analyzer  . 
? APP-Attatchment Resolver ( PPAR ) : This resolver improves the dependency accuracy of prepositions whose part-of-speech tags are IN or TO  . 
The above procedures require a part-of-speech tagger  . Here , we extract part-of-speech tags from the Collins parser?s output  ( Collins ,  1997 ) for section 23 instead of reinventing a tagger . According to the document , it is the output of Ratnaparkhi?s tagger ( Ratnaparkhi ,  1996) . Figure 2 shows the architecture of the system . PPAR?s output is used to rewrite the output of the Dependency Analyzer  . 
2.1 Finding root nodes
When we use SVM , we regard root node finding as a classification task : Root nodes are positive examples and other words are negative examples  . 
For this classification , each word wi in a tagged sentence T = ( w1/p1 ,   .   .   . , wi/pi , .   .   . , wN/pN ) is characterized by a set of features . Since the given POS tags are sometimes too specific  , we introduce a rough part-of-speech qidefined as follows  . 
? q=N if p = NN , NNP , NNS,
NNPS , PRP , PRP $, POS.
? q=V if p = VBD , VB , VBZ , VBP,

? q = J if p = JJ , JJR , JJS.
Then , each word is characterized by the following features  , and is encoded by a set of boolean variables . 
? The word itself wi , its POS tags pi and qi , and its base NP tag bi = B , I , O . 
We introduce boolean variables such as current word is John and current rough POS is J for each of these features  . 
? Previous word wi?1 and its tags , pi?1 , qi?1 , and bi?1 . 
? Next word wi+1 and its tags , pi+1 , qi+1 , and bi+1 . 
? The set of left words w0, .   .   . , wi?1, and their tags , p0, .   .   . , pi?1, q0, .   .   . , qi?1, and b0, .   .   . , bi?1 . We use boolean variables such as one of the left words is Mary  . 
? The set of right words wi+1, .   .   . , wN , and their POS tags , pi+1, .   .   . , pN  and  qi+1, .   .   . , qN . 
? Whether the word is the first word or not.
We also add the following boolean features to get more contextual information  . 
? Existence of verbs or auxiliary verbs ( MD ) in the sentence . 
? The number of words between wi and the nearest left comma  . We use boolean variables such as nearest left comma is two words away  . 
? The number of words between wi and the nearest right comma  . 
Now , we can encode training data by using these boolean features  . Each sentence is converted to the set of pairs ( yi , xi ) where yi is +1 when xi corresponds to the root node and yi is ?1 otherwise . 
For Preference Learning , we make the set of triplets ( yi , xi . 1, xi . 2), where yi is always+1, xi . 1 corresponds to the root node , and xi . 2 corresponds to a nonroot word in the same sentence  . Such a triplet means that x i . 1 is preferable to xi . 2 as a root node . 
2.2 Dependency analysis
Our Dependency Analyzer is similar to Ya-mada?s analyzer  ( Yamada and Matsumoto ,  2003) . While scanning a tagged sentence T = ( w1/p1, .   .   .   , wn/pn ) backward from the end of the sentence , each word w i is classified into three categories : Left  , Right , and Shift . 1 ? Right : Right means that wi directly modifies the right word  wi+1 and that no word in T modifies wi . If w i is classified as Right , the analyzer removes w i from T and w i is registered as a left child of  wi+1  . 
? Left : Left means that wi directly modifies the left word  wi?1 and that no word in T modifies wi . If w i is classified as Left , the analyzer removes w i from T and w i is registered as a right child of  wi?1  . 
? Shift : Shift means that wi is not next to its modific and or is modified by another word in T  . If w i is classified as Shift , the analyzer does nothing for wi and moves to the left word  wi?1  . 
This process is repeated until T is reduced to a single word  ( = root node )  . Since this is a three-class problem , we use ? one vs . rest ? method . First , we train an SVM classifier for each class . Then , for each word in T , we compare their values : fLeft(x ) , fRight(x ) , and fShift(x ) . IffLeft(x ) is the largest , the word is classified as Left . 
However , Yamada?s algorithm stops when all words in T are classified as Shift  , even when T has two or more words . In such cases , the analyzer cannot generate complete dependency trees  . 
Here , we resolve this problem by reclassifying a word in T as Left or Right  . This word is selected in terms of the differences between SVM outputs : ? ? Left  ( x ) = fShift ( x ) ? fLeft ( x )  , ? ? Right(x ) = fShift(x ) ? fRight(x ) . 
These values are nonnegative because fShift ( x ) was selected . For instance , ? Left ( x ) '0 means that fLeft ( x ) is almost equal t of Shift ( x )  . If ? Left ( xk ) gives the smallest value of these differences , the word corresponding to xk is reclassified as Left  . If 1Yamada used a two-word window , but we use a one word window for simplicity . 
? Right(xk ) gives the smallest value , the word corresponding to xk is reclassified as Right  . Then , we can resume the analysis . 
We use the following basic features for each word in a sentence  . 
? The word itself wi and its tags pi , qi , and bi , ? Whether wi is on the left of the root node or on the right  ( or at the root node )  . The root node is determined by the Root-Node Finder  . 
? Whether wi is inside a quotation.
? Whether wi is inside a pair of parentheses.
? wi?s left children wi1, .   .   .   , wik , which were removed by the Dependency Analyzer beforehand because they were classified as ? Right  . ? We use boolean variables such as one of the left child is Mary  . 
Symmetrically , wi?s right children wi1, .   .   . , wik are also used . 
However , the above features cover only near-sighted information  . If w i is next to a very long base NP or a sequence of base NPs  , wi cannot get information beyond the NPs . Therefore , we add the following features . 
? Li , Ri : Li is available when wi immediately follows a base NP sequence  . Li is the word before the sequence . That is , the sentence looks like: .   .   . Li ? abase NP ? wi .   .   . 
R i is defined symmetrically.
The following features of neigbors are also used as wi?s features  . 
? Left words wi?3, .   .   . , wi?1 and their basic features . 
? Right words wi+1, .   .   . , wi+3 and their basic features . 
? The analyzer?s outputs ( Left/Right/Shift ) for wi+1 ,   .   .   . , wi+3 . ( This analyzer runs backward from the end of T . ) If we train SVM by using the whole data at once , training will take too long . Therefore , we split the data into six groups : nouns , verbs , adjectives , prepositions , punctuations , and others . 
2.3 PP attachment
Since we do not have phrase labels , we use all prepositions ( except root nodes ) as training data . 
We use the following features for resolving PP attachment  . 
? The preposition itself : wi.
? Candidate modific and wj and its POS tag.
? Left words ( wi?2, wi?1) and their POS tags.
? Right words ( wi+1, wi+2) and their POS tags.
? Previous preposition.
? Ending word of the following base NP and its
POS tag ( if any).
? i ? j , i . e . , Number of the words between wi and wj . 
? Number of commas between wi and wj.
? Number of verbs between wi and wj.
? Number of prepositions between wi and wj.
? Number of base NPs between wi and wj.
? Number of conjunctions ( CCs ) between wi and wj . 
? Difference of quotation depths between wi and wj  . If w i is not inside of a quotation , its quotation dep this zero . If wj is in a quotation , its quotation depth is one . Hence , their difference is one . 
? Difference of parenthesis depths between wi and wj  . 
For each preposition , we make the set of triplets ( yi , xi , 1 , xi , 2) , where yi is always +1 , xi , 1 corresponds to the correct word that is modified by the preposition  , and xi , 2 corresponds to other words in the sentence . 
3 Results 3.1 Root-Node Finder
For the Root-Node Finder , we used a quadratic kernel K(xi , xj )  =  ( xi ? xj+1 ) 2 because it was better than the linear kernel in preliminary experiments  . 
When we used the ? correct ? POS tags given in the Penn Treebank  , and the ? correct ? base NP tags given by a tool provided by CoNLL  2000 shared task2  , RNF?s accuracy was 96 . 5% for section 23 . When we used Collins ? POS tags and base NP tags based on the POS tags  , the accuracy slightly degraded to 95 . 7% . According to Yamada?s paper ( Yamada and 2http://cnts . uia . ac . be/conll200/ chunking/Matsumoto ,  2003) , this root accuracy is better than Charniak?s MEIP and Collins ? Model  3 parser . 
We also conducted an experiment to judge the effectiveness of the base NP chunker  . Here , we used only the first 10 , 000 sentences ( about 1/4) of the training data . When we used all features described above and the POS tags given in Penn Treebank  , the root accuracy was 95 . 4% . When we removed the base NP information ( bi , Li , Ri ) , it dropped to 94 . 9% . Therefore , the base NP information improves RNF?s performance  . 
Figure 3 compares SVM and Preference Learning in terms of the root accuracy  . We used the first 10 , 000 sentences for training again . According to this graph , Preference Learning is better than SVM , but the difference is small .   ( They are better than Maximum Entropy Modeling3 that yielded RA=91  . 5% for the same data . ) C does not affect the scores very much unless C is too small  . In this experiment , we used Penn?s ? correct ? POS tags . When we used Collins ? POS tags , the scores dropped by about one point . 
3.2 Dependency Analyzer and PPAR
As for the dependency learning , we used the same quadratic kernel again because the quadratic kernel gives the best results according to Yamada?s experiments  . The soft margin parameter C is 1 following Yamada?s experiment . We conducted an experiment to judge the effectiveness of the Root-Node Finder  . 
We follow Yamada?s definition of accuracy that excludes punctuation marks  . 
Dependency Accuracy ( DA ) =  #correct parents /  #words ( = 49 , 892)
Complete Rate ( CR ) =  #completely parsed sentences/#sentences According to Table  1  , DA is only slightly improved , but CR is more improved . 
3http://www2 . crl . go . jp/jt/a132/members/mutiyama/software . html

Preference Learning
Ac cu ra cy (% )
C0 . 10 . 030 . 010 . 0030 . 0010 . 00030 . 0 001   96 ? ? ? ? ? ? ? ? ? ? ? ? ? ? Figure 3: Comparison of SVM and Preference Learning in terms of Root Accuracy  ( Trained with 10 , 000 sentences )
DARACR without RNF 89 . 4% 91 . 9% 34 . 7% with RNF 89 . 6% 95 . 7% 35 . 7%

Dependency Analyzer was trained with 10 , 000 sentences . RNF was trained with all of the training data . 
DA : Dependency Accuracy , RA : Root Acc ., CR:
Complete Rate
Table 1: Effectiveness of the Root-Node Finder
Ac cu ra cy (% )

Preference Learning
SVM 0 . 1 0 . 03 0 . 01 0 . 003 0 . 001 0 . 0003 0 . 0 00170 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 4: Comparison of SVM and Preference Learning in terms of Dependency Accuracy of prepositions  ( Trained with 5 , 000 sentences ) Figure 4 compares SVM and Preference Learning in terms of the Dependency Accuracy of prepositions  . SVM?s performance is unstable for this task , and Preference Learning outperforms SVM .   ( We could not get scores of Maximum Entropy Modeling because of memory shortage  . ) Table 2 shows the improvement given by PPAR . 
Since training of PPAR takes a very long time , we used only the first 35 , 000 sentences of the training data . We also calculated the Dependency Accuracy of Collins ? Model  3 parser?s output for section 23  . According to this table , PPAR is better than the
Model 3 parser.
Now , we use PPAR?s output for each preposition instead of the dependency parser?s output unless the modification makes the dependency tree into a non -tree graph  . Table 3 compares the proposed method with other methods in terms of accuracy  . This data except ? Proposed ? was cited from Yamada?s paper  . 
INTO average
Collins Model 3 84.6% 87.3% 85.1%
Dependency Analyzer 83.4% 86.1% 83.8%
PPAR 85.3% 87.7% 85.7%
PPAR was trained with 35,000 sentences . The number of IN words is 5 , 950 and that of TO is 1 , 240 . 
Table 2: PP-Attachment Resolver
DARACR with MEIP 92 . 1% 95 . 2% 45 . 2% phrase info . Collins Model 391 . 5% 95 . 2% 43 . 3% without Yamada 90 . 3% 91 . 6% 38 . 4% phrase info . Proposed 91 . 2% 95 . 7% 40 . 7%
Table 3: Comparison with related work
According to this table , the proposed method is close to the phrase structure parsers except Complete Rate  . Without PPAR , DA dropped to 90 . 9% and CR dropped to 39 . 7% . 
4 Discussion
We used Preference Learning to improve the SVM -based Dependency Analyzer for root node finding and PP-attachment resolution  . Preference Learning gave better scores than Collins ? Model  3 parser for these subproblems . Therefore , we expect that our method is also applicable to phrase structure parsers  . It seems that root node finding is relatively easy and SVM worked well  . However , PP attachment is more difficult and SVM?s behavior was unstable whereas Preference Learning was more robust  . We want to fully exploit Preference Learning for dependency analysis and parsing  , but training takes too long . ( Empirically , it takes O(`2) or more . ) Further study is needed to reduce the computational complexity  .   ( Since we used Isozaki?s methods ( Isozaki and Kazawa ,  2002) , the runtime complexity is not a problem . ) Kudo and Matsumoto ( 2002 ) proposed an SVM-based Dependency Analyzer for Japanese sentences  . Japanese word dependency is simpler because no word modifies a left word  . Collins and Duffy ( 2002 ) improved Collins ? Model 2 parser by reranking possible parse trees . Shen and Joshi ( 2003 ) also used the preference kernel K ( xi . ?, xj . ?) for reranking . They compare parse trees , but our system compares words . 
5 Conclusions
Dependency analysis is useful and annotation of word dependency seems easier than annotation of phrase labels  . However , lack of phrase labels makes dependency analysis more difficult than phrase structure parsing  . In this paper , we improved a deterministic dependency analyzer by adding a Root-Node Finder and a PP-Attachment Resolver  . Preference Learning gave better scores than Collins ? Model  3 parser for these subproblems , and the performance of the improved system is close to state-of-the-art phrase structure parsers  . It turned out that SVM was unstable for PP attachment resolution whereas Preference Learning was not  . We expect this method is also applicable to phrase structure parsers  . 

Eugene Charniak .  2000 . A maximum-entropy-inspired parser . In Proceedings of the North American Chapter of the Association for Computational Linguistics  , pages 132?139 . 
Michael Collins and Nigel Duffy .  2002 . New ranking algorithms for parsing and tagging : Kernels over discrete structures  , and the voted perceptron . In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics  ( ACL )  , pages 263?270 . 
Michael Collins .  1997 . Three generative , lexicalised models for statistical parsing . In Proceedings of the Annual Meeting of the Association for Computational Linguistics  , pages 16?23 . 
Michael Collins .  1999 . Head-Driven Statistical Models for Natural Language Parsing  . Ph . D . 
thesis , Univ . of Pennsylvania.
Nello Cristianini and John ShaweTaylor .  2000 . An Introduction to Support Vector Machines . Cambridge University Press . 
Jason M . Eisner .  1996 . Three new probabilistic models for dependency parsing : An exploration  . 
In Proceedings of the International Conference on Computational Linguistics  , pages 340?345 . 
Ralf Herbrich , Thore Graepel , Peter Bollmann-Sdorra , and Klaus Obermayer .  1998 . Learning preference relations for information retrieval  . In Proceedings of ICML-98 Workshop on Text Categorization and Machine Learning  , pages 80?84 . 
Ralf Herbrich , Thore Graepel , and Klaus Ober-mayer ,  2000 . Large Margin Rank Boundaries for Ordinal Regression  , chapter 7 , pages 115?132 . 
MIT Press.
Julia Hockenmaier and Mark Steedman . 2002.
Generative models for statistical parsing with combinatory categorial grammar  . In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics  , pages 335?342 . 
Hideki Isozaki and Hideto Kazawa .  2002 . Efficient support vector classifiers for named entity recognition  . In Proceedings of COLING 2002, pages 390?396 . 
Thorsten Joachims .  1999 . Making largescale support vector machine learning practical  . In B . Scho?lkopf , C . J . C . Burges , and A . J . Smola , editors , Advances in Kernel Methods , chapter 16 , pages 170?184 . MIT Press . 
Thorsten Joachims .  2002 . Optimizing search engines using clickthrough data  . In Proceedings of the ACM Conference on Knowledge Discovery and Data Mining  . 
Taku Kudo and Yuji Matsumoto .  2001 . Chunking with support vector machines . In Proceedings of
NAACL 2001, pages 192?199.
Taku Kudo and Yuji Matsumoto .  2002 . Japanese dependency analysis using cascaded chunking  . 
In Proceedings of CoNLL , pages 63?69.
Mitchell P . Marcus , Beatrice Santorini , and Mary A . 
Marcinkiewicz .  1993 . Building a large annotated corpus of english : the penn treebank  . Computational Linguistics , 19(2):313?330 . 
Adwait Ratnaparkhi .  1996 . A maximum entropy part-of-speech tagger . In Proceedings of the Conference on Empirical Methods in Natural Language Processing  . 
Libin Shen and Aravind K . Joshi .  2003 . An SVM based voting algorithm with application to parse reranking  . In Proceedings of the Seventh Conference on Natural Language Learning  , pages 9?16 . 
Daniel Sleator and Davy Temperley .  1991 . Parsing English with a Link grammar . Technical Report CMU-CS-91-196 , Carnegie Mellon University . 
Kiyoshi Sudoshi Sekine , and Ralph Grishman . 
2003 . An improved extraction pattern representation model for automatic IE pattern acquisition  . In Proceedings of the Annual Meeting of the Association for Cimputational Linguistics  , pages 224?231 . 
Jun Suzuki , Tsutomu Hirao , Yutaka Sasaki , and Eisaku Maeda .  2003 . Hierarchical direct acyclic graph kernel : Methods for structured natural language data  . In Proceedings of ACL 2003, pages 32?39 . 
Vladimir N . Vapnik .  1995 . The Nature of Statistical Learning Theory . Springer . 
Hiroyasu Yamada and Yuji Matsumoto .  2003 . Statistical dependency analysis . In Proceedings of the International Workshop on Parsing Technologies  , pages 195?206 . 
