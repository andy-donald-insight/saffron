A Fast Algorithm
for the Generation of Referring Expressions
Abstract
We simplify previous work in the development of
algorithms for the generation of referring expre ~ sions while at the same time taking account of psycholinguistic findings and transcript data  . The result is a straightforward algorithm that is computationally tractable  , sensitive to the preferences of human users , and reasonably domain-independent . We provide a specification of the resources a host system must provide in order to make use of the algorithm  , and describe an implementation used in the IDAS system  . 
Introduction
In previous work\[Da189 , DH91 , Rei90a , R ei90b \] we have proposed algorithms for determining the content of referring expressions  . Scrutiny of the psycholinguistics literature and transcripts of human dialogue shows that in a number of respects the behaviour of these algorithms does not correspond to what people do  . In particular , as compared to these algorithms , human speakers pay far less attention to reducing the length of a referring expression  , and far more attention to making sure they use attributes and values that human hearers can easily process  ; in the terms introduced in\[Da188 , Da189\] , hearers are more concerned with the principle of sensitivity than with the principle of efficiency  . We have designed a new referring expression generation algorithm that is based on the ~ observations  , and believe that the new algorithm is more practical for realworld natural language generation systems than the algorithms we have previously proposed  . In particular , the algorithm is : ? fast : its runtime is linear in the number of distractors  , and independent of the number of possible modifiers  ; ? sensitive to human preferences : it attempts to use easily perceivable attributes and basic-level  \[   Ros78\] attribute values ; and ? Supported by SERC grant GR/F/36750 . Email address is E . Reiter@ed . ac . uk . 
tAi so of the Centre for Cognitive Science at the University of Edinburgh  . Email addressiR . Dale Qed . ac . uk . 
Ehud Reiter * and Robert Dalef
Department of Artificial Intelligence
University of Edinburgh
Edinburgh EH 11t lN
Scotland ? domain-independent : hecore algorithm should work in any domain  , once an appropriate knowledge base and user model has been setup  . 
A version of the algorithm has been implemented within the IDAS natural language neration system \ [   RML92\]  , and it is performing satisfactorily . 
The algorithm presented in this paper only generates definite noun phrases that identify an object that is in the current focus of attention  . Algorithms and models that can be used to generate pronominal and one-anaphoric referring expressions have been presented elsewhere  ,   . g . ,\[ Sid81, GJW 83, Da 189\] . We have recently begun to look at the problem of generating referring expressions for objects that are not in the current focus of attention  ; this is discussed in the section on Future Work . 

Distinguishing Descriptions
The term ' referring expression ' has been used by different people to mean different things  . In this paper , we define a referring expression i intentional terms : a noun phrase is considered to be a referring expression if and only if its only communicative purpose is to identify an object to the hearer  , in Kronfeld's terminology\[Kro86\] , we only use the modal aspect of Donefian's distinction between attributive and referential descriptions\[  Don66\]  ; we consider a noun phrase to be referential if it is intended to identify the object it describes to the hearer  , and attributive if it is intended to communicate information about that object to the hearer  . This usage is similar to that adopted by Reiter\ [   Rei90b \] and Dale and Haddock\[ DH91\]  , but differs from the terminology used by Appelt \ [   App85\]  , who allowed ' referring expressions ' to satisfy any communicative goal that could be stated in the underlying logical framework  . 
We here follow Dale and Haddock\[ DH91\] in assuming that a referring expression satisfies the referential communicative goal if it is ad is tinguishing de-acription  , i . e . , if it is an accurate description of the entity being referred to  , but not of any other object in the current context set  . We define the context set to be the set of entities that the hearer is currently assumed to be attending to  ; this is similar ACRESDE COLING-92 , NAt CrES , 2328 AO't~q"1992232 PROC . OFCOLING-92, NANTES , AUG .  2328 ,   1992 to the notion of a discour ~ focus space \ [ GS86\]  . We also define the contrast set to be all elements of the context set excep the intended referent  . The role of tile conlpon cnts of a referring expression can then be regarded as ' ruling out ' members of the contrast set  . For example , if the speaker wished to identify a small black dog in a situation wlm retile contrast set consisted of a large white dog and a small black cat  , she might choose the adjective black in order to rule out the white dog and the heart noun dog in order to rule out the eat  ; this results in the referring expression the black dog  , which matches the intended referent but no other object in the current context  . 
The small dog would also be a succ~csful referring expre ~- ~ ionithis context  , under the distinguishing description model . 
Unnecessary Modifiers
A referring expression must communicate enough in -fornlation to be able to uniquely identify the intended referent in the current discourse context  ( i . e . , it must adhere to the principle of adequacy \[ Da188  , Da189\]) . But . this is not the only constraint a good referring expression must obey  ; it is clear that many referring expressions that meet this constraint are inappropriate because they couvey incorrect and unwanted conversation a limplieatures\[  Gri75  , Rei90a \] to a human hearer . 
One source of such false implicatures can he the pre~ence of redundant or otherwise unnecessary modifiers in a referring expression  . For example , consider two possible referring expressions that a speaker might use to requestilata hearers it by a talfle:  ( l ) a . Sit by the table . 
h . Sit by the brown wooden table.
If the context was such that only one table was visible  , and this table was brown raid made of wood , utterances ( In ) and ( lb ) would both be distinguishing descriptions that unran biguously identified the intended referent o the hearer  ; a hearer who heard either utterance would know where he was supposed to sit  . However , a hearer who heard utterance ( lb ) in such a context might make the additional infer -enee that it was important to the disc~mrse that the tM  ) le was brown and made of wood ; for , tile hearer might reason , why else would the speaker include information about the table's colour and material that was not necessary for the reference task ? This infer -enc ~ is an example of a conversational implicature caused by a violation of Grice's maxim of Quantity\[  Gri75\]  . 
Inappropriate Modifiers
Unwanted conversational implicatures can alse be caused by the use of overly specific or otherwise unexpected modifiers  . One example is ms follows : (2) ILl , ook at the doq . 
b . Look at the pilbull.
In a context where there is only one dog present , tile hearer would nommlly expect utterance ( 2a ) to be used , since dog is a basic-level class \ [ Ros78\] for most native speakers of English . Hence the use of utterance ( 2b ) might implicate to the hearer that the speaker thought it was relevant that the animal was a pit bull and not some other kind of dog\[  Cru77\]  , perhaps because the speaker wished to warn the hearer that the animal might be dangerous  ; if the speaker had no such intention , site should avoid using utterance (2b ) , despite the fact that it fulfills the referential communicative goal  . 
Previous Work
In previous work\[Dalgg , D191 , Rei90a , R ei90b \] we have noted that the presence of extra information in a referring expression can lead the hearer to make false implicaturcs  , and therefore concluded that a referring -expression generation system should taake a strong attempt to ensure that generated referring expressions do not include unnecessary information  , either as superfluous NP modifiers or as overly -specific head nouns or attribute values  . Dale\[DaI 88 , DaI89 , D H91\] ins suggeste doing this by requiring the generation system to produce mtnimal distinguishing descriptions  , i . e . , distinguishing descriptions that include as few attributes of the intended referent as possible  . Keiter\[Keig0a , Rei90b\]ha . spointed out that this task is in fact NP-Hard , and has proposed instead that referring expressions should obey three rules : No Unnecessary Components : all components of a referring expremion must be necessary to fulfill the referential goal  . For example , the small black dog is not acceptable if the black dog is a distinguishing description  , since this means small is an unnecessary component  . 
Local Brevity : it should not be po ~ ible to produce a shortere ferring expression by replacing a set of existing modifiers by a single new modifier  . 
For exanlple , the sleeping female dog should no the treed if the small dog is a distinguishing description  , since the two modifiers sleeping and female can bc replaced by the single modifier small  . 
Lexical Preference : this is an extension of the ba  . sicAcvel preference proposed by Cruse\[Cru77\] ; more details are given in \[ Reigl\] . 
A referring expression that mt . ~ tsl\[eiter's constraints cart be foumt in polynomial time if the lexical preferencer lation meets certain conditions \ [  Rei90a  \] ; such a referring expression cannot , imwever , always be found in linear time . 
Psychological and Transcript Data
Psychological Evidence
Subsequent to performing the above research , we have looked in some detail at the psychological literature on human generation of referring expressions  . This research ( e . g . , \[ FO75 , Whi76 , Son85 , A(X'ES DECOLING-92 , NAN rES , 2328 AOl'rl'1992233I'SOC . hi:COLING-92, NANTES , AUG .  2328 , 1992 Pec891;\[Lev 89 , pages 129-134\] is a useful summary of much of this work ) clearly shows that in many cases human speakem do include unnecessary modi-tiers in referring expressions  ; this presumably implies that in many cases human hearers do not make implicatures from the presence of unnecessary modifiers  . 
For example , if human subjects are shown a picture of a white bird  , a black cup , and a white cup , and are asked to identify the white bird , they frequently say the white bird , even though just the bird would have been sufficient in this ease  . 
A partial explanation for this use of redundancy may be that human speakers generate referring expressions incrementally \[  Pee89\]  . An incremental generation algorithm cannot always detect unnecessary modifiers  ; in the above example , for instance , one could imagine the algorithm choosing the adjective white to rule out the black cup  , and then the noun bird in order to rule out the white cup  , without the nerasing white because the black cup is also ruled out by bird  . 
Another explanation of redundancy might involve the speaker's desire to make it easier for the hearer to identify the object  ; the speaker might believe , for example , that it is easier for the hearer to identify a white bird than a bird  , since colour may be more immediately perceptible than shape  .  1 . 
Both of the above explanations primarily justify adjectives that have some discriminatory power even if they are redundant in this particular context  . In the above example , for instance , white possesses some discriminatory power since it rules out the black cup  , even though it does happen to be redundant in the expression the white bird  . It would be harder for either of the above factors to explain the use of a modifier with no discriminatory power  , e . g . , the use of white if all objects in the contrast set were white  . 
2 qaere is some psychological research ( e . g . , \[ FO75\] ) that suggests that human speakers do not use modifiers that have no discriminatory power  , but this research is probably not conclusive . 
Th c argument can be made that psychological realism is not the most important constraint for generation algorithms  ; the goal of such algorithm should be to produce referring expressions that human hearers will understand  , rather than referring expressions that human speakers would utter  . The fact that human speakers include redundant modifiers in referring expressions does not mean that NL generation systems are also required to include such modifiers  ; there is nothing in principle wrong with building generation systems that perform more optimizatious of their output than human speakers  . On the other hand , if such beyond-human-speaker optimizations 1Another possible explanation is that speakers may in some cases use precompiled ' reference scripts ' instead of computing a referring expression from scratch  ; such refer-enoescript specify a set of attributes that are included as a group in a referring expression  , even if some members of the group have no discriminatory power in the current context are computationally expensive and require complex algorithms  , they may not be worth performing ; they are clearly unnecessary in some sense , after all , since human speakers do not perform them . 
Transcript Analysis
In addition to the l~ychological literature review  , we have also examined a transcript of a dialogue between two humans performing an assembly task  . ~ We were particularly interested in questions of modifier choice  ; if a discriminating description can be formed by adding any one of several modifiers to a head noun  , which modifier should be used ? In particular ,  1 . Which attribute should be used ? E . g . , is it better to generate the small dog , the black dog , or the female dog , if these are discriminating descriptions but jnst the dog is not ?  2  . Is it preferable to add a modifier or to use a more specific head noun ? E  . g . , is it better to say the small dog or the chihuahua ?  3  . Should relative or absolute adjectives be used ? E  . g . , is it better to say the small dog or the one foot high dog ? In our analysis  , we observed several phenomena which we believe may generalise to other situations involving spoken  , face-to of a celanguage : 1 . Human speakers prefer to use adjectives that communicate size  , shape , or colour in referring expres?sions . In tile above examples , for instance , a human speaker would probably prefer the black dog and the small dog over the female dog  . 
2 . Human hearer sometimes have trouble determining if an object belongs to a specialized class  . In the above example , for instance , the chihuahua should only be used if the speaker is certain the hearer is capable of distinguishing chihuahuas from other types of dogs  . If there is any doubt about the heater's ability to do this  , adding an explicit modifier ( e . g . , the small dog ) is a better strategy than using a specialized head noun  . 
3 . Human speaker seem to prefer to use relative adjectives  , and human hearer seem to have less trouble understanding them  . However , human-written instructional texts sometimes use absolute adjectives instead of relative ones  ; this may be a consequence of the fact that writers cannot predict the context heir text will be read in  , and hence how readers will interpret relative adjectives  . In the above example , therefore , a speaker would be expected to use the small dog , but a writer might use the one foothigh dog . 
Z The transcript was made by Phil Agre and John Batali  , from a videot a petaken by C and y Sidser . We are very grateful to them for allowing us to use it  . 
ACTF~DECOLING-92 , NAN qT ~ , 2328 Aotrr 1992 234 PROC . OFCOLING-92, NANTES , AUG .  2328, 1992
The Algorithm
Based on the above considerations , we have created a new algorithm for generating referring expressions  . 
This algorithm is simpler and faster than the algorithms proposed in \[  Dai89  , R ei90a \] because it performs much less length-oriented optimization of its output i we now believe that the level of optimization suggested in \[  Da189  , ReigOa \] was unnecessary and psycholinguistically implausible  . The algorithm has been implemented as part of a larger natural language generation system  , and we are pleased with its performance to date . 
Assumptions about the Knowledge Base Our algorithm is intended to be reasonably domain -independent  . We . do , however , make some assumptions about the structure of the host system's underlying knowledge base  , and require that certain interface functions be provided  . 
in particular , we assume that : ? Every entity is characterised in terms of a collection of attributes and their values  . Anattrii ) ute-value pair is what is sometimes thought of as a property  ; an example is ( colour , red ) . 
Every entity has a sone of its attribute some type  . This is a special attribute that corresponds to the kinds of properties that are typically real - iT  , ed by head nouns ; an example is ( type , dog ) . 
? The knowledge base may organize some attribute values in a subsumption taxonomy  ( e . g . , as is done in KI:ONE \[ BS85\] and related KR systems )  . Such a taxonomy might record , for example , that an-imM subsumes dog , and that red subsumes car-let . For such taxonomically-organized values , the knowledge base or an associated user model should specify which level of the taxonomy is basic-level for the current user  . 
We require that the following interface functions be provided : value  ( object , attribute ) returns the value ( if any ) that an attribute has for a particular object . Value should return the most specific possible value for this attribute  , e . g . , chihuahua instead of dog , and s car let instead of red . 
taxonomy-children ( value ) returns the immediate children of a value in the taxonomy  . For example , taxonomy-children ( animal ) might be the set dog , cat , horse ,   .   .   .   . 
basle-level-value(object . attribute ) returns the basic ~ level value of an attribute of an object  . FO rexample , basic-level-value ( Garfield , type ) might be cat . 
The knowledge-representation system should in principle allow different basic-level classes to be specified for different users \[  Ros78  , Rei91\] . 
user-knows ( object , atribute-value-pair ) returns true if the user knows or can easily determine  ( e . g . , by direct visual perception ) that the attribute-valuc pair applies to the object  ; false if the user knows or can easily determine that the attribute-value pair does not apply to the object  ; and unknown otherwise . FO rexmnple , if object x had the attribute-value pair ( type , chihuahua ) , and the user was capable of distinguishing dogs from eats  , then user-knows(x , ( type , dog )) would be true , while user-knows(x , ( type , cat )) would be false . If the user was not , however , capable of distinguishing different breeds of dogs  , and had no prior knowledge of x's breed , then user-knows(x , ( type , chihuahua )) and user ~ knows(x , ( type , poodle )) would both return unknown , since the user would not know or be able to easily determine whether x was achihuahua  , poodle , or some other breed of dog . 
Finally , we a~ume that the global variable * p~eferred -attributes * lists the attributes that human speakers and hearers prefer  ( e . g . , type , size , shape , and colour in the ~ . , ~ embly task transcript mentioned above ) . These attribute should be listed in order of preference  , with the most preferable attribute flint . 
The elements of this list and their order will vary with the domain  , a ~ ld should be determined by empirical in v ~tigation  . 
Inputs to the Algorithm
In order to construct a reference to a particular emtity  , tile host system must provide : - a symbol corresponding to the intended referent  ; and ? a list of symbols correspondiug to the members of the contrast set  ( i . e . , the other entities in focus , besides the intended referent ) . 
The algorithm returus a list of attribute-value pairs that correspond to timromantic ontent of the referring expression to be realized  . This list can then be converted into an SPL\[ K&~9\] term , as is done in the II ) AS implementation ; it can also be converted into a recoverable semantic structure of the kind used in Daie's EPICO ltEsystem\[  Da188  , Dai89\] . 
The Algorithm
In general terms , the algorithm iterates through the attributes in * preferred-attributes *  . For each attribute , it checks if specifying a value for it would rule out at least one member of the contrast set that has not already be curuled out  ; if so , this attribute is added to the referring ~ t , with a value that is known to the User , rules out as many contrast set members as possible  , and , subject to these constraints , is as cl(~e as possible to the basic-level value . The process of adding attribut ~ value pairs continues mt-tila referring expression has been formed that rules out every member of the contrast set  . There is no backtracking ; once an attribute-value pair has been added to the referring expression  , it is not removed even if the addition of subsequent attribute-value pairs make it unnecessary  . A head noun ( i . e . , a value for tim type attribute ) is always included , even if it Acres DECOLING-92 , NANTES , 2328 AO~n'1992235 PROC . OVCOTING-92, NANTES , AUG .  2328 , 1992 lmake-referring-expression(r , C , P ) I
L * - -
D,-C for each member A ~ of list P do
V = flnd-best-value(A ~ , basl c-level-value(r , A ~))
If V~nilA rules-out (( A  ~ , V )) ~ nil then L~LU(AI , V )
D~D-rules-out((At,V )) end lf
If D = then if ( type , X )   ( : L for some X then return L else return LU ( type , basic-level-value(r , type )) end if end if next return failure
If ind-best-value ( A , initial-valse ) lff user-knows(r , ( A . initial-value ) ) = true then value ~-- initial-value else value ~ nil vndlfforv ~ E taxonomy-children  ( initial-value ) lfv ~ subsumes value ( r , A ) A(new-value ~ find-best-value(A , vi )) ~ nilA(value=nllYI rules-out((A , new-value )) I > I rules-out (( a , valse ) l ) then value ~ new-value end if next return value \ [   , ul ; s-out (< A , v >)\[ return x : xEDA user-knows(x , ( A , V )) = false
Figure 1: The Algorithm has no discriminatory power ( in which ease the basic level value is used )  ; other attribute values are only included if , at the time they were under consideration , they had some discriminatory power . 
More precisely , the algorithm is as shown in Figure 1 . 
Here , r is the intended referent , C is the contrast set , P is the list of preferred attributes , D is the set of distractom ( contrast set members ) that have not yet been ruled out , and L is the list of attribute-value pairs returned  , a make-referring-expression is the toplevel function  . 
This returns a list of attribute-value pairs that specify a referring expression for the intended ref-aFor simplicity of  expo6ition   , the algorithm as described here returns failure if it is not pesaible to rule out all the mern bem of the contrast set  . A more robust algorithm might attemptopur ~ m other strategies here  , e . g , generating a referring expression of the form one of the X s  , or modifying the contrast set by adding navigation if ormation  ( navigation is discussed in the section on Future

erent . Note that the attributes are tried in the order specified in the * preferred-attributes * lit  , and that a value for type is always included , even if type has no discriminatory power . 
find-best-value takes an attribute and an initial value  ; it returns a value for that attribute that is subsumed by the initial value  , accurately describes the intended referent ( i . e . , subsumes the value the intended referent possesses for the attribute  )  , rules out as many distractors as possible , and , subject to these constraints , is as close as possible in the taxonomy to the initial value  . 
rules-ou takes an attribut ~ . ~value pair and returns the elements of the set of remaining distractom that are ruled out by this attribute-value pair  . 
An Example
Assume the task is to create a referring expression for Object l in a context that also includes  Object2 and Object3:  ?  Object1:   ( type , chihuahua ) , ( size , small ) , ( calour , black ) ? Object 2: ( type , chihuahua ) , ( size , large ) , ( colour , white ) * Object 3: ( type , siamese-cat ) , ( size , small ) , ( colour , black ) In other words , r = 0bject l and (7 = Objest 2 , Object 3 . Assume that P = type , colour , size, .   .   .   . 
When make-referring-expressionicalled in this context  , it initializes L to the empty set and D to C , i . e . , to Object2, Object 3 . Find-best-value is then caUed with A = type , and initial-value set to the basic-level type of Object1  , which , let us assume , is dog . 
Assume user-knows ( Object 1 , ( type , dog )) is true , i . e . , the user knows or can easily perceive that Object l is a dog  . Find-best-value then sets value to dog , and examines the taxonomic descendants of dog to see if any of them are accurate descriptions of  Ob-ject1   ( this is the subsumption test ) and rule out more distractors than dog does . In this case , the only accurate child of dogischihuahua , but ( type , chihuahua ) does not have more discriminatory power than ( type , dog ) ( both rule out Object3) , so find-best-value returns dog as the best value for the type attribute  . Make-referring-expression the verifies that ( type . dog ) rules out at least one distraet or , and therefore adds this attribute-value pair to L  , while removing rules-out (( type , dog )) = Object3 from D . 
This means that the only remaining distraet or in D is  Object2  . Make-referring-expression ( after cheek-ing that D is not empty ) calls find-best-value again with A = colour ( the second member of P )  . Find-best-value returns Object l~s basic-level colour value  , which is black , since no more specific colour term has more discriminatory power  . Make-referring-expression then adds ( colour , black ) to L and removes rules-out (( colour , black )) = Object2 from D . D is then empty , so the generation task is completed , ACTESDECOLING-92 , NANTES , 2328 AO~r 1992 236 PROC . OFCOLING-92, NArCrEs . AUG .  2328 . 1992 and make-referring-expression returns ( type , dog ) , ( celour , black ) , i . e . , a specification for the referring expression the black day  . Note that if P had been type , size , colour, .   .   . instead of type , cnlour , size, .   .   .   , make-referring-expeession would haverc~turned ( type , dog ) , ( size , small ) instead , i . e . , thesra all do # . 
Implementation
The algorithm is currently being used within then ) AS system\[RML92\] . ll ) hS is a natural m~guage generation system that generates online documentation and help texts from a domain a ridlinguistic knowledge base  , lining user expertise models , user task models , and discourse models . 
IDAS uses a KL-ONE type knowledge repr ~ entation system  , with roles corresponding to attributes and lillem to values  . The type attribute is implicit in the position of an object in the taxonomy  , and is not explicitly represented . The value and taxonomy-children functions are defined in terms of standard knowledge base access functions  . 
A knowledg ~ base author can specify explicit basic -level attribute values in IDAS user models  , but IDAS is also capable of using heuristics to guess which value is basic-level  . The heuristics are fairly simple ( e . g . , " nse the most general value that is not in the upper-model\[  BKMW90\] and has a one word realization " )  , but they seem ( so far ) to be at least somewhat effective . A * preferred-attributes * lit has been crcated for IOAS's domain  ( complex electronic machinery ) by visual inspection of the equipment being documented  ; its first members are type , colour , and label . The user-knows function simply returns true if the attribut c~value pair is accurate and false otherwise  ; this essentially assumes that the user can visually perceive the value of any attribute in * preferred-attributes *  , which may not tie true in general . 
The referring expression generation model seems reasonably successful in IDAS  . In par Lieular , the algorithm lure proven to be useful because : 1  . It is fast . The algorithm runs in linear time in the number of distractors  , which is probably impossible for any algorithm that includes an explicit brevity requirement  ( e . g . , the algorithms of\[Da189, Rei90a\]) . Of equal importance , its runtime is independent of the number of potential attributes that could be used in the referring ex-preszion  . Thisks a consequence of the fact that the algorithm does not attempt to find the attribute with the highest discriminatory power  , but rather simply takes attributes from the * preferred-attributes * list until it has built a successful referring expression  . 
2 . It allows human preferences and capabilities to be taken into consideration  . The * preferred-attributes * list , the preference for basic-level values , and the user ~ knows function are all ways of biasing the algorithm towards generating referring expressions that use attributes and values that hu-taan hearers  , with all their perceptual limitations , lind easy to process . 
Almost all referring expressions generated by IDAS contain a head noun and zero  , one , or perhaps at most two modifiers ; longer referring expressions are rare . The most important task of the algorithm is therefore to quickly generate asy-to-understand referring expre ~ mions in such simple cases  ; optimal handling of more complex referring expression silees important  , although the algorithm should be robuate nough to generate something plausible if along referring expression is needed  . 
Future Work
Navigation
As mentioned in the introduction , the algorithm presented here assumes that the intended referent is in the context set  . An important question we need to address is what action should be taken if this is not the c ~  . se , i . e . , if the intended referent is not in the current focus of attention  . 
Unfortunately , we have very little data available ou which to bose a model of the generation of such referring expressions  . Psyclm linguistic researchers seem to have paid relatively little attention to such eases  , and the transcripts we have ( to date ) examined have contained relatively few instances where the intended referent was not already salient  . 
l to wever , we take the view that , in the general case , a referring expression contains two kinds of information : navigation and discrimin at ion  . Each de~scriptor used in a referring expression plays one of these two roles  . 
? Navigational , or attention-directing information , is intended to bring the intended referent into the hearer's focus of attention  . 
? Discrimination information is intended to distinguish the intended referent from other objects in the hearer's focus of attention  ; such information has been the subject of this paper  . 
Navigational information is not needed if the intended referent is already in the focus of attention  . 
If it is needed , it frequently ( although not always ) takes the form of loeational information . The IDAS system , for example , can generate referring expressions such as tl~e black power supply in the equipment rack  . In this case , in the equipment rack is navigation information that is intended to bring the equipment rack and its components into the hearer's focus of attention  , while black power supply is discrimination information that is intended Ix  ) distinguish the intended referent from other members of the context ~ t  ( e . g . , the white power supply that is also present in the equipment rack  )  . 
The navigation model currently implemented in If ) A Sissimplistic and not theoretically well -justified  . We hope to do further research on building a better-ju ~ stified model of navigation  . 
AcrEs DECOLING-92, NA brlns . 2328^o ~ r1992237 PRoc , OFCOLING-92 , NANTES , AtJcl .  2328, 1992
Relative Attribute Values
As mentioned previously , the transcript analysis shows that human speakers and hearers often prefer relative instead of absolute attribute values  , e . g . , small instead of one in ch . Knowledge bases sometimes explicitly encode relative attribute values  ( e . g . , ( size . small )) , but this can cause difficulties when referring expressions need to he generated in different contexts  ; a one-inch screw , for example , might be considered to be small in a context where the other screws were all two-inch screws  , but large in a context where the other screws were all half-inch screws  . 
A better solution is for the knowledge base to record absolute attribute values  , and then for the generation algorithm to automatically convert absolute values to relative values  , depending on the values that other members of the context set puss c ~ for this attribute  . Thus , the knowledge base might record that a particular screw had  ( size . one-inch ) , and the generation system would choose to call this screw small or/ary edepending on the size of the other screws in the context set  . We hope to do further research on determining how exactly this proces should work  . 
Conclusions
We have presented an algorithm for the generation of referring expressions that is substantially simpler and faster than the algorithms we have proposed in previous work\[  Da189  , Rei 90 a\] , largely because it performs much less length -oriented optimization of its output  . We have been guided in this simplification effort by psycholinguistic findings and transcript analyses  , and believe that the resulting algorithm is a more practical one for natural anguage generation systems than the ones we proposed previously  . 
Reference slapp 85\]DouglasEAppelt . Planning English Sentences . 
Cambridge Univemity Press , New York , 1985.
\[BKM WgO\]John Bateman , Robert TK asper , Johanna D Moore and Richard A Whitney . A General Organization of Knowledge for Natural Language Processing:  . The Penman Upper Model . Unpublished technical report , Information Sciences Insti-tute/University of Southern California  ,  1990 . 
\[BS85\]Ronald Brachman and James Schmolze . An overview of the KL-ONE knowledger presentation system  . Cognitive Science 9: 171-216, 1985 . 
\[Cru77\]D . Cruse . The pragmatics of lexieal specificity . 
Journal of Linguistics , 13:153-164~1977.
\[ Da188\] Robert Dale . Generating Referring Expressions in a Domain of Objects and Processes  . PhD Thesis , Centre for Cognitive Science , University of Edinburgh ,  1988 . 
IDol89\] Robert Dale . Cooking up referring expre ~ ens . 
In Proceedings of the $7th Annual Meeting of the Association for Computational Linguistics  , pages 68-75 .  1989 . 
\[DH91\] Robert Dale and Nicholas Haddock . Content determination i the generation of referring expressions  . Computational Intelligence , 7(4), 1991 . 
\[Don66\] Kelth Donnellan . Reference and definite description . Philosophical Review , 75:281-304, 1986 . 
\ [17075\] William Ford and David Olsea The elaboration of the noun phrase in children's de ~ riptton of objects  . Journal of Ezpemmentol Child Psychology ,  19:371-382 ,  1975 . 
\[GJW83\]Barbara Gresz , Aravind Jeshi , and Scott Weinstein . Providing a unified account of definite noun phrases in discourse  . In Proceedings of the ?1st Annual Meeting of the Assoeiation for Computafional 
Linguistics , pages 44-50.1983.
\[Gri75\]H . Paul Grice . Logic and conversation . In P . Cole and J . Morgun ~ editors , Syntax and Semantics : Vat3 , Speech Acts , pages 43-58 . Academic Prese , New
York , 1975.
\[GS86\]Barbara Grenz and Candaen Siduer . Attent lon ~ intention , and the structure of discourse . Compu-tatioual Linguistic , 12:175-208, 1986 . 
\[ Kns89\] Robert Kasper . A flexible interface for linking applications to Penman's sentence generator  . Proceedings of the 1989 DARPA Speech and Natural
Language Workshop ~ pages 153-158.
\[Kro86\]Amichai Kronfeid . Donnsllan's distinction and a computational model of reference  . In Proceedings of the ~ . ~th Annual Meeting of the Association for Computational Linguistics  , pages 185-191 .  1986 . 
\[Lev89\] Willera Levelt . Speaking : bq ~ om Intention to Articulation . MIT Pre~s , 1989 . 
\[ Pec89\] Thomas Pechmann . Incremental speech production and referential overspeeificatlon  . Linguistics , 27:89-110, 1989 . 
\[ Rdg0a\]E hud Reiter . The computational complexity of avoiding conversational implicatures  . In Pea ~ . 2A-ings of the ~8th Annual Meeting of the Association for Computational Linguistics  , pages 97-104 .  1990 . 
\ [ Relg0 b \] E hud Reiter . Generating Appropriate Natural Language Object Descriptions  . Phi3 thesis , Aiken Computation Lab , Harvard Univeraity ,  1990 . Also available as Aiken Computation Labtechnical report  TK-10-go  . 
\[Rei91\]E hud Reiter . A new model of lexical choice for nouns . Computational Intelligence , 7(4), 1991 . 
\[RML92\]E hodRelter , Chris Mellish , and John Levine . 
Automatic generation of on-line documentation In t be IDAS project  . In Proceedings of the Third Cor~ferenee on Applied Natural Language Pn ~  . p_ . ~sing , pages 6471 .  1992 . 
IRes 78\]E lean or Rcech . Principles of categorization . In E . Resc ~ and B . Lloydjeditors , Cognition and Categorization , pages 27-48 . Lawrence Erlbaum , Hillsdale , NJ , 1978 . 
\[Sid81\]C and a~e Sidner . Focusing in the comprehension of definite anaphora  . In M . Brady and R . Berwick , editors , Computational Models of Discourse , pages 267-330 . MIT Press , Cambridge , Mass , 1981 . 
\[Son85\]Susan Sonnenschein . The development of rder-ential communication skills : Some situations in which speakers give redundant messages  . Journal of Paycholingnistic Research ,  14:489-508 ,  1985 . 
\[ Wh176\]GraverWhite hurst . The development of commu-nicatiea : Changes with age and modeling  . Child
Development , 47:473-482, 1876.
ACIds DECOLING-92 , NANI~S , 2328 AOUT 1992 238 PROC . OFCOL1NG-92, NANI'ES , AUG .  2328, 1992
