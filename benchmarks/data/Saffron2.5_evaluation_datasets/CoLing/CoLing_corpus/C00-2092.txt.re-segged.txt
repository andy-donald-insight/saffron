Data-Oriented Translation
Arjen Poutsma
Department of Computational Linguistics
University of Amsterdam
the Netherlands
pouts ma@wins , uva . nl

In this allicle , we present a statistical approach to machine translation that is based on Data-Oriented Parsing : l  ) at a-Oriented Translation ( DOT )  . In DOT , we use linked subtreel mirs for creating a derivation of a source sentence  . Each linked subhee pair has a certain probability  , and consists of two trees : one in the source language and one in the target language  . When a derbation has been formed with these subtree pairs  , we can create a translation from this deriwition . Since there are typically many different derivations of tile same sentence in the source language  , there can be as many dilTemn translations for it . The probability of a translation can be calculated as the total probability of all tile derivations that form this translation  . We give the computational aspects for Ibis model , show tlm t we can convert each subtree imir into a productive rewrite rule  , and that tile most probable translation can be com-imted by means of Monte Carlo disambiguation  . H-nally , we discuss some pilot experiments with the
Verbmobil COl\]mS.
1 Introduction
The Data-Oriented Parsing model has been presented as a promising paradigm for natural hmguage processing  ( Scha , 1990; Bod , 1995; Bod ,  1998) . 
It has been shown that DOP has the ability to locate syntactic and semantic dependencies  , both of which are quite important for machine translation  . 
We hope that , by basing our model on DOP , we can inherit these advantages , thus obtaining a new and interesting way to perform machine translation  . 
In section 2 , we describe this novel model by identifying its parameters  , in section 3 , we describe its comlmtational spects ; in section 4 , we discuss some pilot experiments with this model  ; and finally , in section 5 , we give some issues open for filtum research . 
2 The Data-Oriented 35 ranslation Model
In this section , we will give the instantiation of a model that uses DOP for MT purposes  , which we will call Data-Oriented Translation ( DOT )  . t This model is largely based on DOPI(Bod ,  1998 , chapt . 

In DOT , we use linked subtree pairs as combi-national flagments  , a Each linked subtree pair has a certain probability  , and consists of a trec in the source language and a tree in the target language  . 
By combining these fragments to form an an analysis of the soume sentence  , we automatically generate a translation , i . e . we form a derivation of both source sentence and target sentence  . Since there am typically many different derivations which contain the same source sentence  , there can be equally many different ranslations t \ ~ rit  . Tile probability of a translation can be calculated as the total probability of all the derivations that form this translation  . 
Tile model presented here is capable of translating between two hmguages only  . This lilnitation is by no means a property of the model itself  , but is chosen for simplicity and readability reasons only  . 
The following parameters should be specified for a DOP-like approach to MT :  1  . tile representations of sentences that are as-sl imed  ,  2 . the fragments of these representations that can be used for generating new representations  ,  3 . the operator that is used to combine the flag -ments to form a translation  , and IThis is actually the second instantiation of such a framework  . The original model ( Poutsma , 1998; l ) outsnm , 2000) had a major flaw , which resulted in translations that were simply incorrect  , as pointed out by Way (1999) . 
2Links between tree nodes were introduced for TAG trees  , in ( Schieber and Schabes ,  1990) , and put to use for Machine
Translation by Abeilld et al (1990).
635, If
NP VP

Charles VNP
II likes Anne " S
NP "-. VP\Anne V-.PP
I /'> .
pla ~ tP NP
II a Charles
Figure 1: A linked tree pair ( T , ., T~).
4 . the model that is used for determining the probability of a target sentence given a source sentence  . 
In the explanation that follows , we will use a subscripts to denote an element of the source language  , and a subscript t to denote one of the target language  . 
2.1 Representations
In DOT , we basically use the same utterance-analysis as in DOPI  ( i . e . syntactically labeled phrase structure trees ) . To allow for translation capabilities in tiffs model  , we will use pairs of trees that incorporate semantic infonnation  . The amounl of semantic information eed not be very detailed  , since all we are interested in is semantic equivalence  . Two trees 7\] and T2 are said to be semantic equivalents ( denoted as TI ""7 ~ ) iff TI can be replaced with T2 without loss of meaning . 
We can now introduce the notion of links : a link symbolizes a semantic equivalence between two trees  , or part of trees . It can occur at any level in the tree structure , except for the terminal level . 3 The representation used in DOT is a 3-tuple ( T , , Tt ,  ?) , where ~ is a tree in the somce language , T t is a tree in the target language , and ? is a function that maps between semantic equivalent parts in both trees  . In the rest of this article , we will refer to this 3-tuple a stile pair ( T , , g ) . 
Because of the semantic equivalence , a linknms texist at the top level of the tree pair  ( Ts , T t ) . Figure 1 shows an example of two linked trees , the links are depicted graphically as dashed lines  . 
3 Links cannot occur at the term in a level , since we map between semantic equivalent parts on the level of syntactic categories  . 
2.2 Fragments
Likewise , we will use linked subtrees as our flag-ments . Given a pair of linked trees ( T  ~ , T t ) , a linked subtree pair of ( T  ~ , T t ) consists of two connected and linked subgraphs ( t  ~ , 6) of (77~ , 7) such that : 1 . for every pair of linked nodes in ( t . , . ,6) , it holds that : ( a ) both nodes in ( ts , lt have either zero daughter nodes , or ( b ) both nodes have all the daughter nodes of the corresponding nodes in  ( T , , Tt ) and 2 . every non-linked node in either t ~ .   ( or 6 ) has all the daughter nodes of the corresponding node in T  , ( T , ) , and 3 . both t , and ~ consist of more than one node . 
This definition has a number of consequences.
First of all , it is morn restrictive than the DOPI definition for subtrees  , thus resulting in a smaller or equal amount of subtrees per tree  . Secondly , it defines a possible pair of linked subtl'ees . Typically , there are many pairs of linked subtrees for each set of linked trees  . Thirdly , the linked tree pair itself is also a valid linked subtree pair  . Finally , according to this definition , all the linked subtree pairs are semantic equivalents  , since the semantic daughter nodes of the original tree are removed or retained simultaneously  ( clause 1 )  . The nodes for which a semantic equivalent does not exist are always retained  ( clause 2 )  . 
We can now define the bag of linked subtree pail w , which we will use as a grammar . Given a corpus of linked trees C , the bag of linked subtree pairs of C is the bag in which linked subtree pairs occur exactly as often as they can be identified in C  .   4 Figure 2 show the bag of linked subtree pairs for the linked tree pair  ( T , , Tt) . 
2.3 Composition operator
In DOT , we use the leftmost substitution operator for forming combinations of gramma rules  . 
The composition of tile linked tree pairts , 6 ) and 4The similarity between Example-based MT ( Nagao ,  1984 ) and DOT is clear : EBMT uses a database of examples to form a translation  , whereas DOT uses a bag of structured trees . 
636, q~....S
NP VP NP ~- VP
I ~ ~ ~" IL ? ~
Charles VNP Anne V\-.PP likes Annel ; I aft PNP
II ? ~ Charles/\
NP VP NP "- VP l ~, "> Q--..
Charles VNP V\PP likes plait PNP
II?t Charles ?\
S---S
NP VP NP-VP ~ ~" I"v > ~'-...\ p
VNP Anne Plikes Anne pla ) PNPI
NP VP NP ".. VP/
VNP V\PP likes " pla ) PNP
Igt
NP NP NP NP
IIII
Charles Charles Anne Anne
Figure 2: The bag of liuked subtree pairs of ( T , , 7~) ( us , u , ) , written as ( ts , t t ) o(u . , . , u , ) , is deiined if fhelabel of lheleftmost nonterlninal \] inkedfi ' on tieruo cle and the label of its linked counterpart are identical to the labels of the root nodes of  ( u . ~ . , ur ) . If this composition is defined , it yields a copy of ( t ,  . , tt ), in which a copy of u . , . has been substituted on t . , . 's leftmost nonterminal linked frontier node , and a copy of ut has been substituted on the node 's linked counterpart  . The coln position operation is illustrated in figure  3  . 
Given a bag of linked subtree pairs B , a sequence of compositions ( ts ~ , it , ) o .   .   . ot . ~N , bN ), with ( t . ~i , b ~) EB yielding a tree pair ( T , ,Tt ) without nonterminal leaves is called a derivation D of  ( 7~ . , 7~) . 
2.4 Probability calculation
To compute the probability of the target composition  , we make the same statistical assumptions as in DOPI with regard to independence and represent a -tiou of the subtrees  ( Bed ,  1998 , p .  16) . 
The probability of selecting a subtree pair ( ts ~ bl is calculated by dividing the frequency of the subtree pair in the bag by the number of snb trees that have the same root node labels in this bag  . In other words , let I(t . , , t , ) l be the number of times the subtree pair ( t ,  . ,tr  occurs in the bag of subtree pairs , and r(t ) be the root node categories of t , then the probability assigned to ( is , b ) is p((t , , , t , ))- I(l . , , t , ) lEl , , , , , , > : ~( , , , . )=,-(,, . )~ , -( , , , ): , -( , ,) I (" ,  " ,   ) 1  ( 1 ) Given the assumptions that all subtree pairs are independent  , I he probability of a derivation ( ts ~ , hi ) o .   .   . o(GN , t tN ) is equal to the product of the probabilities of the used subtree pairs  . 
P(0 . ~, , t , , ) o .   .   . o(ts , ,t , N >) = l-\[p((t , , , t , i ) ) i ( 2 ) The translation generated by a derivation is equal to the sentence yielded by the target trees of the derivation  . Typically , a translation can be generated by a large number of different deriwltions  , each of which has its own probability . Therefore , the probability of a translation ws~wt is the sum of the probabilities of its derivations : 
P ( w ~, w , ): ~_P ( D(ws,w , )(3)/\~ S~-~S
NP VP NP\VP)
VNP V\PP likes plait PNP

NP VP NP
NP ' NPI \]\] = Anne VNP'
At meAnne\]likes

VP Pplaft PNP
II a Anne
Figure 3: The composition operation
The justification of this last equation is quite trivial  . As in any statistical MT system , we wish to choose the target sentence w ~ so as to maximize P  ( wtlw , ) ( Brown et al ,  1990 , p .  79) . if we take the sum over all possible derivations that weleformed from Ws and derivewt  , we can rewrite this as equation 4 , as seen below . Since both ws and wt are contained in Dlw , , w ,  ) , we can remove them both and arrive at equation 5 , which -- as we maximize over wt--is equivalent to equation  3 above . 
maxP ( wtlWs ) =
Wt = maxE wrDws , wt ) 11 qaxEwtD(ws.Wt)
P ( w , , D < w , , w , ) lw . d(4)
P ( D(w , , w , )) (5) 3 Computational Aspects
When translating using the DOT model , we can distinguish between three computational stages : I  . parsing : the formation of a derivation forest ,  2 . translation : the transfer of the derivation forest from the source language to the target language  ,  3 . disambiguation : the selection of the most probable translation from the derivation forest  . 
3.1 Parsing
In DOT , every subtlee pair ( t ~ , tt ) can be seen as a productive rewrite rule : ( root ( t~ )  , root(tt )) ( frontier(ts ) , frontier(tt )) , where all linkage in the frontier nodes is retained  . The linked nonterminals in the yield constitute the symbol pairs to which new roles  ( subtlee pairs ) are applied . For instance , the rightmost subtree pair in tigure 3 can be rewritten as \\[ ( S , S ) --+ (( Anne , likes , NP ) , ( NP , pla~t , ~' t , Aune ) ) This rule can then be combined with nfles that have the root pair  ( NP , NP ) , and so on . 
If we only consider the left side part of this rule  , we can use algorithms that exist for contextfree grammars  , so that we can parse a sentence of n words with a time complexity which is polynomial in n  . These algorithms give as output a chart-like derivation forest  ( Sima'an et al ,  1994) , which contains the tree pairs of all the derivations that can be formed  . 
3.2 Translation
Since every tree pair in the derivation forest contains a tree for the target language  , the translation of this folest is trivial . 
3.3 Disambiguation
In order to selec the most probable translation , it is not efficient o compare all translations , ince there can be exponentially many of them . Furthermore , it has been shown that the Viterbi algorithm cannot be used to make the most probable selection from a DOP-like derivation forest  ( Sima ' an ,  1996) . 
Instead , we use a random selection lnethod to generate derivations from the target derivation forest  , otherwise known as Monte Carlo sampling ( Bod ,  1998 , p .  4649) . In this method , the random choices of derivation sale based on the probabilities of the nnderlying subderivations  . If we generate a large number of samples , we can estimate the most probable translation as the translation which results most often  . The most probable translation can be estimated as accurately as desired by making the number of random sample sufficiently large  . 
4 Pilot Experiments
In order to test the DOT-model , we did some pilot experiments with a small part of the Verbmobil corpus  . This corpus consists of transliterated spoken appointment dialogues in German  , English , glish datasets , which were aligned at sentence level , and syntactically annotated using different annotation schemes  . 5Naturally , the tree pairs in the corpus did not contain any links  , so--in order to make it useful for l ) OT--we had to analyze each tree pair , and place links where necessary . We also corrected tree pairs that were not aligned correctly  . Figure 4 shows an example of a corrected and linked tree from our colrection of the Verb mobil corpus  . 
We used a blind testing method , dividing the 266 trees of our corpus into an 85% training set of 226 tree pairs , and a 15% test set of 40 tree pairs . We carried out three experiments , in both directions , each using a different split of training and test set  . 
The 226 training set tree pairs were converted into fragments  ( i . e . subtree pairs ) , and were enriched with their corpus probabilities  . The 40 sentences from the lest set served as input sentences : they were translated with the fragments from the training set using a bottom-up chart parser  , and disambiguated by the Monte Carlo algorithm . The most probable translations were estinmted from probability distributions of  1500 sample derivations , which accounts for a standard deviation ?5 < 0 . 013 . Finally , we compared the resulting trauslations wilh the original translation as given in the test set  . We also fed tiletes ! sentences in lo another MT system : Alta Vista's Babelfish  , which is based on Systran .  6 4 . 1 Evahmtion In a manner similar to ( Brown et al ,  1990 , p .  83) , we assigned each of the resulting sentences a category according to the following criteria  . If the produced sentence was exactly the stone as the actual Verb mobil translation  , we assign edit the exact cat-ego W . If it was a legitimate translation of the source sentence but in different words  , we assign edit the alternale category . If it made sense as a sentence , but could not be interpreted as a valid translation of the source sentence  , we assign edit the wrong category . If the translation only yielded a part of the source sentence  , we assign edit the partial category : either partial exact if it was a part of the actual Verbmobil translation  , or partial alternate if it was part of an alternate translation  . Finally , if no translation 5The Penn Treebank scheme for English ; the Tiibingen schelne for Gernlan . 
6 This service is available on the lnte , 'net via http://babelfish . altavista , com . 


Translated as:
That wouklbe very interesting.
I ) as wiirese hrinte , essant.
l ) as w~h'es e hr inte ressant.


Translated as:


Translated as:
Parlial Exact

I will book the trains.
ich buche die Z fige.
Ich werd c(tie Ziige reservieren.
Esist jakeine Behgrde.
It is not an administrative office you know.
There is not an administrative office you know.
Translated as:
And as said 1 think the location of the branch office is posh . 
Und witges agtich denke die Lagezur
Filiales pricht Bi in deist.
ich denke die Lagel ' artial Alternatel ch habe Preiseve to Parkhotelltannover da  . 
Verbmobil : 1 have got prices for Hatmover
Park hotel here.
Translated as : for Parkhotel Hannover
Figure 5: Translation and classification examples.
was given , ?" we assigned it tilen one category . Tile re-suits we obtained from Systran were also evaluated using this procedure  . Figure 5 gives some classiIi-cation examples . 
The method of evaluation is very strict : even if ore " model generated a translation that had a better quality than the given Verb mobil translation  , we still assign edit the ( partial ) alternate category . This can be seen in the second example in figure 5  . 
4.2 Results
The results that we obtained can be seen in table 1 and 2  . In both our experiments , the number of exact translations was somewhat higher tlmnSys-trmfs  , but Systran excelled at the number of alternate translations  . This can be explained by the fact that Systran has a much larger lexicon  , thus allowing it to form much more alternate translations  . 
While it is meaningless to compare results obtained from different corpora  , it may be interesting to note that Brown et al ( 1990 ) report a 5% exact match in experiments with the Hansard corpus  , indicating that an exact match is very hard to achieve  . 
The number of ungrammatical translations in our
SS
LK1 V~F "~_- MD:-NP=-_VP
I-I machen NX NX ADVX/shall we VB NP ADVP
IIItI\[wiresdanndoit then
Figure 4: Example of a linked tree pair in Verbmobil
Corpus Categorical ccuracy
Max . Size Correct Incorrect Partial
Depth Exact Alternate Ungn Wrong Exact Alternate 1   1263   16  . 22% 2  . 70% t 8 . 92% 18 . 92% 18 . 92% 24 . 32% 2 2733 16 . 22% 2  . 70% 32 . 43% 5 . 41% 27 . 03% 16 . 22% 24 . 32% 13 . 51% 3 8228 4 14192 18 . 92% 5 . 41% 18 . 92% 5 . 41% 32 . 43% 5 . 41% 32 . 43% 5 . 41% 24 . 32% 13 . 51% 5 22147 18 . 92% 5  . 41% 32 . 43% 5 . 41% 24 . 32% 13 . 51% 6 27039 18 . 92% 5  . 41% 32 . 43% 5 . 41% 27 . 03% 10 . 8 t % 18 . 92% 5 . 41% 33479
Systran 32 . 43% 5 . 41% 18 . 92% 35 . 14% 8 . 11% 37 . 84% 24 . 32% 13 . 5 1%   0%   0% Table 1: Results of English to German translation experiments English to German experiment were much higher than Systran's  ( 32% versus Systran's 19% )  ; vice versa it was much lower ( 13% versus Systran's 21% )  . Since the German grammar is more complex than the English grammar  , this result could be expected . It is simpler to map a complex grammar to a simpler than vice versa  . 
The partial translations , which are quite useflfl for forming the basis of a postedited  , manual translation , varied around 38% in our English to German experiments , and around 55% when translating from German to English . Systran is incapable of forming partial translations  . 
As can be seen from the tables , we experimented with the max in mm depth of the tree pairs used  . We expected that the performance of the model would increase when we used deeper subtree pairs  , since deeper structures allow for more complex structures  , and therefore better translations . Our experiments showed , however , that there was very little increase of performance as we increased the maximum tree depth  . A possible explanation is that the trees in our corpus contained a lot of lexical context  ( i . e . terminals ) at very small tree depths . Instead of varying the maximum tree depth , we should experiment with varying the maximum tree width  . We plan to perform such experiments in the future . 
5 Future work
Though the findings presented in this article cover the most important issues regarding DOT  , there are still some topics open for future research  . 
As we stated in the previous section , we wish to see whether DOT's performance increases as we vary the maximum width of a tree  . 
In the experiments it became clear that DOT lacks a large lexicon  , thus resulting in less alternate translations than Systran  . By using an external lexicon , we can form a part-of-speech sequences fiom the source sentence  , and use this sequence as input for DOT . The resulting target part-of-speech sequence can then be reformed into a target sentence  . 
The experiments discussed in this article are pilot experiments  , and do not account for much . In order to find more about DOT and its ( dis ) abilities , more experiments on larger corpora are required . 
6 Conclusion
In this article , we have presented a new approach to machine translation : the Data-Oriented Translation model  . This method uses linked subtree pairs for creating a derivation of a sentence  . Each subtree-pair consists of two trees : one in the source language and one in the target language  . Using these subtree pairs , we can form a derivation of a given source sentence  , which can then be used to form a target sentence . The probability of a translation can
Max . Size Correct l ) epth Exact Alternate 1126 3227 3338 22815 . 38% 2 . 56% 12 . 82% 7 . 69% 12 . 82% 1) . 26%

Ungr . Wrong 12.82% 12.82% 12.82% 12.82% l'artial
Exact Alternate 41 . 03% 15 . 38% 35 . 90% 17 . 95% 38 . 46% 17 . 95% 12 . 82% 7 . 69% 4 14192 15 . 38% 7 . 69% 12 . 82% 10 . 26% 35 . 90% 17 . 95% 5 22147 15 . 38% 5 . 13% 12 . 82% 12 . 82% 35 . 90% 17 . 95% 6 27039 15 . 38% 5 . 13% 12 . 82% 10 . 26% 38 . 46% 17 . 95% oo 334 7915 . 38% 7 . 69% 12 . 82% 7 . 69% 38 . 46% 17 . 95% Systran 12 . 82% 25 . 64% 2(/ . 51% 41 . 03% 0% 1 ) % Table 2: Results of German to English translation experiments then be calculated as the total probability of all the derivations that form tiffs translation  . 
The computational aspects of DOT have been discussed  , where we introduced a way to reform each subtree pair into a productive rewrite role so that wellknown parsing algorithms can be used  . We de-l:ermine the best translation by Monte Carlo sampling  . 
We have discussed the results of some pilot experiments with a part of the Verb mobil corpus  , and showed a method of evaluating them . The ewflua-tion showed that DOT produces less correct rans-lation than Systran  , but also less incorrect ransla-tions . We expected to see an increase in performance as we increased the depth of subtree pairs used  , but this was not the case . 
Finally , we supplied some topics which art open l ' or future research  . 

A . Abeill 6, Y . Schabes , and A . K . Joshi .  1990 . Using lexicalized tags for machine translation . In Proceedings of the 13th international col ~\ [ erence on computational linguistics  , volume 3 , pages 16 , Helsinki . 
R . Bod .  1995 . Enriching linguistics with statistics : Pelformance models of natural language  . Number 1995-14 in ILLC Dissertation Series . Institute for Logic , Language and Computation , Amsterdam . 
R . Bod .  1998 . Beyond grammar : an exl ) erience-based theory of language . Number 88 in CSL Ilecture notes . CSLI Publications , Stanford , California . 
J . Brown , J . Cocke , S . Della Pietra , V . Della Pietra , E Jelinek , J . Lafferty , R . Mercer , and R Rooss in . 
1990 . A statistical approach to machine translation . Computational Linguistics , 16(2):79-86 . 
M . Nagao .  1984 . A framework of a mechanical translation between Japanese and English by analogy principle  . In A . Elithom and R . Banmji , editors , Artificial and Human Intelligence , chapter 11 , pages 173-180 . North-Holland , Amsterdam . 
A . Poulsma .  1998 . Data-Oriented Translation . In Ninth Conference of Computational Linguistics in the Netherlands  , Leuven , Belgium . Conference presentation . 
A . Poutsma .  2000 . Data-Oriented Translation : Using the DOP framework for MT  . Master's thesis , Faculty of Mathematics , Computer Science , Physics and Astronomy , University of Amsterdam , the Netherlands . 
R . Scha .  1990 . Taaltheorientaal technologie ; competence en performance . In Q . A . M . de Kort and G . L . J . Leerdam , editors , Coml ) utertoel ) assiu-genin de Neerlandistiek . Landelijke Verenigmg van Neerlandici , Ahnere , the Netherlands . 
S . M . Schieber and Y . Sehabes .  1990 . Synchronous tree-adjoining grammars . In Proceedings of the 13th international cor ( ference on computational linguistics , volume 3 , pages 253-258 , Helsinki . 
K . Sima'an , R . Bod , S . Krauwer , and R . Scha . 
1994 . Efficient disambiguation by means of stochastic tree substitution grammars  . In Proceedings International Crmference ou New Methods in Language Processing  , Manchestel ; UK . 

K . Sima ' an .  1996 . Computational Complexity of Probabilistic Disambiguation by means of Tree Grammars  . In Proceedings COLING96, Copenhagen , Denmark . 
A . Way .  1999 . A hybrid amhitectum for robust MT using LFG-DOP . Journal of Experhnental and Theoretical Artificial Intelligence  ,  11(3) . 

