ARGUING ABOUTPL ANNING ALTERNATIVES
ALEX QUILICI
Department of Electrical Engineering
2540 Dole Street , Holmes Hall 455
University of Hawaii at Maaoa
Honolulu , HI , 96822

In discourse processing , two major problems are understanding the underlying connections between suc-ce ~ ivedia log utterances and deciding on the content of a coherent dialog response  . Thin paper presents a computational model of these tasks for a restricted class of argumentative dialogs  . In these dialogs , each response presents a belief that justifies or contradicts another belief presented or inferred earlier in the dialog  . Understanding a response involves relating a stated belief to these earlier beliefs  , and producing a response involve selecting a belief to justify and deciding upon the set of beliefs to provide as its justification  . Our approach is knowledge baaed , using general , common sense justification rules to recognize how a belief in being justified and to form new justifications for beliefs  . This approach provides the ability to recognize and respond to never before seen belief justifications  , a necessary capability for any system that participates in dialogs involving disagreements  . 
1 Introduction
In discourse processing , two major problems are understanding the underlying connections between successive dialog responses and deciding on the content of a coherent dialog response  . This paper presents an initial model that accomplinhes these tasks for one class of argumentative dialogs  . In this class , each dialog respouse presents a belief that justifies or contradicts a belief provided earlier in the dialog  . 
The following dialog fragment is an example : ( 1 ) TIDY : The members of the AIlab should clean it themselves  . 
(2 ) ScguPPY : But that interferes with doing research . 
(3) TIDY : There's no other way to keep it clean.
(4 ) SCRUFf'Y : We can pay a janitor to keep it clean . 
(5) TIDY : We need money to pay a janitor.
(6 ) SCRUFFY : We can transfer the money from the salary fund  . 
(7 ) TIDY : But doing that interferes with paying the labmembers  . 
(8 ) SCRUVFY : It's more desirable to have a clean lab than to pay the labmembers  . 
Each response states one or more plan-oriented beliefs  , usually as part of a short chain of reeanning justifying or contradicting a belief provided earlier in the dialog  . 
In (1) , TIDY begins by stating a belief : the labmembers hould execute the plan of cleaning the lab  . 
In (2) , SCRUFFY responds with a belief that the lab members executing this plan interferes with their doing research  . This belief justifies SCRUFFY~s unstated belief that the labmember should not execute the plan of cleaning the lab  , which contradicts TIvY's stated belief in (1) . SCRUFPY's underlying reasoning is that the lab member shouldn't clean the labbe cause it interferes with their executing the more desirable plan of doing research  . 
In (3) , TIDY presents belief that there's no alternative plan for keeping the labcle an  . This belief justifies TIDY's belief in (1) . TIDY's underlying reasoning is that the lab member should clean the labbe cause it's the best plan for the goal of keeping the labcle an  , and it's the best plan because it's the only plan that achieves the goal  . 
Finally , in (4), Scs . uF ta'y states a belief that paying a janitor achieves the goal of keeping the labcle an  . This contradicts TIDY's stated belief in (3) . 
It also justifies a belief that the labmembers cleaning the labisn't the best plan for keeping the labcle an  , which contradict ~ one of the beliefs inferred from  ( 3 )  . SCRUFFY's reasoning is that paying a jan-itor is a more desirable plan that achieves thin goal  . 
The remaining responses follow the same pattern.
Understanding responses like these involves relating a stated belief to beliefs appearing earlier in the dialog  . That requires inferring the participant's underlying reasoning chain and the beliefs it justifies  . 
Producing these responses involve selecting a belief to justify and deciding upon the set of beliefs to provide as its justification  . That requires constructing an appropriate reasoning chain that justifies holding any unshared beliefs  . 
Our focus in this paper is on an initial method for representing  , recognising , and producing the belief justifications underlying dialog responses that provide coherent defenses of why beliefs are held  . 
ACRESDE COLING-92 , NANTES , 2328 A Ol~'r1992906 PROC . OFCOLING-92, NANTES , AUG .  2328 .   1992 The behavior model edit limited in several significant ways  . Fir Jt , we do not try to recognite when an trguer's response contradicts one of his earlier esponses  , such as the contradiction between (2) and (8) , nor do we try to avoid producing such responses . 
Second , we do not try to recngnise or make use of high -level arguing strategies  , uchas reductioedab*urdum . Third , we restrict ourselves to a small clam of beliefs involving planning  . Finally , we start with represent at in ~ of beliefs and ignore the linguistic issues involved in turning responses in to be \] ida  . 
Clearly , all these limitations must eventually be exi -dressed in order to produce a more realistic model of debate  . Our belief , however , it that an initial model of the process of rccognising and producing belief justifications i a useful and necessary first step  . 
2 Our Approach
Our approach to these tasks rests on a simple assumption : Dialog participants jus L if ~ beliefs within-stantialions of general  , common sense justification ra/es . For plan-oriented beliefs , a justification rule corresponds to a planning heuristic that's based solely on structural features of plans in general  , not on characteristics of specifc plans themselves  . 
The first few responses in this dialog illustrate several justification rules  . In (2), SCRUI~F'?uses the rule : O ~ ere . on wh ~ a plan should n ' ~ beezecuted is that it conflicts with assenting a more desirable plan  . 
Similarly , in (3) , TXDY chains together a pair of these rules : One reason why a plan should be ezecuted is that it's the be  , tplan/or achieving a goal , and One reason why a planil the be , t plan for a goal is that if ' #the onl ~ plan that achieves the goal  . 
Given our assumption , understanding a response it equivalent to recogniting which justification rules were chained together and instantiated to form it  , determining which belief to address in a response it equivalent to determining which beliefs in a chain of instantiated justification rules axe not shared  , and producing a justification is equivalent o selecting and instantiating justification rules with beliefs from memory  . 
We make this assumption for two reasons . First , dialog participant should be able to understand and respond to never before seen belief justifications  . 
That suggests applying general knowledge , such as our jtmtification rules , to analyse and produce specific juJtifications , as that knowledge is likely to be shared by different participants  , even if they hold different beliefs about specific courses of action  . And second , dialog parlieipants should a bobeable iouse the same knowledge for different foJks  . That suggests that arguments about planning should use the Msneknowledge as planning itsel ? The justifies-tion rules for plan-oriented  belief1 describe knowledge that a planner would aim find nsdul in welectlng or constructing new plans  . 
Our approach diffem in two ways fzom previons modeh of participating in dialogs  . First , the *? models emphe ~ is edplan recognition : the task of recognising and inferring the underlying plans and goal J of a dialog paxtlcipant  \[4  ,  10 ,  17 ,  18 ,  2\] . They view utternnces as providing steps in plans ( typically by describing o also ractions ) and tie them together by inferring an underlying plan  . But in an argument not only must the participant 's plans and goals be inferred  , but alto their underlying belie/s about those plans and goals  . Our approach suggests a model that infers these beliefs as a natural consequence of trying to understand connections between successive di Mog utterances  . In contrast , existing approaches to inferring participant beliefs take a stated belief and try to reason about possible justifications for it  \[12  ,  9\] . Previous models have also tended to view providing a dialog response solely as a part of the question answering process  . In contrast , our approach suggests that responses arise as a natural consequence of trying to integrate newly -encountered beliefs with current beliefs in memory  , and trying to understand any contradictions that result  . 
3 Justification Rules
The argumentative dialogs we've examined have two types of plan-oriented beliefs :  facts61 and evalus-flee \[1\]  . Factual beliefs are objective judgements about planning relationships  , uc has whether a plan has a particular effector enablement  . They represent the planning knowledge held by moat previous plan-understanding and plan -constructing systems  . 
Evaluative beliefs , on the other hand , are subjective judgements about planning relation Jhipe  , such as whether or not a plan should be executed . Although these beliefs have generally been ignored by previous systems  , they are crucial to participating in arguments involving plan-oriented beliefs  . 
Our assumption is there exists a small set of justification rules for each planning relationship  . Each rule is represented as an abstract configuration of planning relationships that  , when instantiated , provides a reason for holding a particular belief  . For example , the rule that a plan shouldn't be executed if it conflicts with a preferred plan is represented as : IF interforso  ( occtur ( P )   . occtn'(P ')) tlIDfavoxa(occu ~ r(P'), occ~(p))
THENought(not(occu . ~ OF )))
That is , a plan shouldn't be executed if ( 1 ) it inter-fere B with another plan , and (2) that plan is preferred to it . Figure 1 lists our current justification rules for ACRES DE  COLING-92  . NANTES . 2328 AO~r1992907 I~ROC . OVCOLING-92 . NANTES . AUG .  2328 .   1992 \] ~ tee ~' ot ~ why execuginl ~ plan X/n desirable :
XiJ the be ~ tplt ~ forgg 0 al.
Executing Xhaa enable meatforn goal.
_Re . as p as why execnt in l ~ plan X . IS undesirable : X conflicts with a more desirable plan  . 
X has a nua defirable ffect.
X h ~ an undefirable nable meat.
Remtoua why plan XiJ the best plan for n~o a\]: X hi the only plaza that a chiev ~ the goal  . 
No plan more desirable than X achieves the goal.
Re ~ oas why plan X is not the best plan for a goal :
X hatan unachievable ensblement.
X's execution is undesirable.
Some more desirable planschieve ~ the goal.
Retts on swhy plan X is more desirable than plan Y : X heat a desirable ffect that Y doesn't have  . 
X doesn't have an undesirable effect that Y h ~.
X doesn't have an undesirable enablement that Y has  . 
Y conflicts with a more desirable plan and X doesn 't  . 
Xi * an enablement of a mote desirable plan than Y  . 
X has an effect more des~nble than Y .
Re~onswhy achieving oal G is undesirable : The only plan for achieving G in undesirable  . 
Achieving G has an undesirable effect S.
Reasons why achieving oal Gi ~ desirable: Achieving G in an enablement for another goal  . 
Not achieving G has an undesirable effect S.
Figure 1: Justification rules.
evaluative beliefs ( ~ ee \[13\] t br representational details and criteria for dedding what is a reasonable justification rule  )  . These rul ? ~ were abstracted from examining a variety of different plan-oriented argumentative dialogs  . 
The power of these justification rules comes from their generality : A single rule can be instantiated in different ways to provide justifications for different beliefs  . In (2) , SCRUFFYUSes the above rule to justify a belief that the labmember shouldn't clean the lab themselves  . In (7) , TIDY uses the same rule to justify a belief that the labmembers shouldn't transfer money front the salary f nnd  . Here , TIDY's justification is that tranderring the money interferes with the more desirable plan of paying researchers  . 
4 Recognizing Justifications
The proee ~ of understanding a dialog response is modeled as a forwar & chaining search for a chain of instantiated justification rules that  ( 1 ) contains the user ~ stated belief , and ( 2 ) jastifies an earlier dialog belie for its negation  . 
We briefly illustrate this proce ~ by showing how SCRUt'FY understands TIDe's response in  ( 3 )  . The input belief is that the labmembers denning the lab is the only plan that achieves the goal of keeping the labcle an  . This belief matches an antecedent in a pair of justification rules  , so the process begins by inetantiating these rules  , resulting in pair of possible justification chains that contain TIDY's stated belief :  ( 1 ) the labmembers cleaning is the beef plast for ~ ep-lag the labcle an becalst it's the only pianist keeping the labcle an  , and ( 2 ) the labshon ldntl~keptc/cart because the only plan for that goal is the wades ~ ble plan of having the labmembers cleaning iLN either justification directly relates to the dialog  , so the next step is to determine which one to pursue further  , and whether either can be eliminated from further consideration  . Here , the second justification contains a belief that the labmembers cleaning the labis undesirable  , which contradicts TIDY's stated belief in (1) . Applying the heuristic " D/a eard any potential justification containing beliefs that contradict the speaker's earlier beliefs " leaves only the first justification to pursue further  . It's consequent in the antecedent of a single justification rule  , and instan-tinting tiffs rule leads to this justification chain : the labmembers should clean the labbe cause their elear ~  . 
lag the lab is the best plan for the goal of keeping the labclear * because it's the only plan for keeping tlAe labcle an  . The justified belief is TIDY's belief in (1) , so the process stops . 
In general , the understanding proceuit more complex , since justification rules may not be completely instantiated by a single antecedent  , and may therefore need to be further i astantiated from beliefs in the dialog context and memory  . There ahm may be many possible chains to pursue even e ~ ter heuristically discarding some of them  , requiring the tree of other heuristics to determine which path to follow  , such as " Pursue the reasoning chain whidt e oltains the most beliefs found in the dialog e o n t e a ~  . 
5 Selecting A Belief To Justify
After recognizing a participant's reasoning chain , it's necessary to select a belief to justify as a response  . 
This task involves determining which beliefs are not shared  , and selecting the negation of one of tho ~ beliefs to justify  . 
An intuitive notion of agreement is that a belief is shared if it it's found in memory or can be justified  , and it's not shared if its negation it found in memory or can be justified  . But this notion is computationally expensive , since it could conceivably in . 
volvo trying to justify all the beliefs in the lmrtie-ipant ' a reasoning chain  , as well as their negatinas . 
As mlalternative , our model determines whether a belief is shared by searching memory for the belief and its negation and  , if that fails , applying a small Acrl~s DECOLING-gZN arcH~s , 2328 Ao(rf 1992908 PROC . OFCOLING-92 . NANTES . AUG . 2328,1992 set of agreement heuristics . One such heuristic is " Assume a belief is sassed if a justil ~ ling geaera / / za-lion is found in tattooer  . So , for exanlpie , if the belief " keep everything clean " is found in memory  , the belief * keep the AI lab clean ~ is considered to he shared  . If no agreement heuristic applies , the belief is simply marked as U unknown " . 
After determining whether each belief in the par -ticipant's reasoning chain is shared  , the model first searches for an existing justification for an unshared belief's negation  . If that fails , it then tries to create a new justification for an unshared belief's negation  . 
And if that fails , it tries to create a new justification for the negation of one of the unknown beliefs  . 
This way existing justifications are presented before an attempt is made to construct new ones  . If none of these steps succeed , the assumption is that the rea-Boning chain is shared  , and an attempt is made to form a new justification for the belief it contradicts  . 
Thus , the belief our model addresses in a response arises from trying to discover whether or not it agrees with another participant's reasoning  . 
6 Forming Justifications
To form a new justification for a belief , our model performs a backward chaining search fo ~" a chain of justification rules that justify the given belief and that can be iustantiated with beliefs from memory  . 
We briefly illustrate this process by showing how SCRU ~' Fyforms the response in  ( 2 )  . The belief to justify is that it's not desirable to have the labmembers clean the lab  . The first step is to instantiate the justification rules that have this belief as their consequent  . That results in several possible justifications :   ( 1 ) there's an undesirable nablement of cleaning the lab  ,   ( 2 ) there's an undesirable effecf of cleaning the lab  , or ( 3 ) the labmembers cleaning the labconflicts with a more desirable action  . 
The next step is to try to fully i astantiate one of these rules  . Applying the heuristic " Pursue the most instantiafed justification rule " suggests working on the last rule  . Here , SCRUFFY instantiates it with a belief from memory that research is more desirable than cleaning  . Once a rule is instantiated , it's necessary to verify that the beliefs it contains are shared  . 
Here , that involves verifying that cleaning conflicts with research  . It does , so the instantiated rule can be presented an the response  . 
In general , the process is more complex than outlined here , since not all of the belief in an iustantiated justification rule may be shared  , and there may be several ways to instantiate a particula rule  . Those rules containing unknown beliefs require further justification  , while those rules containing unshared be ~ lids can be discarded  . 
7 Background
The closest related system is ABDUL/ILANA\[8\] , which debated the responsibility mad cause for hlstot-ical events  . It focused on the complementary problem of recogniling and providing episodic just if i ? ~ tions  , rather than justifications b~ed on the rel ~ . 
tionships between different plans.
There are several models for recognising ther ? -lationship between argument propositions  . Cobea's \[5\] taken each new belief and checks it for a justification relationship with a subset of the previnusly-stated belief ~ determined through the use of dipsing structure and clue words  . That model ture en the existence of an evidence oracle capable of determining whether a justification relationship holds between may pair of beliefs  . Our modelira . 
plements this oracle for a particular clam of plan -oriented belief justifications  . OpFkt\[3\] recognise tbo . 
lief justifications in editorial salmut economic planning through the use of argument units  , a knowbedge structure that can be viewed as complex cow figurations of justification rules  . The approaches are complementary , just as scripts\[7\] and plans\[6 ,   18\] are both useful methods for recognising the cam nections between events in a narrative  . 
Several systems have concentrated on producing belief justifications  . Our own earlier work\[14 ,  15 ,   16\] used a primitive form of j~tstification rules for factual beliefs as a template for producing corre ~tive responses for user misconceptions  . Our current model extends this work to use these rules in both understanding and responding  , and provides additional rules for evaluative beliefs  . 
ROMPER \[11\] provid as justifications for belidk about an object's class or attributes  . But it profides these justifications purely by template matching  , not by constructing more general reasoning chains . 
8 Current Status
We've completely implemented the model di~umed in this paper  . The program is written in Quintu ~ Prolog and runs on anl IP/APOLLO workstation  . 
Its input is a representation for a stated participant belief  , and its output is a representation form , up . 
propriate response . It currently includes 30 just itk a ~ tips rules and over 400 beliefs about various plans . 
We've used the program to participate in short ar- . 
gumentative dialogs in two disparate domains : day -to~day planning in the A!lab  , and removing and recovering files in UNIX . We ' recurrently using it to experiment with different heuristics for controlling the search process involved in rer  . ogn is btg and c~u . -strutting these reasoning ch ~ in ~ . 
Our xxmd clhe~eevt . L ' ~ l / ~ ey Ib ~ dt ~ tion ~ ~ ee ~ ec , ~_dyAcrEs DECOLING-92 , NAN 1 q ~ , 2328 hofrr 1992 909 Ptoct 1: COLING . 9" ~~ q t , l , ~' n ! s , Aut ; . 2328,1992 now starting to addrem . First , it views plans as atomic units and comiders only a small set of " all or nothing " plan-oriented beliefs  . This means it can't produce or understand justifications involving a tel ~ in a plan  , conditional planning relationships , or beliefs not directly involving plans . Second , our model can understand only those responses that jnstify an earlier belief  . It can't , for example , understand a response that contradicts an inferred justification for an earlier belief  . These more complex relationships can be represented using juetificnt in n rules  , but our model must be extended to recognise them . Third , our model is reactive rather than initiatory : it produces respon ~ only when there's n perceived in-  . 
agreement . It needs to be extended to know why its in an argument  , and to be aware of the underlying goals of the other argument participants  . 
9 Conclusions
Previous dialog models have focused primarily on recognising a participant's plans and goals  . But to participate in an argument i's also necessary to recognize when participants are providing beliefs about their planl and goals and how the y ' rejustifying these beliefs  . It's also necessary to be able to determine which beliefs require further justification and to formulate justifications for these beliefs  . This paper suggests a knowledge-based approach for these tasks  . 
Our approach has several attractive features.
Firs Lit builds It model of many relevant but unstated participant beliefs as a side-effect of trying to relate their utterance to the dialog  . Second , it decides which belief to address in n response as a natural consequence of trying to understand why it disagrees with another participant's belief  . Third , it understands belief jnstifieations using the same general  , common sense planning knowledge that it uses to formulate them  . Finally , it suggest shownever before seen belief justifl catinns can be understood  , so long as they were formed from general justification rules known to the participants  . That ability is crucial for participating in dialogs whose participants hold differing beliefs  . 
References\[1\]R . Abelson . Differences Between Beliefs and Knowledge Systems  . Cognitive Science 3, 1979 . 
\[2\] J . F . Allen and C . R . Perranlt , Analysing Intention in Utterances . Artificial Intelligence 15, 1980 . 
\[3\] S . J . Alvarado . Understanding Editorial Tezt : A Computer Model of A~umsnt Comprehension  . 
Kluwer , Boston , MA , 1990.
\[4\] S . Carberry . Modeling the Uxr'-Plans and Goals . Computational Li~q~isties , 14(3), 1988 . 
\[5\]R . Cohen . Analysing the Structure of Argu-ment stive Discourse  . Compwfational Linlmistios j 13(1), 1987 . 
\[6\] M . G . Dyer . ln , -depCtU~derrlanding : A Corn-pater Model of Narrative Comprehension  . MIT
Press , Cambridge , MAj 1983.
\[7\] M . G . Dyer , R . E . Cullingford , and S . 5 . Alvare ~ lo . 
Script ~ . In Encyclopedia of Artificial Inte Sigeuce,
John Wiley , NY , NY , 1987.
\[8\] M . Flowers , R . McGuire , and L . Birnbanm . Ad-versary Arguments and the Logic of Personal Attacks  . In Strategies/or Natural Language Procen-ing . Lawrence Erlbaum , Hilljiale , NJ , 1982 . 
\[9\] R . Kaas . Building n User Model Implicitly from a Cooperative Advisory Dialog  . User Modeling and
User-Adapted Inieraclion ~1(3), 1991.
\[19\]D . J . Litman and J . F . Allen . A Plan Recognition Model for Subdialogue a in Conversatinns  . C0gn/-live Science , 11, 1987 . 
\[11\]K . McCoy . Reasoning on n Highlighted User Model to Respond to Misconceptions  . Comp ~ la-tional Linguistics , 14(3), 1988 . 
\[12\]M . Pollack . Inferring Domain PlansiB Question-Answering . PhD Thesis , University of
Pennsylvania , Philadelphia , PA , 1986.
\[13\]A . Qnilici . The Csrre~ion Machine : A Computer Model Of Rscogniu'ng and Producing Belief Jnstifications in Argumentative Dialogs  . PhD Thesis , University of California , LA , CA ,  1991 . 
14\]A . Quilici . Participating In Plan-Oriented Dialogs . In Proceeding so/tAel ~ h Conference of tits Cognitive Science  5ocietj   , Boston , MA ,  1990 . 
\[15\]A . Qnilici . The Correction Machine : Formulating Explanations for User Misconceptions  . In Proceedings of ~ he 1989 International Joint Conference on Artificial Intelligence  , Detroit , MI ,  1989 . 
\[16\]A . Quilici , M . G . Dyer ~ and M . Flower L Recognising and Responding to Plan -oriented Miscon-ceptions  . Computational Linguistics , 14(3), 1988 . 
\[17\]C . L . Sidner . Plan Parsing For Intended Response Recognition in Discoume  . Compugatiomal
Intelligence , 1(1), 1985.
\[18\]IL Wileneky . Planning and Understanding . Ad-dison Wesley , Reading , MA , 1983 . 
A c'r ~ DECOLING-92 . NANTES , 2328 AOfrr 1992910 PROC . OFCOLING-92, NANI'ES . AUG .  2328 .  1992
