Aspects of Pattern-matching in Data-Oriented Parsing 
Guy De Pauw

University of Antwert)
Abstract
Data-Oriented Parsing ( DOP ) ranks mnong the best parsing schemes , pairing state-of-the art parsing accuracy to the psycholinguistic insight that larger clmnks of syntactic structures are relevant grammatical and probabilistic units  . Parsing with the DOp-model ~ however , seems to involve a lot of CPU cycles and a considerable a momtt of double work  , brought on by the concept of multiple derivations  , which is necessary for probabilistic processing , lint which is not convincingly related to a proper linguistic backbone  . It is however possible to reinter prethep oP -model as a pattern-matching model  , which tries to maximize the size of the substructures that construct the parse  , rather than the probability of the parse . By emphasizing this memory-based aspect of the DoP-model  , it is possible to do away with multiple derivations  , opening up possibilities for efiqcient Viterbi -style optimizations  , while still retaining acceptable parsing accuracy through enhanced context sensitivity  . 
1 Introduction
The machine learning paradigm of MemoryBased Learning  , based on the assumption that new problems are solved by direct refbrence to stored experiences of previously solved problems  , has beest successfully applied to a number of linguistic phenomena  , such as part-of-speech tagging , NP-clmnking and stress acquisition ( consult Daelemans ( 1999 ) for an overview )  . 
To solve these particular problems , linguistic information eeded to trigger the correct disambiguation  , is encoded in a linear feature value representation ad presented to a memory based learner  , such as TiMBL ( Daelemans et al ,  1999) . 
Yet , many of the intricacies of the domain of syntax do not translate well to a linear representation  , so that established MBL-methods are necessarily limited to low-level syntactic analysis  , like the at brementioned NP-chunking task . 
Data Oriented Parsing ( Bod ,  1999) , a state-of-the art natural language parsing system  , translates very well to a MemoryBased Learning context  . This paper describes are interpretation of the so P-model  , in which the pattern-match , in flaspects of tim model are exploited , so that parses are analyzed by trying to match a stew analysis to the largest possible substructures recorded in memory  . 
A short introduction to Data Oriented Parsing will be presented in Section  2  , followed by an explanation of the term pattern-matehin9 in the context of this paper . Section 4 describes the experimental setup and the corlms . The parsing phase that precedes the disambiguation phase will be outlined in Section  5 and a description of the 3 disambiguating models , POFG , PMPG and the combined system PCFG@PMPG ( : an be found in Sections 6 , 7 and 8 . 
2 Data Oriented Parsing
Data Oriented Parsing , originally conceived by RemkoScha ( Scha ,  1990) , has been successfully applied to syntactic natural language parsing by ll  , ens Bod (1995) ,  (1999) . The aim of Data Oriented Parsing ( henceforth DOP ) is to develop a per\[or mane e model of natural anguage  , that models language use rather than some type of competence  . It adapts the psycholinguistic insight that language users analyze sentences using previously registered constructions and that not only rewrite rules  , but cornt ) lete substructures of any given depth cast be linguistically relevant miltstbrparsing  . 
2.1 Arehiteeture
The core of a DOP-system is its TREEBANK : an annotated corlms is used to induce all substruc - t  , ures of arbitrary depth , together with their respective probabilities , which is a expressed by
J  ~
NP\111


VP killed NPi ~ raccooll
SNP\;P
NP VP Peter killed NP
NI '. ~ lraCCOOll
Figure 1: Multiplel ) eriw ~ tions its fl : equency in the TREEBANK relative to l  ; henuml ) er of substructures with the Sanleroot node . 
Figure 1 shows the coral ) nationol ) eral ; ion that is needed to tbrm the correct l ) arse tree for the sentence Peter " killed a racco on  . Given a tree t ) ank of substructures , the syst clntries to match the leftmost open nod (  ; of a substructure ; hatis consistent with the parse tree , with the top node of another sul ) structur ( ; , consistent with the parse tree . 
Usually , ditferent conlt)inations of sul ) struc-tllrO . s are possible , as isi~l ( ti ( : a ted in Figure 1: in the examl ) leat the lefthand side the tree structure ( : ant ) ebuiltl ) y ( : o11111 ining all S-structure wil ; hast ) coiffed NP a . lld aflllly spe(:i-fledvp-structure . The right example shows another possible Colnl ) ination , where a parse tree is 1 ) uiltt ) yconll ) ining the \] ninimals ut ) s ; rltcl ; ures . 
Nol ; ethatt\]\]cseare(:(msisl ; ( mtwit\]lol'dinary rewrite-rules , such as s-+NP VP . 
One t ) art it : ul ; ~ r1)~trse tree may t ; hus(:()\]lsist()f several(lill . ( u ' ( ml ; deriva , tio'n . s . .To lind l;hc 1)rot ) - al)ility ( If ; I , ( terivation , welnult it ) lytimt ) rot ) a-1 ) ilities of the substructures thai ; were used to l . ( ) rm the derivation . To lind the t ) robal ) ility of a parse , we must ; intlrilmit ) le sum the t ) rol ) at ) il-ities of all its deriw ~ tions . 
It is COlnl/utationally hardly tra(:tat)h ; to COil-sider all deriw ~ tiollst . () reach pars (' . Since VITF , RB Iol ) timization only su ( ' ( : ceds in finding the most 1 ) robal ) h ' ~ ( teriw ~ tion as opposed to the most 1 ) robal ) lel ) arse , the MONTECARLO algorithm is introduced as a properal  ) proxima-tionI ; hat randomly generates a large nlmfl ) er of deriw ~ tions . The most prol ) al/lel ) arse is ( : on si ( t-ered to be the parse that is most often observed in this derivation forest  . 
2.2 Experimental Results of HOP
The basic 1) op-model , POP1 , wastest c , ( t ( 111 a manually edited version ( if the ATIS-corlnlS ( Marcus , Sant ( lrini , and Marcinkiewicz , 199a ) . 
The syst ; eln was trained on 603 Sell telmes(t ) arl ; -ofstmech tagsequel mes ) and ( ; w fluated on a test set ( if 75 SC ld ; ences . Parse accuracy was used as an evahlation metric , expressing t ; 11( ; percentage of sentences in the test set for which the tlarsel  ) rOl ) osed by the system is COlnpletely identical to the one in l  ; lle originale or t ) us , l ) if l ' er-ea texl ) erilnents were conducted in which max-1111111111111111111111 1111 1111 1111 1111 1111 1111 1111 1111 1111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 111 11 ) structure size was varied . With DoPl-lill fited to a sul ) sl ; ructure-size ( If 1 ( equiw ~ lenl ;  1 ; OaPCFG ) , t ) arse accuracy is 47% . hithe(/p-l;ima \] D()l'-mo(lel , in whi(:hsut ) stru(:ture-siz(; is 1 lot limited , a1) arse accuracy of 85% is ( ll ) - tni\]lc(t . 
2.3 Short Assessment of DOP
DOI ' I in its ot ) tinlal for nl achieves a very high parse accuarcy . The comt ) utational costs of the syste111 , however , are equally high . Bed (19951 reported an average t/arsetilne of 3 . 5 hours 11 ( ; 1 . 
Sellte . n(:e . Even though (: urrent1) arset ilne is rcl ) or tc . d to l ) e11 , or (; reasollal ) le , tileoi ) timal D ( )P algoril:lml in whi ( : hn ( / ( 'onstr ; dlts are made on tll ( ' ~ size ( 1t ' sut ) structures , nlay not yet 1) etract ; able for life-siz(~ . COl'l ) () ra . 
In a contextfree grammar framework ( consistent with \] ) ( ) P limited to a sutlstru ( : tm : e-size ( If 1 )  , there is only ( meway at/arse tree cant ) et ' or nmd ( t ' ( /1: exalnl/le , the right hand side of Figure \]) , n leaning that there is Olfly one del:iva-tioll for a given  1  ) arse tree . This allows efficient
VIT Ell . BI style Ol ) till fization.
To elmo(le (: on text-sells itivity in the systeln , DOP is tbr ( : ed to introduce multiple deriw ~ tiolls , so that repeatedly the same l ) arse tree needs to 1 ) eg (  ; lm rated , l ) rillging at/(/utalot of COlll\])llta , -tional overhead . 
Even though the use of larger syntactic coil-texts is highly relew mtfl'o mapsy choling u is I  , ict ) oint-of view , there is 11 o explicit l ) reference l ) e-ingl nade t ' ( /1' larger substructures in the DOPn lodel . While the MONTECARLO optimizatiol x scheme nlax in lizes the prot  ) ability of the ( teriw > tions and seelns to 1 ) refer derivations n lade up of larger substructures  , it ; may 1) eild ; eresting to


I'CFG@PMP ( I(a ) Correct Analysis

NP-SBJ VP , prp vbp NP
NP PP PP dtm ~ in NP to NP
IIltIlI ) \] HIt ) % F\]Parse Accuracyoi * parsable sentences ( /456 )  % 373 66 . 4 83 . 0I 37 33 27 58 . 2 75 . 1 327 402 71 . 5 85 . 2 402
Table 1: Exl)erimental Results ( b ) PCFG-Analysis

NP-SBJ VP
Iprp vbp NP PP PP 81 . 8 71 . 7 88 . 2 ell ; mt in NP to NP
II nil pnnt )
Figure 2: PCFG Error AnMys is see if we can make this assumption explicit  . 
3 Pattern-matching
When we look at natural anguage parsing fl : om a memory-based point of view  , one might say that a sentence is analyzed by looking ut  ) the most similar structure for the different analyses of that sentence in meinory  . The parsing system described in this paper tries to mimic this  1  ) e havior by interpreting the pop-model as a memory -t  ) ased model , in which analyses are being matched with syntactic patterns recorded in memory  . Similarity t ) etween the proposed analysis and tile patterns in memory is com- 
Imted according to : ? the number of patterns needed to construct a tree  ( to be minimized ) ? the size of the patterns that are used to construct a tree  ( to be maximized ) Tile nearest neighbor t bragiven analysis can be defined as the derivation that shares the largest amount of common nodes  . 
4 The exper imenta l Setup 10-tbld crossvalidation was used to appropriately evaluate the algorithms  , a stile dataset ( see Section 4 . 1) is rather small . Like DoPl the system is trained and tested on part-of-speech tag sequences  . In a first phase , a simple bottom-up chart parser , trained on the training partitions , was used to generate parse forests t br the 1 ) art-of speech tag sequences of the test partition . Next , the parset brests were sent to the 3 algorithms ( hence t brth the disambiguators ) to order these parse forests , the first parse of the ordered parse forest being the one proposed by the disanfl  ) iguator . 
In this paper , 3 disambiguators are described : ? PCFG : siml ) le Prol ) a bilistic ContextFree ( ~ ralnlnar ? PMPG : the DOP approximation , Patten >
Matching Probabilistic Grammar ? PCFG q-PMP G : a combined system  , integrating PCFG and PMPG The evaluation metric used is pars  (  ; accuracy , but also tile typical parser evaluation metric Fmeasure  ( precision/recall ) is given ms a means of reference to other systems . 
4.1 The Corpus
The ext ) eriments were conducted oil all edited version of tile ATIS-II-corpus  ( Marcus , Santorini , and Marcinkiewicz ,  1993) , which consists of 578 sentences . Quite a lot of errors and inconsistencies were found  , but not corrected , since we want our ( probabilistic ) system to be ti ( : ally oriented tlags like-TMP all ( 1-Dill , , lllOSI ; often used in conjmml;ion with l'p , have been renlove ( t ~ since l ; here is no way of rel ; rieving this kind of semanti (: intbrmation from t ; 11( ; t ) art ; -o5sl ) ee(:htags of the ATIS-(: or tms . Synta(:ti (: flags like-sILL on the other hand , \] lave 1) een main-taine(t . Internal relations ( denoted by llllllerict lags ) were removed and tbr1 ) ractical reasons , scntenee-lellgth was limited 1; o15 words max . 
The edited (: or l ) us retained 562 sentences.
5 Parsing
As a first phase , a1) ottom-ut ) (: hart parser i ) al"sedt ; he test sol ;  . This t ) roved to t ) equite l ) rol ) lemati (: , since overall ,  1 ( )6 out of 562 sen-ten ( : es ( 190/ ( 0 ) could not 1 ) et ) arsed ,  (111(' , to the sl , arsencss of the gram nmr , meanil , gI ; ha ( ; l ; heat ) l ) ropriate rewrite rule needed to ( : onstru ( ' l ; the (: or re(:tt)~lrsetreet brasenten(:c , in the test set , wasn't featured in the , in(tu(:ed grammar . NP-annol ; at ; ion seem ( ~( t1; o1)(; t ; llelml , in (: aus(~\]' or 11n-l ) arsal ) ility . An NP like restriction code AP/57 is repres ( ml ; ed1) y the , rewrite rule:
NP-~NNNN symsym  C\])CD
Highlyst ) ccitt(:andtintstru (: tur ( ; slike these ares ( :ar ( : ean ( t are usually ll ( )t induced from the training set whell nee ( h ; d to parse the test set . 
Ongoing re , sear(:htries1; oiml)h;ln(ml;gl"am-mal;i(:a . 1 SlnOothing ; ts:tsoluti(m to ; his1) rol)hml , but one might also (: on sid (' a:genera . ling parsefol " eSi ; S with an in ( tep(mdent ~ , ; l " all lll ; Ll ' , il Mu ( :e ( l fronltheentire ( : or lms ( training set q-t ( '~si ; s(' , l ;) or a difl'erent corlms . 111 t ) ( ) th cases , however , we would need to apply 1 ) robal ) i list i ( " smoothing to beal ) le to assign t ) rot ) at ) ilities tolllkllowns (  ;  , l ; llc-lures/rules . Neither grammatical , nor t ) rot ) a-bilistic smoothing was imt ) lemented in the (  ; ell-text of the exl ) eriments , (les (: ril)ed in this 1) at ) er . 
The sl/ars ( mess of the grammar 1) rovest ; ol ) ea serious 1) ot l ; hme(:kfi ) rpars (' , a (: (: ura(:y , limiting our ( lisamlliguators t ; o a maximulnt larscact : u-racy of 81% . 
6 PCFd-experiments a PCFG constru ( : ts parse trees by using simple rewrite-rules . The prot ) al ) ility of ~ parse tree ( ; ~7 tll)e (: omlml ; edl ) ymull ; it ) lying the t ) robat ) ili-ties(1t " the . rewrite-rules that w(~ . reused to (: on-st ; fuel ; the t ) ars (: . Note that al ' CF disi(h ; nti(:altODOP\]whell we limit I ; he maximum sul ) Stl ' UC-tures size to \] , only Mlowing deriwd ; ions of the type found at the right-hand side of Figure  1  . 
6.1 Experimental Results
The first line of Tat)le I shows the , rc , sull ; s for the l ' CF (~-(' , xl ) eriments : 66 . 4% parse accuracy is an adequate result for this baseline model  . We also look at l ) arscaccuracy for parsable sentences ( an estimal ; e of the parse accuracy we 1night get if we had a more suited parse forest generator  ) and w (  ; notice that we are able to a (: hieve a 81 . 8% parseae (: ur ~ my . This is already quite high , trotonexm nining the parsed data , serious and flu Manmntal limitations to the POPO -mo  ( lcl can beel ) served 6 . 2 Error Analysis Figm'c 2 , disl ) lays the mosl ; commontyl ) c of mistake mad ( ; l ) y1) CFG ~ S . :\]' lit ;(; or r0 , cl ; t ) arse l ; ree (' ou htr ( ; i ) res(mtanmlalys is for 1 ; 11( ; senten(:e:I" . ; anto , fli . qhtfrom\]h'us . scl . sto2bronto . 
This examt ) le shows thai ; ~ tPCFG h~lsaI ; (~ , n-dency to prct brt latters trueture , so veremt ) ed de , dstru(:t ; ures . This is a trivial effect of 1 ; 11( ; math c-mat ; it ' lltbrmula used to conqml ; ethet ) rol ) at ) il-il ; y of aI ) arse-tr ( ; ( ; : emt/cdded structure require more r ( ; writ('rules , adding more fat : tots to the multii ) li(:ation , whi (: hwillalm (/ stilw , vit~d)lyr(;-suitin:t lower l ) rol ) al)i lit ; y . 
11 ; is all 1111J '() ri ; llllal ; e1) r()I )( ; rl ; y of I ' CF G~s t ; hal ; the mmf l ) er of no ( l ( ; s in the 1) atsetree is invers(~ly1) rot ) or tiom d ; et oil ; s t ) rol ) al ) ility . ( ) n ( ; might t ) einclin ( xlt on ( n'maliz caparse tree's pr ( )bat ) ility relative t ( /themnnt ) er of nodes in the tree , but a more linguistically solm d alternative is at hand : the enhancenmnt of context sensii  ; ivity through the use of larger synl ; tt(:ti (: (: ont ( ; xt ; within t ) arse tre(:s(: ; / , 11 make our disaml ) iguat ; or lnore rolmst . 
7 pMpo-experiments
The 1) at t ( ; rn-Matching Prol ) al ) ilistie Gramnmr is a memory-based interpretation of a \]  ) OI'-model , in which as ( mtence is analyzed t ) y matching the largest , possible chunks of syn-t ; acti (" strut:lure Oll the sentence . To COml ) ilet/~rse trees into pat , terns , all substruct m ' esill thel ; raining set are eneo(ted1) y assigning l ; hem specific indexes , NP ( o ) 345 e . g . denotil ~ gafully specified NP-sl ; ruel ; urc . This apt ) roa ( : h was in-sl ) ired 1 ) y Goodman ( 199 ( i )  , in which Goodman trees to transform DOP into a Sl equivalent PCFG  . 
The system of indexing ( which is detailed in DeP auw ( 2000 ) ) used in tim experiments described in this paper , is however specifically geared towards encoding contextual in tbn nationi parse trees  . 
Gives , an indexed training set , indexes can then be matched on a test set parse tree in a bottom-up fashion  . In the tbllowing example , boxed nodes indicate nodes that have been retrieved from memory  . 
S v p p r p v b p\[~d t n n i
Imlp
In this example we can see that an NP , consisting of aflflly specified embedded NP and l'P  , has l ) een completely retrieved from men > or y , meaning that the NP in its entirety can be observed in the training set  . However , nov p was t bund that consists of a VBP and that particular NP  . Disambiguating with PMPG coil-sequently involves pruning all nodes retrieved froluille il lory : 

NP-SBJ VP vbp NP
Finally , the probability for this pruned parse tree is computed in a pCFO-type manner  , not adding the retrieved nodes to the product : P ( parse ) = P ( s - - + NP-SBJ VP )   . P ( vp--+vbI)NP ) 7 . 1 Exper imenta l Resu l ts The results tbr the PMPG-exI  ) erinmnts can be ibund on the second line of Table 1  . On some partitions , PMPG pcrtbrmed in significantly better than PCFG , but Table 1 shows that tile results for the context sensitive scheme are much worse  .  58 . 2% over all parse accuracy and 71 . 7% parse accuracy on parsable sentences indicates that PMPG is * sotavalid approximation of DOP'S context sensitivity  . 
7.2 Error Analysis
The dramatic drop in parsing accuracy calls t bran error analysis of the parsed data  . Figure 3 is a prototypical mistake PMPG has made . The correct analysis could represent a parse tree for a sentence like : What flights can I get fi rmB russels to  2brvnto  . 
The PMPG analysis would never have been considered a likely candidate by a common PCFG  . This particular sentence in fact wasef t brtlessly disambignated by the PCFG  . Yet the fact that large chunks of tree structure are retrieved Dora memory  , make it the preferred parse for the PMPG . We notice t br instance that a large part of the sentence can be matched on an SBAR structure  , which has no relevance whatsoever . 
Clearly , PMPG overestimate substructure size as a feature for disambiguation  . It's interesting however to see that it is a working implementation of contextsensitivity  , eagerly matching patterns from memory . At the same time , it has lost track of common sense PCFG tactics , it is in the combination of the two that one may find a decent disambiguator and accurate implementation of context sensitivity  . 
8 A Combined System ( PMP G@PCFG)
Table 1 showed that 81 . 8 (/ o of the time , a PCFG finds the correct parse ( Ibrt ) arsable sentences )  , meaning that the correct parse is at the first place in the ordered parset brest  . 99% of the time , the correct parse can be tbund among the 10 most probable parses in the ordered pars (  ; forest . This opens up a myriad of possibilities tbr opt in  , ization . One might for instance use a bestfirst strategy to generate only the  10 best parses , significantly reducing parse and disambiguation time  . An optimized is an Niguator might the retbre include a preparatory phase in wt fich a common sense PCFG retains the most probable parses  , so that an lore sophisticated t bllow-up scheme , teed not bother with sense-less analyses . 
In our experiments , we combined the common sense logic of a PCFG and used its output as the  PMPG'8 input . This is a well-established technique usually refi ~rred to as sys-tent combination  ( see van Halteren , Zavrel , and Daelemans (1998) for an application of this



I , , no s , : In ' obable parso , s\[


I \] most In ' obablcImrse\[
Weart ' . also presented with th (' , possibility to assign a weight to each algorithm 's decision  . 
The probability of a parse can the ) e described with the following formula:
I~/ , (rewrito , - rule ) iil )(\] m'""s'(O = ( #non-inde , xed nodes ) , , The weight of ea(:h algorithm's ( lc(:ision , as well as them nnt ) er of 1HOSt ) robM ) h ; parses that m : e extrat ) olated for the 1 ) attern-m~tt:hingalo-rithnq are parameters to 1 ) e optimized . Futm : e work will include evaluation on a validation set to retrieve the ol  ) timal va , hles for these 1) a ram-e , tcrs . 
8.1 Results
The third line in Tattle 1 shows that the com-1 ) ined system 1 ) ert ' or lns better them either one , wit; haparse accuracy of 71 . 5% and close I ; o90%1)~trs ( ; at : curacy on t ) arsal ) l(~scnt(m(:es , whi (: hw (' , (- nn consider an at ) l ) roximat ; ion of results rc-porte A for DOP1 . Error annlysis shows that the combined system is ilMe  , edM ) I e to over t : or e ( ; difficulties of both Mgorithms . The examt flo , in Figure 2 as well as the , ex ~ mlple in Figure 3 were disanllfiguated correctly using the combined syst  (  ; m 9 Future Research Even thoug \] lt\]le PMPG shows a lot of promise in its parse at : curacy  , the following extensions ne , ed to be researched :
Optimizing PMPG@PCFG for comtmta-tionalet fieieney : the graph in Section  8 shows a possible optimized parsing system , in which a preprocessing POF ( I generates then most likely candidates to 1 ) e extrapolated t br the actual disant biguator . Full parse forests were generated for the experiments descrit  ) e , d in this paper , so that the efiiciency gain of such a system Calmott  ) eprot ) erly estimated . 
PMPG@PCFG as all approximationeeds to be compm'ed to actual D  ( )P ~ by having DOP parse the data used in this experiment  , and by having PMPG-I-I'CFG parse the data used in the exl  ) erilnents described in Bod ( 1999 )  . 
Thel ) ottlelm ck of the sparse grammar 1 ) roblem prevents us from flflly exploiting the disambiguating power of the pattern-matching algorithl n  . The ORAEL-system ( G Rammar Adaptation , Evolution and Learning ) that is currently being devel-olm d , tries to address the t ) roblem of grammatical spars ( mess by using evolution ary te ( :lmiquestog ( ' ncrate , , Ol)l;imizo , and com-l ) lemeld , g ~ rallllllars . 
10 Conclusions
Even though l ) ( ) l '\] exhil ) its outstanding parsing 1 ) eh ~ vior , the eticiency of the model is rathe , r problematic . The introduction of mul-tit fieder iwd ; ions causes a considerable amount of computational overhead  . Neither is it clear how the concept of multiple deriwd  ; ionstrans-lal ; esto at ) sycholinguistic context : there is no proof thai ; lml guage users consider ( titf '( ; r c n t i n-s t ~ m t i a t i on s of t h ( ; same parse , whmt deciding on the correct an Mys is for a given sentence  . 
A 1) M ; tcrn-m~t : chil~gschcnmw ~ s1 ) rcsen Lcd that tried to dis~mfl ) iguate parse forests by trying to maximize the size of the sul  ) strnc-tures that can 1 ) eretrie , ved from inoanory . 
This straightforward memory-based intert ) rcta-tion yields sut ) -st and m'd parsing accuracy . But the ( : oml ) in a tion of common sense l ) robal ) ili-tiesnnd enhanced context sensitivity provides a work M  ) let ) arse forest disambiguator , indicating that language users might exert a CO ml ) lexcorot ) libation of memory-based recollection techniques and stored statistical data to analyze utterances  . 

Bod , R .  1995 . Enriching linguistics with stat:istics : Per -fornmnce models of natural anguage  . Dissertation,
II , LC , Univ crsite it wm Alnsterdanl.
Bod , l . ens .  1999 . Be , pond Grammar AnE : rpericn cc-Based ~ lTu:ory of Language  . Cambridge , Fngl and:
Cambridge University Press.
Daelcmans , W . , J . Zavrcl , K . Van der Sloot , and A . Van den Bosch .  19!)9 . TiMt 3L : Tillmrg Memory

WHNP SQ
WHNP PPPP wd tmmx xx/ ( b ) PMPG Analysis vbp NP-SBJ VP ' prp vbNPP PP xx in NP to NP 
IIn npmt pwdtfills

NP-SBJ\[~\]/~n npml p x x x
INP-,
X X X V
Iprp
Figure 3: PMPG Error Analysis
Based Learner , version 2 . 0, reference manual . Technical Report ILK-9901 , ILK , Tilburg University . 
Daelemans , Walter .  1999 . Memory-based language pro-ccs sing . Journal for Ez'perim cntalnd Theoretical Artificial Intelligence  ,  11:3:287 467 . 
DePauw , Guy .  2000 . ProbabilistischcParsers-Con-te ~: tg cvocligheid cn Pattcrn-Matehin  9  . Antwerpen , Belgium : Antwerp Papers in Linguistics . 
Goodman , Joshua .  1996 . Efficient algorithms for parsing the dop model . In Proceedings of the Co@fence on Empirical Methods in Natural Language Processing  . 
pages 143152.
Marcus , M . , B . Santorini , and M . A . Marcinkiewicz . 
1993 . Building a large a mlot at cd corpus of english : The Petal Tl-eebank  . Computational Lingnis-ties , 19(2):313-330 . 
Scha , R .  1990 . Ta altheorie entaaltectmologie : competence cn performance  . In Q . A . M . dtKort and G . L . J . Lcerdam , editors , Computcrtocpass in- . qcn in dcNc crlandistick , LVVN jaarboek . Landelijke
Vereniging van Ncerlandici.
van Halteren , It . , J . Zavrel , and W . Daclemans .  1998 . 
Improving datadriven word class tagging by system combination  . In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics  , Montr'eal , Quebec , Canada , pages 491-497 , Montreal , 
Canada , August 1014.

