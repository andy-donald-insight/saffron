Proceedings of the 22nd International Conference on Computational Linguistics ( Coling 2008), pages 345?352
Manchester , August 2008
Modeling Chinese Documents
with Topical Word-Character Models
Wei Hu1 Nobuyuki Shimizu2
1Department of Computer Science
Shanghai Jiao Tong University
Shanghai , China 200240
{no_bit,hysheng}
@sjtu.edu.cn
Hiroshi Nakagawa2 Huanye Sheng1
2Information Technology Center
The University of Tokyo
Tokyo , Japan 113-0033
{shimizu , nakagawa}
@r.dl.itc.u-tokyo.ac.jp
Abstract
As Chinese text is written without word boundaries , effectively recognizing Chinese words is like recognizing collocations in English , substituting characters for words and words for collocations.
However , existing topical models that involve collocations have a common limitation . Instead of directly assigning a topic to a collocation , they take the topic of a word within the collocation as the topic of the whole collocation . This is unsatisfactory for topical modeling of Chinese documents . Thus , we propose a topical word-character model ( TWC ), which allows two distinct types of topics : word topic and character topic . We evaluated
TWC both qualitatively and quantitatively to show that it is a powerful and a promising topic model.
1 Introduction
Topic models ( Blei et al , 2003; Griffiths & Steyvers 2004, 2007) are a class of statistical models in which documents are expressed as mixtures of topics , where a topic is a probability distribution over words . A topic model is a generative model for documents : it specifies a probabilistic procedure for generating documents . To make a new document , we choose a distribution over topics . Then , for each word in this document , we randomly select a topic from the distribution , and draw a word from the topic . Once we have a topic model , we can invert the generating process , inferring the set of topics that was responsible for generating a collection of docu -? 2008. Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unported license ( http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved.
ments.
Although most topic models treat a document as a bag-of-words , the assumption has obvious shortcomings . Suppose there are many documents about art and musicals in New York . Then , we find a topic represented by words such as ? art ?, ? musical ?, and ? New ?, instead of getting ? New York ?. The bag-of-words assumption makes the topic model split a collocation?a phrase with meaning beyond the individual words?into individual words that have a different meaning . One example of a collocation is the phrase ? white house ?. In politics , it carries a special meaning beyond a house that is white , whereas ? yellow house ? does not.
While it is reasonable for English , the equivalent bag-of-characters assumption is especially troublesome for modeling Chinese documents , where almost all basic vocabularies are the equivalents of English collocations . In Chinese , some of the most commonly used couple-of-thousand characters are combined to make up a word , and no word boundary is given in the text.
Effectively , Chinese words are like collocations in English . The difficulty is that there are overwhelmingly more of them , enough to render a bag-of-character assumption unreasonable for Chinese . Therefore , a topical model for Chinese should be capable of detecting the boundary between two words , as well as assigning a topic to each word.
While topic models for Chinese documents bear some similarity to collocation models in English , existing topical collocation discovery models , such as the LDA ( latent Dirichelt allocation ) Collocation model ( LDACOL ) ( Griffiths et al ., 2007) and the topical Ngram model ( TNG ) ( Wang et al , 2007), do not directly assign a topic to a collocation . These models find the boundaries of phrases and assign a topic to each word.
The problem is in the next step?the topic of the collocation is exactly the same as one of the words . This is like saying that the topic of ? white ? house ?. We propose a new topical model , the topical word-character model ( TWC ), which aims to overcome these limitations.
We evaluated the model both quantitatively and qualitatively . For the quantitative analysis , we compared the performance of TWC and TNG using a standard measure perplexity . For the qualitative analysis , we evaluated TWC?s ability to discover Chinese words and assign topics in comparison with TNG.
The rest of the paper is organized as follows.
Section 2 reviews topic models that aim to include collocations explicitly in the model and analyzes their limitations . Section 3 presents our new model TWC . Section 4 gives details of our consideration on inference for TWC . Section 5 presents our qualitative and quantitative experiments . Section 6 concludes with a summary and briefly mentions future work.
2 Topic Models for Collocation Discovery Since Chinese word discovery is similar to English collocation discovery , we first review some related topic models for collocation discovery.
Although collocation discovery has long been studied , most methods are based on frequency or variance . LDACOL is an attempt to model collocations in a topical scheme . Starting from the LDA topic model , LDACOL introduces special random variables xG . Variable xi = 1 implies that the corresponding word wi and previous word wi1 belong to the same phrase , while xi = 0 implies otherwise . Thus , LDACOL can decide the length of a phrase dynamically.
TNG is a powerful generalization of LDACOL.
Its graphical model is shown in Figure 1.

Figure 1: Topical ngram model.
The model is defined in terms of three sets of variables : a sequence of words w
G , a sequence of topics z
G , and a sequence of indicators x
G . TNG assumes the following generative process for documents.
1. For each document d , draw ? d ~
Dirichlet(?).
2. For each topic z , draw ? z ~ Dirichlet(?).
3. For each topic z and each word w , draw ? zw ~ Dirichlet(?).
4. For each topic z and each word w , draw ? zw ~ Beta(?).
5. For each word wd,,i in document d : ( a ) draw xd,,i ~ Bernoulli ( d , i 1 d , i 1z , w ? ? ? ), ( b ) draw zd,,i ~ Discrete(?d ), ( c ) draw wd,,i ~ Discrete ( d , iz ? ) if xd,,i=0, draw wd,,i ~ Discrete ( d , i d , i 1z , w ? ? ) if xd,,i=1, where ?, ?, ? are Dirichlet priors and ? is a Beta prior , zd,i denotes the ith topic assignment in document d , wd,i denotes the ith word in document d , and xd,i denotes the indicator between wd,i-1 and wd,i . Note that the variable xd,i = 1 implies that word wd,i-1 and its neighbor wd,i belong to the same phrase , while xd,i = 0 implies otherwise.
However , the topics assigned to them ( zd,i-1 and zd,i ) are not required to be identical to each other.
To decide the topic of a phrase , we can simply take the first ( or last ) word?s topic or the most common topic in the phrase . The authors of TNG prefer to choose the last word?s topic as the phrase topic because the last noun in a collocation is usually the ? head noun ? in English.
However , this simple strategy may be ineffective when we apply TNG to Chinese documents.
The topics of ???? ( game ) and ????? ( tournament ) should be represented by their last characters while those of ???? ( farmer ) and ???? ( agriculture ) should be represented by their first characters . And occasionally , the topic of a Chinese word is not identical to any topic of its component characters . For example ???? ( Bluetooth ) is neither a color nor a tooth.
To overcome the limitation of TNG , we must discard its underlying assumption : that the topic of a whole word is the same as the topic of at least one of its components.
3 Modeling Word Topic and Character
Topic
This section describes our topical word-character model ( TWC ), which models two distinct types of topics : word topic and character topic.
? ?? ? ? ? ?? ??? ??? ??? ??? ??? ??? zi+1 zi+2 zi zi1 xi+2 xi+3xi-1xi xi1 wi+1 wi+2 wi wi1 To solve the problem associated with the ?? ?? ( Bluetooth ) example , we need to distinguish between the topics of characters and words.
Therefore , we introduce a new type of topic for words in addition to the topics assigned to characters . When generating a Chinese character , we first draw a word topic and then choose a character topic . A schematic description of this model is shown in Figure 2.

Figure 2: Schematic description of modeling Chinese documents with character and word topics.
Here , we use random variables z
G and t
G to denote word and character topics , respectively.
Note that the word topic and character topics have a hierarchical treelike structure ( upper layer in Figure 2), whereas character topics and characters form a hidden Markov model ( HMM ) ( lower layer in Figure 2).
3.2 Topical word-character model ( TWC)
There are some indicators in the upper-right corner of each character topic in Figure 2. They help us to tell whether the current character belongs to the same word as the previous one . Now the question left is how to probabilistically draw these indicators , i.e ., how to determine the length of the Markov chain.
There are two ways to set the values of the indicators . One is similar to that applied in the hidden semi-Markov model ( HSMM ), which generates the duration of a segment from the state . Accordingly , we could first choose the length of a word from the distribution associated with the word topic and then assign 0 or 1 to each indicator . The other method is to directly draw indicators from the distribution associated with the previous character and topic , just as LDACOL and TNG do . The difference between these two methods is that the former determines the length of a word in advance while the latter increases the length dynamically.
We prefer the second choice because it takes into consideration a lot of context information . In fact , our experimental results indicate that it has better performance.
The formal definition of our model with word and character topics is as follows.

Figure 3: Topical word-character model.
TWC has four sets of variables : a sequence of characters c
G , a sequence of character topics t
G , a sequence of word topics z
G , and a sequence of indicators x
G . A document is generated via the following procedure.
1. For each document d , draw ? d ~
Dirichlet (?); 2. For each word topic z , draw ? z ~
Dirichlet (?); 3. For each word topic z and each character topic t , draw ? zt ~ Dirichlet (?); 4. For each word topic z , each character topic t and each character c , draw ? ztc ~ Beta (?); 5. For each character topic t , draw ? t ~
Dirichlet (?); 6. For each character cd,,i in document d : ( a ) draw xd,,i ~ Bernoulli ( ? ? ? d , i 1 d , i 1 d , i 1z , t , c ? ); ( b ) draw zd,,i ~ Discrete (? d ) if xd,,i=0; zd,,i = zd,,i-1 if xd,,i=1; ( c ) draw td,,i ~ Discrete ( d , iz ? ) if xd,,i=0; draw td,,i ~ Discrete ( ? d , i d , i 1z , t ? ) if xd,,i=1; ( d ) draw cd,,i ~ Discrete ( d , it ? ).
Here , ?, ?, ?, ? are Dirichlet priors and ? is a Beta prior , zd,i denotes the ith word topic assignment in document d , td,i denotes the ith character topic assignment in document d , cd,i denotes the ith character in document d , and xd,i denotes the indicator between cd,i-1 and cd,i.
Note that compared with the schematic model in Figure 2, each character has its corresponding
Character topic
Word topic ? z2 z1 t13 t21 t12 t11 c13 c21 c12 c11 t22 c22 1 1 0 1 0 ? ?? ? ?? ?? ? ? ??? ??? ??? ??? ??? ??? ??? ??? zi+1 zi+2 zizi-1 ti+1 ti+2 titi-1 ci+1 ci+2 cici-1 xi+2 xi+3 xi-1xixi-1 we cannot decide how many words there will be in a document and how many characters there will be in a certain word in advance . In other words , the structure of the ideal model is not fixed . Therefore , we duplicate word topic variables for each character.
4 Inference with TWC
Many approximate inference techniques such as variational methods , expectation propagation , and Gibbs sampling can be applied to graphical models . We use Gibbs sampling to perform our
Bayesian inference in TWC.
Gibbs sampling is a simple and widely applicable Markov chain Monte Carlo ( MCMC ) algorithm . In a traditional procedure , variables are sequentially sampled from their distributions conditioned on all other variables in the model.
An extension of the basic approach is to choose blocks of variables first and then sample jointly from the variables in each block in turn , conditioned on the remaining variables ; this is called blocking Gibbs sampling.
When sampling for TWC , we separate variables into three types of blocks in the following manner ( as shown in Figure 4).
1. character variables ti 2. indicators xi , whose value is 1 after n iterations 3. word topics zi , zi+1, ?, zi+l-1 and indicator xi , satisfying xi=xi+l=1 and xj=0 ( j from i to i+l-1) after n iterations Figure 4: Illustration of partition in a certain iteration.
Note that variables , , ,? ? ? ?
G G GG and ?
G are not sampled . This is because we can integrate them out according to their conjugate priors . We only need to sample variables z
G , x
G , and t
G .
Before discussing the inference of conditional probabilities , let us analyze our partition strategy in detail . We will explain the reasons for (1) sampling zi , zi+1, ?, zi+l-1 together and (2) sampling z
G and xi together 1. Why do we sample zi , zi+1, ?, zi+l-1 together ? Assume that we draw zi , zi+1, ?, zi+l-1 one by one , and it is now time to sample zi+1 according to the conditional probability ( )( | , , , , , , , , ) i 1 i 1P z j z x t c ? ? ? ? ?+ ? += GG GG , where ( ) i 1z ? +
G denotes a word topic except i 1z + .
Recall step 6b in the generative TWC model : it says ? If xd,,i=1, then zd,,i = zd,,i-1?, which implies ( | , ) ( ) i 1 i i 1 i 1 iP z z x 1 I z z + + += = = .
As this probability is a factorial of the target probability , it follows that ( )( | , , , , , , , , ) i 1 i 1P z j z x t c ? ? ? ? ? 0+ ? += = GG GG for all ij z ? . In other words , zi+1 should be equal to zi and not change during sampling.
It seems that step 6b in the generative model causes the problem . But supposing that we do not set zi+1 to zi ; it is still more reasonable to sample z
G together . According to our partition principle , xi , xi+1, ?, xi+l-1, xi+l is a continuous indicator sequence whose head and tail are both 0 and the rest are 1, which implies that character string ci , ci+1, ?, ci+l-1 forms a word and has the same word topic . Recall the schematic model in Figure 2: the word topic and character topics have a treelike structure and each word has only one word topic node . We add some auxiliary duplicates just because the ideal model is not fixed.
Therefore , it is natural to sample the word topic together with its duplicates.
2. Why do we sample z
G and xi together?
Let us consider the probability of converting xi from 0 to 1 in the current sampling iteration . Assume that the number of word topics is 3, zi-1=2, and ( ... | ) / ( ) , ( | ... ) / ( ) , i i l 1 i i i i l 1
P z z j x 0 1 3 1 j 3
P x k z z 2 1 2 0 k 1 + ? + ? = = = = = ? ? = = = = = ? ? where other variables and priors are omitted . If we first sample z
G and next sample xi , then the probability of drawing 1 for xi is 1/6, according to the multiplication principle . If we sample z
G and xi together , the probability of drawing 1 for xi is ( ... , ) i i l 1 iP z z 2 x 1+ ?= = = = .
Since ( ... , ) ( ... , ) ( ... , ) ( ) 1 3 i i l 1 i k 0 j 1 j 1 i i l 1 i i 1 1 P z z j x k
P z z j x 0 z z 2 x 1 z 2 + ? = = + ? = + ? ? = == = = = = = = = = = = + = = = = = ?? ? and ( ... , ) i i l 1 iP z z 2 x 1+ ?= = = = 1 0 1 1 0 ??? ??? ??? ti ti+2 ti1 ti2 ??? zi zi+1 zi1 zi2 zi+2 ti+2 ??? ??? ( ... , ) ( ) i i l 1 i i i l 1 i
P z z 2 x 0
P z z j x 0 1 j 3 + ? + ? = = = = = = = = = = ? ? , we get ( ... , ) / / i i l 1 iP z z 2 x 1 1 4 1 6+ ?= = = = = > .
In conclusion , the model is more likely to form long words , if we sample z
G and xi together . This is preferred because both TNG and TWC tend to produce shorter words than we would like.
For each type of block , we need to work out the corresponding conditional probability.
, , , , , , , , , ( : ) , ( | , , , , , , , , ) ( | , , , , , , , , ) ( , | , , , , , , , , ) d i d i d i d i d i d i 1 d i l 1 d i d i i l 1 d i
P t s z x t c ? ? ? ? ?
P x k z x t c ? ? ? ? ?
P z z z j x k z x t c ? ? ? ? ? P ? ? + + ? ? + ? ? = = = = = = =
GG GG
GG GG "
GG GG where , d it ?
G denotes the character topic assignments except td,i , , d ix ?
G denotes the indicators except xd,i , and , ( : ) d i i l 1z ? + ?
G denotes the word topic assignments except , d jz
G ( j from i to i+l-1). Details of the derivation of these conditional probabilities are provided in Appendix A.1.
5 Experiments
In this section , we discuss our evaluation of TWC in Chinese document modeling and Chinese word and topic discovery.
5.1 Modeling documents
To evaluate the generalization performance of our model , we trained both TWC and TNG on a Chinese corpus and computed the perplexity of the heldout test set . Perplexity , which indicates the uncertainty in predicting a single character , is a standard measure of performance for statistical models of natural language . A lower perplexity score indicates better generalization performance.
Formally , the perplexities for TWC and TNG are defined as follows.
( ) ? ? ? ?? log ( | , , , , ) exp { }
TWC test
D d d 1
D d d 1 perplexity D p c ? ? ? ? ?
N = = = ? ? ?
GG G G GG , where Dtest is the testing data , D is the number of documents in Dtest , Nd is the number of characters in document d , , d ? z ? is simply set to 1/Z ( Z is number of word topics ), and ? ? ??, , ,? ? ? ?
G G GG are posterior estimates provided by applying TWC to training data . Details of the parameter estimation for TWC are provided in Appendix A.2.
( ) TNG testperplexity D ? ? ?? log ( | , , , ) exp { }
D d d 1
D d d 1 p c ? ? ? ?
N = = = ? ? ?
GG G GG , where Dtest , D , and Nd are the same as defined for the TWC perplexity , , d ? z ? is simply set to 1/T ( T is number of topics ), and ? ??, ,? ? ?
G GG are posterior estimates provided by applying TNG to training data.
Now , the remaining question is how to work out the likelihood function in the definition of perplexity . The likelihood function can be obtained by marginalizing latent variables , but the time complexity is exponential . Therefore , we propose an efficient method of computing the likelihood that is similar to the forward algorithm for an HMM . Details of the forward approach to computing likelihood for TWC and
TNG are provided in Appendix B.
In our experiments , we used a subset of Chinese corpus LDC2005T14. The dataset contains 6000 documents with 4476 unique characters and 2,454,616 characters . We evaluated both TWC and TNG using 10-fold cross validation . In each experiment , both models ran for 500 iterations on 90% of the data and computed the complexity for the remaining 10% of the data.
TWC used fixed Dirichlet ( Beta ) priors ?=1, ?=1, ?=1, ?=0.1 and ?=0.01 while TNG used ?=1, ?=0.01, ?=0.01, and ?=0.1.
0 No . of topics ( TNG)
No . of charcter topics * no . of word topics
Pe rp le xi ty
TNG TWC
Figure 5: Perplexity results with LDC2005T14 corpora for TNG and TWC.
The results of these computations are shown in Figure 5. Note that the abscissa for TWC is the number of word topics ( Z ) multiplied by the ( TWC ) scissa for TNG is the number of topics ( T ). They both represent the number of partitions into which the model classifies characters.
Chance performance results in a perplexity equal to the number of unique characters , which was 4476 in our experiments . Therefore , both TWC and TNG are competitive models . And the lower curve shows that TWC is much better than
TNG.
We also found that both perplexity curves increased with the number of partitions . In other words , both models suffer from overfitting issues.
This is because the documents in a test set are very likely to contain words that do not appear in any of the documents in the training set . Such words will have a very low probability , which is inversely proportional to the number of partitions.
Therefore , the perplexity of TWC increased from 7.3513 ( Z*T=2*2) to 8.9953 ( Z*T=10*10), while that of TNG increased from 20.3789 ( T=5) to 193.6065 ( T=100).
5.2 Chinese word and topic discovery
As shown in the previous subsection , TWC is a competitive method for topically modeling Chinese documents . Next , we show its ability to extract Chinese words and topics in comparison with TNG.
In our qualitative experiments , the task of Chinese word and topic discovery was addressed as a supervised learning problem , where a set of words with their topical assignments was given as a seed set . Each seed can be viewed as some constraints imposed on the TWC and TNG models . For example , suppose that ???? ( teacher ) together with its assignment ? education ? is a seed . This assumption implies that the indicator between characters ??? and ??? is 1 and that the ( word ) topic for each character is ? education?.
We make use of such constraints in a simple but effective way . In each sampling iteration , we first sample all variables as usual and then reset observed variables according to the constraints.
We used 8000 Chinese documents in the Chinese Gigaword corpus ( LDC2005T14) provided by the Linguistic Data Consortium for our experiments . The dataset contains 4651 unique characters and 3,295,810 characters.
The number of word topics in TWC , the number of character topics in TWC , and the number of topics in TNG were all set to 15. Furthermore , 16 seeds scattered in 4 distinct topics were given , as listed in Table 1 column ? seed ?. Dirichlet ( Beta ) priors were set to the same values as described in the previous subsection.
Word and topic assignments were extracted after running 300 Gibbs sampling iterations on the training corpus together with the seed set . For the TNG model , we took the first character?s topic as the topic of the word . We omitted one-character words and ranked the extracted words using the following formula ( ) ( ) i i 1 occ W occ W = ? , where occi(W ) represents how many words were assigned to ( word ) topic i . The top50 extracted words are presented in Table 1.
We find found that both TWC and TNG could assemble many common words used in corresponding topics . And the TWC model had advantages over the TNG model in the following three respects.
First , TNG drew more words related to the seeds . In Table 1, highly related words are marked in pink ( underline ) and partly related words are marked in blue ( italic ). It is clear that the TWC column is more colorful than the TNG column.
Secondly , we found that many words extracted by TNG had the same prefix . For example , consider the topic ? agriculture ?: there are 14 words marked with superscript 1 in Table 1. They all have the prefix ???. This is because we took the first character?s topic as the topic of the word.
Although this strategy is beneficial in some cases , such as for words with prefix ???, it is detrimental in other cases . For example , ??? ? ( sugar cane ) and ???? ( Gansu ) have the same prefix and topic assignment , but the latter is a name of a province in China and is not related to agriculture . Similarly , even though the character string ???? does not form a Chinese word , this string ???? and ???? ( Iran ) are classified in the same cluster . Compared with TNG , TWC can also extract words whose topics are identical to the topic of any character . For example , the topics ????? ( freestyle swimming ), ????? ( medley swimming ), and ??? ? ( butterfly stroke ) depend on their suffixes.
Thirdly , although TNG stands for ? topical ngram model ?, it infrequently draws words containing more than two characters . On the other hand , the TWC model extracts many n-character words , such as ???????? ( president of
United States , George Bush ), ??????? ( individual medley ), and ?????? ( four per-??(football ) ??( player ) ??( match ) ?? ( championship ) ??,??,??,??,??,??,??,??,? ?,??,??,??,??,??,??,??,??, ??,??,??,??,??,??,??,??,? ?,??,??,??,??,??,??,??,??, ??,??,??,??,??,??,??,??,? ?,??,??,??,??,??,??,?? ??,?? 2,??,???,???,??,??,??,? ?,??,??,??,??,??,??,???,??,? ?,??,???,??,??,??,?? 2,??,???, ??,???,??? 2,?????,??,??,??,? ?,??,??,??,??,??,???,???,??,? ?,??,??,??,??,??,???,?? ?? ( foodstuff ) ?? ( country ) ?? ( farmer ) ?? ( paddies ) ??,?? 1,??,?? 1,??,?? 1,?? 1,?? 1,??,?? 1,?? 1,??,?? 1,?? 1,??,? ?,?? 1,??,?? 1,??,?? 1,?? 1,??,? ? 1,??,??,??,??,??,??,??,??, ??,??,??,??,??,??,??,??,? ?,??,??,??,??,??,??,??,??, ?? ??,??,??,??,??,??,??,??,??,? ?,??,???,??,??,??,??,??,??,?? ?,??,?? 2,??,??? 2,??,????,?? 2, ??,??,??,??,??? 2,??,??,??,??, ??,??,??? 2,??,??,??,?? 2,?? 2,? ?,????,??,??,??,??,?? ?? ( school ) ?? ( teacher ) ?? ( student ) ?? ( education ) ??,??,??,??,??,??,??,??,? ?,??,??,??,???,??,??,??,? ?,??,??,??,??,??,??,??,??, ??,??,??,??,??,??,??,??,? ?,??,??,??,??,??,??,??,??, ??,??,??,??,??,??,??,?? ??,??,??,??,??,??,??,??,??,? ?,??,??,??,??,??,??,??,??,??, ??,????,??,???,??,??,??,??,? ?,??,??,???,??,??,??,??,??,? ?,??,??,??,??,??,??,??,??,??, ??,??,??,?? ?? ( war ) ?? ( solider ) ?? ( general ) ?? ( weapon ) ??,??,??,??,??,??,??,??,? ?,??,??,??,??,??,??,??,??, ??,??,??,??,??,??,??,??,? ?,??,??,??,??,??,??,??,??, ??,??,??,??,??,??,??,??,? ?,??,??,???,??,??,??,??? ??,??,??,??,???,??,??,??,??,? ?????,??,????,????? 2,??? 2,? ??,????,??,??,????,??,????,? ?,??,???,??,??,??,??,????,??, ???,??? 2,??,??,??,??,??,??,? ?,??,??,??,??,??,??,??,??,??, ??,?? Table 1: Top50 words extracted by TWC and TNG.
cent ). This is partly due to our sampling strategy , discussed in subsection 4.1, which increases the probability of forming long words.
We also found that some extracted character strings were very close to real Chinese words.
For instance , ???? is a substring of ????? ( tournament ); ????? is a suffix of ???? ?? ( Chinese player ), ?????? ( American player ), and ?????? ( French player ); and ????? is a substring of ?????? (100 thousand kilograms ) and ?????? (50 thousand kilograms ). ( Such substrings are marked with superscript 2 in Table 1.) We believe that this result occurred because the training corpus was not large enough and that TWC will achieve better performance with a large dataset.
6 Conclusion and Future Work
In this paper , we presented a topical word-character ( TWC ) model , which models two distinct types of topics : word topic and character topic . The experimental results show that TWC is a powerful approach to modeling Chinese documents according to the standard evaluation measure of perplexity . We also demonstrated TWC?s ability to detect words and assign topics.
Since TWC is a straightforward improvement that removes the limitations of existing topical collocation models , we expect that its application to English collocation will also result in higher performance.
Appendix A.1 Gibbs Sampling Derivation for TWC Symbols used here are defined as follows.
C is the number of unique characters , T is the number of character topics , and Z is the number of word topics.
Nd denotes the number of characters in a document d.
( ) I ? is an indicator function , taking the value 1 when its argument is true , and 0 otherwise.
qd,z,0 represents how words are assigned to topic z in document d ; pz,t,c,k represents how many times an indicator is k given the previous character c , the previous character topic t , and the previous word topic z ; nz,t,0 represents how many times a character topic is t given a word topic z and the corresponding indicator 0; mz,v,t,1 represents how many times a character topic is t given a word topic z , the previous character topic v , and the corresponding indicator 1; and rt,c represents how many times character c is assigned to character topic t.
' ' ', ' ', ' ', ' , , ' ', ' ', ' ', ' ' ' ' ' ', ', ' ', ' , , ' ' ' ' ' ' ( | , , , , , , , , ) ( | ) ( | , , ) ( | ) ( | ) ( d d d i 1 d i 1 d i 1 d i d i
ND D d d i d i d i 1 d d 1 d 1 i 1
NZ T C D z t c d i z t c z 1 t 1 c 1 d 1 i 1 z
P t s z x t c ? ? ? ? ?
P ? ? P z x z ? d?
P ? ? P x ? d?
P ? ? ? ? ? ? = = = = = = = = = ? ? ? ? ? ? ? ? ??? ??? ???
GG GG
G G G
G G G
G ' ', '', ' ', ' ', ' ' ' ' ' ' | ) ( | ) ( | , , d d i
NZ Z T D z t d i d i z z 1 z 1 t 1 d 1 i 1 ? P ? ? P t x ? = = = = = ? ?? ?? ???? GG ', ', ', ' ', ', ' ', ', ' , , ,, , , ' ', ' ' ' ' ' ' ' ' ' ' ' ) ( | ) ( | ) ?( ) ( ) ( ) ?( ) ?( ) ?( ) d d i d i 1 d i z t c x z t c z t c d i 1 z s cd i d i
NT D z t t d i t t 1 d 1 i 1
Z T C 2 2 px ? 1 x x
C t ? d ? d ? P ? ? P c ? d ? 2? ? ? ?
C ? ? d ? ? ? + = = = ? = = = = = ? ? ? ? ? ? ? ? ? ? ? ? ??? ??? ? ??
G G G GG G
G ', ' , ' ' ', ', ,', ', ', , , ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ', ' ', ' ' ' , ( ) ( ) ?( ) ?( ) ( ) ( ) ?( ) ?( ) ( ) ( ) t c d i t t s z t 0 d iz t v 1 d i d i 1
T C C r cc ? 1 c 1 c 1 c 1
Z T T Z T nt ? 1 t z zT T z 1 t 1 t 1 z 1 t 1 sT T zmv ? 1 v z t z t v 1 v 1 z t ? ? ? d?
T ? T ? ? ? ? ? ? ? ? ? ? ? = = = ? = = = = = ? = = ? ? ? ? ? ? ? ? ? ? ? ?? ? ? ? ???? ? ?
G , ,, , , , , ,, , , , , , , , , ,*,, , , , , , ,, , ,* ,* , , ,*, d i d id i d i d i d i d i d i 1d i d i d i d i 1 d i s d i z s 0 d i z 0z s c x s c z t s 1z s c s d i z t 1 x 0 d ? d ? x 1 n ? x 0 n T?p ? r ? m ? p 2? r C ? x 1 m T ? ? ? ? =? ? ?? =?? +? =? ++ + ?? ? ? ? ++ + ? =? +?
G G
Similarly , , , , , , , , , , , , , , , , , , , , , , , ,*, , , ,* , , , , ,*, , , , , ,*, ( | , , , , , , , , ) ( ) d i d i 1 d i 1 d i 1 d i 1 d i 1 d i 1 d i d i d i d i d i 1 d i d i d i 1 d i d i d z 0 z t c k d 0 z t c d i d i 1 z t 0 z 0 z t t 1 z t
P x k z x t c ? ? ? ? ? q ? p ? k 0 q Z ? p 2?
I z z k 1 n ? k 0 n T ? m ? m ? ? ? ? ? ? ? ? ? ? = +? +=? +? ? ?? +? = =? + +? =+
GG GG
T ? ????? =? +? , , , , , , , , , , , , , , ( : ) , , , , , , ,*, , , ,* , , , , ( , | , , , , , , , , ) ( ) d i 1 d i 1 d i 1 d i 1 d i 1 d i 1 d i u 2 d i u 2 d i u 1 d i d i 1 d i l 1 d i d i i l 1 d i d j 0 z t c k d 0 z t c d i 1 j t c x j
P z z z j x k z x t c ? ? ? ? ? q ? k 0 p ? q Z ? p 2?
I z j k
P p ? ? ? ? ? ? + ? + ? + ? + + ? ? + ? ? ? = = = = = +? = +? +? ? ?? ? +? = =? +
GGG"
G , , , , , , , , , , , , , , ,* , ,* , , ,*, , , , , ,*, ( ) d i u 2 d i u 1 d i u 2 d i u 2 d i u 2 d i d i 1 d i d i 1 l 1 l j t t 1 u 2 u 2t c j t 1 j t 0 j 0 j t t 1 j t 1 m ? 2? m T ? n ? k 0 n T ? m ? k 1 m T ? + ? + ? + ? + ? + ? ? ? + = = ? +? ?+ + +? =? +?? +? =? +? ? ?
Appendix A.2 Parameter estimation for
TWC
After each Gibbs sampling iteration , we obtain posterior estimates ? ? ??, , ,? ? ? ?
G G GG and r by , , , , , , , , , ,*, , , ,* , , , , , , , , , ,*, ,*, , , ,* ? ? ?? ? d z 0 z t c k d z z t c k d 0 z t c z v t 1 z t 0 z v t z t z v 1 z 0 t c t c t q ? p ? ? ? q Z ? p 2? m ? n ? ? ? m T ? n T ? r ? ? r C ? + += =+ + + += =+ + += + , where the symbols are the same as those defined in Appendix A.1. These values correspond to the predictive distribution over new word topics , new indicators , new character topics , and new characters.
Appendix B . Likelihood Function Derivation for TWC and TNG To compute the likelihood function for TWC , a qua-ternion function gi is defined as follows : ( formula has a broken character ) , , , , , , , ( , , , ) ( , ,..., , , , ? ? ? ??, | , , , , ) i d 1 d 2 d i d i d i d i 1 d i g r s u v P c c c z r x s t u t v ? ? ? ? ??===== = = = ====== 
G G G GG .
Then , it is clear that ? ? ? ??( | , , , , ) ( , , , ) d
Z 1 T T d N r 1 s 0 u 1 v 1
P c ? ? ? ? ? g r s u v = = = = =???? GG G G GG , where Z is the number of word topics and T is the number of character topics . The function gi can be rewritten in a recursive manner.
, , ,, , , ( , , , ) ?( , , , ) ? ?( , , , ) ( , , , ) ? ? ?( ) d 1 d i 1 d i 1 d r v
Z 1 T cs i 1 i j u c v j 1 k 0 l 1 vr rd v r u g r 1 u v 0
T g r s u v g j k l u ? ? s 0?? s 1?I r j + + = = = = = ? ? ? = ? ? ? =?? ?? ==?? =========== ??? Similarly we can define function hi to help compute the likelihood for TNG . ( formula has a broken character ) , , , , , , , , , ? ? ??( , ) ( ,..., , , | , , , ) ? ? ??( | , , , ) ( , ) ( , ) ? ?( , ) ???( , ) ( , ) ? d d 1 d i 1 d i d i 1 d i i d 1 d i i i
Z 1 d N r 1 s 0 1 d r c
Z 1 rs r i 1 i j c d c j 1 k 0 r c h r s P c c z r x s ? ? ? ?
P c ? ? ? ? h r s h r 1 0 h r 0 ? ? ? s 0 h r s h j k ? ? s 1? + + = = + = = = = = = = ? ? =?= ? ? ? ? =?? ?? ??
G G GG
GG G GG
References
Blei , D . M ., Ng , A . Y ., and Jordan , M . J . 2003. Latent Dirichlet alocation . Journal of Machine Learning
Research , 3:993?1022.
Griffiths , T . L . and Steyvers , M . 2004. Finding Scientific Topics . Proceedings of the National Academy of Sciences , 101 ( suppl . 1), 5228-5235.
Steyvers , M . and Griffiths , T . L . 2007. Probabilistic topic models . Latent Semantic Analysis : A Road to
Meaning . Laurence Erlbaum.
Griffiths , T . L ., Steyvers , M ., and Tenenbaum , J . B . T.
2007. Topics in Semantic Representation Psychological Review , 114(2), 211-244.
Wang , X ., McCallum , A ., and Wei , X . 2007. Topical Ngrams : Phrase and Topic Discovery , with an Application to Information Retrieval . Proceedings of the 7th IEEE International Conference on Data
Mining ( ICDM 2007).
352
