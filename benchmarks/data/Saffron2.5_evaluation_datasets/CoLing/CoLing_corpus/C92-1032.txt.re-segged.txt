LEFT-CORNER PARSING ANDP SYCHOLOGICAL PLAUSIBILITY
Philipl ~ esnik
Department of Col Imuter and information Sc ience 
University of Pennsylvania , Philadelphia , PA 19104 , USA

Abstract
It is wellknown that even extremely limited center-embedding causes people to have difficulty ill comprehension  , but that left - and right-branching constractions produce no such effect  . If the difficulty in comprehension is taken to be a result of processing load  , as is widely assumed , then measuring the processing load induced by a parsing strategy on these constructions may help determine its plausibility as a psychological model  . On this basis , it has been ~ rgued\[AJ 91 ,   JL83\] that by identifying processing load with space utilization  , we can rule out both topdown and bottom-u parsing as viable candidates for the human sentence processing mechanism  , att d that left-corner parsing represents a plausible Mternative  . 
Examining their arguments in detail , we find difficulties with each presentation . In this paper we revise the argument and validate its central claim  . In so doing , we discover that the key distinction between the parsing methods in ot the form of prediction  ( top down vs . bottom-up vs . left-corner ) , but rather the ability to i astantiate the operation of composition  . 
1 Introduction
One of our most robust observations about language --dating back at least to the seminal work of Miller and Chomsky\[  MC63\] -- is that right - and left-branching constructions such as  ( la ) and ( lb ) seem to cause no particular difficulty in processing  , but that multiply center-embedded constructions such as  ( lc ) are difficult to understand . 
a . \[\[\[ John's\]brother's\]eat\]despises rats  . 
b . This is\[the dog that chased\[the cat that bit\[therat that a tet be cheese \]\]\]  . 
c . #\[ The rat that \[ the cat that lilled og\] chased \] bit\]ate the cheese  . 
The standard explanation for this distinction is a tight bound on space in the human sentence processing mechanism : center-embedded constructions require that the head noun phrase of each subject be stored until the processing of the embedded clause is complete and the corresponding verb is finally encountered  ) Alternative accounts have been proposed , most sharing the premise that the parser's capacity for recursion is limited by bounds on storage  . ( See , for exmn-pie , \[ Kim 73\] and \[ MI 64\] ; for opposing views and other pointers to the literature see\[  DJK+82\]  . ) The distinction between center-embedding and left/right-branching has important implications for those who wish to construct psychologically plausible models of parsing  . Johnson-Laird\[ JL83\] observes that neither the topdown nor the bottom -up methods of constructing a parse treefit the facts of  ( 1 )  , arid proposes instead the lesS-well-known alternative of left-corner parsing  . Abneymid Johnson\[AJgl\]discuss a somewhat more general version of Johnson-Laird's argument  , introducing the abstract notion of a parsings f ~ ntegy in order to characterize what is meant by bottom-up  , topdown , and left-corner parsing . 
In this paper , we examine the argument as presented by Abney and Johnson and by Johnson-Laird  , and point out a central problem with each variation  . 
We then present he argument in a form that remedies those difficulties  , and , in so doing , we identify a previously underrated aspect of the discussion that turns out to be of central importance  . In particular , we show that the psychological plausibility argument hinges on the operation of composition and not left-corner prediction per se  . 
2 Comparing Strategies 2 . 1 Summary o f the Argument For expository purposes  , we begin with tile discussion in \[ AJ91\] . Abney and Johnsonse same , as we shall , that the hunmn sentence processing mechanism con -struets a parse tree  , consisting of labelled nodes and arcs , incrementally over the course of interpreting an utterance  , thoughtile global parse tree need never " exist ill its entire tyat any point  . " They define a parsing IT his oh ~ rvatlon is by t to rne~n~lant tnaage specific  , though in SOV langttages it is embedding on objects  , not subject l , that causes ditllct dty . 
ACRESDE COLING-92 , NANTES , 2328 Aour 1992 191 PROC . oF COLING-92, NANTES , AUG .  2328, 1992

Figure 1: A parse tree strategy to be " a way of enumerating the nodes and arcs of parse trees  . " This is , in fact , a generalization of the concept of a traversal \ [   ASU86\]  . 
A topdown strategy is one in which each node is enumerated before any of its descendants are  ; a bottom-up strategy is one in which all the descendants of a node are enumerated before it is  . So , for example , a topdown strategy would enumerate the nodes of the tree in Figure  1 in the order ABC DEFGHI , and a bottom-up strategy would enumerate them in the order CEF DB-HIGA  . In a left-corner strategy , for each node ~1 , the leftmost child of T/ise numerated beforer / , and the siblings of the leftmcet child are enumerated after r/  . The strategy takes its name from the fact that the first item on the right-hand side of a contextfree rule  ( its left corner ) is used to predict the parent node . For example , having recognized constituent C in Figure 1 , the parser predicts an invocation of rule B --4 CD and introduces node B . The complete left-corner enumeration of the tree is CBED FAHGI  . 
Thus far , we have discussed only the order of enumeration of nodes  , and not ares . Abney and Johnson define as arc-eager any strategy that enumerates the arc between two nodes as soon as both nodes are present  . An are-standard strategy is one that enumerates the connecting arc once either none or all of the subtree dominated by the child has been enumerated  . For example , the arc-eager left-corner enumeration of the tree in Figure  1 would introduce arc ( B , D ) just after node D was enumerated , while the arc-standard version of the left-corner strategy would first completely enumerate the subtree containing E  , D , and F , and then enumerate arc(B , D ) . 
In order to characterize the space requirements of a parsing strategy  , two more definitions are required . 
A node is said to be incomplet either if its parent has not yet been enumerated  ( in which case the parser must store it until it can be attached to the parent node  )  , or if some child has not yet been enumerated ( in which case the parser musts to retire node until its child can be attached  )  . The space requirement of a parsing strategy , given a grammar , is the maximum number of incomplete nodes at any point during t been umeration of any parse tree of the grammar  . 
Having established this set of definitions , the goal is to decide which parsing strategies are psychologically plausible  , given the facts about the human pars-x/ec ?/- , , , ' ? z~n/\vz left-branching center-embedded right-branching 
Figure 2: Branchings lr~e ~ are sing mechanism as exemplified by  ( 1 )  . The central claim is summarized in the following table: 
Strategy "- Spaxze required'\]
No #, ? A ~ c,L ~ / tce.t ~ Im~hLl
Top-d . . . . . ithero ( ,) O(u ) Io (1) I
Bottom-up-either 0(1) O(n)\]JO(n ) "
Left .   .   .   .   .   .   .   . t and ard 0 (1) O(n ) O(n )  Left .   .   .   .   .   .   .   .   . get , 0(1) O(a )\] Q(1) JW hat people do . . O(1) O ( . ) I o ( 1 ) I The table can be explained with reference to Figure  2  . A topdown enumeration of the left-branching tree clearly requires storage proportional to n  , tile height of the tree : at the point when Z is enumerated  , each of A , B ,   .   .   .   , X remains in emnplete because its rightmost child has not yet been encountered fl The same holds true for the center-embedded structure : using a topdown enumeration  , each of A , C , D ,   .   .   .   , X remains incomplete until the subtree it dominates has been entirely enumerated  . In contrast , the topdown strategy requires only constant space for timright-branching structure : each of A  , C .   .   .   .   , X becomes coru-plete as soon as its rightmost child is enumerated  , so the number of incomplete nodes at any time is at most two  . We conclude that if the human sentence processing strategy were topdown  , people would find increasing difficulty with both multiply left-branching and multiply center -embedded constructions  , but not with right-branching constructions . The evidence x empli-fied by ( 1 ) suggests that this is not the case . 
A similar analysis holds for the bottom-up strategy  . 
The left-branching structure requires only constant space  , since each of X ,   .   .   .   , B , A becomes complete as soon as both children have been enumerated  . In con-trust , enumerations of the right-branching and center -embedded constructions require linear space  , since every left mest child remains incomplete until the subtree dominated by its right sibling has been entirely enumerated  . The left-corner strategy with arc-standard enumeration behave similarly to the bottom-up strategy  , since every parent node remains in complete until the subtree dominated by its right sibling has been  2Abney and Johngoadi * cuss space complexity with r ? apect to the length of the input string  , not the height of the ptmm tree , but if wet~stt me the grammar in finitely ambiglt otm this distinction is of no hnportaxt ce  . 
AcI~DECOLING-92 . NANTEs , 2328 AOI\]T 1992 192 PROC . OFCOLING-92, NANTES , AUO .  2328 . 1992 entirely enumerated . If increased memory load is responsible for increased processing difficulty  , as we have been assuming , then both the bottom-up strategy and the arc -standard left-corner strategy predict that people have more difficulty with right-branching than with left-branching structures  . Our conclusion is the same as for the topdown strategy : the asymmetry of the prediction is not supported by the evidence  . 
On the other hand , arc-eager enumeration makes a critical difference to the left-corner strategy when applied to the right-branching structure  . Recall that the left-corner enumeration of nodes for this structure is BADC  . . . . Notice that after node (7 has been enumerated , arc(A , C ) is introduced immediately , and as a result , node A is no longer incomplete . In general , the arc-eager left-corner strategy will enumerate the right-branching structure with at most three nodes incomplete at any point  . ~ ktrthermore , as was the case for the bottom-up strategy , the left-branching structure requires constant space  . We see that only tile center-embedded structure requires increased storage as the depth of embedding increases  . Thus of the four strategies , the arc-eager version of the left-corner strategy is the only one that makes predictions consistent with observed behavior  . 
2.2 Two Problems
Under the assumptions made by Abney and Johnson , the discussion sketched out above does make a case for a left-corner strategy being more psychologically plait-sible than topdown or bottom-up strategies  . However , there are two difficulties with the argument as it is presented  . 
First , by abstracting away from parsing algorithms and placing the focus on parsing strategies  , Abneyanti Johnson make it difficult to fairly compare space requirements across different methods of parsing  . Without a formal characterization f the algorithms themselves  , it is not clear that their abstract notion of space utilization means the same thing in each case  . 3 ~ brexample , consider the augmented transition network ( ATN ) in Figure 3 , where the actions on tile arcs are as follows :
II : npl ~*
I2: result ~ ( S ( npl * ) ) 13: dell ~ * 14: result ~ ( NP ( dell * ) )
I5: result ~ a
I 6: result ~ the
Uppercase are labels represent PUSH operations , and lower case labels representerminal symbols . In the pseudolanguage used here for a reactions , npl , dell ,   3\] amgrateful to Stuart Shleber for this observation  . 
a(t5) tl~(16)
Figure 3: Fragment of an ATN and resull are registers , the leftward arrow ( +-- ) indicates an assigmnent statement , hepop arc transmits control ( audtile contents of the ~ es altregister ) to the calling subuetwork , and the asterisk ( * ) represents the values otransmitted ( cf . \[ WooT0\]) . So , for instance , action I4 constructs an NP dominating the structure in the ddl register on tile left  , and , on the right , tile noun structure received on retnrn froln a push totile 
N subnetwork.
Now~tile ATN is perhaps one of the mo~t common examples of a parser operating in a topdown fashion  . 
Yet according to the definitions proposed by Abney and Jolmson  , the enumeration performed by the ATN parser given above would seem to make it  , an instance of a bottom-up strategy . For example , in parsing the noun phrase them an , the ATN above wonld recognize tile determiner the  , then the nonnman , and finally it would build and return the structure \ [  , v theman \] from the NP subnetwork . The source of difficulty lies in the decoupling of the parser's hypotheses from the structures that it builds  . When the determiner the isen countered , no parse tree structures have been built , but the mechanism controlling the ATN's computation has stored the hypotheses that we were parsing an S  , that we had entered the NP subnetwork , and that we had subsequently entered the DET subnetwork  . These correspond precisely to the nodes we expect to see enumerate during the course of a topdown strategy  . 
One could , of course , choose in this case to identify the space utilization of this parser with the hypotheses rather than the structures built  . I to we ver , that leaves the status of the structures themselves in question  . More to the point , re-characterizing tile storage requirements of a particular algorithm is exactly the sort of manipulation that the abstract notion of parsing strategies should help us avoid  . 
Tile second difficulty with Abney and Johnson's discussion concerns the distinction between arc -eager and arc-standard strategies  . As they point out , for both topdown and bottom-up strategies , the two forms of AcrEsi~COL 1NG-92 , NANYES , 2328 AOflr 1992193 PROC . OFCOLING-92, NANTES , AUO . 2328,1992 arc enumeration are indistinguishable . In addition , left-corner parsing with arc-standard enumeration is  , at least for the purposes of this discussion , virtually identical to bottom-uparsing , having no distinguishable effects either with respectospace utilization or even with respect othe hypotheses that are proposed  . 4 So it seems omewhat odd to introduce a distinction between " eager " versus " standard " when it turns out to distinguish only one of six possible combinations  ( top-down/eager , top-down/standard , etc . ) . The question of exactly what " eager enumeration " does would seem to merit further attention  . We shall give it that attention shortly , in Section 4 . 
3 Comparing Automata
Abney and Johnson's argument is largely an independent account quite similar to one made earlier in \[  JL83\]  . 
Here we present a brief summary of the argument as presented there  . Johnson-Laird's presentation , though it encounters a difficulty of its own , turns out to complement Abney and Johnson's and to make clear how to solve the difficulties in both  . 
Following the standar description in the compilers literature  ( see , e . g . , \[ ASU86\]) , Johnson-Laird adopts the definition of a topdown parser as one that operates by recursive descent : it begins with the start symbol of the grammar and successively rewrite stile leftmost nonterminal until it reaches a terminal symbol or symbols that can be matched agains the input  . Parsing in this fashion , the parse tree is constructed topdown and from left to right  . A bottom-u parser builds the tree by working upward from the terminal symbols in the input string  , constructing each parent node after all its children have been recognized  . Aleft . corner parser recognizes the left-corner of a contextfree rule bottom-up  , and predicts the remaining symbols on the right -hand-side of the rule topdown  . 
Johnson-Lair dexamines the psychological pausibil -ity of parsers  , not parsing strategies , but otherwise his argument is very much the same as the discussion in the previous section  . He concludes that the symmetry of human performance on left - and right-branching structures counts agains the topdown mid bottom -upparsers  , and that the left-corner p~trser is a viable alternative that appears to be consistent with the evidence  . 
He then provides a more formal characterizatim l of the various parsers by expressing each as a pushdown automaton  ( PDA )  . Such a characterization immediately * Although topdown filtering can be added  ( see , e . g . ,\[ PS 87, p . 182 D , Schabea ( personal commt trd cation ) points out that left-corner parsing with topdown ffltethtgiSe ~ entially the same a  . sLR parsing . Top-down filtering restricts the non -determinlstic choices made by the parser  , bat does not affect the bottom-up construction of the parse tree along a single computation path  . 
remedies the first difficulty we found in \[ AJ91\]: the formal specification of each parsing algorithm permits us to express pace utilization uniformly in terms of the automaton's stack  . 
The topdown and bottom-up automata behavex-actly as we would expect  . The stack of the bottom-up automaton never grows beyond a constant size for left-branching constructions  , but is potentially unbounded for center-embedded and right-branching constructions  . The topdown automaton displays the opposite behavior  , the size of its stack size being bounded only for right-branching constructions  , sOf particular interest is Johnson-Laird's construe-tion of a PDA for left-corner parsiug  , which we consider in more detail . The stack alphabet for the left-corner PDA includes not only terminal and nonterminal symbols from thcgrau ~ nar  , but also special symbols of tile form\[XY\] , where X mad Y are nonterminals . The first symbol in such a pair represent stile topdown prediction of a node  , and tile second a node that has been encountered bottom-up  . The use of these pairs permits a straightforward combination of left-corner prediction  , which is bottom-up , and topdown prediction and matching agains the input in the style of a topdown automaton  . 
tiere we consider an extremely simple left-corner automaton  , constructed from a grammar having the following productions :  ( 1 ) S ~ NP VP ( 2 ) NP ~ John \] Mary ( 3 ) VP ~ VNP ( 4 ) v-~nke ~ The rules of tlle automaton areas follows :\[  . I'In pn'tIStac . k " I New top of stacl f . .I 1 John . . . 
2 Mary ...
3 likes 4 iynored X John 5 ignored . . . X Mary '6 ignored . . . X likes '7 ignored . . . \[XNP\]'-~ic , .   .   .   . a .   .   . \[ xv\]9' ignored: . . IX X \] .   .   . John . . . Mary . .  . likes, . . \[? . NP 1 . . . \[ xNPJ . . . Ixv3 . . . \[ xs ) vv .   .   . \[XVB\]SP The top of the stack is at right , and rules 49 are actually schemata for a set of rules in which X can be replaced by each of tile nonterminals  ( S , NP , VP , and V ) . Tile parser begins with S on top of the stack , and a string has been successfully recognized if the stack is empty and the input exhausted  . 
5 The a ~ mlys is bein~stralghtforward , we omit the details here ; for n complete discussion of the construction of PDAs for topdown and bottom-uppm~ing  , see ~ LP 81 ,  ?3 . 6\] . 
ACRESDE COLING-92 , NANTES , 2328 AO~T 1992 194 PROc . oF COLING-92, NANTES , AUO . 2328, 1992/s~i
Iivel
V(NP i
I\~/likes " J r . . . . . .
Figure 4: Distinguishing the topdown view of a node b'om the bottom-up viewRules  13 simply introduce texical items onto the stack as they are scanned  . Rules 4--6 represent bottom-up reductions according to the lexical productions of the grammar  ( productions ( 2 ) and ( 4 ) ) ; for example , rule 4 states that if a constituent X has been predicted topdown  , and the word John is scanned , we continue seeking X topdown with the knowledge that we have identified an NP bottom-up  . Rules 7 and 8 implement left-corner prediction : if the left -corner node of a rule has been recognized bottom -up  , then we hypothesize the parent node in bottom-up fashion and also predict the right siblings topdown  . For example , rule 8 states that if a V has been recognized bottom-ul )   , we should hypothesize that a VP is being recognized and also predict the remainder of the VP  , namely an NP , topdown . 
Finally , rule 9 pops a symbol off the top of the stack if we have predicted a constituent X topdown and then succeeded in finding it bottom-up  . 
In examining the behavior of this automaton for the sentence John likes Mary  , a problem immediately becomes apparent . The contents of the stack at each step during the parse are as follows : !!  . . . . . =1 Joha VP VP V\] vp VP\]? , -S~ISNP\]ISS lISS lISSI ( Ij )   ( a )   ( 4 ) s ( ~ ) " r!\]!lit .   .   .   . NP IN PNP ? . \[vtr'vP\]\[VP VP\]\[Vp VP ) Issl ISslssIss ( 6 )   ( 9 ) o ( ~ ) As the sentence --- a right-branching structure - -- is recognized  , we find that the stack is accumulating symbols of the form \[ XX\]  . It is clear that as the depth of right-branching increases  , the number of stacked-up symbols of this form will also increase  , without upper bmmd . Why is this happening ? Let us distinguish between the topdown " view " of a node and the bottom-up  ( left-corner ) " view " of that node . Figure 4 makes this distinction explicit : the VP predicted topdown by the rule S--*NP VP is distinct from the VP predicted in left-corner fashion using VP--~VNP  . These are , in fact , precisely the two VPs in the symbol \[ VP VP\] . Now , enumerating the arc between VP and S in the final parse tree is equivalent to identifying these two views  ( dotted ellipses in the figure )  . As long as we have not identified the two views of VP as the same node  , the arc is not enumerated --- and the parent S remains incomplete in the sense defined by Abney and Johnson  , It is rule 9 in the automaton that effects this identification : popping\[VP VP\]amounts to recognizing that the topdown view and the bottom-up view match  . Since the operation of the automaton prevents the symbol from being popped until the botto In-up view has been completed  , it is clear that this automaton implements an arc -standard strategy rather than an arc-eager one  . Itence it is not sur-prisiug that the antomaton fails to support Johuson-Laird's argument : far from being bounded  , the stack of such automaton can grow without bound as the depth of right-brml ching increases  . 
4 Arc-eager Enumeration as
Composition 4.1 An Easy Fix ...
To summarize thus far ,  \[  AJ91\] and \[ JL83\] present wo forms of the same argument , but each presentation suffers from a central shortcoming  . Abney and Johnson , discussing parsing strategies rather than parsers  , fail to characterize topdown , bottom-up , and left-corner parsing in a way that permits a fair comparison of space utilization  . Johnson-Laird , ibrmalizing parsers as pushdown automata , provides a characterization that clearly defines the terms of the comparison  , but his left-corner automaton lacks the properties needed to make the argument succeed  . 
Modifying the left-corner automaton so that it performs arc-eager enumeration is straightforward  . As discussed toward the end of the previous ection  , " attachment " of a node X to its pareut occurs when the symbol IXX\]  , representing the topdown and bottom-up views of that node  , is removed from the stack . In order to attach the node ( i . e . , enumerate the arc ) eagerly , we should pop the symbol as soon as it is introduced  . For the automaton in the previou section , this amounts to augmenting rule schema 8 with the rule I\[Input ISt ~ kINew to pots t ~k I\  [8   '1 ign ? redl .  ,  . \[ vP V\]I , , . NP\[and , in general , augmenting the rules of left-corner prediction so that symbols of the form\[XX\]are not introduced obligatorily  . 
It is easy to show that the automaton , modified in this fashion , requires only a finite stack for arbitrarily ACTESDE  COLING-92  , NANTES , 2328 Aotrr 199219 $ Pgoc . OFCOLING-92, NAN'I~S , AUG .  2328 . 1992 ( A'--~B1 . . . B  ~ . )( A ~? BI . . . B~:1(Bt-- .  3'~) . . . ( B  ~ . rk ) Figure 5: Inferenre-ru & , liar , ncter ~: alion of bottom-up reduction step ( left ) ~ dt , ,p . down prediction step (, ~ ght ) . 
deep left - and right-l , ranehmg constructions , but requires increasing stack~t , ,trq " fi , rc , ' nter-embedded constructions as the depth-f , -mb~-dding increases . Thus we have succeeded in pr , -~ enlnga complete version of the argument in \[ AJ91\] and \[ JL83\] in the sense that 1  . topdown , bottom-up , and left-corner parsing are characterized in a formally precise way  ,  2 . the chaxacterizations are abstract , in the sense that the logic of the algorithms ( in the form of nondeterministic pushdown automata  ) is separated from their control ( namely the control of how the automata's nondeterministic choices are made  )  ,  3 . the notion of space utilization ( namely stack size ) is the same for each case , permitting us to make a fair comparison , and 4 . the conclusion , as expected , is that topdown and bottom-up parsing both make incorrect predictions  , but a form of left-corner parsing is consistent with the apparent behavior of the human sentence processing mechanism  . 
4  . 2  .   .   . and its Implications The import of the " fix " in the previousection is not simply that the automaton can be made to display the appropriate behavior  . It is that the " arc-eager " enumeration strategy is a different  ( and perhaps misleading ) description of a purser's ability to perform composition on the structures that it is building  . 
If we describe the parsers assets of inference rules rather than automata  , s the inference permitting arc-eager enumeration i the left-corner parser turns out to be a rule of composition : A ~ c ~? B and B  ~/3   . 7 can be composed to form the dotted item A ~/3 .  3' . 
For instance , the effect of rule 8' is to predict VPV ? NP from V , and then immediately compose this new item with S  --  , NP . VP . Equivalently , the rule first predicts the VP structure in Figure  4 from the V ( giving us\[VP VP\] , corresponding to the two VP nodes the figure ) , and then immediately identifies the lower VP node with the upper one  ( which removes \[ VP VP\] )  , leaving just an S structure that lacks an NP . 
ST wo descriptior ~ that are formally equivalent.
In contrast , even if one were to add a rule of composition to the inferential description of topdown and bottom -uparsers  , it would have no effect . Neither the topdown nor the bottom-up arse rever introduces a configuration in which the A constituent and B constituent are both only partially completed  ( and thus can be composed )  . Instead , these parsers rewrite the entire righthand side of a rule at once  ( see Figure 5 )  . 
In order for a rule of composition to be relevant , it is necessary that the parser introduce both the topdown view of a constituent  ( e . g . B in A---*~?B ) and the bottom-up view of that constituent ( e . g . Bin B ~ f t . 3`) so that they may later be identified . Unlike topdown and bottom-u parsers , a left-corner parser meets this criterion . 
By presenting a complete version of the argument in\[  AJ91\] and \[ JL83\]  , we have essentially rediscovered proposals made by Puhnan\[  Pu185  , Pul86\] and Thompson et al\[TDL91\] . Both propose parsers with left-corner prediction and a composition operation added  . 
Pulman motivates his purser's designon grounds of psychological plausibility  , though he does not present a complete version of the argument discussed here  . 
Thompson et al are motivated by issues in parallel parsing  . In addition , we should note that Johnson-Laird introduces a parser with a composition-like operation later in his discussion  , thoughout side the context of a formal comparison among parsing methods  . 
Abney ( personal communication ) points out that , though psychologically plausible in terms of the space utilization argument we have discussed  , the automaton presented here may nonetheless fail to be plausible because of its behavior with regard to local ambiguity  . If we opt to compose whenever possible ( e . g . , always preferring rule 8' to rule 8 when X = VP )  , which seems natural , then left-recursive structures will lead to counterintuitive rsults -- for example  , in processing (2) , the automaton will prefer to attach the NP the cat as the object of the verb  , rather than waiting for the full NP the cat's dinner  . 
2 John prepared\[\[tlmeat\]'s dinner\].
More generally , as Abney and Johnson discuss , there is a tradeoff between storage , which is conserved by strategies that perform attachment " eagerly  , " and ambiguity , which is avoided by deferring attachment until more information is present ore solve it  . On the basis of the observations we have made here  , it appears that this tradeoff is expressed most naturally not in terms of a comparison between different parsing strategies  , but rather in terms of the criteria for when to invoke a composition operation that is available to the parser  . 
ACTESDECOLING-92 , NANTES , 2328 Ao~Yr 1992 196 PROC . OFCOLING-92, NANTES , AUO .  23 -28 , 1992 5 Conclusions In this paper , we have considered a space-utilization argument concerning the psychological plausibility of different parsing methods  . Both\[ AJ91\] and \[ JL83\] make the same basic claim , namely that topdown and bottom-up parsing lead to incorrect predictions of a symmetry in human processing -- predictions that can be avoided by utilizing a left-corner strategy  . We have demonstrated difficulties with both of their formulations and presented a more precise account  . In so doing , we have found that composition , rather than left-corner prediction per se , plays the central role in distinguishing parsing methods  . 
In making the argument , we were forced to aband on the abstract characterization f parsing methods in terms of strategies  , and return to defining parsers in terms of their realizations as automata  . This has the unfortunate consequence of tying the argument to contextfree gramnrars  , losing tire attractive for nralism-independent quality evoked in \[  AJ91\]  . 
Since context : free grammars are no longer generally considered likely models for natural language in the general case \[  Shi85\]  , one wonders how the discussion here might be extended to parsing within more powerful grannnatical frameworks  . It is interesting to note the relationship between the style of left-corner parsing described here and one such framework  , combinatory categorial grammar ( CCG )\[ Ste 90\] . Composition is an integral part of CCG , as is the notion of typeraising , which resembles left-corner prediction .   7 The operation of a left-corner parser with composition can fairly be described as being in the style of CCG  , but retaining the contextfree base . Since one attractive feature of CCG is its inherent left-to-right  , word-by-word incrementality , it is perhaps not surprising to find that parsers of CCG tend naturally to meet the criteria for psychological pausibility discussed bere  . 
CCG is one instance of a general class known as the mildly context-sensitive grammar formalisms \ [   JVSW88\]  . We are currently investigating a generalization of the argument presented here to other formalisms within that class  . 

This research was supported by tile following grants : A RODAAL  03-89-C-0031  , DARPAN 00014-9 O-J-1863 , NSF IRl 90-16592 , and Ben Franklin 91 S . 3078C-1 . I would like to thank Steve Abney , Mark Johnson , Aravind Joshi , Yves Schabes , Stuart Shieber , and members of tile CLIF F group at Penn for their help fifl discussion and criticism  , rFor example , NP can be type-ralsed to S/(S\NP) , whid ~ roughly corresponds to S~NP . VP . 
References\[AJgl\]Steven Abney and Mark Johnson  . Memory requirements and local ambiguities for part tiag strategies  . Journal of Psycholinguistic Research ,  20(3):233--250 ,  1991 . 
\[ ASU86\] Alfred Aho , Ravi Sethi , and Jeffrey Ullmu . 
Compilers : Principles , Techniques , and Toe , t.
Addison Wesley , 1986.
\[DJK+82\]A . De Roeck , R . Johnson , M . King , M . 
net , G . Sampson , and N . Varile . A mythabout ceutre-embedding . Lingua , 58:327-340, 1981 . 
\[JL83\]PhilipN . Johnson-Laird . Mental Models . Harvard University Press , 1983 . 
\[ JVSW88\]A . K . Joshi , K . Vijay-Shanker , and D . J , Weir . The convergence of mildly context-sensitive grammatical formalisms  . In P . Selht and T . Wasow , editors , Processing of Lingutsl , cStructure . MIT Press , Cambridge , MA , 1988 . 
J . Kimball . Seven principles of surface-structure parsing in natural language  . Cognition , 2:15-47, 1973 . 
Itarry Lewis and Christos Papadimitrion . Ele.
ments of the Theory of Computation . Prentice-
Itall , 1981.
George Miller and Noam Chomsky . Finitary models of language users . In R . Luce , R . Bush , and E . Galanter , editors , Handbook of Math . 
ematical Psycholcfy , Volume 2. John Wiley , 1963.
G . A . Miller and S . Isard . Free recall of self-embedded English sentences . Information and
Control , 7:292--303, 1964.
l " ernando C . N . Pereira and Stuart M . Shieber.
Pralog and Natural Language Analysis . Cen-.
ter for the Study of Language and Information ,  1987 . 
Stephen Pulman . A parser that doesn't . In Proceedings of the 2nd European ACL , pages 128-135 ,  1985 . 
Stephen Pulman . Grammars , parsers , and memory limitations . Language and Cognitive Pro . 
cesses , 1(3):197-225, 1986.
S . M . Shieber . Evidence agains the context-freeness of natur M language  . Linguistics and
Philosophy , 8:333-343, 1985.
Mark Steedman . Gapping as constituent coordination . Linguistics and Philosophy , 13:207-263,
April 1990.
H . Thompson , M . Dixon , and J . Lumping.
Compose-reduce parsing . In Proceedings o . f the 29th Atmual Meet it , y of the ACL , pages 87-97 , 
June 1991.
William A . Woods . Transition network grammars for natural anguage analysis  . Commu . 
nications of the ACM ,  13(10):591-606 , October 1970 . 
\[I ( i m73\]  \[  LP81\]  \[  MC63\]  \[  MI84\]  \[  ps871  \[  Pu185\]  \[  Pul86\]  \[  Shi85\]  \[  Ste9O \]\[ TI ) L911\[WooT0\]ACTES DECOLING-92 , NAN ' NS , 2328 Aotrr 1992197 PROC . OFCOLING-92, NAI~t'ES , AUO .  2328, 1992
