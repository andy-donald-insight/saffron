THE " WHITEBOARD " ARCHITECTURE :
AWAYTOINTEGRATEHETEROGENEOUS COMPONENTSOFNLP SYSTEMS 
Christian Boitet Mark Scligman
GETA , IMAG ( UJF&CNRS),
150 rue de la Chimie , BP 53
38041 Grenoble Cedex 9, Fr , ' mce
Christian . Boitet@i mag . fr
NI'R Interpreting Telecon ununicafions Research Labs 
22 ltikari-dai , Seikacho , Somku-gun
Kyoto 619-02, Japan
seligman@iti.atr.co.jp

We present a news of tw , ' u'e architecture for NLP systems made of heterogeneous components  , and demonstrate an architectural prototype we have built at ATRiu the context of Speech Translation  . 
KEYWORDS : Distributed NLP systems , Software architectures , Whiteboard . 

Speech translation systems must integrate components handling speech recognition  , machine translation and speech synthesis . Speech recognition often uses special hardware . More components may be added in the future , for task understanding , multimodal interaction , etc . In more traditional NLP systems , such , ' cqMT systems for written texts , there is also a trend towards distributing various tasks on various machines  . 
Sequential , architectures\[10 , 11\] offer , an easy solntion , but lead to loss of information and lack of robustness  . On the other hand , reports on experimenls with blackboard architectures  \[16  ,  13 , 20\] show they also have problems . 
We , are exploring an intermediate architecture , in which components are integrated under a coordinator  , may be written in various programming languages , may use their own data structures and algorithms , and may run in parallel on different machines . The coordinator maintains in a whiteboard an image of the input and output data structures of each component  , at a suitable level of detail . The white hoard fosters reuse of partial results and avoids wasteful recomputation  . Each component process is encapsulated in a manager  , which transforms it in loaser ver , commuuicating with external clients ( including the coordinator ) via a system of mail boxes . Managers handle the conversions between internal ( server ) and external ( client ) data formats . This protocolenhances modularity and clarity , because one needs to to explicitly and completely declare fl ~ e appearance of the partial results of the components on the while board  . 
Managers may also make batch components appear : is incremental components by delivering outputs in a piecewise fashion  , thus taking a first step toward systems simulating simultaneous translation  . 
We have prc~luced a rudimentary architectural prototype  , KAS UGA , to demonstrate he above ideas . 
Infl ~ e first section , our four main guidelines , are detailed : ( 1 ) record over all progress of components in a whiteboard  ;   ( 2 ) let a coordinator schedule the work of components  ; (3) encapsnlate components in managers ; and ( 4 ) use the managers to simulate Incremental Processing  . In the second section , some high-level aspects of the KAS UGA prototype , are first described , and a simple demonstration is discnssed , in which incremental speech translation is simulated  . Lower-level details are then giveu on some internal aspects  . 
I . TIlEWII1TEBOARDARCHITECTURE1 . Record over all progress in a whitelmard The whiteboard  , architecture is inspired by the chart architecture of the MIND system  \[8\] and later systems or formalisms for NLP \[1  ,  5\] , as well as by the black bo~u'd architecture , first introduced in HEARSAY-II\[6 , 13\] for speech recognition , l lowever , there is a significant difference : tile components do not access the whiteboard  , and need not even know of its existence . 
There are 2 main problems with the sequential pproach . 
? Pl : loss of information
If components , are simply concatenated , as in Asnra\[10 ,  11\] , it is difficult for them to share partial results . 
Information is lost at subsystem interfaces and work has to be duplicated  . For example , the cited system uses an LR parser to drive speech recognition  ; but syntactic structures found are discarded when recognition candidates are passed to MT  . Complete reparsing is thus needed . 
? P2: lack of robustness
Communication difficulties between subsystems may also dmnage robus mess  . During reparsing for MT in ASURA , if no wellformed sentences are found , partied syntactic structures are discarded before semantic analysis  ; thus there is no chauce to tr , ' mslate partially , or to use semantic inlonnation to complete the parse  . 
The pure blackboard approach solves P1 , but not P2 , and introduces four other problems . 
? P3: control of concurrent access
In principle , all components are allowed to access the blackboard : complex protection and synchronization mechanisms must be included  , and fast components may be considerably slowed down by having to wait for permission to read or write  . 
? P4: commnnication overloads
The amount of information exchanged may I~large . I1" components rnn on different machines , such : is is often the case for speech-related componeuts  , and may be the case for Example-Based MT con ~ponents in the future  , commmfication overloads may annihilate the bcuciit of using spcck dized or distributed hardware  . 
? P5: efficiency problems
As components compute directly on the black bo , ' u'd , it is a compromise by necessity , and cannot offer the optimal kind of data structure h ~ reach component  . 
? P6: debugging problems
These , are due to the complexity of writing each component with the complete blaekbo  , ' u ' dinmind , and to the parallel nature of the whole computation  . 
In the " whiteboard " approach , the global data structure is hidden from the components  , and accessed only by a " coordinator " . ( The white boar drawing is expanded later . ) ( . ~ Olll porlell\[
I(~OlllpOIl(3ll\[ol
Figurel : the " whitelmard " arc'h#ect , ~ reThis simple change makes it possible to avoid problems  P3-P6  . It has also at least two good points : - It encourages developers to clearly define and publish what their inputs and outputs are  , at least to the level of detail necessary to represent them in the whiteboard  . 
- The white board can be the central place where graphical interfaces are developed to allow for e ~Lsy inspection  , at v , ' u ' ious levels of det ~ fil . 
As long as an NLP system uses a central record accessed ouly by a " coordinator ' and hidden fi ' om the " coml xmeuts "  , it caube said to use a whiteboard architecture . It remains open what dala structures the white board itself should use  . 
As in \[21 , we suggest the use of a time-aligned lattice , in which several types of nodes can be distinguished  . In stating our preference for lattices , we must first distinguish them from grids , and then distinguish true lattices from 2 types of quasi-lattice , charts and Q-graphs ( fig .  2 & 3) . 


NP INP2\]e ~ . ~ W  ~ . .= . .__ . 41 ~_~ O_ . ._ . _ . _ii ~ tornove and ~ lrOund fur 4t , ~ kIINPSIIII . /paSent i='~=1\[NP4_, . .-
IS enl2
S ent3 r ~ or uove tiller cap and ground fueltank Fig .   2: chart built on a syntactically ambiguous a ' et /tence 

N(can , N(P , . V(spackle , P(3pI)..,'k.=.
A(Iklht,N(S,~N(can,NtS .  , . Vsparkle , P(3sffi . . . ~
GM ). f ~,--. V(spafl de , lnf , ..)
G(M ../
I4 ( light , N(S ) , N(ean , N(S ) , IV(s parkle , N(Pau I , PN , k ( light , N(S , PlIV(can , T ( P ) , Il N(soa , ~ , ~' AV(slight . . , L~n ~ ~ iP "-'~ ~ ONb(sfl) , PossIGIM , F ) ~ N(S) , G(M ) , Iv(light ''')! IN ~ , , . , l  ~ . IIN ! spark ~'
IG(M ), ..) N(P ), GM))
N(light,N(S ), V(cin,T ( P ~ ,=~ . . . . e V(spathle,lnf, . ,)
G(M )..,\] ~ V(mod))
Paul's ligh ! can(s)s parl do(s ) slightty
Fig .   3: AQ-graph for a phonetically ambiguous sentence Grids have no arcs  , but nodes Co , Tesponding to time spans . Ancxle N spanning It132\] is implicitly connected to another node N ' spanning\[t'l  , t'2\] iff its pan begins earlier 01gt'l ) , ends strictly earlier ( t2<t'2) , and the respectives p , ' ms ( a ) are not too far a partant i ( b ) don't overlapt c ~ ) much ( t2-max-gap_<t'l~t2+max-ovorlap )  . max-gap and max-overlap are gapping and overlapping threshokts  \[12\]  . Because t2 < t'2, there can be no cycles . 
ht a lattice , by contrast , nodes and arcs are explicit . 
Cycles are also for hidd cn , and there must be a unique first node and a unique last node  . 
(; rids have often been used in NLP . l " or example , I he output of the phonetic omponent of Kt ~ AL \[121 was a word grid , and certain speech recognition programs at NI'Rl ~r  ( ? luce phoneme grids 1 . In gener ~ d , each uc ~ le bears a time span , a label , and a score . Grids can also be used to represent an input text obtained by scanning a bad original  , or as tenotypy tape\[9\] , and to implement some working structures ( like flint of the Cocke algorithm )  . 
l lowever , we will require explicit arcs in order to explicitly model possible sequences  , sometimes with associated information concerning sequence probability  . 
Thus mwgrkls amin sufficient for our whiteboards.
Two kinds of quasi-lattices have been used extensively  , in two wtrietics . First , chart structures have origi , mlly been intr(~luccd by M . Kayin the MIND system around 1965\[8\] , Inach : ut , as understood tt ~ lay ( Kay's charts were more general )  , the nodes , are arranged in a row , so that there is always a path between any two given nodes  . The arcs bear the information ( label , score ) , not the nodes . Ch\[u'ls are also used by many unification-based natural hmguage analyzers  \[141  . 
Chart structures are unsuitable for represcnting rest flts on a whiteboard  , however , because they are tmable to represent alternate sequences  . Consider the alternate word sequences of Figure 4 . It is not possible to arr . 'mgethe words in a singlemw so that all and only the proper sequences can be read out  ,   1   1 it if you came I would like you to comeemly tomorrow earlier Figure  4: A sentence with alternate form tdationsA second type of quasi-lattice is the Q-graphs of  \[15\] and their exteasiou \[17\]  , the basic data structure for text representation in tile METI ~  , O\[14\]aud TAUM-Aviation\[71 systems . AQ-graph is a loop-free graph wilh at mique entry node and a uni  ( lue exit node . Asiu charts , the inlonnalion is carried on the arcs . It cousisls in labeled or at motaled trees . As there may be nol ) ath between two nixies , Q-graphs can indeed faithfully represent alternate sequences like those of Figure  4  . But in this case it is necess ; uy to use , on more thau one arc , identica labels referring to the same span of the input  . For representation on a whitcl ? mrd , such duplication is a drawback . 
To simplify bookkeeping and visual presentation , we prefer a representation in which a given label referring to a given span appea Jw in only one place  . A true lattice , like flint of Figure 5 , makes this possible . 
" lhe decomposition of the laltice in htyers seems natural  , aud leads to more clarity . Fach layer contains results of 1115, 16\] . By contrast , tile IIWIM \[20\] system used a " phonetic lattice " on which an extended ATN operated  . 

The " Whiteboard " Architecture : a way to integrate  . . . Boitet & Seligmcm , COLING94 one component , selected to the " appropriate level of detail " . It stime-aligned character makes it possible to organize it in such a way that everything which has been computed on a certain time interval at a certain layer may be found in the same region  . Each layer has three dimensions , time , depth and label ( or " class ") . A node at position ( i , j , k ) corresponds to the input segment of length j ending at time i and is of label k  . All realizations of labelk corresponding to this segment are to be packed in this node  , and all nodes corresponding to approximately equ , ' dinput segments amthus geometrically clustered . 
In other words , ambiguities are packed so that dynamic programming techniques may be applied on direct images of the whiteboard  . Figure 6 gives a nex , ' unple , Where the main NP has been obtained in two ways . 
GQI IIII%
Figure 5: A word lattice ( representing a sentence with alternate for nudations Arcs may optionally be augmented with activation or realistic hoice of layers  , however . 
inhibition weights , so that ideas from the fast-developing lield of neural networks may be applied  . 
language u~q ~ layers layers
Figure 6: The whiteboard as a factorizing data structure The true lattice  , then , is our preferred structure for the whiteboard . 
We said that the whiteboard could be a central place for transp  , ' u'ent inspection , at suitable levels of detail . We use the notion of " shaded nodes " for this . 
-" White " nodes are the real nodes of the lattice  . They contain results of the computation of the component associated with their layer : a white node contains at least a label  , legal in its layer , such as NP , AP , CARDP , VP . . . in the example above , and possibly more complex information , as allowed by the declaration of the layer in the whitelx ~ ard  . 
-" Grey " nodes may be added to show how the white nodes have been constructed  . They don't belong to the lattice structure proper  . In the example above , they stand for rule instances , with the possibility of m-->n rules . In other cases , they may be used to show the correspondences betwee nodes ot two layers  . 

Figure 7: White and grey nodes corresponding to rule Rn :
X1 X2 . . . Xp->Y1Y2 . . . Yq-"Black " nodes may be used to represent finer steps in the computation of the component  , e . g . to reflec the active edges of a chart parser . 
Whiteboard larers are organized in alc , op-li'e e dependency graph . Nonlinguistic as well as linguistic information can be recorded inappropriate layers  . For example , in a multimodal context , the syntactic analyzer might use selected information from a map layer  , where pointing , etc . 
could be recorded . Interlayer dependencies should be decl~u'ed , with associated constraints , stating for instance that only nodes with certain labels can be related to other layers  . 
I lere is an illustration of that idea , wilh out any pretense to propose a ~ Ot\]llell U layer layer Figure  8: A hierarchy of layers in an hypothetical whiteboard for anndtimodal NLP  , ~' ystem 2 . Let a coordin'ator schedule tile components In its simplest form  , a coordinator only transmits the results of a component to I he next component  ( s )  . 
l lowever , it is in a position to carry out global strategies by filtering low-ranking hypotheses and transmitting only the most promising part of a whitcboard layer to its processiug component  . Further , if certain components make usel hl predictions , the coordinator can pass these to other components as constraints  ,   , along with input . 
3. Encapsulate components in managers
Developers of components should be free to choose and vary their algorithms  , data structures , programming languages , and possibly hardware ( especially solor speech-related components )  . Our approach is to encapsulate existing components in managers  , which hide them and transform them into servers . This strategy has the furlher adv , ' mtage of avoiding any direct call between coordinator and components  . To plug in a new component , one just writes a new manager , a good part of which is generic . 

The " Whiteboard " Architecture : a way to integrate  . . . Boitet & Seligman , COLING94 Am , ' mager has a request box where clients send requests to open or close connections  . A connection consists of a pair of in and out mail boxes  , with associated locks , mid is opened with certain paraneters , uch as its sleep time and codes indicating pre -agreed import and export formats  . The coordinator puts work to do into in boxe said gets results in corresponding out-boxes  . 
As illustrated in Figure 1 above , a client can open more than one connection with the sane manager  . For exanple , au online dictionary might be called for displaying " progressive " word for word translation  , as well as for , ' mswering tern finological requests by a human interpret cr supervising several dialogues and l ~ ddn gover if needed  . 
And a malager can in principle have several clients  . 
llowever , this potential is not used in KAS UGA.
4. Simulate incremental processing
In real life , simullanexms interpretation is often preferred over consecutive interpretation : although it may be less exact  , one is not forced to wait , and one can react even before the end of tile speaker's utterance  . Incremental processing will thus be an i in portant aspect of future machine interpretation systems  . For instance , a sem . ' mlic processor might begin working on the syntactic structures hypothesized for early parts of an utterance while later parts  , are still being syntactically an , ' dyzed\[19\] . 
Even if a component ( e . g . , a W cun'ently existing speech recognizer ) has to get to file end of the utterance before producing any result  , its n mnager may still m ; tke its processing appear incremental , by delivering its result piecewise and iuthe desired order  . Ilence , this organiz ' ~ tion makes it possible to si in t fiate future incremental components  . 
11 . TIlEKASUGAPROTOTYI'E1 . External level The coordinator ( KAS . COORD ) is writt cn in KEKTM , au object-oriented xpert systems hell with excellent interface-building tools  . The whiteboard is declared illKEF\]s object language  . KEE itself is written ill Common lisp . 
Three components are in w/lved:-speech recognition  ( SP . REC ) providing : t3-level grid , progrmn mcd in C\[15\] ; -ish'md-driven syntactic hart-parsing ( SYNT . AN ) deriving words and higher-level syntactic units , programned in C ; -word-for-word translation ( WW . TRANS ) at file word level , written in Caid running on another machine . 
The tanagers are written in L is p ,   , ' rod run independently , in three Unix processes . Each manager , and the c ( gmlinator can rat in different Unix shells . Although WW . TRANS is already accessible as a server on a distant machine  , we had to create a manager l brit to get the intended behavior  . 
With only these components , it is possible to produce a simple demonstration in which incremental speech translation is simulated and the transparency gained by using a whiteboard is illustrated  . The phonemes produced by SP . REC are assembled into words and phrases by SYNT . AN . As this goes on , WW . TRANS produces possible word-for-word translations  , which are presented on screen , ' u , ~a word lattice . 
KASUGA's whiteboard has only three layers : phonemes  ; source words and phrases ; and equivalent target words . At the first layer , the phoneme lattice is represented with phonemes in nodes  . At the second layer , we retain only the complete substructures produced by SYNT  . AN , that is , the inactive exlges . Phonemes used in these slructures appear again at that layer  . 
In KEE , we define a class of NODES , with subclasses WHITE . NODES , GREY . NODES , PIlON . LAYI~P, . NOI ) ES , aud SYNT . I , AYER . NODES in tile syntactic htycr . NODES have a generic display method , and subclasses have specialized variants ( e . g . , the placing of white nodes depends on their time interval  , while that of grey nodes depends on that of the white nodes they  cermet0  . 
2. Internal level
When a manage receives a Make . Conuection request frola a client , it creates an inbox and an outbox ( and associated locks , used to prevent interference between components ) , through which information is p . ' ~ ssed to and from the client . The Make . Connection request includes codes showing in which format  ( s ) the client is expecting to deposit data in theiu box and read data from tile out box  , l br that connection . 
Mlhough data transfer could be programmed more efficiently  , e . g . nsinglh fix sockets , our method is more general , as it uses only the file system , and we believe its overhead will be negligible in comparison with tile processing times required by the compouents  . 
Ikneachout box , the client ( KAS UGA ) act batcs a reader process and tile relew mt mauagcractiw ttesawriter process  . 
Conversely , for each in box , tile client activates at writer process and the manager activates a reader process  . A zeader process wakes upregul : uly and checks whether its mail box is both nonempty and nn locked  . If so , it locks the mail box ; reads ils contents ; emptiest il email box ; unlocks it ; and goes to sleep again . A writer process , by comparison , wakes up regul : uly and checks whether its mail box is both empty and unlocked  . If so , it locks the box , fills it with appropriate data , unlocks it , and goes back to sleep . For example , the writer associated with SYNT . AN will deposit in the appropriate outbox the image of all tile in active arcs created since the lm  ; tdeposit . 
SItI? , EC provides , loreach of 40 pre recorded bunsetsu ( elementary phrase )  , a set of about 25 phone memalrices , one for each phoneme . A malrix cell contains the score for a given phoneme with a given begim fiug/ending speech frane pair  . The senmtrices are then compared , and 3 other in a trices are computed . The tnp-scoring ln : llrix contains in each cell the tnl~-scnring phone and its score for I he corresponding begim liug/cnd  . The 2nd-scoring a ~ d 3rd-scoring matrices are computeds in filarly . These three mauices are used to build the first layer of the whiteboard  . 
To build the whilc board's second layer , an is hmd-driven clmrt parser is used , where the matrices are cousklered as initialized charts  . The over : dlbest-scoring cell in the top matrix is established as the only anchor  , and hi-directional searching is carried out wilh in the  ( hand set ) limits set by max-gap and max-overlap . A CFG written by J . llosaka fortile ASURA demos is now used as is . Parsing results are convert cd to syntact Jc : . \] at <5 ce . N ( by Olt\[chart-to-lattice filter ) and brought into KEF ~ . 
Then an image lattice , ww . latt\]ce . N , is comptlted as the whiteboard's third layer , using a C-base dou-tine J-l dictionary . Each lexieal syntactic node gives rise tooue Fmglish word for each meafing  . For example , ~ gives yes , yes-sir , the-lungs , ashes , etc . 
Layers of the whiteboard are represented by KEF , " planes " . We can move planes reht live to e\[ich olher ; ztx~min various ways ; put various information in the nodes ( label , rule responsible , i d , time span , score ); exp , ' md the nodes ; open & close the nodes selectively . And we can color the nodes according to their score  . It is possible to show or hide various parts of the whiteboard  . In Figure 9 , the first layer , the time grid , the lattice lines , and the initial/final lattice nodes have been hidden  . Alternatively , we could hide constnlction ( dotted ) lines , rule boxes , label boxes , etc . The view of any part of the whiteboard can he changed for emphasis : one can for instance interactively select only the nodes above a certain confidence threshold  . 
Overall processing can be inten'upted for examination  . 
WW.lattice , N,~.~I start II
I , II?I ', , , , ose , s., l
III
II " --" oI ~. Io ~#%%

Figure 9: a view of KASUGA's whiteboard If this architecture is to be further developed in the future  , one could use instead of KEE a general-purpose , portable interface building toolkit in order to avoid the oved ~ ead  , ' rod overspecialization , associated with using a complet expert system shell  . 
KAS . COORD writes and reads data to and from the managers in a LISP-like format  , and handles the transformation i to KEE's internal for nmt  . Each manager translates back , and forth between that format and w batever format its associated component happens to be using  . 
Ilence , formats must be precisely defined . For inst , ' mce , the edges produced by the speech recognizer are of the form  ( begin end phoneme score )  . The nodes and edges of the conesponding phoneme layer in the whiteboard are of I he form  ( node-id begin end phoneme score ( in-arcs )   ( out-arcs ) ) , with a resbeing of the form ( are - id originext remity weight )  . 

Although the concept of the whiteboard architecture Ires emerged in the context of rese  , -u'chin Speech Translation , it can be useful in other areas of NLP . It has already been used , in a prelim in , ' u ' y form , in dialogue-b~sed MT \[3\]: the tasks are distributed between the authoring stations and an MT server  , m~d the coordinator maintains in a unique data structure all intermediate stages of processing of all units of translation  . 
The whiteboard , architecture might be used with profit in all situations where it is important o integrate new or existing components  , e . g . to build generic environments for developing heterogeneous NLP systems  . Researchers would thereby gain twice : by getting a clearer view of what they  ( and others )   , are doing ; and by being able to use generic interlace tools provided by the coordinator for debugging and illustrating purposes  . 
ACKNOWI , EDGMENTS
We are grateful to M . Fiorenthm from Intellicorp , Inc . 
and K . Kurokawa from CSK , Inc . , for providing a demo copy of KEE TM and valuable technical support  ; to Dr . 
Y . Yamazaki , President of ATP ,- IT , and T . Morimoto , llead of Dept . 4, for their support and encouragement ; to II . Singer , T . ltayashi , Y . Kitagawa , I\[ . Kashioka , and J . Hosaka , for their help in developing the components ; and to K . II . Loken-Kim , for sfimt flating discussions and proposing the term " whitelx ~ ard "  . 
REFERENCES\[1\]BarnettJ . , Knight K . , Mani I . & Rich E . 
119901 Knowledge and Natural Language Processing.
Comm . ACM , 33/8, 50-71.
\[2\] Bolt et C .   ( 1 9881 Representation and Computation of Units of Translation for Machine Interpretation of Spoken 
Texts . Comp . & AI , 6, 505--546.
\[3\]BaiterC . & Blanch on 11 .   ( 1993 ) Dialogue-Based MT for Monolingual Authors and the LIDIA project  . Proc . 
NLPRS'93, Fukuoka , 208--222.
\[4\] Ch and loux J . & Gu~rard M . q *' .   ( 1981 ) METEO : unsysltSme?tl'tC preuved utemps . META , 1, 17--22 . 
\[5\] Colmeraner A . (1970) Les syst dmes-Q , unformalisme pour analyseret synthd tiser des phrases surordinateur  . TAIJM , Univ . de MontrSal , dec .  1970 . 
\[61 Erman L . D . & Lesser V . R .   ( 1 9801   771e Hear say-H Speech Understanding System : A Tutorial  . ht " Trends in Speech Recognition ", W . A . Lea , ed . , Prcntice-lhdl , 361-381 . 
\[7 il sabelle P . & Bourbeau L .   ( 1984 ) TAUM-A VIA770N : its technical features and some experbnent al results  . Comp . Ling . , I1/I , 1827 . 
\[8\] Kay M . (1973)77 ~ eMIND system . In " Courant Computer Science Symposium 8: Natural Language Processing " , R . Rustin , ed . , Algorithntics Press , 155-188 . 
\[9\]M 6 rlal do 1 .   ( 1988 ) Multilevel decoding for Vety-Large-Size -Dictionaly speech recognition  . IBM Journal of
R&D , 32/2, March 1988, 227-237.
\[101 Morlmoto T . , St , zuki M . , Takezawa T . , Klknl G . -( . , Nagata M . & Tnmokiyo M .   ( 1 9921 A Spoken Language Translation System : SL-TRANS2  . Proc . COLING-92, Nantes , vol .  3/4, 1048--1052 . 
\[11\] Morlmoto T . , Takezawa T . , Yato F . , Sagayama S . , Tashlro T . , Nagata M . & al . (1993) ATR's Speech Translation System : A SURA . EuroSpeecb'93 . 
\[12\]QnlntonP .   ( 19811 ) Contribution h l are connaissanced lapa role . Utilisation tiemdthodes heuristiques pour la reconnaissance tiphrases  . TbSsed'Etat,
Univ . de Rennes , 239p.
\[13\]ReddyR . (19811) Machine Models of Speechl ' erception . In " Perception and Production of Fluent Speech ",
Cole , ed ., Erlbaum , N.J ., 215-242.
\[141 Schleber S . M . (19861 Anit , troduction to unification-based approaches to grammar  . CSLILect . Notes 4 . 
\[15\] Singer II . & Sagayama S .   ( 1 9921 Matrix Parser and its Application to 11MM-based Speech Recognition . 
IEICE , 111/SP92-76, I)SP92-61, 2126.
\[16\]SingerI1 . & Sagayama S .   ( 1992 ) Matrix Parsing applied to TDNN-based Speech Recognition  . Japanese Journal of Speech Processing ,  1992/3 ,  89-90 . 
\[17\] Stewart G . (1975) Manueldu langage REZO.
TAUM , Univ . de Montrfal , 120p.
\[18\]TomltaM .   ( 19911 )   77~e Generalized LR Parser/Compiler V8-4 : a Software Package for Praclical NLl'rojects . Proc . COLIN (;-90, vol .  1/3, 59-63 . 
\[19\] Waldster W . (1993) Planning Multimodal Discourse . Proc . ACL93, Columbus , Ohio , 95-96 . 
\[20\]Wo lfJ . J . & Woods W . A . (19811)771e IIWIM Speech Understanding System . In " Trends in Speech Recognition ", W . A . Lea , ed . , Prentice-llall , 316-339 . 

