Linear Encodings of Linguistic Analyses
Samuel S . Epstein
Bell Communications Research
445 South Street , 2Q-350
Morristown , NJ 07960-1910

epstein@flash.bellcore.com
1. Introduction
Natural languages contain families of expressions such that the number of readings of an expression is an exponential function of the length of the expression  . 
Two wellknown cases involve prepositional phrase attachment and coordination  . Other cases discussed below involve anaphora and relative operator scope  . 
For example , in ( l ) John said that Bill said that . . . that Harry said that he thinks that he thinks that  . . . that he thinks that it is raining . 
each he can refer to any of John , Bill . . . . . Harly ? Thus if ( 1 ) contains n names and m occurrences of he , this sentence has nm readings ( assuming that all anaphoric relationships are intrasentential  )  . We discuss below families of expressions whose ambiguities grow as various exponential functions  ( factorial , Fibonacci , 
Catalan ) of expression length.
It appears , then , that exhaustive linear-time processing of natural language is impossible  . An exponentially ong answer cannot be produced in linear time  . 2 On the other hand , human processing of natural language seems to be at least linearly fast  . The ready explanation for this is that people do not recover all readings of ambiguous expressions  . This is clearly correct , as far as it goes . 
This paper shows how to encode in linear space the exponentially argenumbers of readings associated with various families of expressions  . The availability of these encodings removes an apparent obstacle to exhaustive analyses of these expressions in linear time  . 
The encodings may thus be useful for practical computational purposes  . They may also provide a better 1 . (1) is of course highly unnatural in a sense . However , it effectively isolates for study a phenomenon that is intrinsic to natural language  . Similar observations apply to the examples below . 
2 . It is of course also the case that an exponentially long answer caunot be produced in polynomial time  . If the problem cannot be reformulated so that answers are not exponentially long  , the question of tractability does not arise . See\[Garey and Johnson 79\] and \[ Barton , Berwick , and Ristad87\] for relate discussions . 
basis than exponential-space encodings for explanations of how humans process language  . 
For each of the linguistic constructions discussed in this paper  , there is a simple program that generates analyses of the construction  . If there are no constraints on what counts as a linguistic analysis  , then a specification of a program , which requires constant space , together with a specification of an input expression  , which requires linear space , could count as a linear encoding of an analysis of the input  . Intuitively , there is a vast qualitative divide between a ( program , input ) pair on one hand , and , for instance , a forest of constituent structure trees on the other hand  . 
More generally , a question arises of how to distinguish analyses from procedures that yield analyses  . This paper will not attempt to answer this question definitively  . The analyses presented in Sections 2  -  4 all satisfy a notion of " legal " analysis that excludes  ( program , input ) pairs . Sections 2 and 3 discuss polynomial space analyses . Section 4 adds are presentational device to the repertory of Sections  2 and 3  , so that linear space analyses are possible . Section 5 in fol~mally discusses a variety of issues , including the distinction between analysis and procedure  . 
2. Analyses in Conjunctive Normal

Assume that example ( 1 ) involves no ambiguities except for antecedents of pronouns  . Assume further that the length of the analysis of  ( 1 )  , aside from the specification of antecedents of pronouns  , grows linearly ) Let the proposition q comprise all aspects of the analysis of  ( 1 )  , aside from specifications of antecedents of pronouns  . 
Let the proposition p . . comprise the specification that the jth name in ( 1I ' J is the antecedent of the ith occurrence of he  . ( For example , Pl , 2 comprises the 3 . These assumptions , and similar assumptions for other examples below , permit a briefer discussion than would otherwise be possible  . Reservations about these assumptions do not affect the substance of the discussion  . Our concern with ( 1 ) focuses on exponentially growing possibilities for assigning antecedents opronouns  . 
1 08   1 specification that Bill is the antecedent of the most shallowly embedded he  . ) Let n be the number of names in ( 1 ) and let m be the number of occun'ences of he . 
Then an exhaustive analysis of ( 1 ) can take the following form: ( l-a )   ( q&P l , ~& P2 , 1&"'"& Pro , 1) v(q&P l , 1 & P2 , I & "'" & Pro-l , 1& Pra , 2) v(q&P l , n&P2 , n & "'" & Pm , n ) ( l-a ) , which contains nm disjuncts , is in Disjunctive Normal Form ( DNF ) . Each disjunct fully specifies a possible interpretation of  ( 1 )  . It is an implicit assumption in much of the literature that the proper form for linguistic analyses is DNF  . An analysis in DNF amounts to a listing of possible global interpretations  . 
(l-a ) is logically equivalent to the following : a atement in Conjunctive Normal Form  ( CNF ) :  ( l-b ) q & ( Pl , 1 vP l , 2v . . . vP l . n ) & ( P2, 1vP2,2v . . . vP2, n ) (3) the block in the box on the table . . . in the kitchen As\[Church and Patil 82\] discuss , examples like ( 3 ) are similar to other structures with systematic attachment ambiguities  , such as coordination structures . While the number of readings of ( 3 ) 4 is thus exponential in the length of ( 3 )  ,   ( 3 ) has an O ( n ) length analysis in CNF as follows: ( 3a ) q & ( Pl , 0) & (3~a-2) ( P2 , 0 v P 2 , 1) & (3-a-3) ( P3 , 0 vP 3 , 1 vP 3 , 2) & ( P3 , 1 zoP 2 , 1) & (3-a-4) ( P4 , 0 vP 4 , 1 vP 4 , 2 vP 4 , 3) & ( P4 , 1 DP 2 , l ) & ( P4 , 1D ( P3 , IvP3 , 2)) & ( P4 , 2 DP 3 , 2) & ( Pro , 1 v Pro . 2v . . . vpm,n ) ( l-b ) contains m + l conjuncts . The length of an exhaustive analysis of ( 1 ) is exponential in the number of pronouns in ( 1 ) when the analysis is given in DNF , but linear in the number of pronouns when the analysis is given in CNF  . However , ( l--b ) is not linear in the length of (1) , because each of m conjuncts contains n clisjuncts  , so that a total of mxn literals is required to specify anaphoric possibilities  . 
The following example has an analysis in DNF that grows as the factorial of the length of the input  :   ( 2 ) Johntold Bill that Tom told him that Fred told himtha !  . . . that Jim told him that Harry told him that it is raining  . 
The first occurrence of him can have John or Bill as antecedent  . The second occurrence of him can have John or Bill or Tom as antecedent  , and so on .   ( 2 ) has an obvious analysis in CNF whose length is a quadratic function of the length of the input  , namely (2:a ) q&(PL1vPl2)&;(P2 , 1 v P 2 , 2v ,  -~2 , 3) & ( Pro , 1 v Pro , 2v . . . vPm , m + l ) where the notation follows the same conventions as in  ( l-a , b) . 
The number of readings for the following noun phrase grows as the Catalan of the number of prepositional phrases :  ( 3-a-k )   ( Pk , 0 vPk , 1V . . . VP k , k_l )&; (3-a-k , 1) ( Pk , lDP2 , 1)& ( Pk , lD(P3 , 1 vP 3 , 2))& ( Pk , 1D(Pk-l , lvPk-l , 2v . . . vPk_l , k . 2)) & (3-a-k , m ) ( Pk , m ~ P m + l , m ) & ( Pk , ,n D ( Pm+2 , mvPm+2 , m + l )) & ( Pk , mD(Pk-l , mVP k-l , m + lV . . . VP k_l , k_2)) & In (3a ) , Pi ' comprises the specification that constituent i  , J .   .   .   . attaches to constmmnt j , where the block~s constituent 0 , in the box is constituent 1 , on the table is constituent 2 , and so on . Constituent k must attach to some constituent that lies to its left  . If constituent k attaches to 2109 constituent m , then the constituents between constituent m and constituent k cannot attach to constituents to the left of constituent m  . 4 For each pair ( k , m ) , the number of atoms in (3-a-k , m ) is fl(k , m ) = , ~' i . fl(k,m ) is quadratic in k . For each k , then , the number of atoms in ( 3-a-k ) is f2 ( k ) = k + l ( k , i ) , a cubic function in k . The number of atoms in ( 3a )   ( excluding atoms hidden in q ) is thus =  f2 ( i )   , a quartic function in n .   ( 3a ) is certainly not the most compressed CNF analysis of  ( 3 )  . It is , however , easy to describe . 
Given an exhaustive analysis in DNF , choosing a global interpretation requires exactly one operation of selecting a disjunct  . Foi " ( l-b ) and (2a ) , choosing a global interpretation requires a number of selections that is linear in the length of the input  . I am aware of no other reason for preferring DNF to CNF for analyses of examples like  ( 1 ) and ( 2 )  . In favor of preferring CNF there is the practical advantage of polynomial-space output  , with its implications for speed of processing . 
There is also the possibility of more accurate psycholinguistic modeling  . It seems likely that people make decisions on antecedents for pronouns in examples like  ( 1 ) and ( 2 ) locally , on a pronoun-by-pronoun basis , and that they do not choose among global analyses  . 5 In contrast , the conjuncts of ( 3a ) clearly do not correspond one-to-one with processing decisions  . Section 4 discusses an analysis of ( 3 ) whose components may correspond to local decisions on attachment sites  . 
3 . Encodings with non-atomic propositional constants It is possible to get a cubic length analysis of  ( 3 ) by introducing constants t br non-atomic propositions  . For m < k , let r . bevPk_l . k_2) . K , m(Pk-Im+lVP k-Im+2v . . . 
Then (3-a-k , m ) is equ3) alent to : ' (3-b-k , m ) ( PkmDPm+lm ) & ( Pk , ~ nD(Pm+2 , mvPm+2 , m + t )) & ( PkmD(Pk2m Vrk . 1m )) ( PklmD(Pk~l' , mv , ' k , n ~)) Of course , the space required to define the rkm must figure in the space required to encode an analys\[s of  ( 3 ) along the lines of ( 3-b-k , m ) . rk,m_l -= ( Pk-l , mvrk,m ), so 4 . ( Pl~(P2v . . . vpj )) is equivalent to (-' PlvP2v . . . vpj ), SO that (3a ) is in CNF . 
5 . This is not to suggest that people produce an exhaustive analysis in CNF prior to choosing a reading  . The hypothesis rather that fragments of a CNF representation are produced  ( in some sense ) during processing . 
it requires quadratic space to define all the rk . m . A revised version of (3) with (3-b-k , m ) in place of (3-a-k , m ) throughout requires cubic space .   6 Tree representations of single readings for examples like  ( 3 ) may be viewed as follows : edges correspond to atomic propositions that comprise specifications like " constituent i attaches to constituent j " or " constituent i projects to constituent j  . , , 7 A nonterminal node A corresponds to a constituent  , but also corresponds to the conjunction of the atomic propositions that correspond to edges that A dominates  . Thus the root node of the tree corresponds to a proposition that comprises a full specification of constituent structure  . 
The situation is essentially the same \[' or shared forests  .   ( \[ Tomita 87\] discusses shared forests and packed shared forests  . ) Edges ill shared forests correspond to atomic propositions  , and nonterminal nodes correspond to non-atomic propositions  . To extend this perspective , shared forests compress the information in nonshared forests by exploiting the introduction of constants for non-atomic propositions  . 
In a shared forest , the subtree that a node dominates is written only once  . In effect , then , a constant is introduced that represents the conjunction that corresponds to the node  . This constant is a constituent of the fornmlas that correspond to superior nodes  . 
While shared forests are more compressed than unshared forests  , the number of nodes in the shared forest representation of  ( 3 ) is still exponential in the length of ( 3 )  . 
In a packed shared forest , a packed node that does not dominate any packed nodes corresponds to a disjunction of conjunctions of atomic propositions  . 
Packed nodes that dominate other packed nodes correspond to disjunctions of conjunctions of atomic and non-atomic propositions  . In effect , for each node ( packed or non-packed ) , a constant is introduced that abbreviates the formula that corresponds to the node  . 
Exploitation of constants for non-atomic proposition spemfits more significant compression for packed shared forests than for shared forests  . The packed root node of a packed shared forest for  ( 3 ) cot x esponds to a disjunction of conjunctions whose size in atoms is exponential in the length of  ( 3 )  . However , the number of nodes of a packed shared forest for  ( 3 ) goes up as the square of the length of ( 3 )  . The number of edges of the packed shared forest ( a more authentic measure of the size of the forest  ) goes up as the cube of the length . 
6 . Further compression is possible if we allow quantification over subscript indices  . However , quantification over artifacts of representation may uncontroversially involve crossing the divide between analysis and procedure  . 
7 . Details of constituent sructure are not relevant to the discussion here  . For example , we will not distinguish " X attaches to V " from " X attaches to VP  . " 4 . Encodings that introduce structural constants A linear length encoding of an analysis of  ( 1 ) is possible if we use the constant A = John , Bill .   .   .   .   . Harry in the encoding as follows: ( l-c ) q & ( antecedent ( pronoun l ) eA )  &  ( antecedent ( pronoun 2 ) eA )  &  ( antecedent ( pronoun m ) eA ) Note that " x~Y " is shorthand for the disjunction of the statements " x = y  , " where y ranges over Y , so that ( l-c ) is not very different from ( l-b )  . Examples below involvet Yeer use of constants that correspond to sets of linguistic entities  , I will call such constants " structural . "A linear analysis of ( 2 ) is possible if we introduce constants A1 .   .   .   .   . A , where A . = John , Bill , A ;= A1u Tom , A3 = A2 ~ Fred ,   . . l . , Am = Am-I U  Jim : ( 2b ) q & ( matecedent ( pronount ) EA 1 )  &  ( antecedent ( pronoun2 ) ~ A2 ) & quantifier Qi takes scope over Qi-I to its immediate left  , then the quantifier Qi+l to the immediate rate right of Qi cannot take scope over Qi "  ( See\[Epstein 88\] for a discussion of relative operator scope . ) It follows that the number of relative operator scope readings for  ( 4 ) grows as the Fibonacci of the length of ( 4 )  . 8 However , a linear encoding of an exhaustive analysis of ( 4 ) is as follows: ( 4a ) q &\[ ( ( Q1 > Q2 )  &  ( L \] = Q2 ) ) v ( ( Q2 > Q1 )  &  ( L1=T ) )\[ & \[ Q1 > Q3 \] & \[ ( ( L 1 > Q3 )  &  ( L2 = Q3 ) ) v ( ( Q3 > L1 )  &  ( L2=T ) )\] & \[ Qk-2 > Qk+l \] & \[ Qk-t > Qk+l \] & \[ ( ( Lk_ 1 > Qk+l )  &  ( Lk = Qk + l ) ) v ( ( Qk+l > Lk-1 )  &  ( Lk = T ) )\] &  ( antecedent ( pronoun m ) EAm ) Because A . can be defined in terms of Ai_l , only linear convenient omix definitions of constants with other aspects of the encoding of  ( 2 )  , as follows : (2c)q&Aa = John , Bill & ( antecedent(pronounl ) ~ Ai &
A2 = ( A1u Tom ) ) &  ( antecedent ( pronoun2 ) EA2 &
A3 = ( A2 uF red )) &
He req represents aspects of the analysis of ( 4 ) aside from the specification of relative operator scope  , and Qi represents the ith quantifier in (4) , reading from the left . 
The L . are introduced constants corresponding to deeply embedded quantifier  . " Q > Q . " means that Q . i has higher scope than Qj . For all Q , ''~< T " is true and " Q > T " is false . 9Note that if we delete from ( 4a ) propositions that assign values to introduced constants  , such as "( Ll = Q2) , " the resulting statement is in CNF . 
Section 3 discussed cubic length analyses of ( 3 ) with propositional constants .   ( 3 ) has a linear analysis with structural constants as follows:  ( antecedent ( pronoun . ) ~ A .  &
I 11-211 11-1
Am = ( Am . 1u Jim ) ) &  ( antecedent ( pronoun m ) ~ Am ) For ( 2 )  , the introduction of structural constants permits a linear encoding  . For the following example , the introduction of structural constants likewise permits a linear encoding :  ( 4 ) Many teachers expect several students to expect many teachers to expect several students to  . . . to expect many teachers to expect several students to read some book  . 
Each quantifier in ( 4 ) can take scope over the quantifier to its immediate left  ( if any )  , and can take scope over the quantifier to its immediate right  ( if any )  . However , if a 8 . The most deeply embedded clause in ( 4 ) has 2 possible relative scope readings . The second most deeply embedded clans c in ( 4 ) has 3 possible relative scope readings ( many > several > some , many > some > several several > many > some ) . Let S . be the kth .   .   .   . kmost deeply embedded clausem (4) . ( Skts immediately embedded in S . . ; ) ' Given that ' S_ has a total ofn ( relative operator ) scope read ~ l~s , and that S~t as a total of m scope readings then the ? k-!""' subject of S  . can take scope over all the quantifiers in S?k +\[   ,   . k ' accounting f6rnglobal readings over S . ., . Alternatively , the subject of S can take scope over the subj\[~ of SThen both  . kk + . l " these subjects take scope over all the qu ' mtifiersmS  . The k-l . second alternative thus accounts form additional global readings over Sk +  1  . 
9 . (4a ) does not explicitly state , for example that Q>Q . but this t . 3' fact can be derived from ( 4a ) through apph cat mn of the transitivity of relative operator scope ? Generally speaking  , linguistic representations don't explicitly include all their consequences  . 
4 111  ( 3c ) q &\[ ( ap ( PP1 ) = NP )  &  ( AP1 = NP )  &  ( RE1 = NP , PP1 ) I &\[ ( ap ( PP2 ) eREI )  &  ( AP2 = ap ( PP2 ) ) &  ( RE2 = ( RE1 q"AP2 ) uPP2 ) \] &  ( 5- a ) q &\[ ( ap ( PP t ) eVP , NP ) &( AP t = ap(PP1))&(RE~=VP , NPTAP ~ )  &  ( OG1 = VP-API ) )\] & \[ ( ap ( PP2 ) EREI )  &  ( AP2 = ap ( PP2 ) ) &  ( RE2 = ( REl $ AP2 ) uPP2 )  &  ( OG2 = OGt-AP2 ) \] & \[ ( ap ( PPk ) ~ REk .  1 )  &  ( APk = ap ( PPk )  &  ( RE k = ( REk_1 1"APk ) uPPk ) \]& Here q represents aspects of the analysis of ( 3 ) aside from the specification of attachment points for the prepositional phrases  . The desired solutions consist of specifications of attachment possibilities  , stated in the form " ap ( PPk ) eX " ( " attachment point of the kth PP is one of the elements of X '  ) in ( 3 ~ c )  . The APk and REK are introduced constants . APk is the attachment point ot 10 o PP k " RE k represents the right edge of a constituent structure tree for the string consisting of the block and the first kPP's  . (3c ) is in a sort of relaxed CNF , as discussed above in connection with (4a ) , and in Section 5 below . " T " in ( 3c ) is defined so that RETAP = APuX ~ REIX precedes AP  . ( When PPk to the right of PP . attaches above PP . , PP . is not in the right edge of 11 l . 
the resulting structure , and is unavadable for attachment by material to the right of PPk  . ) As for (3) , the number of readings of the following exmnple ( from\[Church and Patil82\] ) grows as the Catalan of the number of prepositional phrases :  ( 5 ) Put the block in the box on the table . . . in the kitchen . 
However , there is an important difference between ( 3 ) and ( 5 )  . In (3) , any number of PP's can attach to the block , any number of PP's can attach to the box , and so on , No NP in (3) requires complements . ( Dr the box must attach to the block , but only because the block is the only NP that lies to the left of in the box  . ) In (5) , on the other hand , put requires on eNP argument and one PP argument , and cannot accept any other complements . 11 An analysis of ( 5 ) along the lines of ( 3c ) would incorrectly include readings where more than one PP attaches to put  , and readings where no PP attaches to put . A linear analysis of (5) is as follows : 10 . " PP attaches to PP " really means that PP attaches to the object of i  .   . k . i .   . the preposmonhead of PP . Thts usage permits a brtefer ? k . discussion than would otherwise be possible . 
1 l . This characterization of put is not strictly speaking correct  , but the necessary qualifications are irrelevant to the discussion here  . 
\[ ( ap ( PPk ) ERE k_l )  &  ( APk = ap ( PPk )  &  ( RE k = ( REk . 1TAPk ) L . ) PPk ) & ( OGk = OGk . 1-APk )\] &\[( ap(PPn ) EOG n_l\[\]REn . 1)\](5-a ) is similar to (3c) , but includes the additional constants OGOG is the open  ( the ta- ) gnd for the k " k substructure corresponding to put the block followed by the first kPP's  . OGk is either VP , if none of the first kPP's is attached to V , or is empty Non-empty OG"k indicates that for each constituent X in OG  . , some PP . ,  .   . K1 k < i ~ n , must attach to X . \[\] in ( 5- a ) is defined so that A\[\]B is equal to A if A is nonempty  , and is otherwise equal to B . The final conjunct in ( 5- a ) captures the requirement that if none of the first  n1 PP's attaches to put , then the final PP must attach to this verb . 
5. Issues
The example constructions presented above illustrate a variety of abstract cases  . In (1) , local ambiguities are independent of each other . The assignment of an antecedentoa pronoun in ( 1 ) does not affect the possibilities of antecedent assignment for other pronouns in  ( 1 )  . An analysis of ( 1 ) in CNF need not include more than one appearance of any literal  . (2) is similar to (1) in this respect . In (4) , local ambiguities are interdependent , but local ambiguity possibilities depend only on ambiguity possibilities in neighboring clauses  . There is thus a bound on how many ambiguities can interact  . In (3) , on the other hand , there is no such bound . Choosing an attachment site for PP . 
.  .   .   . J affects the attachment posslbdlttes for PP . . . . . no matter how large k is . (5) is similar to (3) , but at s * ~ involves a global filter associated with the verb put  .   ( 3c ) and ( 5- a ) employ a richer repertory of operators on structural constants  ( - ,   , q ') than is found in ( l-c ) , (2b ) , (2c ) , and (4a ) . 
( l-b ) may qualify relatively easily as an exhaustive analysis of  ( 1 )  , according to a common conception of what constitutes an analysis  . (3c ) , on the other hand , appears to have some of the ealx narks of a procedure  . 
The similarity of introduced constants to local variables is obvious  . In particular , the constants AP ; and RE ; of 112c5 ( 3c ) conld be replaced with two local variables AP and RE that receive destructive assignment  . (3--c ) also en '@ oys the operators "$" and "~" , which might be regarded as corresponding to procedures  . Whether ( 3--c ) is an analysis or a procedure for computing analyses is ultimately a matter of selecting a definition for " linguistic analysis  . " Criteria Iot a successful definition of " linguistic analysis " might appeal to psychological reality  . One possible requirement is that components of analyses correspond to partial analyses built during human processing  . When definitions of constants ( assignments to local variables ) are blended into what is otherwise a CNF formula , as in (2c ) , (3c ) , (4a ) , and (5-a ) , the result might be called " relaxed CNF . " Somewhat more precisely , suppose that a formula in " relaxed CNF " is a conjunction of " relaxe disjunctions  , " where a " relaxed disjunction " is the conjunction of a " generalized disjunction " with an " assignment formula  . " A " generalized disjunction " rnay be either a disjunction of atoms  , or a statement of the form " xcA . " An " assignment formula " is a conjunction of statement shat assign values to constants  . Given such a relaxed CNF formula as an exhaustive analysis  , obtaining an analysis of a single reading requires for ' each generalized disjunction the choice of a disjunct or the selection of an element  . Such single--reading analyses may be produced by deterministic variants of non-detemfinistic processing models that produce exhaustive analyses in relaxed CNF  . Relaxed CNF is compatible with a variety of processing models  . For example , a component of the form " xeA " might be produced before components that specify the corrtents of A  . 
Recent work on Kolmogorov complexity might provide alternative criteria for the definition of " linguistic analysis  . "  ( \[Li and Vitffnyi 89\] is a recent surveyol ' work on Kolmogorov complexity  . ) In particular , notions of time-bounded algorithmic complexity , such as the " logical depth " of \[ Bennett88\] , may be relevant . Following a third alternative , a satisfactory definition of " analysis " may involve a correspondence principle  , along lhelollowing lines : N 13" it every component of al % alanalysis specifically mentions one or more components of the input  . For this to work , " component " and " mention " themselves require appropriate definitions  . Arbitrary fragments of analyses cannot count as " components  . " Mention " should be transitive . 
Analyses in relaxed CNF may be more compatible with principle-based grammars than are tree-based analyses  .   ( \ [ Chomsky 81\] is the seminal work on principle-based grammars . ) Constituent structure does not occupy as central a place in the principle obased paradigm as in other ' grammatical paradigms  . Each generalize disjunction ( or its deterministic counterpart ) supplies a piece of information about tire analyzed expression  . Assignment formulas capture logical dependencies among thcse pieces  . Each of the examples in this oar ~ erillustrates a sinele N lenomenon  . Relaxed CNF can also capture interactions among ptmn~mmr  ~ a  . 
Analyses like ( 3c ) are probably less easily readable than packed shared forests  . Full analyses that specify constituent structure information together with relative operator scope information  , information on anaphora , and so on , will be even less readable . It may be possible to devise a more graphically oriented notation for linear encodings of linguistic analyses  . 
Wlmtever cozrception of " linguistic analysis " may ultimately prove most useful  , it seems clear that working with relaxed Conjunctive Norrnal Form has advantages over working with Disjunctive Normal Formlor computational pplications  . Relaxed CNF also appears to have advantages over DNF for psycholinguistic modeling  . Introduced constants ( local variables ) have obvious utility in implementations . They may also play a role in human processing of language  . In particular , as human processing proceeds , explicit details of previously encountered structure may recede into the background yet remain accessible  . 
Acknowledgments 1 am in debted for comments and discussions to Steven Abney  , Yves Caseau , and Andrew Ogielski . 
Responsibility tbr errors is entirely mine.

E . Barton , R . Berwick , and E . Ristad , Computational Complexity and Natural Ixmguage , M_IT Press , 
Cmnbridge , Massachusetts , 1987.
C . Bennett , " Logical Depth and Physical Complexity , " in R . Herken ( ed . ) , The Universal J ~ ring Machine ; A Half-Century Survey , pp . 227-258, Oxford University
Press , Oxford , 1988.
N . Chomsky , Lectures on Government and Binding,
Foris Publications , Dordrecht , 1981.
K . Church and R . Patil , " Coping with Syntactic Ambiguity or How to Put the Block in the Box on the Table  , " American Journal of Computational Linguistics ,  8:3-4 , pp .  139-:149, 1982 . 
S . Epstein , " Principle-Based Interpretation of Natural Language Quantifiers  , " Proceedings of the Seventh National Conference on Artificial Intelligence  ( AAAI-88 )  , pp .  718~723, 1988 . 
M . Garey and D . Johnson , Computers and Intractability,
W.H . Freeman , San Francisco , 1979.
M . Li and P . Vitgnyi , Kolmogorov Complexity and Its Applications ( Revised Version )  , Report CS-R8901 , Centretor Mathematics and Computer Science , 
Amsterdam , 1989.
M . Tomita , " An Efficient Augmented-Context-Free Parsing Algorithm  , " Computational Linguistics ,  13:1-2 , pp .  3146, 1987 . 

