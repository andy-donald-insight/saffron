A Probabilistic Approach to Compound Noun Indexing in 
Korean Texts
Hyouk R . Park and Young S . Han and Kang H . Lee
Korea R&D Information Center/KIST
P . O . Box 122 Yu Song Taejon ,  305-600 , Koreahrpark , yshan , khlee@stiss bs . kordic . re . kr
Key-Sun Choi
Computer Science Department KAIST
Yu Song Taejon ,  305-701 , Koreakschoi ( O ) world , kaist , ac . kr
Abstract
In this paper we address the problem of compound noun indexing that is about segmenting or decomposing compound nouns into promising index terms  . Compound nouns as index terms that usually subscribe to specific notions tend to increase the precision of retrieval performance  . The use of the component nouns of a compound noun as index terms  , on the other hand , may improve the recall performance , but can decrease the precision . 
Our proposed method to handle compound nouns with a goal to increase the recall while preserving the precision computes the relevance of the component nouns of a compound noun to the document content by comparing the document sets that are supported by the component nouns and the terms of the document  . The operational content of a term is represented as the probabilistic distribution of the term over the document set  . 
Experiments with a set of 1 , 0 00 documents show that our method gains 33% increase of retrieval performance compared to the indexing method without compound noun analysis  , and is as good as manual decomposition by human experts  . 
1 Introduction
Automatic indexing renders a form of document representation that visualizes the content of the document more explicitly  . Indices that are carefully chosen to represent a document will bring about the improvement of retrieval performance in accuracy and time efficiency  . The potential of a candidate index is often judged on the basis of its discriminating power over a docmnent set as well as its linguistic significance in the document  . 
Thus , a good index term should distinguish a certain class of documents from the rest of the documents and be relevant o the subject matters of the class of documents to be indexed by the term  . 
In general , automatic indexing consists of the identification of index terms and the assignment of weights to the terms  ( Salton 1983 )  . 
An index term can be either a simple noun or a compound noun composed of more than one simple nouns  . Compound nouns tend to carry more specific contextual information than simple nouns  , thus they are likely to contribute to the retrieval precision  . Compound nouns may contain useful simple nouns that usually refer general contexts  , and thus will boost the recall of retrieval . Processing compound nouns is decomposing them into simple nonns and evaluating the simple nouns as potential index terms  . In both identifying and evaluating index terms , compound nouns require a different strategy from that for simple nouns  . 
The identification of compound nouns involves a certain degree of linguistic or statistical analysis that varies from simple stemming to morphological analysis  ( Fagan 1989 )  . 
What makes it even more complicated to handle compound nouns in Korean documents lies in the convention of writing compound nouns  . In Korean , it is allowed to write compound nouns with or without intervening blanks between constituent nouns  . Arbitrarily long compound nouns are possible and not rare in real texts  . The decomposition of a compound noun is particularly problematic because of the severe ambiguity of segmentations  . 
In this paper , we propose a method to identify and evaluate the candidate index terms from compound nouns  . First , each possible decomposition of a compound noun is identified  . To'see the potential of the component nouns of the decomposition  , we observe how the component nouns are distributed over the total document set  , and of the current document are distributed over the same document set  . The similarity of the two distributions implies how consistently the two term sets will behave given a query at retriew d time  . 
The proposed method assumes a dictionary of nouns that is automatically constructed from the document set  .   3'his is the practice that has never been tried in Korean document indexing  , but has some important merits . A laborious work for the manual construction of nominal dictionaries is not needed  . Since the noun dictionary contains only those in a document set  , the ambiguity in analyzing words is greatly reduced  . 
Previous researches on the problem of compound noun indexing in Korean have been done in two directions  . One approach adopts a fullscale morphological nalys is to decompose a word into a sequence of the smallest morpheme units that are all treated as index terms  . The other approach tries to avoid the complexity of the full scale analysis by using bigrams as in  ( Fnjii 199'3 ; l , ee 11996; Ogawa 1993) . Since these methods take all the components of compound nouns as index terms without evaluation  , irrelewm terms can decrease retrieval precision . 
Experiments on 1000 document show that our evaluation scheme gave results closet " to the human intuition and maintained the highest precision ratio of tile existing methods  . 
In the following section , a brief review of related work on automatic indexing for Korean doc-nments is made  . Section 3 explainstile proposed method in detail . The verification of the method through experiments is described in section  4  . 
Section 5 concludes the paper.
2 Related Work
The previous approaches to compound noun indexing are based either on fullscale morphological analysis  ( Kang : 1995 ; Kim 1983 ; Lee 1995 ; Seo 1993 ) or on the syllabic patterns ( Fujii 1993 ; Lee 1996 ; Ogawa 1993) . Morphological analysis will return morphologically valid component words constituting a given compound word  . Since this method does not exclude invalid or meaningless words  , it can result in the degradatiou of precision . Besides the employment of full morphological analysis is often too expensive and requires costly maintenance  . 
Simpler method segment componnd nouns me -chanically into unigram or bigram words that are all regarded as index terms  ( Lee 1996 )  . Bigram indexes shows better precision than unigrams  , but can suffer from big index size . In general , the existing methods for compound noun analysis have been focused mainly on recall performance with little attention to the precision  . The work presented in this paper t ' ries to achieve the improvement of recall without the deterioration of preci-documents 
Dictionary a document making 7?" epiz!ng .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 
Compound noun . . . . . . .
-idictionaries : single nouns .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . : compound nouns
Index weighting l weighted indices
Figure 1: Compound noun indexing.

3 Probabilistic Compound Noun
Indexing
In this section , we describe the algorithm to recognize and evaluate candidate index terms from compound nouns  . Figure 1 summarizes the algorithm . The tokenizer produces a list of simple and compound nouns by utilizing the noun dictionary and the basic stermning rules  . The noun dictionary is used to identify whether a noun is simple or compound  , and the basic stemming rules are used to differentiate nonfinal words from others such as function words and verbs  . The noun dictionary is automatically constructed from the obser-w ~ tion on the document set  . The compound noun analyzer in w : stigates if the components of compound nouns are appropriate as indexes  . The index terrns that include simple nouns prodnced as a result of compound noun analysis are weighted  , which finishes the indexing . 
l , e t ? ' and C denote the sets of simple and compound nouns  , respectively . Simple nouns are , by definition , those that do not have any of their substrings as a noun according to the dictionary  . 
Compound nouns are those one or more substrings of which are recognized as nouns  . Let T = 7' l , 7~, . . . , 5/~ = EUC be the set of all simple and compound nouns of a document set  . 
Also , let I ) = D1,Du , .   .   . , Dg  be the set of all documents . A document is represented as a list of term -weight  ( 2~ , Wi ) pairs . 
For a compound noun Ci of a document , a de-lit in any cases , there are there than one de (' oll-position , but only a few of the nl are sensible with respect otim conte  . xt of the document . In discreet use of the component nouns lnay bring about the improvement of reall  , but can lead to the significant decrease of i ) recision . In the following discussions , we dest ; ril ) e the details of the algorithm to select useful coinponent nouns from e Oliipound 

3.1 Dictionary ) uil duI)
It is very difficult to provide an \[ tsysteni with the suf\[icient list of \[ iOtlllS  . tlee a use the nominal so/itnll lnbertndgrow faster than other categories of words  , it is more elticient to halidle non-nominal words mamlally  . We consider buihl . 
ing noun dictionary by identifying the remaining string as a no/illai % ereliminating non -ll Oiliil-lal part of a word  . The non-nominals are verbs , adverbs , adjectives , prelixes , auds uf\[ixes . 
The words in non-nominal dictionaries do not include those that can also be used as not in s  , whi(;his not a probleni since unlike , in English , the lmllti-eategorial words in Korean telld to be invariant of meaning  . The non-nonlinal dictionar-ie saye made usually hymanual work  . 
Those recognized as non-nominal words but not as \ [ unction words are regarded as llOl  . lns . '\[' here can he multiple interpretations in segnient higaword due to the ambiguity of fi inction words as illustrated in the following examp\]e  . 
Wellcalo .   .   .   .  )  . we nealo ( reactor ) , wenca-t-lo ( with atom ) atom ? lNST 17 ~ UMI' ; NTAL One way to deal with the probleniis tonse timi  ) robability of each function word and choose the one with the highest vahie  . More accurate measure woukl be made using at l idden Marker Model that is about a stochastic process of fun  ( >-lion words . ' Fhe function words are ( : lassified into 32 groups according to their roles and position in sentences  . In particular , each segmentation of a word is evahtated as follows  . 
P ( GIG-,)P ( n ) l'(i l n )?
P ( CiICi-1 ) is the probability of tl~e function category of current word given the category of the previous word  . P ( n ) is the probability of candidate noun and l ' ( f l n ) is the prot ) ability erafun ( > tion word given the candidate noun . The best sequence of these segmentations for a sentence can be obtained  . The candidate nouns n of the best sequence are then ' added to the noun dictionary  . 
a . 2 Tokenizing and eol ni ) ound noun analysis Tokenizing aims at recognizing simple and compound nouns fronia text and report ing them as the the final index terms  . The method for di(-tio-nary making is also us (' . d for tokenizing . Since the dictionary making method gives a list  ; of candidate nouns , we only need to ctaeck if a candidate is a CO ml ) ound noun and judge if the eompotl ( mtsel " the candidate compound noun e ~ re consistent  , with the content of the deemneat ,  . 
To deal with the notion of consistency , we have to deiiue tilen waning of a term or a set o \[" terms  , It is a well recognized practice to regard the dis-criminating power of a terln as the value of the term  . The quality of the ( liseriniinating power is the distribution of the tel'ill over a document set  . 
We define the distribution of a terln as the Hleati  . .
ing of the ter in . Similarly the meaning of a set of terms is the distribution of terms on the dec/anent set  . 
l , et M be the distribution of a term ~/ i ~ over a document set l  ) = I ) 1  .   .   . I ) . , ~ snch that

On (" dei inition of 54 (.) lnay be as follows.
f , .,: q('/;,:)j):)5)-=)_ZkDk)
For the case o\['multii ) le terms:
E-:i'l'he similarity t ) etween two Lel'lT is ( or sets o\["terms ) can be defined as any of vector similarity liieaslires  . The Iile as llrl ; ln ( Hlt of relative in fortua-lion of the two distril ) utions corresponding to the two tertns Rives the distauce between the distri-t  ) utions . Given two ( list ril ) utions Mi and k4 ) for ~ l and : 1 ) respectively , tim discrimination L ( ) is defined as follows ( la ; lahut ,  \[988) . 
I--tM~.
i = 0Mj
Since we want the dissimilarity between two dis~tributions  , divergence that is a symmetric version of diser imhiation is nlore appropriate for our case  . 
It is defined as follows ( Ilahut , 1988).
/;(M ~, MS ) = i . ( M ~ , a 45) +/; ( Ms , Md.
I , ' igure 2 il histrates the different distributions ( ) t ' terms over tile same docl i lnel l t set suggesting the usefulness of the distributions as the representation of timterms  . '\[' he divergence ~( . ) gives about tile itfforination ( uncertainty ) el ' the two dist . ributions as cornpared with each other , and \] las the following characteristics . 
* Timmore uniform the distril ) ution is , the larger L(-) will be . 
o'\['lielil OrO the two distributions agree , the less L ( . ) will he . 

D1 D2 D3 D4 . . . Dnl"iD lre2:ll histra . l , ioTio ~ ' tern\[disl , ributions over th ( , , S&TIIe (\] Oelllil("ltl , Sel ,  . 
Theeh ; : u'a , cl , erisi , ics are useful because good hi- . 
(\[ extcrtns should be less ilnil ' or nia , n(\[slt are sim-ihu'eoii , exts with other terTils in ad OCT Tlile Ut . 
\]1 lrids respect , , i\]l\]'Ol;illa , ; iOlil , heoreti('niea . sTire is it\[oreeollerete and l , \] u is possibly It\[ore&(W . ilrai , et\[ia . i i w . ) ellors iTnilarity Tilea stircs . 
l"ore~-~(:hde(:olnposition('/i ,  ' "  , ' l )) o\['a (: OtTT-l ) OTU id IIOITTICk , whal , we want 1 , osee is how dir-feren , l , he deconiposed terTns and t , he do CTil Ue Ul , i , (; T ' TTtSa . , ' e . Thai , is,/,(rl , .  -  . , ; l ), Ds <) I . x'o~ . cstile score of the imrticulm ? deceiT\ [ position  . D?h ; tl , we select he . reisOIT (; decoll pOSitiOll wit , hth (: low--est diverg(;nce . Let , l;iug ~ van(\[r'denol , e a , t\[ee Olll-position and l ; he I ) esl , (leeoluposit , ionresl ) eetive/y , i7-~artTlTin /)( r , \]) k ) . 
T'l'i ~ efollowingS liiTii ~ tt'iZeSttwl)roc( ; (hlre o \[' ex-tra ( 'tingshnpleTiOlll\]S\['roilCOl\[tl ) ounctllOll ltS . 
i . I . ei iTO Vei Ton-nonli Tta . \[ words us higtile tTTel , i lod for die tio T lary Ilial , :itTg . 
2 . ldcni , if ycO\[ill ) el\[lidii Oliiisll Sillgli Oitt-ht Mdiel , iona . ry . 
3 . For ( xtelt(t(~conlt)osit,onmio1"a . COl\[l-pound IIOTIII(\]i , colnpul , e\]\](rf , D ) . 
4. Select , " ~ i with the lowest L(ri , l)).
3.3 hl ( loxweighting
There are three wellknown liter , hods\[()1'weighl , -ingii T de?l:erius . ' l ' liey are based oit the infor--T/l ~ tiO/l of ill verse  ( \[ OCtlltteiitfre ( tuency , (\] is crilii liiht ~ t-l , i on w th i ( ; , a . ndl ) rol ) abilisi , cvMue(Sail , on 1) 88) . 
It , turned ( ) t i t i , \] iati ; hese\[no\[hods lead to similar per\['or Tn~mcc , bTll ~ inverse do cunient frequency is by fax\[l ieshnplest of \[\[ toniiitl  ; ei ' u is of l ; hne(x)i )) ple?il , y ;- Hidr ( . ' ( iuire(lresollFt:(s(Sa,lt,()li1)8 87
I\[arT~i3Atn19<(J7).
\] llverse(\]O (: lTittelll , t'req Ttelleyltietho disal SO shown to work with little t  ) erl ' or rnanc (  ; varbtl , iona , cross(\]\]l'ercnl , (\] onl <' . -tins . Fortllisr ( ~ . IISOTT > we ~ MO l ) tCd inverse ( \] o ( -ulnent fre ( luency hi Liiecxi ) er-in mll LS . \[ t is defined as follows . 
,,,, q ---, ~ J : ~. j ? log(~/)
Tnl ) leI : The prol ) ortion of COiTiltOIT itd ii OiillSinl , he 1000 sciett cea , I ) st , ra('t . At ) oTil ,, ()% of lie\[illSaA'e
COITII)O1TIIdTtOIITIS.
ITO . O~tX . ) IIIt)OII(tlIt ; Slt()TllLqltrOl ) ortion .   .   .   .   .   .   . T-4~)63990 . 55 ~-2 4665 8,50 % 3 469  . 85 % 4 53  . 09 % 5 6  . 01 (7 o where wij is l , he weight , of ' i ; hci'l . hLer Tt liu the . i ' l . hdoelll Teill , , ~* . 7 is the \] llTi iii ) ero\["oc('llrreTl(x ; s(if'l , hei'l , hl , erlnhil , hej'th(toeuliTenl , , and dfi is the liliTiibcr of dOCTITIielTI , Shi which thei'i , lil , ernT
OCCT lrD4 l , \] xperiments'l'h cgoalo\[experilu < its is to vali & ~ t  , e the proposed algoril , hnlfora . na . lyzing compo . udnou . s by conp ; u ' ingil , with the mmmala . nalysis and l , hebigranllnel , hod . 
Thel , esl , dal , a set consists of 1000 science al ) stra (: ~ , swritl . en in Kore ~ ul ( Kitu1 . 99 d ) . Allnomi Nalsnix > manually\[aleut\[fled and eo in poulidl to ill is were deconq  ) oscd into ~ q ) proprinte simple nouns by & t\[expert in ( lexcr . In the iirst (; xp(;rit . ent , our proposed Mgoril , hu is asked to do t , \] lcsa . nmtiring over the test ( lnta . , and retri (; wdperl ' or I muwes ou 1 . he two ttitf (; reut , ou L('om(:s(m~m unllyimlexed and aul ; olual ; icall yiudexedal ) stracl ; s ) are (' ompare c\[ . lIT t , lwS , eCOTT(\[exl)erinl(?lH , S , the l ) er-f ' or ma . nc ( ~ so \ [" the proposed m ( 'tho ( l and t ) igram T ttetb . o(Ia . reeoutl ); u : e(I toolo serve how Lit (; preci . 
sion is all ' eel , cal.
As is show u at t ~ d ~ lel , the portion o\[' ( :Ollll ) Otltidit Olll/SiSat ) oIIt ; 9 ~/) O\["I ; OI , MI\[OIlIIS\['()lindillI;\[TCLestset , , but . (: mlTTtM ? C criticale il >(' l , sontile retriewd\[)er for lila . tleebec & llSeoN ; ell COIIT\[ ) OtlII ( IJIOIIIIS e&rryiT\[gnloresl ) eeili ( " information become t ,  . morea , e-(:llT ' ~ l , t(':ill(it ; ,* ( too\[ . hed OCI llltC'lttS . 
Figure 3 and ' l'~t ble 2 summarize the perforumnce of the indexing me . t . hods:mauu MnnMys is , tim propose ( Ii ) rolmbilisl , ie method , and the bigr ~ miutet\[lod . <\[' lit ; proposed mel , ho ( l showed a slightly bct ; terpeT ' f ( )riilatlee ( around 3% )  - 4 ( ~ J ) them nlmnlM indexing or bigr~mitilde?trig . However , otir method lifts wa . sil\]oree\[lieient thant)igi'a , lit indexing in l , errnso\['thellUli~bero\['inde ~ xLerlliS midti~e~werageiilllTii  ) er of retrieved do eu-ltietltSper ~ query . 
The ~ tverage anlbiguity of a colil poi/lidi loliltis  1  . 43, and this low anibiguity niustha . ree on-l ; ributed l , otile iiigli & grec nlent ratio of tile \] proposed indexing  ; method with lil & lill & lindexing . 
Timlow ~ mibiguil , yispari , ly~tl , ixibuted 1 ; othell O Tllidictionary that has 11o ii Tilleeessa + ryentries 57_7 
Recall Man . Prob . Big . NoAnal.
0 . 00 0 . 871 . 9 0 . 8579 0 . 8406 0 . 7957 0 . 10 0 . 7719 0 . 7587 0 . 7841 0 . 6455 0 . 20 0 . 7122 0 . 6981 0 . 6812 0 . 5894 0 . 30 0 . 5895 0 . 6312 0 . 5939 0 . 4931 0 . 40 0 . 5458 0 . 5854 0 . 5637 0 . 4103 0 . 50 0 . 4957 0 . 5287 0 . 5240 0 . 3646 0 . 60 0 . 4272 0 . 4438 0 . 4370 0 . 2844 0 . 70 0 . 3304 0 . 3665 0 . 3322 0 . 2311 0 . 80 0 . 2552 0 . 2876 0 . 2569 0 . 1695 0 . 90 0 . 2102 0 . 2280!0 . 2028 0 . 0900 1 . 00 0 . 1428 0 . 1724 0 . 1600 0 . 0514: Table 2: Performance of Manual , Prob . , and Bigram Indexing 0 . 9  , i ! i " Manual " ~--!" Proba ~ itlstic " -4--  . 
0 . 8 ~  .   .   .   .   .   .   .   .   .   .   .   .   . ~ i .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . i .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . "Btgram ~, o--?'~I .   .   .   . 
X ."'5,",~,.
0 0.2 0.4 0.6 0,8 i
Figure 3: Recall-Pree is on curve of indexing methods not found at the documents  . 
5 Conclusion
The compound analysis in automatic indexing aims at the improvement of recall performance by extracting useful component nouns fi'om compound nouns  . The task for Korean texts requires extra efforts due to the complexity of inflections  . 
The proposed method gives better potential of sustaining the precision while improving the recall than other approaches by making use of probabilistic distributions of terms as the representation of meaning of the terms  . 
The proposed method to evaluate the coinpo-nents of compound nouns is unique in that it de-lines and uses term representation  , which explains the superiority of the method to other methods  . 
The method requires little human involvement and is very promising for the implementation f practical systems by achieving efficiency and accuracy at the same time  . 
References
Blahut , Richard E .  (1987) . Principles and Practice of Information Theory . Addison-Wesley . 
Fagan , J . L .  (1989) . The effectiveness of a Nonsyn-tactic Approach to Automatic Phrase Indexing for Document Retrieval  , Journal of American Society for Information Science  , Vol . 40, No .  2 . 
iIarman , D .  (1992) . " Ranking Algorithms " in Information Retrieval : Data Slructure and Algorithms  , ( Frakes , W . B . , and Baeza-Yates , R . 
ed .) Prentice Hall.
Fujii , lI . , and Croft , W . B .  (1993) . "A comparison of indexing techniques for Japanese text retrieval  , " In Proceedings of 16'th ACM SIGIR

Kang , S . S .  (1995) . " Role of Morphological Analysis for Korean Automatic Indexing  , " , In Proceedings of the 22rid Korea Information Science
Society Conference.
Kim , Y . It .  (1983) . Automatic Indexing System of Korean . Texts mixed with Chinese and English M . S . Thesis , Dept . of Computer Science , Korea Advanced Institute of Science and Technology  . 
Kim , S . H .  (1994) . A Development of the ' l~st Collection for Estim ~ting the Retrieval Performance of an Automatic Indexer  , Journal of Korea Information Management Society , Vol .  11,
No . 1.
Lee , J . lI .  (1996) . " n-Gram-Based Indexing for Effective Retrieval of Korean Texts  , " In Proceedings of 1st Australian Document Computing Symposium 1996 Lee , It . A .  (1995) . " Implementation fan Indexing System Based on Korean Morpheme Structural Rules  , " , In Proceedings of Spring Conference of
Korea Information Science Society.
Ogawa Y .  (1993) . " Simple word strings as corn-pound keywords : An indexing and ranking method for Japanese texts  , " , In Proceedings of 16'th ACM SIGIR Conference . 
Salt ; on , G . , and McGill M . J .  (1983) . Introduction loModern Information Retrieval McGraw Hill 

Salton , G . , and Buckey , C .  (1988) . Term Weight--ing Approaches in Automatic Text P ~ etrieval  , lnformalion Processing 2~ and Management , 
Vol . 24, No . 5.
Seo , E . K .  (1993) . An Experiment in Automatic Indexing with Korean Texts : A Comparison of Syntactico-Statistical and Manual Methods  , Journal of Korea Information Managemenl Society , Vol . 10, No .  1 . 

