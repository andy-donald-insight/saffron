Proceedings of the 22nd International Conference on Computational Linguistics ( Coling 2008), pages 993?1000
Manchester , August 2008
Domain Adaptation for Statistical Machine Translation with Domain
Dictionary and Monolingual Corpora
Hua Wu , Haifeng Wang
Toshiba ( China ) R&D Center
Beijing , 100738, China
wuhua@rdc.toshiba.com.cn
wanghaifeng@rdc.toshiba.com.cn
Chengqing Zong
NLPR , Institute of Automation
Chinese Academy of Sciences
Beijing 100080, China
cqzong@nlpr.ia.ac.cn

Abstract tra
Statistical machine translation systems are usually trained on large amounts of bilingual text and monolingual text . In this paper , we propose a method to perform domain adaptation for statistical machine translation , where indomain bilingual corpora do not exist . This method first uses out-of-domain corpora to train a baseline system and then uses indomain translation dictionaries and indomain monolingual corpora to improve the indomain performance . We propose an algorithm to combine these different resources in a unified framework . Experimental results indicate that our method achieves absolute improvements of 8.16 and 3.36 BLEU scores on Chinese to English translation and English to French translation respectively , as compared with the baselines using only out-of-domain corpora.
1 Introduction
In statistical machine translation ( SMT ), the translation process is modeled to obtain the translation of the source sentence f by maximizing the following posterior probability ( Brown et al , 1993).
beste )()( maxarg )( maxarg | | ee fee fe e
LM best pp p = = (1)
State-of-the-art SMT systems are trained on large collections of bilingual corpora for the ? C 2008. Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unported license ( http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved.
nslation model )( | efp and monolingual target language corpora for the language model ( LM ) )( eLMp . The trained SMT systems are suitable for translating texts in the same domain as the training corpus . However , for some specific domains , it is difficult to obtain a bilingual corpus . In this case , the performance of SMT systems will be degraded.
Generally , it is easier to obtain indomain monolingual corpora in either source or target language . Moreover , in some specific domains , although indomain bilingual corpora do not exist , indomain translation dictionaries , which usually contain domain-specific terms and their translations , are available . And even if such dictionaries are not available , it is easier to compile one than to build a bilingual corpus . Thus , in this paper , we address the problem of domain-specific SMT , where only domain-specific dictionaries and/or monolingual corpora exist . In a specific domain , there are two kinds of words : common words , which also frequently occur in out-of-domain corpora , and domain-specific words , which only occur in the specific domain.
Thus , we can combine the out-of-domain bilingual corpus , the indomain translation dictionary , and monolingual corpora for indomain translation.
If an indomain translation dictionary is available , we combine it with the out-of-domain translation model to improve translation quality.
If an indomain target language corpus ( TLC ) is available , we use it to build an indomain language model , which can be combined with the out-of-domain language model to further improve translation quality . Moreover , if an indomain source language corpus ( SLC ) is available , we automatically translate it and obtain a synthetic indomain bilingual corpus . By adding this synthetic bilingual corpus to the training data , we rebuild the translation model to improve the indomain source language corpus with the improved model until no more improvement can be made . This is similar to transductive learning described in ( Ueffing et al , 2007).
We perform domain adaptation experiments on two tasks : one is the Chinese to English translation , using the test set released by the Internatio inese to English translation an
MT system lation model and language model adapta-in domain adaptation for del adaptation has been shared task focused on domain adaptation for machine translation among
Eu rpora . Adding the extracted bilingu the performance of a SMT system tra
Moses ( Koehn et al , 2007). In babilities , reorder-e model probabili-nal Workshop on Spoken Language Translation 2006 ( IWSLT 2006), and the other is the English to French translation , using the data released by the Second Workshop on Statistical Machine Translation ( WMT 2007) ( Callison-
Burch et al , 2007).
Experimental results indicate that our method achieves absolute improvements of 8.16 and 3.36
BLEU scores on Ch d English to French translation respectively , as compared with the baselines only using the out-of-domain corpora . The results on both translation tasks also show that the translation quality achieved by our methods is comparable to that of the method using both indomain and out-of-domain bilingual corpora . Moreover , even if indomain and out-of-domain bilingual corpora are available , adding an indomain dictionary also helps to improve the translation quality.
The remainder of the paper is organized as follows . In section 2, we describe the related work.
Section 3 briefly introduces the baseline used in our experiments . Section 4 describes our domain adaptation method of using indomain dictionary and monolingual corpora . And then we present the experimental results in sections 5. In the last section , we conclude this paper.
2 Related Work
Trans tion are usually used
SMT . Language mo widely used in speech recognition ( Bacchiani and Roark , 2003). In recent years , language model adaptation has also been studied for SMT ( Bulyko et al , 2007). They explored discriminative estimation of language model weights by directly optimizing machine translation performances such as BLEU score ( Papineni et al , 2002).
Their experiments indicated about 0.4 BLEU score improvement.
A shared task is organized as part of the Second Workshop on Statistical Machine Translation . A part of this ropean languages . Several studies investigated mixture model adaptation for both translation model and language model in SMT ( Civera and Juan , 2007; Foster and Kuhn , 2007). Koehn and Schroeder (2007) investigated different adaptation methods for SMT . Their experiments indicate an absolute improvement of more than 1
BLEU score.
To enlarge the indomain bilingual corpus , Munteanu and Marcu (2005) automatically extracted indomain bilingual sentence pairs from comparable co al corpus to the training data improved the performance of the MT system . In addition , Ueffing et al (2007) explored transductive learning for SMT , where source language corpora are used to train the models . They repeatedly translated source sentences from the development set and test set . Then the generated translations are used to improve the performance of the SMT system.
This kind of transductive learning can be seen as a means to adapt the SMT system to a new type of texts.
In this paper , we use an indomain translation dictionary and/or indomain monolingual corpora ( in both source language and target language ) to improve ined on the out-of-domain corpora . Thus , our method uses these resources , instead of an indomain bilingual corpus , to adapt a baseline system trained on the out-of-domain corpora to indomain texts.
3 Baseline MT System
The phrasebased SMT system used in our experiments is
Moses , phrase translation pro ing probabilities , and languag ties are combined in the loglinear model to obtain the best translation beste of the source sentence f : ? = ? =
M p | )( maxarg fee ebest (2) m mmh
The weights are set by a discriminative training method using a heldout data set as describ in ( Och , 2003). The models or features which are employed by the decoder are ( a ) one or several ph ed rases tables , ( b ) one or more language models trained with SRILM toolkit ( Stolcke , 2002), ( c ) distance-based and lexicalized distortion models , ( d ) word penalty , ( e ) phrase penalty.
994
Input Out-of-domain training data OL ary D Indomain translation diction I Indomain target language corpus IT ( optional ) Ind , where represents the general model.
e f
If e ing step : Translate with to get a synthetic bilingual corpus
Un
E
End
Outpu l for indomain translation omain source language corpus I ( optional ) S Begin Assign translation probabilities to ID
If IT is available
Training step : ( L Estimate = ),, IIO TD?
Els ?
Training step : ),( IO DL Estimate=?
End i S is availableI ? =)0( ?, 0=i
R peat 1+= ii
Label IS )1( ? i ? IL
Training step : ),, IIO til no more improvement can be achieved ()( i LDL estimate-Re =? )( i ? ?= nd if t Mode ? Figure 1.The domain adaptation algorithm 4 The Framework n about the algorithm is our algorithm , a phrase available , we train an indomain LM , which is co ilable , we use the built - linear m ). With the order to to assign prob-nary.
m 4.1 The Algorithm
The detailed informatio shown in Figure 1. In table and a language model are first constructed based on the out-of-domain corpus OL . Then probabilities are automatically assigned to the entries in the indomain translation dictionary ID , from which another phrase table is constructed . At last , the two phrase tables are com-d . This is the procedure of the training step ),( IO DL Estimate =? .
If an indomain target language corpus is bine mbined with the out-of-domain LM . The built phrase tables and LMs are integrated in the loglinear model as described in section 3. This is the procedure of ),,( IIO TDL Estimate =? .
Moreover , if an indomain source language corpus is ava log odel to translate the indomain source texts and obtain a synthetic bilingual corpus . And then we add the synthetic bilingual corpus into the training data to improve the current loglinear model improved model , we repeatedly translate the indomain source texts until no more improvement on a development set can be achieved.
4.2 Dictionary Probabilities
In general , there is no translation probability in a manually-made translation dictionary . In ( estima-Re =? ),,()( IIOi LDL te construct a phrase table , we have abilities to the entries in the dictio Uniform Translation Probability : Since we have no parallel corpus to estimate the translation probabilities , we simply assign uniform probabilities to the entries in the dictionary . With this ethod , if a source word has n translations , then we assign n1 to each translation of this phrase for all the four scores of the phrase pair.
Constant Translation Probability : For each entry in the dictionary , we as ign a fixed score.
In this case e sum of the translation probability is not necessarily equal to 1.
s , th anslation probabilities for the entries in the dictionary . And for the Corpus Translation Probability : If an indomain monolingual source corpus exists , we translate it with the method as described in Figure 1, and then estimate the tr s.
mmonly-used tries whose translation probabilities are not estimated , we assign average probabilities that are calculated from the entries that have obtained probabilities.
4.3 Combining Phrase Tables
In the algorithm , there are two kinds of phrase tables . We need to combine them to translate the indomain text
Mixture Model : The most co method is linear interpolation.
)()1()()( ||| fefefe oI ppp ?? ?+= (3)
W ion probabilities.
here )( | feIp and )( | feop are the indomain and out-of-domain translat ? is the interpolation weight.
Discriminative Model : An alternative is to two tables in the log-linea ranslation oses , for en uses them fo he out-of-domain lan-tigate two ation and two different tasks : one is the Chinese to English translation in IWSLT the other is the English to n adaptation translation in the red task.
airs , with about 3 millio combine the r model.
During t with M each phrase in the sentence , the decoder obtains all of its translations in both phrase tables , and th r translation expansion.
4.4 Combining Language Models
If an indomain target language corpus exists , we use it to construct an indomain language model , which is combined with t guage model . In this paper , we inves combination methods : linear interpol loglinear interpolation.
5 Experiments 5.1 Setting
We ran experiments on 2006 evaluation , and
French domai
WMT 2007 sha
For the Chinese to English translation task , we use the ChineseEnglish bilingual corpus provided by the Chinese Linguistic Data Consortium ( CLDC)2 as the out-of-domain corpus . It contains 156,840 sentence p n English words and about 5 million Chinese characters . In addition , we use the Basic Traveling Expression Corpus ( BTEC ) released by IWSLT 2006 ( Paul , 2006) to construct an indomain phrase table , as a comparison with that one constructed with the indomain dictionary.
2 http://www.chineseldc.org/EN/index.htm . The catalog number is CLDC-LAC-2003-004. It is a balanced corpus containing sentence pairs in multiple domains.
Corpora Sentences OOV
CLDC 156,840 89 (6.31%)
BTEC 39,953 179 (12.69%)
IWSLT06-dev4 489 NA
IWSLT06-test 500 NA
Table 1. Ch sh cor aries Entries inese-Engli pora
Diction OOV
LDC 82,090 228 (16.16%) indomain 32 .57%) ,821 121 (8
T s ces able 2. ChineseEnglish dictionarie
Corpora Senten OOV
Europarl 949,410 412 (5.90%)
NC 43,060 599 (8.58%)
WMT07 dev 1,057 NA
WMT07 test 2,007 NA
Table 3. E ch cor
I art and rt a sed a in m ual exp ta rts of CLDC and BTEC are used for language m construction ( see Tab uation , nglish-Fren pora ts source p target p g a re separately u ra in ours the in-doma e onolin corpo eriments . Th rget pa both odel le 1). From the IWSLT 2006 eval we choose the devset4 as our development data.
Evaluation was performed on IWSLT 2006 test set . The references for the test set contain lowercase words and punctuations . The detailed information is shown in Table 1.
We use two kinds of manually-made dictionaries for comparison : one is the LDC ChineseEnglish Translation Lexicon Version 3.0 ( LDC2002L27), and the other is the indomain spoken language dictionary made by ourselves , which contains indomain Chinese words and their English translations . The dictionary is manually constructed . Some entries of the dictionary are collected from phrase books . Some of them are collected from the general-domain dictionaries . And then , the entries are filtered and modified by a Chinese native speaker specialized in English . The detailed information is shown in Table 2. If a source word has two translations , it is counted as two entries . The OOV rates of the test set uncovered by the LDC dictionary and the indomain dictionary are 16.16% and 8.57%, respectively.
For the English to French translation task , the out-of-domain corpus is the Europarl corpus distributed for the shared task of WMT 2007 ( Calli-son-Burch et al , 2007)3. We filter the sentence pairs whose lengths are above 40 words . For the 3 http://www.statmt.org/wmt07/shared-task.html ( NC ) corpus distributed in WMT 2007. We also use the same development set and test set in the domain adaptation shared task ( see Table 3). We manually built an indomain English-French dictionary according to the indomain bilingual corpus , which includes 26,821 entries . It contains indomain English words and their French translations . The OOV rate of the test set uncovered by this dictionary is 22.34%.
5.2 Evaluation Measures
To perform phrasebased SMT , we use the rt training scripts.
efault settings and ary rase table . With the indomain translation dictionary , we construct indo lts , loglinear tra tionary into the training corpus . In th omain corpus to train a phrase table . Then we use both
Moses decoder and its suppo
We run the decoder with its d then use Moses ' implementation of minimum error rate training ( Och , 2003) to tune the feature weights on the development set . Translation quality was evaluated using BLEU score ( Papineni et al , 2002).
5.3 Results on ChineseEnglish Translation
Translation Diction
With the out-of-domain bilingual corpus , we train an out-of-domain ph main phrase tables by assigning different translation probabilities with two different methods : uniform and constant . For the constant translation probability , we set the score using the development set . In our experiments , we set it to 1. We use the target part of the out-of-domain corpus to train a language model4.
With two phrase tables , we combine them in a linear or loglinear method as described in section 4.3. In our experimental resu nslation models outperform the linear models (16.38 vs . 15.12), where the entries of the dictionary are assigned with the constant translation probabilities . Thus , we will use loglinear models for phrase table combination in the following experiments.
Another method to combine the out-of-domain corpus and the translation dictionary is to add the indomain dic is case , only one phrase table is trained.
Table 4 describes the results using the out-of-domain corpus and the indomain dictionary . The baseline method only uses the out-of-d 4 We also used LDC English Gigaword to train a large language model . However , this language model did not improve the translation quality.
Methods Resources Used BLEU (%) baseline out-of-domain corpus 13.59 + dictionary as corpus 15.52 + uniform prob . 16.00 + constant prob . 16.38 baseline + dictionary + corpus prob . 16.72 Table 4. Translation results of using out-of-d ictionary the out-of-dom the indom c5 . The also im bine it with the linear interpolation and loglinear interpolation . The expe oves th ifference betw omain corpus and indomain d ain corpus and ain di tionary . The results indicate that adding an indomain dictionary significantly improves the translation quality by 2.79 BLEU score methods using the dictionary as a phrase table outperform the method adding it to the training corpus . And the method using constant translation probabilities significantly outperforms that using the uniform translation probabilities.
For comparison , we also assign corpus probabilities to the entries in the dictionary by translating the source part of the BTEC corpus with the method described in Section 4.2. This proves the translation quality.
InDomain Monolingual Corpora
We use the target part of the BTEC corpus to train an indomain LM . We com out-of-domain LM in two methods : rimental results indicate that linear interpolation outperforms loglinear interpolation (17.16 vs . 16.20). Thus , we will use linear interpolation for LMs in all of the following experiments.
Table 5 describes the results of using the interpolated language model . As compared with the results in Table 4, it can be seen that adding the indomain language model greatly impr e translation quality . It achieves an absolute improvement of 3.57 BLEU score as compared with the baseline model . If the indomain translation dictionary is used , the translation quality is further improved by 4 BLEU score.
If the indomain source language data is available , we translate it and obtain a synthetic bilingual corpus . Then we perform transductive learning as described in Figure 1. The d een our method and that in ( Ueffing et al , 2007) is that we translate a larger indomain source corpus , and we use 1best translation 5 We use the method described in ( Koehn and Monz , 2006) for significance test . In this paper , significant improvement means method A outperforms method B on a significance level of no less than 95%.
997
Methods Models Resources used BLEU (%) baseline Model 1 out-of-domain corpus 13.59 baseline + TLC Model 2 + indomain TLC 17.16 Model 3 + indomain TLC + dictionary ( uniform prob .) 20.83 baseline + TLC dictionary ( constant prob .) + dictionary Model 4 + indomain TLC + 21.16
Model 5 + indomain SLC 15.98
Model 6 + indomain SLC and TLC 18.19 transductive learning nd TLC + dictionary ( corpus prob .) Model 7 + indomain SLC a 21.75
Table 5.
Diction BLEU(%)
Translation results of using indomain resources ary types Entries OOV general domain LDC 228 (1 %) 82,090 6.16 19.11 manual 121 ) 32,821 (8.57% 21.16 indomain extracted 11,765 330 (23.39%) 19.88 LD al C + manu 106,572 45 (3.19%) 21.34 combined LDC + extracted 95,660 202 (14.31%) 20.49
Table 6. Comparison of ictionar result with full retraining.
that transductive learning ed , the translatio tionaries with concern to the translation quality . Besides ld as described in ( Wu and
Wang , 2007).
phrase table , extract the their translations.
us when an indomain bim d different ies The results indicate improves translation ? From the filtered
Chinese words and quality in all cases . For example , Model 5 achieves an absolute improvement of 2.39 BLEU score over Model 1, and Model 6 achieves 1.03 BLEU score improvement over Model 2. Model 7 uses the indomain dictionary with corpus translation probabilities , which are obtained from the phrase table trained with the synthetic bilingual corpus . The results indicate that Model 7 outperforms Model 4, with a significant improvement of 0.59 BLEU score.
The results also indicate that when only the indomain monolingual corpus is us n quality is improved by 4.6 BLEU score ( Model 6 vs . Model 1). By adding the indomain dictionary , the translation quality is further improved , achieving an absolute improvement of 8.16 BLEU score ( Model 7 vs . Model 1).
Comparison of Different Dictionaries
We compare the effects of different dic the manually-made indomain dictionary , we use other two dictionaries : the LDC dictionary and an automatically built dictionary , which is extracted from the BTEC corpus . This extracted dictionary only contains Chinese words and their translations . The extraction method is as follows : ? Build a phrase table with the indomain bilingual corpus.
? Filter those phrase pairs whose values are below a thresho ? Assign constant translation probabilities to the entries of the extracted dictionary.
Table 6 shows the translation results . All of the methods use the out-of-domain corpus , the indomain target language corpus , and the corresponding translation dictionaries with constant translation probabilities . The results indicate that ing the general-domain dictionary also improves translation quality , achieving an improvement of about 2 BLEU score as compared with Model 2 in Table 5. It can also be seen that the indomain dictionaries significantly outperform the LDC dictionary although the extracted dictionary has a higher OOV rate than the LDC dictionary . Further analysis shows that the LDC dictionary does not contain the indomain translations of some words . Results also indicate that combining the two kinds of dictionaries helps to slightly improve translation quality since the
OOV rates are reduced.
Comparison with Indomain Bilingual Corpus The aim of this section is to investigate whether the indomain dictionary helps to improve translation quality lingual corpus is available . And we will also compare the translation results with those of the ethods only using indomain dictionaries and monolingual corpora.
To train the indomain translation model , we use the BTEC corpus . The translation results are 100k 200k 300k all
Indomain sentence pairs
BL
EU ( %)
CLDC
BTEC
CLDC+BTEC
CLDC+BTEC+Dic
CLDC+Mono+Dic
Figure 2. Comparison of different methods using different resources.
shown in Figure 2. CLDC and BTEC represent s linear interpolation of the
Interpolated LM " means that the methods that only use the out-of-domain and the indomain corpus , respectively . The method " CLDC+BTEC " use phrase tables and LMs trained with CLDC and BTEC . " Dic " means using the indomain dictionary , and " Mono " means using indomain source and target language corpora.
From the results , it can be seen that ( a ) even if an indomain bilingual corpus exists , the indomain dictionary also helps to improve the translation quality , as " CLDC+BTEC+Dic " achieves an improvement of about 1 BLEU score in comparison with " CLDC+BTEC "; ( b ) the method " CLDC+Mono+Dic ", which uses both the indomain monolingual corpora and the indomain dictionary , achieves high translation quality . It achieves slightly higher translation quality than " CLDC+BTEC " that uses the indomain bilingual corpus (21.75 vs . 21.62) and achieves slightly lower translation quality than " CLDC+BTEC+Dic " (21.75 vs . 22.05). But the differences are not significant . This indicates that our method using an indomain dictionary and indomain monolingual corpora is effective for domain adaptation.
5.4 Results on English-French Translation We perform the same experiments for English to French translation . Table 7 describes the domain adaptation results . " we use the target part of the NC corpus to train an indomain LM , and then linearly interpolate it with the out-of-domain LM trained with the Europarl corpus . The results indicate that using an indomain target corpus significantly improves the translation quality , achieving an improvement of 2.19 BLEU score ( from 25.44 to 27.63).
Methods Out-of-domain LM
Interpolated
LM
Europarl 25.44 27.63
Europarl+Dic 26.24 28.22 transductive learning - 2 8.80
Europarl+NC - 29.19
Europarl+NC+Dic - 29.41
T ation result f using i in d onolingual corpora Using the indomain translation dictionary im-used ( from 29.19 to 29.41).
n Table 7. The results indicate th 28.80). Although the translation quality is
This an out-of-domain corpus to ystem , and then used an in-ion dictionary , to improve the transla-able 7. Transl ictionary and m s o n-doma proves translation quality in all cases , even when the indomain bilingual corpus is We also perform transductive learning with the source part of the NC corpus . The model used to translate the corpus is that one created by " Europarl+Dic " i at transductive learning significantly improves translation quality , achieving an absolute improvement of 0.58 BLEU score ( from 28.22 to 28.80).
In summary , using an indomain dictionary and indomain monolingual corpora improves the translation quality by 3.36 BLEU score ( from 25.44 to slightly lower than that method of using both indomain and out-of-domain bilingual corpora , the difference is not statistically significant.
6 Conclusion
This paper proposed a domain adaptation approach for statistical machine translation.
approach first used build a baseline s domain translation dictionary and indomain monolingual corpora to adapt it to the indomain texts . The contribution of this paper lies in the following points : ? We proposed a method to integrate a do-main-specific translation dictionary into a phrasebased SMT system for domain adaptation.
? We investigated the way of using indomain monolingual corpora in either source or target language , together with the indomain translat tion quality of a baseline system.
999
We performed experiments on both Chinese to English and English to French translation . Experimental results on Chinese to English translatio - vised Language Model Adaptation . In Proc . of the ational Conference on Acoustics , Signal Processing ( ICASSP2003),
Br n . Computational Linguistics,
Bu f the 32nd International Confer-
Ca Statistical Ma-
Ci , pages 177-180.
Fo
Ko ation of Machine Translation be-
Ko erico , Nicola Bertoldi,
Ko istical Ma-
M
Oc ranslation . In Proc . of
Pa 2. BLEU : a Method for Auto-
Pa ation Campaign . In Proc . of the International
St Proc . of International
Ue rning for Statistical
W ics and Phrase-n indicate that all of the indomain resources are useful to improve indomain translation quality , with an overall improvement of 8.16 BLEU score as compared with the baseline trained with out-of-domain corpora . Results on English to French translation also show that using indomain translation dictionaries and indomain monolingual corpora is effective for domain adaptation , achieving an absolute improvement of 3.36 BLEU score . And the results on both translation tasks indicate that the translation quality achieved by our methods is comparable with that of the method using both indomain and out-of-domain bilingual corpora . Moreover , even if indomain and out-of-domain bilingual corpora are available , adding an indomain dictionary also helps to improve the translation quality.
In the future work , we will investigate to assign translation probabilities to the dictionaries using comparable indomain corpora and examine its effect on the MT performance . And we will also examine the effect of an indomain dictionary on transductive learning in more details.
References
Bacchiani , Michiel and Brian Roark . 2003. Unsuper 28th Intern
Speech , and pages 224-227.
own , Peter F ., Stephen A . Della Pietra , Vincent J.
Della Pietra , and Robert L . Mercer . 1993. The Mathematics of Statistical Machine Translation:
Parameter Estimatio 19(2): 263-311.
lyko , Ivan , Spyros Matsoukas , Richard Schwartz , Long Nguyen , and John Makhoul . 2007. Language Model Adaptation in Machine Translation from
Speech . In Proc . o ence on Acoustics , Speech , and Signal Processing ( ICASSP-2007), pages 117-120.
llison-Burch , Chris , Cameron Fordyce , Philipp Koehn , Christof Monz , and Josh Schroeder . 2007.
(Meta -) Evaluation of Machine Translation . In
Proc . of the Second Workshop on chine Translation , pages 136-158.
vera , Jorge and Alfons Juan . 2007. Domain Adaptation in Statistical Machine Translation with Mixture Modelling . In Proc . of the Second Workshop on Statistical Machine Translation ster , George and Roland Kuhn . 2007. Mixture-Model Adaptation for SMT . In Proc . of the Second Workshop on Statistical Machine Translation , pages 128-135.
ehn , Philipp and Christof Monz . 2006. Manual and
Automatic Evalu tween European Languages . In Proc . of the HLTNAACL 2006 Workshop on Statistical Machine
Translation , pages 102-121.
ehn , Philipp , Hieu Hoang , Alexanda Birch , Chris
Callison-Burch , Marcello Fed
Brooke Cowan , Wade Shen , Christine Moran , Richard Zens , Chris Dyer , Ondrej Bojar , Alexandra Constantin , and Evan Herbst . 2007. Moses : Open Source Toolkit for Statistical Machine Translation.
In Proc . of the 45th Annual Meeting of the Association for Computational Linguistics ( ACL2007), demonstration session , pages 177-180.
ehn , Philipp and Josh Schroeder . 2007. Experiments in Domain Adaptation for Stat chine Translation . In Proc . of the Second Workshop on Statistical Machine Translation , pages 224-227.
unteanu , Dragos Stefan , and Daniel Marcu . 2005.
Improving Machine Translation Performance by Exploiting Non-Parallel Corpora . Computational
Linguistics , 31(4): 477-504.
h , Franz Josef . 2003. Minimum Error Rate Training in Statistical Machine T the 41st Annual Meeting of the Association for Computational Linguistics ( ACL2003), pages 160-167.
pineni , Kishore , Salim Roukos , Todd Ward , and
WeiJing Zhu . 200 matic Evaluation of Machine Translation . In Proc.
of the 40th Annual Meeting of the Association of Computational Linguistics ( ACL2002), pages 311-318.
ul , Michael . 2006. Overview of the IWSLT 2006
Evalu
Workshop on Spoken Language Translation ( IWSLT2006), pages 115.
olcke , Andrea . 2002. SRILM -- an Extensible Language Modeling Toolkit . In Conference on Spoken Language Processing ( ICSLP2002), pages 901-904.
ffing , Nicola , Gholamreza Haffari , and Anoop
Sarkar . 2007. Transductive Lea
Machine Translation . In Proc . of 45th Annual Meeting of the Association of Computational Linguistics ( ACL2007), pages 2532.
u , Hua and Haifeng Wang . 2007. Comparative
Study of Word Alignment Heurist
Based SMT . In Proc . of Machine Translation
Summit XI , pages 507-514.
1000
