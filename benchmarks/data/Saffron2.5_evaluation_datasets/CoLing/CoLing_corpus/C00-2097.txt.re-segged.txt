Compiling Language Models from a Linguisti cally Motivated 
Unification Grammar
Manny Raynert > , Beth Ann Hockeyt , Frankie Jamest
Elizabeth Owen Bratt ++ , Sharon Goldwater ++ and Jean Mark Gawron ~
t Resea . rch Institute for
Advanced Computer Science
Mail Stop 1939
NAS AAmes Research Center
Moffett Field , CA 94035-1000

Systems now exist which are able to con:pile unification gralm nars into language models that can be included in a speech recognizer  , but it is so far unclear whether nontrivial linguistically principled graln lnars can be used for this purpose  . We describe a series of experiments which investigate the question empirica  . lly , by incrementally constructing a grammar and discovering what prot  ) lems emerge when successively larger versions are compiled into finite state graph representations and used as language models for a medium -vocabulary recognition task  . 
1 Introduction ~
Construction of speech recognizers for n : ediuln -vocabulary dialogue tasks has now be colnean important I  ) ractical problem . The central task is usually building a suitable language model  , and a number of standard methodologies have become established  . Broadly speaking , these fall into two main classes . One approach is to obtain or create a domain corpus  , and froln it induce a statistical anguage model , usually some kind of Ngram grammar ; the alternative is to manually design a grammar which specifies the utterances the recognizer will accept  . There are many theoretical reasons to prefer the first course if it is feasible  , but in practice there is often no choice . Unless a substantial domain corpus is available , the only method that stands a chance of working is hand-construction fanex-i The majority of the research reported was performed at IIACS under NASA Cooperative Agreement ~ Number NCC  2-1006  . The research described in Section 3 was supported by the Defense Advanced Research Projects Agency under Con~racl ~  N66001-94   C-6046 with the Naval Command , Control , and Ocean Surveillance Center . 
SRI International 333 Ravenswood Ave
Menlo Park , CA 94025* net decisions
Wellington House
East Road
Cambridge CB 11 BH
England plicit grammar based on the grammar-writer 's intuitions  . 
If the application is simple enough , experience shows that good grammars of this kind can be constructed quickly and efficiently using commercially available products like ViaVoice SDK  ( IBM 1999 ) or the Nuance Toolkit ( Nuance 1999 )  . Systems of this kind typically allow specification of some restricted subset of the class of contextfree grammars  , together with annotations that permit the grammar -writer to associate selnantic values with lexical entries and rules  . This kind of framework is fl : lly adequate for small grammars  . As the gran:mars increase in size , however , the limited expressive power of contextfree language notation be-conies increasingly burden some  . The grain n : a , r tends to be conie large and unwieldy , with many rules appearing in multiple versions that constantly need to be kept in step with each other  . 
It represents a large developn:ent cost , is hard to maintain , and does not usually port well to new applications  . 
It is tempting to consider the option of moving towards a ::: or e expressive grammar tbrmal - isln  , like unification gramnm . r , writing the original grammar in unification grammar form and coml  ) iling it down to the contextfree notation required by the underlying toolkit  . At least one such system ( Gemilfi ;   ( Moorectal 1997 ) ) has been implemented and used to build successful and nontrivial applications  , most notably Comnmnd Talk ( Stent ctal 1999) . Gem-ini accepts a slightly constrained version of the unification grammar formalism originally used in the Core Language Engine  ( Alshawi 1992 )  , and compiles it into contextfree gran:nmrs in the GSL formalism supported by the Nuance Toolkit  . The Nuance Toolkit con:piles GSL gran:mars into sets of probabilistic finite state model  . 
The relative success of the Gemilfi system suggests a new question  . Ulfification grammars ha . rebeen used many times to build substantial general gramlnars tbr English and other na  . tu-ra\[languages , but the language model oriented gra . mln ~ rso far developed fi ) r Gemini ( including the one for Colnm and Talk ) have a . ll been domain-sl)ecific . One naturally wonders how feasible it is to take yet another step in the direction of increased genera  . lity ; roughly , what we want to do is start with a completely general  , linguistically motivated gramma . r , combine it with a domain-specific lexicon , and compile the result down to a domain-specitic contextfree grammar that can be used as a la  . nguage model . If this 1) tetra . mine can be rea . lized , it is easy to believe that the result would 1) ea . next remely useful methodology t brrapid construction of la  . nguage models . It is iln portant onote tha . t there are no obvious theoretical obstacles in our way  . The clailn that English is contextfree has been respectable since a  . tleast the early 8 ( Is ( Pullum and Gazda . r1982)' e , and the idea . 
of using unification grammar as a . compact wa 5 , oftel ) resenting an ulMerlying context-fl'e ~ e , language is one of the main inotivations for GPSG  ( Gazdar et al 1985 ) and other formalislns based on it . The real question is whether the goal is practically achievable  , given the resource limitations of current technology  . 
In this l ) a . 1) er , we describe work aimed at the target outlined above  , in which we used the Gemini system ( described in more detail in Section 2 ) to a . ttemp to compile a . va . riety of linguistically principled unification graln lna  . rs into la . ngua . gelnodels . Our first experiments ( Section 3 ) we repert brmed on a . large preexisting unification gramlna . r . These were unsuccessful , for reasons that were not entirely obvious ; in order to investigate the prol ) lemmore systematically , we then conducted a second series of experilnents  ( Section 4 )  , in which we increlnen-tally 1 ) uiltupasm Mlergra . lnlna . r . By monitoring ; the behavior of the compilation process and the resulting langua  . ge model as the gra . lmnar~s2 ~1em'e awarel , hal , this claim is most ~1) robably not l ; rue for natural languages illgelm raI ( lh'csnall clal 1987 )  , but furl ~ h cr discussion of t . his point is beyond I . he scope of t , llC paper . 
cover ~ ge was expanded , we were a . ble to iden-tit ~ the point a , t which serious problems began to emerge ( Section 5 )  . In the fina . 1 section , we summarize and suggest fltr ther directions . 
2 TileGenfini Language Model
Compiler
To l nake the paper n lore self-contained , this section provides some background on the method used by Geminito compile unifica  . tion grain-mars into CFGs , and then into language models . The ha . sic idea . is the obvious one : enu-mera . te all possible instantiations of the feal ; ures in the grammar rules and lexicon entries , and thus tra . ns for mesch rule and entry in the ( ) rig-inal unification grammar into a set of rules in the derived CFG  . For this to be possible , the relevant fe~ttul'esInust be constrained so that they can only take values in a finite predefined range  . The finite range restriction is in convenient for fea  . tures used to build semantic representations , and the tbrmalism consequently distinguishes syntactic and semantic features  ; se-lmmtic features axed is carded a . t the start of the compilation process . 
Anaiveiml ) lelnentation of the basic lnethod would be iml ) raetical for any but the smallest a . nd simplest grammars , and consider a . bleing emfity has been expended on various optimizations  . Most importantly , categories axe expanded in a demand-driven fa . shion , with inferlnatiot l being percolated 1 ) otht ) ott on > up ( from the lexicon ) and topdown ( fl ' om the grammar's start symbol )  . This is done in such a . way that potentially valid colnl ) inations of feature instantiations in rules are successively filtered out if they are not licensed by the topdown and bottom-ul  ) constra . ints . Ranges of feature values are also kept together when possible  , so that sets of contextfree rules produced by the mdve algorithm may in these cases be merged into single rules  . 
By exploiting the structure of the grammar a . nd lexicon , the demand-driven expansion l nethod can often effect substa  . ntial reductions in the size of the derived CFG .   ( For the type of grammar we consider in this paper  , the reduction is typically by , ~ fa . et or of over 102?) . 
The downside is that even an app ~ trently sln all cha  . nge in the syntactic t > atures associated with a . rule may have a large eIfect on the size of tant percolation path  . Adding or deleting lexicon entries can also have a significant effect on the size of the CFG  , especially when there are only a small number of entries in a given grammatical category  ; as usual , entries of this type behave from a software ngineering standpoint like gramma rules  . 
The language model compiler also performs a number of other nontrivial transformations  . 
The most important of these is related to the fact that Nuance GSL grammars are not allowed to contain left-recursive rules  , and left-recursive unification-grammar rules must consequently be converted into a non-left -recursive for t ::  . Rules of this type do not however occur in the gramlnars described below  , and we consequently omit further description of the method  . 
3 Initial Experiments
Our initial experiments were performed on a recent unification grammar in the ATIS  ( Air Travel Information System ) domain , developed as a linguistically principled grammar with a domain-specific lexicon  . This grammar was crejted for an experiment COl :: t ) aring coverage and recognition performance of a handwritten grammar with that of a  . uto ::: atically derived recognition language models  , as increasing amounts of data from the ATIS corpus were made available for each n : ethod  . Examples of sentences covered by this graln lnar are " yes "  , " onfriday " , " i want to fly from bost onto denver on united airlines on friday septem-bert wenty third "  , " is the cheapest one way fare from bost onto denver amorning flight "  , and " what flight leaves earliest from bost onto sanfrancisco with the longest lay over inden-ver  "  . Problems obtaining a working recognition grammar from the unification grammar ended our original experiment prematurely  , and led us to investigate the factors responsible for the poor recognition performance  . 
We explored severalikely causes of recognition trouble : number of rules  , : : umber of vocabulary items , size of node array , perplexity , and complexity of the grammar , measured by average and highest number of transitions per graph in the PFSG form of the grammar  . 
We were able to in : mediately rule out simple size metrics as the cause of Nuance's difficulties with recognition  . Our smallest air travel grammar had 141 Gemini rules and 1043 words , producing a Nuance grammar with 368 rules . 
This compares to the Con:m and Talk grammar , which had 1231 Gemini rules and 1771 words , producing a Nuance gran:n : ar with 4096 rules . 
To determine whether the number of the words in the grammar or the structure of the phrases was responsible for the recognition problems  , we created extreme cases of a Word+grammar ( i . e . a grammar that constrains the input to be any sequence of the words in the vocabulary  ) and a one-word-per-category grammar . We found that both of these variants of our gralm nar produced reasonable recognition  , though the Word+grammar was very inaccurate . However , a three-words-per-category grammar could not produces nccessflfl speech recognition  . 
Many that ure specifications can lnake a grammar : : to reaccurate  , but will also result in a larger recognition grammar due to multiplication of feature w ~ lues to derive the categories of the eontext-fl'ee grammar  . We experimented with various techniques of selecting features to be retained in the recognition grammar  . As described in the previou section , Gemini's default method is to select only syntactic features and not consider semantic features in the recognition grammar  . We experimented with selecting a subset of syntactic features to apply and with applying only se:nantic sortal features  , and no syntactic features . None of these grammars produced successful speech recognition  . 
/ . Fro ::: these experiments , we were unable to isolate any simple set of factors to explain which grammars would be problematic for speech recognition  . However , the numbers of transitions per graph in a PFSG did seem suggestive of a factor  . The ATIS grammar had a high of 1184 transitions per graph , while the semantic grammar of Command Talk had a high of  428 transitions per graph , and produced very reasonable speech recognition . 
Still , at ; the end of these attempts , it beca . me clear that we did not yet know the precise characteristic that makes a linguistically motivated grammar intractable for speech recognition  , nor the best way to retain the advantages of the handwritten grammar approach while providing reasonable speech recognition  . 
672 4 Incremental Grammar
Development
In our second series of experiments , we in-crelnent a . lly developed a . new grammar fronts (' ra . tch . The new gra . mma . r is basic a . llyas (' a . led-down and a . dapted version of tile Core Language Engine gramme \ for English  ( Puhnan 1! ) 92 ; Rayner 1993) ; concrete development work a . nd testing were organized a . round a . speech in-terfac ( ; to a . set ; of functionalities of lhred by a simple simula , tion of the Space Shuttle ( Rather , Hockeygll ( lJames 2000) . Rules and lexical entries were added in sma . ll groups , typically 23 rules or 5   10 lexical entries in one increment . After each round of exl ) a . nsion , we tested to make sure that the gramlnar could still  1  ) e compiled into a . usa . bh ; recognizer , a . nda . tsev-ere . 1 points this suggested changes in our iln-1 ) \] ementation strategy . The rest of this section describes tile new grmmn arinn lore detail  . 
4.1 Overview of Rules
The current versions of the grammar and lexicon contain  58 rules a . nd30 J . Ulfinflectes lentries respectively . They (: over the tbllowing phenom-eli:~IZ1 . Top-levelutl ; er ~ tnces : declarative clauses , WH-qtlestions , YN questions , iml ) erat ; ives , et lil ) tical NPs and I ) Ps , int (; rject . ions . 
~../\]9\,~H-l novement of NPs and PPs.
3 . The fbllowing verb types : intr~nsi-tive , silnple transitive , PP con:plen-mnt , lnodaJ/a . uxiliary , -ingVP con-q ) len:ent , par-ticle q-NP complement , sentential comple-lnent , embedded question complement . 
4 . PPs : simple PP , PP with postposition (" ago ") ~ PP l nodifica , tion of VP and NP . 
5 . Relat ; ive clauses with both relative NP pro-1101111 ( " tit (  ; telnperature th , ttI measured ) and relative PP ( " the ( loci : where I am " )  . 
6 . Numeric determiners , time expressions , and postmodification of NP1 ) ynun:eric expressions . 
7. Constituent conjunction of NPs and cl~ulses.
Tilt following examl ) le sentences illustrate current cover a , ge:3'- . , ': how ~ d ) outscenario three . ?" , " wha , t is the temperature ?" , " measure the pressure a , tflight deck " , " go totile cre wha . t cha . nd(:loseit ", " what were ten:per-a . tttt'ea , ndpressure a . ti if teenoh five ?", " is the teln per a . turegoing ttp ' . ~', " do the fi?ed sensors sa . y tha . t the pressure is decreasing . , " find out when the pressure rea . ched fifteen psi .   .   .   . wh~t1 is the pressure that you mea . sured ? ", " wha . t is the tempera . lure where you a . re ?", ?~(: a . n you find out when the fixed sensor say the temperature at flight deck reached thirty degrees celsius ?"  . 
4 . 2 Unusua l Features o f the Grammar Most of the gramn : ~ u '  , as already sta . ted , is closely based on the Core Language Eng ! negra . nlnla . r . \? e briefly sllnllna . rize the main divergences between the two gramnlars  . 
4.2.1 Inversion
The new gramlna , ruses a . noveltrea . tment of inversion , which is p~trtly designed to simplify the l ) l ' ocess of compiling a , fea , turegl ' anllna , r into contextfree form . The CLE grammar's trea . t-ll tent of inversion uses a , movement account , in which the fronted verb is lnoved to its notional pla  . ce in the VP through a feature . So , t brexample , the sentence " is pressure low ?" will in the origina  . 1CLE gramma . rha . re the phrase-structure : :\[\[ iS\]l "\[ p ressure \] N/  , \[\[\] V\[IO\V\]AI) , \]\]V'\]'\] , ' g " in whk : h the head of th ( , VP is a V gap coindexed with tile fronted main verb  1  , ~  . 
Our new gra . mn : ar , in contrast , hal:dles inversion without movement , by making the con > bination of inverted ver\] ) and subject into a . 
VBAR constituent . A binary fea . ture in vsubj pick so : ll ; these VBARs , a . nd there is a . question-forma , tion rule of tiltform
S - -> VP : E in vsubj = y\]
Continuing the example , the new grammar a . ssigns this sentence tilt simpler phrase-structure "\[\[\[ is\]v\[press:ire\]N*'\]v  . A . \[\ [ low\]J\]V . \] S "4 . 2 . 2 Sortal Constraints Sortal constra , ints are coded into most gr~un : nnr rules as synta . ctic features in a straightforward lna . nner , so they are available to the compilation mar , ~ndultimately tile language model . The current lexicon allows 11 possible sortal values tbr nouns , and 5 for PPs . 
We have taken the rather nonstandard step of organizing tile rules for PP modification so that a VP or NP cannot be modified by two PPs of the same sortal type  . The principal motivation is to tighten the language model with regard to prepositions  , which tend to be phonetically reduced and often hard to distinguish from other function words  . For example , without this extra constraint we discovered that an utterance like measure temperature at flight deck and lower deck would frequently be m is recognized as measure temperature at flight deck in lower deck  5 Experiments with Incremental
Gram 111 ar S
Our intention when developing the new grammar was to find out just when problems began to emerge with respect to compilation of tan-gm~ge models  . Our initial hypothesis was that these would l ) robably become serious if the rules for clausal structure were reasonably elaborate  ; we expected that the large number of possible ways of combining modal and auxiliary verbs  , question for lnation , movement , and sentential complements would rapidly combine to produce an intractably loose language model  . Interestingly , this did not prove to be the case . Instead , the rules which appear to be the primary ca . use of difficulties are those relating to relative clauses  . We describe the main results in Section 5 . 1 ; quantitative results on recognizer per-t brmance are presented together in Section  5  . 2 . 
5.1 Main Findings
We discovered that addition of the single rule which allowed relative clause modification of an NP had a dr~stic effect on recognizer per for -lnance  . The most obvious symptoms were that recognition became much slower and the size of the recognition process much larger  , sometimes causing it to exceed resource bounds . The falser eject rate ( the l ) roportion of utterances which fell below the recognizer's mininmnl confidence the shold  ) also increased substantially , though we were surprised to discover no significant in-crea  . se in the word error rate tbr sentences which did produce a recognition result  . To investigate tile cause of these effects , we examined the results of perfornfing compilation to GSL and PFSG level  . The compilation processes are such that symbols retainm nemonic names  , so that it is relatively easy to find GSL rules and gral  ) hs used to recognize phrases of specified graln mat -ical categories  . 
At the GSL level , addition of the relative clause rule to the original unification grammar only increased the number of derived Nuance rules by about  15%  , from 4317 to 4959 . The average size of the rules however increased much more a  . It , is easies to measure size at the level of PFSGs , by counting nodes and transitions ; we found that the total size of all the graphs had increased from  48836 nodes and 57195 tra . nsitions to 113166 nodes and 140640 transitions , rather more than doubling . The increase was not distributed evenly between graphs  . We extracted figures for only the graphs relating to specific grammatical categories  ; this showed that , the number of gra . 1) hsfbr NPs had increased from 94 to 258 , and lnore over that the average size of each NP graph had increased fronl  21 nodes and 25  . 5 transitions to 127 nodes and 165 tra . nsi-tions , a more than six fold increase . The graphs for clause ( S ) phrases had only increased in number froln 53 to 68  . Theyha . dhowever also greatly increased in average size , from 171 nodes and 212 transitions to 445 nodes and 572 transitions , or slightly less than a threefold increase . 
Since NP and S are by far the most important categories in the grammar  , it is not strange that these large changes m~tke a great difference to the quality of the language model  , and indirectly to that of speech recognition . 
Coln paring the original unification gramlnar and the compiled CSL version  , we were able to make a precise diagnosis . The problem with the relative clause rules are that they unify feature values in the critical S and NP subgraln lnars  ; this means that each constrains the other , leading to the large observed increase in the size and complexity of the derived Nuance grammar  . 
a GSL rules are written in all notation which allows disjunction and Klccnestar  . 

Specifically , agreement ilffbrmation and sortal category are shared between the two daughter NPs in the relative clause modification rule  , which is schematically as follows:
Igp :\[ agr = A , sort = S\]--+
NP:\[agr = A , sort = S\]
REL:\[agr = A , sort = S\]
These feature settings ~ reneeded in order to get tile right alternation in pairs like the robot that * measure/measures the teml  ) erature\[agr\]the*deck/teml ) era . ture tha . tyoumeasured\[sort\]We tested our hypothesis by colnlnenting  ( ) lit the agr and sor t features in the above rule . 
This completely solves the main 1) robh ; in of ex-1 ) lesion in the size of the PFSG representation ; tile new version is only very slightly larger than tile one with no relative clause rule  ( 5 0647 nodes and 59322 transitions against 48836 nodes and 57195 transitions ) Most in L1 ) ortantty , there is no great increase in the number or average size of the NP and Sgraphs  . NP graphs increase in number froin 94 to 130 , and stay constant in a . v-erage size . ; Sgraphs increase in number fom 53 to 64 , and actually decrease , in a a ; erage size to 13 , 5 nodes and 167 transitions . Tests onst ) eech(l~t ; a . show that recognition quality is nea ~ rly : lies a . meas for the version of the recognizer which does not cover relative clauses  . Although speed is still significantly degraded , the process size has been reduced sufficiently that the  1  ) roblen : s with resource bounds disappear . 
It would be rea . sonal ) le 1:o expect tim : removing the explosion in the PFSG ret  ) resentation would result in mL underconstrained language model for the relative clause paxt of the grammar  , causing degraded 1 ) erformance on utterances containing a , relative clause . Interestingly , this does not appear to hapl ) en , though recognition speed under the new grammar is signif-ica atly worse for these utterances CO ml  ) ared to utterances with no relative clause . 
5.2 Recognition Results
This section summarizes our empirical recognition results  . With the help of the Nuance Toolkit batch rec tool  , weevah : ated three versions of the recognizer , which differed only with respect totile language model  , no_rels used the version of the language model derived fl ' on I agran Ln : a  . r with the relative clause rule removed ; rels is the version derived from the fltllgram - lnar  ; and unlinked is the coln l ) romise version , which keeps the relative clause rule but removes the critical features  . We constructed a corpus of 41 utterances , of mean length 12 . 1 words . 
The utterances were chosen so that the first ,   31 were within the coverage of all three versions of the grammar  ; the last 10 contained relative clauses , and were within the coverage of re:s and un : inked but : to to f no_rels  . Each utter-a neew as recorded by eight different subjects  , none of whom had participated in development of the gra  . mmar or recognizers . Tests were run on a dual-processor SUN Ultra60 with 1  . 5GB of RAM . 
The recognizer was set , to reject utter a . nces if their a . ssociated confidence measure fell under the default threshold  . Figures 1 and 2 summarize there . suits for the first 31 utterances ( no relative clauses ) and the last 10 uttera:Lces ( relative clauses ) respectively . Under ' ? RT ' , we give in ean recognition speed ( averaged over subjects ) e?pressed as a multiple of realtime ; ' PRe . j ' gives the falser eject rate , the : heart l ) er-centage of utterances which were reiectedue to low confidence measures  ; ' Me:n'gives the lnean 1)ercentage of uttera . nces which fhiled due to the . 
recognition process exceeding in emory resource bounds  ; and ' WER , ' gives the meanword error rate on the sentences that were neither rejected nor failed due to resource bound problems  . Since the distribution was highly skewed , all mea . ns were calculated over the six subjects renm . i : fing after exclusion of the extreme high and low values  . 
Looking first at Figure 1 , we see that rels is clearly inferior to no_re lsontile subset of the corpus which is within the coverage of both versions : nea  . rly twice as many utterances are rejected due to low confidence values or resource  1  ) roblems , and recognition speed is about five times slower , unlinked is in contrast : tot significantly worse than no_rels in terms of recognition performance  , though it is still two and a half times slower . 
Figure 2 compares rels and unlinked on the utterances containing a relative clause  . It seems reason a . ble to say that recognition performance no_rels1 . 04 9 . 0% - 6 . 0% rels 4 . 76 16 . 1% 1 . 1% 5 . 7% unlinked 2 . 60 9 . 6% - 6 . 5% Figure 1: Evaluation results for 31 utterances not containing relative clauses , averaged across 8 subjects excluding extreme values . 
Grammar x RTF Rej Men : WER \] rels 4 . 60 26 . 7% 1 . 6% 3 . 5%\] unlinked 5 . 29 20 . 0% - 5 . 4%J Figure 2: Evaluation results for i0 utter ~ mces containing relative clauses , averaged across 8 subjects excluding extreme values . 
is comparable for the two versions : rels has lower word error rate  , but also rqjects more utterances . Recognition speed is marginally lower for unl inked  , though it is not clear to us whether the difference is significant given the high variability of the data  . 
6 Conclusions and Further
Directions j
We found the results presented above surprising and interesting  . When we 1 ) egal : our programme of attempting to compile increasingly larger linguistically based unification grammars into language models  , we had expected to see a steady combinatorial increase  , which we guessed would be most obviously related to complex clause structure  . This did not turn out to be the case . Instead , the serious problems we encountered were caused by a small number of critical rules  , of which the one for relative clause modification was by the farthe worst  . It was not immediately obvious how to deal with the problem  , but a careful analysis revealed a reasonable con : promise solution  , whose only drawback was a significant but und is a strous degradation in recognition speed  . 
It seems optimistic to hope that the relative clause problem is the end of the story  ; the obvious way to investigate is by continuing to expand the gramlnar in the same incremental fashion  , and find out what happens next . We intend to do this over the next few months , and expect in due course to be able to l ) resent further results . 

H . Alshawi . 1992. The Core Language Engine.
Cambridge , Massachusetts : The MIT Press.
J . Bresnan , R . M . Kapla . n , S . Peters and A . Zaenen . Cross-Serial Dependencies in Dutch . 
1987 . In W . J . Savitch et al(eds . ) , The For-real Complexity of Natural Languagc , Reidel , 
Dordrecht , pages 286-319.
G . Gazdar , E . Klein , G . Pullum and I . Sag.
1985 . Generalized Phrase Structure Grammar Basil Blackwell  . 
IBM .  1999 . Via Voice SDK tbr Windows , version 1 . 5 . 
R . Moore , J . Dowding , H . Bratt , J . M . Gawron,
Y . Gorfl : , and A . Cheyer .  1997 . Com-mandTalk : A Spoken-Language Interface tbr Battlefield Simulations  . Proceedings of the Fifth Conference on Applied Nat-ura I Languagc Processing  , pages 17 , 
Washington , DC . Available online from http://www . ai . sri . com/natural-language/projects/arpa-sls / command talk  . html . 
Nuance Communications .  1999 . Nuance Speech Recognition System Developer's Manv  , aI , Version 6 . 2G . Pullum and G . Gazdar .  1982 . Natural Languages and ContextFree Languages . Linguistics and Philosophy , 4, pages 471-504 . 
S . G . Puhnan .  1992 . Unification-Based Synta . c-tic Analysis . In ( Alshawi 1992) M . Rayner .  1993 . English Linguistic Coverage . 
In M . S . Agn~s et al 1993 . Spoken Language Translator : First Year Report . SRI Technical Report CRC-043 . Available online from http://www . sri . com . 
M . Rayner , B.A . Hockey and F . James . 2000.
Turning Speech into Scripts . To appear in P~vceedings of the 2000 AAAI Spring Symposium on Natural Language Dialogues with 
Practical Robotic Devices
A . Stent , J . Dowding , J.M . Gawron , E.O.
Bratt , and R . Moore .  1999 . The Coin-m and Talk Spoken Dialogue System . P'rv-cecdings of the 37th Annual Meeting of the ACL , pages 183-190 . Available online from http://www . ai . sri . com/natural-language/projects/arpa-s : s/command ta:k  . html . 

