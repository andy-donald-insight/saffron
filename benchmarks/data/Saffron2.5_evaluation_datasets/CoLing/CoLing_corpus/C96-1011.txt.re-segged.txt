Unsupervised Learning of a Rule-based Span ish Part of 
Speech Tagger
Chinatsu Aonea , nd Kevin Hausman
Sysl , clnsl . cs(;eu'<:t~al(lAl)l)li<:d;ions(~or pora . 1; ioll ( SIIA)
4300 Vairl , akcs(\]ottt %
l <' a , it ' fax , VA 22033
LoI1 cc((i \] Sl'i, . (: oiii ~ h~1 , 1 isi/1311 ((1~sl-iL colil
Abstract
This I ) at ) crd(~scrihcsa,SI ); ulish l ' arl . -ol : Speech(I'()S ) l~a . gger which al , i ) lie saad extends Ih'ill's a , lguril ; hn ~ for ' u ,  . super-vised learuing of ruh ' , -based l , aggcrs ( llrill ,  1995) . First , we discuss our general ; ttt-proach including extensions werim(It 1 ; ~) the algorithm in order t , oh a mllcml kllOWl ~ w(H'(IHa , l~dpa . ra , ll~(d , crize\[ea . l ' llillg ; and ta . ggingot ) l;ions . NcxLwerCl ) or l , and amdyz coure Xl)cri~mm , alrcsull ; sus-iug dill'crenl , l ) ara ~ ilel : crs . Thcll , wc(h~s('ril ) eour " hyhri(l " a . t)l ) roach which was ll(~C~Hs ;- tl ' yillOl'(l(w1 , ooV(~l'(:(Hl~l(~;1\['un(lamcni , all i ~ lit ; al , ion in I h'i ll's origiual al-goril , hnLF in all y , wccOral ) at (> our tagger wil ; hIlid(Icn Mark () vModel(IlMM)-based ( , aggers . 
: l Introduction
We have develol ) eda . Spanish l ' arb-(, l :- HI ) ('( ~ ch(I )() S)'l'a . g : g(~r which al ) l ) lies and extends Ih ' ill's alg ( , rilJ ~ u for unSUl ) crvised l('a . rniug(llrill , l . q !), 5) to cr(~a . l ; e a . set of rules (; hal , r ( ~( luce the aml ) iguil . y of I '() S tags on words . We have ch()scuan unsupervised Ica' , ' ning algori / , hn~l ) (~ ca . u , s(~il , does not require a . larg ; (' I )() S-l;aggedt , raining (- or l ) us . Since there was n()I )() S-t . agged Spanish c ( , rt ) us avail-abh'1 ; ( ) us and since creating a large h and - l , ; tgp ; ( xl corltus is both cosl , lyaud I)r ( ) nel , o in consislamcy , G c decision was also al ) ra , ci , icalone . W c have decided 1; o develop a rule-based I\[ , ; xgg crl(!caus csuch a . tagger lea . rusasel , of declarative rules m~d also because we wautt'd 1  , oc(tml ) are it , with Ili(Id(:n Markov Modd(I1MM)-/) as cd1 ; aggers . 
We extcude < lIh'ill's algol'it lnuins cwwalways.
l " irsl , , we cxtcnd(;d it ,  ( , oImn ( Ih ~ unknowu words in the training and test texl , s . Scc(m(l,w ( . , i)aram (~ terized learniug and t;ag , ghw ; ol ) tions . Finally , wccxp(;rinmnl , edwil ; ha " hyhrid " solul , io , , where we tls(~d a . v ( ; rysinful\[Iltllll)cro\['hm~(I-(lis:nnhigual , ( ~ dtexi ; s during training to overcom(~atiu ~( lan~(ml , a . l limitation in t it (: learning algorithm . 
2Conipollenl ; s()urSpa , is hl'()Nl , agg(w cons is L s of l , hre(~(:()lHl)(~nenl ; s:thelnil , i alSt , al , eAnuol , al . or , thel , carncr ,; Ul(ll , hel . uh "' l'; tgg cr , c ; tcho\['which is d(~scrih ~' ( lhelow . 
2.1 Initial State Annotator
This conq ) onent is used t ( )assign all I>ossil ) leI > ( )S l . ag ~1 . o a . given Spanish word . It consisl ; sofh ~ xi conh > ( ) kul) , nl (> rphol()gical nalysis , a . ndult . 
k . ( . wn word ha , ndlin ~ , , . ' l'heSpanishI'()Sl , ags . ~ l , us(~din1 , hisw()rkc(msisl , s()fl , hcl ' olh , wi . v;l ; ~ gs : AI ) J . AI)V , BI + , ( form of scr()r~s/ar ) , ( II , ()(~1('I'IME , ( ; ( ) I , () N , ( ; ( ) MMA , (; ON . l , I ) ATE , I ) 1 , '/1' , HAVE ( formufha & ' ,  9  , IIYI'IIEN , LET'I'I , ~ I , , I , I'AREN , M()I ) EL , MUI , TII'IAEH , , N , NUMBI , , 1 ) , I ) EI , I ( ) I ) , I ) I , EI ~' JX , PI , () , J ) I , () PN , QIJEH- . 
MAll , I(, QIJ()TE , IlL ) MAN , I . I'AIEN , SEMI-(;()l , () N,SI,ASll,SIJIRX)N . I , S//F VIX , TIIEILE ( hater used i u " l , h(~re " collsl , ructi()ns ) , WII\[)ET ( , : ,  . * l/il , ,' , ,) , WIJNJ'(q , t , r ) , Wt\[l'l'(da , ~ d~O ,  ; . , dV ( S(>'l'ahl(';/) . 
2.\].1 L(;xic(mLookup and Morl)h () logi(-al
Analysis
I in lile Ib ' ill's English l ; agg crcxl ) ( wint(mt ; ( lescribed in ( l + rill ,  1!)!) , ~) ,   , ~o large I )() H-1; ag ; gc(ISpanish c ( , rpus was a . vailable Lous Dotal which ; th tr geh~xi concaube(\]eriv(~(l . As a resull ,, we-ci(h~d1; opars("l,h(' . ( m-line ( kdlinsSpanish--I !' , uglish l ) icl , ionary t , ~tlt(\[d (' , riv(' , da , largeh ' , xicon from it ; ( al ) ( ) ul ,  45 , 000 cnl ; rics) . W (' used only the () pen class entries from ; Irisl < ~ xi('on , and then aug-m(~nl , c(Iil , with irr ( ~ gular verb forms and ; tnullll ) cr of closcd-(-iass words . () urnlorl ) hological analyzer uses as el ; of rewrite rules to sl , ril > offall(l/ormo(lif ' , '/ word endings I ; ol ind root form suf words . 
2.1.2 ( Jnknown Word Ilan ( ling
Hittcel , hc lexicon and JU Url ) hological almlys is will not cov ( ' rcv ( 'ry single wor ( lI , ha / , cmiapl ) Caritla ,  1 , exl , , ;- mal , t , : : nlpl , isi wulc;/l , l , his , '-; l , ag : cI , i classify It llkl to wIIWO l'dS . Any word which did no Lgel , ; ts signed one or more p ; trl , s-o\[Lslme . chinl W (: h ; ~ v (' , obta , in (: d ~ license to the , dictionary . 
5 3 the lookup/morphology phase is examined for certain traits often indicative of particular parts-of-speech  . This task is similar to what was done by the guessers for the HMM-based French and German taggers  ( Chanod and Tapanainen ,  1995;
Feldweg , 1995).
For example , words ending in the letters " mente " are assigned the tag of ADV  ( adverb )  . 
Those words ending in " and o " or " endo " are assigned the tag V-CONTINUOUS-NIL  ( continuous form of the verb )  . Table 1 shows a list of unknown word handling rules . 
Table 1: Unknown Word Handling Rules
Iteuristics POS tag num > 1600  & <  2100 roman numeral 19 - and , - endo-ido , -ado , -ida , -ada-er , -ir , -ar-erse , -irse , -arse-cidn , -idad , -izaje-mente-able capitalized





V-NIL-NIL-CLITIC ~




Performing these simple checks reduces the number of unknowns in our test set of  17  , 639 words from 737 (4 . 2%) tola8 (0 . 9%) . The remaining unknowns are assigned a set of ambiguous open-class tags of N  , V , ADJ , and ADV so that they can be disambiguated by the Learner  . 
2.2 Learner
The Learner takes as input ambiguously tagged texts produced by the Initial State Annotator  , and tries to learn a set of rules that will reduce the ambiguity of the tags  . Output is a file of rules in the following form : context = C : P ~\]  . . . \]1 ~ I . . . IP , ~--+ t , where context is one of PREVWORI ) , 
NEXTWORD , PREVTA ( ~ or NEXTTAG , 2
C is a word or tag,
P1, . . . , t  ~ , . .  .   , Pn are the ambiguous parts-of-speech to be reduced  , Pi is the part-of-speech tatre places
P1, ..., Pi , ..., Pn.
llere are some examples taken from the actual learned rules : * NEXTWORD = DE:PIN--~N  . PREVWOID=EN:DETIADV--+DET*PREVTAG=DET : VIN---+N*NEXTTAG=SUBCONJ:BEIV--+V  2PREVWORD = previous word , PREVTAG = previous tag . 
The Learner applies Brill's algorithm for unsupervised learning to try to reduce the ambiguity of the tags in the input corpus  . The following steps are taken : 1 . The Learner examines each ambiguously tagged word and creates a set of contexts for the word  . Two of these contexts will be PRE-
VWORD and NEXTWORD . The remainder consist of PREVTAG and NEXTTAG contexts as required by the tag  ( s ) on the preceding and following words . For example , if the word preceding the ambiguously tagged word is ambiguously tagged with two tags  , then the Learner must generate two PREV rI'AG contexts  . 
2 . An attempt is made to tind unambiguously tagged words in the corpus that are tagged with one and only one of the tags on the ambiguously tagged word  . For example , if the word in question has both N and V tags , then the Learner would search for words with only an N tag or only a V tag  . 
3 . If such a word is found , the contexts of that word are examined to determine if there is an overlap between them and the contexts generated for the ambiguously tagged word  . One issue for this determination is hownmch ambiguity should be tolerated in the context of the unambiguously tagged word  . For example , if one of the possible contexts is PRE-VTA ( I = N and the word preceding the unambiguously tagged word has both N and V tags  , should the context apply ? To permit various approaches to be tried  , we extended the Learner to accept a parameter ( i . e . , free-dora ) that determine shownmch ambiguity will be accepted on the context words for the context to match  . 
4 . If a context matches for this unambignously tagged word  , the count of unambignously tagged words with the particular part of speech occurring in that context is incremented  . 
5 . After the entire corpus is examined , each of these possible reduction rules ( of the form " Change the tag of a word from X to Y in the context C where YCX "  ) is ranked according to the following . First , for each tag
ZC ; g , Z?Y , the Learner computes : ? in contcxt(Z , C ) , where freq ( Y ) = number of occurrences of words unambiguously tagged with Y  , freq ( Z ) = number of occurrences of words unambiguously tagged with Z  , in conlext ( Z , C ) = nmn ber of times a word unambiguously tagged with Z occurs in context  ( I . 

The tag Z that gives the highest score from this formula is saved as R  . Then the score for a particular transforrnatiotr is  ( : ) -* te ,  (:) 6 . If the highestranked transforn lation is trot positive  , the Learner is done . ( ) therwise , the highestranked transformation is appended to the list of transformations learned  . The Learner then searches this list for the trans -forlnation that will result in the most reduction of ambiguity  ( whi , ;h will always l ) ethelates truleh:arned ) and applies it ;  . This pro-tess continues until no further reduction of ambiguity is possil  ) h ;  , lh ; re , we also extended tirel , earner to + ~ c cept a different parameter ( i . e . , l-ta . qJ ' reedom ) that deterntines how tHlt ( : hambiguity will I ) eaccepted on a word that is used for ( ' onte+xt during ambiguity rcducliou , that is , when the l + earn ( >r hast bundaruh ~ and is a pl ) lying it to the trMning text . Note that sl ) ecifying too small a value for this t ) a rame-ter can cause the \] . , e&r'lr(-: , i ' to go irr to , : tIle trd-less loop , as restricting the valid r:ollt ( : xtsItray have the effect of nullifyiug the just -learned rule  . 
7 . The Learner thee returns to step 1 to begin the process again . 
2.3 R , uleT agg ('+ : t '
This c + ot nl ) otm nt reads tagged texts l ) ro(h , : ed by the lnitiMState Atmotator and rules produced by the Learner and applies the learned rules to timt a  , gged texts to reduce the aml ) iguity of the tags . 
We extended the + l , uleTagger to haw ~ two i ) os-sible modes of operation ( i . e . , best-rule-first and learued-sequcuce mo ( lescontrolled by t , hc scq parameter ) for using the , learned rules to reduce ambiguity : I . The RuleTagger can use an algorithm similar to that used in step  7 of the l , earner . Each possible reduction rule is examined against the text to determine whid ~ ruh '  , results in the greatest reduction of aml ) iguity . 
2 . The R , uleTagger can use a sequeutial application of the learned rules in the order tha  . t the rules were learned . After each rule has been applied in sequence , all of the rules preceding it are reapplied to take adwml  . age of ambiguity reductions made by the latest rule apl  ) lied . 
The R , uleTagger allows one to specify , as in the / , earner , how much ambiguity will be , tolerated for a context to match . For example , one can be very restrictive and require that a tag context  ( e . g . , PREVTAG=N ) that ch only an unambiguously tagged word ( in this ease , a word with only an N tag ) . This parameter ( i . e , . , r-lagJi'ccdom ) Sl ) eeifies the maximunl ambiguity Mlowed on a context word for a  ( : on text tag tollrate h : I requires that the context word be unatn biguously l  , agg(' . ( l ,   2 requires that tlmr c be no more than two tags on the word  , and so on . 
3 Experiments and Results
I " or training and testing of the tagger , we have randomly l ) icked articles from a large ( 274 MB ) "H Norte " Mexican newspaper corl ) uS , and sel ) -arated tlwm into the training and tests ( + ts . The test set ; (17 , 639 words ) wast , ngged matmally for comparison agaitts the system -tagged texts  . For training , wc partitioned the de , velopment set into sev ( : raldilt'erent-sized sets in order to st ( : the el-feels of training corpus sizes . The 1) reak down can
I ) e Found in Table 2.
' l'al)h~2:Ambiguously tagged Training sets
S0t ; Words
Tiny 1322 words
Small 3066 words
M(~dittm 5591 words
Full 12795 words
If one randomly picks one of the possible tags ( + t + each word in the test set , the accuracy is 78 + 0% (78 . 0% with the simple verh tagset) . The awwage I '( ) Samhiguity per word is 1 . 52 (1 . 49) including t ) unctuation tags arr ( I1 . 58 (1 . 56) excluding l ) Unc-tuat , ion tags . For co , nparison , the accuracy of lh'ill's unsupervised English tagger was  95  . 1% using 120,000-word Penn Treel ) ank texts . Ills initial state tagging accuracy was 90 , 7% , whict l is considerably higher than our S l ) a , ish case (78 . 6%:) . 
3.1 Eth ; ctof Tag Set
Our tirst set of experiments tests the etDct of the I '  ( )S tag eomt ) lexity . We used both the Siml ) leverl ) tagset ( 5 tags ) and the c , otn plex verb tagset (42 tags ) , which is shown in " l ' ~ l ) le 3 , where * can be either IS(l , 2S(~ , 3S ( ; , IPL , 2PI + , or 3PI+ . Intim case of siml ) leverb tagset , tense , person and numl)er information is discarded , leaving only a " V " tag and the lower four tags in the table  . 
The scores witlr the siml ) leverb tagset fur different sizes of training sets are found in Tabh ~  4  , and those with the complex verb tagset in ' l'a ble  5  . For these two experiments ,   ( he Learner was set to have a tight restriction on using context for learning  ( i . c , the freedom parameter was set to 1 ) and a loose restriction on context tbrapplying the learned rules  ( i . e . , l-lagfrecdom10) . q ' hel , uleTagger was given a moderately-tight restrictiot bonusing context for reduction rule application  ( i . e . , r-lagJ ' rccdom2) . 
In goner ' M , the scores are slightly higher using the siml ) leverbt ~ gset over the complex verb
V-(~()NI ) ITIONAL -*

V-IMPERFECT-*
V-IMPEI~FECT-SU 13.1 UNCTIVE-fA-*
V-IMPER , FECT-SUI\]JUNCTIVE-SE-*

VPRESENT-SUBJUN(JTIVE - *
V-PRETERIT -*




Tabled : Ambiguously tagged texts , Sirnple Verbs
Set  #of rules learned



Full ( none ) ~ core 13182 . 5% 211 91 . 5% 287 91 . 8% 434 83 . 0% 0 78 . 6% This rule was learned h~te in tile learning process when most  I'/SU1KJONJ pairs had already been reduced , l lowever , a solle C all see froll lt\]le CO il-text of the rule  , it will apply in a large number of eases in a text  . The RuleTagger notes this and applies the rule early  , thus incorrectly changing many P/SUI~C ( )NJ pairs to SUBC ( )NJ and reducing the accuracy of t , he tagging . Since this phenomenone ver occurred in any of the other learning rims  , one can see that the learning proeess can be heavily influenced by the choice of it  , put texts . 
3.2 Effeet of Rule Application

The next tests performed involved using rules generated above and changing  1  ) arameters to the Rule'l'agger to see how the scores wouhl be influenced  . 
In the following test , we used tile simi ) leverb tagset rules but varied ther-tag frcc dom parameter and the scq parameter  . The results can be found in Table 6 . 
tagset (91 . 8% vs .  90 . 3% for the " Medimn " corpus) . This behavior is most likely due to the fact that  , some verb tense/person/number combinations e~m not easily be distinguished from context  , so the Learner was unable to find art fle that would disambiguate them  . 
As can be seen from the tables , performance increased as the size of the learning set incre  , ased up to the " Medium " set , where the score levelled otf . With very small learning sets , the system was unable to tlnd sulticient examples of phenomena to produce reduction rules with good coverage  . 
One surprising data t ) oint in the simple verb tagset experiments was the " Full " score  , which dropped M most 9% fi'om the " Medium " score . After analyzing the results more closely , it was found that the l , earner had learned a very spec , i \ [ ie rule regarding tile reduction of prel ) osition/subordinate ~- conjunctione ombina-tions late in the learning process  . The learned rule was : I'II'3V'I'A ( ~= N : I'\]SUBCONJ-~S\[IB ( ff ) NJ ' Fable 5:

Ambiguously tagged texts , (~ omplex
Set  #of rules learned Score
Tiny 1258 1.7%
Small 212 89.6%
Medium 323 90.3%
Full 564 90.2% ( none ) 0 78.0%
Table 6: Ambiguously tagged texts , Simple Verbs
Set R,-Tag-freedom
Tiny1 ( best-rule-first ) 82 . 7% 82 . 5% 82 . 1% 81 . 9% 90 . l % 91 . 5% 91 . 5% 91 . 5% 90 . 5% 91 . 8% 91 . 8% 91 . 8% 82 . 4% 83 . 0% 81 . 7% 81 . 5% ~ eore ( learned-seqtlellce ) 80 . 2% 80 . 6% 80 . 5% 80 . 5% 89 . 8% 89 . 9% 89 . 9% 89 . 9% 90 . 6% 90 . 5% 90 . 5% 90 . 5% 79 . 8% 80 . 0% 80 . 0% 8O . 0% Although the wu'iations are slight , the best value for the r'-lagfl'c , edoml ) arameter seems to be at an ambiguity level of 2 . It seellls that the strategy of reducing the ambiguity as quickly as possible  ( best-rule-first ) is better than following the ordering of the rules by the l  , earner . This\[nay well be due to the fact that the ordering of the rules as produced by the Learner is dependent on the training texts  . Since the test set was a differ-e at set of texts , the ordering of the rules was not as applicable to them as to the training texts  , and so the tagging performance suffered . 
56 3 . 3Et fe , rt(ffHand+taggedTex ( ; sAfl ? erex + ttnining l ? h( ; result ; sfi'oml ? heaJ )( ~ v ( ~ expcrim cnl?s , wcrea , lized l ? hal , sonm of ( , he(:h~scd-cl ; uss words in Spanish ; ~ rea , ltnosl , always amhiguous ( e . g . , preposil?ions are usually ~ unl~ig;uous bel , we(m1) IEP a , ndSUB(:()NJ , a , nd de l?err nine , rsbel ; we (' nI)1'3'1'a , ud1'R ( )) . Thism (' aus ( ; hal , l ; h(~l , e a , rncr will ? ~ , ever\[ea , rna rule I?o dismul ) igu ; tl , el , hcs(~clos(:d-class(:~+sesI)e(:+msel , here will r ; u ' ely heulmml ) i , gtt-ot is C ( ) ll (; c : xl ? sillI , hel ? raining I , ex( , s( , agge(\[hy1;11('ini(;iaJSi;al , eAlttlO( , & (; or . ' l'ha ( , is , un \] i \] , m ()\[)(' II-(:\[&SSw( , ' ds , wc will no ( , littd . cwltJta , ntl ) iguottsch , s('d class words i , l ? exl ? sprcc is (~ lyI ) (:(: ; mset here is oil\[ya closed set ; of t ; hcm+'l'hus , wc decided I ? oilll , ro-(bite a , st~la\[\]tltll\]t\])(:rof'\]la , lt(\[-( , ~ Lgg(x\]l ; exl ; sill l , o ( , lie1; l:a , ining sel ; given ( ; othel , earner . Since t , hel ; m(\[t ~ tgg ( ; (\[ 1 ; exI ; s\[l&Ve ~? corI'(~C ( ; "( ~ X&III\[)I0S()\[+V , ~ LI'IOIlSl ) h(:notn (' , u  a , , I , hel . eartmrs\]toul(l\])ea\])\[e( , ?) lin(I good exa , nq ) lesint ; h( , ~+ I?( , learnl'ro ~+ . 
For our t , esl?s , wc(h~litmd four set , so \[" hat . l-t , agged text st , h;U , wca , ddedt+()the"Sttmll " (306(~wor(\[s)set , o\['at~tbigu()us\[yl , agg cdl , exl , s . The 1) reakdown is in Table T . 
Ta , hh'7: Ila , tM-l?a+gg(+d'l'raini , gs(:l~s
Set , Words
Snta , I 1218 wor(Is
Me(liutn588 wor(lsl.a,rgc !)()( i words
Full 1791 words
Again , ( , hel , e ~ rnerw~m , ' set l , o have a . l ? i , ghl , rc-sl?rici ; io nonusing cont , exl ? for h + arn \] ttg ( , fr'ccdoml ); m(I aloose restric ( , io noncoll ? ex ( , \[' or ; t . ppJyill , gl?hc\],:,a,rn(;(\[rul(;s(la . qJ ' ec dom , 10) . ' l'h(>I , ulc Tagger wa , sgiw ~ nait lode ra , l , ely-t , igh l ? rest , riot , ionon using (: OIl\[ , (; Xl ?\[ ' or t ' : ( lll('l , iotl rule a , i ) l + lit:al , i()t ~( J:r'rc dom2) . 
The bcst-rul (' , -Jir , sl mode of I , helm le Tagg (: r was
Ilscd,
The resull , ~ , ~sshow niu Table 8 , a + rcsligi ~ l , lybelA , (' rl , hanwh(;nusing only ; m~l ) igt to usly Lagged l?eXl , S , It is in l ; eresl , it ~ gI ? onotetl\];d , l , \] m higher ~- tc(:tu'a , cyw:-ts achieved wit , h fewer ruh ' . s . Itl fact , ; d\[expe , rimcnl , s resull ; ediu\[ea , rnhtgalil , l?h'(~ver 200 tithes . 
' I'M)h ; 8: At nl ) iguous/l\]t ~; tnd , igu . us'l'cxts,Sit~tple

Set ,  #of rul (': sh ' art mdS(:(,'c
I , argc205 Ii % l " ull 202L ~ . q2 .  1% (  . o ,   , (9_ : 2 + tIlu + ul(lil ? i ( ) t~t , oLira(+?l ) (' rhu(mts;Ll ) ov (> , wcwa , l l ?( x I l , oknuwi\["( , he itfl ; r ( )( lucl , iono\['ha , rid-ULgg ; cd texts in to tim " Full " aud ~ iguo ,  . tsly1, a~g , . ' d set would improw ~ il , sr , M ; h . erlow score ( or . ' l'a-hie4) . Wc performed an experil Jt cnl , using sitn plcw ~ rb tags , the " l , ' ull " ambiguously tagged text ; s , ~md the " Full " ha , nd-t;agged l ? exts . Timresu\[l?s were d22 rules learned with :-~ score of 92  . 1% , which tied with ( , he " Sm'MI " ambiguously l , agged set , for achieving l , he highest ,   . , tccura . (' y of allo\["thelem ' , - i , g/ta , ggine ; runs , a ~ full 13 . 5% higher than using , oI, . ~;-u'nittg . 
4 Problems and Possible hn provements All ; hough our Sl ) ; mish P ( ) Sl ; agg cr l ) er\['or n , edr cason ; dflywell , ~ whieving ~ Lltit , q ) rovcment of 13 . ,5% ill ; tc(:tlra , cy over r&llC\[(()ttly picking tags , l?hcrew crose wwa Jlwol ) lcmst , lt ~ ui ; ln'evenl , e , ,l the sysl?cln I'ronll re ; whiugan cwm higher score . 
4.11. Learning Proi~h ', ln
As discussed iu Sccl ? ion 3 . 3, ~ u , , l ) iguous closed class words ( e . g . , prep()sil , ions , det , crminers , etc . ) ca , n nol ; b creduced when l , here a , renouna atll ~ iguous exa . nlples o\['l ? heullinl , hel , i'n , i u i l g ; t , exl , s . This is prev ; flent , in Slmnish , where most I ) reposil ? ionsC~ll&\]SO~)0:- ; tl \]) H'dill ; t\[ , o(;IIj/LIICI , io IIS~d c ( , orlllill Ol ' Sc ; ube prolot ll S , (': l ; c . A\['e wh ~ tll(I ( , ; Lggedl , exl?s ; ~ rcrequired l , oIc , ,rn goud rules for reducing l , he ; uld ~ iguil ? yun I hesc words . Itixl > ossihle , t , ~ w-(wcr , t , hM such l ? exts c . % tlI)ed is :- md ) igut d , ed only for t , heir :- ~ lways ambiguous ch ) scd-ck-ts swords bul ? llol , tlllaJlli ) igtlOtlSclos('(l-cla , '- . ; swords or o\])0, 11-class words . Such an cxperim ( ml ; similar 1 , oseleclivo , samplin . q ( . \[ is ctl sscd in l ) agan and lengels ( m(1) a-g ; ul ~- mdl " , l + gelscm , 1+)/)5) wo . hlI ' , e useful in the \[' ul , tll'(:\[)c:c;~llse , i l ' i t , is t , ruc , it ; will reduce t , hc cost ; or tll:-I , tlll ;- t\]l , ~+ gging (- onsid cr + flfly . 
4.2 Lexicon Prol)le , nJ.
I ) rohlemsl ; ha , t;\]m (' anlca , ppar(ml ? . a , swer a , ltlll()r(:t , csl?s were ( , he in cot nl ) l(~l?en(~ss~tl ~( It\[tisl , a , kesint , he lexic(m . While I , h(+lexicotl , derived l ' r ( ) lllth(~(' , ollins Spa , nish-lgnglish dict , iot\]&ry , w~squit , (' richinw(~r(ls , i l ? s l ; :- t g set ,   ,  . liduol ? a , lwa + ystmd , chl ; het , agd cliuit , ions we (' ml ) loy c(t . l , ' or (~ xampl(~ , our l , agsol , (\] isl , inguislmspr(>l ) (: rn()uus(I)I . ()I>N ) & lidIt()/lllS(N ) , whereas the ( ~ ollinsdi (: l?ion vxyt ~ ark(+d h(>l ; has nout+s(N ) . We have a ( ldedottr existing 1) t't ) l)t wha . rimlisl ? s1 , ot , he lexicon t , ot > + u ' l ; bdly solve / , hisi ) rol+lem , Iml , the list , s + u'e currenl ; ly limil , cdI , oh > (: ~ l , l?i(lliII ;_I , llI(~S(tll(ilmol)hCslh'sl;n , ~tln(>s . 
Wea , lsol~'Otlltds(w(q'0 , 1 mistakes ht late(\]olli , sdclinil , i( , ns(e . g . , severed adverbs ending " ltt(~llt , ( "? , wcr (" classified a(Ij(~cl ; ives ) . All , lt()ug\]~we fixed l ? hese mist , akcs as wet~ol , iccd(hcltl , it , ix diflicult ,  1 , OkllOWh(lwnl , ~ ulysucll(Wl ' or ssl , ill remain in tim\[(~xic:()n . 
It l ? urtt(xtout ; I ; hal , the h + compl cle , nc,s . so 1' the lex-ic . n was auol , h(wfunda+m(ml , a , ll ) rol ) h ~ : : ~ I ? o I ~ rill'sunsul > erviscd h'~-u'ning algoril : Inm Tha J  , is , when word , the tagger is very likely to make a mistake . 
This is because the learner is trained to reduce the ambiguity of possible tags of a word  ( say N , V , ADJ tags ) , but if the lexicon lists only a subset of the possible tags  ( say N and V tags )  , the system will never learn to assign an ADJ tag even when the word is used as an adjective  . 
This type of problem was observed frequently when words are ambiguous between proper nouns and some other parts-of-speech such as " Flo-  , ' es(ADJ/PROPN ) , "" Lozano ( ADJ/PROPN ) , "" van(V/PP~OPN )'' a , " Serra ( V/l'i , OPN ) , " etc . 
because not all the proper nouns are in the lexi-
COIl.
The problems described above did not occur in Brill's experiments because he derived the lexicon fi'om a POS-tagged corpus and used the untagged version of the same corpus for training and testing  . Thus , he used an " optimal " lexicon which contains all the words with only parts-of-speech which appeared in the corpus  . In addition , in such a corpus , rarely used POS tags of a word are less likely to occur  , and words are less likely to be ambiguous . Thus , in a sense , his " unsupervised learning " experiments did take advantage of a large POS-tagged corpns  . 
5 Related Works
It is very ditIicult to compare performances between taggers when accuracy depends on quality of corpora and lexicons  , and may be on character-is , its of languages . But in this section , we corn-pare our tagger with Hidden Markov Model -based taggers  . 
A more widely used algorithnl for unsupervised learning of a POS tagger is Hidden Markov Model  ( I 1MM )  . Cutting el al . (( hitting et al , 1992) and Melialdo ( Merialdo ,  1994 ) used IIMM to learn English POS taggers while Chanod and ' I ' a panainen  ( Chanod and Tapanainen ,  1995) , Feldweg ( Feldweg ,  1995) , and Ledn and Serrano ( l , e6n and Serrano , 1995) ported tile Xerox tagger ( Cutting et al , 1992) to French , German , and Spanish respectively . One of tile drawbacks of antlMM-based approach is that laborious manual tuning of symbol and transition biases is nec : essary to achieve high accuracy  . Without tuned biases , the C , erman Xerox tagger achieved 85 . 8 9% while the French Xerox tagger achieved 87% accuracy . After one man-month of tuning biases , the accuracy of the French tagger increased to 96 . 8% . 
One could derive such biases fronla corpus , as discussed in ( Merialdo , 199d ) , but it unfortunately requires a tagged cort/us . 
' Fhe best accuracy of the Spanish Xerox tag : ger was  91  . 51% for the reduced tagset ( 174 tags ) lit can be a part of a last name as it , " van Mahler " , but also is an inflected form of " it " . 
with the hase accuracy ( i . e . no training ) of 88 . 9 8% while the best accuracy of our tagger is currently  92  . 1% for the simple tagset ( 39 tags ) with the base accuracy of 78 . 6% . The lower base accuracy in our exl > eriment is probably due to the large number of entries in the Collins dictionary  . 
6 Summary
Our Spanish Part of Speech Tagger is a successful implementation and extension of Brill's unsuper : vised learning algorithm that reduces the ambiguity of part-of-speech tags on words in Spanish texts  . 
The system requires few , if any , handtagged texts to bootstrap itself . Rather , it merely requires a Spanish lexicon and morphological analyzer that can tag words with all their possible parts-of-speech  .   ( liven that the system performs at approximately 92% accuracy even with the aforementioned problems and with the inch > sion of unknown words  , we would expect that this system could achieve better results  , approaching those of similar English language POS taggers  , when these problems are rectitied . 

Eric Brill .  1995 . /Jnupervised learning of disambiguation rules for part of speech tagging  , hi Proceedings of the 3rd Workshop on Very Large

Jean-l ) ierre Chanod and Pasi Tal ) an ainen . 1995.
Tagging French- ( ~omparing statistical and a constraint-based method  . In Proceedings of the
I,/ACL-95.
D . Cutting , J . Kupiec , J . Pedersen , and P . Sibun . 
1992 . A Practical Part-of-Speech Tagger . hi Proceedings of the 7'hird Conference ou Applied
Natural Language Processing.
Ido Dagan and Scan I ) . Engelson .  1995 . Selective Sampling in Natural I , anguage Learning . In Proceedings of the IJCAI Workshop on Nero Approach to L carning for " Natural Language Processing  . 
llelnlut Feldweg .  1995 . Implementation ad Evaluation of a . Germanll MM for POS Disambigua-lion . In Proceedings of lheIs " ACL , 91C1)A7'

Fernando Smchez I , edn and Amalio F . Nieto Ser-ran () .  1995 . l ) evelot > ment of a spanish version of the x erox tagger  . Illl ' roceedings of the XI Congrcsode la , 5' ocic dad I , ,'spar~ola para el Proce = samiento dcl Lenguaje . Nalural (, gEl ' LN'995) . 
Bernard Merialdo .  1994 . Tagging English Text with a Probabilistic Model . Compnialional Lin-guislics , 20(2) . 

