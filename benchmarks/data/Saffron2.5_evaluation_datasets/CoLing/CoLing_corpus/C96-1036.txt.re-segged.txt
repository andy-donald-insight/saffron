Nth Order Ergodie Multigram HMM for Modeling of
Languages without Marked Word Boundaries
Hubert Hin-Cheung LAW
Dept . of Computer Science
The University of IIong Kong
hhcJ_aw?cs.hku.hk
Chork in CHAN
Dept . of Computer Science
The University of Itong Kong
cchan Ocs , hku.hk

I , ;rgodie IIMMs have been successfully used for modeling sentence production  . 
llowever for some oriental languages such as Chinese  , a word can consist of multiple characters without word boundary markers between adjacent words in a sentence  . This makes word segmentation the training and testing data necessary before ergodic It MM can be applied as the langnage model  . This paper introduces the Nth order Ergodic
Mnltigram HMM for language modeling of such languages  . Each state of the
IIMM can generate a variable number of characters corresponding to one word  . 
The model can be trained without word-segmented and tagged corpus  , and both segmentation and tagging are trained in one single model  . Results on its applic wLion on a Chinese corpus are reported  . 
1 Motivation
Statistical language modeling offers advantages including minimal domain specific knowledge and handwritten rules  , trainability and scalability given a language corpus  . Language models , such as Ngram class models ( Brown et al , 1992) and Ergodic Hidden Markov Models ( Kuhnel , al . , 1994 ) were proposed and used in applications such as syntactic lass  ( POS ) tagging for English ( Cutting et al ,  1992) , clustering and scoring of recognizer sentence hypotheses  . 
I Iowever , in Chinese and many other oriental languages , there are no boundary markers , such as space , between words . Therefore preprocessors have to be used to perform word segmentation i order to identify individual words before applying these word-based language models  . As a result current approaches to modeling these languages are separated into two seperated processes  . 
Word segmentation is by no means a trivial process  , since ambiguity often exists . Pot proper segmentation of a sentence , some linguistic information of the sentence should be used  . i Iowever , commonly used heuristics or statistical based approaches  , such as maximal matching , fl'equency counts or mutual information statistics  , have to perform the segmentation without knowledge such as the resulting word categories  . 
To reduce the impact of erroneou segmenta-tion on the subsequent language model  , ( Chang and Chan ,  1993 ) used an Nbest segmentation i-terface between them  . l lowever , since this is still a twostage model , the parameters of the whole model cannot be optimized together  , and an Nbest interface is inadequate for processing outputs from recognizers which can be highly ambiguous  . 
A better approach : is to keep all possible segmentations in a lattice form  , score the lattice with a language model , and finally retrieve the best candidate by dynamic programming or some searching algorithms  . Ngram models arc usually used for scoring ( Gu et al , 1991) ( Nagata ,  1994) , but their training requires the sentences of the corpus to be manuMly segmented  , and even class-tagged if classbased Ngram is used  , as in ( Nagata ,  1994) . 
A language model which considers segmentation ambiguities and integrates this with a Ngram model  , and able to be trained and tested on a raw , unsegmented and untagged corpus , is highly desirable for processing languages without marked word boundaries  . 
2 The Ergodie Multigram HMM
Model 2.1 Overview
Based on the Hidden Markov Model , the Er-godic Multigram llidden Markov Model ( l , aw and Chan ,  1996) , when applied as a language model , can process directly on unsegmented input corpus word class  . Other than that its prol ) erties are sin > liartol ' ; rgodict lidden Markov Models ( Kuhnctal . , 1994) , that both training and scoring can be done directly on a raw  , unCagged corpus , given a lexicon with word classes . 
Specifically , the N-O h order F , rgodic Multigram It MM , as in conventional classbased ( N+I ) -gram model , assumes a ( loubly stochastic process in sentence product ion  . The word class sequence in a scalene ( : follows CheN-Ohorder Markov assulnl > tion , i . e . tile identity of a (: lass in the s(' . lite\[Ic ( ~ delmn ( Is only on tim previous N classes , and the word observed depelads only on the class itl  ) e-longs to . The difference is thai , this is a multi-gram model ( Doligne and Bimbot , 1995) in the sense Chateach state ( i . e . node in the IIMM ) (: a , tgenera . reawu-iable number of ot ) served character sequences . Sentence boundaries are i node lcd as as l ) ecial class . 
This model can be apl/lied to a . ll input sent ( race or a characCerlattice as a language model . ' Fhe maxinn unlikelihoods cat ( : sequence through l , he model , obtain e ( t using the ViCerl ) i or Stack I ) (> coding AlgoriChln , ret ) resenCs the 1 ) est particular segmentation and class-tagging for the input sentence or lattice  , since transition of states denotes a wor ( t boundary and state identity denote stile
ClU'rent word class.
2.2 Le . xi('on
A lexicon(CK\]P , 1993) of 78 , 322 words , each con~tainiug up to 10 characters , is awd labh ~ for use ill this work . l'r actically all characters have an cnCl:y ill the lexicon  , so Chatout of-vocal mlary words are modeled as indivi  ( hlale haracters . There is a total of 192 syntactic classes , arranged in a hierarchical way . For example , the month names arc deuoted by the class Nd abc , wherelg denotes Nouu , Nd denotes ' lbmporal Nouns , Igda\['or'l'im ( ~ lmmes and Ndab for reusabh'tilne names . '\[' here ~ is a total of 8 major categories . 
Each word ill the dictionary is aullol , al . cd  with one or nlore syntactic tags , tel ) resenting dilferent syntactic classes Che word cnn possibly belong to  . 
Also , a frequ(mcy count tbreach word , base ( lona certain corpus , is given , bill without inforniation on its distribution over different syntactic classes  . 
2.3 T (: rminoh)gy
I , el , ) 42 be the set of all Chinese words in l , hc lexicon . A word " wkCW is made up of one or more characters  , l , et , s ~ r = ( . ~; I , . ';'21 . . . .";T ) denote , a sentence as a T-character sequence . AfuncCion (5 ~ , , is defined such Chat ( Sw (~ Vk , sit+r-I ) is \] if w ,  . is ar-character words t .   .   . s t +, ,-1, and 0 otherwise . 11, et/2 be the Ul ) per bound of r , i . e . t , 11 om axinnt muumber of characters illa word ( 10 ill this paper )  . 
I , et (2/= cl .   .   . cL  be the set , of syntactic classes , where L is then mn be r of syntactic ( : lasses in the lexicon ( 192 in our case )  . Lott ? CW ? ( /denote Cherela Cion for all syntactic classitications of the  . lexicon , such ChaC (' to t : , el ) @ Cill'clis one of the syntactic classest br ' wk  . Each word wkllltlSt belong to one or more of the classes  . 
A path Chrough the model represents a particular segnmn Cation and  ( : lass Lagging for the Sell--I , ( ~ IIC (' . . I,et ?7 = ( ' wt , ( : I t ; ? .   . ; " Wig , ClK)t ) ea particu-lar segmentation and ( ; lass tagging for the sentences ~' , where Wk is the kth word and elkdCllOtCStllc ( ; lass assigned to w , : , as illustrated below . 
11) I~cl1ltJk,elk~I)K ~ Cll,(( . Sl ?? . Stl-I .  ?  . , Stk_1 .   .   . Stk-1 ? ? . 8 IK_l ? .   . S'I')l "(, rCCobe proper , I1'2, . ,~_ , ) 1aml(wk , cl ~) Cl ' must be sa Cistied , where t0 = 1 , tic=7'+1 and tk-j<l , , for 1 < k < K . 
2 . 4It MMS : a ; esforl; . he N - th order model In Chetirstorder IIMM ( class 1 ) it ( am ) l node l , each I1MM state corresl ) onds directly to the word class of a word . lhlt in general , for an N-Ohorder IIMM model , siuce each class depends on N previous classes , each state has to rel ) l Jesellt C\]I ( tCOlil\] ) illa-t , ion of the classes of the most recelfl ; N words , iuctlading the current , word . 
I , et Qi represent a stal , (~ of the N - th order Er-go ( lit Multigraul IIMM . Thus Qi = ((% .   .   . ci ~_ , ) where tieiS the current word (: lass , ci , is the previ-()us word class , etc . '\[' here is a CeCal of LN states , which may nle all too many l ) aranl ( ' ters ( l/v+l possible state transitions , each state can transit to L other states ) for the model if N is anything greater than on t . 
'1' osolve this l ) rol ) lem , a reasonal ) leaSSlllli-lion can ) cluade that the d ( ' taih'xl (  ; lass idea titles of a mor(~(listanl , word have , in general , less influence than the closer ones Co the current word class  . Thus instead of using Castim clas-sitication relation for all l  ) revious words , a set of I~I'he ; algorithm to bc described ; tS SU ln Cstlt ~ Lt , th ( ,  . 
(:ha . r ; tctcr identities arc known for the S(!lltCttC(~8 ;  ? , ) It ( , it can * also beal ) plicd when ca . ch chart t c t c r positions L becomes a . set of possible (: h~u'a (: ter(:~Lndida . t , es by simply letting & , ,( wk , sl+''-I ) -- i for all words wk which can be constructed from the c\]mr~t  ( : terpositions st .   .   . s t +, 1 of the input c\]m ract cr lattice . Thisen-al ) les the mo ( M to 1 ) e used as the languz Lgc model component for r ( ! ( : ogniz crs and for decoding phonct i ( : input . 
205 classification relations C (?), C(1), . . . C(N-l ) can be used , where C (?) = C represents the original , most detailed classification relation for the current word  , and C ( n ) is the less detailed classification scheme for the nth previous word at each state  . Thus the number of states reduces to LQ----L ( ? ) L ( 1 )   .   .   . L(N-l ) in which L('0_<L . 
Each state is represented as Qi = ( c ~ ? o) . . . elN-_~O ) where C(n ) = cln) , 1 < I < L ( n ) is the class tagset for the nth previous word . 
However , if no constraints are imposed on the series of classification relations COo  , the number of possible transitions may increase despite a decrease in the number of states  , since state transitions may become possible between every two state  , resulting in a total of L (?) 2L (02 . . . L(N-1)2 possible transitions . 
A constraint is imposed that , given that a word belongs to the class cln ) in the classification C ( n )  , we can determine the corresponding word class c , ~+0 the given word will belong to in C(~+1) , and for every word there is no extra classifications in C  ( n + l ) not corresponding to one in C ( n )  . 
Formally , there exist mapping functions 5c ( '0: COO~C ( "+0 , 0_< n_<N2 , such that if C(n ) ~( n + l )\] ~ . ~'( n ) then (( wk , cln )) 6C(n )) = >
I '' ~1~) , ( n + l) , C(n+l )) ( wk , cv ) 6 for all wk6W , and that y(n ) is surjective . In particular , to model sentence boundaries , we allow $ to be a valid class tag for all C(n ) , and define 5 e('~)($) = 2 . 
The above constraint ensures that given a state
Q , : , ( c !?), o . cl : , 1)) it can only transit to Qi = ( c5~) , br ( ? ) (c ~ ) )' ' ' J- ( N-2 ) (c~N--~u ) ) ) where c~? ) is any state in C ( ? )  . Thus reducing to the maximum number of possible transitions to 
L (?) 2 L0) . . . L(N-1).
This constraint is easily satisfied by using a hierarchical word class cheme  , such as the one in the CKIP lexicon or one generated by hierarchical word-clustering  , so that the classification for more distant words  ( higher n in C ( n ) ) uses a higher level , less detail tagset in the scheme . 
2 . 5 Sentence L ike l ihood Formulat ion Let ? be the set of all possible segmentations and class taggings of a sentence  . Under the Nth order model ( . ) N , the likelihood of each valid segmentation and tagging  12 of the sentences T , /~ (8T , ~\[ oN ) , can be derived as follows . 
P ( w , , c **; w =, c ~=; .   .   . ; Wg , e~,, . ION ) = P ( W1\]Cll)P ( cl1I$N)P($MK . . . el . ~_ , , , + ,   ) ? K ( \[Ik := P ( W ~\] Clk ) P ( clkIC ~*-1""" elk_N ) ) = P ( w ~ lc ,   , ) P ( O ,   , lSN ) p($lO , K ) ?
K(\[Ik=uP(w~lclk)P ( Ql~IQ , k - ~ ) ) using Nth order Markov assumption and representing the class history as HMM states  . $ denotes the sentence boundary , elk is $ for k_<0 , and Q~kre ( ? ) c!N-l ) \] Note that Qlk can be de-
Ilk *"'~ k--N+l "" termined from clk and Qlk -~ due to the constraint on the classification  , and thus P ( Qzk\]Qlk_~) =
P ( ct  ~ IQl~-~).
The likelihood of the sentences T under the model is given by the sum of the likelihoods of its possible segmentations  . 
v(slo ) = v(sL no3 The Algorithms 3 . 1 The Parameters As in conventional HMM , the Ergodic Multigram HMM consists of parameters E  ) N ~ -- A , B , in whichA = aij\] , 0 < i , j <_ LQ ( Total number of states) , denotes the set of state transition probabilities from QitoQi  , i . e . P ( Q31 Qi) . In particular , a0i = P ( Qi\[$N ) and ai0 = P ( $\]  Qi ) denote the probabilities that the state Qi is the initial and final state in traversing the HMM  , respectively , a00 is left undefined . H = bj(w ~)\] , where 1 < j < LQ , denotes the set of word observation probabilities of wk at the state Qj  , i . e . 

The B matrix , as shown above , models the probabilities that wk is observed given N most recent classes  , and contains LQ\[W\]parameters ( recall that LQ = L ( ? ) L ( 1 )  . . . L(N-1)) . Our ~ assumption that wk only depends on the current class reduces the number of parameters to L  ( ? ) \] W\[for the/3 matrix . Thus in the model , bj ( wk ) representing P ( Wk\[Qj ) are tied together for all states Qj with the same current word class  , i . e . 
P ( wklOj ) = P ( welc , ) if 03 = ( c, .   .   .  )  . Also , aij is 0 if Qi cannot trans it to Qj . As a resul ~ the number of parameters in the A matrix is only L  ( ? ) LQ . 
Given the segmentation and class sequence ? of a sentence  , the state sequence ( Qz ~ . .  . QI ~) can be derived from the class sequence ( eh . . . ci~ . ) . 
Thus the observation probability of the sentence ? P ~ d'?/ON  )   , cans ~~ given ? and the model ON ,  1 , be reformulated as bll ( wl ) aol I ( mostly similar to that of the first order Ergodic 
Mnltigram HMM.
3.2 Forward and Backward Procedure
The forward variable is defined as
O't(i ) = P ( S1 .  -  . St , QI ( t ) -" Qi \[ ~ ) N ) where Q ~ ( t ) is the state of the \[ IMM when the word containing the characters tas the last character is produced  . 
The recursive equations for c~t ( i ) are ~ t ( j ) = ~ t ( j ) = 0 brt < 1 w ~ . 1 ~ LQ~\[~c~t- , '( i ) alj bj(w ~) l ~ w(Wk , stt-r+l)\['orl<t < 7' Similarly , the backward variable is defin ( ' daslit ( i )  7- \ [ '  ( St-b1 .   .   . stIq+( , ) = Qi , ON ) ' l ' he recursive equations for fit ( i ) are fit ( i )  - -9  ( i ) = fit ( i ) 0 for t > Taio
It LQr = lwkE la2j == l~~o(wk,t + , . ~ St+l ) for I < t < T--1 As A , Harrays and the 5~ , fimction are mostly 0s , considerable simplification can be done in irnph ' . -mentation . 
The likelihood of the sentence given the model can be evaluated as 

P ( s'('lON ) = ~ f_~., r(i ) aioi=1
The Viterbial go , ' ithm\[br this model can be obtained by replacing the summations of the forward algorithm with maximizations  . 
3 . 3 Re-estimation Algorithm & ( i , j ) is detined as the probability that given a sentence  . s ~' and the model (_) N , a word ends at the characters t in the state Q i an  ( ltile next word starts at the characters t+lin the state Qj  . Thus ~ t(i , j ) can be expressed as
R s , +,( j)r = lwkCW
P ( sY'leN)\['orl < t < fl'--I1<i,j<LQ . turther more dellne % ( / ) to be the probahility that , given Slr and ON , a word ends at the characters t in the state Q i . Thus ctt(i)/3,(i ) for 1<t < 7', 1<i<LQ . 7, ( i ) - p(sy . l ? N ) Sulnlnation of ( t(i , j )   ( ) vet " t gives tile expected number of times that state Qi transits to slate Qj in the sentence  , amlstunmation of 7 t ( i ) overt gives the expected number of state Qi occurring in it  . Thtts the quotient of their summation over t gives a ij  , the new estimation for aij . 
"1'-1(l'aij--~_\[ , ~' t ( , ,Y)/~_~ 7 , ( i ) for 1_< i , j . : : ( LQt = lt in 1The initial and fi , ,a \[ class probability estimates , a0j and ai0 can be reestimated as follows . 
Itr = lwkE'VV = t(si"leN)
Paio--c ( i ) aio/~'Tt(i)
To derive bj(w~:) , first define ctt ~ ( i ) as the probability of the sentence prefix ( sl ?? . s t ) with ' w a , in state Qi as the last coml ) lete word . Thus
It 1 , ~ r = li = l(():t-- ; ( i ) aij bj(wk ) ~ w('u)k , Stt--r + l ) ) This represents the contribution of wk , occurring as the last word in sl , to , ~ , ( j ) . Also define 7' t ? ~ ( j ) to be the I ) robability that , given the sente . nce , s ' ~" and the model , we is observed to end at characters t in the state Qj  . 
(, ~\[~( j)fJt(j ) 7~" ~( J)-p(8~'lON )
Let QjoQj , denot (; s the relation that both Qj and Qj , represent the s~mecurrent word class . 
Thus summation of 71~k(j ) ow : rt gives the e . x-petted munber of times that wk is observed in the total expected number of occurrence of state Qj  . Since states with the same current word class are tied together by our assumption  , the required value of bj(wk ) is given by
EJ'E~I , ./~ ok(j ,)- Dj(Wk ) = Q.ioQj,
E ; ET 17 t(J')
QjoQj , 4 Experimental Results 4 . 1 Setup A corpus of daily newspaper articles is divided into training and testing sets for the experiments  , which is 21M and 4M in size respectively . Th ( ' first order ( N = I ) algorithms are applied to the training sets , and parameters obtained after different iterations are used for testing  . 
The initial parameters of the HMM are set based on the frequency counts from the lexicon  . 
The class-transition probability aij is initialized as the a priori probability of the state P  ( Qj )  , estimated fl ' om the relative frequency counts of the lexicon  , bj ( wk ) is initialized as the relative count of the word wk within the class corresponding to the current word class in Qj  . Words belonging to multiple classes have their counts distributed equally among them  . Smoothing is then applied by adding each word count by  0  . 5 and normalizing . 
After training , the Viterbi algorithm is used to retrieve the best segmentation and tagging ?* of each sentence of the test corpus  , by tracing the best state sequence traversed . 
4.2 Perplexity
The test set perplexity , calculated as m '= exp(-M\]--log(J'(Z' , i where the summation is taken over all sentences s ~' ~ in the testing corpus  , and M represents the number of characters in it , is used to measure the performance of the model . 
The results for models trained on training corpus subsets of various sizes  , and after various iterations are shown ( Table 1) . It is obvious that with small training corpus , over training occurs with more iterations . With more training data , the performance improves and over training is not evident  . 
4.3 Phonetic Input Decoding
A further experiment is performed to use the models to decode phonetic inputs  ( Guetel . , 1991) . 
' Daining Size 2d 6898K 194 . 009 214 . 096 246 . 613 286 . 721 1 . 3M 126 . 084 122 . 304 121 . 606 121 . 776 6 . 3M 118 . 531 113 . 600 111 . 745 110 . 783 21M 116 . 376 11 . 1 . 275 109 . 282 108 . 1 /12 Table 1: Test Set Perplexities of testing set after different iterations on subsets of training set This is not trivial since each Chinese syllable can correspond to up to  80 different characters . 
Sentences from the testing corpus are first expanded into a lattice  , formed by generating all the common homophones of each Chinese character  . Tested on 360K characters , a charactere cog-nition rate of 91 . 2 4:% is obtained for the model trained after 8 iterations with 21M of training text . The results are satisfactory given that the test corpus contains many personal names and  ( ) tit of vocabulary words , and the highly ambiguous nature of (; he problem . 
5 Discussion and Conclusion
In this paper the Nth order Ergodic Multigram IIMM is introduced  , whose application enables integrated , iterative language model training on nn-tagged and unsegmented corpus in languages such as Chinese  . 
The pertbrmanee on higher order models are expected to be better as the size of training corpus is relatively large  . I to we ver some form of smoothing may have to be applied when the training corpus size is small  . 
With some moditication this algorithm would work on phoneme candidate input instead of character candidate input  . This is useful in decoding phonetic strings without character boundaries  , such as in continuous Chinese ~ Japanese ~ Korean phonetic in pnt  , or speech recognizers which output phonemes . 
This model also makes a wealth of techniqnes developed for HMM in the speech recognition field available for language modeling in these languages  . 

Brown , P . F . , deSouza , P . V . , Mercer , 11 . .L . , Della Pietra , V . J . , Lai , J . C .  1992 . Class-Based ngram Models of Natural Language . In Compu-lalional Linguistics , 18:467-479 . 
Chang , C . II . , Chart , C . 1) .  1993 . A Study on Integrating Chinese Word Segmentation and l  ) ~ rt-3 , No . I , pp . 69-77 . 
Chinese Knowledgelnt brmation Group1 !)!) 3 . In Technical Report No .  93-05 . \[ nstitul . e of lnt ' ofmation Science , Academia Sinica , ' l ' ai wan . 
Cutting , K . , Kupic (', J . , l ) cd crs(:n , J . , Sibun , P . 
1992 . A Practic MI ) ar,-of-Sl ) CechTagger . In Proceeding , s of the Third Confercu . ccon Appli (: d Natural Language Procc . s , s in 9, p p .  133-140 . 
I ) clignc , S . , Bimbot , F . t9i ) 5, l,~mgu;~g(' . Modeling by Vt ~ ritd ) le Length S( ; quenccs : Th cor (' . tical Formul ~ ttion ~ mdEvahm tion of Multigrams . In 1CAb'5'P95, Pl) .  169-172 . 
Gu , II . Y . , Tscng , C . Y . , l , cc , I, . S .  1991 . Markov Modeling of Mmldar in C'hincsc for decoding the phonc ~ ic sequence into Chinese ch  ; ~r~cl . ( ws . In Uompuler , 5' pooch and Language , Vol5, pl) . 363-377 . 
Kuhn , T . , Nicmann , H . , Schukat?TM~tmazz-ini , E . G .  1994 . Ergodict/idd cn Markov Model strod Polygr ~ ms for I  , anguage Modeling . In
ICA , gSP94, pp . 357-360.
L ~ tw , t\[ . II . C . , Chan , (3 .  1996 . Ergodi ( " Multi-grotto IIMM Integrating Word Segmc'ntal , iou ; rodClass Tagging for ( Jhinesc I , mlguagcMod-e\]ing . ' Fo appear in 1CAHS'I ~95 . 
Na . gata , M .  1994 . A Stochastic , \] ap~mcs ( ~ Morphok ) gical An MyzcrUsing ~ l , 'orwa . rd-l)PB~L <; kwa . rd-A * NBestSear <: h Algorithm . In
COL1NG94, I)1). 201-207.

