An Acquisition Model for both Choosing and Resolving 
Anaphora in Conjoined Mandarin Chinese Sentences
Benjamin L . Chen
Application Software Department
Computer and Communication Research Laboratories Industrial Technology Research Institute 
W300/CCL , #1RA~DRd.I,
Hsinchu , 30045, Taiwan , R.O.C.
email : blchen@jaguar.ccl.itri.org.tw
Von-Wun Soo
Department of Computer Science
National Tsing-Hua University
Hsinchu , 30043, Taiwan , R.O.C.
email : eoo@es.nthu.edu.tw
Abstract
Anaphoric reference is an important linguistic phenomenon to understand the discourse structure and content  . In Chinese natural language processing , there are both the problems of choosing and resolving anaphora  . In Mandarin Chinese , severalinguists have attempted to propose criteria to ezplain the phenomenon of anaphora but with controversial results  . 
On the other hand , search-based computational techniques for resolving anaphora are neither the best way to resolve Chinese anaphora nor to facilitate choosing anaphora  . Thus , to facilitate both choosing and resolving anaphora with accuracy and efficiency  , we propose a case-based learning model G-UNIMEM to automatically acquire anaphorie regularity from a sample set of training sentence si  , which are annotated with a list of features . The regularity acquired from training was then tested and compared with other approaches in both choosing and resolving anaphora  . 
Keywords : anaphoric reference , semantic roles ( case ) , natural language acquisition , ease-based learning . 
i Introduction
In discourse , there may be anaphora in two consecutive sentences  . When anaphor appear in a pair of consecutive sentences  , the two consecutive sentences are called conjoined sentences  . In real life conversation , we frequently choose and resolve anaphora to understand the utterances  . There are primarily three types of anaphora in Mandarin Chinese : zero  ( ellipsis )  , pronominal ( using pronoun ) and nominal anaphor a\[4\] . Let's take the conjoined Chinese sen-tence z in ( B ) to illustrate the phenomenon . The con-1This paper is partially supported " by the Minister of Economic Affairs  , R . O . C . under the project no . 
3 3H3100 at ITRI and by National Science Councial of R . O . C . under the grant no .   NSC81-0408-E007-02 at National Tsing Hua University . The authors also want to thank Dr . Martha Pollack for valuable comments during 1991 UCSC linguistic institute . 
joined sentence in ( C ) is the English translation of the
Chinese sentences m(B).
( B ) Yueh-ha as heng-bing . \[\] i-thinghui-chia-le . 
John got sick\[\] already gone home ( C ) Because John . assick , he has gone home . 
Because the anaphora in ( B ) is a zero anaphor and there is no zero anaphora in English  , the antecedent of zero anaphora in ( B ) must be resolved first before choosing an appropriate pronominal anaphora in Chinese to English translation  . In the translation from ( C ) to ( B ) , it is not good to directly translate an English pronoun to a Chinese pronoun  . A better way is to resolve the anaphoraire ( C ) and then choose an appropriate type of anaphora in Chinese  . 
In natural anguage processing , bettere sult seems to be attainable if rich linguistic or domain knowledge is available  . However it generally costs much and doesn't seem to be realistic  . The same situation applies for resolving and choosing anaphora in Mandarin Chinese  . If we only used search-based ap-proaches ( those that merely used heuristic and algorithmic methods without much linguistic knowledge  )  , the performance was limited . However , when we intended to adopt linguistic knowledge , we found linguists ' theories tended to be controversial and less computable  . Thus , it motivated us to pursue an acquisition model that could acquire linguistic regularity from corpora and then used the regularity to resolve and choose anaphora  . 
2 Review of previous approaches 2 . 1 Search-based approaches Both history list \[1\] and Hobbs's naive syntactic . algorithm \[7\] a research-bused approaches for re~lv-mg anaphora  . However , it's not quite obvious to tell which was better than the other with only few exam-pies  . Thus , we collected 120 testing instances to test them . Those instances were selected from linguists ' ACTESDE  COLING-92  , NANTES , 2328 Aotrr 1992 274 PROC . OFCOLING . 92, NANTES , AUO .  2328 , 1992 examples , textbooks , essays and novels . Half of them contained zero anaphora the other pronominal  . 
The result showed that the correct number was 111 ( 92 . 5%) with Hobbs's syntactic algorithm and 87 (72 . 5% ) with the history list approach if first matched were selected  . There was 109 (90 . 8% ) correct for history list if the last matched were selected  . 
It seemed that both approaches were applicable to resolve anaphora  , tiowever , when there are several NPs with the same semantic features  , both approaches may get into troubles . \] ~ harthermore , both cannot be used to choose anaphora . 
2.2 Linguis Cscriteria
Among linguists ' works \[31   \[51 \[ lll \[121   \[141  , Tai's criteria \[14\] was applicable to both choose and resolve anaphora  . Others ' suffered from difficulties of extracting features or resolving anaphora  . Table 1 shows 4 coreferences for Tai's citeria , which are all applicable when coreferred NPs are human  . For example , consider the following conjoined sentences : Tai :\[ Lao Zhang \] dao-le Meiguoyihou  . \[\] jiac-lehen-duopengyou . 
John came U . S . Aafter\[\] made many friends Since John came to the U  . S . A . , he has made many friends . 
The subject in the first sentence is human and coreferred by the subject in the second sentence  , so this is a subject-subject o-reference . According to Table 1 , zero anaphora is preferred to the pronominal one and nominal anaphora is not permitted in this example  . Though Taldidn't propose the criteria for resolving anaphora  , it was possible to get these criteria just by transforming the choosing criteria in reverse order  . 
After Tai's criteria were applied to choose and resolve anaphora on the  120 testing instances , we got the success numbers 86 (71 . 7%) and 65 (54 . 2%) respectively . The results failed to meet our satisfaction . 
Through above paragraphs , it appears that search-based methods have their limitations due to lack of enough linguistisc knowledge and Tai's criteria seems to be applicable to both choose and resolve anaphora  . 
It might be that Tai's criteria were too general to lead to a high success rate  . More reliable method to acquire regularity might be required to promote the success rate  . We hypothesized the regularity of anaphora could be accounted by causal relations between the features in the conjoined sentences and the antecedents  . In the following section , an acquisition model is introduced . 
3G-UNIMEM : A Case-Based
Learning Model
In natural language acquisition problem , the restriction of positive-only examples \[2\] has prohibited many machine learning models as a feasible natural language model  . However , a case-baaed learning approach such as Lebowitz's UNIMEM  \[9\]  \[  I0\] seems to be a candidate due to its capability to form concepts incrementally from a rich input domain  . Nevertheless , to apply UNIMEM directly to the acquisition of anaphoric regularity in Mandarin Chinese is still not sufficient  . We have therefore modified UNIMEM into

G-UNIMEM , a modified version of UNIMEM , is an incremental learning system that uses GBM ( Geueralized-based Memory ) to generalize concepts from a large set of training instances  . The program was implemented in Quintus PROLOG and on SUN workstation  . 
G-UNIMEM differs from UNIMEM in two respects.
Firstly , if a drinker got drunk many times after taking either whiskey and water or brandy and water  , he would induce that watermade him drunk with UNIMEM  . This is intuitively incorrect . Whereas , with G-UNIMEM , he would induce that whiskey and water , brandy and water or water would cause him drunk . In this case , G-UNIMEM retains the possible causal accounts without committing to erroneous conclusion  . Secondly , G-UNIMEM can extract explicit causal rules from memory hierarchy  . 
Similar to UNIMEM , G-UNIMEM organizes input training instances into a memory hierarchy according to the frequencies of features  . However , its goal is to explicitly express the generalized causal relationships between two specified types of features : cause features and goal features  . Since there may be inconsistency due to lack of cause features  , further refine-meat is needed to obtain consistent causal relations  . 
Thus , there are four different modules in G-UNIMEM to complete different functions in order to achieve this purpose  . 
3.1 The classifier
The classifier is the first module that processes all training instances for G-UNIMEM  . Its function is close to UNIMEM that organizes a hierarchy structure to incrementally accommodate a training instance and at the same time generalize the features based on similarities among training instances  . The forming hierarchy is organized as either ag -c-hierarchy orac-g-hierarchy depending on the setup of system  , which is defined in Definition 1 . In Appendix A we show the basic classifier algorithm  . 
Definition 1 Ag-c-hierarchy is the hierarchy that every generalized goal feature resides in a GEN-NODE and there is no generalized cause feature that resides between the root node and this GEN-NODE  . Ac-g-hierarchy doesn't allow any generalized goal feature to reside in the GEN-NODE between the root node and any GEN-NODE where generalized cause features reside  . 
Figure 1 and Figure 2 show the forming g-c-hierarchy and c-g-hierarchy respectively after  13 annotated training sentences are entered into G -UNIMEM  . 
Generally , g-c-hierarchy would be chosen since it retained all possible causal accounts  . For example , the drinker with g-c-hierarchy would induce that whiskey and water  , brandy and water or water would cause him drunk ; whereas , he would induce whiskey and Ac'rY . s DECOLING-92 , NANTES , 2328 AOU'r 1992275 FROC . OI'COLING-92, NAhrrES , AUG .  2328 , 1992 water , brandy and water with e-g-hierarchy . The c-g-hierarchy is more efficient since no rules are needed to be generated  . Fig . 3 and Fig .   4 show the updating of a GBM before and after inserting a new training instance  . 
3.2 The rule generator
Once a hierarchy has been constructed by the clas -sifter  , the causal rules can be extracted . The rule generator module serves as the role to extract causal rules from the hierarchy  . It generates all causal rules from the hierarchy as the regularity is retrieved for predictions  . 
In Fig .  6 , if a testing instance is given for choosing anaphor a with a query feature list \[  ( g , type (*?)) , ( g , ante(theme )) , ( c , fl(theme )) , ( c , anaphor ( theme )) , ( c , s2(obj )) , ( c , p(pv ))\] , the retrieval process is searched with a postorder traverse  , namely , in the order sequence of the node number 1 ,  2 ,  3 ,  4 , 5 and 6 . Since there may be more than one candidate , the system can be set up to select either the first or the most specific one  . If the first one is preferred , type(nil ) is yielded as the prediction . If the most specific answer is preferred , all possible rules will be tried and the one with the most number of contingent features matched will be the answer  ( i . e , type ( pronoun )) . 
The sample rules generated from Fig . 1 are shown in Fig .  5 . Before generating rules , the GBM is adjusted so that all children of a GEN -NODE are ordered according to their confidence scores of features  . 
Then all rules are generated in a postorder traversal  . 
3.3 The rule filter
The rule filter removes those rules that are ill -formed and useless  . For example , the causal rule 5 in Fig . 5 has no causes which is not a wellformed rule . It also detects conflicting rules . Conflicting rules are those that have different goal feature descriptions  , which are accounted by the same cause . For example , the rule I and rule 6 in Fig . 5 are conflicting . These rules will be detected in this module and then to be resolved by the feature selector  . 
3.4 The feature selector
Any two conflicting rules are resolved by the feature selector through augmenting the two rules with mutual exclusive contingent cause features  , which are prepared in advance . Dominant features were used in initial regularity acquisition stage  ; whereas contin-gent features were used in feature selection stage  . The ominant feature such as goal features are assumed to be those that must be present in every anaphoric rule  . 
Contingent features are optional . Fig .  6 . shows the GBM with g-chierarchy after feature selection  pro-c?08  . 
4 Tests us ing sentences anno-ta ted with mixed features We trained G-UNIMEM with  30  ,  60 ,  90 ,   120 instances using those features mentioned by Tai , and used all the 120 instances as testing instances . It showed that the approach using Tai's criteria was not promising  . There are two reasons . First , none of the success rates was as high as those using the history list approach or II obbs's algorithm  . Second , many conflicting rules remained conflicting due to either that no further features from feature selection were available or too many specific training leading to too many specific rules  . These factors decreased the success rate . 
4.1 Selecting mixed features
Since Tal's features were not sufficient , more semantic features were considered . Among several linguists ' works , we tentatively selected some computational feasible syntactic and semantic features from different sources  \[3\]   \[5\]   \[11\]   [12\]   [13\]   [14\]   [15\] as in Table 2  . An example with annotated features is shown below  . Tile notation\[\] represents zero anaphora . 
( C)\[Laozheng\]i  qu-leji-ge\[nurenli . \[\] jhenhuizuo-cai . 
John married a woman t \] wetl can cook.
agent the meagent hmsub , by nondefinite
John married awoman , and the woman cooked well.
The training feature list for the sentences ( C ) is :\[ ( gante ( theme ) ) , ( g , type(nil )) , ( c , f l(agent )) , ( c , f2 ( theme )) , ( c , anaphor(agent )) , ( c , p(bv )) , ( c , s2(sub )) , ( c , h ( hm ) )  ( c d ( nondefinite ) ) where the notations g and c represent goa and cause features respectively  . 
4.2 Testing using mixed features
After semantic features has been determined , we trained G-UNIMEM with 30 ,  60 ,  90 ,   120 instances and used all the 120 instances as testing instances each time . We hypothesized to choose semantic roles ( i . e . 
ease ) as dominant cause features . The feature such as ante(CASE ) , type(X ) , anaphor ( CASE ) and fi ( CASE ) are dominant features and the number of fi is variant  . The hypothesis was motivated by Sidner \[13\] who used semantic roles to determine focus and resolve definite anaphora  . The others such as h(Hm ) , p(POS ) , s2(SYN);d(D ) , con(s ) belong to contingent features . 
4.3 The experimental results
It is interesting that the success numbers in Table  3 increased with the number of training instances . 
Finally , our results showed that experiments with c-g -hierarchy had a little high accuracy rates  ( 95 . 8% for resolving and 90 . 8% for choosing anaphora with 120 training instances ) than thoee with g-e-hierarchy . 
Both accuracy rates were higher than those with TaPs criteria  \[14\]  . Thus , G-UNIMEM with semantic roles as dominant features promised much higher accuracy rate  . 
In Appendix B we show some sample rules acquired in Horn-like clauses  . After examination , either the agent or ~ heme of first sentence is most likely to AcrEs DE  COLING-92  , NANTES ,  2 . 3-28 AOOT 1992 276 PROC . OFCOLING-92 . NANTES , AUG . 2328,1992 act as antecedents of anaphora . T iff sphenomenon is in coinco incidence with the investigation on anaphora by Sidner  . That is , the agent often appeared as actor focus and theme as default focus  . This is similar to Tai's criteria but is in more compact interpretation  . 
5 Discussion
There are two concerns in implementing G-
UNIMEM : ( 1 ) The feature set : Is the assignment of dominant features and contingent features objective ? If there is any contingent feature in the assignment that obvi ~ ously improves the accuracy rate  , it shonld be assigned as dominant feature . We use statistical methods \[8\] to analyze if contingent features actually improve accuracy rates  . If there is no obvious improvement with contingent features  , the division of dominant and corr-tingent features is acceptable  . 
We made the null hypothesis " G-UNIMEM with c-g -hierarchy doesn't have obvious improvement with contingent features " and the alternative hypothesis " G-UNIMEM with c-g-hierarchy has obvious improvement with contingent features "  . We titangot two test values from test statistics : tl =  0  . 8472 and t2 < 0 . Both test statistic . q were less than t ~ = . 05 (= 1 . 734 with d . f .  = 18) . Thus , the null hypothesis " G-UNIMEM with c-g -lfierarchy doesn't have obvious improvement with contingent features " was not rejected  , which justified that G-UNIMEM using semantic roles as dominant features was valid  . 
(2 ) The sample size : Compared with actualing uis-tic domain  , the 120 training and testing instances are small . A large corpus is desirable to test the system's performance  . If it becomes available , our resnlts would be more objective and reliable . 
6 Conclusion
We have illustrated a way of using machine learning techniques to acquire anaphoric regularity in conjoined Mandarin Chinese sentences  . The regularity was used to both choose and resolve anaphora with considerable accuracy  . Table 4 shows a comparison between different approaches . 
In comparison to other approaches , tire proposal of using G-UNIMEM as the acquisition model and using semantic roles as dominant features is practical and serves multiple purposes  . 
References\[1\]James Allen (1987) , Natural Language Understanding , The Benjamin/Cummings Publishing

\[2\] Robert Berwick (1986) , Learning From Positive-only Examples : The Subjective Principle and Three Case Studies  . In R . Michalski , J . Carbonell , and T . Mitchell ( cal . ) Machine Learning : An Artificial Intelligence Approach Vol  . \[ I , Chapter 21 , Morgan Kanfmann Publishers , Inc . 

Ping Chen (1984) , A Discourse Analysis of Third Person Zero Anaphora in Chinese  , reproduced by Indianan University Linguistics Club  , 
Bloomington , Indiana.
Ping Chert (1987) , A Discourse Approach to ZeroAnaph 0 rain Chinese , ghonggua Ynwen , 

Chauncey C . Chu (1983) , Definiteness , Pre-suppositimt , Topic and Focus in Mandarin Chi-ne~e , Student Book company , Taipei , Taiwan . 
John H , Gennari , Pat Lmlgley and Doug.
Fisher (1989) , Models of Incremental Concept Formation , Artificial Inielligence 40 . 
\[7\] J . llobbs (1978) , Resolving Pronoun References , Lingua 44 , North-tloll and Publishing Co . 

Richard Johnson and Court Bhat-tacharyya ( 1985 )  , Statistics : Principles and Methods , Chapter 6 ( Section , I ) ,  9 ,  10 ,  11 , Published by Hohn Wiley & Sons . 
Michael Lebowitz (1986) , Concept Learning in ~ Rich Input Domain : Generalization-based Memory  . IraIt . Midralski , J . Carbonell , and T . Mitcholl ( ed . ) Machine Learning : An Artificial \[ nlelligence Approach Vol  . 1I , Chapter 8, Morgan Kaufmann Publishers , Inc . 
Michael Lebowitz (1986) , Integrated Learning : Cmttrolling Explanation , Cognitive Science 10 . 
\[11\] Cherrylug Li (1985) , Participant anapho ~ in Mandarin Chinese , University of Florida Press . 
\[12\] Met Du Li (1986) , Anapho , ~ c Structure of Chinese , Student Book CO . , Taipei , Taiwan . 
\[131\[1,11\[ix\]
C . Sidner (1983) , Focusing in tim Compre~hens\[on of Definite Anaphora  , Iu Computational Models of Discourse , M . Brady and R . Berwick , eds . , MIT Press . 
J . lI . Tat (1978) , Auaphoric Constraints in Mandarin Chinese Narrative Discourse  . In J . 
Hinds ( ed .) Anaphora in Discourse , Linguistic
Research , Edmonton , Alberta.
Ting-Chichanties Tang (1975) , A Case Grammar Classification of Chinese Verbs , H a \[-
Gun Book Company , Taipei , Taiwaa.
A(:Iq ~ SI ) ECOLING-92 , NANTES , 2328 Aolrr 1992 277 PRO : . ov COLIN ( L92, NANTES , AUG .  2328 , 1992 g , type(nil )): 111'(g , type(pronoun )): 2I\\[(g , ante(theme )): 2i\I(c , anaphor ( theme )): 2(g , ante(theme )): 7\\[ic , fl(thenm )): 2(c , fl(theme)):7\[\~c , anaphor ( the ine ) : 7 i \ . 
(g , ante(agent )): 4\[/ I(c , fl(agent )): 4"--l~x~tc , anaphor ( theme )): 2(c , f2(theme )): 2i\[ ,   . \] I(c , anaphor(agent )): ~
Fig .  1 . Ag-c-hierarchy of GBM(g , ante(theme )):3 inst:\[(g , type(pronoun ))\] j Fig . 4 A new GBM after inserting a new instance\[(g , type(pronoun)) , (g , ante(theme ))\] 1:\[( g , type(nil)) , (g , ante(theme ))\]:-\[( c , :fl(gheme)) , (c , anaphor ( theme ))\] 2:\[( g , type(nil)) , (g , aute(agent))\]:-\[(c , :fl(agent)) , (c , anaphor ( theme ))\] 3:\[( g , type(nil)) , (g , ante(agen?))J:-\[(c , fl(agent)) , (c , : f2 ( theme )) ,  (? , a . aaphor(agent )) ' 14:\[( g , type(nil )) , ( g , ante(agent ))\]:-\[( c , fl(agent ))\] 5:\[( g , type(nil ))\]:-\[\]( c , fl(theme)):9\]I(c'fl(agent )):416:\[( g , type ( pronotm )) , ( g , a . ate(gheme ))\] :- c , a maphor ( theme )): 9\[/\[(c , : f1 ( theme )) ( c , a aaphor(th~e ))\]( g , ante(theme )): 9\[Fig . 5The sample rules generated : from\[(c , anaphor ( theme )): 2" Fig .  1 . 
\[( g , type(nil )) : 2 ~ g , ante(agent )): 2 , lg , type(pr?n ? un )) i ~( c , f2(theme )): 2\[(g , type(nil )): 2g , type(nil )): 7\]( g , ante(agent )): 2 ,  .  \[
Fig .  2 . Ac-g-hierarchy of GBM(g , type(nil )): 2(g , ante(theme )): 21 Fig . 3 AGBM with two training instances
I/c , anaphor/theme /):'// k(c , anaphor\[theme )): 2\(g , type(nil )): 2\2~\]( g , ante(agent )): 2\(g , type(pronoun )): 2'5 ( c , anaphor(agent )): ~( g , ante(theme )): 2(c , f2(theme )): 2(c , s2(obj )): 2(g , type(nil )): 2/( c , p(pv )) : 2(g , ante(agent )): 2\](g , ante(theme )): 71 Fig .   6 The cg hierarchy after conflict resolution from Fig  . 2 Acr Es DECOLING-92 . NANTES . 2328 AO0r1992278 PROC . OFCOLING-92, NANTES . AUG .  2328 ,   1992 Table 1: TaPs criteria for choosing anaphora I .   .   .   . wneos_e\]zero--5-~ro ~ no noun ~ rouou ~-~~_ nom , naljIN otpermitted\[nominal\]zero\[~- ~ - - -- ze-m\]no ~  . uj ~ ~ J . c~-#& Tgf &__ ~ ~ i ~ pre~g 87
T ~ m : esande feeces ~ . feature ~ notation .   .   .   . 
semantic antecedentante CASE ) ~ semantt crole\[ti ( CASE ) ' ~ I -- anaphori CXSE ) -- ~ an or no ~ ~ ~ n ( nonhm~--syntactic anaphora\[type ( X ) 1___ subject . . . . \[ sz(suz ~, . . . 
I position of an ap\[i-o ~-- p~gv~o~--\]I bv : before verb  ; pv:post-verb\[______ . ~ finite . \[ d(u on definite )__
Ico . ne ~ or Ic?n/s ) notation : ' GAS rE-rE-rE-rE-rE-~g-presents a variable for a semantic role an ~ order in a sequence of roles  . 
Tans = Chm~--ru~-lg number
Accuracy rate ( for resolving )
Accuracy rate ( for choosing ~ g number ( after resolving ) Table 3: Comparison of G-UNIMEM using ~ against Tai's criteria Group Candidate E  3  ~  Y-~"qS"0-  ~ - -~- '  E~20  " ~  TaP~4- -choice criteria 30** specific 58/62   87/96   104/110   114/120=95  . 0%,, one 58/6286/96104/1101 11/120=:92 . 5% trst\]'?/~53/6083/99-~95/109 109/120:-'90 . 8?70-~TT2ffg"TE=71 . 7 ~ U-o specific 56/6583/10088/110 101/120=84 . 2% 4-----~*~--notation :* . " for choos in ~ r ~ ~ g ; accuracy ~ g~'cc~s-that have applicable rules  ; none , in column 2 means no contingent features are used . 
Table 4: Comparison of different approach ~
MethOds ~_ ~ os ~1 . --history list L~___~_~esearch\["llobbs "s syntactic --~'-----/'-~-~~ e ~ y-~-y ~\]  . al ~; or it hm~J~smter , ' g-----coo~pre , ct ~ ~\ [ . ~ Chen's criteria \ [ eho ~ pl_~ . dlc~not easy J\]G-UNIMEM & dominant \] choose & relive\]predict\]easy J 
I features \] ~_ ~ J
AcrEsDECOLING-92 , NANTES , 2328 noun "1992279 PROC . OFCOL1NG-92, NANTES , AUG . 23-28, 1992 Appendix A . The basic classifier algorithm input : The current node N of the concept hierarchy  . 
The name I of an unclassified instance.
The set of I's unaccounted features F.
Results : The concept hierarchy that classifies the instance  . 
Top-level call : classifier ( Top-node , I , F ) Variables : N , N ' , C and NC are nodes in the hierarchy . 
G , H , and K are sets of features.
J is an instance stored on a node.
P ~ is a variable of set.
Classifier(N , 1, F).
Let G be the set of feature stores in N .
Let H be the features in F that match features in G  . 
Let K1 be the features in F that do not match features in G  . 
Let K2 be the features in G that do not match features in H  . 
Let H ' , KI ' and K2' be the sets of features after Adjust ( H , K1 , Ki , H' , KI ' , Ki ' ) /* adjust goal and cause features for g-c -hierarchy or c-g-hierarchy */ if N is not the root node  , then if H is empty set /* no features match */ then return False else if both H ' and KI ' are not empty sets then ~/* split node N*/sphtN into N ' and NC where NC is a child of N '  ; N ' contains features in H ' with confidence scores and I as a instance with features KI'  ; each confidence score in tl ' is increased by 1 ; the remaining features and instances belong to NC  ; return Split . 
) else if It ' and H are equal /* all features match */ then increase a ch confidence score in N by  1  . 
for each child C of node N /* continue match remaining features*/call Classifier  ( C , IKI ') and collect returns to the set It , if any Classifier(C , IKI') call return True or Split then break . 
if 1% is \[ False \]/* All trials fail , try to do generalization */ then for each instance J of node N call Generalize  ( N , J , I , KI ') and collect returns to the set 1% , if any Generallze(N , J , I , KI ') call return True then break . 
if t t is \[ False \] /* All trials fail , insert I as an instance of N*/then store I as an instance of node N with features KI'  . 
return True.
Appendix B . Sampler tllen of regularity ~ it h high probability of appearance in Horn-like clauses \ [ an  ~  ; e(agent ) , gype(nil)\]:-\[anaphor(agent ) , f2 ( theme ) , ft(agent)\]\[ante(agent ) , type(nil)\]:-\[anaphor(agent ) , fl(agent)\]\[ante(theme ) , type(nil)\]:-\[p(bv ) . s2(sub ) , d(nondefinite ) , anaphor ( agent ) , fI(agent ) , f2(theme)\]\[ante(theme) . type(nil)\]:-\[h(nonhm ), anaphor ( theme ) . fi(agent ), fi(theme)\]\[ante(theme) . type(nil)\]:-\[anaphor ( theme ) , fl(theme)\]\[ante(arE ) . type(pronoun)\]:-\[anaphor(agent ) , f2(pred ) , fl(arg)\]\[ant ? ( agent ) . type(pronoun)\]:-\[h(hm ) . d(definite ), con(s ) 0 anaphor ( agent ) . f2 ( theme ) , fi(agent)\]\[unte(art ) , type(pronoun)\]:-\[k(hm ) , anaphor ( a~ent ) , f2(pred ) , fl(arg)\] . .
\[ ante ( theme) . type(pronoun)\]:-\[s2(obj ) , p(pv ) , d(definite ) , anaphor ( theme ) , fI(agent ) , f2 ( theme ) J\[ante(art ) . type(pronoun)\]:-\[h(hm ), anaphor ( theme ) . f2(pred ) , fI(arg)\]Acrf~s DECOLING-92 , NAN'IT . S , 2328 Ao U'r 1992 280 PROC . OFCOLING-92, NANTES . AUG .  2328, 1992
