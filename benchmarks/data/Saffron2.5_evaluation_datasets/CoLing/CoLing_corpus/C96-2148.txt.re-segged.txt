POS Tagging Using Relaxation Labelling
Llufs Padr6
Departamenl ; de Llenguatge si Sistemes Informgtics
Universitat Polit6cnica de Catahmya
Pan Gargallo , 5.08071 Barcelona , Spain
padro ~ lsi , upc.es

Relaxation labelling is an optimization technique used in many fields to solve constraint satisfael  , ion problems . The algorithm finds a combination of values for a set of variables such that satisfies-to the maximum possible degree-a set of given constraints  . This paper describes some experiments performed applying it to POS tagging  , and the results obtained , it also ponders the possibility of applying it to Word Sense Disambiguation  . 
1 Introduction and Motivation
Relaxation is a wellknown technique used to solve consistent labelling problems  . Actually , relaxation is a family of energy-function -minimizing algorithms closely related to Boltzmann machines  , gradient step , and Hopfieldnets . 
A consistent labelling problem consists of , giwm a set of variables , assigning to each variable ala-bc'l compatible with the labels of the other ones  , according to a set of compatibility constraints . 
Many problems can be stated as a labelling problem : the travelling salesman problen  4 n-queens , corner and edge recognition , image smoothing , etc . 
In this paper we will try to make a first , insight into applying relaxation labelling to natural language processing  . The main idea of the work is that NLP problems uchas POS tagging or WSD can be stated as constraint satisfaction problems  , thus , they could be addressed with the usual techniques of that field  , such as relaxation labelling . 
It seems reasonable to consider POS tagging or WSD as combinatorial problenrs in which we have a set of variables  ( words in a sentence ) a set , of possible labels for each one ( POS tags or senses )  , and a set of constraints for these labels . We might also coinbine both problems in only one , and express constraints between the two types of tags  , using semantic information to disambiguate POS tags and visce versa  . This is not the point ; in this paper , but it will be addressed in fllrther work . 
2 Relaxation Labelling Algorithm
Relaxation labelling is a generic name for a family of iterative algorittuns which perform function optimization  , based ( mlocal infi ~ rmation . See ( Tor-ras89) for a clear exposition . 
Let V = v l , v2, .   .   .   , v , ,~ be a set of variables Let t = ,   , , , ~ be the set of possilfle labels for variable vi . 
Let Cb ' be a set : of constraints between the labels of the variables  . Each constraint CCCS states a " compatibility value " C  ,  . ibra coln bina-lion of pairs variable-label . Constraints can be of any order ( that is , any number of variables may be involved in a constraint  )  . 
The aim of the algorithm is to find a weighted labelling such that " global consistency " is maximized  . A weighted labelling is a weight assign a-tion for each possibh '  , label of each variable . Max-infizing " Global consistency " is defined as maxi-  ) i ) i is the weight mizing ~ j tjxSij , Vvi . Where Ij for label j inw triable vi and Sij the support received by the same combination  . The support for a pair w ~ riable-label x presses how compatible is that pair with the labels of neighbouring variables  , according to the constraint set . 
The relaxation algorithm consists of : ? start in a randoln weighted labelling  . 
? f breach variable , compute the " support " that each label receives froln the current  . weights for the labels of the other variabh ; s . 
? Update the weight of each variable label ac- ( : ording to the support obtained . 
? iterate the process until a convergence criterion is met  . 
The support computing and label weight changing must be perforn md in parallel  , to avoid that changing the a variable weights would affect t  ; he support colnputation of the others . 
The algorithm requires a way to compute which is the support for a wn'i able label given the others function "  . 
Several support , functions are used intire literature to define the support received by label j of variable i  ( Sij )  . 
Being : 1"1?'dR ~ j=,"Ir--\[(v , , , tk ~), . . . , (~, *) ,  .   .   . , ( v , . , , t . k , , ) \] tile set of constraints on label j for variable i  , i . e . the constraints formed by any coinbination of pairs variable-label that includes the pair  ( vi , t ) . 
rll ) k,(m ) the weight assigned to label t ~ . ~ for variable v ,, ~ at time m . 
TO ( V ) the set of all possible subsets of variables in

R  ~ ( for GET ? ( V ) ) the set of constraints on tag i ieor word j in which the involved variables are exactly those of G  . 
Usual support flnmtions are based on coinput-ing , for each constraint r involving ( v i , t ) , tile " constraint influence " , Inf(r ) = C ,  . xp~'(m)x . .  . xp~Z . , ( m ) , which is the product of tile current weights for the labels appearing the constraint except  ( vi , t )   ( representing how applicable is tile constraint in the current context  ) multiplied by C . , . 
which is the constraint compatibility value ( stating how compatible is the pair with the context  )  . 
The first formula combines influences just adding them :  ( 1 . 1 ) Sij = ~ Inf ( r ) rGRij The next for nmla adds the constraint influences grouped according to the variables they involve  , then multiplies the results of each group to get the final value :  ( 1 . 2 )  &- -  11 The last formulaistile same than the previous one  , but instead of adding the constraint influences in the same group  , just pick stile maximum . 
(1.3) Sij = II maxInf(r)
The algorithm also needs art " updating function " to compute at each iteration which is tile new weight for a variable label  , arrd this computation must be done in such a way that it can be proven to meet a certain convergence criter km  , at least under appropriate conditions 1 Several formulas have been proposed and some of them have been proven to be approximations of a gradient step algorithin  . 
Usual updating flmctions are the following.
~Convergence has been proven under certain conditions  , but in a complex application such as POS gagging we will lind cases where it is not necessarily achieved  . Alternative stopping criterions will require further attention  . 
Tile first formula increases weights for labels with support greater than  1  , and decreases those with support smaller than 1 . The denonfinator expression is a normalization factor  . 
(2 . 1) p(m+1) = ~; ~ where S , ij > 0 ik I The second formula increases weight for labels with support greater than  0 and decreases weight , for those with support smaller than 0 . 
~( ~ , , ) x (1 + & j ) (2 . 2) + 1) = k = l where - l < Sij <_+1
Advantages of the algorithm are : ? Its irighly local character  ( only the state at , previous time step is needed to compute each new weight  )  . This makes the algorithm highly parallelizable . 
? Its expressivity , since we state the problem in terms of constraints between labels  . 
? Its flexibility , we don't have to check absolute coherence of constraints  . 
? Its robustness , sin(:( , ' it can give an answer to problenls without an exact solution  ( incompatible constraints , insufficient data . . . ) ? Its ability to find local-optima solutions to NP problems in a nonexponential time  . 
(Only if we have an upper bound for the nun > ber of iterations  , i . e . convergence is fast , or the algorithm is stopped after a fixed number of iterations  . See section 4 for further details )
Drawbacks of tire algorithm are : ? Its cost . Being n the number of variables , v the average number of possible labels per variable  , c the average number of constraints per label , and Itire average number of iterations until convergence  , tile average cost is n x v x c x i , an expression in which the inulgi~plying terms , night ; be much bigger than n if we deal with probh ' , ms with many values and constraints , or if convergence is not quickly achieved . 
? Since it acts as an approximation of gradient step algorithms  , it has similar weakness : Found optima are local , and convergence is not always guaranteed . 
? Inge , ne , ral , constraints must be written mann-ally , since they at (' , the modelling of the problem . This is good for easily model able or reduced constraint-set problems  , but in the case of POS tagging or WSD constraints are too many and too complicated l  ; o be written by hand . 
8'78 ? The diificulty to state which is the " ( : omt ) at-ibility value " for each constraint . 
? The , difficulty to choose the support and updating fun ( ' tions more suitable for ea ( : hl ) ar-titular prol ) lem . 
3 Application to POS Tagging
In this section we expose our application of relaxation labelling to assign  1  )  ; u't of speech tags to the words in a sentenc , e . 
Addressing tagging problems throughot ) timiza-tion methods has been done in ( Schmid 94 )   ( POS tagging using neural networks ) and in ( Cowie et al . 92) ( WSD usings in mlated annealing ) .   ( Pelillo & Iefice 94 ) use a toy POS tagging l ) i ' oblenl to ex-t ) eriment heir methods to improve the quality of e o I n t  ) atibility coeflh:ients for the constraints used by a relaxation labelling algorithm  . 
The model used is liet blh ) wing : each word ill the text is a variable and may take several hfl  ) els , which are its POS tags . 
Since the number of variabh ~ slind word position will vary from one senten  ( : e to another , constraints are expressed in relative terms ( e . g . 
\[(vi , Determiner ) ( v . i , , , Adjective ) ( vi , 2, Nou'r0\]) . 
The Conshnint Setlelaxation labelling is a . bh ~ to deal wil ; h constraints 1) etween any subset of wn ' ial ) les . 
Any rehttionship between any subset of words and tags may  1  ) e expressed as constraint and used l ; of eed th (: algorithm . So , linguisl ; s are fre( , to express ; my kind of constraint an ( l are not restricted I : opreviously decided patl ; erns like in ( Brill 92) . 
Constraints for subsets of two and three variables are automati  ( : ally acquired , and any other subsets are left , to the linguists ' criterion . That is , we are establishing two classes of constraints : the autoinatically acquired  , and them mmally written . This means that we ha . veagreat model flexibility : we can choose among a completely handwritten model  , where , a linguist has written all l ; he constraint ; s , a comph~telymm ) mat , ically lie-rived model , orally interinediate (: olnl ) in a tion of (' , on strailfl ; sfl'omea , ch(;ype . 
We can use the same information than HMM taggers toot  ) tain automatic ( : on straints : the 1 ) robability 2 . of transition fl ' om one tag to another ( bigram-or binary constraint-probability ) will give us an idea of howeomt ) atible they are in the positions i and i+1 ,   ; rod the same for l ; rigrain-orternary cbnstraint-probabilities . Extending ~ Esl ; imated fi'om occurrences in tagged (: or t ) or ~ t . 
W ( : prefertll ( : use of supervis ( :d training ( sin ( : e large enough corpora arc available ) because of the difficulty of using an unsut ) ervised method ( such as Bmm > Welch reestimation ) when dealing , as in our case , with heterogeneous constraints . 
this to higher order constraints is possil ) le , but ; would result in prohibitive comtmt ; ational costs . 
l ) ealing with han ( l-written constraints will not be so easy , since it ; is not obvious \] low to compute " transition probabilities " for a comph:x con-straint Although accurate-but costly-methods to estimate comt  ) al ; ibility values have been proposed in ( Pelillo & Hetice 94 )  , we will choose a simpler an ( t much ( : heap tw ( : Olntmtationally solution: ( JO Hll ) llt-ing the compatibility degree fl ) r the manually written constraints using the number of occurr  ( ' nees of the consl ; raint pattern in the training ( : or Ims to comtmte the prol ) ability of the restricted word-tag pair given the contexl  ; defined by the constraint a II . elaxation doesn't need-as HMMs ( h ) -the prior prot ) at ) ility of a certain tag for a word , since it is not a constraint , but il ; Call \]) ell SCd to SOt ; the initial st ; at ( ; to a 11 ot temple t ; elyrall ( lol\[IOll C . hfi-tially we will assign to each word il ; smost I ) ro /) able tag , so we start optimization in a biassed point . 
Alternative Supportl , %nctions
The sut ) port functions described in section 2 are traditionally used in relaxation algorithnts , it seems better for our purt ) ose to choose an additive one , since the multiplicative flm ( : tions might yiehlze roortiny values when-as in Ollrcose-for  , q ( : crtain val'iable or tag no constraints are available for a given subsel  ; of vm'ial ) les . 
Since that fllnt : tions are general , we may try to lind ; ~ suI ) I ) or tflmctkm more speciiict brourt ) rol ) - h:m . S in ( : eIIMM slind the max in mmsequ ( :n ( : e probat ) ility and relaxation is a maximizing algo-rii ; hm , we (: an make relaxation maximize th ( , 'se-(lllenc(?t ) robability an ( lwe should gel ; tile same results . To a (: hieve this we define a newSul ) portflmc , l ; ion , which is the sequence i ) robability :
Being : tktile tag for varial ) h : ' vk with highest weight value a ~ the current tilne step  . 
7 r(Vt , t1)\[ ; he probal ) ility for t ~ he sequence to sl ; art in tag tI . 
P(v , t ) the lexical probability for the word repre-se\ ] tted by v to have t  ; agt . 
T ( tl , I2) the probability of tag t 2 given that I ; he previous one is t l . 
~ itj the set of all tern m'y constrain l ; son tagj for wordi . 
II , q?H . . . the : (: to fall hand written constraints On ( ; ag3k ) r wordi . 
We define := ? t )?
N ! k--l .   , k/i a This is an issue that will require fitrtl , erati:en-lion , since as constraints can be expressed in several degrees of g  ( merality , l ; he estimated probabilities may vary greatly del ) ending on how t ; he constraint was expressed . 

To obtain the new support function : (3.1)
Compatibility Values
Identifying compatibility values with transition probabilities may be good for ngram models  , but it is dubious whether it can be generalized to higher degree constraints  . In addition we can question the appropriateness of using probability values to express compatibilities  , and try to find another set of values that fits better our needs  . 
We tried several candidates to represent compatibility : Mutual Information  , Association Ratio and Relative Entropy . 
This new compatibility measures are not limited to  \[0  , 1\] as probabilities . Since relaxation updating functions (2 . 2) and (2 . 1) need support values to be normalized , we must choose some function to normalize compatibility values  . 
Although the most intuitive and direct scaling would be the linear function  , we will test as well some sigmoid-shaped hmctions widely used in neural networks and in signal theory to scale free-ranging values in a finite interval  . 
All this possibilities together with all the possibilities of the relaxation algorithm  , give a large amount of combinations and each one of them is a possible tagging algorithm  . 
4 Experiments
To this extent , we have presented the relaxation labelling algorithm family  , and stated so ine considerations to apply them to POS tagging  . 
In this section we will describe the experiments performed on applying this technique to our par -tieular problem  . 
Our experiments will consist of tagging a corpus with all logical combinations of the following parameters : Support function  , Updating function , Compatibility values , Normalization function and Constraints degree , which can be binary , ternary , or handwritten constraints , we will experiment with any combination of them , as well as with a particular combination consisting of a backoff technique described below  . 
In order to have a comparison reference we will evaluate the pertbrmance of two tuggers : A blind most-likely-tag tagger and a HMM tagger  ( Elworthy 93 ) performing Viterbi algorithm . The training and test corpora will be the same for all tag-germ All results are given as prec is ion percentages over ambiguous words  . 
4.1 Results
We performed the same experiments on three different corpora : Corpus SN  ( Spanish Novel ) train:15Kw , test : 2Kw , tagset size : 70 . This corpus was chosen to test the algorithm in a language distinct than English  , and because previous work ( Moreno-Torres 94 ) on it provides us with a good test bench and with linguist written constraints  . 
Corpus Sus(Susanne ) train:141Kw , test : 6K w , tagset , size : 150 . The interest of this corpus is to test the algorithm with a large tagset  . 
Corpus WSJ ( Wall Street Journal ) train : 1055Kw , test : 6K w , tagset size: 45 The interest of this corpus is obviously its size  , which gives a good statistical evidence for automatic on straints acquisition  . 
Baseline results.
Results obtained by the baseline tuggers are found in table  1  . 

Most-likely\[MM 94.62%
SusWSJ 86.01% 88.52% 93.20% 93.63%
Table 1: Results achieved by conventional tuggers.
First ; row of table 2 shows the best results obtained by relaxation when using only binary constraints  ( B )  . That is , in the same conditions than HMM taggers . In this conditions , relaxation only performs better than HMM for the small corpus SN  , and tile bigger the corpus is , tile worse results relaxation obtains . 
Adding handwritten constraints ( C).
Relaxation can deal with more constraints , so we added between 30 and 70 handwritten constraints depending on the corpus . The constraints were derived ~ malyzing the most frequent errors committed by tile HMM tagger  , except for SN where we adapted the context constraints proposed by  ( Moreno-Torres 94 )  . 
The constraints do not intend to be a general language model  , they cover only some common error cases . So , experiments with only handwritten constraints are not performed  . 
The compatibility value for these constraints i coinputed from their occurrences in the corpus  , and may be positive ( compatible ) or negative ( incompatible )  . 
Second row of table 2 shows the results obtained when using binary plus handwritten constraints  . 
In all corpora results improve when adding handwritten constraints  , except in WSJ . This is because the constraints used in this case are few  ( about 30 ) and only cover a few specific error cases ( mainly tile distinction past/participle following verbs to have or to be  )  . 
Using trigram information ( T).
We have also available ternary constraints , extracted from trigram occurrences . Results ob-_cJ96 . 54%
Sus 91.65%
WSJ ~79 . 34 V 7/0 92 . 50% 89 . 24% 88 . 6 oo of 8-97~33~-89 . 83% ~ . y80/ ~0 , Table 2: Best relaxation results using every combination of constraint kinds  . 
tained using ternary constraints in combination with other kinds of information are shown in rows 
T , BT , TC and BTC in table 2.
There seem to be two tendencies in this table : First  , using trigrmns is only help flfl in WSJ . 
This is becmme the training cortms for WSJ is much bigger than in the other cases  , and so the tri-grmn model obtained is good , while , for the () tilerc <) r pora , the training set ; seems to t ) e too small to provide a good trigram iniormation . 
Secondly , we can observe that there is a general tendency to " the more information  , the better re-suits " , that ix , when using BTC we get l ) etter re-suits that with B ~ , which is in turn better than
T alone.
Stopping before e onve ~ yenee.
All above results at '(; obtain e . dst opt ) ing the relaxation ; algorithm whimit reaches convergence ( no significant cbmges are l ) rodu ( : edfl ' om one iteration to the next )  , but relaxation algorithms not necessarily give their l  ) est results at convergence 4 , or not always need to achieve convergence to know what the result will be  ( Zucker et al 81 )  . So they are often stoplm d after a few iterations . Actually , what we arc ( loing is changing our convergen ( ' e criterion to one more sophisticated than " s to 1  ) when dlere are no In ore changes " . 
The results l ) resented in table 3 are tit ( ; best overall results dmt we wou M obtain if we had a criterion which stopped tit  (  ; iteration f ) rocess when the result obtained was an optimum . The number in parenthesis is the iteration at , which the algorithm should be stopped . Finding such a criterion is ~ point that will require fllrther research  . 

Table 3: Best results stopping before conw . ~rgence . 
4 This is due to two main reasons : (1) 2t , (' . optimum of tit (* , supI ) or t function doesn't corresponde a ; actly to the best solution for the problem , that is , the chosen flmction is only a , n approximation of the desired one . 
And ( 2 ) performing too much iterations can produce a more probable solution  , which will not necessarily be the correct one . 
These results are clearly better than those obtained at  ; relaxation convergence , and they also outperform HMM taggers . 
Searching a more specific support flLnction.
We have t ) een using support fimctions that are traditionally used in relaxation  , but we might try to st ) ecialize relaxation labelling to POS tagging . 
Results obtained with this specifics ut ) t ) or tfun ( :- tion ( 3 . 1) are sumntarize . dintable 4
SNS us
Table 4: Best results using a specific support fun < :- :- tkm . 
Using this news upt ) or tfun ( : tion we obtain re-suits slightly below those of the IIMM tagger  , Oursut ) i ) or tfun ( : tionistim sequence 1 ) robal ) il-ity , which is what Viterbi maxin fizes , 1) ut we get worse , results . Tlmrc are two main reasons for that . The first one is that relaxation does not maximize the sui  ) t ) or t ; flln('tion but the weigh , ted support for each variable , so we ' are not doing exactly the same than a HMM tagger  . Second reason is that relaxation is not an algorithm that finds global optima an  ( 1 can be trapl ) ed in local maxima . 
Combining information in all ack-off h , ierarchy.
Wh can confl ) inebigram and ti'i granlinfi'oma-tionina . backoff mechanism : Use trigrams if available and bigrmns when not  . 
Results o ) tained with that technique at '(' , shown in table 5
SusWSJ\[92 . 31% (3'-~)_t 93 . 66% (4) t 94 . 29% (4)\] Table 5: Best ; results using ~* backoff'technique . 
The results he , repoint to the same conclusions than the use of trigrams : il ! we have a good trigrmn model  ( as in WSJ ) then the backoff " technique is use fifl , and wege there the best overall result for tiffs corlms  . If the trigram model ix not so good , results are not better than the obtained with l ) igrams a hme . 
5 Application to Word Sense
Disambiguation
We can apply the same algorithm to the task of disambiguating tile sense of a word in a certain context  . All we need is to state tile <' , on sl xaints between senses of neighbour words . We can coin-bine this task with POS tagging , since t , here ~ are also constraints between the POS tag of a word at tdits sense  , or the sense of a neighbour word . 

Preliminary experiments have been performed on SemCor  ( Miller et al 93 )  . The problem consists in assigning to each word its correct POS tag and the WordNet file code for its right sense  . 
A most likely algorithm got 62% ( over nouns apperaring in WN )  . We obtained 78% correct , only adding a constraint stating that the sense chosen for a word must be compatible with its 
POS tag.
Next steps should be adding more constraints ( either handwritten or automatically derived ) on word senses to improve performance and tagging each word with its sense in WordNet instead of its file code  . 
6 Conclusions
We have applied relaxation labelling algorithm to the task of POS tagging  . Results obtained show that the algorithm not only can equal markovian taggers  , but also outperform them when given enough constraints or a good enough model  . 
The main advantages of relaxation over Markovian taggers are the following : First of all  , relaxation can deal with more information ( constraints of any degree )  , secondly , we can decide whether we want to use only automatically acquired constraints  , only linguist-written constraints , or any combination of both , and third , we can tune the model ( , ~dding or changing constraints or compatibility coefficients  )  . 
We can state that in all experiments , the refinement of the model with handwritten constraints led to an improvement in performance  . 
We improved performance adding few constraints which were not linguistically motiwtted  . Probably adding more " linguistic " constraints would yield more significant improvements  . 
Several parametrizations for relaxation have been tested  , and results seem to indicate that : ? support function  ( 1 . 2) produces clearly worse results than the others . Support flmction (1 . 1) is slightly a head (1 . 3) . 
? using mutual information as compatibility values gives better results  . 
? waiting for convergence is not a good policy , and so alternative stopping criterions must be studied  . 
? the backoff technique , as well as the trigram model , requires a really big training corpus . 
7 Future work
The experiments reported and the conclusions stated in this paper seem to provide a solid background for further work  . We intend to follow several lines of research : ? Applying relaxation to WSD and to WSD p ! us POS tagging  . 
? Experiment with different stopt ) ing criterions.
? Consider automatically extracted constraints ( Mhrquez & Rodrlguez 95 )  . 
? Investigate alternative ways to compute compatibility degrees for handwritten constraints  . 
? Study backoff techniques that take into account all classes and degrees of constraints  . 
? Experiment stochastic relaxation ( Sinmlated annealing )  . 
? Compare with other optimization or constraint satisfaction tee hl fiques applied to 
NLP tasks.

I thank Horacio Rodfguez for his help , support and valuable comments on this paper . I also thank Kiku Ribas , German Rigau and Pedro Meseguer for their interesting suggestions  . 

Brill , E . ; A simple rule-based part-of-speech tagger . ANLP 1992 Cowie , J . ; Guthrie , J . ; Guthrie , L . ; Lexical Disam-biguatio'n using Simulated Annealing DARPA Speech and Natural Language  ; Feb . 1992 Elworthy , D . ; Part of Speech and Phrasal Tagging . 
ESPRITBRA-73 15 AcquilexiI , Working Paper 10 , 1993 Mhrquez , L . ; Rodrfguez , H . ; Towards Learning a Constraint Grammar from Annotated Cool  , ova Using Decision Trees . ESPRITBRA-73 15 Ac-quilexII , Working Paper , 1995 Miller , G . A . ; Leacock , C . ; Tengi , R . ; Bunker , R . T . ; A semantic concordance ARPAWks on
Human Language Technology , 1993
Moreno-Torres , I . ; A morphological disambigua ~ tion tool ( MDS ) . An application to Spanish . ES-PRIT BRA-73 15 Acquilex II , Working Paper 24 , 1994 Pelillo , M . ; Refice M . ; Learning Compatibility Coefficients for Relaxation Labeling Processes  . 
IEEE Trans . on Patt . An . & Maeh . Int . 16, n.

Schmid , It .; Part of Speechlhgging with Neural
Networks COLING 1994
Torras , C . ; Relaxation and Neural Learning : Points of Convergence and Divergence  . Journal of Parallel and Distributed Computing 6 , pp . 217-244 (1989) Zucker , S . W . ; Leclerc , Y . G . ; Mohammed , J . L . ; Continuous Relaxation and local maxima selection : Conditions for equivalence  . IEEE Trans . 
on Patt . An . & Mach . Int . 3, n . 2(1981)
