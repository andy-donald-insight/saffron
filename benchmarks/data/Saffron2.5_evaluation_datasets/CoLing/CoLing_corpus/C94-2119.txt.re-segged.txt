Generalizing Automatically Generated Selectional

Ralph Grishman and John Sterling
Comlmter Science Department , New York University
715 Broadway , 7th Floor , New York , NY 10003 , U . S . A . 
grishlnan , sterling(O)cs.nyu.cdu
Abstract
Frequency information on cooccurrence patterns can be atttomatically collected from a syntactically analyzed corpus  ; this information can then serve as the basis for selectional constraints when analyzing new text  ; from the same domain . Tiffs information , however , is necessarily incomplete . We report on measurements of the degree of selectional coverage obtained with ditt\~r-ent sizes of corpora  . We then describe a technique for using the corpus to identify selectionally similar terms  , and for using tiffs similarity to broaden the seleetional coverage for a tixed corpus size  . 
1 Introduction
Selectional constraint specify what combinations of words are acceptable or meaningful in particular syntactic relations  , such as subject-verb-object or head-modifier relations  . Such constraints are necessary for the accurate analysis of natural anguage text + Accordingly  , the acquisition of these constraints is an essential yet time-consuming part of porting a natural language system to a new domain  . Several research groups have attempted to automate this process by collecting cooccurrence patterns  ( e . g . , subject-verb-ol ) ject patterns ) from a large training corpus . These patterns are then used as the source of seleetional constraints in attalyzing new text  . 
The initial successes of this approach raise the question of how large a training corpus is required  . Any answer to this question must of course be relative to the degree of coverage required  ; the set of selectional patterns will never be 100% complete , so a large corpus will always provide greater coverage  . We attempt to shed to some light on this question by processing a large corpus of text from a broad domain  ( business news ) and observing how selectional coverage increases with domain size  . 
In many cases , there are limits on the amount of training text , available . We therefore also consider how coverage can be increased using a tixed amount of text  . 
The most straightforward acquisition procedures build selectional patterns containing only the specific word combinations found in the training corpus  .   ( are ater coverage can be obtained by generalizing fl ' om the patterns collected so that patterns with semantically related words will also be considered acceptable  . In most cases this has been ( lot to using manually-created word classes , generalizing fi ' oul specific words to their classes  \[12  , 1 , 10\] . If a preexisting set of classes is used ( as in \[10\] )  , there is a risk that the classes awdlable may not match the needs of the task  . If classes are created specifically to capture selectional constraints  , the relnay be a substantial manual I > urden in moving to a new domain  , since at least some of the semantic word classes will be domain-specill c  . 
We wish to avoid this manual component by auto: maritally identifying semantically related words  . This can be done using the cooccurrence data , i . e . , by idea : tifying words which occur in the same contexts  ( for example , verbs which occur with the same subjects and objects  )  . From the cooccurrence data o110 Callcoiil . -pute a similarity relation between words\[8, 7\] . This similarity information can then be used in several ways  . 
One approach is to form word clusters based on this similarity relation  \[8\]  . This approach was taken by Sekine et alat UMIST , who then used the sechlsters to generalize the semantic patterns  \[11\]  . l'ereira et al \[9\] used a variant of this approach , " soft clusters " , in which words can be members of difl'erent clusters to difl'erent degrees  . 
An alternative approach is to use the word similarity information directly  , to in Dr information about the likelihood of a cooccurrence pattern from information abont patterns involving similar words  . This is the approach we have adopted for our current experiments  \[6\]  , and which has also been employed by 17 ) agan et al .  \[2\] . We corl : lttt l ; e from the co + occurrence data a " confitsion matrix "  , which measures the interchangeability of words in particular contexts  . We then use the confllsion matrix directly to geueralize the selllan-tic patterns  . 
2 Acquiring Semantic Patterns
Based on a series of experitnents overtile past two years  \[5  , 6\] we have developed the following procedure l , l ) ~ rsc the trMning corpus using a hro~d-cover ~ g ( ~ ~ r ~ i , illl\]~tr)a\[ldi'(~ll\]~triT , eLli (!) arse , ~ I , () I ) roduce solllethil/gM dilto ~ ull , F ( lf- . sl , ructure , with explicil , lylal ) olod synl ; icctici ' el a , tions su0ha , qSUII . 1 li\]CT and Ol : lJF , CT . i2 . Extract froH i ( , here gul ~ rizcdl ) ; u ' s e ; ~series of ~ riplcs of I ; hc for ni helm syntactic-reh ~ tion head-ot L argumenl , /modiic rWc will usel , heiiot M , ion < ' lui ~' ' ll/j > for ~ ilch ; t triple , ~ li ( I < ru : j > for art ~ la J : ion - - arguin < ~ ntpah ' . 
3 .  (' , Olnptite thoh'eqliency P'ofe~tc : h head and e ~ch triple in the corpllS  . If a , Selit Cilce prod liCe 8N pm'ses , ~ l , rilAegOlier ~ ti , ed froln a , singh ~ pa , rse has weight 1 IN in the t , of , M . 
Wc will Il S(~th ( ~ liot , lt tion 1 ' ~ ( < ' ll ) ir wj > ) \[' ortim froqlion 0 y of a triple , ; Uld P ' A ( , ,l('wi ) t()r the fr l ! q i le l lCy wil , hw\[iichwia4il ) eai'sas~thea , tii aplti'sei , I'C(L7
For exaniple > the S(~III rOI/C .2
Mary likes younglhiguists fron iI , hn(n'ick.
would produc ( : the regul~trized synthetic structure ( slike ( subjcci . ( upMary )) (/ , bj ( , ci ; ( nilliuguist ( apos young ) ( ffol , i(npI , i: ,   , Mck ))))) fl'Olli which i , hc following folirt , ripl0 sarcgjelit ~ i '; tl , e(\[: like subject Mary like , lbj(~otliugliistlhig;uist*>t)osyOIlllif , linguist froinl , iulerick ( ~ i wmtJlcfre(lllCncyhiforiilM ; iolll+'~weC~tlll , henestiin ~ tte the prob ; d)i lii , yi , ha , i , ~ particular headwial ) pem's with a parl , iculnr ; tl ' ~ lilllOiil , or uiodilier < r'wj > :/"_(<"2~"~" ~ , >) ~ Tli is probahility in for nl ~ ctiou would l , hcnbcused in scoi'hlgaltei'naA ; ivep~trse( , rots ,  \[ , ' or ( , lieeva , itla , tiOlll)0-lOW:how cvcr , wc will I18 (7 l , h(~t'r(~tl tl011 cytl&~a , l " directly . 
Stall3(i , he I , riplese xLr ~- tcl , in i ) n chl(tes ; tliilllil ) ( ~ r of spccial cases : l-hiewlt . hSOll low h&l , ii1o 1"1:i'egliaril , ;tl01oil \[ ltli iS I'l()litl ill I J "( i ; in pal'ticill & r ~ l ) a . ssivl:strll(:l . lircs & l'(lCilllVCl > t . edt OclnTeSllOlidillg activef()\['illS , ~ N , ) Ce that l') , < , < , ~ ( wi ) is different ~? on i/a ( wiapl > ears as a head in a triple ) sin ( casing lehc&dinal ) & rsetrecln~lyl ) rodu ( : esorer & slich triples , one forea Cll & rgtlll Clit OFnio ( lii ( ! roft lutthead . 
( a ) if ; tvm ' b has a sep ~ l~i-M ) lelmrtic lc(e . g . , " ouL " in " c , ~ u-ryout ") , this is hi ; filched t , othe head ( to crc-~ , i ; e the ticked carry-oul ) ttll(Il\]()tuIH'o ~ Li , cds is ; % Sel ) ~ trater clatioii . I ) ilfereilt p ~ v q . iclcs often corl ' oSpolid to very dilf crenl , senses of ~ w ~ r b , so this avoids coutlt ~ thigi , he subject ~ md object distrilnitious of these different selises  . 
( b ) if the Vel'llis(<tie >> , We genera , l ; ( ~ ai ' eltttion bc-COml ) lcmcnlbei , we euthesuliject and the pre(licat(~
COilli Ji Clll(!lli;.
((:) triples in which either I , he heard or l , hea , rg~tt lll(!llis ; 3 . I ) l ' OliOtlll ; IA '( ~ disc~crdcd(d)l , rilfl Cs in which the a a ' < gliln ( ~ nti ~; i , su\[ioi'din ~ tte ( : l~uise ~ credisc*ci'dcd ( lihi siuchidessut ) or ( Ihi ; ~l ; e toll junctions ; uulwn't ) sl , a , kingcl ~ tils;_c\]~_n'gunienl , s ) ( e ) l . l'iples in dic:cling negMiio/i(with&ll;M'<ff . iil\[10 ill , Of " not " or " newer " ) are ignored 3 Generalizing Semantic Patterns The proc . rdurc described M ) ow ~, producers a . set , of I~'c(luencies 3, 11dI)r(dmbilit . yesl . in tatcs based on Sl ) ccilic wordm The " trnditi<mM " ~ tp proach to gcner Mizing tiff <  ; inl\: , rm a , tionha ~ I : , ccui ; o assign the word , '-; t , () a set or ScIIl ~ tllti('C\[asges , g % n(\[thrill\[ , 0 collect the f req i le i i cy in-\[brm~tion COlnbinations of sen  , anticla . sscs\[12,1\] . 
Since ~ t ; least some of t , hese classes will be domain Sl ) ecilic , there has\[)t ! ellinl . erest in ml to mating the acquisition of these classes ~ ts well  . This c~m be done by ch , st , ( , ringl , oge to hcr words which appear in the s;m , ccontexl . . Sl . arting from the lile of l ; riplcs , thi:s involves : I . collecting for e ; ~ l:hwoMi ; he\['re ( lucil cy with which it occurs in each possil Ac context  ; f . rcx ~ mplc , for n nou , l we wouhl collect the frequency with which it occurs as the slll  ) jeci ;   ; till\[1 . he object ot '(~ ach verb 2 . delini , ga similarity ll~tSill '(': between words , which rellccts thctllll ltll'JerOfCO\]\[III \[I Oll COIIt  ( ! XL8 ill which l ; ticy nppc ~ r3 . foruiing clusters Imsod on this similarity m ~ a sul ' cSuch a procedure was performed by Sekincct M  . ~tt\[lMIS'l'\[ll\]; the sechnsl . crs were then manually r ( ! viewed~tnd thei'eStlltilt~clus ters wet '  ( ! used 1 ( ) F , Clml'-MizcSelC : Ctioll : rllpa , tller n , ' s . As it nila , rafq ) roa , ch Lo word clust errormatiouw ~ s dcscril cd by llirschum  . u ( , tal . 
in 1975 V\] . M(iro , '( .   .   . < y , rer , , ira, . t ; ~1 . \[(4l , . v . ( l , ,-sci'ibcd ; ~wordchlstei'hlgnicthodushig"softcinsl , ers ': hi which a , word C ; lll belong to several chlsl , er , q , with dil N~i'enl , chtsl , ermenll ) ership I , ' obal filith~s , ( Jlusl , ercreal ; iou has ( , he ; Ld wull , a gol , ha . I , the clusl , ers ; tr0aAIlCll~tl ) lcl ; oln ~ Ullt ; cl review and correction , () ll L he other haucl , olir experience illdicates 1 , h;~t stlcccs . qrulchlsterg ~01 if ; ra , lioildepends Oilral ; herd clh : ~ i ; cad-jltsl ; li ~ ienl , of the chlst crillg criteria . We haw ~ l , hcrc fore of similarity measnre to smooth ( generalize ) the prob- . 

Cooccurrence smoothing is a method which has been recently proposed for smoothing ngram models  \[3\]  . a The core of this method involves the computation of a cooccurrence matrix  ( a matrix of e on f l , sion probabilities ) Pc : ( wjIw0 , which indicates the prol ) ability of word wj occurring in contexts in which word wi occurs  , averaged over these contexts . 
: L , ( wjlwi ) : ~ P ( wjI , ~) P (. ~ I'~, O
E . P ( , o ~ Is ) v ( , , , ~l , ~ ) V ( s ) p ( wi ) where the sum is over the set of all possible contexts  . 
In applying this technique to the triples we have collected  , we have initially chosen to generalize ( smooth over ) the first element of triple . Thus , in triples of the form word l relation word2 we focus on word l , treating relation attd word2 as the context : :' . (~, ~ lw5 = ~ rO . : l <, '~ j >) . v(<, . w , > In , :) r:v ~
F (<~,-~ j >)/,(< w ~,,~" 5>)
Informally , we ear , say that a large value of /) C'( , ,il , ) I ) indicates that wi is selectionally ( semantically ) acceptable in the syntactic contexts where word w ~appears  . 
For example , looking at the verb " convict " , we see that the largest values of P (: ( eonvict , x ) are for a :=" acquit " and x = " indict " , indicating that " convict " is selec-tionally acceptable in contexts where words " acquit " or " indict " appear  ( see Figure 4 for a larger example )  . 
How do we use this information to generalize the triples obtained from the corpus ' ? Suppose we are interested in determining  (  . heacceptability of the pattern convict-object -owner  , ven though this triple does not a pl ) ear in our training corpus . Since " convict " can appear in contexts in which " acquit " or " indict " appear  , and the patterns acquit-object-owner and indic b o /  ) ject-owner appear in the corpus , we can conchlde thai , the pattern convict-object-owner is acceptable too  . More formally , we compute a smoothed triples frequency lP . s ' from the observed frequency / i ' by averaging over all words w ~  , incorporating frequency information for w ~ to the extent that its contexts are also suitable contexts for wi::':~*  ( < * , : i ,  .   , , , j >) --~ r "("' i l *"; ) " : : (< , , , ~  , ,  , ,: j > ) ~t Jlitor ( ler to avoid the generation of confltsion table entries from a single shared context  ( which quite often a we wish to thank Richard Schwartz of BBN for referring us to this method & lid article  . 
is the result of an incorrect I ) arse ) , we apply a filter in generating Pc : for i ? j , we generate a nonzer oPc ( wjIwj only if the wi and wj appear it * at leant two e oitll noncontexts  , and there is somee Olnnlon context in which both words occur at least twice  , l , ' urther-more , if the value computed by the formula for Pc'is less than some thresbold re :  , the value is taken to be zero ; we have used rc = 0 . 001 in the experiments reported below .   ( These tilters are not applied for the case i = j ; the diagonal elements of the confusion matrix are always e omputed exactly  . ) Because these filters may yeild an an-normalized confltsion matrix  ( i . e . , E ~ t>(*vJlv'i ) < l ) , were nor n , alize then \] at rix so that ~ ,  . j\[g , ( wi\[wi ) = 1 . 
A similar approach to pattern generalization , using a sirnilarity measnre derived fi'om cooccurrence data  , has been recently described by l ) aganeta\] .  \[2\] . The h'approach dill'ers from the one described here in two sign * titan * regards : their cooccurrence data is based on linear distance within the sentence  , rather than on syntactic relations , and they use a different similarity measure , based on mutual information . The relative merits of the two similarity rneasures may need to be resolved empirically  ; however , we believe * bat , there is a virtue to ourll On-sylnlnetri clile a Slll'e ~ be catlse  8tll  ) -stitutibility in seleetional contexts is not a symmetric relation  . 44 Evaluation 4 . 1 Eva luat ion Metr ic We have previously \[5\] described two methods for the evaluation of semantic on straints  . For tile current ex--periments , we have used one of these methods , where the constraints are evaluated against a set of manually classitied semantic triples  . ' ~ For this ( waluation , we select a small test corpus separate fl ' om the training corpus  . We parse the corpus , regularize the parses , and extract triples just as we did t br the semantic acquisition phase  . We then manually classify each triph " as valid or invalid  , depending on whether or not it arises fl ' om the correct parse for the sentence  . G We then estahlish a threshold 7' for the weighted triples counts in our training set  , and deline 4If v : lallows ahi'o , taler range of argulnents than w2 , then we can replace w2 by vq , butllOIbgiC(~versa , For (': x anlple ~ w ( ; can repla ( : e " speak " ( which takes a human subject ) by " sleep " ( which takes an animate subject )   , and still have a selectionally valid pattern , \]) tit . not the other wety around . 
~" l'his is similar to tests conducted by Pcreira ctal  . \[9\] and l ) agan et al\[2\] . The cited tests , howev cl ' , were based , m selected words or word pairs of high frequency  , whereas ore " test sets involve a representative set of high and low frequency triples  . 
stiffs is a different criterion fl ' om the one used in our earlier papers  . In our earlier work , we marked a triple as wdid if it could be valid in some sentence in the domain  . We found that it was very ( lilIicult to apply such a standard consistmltly , and have therefore changed to a criterion based on an individual sentence  . 
744 recall 0.70 0.60 0.50 0.40 0.30 0.20 0.10
RIIl I\]O00 ? ?' boo0o ~% o
Oo ?% 0 . 00 i --7 l ? ?0,60 0 . 65 0 . 70 0 . 75 0 . 80 0 . 85 precisionFigm'c1:l/ . a call/prccision trade-oil using eutire corpus . 
vq numl ', er of l . rill lcs in test set which were (' . lassitied as vMid and which a , F , l ) em'ediu training sct with count > "/' V__ lllllll  ) or oVtril ) lcs in tcsl , set which were classilic das valid m , d which n l ) pear c(I in training set with
COIl I/I . < ~/' iI-lmn , I ) eroft ril ) lcsint , est set . which were class it lcd as in wdid and which al ) peared ilt trahfing set with
CO\[llti , > "\[' and then delincv-I recall = .   .   .   . 
tJi + v_
I ) reci,~hm_-:_vq~..
v + + i q
By wu ' yingthel , hreshold , we can a ~ lcct dill \ went trade-olfs ( ) f recall and precisioli ( at high threshold , we seh ~ ct : only a small n ,   , mher of triph : s which a pl ) eared frequ ( mtly and in which wel . here for chave \] ligh conli--(h!nce , t ; hus obtaining a high precision lm ( , \]() w recall ; conversely , a tah ) wt , hrc shohl we adnd tauuuch larger n und ) er of i . riplcs , obt , aiuiug ~ high recall but lower precisiol 0 . 
4.2. t.s ~ Data
The trai , fing and Icst corpora were taken from the Wall Street  , hmrna J . In order to get higher-quality parses or I , \] l cs e , q(ml ; elices , we disahlcd some of the recovery mechanisms normally t > ed in our parser  . Of the 57 , 366 scnte , l CC Shiourt , rMidng corpus , we oht Mned comph % epars (' sIbr 34 , 4 14 and parses of initial substrings for an add it ional  12  , 441 s(mtenccs . Thesei ) m'ses were th ( m regularized aim reduced to t , riph~s . W c g c n e r a t ; ( ; d a total of 27 q , 233 distinct triples from the corpus . 
The test corpus used to generate l , hetriph~s which were mamlally classified consisl , edofl 0 art Mcs , also 0 . 60 0 . 58 0 . 56 0 . 54 0 . 52 0 . 50 0 . 48 recall 0 . 46 0 . 44 0 . 42 0 . 40 0 . 38 0 . 36 0 . 34 x .   .   .   .   .   .  1-  .   .   .   .  7  .   .   .   .   .   .   . 1 - - I 0 25 50 75 100 percent ; ~ge of corplls Figure 2: Growth of recall as a fimction of corpus size ( percentage of totM corpus used )  , o = at 72% l ) reci-siou ; ? = maxim mn reca\[\] , regardless of precision ; x : -
I ) redicted values R ) rm ~ Lximum recall
D <) m the Wall S ~; reet Journal , distinct from those in the training set . These articles produced at cs ( . set ; containing a totM of i ) 32 triples , of which 1107 were valid ; rod 825 were\[nvMid . 
4 . 3 Results 4 . 3 . 1 Growth with Corl ) usSizeW cbegan by generating triples from the entire corpus and cwdmLt  , ing the selectional patterns as < lescribed above  ; tile result h/g recall/l ) recision curve generated by wu'ying the threshold is shown in Figure  1  . 
To see how pattern coverage iwl ) roves with corpus size , we divided our training corpus into 8 segments and coHll/uted sets of tril ) lcs based on the lirst Seglllell , , the Ih'st two segments , etc . We show iu Figure 2 a plot of recall vs . corpus size , both at ~ consl , ant precision of 72% and for maximum recall regardless of precision . 7 The rate of g ; row th of the maximum recall caube understood in teruls of the frequency distribution of triples  . In our earlier work \[4\] we lit the growth data to curw~s of the form 1 -exp ( - fia : )   , ontile assump-t . i on that all selection a limtterns are t ~ qually likely  . 
This lt tay have 1 ) eelaroughly accurate assumption for that app\] ication  , involving semantic-classbased patterns ( rather t , hanword-based l );-ttl ; erns ) , and a rather sharply circumscribed sublanguage ( m ( xlical reports )  . 
For the ( word level ) pal ; i , crlls described here , howev cr , the distribution is quite skewed , with a small number of very-high-frequency l ) at l , erns , a which results in di\[:rN , , (1 , ttapoint is shown for 72% precision for the first seg-lll ( : ilt & lone ; e(:allSewe~tl'cnl)\[abletore&oh ; tprcci . % l Oll of 72 ~ with a single seglnent . 
a ' l'h c number of high q'r e ( luency patterns is m : ( : enl , u ; tted by number 100 of triples with this frequency i0  % % % **  . .- . .
I00 fl " igure 3: Distribution offre ( tuencies of triples in training corpus . Vertical scale shows number of triples with a given frequency  . 
fereat growth curves . Figure 3 plots the number of distinct triples per unit frequency  , as a function of fi'e-quency , for the entire training corpus , This data can be very closely approximated by a fimction of tile form 
N(t , ') = al ; '-~, where r~=2 . 9 . 9 q'o derive a growth curve for in axin mln recall , we will assunle that the fl'equeney distribution for triples selected at random follows the same tbrm  . Let I ) (7 ) represent he probability that a triple chosen at ran-dorn is a particular triple T  . l , et P ( p ) be the density of triples with a given probability  ; i . e . , the nmnber of triples with probal ) ilities between p and p + ( ise P ( p )   ( for small e )  . Then we areass , ,ming that P(p ) = ~ p -~ , for pranging fl'om some minimum probability Pm in to  1  . For a triple T , the probability that we would lind at least one instance of it in n corpus of w triples is approximately i--c-~p  ( T )  . The lnaximum recall for a corpus of ~- triples is the probability of a given triple  ( the " test triple " ) being selected at random , multiplied by the probability that that triple was found in the training corpus  , summed over a . ll triples : ~)( r ) .   ( 1-e-~" ( "' ) ) 7' which can be coral ) uteriusing the density function ~1P'P ( P )  '  ( 1e-"V ) dpm , nfl-~(1c-T ~' = rap . p- . ) alp , ,~ ia By selecting an appropriate value of a ( and corresponding l )  , ~ i , ~ so that the total probability is 1) , we can get a the fact that our lcxicM scmmcr replaces all identitiabl cCO lll-lYally lll Lllle S by tile token a-company  , all C/llTellcy values by a-currency , etc . Many of the highest frequency triples involve such tokens  . 
9Thls is quite shnilm ' to a Zipf's law distribution  , for which wf'c ( bondlw ) eurobond for aym ortgage objective marria genote maturity subsidy veteran commitment debenture activism mile coupon security yield issue  0  . 133 0 . 128 0 . 093 0 . 089 0 . 071 0 . 068 0 . 057 0 . 0 d 60 . 046 0 . 046 0 . 044 0 . 043 () . 038 0 . 038 0 . 037 0 . 036 0 . 0 35 Figure 4: Nouns closely related to the IIOUII'q ) ond ": ranked by t ) :  . 
good match to the actual maximum recall values ; these computed values are shown as x in Figure 2 . Except\['or the smallest dataset , the agreement is quite good considering the wwy simple assumpt  , ions made . 
4.3.2 Smoothing
In order to increase our coverage ( recall ) , we then applied the smoothing procedure to the triples fi'om our training corpus  . In testing our procedure , we lirst generated the confusioll matrix Pc and examined some of the entries  , l " igure 4 shows the largest entries in f'c for the noun " bond "  , a common word in the Wall Street Journal . It is clear that ( with some od d exceptions ) most of tile words with high t ) : wtlues are semantically related to the original word  . 
' lk ) evaluate the etl\~ctiveness of our smoothing procedure  , we have plotted recall vs . precision graphs for both unsmoothed and smoothed frequency data  . The results are shown in l , ' igure 5 . Overtile range of precisions where the two curves overlap  , the smoothed at a performs better at low precision /high recall  , whereas the unsmoothe data is better at high precision/low recall  . In addition , smoothing substantially extends the level of recall which can be achieved for a given corpus size  , although at some sacrilice in precision . 
Intuitively we can understand why these curves should  ( : ross as they do . Smoothing introduces a certain degree of additional error  . As is evident from Figure 4 , some of the confllsion matrix entries arc spurious  , arising from such SOllrces as incorrect l ) arses and the conIlation of word senses . In addition , some of the triples being generalized are themselves in correct  ( note that even at high threshold the precision is below  90%  )  . 
The net result is that a portion ( roughly 1/3 to 1/5 ) of recldl 0 . 70 0 . 60 0 . 50 0 . 40
O . 3 O0 . 20 0 . 6Oo ~' . +l , , ll , ++ ?~? - ooo*~ . Oa O0 oo* . o"t8?, ?? oo--\[ .   .   .   . I--T--?0 . 65 0 . 70 0,75 0 . 80 precision l " ip ; ure 5: Benel : its of stnoot , hiug for la . r , gcsleorl ) us : o--unstn < > othe(Ida . i , & , ? = : sH , oothed(tati ~ . 
the tril > les added by smoothing ~ l'e in corre et . At low levels of 1) recision , (, hisl ) ro(uces a . net gld nont . hel ) re-eision/rec+dl curve ;+ tt , highe , ' levels o1' precision , ' ? here ix a . netloss . In a . ny event , smoothing ( toes allow for sul ) stlml , ially higher levels o1' recall than are possible without+smoothing . 
5 Conclusion
We\]tl tve demonstrated how select . to nalp ; ~ tl ; ernsca . nlyeattt ; otn~tic ; dly acquired from it corpus , ~md howseleetion de , :) w ~ ragcgr~t(hlally it ~ ere ~ . tses with the size (, f the tra . in in georl + us . We h~tve+dso domonst ; r ; tted that I ' ~ ) rit given corpus size e ovcr ++ ge can I ) esig , tilicantly improved 1) y using 1 , he corpus1 ; o identify selectionally related ternts , and using these simila . rit . ies to generlt lize the patterns tybserved in the training eorums  . ' l ' his is consisteut with other reeo , tt results using , +e Httedl , eeh-niques\[2 , 9\] . 
We believe th+tl;LheseLt:ehniquese+tnI)eft , t'ther improved in several wa . ys . The exl)er in , ent . srel ) or l . e + labove ha+ve only get'~ra\]ized overt , ht , lit'st ; ( head ) po-sitioI , of t , he triples ; we need to melrstu ' othee Il ' < ~ ctofg cn crldizhtg ow ~ r theal ' gum < mtl ~ OSh  ; ion as w < ql . With btrgere or por ~ it , I ~ , ~ tyitlsobe I > asil ) l ~ to use lirl+ger patterns , including in p~tr tieularst , b . jeet-verl)-~d) . ieet i ) ~ tl; . 
terns , ; tnd thus reduce the confusion due to tre~t t , ing(lit'et'e , lL words senses its e Olll ltl Olle on texLs . 
References\[I\] . ling-Shin(~haatg , Yih-l:enl , uo , + rodl ( eh--Yih Su . G\[)SM : Aeo ; enera . lized l ) robt d ) il islics < . ~tllltllt ; ic model for n . ln biguil , y resolution . It+l+r occcdiP+\[Is of the , \[\] Oh Annual Meeting of tile Assn . for (/ o ~ tpu-rational Linguistic . s , l ); tges17718/t , Newark , I)E,
June 1992.
\[2\]I dol ) aglm , Hh ~ ml M~u'cus , ; tud~q h~ul M~rkovilch . 
(3ontextual word similarity ~ md estintnti ( m from spa . rse(lal , c ~ . Inl " l " oeeedings of the , "7/ . stAn~lualMt:t' . li~g+4"lhtAssn . for Co~ltp ' atatio ~ tal Li ? Jg'llis-tics , pa . ges3l37, ( lolunibus , 011, . lune 1993 . 
\[5;\]U . Essen+rod V . Steinbiss . ( ~ oocct a ' renc~stnoothing for st . oe hltst ; ielangua . gernodeliug . It , 1(7A , %', <?1'9?~?, pages I161I164, S;I . tlI " rll . tl : isel ~, (3A , M ; i . y1992 . 
\[14\]R . (;visluna . n , I, . l/irschma . n , and N . T . Nllctn . Dis- . 
e overy procedures for sul)l~mgui~geslcet , iot ~; tlpitt-terns:Initi+dexl ) eriments . (/ oTitp+ttatio'~talLi'a-(luislic . % 1213):205 16, 1986 . 
\[5\]l + ~ rll)h(lrishm+maztd,lohnSterling . Acquisitiott of seleetion ~ di ) aAA , erns . Inl'roc . 14th lnt'l ( / o ~+ \] ~ ( /o ~ l ~ pulatio ? lalLing ~ lislics ( COLIN ( ? 92 )  , N~mtes , 
France , July 1!)92.
\[6\]R . ~ dph(~t'ishmatl~mdJohnS ( . erlhlg . Su toot Ating of autotn++t , ic . ally generltted selecl , ionttl consLr ++ int . s . 
Ill\[s ? ' occt, . diTt\[\]sojIhc\[htTtta~tL(t?ty\]~ta . qt''\]'t!c\[ti~olo(\]\[qWorkshop , l ' rine el , on , NJ , March 1993 . 
\[7\] l ) on ldd I lindle . Nounel ~ , ssitica , tAonI'rol , t predica , t+e-~+rgutnents truel ; ures . In l '?' occeding . so \] the , ? SlhAn+l+tal Meeting of the Ass'it . \] br (; o ++ + pu-talio ~ . al Ling+ui . slics , i ) a . ges268275, Jitne 1990 . 
\[8\]l , ynet\[,ellirschtnlm , H . idl)h(h'isht,t+L,l,;ttk(lNi~ot,tiS ; tger . ( ~ r&tnlrlitl+ieltlly-I ' ~ itsed~tUl ; Oln&Lic word cli~sst ' ortnlttion , l~tJ ' or ~ ttalio ? tl ' ~' occ . ~ si~t:la ', d
Manage . mcnl , 1/2):3D 57, 1975.
\[9\]l"ern+mdol'ereiri%N~d't ;; di Tishl)y , imd I , illilm Lee . 
l ) is tribution a , l clustering of English words . Inl"r'occedi~l\[lsoflhe<YlslAn~vualM celin : l of the Ass?t  . j'o't"(/omp , zlalional Li?ig+tistics , plt ges183 l'()0 ,  (' , olund ) us ,  ()11 , June 1903 . 
\[10\]PhilipR . esnik . A elass-b ; tsed:tlH)t'oach to lexical discovery , luI ' rocecdi?tgs of tile , Y OlhA?t'nualMecli'ago J " thtAssn . for ( \] o ~ lpulional L4 ' ~ . /li ~ is-lies , Newark , I)F , , . lun+~1992 . 
\[1 I\]S ~ t to shi Sekine , Soti ~ A , ~ a . niadou, . lere , , , y(~*tr , ' oll , ; u , d . lun'ie hi:l'sujii . 15 nguistie knowl <> dg ; eget,-el'i ; ~ l , or . hil'roc , l / ~ lh , htt'l(\]onfl(/omp , lialio ? lal Li ? tg ~ ti . slies((/OLIN(I9, ?3), pa . ges5G05(i6,N ; tnl . es,
I :,' a.nce , . luly 19.92.
\[12\]l :' a . ola . Vela . rdi , M ; tria . ' l'eros~t Pazienza , and Miehel+~Fa . solo . l Iow to encode semantic knowledge : Al nethod for lne~+ning represent ~ ttim and compuLer-~dded ~ cquisitic m  . (7o?np+tlalio'nalI , i ~ l-gu is lics ,  17(2):153 170 , I , 1t( . )1 . 

