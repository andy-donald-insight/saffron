Finitestate Multimodal Parsing and Understanding
Michael Johnston
AT&T Labs-Research
Shannon Laboratory , 180 Park Ave
FIorham Park , NJ 07932, USA
johnston@research , att.tom
Srinivas Bangalore
AT&T Labs-Research
Shannon Laboratory , 180 Park Ave
Florham Park , NJ 07932, USA
srini@research , art . tom

Multimodal interfaces require effective parsing and nn  ( lerstanding of utterances whose content is distributed across multiple input modes  . Johnston 1998 presents an approach in which strategies lbr multimodal integration are stated declaratively using a unification-based grammar that is used by amnlti-dilnensional chart parser to compose inputs  . This approach is highly expressive and supports a broad class of interfaces  , but offers only limited potential for lnutual compensation among the input modes  , is subject o signilicant concerns in terms o1' COml ) uta-tional complexity , and complicates selection among alternative multimodal interpretations of the input  . 
Intiffspapeh wel ) resent an alternative approa clain which multimod all mrsing and understanding are achieved using a weighted finite-state device which takes speech and gesture streams as inputs and outputs their joint interpretation  . This approach is significantly more efficienl , enables tight-coupling of multimodal understanding with speech recognition  , and provides a general probabilistic fralnew or k for multimodal ambiguity resolution  . 
1 Introduction
Multimodal interfaces are systems that allow input and/or output o be conveyed over multiple different channel such as speech  , graphics , and gesture . They enable more natural and effective interaction since different kinds of content can be conveyed in the modes to which they are best suited  ( Oviatt ,  1997) . 
Our specific concern here is with multimodal interfaces supporting input by speech  , pen , and touch , but the approach we describe has far broader applicability  . These interfaces stand to play a critical role in the ongoing migration of interaction fi'oln the desktop to wireless portable computing devices  ( PI ) As , next-generation phones ) that offer limited screen real es-tale , and other keyboard-less platform such as public information kiosks  . 
To realize their full potential , multimodal interfaces need to support not just input from multiple modes  , but synergistic multimodal utterances optimally distributed over the available modes  ( Johnston et al ,  1997) . In order to achieve this , an effcctive method for integration of content fi ' Oln dillferent modes is needed  . Johnston ( 1998b ) shows how techniques from natural language processing  ( unification-based gramumrs and chart parsing ) can be adapted to support parsing and interpretation of utterances distributed over multiple modes  . In that approach , speech and gesture recognition produce ~ , -best lists of recognition results which are assigned typed feature structure representations  ( Carpenter ,  1992 ) and passed to aluultidimensioual chart parsel ? that uses alnultimodal unification-based granunar to combine the representations assigned to the input elements  . Possible multimodal interpretations are then ranked and the optimal interpretation is passed on for execution  . This approach overcomes many of the limitations of previous approaches to multimodal integration such as  ( Bolt , 1980; Neal and Shapiro , 1991) ( See ( Johnstonctal . , 1997)(1) .  282)) . It supports speech with multiple gestures , visual parsing of unimodal gestures , and its dechu'ative nature facilitates rapid l ) rototyping and iterative develol ) meut of multimodal systems . Also , the unification-based approach allows for mutual COlnpensatiou of recognition errors in the individual modalities  ( Oviatt ,  1999) . 
However , the unification-based approach does not allow for tight-conpling of nmltimodal parsing with speech and gesture recognition  . Compensationel Lfects are dependent on the correct answer appearing in the ~  ;  , -best list of interpretations a signed to each mode  . Multimodal parsing cannot directly influence the progress of speech or gesture recognition  . 
The multidimensional parsing approach is also subject to significant concerns in terms of computational complexity  . In the worst case , the multidimensional parsing algorithm ( Johnston , 1998b ) ( p .  626 ) is exponential with respect o the number of input elements  . Also this approach does not provide a natural fiamework for combining the probabilities of speech and gestur events in order to select among multiple competing multimodal interpretations  . Wuet . al .   ( 1999 ) present a statistical approach for selecting among multiple possible combinations of speech proach will scale to more complex verbal language and combinations of speech with multiple gestures  . 
In this papm , we propose an alternative approach that addresses these limitations : parsing  , understanding , and integration of speech and gesture ampe > formed by a single finite-state device  . With certain simplifying assumptions , multidimensional parsing and understanding with multimodal grammars can be achieved using a weighted finite-state automaton  ( FSA ) running on throetapes which represent speech input  ( words )  , gesture input ( gesture symbols and reference markers )  , and their combined interpretation . We have implemented our approach in the context of a multimodal messaging application in which users interact with a company directoW using synergistic ombinations of speech and pen input  ; a multimodal variant of VPQ ( Buntschuh et al . , 1998) . For example , the user might say email this person and this person and gesture with the pen on pictures of two people on a user interface display  . In addition to the user interface client , the architecture contain speech and gesture recognition components which process in coming streams of speech and electronic in k  , and a multimodal language processing component ( Figure 1 )  . 
u , \[
ASR ~ I ~ esture Recognizer\[
Multimodal Parser/Understander \]

Figure 1: Multimodal alvhitecture
Section 2 provides background on finite-state language processing  . In Section 3 , we define and exemplify multimodal context-fiee grammars  ( MCFGS ) and their approximation as multimodal FSAs . We describe our approach to finite-state representation of meaning and explain how the three-tape finite state automaton can be factored out into a number of finite-state transducers  . In Section 4 , we explain how these transducers can be used to enable tight-coupling of multimodal language processing with speech and gesture recognition  . 
2 Finitestate Language Processing
Finite state transducers ( FST ) are finite-state automata ( FSA ) where each transition consists of an input and an output symbol  . The transition is traversed if its input symbol matches the current symbol in the input and generates the output symbol associated with the transition  . In other words , an FST can be regarded as a 2-tape FSA with an input tape from which the input symbols are read and an output tape where the output symbols are written  . 
Finite state machines have been extensively applied to many aspects of language processing including  , speech recognition ( Pereir and Riley , 1997; Riccardi et al ,  1996) , phonology ( Kaplan and Kay ,  1994) , morphology ( Koskenniemi ,  1984) , chunking ( Abney , 1991; Joshi and Hopely , 1997; Bangalore ,  1997) , parsing ( Roche ,  1999) , and machine translation ( Bangalore and Riccardi ,  2000) . 
Finite state models are attractive n~echanisms for language processing since they are  ( a ) efficiently learnable fiom data ( b ) generally effective for decoding and ( c ) associated with a calculus for composing machines which allows for straightforward integration of constraints fl ' om various levels of language processing  . Furdmrmore , software implementing the finite-state calculus is available for research purposes  ( Mohrieta \ [ . , 1998) . Another motivation for our choice of finite-state models is that they enable tight integration of language processing with speech and gesture recognition  . 
3 Finitestate Multimodal Grammars
Multimodal integration involves merging semantic content fi ' om multiple streams to build a joint interpretation for ainultimodal utterance  . We use a finite-state device to parse multiple input strealns and to combine their content into a single semantic representation  . For an interface with ni nodes , a finite-state device operating over n + 1 tapes is needed . The first ntapes represent the input streams and r ~+\] is an output stream representing their composition  . In the case of speech and pen input there are three tapes  , one for speech , one for pengesture , and a third for their combined meaning . 
As an example , in the messaging application described above , users issue spoken commands such as email this person and that organization and gest m'e on the appropriate person and organization on the screen  . The structure and interpretation of multimodal colnln and s of this kind can be captured e claratively in a multimodal contextfree grammar  . We present a fi'agment capable of handling such commands in Figure  2  . 

S.~VNP g:c:\]) NP-+I ) ETN
CONJ--4 and : E :, NP--+I ) ETNCONJNP
V -+ cmail:g:cmail(\[DET--+his:g:c
V-+page:c:page(\[I ) ET--+lhat:?:c
N --:. person : Gp : person(ENTP,Y
N-4 organization : Go:org(ENTRY
N --+ dcpartment : Gd : dept ( ENTRY
ENTRY -> C:el:elc:g :)
ENTRY -> c:e2:e2c:g :)
ENTRY-4 c:ea:eag : e :)
ENTP , Y--+...
Figure 2: Multimodal grammar fragment
The nonterminals in the multimodal grammar are atomic symbols  . The multimodal aspect sel'the grammar become apparent in the terlninals  . Each terminal contains three components W : G : M corresponding to the nq-  1 tapes , where W is for the spoken language stream , G is the gesture stream , and M is the combined meaning . The epsilon symbol is used to indicate when oue of these is empty in a given terminal  . The symbols in W are woMs from the speech stream . The symbols in G are of two types . 
Symbols like Go indicate the presence of a particular kind of gesturc in the gesture stream  , while those like et are used as references to entities referred to by the gesture  ( See Section 3 . 1) . Simple deictic pointing gestures are assigned semantic types based on tl~en-tities they are references to  . Gp represents a gestural tel'erence to a person on the display  , Goto an organization , and Gdloadepartment . Compared with a feature-based multimodal graln lnar  , these types constitute a set of atomic categories which makeltle relew mt dislinclions for gesture vents prc dicl-lug speech events and vice versa  . For example , if the gesture is G , , then phrases like thLs person aud him arc preferred speech events and vice versa  . 
These categories also play a role in constraining the semantic representation when the speech is underspecified with respecto semantic type  ( e . g . email this one ) . These gesture symbols can be organized into a type hierarchy reflecting the ontology of the entities in the application domain  . For exam-pie , there might be a general type G with subtypes Go and Gp  , where Gv has subtypes G , , , ,~ and Gpf for male and female . 
A multimodal CFG ( MCFG ) can be defined fop really as quadruple < N ,  7' , P , S > . N is the set of nonterminals . 1 ~ is the set of productions of the form A-+ ( ~ where AEN and , ~ , C(NUT )* . S is the start symbol for the grammar . 7' is these to t ' terminals of the l ' or m ( WU e )  :  ( GU e ) : M * where W is the vocabulary of speech , G is the vocabulary of gesture = Gesture Symbols U Event Symbols  ; G csturc Symbol s = Gv , Go , Gpj ' , G  ~ . .,  . . . and a finite collections of \] , gventSymbol s = c , ,c ~ ,   .   .   . , c , , . M is the vocabulary to lel ) rcsent meaning and includes event symbols ( Evenl : Symbol . sCM ) . 
In general a contextfree grammar can be approximated by an FSA  ( Pereira and Wright 1997 , Neder-her 1997) . The transition symbols of the approximated USA are the terminals of the context-fieegrammar and in the case of multimodal CFG as de-tined above  , these terminals contain three components , W , G and M . The multimodal CFG fi'ag-merit in Figurc 2 translates into the FSA in Figure 3  , a three-tape finite state device capable of composing two input streams into a single output semantic representation stream  . 
Our approach makes certain simplil'ying assumptions with respect oternporal constraints  . In multi-gesture utterances the primary flmction of temporal constraints it of orce an order on the gestures  . 
If you say move this here and make two . gestures , the first corresponds to this and the second to here  . Our multimodal grammars encode order but do not impose explicit temporal constraints  , l to w-ever , general temporal constraints between speech and the first gesture can be enforced bel brc the FSA is applied  . 
3.1 Finitestate Meaning Representation
A novel aspect of our approach is that in addition to capturing the structure of language with a finite state device  , we also capture meaning . T iffs is very important in nmltimodal language processing where the central goal is to capture how the multiple modes contribute to the combined interpretation  . Ottr basic approach is to write symbols onto the third tape  , which when concatenated together yield the semantic representation l ' or the multimodal utterance  . Itsu its out " purposes here to use a simple logical representation with predicates pred  (  . . . .) and lists la , b, . . . l . 
Many other kinds of semantic representation could be generated  . In the fl'agment in Figure 2 , the word ema?l contributes email ( \[ to the semantic stape , and the list and predicate arc closed when the rule S--+ VNP e:z:\]  ) applies . The word person writes person ( on the semantic stape . 
A signiiicant problem we face in adding meaning into the finite-state framework is how to reprc -sent all of the different possible specific values that can be contributed by a gesture  . For deictic references a unique identitier is needed for each object in the interface that the user can gesture on  . For ex-alnple , il ' the interface shows lists of people , there needs to be a unique ideutilier for each person  . As part of the composition process this identifier needs or  , mnization:Go:or-(tnat:eps:eps~z~~ . \[3 ~ eps:eZ:e2~_//~e~q'e ~ ~ . \ " \]-- . ~ cps:el~s :) + : ? , , + . 

Figure 3: Multimodal three-tape FSA to be copied from the gesture stream into the semantic representation  . In the unification-based approach to multimodal integration  , this is achieved by feature sharing ( Johnston , 1998b ) . In the finite-state approach , we would need to incorporate all of the different possible IDs into the FSA  . For a person with id objid345 you need an arc e:objid345:objid345 to transfer that piece of information fiom the gesture tape to the lneaning tape  . All of the arcs for different IDs would have to be repeated everywhere in the network where this transfer of information is needed  . Furthermore , the searcs would have to be updated as the underlying database was changed or updated  . Matters are even worse for more complexpen-based at a such as drawing lines and areas in an interactive map application  ( Cohen et al ,  1998) . In this case , the coordinate set from the gesture needs to be incorporated into the senmntic representation  . 
It might not be practical to incorporate the vast nuln-bet of different possible coordinate sequences into an 

Our solution to this problem is to store these specific values associated withincoming gestures in a finite set of buffers labeled el  , e , ) , e a .   .   .   . and in place of the specific content write in the nalne of the appropriate buffer on the gesture tape  . Instead of having the specific values in the FSA , we have the transitions E:CI:C\] , C:C2: C2 , s:e3:e:3 . . , in each location where content needs to be transferred from the gesture tape to the meaning tape  ( See Figure 3 )  . These are generated fi'om the ENTRY productions in the multil nodal CFG in Figure  2  . The gesture interpretation module empties the buffers and starts backate lafter each multimodal command  , and so we amlimited to a finite set of gesture events in a single utterance  . Returning to the example email this person and that organization  , assume the user gestures on entities objid367 and objid893  . These will be stored in buffers el and e2 . Figure 4 shows the speech and gesture streams and the resulting combined meaning  . 
The elements on the meaning tape are concatenated and the buffer references are replaced to yield S : email this person and that organization 
G : Gpcl ' Goe2
M : email(\[person(ct ), org(c2)\])
Figure 4: Messaging domain example email (~) er . son(objid367), or . q(objidS93)\]) . As more recursive semantic phenomena such as possessives and other complex noun phrases are added to the grammar the resulting machines become larger  . However , the computational consequences of this can be lessened by lazy ewfluation techniques  ( Mohri ,  1997 ) and we believe that this finite-state approach to constructing semantic representations is viable for a broad range of sophisticated language interface tasks  . We have implemented a sizeable multimodal CFG for VPQ  ( See Section 1 ) :417 rules and a lexicon of 2388 words . 
3.2 Multimodal Finitestate Transducers
While a three-tape finite-state automaton is feasible in principle  ( Rosenberg ,  1964) , currently available tools for finite-state language processing  ( Mohri et al ,  1998 ) only support finite-state transducers ( FSTs )   ( two tapes )  . Furthermore , speech recognizers typically do not supportile use of a three-tape FSA as a language model  . In order to implement our approach , we conver the three-tape FSA ( Figme 3 ) into an FST , by decomposing the transition symbols into an input component  ( GxW ) and output component M , thus resulting in a function , T : ( GxW)--+M . This corresponds to a transducer in which gesture symbols and words are on the : input a pe and the meaning is on the output tape  ( Figure 6 )  . The domain of this function T can be further curried to result in a transducer that maps  7~:G --> W ( Figure 7 )  . 
This transducer captures the constraints that gesture places on the speech stream and we use it as a Jan-guage model for constraining the speech recognizer based on the recognized gesture string  . In the foplowing section , we explain how " F and 7% are used in conjunction with the speech recognition engine and gesture recognizer and interpreter to parse and  inter-4 Applying Multimodal Transducers There arc number of different ways in which multimodal finite -state transducers can be integrated with speech and gesture recognition  . The best approach to take depends on the properties of the lmrticular interface to be supported  . The approach we outline here involves recognizing esture ilrst then using the observed gestures to modify the language model for speech recognition  . This is a good choice if there is limited ambiguity in gesture recognition  , for ex-an@e , if lhem ~ jority of gestures are unambiguous deictic pointing gestures  . 
The first step is for the geslure recognition and interpretation module to process incoming pen gestures and construct a linite state machine Gesllt Ve corresponding to the range of gesture interpretations  . 
Ill our example case ( Figure 4 ) tile gesture input is unambiguous and the Gest ttrelinite state machine will be as in Figure  5  . \] f the gestural input involves gesture recognition or is otherwise ambiguous it is represented as a lattice indicating all of the possible recognitions and interpretations  o1' tile gesture stream . This allows speech to compensate for gesture errors and mutual compensation  . 
Figure 5: ( ; eslttrelinite-smte machine
This Ge , s'lure linite state machine is then composed with the transducer " R  , which represents the relationship between speech and gesture  ( Figure 7 )  . 
The result of this composition is a transducer Gesl-Lang  ( Figure 8 )  . This transducer represents the relationship between this particulars l  . ream of gestures and all of the possible word sequence stlmt could cooccur with those oes "  , rares . In order to use this in-lbnnation to guide the speech recognizer  , welh cn take a proiection on the output ape ( speech ) of Gesl-Lang to yield a finite-state machine which is used as a hmguage model for speech recognition  ( Figure 9 )  . Using this model enables the gestural information to directly influence the speech recog-nizer's search  . Speech recognition yields a lattice of possible word sequences  . In our example caseity Mds the wol ~ . t sequence mail this person and that organization ( Figure 10 )  . We now need to reintegrale the geslure in l ' ormation that wc removed in the prqjection step before recognition  . This is achieved by composing Gest-Lang ( Figure 8 ) with the result lattice from speech recognition ( Figure 10 )  , yielding transducer Gesl ~& ) eech FST ( Figure 11 )  . This transducer contains the information both from the speech stream and from the gesture stream  . The next step is to generate the Colnbined meaning representation  . To achieve this Gest & ) eech FST ( G : W ) is converted into an FSM Gest Speech FSM by combining output and input on one tape  ( GxW )   ( Figure 12 )  . 
GestSk ) eeckFSM is then composed with T ( Figure 6) , which relates speech and gesture to meaning , yielding file result transducer Result ( Figure 13 )  . 
The meaning is lead from the output tape yielding cm  , dl(\[perso , , , ( ca ) , m ' O(e2)\]) . We have implemented lifts approach and applied it in a multimodal interface to VPQ on a wireless PDA  . In prelilni-nary speech recognition experiments , our approach yielded an average o1'   23% relative sentence-level error reduction on a corpus of  1000 utterances ( Johnston and Bangalore ,  2000) . 
5 Conclusion
We have presented here a novel approach to muI -timodal hmguage processing in which spoken language and gesture are parsed and integrated by a single weighted lh fite-state device  . This device provides language models for speech and gesture recognition all d colll poses content from speech and gcs-lure into a single semantic representalion  . Our approach is novel not just in addressing multimodal hmguage but also in the encoding of semantics as well as syntax in a finile-state device  . 
Compared to previousal ~ proaches ( Johnston el al . , 1997; Jolmston , 1998a ; Wu et al , 1999) which compose elements from ' n . -best lists of recognition results , our approach provides an unprecedenled potential for mutual compensation among the input modes  . It enables gestural input to dynamically alter the hmguage model used tbr speech recogni-lion  . Furthermore , our approach avoids the computational complexity of multidimensional multimodal parsing and our system of weighted finite-stale transducers provides a well understood probabilistic framcwork for combining the probability distributions associated with speech and gesture input and selecting among multiple competing nmlti-modal interpretations  . Since the finite-state approach is more lightweight in coml  ) utational needs , it can more readily be deployed on a broader ange of platforms  . 
In ongoing research , we are collecting a corpus of multimodal data ill order to forlnally evahm te the effectiveness of our approach and to train weights for  1he multimodalinile-state transducers . While we have concentrated here on understanding , in principle the same device could be applied to multimodal  . ~ Goor , ,anization:or , ,( ells tnat:eps ~ z j - ~ ~ ~ a b__ . . . ___________ czps:cz,~op ~ y , : om ~< ~ . _: pg_*>~_>/-_______/---"'W':' , ells_and: , ~-- ells : l) . ? Figure 6: Transduce relating gesture and speech to meaning ( 7-': ( GxW ) -- M ) 
Gd:departmcnte 1 : eps/f~'~/'~Go:organization cps:that~z " ~  -~ -  . . . . . . . ._ . cz : cl)s ~ .   . "/~" MQI~S ?'-- J~--J"J NN .   . e3:ep*-"-"'""~"s_ . ,,-((43) ('7') ~ p , :~ ma , , ~~ : y > . / ~ p , : a , , d " - - - - - - - - - - ~ ' ~ - . ., . . j . . . . . .__eps:pags_ . . . . . . . ac- . .__J - __ Figure 7: Transduce relating gesture and speech ( TE:G---+W ) eps : elllaile ps : lhalGp:person
Figure 8: Gest Lang Transducer ( o : lu ' ganizalion upage ~ this -- Figure 9: Projection of Output tape of Gest Lang Transducer @ email "@ this  . @ person . ~@ and . @ that=@Figure 10: Result from speech recognizer
Figure 11: Gesture Speech FST
Figure 12: Gesture Speech FSM organization ~@ organization_ -Q ~  , , se , l , a , l:Olll , i , ~ , ~ . q ) ot , s_ , , , is : e , , , >@ o , )-p~rso'l: , ' e ~ go " ~> G ~ , ? , , s:el > q)

Figure 13: Result Transducer eps :) ~, Q~~i, . :) > (~) ep , :\] )  > ( ~ ) are also exploring tecl miques to extend compilation fi'om feature structures graln nlars to FSTs  ( Johnson , 19!)8) to nmltimodal unification-based grammars . 

Steven Abney .  1991 . Parsing by chunks . In Robert Berwick , Steven Abney , and Carol Tenny , editors , Principle-based palwing . Kluwer Academic Publishers . 
Srinivas Bangalore and Giuseppe Riccardi . 2000.
Stochastic lhfite-state models for spoken language machine translation  . In Proceedingso /" the Workshop on Embedded Machine Translation Systems  . 
Srinivas Bangalore .  1997 . ComplexiO , of Lexic . al Descriptions and its Relevance to Partial Pmw - ing  . Ph . l ) . tlaes is , University of Pennsylwmia , t ~ hiladelphia , PA , August . 
Robert A . Bolt .  1980 . " put-thal-there ": voic cand gesture at the graphics interface  . Computer
Graphics , 14(3):262-270.
Bruce Buntschuh , C . Kamm , G . DiFabbrizio , A . Abella , M . Mohri , S . Narayanan , I . Zel . ikovic , R . D . Sharp , J . Wright , S . Marcus , J . Shaffer , R . I ) uncan , and J . G . Wilpon .  1998 . Vpq : A spoken language interface to large scale directory information  . In Proceedin , q , so /' ICSLI ', Sydney,

Robert Carpenter .  1992 . The logic q fOT ) ed . /~' alure structures . Cambridge University Press , England . 
Philip R . Cohen , M . Johnston , 1) . McGee , S . L . 
Oviatt , J . Pittman , I . Smith , L . Chen , and J . Clew .  1998 . Multimodal interaction for distributed interactive simulation  . In M . Maybury and W . Wahlster , editors , Readings it z Intelligent httelfiwes . Morgan Kaul'mann Publishers . 
Mark Jollnson .  1998 . Finite state approximation of constraint-based grammars using left-corner grammar transforms  . In Proceeding sq/'COLING-
ACL , pages 619-623, Montreal , Canada.
Michael Johnston and Srinivas Bangalore . 2000.
Tight-coupling of multimodal language processing with speech recognition  . Technical report,
AT&T Labs-Reseamh.
Michael Johnston , ER . Cohen , D . McGee , S . L . Oviatt , J . A . Pittman , and 1 . Smidl .  1997 . Unilication-based multimodal integration . In Proceeding so/lhe 35th ACL , pages 281-288 , Madrid , Spain . 
Michael Johnston . 1998a . Mullimodal language processing . In Proceedingsq/"ICSLP , Sydney,

Michael Johnston . 1998b . Unification-based multimodal parsing . In Proceedings of COLINGACL , pages 624-630 , Montreal , Canada . 
Aravind Joshi and Philip Hopely .  1997 . A parser fiom antiquity . Natural Language Engilzeering , 2(4) . 
Ronald M . Kaplan and M . Kay .  1994 . Regular models of phonological rule systems . Computational
Linguislics , 20(3):331-378.
K . K . Koskenniemi .  1984 .   7ire-level morphology : a general computation model , for word z form recognition and production . Ph . D . thesis , University of

Mehryar Mohri , Fernando C . N . Pereira , and Michael Riley .  1998 . A rational design for a weighted . finite-state transducer librao, . Number 1436 in Lecture notes in computer science . 
Springm ; Berlin ; New York.
Mehryar Mohri .  1997 . Finitestate transducers in language and speech processing  . (7 Oml ~ utational
Linguistics , 23(2):269-312.
J . G . Neal and S . C . Shapiro .  1991 . Intelligent multimedia interface technology . In J . W . Sulliwm and S . W . Tylm , editors , Intelligent Userlnter\['aces , pages 45-68 . ACM Press , Addison Wesley , New

Sharon L . Oviatt .  1997 . Multimodal interactive maps : l ) esigning l ' or human performance . In Hmmut-Computer Interaction , pages 93-129 . 
Sharon L . Ovial t .  1999 . Mutual disambiguation of recognition errors in a in ultimodal architecture  . In Cltl'99, pages 576-583 . ACM Press , New York . 
Fernando C . N . Pereira and Michael I) . Riley .  1997 . 
Speech recognition by composition of weighted finite automata  . In E . Roche and Schabes Y . , editors , FiniteState Devices for Nalttral Language Processitlg  , pages 431-456 . MIT Press , Cambridge , Massachusetts . 
Giuseppe Riccardi , R . Pieraccini , and E . Bocchieri . 
1996 . Stochastic Automata for Language Modeling . Computer Speech and Language , 10(4):265-293 . 
Emmanuel Roche .  1999 . Finitestate transducers : parsing free and fl ' ozen sentences  . In Andrfis Kornai , editol , Extended FiniteState Models el ' Lan-guage . Cambridge University Press . 
A . L . Rosenberg .  1964 . On n-tape finite state accep-ters . FOCS , pages 76-81 . 
Lizhong Wu , Sharon L . Oviatt , and Philip R . Cohen . 
1999 . Multil nodal integration - a statistical view . 
IEEE Transactions on Multimedia , I(4):334-34 l,


