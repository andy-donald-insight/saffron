Multilevel Similar Segment Matching Algorithm
for Translation Memories and Example-Based Machine Translation 
Emmanuel PLANAS
Cyber Solutions Laboratories
24, Hikaridai Seikacho Sorakugun
Kyoto , 619-0237 Japan
planas@soy.kecl.ntt.co.jp
Osamu FURUSE
Cyber Solutions Laboratories
24, Hikaridai Seikacho Sorakugun
Kyoto , 619-0237 Japan


We propose a dynamic programming algorithm for calculaing the similarity between two segmeuts of words of the same language  . The similarity is considered as a vector whose coordinates refer to the levels of analysis of the segments  . This algorithm is extremely efficient for retrieving the best example in Translation Memory systems  . 
The calculus being constructive , it also gives the correspondences between the words of the two segments  . This allows the extension of Translation Memory systems towards 
Example-based Machiue Translation.
\] Introduction\[n Translation Memory ( TM ) or Example-Based l Vlachine Translation ( EBMT ) systems , one of lhedecisive tasks is to retrieve from the database  , the example that best approaches the input sentence  . In Planas ( 1999 ) we proposed a two-step retriewd procedure , where a rapid and rough index-based search gives a short list of example candidates  , and a refined matching selects the best candidates from this list  . This procedure drastically improves the reusability rate of selected examples to  97% at worst , for our English-Japanese TM prototype ; with the classical TM strategy , this rate would constantly decline with the number of nonmatched words  . 
It also allows a better e call rate when searching for very similar examples  . 
We describe here the Multilevel Similar Seglnent Matching  ( MSSM ) algorithm on which is based the second step of the above retrieval procedure  . This algorithm does not only give the distance between the input and the example source segmeuts  , but also indicates which words would in atch together  . It uses F different levels of data ( surface words , lemln as , parts of speech ( POS ) , etc . ) in a combined and uniform way . 
The computation of the worst case requires F * m * ( n-m+2 ) operations , where m and n are respectively the lengths of the input and the candidate  ( m <= n )  . This leads to a linear behavior when m and n have similar lengths  , which is often the case for TM segments L Furthermore  , because this algorithm gives the exact matching links  ( along with the level o1' match ) between all of the words of the input and the candidate sentence  , it prepares the transfer stage of an evolution of TM that we call Shallow Translation  . This involves substituting in the corresponding translated candidate  ( stored in the melnory )  , the translation of the substituted words , provided that the input and the candidate are " similar enough "  . 
1 Matching Principle 1 . 1 The TELA Structure The purpose of this algorithm is to match two segments of words : input i and candidate C  . 
These can each be any sequence of words : phrases , sentences , or paragraphs , for example . 
Let us consider input I of length m , not as a single segment of surface words , but rather as a group of F parallel layered segments r\]  0 < : t < : v ) each bearing m tokens . Such a structure is shown in Figure 1 , and we call it a TELA structure z . On each layer f , the ith token corresponds to one of the paradigms of the ith word of input i  . in our implementation , we use a shallow analyzer that gives three paradigms  ( F = 3 ) for each surface IW e use this algorithm on a sorted list of a h'cady similar sentences  , retrieved with the help of an index . 
2 Tile idea o1' this structure is a h'eady in L a four cade's LEAF ( 1993 )  , and is explained in Planas (1998) . 


C1
C2
C3PN
CIC2C3 noun
C4 verb
C5C 6C7
NqzFa4ty
PN advs tayed stay verb
Iii I2
I~gonystayed
I2 Sony stay 13 PN verb
Figure 1: Example of matching TELA structures word of the segments : the surface word itself  ( f = l )  , its lemma ( f = 2) , and its POS tag ( f : 3) . 
Because we do not need a syntactic analyzer , the time required for this analysis is not an handicap  , moreover such parsers are available for lnany languages  . Let C be a candidate segment of length n , for matching input I of lengthm(n >= m ) . The basic problem involves matching f the elements of the set  ( C i ) f < : ~: .  ~  .   . . to those of ( I~)f <:~: j<= , n-Only three layers are shown in the following examples but other types of layers  , like semantics , or even nonlinguistic information like layout features can be considered  , as in Planas (1998) . Our algorithm is written for the general case ( Flayers )  . 
1.2 Edit Distance based Similarity
We consider a match from C to 1 as an edit distance process . This edition uses a sequence of basic edit operations between the words of the segments  , like in Wagner & Fisher ( 1974 ) who used four basic operations : deletion , insertion , strict and equal substitution between the letters of a word  . This approach as also been followed by Gale & Church  ( 1993 ) for their alignment algorithm , with six operations . Here , we only consider deletions and equalities ( i . e . equal substitutions ) : F+I basic operations in totaP . 
One equality corresponds to each of the Flayers , and a deletion affects all layers at once . In Figure 1 , the items in bold match each other , and the strike through ones have to be deleted . The edition of C into I involves five deletions ( " Nikkei " , " journal " , " reported " , that " , " really ") , one equality at layer1("stayed ") , two at layer 23 Lepage ( 1998 ) also uses deletions and one level of equality lbr calculating his " pseudo-distance "  , forgetting the similarity between two strings . 
CsC9stmMonaay strong Monday adj UO Ull
Matclfing zone
I3I4 strong adj xuesayuoun("stay " , " strong ") , and four at layer3("PN " , " verb " , " adj " , " noun ") . At the Word level , the similarity between the two segments is considered to be the relative number of words of the input segmen that are matched by some word of the candidate segmeut in the matching zone  ( from " NTT " to " Monday " in our example ) :1/4 in Figure 1 . The same similarity can be considered at different levels  . Here , the lemma similarity is 2/4 , and the POS similarity is 4/4 . We consider the total similarity as a vector involving all layer equalities  , plus deletions : G(C ,  1) = (1/4 ,  2/4 ,  4/4 ,  1-1/4 ,  1-5/9 ) The fourth coordinate counts the complementary proportion of deletions in the " matching zone " of the candidate C  . The last coordinate counts the same proportion , relatively to the whole candidate . We take the complement to 1 because , the more deletions the mam , the smaller the similarity becomes . 
When different Ci candidates are possible for matching I  , the greatest ( ~( Cio ,  1) , according to common the partial order on vectors , determines the best candidate Cio . 
1.3 Matching Strategy 1.3.1 Basics
We try to match each word Ca of candidate C , to a word Ij of input I . Ci in atches lj if one of the paradigms of C ~ equals one of the paradigms of Ij at the same level f  , i . e . if Cfi and I\], are equal . 
When a failure to match two words with their paradigms Cfi to i ~ occurs at a given level f  , we try to match the words at the next upper level f + l:Cmi and Ir + ~ j  . When all of the possible layers of the two words have been tried without success  , we try to match the next word Cm to the same Ij . If Ci does not match any word of I at any words of I have to be matched by some word of C:no insertion is allowed  ( see section 1 . 3 . 4) . 
1'.3.2 Lazy match
With TM tools , if some useful candidates are found , they usually utilize words silnilar to the input words because translation memories arc applied within very similar documents  , most of tile time between ancient and newer versions of a same document  . When tile priority is rapidity ( rather than non-ambiguity )  , we can consider that alnatch is reached as so on as a word of C and a word of I match at a certain layer f  . It is not necessary to look at upper levels , for they should lnatch because of tile expected similarity between tile input and tile candidate  . Tile previous example illustrates this . A supper levels are not tested , this allows again in tile number of iterations ot'the algorithm  . Experiments ( see Planas ( 1999 ) ) have confirmed this to be a correct strategy for TM  . That's why , we consider from now on dealing with such a lazy match  . 
1.3.3 Exhaustive match
In the most general case , ambiguity problems prevent us fl'om employing the lazy strategy  , and a correct in atch requires that whenever two items CJ ~ and If  . match at a certain levelf , they should
J match at upper levels . Here is an example: 221 Sonystay c-~4ed   ~2   2 Sony stay ended M~m day ; 23 PNne+m ~? erb noun(~Sony , ~vyedsU~ngef T44esday \]\[ Sonystay strong Tuesday
IPN ~? erbadj " noun
Figure 2: Lemma ambiguity h3C2 , the lemma " stay " of surface word " stay " matchestile lemma " stay " of surl ' ace word " stayed " of I  , but they do not match at the POS level ( noun and verb )  . Tile algorithm should go to this level to find that there is no match  . Once again , however , because this algorithm has been built for TM systems  , such aln biguities hardly occur . 
1.3.4 Insertion
If some items in I are not matched by any iteln of C  , the match involves an insertion . 
Case of Translation Memories
If tile candidate sentences are to be used by a hm nantranslator  , s/he will be able to insert the missing word at the right place  . Accordingly , a match with insertion can be used for pure TM . 
Case of Shallow Translation ( EBMT ) in the EBMT system we are targeting , we plan to use tile matching substring of C for adaptation to I without syntactic rules  . 
Accordingly , we consider that we do not know where to insert the nonmatching item : in this case  , we force tile algorithm to stop if an insertion is needed for matching C and I  . From now on , we will follow this position . 
1.3.5"1)' ace
We want the output of the algorithm as a list of triplets  ( Cri If , op ) ~< i < - . calleda " trace ", where cri
J -_ corresponds to P ; through the " op " operation . We note op = " f " ancquality at level f , and op="0"a deletion . For Example 1 , the trace should be: ( 100 )   ( 200 )   ( 300 )   ( 400 )   ( 513 )   ( 600 )   ( 72
I ) (832) (943) 2Adapting Wagner & Fischer , and
Sellers algorithms 2.1 Algorithm Principle
The Wagner & Fischer ( W&F ) dynamic programming algorithm in Figure 3 givestile edit distance between C and I :
For j = 0 to md\[j , 0\]-i // initiating the cohmms
For i = ltondlO,i \] = i // initiating the rows
For i = 1 to n
For j = ltom
If(l\[j\]=C\[i \]) d=d\[i-l,j-1\]//equality
Elsed=d\[i-1, j-1\]+l//subst.
d\[j,i \] = min(d\[i-1,j\]+l,d\[i,j-1\]+l,d)
End For
End For
Printd\[n,m\]
Figure 3: The Wagner & Fisher algorithm
Tile distance is obtained in m * n operations , by building a n\[re+l , n + l \] array ( see Figure 6) . Ill addition , W&F (1974) proposed a backtracking procedure , shown in Figure 4 , that scans back this array to give a " trace " of the match between words  )  , in(m+n ) operations . The trace is then obtained in ( mn+m+n ) operations in total . This algorithm was previously used in Plan as ( 1998 ) at each layer of a TELA structure to give a trace by layer  . The data fi'om the traces of the different layers were combined afterwards for the purposes of TM and EBMT  . However , this procedure is not optimal for at least two reasons  . 
First , the layers are compared in an independent way , leading to a waste of time in the case of TM , because the lazy match phenomenon is not used . Second , the combination of the results was processed after the algorithm  , and this required a supplementary process . One can imagine that processing the whole data in the flow of the instructions of the algorithm is more efficient  . 
i = i0 ; j = m ; while ( i > 0 ) and ( j > 0 ) // del//if ( d\[i , j \] = d\[i-l , j\]+l)i = i-1//ins // else if(d\[i , j \] = d\[i , j-l\]+1 ) j = j-1 else // equality or substitution print ( i , j ) i=i-1; j=j-I end if end while
Figure 4: W&F backtracking algorithm 2 . 2 Two operation based mininf izat ion If we look back at the W&F algorithm  , shown in Figure 3 , the part in bold represents the code involved in the calculus of the next local distance d\[i  , j \] . It testes which of the four basic edit operations  ( deletion , insertion , equal or strict substitution ) gives the lowest partial distance . Nevertheless , we have shown in section 1 . 3 . 4 that only deletions and equalities
Ii 111 121 13
First press the
Cli0 inf inf inf
C1~First00 inf inf
C12 press010 in f
C1.~only 021 inf
C14 the 0321
Cl . sred04 32
C~6button0543 do interestus . We therefore reduce the test in the algorithm to that shown in Figure  5  . Furthermore , we initiate the columns of the array with infinite values  ( huge values in practice ) to show that initial insertions are not possible , and the rows to "0" , to count the deletions relatively to iuput I . 
See Sellers (1980) for a due explanation.
If ( I\[j\]=C\[i \]) d = d\[i-l , j-I\]//equal:nocostElsed=inf // big integer  , in theory h ~ finited\[j , i \] = rain(d\[i-1 , j \] + l , d ) // deletion or equal ? Figure 5: Matching with deletions and equalities An example of the successive scores calculated with this algorithm are shown in Figure  6  . The total distance ( equal to 1 ) between C and I appears in the lowest right cell . 
The fact that only two operations are used eradicates the ambiguity that appears in selecting the next cell in the W&F algorithm backtracking procedure with four operations  . In our algorithm , either there is an equality ( cost 0) , or a deletion ( cost 1) . The possibility of having the same cost 1 for insertions , deletions , or strict substitutions has been eliminated . 
2 . 3 In t roduc ing one equal i ty per level As mentioned previously  , we need to match items at different layers . We introduce here two new points to deal with this : ? In order to keep the score for each equality deletion  , d\[i , j \] is a vector instead of a number : d\[i , j \]=\[ scorel . . . . . scorer , score\] . 
? In this vector , score ~ throughs cor % store the number of equalities for each layer f  , and score records the number of deletions , as in W&F ( underlined in the arrays ) . 
114 redinf in finf in finf in finf in finf in finf in f
D\[i-1, j-l\]1 in f2W
Figure 6: Successive scores produced by the adapted W&F algorithm 
D\[i-1, j\] deletion

D\[i,j\]


G c ~



Gltii/word lemPOS
Sony Sony PN say that leported that 0 Ii LIs word Sony stays strong lem Sony stay strong 
POSPN verb adj 9000 oo oN9 oo , ,j ooo , , j 9000   100_0   300imf   O00inf   9000   100_1   101_0   000inl  "  9000   1002   101_1   000inf   9000   0010   1012   000inf   90OO   001_1   IH1-0   000inl_"   9000   001_2   311_1   021_0   9000   001_0   3112   021_1   9000   001-1   3113   021_2 verb coati
NT TNT TPN stayed stay Verb strong
Tuesday stronger
Tuesday morning morning

PNnouu



PN900 , , j 300in__f   300inf   300inf   300in__f   300inf   300in__ f Figure 7: Introducing a vector of deletion and layer equalitie scores Figure  7 shows an example of difl brent score vectors involved in a match  . To calculate the successived\[id\] , we use tile algorithm of Figure 5 adapted for F levels in Figure 8  . 
If ( Ir\[j\]=cf\[i \]) d~=\[d'\[i-l,j-1\] . .  .   .   . dV\[i-l,j-l1+1, d--\[i-1j-l\]\]
Elsed~=\[0.....0, inf.I
End\]1"dd=\[d'\[i1,j\] .   .   .   .   . df\[i-Id\] .   .   .   . dV\[i-1d\] , d--\[i-1 , j\]+1\]d\[j , i \] = max(d ~ , d , i ) // equali O , or deletion Figure 8: Adapting the algorithm to Flevels We first try to get the max in mmn mnber of equalities and then tile mininmm of deletions  . 
Each tilne we find a new match in the first colunm  , we start a new path ( see I~matching with CI , C4 and C7 in Figure 7) . It ' one of the vectors of the last column of tile array is such that : SUM k = r <= v  ( scorer ) = In , there is a matching substring of C in which there is a matching word for each of the words of I : this constitutes a solution  . In our example , cell (7 ,  4) , with score 121__0 shows that there is a subchain of the candidate that matchestile input with  1  ,  2 , and 1 matches at the word , lemma , and POS levels and 0 deletions . Cell (8 , 4) indicates a similar naatch , but with 1 deletion Cmorning ") . Tile best path then ends at cell (7, 4) . Starting from this cell , we can retrieve tile full solution using the W&F backtrack algorithm adapted to Flevels  . 
This approach allows us to choose as compact a string as possible  . When there are several possible paths , like in Figure 9 , the algorithm is able to choose the best matching substring  . If we are looking for a similarity involving first lemPOS 
Sony PN say verb strong adj
Tuesday PN and zonj
NTTPN stay Verb strong Adj
Tuesday PN morning noun
CriCl0 word
C ~ Sony
C2 stayed
C3 stronger
C4 Tuesday
Cs and
C6NTT
C7 stayed
Cs stronger
C9 Tuesday
CIomorning 0 lj I2   I3 word Sony stays strong lem Sony stay strong
POSPN verb adjoooooo#_vf 9oo ~ ooo ~ 0000   1000   300inl__"   O00inf   0000   1001   110-0   000in__f   0000   100_2   1101   120_0   0000   100_Q   102   1201   0000   1001   110  . _ 3   1202   0000   001_0   1104   1203   0000   0011   Dll-0   1204   0000   0012   311-1   021_0   0000   001_0   3112   021_1   0000   001_1   3113   0212 Figure 9: Selecting the best concurrent subsegment \ [ , 1


PN)00/~r ) 00 in__f ) 00 int ")00 in_f

Z20! 121 ! speech , then cell (4 , 4) of score 2200 will be chosen . This strategy can be adapted to particular needs : it suffices to change the order of the scores in the vectors  . 
3 Optimizing 3 . 1 Triangularization of the array In this algorithm  , for each Ij , there must be at least one possible matching C ~ . Hence , in a valid path , there are at least m matches . As a match between C ~ and Ij occurs when " stepping across a diagonal "  , the ( m-l ) first diagonals ( from the lower left corner of the array ) cannot give birth to a valid path . Therefore , we do not calculated \[ i , j \] across these small diagonals . 
Symmetrically , the small diagonals after the last full one ( in the upper right corner ) cannot give birth to a valid path . We then also eliminate these ( m-l ) last diagonals . This gives a reduced matrix as shown in the new example in Figure  10  . The computed cells are then situated in a parallelogram of dimensions  ( n-m+l ) and m . 
The results is : only m ( n-m+l ) cells have to be computed . Instead of initiating the first row 0 to " inf " , we initiate the cells of the diagonal just before the last full top diagonal  ( between cell ( 0 , 1) and cell (3 , 4 ) in Figure 10 ) to "000inf " to be sure that no insertion is possible . 
3.2. Complexity
The worst time complexity of this algorithm is F -proportional to the number of cells in the computed array  , which is ln*(n-m+l ) . With the " lazy " strategy , all F levels are often not visited . 
As the number of cells computed by the W&F algorithm is m'n  , our algorithm is always more rapid . The backtracking algorithm takes m+n operations in the W&F algorithm  , as well as in our algorithm , leading to m ( n-m+2 ) +noperations in the MSSM algorithm , and m(n+l ) + n operations in the W&F algorithm . 
The general complexity is then subquadratic.
When the lengths of both segments to be compared are similar  ( like it often happens in TMs )  , the complexity tends towards linearity . 
The two graphics in Figure 11 show two interesting particular cases ( ln = n and m running from 1 to n = 10 )  , comparing W&F and our algorithm . For strings of similar lengths , the longer they are , the more the MSSM algorithm becomes interesting . When n is fixed , the MSSM algorithm is more interesting for extreme values of the length of I:small and similar to n  . 

The first contribution of this algorithm is to provide TM and EBMT systems with a precise and quick way to compare segments of words with a similarity vector  . This leads to an ahnost complete radication of noise for the matter of retrieving similar sentences in TM systems  ( 97% " reusability " in our prototype )  . The second is to offer an unambiguous word to word matching through the " trace "  . This last point opens the way to the Shallow Translation paradigm  . 







Csi /

C1C2C3/I3 wo M!eraPOS ~ ony reported that
NTT stayed stronger
I'uesday morning
Sony ~ ay

NTT~tay ~ trong ? uesday morning
PN verb conj



P noun noun 0   11   12   13   14 word Sony stays strong Tuesday lem Sony stay strong Tuesday 
POS PN verb adj PN 0000   900i  ~  0000   1000   O00'mf   0000   1001   101_0   900/nf   0000   1002   1011   300inf   000~   0000 I ) 01_01012300 inf "000 inf . 
0000 9011 011_0  ) 0 0in_f   000inf  "  0000   0111   D210   000int  "  0000   3211   121_0   0000   1211 Figure 10: Eliminating left and Hght small diagon also 80 
Comparison W&F/MSSM 1   2   3   4   5   6   7   8   9   10 n = m + W&F -- t1~-- MSSM-'--r~---W&F+Bk
I : MSSM+Bkt40~J * 7* 234 5678 910 m ( n = lO ) - ~ I'--W&F-I~---MSSM-/~--W&F4 . Bk:MSSM4 . Bk Figure 11: Comparing the Wagner & Fisher and
MSSM algorithms
For more information about the use of this algorithm  , please refer to Planas (1999) . These two contributions bring in the main difference with relative research  4 concentrating on similarity only , represented by a sole integer . 
The TELA structure , that allows the parallel use of different layers of analysis  ( linguistic paradigms , but possibly nonlinguistic information ) is essential to this work because it provides the algorithm with the supplementary information classical systems lack  . 
The fact that the shallow parser ( lemmas , POS ) is ambiguous or not does not affect significantly the performance of the algorithln  . If the same parser is used for both example and input segments  , parallel errors compens at e each other . 
Of course , these errors do have an influence for EBMT : the nonambiguity is then a must  . 
A first evaluation of the MSSM speed gives 0 . 5 to 2 milliseconds for comparing onlys two randomly chosen English or Japanese sentences over  3 levels ( word , lemmas , POS ) . The 4Cranias et al (1997) , Thompson & Brew (1994) , or in a more specific way , Lcpage ( 1998 )   5 Without hesh allow analysis implementation has been done with a DELL Optiple xGX  1   233 Mhz , Window NT , Java 118 . 
This algorithm can be improved in different ways . For speed , we can introduce a similarity threshold so as not to evaluate the last cells of the columns of the computed array as soon as the threshold is over taken  . For adaptability , being able to deal with a different number of tokens according to each layer will allow us to deal nicely with compound words  . 
In short , if the basis of this matching algorithm is the W & F algorithm  , other algorithms can be adapted similarly to deal with multilevel data  . 

Thanks to Takayuki Adachi , Francis Bond , Timothy Balwin , and Christian Boitet for their useful remarks and fruitful discussions  . 

Cranias , L . , Papageorgiou , H . , & Pipcridis , S . (1997) Example retrieval . fron~a7) zmslation Memory . 
Natural Language Engineering 3(4), Cambridge
University Press , pp . 255-277.
Gale , W . A . & Church , K . W . (1993) A program . for Aligning Sentences in Bilingttal Corpora . Compu-lational Linguistics , ACL , Vol . 19, No .  1 . 
Lafourcade M . (1993) LEAF , ou comment garderl ' Original itd del ' ambiguit d . A clualitd Scienli fiquc-Troisi~mes Journdes Scientifiques Tradu clique-TA-TAO  , Montrdal , Canada , AUPELF-UREF , Vol . 1/1, pp .  165-185 . 
Lepage Y . (1998) Solving amtlogies on words : cm algorithm . Coling-ACL'98, Vol . I , pp .  728-734 . 
Sellers , P . H .   ( 1980 ) The theory and computation of evolution my distances : pauenl recognition  . Journalo1' Algorithms , Vol . 127, pp .  359-373 . 
Thompson Henry S . & Brew Chris ( 1996 ) Automatic Evaluation of Computer Generated text : Final 
Report on the TextEval Project . Human
Communication Research Center , University of

Wagner , A . R . & Fischer M . (1974) 7 he String-to-String Con'ection Problem . Journal of the ACM,
Vol . 21, #1, pp . 168-173.
Planas , E .   ( 1998 ) TELA : Structures and Algorithms for MemoryBased Machine  7  ) ' anslation . Ph . D . 
thesis , University Joseph Fourier , Grenoble.
Planas , E . & Furuse O . (1999) Fom ~ alizing Translation Memories . Machine Translation
St , remit VII , Singapore , pp . 331-339
