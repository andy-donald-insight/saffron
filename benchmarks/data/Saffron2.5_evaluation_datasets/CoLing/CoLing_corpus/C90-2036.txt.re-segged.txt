
A Spelling Correction Program Based on a Noisy Channel Model 
Mark D . Kemighan
Kenneth W . Church
William A . Gale
AT&T Bell Laboratories
600 Mountain Ave.
Murray Hill , N.J ., USA

This paper describes a new program , correct , which takes words rejected by the Unix ? spell program  , proposes a list of candidate corrections , and sorts them by probability . The probability scores are the novel contribution of this work  . 
Probabilities are based on a noisy channel model.
It is assumed that the typist knows what words he or she wants to type but some noise is added on the way to the keyboard  ( in the form of typos and spelling errors )  . Using a classic Bayesian argument of the kind that is popular in the speech recognition literature  ( Jelinek ,  1985) , one can often recover the intended correction , c , from a typo , t , by finding the correction c that maximizes Pr ( c ) Pr ( t l c )   . The first factor , Pr(c ) , is a prior model of word probabilities ; the second factor , Pr(t\[c ) , is a model of the noisy channel that accounts for spelling transformations on letter sequences  ( e . g . , insertions , deletions , substitutions and reversals ) . 
Both sets of probabilities were trained on data collected from the Associated Press  ( AP ) newswire . This text is ideally suited for this purpose since it contains a large number of typos  ( about wo thousand per month )  . 
1. Introduction
The correct program reads a list of misspelled words from the input stream  ( std in )   , and prints a set of candidate corrections for each word on the output stream  ( stdout )  . Correct also produces a probability along with each correction  ( unless there is only one candidate correction )  . Here is some sample output produced by the Unix ? command  , " spell < paper \] correct , " where paper is a text file containing the misspelled words in column  1: 
Typo detered laywer negotations not campaigning progession us usally 
Corrections deterred ( 100% ) metered ( 0% ) petered ( 0% ) lawyer ( 100% ) layer ( 0% ) lawer ( 0% ) negotiations ? ? ? t progressic ~ l ( 94% ) procession ( 4% ) profession ( 2% ) usually 2 . Proposing Candidate Corrections The first stage of correct finds words on a fixed list that differ from the typot by a single insertion  , deletion , substitution or reversal . The list was collected from many sources , including spell , the AP newswire , and several machine readable dictionaries . For example , given the input typo , acress , the first stage generates candidate corrections in the table below  . Thus , the correct word actress could be transformed by the noisy channel into the typo acress by replacing the t with nothing  ,  @ , at position 2 .   2 This unusually difficult example was selected to illustrate the four transformations  ; most typo have just a few possible corrections , and there is rarely more than one plausible correction  . 
Typo Correction Transformation acress actress@t 2 deletion acress cressa # 0 insertion acress caressacca 0 reversal acress access rc 2 substitution acress across e o 3 substitution acress acress # 4 insertion acress acress # 5 insertion 1  . ??? indicates that no correction was found . 
2 . "/' he symbols @ and  #represt mt nulls in the typo and correction  , respectively . " Ilae transformations are named from the txoint of view of the correction  , not the typo . 
205-2-3. Scoring
Each candidate correction , c , is scored by Pr(c ) Pr(tlc) , and then normalized by the sum of the scores for all proposed candidates  . The prior , Pr(c ) , is estimated by ( freq(c ) + 0 . 5)/N , where freq ( c ) is the number of times that the word c appears in the  1988 AP corpus ( N=44 million words ) ) The conditional probabilities , Pr(tlc) , are computed from fbur confusion matrices ( see appendix ) :  ( 1 ) del\[x , y\] , the number of times that the characters x y ( in the correct word ) were typed as x in the training set ,  (2) , add\[x , y\] , the number of times that x was typed as x y , (3) sub\[x , y\] , the number of times that y was type d as x , and (4) rev\[x , y\] , the number of times that xy was typed as yx . Probabilities are estimated from these matrices by dividing by chars\[x  , y\]or charsix \] , the number of times that x y and x appeared in the training set  , respectively . 4 del\[cp_l , cp_~\] if deletion chars\[cp_l , ce\]'add\[cp_l , tp\] , if insertion chars\[ct ,  _ 1 \]
Pr(tlc ) = sub\[tp , cp\] , if substitution chars\[cp\]rev\[cp , Cp+t\]chars\[cp , cp+t\]'if reversal where cp is the pth character of c  , and likewise tp is the p~character of t . The five matrices are computed with a bootstrapping procedure  . 
Initially assume a uniform distribution over the possible confusions  . Then run the program over the training set ( 1988 AP corpus ) to find corrections for the words that spell rejects  . Use these corrections to update the confusion matrices  , and iterate . The matrices are smoothed using the GoodTuring method  ( Good ,  1953) . 
3 . Following Box and Tiao (1973) , we can assume an uninformative prior and reach a posterior distribution for p  . The expectation of this distribution amounts to using r +  . 5 instead of r . We call this the expected likelihood estimate . See Gale and Church ( 1990 ) for a discussion of the shortcomings of this method  . 
4 . The chars matrices can be easily replicated , and are therefore omitted from the appendix . 
Returning to the acress example , the seven proposed transformations are scored by multiplying the prior probability  ( which is proportional to 0 . 5+ column 4 in the table below ) and the channel probability ( column 5 ) to form a raw score ( column 2 )  , which are normalized to produce probabilities ( column 1 )  . 
The final results are : acres (45%) , actress (37%) , across (18%) , access (0%) , caress (0%) , cress (0%) . This example is very hard ; in fi~ct , the second choice is probably right , as can be seen from the context : . . . was called a " stellar and versatile acress whose combination of sass and glamour has defined her  . . . . The program would need a much better prior model in order to handle this case  . In the future , a program might be able to lake advantage of the fact that actress is considerably more plausible than acres as an antecedent for whose  . 
c % Raw freq(c ) Pr(tlc ) actress 37% . 157 1343 55 . /470, 000 cress 0% . 000 0 46 . /3 2,000,000 caress 0% . 000 4  . 95/5 80,000 access 0% . 000 2280  . 98/4,700,000 across 18% . 077 8436 93 . / 10,000,000 acres 21% . 092 2879 417 . /1 3,000,000 acres 23% . 098 2879 205 . /6,000,000 4 . Evaluation Many typos such as absorbant have just one candidate correction  , but others such as a dusted have multiple corrections  . The table below shows examples of typos with less than ten candidate corrections  , the corrections ordered by likelihood . 
 #Typo Corrections 0 admininistration 1 absorbant 2 a dusted 3 ambitios 4 compatability 5 afte 6 dialy 7 poice 8 piots 9 spash absorbent adjusted usted ambitious ambitions ambition compatibility compactability comparability computability after fate aftate antedaily diary dialdimly dilly police price voice poise pice ponce poir epilots pivots riot splots pits pot spints piouss plashs mash slashs pasms tashs wash sash pas Most typos have relatively few candidate corrections  . The table below shows the number of tylx ~ S 5 broken out by the number of corrections in seven month-long samples of the AP newswire  . In March , for example , there were '720 typos with 0 corrections , 1120 typos with 1 correction , 269 with 2 corrections , etc . 
The fired cohtmn shows that there is a general trend for fewer choices  , though the 0-choice case isspe ~ zial .   ( The system was trained on the AP wire li'om 2/88 o 2/89  ; the results below were computed from AP wire during  3/89  -  9/89  )  . 


March April May June 720   604   542   606   1120   997   1037   1007   269   224   209   223   109   92   89   101   58   57   62   45   54   41   20   26 
July Aug Sept Total 492   465   508   3937   958   944   930   6993   199   224   214   1562   79   87   82   639   43   59   43   367   28   24   28   221   35   22   19   19   22   17   23\[   157   20   11   13   7   11   15   17\[   94   19   14   14   5   7   7   16\[   82   15   11   6   11   10   8   16l   77   154   97   79   75   53   77   781   613   2573   2 i - ~ 2090   2125   1902   1927   19~t145~ We decided to look at the 2-candidate case in more detail in order to test how often the top scoring candidate agreed with a panel of three judges  . The judges were given 564 triples and a few concordance lines : absurb absorbabsurd financial community  . * E**S*"It is absurb and probably obscene for any person so engaged to und The first word of the triple was a spell reject  ; the other two were the candidates ( in alphabetical order )  . The judges were given a 5-way forced choice . They could circle any one of the three words , if they though that was what the author had intended  . Alternatively , if they though that the author had intended something else  , they could write down " other " . Finally , if they were n't sure , they conld write ' , 9 , , . The distribution of responses is shown in the following table  . 
choice 0 choice 1 choice 2 other ? total
Judge 1 Judge 2 Judge 3   99   124   93   188   176   167   175   159   151   28   26   30   74   79   123   5~   5~   5~ The results show tlm tspell is rejecting too many words  , since choice 0 ( spell error ) is selected about 20% of the time . In these cases , correct was given a non-problem to correct : acquire es acquirers acquires be acquirers  , as they have been , than acquire es . * E ** S * If the industrials had attracted bids tit Since we were mostly concerned with evaluating the scoring function  , we didn't want to be distracted with errors in spell and other problems that are beyond the scope of this paper  . 
Therefore , we decided to consider only those cases where at least two judges circled one of the two candidates  , and they agreed with each other . This left 329 triples . 
The following table shows that correct agrees with the majority of the judges in  87% of tt le 329 cases of interest . In order to help c~dibrate this result , three inferior methods , are also evaluated . The no-prior method ignores the prior probability  . The no-channel methodignol vs the channel probability  . Finally , then either method ignores both probabilities and selects the first candidate in " all cases  . As the following table shows , correct is significantly better than the three inferior alternatives  . Both the channel and the prior probabilities provide a significant contribution  , and the combination is significantly better than either in isolation  . The second half of the table evaluates the judges against one another and shows that they signiticantly out-perl brm correct  , indicating that there is plenty of room for further improvement  .   6 All three judges found the task more diffmult and time consuming than they had expected  . 
5 . For the purposes of this experiment , a type is a lowercase word rejected by the Unix @ spell program  . 
6 . Judges were only scored on triples for which they selected  "1" or "2  , " and for which the other two judges agreed on "1" or "22' A triple was scored " correct " for one judge if that judge agreed with the other two and " incorrect " if that judged is agreed with the other two  . 

Each judge spent about half a daygrading the 564 triples . 
Method correct no-prior no-channel neither
Judge 1
Judge 2
Judge 3
Discrimination % 286/32987+1 . 9 263/329 80 _+ 2 . 2 247/329 75 _+ 2 . 4 172/329 52 + 2 . 8 271/273 99 + 0 . 5 271/275 99 + 0 . 7 271/281 96 _+ 1 . 1 We were also interested in testing whether the score predicted accuracy  . The figure at the end of this paper shows that this is indeed so  . The horizontal axis shows the score from one of the three predictors  ( as the lines are lable led ) averaged over a group of 20 typos . The vertical axis shows the fraction of this group that were right  . The diagonal line indicates perfection . 
For example , consider a group of typos whose average score was  . 8 . Perfect accuracy would be achieved if exactly 80 percent of this group agreed with the majority opinion of the judges  . 
The curved lines above and below the perfection lines how one standard deviation limits for estimating probabilities from samples of  20  . The observations on correct are outside of the one standardeviation limits about as much as would be called for by chance  , while each of the other two methods has more points outside than would result just by chance  . We conclude that the scores from correct predict accuracy fairly well  ; scores from the other two methods are more problematic  . 
5. Conclusions
There have been a number of spelling correction programs in the past such as Kucera  ( 1988 ) that generated a list of candidates by looking for insertions  , deletions , substitutions and reversals , rauch as we have been doing here . Our contribution is the emphasis on scoring . 
Mcllroy , the author of the Unix spell program (1982) , intentionally focused on the spelling detection problem  , and argued ( private communication ) that spelling correction was a bad ideasolong as the correct or couldn't separate the plausible candidates from the implausible ones  . He felt that it was probably more distracting than helpful to bury the user under along list of mostly implausible candidates  . In this work , we have attempted to show that it is possible to sort the candidates by a likelihood function that agrees well enough with human judges to be helpful  . 
In future work , we would hope to extend the prior model to take advantage of context  . We noticed that the human judges were extremely reluctant to cast a vote given only the information available to the program  , and that they were much more comfortable when they could see a concordance line or two  . Perhaps our program could take advantage of these contextual cues by adopting very simple language modeling technique such as trigrams  , that have proven effective for speech recognition applications  ( Jelinek ,  1985) . Hopefully more interesting language models would improve performance even more  , 

Box , G . E . P . , and G . C . Tiao , 1973 Bayesian Inference in Statistical Analysis , Addison-Wesley , 
Reading , Massachusetts.
Gale , W . , Church , K . , (1990, submitted ), " What's
Wrong with Adding One ?"
Good , I . J . , (1953) , " The population frequencies of species and the estimation of population parameters  , "
Biometrika , v . 40, pp . 237-264.
Jelinek , F .   ( 1985 ) " Self-organized Language Modeling for Speech Recognition  , " IBM Report . 
Kucera , H . , (1988) , " Automated Word Substitution Using Numerical Rankings of Structural Disparity Between Misspelled Words & Candidate Substitution 
Words , " Patent Number : 4, 783, 758.
Mcllroy , M . , (1982) , " Development of a Spelling List , " IEEE Transactions on Communications , Vol . 
COM-30, No . 1.
Accuracy of Probabilities oo d ? 5 u , ) ?5  , /' ~ J0 , 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 predicted probability 6 . Appendix : Confusion Matrices
X b
Cdefghi
Jknop qrstuv wxy
Z@del\[X,Y \] = Deletion of Y after X
Y ( Deleted Letter ) ab , c .   .   .   .   .   . defghijk 1 mnop qrstuv wxyz 0   7   58   21   3   5   18   8   61   0   4   43   5   53   0   9   0   98   28   53   62   1   0   0   2   0   2   2   1   0   22   0   0   0   183   0   0   26   0   0   2   0   0   6   17   0   6   1   0   0   0   0   37   0   70   0   63   0   0   24   320   0   9   17   0   0   33   0   0   46   6   54   17   0   0   0   1   0   12   0   7   25   45   0   10   0   62   1   1   8   4   3   3   0   0   I1   1   0   3   2   0   0   6   0   80   1   50   74   89   3   1   1   6   0   0   32   9   76   19   9   1   237   223   34   8   2   1   7   1   0   4   0   0   0   13   46   0   0   79   0   0   12   0   0   4   0   0   11   0   8   1   0   0   0   1   0   25   0   0   2   83   1   37   25   39   0   0   3   0   29   4   0   0   52   7   1   22   0   0   0   1   0   15   12   1   3   20   0   0   25   24   0   0   7   1   9   22   0   0   15   1   26   0   0   1   0   1   0   26   1   60   26   23   1   9   0   1   0   0   38   14   82   41   7   0   16   71   64   1   1   0   0   1   7   0   0   0   0   1   0   0   0   0   0   0   0   0   1   1   0   0   0   0   0   1   0   0   0   0   0   4   0   0   1   15   1   8   1   5   0   1   3   0   17   0   0   0   1   5   0   0   0   1   0   0   0   24   0   1   6   48   0   0   0   217   0   0   211   2   0   29   0   0   2   12   7   3   2   0   0   11   0   15   10   0   0   33   0   0   1   42   0   0   0   180   7   7   31   0   0   9   0   4   0   0   0   0   0   21   0   42   71   68   1   160   0   191   0   0   0   17   144   21   0   0   0   127   87   43   1   1   0   2   0   11   4   3   6   8   0   5   0   4   1   0   13   9   70   26   20   0   98   20   13   47   2   5   0   1   0   25   0   0   0   22   0   0   12   15   0   0   28   1   0   30   93   0   58   1   18   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   18   0   0   0   0   0   63   4   12   19   188   0   11   5   132   0   3   33   7   157   21   2   0   277   103   68   0   10   1   0   27   0   16   0   27   0   74   1   0   18   231   0   0   2   1   0   30   30   0   4   265   124   21   0   0   0   1   0   24   1   2   0   76   1   7   49   427   0   0   31   3   3   11   1   0   203   5   137   14   0   4   0   2   0   26   6   9   10   15   0   1   0   28   0   0   39   2   111   1   0   0   129   31   66   0   0   0   0   1   0   9   0   0   0   58   0   0   0   31   0   0   0   0   0   2   0   0   1   0   0   0   0   0   0   1   0   40   0   0   1   11   1   0   11   15   0   0   1   0   2   2   0   0   2   24   0   0   0   0   0   0   0   1   0   17   0   3   0   0 I 0   0   0   0   0   0   0   6   0   0   0   5   0   0   0   0   1   0   2   1   34   0   2   0   1   0   1   0   0   1   2   1   1   1   0   0   17   1   0   0   1   0   0   0   1   0   0   0   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   20   14   41   31   20   20   7   6   20   3   6   22   16   5   5   17   0   28   26   6   2   1   24   0   0   2 


Cdefghi
Jknop qrstuv wxy
Z @ add\[X , Y \] = Insertion of Y after X
Y ( Inserted Letter ) a b c d e f g h i j k 1 mnop qrstuv wxyz 15   1   14   7   10   0   1   1   33   1   4   31   2   39   12   4   3   28   134   7   28   0   1   1   4   1   3   11   0   0   7   0   1   0   50   0   0   15   0   1   1   0   0   5   16   0   0   3   0   0   0   0   19   0   54   1   13   0   0   18   50   0   3   1   1   1   7   1   0   7   25   7   8   4   0   1   0   0   18   0   3   17   14   2   0   0   9   0   0   6   1   9   13   0   0   6   119   0   0   0   0   0   5   0   39   2   8   76   147   2   0   1   4   0   3   4   6   27   5   1   0   83   417   6   4   1   10   2   8   0   1   0   0   0   2   27   1   0   12   0   0   10   0   0   0   0   0   5   23   0   1   0   0   0   1   0   8   0   0   0   5   1   5   12   8   0   0   2   0   1   1   0   1   5   69   2   3   0   1   0   0   0   4   1   0   1   24   0   10   18   17   2   0   1   0   1   4   0   0   16   24   22   1   0   5   0   3   0   10   3   13   13   25   0 I 1   69   2   1   17   11   33   27   1   0   9   30   29   11   0   0   1   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   2   4   0   1   9   0   0   1   1   0   1 I 0   0   2   1   0   0   95   0   1   0   0   0   4   0   3   1   0   1   38   0   0   0   79   0   2   128   1   0   7   0   0   0   97   7   3   1   0   0   2   0   I1   1   1   0   17   0   0   1   6   0   1   0   102   44   7   2   0   0   47   1   2   0   1   0   0   0   15   5   7   13   52   4   17 O 34   0   1   1   26   99   12   0   0   2   156   53   1   1   0   0   1   0   14   1   1   3   7   2   1   0   28   1   0   6   3   13   64   30   0   16   59   4   19   1   0   0   1   1   23   0   1   1   10   0   0   20   3   0   0   2   0   0   26   70   0   29   52   9   1   1   1   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   15   2   1   0   89   1   1   2   64   0   0   5   9   7   10   0   0   132   273   29   7   0   1   0   10   0   13   1   7   20   41   0   1   50   101   0   2   2   10   7   3   1   0   1   205   49   7   0   1   0   7   0   39   0   0   3   65   1   10   24   59   1   0   6   3   1   23   1   0   54   264   183   11   0   5   0   6   0   15   0   3   0   9   0   0   1   24 I 1   3   3   9   1   3   0   49   19   27   26   0   0   2   3   0   0   2   0   0   36   0   0   0   10   0   0   1   0   1   0   1   0   0   0   0   1   5   1   0   0   0   0   0   0   1   10   0   0   1   1   0   1   1   0   2   0   0   1   1   8   0   2   0   4   0   0   0   0   0   18   0   1   0   0   6   1   0   0   0   1   0   3   0   0   0   2   0   0   0   0   1   0   0   5   1   2   0   3   0   0   0   2   0   0   1   1   6   0   0   0   1   33   1   13   0   1   0   2   0   2   0   0   0   5   1   0   0   6   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   4   46   8   9   8   26   11   14   3   5   1   17   5   6   2   2   10   0   6   23   2   11   1   2   1   1   2 
Xab
Cd
Cf ghi
Jkno
P q r s t u v w x

Z sub\[X , Y \] = Substitution of X ( incorrect ) for Y ( correct ) 
Y ( correct ) a b c d e f g h i j k 1 mnop qrstuv wxyz 0   0   7   1   342   0   0   2   118   0   1   0   0   3   76   0   0   1   35   9   9   0   1   0   5   0   0   0   9   9   2   2   3   1   0   0   0   5   11   5   0   10   0   0   2   1   0   0   8   0   0   0   6   5   0   16   0   9   5   0   0   0 I 0   7   9   1   10   2   5   39   40   1   3   7   1   1   0   1   10   13   0   12   0   5   5   0   0   2   3   7   3   0   1   0   43   30   22   0   0   4   0   2   0   388   0   3   11   0   2   2   0   89   0   0   3   0   5   93   0   0   14   12   6   15   0   1   0   18   0   0   15   0   3   1   0   5   2   0   0   0   3   4   1   0   0   0   6   4   12   0   0   2   0   0   0   4   1   11   11   9   2   0   0   0   1   1   3   0   0   2   1   3   5   13   21   0   0   1   0   3   0   1   8   0   3   0   0   0   0   0   0   2   0   12   14   2   3   0   3   1   11   0   0   2   0   0   0   103   0   0   0   146   0   1   0   0   0   0   6   0   0   49   0   0   0   2   1   47   0   2   1   15   0   0   1   1   9   0   0   1   0   0   0   0   2   1   0   0   0   0   0   5   0   0   0   0   0   0   0   1   2   8   4   1   1   2   5   0   0   0   0   5   0   2   0   0   0   6   0   0   0   . 4  0   0   3   2   10   1   4   0   4   5   6   13   0   1   0   0   14   2   5   0   11   10   2   0   0   0   0   0   0   1   3   7   8   0   2   0   6   0   0   4   4   0   180   0   6   0   0   9   15   13   3   2   2   3   0   2   7   6   5   3   0   1   19   1   0   4   35   78   0   0   7   0   28   5   7   0   0   1   2   0   2   91   1   1   3   116   0   0   0   25   0   2   0   0   0   0   14   0   2   4   14   39   0   0   0   18   0   0   11   1   2   0   6   5   0   2   9   0   2   7   6   15   0   0   1   3   6   0   4   1   0   0   0   0   0   1   0   0   0   27   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   14   0   30   12   2   2   8   2   0   5   8   4   20   1   14   0   0   12   22   4   0   0   1   0   0   11   8   27   33   35   4   0   1   0   1   0   27   0   6   1   7   0   14   0   15   0   0   5   3   20   1   3   4   9   42   7   5   19   5   0   1   0   14   9   5   5   6   0   11   37   0   0   2   19   0   7   6   20   0   0   0   44   0   0   0   64   0   0   0   0   2   43   0   0   4   0   0   0   0   2   0   8   0   0   0   7   0   0   3   0   0   0   0   0   1   0   0   1   0   0   0   8   3   0   0   0   0   0   0   2   2   1   0   1   0   0   2   0   0 I 0   0   0   0   7   0   6   3   3   1   0   0   0   0   0   0   0   0   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   9   0   0   0   0   0   0   0   0   0   2   0   15   0   1   7   15   0   0   0   2   0   6   1   0   7   36   8   5   0   0   1   0   0   0   0   0   7   0   0   0   0   0   0   0   7   5   0   0   0   0   2   21   3   0   0   0   0   3   0 
X b
Cd
Cf ghi
Jk
Imno
Pqr$tuvwx

Zrev\[X , Y \]= Reversal of XY
Y a b c d e f g h i j k 1 mnop qrstuv wxyz 0   0   2 I 1   0   0   0   19   0   1   14   4   25   10   3   0   27   3   5   31   0   0   0   0   0   0   0   0   0   2   0   0   0   0   0   0   1   1   0   2   0   0   0   2   0   0   0   0   0   0   0   0   0   0   0   1   0   0   1   85   0   0   15   0   0   13   0   0   0   3   0   7   0   0   0   0   0   0   0   0   0   0   0   0   0   7   0   0   0   0   0   0   0   0   1   0   0   2   0   0   0   0   0 I 0   4   5   0   0   0   0   60   0   0   21   6   16   11   2   0   29   5   0   85   0   0   0   2   0   0   0   0   0   0   0   0   0   12   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4   0   0   0   2   0   0   0   0   0   0   1   0   15   0   0   0   3   0   0   3   0   0   0   0   0   12   0   0   0   15   0   0   0   0   0   0   0   0   0   0   0   0   0   0   10   0   0   0   0   0   0   15   8   31   3   66   1   3   0   0   0   0   9   0   5   11   0   1   13   42   35   0   6   0   0   0   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   I1   0   0   12   20   0   1   0   4   0   0   0   0   0   1   3   0   0   1   1   3   9   0   0   7   0   9   0   0   0   20   0   0   0   0   0   0   0   0   2   0   0   0   0   0   0   4   0   0   0   0   0   15   0   6   2   12   0   8   0   1   0   0   0   3   0   0   0   0   0   6   4   0   0   0   0   0   0   5   0   2   0   4   0   0   0   5   0   0   1   0   5   0   1   0   11   1   1   0   0   7   1   0   0   17   0   0   0   4   0   0   1   0   0   0   0   0   0   1   0   0   5   3   ,   6   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   12   0   0   0   24   0   3   0   14   0   2   2   0   7   30   1   0   0   0   2   10   0   0   0   2   0   4   0   0   0   9   0   0   5   15   0   0   5   2   0   1   22   0   0   0   1   3   0   0   0   16   0   4   0   3   0   4   0   0   21   49   0   0   4   0   0   3   0   0   5   0   0   11   0   2   0   0   0   22   0   5   1   1   0   2   0   2   0   0   2   1   0   20   2   0   11   I1   2   0   0   0   0   0   0   0   0   0   0 I 0   0   0   4   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   4   0   0   0   0   1   1   1   0   0   0   0   0   0   0   0   0   8   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   1   2   0   0   0   1   0   0   0   0   3   0   0   0   2   0   1   10   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
