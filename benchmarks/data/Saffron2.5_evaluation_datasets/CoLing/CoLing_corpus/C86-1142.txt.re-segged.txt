The procedure to construct a word predictor in a speech understanding system 
from a task-specific grammar defined illa CFG or a DCG 
Yasuhisa Nilmi , Shigeru Uzuhara and Yutaka Kobayashi
Department of Computer Science
Kyoto Institute of Technology
Matsugasaki , Sakyoku , Kyoto 606, Japan

This paper describes a method for converting a task-dependent grammar into a word predictor of a speech understanding system  , Since tile word prediction is a topdown operation  , left recursive rules induces an infinite looping  . We have solved this problem by applying an algorithm for bottom-up parsing  ,  1 . Introduction which tile ends terminate at different portions on the phonetic sequence  , and the other represents the sequences of syntactic categories  ( called category sequences )  , each of which is associated with one of the word strings  , ln this situation , the controller chooses the word string with tile highest score  , send stile associated category sequence to the word predictor and asks it to predict those syntactic categories which can syntactically follow the selected sequence  . 
In this paper we present a method for converting a task-specific gravm nar into a word predictor  , an important component of a speech understanding system  . 
A contextfree grammar ( CFG ) or an augmented transition network grammar ( ATNG ) have been used to des-cribe task-specl fic constraint  . When a CFG is used , Early's algorithm\[l\] , one of the most efficient topdown parsing algorithms  , has been used to make word prediction \[2\] . When an ATNG is used , word prediction is simply made by tentatively traveling along arcs going out freln a state in an  ATNG\[3\]  , \[4\] , \[5\] . Since the word prediction is a topdown operation , it is difficult to avoid falling into an infinite loop if the task-specific grammar includes a left recurslve rule  . 
F . Perelra and D . Warren have developed a definite clause grammar ( DCG ) \[6\] . The rules described in a DCG are directly converted into a set of Pro\]og clauses  , which works as a parser with an aid of tile powerful pattern matching mechanism of Prolog  . Thus syntactic analysis can be done without writing a specia \] parser working on the rules of the grammar  . Since tile syntactic analysis based on a DCG parser also works in topdown fashion  , it shares the same difficulty as the topdown parsers have  .  ? . Matsumoto et at . have developed a method for converting a set of rules described in a DCG into a bottom-up parser which has overcome thls difficulty without any loss of the advantages of a  DCG\[7\]  . 
We discuss an application of this method to a word predictor  , that is , the method for transforming task-specific l inguistic constraint defined in a CFG or a DCG into a Prolog program which acts as a left-to -right word predictor  . 
2 . Word predictlon in a ~ eech understand lnq~sj ( stem Fig . l shows a typical configuration of a speech understanding system based on a hierarchical model  . 
An acoustic-phonetic processor analyzes of an input uttereance and transforms it into a sequence of phonetically labeled segments  . Provided that a part of an utterance has been dealt with  , the controller manages its interpretations in the two kinds of trees illustrated in Fig  . 2; one represents word strings , of linguistic processor predicted categories ~tegory sequence ~  . roller ) predicted words T-rocog , ied words
I lexlcal processor \]
Phonetic lattice
I ac?ustic-ph?netic processor I speec ~ wave Fig . IA typical configuration of a speech understanding system  . 
category tree word tree sequence of phonetic segments 
C3 #3
Cl#I'~C2'~~,
I\!t % i ~ I "?', /' r " i , l ~. L ~\\ I
W1i'1'I\
Fig . 2 A search space of a speech understanding system . 

The word predictor could parse a given category sequence and predict the categories which can follow it  . It is , however , inefficient to analyze the given sequence whenever asked to predict  . In fact , each node of the category tree is associated with a parsing history on how rules of the grammar have been applied to analyze the category sequence  . The word predictor receives a node and its parsing history from the controller and predicts the syntactic cate ~ gories following the node  . 
3_ . The bottom-up parser and its application to word prediction We give a brief explan at lon of the bottom-up parser proposed by Y  . Matsumoto et al . Assumes imply that the rules of the grammar are described in a CFG  . 
Then , without loss of generality each of the rules can be expressed as either of the followings  . 
c->Cl , C2, . .,c n ( c , c i ( i=l .   .   .   . n ) : nonterminals ) l ) c -> w ( w : a terminal )  2 )   ( l ) These rules are transformed into the following
Prolog clauses.
cI(G,XI,X ):- link(c,G ), goal(c2, Xi,X2)....
goal(cn,Xn_l,Xn ), c(G,Xn,X ) . l ') dict(c,\[wJX\],X ) . ?') X and X ~( i = l .   .   .   . n ) are arguments to denote word strifig to be analyzed as a list  . ' link(C , G ) is a predicate to express that a string of which the leftmost symbol is a nonterminal C can be reduced to a nonterminal G  . G is called a goal argument in this sense . ' link ' is defined as follows : if the rule I ) is included in the grammar , then ' link(cl , c ) ' holds , and if ' link(a , b ) ' and ' link(b , c ) '& old , then ' link(a , c ) ' holds ( transitive law ) , and ' link(c , c ) ' holds for every nonterminal c ( reflective law )  . A predicate ' dict(C , X , Y ) ' , searching the dictionary for the first word of a word string X  , unifies C with its syntactic category and Y with the remaining string  . 
(2) A predicate goal(G,X,Z ) is defined as follows . 
goal(G , X , Z ) :- dict(C , X , Y) , link(C , G ) , exec(C , G , Y , Z ) .  3 ) where ' exec'is a predicate to execute a predicate ' c  ( G , Y , Z)' . 
(3) Furthermore , fGr any nonterminal C , the following assertion called a terminal condition holds : c  ( c , X , X ) .  4 ) The parser for the given grammar consists of all these Prolog clauses  . 
In order to use the bottom-up parser as a left- to-right word predictor  , we change the predicate ' goal ' as follows: goal  ( G , \[ \]  , \[\]) :- llnk(C , G) , terminal ( C ) , output ( C ) , f ai . 3'- I ) exec(C,G,Y,Z ) .  3'-2 ) where ' terminal ( C ) ' is a predicate to be true when a nonterminal C appears in the lefthand side of a product lonof  2  )  . 
The modified parser , receiving a word string from the controller , executes the second of ' goal ' clauses in which the second argument X is unified with the given word string  . Syntactic analysis of X is continued until X becomes empty  . Then , the first of ' goal ' clauses is invoked and predicts all the syntactic categories which make both ' link  ( C , G ) ' and ' terminal(C ) ' hold . 
4 . Word grediction under a left-to-ri ~ In this section we discuss the method for conver -tion of a set of productions defined in a CFG into a set of Prolog clauses which acts as a lef t-to-right word predictor  . In order that this predictor can work without reanalyzing a given category sequence  , we must ) lave a table ( named a history table ) which contains an association of a category sequence with its parsing history  , that is , a history on how productions are used to parse the sequence  . 
Considering a transition network depicted in Fig . 3 for a production ' c -> clc ~ . .c ' , we express a parsing history with a list of Lpai~s of a state name in a transition network and a goal argument appearing in bottom-up parsing  . For the grammar shown in Fig . 4 , a category sequence ' NN'is parsed as shown in Fig  . 5 ( a ) and the corresponding state transition is shown in Fig  . 5(b ) . A parsing history for this sequence can be expressed as a list \[  nps2  , s \] . The state name ' nps2' indicates that the last'N'of the
ClC2 Cn
Fig . 3A transition network for a rule
C -, CIC2 . . Cn .
S -> NP VP NP -> N
NP->NP NVP->VNP
NP -> ARTNP
Fig . 4An example of contextfree grammar.
. ~.>_>-. s/

NN(a ) ( b)
Fig .   5 The parse tree of ' NN ' and the corresponding state transition  . 
sequence ' NN ' has been parsed as ' N ' in the produc-Lion ' NP->NPN'  , and the goal argument's ' indicates that the sequence is the leftmost part of the string derived by the start symbol's'  . 
Now we shall describe the procedure to transform a set of productions described in a CFG into a word predictor  . 
(I ) For a production ' c -> c . c  ~ .   . c' , tile following set ?/ L:n of Prolog clauses Is generated : cI  ( \[GIH\] ) :- link ( c , G) , al(\[GIHI) . 
al(E ) :-. pred(c2,\[a21E\]).
a2(E):--pred(c3 , \[ a31 E\]) , an~l(E):-pred(cn , \[ an JE \]) , an(E ) :- c(E ) . 4- I ) where H and E are the arguments to store parsing histories  , the first element of H is a state name and that of E is a goal argument  . 
(2) For a nonterminal c , the following terminal con-dition holds : c ( \[c , alE\]):-exec(a , E ) ,  42 )   ( 3 ) Corresponding to ' goal ' in the bottom-up parser  , a predicate ' pred ' is defined as follows: pred  ( G , H ) :- link(C , G ) , terminal ( C ) , new face ( No ) , hand to ( No , C ) , make node ( No , C , \[GTH\]) , fail .  43 ) A predicate ' new face ( No ) ' generates a new node number in ' No ' , ' h and_to(No , C ) ' sends a pair of a node number ' No ' and a predicted syntactic category C to the control ler  , and ' make node ( )' stores a node number and i t s corresponding parsing history expressed as ' C  ( \[GIN\] ) ' in the history table . 
(4 ) The controller in a speech understanding system communicates the word predictor through a predi-cate ' want word ' which sends to the word predictor a node number associated with a category sequence which the controller has selected  , while the word predictor returns through'h and to ' a set of the syntactic categories which can follow the selected category sequence  . The definition of ' want word ' is as follows : want word  ( O )  : - !  , pred(s ,  \ [ \ ] )  . 44) want word ( No ): - pick_up(No , Z ) ,  !  , call ( Z ) .  45 ) The symbols in 44 ) signifies the start symbol , and the clause 44 ) is used to make a prediction at the leftmost part of an utterance  . The predicate ' pick up ( No , Z ) ' looks up the history table for a node number ' No '  , and pick supits associated history expressed as ' C  ( \[GIH\] )  '  , the execution of which invokes the clause of 4 -I ) or 42 )  . 
5. Conclusions
In this paper we have proposed the procedure to convert a grammar defined in a CFG or a DCG into a Prolog program which functions as a word predictor  . 
The procedure is give@for the left-to-r ight control  , but it is not difficult to expand it for the island-driven control  . 
To simplify the description , we have given the conversion procedure for a grammar defined in a CFG  , but it is easy to expand it for a grammar defined in a DCG  , As long as one concernes on a speech understanding system in which syntax and semantics are well defined  , one could take an advantage of a DCG in which a nonterminal can have some arguments as parameters  , and could use semantic restrictions effec -tively to interpret an utterance  . In developing a speech understanding system of which the task is to access a database  , we use semantic markers to des-cribe semanticres trictions between an adjective and a noun  , a noun phrase and a postposition ( in Japanese) , and case slots of a verb and its fillers . In this case a rule can be expressed as follows : C  ( So ) ->\[ Po ( So , SI)CI(SI)PI(SI , S2) C2(S2) .   .   . 
Pn_l(Sn_l,Sn )) Cn(Sn ), where S~(i = O , l .   .   .   . n ) is a list of semantic markers , Pi(i = l , 2  .   .   .   . n ) is a predicate to denote a constraint among semantic markers  . Considering a transition network for this DCG rule  , we associate P . with its \] i-th state and let Pi function as a converter of semantic markers  . Since Pi would be defined in the form of a table , this converter could work bidirectionally . In addition , stacking a pair " of a syntactic goal variable and a list of semantic markers in the parsing history  , we can develop a procedure to transform a grammar described in a DCG into a word predictor  . 

This research was supported by the grant-in-a id for the special project research ' In tell igent Processing and Integration of Knowledge Informations in Multi-Media ' by the Minist ry of Education  , Science and Culture of Japan . 
References\[I \] J . Early : An efficient context-free parsing algorithm  , Comm , ACM ,  13--2 (1970) . 
\[2\]T . Sakai and S . Nakagawa : A speech understanding system of simple Japanese sentences in a task domain  , Trans . of IECEJ , E60-1 (1977) . 
\[3\]W . A . Woods et al . : Speech understanding systems -- Final technical progress report  30 October 1974 to 29 October 1976  , BBN Tech . Rep . 3438, vol .  4 (1976) . 
\[4\] D . R . Reddy et al .   ; Speech understanding system--Summary of results of tile five year research effort at Carnegie-Me\]Ion Univ  . , Carnegie Mellon
Univ . Tech . Rep . (1977).
\[5\]Y . Niimi and Y . Kobayashi : A voice-input programming system using BASIC-like language  , Proc . IEEE
Int . Conf . ASSP (1978).
\[6\]F . C . N . Pereira and D . II . D . Warren : Definite clause grammar for language analysis -- A survey of the formalism and comparison with augmented transi-tion networks  , Artificial Intelligence ,  13 (1980) . 
\[7\]Y . Matsumoto et al . : BUP---A bottom-up parser embedded in Prolog , New Generation Computing , I-2 (1983) . 

