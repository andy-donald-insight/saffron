Text Generation from Keywords
Kiyotaka Uchimoto ? Satoshi Sekine?Hitoshi Isahara ? 
? Communications Research Laboratory
2-2-2, Hikaridai , Seikacho , Sorakugun,
Kyoto , 619-0289, Japan

? New York University
715 Broadway , 7th floor
New York , NY 10003, USA


We describe a method for generating sentences from ? keywords ? or ? head words ?  . This method consists of two main parts , candidate-text construction and evaluation . The construction part generates text sentences in the form of dependency trees by using complementary information to replace information that is missing because of a ? knowledge gap ? and other missing function words to generate natural text sentences based on a particular monolingual corpus  . The evaluation part consists of a model for generating an appropriate text when given keywords  . This model considers not only word ngram information  , but also dependency information between words . Furthermore , it considers both string information and morphological information  . 
1 Introduction
Text generation is an important technique used for applications like machine translation  , summarization , and human/computer dialogue . In recent years , many corpora have become available , and have been used to generate natural surface sentences  . For example , corpora have been used to generate sentences for language model estimation in statistical machine translation  . In such translation , given a source language text , S , the translated text , T , in the target language that maximizes the probability P  ( TS ) is selected as the most appropriate translation , T best , which is represented as ( Brown et al ,  1990)
T best = argmax TP ( TS ) = argmax T ( P ( ST ) ? P ( T ) )  . (1) In this equation , P ( ST ) represents the model used to replace words or phrases in a source language with those in the target language  . It is called a translation model . P ( T ) represents a language model that is used to reorder translated words or phrases into a natural order in the target language  . The input of the language model is a ? bag of words  , ? and the goal of the model is basically to reorder the words  . At this point , there is an assumption that natural sentences can be generated by merely reordering the words given by a translation model  . To give such a complete set of words , however , a translation model needs a large number of bilingual corpora  . If we could automatically complement the words needed to generate natural sentences  , we would not have to collect the large number of bilingual corpora required by a translation model  . In this paper , we assume that the role of the translation model is not to give a complete set of words that can be used to generate natural sentences  , but to give a set of head words or center words that a speaker might want to express  , and describe a model that can provide the complementary information needed to generate natural sentences by using a target language corpus when given a set of head words  . 
If we denote a set of head words in a target language as K  , we can express Eq . (1) as
P ( TS ) = P ( KS ) ? P ( TK ). (2)
P ( KS ) in this equation represents a model that gives a set of head words in the target language when given a source-language text sentence  . P ( TK ) represents a model that generates text sentence T when given a set of head words  , K . We call the model represented by P ( TK ) a text-generation model . In this paper , we describe a text-generation model and a generation system that uses the model  . Given a set of head words or keywords , our system outputs the text sentence that maximizes P  ( TK ) as an appropriate text sentence , T best :
T best = argmax TP ( TK ) = argmax T ( P ( KT ) ? P ( T ) )  . (3) In this equation , we call the model represented by P ( KT ) a keyword-production model . This equation is equal to Eq .   ( 1 ) when a source-text sentence is replaced with a set of keywords  . Therefore , this model can be regarded as a model that translates keywords into text sentences  . The model represented by P ( T ) in Eq .   ( 3 ) is a language model used in statistical machine translation  . The ngram model is the most popular one used as a language model  . 
We assume that there is one extremely probable ordered set of morphemes and dependencies between words that produce keywords  , and we express P ( KT ) as
P ( KT ) ? P ( K , M , DT ) = P ( KM , D , T ) ? P ( DM , T ) ? P ( MT) . (4) In this equation , M denotes an ordered set of morphemes and D denotes an ordered set of dependencies in a sentence  . P(KM , D , T ) represents a keyword-production model . To estimate the models represented by P ( DM , T ) and P ( MT) , we use a dependency model and a morpheme model , respectively , for the dependency analysis and morphological analysis  . 
Statistical machine translation and example-based machine translation require numerous high-quality bilingual corpora  . Interlingual machine translation and transfer -based machine translation require a parser with high precision  . 
Therefore , these approaches to translation are not practical if we do not have enough bilingual corpora or a good parser  . This is especially so if the source text -sentences are incomplete or have errors like those often found in OCR and speech recognition output  . In these cases , however , if we translate head words into words in the target language and generate sentences from the translated words by using our method  , we should be able to generate natural sentences from which we can grasp the meaning of the source -text sentences  . 
The text-generation model represented by P ( TK ) in Eq .   ( 2 ) can be applied to various tasks besides machine translation  . 
? Sentence-generation support system for people with a phasia : About  300  , 0 00 people are reported to suffer from a phasia in Japan  , and 40% of them can select only a few words to describe a picture  . If candidate sentences can be generated from these few words  , it would help these people communicate with their families and friends  . 
? Support system for second language writing : Beginners writing in second language usually fined it easy to produce center words or head words  , but often have difficulty generating complete sentences  . If several possible sentences could be generated from those words  , it would help beginners communicate with foreigners or study second-language writing  . 
These are just two examples . We believe that there are many other possible applications  . 
2 Overview of the Text-Generation

In this section , we give an overview of our system for generating text sentences from given keywords  . As shown in Fig .  1 , this system consists of three parts : generation -rule acquisition  , candidate-text sentence construction , and evaluation . 
Figure 1: Overview of the text-generation system.
Given keywords , text sentences are generated as follows . 
1 . During generation-rule acquisition , generation rules for each keyword are automatically acquired  . 
2 . Candidate-text sentences are constructed during candidate-text construction by applying the rules acquired in the first step  . Each candidate-text sentence is represented by a graph or dependency tree  . 
3 . Candidate-text sentences are ranked according to their scores assigned during evaluation  . The scores are calculated as a probability estimated by using a keyword-production model and a language model that are trained with a corpus  . 
4 . The candidate-text sentence that maximizes the score or the candidate-text sentences whose scores are over a threshold are selected as output  . The system can also output candidate-text sentences that are ranked within the top N sentences  . 
In this paper , we assume that the target language is Japanese . We define a keyword as the head word of a bunsetsu  . A bunsetsu is a phrasal unit that usually consists of several content and function words  . We define the head word of a bunsetsu as the rightmost content word in the bunsetsu  , and we define a content word as a word whose part -of-speech is a verb  , adjective , noun , demonstrative , adverb , conjunction , attribute , interjection , or undefined word . We define the other words as function words . We define formal nouns and auxiliary verbs ? SURU  ( do ) ? and ? NARU ( become ) ? as function words , except when there are no other content words in the same bunsetsu  . Part-of-speech categories follow those in the Kyoto University text corpus  ( Version 3 . 0) ( Kurohashi and Nagao ,  1997) , a tagged corpus of the Mainichi newspaper . 
Figure 2: Example of text generated from keywords.
For example , given the set of keywords ? kanojo ( she ) , ? ? ie ( house ) , ? and ? iku(go ) , ? as shown in Fig .  2 , our system retrieves sentences including each word  , and extracts each bunsetsu that includes each word as a head word of the bunsetsu  . If there is no tagged corpus such as the Kyoto University text corpus  , each bunsetsu can be extracted by using a morphological-analysis system and a dependency -analysis system such as JUMAN  ( Kurohashi and Nagao , 1999) and KNP ( Kurohashi ,  1998) . Our system then acquires generation rules as follows  . 
? ? kanojo ( she ) ? kanojo ( she ) no ( of ) ??? kanojo ( she ) ? kanojo ( she ) ga ? ? ? i e ( house ) ? i e ( house ) ni ( to ) ??? iku ( go ) ? iku ( go ) ??? iku ( go ) ? itta ( went ) ? The system next generates candidate bunsetsus for each keyword and candidate-text sentences in the form of dependency trees  , such as ? Candidate 1? and ? Candidate 2? in Fig .  2 , with the assumption that there are dependencies between keywords  . Finally , the candidate-text sentences are ranked by their scores  , calculated by a text-generation model , and transformed into surface sentences . 
In this paper , we focus on the keyword-production model represented by Eq  .   ( 4 ) and assume that our system outputs sentences in the form of dependency trees  . 
3 Candidate-Text Construction
We automatically acquire generation rules from a monolingual target corpus at the time of generating candidate-text sentences  . Generation rules are restricted to those that generate bunsetsus  , and the generated bunsetsus must include each input keyword as a head word in the bunsetsu  . We then generate candidate-text sentences in the form of dependency trees by simply combining the bunsetsus generated by the rules  . 
The simple combination of generated bunsetsus may produce semantically or grammatically inappropriate candidate-text sentences  , but our goal in this work was to generate a variety of text sentences rather than a few fixed expressions with high precision  1  . 
3.1 Generation-Rule Acquisition
Let us denote a set of keywords as KS and a set of rules  , each of which generates a bunset su when given keyword k  ( ? KS )  , as Rk . We then restrict rk ( ? Rk ) to those represented ask ? hkm ? .  (5)
In this rule , hk represents the head morpheme whose word is equal to keyword k  ; m ? represents zero , one , or a series of morphemes that are connected to h k in the same bunsetsu  . Here , we define a morpheme as consisting of a word and its morphological information or grammatical attribute  , such as part-of-speech , and we define a head morpheme as consisting of a head word and its grammatical attribute  . By applying these rules , we generate bunsetsus from input keywords . 
3.2 Construction of Dependency Trees
Given keywords K = k1k2 .   .   . kn , candidate bunsetsus are generated by applying the generation rules described in Section  3  . 1 . Next , by assuming dependency relationships between the bunsetsus  , candidate dependency trees are constructed . Dependencies between the bunsetsus are restricted in that they must have the following characteristics of Japanese dependencies :  1Note that 83  . 33% (3 , 973/4 , 768 ) of the head words in the newspaper articles appearing on January  17  ,   1995 were found in those appearing from January 1st to 16th  . 
However , only 21 . 82% (2 , 295/10 , 517 ) of the headword dependencies in the newspaper articles appearing on January  17th were found in those appearing from January 1st to 16th  . 
( i ) Dependencies are directed from left to right.
( ii ) Dependencies do not cross.
( iii ) All bunsetsus except the rightmost one depend on only one other bunsetsu  . 
For example , when three keywords are given and candidate bunsetsus including each keyword are generated as  b1  , b2 , and b3 , the candidate dependency trees are ( b1 ( b2b3 ) ) and ( ( b1 b2 ) b3 ) if we do not reorder keywords , but 16 trees result if we consider the order of keywords to be arbitrary  . 
4 Text-Generation Model
We next describe the model represented by Eq.
(4); that is , a keyword-production model , a morpheme model that estimates how likely a string is to be a morpheme  , and a dependency model . The goal of this model is to select optimal sets of morphemes and dependencies that can generate natural sentences  . We implemented these models within an maximum entropy framework  ( Berger et al , 1996; Ristad , 1997; Ristad ,  1998) . 
4.1 Keyword-Production Models
This section describes five keyword-production models which are represented by P  ( KM , D , T ) in Eq .  (4) . In these models , we define the set of head words whose frequency in the corpus is over a certain threshold as a set of keywords  , KS , and we restrict the bunsetsus to those generated by the generation rules represented in form  ( 5 )  . 
We assume that all keywords are independent and that ki corresponds to word wj  ( 1 ? j ? m ) when text is given as a series of words w1 .   .   . wm . 
1. trigram model
We assume that ki depends only on the two anterior words w  j?1 and wj?2  . 
P ( KM,D,T ) = n ? i=1
P(kiwj?1, wj?2). (6) 2. posterior trigram model
We assume that ki depends only on the two posterior words w  j+1 and wj+2  . 
P ( KM,D,T ) = n ? i=1
P ( kiwj+1, wj+2). (7) 3. dependency bigram model
We assume that ki depends only on the two rightmost words wl and w  l?1 in the rightmost bunsetsu that modifies the bunsetsu including ki  ( see Fig .  3) . 
P ( KM,D,T ) = n ? i=1
P ( kiwl , wl?1). (8)
Figure 3: Relationship between keywords and words in bunsetsus  . 
4. posterior dependency bigram model
We assume that ki depends only on the head word , ws , and the word on its right , ws+1 , in the bunsetsu that is modified by the bunsetsu including ki  ( see Fig .  3) . 
P ( KM,D,T ) = n ? i=1
P(kiws , ws+1). (9) 5. dependency trigram model
We assume that ki depends only on the two rightmost words wl and w  l?1 in the rightmost bunsetsu that modifies the bunsetsu  , and on the two rightmost words wh and w h?1 in the leftmost bunsetsu that modifies the bunsetsu including ki  ( see Fig .  3) . 
P ( KM,D,T ) = n ? i=1
P ( kiwl , wl?1, wh , wh?1) .  (10) 4 . 2 Morpheme Model Let us assume that there are l grammatical attributes assigned to morphemes  . We call a model that estimates the likelihood that a given string is a morpheme and has the grammatical attribute j  ( 1 ? j ? l ) a morpheme model . 
Let us also assume that morphemes in the ordered set of morphemes M depend on the preceding morphemes  . We can then represent the probability of M , given text T ; namely , P ( MT ) in Eq .  (4):
P ( MT ) = n ? i=1
P ( mim i?1 where mi can be one of the grammatical attributes assigned to each morpheme  . 
4.3 Dependency Model
Let us assume that dependencies di ( 1 ? i ? n ) in the ordered set of dependencies D are independent  . We can then represent P ( DM , T ) in
Eq . (4) as
P ( DM , T ) = n ? i=1
P ( diM , T ). (12) 5 Evaluation
To evaluate our system we made 30 sets of keywords , with three keywords in each set , as shown in Table 1 . A human subject selected the sets from head words that were found ten Table  1: Input keywords and examples of system output . 
Input ( Keywords ) Ex . of system output ??????? ( ????  ( ??? ???? ) ) ?? ?? ???  ( (??? ??? )  ??? )  ?? ?? ??  ( (??? ??? )  ?? )  ??? ??? ??  ( ????  ( ??? ??? ) ) ?? ?? ??? ?? ?? ???  ( (??? ??? )  ??? )  ??? ?? ??  ( (???? ??? )  ?? )  ??? ?? ??  ( ????  ( ??? ????? ) ) ?? ?? ?? ? ?? ??  ( (?? ??? )  ???? )  ?? ?? ???  ( (??? ??? )  ??? )  ?? ?? ??  ( (??? ????? )  ?? )  ?? ?? ??  ( ???  ( ??? ???? ) ) ?? ??? ??  ( (??? ???? )  ?? )  ?? ?? ???  ( ???  ( ??? ???? ) ) ?? ??? ??  ( ???  ( ???? ????? ) ) ?? ?? ?? ?? ?? ??  ( (??? ??? )  ?????? )  ??? ?? ?  ( (???? ?????? )  ? )  ?? ?? ???  ( (??? ??? )  ?????? )  ?? ?? ????  ( (??? ??? )  ?????? )  ?? ?? ???  ( (??? ??? )  ??? )  ?? ?? ?? ??? ?? ??  ( (???? ???? )  ?????? )  ?? ??? ??  ( (??? ???? )  ???? )  ?? ?? ???  ( ???  ( ??? ??? ) ) ?? ??? ????  ( ???  ( ???? ??????? ) ) ?? ?? ???  ( (??? ??? )  ??? ) ??????????????? times or more in the newspaper articles on January  1st in the Kyoto University text corpus ( Version 3 . 0) without looking at the articles . 
We evaluated each model by the percentage of outputs that were subjectively judged as appropriate by one of the authors  . We used two evaluation standards . 
? Standard 1: If the dependency tree ranked first is semantically and grammatically appropriate  , it is judged as appropriate . 
? Standard 2: If there is at least one dependency tree that is ranked within the top ten and is semantically and grammatically appropriate  , it is judged as appropriate . 
We used head words that were found five times or more in the newspaper articles appearing from January  1st to 16th in the Kyoto University text corpus and also found in those appearing on January  1st as the set of head words , KS . 
For head words that were not in KS , we added their major part-of-speech categories to the set  . 
We trained our keyword-production models by using 1  , 129 sentences ( containing 10 , 201 head words ) from newspaper articles appearing on January 1st . We used a morpheme model and a dependency model identical to those proposed by Uchimoto et al  ( Uchimoto et al , 2001; Uchimoto et al , 1999; Uchimoto et al , 2000b ) . To train the models , we used 8 , 8 35 sentences from newspaper articles appearing from January  1st to 9th in 1995  . Generation rules were acquired from newspaper articles appearing from January  1st to 16th   . The total number of sentences was 18,435 . 
First , we evaluated the outputs generated when the rightmost two keywords  , such as ??? and ?? , ? on each line of Table 1 were input . 
Table 2 shows the results .   KM1 through KM5 stand for the five keyword-production models described in Section  4  . 1 , and MM and DM stand for the morpheme and the dependency models  , respectively . The symbol + indicates a combination of models . In the models without MM , DM , or both , P ( MT ) and P ( DM , T ) were assumed to be 1 . We carried out additional experiments with models that considered both the anterior and posterior words  , such as the combination of KM1 and KM2 or KM3 and KM4  . 
The results were at most 16/30 by standard 1 and 24/30 by standard 1  . 
Table 2: Results of subjective evaluation.
Model Standard 1 Standard 2
KM1 ( trigram ) 13/30 28/30
KM1+MM 21/30 28/30
KM1+DM 12/30 28/30
KM1+MM+DM 26/30 28/30
KM2(posterior trigram ) 6/30 15/30
KM2+MM 8/30 20/30
KM2+DM 10/30 20/30
KM2+MM+DM 9/30 25/30
KM3(dependency bigram ) 13/30 29/30
KM3+MM 26/30 29/30
KM3+DM 14/3028/30
KM3+MM+DM 27/30 29/30
KM4(posterior dependency bigram ) 10/30 18/30
KM4+MM 9/30 26/30
KM4+DM 9/30 22/30
KM4+MM+DM 13/30 27/30
KM5(dependency trigram ) 12/30 26/30
KM5+MM 17/30 28/30
KM5+DM 12/30 27/30
KM5+MM+DM 26/30 28/30
The models KM1+MM+DM,
KM3+MM+DM , and KM5+MM+DM achieved the best results , as shown in Table 2 . For models KM1 , KM3 , and KM5 , the results with MM and DM were significantly better than those without MM and DM in the evaluation by standard  1  . We believe this was because cases are more tightly connected with verbs than with nouns  , so models KM1 , KM3 , and KM5 , which learn the connection between cases and verbs  , can better rank the candidate-text sentences that have a natural connection between cases and verbs than other candidates  . 
Next , we conducted experiments using the 30 sets of keywords shown in Table 1 as inputs . We used two keyword-production models : model KM3+MM+DM   , which achieved the best results in the first experiment  , and model KM5+MM+DM , which considers the richest information . We assumed that the input keyword order was appropriate and did not reorder the keywords  . The results for both models were the same : 19/30 in the evaluation by standard 1 and 24/30 in the evaluation by standard 2  . The right column of Table 1 shows examples of the system output . For example , for the input ??? ( syourai , in the future ) , ??? ( shin-shin-tou , the New
Frontier Party ) , and ???? ( umareru , to be born )? , the dependency tree ? ( ??? [ syouraiwa ] ( ???? [ shin-shin-touga ]??????? [ umare rudarou  ]  ) )?  ( ? The New Frontier Party will be born in the future  . ?) was generated . This output was automatically complemented by the appropriate modality ?????  ( darou , will ) , which agrees with the word ???? ( syourai , in the future ) , as well as by postpositional particles such as ? ??   ( wa , case marker ) and ???( ga ) . For the input ???? ( gaikoku-jin , a foreigner ) , ??( kanyuu , to join ) , and ?? ( zouka , to increase )? , the dependency tree ? ( ( ???? [ gaikokujin no ] ???? [ kanyuu sya ga ] ) ?????? [ zoukashiteiru ] ) ?  ( ? Foreigner members are increasing in number . ?) was generated . This output was complemented not only by the modality expression ??????  ( shiteiru , the progressive form ) and postpositional particles such as ??? ( no , of ) and ???( ga ) , but also by the suffix ??? ( sya , person ) , and a compound noun ????? ( kanyuusya , member ) was generated naturally . 
In six cases , though , we did not obtain appropriate outputs because the candidate-text sentences were not appropriately ranked  . Improving the backoffability of the model by using classified words or synonyms as features should enable us to rank sentences more appropriately  . 
6 Related Work
Many statistical generation methods have been proposed  . In this section , we describe the differences between our method and several previous methods  . 
Japanese words are often followed by postpositional particles  , such as ? ga ? and ? wo ? , to indicate the subject and object of a sentence . There are no corresponding words in English . Instead , English words are preceded by articles , ? the ? and ? a , ? to distinguish definite and indefinite nouns , and so on , and in this case there are no corresponding words in Japanese  . Knight et al proposed a way to compensate for missing information caused by a lack of language -dependent knowledge  , or a ? knowledge gap ? ( Knight and Hatzivassiloglou , 1995; Langkilde and Knight , 1998a ; Langkilde and Knight , 1998b ) . They use semantic expressions as input , whereas we use keywords . Also , they construct candidate-text sentences or word lattices by applying rules  , and apply their language model , an ngram model , to select the most appropriate surface text . While we cannot use their rules to generate candidate-text sentences when given keywords  , we can apply their language model to our system to generate surface-text sentences from candidate -text sentences in the form of dependency trees  . We can also apply the formalism proposed by Langkilde  ( Langkilde , 2000) to express the candidate-text sentences . 
Bangalore and Rambow proposed a method to generate candidate-text sentences in the form of trees  ( Bangalore and Rambow ,  2000) . They consider dependency information when deriving trees by using XTAG grammar  , but they assume that the input contains dependency information  . Our system generates candidate-text sentences without relying on dependency information in the input  , and our model estimates the dependencies between keywords  . 
Ratna parkhi proposed models to generate text from semantic attributes  ( Ratnaparkhi ,  2000) . The input of these models is semantic attributes . His models are similar to ours if the semantic attributes are replaced with keywords  . 
However , his models need a training corpus in which certain words are replaced with semantic attributes  . Although our model also needs a training corpus , the corpus can be automatically created by using a morphological analyzer and a dependency analyzer  , both of which are readily available . 
Humphreys et al proposed using models developed for sentence-structure analysis to rank candidate -text sentences  ( Humphreys et al . , 2001) . As well as models developed for sentence -structure analysis  , we also use those developed for morphological analysis and found that these models contribute to the generation of appropriate text  . 
Berger and Lafferty proposed a language model for information retrieval  ( Berger and Lafferty ,  1999) . Their concept is similar to that of our model , which can be regarded as a model that translates keywords into text  , while their model can be regarded as one that translates query words into documents  . However , the purpose of their model is different : their goal is to retrieve text that already exists while ours is to generate new text  . 
7 Conclusion
We have described a method for generating sentences from ? keywords ? or ? head words ?  . This method consists of two main parts , candidate-text construction and evaluation . 
1 . The construction part generates text sentences in the form of dependency trees by providing complementary information to replace that missing due to a ? knowledge gap ? and other missing function words  , and thus generates natural text sentences based on a particular monolingual corpus  . 
2 . The evaluation part consists of a model for generating an appropriate text sentence when given keywords  . This model considers the dependency information between words as well as word ngram information  . Furthermore , the model considers both string and morphological information  . 
If a language model , such as a word ngram model , is applied to the generated-text sentences in the form of dependency trees  , an appropriate surface-text sentence is generated . 
The word-order model proposed by Uchimoto et al . can also generate surface text in a natural order  ( Uchimoto et al , 2000a ) . 
There are several possible directions for our future research  . In particular , ? We would like to expand the generation rules . We restricted the generation rules automatically acquired from a corpus to those that generate a bunsetsu  . To generate a greater variety of candidate-text sentences  , we would like to expand the rules that can generate a dependency tree  . Expansion would lead to complementing with content words as well as function words  . 
We also would like to prepare default rules or to classify words into several classes when no sentences including the keywords are found in the target corpus  . 
? Some of the Nbest text sentences generated by our system are semantically and grammatically unnatural  . To remove such sentences from among the candidate -text sentences  , we must enhance our model so that it can consider more information  , such as classified words or those in a thesaurus . 
? We restricted keywords to the head words or rightmost content words in the bunsetsus  . 
We would like to expand the definition of keywords to other content words and to synonyms of the keywords  . 

We thank the Mainichi Newspapers for permission to use their data  . We also thank Kimiko Ohta , Hiroko Inui , Takehito Utsuro , Man-abu Okumura , Akira Ushioda , Jun?ichi Tsujii , Kiyosi Yasuda , and Masahisa Ohta for their beneficial comments during the progress of this work  . 

S . Bangalore and O . Rambow .  2000 . Exploiting a Probabilistic Hierarchical Model for Generation  . In Proceedings of the COLING , pages 42?48 . 
A . Berger and J . Lafferty .  1999 . Information Retrieval as Statistical Translation . In Proceedings of the ACM SIGIR , pages 222?229 . 
A . L . Berger , S . A . Della Pietra , and V . J . Della Pietra .  1996 . 
A Maximum Entropy Approach to Natural Language Processing  . Computational Linguistics , 22(1):39?71 . 
P . F . Brown , J . Cocke , S . A . Della Pietra , V . J . Della Pietra , F . Jelinek , J . D . Lafferty , R . L . Mercer , and P . S . Rooss in . 
1990 . A Statistical Approach to Machine Translation . 
Computational Linguistics , 16(2):79?85.
K . Humphreys , M . Calcagno , and D . Weise .  2001 . Reusing a Statistical Language Model for Generation  . In Proceedings of the EWNLG . 
K . Knight and V . Hatzivassiloglou .  1995 . Two-Level , Many-Paths Generation . In Proceedings of the ACL , pages 252?260 . 
S . Kurohashi and M . Nagao .  1997 . Building a Japanese Parsed Corpus while Improving the Parsing System  . In Proceedings of the NLPRS , pages 451?456 . 
S . Kurohashi and M . Nagao , 1999 . Japanese Morphological Analysis System JUMAN Version  3  . 61 . Department of
Informatics , Kyoto University.
S . Kurohashi , 1998 . Japanese Dependency/Case Structure Analyzer KNP Version  2  . 0b6 . Department of Informatics,
Kyoto University.
I . Langkilde and K . Knight . 1998a . Generation that Exploits Corpus-Based Statistical Knowledge  . In Proceedings of the
COLINGACL , pages 704?710.
I . Langkilde and K . Knight . 1998b . The Practical Value of Ngrams in Generation . In Proceedings of the INLG . 
I . Langkilde .  2000 . Forest-Based Statistical Sentence Generation . In Proceedings of the NAACL , pages 170?177 . 
A . Ratnaparkhi .  2000 . Trainable Methods for Surface Natural Language Generation  . In Proceedings of the NAACL , pages 194?201 . 
E . S . Ristad .  1997 . Maximum Entropy Modeling for Natural Language . ACL/EACL Tutorial Program , Madrid . 
E . S . Ristad .  1998 . Maximum Entropy Modeling Toolkit , Release 1 . 6 beta . http://www . mnemonic . com/software/memt . 
K . Uchimoto , S . Sekine , and H . Isahara .  1999 . Japanese Dependency Structure Analysis Based on Maximum Entropy Models  . In Proceedings of the EACL , pages 196?203 . 
K . Uchimoto , M . Murata , Q . Ma , S . Sekine , and H . Isahara . 
2000a . Word Order Acquisition from Corpora . In Proceedings of the COLING , pages 871?877 . 
K . Uchimoto , M . Murata , S . Sekine , and H . Isahara . 2000b . 
Dependency Model Using Posterior Context . In Proceedings of the IWPT , pages 321?322 . 
K . Uchimoto , S . Sekine , and H . Isahara .  2001 . The Unknown Word Problem : a Morphological Analysis of Japanese Using Maximum Entropy Aided by a Dictionary  . In Proceedings of the EMNLP , pages 91?99 . 
