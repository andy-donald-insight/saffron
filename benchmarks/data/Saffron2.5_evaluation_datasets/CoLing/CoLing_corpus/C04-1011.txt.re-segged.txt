KullbackLeibler Distance
between Probabilistic ContextFree Grammars
and Probabilistic Finite Automata
Mark-Jan Nederhof
Faculty of Arts
University of Groningen
P.O . Box 716
NL-9700 AS Groningen , The Netherlands

Giorgio Satta
Department of Information Engineering
University of Padua
via Gradenigo , 6/A
I-35131 Padova , Italy


We consider the problem of computing the KullbackLeibler distance  , also called the relative entropy , between a probabilistic contextfree grammar and a probabilistic finite automaton  . We show that there is a closed form ( analytical ) solution for one part of the KullbackLeibler distance  , viz . 
the crossentropy . We discuss several applications of the result to the problem of distributional approximation of probabilistic contextfree grammars by means of probabilistic finite automata  . 
1 Introduction
Among the many formalisms used for description and analysis of syntactic structure of natural language  , the class of contextfree grammars ( CFGs ) is by far the best understood and most widely used  . Many formalisms with greater generative power , in particular the different types of unification grammars  , are ultimately based on CFGs . 
Regular expressions , with their procedural counterpart of finite automata  ( FAs )  , are not able to describe hierarchical , tree-shaped structure , and thereby seem less suitable than CFGs for full analysis of syntactic structure  . However , there are many applications where only partial or approximated analysis of structure is needed  , and where full contextfree processing could be prohibitively expensive  . Such applications can for example be found in realtime speech recognition systems : of the many hypotheses returned by a speech recognizer  , shallow syntactic analysis may be used to select a small subset of those that seem most promising for full syntactic processing in a next phase  , thereby avoiding further computational costs for the less promising hypotheses  . 
As FAs cannot describe structure as such , it is impractical to write the automata required for such applications by hand  , and even difficult to derive them automatically by training  . 
For this reason , the used FAs are often derived from CFGs , by means of some form of approximation . An overview of different methods of approximating CFGs by FAs  , along with an experimental comparison , was given by ( Nederhof ,  2000) . 
The next step is to assign probabilities to the transitions of the approximating FA  , as the application outlined above requires a qualitative distinction between hypotheses rather than the purely boolean distinction of language membership  . Under certain circumstances , this may be done by carrying over the probabilities from an input probabilistic CFG  ( PCFG )  , as shown for the special case of ngrams by ( Rimon and Herz , 1991; Stolcke and Segal ,  1994) , or by training of the FA on a corpus generated by the PCFG  ( Jurafsky et al ,  1994) . See also ( Mohri and Nederhof , 2001) for discussion of related ideas . 
An obvious question to ask is then how well the resulting PFA approximates the input PCFG  , possibly for different methods of determining an FA and different ways of attaching probabilities to the transitions  . Until now , any direct way of measuring the distance between a PCFG and a PFA has been lacking  . As we will argue in this paper , the natural distance measure between probability distributions  , the KullbackLeibler ( KL ) distance , is difficult to compute .   ( The KL distance is also called relative entropy . ) We can however derive a closedform ( analytical ) solution for the crossentropy of a PCFG and a PFA  , provided the FA underlying the PFA is deterministic  . The difference between the crossentropy and the KL distance is the entropy of the PCFG  , which does not rely on the PFA . This means that if we are interested in the relative quality of different approximating PFAs with respect to a single input PCFG  , the crossentropy may be used instead of the KL distance  . The constraint of determinism is not a problem in practice  , as any FA can be determinized , and FAs derived by approximation algorithms are normally determinized  ( and minimized )  . 
As a second possible application , we now look more closely into the matter of determinization of finite-state models  . Not all PFAs can be determinized , as discussed by ( Mohri ,  1997) . This is unfortunate , as deterministic ( P ) FAs process input with time and space costs independent of the size of the automaton  , whereas these costs are linear in the size of the automaton in the nondeterministic case  , which may be too high for some realtime applications  . Instead of distribution-preserving determinization  , we may therefore approximate a nondeterministic PFA by a deterministic PFA whose probability distribution is close to  , but not necessarily identical to , that of the first PFA . Again , an important question is how close the two models are to each other  . It was argued before by ( Juang and Rabiner , 1985; Falkhausen et al , 1995; Vihola et al ,  2002 ) that the KL distance between finite-state models is difficult to compute in general  . 
The theory developed in this paper shows however that the crossentropy between the input PFA and the approximating deterministic PFA can be expressed in closed form  , relying on the fact that a PFA can be seen as a special case of a PCFG  . Thereby , different approximating deterministic PFAs can be compared for closeness to the input PFA  . We can even compute the KL distance between two unambiguous PFAs  , in closed form .   ( It is not difficult to see that ambiguity is a decidable property for FAs  . ) The structure of this paper is as follows . 
We provide some preliminary definitions in Section  2  . Section 3 discusses the expected frequency of a rule in derivations allowed by a PCFG  , and explains how such values can be effectively computed  . The KL distance between a PCFG and a PFA is closely related to the entropy of the PCFG  , which we discuss in Section 4 . Essential to our approach is the intersection of PCFGs and PFAs  , to be discussed in Section 5 . As we show in Section 6 , the part of the KL distance expressing the cross entropy can be computed in closed form  , based on this intersection . Section 7 concludes this paper . 
2 Preliminaries
Throughout the paper we use mostly standard formal language notation  , as for instance in ( Hopcroft and Ullman , 1979; Booth and Thompson ,  1973) , which we summarize below . 
A contextfree grammar ( CFG ) is a 4-tuple G = (? , N , S , R ) where ? and N are finite disjoint sets of terminals and nonterminals  , respectively , S ? N is the start symbol and R is a finite set of rules  . Each rule has the form A ? ? , where A ? N and ??(?? N )? . 
The ? derives ? relation ? associated with G is defined on triples consisting of two strings ?  , ??(?? N )? and a rule pi ? R . We write ? pi ?? if and only if ? is of the form uA ? and ? is of the form u ? ?  , for some u ? ? ? , ??(?? N )? , and pi = ( A ??) . A leftmost derivation ( for G ) is a string d = pi1 ? ? ? pim , m ? 0 , such that ?0 pi1??1pi2 ? ? ? ? pim??m , for some ?0 ,   .   .   .   , ? m?(??N )? ; d =  ( where  denotes the empty string ) is also a leftmost derivation . In the remainder of this paper , we will let the term ? derivation ? refer to ? leftmost derivation ?  , unless specified otherwise . If ?0 pi1 ? ? ? ? pim ? ? m for some ? 0, .   .   .   , ? m?(??N )? , then we say that d = pi1 ??? pim derives ? m from ?0 and we write ?0 d ? ? m ; d =  derives any ?0?(? ? N ) ? from itself . 
A ( leftmost ) derivation d such that S d ? w , w ? ? ? , is called a complete derivation . If d is a complete derivation , we write y ( d ) to denote the ( unique ) string w ??? such that Sd ? w . 
The language generated by G is the set of all strings y  ( d ) derived by complete derivations , i . e . , L(G ) = wSd?w,d?R ?, w ? ? ? . 
It is wellknown that there is a one-to-one correspondence between complete derivations and parse trees for strings in L  ( G )  . 
A probabilistic CFG(PCFG ) is a pair G p = ( G , pG ) , where G is a CFG and pG is a function from R to real numbers in the interval  [0  ,  1] . 
A PCFG is proper if ? pi = ( A ? ? ) pG ( pi ) = 1 for all A ? N . Function pG can be used to associate probabilities to derivations of the underlying CFGG  , in the following way . For d = pi1 ? ? ? pim ? R ? , m ? 0 , we define pG ( d ) = ? m i=1 pG ( pii ) if Sd?w for some w ??? , and pG(d ) = 0 otherwise . The probability of a string w ??? is defined as pG  ( w ) = ? d : y ( d ) = wpG ( d )  . 
A PCFG is consistent if ? wpG(w ) = 1 . Consistency implies that the PCFG defines a probability distribution on the set of terminal strings as well as on the set of grammar derivations  . If a PCFG is proper , then consistency means that no probability mass is lost in ? in finite ? derivations  . 
A finite automaton ( FA ) is a 5-tuple M = (? , Q , q0 , Qf , T ) , where ? and Q are two finite sets of terminals and states  , respectively , q0 is the initial state , Qf ? Q is the set of final states , and T is a finite set of transitions , each of the form s a 7? t , where s , t ? Q and a ?? . A probabilistic finite automaton ( PFA ) is a pair Mp = ( M , pM ) , where M is an FA and pM is a function from T to real numbers in the interval  [0  ,  1] . 1 For a fixed ( P ) FAM , we define a configuration to be an element of Q ?  ??  , and we define the relation ` on triples consisting of two configurations and a transition ? ? T by:  ( s , w ) ? `( t , w ?) if and only if w is of the formaw ? , for some a ?? , and ? = ( sa7?t ) . A complete computation is a string c = ?1 ??? ? m , m ? 0 , such that ( s0 , w0) ?1`(s1 , w1)?2 ` ? ? ? ? m`(sm , wm ) , for some ( s0 , w0) ,   .   .   .   , ( sm , wm ) ? Q ? ? ? , with s0 = q0 , sm ? Qf and wm =  , and we write ( s0 , w0) c`(sm , wm ) . The language accepted by
M is L(M ) = w ? ? ?( q ?, w)c`(s , ), c ?
T ?, s?Q f .
For a PFAMp = ( M , pM ), and c = ?1 ? ? ? ? m ?
T ? , m ? 0 , we define pM ( c ) = ? m i=1 pM ( ? i ) if c is a complete computation , and pM(c ) = 0 otherwise . APFA is consistent if ? cpM(c ) = 1 . 
We say M is unambiguous if for each w ??? , ? s?Qf[(q0 , w)c`(s , )] for at most one c?T ? . 
We say M is deterministic if for each s and a , there is at most one transitions a7? t . Deter-minism implies unambiguity . It can be more readily checked whether an FA is deterministic than whether it is unambiguous  . Furthermore , any FA can be effectively turned into a deterministic FA accepting the same language  . 
Therefore , this paper will assume that FAs are deterministic  , although technically , unambiguity is sufficient for our constructions to apply  . 
3 Expectation of rule frequency
Here we discuss how we can compute the expectation of the frequency of a rule or a nonterminal over all derivations of a probabilistic contextfree grammar  . These quantities will be used later by our algorithms  . 
1Our definition of PFAs amounts to a slight loss of generality with respect to standard definitions  , in that there are no epsilon transitions and no probability function on states being final  . We want to avoid these concepts as they would cause some technical complications later in this article  . There is no loss of generality however if we may assume an end-of-sentence marker  , which is often the case in practice . 
Let ( A ??) ? R be a rule of PCFG Gp , and let d ? R ? be a complete derivation in G p . 
We define f(A ? ? ; d ) as the number of occurrences , or frequency , of A ? ? ind . Similarly , the frequency of nonterminal A in d is defined as f  ( A ; d ) = ? ? f(A ? ? ; d ) . We consider the following related quantities
EpGf(A ? ?; d ) = ? dpG(d ) ? f(A ? ?; d),
EpGf(A ; d ) = ? dpG(d ) ? f(A ; d ) = ? ?
EpGf(A ? ?; d).
A method for the computation of these quantities is reported in  ( Hutchins ,  1972) , based on the socalled momentum matrix . We propose an alternative method here , based on an idea related to the insideoutside algorithm  ( Baker , 1979; Lari and Young , 1990; Lari and Young ,  1991) . We observe that we can factorize a derivation d at each occurrence of rule A ?? into an ? innermost ? part  d2 and two ? outermost ? parts d1 and d3  . We can then write
EpGf(A ? ?; d ) = ? d=pi1???pim,m1,m2, w,?,v,x :
Sd1 ? wA ? , with d1 = pi1 ? ? ? pim1 ? 1 , ( A ??) = pim1 , ? d2 ? v , with d2 = pim1+1 ? ? ? pim2 , ? d3?x , with d3 = pim2+1 ? ? ? pimm ? i=1 pG(pii) . 
Next we group together all of the innermost and all of the outermost derivations and write 
EpGf(A ? ? ; d ) = outGp ( A ) ? pG ( A ? ? ) ? inGp ( ? ) where outGp ( A ) = ? d=pi1???pim , d ? = pi?1 ? ? ? pi?m ? , w , ? , x:
Sd ? wA ? , ? d ? ? x m ? i=1 pG ( pii ) ? m ?? i=1 pG ( pi?i ) and in Gp ( ? ) = ? d=pi1???pim , v : ? d?vm ? i=1 pG(pii ) . 
Bothout Gp ( A ) and in Gp ( ? ) can be described in terms of recursive equations , of which the least fixed-points are the required values  . If Gp is proper and consistent , then in Gp (?) = 1 for each ??(?? N )? . Quantities out Gp ( A ) for every A can all be ( exactly ) calculated by solving a linear system , requiring an amount of time proportional to the cube of the size of Gp  ; see for instance ( Corazza et al ,  1991) . 
On the basis of all the above quantities , a number of useful statistical properties of Gp can be easily computed  , such as the expected length of derivations , denoted EDL ( Gp ) and the expected length of sentences , denoted EWL(Gp ) , discussed before by ( Wether ell ,  1980) . These quantities satisfy the relations
EDL(Gp ) = EpGd = ?
A ? ? outGp(A ) ? pG(A ? ?) ? inGp(?),
EWL(Gp ) = EpGy(d ) = ?
A ? ? outGp(A ) ? pG(A ? ?) ? inGp(?)??? , where for a string ?? ( N ? ? ) ? we write ?? to denote the number of occurrences of terminal symbols in ?  . 
4 Entropy of PCFGs
In this section we introduce the notion of derivational entropy of a PCFG  , and discuss an algorithm for its computation . 
Let Gp = ( G , pG ) be a PCFG . For a nonterminal A of G , let us define the entropy of A as the entropy of the distribution pG on all rules of the form A ?  ?  , i . e . ,
H(A ) = EpGlog = ? ? pG(A ? ?) ? log.
The derivational entropy of Gp is defined as the expectation of the information of the complete derivations generated by Gp  , i . e . ,
Hd(Gp ) = EpGlog = ? dpG(d)?log .   ( 1 ) We now characterize derivational entropy using expected rule frequencies as 
Hd(Gp ) = ? dpG(d ) ? log = ? dpG(d)?log ?
A ? ?() f(A ? ?; d ) = ? dpG(d ) ? ?
A ? ? f(A ? ?; d ) ? log = ?
A ? ? log ? ? dpG(d ) ? f(A ? ?; d ) = ?
A ? ? log ? EpGf(A ? ?; d ) = ?
A ? ? log ? outGp(A ) ? pG(A ? ?) ? inG p (?) = ?
A outGp(A ) ? ? ? pG(A ? ?) ? log ? inGp(?).
As already discussed , under the assumption that Gp is proper and consistent we have in Gp  ( ? ) = 1 for every ? . Thus we can write
Hd(Gp ) = ?
A outGp(A ) ? H(A ). (2)
The computation of outGp ( A ) was discussed in Section 3 , and also H(A ) can easily be calculated . 
Under the restrictive assumption that a
PCFG is proper and consistent , the characterization in ( 2 ) was already known from ( Grenan-der ,  1976 , Theorem 10 . 7, pp .  90?92) . The proof reported in that work is different from ours and uses a momentum matrix  ( Section 3 )  . Our characterization above is more general and uses simpler notation than the one in  ( Grenander ,  1976) . 
The sentential entropy , or entropy for short , of Gp is defined as the expectation of the information of the strings generated by Gp  , i . e . ,
H(Gp ) = EpGlog = ? wpG(w ) ? log , (3) assuming 0 ? log10 = 0 , for strings w not generated by Gp . It is not difficult to see that H ( Gp ) ? Hd ( Gp ) and equality holds if and only if G is unambiguous  ( Soule ,  1974 , Theorem 2 . 2) . 
As ambiguity of CFGs is undecidable , it follows that we cannot hope to obtain a closedform solution for H  ( Gp ) for which equality to ( 2 ) is decidable . We will return to this issue in Section 6 . 
5 Weighted intersection
In order to compute the crossentropy defined in the next section  , we need to derive a single probabilistic model that simultaneously accounts for both the computations of an underlying FA and the derivations of an underlying PCFG  . We start from a construction originally presented in  ( Bar-Hillel et al ,  1964) , that computes the intersection of a contextfree language and a regular language  . The input consists of a CFG G = (? , N , S , R ) and an FAM = (? , Q , q0 , Qf , T ); note that we assume , without loss of generality , that G and M share the same set of terminals ? . 
The output of the construction is CFG G ? = (? , N ? , S ? , R ?) , where N ? = Q?(??N ) ? Q?S ? , and R ? consists of the set of rules that is obtained as follows  . 
? For each s ? Q f , let S??(q0 , S , s ) be a rule of G ? . 
? For each rule A ? X1 ??? Xm of G and each sequence of states s0  ,   .   .   .   , sm of M , with m ? 0 , let ( s0 , A , sm ) ? ( s0 , X 1 , s1)???( sm?1 , Xm , sm ) be a rule of G ?; for m = 0 , G ? hasa rule(s0 , A , s0) ?  for each states 0 . 
? For each transitions a 7? t of M , let ( s , a , t ) ? a be a rule of G ? . 
Note that for each rule ( s0 , A , sm ) ? ( s0 , X 1 , s1)???( sm?1 , Xm , sm ) there is a unique rule A ? X1 ??? Xm from which it has been constructed by the above  . Similarly , each rule(s , a , t ) ? a uniquely identifies a transitions a 7? t . This means that if we take a complete derivation d ? in G ?  , we can extract a sequence h1 ( d ? ) of rules from G and a sequence h2 ( d ? ) of transitions from M , where h1 and h2 are string homomorphisms that we define pointwise as ?  h1  ( pi ? )  =  , if pi ? is S??(q0 , S , s ); h1(pi ?) = pi , if pi ? is ( s0 , A , sm ) ? ( s0 , X 1 , s1)???( sm?1 , Xm , sm ) and pi is ( A ? X1 ? ? ? X m ); h1(pi ?) =  , if pi ? is ( s , a , t ) ? a ; ? h2(pi ?) =  , if pi ? is S??(q0 , S , s ); h2(pi ?) = ? , if pi ? is ( s , a , t ) ? a and ? iss a 7? t ; h2(pi ?) =  , if pi ? is ( s0 , A , sm ) ? ( s0 , X 1 , s1)???( sm?1 , Xm , sm ) . 
We define h(d ?) = ( h1(d ?), h2(d ?)) . It can be easily shown that if S?d ? ? w and h ( d ? )  =  ( d , c ) , then for the same w we have Sd ? w and ( q0 , w)c`(s ,  ) , somes ? Qf . Conversely , if for some w , d and c we have Sd ? w and ( q0 , w)c`(s ,  ) , somes ? Qf , then there is precisely one derivation d ? such that h  ( d ? )  =  ( d , c ) and S?d??w . 
As noted before by ( Nederhof and Satta ,  2003) , this construction can be extended to apply to a PCFGG p =  ( G , pG ) and an FAM . The output is a PCFG G ? , p = ( G ? , pG ?) , where G ? is defined as above and pG ? is defined by : ? pG ?  ( S ? ? ( q0 , S , s )) = 1; ? pG?((s0 , A , sm ) ? ( s0 , X 1 , s1)???( sm?1 , Xm , sm )) = pG(A ? X1???Xm);?pG?((s , a , t ) ? a ) = 1 . 
Note that G ?, p is non-proper . More specifically , probabilities of rules with lefthand side S ? or ( s0 , A , sm ) might not sum to one . This is not a problem for the algorithms presented in this paper  , as we have never assumed properness for our PCFGs  . What is most important here is the following property of G ?  , p . If d ? , d and c are such that h(d ?) = ( d , c ) , then pG?(d ?) = pG(d ) . 
Let us now assume that M is deterministic.
( In fact , the weaker condition of M being unambiguous is sufficient for our purposes  , but unambiguity is not a very practical condition  . ) Given a string w and a transition s a7? t of M we define f ( sa7?t ; w ) as the frequency ( number of occurrences ) of s a 7? t in the unique computation of M , if it exists , that accepts w ; this frequency is 0 if w is not accepted by M . On the basis of the above construction of G ? , p and of Section 3 , we find
EpG f(s a 7? t ; y(d )) = ? dpG(d ) ? f(s a 7? t ; y(d )) = out G ? , p((s , a , t )) ? pG?((s , a , t ) ? a ) ? in G ? , p(a ) = out G ? , p((s , a , t ) )  ( 4 )   6 KullbackLeibler distance In this section we consider the KullbackLeibler distance between a PCFGs and a PFA  , and present a method for its optimization under certain assumptions  . Let Gp = ( G , pG ) be a consistent PCFG and let Mp = ( M , pM ) be a consistent PFA . We demand that M be deterministic ( or more generally , unambiguous ) . Let us first assume that L(G ) ? L(M) ; we will later drop this constraint . 
The crossentropy of Gp and Mp is defined as usual for probabilistic models  , viz . as the expectation under distribution pG of the information of the strings generated by M  , i . e . ,
H(GpMp ) = EpGlog = ? wpG(w ) ? log.
The KullbackLeibler distance of Gp and Mp is defined as 
D ( GpMp ) = EpGlog pG ( w ) pM ( w ) = ? wpG ( w ) ? log pG ( w ) pM ( w )   . 
Quantity D ( GpMp ) can also be expressed as the difference between the crossentropy of Gp and Mp and the entropy of Gp  , i . e . ,
D(GpMp ) = H(GpMp ) ? H(Gp ). (5)
Let G ? , p be the PCFG obtained by intersecting Gp with the nonprobabilistic FAM underlying Mp  , as in Section 5 . Using ( 4 ) the crossentropy of Gp and Mp can be expressed as
H ( GpMp ) = ? wpG ( w ) ? log = ? d pG ( d ) ? log = ? d pG ( d ) ? log?s a7? t ( a7? t )   ) f ( sa7?t ; y(d )) = ? dpG(d ) ? ? s a 7? tf(s a 7? t ; y ( d ) ) ? log a7? t ) = ? sa7?t log a 7? t ) ? ? d pG ( d ) ? f ( sa7?t ; y(d )) = ? s a 7? t log a 7? t ) ? EpG f(s a 7? t ; y(d )) = ? s a 7? t log a 7? t ) ? out G ? , p((s , a , t )) . 
We can combine the above with (5) to obtain
D(GpMp ) = ? sa7?toutG ? , p((s , a , t )) ? log a 7? t ) ? H(Gp) . 
The values of out G ? , p can be calculated easily , as discussed in Section 3 . Computation of H ( Gp ) in closed for misproblematic , as already pointed out in Section 4 . However , for many purposes computation of H ( Gp ) is not needed . 
For example , assume that the nonprobabilistic FAM underlying Mp is given  , and our goal is to measure the distance between Gp and Mp  , for different choices of pM . Then the choice that minimizes H ( GpMp ) determines the choice that minimizes D ( GpMp )  , irrespective of H(Gp ) . Formally , we can use the above characterization to compute p?M = argmax pM 
D(GpMp ) = argmax pM
H(GpMp).
When L(G ) ? L(M ) is nonempty , both
D(GpMp ) and H(GpMp ) are undefined , as their definition simply a division by pM ( w ) = 0 for w ? L ( G ) ? L ( M )  . In cases where the nonprobabilistic FAM is given  , and our goal is to compare the relative distances between Gp and Mp for different choices of pM  , it makes sense to ignore strings in L(G ) ? L(M ) , and define D(GpMp ) , H(GpMp ) and H(Gp ) on the domain L(G ) ? L(M) . Our equations above then still hold . Note that strings in L ( M ) ?L ( G ) can be ignored since they do not contribute nonzero values to D  ( GpMp ) and H ( GpMp )  . 
7 Conclusions
We have discussed the computation of the KL distance between PCFGs and deterministic PFAs  . We have argued that exact computation is difficult in general  , but for determining the relative qualities of different PFAs  , with respect to their closeness to an input PCFG , it suffices to compute the crossentropy . We have shown that the crossentropy between a PCFG and a deterministic PFA can be computed exactly  . 
These results can also be used for comparing a pair of PFAs  , one of which is deterministic . 
Generalization of PCFGs to probabilistic tree -adjoining grammars  ( PTAGs ) is also possible , by means of the intersection of a PTAG and a
PFA , along the lines of ( Lang , 1994).

Helpful comments from Zhiyi Chi are gratefully acknowledged  . The first author is supported by the PIONIER Project Algorithms for Linguistic Processing  , funded by NWO ( Dutch Organization for Scientific Research )  . The second author is partially supported by MIUR ( Italian Ministry of Education ) under project PRIN No . 


J . K . Baker .  1979 . Trainable grammars for speech recognition . In J . J . Wolf and D . H . 
Klatt , editors , Speech Communication Papers Presented at the 97th Meeting of the Acoustical Society of America , pages 547?550 . 
Y . Bar-Hillel , M . Perles , and E . Shamir .  1964 . 
On formal properties of simple phrase structure grammars  . In Y . Bar-Hillel , editor , Language and Information : Selected Essays on their Theory and Application  , chapter 9 , pages 116?150 . Addison-Wesley . 
T . L . Booth and R . A . Thompson .  1973 . Applying probabilistic measures to abstract languages  . IEEE Transactions on Computers,
C-22(5):442?450, May.
A . Corazza , R . De De Mori , R . Gretter , and
G . Satta .  1991 . Computation of probabilities for an island-driven parser  . IEEE Transactions on Pattern Analysis and Machine Intelligence  ,  13(9):936?950 . 
M . Falkhausen , H . Reininger , and D . Wolf.
1995 . Calculation of distance measures between Hidden Markov Models  . In Proceedings of Eurospeech ? 95 , pages 1487?1490 , Madrid . 
U . Grenander .  1976 . Lectures in Pattern Theory , Vol . I : Pattern Synthesis . Springer-

J . E . Hopcroft and J . D . Ullman .  1979 . Introduction to Automata Theory , Languages , and
Computation . Addison-Wesley.
S . E . Hutchins .  1972 . Moments of strings and derivation lengths of stochastic contextfreeg grammars  . Information Sciences , 4:179?191 . 
B . -H . Juang and L . R . Rabiner .  1985 . A probabilistic distance measure for hidden Markov models  . AT&T Technical Journal , 64(2):391?408 . 
D . Jurafsky , C . Wooters , G . Tajchman , J . Segal , A . Stolcke , E . Fosler , and N . Morgan . 
1994 . The Berkeley Restaurant Project . In Proceedings of the International Conference on Spoken Language Processing  ( ICSLP-94 )  , pages 2139?2142 , Yokohama , Japan . 
B . Lang .  1994 . Recognition can be harder than parsing . Computational Intelligence , 10(4):486?494 . 
K . Lari and S . J . Young .  1990 . The estimation of stochastic contextfree grammars using the Inside-Outside algorithm  . Computer Speech and Language , 4:35?56 . 
K . Lari and S . J . Young .  1991 . Applications of stochastic contextfree grammars using the Inside-Outside algorithm  . Computer Speech and Language , 5:237?257 . 
M . Mohri and M . -J . Nederhof .  2001 . Regular approximation of contextfree grammars through transformation  . In J . -C . Junqua and G . van Noord , editors , Robustness in Language and Speech Technology , pages 153?163 . 
Kluwer Academic Publishers.
M . Mohri .  1997 . Finitestate transducers in language and speech processing  . Computational Linguistics , 23(2):269?311 . 
M . -J . Nederhof and G . Satta .  2003 . Probabilistic parsing as intersection . In 8th International Workshop on Parsing Technologies , pages 137?148 , LORIA , Nancy , France , April . 
M . -J . Nederhof .  2000 . Practical experiments with regular approximation of contextfree languages  . Computational Linguistics , 26(1):17?44 . 
M . Rimon and J . Herz .  1991 . The recognition capacity of local syntactic constraints  . 
In Fifth Conference of the European Chapter of the Association for Computational Linguistics  , Proceedings of the Conference , pages 155?160 , Berlin , Germany , April . 
S . Soule .  1974 . Entropies of probabilistic grammars . Information and Control , 25:57?74 . 
A . Stolcke and J . Segal .  1994 . Precise N - gram probabilities from stochastic contextfree grammars  . In 32nd Annual Meeting of the Association for Computational Linguistics  , Proceedings of the Conference , pages 74?79 , Las Cruces , New Mexico , USA , June . 
M . Vihola , M . Harju , P . Salmela , J . Suon-tausta , and J . Savela .  2002 . Two dissimilarity measures for HMMs and their application in phoneme model clustering  . In ICASSP 2002, volume I , pages 933?936 . 
C . S . We therell .  1980 . Probabilistic languages : A review and some open questions  . Computing Surveys , 12(4):361?379, December . 
