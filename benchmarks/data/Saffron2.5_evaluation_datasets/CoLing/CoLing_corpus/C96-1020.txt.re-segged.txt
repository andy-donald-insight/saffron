Beyond Skeleton Parsing : Producing a Comprehensive 
Large-Scale General-English Treebank With Full
Grammatical Analysis
Ezra Black , Stephen Eubank
Hideki Kashioka , David Magerman *
AT\]R , Interpreting Telecommunications

22 Hikaridai , Seikacho
Sorakugun , Kyoto , Japan 619-02
Roger Garside , Geoffrey Leech
I ) epts of Computing
and l , inguistics
University of Lancaster,
Bailrigg , Lancaster LA1 4YT , UK
rgg ? comp , lanes , ac.uk
black,eubank,kashioka@atr . itl . co . jp G . LeechOcentl . lancs . ac . uk
1 Introduction
A treebank is a body of natural language text which has been grammatically annotated by hand  , in terms of some previously-established scheme of grammatical nalysis  . Treebanks have been used within the field of natural anguage processing as a source of training data for statistical partog speech taggers  ( Black et al , 1992; Brill , 1994; Merialdo , 1994; Weischedel et al , 1993) and for statistical parsers ( Black et al , 1993; Brill , 1993; aelinek et al , 1994; Magerman , 1995; Magerman and Marcus ,  1991) . 
In this article , we present he AT'R/Lancaster 7'reebauk of American English , a new resource t br natural language - , processing research , which has been prepared by Lancaster University ( UK ) 's Unit for Computer Research on the English Language  , according to specifications provided by ATR ( Japan ) 's Statistical Parsing Group . First we provide a " static " description , with ( a ) a discussion of the mode of selection and initial processing of text for inclusion in the treebank  , and ( b ) an explanation of the scheme of grammatical annotation we then apply to the text  . Sec . ond , we supply a " process " description of the treebank  , in which we detail the physical and computational mechanisms by which we have created it  . Finally , we lay out plans for the further development of this new treebank  . 
All of the features of the ATR/Lancaster Treebank that are described below represent a radical departure from extant largescale  ( Eyes and Leech , 1993; Garside and McEnery , 1993; Marcus et al , 1993) treebanks . We have chosen in this article to present our treebank in some detail  , rather than to compare and contrast it with other treebanks  . But the major differences between this and earlier treebanks can easily be grasped via a corn -* Currental filiation : Rena is sance Technologies Corp  . , 25 East Loop Road , Suite 211 , Stony Brook , NY 11776 USA ; Consultant , ATR Interpreting Telecommunications Laboratories ,   3-12/94 parison of the descriptions below with those of the sources just  , cited . 
2 General Description of the
Treebank 2.1 Document Selection and

The ATR/Lancaster Treebank consists of approximately  730  , 0 00 words of grammatically-analyzed text divided into roughly  950 documents ranging in length ffmn about 30 to about 3600 words . 
The idea in forming the selection of documents for inclusion in this new treebank was to pack into it the maximum degree of document variation along many different scales --- document length  , subject area , style , point of view , etc . -but without establishing a single , predetermined classification of the included documents J Differing purposes for which the treebank might be utilized may favor differing groupings or classifications of its component documents  . Overall . , the rationale for seeking to take as broad as possible a sample of current standard American English  , is to support the parsing and tagging of unconstrained American English text by providing a training corpus which includes documents fairly similar to almost any input which might arise  . 
Documents were obtained from three sources : the Internet  ; optically-scanned hard copy " occasional " documents  ( restaurant take out menus ; flm draising letters ; utility bills ) ; and purchase from commercial or academic vendors . To illustrate the diverse nature of the documents included in this treebank  , we list , in Table 1 , titles of nine typical documents . 
In general , and as one might expect , the documents we have used were written in the early to mid  1990s   , in the United States , in " Standard " American English . However , there are fairly many 1 as was done , by contrast , in the Brown Corpus ( Kucer ~ t and Francis ,  1967) . 

Empire Szechuan Flier ( Chinese take out food )
Catalog of Guitar Dealer
UN Chart , er : Chapters 15
Airplane Exit-Row Seating : Passenger Information Sheet 
Bicycles : How 31'<) Trackstand
Gow ~' rnment : US Goals at G7
Shoe Store Sale Flier
Hair Losst , ernedy Brochure
Cancer : Ewing's Sarcoma Patient Infbrmation ' Fable  1: Nine Typical Docnments From A'ft/I , an <: aster Teebank exceptions : documents written by Captain John Smith of Plymouth Plantation  ( 1600s )  , by Benjamin Franklin (1700s) , by Americans writing in periods throughouthe 1800s and 1900s   ; documents written in Australian , British , Canadian , and Indian English ; and docnments featuring a . 
range of dialects and region alw trieties of cur = rentAmerican English  . As mattering of such documents is included because within standard English  , these linguistic varieties are sometimes quoted or otherwise utilized  , and so they should be represented . 
As noted a bow = ' , each document within the trek-bank is classified along many different axes  , in order to support a large variety of different task specific groupings of the documents  . Each document is classifed according to tone , style , linguistic level , point of view , physical description of document , geographical background of author , etc . 
Sample values for these attributes are : " friendly "  , " dense " , " literary " , % echnical " , " how to guide " , and " American South " , respectively . To convey domain information , one or more Dewey Decimal System three digit classifiers are associated with each document  . For instance , for the cvo\['af>hys-iologist , Dewey 612 and 616 ( Medical Sciences : \] lumen Physiology ; Diseases ) were chosen . On a more mundane , " bookkeeping " level , values for text title , author , publication date , text source , etc . are recorded as well . 
An SGML like markup language is used to cap-lure a variety of organizational level facts about each document  , such as LIST structure ; TITLEs and CAPTIONs ; and even more recond it events such as POEM and IMAGE  . HIGltLl(? , II'\]'ing of words and phrases is recorded , along with the w~-riety of highlighting : italics  , boldface , large font , e~c . Spelling errors and , where essential , other typographical lapses , are scrupulously recorded and then corrected . 
Tokenization ( i . e . word splitting : Edward's--+Edward's ) and sentence spli~ting ( e . g . tie said , " Hithere . Long time no see . " ~( Sen-tence . l :) Be said , ( Sentence . 2:) " Hithere . ( Sen-tence . 3:) Long time no see . " ) are performed by hand according to predetermined policies  . Hence the treebank provides the resource of multifarious correct instances of word and sentences I > litting  . 
2 . 2 Scheme of Grammatical Annotation tl cret of or e , all existing large=scale treebanks have employed the gra  . nmnatical nalys is technique of skeleton pars in ( \]  ( Eyes and Leech , 1993; Garside and McEnery , 1993; Marcusetel . , 1993) , 2 in which only a partial , relatively sket <' hy , grammatical analysis of each sentence in the treebank is provided  , a In contrast , the AT\[g/Lancaster Tee-bank assigns to each of its sentences a full and  ( : omplete grammatical analysis with respectoa very detailed  , very comprehensive broadcoverage grammar of English  . Moreover , a very large , highly del ; ailed part of speech tagset is used to label each word of each sentence with its syntactica ~  , d semantic ategories . The result is an extremely specific and informative syntactic and semantic diagram of every sentence in the treebank  . 
This shift fi'om skeleton parsing based treebanks to a treebank providing flfll  , detailed grammatical analysis resolves a set of problems  , detailed in ( Black ,  1994) , involved in using skeleton parsing based treebanks as a means of initializing training statistics for probabilistic grammars  ( Blacketel . , 1993) . Briefly , the tirst of these problems , which applies even where the grammar being trained has been induced from the training treebank  ( Shermanel ; al . , 1990) , is thai ; the syntactic sketchiness of a skeleton ~ parsed treebank leads a statistical training algorithm to overcount  , in some circumstances , and in other cases to un-~The 1995 release Penn Treebank adds flmction M intormation to some nonterminals  ( Marcus et al ,  1994) , but with its rudimentary ( roughly 45 tag ) tagset , its nondetailed internal analysis of noun con > pounds and NPs more generally  , its lack of semantic categorization of words and phrases  , etc . , it arguably remains a skeleton parsed treebank , albeit an enriched one . 
a Aditfer cnt kind of partial parse-crucially , one generated automatically and not by hand -characterizes the " treebank " produced by processing the  200 million word Birmingham\[?'niversity ( UK ) Bank of-English text corpus with the dependency grammar-based ENGC Glfels in kiParser  ( Karlssonetel . , 11995) . 
108 der cotlnl , instances of rule firings it + train il , g data ( treel ) a , nk ) pars(s , and thus 1 , o in correcl , lyesti-niat crtth > probal ) ilitics . The second I > rol > leut is that where the gramniar being l  , rain o(l is more detailed syntactically ( , hail I , \[ lesl , :ehq . Oliparsing based trainiilg I , reelm . nk , the training corptts radi- ( : ally tltl ( 1Cq'l ) el '\[' or ll Sillil , scrucial job of speci\['yiilg correct parses For training i > url  ) osrs ( l + lack+1! ) gd )  . 
It ) addition to resolv higgramt na , rt , rahling prol ) len : ls , our Trocl ) a tikl-ir <- ivides at neat is o\['training non grmmnar based parsi  . , gt . Iroc (> dures ( Brill , 1993; Jelinekctal .   , t994; Mag ; erlnalt , 1995) at , new , higher l('v <' lsofgI'~Lttltll ;/ t , i (' al detail and ( ~ Olli ) l + Ch('qD-iiv(~licss . 
'l'r('e IH/,llkS(?lltCll('(~s;a . r(~\[-iarso(I in CI'IIISO\['i , \]1("/17'/~I??Lqlish(Trammar , whose charn (' lerist , icsw <' will bri < qlyd (> scribe . 
' Fhe (; Nmmlar'st ) a . rt of SF , <>(; c\]l < . aSset r('s(qJ > I ) le sllie17!) Lag (' , laws I , agsot(l<w < . l( . Il ) (~< lI > ylY (: II , ;1 , ( Eyes and I , cc <'\[ i , 19!); l ): bul with tiullpr -' . ) usmaj <) ra . , l<lnlinor di\[f'(!r<mc<!s . () nontajor dift'er<~nc(~, for inst , a . c < : , is < , hal , I , he ATII . lags ('<(: al>-lures the ( lifft~r <' , c('t > etwc'(me . g . < < wall < : ov < wi , f ~ wh ( ' r ( > +' ( ' ov ( ~ ril~+g " isal ( ~ xica \] izc < t , ,(> ul / cry<lifts in - inS , and %1 , <" cov('ringo\['alll-i('l . s ", wh <' r <'+'(' ovcr-i . g " is a verb all lOt lll . In (: lawspl +; ICl , iCe ~ I > ot . harc NNI ( sill gular conmlonn , :) u . ) . The A'I'Ii . gselinn ov at x~s the lag1 . y\[>cNV\'(; for verl ) a\] nouns . 
Anoth <: rn : tj or difl ' <' x <' tw <> is t . h < : ( ling\[(, . l ) us ( . <)\[" sul )(' atcgorizati(>n ', (" . g . VI)t+~I , () B , I for ( Io , ibl < . 
ol ) jecl , v(>rt ) s(t<mchBilll , at in , el (+) , Each v <+ r l > , tl(-illt\] , a(lj(~ctiv < , a . < ladver bit , the A'I'R tags ct includes as e . ianlic I : ~ b < '\], (' hos(' . . from 42 n<)uli/a<lj<:ctiv<ja<lv< . ' l ) <'; at . <> goricsan<l2+)verl)/vert)al(t;l+l . (+ gjI'(-il'i <> S , S <> tll('ov(+ , r \] ap('xist , ing b(~lwe('nt , hes('cat <' goryse/ , s . 
'' heses <> tu . ~l . nl . i (: (: al ; (> go+i (' s are in \[ . (~ . ( i('(fbr(*,'+9"%' la~+dardAmericanlCngli . sD"Icml , i . a ~\] do-m , ai ~ . Sani \]) l(~<; al , ( ~ gories in ( hid (':+' physical . a . l , t t : i b , m ? + ( t , o , ,t , s/adjectiv < ~ s/adv(~rbs ) , " ai=ter " ( verbs/vcrl~als ) , and " in tcrp(+rsonal . ; , ct . " (  , , out , s/a < lj < + cl:iv < ~ s/adv < , rl > s/v(+rhs/v < . .I;als ) . ' lh <> ywcrcd < weh-ip <' d by the A'I'R grau lmaria  , a , ,d the . 
prov<' . n and r(~li . ( . (I via < lay in < lay ottl lagging for six mouths at ATI  , by two huu , ul "+ lrccbankers " , l\[len\['or\['ourttlonl , hsat , I , nticast , erI ) y fivel , r (> el ) auk < ~ rs , with daily in t , eract , ions ; llllOligI , r < ~ el ) aukcrs , and I-iei:~w ~ enih < " l . r('el > ank ( q's and i ; h(,
ATRg ~ ralii Iitariali.
\[1' we ignore 1, heseltia all . ic F , orlion o\["A'I'I ~ lags , lliet/-tgsel , cont . ains 165 ( tifl'(q'entl . ags . In (' lu ( I big the S('liia . iil , iccai , egories iiithe tag . ', , i . \[ lere are rougllly 2700i , ags . Asisl , li <> (' as ( ~ in I\]le(ll ; twst . at~sct . , so (: ailed " ditl . <) Lags " (:; IllI ) C('r (> nl . <'( lI)ase(lOila\[ni <) sl . ; iliyl , ; I . .~ of t . h clagset , 17) rth <' l ) urll(-is ~' o1'lal > ellhigtiiul\[ . i wor < l ( , Xl ) l ' (> s sioils . \[" or il/Sl ; ill CO , " willo'the wisp " islal>clle<i ; is a , 4 wordsi \] lgtllar CO ililti Oilli C-ill D . ' l ' li is process <: ; tl\]ad<l < ' olisid cral > lcIiIlll-il-iers 0\[' Lags (  , oI , l ( ! ahoy (" to t , als . 
, <" Jelil , ell (; c~sillI , tie Tre <> lm . nkar(~l > ars(xlwil ; liresl-iecl , t , othe A7'I7 lgnglish(Tran+mar . The (' , rammar , a feature based context fl : cc phrases t . ructtJregrat mnar , is related to the IBM t : , nglish(h ' ammarasl > ul ) lished in ( Blac . kctal .   ,  1993) , but differ stu or c : l ' rolN , the IBM(h'atimlar than our l , agset doest ' roln the (' , laws tagsel ,  . For instance , the notion of " numntol\]iC " hasnoal>plicatioi ~ to the ATI  ( ~rallllllar ; l , hoATRG ratl\]l\[lal " has 67 features and I\] O0 rules , whereas the IBM ( ~ ram+marhad40\[>al , tn'esaud'750 rules , c : t+c . 
' l'hei > reciscly corrcc . tparse ( ; ~sprest+ecific d by a human " trecbanker " ) figures among the parses I-i roduced for any given sentence by tile A'I'R  ( ~ ralnlnar , roughly !) 0% of the time , \[' or l ; ext , o1' the unconsl , rained , wide open SO l'l , that , tim Tree-DaHkix <' on il > OSC d of . The job of the treebatt kersil . olocal . <' this exact ; l > arse ,\[' or each s(mt . <m(:e , and add it to tlp'l'recl > ar~k . 
\[" i ~ tlrel shows , wo Sall l\[: . \[(+ parsc(SOlil , ell C+c+s /' roll ll . heA'll't Treebank ( aud originally l'roma ( thit wse I , akeoul , fOOd\[li(>r ) . l ~ e cat l seil , is inf ' or-nial , ivet , ok now which of the 1   lO0 rtlles is used ntagivoulr (  , ono ( h '+ and sitice the particular " tlon-lernlinal < ' at  , egory " associated wilh any . lode of ll wl , ree is alwa . ysrcc overalfle , 4 nodos are labelled with ATIL ( ~ ratnnlarulenantesratl > t+t . ha , , as is ll ( ) l + etlSllal + with l l Oltl . ( ~ rlttillali HUll CS . 
3 Producing tile Treebank
Illibis I > artoft . \] learticle , we Liirllt ' rolll " what , " t , o " l , ) w ' , a , d discuss then lcchaiiistns by which the A ' 1' I/I , ail (' asler ' Fr <' etm . ul ~ was produced +3 . 1 The Software I lackbone GWBT oo I :
A Trecbanker's Workstation ( a \? B'l'oolisn Mol , if based X-Windows appli(:a-ti (> , t which allows the tree l > anker to in t . era:t with the A'I'Il % glish Gra . mmar in order to produce \[ , hellt Oslac cllrate t , reel: . alikill , heshorl . csLamotuH,o\['t . illl<' . 
Thei , reebauking process begins ill the Treel > ank I : , dh , or sol'te no F the tree l > anker's worksta , tion with a list o\[+ scnl , enccs lagged wii , hpat'l , ol'-st > eechcal . egorics . ' l'he1,1'~>N ) a . nkc'rsh;cl,saSCtll ; eltcc . \[' r Otllthelist , for proccs shig . Next , with t it < + cli < ' k o1"a bttl: . 
lot < , tho Tl'eNmnkl ' Rlitor graphically displays tit <> l > arse\[bt ' <> st+\[br  1  . he s < : lll , ellC (' ill ~1Il IC , /ISO-SCI \] SiI , iVCI'arse Tr <+ rwi , dow ( Figure 2) . Each node disi )\] ay <'( lt'el+r(+s(>ntsa const , ituel+lt , in l , h <+ parse\['()rest . 
A shaded cons < it < to < h : itidicates that there ar<:allernatiw + alialyses of'thai  , constii , uent , only <) it < ! ol + which is displayed , lay clicking t , licright+niottscl ) ul3 on Oll ; I shaded node , t . hc : l . r<~cbanl , : crand is playapOl ) Ul>nicnulist hlgthoall , el ' nat\]w + analy-s('s , atly of which <: at lb (> disl > laycdl > ys < tecl , hlgt , heal ) l > ropriat , eill ( Hillil . et U . (', lick higi , heh>f'tn to use I ) tll , l , Oll ( . Ill~-t COIIS l , iI , IICII , l\]Od CpOpStipaWill dowlisl , htgth of e a , t , llt'ewthlesforl , ha , l , const , it + llCnL . 
411, is contained in the rulet/;i , tneit , stir.
1 09 < S id="39"   count=8> < HIGH rendition="italic ">\[ start\[quo (  _  (  \[  sprpd23  \[  sprime2  \[  ibbar2   Jr2 Please_RRCONCESSIVE r2\]   ibbar2\]  \[  sc3  \[  v4 Mention_VVIVERBAL-ACT\[ nbar4 \[dlthis_DDldl\]\[nlacoup on_NNIDOCUMENT nla \]  nbar4\] \[falwhen_CSWHEN\[vlordering_VVGINTER-ACT vl \] fall  v4\]   sc3\]   sprime2\]   sprpd23\]   ) _ ) quo\]start\]</HIGH > </ S > < S id="48" count = S > < HIGH rendition="large ">\[ start\ [   sprpd22  \[  coord3  \[  cc3 \[ cclOI~_CCORccl\] cc3\]  \[  nbarl3  \[  d3 ONE_MCIWORD d3\] \[jlFREE_JJSTATUSjl\]\[ n4 \[ nlaFANTAIL_NNIANIMAL nla\]\[nla SHRIMPS_NNIFOODnla\]  n4\]   nbarl3\]   coord3\]   sprpd22\] start \] </ HIGH > </ S > Figure 1: Two ATR/Lancaster Treebank Sentences ( 8 words , italicized ; 5 words , large font ) from Chinese
Take-Out Food Flier
V ~ : ivbar2 ~ prl.olJ.I

A , , lsu BsT , , EI
Apos = vbarnum = twon_sem=substance number = V5 tense_aspect = pres vsem=s end vtype = main_verb vp_type = aux Figure  2: The GWBTool Treebanker's Workstation Parse Window display  , showing the parse forest for an example sentence . On the far right , the feature values of the VBAR2 constituent , indicating that the constituent is an auxiliary verb phrase  ( bar level 2 ) containing a present-tense vrb phrase with noun semantics SUBSTANCE and verb semantics SEND  . The fact that the number feature is variable ( NUMBER = V 5 ) indicates that the number of the verb phrase is not specified by the sentence  . The shaded nodes indicate where there are alternative parses  . 
ii 0
The Treebank Editor also displays the number of parses in the parse forest  . If the parse forest is unmanageably arge , the treebanker can partially bracket the sentence and  , again with the click of a button , see the parse forest containing only those parses which are consistent with the partial bracketing  ( i . e . which do not have any constituents which violate the constituent boundaries in the partial bracketing  )  . Note that the treebanker need not specify any labels in the partial bracketing  , only constituent boundaries . The process described above is repeated until the treebanker can narrow the parse forest down to a single correct parse  . Crucially , for experienced Lancaster treebankers , the number of such iterations is , by now , normally none or one . 
3.2 TwoStage Part-Of-Speech Tagging
Part-of-speech tags are assigned in a twostage process :  ( a ) one or more potential tags are assigned automatically using the Claws HMM tagger  ( ? )  ;   ( b ) the tags are corrected by a treebanker using a special-purpose X-windows-based editor  , Xanthippe . This displays a text segment and , t breach word contained therein , a ranked list of suggested tags . The analyst can choose among these tags or , by clicking on a panel of all possible tags , insert a tag not in the ranked list . 
The automatic tagger inserts only the syntactic part of the tag  . To insert the semantic part of the tag , Xanthip pepresents a panel representing all possible semantic continuations of the syntactic part of the tag selected  . 
' lbkenization , sentence-splitting , and speltchecking are carried out according to rule by the treebankers themselves  ( e e2 . 1 above ) . However , the Claws tagger performs basic and preliminary tokenization and sentence-splitting  , for optional correction using the Xanthippe ditor  . Xanthippe retains control at all times during the tag correction process  , for instance allowing the insertion only of tags valid according to the ATR  . Grammar . 
3.3 The Annotation Process
Initially a file consists of a header detailing the file name  , text title , author , etc . , and the text itself , which may be in a variety of formats ; it ; may (' ontain HTML markup , and files vary in the way in which , for example , emphasis is is represented . 
The first stage of processing is a scan of the text to establish its format and  , for large files , to delimit a sample to be annotated . 
The second stage is the insertion of SGML like markup  . As with the tagging I ) rocess , this is done by an automatic procedure with manual correction  , using microemacs with a special set of nlacros . 
Third , the tagging process described in section 3 . 2 is carried out . The tagged text is then extracted into a file for parsing via GWB Tool  ( See 3 . 1 . 1) . 
The final stage is merging the parsed and tagged text with all the annotation  ( SGML-like markup , header information ) for return to ATR . 
3.4 Staff Training ; Output Accuracy
Even though all Treebank parses are guaranteed to be acceptable to the ATR Grammar  , insuring consistency and accuracy of output has required considerable planning and effort  . Of all the parses output for a sentence being treebanked  , only a small subset are appropriate choices , given the sentence's meaning in the document in which it occurs  . The five Lancaster treebankers had to undergo extensive training over a long period  , to understand the manifold devices of the ATR Grammar expertly enough to make the requisite choices  . 
This training was affected in three ways : a week of class room training was followed by four months of daily email interaction between the treebankers and the creator of tile ATR Grammar  ; and once this training period ended , daily Lancaster/ATR email interaction continued , as well as constant consultation among the treebankers themselves  . 
A body of documentation adlore was developed and frequently referred to  , concerning how all semantic and certain syntactic aspects of the tagset  , as well as various gramma rules , are to be applied and interpreted . ( This material is organized via a menu system , and updated at least weekly . ) A searchable version of files annotated to date , and a list of past tagging decisions , ordered by word and by tag , are at the treebankers ' disposal . 
In addition totile constant dialogue between the treebankers and the ATR grammarian  , Lancaster output was sampled periodically at ATR , hand-corrected , and sent back to the treebankers . 
In this way , quality control , determination of output accuracy , and consistency control were handled conjointly via the twin methods of sample correction and constantreebanker/grammarian dialogue  . 
With regard both to accuracy and consistency of output analyses  , individual treebanker abilities clustered in a fortunate manner  . Scoring of thousands of words of sampled at a over time revealed that three of the five treebankers had parsing error rates  ( percentage of sentences parsed incorrectly ) of 7% ,  10% , and 14% respectively , while the other two treebankers ' error rates were  30% and 36% respectively . Tagging error rates ( percentage of all tags that were incorrect )  , similarly , were 2 . 3%, 1 . 7%, 4 . 0%, 7 . 3% and 3 . 6% . Expected parsing error rate worked out to 11 . 9% for the first three , but 32 . 0% for the other two treebankers ; while expected tagging error rates were 2 . 9% and 6 . 1% respectively . 55 Almost all t ~ gging errors were semantic . 

What is fortnnate about this clustering of a bil - it  , ies is that the lessable treebankers were also much less prolific than the others  , producing only 25% of the total treel ) ank . Therefore , we are provisionally excluding this 25% of the treebank ( about 180 , 000 words ) fi'om use fbr parser training , though we are experimeating with the use of the entire treebank  ( expected tagging error rate : 3 . 9%) for tagger training . Finally , parsing and tagging consistency among the first , three tree-bankers appears high . 
4 Conclusion
Within the next two years , we intend to produce Version 2 o\[' our Treebank , in which the 25% of the treebank that is currently suitable for t , rainiug taggers but not parsers , is fully corrected / ~ Over the next several years  , the A'\['R/Lancaster Treebank of American English will form the basis for the research of A'l'l's Statistical Parsing Group in statistical parsing  , part of speech tagging , and related fields . 
References
E . Black , F . Jelinek , J . Lai\['e , rty , 1, . Mercer , S . 
loukos .  1992 . l ) ecision tree models applied to the labelling of text with parts of speech  . In Proceedings , DARPA Speecha~d Natural Lan- . quagcWorkshop , Ardenllouse , Morgan Kaufman Publishers . 
E . Black , ILGa . rside , and G . l , eech , Editors . 
1993 . Statistically Dr'ir on Computer Grammars Ofl' ; nglish : The IBM/Lanca , sler Approach , l . odopi Editions . Amsterdam . 
E . I ~ lack . 199 d . An experiment in customizing the Lancaster Treebank  . In Oostdijk and de Ilaan , 1!)9/1, pages 159-168 . 
E . Brill .  1993 . Automatic grammar induclion and parsing fl'ee text : A fransf'ormation I  ) ased approach . In Proceedi'ngs , 31sl Annual Mceli'ogo Jlk <- Associalio ~ for ( /o'mpulalional Linguislics , Association for Computational Linguistics . 
E . Brill . 1994. Some Advances in
Trans R ) rmation Based Part of Speech Tagging.
In Proceedings of lkcTwelfll ~ National ConJ'cr -encco ~ Arlificial h~telligencc  , pages 722-727 , Seattle , Washington . American Association for
Artificial Intelligence.
E . Eyes and G . Leech .  1993 . Syntactic Annol . a-lion:I , inguistic Aspects of ( h : ammatical Tagging and Skeleton I ) arsing . (' hapter 3 of It lacket . al .  1993 . 
c'Scv(mtenths of ~ his 25% is already correct , so that lhetask involved is rel ) axsing 30% of '25% (  -7 . 5%) of the trceba . nk . 
IL Garside and A . McEnery .  1993 . Treebank-ing : The Compilation of ' a Corpus of Skeleton Parsed Sentences  . C ' hapter 2 of Blackel; . al . 

F . , lelinek , J . l , afferty , 1) . Magennan , R . Metcer , A . Ratnaparkhi , S . R . oukos .  1994 . Decision Tree Parsing using a Hidden Derivation Model  . In Proceedings , ARPA Workshop on Human Language Technology , pages 260-2(55 , 
Plainsboro , New Jersey , AIPA.
F . Karlsson , A . Voutilainen , J . Heikkila , amlA . Anttila .  1995 . Constraint Grammar : A Language Independent System for Parsing Unrestricted Text  . Mouton de Gruyter : Berlin and
New York.
11 . Kucera and W . N . Francis .  1967 . Compltla-lional Analysis of Presenl-Day Americanl ~%-qlish  . 13 row nU niversity Press . Providence , RI . 
I ) . M . Magerman and M . P . Marcus .  1991 . Pearl : A Probabilistic Chart Parser . In Proceed in qs , l ' Juropean ACL Conference , March 1991 , Berlin , 

l ) . M . Magerman .  1995 . Statistical Decision Tree Models for Parsing . In Proceedings ,   33rd A ~- nual Mecling of lhe Association for ' Compula -tional Linguistics  , pages 276283 ,  ( , % m h r i d g e , Massachusetts , Association for ( k ) m putational

M.1). Marcus , B . Santorini , and M.A.
Marcinkiewicz .  1993 . Building a Large Annotated ( : or pus of English : The Pe\]m Treebank . 
Computalional Ling , 6slics , 19.2:313-330.
M . Mar(ms , G . Kim , M.A . Marcinkiewicz , R.
Maclntyre , A . Bies , M . Ferguson , K . Katz , and B . Schasberger .  1994 . The Penn Treel ) ank : Annotating Predicate Argument Structure . Pro-ceed in qs , ARP Alluman Language " l > chnology Workshop , Morgan Kaufinann Publishers Inc . ,
San Francisco.
1~ . Merialdo .  1!)94 . Tagging English Text with a Probabilistic Model . Compulalional Li'ngu~s-ties , 20 . 2:155- 171 . 
N . () ostdi . jl ~ and P . della . an , l'3 ditors . 199 d . 
C or7ms Ilased l~cscarch Into Language : h~kou-our@ Yau Aarls  . Rodol ) il ~; ditions . Amsterdal // . 
I . A . Sharmau , F . , I cliuek , and 1 . Mercer .  1990 . 
Generating a (~ rammar for Statistical Training . In Proceed in/is , DARPA ?' peeckaud/v % t-ural Lan . quagcWorkshop , Hidden Valley , Pennsylvania . 
l . Weischedel , M . Meteer , 1. Schwartz , I,.
lamshaw , and J . I ) almuc('i .  1993 . Coping with Ambiguity and iJn known Words through Probabilistic Models  . (7 ompalational Liuguis-lies , 19 . 2:359-382 . 

