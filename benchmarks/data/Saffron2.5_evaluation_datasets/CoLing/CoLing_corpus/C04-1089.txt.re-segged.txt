Mining New Word Translations from Comparable Corpora 
Li Shao and Hwee Tou Ng
Department of Computer Science
National University of Singapore
3 Science Drive 2, Singapore 117543
shaoli , nght@comp.nus.edu.sg


New words such as names , technical terms , etcappear frequently . As such , the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations  . Comparable corpora such as news documents of the same period from different news agencies are readily available  . In this paper , we present a new approach to mining new word translations from comparable corpora  , by using context information to complement transliteration information  . We evaluated our approach on six months of Chinese and English Gigaword corpora  , with encouraging results . 
1. Introduction
New words such as person names , organization names , technical terms , etc . appear frequently . In order for a machine translation system to translate these new words correctly  , its bilingual lexicon needs to be constantly updated with new word translations  . 
Much research has been done on using parallel corpora to learn bilingual lexicons  ( Melamed , 1997; Moore ,  2003) . But parallel corpora a rescarce resources , especially for uncommon language pairs . Comparable corpora refer to texts that are not direct translation but are about the same topic  . For example , various news agencies report major world events in different languages  , and such news documents form a readily available source of comparable corpora  . Being more readily available , comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations  , although relatively less research has been done in the past on comparable corpora  . Previous research efforts on acquiring translations from comparable corpora include  ( Fung and Yee , 1998; Rapp , 1995; Rapp ,  1999) . 
When translating a word w , two sources of information can be used to determine its translation : the word w itself and the surrounding words in the neighborhood  ( i . e . , the context ) of w . Most previous research only considers one of the two sources of information  , but not both . For example , the work of ( Al-Onaizan and Knight , 2002a ; Al-Onaizan and Knight , 2002b ; Knight and Graehl , 1998) used the pronunciation of w in translation . On the other hand , the work of ( Cao and Li , 2002; Fung and Yee , 1998; Koehn and Knight , 2002; Rapp , 1995; Rapp ,  1999 ) used the context of w to locate its translation in a second language  . 
In this paper , we propose a new approach for the task of mining new word translations from comparable corpora  , by combining both context and transliteration information  . Since both sources of information are complementary  , the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone  . We fully implemented our method and tested it on ChineseEnglish comparable corpora  . We translated Chinese words into English . That is , Chinese is the source language and English is the target language  . We achieved encouraging results . 
While we have only tested our method on Chi-nese -English comparable corpora  , our method is general and applicable to other language pairs  . 
2. Our approach
The work of ( Fung and Yee , 1998; Rapp , 1995; Rapp ,  1999 ) noted that if an English word e is the translation of a Chinese word c  , then the contexts of the two words are similar . We could view this as a document retrieval problem  . The context ( i . e . , the surrounding words ) of c is viewed as a query . The context of each candidate translation ' e is viewed as a document  . Since the context of the correct translation e is similar to the context of c  , we are likely to retrieve the context of e when we use the context of c as the query and try to retrieve the most similar document  . We employ the language modeling approach ( Ng , 2000; Ponte and Croft , 1998) for this retrieval problem . More details are given in
Section 3.
On the other hand , when we only look at the word w itself , we can rely on the pronunciation of w to locate its translation  . We use a variant of the machine transliteration method proposed by  ( Knight and Graehl ,  1998) . More details are given in Section 4 . 
Each of the two individual methods provides a ranked list of candidate words  , associating with each candidate a score estimated by the particular method  . If a word e in English is indeed the translation of a word c in Chinese  , then we would expecte to be ranked very high in both lists in general  . Specifically , our combination method is as follows : we examine the top M words in both lists and find kee e  ,  . . . , , 21 that appear intop M positions in both lists . We then rank these wordskeee, . . . , ,   21 according to the average of their rank positions in the two lists  . 
The candidate e i that is ranked the highest according to the average rank is taken to be the correct translation and is output  . If no words appear within the top M positions in both lists  , then no translation is output . 
Since we are using comparable corpora , it is possible that the translation of a new word does not exist in the target corpus  . In particular , our experiment was conducted on comparable corpora that are not very closely related and as such  , most of the Chinese words have no translations in the English target corpus  . 
3. Translation by context
In a typical information retrieval ( IR ) problem , a query is given and a ranked list of documents most relevant to the query is returned from a document collection  . 
For our task , the query is ) ( cC , the context ( i . e . , the surrounding words ) of a Chinese word c . Each ) ( eC , the context of an English word e , is considered as a document in IR . If an English word e is the translation of a Chinese word c  , they will have similar contexts . So we use the query ) ( cC to retrieve a document ) ( * eC that best matches the query . The English word * e corresponding to that document  ) ( * eC is the translation of c . 
Within IR , there is a new approach to document retrieval called the language modeling approach  ( Ponte & Croft ,  98) . In this approach , a language model is derived from each document D . Then the probability of generating the query Q according to that language model  , )( DQP , is estimated . The document with the highest ) ( DQP is the one that best matches the query . 
The language modeling approach to IR has been shown to give superior retrieval performance  ( Ponte & Croft , 98; Ng ,  2000) , compared with traditional vector space model , and we adopt this approach in our current work . 
To estimate ) ( DQP , we use the approach of ( Ng ,  2000) . We view the document D as a multinomial distribution of terms and assume that query Q is generated by this model : ?? = tctttDtPnDQP c  ) ( ! ! ) ( where t is a term in the corpus , tc is the number of times term t occurs in the query Q  , ? = ttcn is the total number of terms in query
Q .
For ranking purpose , the first fraction ? t tcn !/! can be omitted as this part depends on the query only and thus is the same for all the documents  . 
In our translation problem ,   ) ( cC is viewed as the query and ) ( eC is viewed as a document . So our task is to compute ) ) ( ) ( ( eCcCP for each English word e and find the e that gives the highest  ) ) ( ) ( ( eCcCP , estimated as : ?? ) (  ) ( ) ) ) ( ( ( cCt tq cc c ceCTtP Term ct is a Chinese word .   ) ( ctq is the number of occurrences of ct in ) ( cC .   ) ) ( ( eCTc is the bag of Chinese words obtained by translating the English words in  ) ( eC , as determined by a bilingual dictionary . If an English word is ambiguous and has K translated Chinese words listed in the bilingual dictionary  , then each of the K translated Chinese words is counted as occurring  1/K times in ) ) ( ( eCTc for the purpose of probability estimation . 
We use backoff and linear interpolation for probability estimation :  ) ( ) 1 (   ) ) ) ( ( ( ) ) ) ( ( ( cml ccmlcc tP eCTtPeCTtP ??+ ?= ? ? ? ? = ) ) ( (  ) ) ( (  ) ) ( (  ) (  ) (  ) ) ) ( ( ( eCTt eCT ceCT ccml c c c td td eCTtP where ) (? mlP are the maximum likelihood estimates ,   ) ( ) ) ( ( ceCT td c is the number of occurrences of the term ct in  ) ) ( ( eCTc , and ) ( cml tP is estimated similarly by counting the occurrences of ct in the Chinese translation of the whole English corpus  . ? is set to 0 . 6 in our experiments . 
4. Translation by transliteration
For the transliteration model , we use a modified model of ( Knight and Graehl , 1998) and ( Al-
Onaizan and Knight , 2002b).
Knight and Graehl ( 1998 ) proposed a probabilistic model for machine transliteration  . In this model , a word in the target language ( i . e . , English in our task ) is written and pronounced . This pronunciation is converted to source language pronunciation and then to source language word  ( i . e . , Chinese in our task ) . Al-Onaizan and Knight ( 2002b ) suggested that pronunciation can be skipped and the target language letters can be mapped directly to source language letters  . 
Pinyin is the standard Romanization system of Chinese characters  . It is phonetic-based . For transliteration , we estimate ) ( ceP as follows : ?? ? = = = a i i a i a plP pinyinae Ppinyine PceP  ) (  )  , ( )( )( First , each Chinese character in a Chinese word c is converted topiny in form  . Then we sum over all the alignments that this pinyin form of c can map to an English word e  . For each possible alignment , we calculate the probability by taking the product of each mapping  . ip is the ith syllable of pinyin , ail is the English letter sequence that the ith pinyin syllable maps to in the particular alignment a  . 
Since most Chinese characters have only one pronunciation and hence one pinyin form  , we assume that Chinese character-to-pinyin mapping is one-to-one to simplify the problem  . We use the expectation maximization ( EM ) algorithm to generate mapping probabilities from pinyin syllables to English letter sequences  . To reduce the search space , we limit the number of English letters that each pinyin syllable can map to as  0  ,  1 , or 2 . Also we do not allow cross mappings . 
That is , if an English letter sequence 1e precedes another English letter sequence 2e in an English word , then the pinyin syllable mapped to 1e must precede the pinyin syllable mapped to 2e   . 
Our method differs from ( Knight and Graehl , 1998) and ( Al-Onaizan and Knight , 2002b ) in that our method does not generate candidates but only estimates  ) ( ceP for candidates e appearing in the English corpus  . Another difference is that our method estimates ) ( ceP directly , instead of )( ecP and )( eP . 
5. Experiment 5.1 Resources
For the Chinese corpus , we used the Linguistic Data Consortium ( LDC ) Chinese Gigaword Corpus from Jan 1995 to Dec1995 . The corpus of the period Julto Dec 1995 was used to come up with new Chinese words c for translation into English  . The corpus of the period Janto Jun 1995 was just used to determine if a Chinese word c from Julto Dec  1995 was new , i . e . , not occurring from JantoJun 1995 . Chinese Gigaword corpus consists of news from two agencies : Xinhua News Agency and Central News Agency  . 
As for English corpus , we used the LDC English Gigaword Corpus from Jul to Dec  1995  . The English Gigaword corpus consists of news from four newswire services : Agence France Press English Service  , Associated Press Worldstream English Service , New York Times Newswire Service , and Xinhua News Agency English Service . To avoid accidentally using parallel texts , we did not use the texts of Xinhua News Agency
English Service.
The size of the English corpus from Julto Dec 1995 was about 730M bytes , and the size of the Chinese corpus from Julto Dec  1995 was about 120M bytes . 
We used a ChineseEnglish dictionary which contained about  10  , 0 00 entries for translating the words in the context . For the training of transliteration probability , we required a ChineseEnglish name list . We used a list of 1 , 5 80 Chi-nese-English name pairs as training data for the 
EM algorithm.
5.2 Preprocessing
Unlike English , Chinese text is composed of Chinese characters with no demarcation for words  . So we first segmented Chinese text with a Chinese word segmenter that was based on maximum entropy modeling  ( Ng and Low ,  2004) . 
We then divided the Chinese corpus from Julto Dec  1995 into 12 periods , each containing text from a half-month period . Then we determined the new Chinese words in each half-month per iod p  . By new Chinese words , we refer to those words that appeared in this period p but not from Janto Jun  1995 or any other periods that preceded p . Among all these new words , we selected those occurring at least 5 times . These words made up our test set . We call these words Chinese source words . They were the words that we were supposed to find translations from the 
English corpus.
For the English corpus , we performed sentence segmentation and converted each word to its morphological root form and to lower case  . 
We also divided the English corpus into 12 periods , each containing text from a half-month period . For each period , we selected those English words occurring at least  10 times and were not present in the 10  , 0 00-word ChineseEnglish dictionary we used and were not stopwords  . We considered these English words as potential translations of the Chinese source words  . We call them English translation candidate words . For a Chinese source word occurring within a half -month per iod p  , we looked for its English translation candidate words occurring in news documents in the same period p  . 
5.3 Translation candidates
The context ) ( cC of a Chinese word c was collected as follows : For each occurrence of c  , we set a window of size 50 characters centered at c . 
We discarded all the Chinese words in the context that were not in the dictionary we used  . The contexts of all occurrences of a word c were then concatenated together to form  ) ( cC . The context of an English translation candidate word e  , )( eC , was similarly collected . The window size of English context was 100 words . 
After all the counts were collected , we estimated ) ) ( ) ( ( eCcCP as described in Section 3 , for each pair of Chinese source word and English translation candidate word  . For each Chinese source word , we ranked all its English translation candidate words according to the estimated  ) ) ( ) ( ( eCcCP . 
For each Chinese source word c and an English translation candidate word e  , we also calculated the probability ) ( ceP ( as described in Section 4 )  , which was used to rank the English candidate words based on transliteration  . 
Finally , the English candidate word with the smallest average rank position and that appears within the top M positions of both ranked lists is the chosen English translation  ( as described in Section 2 )  . If no words appear within the top M positions in both ranked lists  , then no translation is output . 
Note that for many Chinese words , only one English word e appeared within the top M positions for both lists  . And among those cases where more than one English words appeared within the top M positions for both lists  , many were multiple translations of a Chinese word  . This happened for example when a Chinese word was a non-English person name  . The name could have multiple translations in English  . For example , ???? was a Russian name . Mirochina and Miroshina both appeared in top 10 positions of both lists . Both were correct . 
5.4 Evaluation
We evaluated our method on each of the 12 half-month periods . The results when we set M = 10 are shown in Table 1  . 

Period  #c  #e  #o  #Cor Prec.

Total 4499 19252 11471 1578.2
Table 1 . Accuracy of our system in each period ( M = 10 ) In Table 1 , period 1 is Jul01 ? Jul 15 , period 2 is Jul 16?Jul 31 ,  ? , period 12 is Dec16 ? Dec31 .  #c is the total number of new Chinese source words in the period  . #e is the total number of English translation candidates in the period  .  #o is the total number of output English translations  . 
 #Cor is the number of correct English translations output  . Prec . is the precision . The correctness of the English translations was manually checked  . 
Recall is somewhat difficult to estimate because we do not know whether the English translation of a Chinese word appears in the English part of the corpus  . We attempted to estimate recall by manually finding the English translations for all the Chinese source words for the two periods Dec  01 ? Dec 15 and Dec 16 ? Dec 31 in the English part of the corpus . During the whole December period , we only managed to find English translations which were present in the English side of the comparable corpora for  43 Chinese words . So we estimate that English translations are present in the English part of the corpus for  3624499  ) 205329 ( 43 = ?+ words in all 12 periods . And our program finds correct translations for 115 words . So we estimate that recall ( for M = 10 ) is approximately % 8 . 31362/115 =  . 
We also investigated the effect of varying M.
The results are shown in Table 2.

M Number of output
Precision (%)
Recall (%) 3037838 . 1 39 . 8 20 246 53 . 3 36 . 2 10 147 78 . 2 31 . 8 5 93 93 . 5 24 . 0 3 77 93 . 5 19 . 9 1 35 94 . 3 9 . 1 Table 2 . Precision and recall for different values of

The past research of ( Fung and Yee , 1998; Rapp , 1995; Rapp ,  1999 ) utilized context information alone and was evaluated on different corpora from ours  , so it is difficult to directly compare our current results with theirs  . Similarly , Al-Onaizan and Knight (2002a ; 2002b ) only made use of transliteration information alone and so was not directly comparable  . 
To investigate the effect of the two individual sources of information  ( context and transliteration )  , we checked how many translations could be found using only one source of information  ( i . e . , context alone or transliteration alone ) , on those Chinese words that have translations in the English part of the comparable corpus  . As mentioned earlier , for the month of Dec1995 , there are altogether 43 Chinese words that have their translations in the English part of the corpus  . 
This list of 43 words is shown in Table 3 .   8 of the 43 words are translated to English multiword phrases  ( denoted as ? phrase ? in Table 3 )  . Since our method currently only considers unigram English words  , we are not able to find translations for these words  . But it is not difficult to extend our method to handle this problem  . We can first use a named entity recognizer and noun phrase chunker to extract English names and noun phrases  . 
The translations of 6 of the 43 words are words in the dictionary ( denoted as ? comm . ? in Table 3 ) and 4 of the 43 words appear less than 10 times in the English part of the corpus ( denoted as ? in suff ? )  . Our method is not able to find these translations  . But this is due to search space pruning . If we are willing to spend more time on searching  , then in principle we can find these translations . 

ChineseEnglish Cont.


rank ? ? Bork 11 ? ? ? ? ? Dabwali 11 ? ? ? ? ? ?
K hasbulatov 1   1 ??? Nazal 1   1 ??? ? Ousland 1   1 ??? Douala 1   2 ??? Erbakan 1   2 ??? Yilmaz 1   120 ??? Bazelya 1 NA ?? crucible 1 NA ? ? ? Fatah 2   1 ???? Kardanov 2   1 ???? Mirochina 3   2 ???? Matteoli 4   2 ??? Tulkarm 8   7 ? ? ? Preval 8 NA ?? Soho 9   1 ???? Lamassoure 9   3 ???? Kaminski 10   1 ? ? Muallem 19   52 ???? Cherkassky 46   2 ??? Erbakan 49   2 ??? Laitinen 317   2 ??? Courier 328   21 ? ? leopard 1157 NA ???? Naumovin suff ??? Shangzhouin suff ??? Voeller in suff ???? Wassena arin suff ?? bald comm ?? base comm ??? Christmas comm ?? decrease comm ? ? pension comm ??? ? Saudi comm ??? ?-??? 

Herce gov in a phrase ?? ? Christmas
Card phrase ?? ? exhibition hall phrase ? ? hatch egg phrase ???? Kawasaki 
Steel Co.
phrase ???? Mount San phrase
Jose ??? Our Home Be
Russia phrase ? ? Union
Election phrase
Table 3 . Rank of correct translation for period Dec 01 ? Dec 15 and Dec 16 ? Dec 31  . ? Cont . rank ? is the context rank , ? Trans . Rank ? is the transliteration rank . 
? NA ? means the word cannot be transliterated . ? in suff ? means the correct translation appears less than  10 times in the English part of the comparable corpus  . 
? comm ? means the correct translation is a word appearing in the dictionary we used or is a stop word  . 
? phrase ? means the correct translation contains multiple English words  . 

As shown in Table 3 , using just context information alone , 10 Chinese words ( the first 10 ) have their correct English translations at rank one position  . And using just transliteration information alone ,   9 Chinese words have their correct English translations at rank one position  . 
On the other hand , using our method of combining both sources of information and setting M = ?  , 19 Chinese words ( i . e . , the first 22 Chinese words in Table 3 except ??? , ?? , ??? ) have their correct English translations at rank one position  . If M = 10, 15 Chinese words ( i . e . , the first 19 Chinese words in Table 3 except ??? , ??? , ?? , ??? ) have their correct English translations at rank one position  . Hence , our method of using both sources of information outperforms using either information source alone  . 
6. Related work
As pointed out earlier , most previous research only considers either transliteration or context information in determining the translation of a source language word w  , but not both sources of information . For example , the work of ( Al-Onaizan and Knight , 2002a ; Al-Onaizan and Knight , 2002b ; Knight and Graehl ,  1998 ) used only the pronunciation or spelling of w in translation  . On the other hand , the work of ( Cao and Li , 2002; Fung and Yee , 1998; Rapp , 1995; Rapp ,  1999 ) used only the context of w to locate its translation in a second language  . In contrast , our current work attempts to combine both complementary sources of information  , yielding higher accuracy than using either source of information alone  . 
Koehn and Knight ( 2002 ) attempted to combine multiple clues , including similar context and spelling . But their similar spelling clue uses the longest common subsequence ratio and works only for cognates  ( words with a very similar spelling )  . 
The work that is most similar to ours is the recent research of  ( Huang et al ,  2004) . They attempted to improve named entity translation by combining phonetic and semantic information  . 
Their contextual semantic similarity model is different from our language modeling approach to measuring context similarity  . It also made use of part-of-speech tag information  , whereas our method is simpler and does not require part-of-speech tagging  . They combined the two sources of information by weighting the two individual scores  , whereas we made use of the average rank for combination  . 
7. Conclusion
In this paper , we proposed a new method to mine new word translations from comparable corpora  , by combining context and transliteration information  , which are complementary sources of information . We evaluated our approach on six months of Chinese and English Gigaword corpora  , with encouraging results . 

We thank Jia Li for implementing the EM algorithm to train transliteration probabilities  . This research is partially supported by a research grant  R252-000-125-112 from National University of
Singapore Academic Research Fund.

Y . Al-Onaizan and K . Knight . 2002a . Translating named entities using monolingual and bilingual resources  . In Proc . of ACL . 
Y . Al-Onaizan and K . Knight . 2002b . Machine transliteration of names in Arabic text . In Proc . of the ACL Workshop on Computational Approaches to 
Semitic Languages.
Y . Cao and H . Li .  2002 . Base noun phrase translation using web data and the EM algorithm  . In Proc . of

P . Fung and L . Y . Yee .  1998 . An IR approach for translating new words from nonparallel  , comparable texts . In Proc . of COLINGACL . 
F . Huang , S . Vogel and A . Waibel .  2004 . Improving named entity translation combining phonetic and semantic similarities  . In Proc . of HLTNAACL . 
K . Knight and J . Graehl .  1998 . Machine transliteration . Computational Linguistics , 24(4):599-612 . 
P . Koehn and K . Knight .  2002 . Learning a translation lexicon from monolingual corpora  . In Proc . of the ACL Workshop on Unsupervised Lexical Acquisition  . 
I . D . Melamed .  1997 . Automatic discovery of noncompositional compounds in parallel data  . In Proc . 
of EMNLP.
R . C . Moore .  2003 . Learning translations of named-entity phrases from parallel corpora  . In Proc . of

H . T . Ng and J . K . Low .  2004 . Chinese part-of-speech tagging : one-at-a-time or all-at-once ? word-based or character-based ? To appear in Proc of EMNLP  . 
K . Ng .  2000 . A maximum likelihood ratio information retrieval model  . In Proc . of TREC8 . 
J . M . Ponte and W . B . Croft .  1998 . A language modeling approach to information retrieval  . In Proc . of

R . Rapp .  1995 . Identifying word translations in nonparallel texts  . In Proc . of ACL ( student session ) . 
R . Rapp .  1999 . Automatic identification of word translations from unrelated English and German corpora  . In Proc . of ACL . 
