A Self-Learning Universal Concept Spotter
Tomek Strzalkowski and Jin Wang
(\]ECort ) or ~*, Resear (: h and Dev (', lopment
P.O . Box 8
Schealect , a ( ly , NY 12301

strzalkowski , wangj@crd.ge.com
Abstract
We describe the Universal Spotter , a
system for identifying in-text references to entities of an arbitrary  , user-sl ) ecitied type , such its people , organizations , equipment , products , materials , etc . 
Starting with some initial seed examples , and a training texteort ms , I ; he system generates rules that will find fllrther concepts of the stone type  . The initials e , ed information is t ) rovided by the user in the form of a typical lexical context in which the enl  , ities to be spotted occur , e . g . , " the name ends with Co . " , or % otheright of produced or made " , and so forth , or by simt ) lysupplying examples of the concept itself , e . g . , Ford Tau'r'as , gasturbine , Bi 9 Mac . In addition , negative exaln plescant ) esupplied , if known . 
Given a suf\[ieiently arge training corpus , an unsupervise ( t learning process is initiated in which the system will:  ( 1 ) tindi ilstanees of the sought-after concept using the seed-eoll text infor I nation while maxiinizing recall and precision  ; (2) find , ~dditional contexts in which these entities occur  ; and ( 3 ) expand the initial seed-context with selected new com  ; extst ; of indevenlll Ore entities . Preliminary results of creating spotters for organizations and products are discussed  . 
1 In t roduct ion hlentifying concepts in natural language text is an important intbrmation extraction task  . Depending upon the current information needs one may be interested in finding all references to people  , locations , dates , organizations , companies , products , equipment , and so on . These concepts , along with their classification , can be used to index any given text for search or categorization purposes  , to generate suimnaries , or topopulate database records . However , automating the process of concept identification in untbrmatted text has not been an easy task  . Various single-Imr poses potters have been developed for specific types of conce  . pts , including people mm~es , com'-pa . nyn & ines , location names , dates , etc . ) lit ; those were usually either handcrafted for particular applications or domains  , or were heavily relying on a priori lexical clues , such as keywords ( e . g . , ' Co . '), case ( e . g . , ' John K . Big '), predicatable format ; ( e . g . , 123 Maple Street ), or a combination of thereof . This makes treat , ion and extension of stlehs potters an arduous mamml job  . Other , lesss ; tlient entities , such as products , equipnmilt , foodstuff ' , or generic refcrenc . es of any kind ( e . g . , ' a , lapanese automaker ' ) could only be i ( lenti-fled if a sut\[iciently detailed domain model was available  . Domain-model driven extraction wits used in ARPA -sponsored Message Understanding  Colltc1'eilc  ( ! s ( MUC )  ; a detailed overview of current research can be found in the procecdil ~ gs  ot7   MUC-5   ( nmcS , 1993) and the recently concluded MUC6 , as well as Tipster Project meetings , or ARPA's Human Language q > chnology workshops ( tipsterl ,  1993) , ( hltw ,  1994) . 
We take a somewh~t different approach to identify various types of text entities  , both generic and specific , without a ( let , ailed underst , and ing of the text domain , and relying instead on a comlfination of shallow linguistic processing  ( to ident i ( y candidate lexical entities )  , statistical knowledge acquisition , unsupervised learning techniques , and t ) os-sibly broa ( 1  ( m fiversal but often shallow ) knowledge , sources , such as online dictionaries ( e . g . , WordNet , Comlex , () ALl ), etc . ) . Our method IllOVeSt ) eyt md the traditional names i ) otters and towards a universals potter where , the requirements on what to spot can be specified as input paraineters  , and a specific-purposes potter c . ouh t be generated automatically . In this paper , we describe a method of creating spotters for entities of a specified category given only initial seed examples  , and using an unsupervised learning t ) rocess to discover rules for finding more instances of the e once t  ) t . At this time we place no limit on what kind of things one may want to build as potter for  , al@lough our extmriments thus far concentrated on entities customarily re -" gas turbine assembly "  )  , tools ( e . g . , " adjustable wrench "), products ( e . g . , " canned soup " , " Arm & I lammer baking soda ") , orgmfizations ( e . g . , American Medical Association ), locations ( e . g . , Albany County Airport ), people ( e . g . , Bill Clinton ), and so on . We view the semantic categorization problem as a case of disambiguation  , where for each lexical entity considered ( words , phrases , Ngrams ) , a binary decision has to be made whether or not it is an instance of the semantic type we are interested in  . The problem of semantic tagging is thus reduced to the problem of partitioning the space of lexical entities into those that are used in the desired sense  , and those that are not . We should note here that it is acceptable for homonymentities to have different classification depending upon the context in which they are used  . Just as the word " bank " can be assigned if -ferent senses in different contexts  , so can " Boeing 777 jet " be once a product , and another time an equipment and not a product , depending upon the context . Other entities may be less context dependent ( e . g . , company nan'ms ) if their definitions are based on internal context  ( e . g . , " ends with Co . ") as opposed to external context ( e . g . , " followed by mauufactures ") , or if they lack negative contexts . 
The user provides the initial information ( seed ) about what kind of thing she wishes to identify in text  . This infortnation should be in a form of a typical lexical context in which tile entities to be spotted occur  , e . g . , " the name ends with Co . " , or " to the right of produced or made " , or " to the right of maker of ' , and so forth , or simply by listing or highlighting a number of examples in text  . 
In addition , negative examples can be given , if known , to eliminate certain ' obvious ' exceptions , e . g . , " not to the right of made foal ' , " not tooth-brushes " . Given a sufficiently large training corpus , an unsupervised learning process is initiated in which the system will:  ( 1 ) generate initial context rules from the seed examples  ;   ( 2 ) find further instances of tile sought-after concept using the initial context while maximizing recall and precision  ;   ( 3 ) find additional contexts in which these entities occur  ; and ( 4 ) expand the current context rules based on selected new contexts to find even more entities  . 
In the rest of tlle paper we discuss the specifies of our system  . We present and evaluate preliminary results of creating spotters for organizations and products  . 
2 What do you want to f ind : seed se lec t ion If we want to identify somethings in a stream of text  , we first need to learn how to distinguish them from other items  . For example , company names are usually capitalized and often end with ' Co  . ',' Corp . ', ' Inc . ' and so forth . Place names , such as cities , are nonmflly capitalized , sometimes are followed by a state abbreviation ( as in Albauy , NY ) , and may be preceded by locative prepositions ( e . g . , in , at , from , to) . Products may have no distinctive lexical appearance  , but they tend to be associated with verbs such as ' produce '  , ' manufacture ' , ' make ' , ' sell ' , etc . , which in turn may involve a company name . Other concepl ; s , such as equipment or materials , have R~'w if any ot ) vious associati ( ms with the surrounding text , and on ( ; may prefer just to iioint them out directly to the learning progra in  . There are texts , e . g . , technical manuals , where such specialized entities occur more often than elsewhere  , and it may be adwm-tagous to use these texts to derives potters  . 
The seed can be obtained either by handtagging some text or using a naives potter that has high precision but presumably low recall  . A naives potter may contain simple contextual rules such as those mentioned above  , e . g . , for organizations : a noun phrases ending with " Co  . " or " Inc . " ; for products : a noun phrase following " manufacturer of "  , " producer of " , or " retailer of " . When such naives potter is ditlicult to come by , one may resort to hand tagging . 
3 From seeds to spotters
The seed should ident it ~ y the sought-after entities with a high precision  ( thougil not ; necessarily 100%) , however its recall is assumed to be low , or else we would already have a goods potter . Our task is now to iucrease tile recall while maintaining  ( or ( ' . veil increase if possible ) the precision . 
We proceed by examining the lexical context in which tlle seed entities occur  . In the silnplest instance of this process we consider a contexto coil-sist of N words to the left of the seed and N words to the right of tile seed  , as well as the words ill the seed itself . Each piece of significant contextual evidence is then weighted against its distribution in the balance of the training corpus  . This in turn leads to selection of some contexts to serve as indicators of relevant entities  , in other words , they become the initial rules of the emerging spotter  . 
As an exami ) le , let's consider building as potter for company names  , starting with seeds as illustrated in the t bllowing fragments  ( with seed con-t , exts highlighted ): . . . HENRYKAU FMAN is president of Henry Kaufmau C ~ Co  . , a . . . Gabelli , chairman of Gabellil % nds Inc . ; Claude N . Rosenberg . . . is named president of
Slmndinaviska Enskilda Banken . . . become viee chairman of the state-owned electronics giant Thomson S  . A . . . . banking group , said the formal merger of
Sl~anska Banken into ... watermaker
Source Perrier S . A . , according to French stock . . . 
932 l taving " Co . "" htc . " to pick out " Henry Kaufmmn & Co . " r and " Gabelli IA mds Inc . " as seeds , we proceed to find new evidence in the training corlms  , using an unsul ) ervised lemrning process , mnd discover thmt " chmirman of " r and " t ) resid cnt of " rare very likely to precede , cOral ) any nalnes . We expand our initial set of rules , which t allows us to spot more COml ) anies : . . . ltENIYKAUFMAN is pres-ident of lh ; nryK aufm . an ~'4 Co . , a . . . Gabclli , chairman of Gabclli\[,mdsInc . ; Clmude N . \] osenl ) erg . . . is nmmed president of Sk and i'naviska Enskilda Bankcn  . . . be , comevice (' hairntan of lhe . state-o'wnc delectronics giant Thomson S . A .   .   .   . banldnggroul ) , said dw , for-real merger of Skansl ~ lanken into . . . 
winter in a ker Sotnce Perrier S . A . , according to French stock . . . 
This evidence discovery ( : an be relmated in m bool ; strmpl ) ing process l ) yret ) la ( : ing the initiml set ; of seeds with the new set ; of entities obtained froln the lmstitermtion . Int ~ embove examt ) le now have . "Slamdinaviskm FmskihlaBank(m " and " l ; hcstmte-owned electronics giant'\]'homsonS . A . " in nmddition to the initiml two names . Aflu'therit(w-ationma , ymdd " S . A . " rand " Bmnken " ; ol ; hc set of cont cx tuml rules , and so forth , in generml , ( ml ; ities can 1) e both added mnd de h ; ted from the evolvings ( ; to f examples , det ) ending on howux mctly the cv-id ( ; n (: e is weighted and combin ( ; d . The details are exl ) lained in the following sections . 
4 Text preparation
Inill () S ~ , (; asc , sl ; he text needs to t ) epreprocessed to is olm te1 ) a sic lexi ( : altok ( ' , ns(words , ml ) l)r (! viations , symbols , mnnol ; a ; ions , el ; (:) , and sl ; ru (: turmlunits ( sections , pmragrat ) hs , entences ) wh(meverapi ) li-cmt)le . In addition , t ) mrt-of-speechtmggingixusu-ml\]y desirm ble , in which case tim tagger mmy needl ; obe retrained on a text saml ) le1 ; ool ) l ; ilnize its performance ( Brill ,  1993) , ( Mercer , Schwartz & W ( ; isched cl ,  1)91) . Finmlly , a limited amount of lexicml normalization , or stemming , In a y bef ) er-lormed . 
The entities we rare looking for inay be exl ) ressed ) y certaint y t ) es of phrases . For example , people nmmesm ' eusually sequences of i ) rot ) er nouns , while equipment nmmes rare contained within noun phrmses  , e . g . , ' for wmrd looking in t>m'edradar' . We use 1 ) art of speech information to delinemt ethoses e ( lllelt (  ; es of lexicmll ; okenst ; hatar c likely to (: on-t ; mill ( Olll " ~ enl ; itics . \]~' l'()inl ; h(' , ll Oil we restrict tony further t ) rocessing on these sequences , and their contexts . 
These preparatory steps are desirable since they reduce the amount of noise through which the lemrning process needs to plow  , but theymrenot , strict lyst ) eaking , ne (: essary . Further experiments r are required to deterlnint ~ the level of preprocessing required I  ; o opt in fize the t ) er for lnanee of the \[ h fiversal S l ) otl ; er . 
5 Evidence items
The smn mnl ; i ( : categorization problem described here displmys ome pmr mll cls to the word sense disambigumd on problem where ho In onylll word sileed to be ms signed to one of several possible senses  , ( Yarowsky ,  19!)5) , ( Gale , Chm'ch & Yarowsky , lt ) 92) , ( Brown , Pictra , Pietra & Mercer ,  \]991) . 
'Fhcremrct woitn portant difl'erenc (', s , however . 
First , in the semantic cat , cgorizal ; ion l ) ro ) lem , t , here is al ; lemsl , one Olmn-ended catc , gory serving as mgrml )  1 ) rag for roll things nonrelevant ;  . This c , mt-e , gory Inay be hard , if not impossible , to describe by any finit (; set of rules . Second , unlike the word sense disambigumtion where the it ; eros1 ; obe clms si-tied arc known apriori , we attempt to acconq flish two things at the smnm time :  1  . discoverl ; he items Lobe (: on sidcred for c , mte-gorization ; 2 . a cl ; ually decide if an item 1 ) elongs to a given category , or falls outside of il ; . 
'\]' hcmt cgorization of a lexical token its belonging l  , omp ; ivellselnalltic , clmss is based llp Ottt , l ( ': information provided by the words occurriug in 1  , he token itself , ms well as the word sthm Ll ) re-cedem M follow it ; int(~xl ;  . Ill addition , i ) osition ml relal ; ionshil ) s among l ; hes ( ; words mmy be of im-portaalce . ~lbcapturel ; his informal ; ion , we define the notion of an e . 'videnccsetl bralexic mlunil;W,//V2 . . . IA < , , . (mphrase , or an Ngram ) its follows . 
Let .   .   .   . W . . ,  . . . . W . IW ~ .   .   . W ,, W,W+ . 2  .   .   . W , , . . . . be m string of subsequellt , tokens ( e . g . , words ) in text , such L hat W ~ W ~ . . . .I/Km is a unit of interesl , ( e . g . , a noun phrase ) r and n is the maximum size of the context window on either side of the unit  . Themt:-\[;ual window size , mmyl ) elimited by boundaries of strllcturm lmfit , ssm ; hits sentences or parm graphs . 
For each unit W1 Wu . . . l/g , ,~ , ase ~ of evidence , ilcms is colh ; cted as a set union of the following foursel ; s : 1 . Pmirsof ( word , position ) , where position p , s , f indicates whethex word is fount \[ ill the context preceding  ( p ) the central refit , following ( t ) it , , or whe;her il ; come , sflom I ; he centra . 1 unil ; itself is ) . El = ( w . . . . . p ) . . . . . . ( w . ~,, p ) ( w_,,p)\](w ~,,~), ( w,~,~) . . . . . . ( w ,,~, ~) ( Wtt , f ) , ( Wv2,f ) . . . . . . ( W4,~,f ) 2 . Pairs of ( bigram , position ) to capture words e , quence informmtion . E2 = ( W . . . . . W --( , ~- l )), p) . . . ((1/V . _:~, W__t ), p)((w , , w~), . ~)  . . . (( w ,, ~_ , , w , ,& ~)(( w + l , w + ~), f) . . . (( w +/ , ~_l ) , w + , o , f ) distance indicates how far word is located relative to  W1 or I/V , ~ . E a = 4 . 
( W . . . . p , n ) ( Wl , s , m ) ( W+I , f , 1) . . . ( W_l , p , 1) . . . ( w , ,, ~, 1) . . . ( W + , ~ , f , n ) 3-tuples ( hi-gram , position , distancc ) . E4 = (( W . . . . W ( n_D ) , p , n -1) .   .   . ( W2 , Wt ) , p , 1)\]((Wl , W2) , s , 7D ,  - 1)  . . . . . . (( W . . . . 1, Win ), s , 1) (( w+l , w +~), f , 0 . . . ((w+( , ~_ ~) , w + ~) , f , n-1) For example , ill the fl ' agment below , tile central phrase the door has the context window of size  2:   . . . boys kicked the door with rage . . . 
The set of evidence items generated for this fl'ag -inent  , i . e . , E1 UE2 UEaUE4 , contains the following elements : ( boys , p ) , ( kicked , p ) , ( the , s ) , ( door , s ) , ( with , f ) , ( rage , f ) , (( boys , kicked ) , p ) , (( the , door )) , s ) , (( with , ' age ) , f ) , ( boys , p ,  2) , ( ki&ed , p ,  1) , ( the , s ,  2) , ( door , s ,  1) , ( with , f ,  1) , ( rage , f ,  2) , (( boys , kicked ) , p ,  1) , (( the , door )) , s ,  1) , (( with ,   , ' age ) , f ,  1 ) Items in evidence sets are assigned significance weights  ( SW ) to indicate how strongly they point towards or against the hyphothesis that the central unit belongs to the semantic ategory of interest to the spotter  . The significance weights are acquired through corpus-based training  . 
6 Training
Evidence items for all candidate phrases in the training corpus  , for those selected by tile initial used-supplied seed  , as well as for those added by a training iteration  , are divided into two groups . 
Group A items are collected from the candidate phrases that are accepted by tiles potter  ; groupR items come from the candidate phrases that are rejected  . Note that A and 1% may contain repeated elements . 
For each evidence item t , its significance weight is computed as : f(t , A)-f(t , R ) f(t , A ) + f(t , R ) > sf(t , A)+y(t , R)
SW(t ) = 0 otherwise (~) where f(t , X ) is the fl'equency ofting roup X , and s is a constant used to filter the noise of very low frequency items  . 
As defined SW ( t ) takes values from-1 to 1 interval . SW ( t ) close to 1 . 0 means that t appears imarly exclusively with the candidates that have been accepted by tile spotter  , and thus provides the strongest positive evidence  . Conversely , SW(t ) close to -1 . 0 means that t is a strong negative indicator since it occurs nearly always with the rejected candidates  . SW(t ) close to 0 indicates neutral evidence , which is of little or no consequeuce to the spotter  . In general , we take SW ( t ) > e > 0 as a piece of positive evidence , and SW(t ) <- e as a piece of negative evidence , as provided by item t . Weights of evidence items within an evidence set are then combined to arrive at the compound context weight which is used to accept or reject candidate phrase  . 
At this time , we make no claim as to whether ( 1 ) is an optimal fornmla for cah : ulating evidence weights  . An alternative method we considered was to estimate certain conditional probabilities  , similarly to the formula used in ( Yarowsky , 1995): SW(t ) logP ( pCA/t ) f(t , A ) f(A ) = ~ log(2)
P ( pCR/t ) f(t , . R ) f(.l ~)
Here f ( A ) is ( an estimate of ) the probability that any given candidate phrase will be accepted by the spotter  , and f ( R ) is the probability that this phrase is rejected , i . e . , f(R ) = l-f(A) . Thus fin ' our experinmnts show that ( 1 ) produces better results than ( 2 )  . We continue investigating other weighting schemes as well  . 
7 Combining evidence weights to classify phrases In order to classify a candidate phrase  , all evidence items need to be collected from its coil-text and their SW weights are combined  . When the combined weight exceeds a threshold value  , the candidate is accepted and the i ) hrase becomes available for tagging by the spotter  . Otherwise , the (' andidate is reje(:te(l , although it may be reevaluated in a fllture iteration  . 
There are many ways to combine evidence weights . In our experiments we tried the following two options : x + y-xy if x > O and y > OxOy = x + y + xy if x < O and y < O  ( 3 ) x + y otherwise and ( Dy ~ ~ xifabs ( x ) > abs ( y ) y otherwise ( 4 ) kIn ( 3 )  , x ( I ) y is greater than either xory when both x and y are positive  , and it is less than both x and y for negative x and y  . In all cases , x0)y remains within \[-1 , +1\] interval . 
In (4) only the dominating evidence is considered . This formula is more noise resistant han (3) , but produces generally less recall . 
9348 Bootstrapping
The eviden : e , training and candidate sele:tion ( : y-cle form sal ) ootstrapIing trocess , as folh ) ws:
Procedure Bootstrapping
Collect seeds loop
Training l ) hase
Taggingt ) hase until Satisfied.
The bootstrapping t ) rocess allows fin ' colle:t-ing more and new ( ' . oni ; exl , uale viden : e and increase recall of the spotter . This is possible thanks to overall redundancy and rep  (  ; titiveness of information , particularly local : on text information , in large bodies of text . For exanqle , , in our three , -sectional contexl , ret ) resent , ation(tre(:eding , self , following ) , if one section contains strong evidence that the candidate t  ) hrase is select at le , eviden (: ef(mnd in others e , : tions will te consider e , d in tile next training cy : le , in order to sele (: t additional candidates . 
An imiortmlt consideration he , reistomain-lain all overall precision level throughout heell-tire process  . AM mugh , it ; may t ) e possible to rec ( verfl'om some miselassiti:ation errors ( e . g . , ( Ym'owsky , 1995)), (: a . reshou hl1 ) e taken when adjusting the process larameter so that  1  ) r ; eision does not deteriorate too rapidly . For insl ; ance , a (:-(; eltan ( ; e thresholds of evide , nce weights , initially set , higll , can be gradually decreased to allow more recall while keeping lrecision at a reasonable level  . 
In additioil , ( Yarowsky ,  1995) , ( Gale , Church & ; Yarowsky , 1992) pointou ; that there is as t , rent tenden (: y for words 1 ; O occur in ( Ile sense within any given dis:ourse ( " one sense p e , rdis:ourse ") . 
Th ( ; same seems to at ) plyto(:oncel ) tsele(:l ; ion , thai , is , In ultilleo ( : ( :m'ren ( : es of a ( : anlidate 1hrase within ~ t discurse should all 1e eithe\]'a ( :eel ) telorreje , (: t ; ( ; t\[)y the SlOl , te\]' . This in turn allows fr toot stratt ) ing pr ( cess to gather more contextual evideal : cmore quickly  , and thus to (: on wuge fastert ) rodu:ing , better results . 
9 Experiments and Results
We , used the Universal St ) ot ; ter to find organizations an1 products in a 7 MB ytes cortms consisting of al'ti ( : lesfl'omi ; ll (' , Wall Street Journal . l , ' irst , welre-t ) rocess ~ d thel ; ext with a larl ; -of-sl~ech tagger and denttied all simple noun groups to l  ) e used as : and date 1 hrases . 10 art Meswereset , aside and ha . ridl , agged as keyfore valual ; ion . 
Subsequently , seeds were construct , edma . nually in for ln of contextual rule , s . l~i ) rorgmfizatins , the senita . 1 rule shall a 98% i )\]' e ; ision and 4%) recall ; for products , the correslonding numbers were 97% and 42% . (4) is used to combine evidences . No lexi : on veriti:ation ( see later ) has been used in order l ; o show m ( )re clearly the behavior the learning nmthod itself  ( the lerformance can precision \] 0 ullllllIlllllll~l .   . 
"Seeds?1st loo1?4th loop 2040 . . . . . . . . . . "  . . . . . . . i ? " L .   .   .   .   .   .   .   .   .   .   .   .   .   .  , . , il Li , il Liii .   .   .   .   .   .   . ~ J ~ recall 60   80   100 Figure 1: Organizations potter results . 
be enhanced by lexicon verification ) . Also note that the quality of the , ~eeds affects the per'for-malice of the finals I ) otl ; er since they define what type of ; () I1(' , (;1)1; the system is supt ) osed to look for . The seeds that we used in our exlmrimenl ; sare quit ( ; simple , perhaps too simple , l letter seeds may be need e . d(possibly developed through all in-l ; era ?' tion with the user ) t ; o obtain str(mgr~stllts for some (: ~ l , cgories of conccil ; s . 
For orgmdzation tagging , the recall and precision results obtained after the tirst mid the follrth tootstrat  ) t ) in geyt ' . le are given in Figm'e1 . 
The poinl ; with the in a ximmn precision * recall in the ftmrth rllllis  950/  ) pre ( : ision and 90% recall . Examples of extracted organizations in -: lude : " l  , h , e State Statistical bts tit , ntc , lst , , , t , " , " We . rl , heim Sch , roder #4 Co " , " Skandi'naviska En-skilda Ha'nken " , " Statistics Canada " . 
The results for products tagging are given in Figure  2 on the next page . Examph~s of extracted products include : " the Mercury Grand Marquis and Ford Crown Victoria cars "  , "(_ ~ tevro-letPrizm " , " Pumpshoe " , ' MS/doe " . 
The efl'ect of bootstrapping is clearly visible in both charts : it improves the recall while  , main-raining or even i in proving the pre , (: ision . We may also nol ; ice that some misclassifications due to alliml ) ext'e , t : t seed ( e . g . , see the first dip in t ) re ( : ision ( )11 the 1tOdllt ; l ; schart )   ( : all illt ' aet t ) ecorrected in further tootstrapping loops . The generally lower performance levels for the product  ; spotl ; er is prol ) - ably due to the . fact t ; hat the ( ; on cel ) t of produ(;t , is harder to eirt ' . mnscril)e . 
10 Further options 10 . 1 Lex icon ver i f i ca t ion The itenlS identified in the second step can be further wflidated fl  ) r their broad semantic classification using online lexical  ( lat ~ J ) asc8 such as Corn-:"::7:!: . .
i :'! i ""' 1' l.i!?1st loop ? 4th loop '"" ~ l
J 20 40 60 80 100 recall
Figure 2: Product spotte results.
lexorLongman Dictionary , or Princeton's WordNet ; ( Miller , 1990) For example , " gasturbine " is an acceptable quipment /machinery name since ' turbine ' is listed as " machine " or " device " in WordNet hierarchy  . More complex validation may involve other words in the phrase  ( e . g . , " circuit breaker " ) or words in the immediate context . 
10.2 Conjunctions
The current program cannot deal with conjunction . The difficulty with conjunction is not with classification of the conjoined noun phrases  ( it is easier , as a matter of fact , because they carry more evidences ) but with identification of the phrase itself because of the structural ambiguities it typically involves that cannot be dealt with easily on lexical or even syntactic level  . 
11 Conclusions
In this paper we presented the Universal Spotter , a system that learns to spot in-text references to instances of a given semantic lass : people  , organizations , products , equipment , ools , to nmne just a few . A specific class spotter is created through an unsupervised learning process on a text corpus given only an initial nser-supplied seed : either a number of examples of the concept  , or a typical context in which they can be found . The experiment shows that this method indeed can produce useflfl spotters based on easy-to -construct seeds  . Tile results shown here are promising , can be further improved by using lexicon verification  . 
Different methods of computing SWs , combining SWs , and parameter adjust menting for the bootstrapping process need to be explored as we believe there is still room for improvement  . The method is being continuously refined as we gain more feedback from empirical tests across several different applications  . 
We believe that tile Universal Spotter can replace much of the need to create handcrafted concept spotters commonly used in text extraction operations  . In can also be applied to building other than the most commons potters such as those for people names  , place names , or company names . In fact , is can be used to create more-or-less on-demand spotters  , depending upon the applications and its subject domain  . In particular , we believe such spotters will be required to gain further advance in intelligentext indexing and retrieval applications  , text summarization , and database apI ) lications , e . g . , ( Harman , 1995), ( Strzalkowski , 1995) . 
Reference shltw .  1994 . Proceedings of the Human Language Technology Workshop  , Princeton . San Francisco , CA : Morgan Kaufman Publishers . 
nine5 .  1993 . Proceedings of 5th Message Understanding Conference , Baltimore . San Francisco,
CA : Morgan Kaufman Publishers.
tipsterl . 1993. Tipster Text Phase 1.:24 month
Conference , Fredericksburg , Virginia.
Brill , E .  1992 . A Simple Rulebased Part of Speech Tagger . Proceedings of 3rd Applied Natural Language Processing , San Francisco , 
CA : Morgan Kaufman Publishers.
Brown , P , S . Pietra , V . Pietra and R . Mercer.
1991 . Word Sense Disambiguation Using Statistical Methods  . Proceedings of the 29h Annual Meeting of the Association for Computational 
Linguistics , pp . 264-270.
Gale , W . , K . Church and D . Yarowsky .  1992 . A Method for l ) is ambiguating Word Senses in a Large Corpus . Computers and the Humanities , 26, pp .  415 439 . 
Harman , D .  1995 . Overview of the Third Text REtrieval Conference . Overview of the l'hird Text REtrieval Conference ( TREC3 )  , pp . 1-20 . 
Meteer , M ., R . Schwartz , and I . Weischedel.
1991 . Studies in Part of Speech Labeling . Proceedings of the ~th DARPA Speech and Natural Language Workshop  , Morgan-Kaufman , San
Mateo , CA . pp . 331-336.
Miller , G .  1990 . WordNet : An ( ) n-line Lexical Database . International Journal of Lexicography , 3, 4 . 
Strzalkowski , T .  1995 . Natural Language Information Retrieval . Information Processing and Management , vol . 31, no . 3, pp .  397-417 . 
Yarowsky , D .  1995 . Unsupervised Word Sense Disambiguation Rivaling Supervised Methods  . 
Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics  , pp . 


