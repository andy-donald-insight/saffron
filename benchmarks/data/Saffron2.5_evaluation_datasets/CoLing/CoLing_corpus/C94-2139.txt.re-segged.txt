Analysis of Japanese Compound Nouns
using Collocational Information
KOBAY ASI Yosi yuki TOKUNAGA Take no buTANAKAHo zunfi
Department of Computer Science
Tokyo Institute of Technology
yashi , take , tanakaO , t?tech.ac.jp
Abstract
Analyzing compound nouns is one of the crucial issues for natural language processing systems  , in particular for those systems that aim at a wide coverage of domaius  . In this paper , we propose amcthod to analyze structures of Japanese compound nouns by using both word collocations statistics and a thesaurus  . An experiment is conducted with 160 , 0 00 word collocations to analyze comlmund nouns of with an average length of  4  . 9 characters . The accuracy of this method is about 80% . 
1 Introduction
Analyzing compound nouns is one of the crucial issues for natural language processing systems  , in particular for those systems that aim at a wide coverage of domains  . Registering all compound nouns in a dictionary is an impractical att proach  , since we can create a new conll ) oundlloun by conl-bluing nouns . Therefore , a mechanism to analyze the structure of a con , pound noun front the individual nouns is necessary . 
In order to identify structures of a compound noun  , we must first find a set of words that compose the compound noun  . This task is trivial for language such as English  , where words are separated by spaces . The situation is worse , however , in Japanese where no spaces are placed betwem , words . The process to identify word boundaries is usually called segmentation  . In processing language such as Japanese , ambiguities in segmentation should be resolved at the same time as an n?lyzing structure  . I " or instance , thcJapanese compound noun "$) ~\] llJ ~ ; I ~"( n cw indirect tax) , produces t6 ( = 2 4 ) segcmentations possibilities for this case . ( By consulting a/lai ) an esedictionary , we would filter out some . ) In this case , we have two remaining possibilities : "50\[" ( new )  /~  ( type ) /lllJ ~' ~ ( indirect ) / t  ~ ( tax ) " and " ) ~#~  ( new ) /lll\]~ ) ~  ( indirect ) / :~ , ( tax ) . '' iW cnmst choose the correct segmentation , "~)?~' J .   ( new ) /llll ~ ( indirect )  /~  ( tax ) " and analyze structure . 
1Here "/" denotes ~ L bound ~ try of words , Segmentation of Jal ) anese is difficult only when using syntactic knowledge  . Therefore , we could not always expect a sequence of correctly segmented words as an input to structure analysis  . 
The information of structures is also expected to improve segmentation accuracy  . 
There are several researches that are attacking this problem  , l ) ' uzisaki et al applied the ItMM model to scg , nentatimt and probabilistic CFG to analyzing the structure of compound nouns  \[3\]  . 
The accuracy of their method is 73% in identifying correct structures of kanzi character sequences with average length is  4  . 2 characters . In their approach , word boundaries are identified throughtmrely statistical information  ( the IIMM model ) without regarding such linguistic knowledge , as dictionaries . Therefore , the HMM nrodel may suggest an improper character sequence as a word  . 
Purthermore , since nonterminal symbols of CFG are derived from a statistical analysis of word collocations  , their number tends to be large and so the muuber of CFG rules are also large  . They assumed COml ) ound nouns consist of only one character words and two character words  . It is questionable whether this method can be extended to handle cases that include nmre than two character words without lowering accuracy  . 
ht this palter , we protms camethod to analyze structures of Japanese compound nouns  1  ) y using word collocational information and a thesaurus  . 
The callocational information is acquired from a corpus of four kanzi character words  . The outline of procedures to acquire the collocational information is as follows : ? extract collocations of nouns from a corpus of four kanzi character words ? replace ach noun in the collocations with thesaurus categories  , to obtain the collocatkms of thesaurus categories ? count occurrence frequencies for each collocational pattern of thesaurus catcgorics For each possible structure of a compound noun  , the preference is calculated based on this colloo cational information and the structure with the highest score w in s  . 

Hindle and Rooth also used collocational information to solve ambiguities of pp-attachment in English  \[5\]  . Ambiguities arc resolved by comparing the strength of associativity between a preposition and a verb and the preposition and a nominal head  . The strength of iLssociativity is calculated on the basis of occurrence frequencies of word collocations in a corpus  . Besides the word collocations information , we also use semantic knowledge , nanlely ~ a thesaurus . 
TILe structure of this paper is as follows: Section  2 cx plains the knowlcdge for structure analysis of compound nouns and the procedures to acquire it from a corpus  , Section 3 describes the analysis algorithm , and Section 4 describes the experiments that arc conducted to evaluate the performance of our method  , and Section 5 summarizes the paper and discuss cs future rescarch directions  . 
2 Collocational for Analyzing



This section describes procedures to acquire collocational information for analyzing compound nouns from a corpus of four kanzi character words  . 
What wencc disoccurrence frequencies of all word collocations  . It is notr calistic , how cvcr , to collect all word collocations . We use collocations from thesaurus categories that are word abstractions  . 
The procedures consist of the following four steps  :   1  . collect four kanzi character words ( section 2 . 1) 2 . divide the above words in the middle to product pairs of two kanzi charact cr words  ; if one is not in the thesaurus , this four kanzichar-aeter word is discarded ( section 2 . 1) 3 . assign thesaurus cat cgorics to both two kanzi character word  ( section 2 . 2) 4 . count occurrence frequencies of category collocations  ( section 2 . 3) 2 . 1 Co l lec t ing Word Co l locat ions We use a corpus of four kanzi character words as the knowledge source of collocational information  . 
The reasons are its follows : ? In Japanese , kanzi character sequences longer than three are usually compound nouns  , This tendency is confirmed by comparing tile occurrence frequencies of kanzi character words in texts and those headwords in dictionaries  . 
We investigated the tendency by using sample texts from newspaper articles and encyclopedias  , and Bunrui Goi II you ( BGH for short) , which is a standard Japanese thesaurus . The sanlple texts include about 220,000 sentences . 
We found that three character words and longer represent  4% in the thesaurus , but 71% in the sample texts . Therefore a collection of four kanzi character words would bc used as a corpus of comi  ) ound nouns . 
Four kanzi character sequences are useful to extract binary relations of nouns  , because dividing a h ) urkanzi character sequence in the middle often gives correct segmentation  . 
Our preliminary investigation shows that the accuracy of the above heuristics is  96  %  ( 961/1000 )  . 
There is a fairly large corpus of four kanzi character words created by Pr of  . Tanaka Ya-suhito at Aiti Syukutoku college\[8\] . The corpus w ~ Ls manually created from newspaper articles and includes about  160  , 000 words . 
2.2 Assigning Thesaurus Categories
After collecting word collocations , wc must assign a thesaurus category to each word . This is a difficult task because some words are ms signcd multiple categories  . In such cases , we have several category collocations from a single word collocation  , some of which are incorrect . TILe choices arc as follows ;   ( 1 ) use word collocations with all words is assigned a single category  . 
(2 ) equally distribute frequency of word collca-tions to all possible category collocations  \[4\]   ( 3 ) calculate the probability of each category collocation and distribute frequency based on these probabilities  ; the probability of collocations are calculated by using method  ( 2 )  \[4\]  ( 4 ) determine the correct category collocation by using statistical methods other than word collocations  \[2  ,  10 ,  9 , 6\] Fortunately , there are few words that are its-signed multiple categories in BGH  . Therefore , we use method (1) . Word collocations containing words with multiple categories represent about  1/3 of the corpus . If we used other thesauruses , which assign multiple categories to more words , we would need to use method (2) ,  (3) , or (4) . 
2  . 3 Count ing Occur rence o f Cate - gory Co l locat ions After assigning the thesaurus categories to words  , wc count occurrence frequencies of category collocations as follows:  1  . collect word collocations , at this time we collect only patterns of word collocations  , but we do not care about occurrence frequencies of thc patterns produce category collocation patterns  3  . count the number of category collocation patterns Note : w c do not care about frequencies of word collocations prior to replacing words with thesaurus catcgorics  . 
3 Algorithm
The analysis consists of three steps : 1 . enumerate possible segmentations of an input coln pound nmm by consulting head words of the thesaurus  ( BGH )  2 . assign thesaurus categories to all words3 . calculate the preferences of every structure of thc compound noun according to tlm frcquen--tics of category collocations We assume that a structure of a compmmd noun caube expressed by a binary tree  . We also as-stone that the category of the right branch of a  ( sub ) tree represent stile category of tile ( sub ) tree itself . Tiffs assumption exsists because Japanese is a head-final language  , a modifier is on the h' . ft of its modifiee . With these assuml ) tions , a preference vahte of a structure is calculated by recurslve function p as follows:  1 if t is leaf v ( t ) = p ( l ( t )   )   , v(r(t )) , cv(cat(l(t )) , cat ( r ( t ) ) ) otherwise where flmction l and r return the left and right subtree of the tree respectively  , cat returns thesaurus categories of the argmnent . If the argument of cat is a tree , cat returns the category of tl , erightmost leaf of tile tree . Function cv returns an assoeiativity measure of two categories  , which is calculated from tile frequency of category collocation described in the previous ection  . We wmtld use two measures for cv : P ( catl , cat = ) returns the relative frequency of collation cat 1 , which appears on the lefts ideal , d cat 2 , wl fich appears on the right . 
Probal fility : cvl = l "( cat l , cat . ~)
Modified , nutual information statistics ( MIS):
P ( catx , cat2)cv2 = I'(catl,*) . 1'(*, cat 2) where * llleans ( to n~t care . 
MIS is similar to nmtual infromation used by Church to calculate semantic dependencies between words  \[1\]  . MIS is different from mutual information because MIS takes account of the position of the word  ( left/right )  . 
Let us consider an example '9~) iJ~III\]~:~;~".
Segmentation : two possibilities ,  (1)  , ,~ zF ~ ( new ) /Iz~t ~ ( indirect ) /~  ( tax ) " and ( 2 )  " ) ~ ) i " ( new ) p_f  ~ ( type ) /Hi \] ~ ( indirect ) / ; \[~( tax ) " remain as mentioned in section 1 . 
Category assignment : assigning tlms aurus categories provides  ( 1 ) '" W2~\[ll8\]/llIl~:\[311\]/~g\[137\]" and ( 2 ) ' "* , ~\[316\]/)J ~' J\[118:141:111\]/b11~'\[~\[31~1/ , ~ \[1371 . " A three-digit number stands for a thesaurus category  . A colon ":" separates multiple categories assigned to a word  . 
Preference calculation : For the case (1) I , the pos-sine structures are \[\[118 ,  3111 , 1371 and\[118 ,  \[311 ,  1371\] . 
We represent a tree with all stnotation . Fortile ease ! 2 ~ . there is an anfl ) iguity with tile category , , , ,"' \[118:141:111\] , We expand the ambiguity to 15 possible structures . Preferences are calculated for 17 cases . For example , the l ) reference of structure \[\[118 ,  311\] , 137\] is calculated as follows : p(\[\[118 , 311\] , 37\]) = p(\[118 , an D . p(,a7) . ~ o(311,137) = p(118) . p(311) , cv(118 , 311) , cv(311 , 137) = cv(118 , 311) , cv(311 , 137) 4 Experiments 4 . 1 Data and Analysis We extract kanzicl , a racter sequences from newspaper editorials and colunms and encyclopedia text  , which has no overlap with the training corpus : 954 compound nouns consisting of four kanzi characters  ,   710 compound nouns consisting of five kanzi characters  , and 786 coml ) ound nouns con-sistiug of six kanzi characters are mammlly extracted from the set of the above kanzi character sequences  . These three collections of compound nouns arc used for test data  . 
Wcusea thesaurus BGII , which is a standard machine rea ( lbleJat ) anese thesaurus . BGH is structured as a tree with six hler arcl fie all evels  . Table 1 shows the number of categories at all levels , in tlf is experiment , we use the categories at level 3 , If we have more eoml ) ound nolln8\[LS knowledge , we could use a liner hierarchy level . 
'\]_' able1 Themmd ) er of categories of 4 As mentioned in Section 2 , we create a set of collocations of thesaurus categories from a corpus of four kanzi character sequences and BGt\[  . 

We analyze the test data accorditlg to the procedures described in Section  3  . In segmentation , we use a heuristic " minimizing the number of content words " in order to prune the search space  . This heuristics is commonly used in the Japanese morphological analysis  . The correct structures of the test data manually created in advance  . 
4.2 Results and Discussions
Table 2 shows the result of the analysis for four , five , and sixkanzi character sequences . " oc " means that the correct answer was not obtained because the heuristics is segmentation filtered out from the correct segmentation  . The first row shows the percentage of cases where the correct answer is uniquely identified  , notie . The rows , denoted " ~ n " , shows the percentage of correct answers in the n th rank  .  4  , , , shows the percentage of correct answers ranked lower or equal to  4th place . 
Table 2 Accuracy of analysis\[%\] 4 kanzi 5 kanzi 6 kanzirank cvl cv2 cvl cv2 cvl cv2   1   96   96   63   59   48   53   1   97   96   71   68   54   68   2   99   99   91   91   89   93   , - ,  3 99 99 92 92 91 94 4 , -0 0 . 1 0 . i 2244 c~11:6652 Regardless , more than 90% of the correct answers are within the second rank  . The probabilistic measure cvl provides better accuracy than the mutual information measure  cv2 for five kanzi character compound nouns ~ but the result is reversed for six kanzi character compound nouns  . 
The results for four kanzi character words arc almost equal  . In order to judge which measure is better , we need further experiments with longer words . 
We could not obtain correct segmentation for 11 out of 954 cases for four kanzi character words ,   39 out of 710 cases for five kanzi character words , and 15 out of 787 ca~es for six kanzi character words . Therefore , the accuracy of segmentation candidates are 99% ( 943/954 )  ,  94 . 5% (671/710) and 98 . 1% (772/787) respectively . Segmentation failure is due to words missing from the dictionary and the heuristics we adopted  . As mentioned in Section 1 , it is difficult to correct segnlent a-tion by using only syntactic knowledge  . We used the heuristics to reduce ambiguities in segmentation  , but ambiguities may remain . In these experiments , there are 75 cases where ambiguities cannot be solved by the heuristics  . There are 11 such c ~ ses for four kanzi character words , 35 such cases for five kanzi character words , and 29 cases for six kanzi character words . For such cases , the correct segmentation can be uniquely identifed by applying the structure analysis for  7  ,  19 , and 17 eases , and the correct structure can be uniquely identified for  7  ,  10 , and 8 cases for all collections of test data by using CV l  . Oathe other hand ,  4 ,  18 , and 21 cases correctly segmented and 7 ,  11 , and 8 cases correctly analyzed their structures for all collections by using  cv2  . 
For a sequence of segmented words , there are several possible structures . Table 3 shows possible structures for four words sequence and their occurrence in all data collections  . Since a compound noun of our test data consists of four  , five , and six characters , there could be cases with a eol wpound noun consisting of four  , five , or six words . 
hit he current data collections , however , there are no such cases . 
In table 3 , we find significant deviation over occurrences of structures  . This deviation has strong correlation with the distance between modifiers and modifees  . The rightmost column ( labeled Y \] d ) shows sums of distances between modifiers and modifiee contained in the structure  . The distance is measured based on the number of words between a modifier and a modifiee  . For instance , the distance is one , if a modifier and a modifice arc inlmcdiately adjacent  . 
The correlation between the dist ; ancc and the occurrence of structures tells us that a modifier tends to modify a closer modifiee  . This tendency has been experimentally proven by Maruyama  \[7\]  . 
The tendency is expressed by the formula that follows : q  ( d )  =0 . 54 . d-I"s DG where d is the distance between two words and q  ( d ) is the probM ) ility when two words of said distance is d and a modification relation  . 
We redifined cv by taking this tendency as the fonm dath at follows : cv ' = cv  . q(d ) where cv ' is redifinc dcv . ' fable 2 shows the result by using new cvs . W cobtaiued significant improvement in 5 kauziaud 6 kanzi collection . 
Table 3 Table of possible structures \[ structure '"" 5 kanzi
WW l ~_~~___ . 0 _  . 268\] 269 l\[w l ,  \[-2 ,   , , s\]f 96 \[ , , , 1 ,  ~ , ~\] ,  \[~ , mH13 , , , 1 ,  %  3   6 kanzi 4o5 oI 4 L " I 4 kanz ~ 5 kanzl-~-~rank'cv ~ cv2 ' cvl: v2 evlc ~ J--i 96   97   "73-   79   662170 I , -~ 1 97 97 73 79 62 2 \[ 99 99 91 92
N 31.99 9993 9392 I_~~41 o . 10.1
We assumed that the thesaurus category of a tree bc represented by the category of its right branch subtree because Japanese is a head-final language  . However , when a right subtrce is a word such as suffixes , this assumption does not always hold true . Since our ultimate aim is to analyzesc--mantle structures of compound nouns  , then dealing with only the grammatical head is not enough  . 
We should take semantic heads into consideration.
In order to do so , however , we need knowledge to judge which subtrce represent stile semantic features of the tree  . This knowledge may be extracted from corpora and machine readable dictionaries  . 
A certain class of Japanese nouns ( called Sahenmeisi ) may behave like verbs . Actually , we can make verbs frmn these nouns by adding a special verb "- suru  . " These nouns have case frames just like ordinary verbs  . With compound nouns including such nouns , wccoukluse case frames and sc-lectional restrictions to an Myzc structures  . This process would beah nos the same as analyzing ordinary sentences  . 
5 Concluding Remarks
We propose a method to analyze Japanese compound nouns using collocational information and a thesaurus  . W c also describe a method to a ( :-quire the collocational information from a corpus of four kauzi character words  . The method to acquire collocational information is dependent on the Japanese character  , but the method to calculate preferences of structures si applicat  ) leto any lan-guag c with coml mund nouns . 
The experiments show that wh cn the method an Myzes compound nouns nouns with an average length  4  . 9, it produces an accuracy rate of about 83% . 
We are considering those future works that follow :   , incorl ) or at e other syntactic information , such as affixes knowledge ? use another semantic information as well as thesauruses  , such as sclcctional restriction ? apply this method to disambiguate other syntactic structures such as dependency relations 
References\[1\]K . W . CMrch , W . Gale P , Hanks , and D . Hindle . Using statistics in lexical an Mysis . In Lex-eal Acquisit in , chapter 6 . Lawrence Erlbaum
Associates , 1991.
\[2\] J . Cowie , J . A . Guthrie , and L . Guthrie . Lcxi-cal disambiguation using simulated amlealing . 
In COLINGp 310, 1992.
\[3\]T . Fujisaki , F . Jelinek , J . Cocke , and E . Black T . Nishino . A probabilistic parsing method for sentences disambiguation  . In Current Issues in Parsing Thchnology , chap-tcr10 . Kluwer Academic Publishers , 1991 . 
\[4\] R . Grishman and J . Sterling . Acquisition of sclcctional patterns . In COLINGp 658, 1992 . 
\[5\]D . Hindle and M . Rooth . Structual ambiguity and lexocal relations . In ACLp 229, 1991 . 
\[6\] M . E . Lesk . Automatic sense disambiguation using machine readable dictionaries : How to tell a pine cone from an ice cream cone  . In
ACM SIGDOC , 1986.
\[7\] H . Maruyama and S . Ogino . A statistical property of Japanese phrase-to -phrase modifications  . Mathematical Linguistics , Vol .  18,
No . 7, pp . 348-352, 1992.
\[8\]Y . Wallaka . Acquisition of knowledge for natural language ; the four kanji characters cqucn-tics ( in japanese )  . In National Conference of Information Processing Society of Japan  ,  1992 . 
\[9\] J . Veronis and N . M . Ide . Word sense disambiguation with very large neural networks extracted from machine readable dictionaries  . 
In COLINGp 389, 1990.
\[1O\]D . Yarowsky . Word-sense disamibiguation using stastistical models of roget's categories trained on large corpora  . In COLINGp , ~5~, 1992 . 

