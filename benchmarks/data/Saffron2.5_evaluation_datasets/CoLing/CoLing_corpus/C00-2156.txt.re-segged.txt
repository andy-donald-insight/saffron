Decision-Treebased Error Correction for Statistical Phrase Break 
Prediction in Korean *
Byeongchang Kim and Geunbae Lee
Del ) artment of Computer Science & Engineering
Pohang University of Science & Technology
Pohang , 790-784, South Korea
bckim , gblee((~postech.ac.kr
Abstract
tn this paper , we present a new 1 ) hrasebreak prediction architecture that integrates probabilistic apt  ) roach with decision-tree based error correction . The probabilistic method alone usually sufl'crs fronl performance degradation due to inherent data sparseness l  ) rolf lems and it only covers a limited range of contextual information  . Moreover , the module cannot utilize the selective morpheme tag and relative distance to the other phrase breaks  . The decision-tree based error correction was tightly integrated to overt : ohm these limitations  . 
The initially phrase break tagged morph cnm sequence is corrected with the error correcting decision tree which was induced by  C4  . 5 fl'om the correctly tagged corpus with the outtmt of the  15mbabilistic predictor . The decision tree-based posterror correction l ) rovided improved results even with the phrase break predictor that has l  ) o or initial performance . Moreover , tim system can be flexibly tamed to new corI ) uS without massive retraining . 
1 Introduction
During 1; 15 (; past thw years , there has l ) een a great deal of interest in high quality text -to-speech  ( TTS ) systelns ( van Santen et al ,  1997) . 
One of the essential prolf lenl Sill developing high quality TTS systems is to predict phrase breaks flor a texts  . Phrasebreaks are especially essential fbr subsequent processing in the TTS system such as grapheme-to-if llone me conversion and prosodic feature generation  . Moreover , gral ) helnes in the phrase , -break boundaries are not phonologically changed and should be i  ) ronommed as their original corresponding p honenles  . 
There have been two apln ' oaches to predict phrase breaks  ( Taylor and Black ,  1998) . The * This paper was supported by the University Research Program of the Ministry of Intbrmation & Communication in South Korea through the IITA  ( 1998 . 7-2000 . 6) . 
first : uses some sort of syntactic information to In : edict prosodic boundaries based on the fact that syntactic structure and prosodic structure are corelated  . This method needs a reliable parser and syntax-to -prosody  1nodule   . These modules are usnally implemented in rule -driven methods  , consequently , they are difficult to write , modi(y , maintain and adapt to new domains and languages . Ill addition , a greater use of syntactic information will require  , more con > lmtation for finding n more detailed syntactic parse  . Considering these shortcomings , the second approach uses some probabilistic methods on the crude POS sequence of the text :  , and this lnethod will be fln : ther developed in this paper  . 
However , t:he . probabilistic method alone usually sufl'ers front pertbrmance degradation due to inherent data sparseness problems  . 
So we adopted decision tree-based error COl'-re , c tion to overconm these training data limitations  . Decision tree induction iv 1 ; t5( ; most widely used \] calming reel ; hod . Espcci ~ fllyinlla ~ l ; -m : allanguage and speech processing , decision tree learning has been apt ) lied to many prob-h ,  . nls including stress acquisition fl ' om texts , gralflm metophonenm conversion and prosodic phrase  , modeling ( Daelemans et al , 1994) ( van Santen et al , 1997) ( Lee and Oh ,  1999) . 
In the next section , linguistic fb , atures of Korean relevant ophrase break prediction are described  . Section 3 presents the probabilistic phrase break prediction method and the tree-based error correction method  . Section 4 shows experimental results to demonstrate het > erfor-mam:e of the method and section  5 draws st ) meconclusions . 
2 Features of Korean
This section brMly explains the linguistic char -acterists of spoken Korean before describing the phrasebreak prediction  . 
1 ) A Korean word consists of more than one morpheme with clear cut morphenm boundaries  ( Korean is all agglutinative language )  . 
1051 2 ) Korean is a postpositional language with many kinds of noun-endings  , verb-endings , and prefinal verb-endings . These functional morphemes determine a noun's case roles  , a verb's tenses , modals , and modification relations be-tw cen words .  3 ) Korean is basically an SOV language but has relatively free word order compared to other rigid word-order languages such as English  , except br the constraints that the verb must appear in a sentence-final position  . However , in Korean , some word-order constraints actually do exist such that the auxiliary verbs representing modalities must follow the main verb  , and modifiers must be placed betbre the word ( called head ) they modify . 4) Phonological changes can occur in a morpheme , between morphemes in a word , and even between words in a phrase , but not between phrases . 
3 Hybrid Phrase Break Detection
Part-of speech ( POS ) tagging is a basic step to phrase break prediction  . POS tagging systems have to handle out of vocabulary  ( OOV ) words for an unlimited vocabulary TTS system . Figure 1 shows the architecture of our phrasebreak predict or integrated with the POS tagging system  . The POS tagging systemera-ploys generalized OOV word handling mechanisms in the morphological analysis and cascades statistical and rule-based approaches in the two-phase training architecture tbr POS disambiguation  . 
Morphological !; i
I analyz cr ~ M~plme . . . . . ~se~i ~': ~,: . / Protmbilist lcIrt gram ~ z ' ~* t $

Figure 1: Architecture of the hybrid phrasebreak prediction  . 
Tire probabilistic phrase break predictor segments the POS sequences into several phrases according to word trigram probabilities  . Ti reirdtial phrase break tagged morpheme sequence is corrected with the error correcting tree learned by the  C4  . 5 ( Quinlan , 1983) . 
Tire next two subsections will give detailed descriptions of the probabilistic phrase prediction and error correcting tree learning  . The hybrid POS tagging system will not l ) e explained in this paper , and the interested readers can see ( Cha et al , 1998) tbr further reference . 
3.1 Probabilistie Phrase Break
Detection 3 . 1 . 1 Probabi l l s t ie Models For phrase break prediction  , we develop tire word POS tag trigrmn model . Some experiments are performed on all the possible trigram sequences and ' word-tag word -tag break word-tag'sequence turns out to be the most fl ' uitful of any others  , which are the same results as the previou studies in English  ( Sanders ,  1995) . 
The probability of a phrase break bi appearing after the second word POS tag is given by 
P ( bilt ?, 2 ta ) = C(tlt2 bit3)
Ej=o , ~ , 2C ( ht2 bjt3 ) ' where C is a frequency count flmction and b0 , bl and b2 mean no break , minor break and major break , respectively . Even with a large number of training patterns it is very clear that there will be a number of word POS tag sequences that never occur or occur only once in the training corpus  . One solution to this data sparseness problem is to smooth the probabilities by using the bigram and unigram probabilities  , which adjusts the fl'equency counts of rare or non-occurring POS tag sequences  . We use the smoothed probabilities :
P(biltlt , 2ta ) = ) qC(trt2bit3)~j = o , l , 2C(t~t2bjt3)
C(t2bita)q-A2~j = 0 , 1 , 2C(t2bjt3)+C(t2bi )) t3j=_0 , 1 , 2C(2bj ) ' where ) ~1 , A2 and ) , a are three nommgative constants such that h Iq -   ~2 Jr- ~3  =  1  . In some experiments , we can get the weights ~1 , ~2 and
A3 as 0.2, 0.7 and 0.1, respectively.
3.1.2 Adjusting the POS Tag
Sequences of Words
Previous researchers of phrasebreak prediction used mainly content-flnmtion word rule  , where l ) ya phrase break is placed before every flmction word that follows a content word  ( Allen and Hmmicut , 1987) ( Taylor et al ,  1991) . The function , content ~ mdt ) lmctuation i the rule . 
However , Korean is a postpositional aggln-tinative language  . If the e ontent-t'unction word rule is to be adapted in Korean  , the rule nmst be changed so that a phrasebreak is placed before every content mort/henm that R  ) llows a fimction morl ) heme . Unfortunately this rule is very inet\[icient in Korean since it tends to create too many pauses  . In our works , only the POS tags of Nnction mort ) heroes are used be , cause the function morphelnes constrain the classes of precedent n : or pheanes and t  ) b\y important roles in syntactic relation . So , each word is represented by the I?OS tag of its fimction morpheme  . In the case of the word which has no function mort  ) heine , simplified POS tags of content mort ) henms are used . Then mnber of POS tags use , d in this rese , m ' ch is a 2 . 
3.2 Decision-Tree Based Error
Correction
Thet ) robabilistic phrase break prediction only covers a limited range of contextual infornm-tion  , i . e . two preceding words and one . following word . Moreove , r the module cannot utilize them or l ) heme tagse . lectively and relative distance to the other phrase breaks  . For this reason we designed error correcting tree to con > pensate for  ; tielimitations of the . probal ) ilistic phrasebreak prediction . However , designing error corre , cting rules with knowledgengineeringiste , dious and error-prone , , lTnstead , we , adopte , d decision tree learning ai ) proa ( : h to auton ~ atically learn the error correcting rules froln a correctly t :  , hrasebreak tagged e or lms . 
Most algorithms th : ~ thavet ) een develope , d for l milding decision trees employ a topdown , greedy search through the space of possible decision trees  ( Mitchell ,  1997) . The04 . 5 ( Quinlan ,  1983 ) is adequate to buiht a decision tree easily for successively dividing the regions of feature vector to minimize the prediction error  . It also uses intbrmation gain which lneasures how well a given attril  ) ute separates the training vectors according to their target classification in order to selec the most critical attrilmtes at each step while growing  ; tittree ( hence then mne is IG-~l'ree ) . Now , we utilize it for correcting the initially phrase break tagged POS tag sequences generated by probabilistic predictor  . 
However , w c invented novel way of using the decision tree a strml sibrmation-t  ) ased rule induction ( Brill ,  1992) . l ? igure ,   2 shows the tree learning architecture tbr phrase break error correction  . The initial phrase breaktagged POS tag sequences upport the ti  ; a ture vectors t brattributes which are used tbr decision making  . Because the ii ; at m'e vectors include phrase break sequences as well as POS tag sequences  , a learned decision tree can check ; lie morphenm tag selectively and utilize ; lie relative distanee to the other phrase breaks . The correctly phrase break tagged POS tag sequence support the classes into which the feature vectors are classified  . C4 . 5 lmilds a decision treefl'om the t ) airs which consist of the feature vectors and their classes  . 
.  .   .   . IProl , al , ilislic 11l'I , rasel , reakta~ged
POS tag sequene e .   .   .   .   . Correctly plmlse break tagged ~-~\]:: . . . . . . . .  . . . . . . POS tag sequence
ErrOr Correcting decision tree
Figure 2: Architecture of the error correcting decision tree learner  . 
4 Experimental Results 4.1 Corpus
The , experiments are t ) ertbrmed on a Korean news story database , , called MBCNF , WS\])I ~ , of spoken Korean directly recorded from broadcasting news  . The size of th (; database is now 6 , 111 sentences (75 , 647 words ) and itise on tin-nously growing .  '12 ) lm used in the phrasebreak prediction experiments  ,   ; tie database has been POS tagged and break-b ~ beled with major and minor phrase breaks  . 
4.2 Phrase Break Detection and Error
Correction
We , I ) eribrmed three experiment so show synergistic results of probabilistic method and tree-based error correction method  . First , only probabilistic method was used to predict phrase breaks  . % ' igrams , bigrams and unigrams for phrasebreak prediction we retrained fl : om the break-labeled an  ( 1 POS tagged 5 , 4 92 sentences of the MBCNEWSDB by adjusting the POS sequences of words as described insut  ) section 3 . 1 . 2 . The other 619 sentences are used to test the t ) ertbrnum ( : e of the probabilistic I ) hrasebreak predictor . In the second experiment , we made a decision tree , which can be used only to predict phrase breaks and cannot be used to Also the  619 sentences were used to test the performance of the decision tree-based phrase break predictor  . The size of feature vector ( the size of the window ) is w ~ ried fi'om 7 ( the POS tag of current word , preceding 3 words and following 3 words ) to 15 ( the POS tag of current word , preceding 7 words and following 7 words ) . 
The third experiment utilized a decision tree as posterror corrector as presented in this paper  . 
We trained trigrams , bigrams and unigrams using 60% of totM sentences , and learned the decision tree using 3 ( 1% of total sentences . For the other experiment ,   50% aim 40% of total sen-fences are used tbr probability training and tbr decision tree learning  , respectively . Timother 10% of total sentences were used to test as in the prevkm sext  ) eriments ( Figure 3 )  . For the decision tree in the tl fird experiment , hough the size of the window is also varied from 7 words to 15 words , the size of feature vector is varied from 14 to 30 because phrase breaks tagged by probabilistic predictor are include in the feature vector  . 
ElI : o I prababililies trai , lingDFor decision Iree induction\[3I : ( ! r lest ( 9 ;   .   .   .   .   . 
PIol~:lbilislicIil ~ , lhodonly\[\]GIT teemllyProlmbilisfic ii~01\]lod Pmballilistic iii0thod lind posterror ~ tlltlIX ) s t ? Il Olc Dr locliOll ( 6:3  ) t ' olteclJoll ( 4:5  ) Fignre 3: The number of sentences for the probability training  , the decision tree learning and the test in the experiments  . 
T i t ( ; performance is assessed with reference to N , the total number of junctures ( spaces in text including any type of phrase breaks  )  , and B , the total number of phrase breaks ( only minor ( b1 ) and major ( b , )) breaks ) in the test set . The errors can be divided into insertions , deletions and substitutions . An insertion ( I ) is a break inserted in the test sentence , where there is not a break in the reference sentence  . A deletion ( D ) occurs when a break is marked in the rethrence sentence but not in the test sentence  . A substitution ( S ) is an error between major break and minor break or vice versa  . Since there is no single way to measure the performance of phrasebreak prediction  , we use the following peribr-mance measures ( Taylor and Black ,  1998) . 
Break_Correct-
B-DS
B x 100%,
N-D-S-I
Juncture_Correct = x100%

We use another pert brmancen masure , cM led adjusted score , which refer to the prediction accuracy in proportion to the total n mnber of phrase breaks as following performance measure proposed by Sanders  ( Sanders ,  1995) . 
Adjusted_Score- , IC-NB 1N   I3 ' where NB 1 means the proportion of no breaks to the number of interword spaces and  , lC means the Juncture_Correct/lO0 . 
Table 1 . shows the experimental results of our phrase break prediction and error con:ection method on the  619 open test sentences ( 10% of the total corpus )  . In the table , W means the that ure vector size t br the decision tree  , and 6:3 and 4:5 mean ratio of the number of sentences used in the probabilistic train and the decision tree induction  . 
The performance of probabilistic method is better than that of IG-tree method with any window size in U'reak_Cor'rect  . However , as the ti ; a ture vector size is growing in IO-tree method ,   . lv , nctv , re_Co'rrect and Adj ,   ( sled_Score become better than those of the l ) robM ) ilitic method . 
From the fact that the attribute located in the first level of the decision trees is the POS tag of preceding word  , we can see that the POS tag of preceding word iv the most useful attribute for predicting phrase breaks  . 
The pertbrmance before the error correction in hyl ) rid experiments iv worsettlant hat of the original  1  ) robabilistic method because the size of training corlms for probabilistic method is only  66  . 6% and 44 . 4:% of that of the original one , respectively . However , the performance sets improved by the post error correction tree  , and be-corns finally higher than that of both the probabilistic nmthod and the IG-tree method  . The attribute located in the first level of the decision tree is the phrase break that was predicted in the probat filistic method phase  . Although the initial pertbrmmme ( be ibre error correction ) of the exImriment using 4:5 corpus ratio is worse than that of the experiment using  6:3 corlms ratio , the final perfbrmance gets impressively improved as the decision tree induction corpus  1NB _N-I ~
N\]3r(;~k_Correet1 rol ) al)ilis ; ic method only
IG-Tree only
Prol)al)ilisl ; iemel ; ho(t6:3~I~15 ( Il ) oster rore or r(~ , e ; ioll 4:5

W-11
W = 15 l ) efore error e orre ; t;iol ~

W = ll
W = 151)~; ~, ore , error con'e(;tion

W--ll
W--1552 . 17% 5O . 58% 51 . 66% 51 . 77% 52 . 03% 57 . 34~) 59 . 8 O % 60 . 75% 51 . 30% 59 . O4% 61 . 83% 62 . 7/1%
Jun : l;~re_CorreeI ; 81 . 39% 81 . 39% 81 . 65% 81 . 71 . % 81 . 29~/~0 83 . 67% 84 . 69% 85 . O6% 80 . 85'~ 84 . 42% 85 . 16% 85 . 57%
Adjuste(l_-~0 . 48 O 0 . 480 0,487 0 . 488 0 . 477 0 . 543 0 . 572 0 . 582 0 . 465 0 . 564: 0 . 585 0 . 597 incre , ~sesfrom30% 1; o50% of the to ; al(:()rims . 
~l ) his result ; shows t ; h~t ; he prol ) osed ~ rehilx ~ e-ture c~m1) rovi(te , improved results evell with the phrase 1 ) re~k\] ) re ( tie:or:h~l ; h~sI ) o or nita , 1 per-f()z'51 I~L 51(;(~ ,  . 
5 Conclusion
Thist ) ~ l)erl)r ; s(:nts;~new1)hr ; tset ) rea . k predic- ; ion ~ rt : hil ; eel ; urel ; h  ~ l ; in l ; egr ~ testhetrob ~ bilis-tie ; ~t ) t ) ro~eh wii ; hthe(le(:ision-tree1)~s'(t~tl)-t)ro~te hin ; ~ synergistic w ~ y . Ourm ~ in contl'it ) u- , ions include presenl ; ing ( leeision ; ree-1) ; ~se(1r-ror correction for 1) hr ~ set ) re~k prediel ; ion . Also , i ) rol ) ~fl ) ilistic t ) hrasebreak prediction w~tsim-t ) leme , nt ; e(la . s ; I , 51 in il : i ~ tlam ~ ot ~ l ; or f the ( tet:i-si ( ) n tree-t ) ~ tse(te , rror (: or re('tiolLThem:ehite , ('-ture ( ' ~ mt ) rovi ( teimt ) l"ove ( t results even with l ; he1)\]u : ; ~set ) re ; ~kl ) redi:tor ; h ; t ; ha , sl)o ) rinil;i ~ dter-t ' orm ~ mee . Moreover , I ; he , syslxun(:~m1)et texit)lyt;u , nedt ; one we ort ) us wil ; houl ; nmssive rel ; r ; ~ ining which is necess ~ ry in the t ) rol ~ bilisti ( : metho ( t . 
As shown in the result , t)erli)rmamce , of the hy-t ) ridt ) hr ~ se1 ) re~kt ) redietion is dctermil mdt ) y how well the error e or re ( ' ; or ( ; t in ( ; Ollll ) ellS ; ~l ; ei ; he defi ( : iencies of the 1 ) rol ) ~d ) ilisti et ) hrase 1 ) re~k predict ; ion . 
The next sl ; el ) will 1 ) eto:m ~ lyzethele ~ rned de ( : ision treese are flfllyI ; ( ) exi ; r ; ~(: l ; more desir ~ t)leti ' , ~tl ; ur (' , veel ; or s . W (' , ~renow working055 in (: or-l ) or ~ ; ing this i ) hl ' ~ se , break1) redi('tion5he hod into the ext ) erimenl ; ~lKorean'I ? '\] ? Ssysl ; em . 

J . Alle , n~md S . Humli'ut .  1987 . l ' " ro~tt ! l'ext to St ) eech : the : MITalkSystcnt . Cmnl ) ridge University Press . 
IB . \]\] rill .  1992 . A simple rule-based p~rl ;- ot/-speecht ~ gg'r . In \] roccediT ~, . qs of the co ~: fer-c ' ~ tcco ~ , applied ~ , at ' u , r ' rdla ~ zg ' aage processi ~ zg . 
.\]eos\]gwon Ch ~ , Oeunbae \] ~( ; e , ~md Jong-Hyeok Lee .  :1998 . Ge , nc , r ~ dized ml known morphelne guessing for hybrid P ( )S l ; ~tgging of Kor (' ; m . 
In P ' lw('ec di'l ~, gsofth . (: Sizth , Wo'r ~: . sh , opo'l ~ , l&-"cqha'rqcCo'rp ora , t ~ g('s8593 . 
Dvr ~ ll ) a(; lem~ms , St ; ev ' n Gills , ~mdGerl;\])urieux .  1994 . ' l'he ~ cquisil ; ion of sl ; ress : Ad~l ; a-orienl ; ed~l ) l ) ro ~: h . Co~tI ) ~ tl , al , io'l ~ , al Liw , -g'ltil , ics ,  20(3):421 451 . 
Sangho Lee ; rod Yung-Itw ~ mO 15 .  1999 .  ~ . lS:ee-t ) ~ rse(tmo(l(;ling of 1) roso(tie1) hr , ( sing ~ 1 ~ ( tsegmei ~ l ; ~ tl(m : a . t;ion for kore ~ m . t . q , qy , qJ ; elll , q . 
, ~ peech , Co'll ~, ~ n,'w ~, i catio ~,, 28(4):283300.
q ' om M . Mil ; ehell .  1!)97 . Mach , i ~, cLea ' ~" H , i ~, g . 

J . l / , . ~ u in Dm .  1 . 983 . C  ~ . 5: l'ro!tr(~msfo'~"M(z-ch , i ' ~ , eLea'r ' ~ , i ' ~9 . Morgan K ~ mflnlmn . 
\] Brie S~mtlers .  1 . 9!t5 . Using protal ) ilistic mel : h-() < Is ; ot ) re ( lict phr~set ) oundaries for ~ t text-to-sl ) eeeh system . Master's thesis , University of Nijmegen . 
P ~ mlT  ~, ylor ; md Alum W . BD~t : k .  1!)98 . Assigning phrase l ) r c , ~k skom1) ar;-of-st)e , eehsc , -qllel Ices . Cotlt\])'lttcrSpeech(t~tdLa~tg'~t ( , ,gc ,  12(2):9! 1~7 . 
lhmlA . ' l?aylor , I . A . N ~ irn , A . M . fiutherl ; md,~mdM . A . .l~ck .  1991 . A re~ltime speech synthesis ystem . 115 1l ~ rot : ce , dirt . q . softh , c\]~'~t-rospecch,'9 . l . 
, l~mP . II . wm Santen , l/ . ie hard W . Sl ) roat , 3osel ) hP . Olive , ~ md . luli ~ Hirse  l ) erg . 
1997 . lrog'r(;ssinSpeechSy ~, th , esis . Springer-


