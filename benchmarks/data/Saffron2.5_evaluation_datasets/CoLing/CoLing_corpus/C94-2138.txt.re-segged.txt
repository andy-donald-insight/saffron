A Reestimation Algorithm fi ~ rI ' robabil istic Recto'sire ~ lYansition 

) " YoungS.tIan , mtdKey-Sun(,tul
(; enter for Artificial Intelligence
(; omputer Sciencel ) epart me ' at
(, ~ vnter for Artificial Intelligence Rese , arch
Korea Advanced lrtstitute of Science and Technology 
Tacjou , 305-70I , Korea
yshau@cskiug . kaist , ac . kr,kschoi(~cskiug . kaist,ac . kr

Prob ~ bilistic l , ecursive Tr~msition Network ( Pl~TN ) is an elevated version of t51'N to model and process lan- . 
guages in stoch~st , ic parameters . The representation is a direct derivation front the H  , TN and keeps much the spirit of ltidden Markov Model at the same tint  (  ,  . 
We present a reestimation algorithm \[' or Ptl , TN that is ~ variation of Inside-Ontside algorithm that comput  , es the v Mues of the probabilistic parameters from sample sentences  ( parsed or unparsed )  . 
1. lntrodu (: tion
In this pal)er , we introduce a network representation , Probabilistic Recursive Transitio . Network that is directly derived fl'Oln R'CN and It MM  , and present an estimation algorithm lottile probabilistic paraHteters  . PR ; 1 2N is a \]\[\] TN mJgmented with probabilities in the transitions ~ md states and with the lexical distributions in the transi--tions  , or is the Hidden Markov Model augmented with a stack that makes some traltsitions deter ministic  . 
The paramete . resthnation of PI ; I'N is developed as a wu'iation of Inside ( )utside algorithm . 
The hlsid c()utside algorithm has be cn applied e( , 10 t , I ; o ~ ,   ,  . ~* recently by Jelinek(1t9/) and \], ari (1991) . The algorithm was first introduced by Baker in 1 . 9 79 and is the contextfreelmt guage version o\[ Forward-  . Backw ~ rd algorithm in II id- . 
* This research is partly supported by KOSEF ( Km:ea Science alt d Teclntologyl " oundation ) undertit = title " A Studymt the B nilding '\[ ~ echni  ( lues for\[txdmstKm~wl edge based Systems " from 19911 through 1994  . 
den Markov Models . Its theoretical lbund ~ Ltion is laid by Baamaud Weh : hin the late  6l  ) ' s , which in tarn is a type of the F , MMgorithm in statistics ( Rabiner ,  1989) . 
Kupiec (1991) introduced at rell is based es-.
timation Mgorithm of Hidden SCFG that a e commodates both ilnside-Outside ~ dgorithm and l !brward-  . Backward ", flgorithm . The meaning of our work can be sought from the use of more plain topology of ITN  , whereas Kupiec's work is a unilied version of tbrward-  . back word and Inside Outside ~ lgorithms . Nonetheless , the implemen . 
ration of reestimation Mgorittun carries no more theoretical significance than the applicative fli ciency and variation for differing representations since B~ker first apt  ) lied it to CI " Gs . 
2 . Probabilistic Recursive Tran-sition Network A probabilistic ff  . l . ' N(PRTN , hereafter ) denoted by A is ~4 tuple . 
A is ~ transition m ~ trix containing tr~n . sition probabilities , ~ tnd 13 is ai L observation matrix containing probability distribution of the words observable at each term in M transition where row and column correspond to term in M transitions and a list of words respective  , ly . Fspecilies the types of transitions , and D2 denotes a stack . The first two model parameters are the same as that of I\[MM  , thus typed transitions and the existence of a stack art '  , what distinguishes I't tTNfl'omt\[MM . 

The stack operations are associated with transitions  . According to the stack operation , transitions are classified into three types . The first type is push transition in which state identification is pushed into the stack  . The second type is pop transition which is selected by the content of stack  . Transitions of the third type are not committed to stack operation  . The three types are also accompanied by different grammatical implication  , hence grammatical categories are assigned to trartsitions except pop transitions  . Push transitions are associated with nonterminal categories  , and will be called nonterminal transition when it is more transparent in later discussions  . In general , the grammar expressed in PRTN consists of layers . A layer is a fragment of network that corresponds to a nonterminal  . The third type of transition is linked to the category of terminals  ( words )  , titus is named terminal transition . Also a table of probability distribution of words is defined on each terminal transition  . In the context of HMMs , tile words in the terminal transition are observations to be generated  . Pop transitions represent returning of a layer to one of its possibly multiple higher layers  . 
The network topology of PI~TN is not different fi -om that of RTN  . In a conceptual drawing of a grammar , each layer looks like an independent network . Compared with conceptual drawing of the network , an operational view provides more vivid representation i which actual paths or parses are composed  . The only difference between the two is that in operational view a nonterminal transition is connecte directly to the first state of the corresponding layer  . In this paper , the parses or paths are assumed to be sequences of dark-headed transitions  ( see Fig . I for example ) . 
Before we start explaining the algorithms let us define some notations  . There is one start state denoted by 8 , and one final state denoted by f . Also let usca\]\] states immediately following a terminal transition terminal state  , and states at which pop transitions are defined pop state  . Some more notations are as follows . 
? first ( l ) returns the first state of layer I . 
? last(l ) returns the last state of layer 1.
? layer(,s ) returns the layer states belongs to .
? bout ( l ) returns the states from which layer l branches out  . 
? bin ( l ) returns the states to which layer I returns . 
? terminal ( 1 ) returns a set of terminal edges in layer I . 
? nonterminal ( l ) returns a set of nonterminal edges in layer 1 . 
? ij denotes the edge between states i and j.
?\[ i , j \] denotes the network segment between states i and j  . 
? Wa ~ b is an observation sequence covering from ath to bth observations  . 
3. Reestilnation Algorithm
PRTN is a RTN with probabilistic transitions and words  1 that can be estimated from sample sentences by means of statistical techniques  , we present a reestimation algorithm for obtaining the probabilities of transitions and the observation symbols  ( words ) defined at each terminal transition . Inside-Outside algorithm provides a formal basis for estimating parameters of contextfree language such that the probabilities of the observation sequences  ( sample sentences ) are maximized . The reestimation algorithm iteratively estimates the probabilistic parameters until the probabil ity of sample sentence  ( s ) reaches a certain stability . The reestimation algorithm for PItTN is a variation of Inside-Outside algorithm customized for the representation  . The algorithm to be discussed is defined only for wellformed observation sequences  . 
Definition 1 An observation sequence is wellformed if there exists at least a path that generates the sequence in the network and starts at 
S and ends at 2:'.
Let an obserw ~ tion sequence of length N denoted by
W-W~W ~... Wu.
We start explaining the reestimation Mgorithm by defining Inside-probability  . 
The Inside probability denoted by PI ( i ) s~t of state i is the probability that a portion of layer  ( i )   1we do not consider probabilistic states in this p ~per  . 

E--+T ~. E
E--+T'IF--*F*T 3'.--~F
F--+(E )
F --* a calling returu states !; ta . t cs ( F~1 . oo , 40-1, O0304-__ ~ .   .   .   .   .   .   .   . ~-~~"-? Figure 1: Illustration of PI?TN . A parse is composed of dard-heat ded transitions . 
(front state i to the last state of the layer ) generattes W ; ~ t . That t is , it is the probatbility that t a certain fragment of a layer generates at certain segment of an input sentence  , and this can be computed by summing the probabilities of all the possible paths in the layer segmen that generate the given input segment  . 
where c = last(layer(i)).
More constructive re . presentation of Inside probatbility is then kt wh  , creikCtcrminal(h~ycr(i )) , ia < i )) ,   ,   ,  = ) , v ~ bin(layer(j )) ,  '\] . ' he paths starting at state iarc classilied into two cases according to the type of hnme di~te transi--tionfl'omi:it can be of terminal or nonterminal type  , In ease of terminal , ~ J'ter the probatbility of the terminal transition is taken into account  , the rest of the layer segment is responsible for the input segment short of one word just generated by the term in M tratnsition  , in caase of nontmm in M , first the transition probabilities ( push and respective pop tratnsitions ) at remult iplied , then depending on the coverage of the nonterminal transition  ( sublatyer ) the rest of the current latyer is responsible for the rmnaining input sequence after done by the sublay cr  . After the last observation is made , the last state ( pop state ) of layer ( i ) should be reached . 
: 1iri := l :) I(i ) vH~t = 0 otherwise.
Fig . 2 is the pi ( ' to riM view of the Inside probability . A wellformed sequence can beginoidy at state , S , thus to be strict , t ~ (5) has additional product term F ( , 5 ) that can be computed also using InsideOutside algorithm  . Now define the
Outside probability.
The Outside probatbility denoted by Po(i , j).,~~.
is the probatbility that t patrtial sequences , W l ~ . ,q and Wt+1~N , are generated provided that the par-tiatt sequence  , Ws~t , is generated by \[ i , j \] given ruodel , A . This is a complementary point of Inside -probability  . This time , we look at the outside of given layer segme , t t and input segment . 
Assunfing a given latyer segment generates a given input segment  , we want to colnpute the probat-bility that the surrounding portion of the whole I'R : i'N generates the rest of the input sequence  . 
861 layer(i ) ~._:,,.. ik._(~)__~...(~___~...
layer(j ) (~???--' ~
IS
Figure 2: Illustration of Inside probability.
The Outside probability is computed first by considering the current layer consisting of two parts after ' excluding\[i  , j \] that are captured in Inside-probability . Beyond the current layer is simply an Outside probability with respec to the current layer  . 
And by definition,
Po(i , j) , ~t = p(\[s , i \] ~ w > ~_ ~ , \[ j , y\]-~
W , + I~NIA ) axfa evXxa = lb = t + l*~(f , i ) o ~ , ~ Pdj ) ~ ~ beo(~ , y ) o ~ ~ . 
P ; ( f , i ) ~ ~ t
Fig .   3 shows the network configuration in computing the Outside probability  , t ' ~( f , i ) = ~ ~_ t is the probability that sequence , W = ~~ I , is generated by layer ( i ) left to state i . PI ( j ) t+l~b is the probability that sequence Wt+l~b is generated by layer  ( i ) right to state j . The portions of W not covered by W = ~ b is then left to the parent layers of layer  ( i )  . 
P ~( f , i) . ,~t is a slight wriation of Inside probability in which PI  ( f ) = ~ b'S in the Inside probability formula are replaced by P ~  ( f , i ) a ~ b . \[ tsactual computation is done as follows:
PI(f) , ~tifs_<t , 1 if s > t and f = i , 0 if s > t and f ) Ai . 
whe TexEbout(layer(i )) , yeb~n(layer(i )) , f = first(layer(i )) , e = last(layer(i )) , layer(i ) = layer(j ) , layer (~) = layer(y ) . 
x represents a state from which layer ( i ) branches out , and y represents a state to which layer ( j ) returns to . Every time a different combination of left and right sequences with respect to W~~t is tried in the layer states i and j belong to  , the rest of remaining sequences i the Outside probability at the layer above layer  ( i )  . 
When there is no subsequence to the right of
W ~ ~ b(i.e ., b = N),
Po(i , j)a ~ N=1.
It is basically the same as Inside probability except that it carries a state identification i to check the vMidity of stop state  . If there are observations left for generation ( s_<t )  , things are done just as in computing Inside probability  , ignoring i . When boundary point is reached ( s > t ) , if the last state is i , it returns 1 , and 0 , otherwise . 
The probability of an observation sequence can be computed using Inside probability ~ sp  ( wJA ) -- P ( \[ s , a = \]-~ w > NI ~) = P , ( s)I_N . 
Now we can derive the reestimation algorithm for Ji and / ~ using the Inside and Outside probabil L ties  . As the result of constrained maximization of Bantu's auxiliary function  , we have the following form of reestimation for each transition  ( Rabiner 1989 )  . 
862 layer(x ) ~) layer(i ) @__ ,  _ 1  , , , - ,   ,   , Il+lNI Figure 3: Illustration of Outside probability . 
expected no . of transitions from i to j d~j = expected no . of transitions frontiThe expected frequency is defined for each of the thre  (  , types of transition . For a terminal transition , ~ N?E, . = ~ aij b(ij , W, . ) l'o(i , j), . ~, . E t(ij ) == r(wIa)
For a nontcrminal transition , aljEt(ij)~2~e~(ik)+)2ke ,  . (i~)
For nonterminal transitions , aij
E , a(ij ) ? 2ke ~( ik ) + ?; ke , .( ik)
And for pop transitions , notice that only pop transitions are possible at apop state  ,  __ , ~N  ~ . ~ aijPi(j ) . ~~ ta ~,, I'o(i , v ) ~~ tE ~, ov(ij)
P) , ~t(ij ) = -'~= ~ aij-~'(wI ~) E~z , : , ,o ; , ( i ~) - . +" lDh ?7'~'\[' = la . ~ t(lf , ~y ??'( j )) ,  ' , J ~ bin(layer(j )) , For a terminal transition ij aud ~ I , observation 1 ,   , y , ~( i ) = 1 . y , ~( . 0), l . : , j  ~ . ( j ) : ~ l ~' r ( ,   , ly ' " b ? l " uv is a pop transition . 

Y '-, t . , . t . wt = ~, aij b(ij,Wt)l'o(i,J)t~t
For a pop transition , b(ij , w)~:~\]V-~)2 t=~aij b(ij , Wt ) l'o ( i , j)t~t
Epov ( ij ) : : v ( wIa ) where uE : bout ( layer ( i ) ) , j ( ~ bin(layer(i )) , v := first(layer(i )) , l .   . , j  ~ , . (, . )- l . y  ~ , , . ( j ) , l < , jc , , , (~)-l~y  ~ , ,(0 , u ' ~ is a nonterm in M transitiol t . 
Considering that tr~msitions of terminal and nonterminM types can occur together at a state  , the reestim ~ tion\['or terminal tr~msitionsi done as follows : ' file reestimation process co ~ltinues until the probability of the observation sequences reaches a certain stability  . It is not nnusuM to assume that the tra . i Hing set can be very large , and even grow indefinitely in nontrivial applications in which case additive traini~tgc~n be tried using a smoothing tect miquc as in  ( Jarre and I'ier accini \]987 )  . 
The complexity of \[ It side-Outside ~ dgorithm is O  ( Na ) both in the mnnber of states and input length ( l ~ ari 1990 )  . The ei\[iciency comes from the fact that the algorithm successfully exploits the context -freeness  , l ! brinstance , the ge~mration of substrings by a nonterminal is independent of t it  (  ; surroundings of the . a on terminal , and this is \] to w the product of the Inside and Outside probabilities works and the COlnplexity is derived  . 
8634. Conclusion
Recently several probabilistic parsing approaches have been suggested such as SCFG  , probabilistic GLR , and probabilistic link grammar ( Lafferty ,  1992) . Kupiec extended the reestimation algorithm for SCFG to cover non-Chomsky normal forms  ( Carroll ,  1993) . This paper further advances the trend by implanting the Inside-Outside algorithm on the plain topology of RTN which distinguishes itself from Kupiec's work  . 
\[8\] Lari , K . ; and Young , S . J .  (1991) . " Applications of stochastic on text-free grammars using the Inside-Outside algorithm  . " Computer
Speech and Language 5: 237-257.
\[9\] Rabiner , Lawrence R .  (1989) . A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition  . Proceedings of the
IEEE ~27(2).
References\[1\]Baker , J . K .  (1979) . Trainable Grammars for Speech I~e cognition . Speech Communication Papers for the 97th Meeting of the acoustical Society of America ( D . H . Klatt & J . J . Wolf , eds):547-550 . 
\[2\] Baum , L . E .  (1972) . An Inequality and Associated Maximization Technique in Statistical Estimation for Probabilistic Functions of a 
Markov Process . " Inequalities 3:18.
\[3\] Carroll J . , and Briscoe E .  (1993) . Generalized probabilistic LR parsing of natural language  ( Corpora ) with unification-based grammars . ACL 19(1) .  2559 . 
\[4\] Jarre , A . , and Pieraccini , R .  (1987) . "Some Experiments on HMM Speaker Adaptation , "
Proceedings of ICASSP , paper 29.5.
\[5\] John Lafferty . , Daniel Sleator . and Davy Temperley .  (1992) . Grammatical trigrams : a probabilistic model of link grammar  . In Proceedings of AAAI Fall symposium on Probabilistie Approaches to Natural Language Processing  , Cambridge , MA .  89-97 . 
\[6\] Jelinek , F . Lafferty , J . D . and Mercer R . L . 
(1990) . Basic Methods of Probabilistic ContextFree Grammars  . IBM RC 163 74 . IBM Continuous Speech Recognition Group . 
\[7\] Kupiec , Julian (1991) . A Trellis-Based Algorithm For Estimating the Parameters of attid-den Stochastic ContextFree Grammar  . Proceedings , Speech and Natural Language Workshop . sponsored by DARPA . Pacific Grove : 241-246 . 

