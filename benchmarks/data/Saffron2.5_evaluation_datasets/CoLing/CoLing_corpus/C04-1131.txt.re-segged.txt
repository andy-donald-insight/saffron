Word sense disambiguation criteria : a systematic study 
Laurent AUDIBERT
DELIC , Universit ? de Provence
29 Av . Robert SCHU MAN ?136 21
Aix-en-Provence Cedex 1, FRANCE



This article describes the results of a systematic in depth study of the criteria used for word sense disambiguation  . Our study is based on 60 target words : 20 nouns , 20 adjectives and 20 verbs . Our results are not always in line with some practices in the field  . For example , we show that omitting noncontent words decreases performance and that bigrams yield better results than unigrams  . 
1 Introduction
The task of word sense disambiguation ( WSD ) is to identify the correct sense of a word in context  . 
WSD is usually performed by matching information from the context in which the word occurs with disambiguation knowledge source  . Our approach uses supervised machine learning techniques to automatically acquire such disambiguation knowledge from sense-tagged corpora  . At present , this type of approach is widely used and seems to yield the best results  ( Kilgarriff , 
Rosenzweig , 2000; Ng , 1997b).
Information conveyed by context words ( morphological form ) is enriched with further annotations : part-of -speech tag  , lemma , etc . Each individual piece of information is called a feature  . 
A good feature should capture an important source of knowledge that is critical in determining the sense of the word to be disambiguated  . The choice of the appropriate set of features is an important issue for WSD  ( Bruce , Wiebe , Perdersen , 1996; Ng , Zelle , 1997; Pedersen ,  2001) . Thus , this paper describes the results of a systematic and indepth study of homogenous criteria  ( i . e . set of features ) that can be used for WSD . 
2 Methodology 2.1 Corpus
The corpus we worked on is composed of different types of texts and comprises  6   468   522 words . It was put together within the framework of the SyntSem project that aims at producing a French corpus which is morphologically and syntactically tagged  , lemmatised and that comprises a light syntactical tagging as well as a lexical tagging of  60 target words selected for their strongly polysemous nature  ( V?ronis ,  1998)1 . 
These 60 target words are evenly divided into 20 nouns , 20 adjectives and 20 verbs , having a total of 53   796 occurrences in the corpus . 
The inadequacy of standard dictionaries ( V?ronis , 2001) and computational lexicons ( Palmer ,  1998 ) for natural language processing is presently one of the major difficulties encountered in word sense disambiguation  . For instance , by using these dictionaries , the interannotator agreement may sometimes reach only  57%   ( Ng , Lee ,  1996 ) or may simply be equivalent to random sense allocation  ( V?ronis ,  1998) . To overcome this weakness , a dictionary more specific to natural language processing is being developed in our team  ( Reymond ,  2002) . It has been used to tag the occurrences of the 60 target words of the SyntSem corpus . 
Table 8 in the appendix gives quantitative information for each target word  . The number of senses per word may be very large for it includes idioms and phrasal verbs such as : ? mettre surpied ?  , ? mettre ? pied ? , ? pied denez ? , etc . 
A general agreement seems to emerge according to which morpho-syntatic disambiguation and sense disambiguation can be disentangled  ( Kilgarriff , 1997; Ng , Zelle ,  1997) . We have entrusted the part-of-speech tagging of our corpus to the Cordial software  ( developed by Synapse
D?veloppement company ) as it offers lemmatisation and part-of-speech tagging of a satisfactory accuracy  ( Valli , V?ronis ,  1999) . 
m form lemma emscgems sense mettremet tre VINF VINF  1  . 12 . 7 fin fin NCFS NCOM ? ? PREP PREP la le DETDFS DET pratique pratique NCF SNCOM des de DET DPIGDET d ? tentions d?tention NCFPNCOM  1 Table 1: SyntSem tagged corpus extract . 
1 These words are those used in the French part of the  Senseval1 evaluation exercice ( Segond ,  2000 ) but the corpus and dictionary are different in the present study  . 
Table 1 displays an extract of the SyntSem corpus . It shows all the tags of each word . We use the information provided by these tags in our lexical disambiguation criteria  . 
2.2 Criteria
The aim of our study is to evaluate a large variety of homogenous criteria  ( i . e . set of features ) . 
The name of each criterion specifies its nature and takes the following form [  par1par2par3par4]  . 
Parameter par1 indicates whether the criterion takes into account unigrams  ( par 1=1gr )  , bigrams(par1=2gr ) or trigrams(par1=3gr ) , knowing that an ngram represents the juxtaposition of n words  . 
Parameter par2 indicates which word tag is considered : morphological form  ( par2 = m form )  , lemma ( par2 = lemma ) , part-of-speech ( par2 = ems ) or coarse-grained part-of-speech ( par2 = cgems )  . 
Parameter par3 indicates if we take into account word positions ( par3 = position )  , if we only distinguish left from right context ( par 3= left right )  , or if we simply consider unordered set of surrounding words  ( par 3= unordered )  . Lastly , parameter par4 shows whether the criterion takes into account all the words  ( par4=all ) or content words only ( par3=content )  . We call these criteria ? homogeneous criteria ? as the four parameters together determine the nature of all pieces of contextual evidence selected by the criterion  . 
For contexts within a range of ?1 to ?8 words , the combination of all parameters generates 576 ( 3?4?3?2?8 ) distinct criteria . We have systematically evaluated each one of these criteria as well as other criteria in order to answer specific questions and to validate or invalidate certain hypothesis  . 
Within the framework of this study , we have developed an application used to model these criteria and to further apply them to the corpus in order to generate feature vectors used by our classifiers  ( Audibert ,  2001) . 
2.3 Classifiers
We have selected two complementary classifiers.
We have chosen the Na?ve-Bayes classifier ( NB ) for its simplicity and widespread use , as well as for its wellknown state-of-the-art accuracy on supervised WSD  ( Domingos , Pazzani , 1997; Mooney , 1996; Ng , 1997a ) . The NB classifier assumes the features are independent given the sense  . During classification , it chooses the sense with the highest posterior probability  . We have also selected a decision list classifier ( DL ) which is similar to the classifier used by ( Yarowsky , 1994) for words having two senses , and extended for more senses by ( Golding ,  1995) . DL classifier is further developed in ( Audibert ,  2003) . In DL , features are sorted in order of decreasing strength  , where strength reflects feature reliability for decision making  . The DL classifier distinguishes itself clearly from the NB classifier as it does not combine the features  , but bases its classifications solely on the single most reliable feature identified in the target context selected by the criteria  . We will make a use of this decision making transparency several times in this article  . Some other advantages of DL classifier are its significant simplicity and its ease of implementation  . 
Both of the classifiers we used require probability estimates  . Given the data sparseness , we have to deal with zero or low frequency counts . 
For this reason , we have decided to use m-estimation ( Cussens ,  1993 ) rather than classical estimations of probabilities or Laplace  ( ? add one ? ) smoothing . 
When a classifier is not able to disambiguate a target word  , which is very rare , it selects the most frequent sense from the training data  . Thus , all occurrences are tagged . As in this case precision equals the recall , the present articles peaks in terms of precision only  . 
To evaluate a criterion in the corpus , we use a k-fold crossvalidation method ( in accordance with the common use , in our experiment , k = 10) . Despite the fact that this method takes much computing time  , it enables the evaluation of the criterion in the whole corpus  . 
Throughout the tests , the two classifiers have generally obtained comparable accuracy  , although the NB classifier has almost systematically outperformed the DL classifier  . 
3 Results 3.1 Best criteria precision
Table 2 displays for each of the target words studied the optimal context size and the disambiguation precision obtained by the best unigram  , bigram and trigram-based criteria . 
This table points out that best criteria take into account all words in the context  . Section 3 . 2 will concentrate on feature reliability according to their part-of-speech  . Then , section 3 . 2 will deal with the impact of different feature selections based on Nouns Adjectives Verbs 
Criterion P % SP % SP % S [1grlemma ordered all ] 81  . 9 ?2 76 . 8 ?1 71 . 8 ? 3[2gr lemma left right all ] 83 . 6 ?4 77 . 9 ?3 74 . 0 ? 4[3gr lemma left right all ] 82 . 3 ?5 72 . 7 ?3 71 . 2?5 Table 2: Optimal context size ( S ) and criteria precision ( P % ) using NB classifier . 
features part-of-speech . Nouns Adjectives Verbs P % U % P % U % P % U %
NCOM 93.0 12.7 93.7 25.3 87.7 26.0
DET 73.8 30.2 69.8 21.3 48.1 12.7
PREP 78.3 24.2 61.4 15.2 62.9 17.4
ADJ 94.9 13.7 80.3 2.2 65.5 3.2
ADV 57.0 1.17 9.2 9.66 0.3 5.7
PROPE 63.6 0.66 7.0 2.6 65.9 11.4
PCTFORTE 72.1 3.00 69.0 4.2 80.9 2.9
VCON 67.9 1.55 3.9 2.45 4.4 3.0
SUB 78.6 0.65 8.1 1.8 79.9 2.7
VINF 90.2 0.98 0.7 0.78 7.2 2.9
NPRO 86.8 0.39 2.0 0.68 1.8 1.2
VPAR 89.7 0.35 0.0 0.28 1.0 0.9
PRODE 100 0.0 35.0 0.26 8.6 0.8
Table 3: Precision ( P % ) and usage proportion ( U % ) by coarse-grained part-of-speech of most reliable contextual evidences using DL classifier  . 

Figure 1: space distribution by part-of-speech of most reliable pieces of contextual evidence used for disambiguation with DL classifier  . 
According to the Table 2 , the optimal context size comprises ?1 to ?5 words . Further developments of the context optimality will be made in section  3  . 4 . 
Surprisingly , Table 2 outlines the fact that the criterion that obtains the best precision is based on bigrams and not on unigrams  . This subject is dealt with in section 3 . 5 . 
3.2 The most reliable parts-of-speech
In this section , we aim at learning the part-of-speech and the space distribution of all pieces of contextual evidence used for disambiguation  . 
To this end , we use the DL classifier because it bases its classifications solely on the most reliable piece of evidence identified by the criteria  . Thus , DL classifier enables us to learn which is the part-of-speech and the space distribution of this indicator by using the criterion  [1grmformorderedall  ] . Table 3 and graphics presented in Figure 1 synthesize this study results . 
Table 3 enables to bring out the following results ( we quote between brackets and in order the main results obtained for the nouns  , the adjectives and the verbs ) : ? common nouns ( NCOM ) obtain one of the best precisions ( 93 . 0%; 93 . 7%; 87 . 8% ) and represent one of the most used indicators ( 12 . 7%; 25 . 3% ; 26%) for the three term categories ; ? adjectives ( ADJ ) represent good indicators for nouns ( p = 94 . 9%) and adjectives ( p = 80 . 3%) , but they are especially useful for nouns since they are used in  13  . 7% of the cases against 2 . 2% only for the adjectives ; ? adverbs ( ADV ) are mainly useful for adjectives ; their precision reaches 79 . 2% and they are used in 9 . 6% of the cases ; ? verbs in the infinitive ( VINF ) are very reliable indicators for the three parts -of-speech  ( 90 . 2%; 80 . 6%; 87 . 2%) , but they are rarely used as they are not very often encountered in the context  ( 0 . 9%; 0 . 7%; 2 . 9%) ; ? conjugated verbs ( VCON ) obtain poor precision ( 67 . 9%; 53 . 9%; 54 . 4%) . 
Figure 1 graphics show the space distribution of the main parts-of-speech of the indicators used to disambiguate each one of the three term categories  . 
The dissymmetric shape of verbs , and more precisely , the strong prevalence of indicators located in position  +1  ,  +2 ,  +3 , makes us believe that disambiguating verbs is done more accordingly to their object than to their subject since as a rule the main form encountered is subject ? verb ? object  . 
Table 4 summarizes these graphs . Our results and those of ( Yarowsky , 1993) agree in many respects , although his study applies only to pseudo words having only two ? senses ?: ?? Adjectives derive almost all of their disambiguating information from the nouns they modify ?  ; ? ? Nouns are best disambiguated by directly adjacent adjectives or nouns ?  ; ? ? Verbs derive more disambiguating information from their objects than from their subjects ?  . 
3 . 3 The importance of stopwords Nouns Adjectives Verbs 
Unigrams 0.3% 2.5% 6.9%
Bigrams 2.7% 3.4% 13.5%
Trigrams 12.4% 15.9% 20.2%
Table 5: precision decrease when omitting noncontent words using DL classifier  . 
Many studies do not consider all the words of the context  ( El-B?ze , Loupy , Marteau , 1998; Mooney , 1996; Ng , Lee ,  2002) . The assumption according to which content words represent the most reliable indicators under lies the choice to use only content words based criteria  . This seems to be obvious , but it is not confirmed in Table 2 . Table 5 shows the average decrease of the precision of the content words based criteria  ( [ par1 par2 par3 content] ) compared to the same criteria based on all words ( [ par1 par2 par3 all] )  . 
This table shows that the decrease is low when the criteria are based on unigrams and are used to disambiguate nouns  , but it can become very high in the other cases , and in particular for verbs disambiguation . 
Table 3 informs us about the disambiguation precision according to the coarse-grained part-of-speech tag  . This table shows that using content words only is probably not the most appropriate feature selection  ( for example inflected verbs are not relevant )  . We have therefore chosen to try out a more subtle selection  ( we refer to it by par 4= selected ) by restraining to the most reliable parts-of -speech according to Table  3: ? For nouns , we use indicators having the following coarse -grained part-of-speech tagging : NCOM  , PREP , ADJ , SUB , VINF , NPRO , VPAR or PRODE ; Most reliable contextual evidences NCO MADJ ADV DET PREP 
Nouns-2,+2+1-1-1,+1
Adject.-1,+1-1-1,-2+1
Verbs+2,+3+1,+2+1
Table 4: space distribution of most reliable pieces of contextual evidence used for disambiguation with DL classifier  . 
Nouns Adj . Verbs [ 1grmform ordered all ] 81 . 5 75 . 7 71 . 0 [1grmform ordered content ] 78 . 9 71 . 6 59 . 5 [1grmform ordered selected ] 79 . 2 71 . 5 66 . 3 Table 6: precision with and without feature selections using NB classifier  . 
? For adjectives , we use indicators having the following coarse -grained part-of-speech tagging : 
NCOM , DET , ADJ , ADV , VINF or NPRO ; ? For verbs , we use indicators having the following coarse -grained part-of-speech tagging : NCOM  , ADJ , PROPE , PCTFORTE , SUB , VINF , NPRO , 
VPAR or PRODE.
Table 6 gives a comparison of the precision reached by the following  3 criteria : ? [1grmformorderedall  ] , ? [1 grm form ordered content] , ? [1 grm form ordered selected ] . 
We observe that this subtler selection lowers the disambiguation precision too  . We assume then that all words , whatever their part-of-speech , contribute to the disambiguation . 
3 . 4 Optimal context 3 . 4 . 1 Size and symmetry We tested up to ?10   000 word contexts . 
However , the best precision is always obtained for short contexts ranging from  ?1 to ?5 words . These results are similar to those obtained by many other researches  ( El-B?ze , et al , 1998; Yarowsky ,  1993; 2000) . 
Optimal context size is criteria , target part-of-speech and ngram size dependent . In particular , it increases with the ngram size . 
Table 7 shows , for all the criteria we examined , the average size of the optimal context by the ngram size and by the part-of-speech  . 
The main indicators used to disambiguate nouns and adjectives surround roughly symmetrically the word we want to disambiguate  . On the contrary , indicators for verbs tend to be mainly situated after the verb  . Therefore , a non-symmetrical context shifted forward by a word proves to be more appropriate  . Our experiments show that the use of this shifted context improves the precision of the verbs disambiguation by  0  . 75% in average . 
Nouns Adjectives Verbs
Unigrams 1.5 1.1 1.8
Bigrams 2.42 2.8
Trigrams 3.1 3.4 3.8
Table 7: optimal context size using both classifiers . 
3 . 4 . 2 Do ngrams have to contain the target word ? The lemma being unique for a given word  , if only lemmas are considered , an ngram which is adjacent to the target word contains precisely the same information as the same ngram to which the target word is added in order to compose a  ( n + 1 ) -gram . The ngram that is situated next to the word to disambiguate can thus be assimilated to the  ( n + 1 ) -gram which contains it . Consequently , the question becomes : do ngrams have to contain the word to disambiguate or at least to be adjacent to it ? Several studies set themselves this constraint probably because ngrams are used to capture fixed constructions containing the word to disambiguate  . 
Table 2 shows that the optimal context size for best bigram or trigram-based criteria does not fit this constraint  . The relevant ngrams do not necessarily contain the target word and are not necessarily adjacent to it  . For example , for nouns and verbs , the ?4 words context is the optimal context size of the bigram-based criteria which obtains the best disambiguation precision  . This criterion produces some bigrams separated from the target word by one or two words  . However , this single observation cannot enable us to abandon the constraint in terms of containing or being adjacent to the target word  . Indeed , the bigram increasing distance may help locating an information which could be captured by the joint use of one or several larger ngrams  . We have thus evaluated a combination of criteria in which all ngrams contain the target word in a context up to  ?4 words : ? [2grlemmaleftrightall ] with context size of ?1 words ;  ?  [3grlemmaleftrightall ] with context size of ?2 words ;  ?  [4grlemmaleftrightall ] with context size of ?3 words ;  ?  [5grlemmaleftrightall ] with context size of ?4 words . 
This combination leads to a disambiguating precision of  74  . 3% , which is lower than the one obtained using the criteria  [2grlemmaleftrightall ] alone with a ?4 words context . This experiment confirms that constraining the context to contain the word to disambiguate  , or at least to be adjacent to it , decreases disambiguation accuracy . Consequently , nothing justifies this constraint on criteria . 
3.5 Surprising bigrams accuracy
Contrary to all expectations , Table 2 shows that the best unigram-based criterion ( [1 gr lemma ordered all ] ) is definitely less accurate than the best bigram -based criterion  ( [2gr lemma left right all ] )  . However , in practice , bigrams and trigrams are seldom used alone . When used , they are taken in conjunction with unigrams which are supposed to convey the most reliable piece of information  . 
Why does the criterion [2grlemmaleftrightall ] work so well ? First , since this criterion is a juxtaposition of lemmas  , among the features generated by this criterion , the left and the right unigrams are to be found , even if these unigrams are disguised as bigrams ( cf . section 3 . 4 . 2) . As these pieces of contextual evidence are certainly the most important ones  ( cf . section 3 . 4) , it makes sense that this bigram-based criterion obtains good results  . 
Second , in a context of a higher size , the juxtaposition of two words seems more relevant than one isolated word  . For example , to disambiguate the word utile , the bigram pour_le is relevant , whereas the single unigrams pour and leare not of much help  . 
Lastly , sometimes , the presence of a unigram noncontiguous to the target word can be sufficient to solve the ambiguity  . But using bigram-based criteria does not necessarily lose the piece of information conveyed by unigram-based criteria  . 
For example , a determiner , a preposition or an apostrophe often precedes a common noun  . The lemmatisation variability of this determiner , this preposition or this apostrophe is low for a given common noun located at a given distance from a given target word  . Therefore , the piece of information brought out by the juxtaposition of the noun and the preceding word is often very similar to the piece of information provided by the noun alone  . 
4 Conclusion
We have described here the results of a systematic and in depth research on WSD criteria  . 
This may be the first research of this extent carried out within a unified framework  . This study enabled us to confirm certain results stated in the field literature such as : ? importance of short contexts  ; ? importance of adjacent noun or adverb for adjective disambiguation  ; ? importance of adjacent adjective , or noun in a very short context for noun disambiguation  ; ? importance of the noun in the area after the verb and use of dissymmetrical contexts for verb disambiguation  . 
We have also obtained more original results , not always in line with some practices in the field such as : ? importance of stop words whose withdrawal decreases the performance almost systematically  ; ? better results obtained by bigrams taken alone than unigrams alone  ; ? unnecessary constraint of including or be adjacent to the target word  . 
Disambiguation accuracy could probably be improved by the study of other sources of information useful in disambiguation  , such as : ? criteria based on binary syntactic relations  ( noun-noun , noun-verb , adjective-noun , etc . ) to capture information which can be absent from short contexts  ; ? the use of thesauri or other sources of information to carry out generalizations on context words to overcome data sparseness problem  ; ? topical text information ; ? selectional restrictions . 
This preliminary study focuses on homogenous criteria  ( for example : lemmas located from ?2 to +2 position )  . To improve the disambiguation accuracy , we have to look for heterogeneous criteria by gathering the most relevant pieces of contextual evidence not necessarily of the same type  ( for example : lemma in position ?2 , part-of-speech in position ?1 , morphological form of target word and lemma in position  +2  )  . This feature selection leads to a combinatorial explosion that can be solved by genetic algorithms  ( Daelemans , Hoste , Meulder , Naudts ,  2003) . 


Audibert L .  (2001) , LoX : Outil Polyvalent pour l ' Exploration de Corpus Annot?s  , 5 ? me
Rencontre des ? tudiants Cher cheursen
Informatique pour le Traitement Automatique des Langues  ( RECITAL-2001 )  ,  411-419 . 
Audibert L .  (2003) , Etude des Crit?res de D?s ambigu?sation S?mantique Automatique : R ? sultats surles Cooccurrences  ,   10?me conf?rence sur le Traitement Automatique des Langues Naturelles  ( TALN-2003 )  ,  3544 . 
Bruce R ., Wiebe J ., Perdersen T . (1996), The
Measure of a Model , 1st Conference on
Empirical Methods in Natural Language
Processing ( EMNLP-1996), 101-112.
Cussens J .  (1993) , Bayes and Pseudo-Bayes Estimates of Conditional Probability and their Reliability  , 6th European Conference on Machine Learning ( ECML-1993 )  ,  136-152 . 
Daelemans W . , Hoste V . , Meulder F . D . , Naudts B . 
(2003) , Combined Optimization of Feature Selection and Algorithm Parameter Interaction in Machine Learning of Language  , 14th European Conference on Machine Learning ( ECML-2003 )  ,  84-95 . 
Domingos P ., Pazzani M . (1997), Beyond
Independence : Conditions for the Optimality of the Simple Bayesian Classifier  , Machine
Learning , 29:103-130.
El-B?ze M . , Loupy C . d . , Marteau P . -F . (1998), WSD Based on Three Short Context Methods,
SENSEVAL Workshop , in press.
Golding A . R .  (1995) , A Bayesion Hybrid Method for Context-Sensitive Spelling Correction  , 3th Workshop on Very Large Corpora ,  39-53 . 
Kilgarriff A .  (1997) , Evaluating Word Sense Disambiguation Programs : Progress Report  , Speech and Language Technology ( SALT-1997)
Workshop on Evaluation in Speech and
Language Technology , 114-120.
Kilgarriff A . , Rosenzweig J .  (2000) , English Senseval : Report and Results , 2nd International
Conference on Language Resources and
Evaluation ( LREC2000), 3:1239-1244.
Mooney R . J .  (1996) , Comparative Experiments on Disambiguating Word Senses : an Illustration of the Role of Bias in Machine Learning  ,   1st Conference on Empirical Methods in Natural Language Processing  ( EMNLP-1996 )  ,  82-91 . 
Ng H . T . (1997a ) , Exemplar-Based Word Sense Disambiguation : Some Recent Improvements  ,   2nd Conference on Empirical Methods in Natural Language Processing  ( EMNLP-1997 )  ,  208-213 . 
NgH.T . (1997b ), Getting Serious About Word
Sense Disambiguation , Association for
Computational Linguistics Special Interest Group on the Lexicon  ( ACL-SIGLEX-1997 ) :
Workshop " Tagging Text with Lexical
Semantics : Why , What , and How ? "17.
Ng H . T . , Lee Y . K .  (1996) , Integrating Multiple Knowledge Sources to Disambiguate Word Sense : An Exemplar-Based Approach  ,   34th Annual Meeting of the Society for Computational
Linguistics , 40-47.
Ng H . T . , Lee Y . K .  (2002) , An Empirical Evaluation of Knowledge Sources and Learning Algorithms for Word Sense Disambiguation  ,   7th Conference on Empirical Methods in Natural Language Processing  ( EMNLP 2002 )  ,  4148 . 
NgH.T ., Zelle J . (1997), Corpus-Based
Approaches to Semantic Interpretation in Natural Language Processing  , Artificial Intelligence Magazine-Special Issue on Natural Language 
Processing , 18:4564.
Palmer M . (1998), Are WordNet Sense
Distinctions Appropriate for Computational Lexicons  , Association for Computational Linguistics Special Interest Group on the Lexicon  ( ACL-SIGLEX-1998 ) : Senseval , in press . 
Pedersen T .  (2001) , Machine Learning with Lexical Features : the Duluth Approach to  Senseval2  ,   2nd International Workshop on Evaluating Word Sense Disambiguation Systems  ( Senseval2 )  ,  139-142 . 
Reymond D . (2002), M?thodologie pour la
Cr?ationd ' unDictionnaire Distributionnel dans unePerspectived'?tiquetageLexicalSemi -Automatique  ,   6?me Rencontre des ? tudiants Cher cheursen Informatique pour le Traitement Automatique des Langues  ( RECITAL-2002 )  ,  405-414 . 
Segond F .  (2000) , Framework and Results for French , Computers and the Humanities ,  34: 49-60 . 
Valli A ., V?ronis J . (1999), ? tiquetage
Grammatical de Corpus Oraux : Probl ? me set Perpectives  , Revue Fran?aise de Linguistique
Appliqu?e , 4:113-133.
V?ronis J . (1998), A Study of Polysemy
Judgements and Inter-Annotator Agreement,
Programme and Advanced Papers of the
Senseval1 Workshop , 24.
V?ronis J .  (2001) , Sense Tagging : Does It Make Sense ? , Corpus Linguistics Conference , http://www . up . univ-mrs . fr/~veronis/pdf/2001-lancaster-sense . pdf . 
Yarowsky D . (1993), One Sense Per Collocation,
ARPA Workshop on Human Language
Technology , 266-271.
Yarowsky D .  (1994) , A Comparision of Corpus-Based Techniques for Restoring Accents in Spanish and French Text  , 2nd Annual Workshop on Very Large Text Corpora ,  19-32 . 
Yarowsky D .  (2000) , Hierarchical Decision List for Word Sense Disambiguation  , Computers and the Humanities ,  34: 179-186 . 


Nouns FSH MFS barrage 92 51 . 2 76 . 1% chef 1133 111 . 5 76 . 0% communication 1703 132 . 4 40 . 6% compagnie 412 121 . 6 71 . 4% concentration 2466 245 . 1% constitution 422 61 . 6 50 . 0% degr ? 5071 82 . 5 58 . 6% d?tention 112 20 . 9 72 . 3% ? conomie 930 102 . 2 49 . 1% formation 1528 91 . 7 63 . 8% lancement 1385 179 . 7% observation 572 30 . 7 86 . 0% observation 572 30 . 7 86 . 0% organe 36 62 . 2 38 . 3% passage 600 192 . 7 37 . 0% pied 9606 23 . 5 37 . 6% restauration 104 51 . 8 43 . 3% solution 880 40 . 4 93 . 3% station 266 82 . 6 32 . 0% suspension 110 51 . 5 61 . 8% vol 278 102 . 2 40 . 3%
Average 568 14.2 1.9 57.3%
Adjectives FSH MFS biologique 475 40 . 5 89 . 9% clair 557 203 . 1 29 . 3% correct 116 51 . 8 53 . 4% courant 170 40 . 6 90 . 0% exceptionnel 226 31 . 4 53 . 1% frais 184 183 . 1 36 . 4% haut 1017 293 . 5 25 . 0% historique 620 30 . 7 87 . 4% plein 844 354 17 . 1% populaire 457 52 47 . 9% r?gulier 181 112 . 5 32 . 6% sain 129 102 . 4 40 . 3% secondaire 195 51 . 7 53 . 8% sensible 425 112 . 6 29 . 9% simple 105 11 42 . 1 41 . 3% strict 220 92 . 2 45 . 5% s?r 64 51 42 . 6 45 . 9% traditionnel 447 20 . 5 89 . 5% utile 359 92 . 4 42 . 9% vaste 368 62 . 1 42 . 4%
Average 434 14.1 2.3 46.4%
Verbs FS HMF Sarr?ter 916 153 23 . 9% comprendre 2 145 132 . 8 32 . 6% conclure 727 162 . 4 45 . 5% conduire 1093 152 . 3 38 . 2% conna?tre 1635 162 . 2 40 . 1% couvrir 543 223 . 3 33 . 3% entrer 1258 393 . 7 26 . 6% exercer 698 81 . 5 59 . 5% importer 576 82 . 6 27 . 6% mettre 52 46 140 3 . 7 42 . 2% ouvrir 9194 13 . 8 26 . 0% parvenir 654 82 . 3 36 . 7% passer 2556 844 . 5 15 . 8% porter 234 7594 29 . 4% poursuivre 978 162 . 7 36 . 2% pr?senter 2142 182 . 6 40 . 1% rendre 1990 272 . 9 46 . 4% r?pondre 252 991 78 . 3% tirer 100 24 73 . 9 28 . 9% venir 3797 333 . 2 24 . 9%
Average 1688 47.4 3.1 37.2%
Table 8: target word frequency ( F ) , average number of senses ( S ) , sense repartition entropy ( H ) and baseline accuracy ( Most Frequent
Sense : MFS).

