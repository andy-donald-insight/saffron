Lexicalized Hidden Markov Models for Part- of-Speech Tagging 
Sang-Zoo Lee and Jun-ichi Tsujii
Del ) art Inent of Inforlnation Science
Graduate School of Scien(:e
University of Tokyo , Hongo 7-3-1
Bunkyoku , Tokyo 113, Ja , l ) 3, iI
lee , tsujii((~is.s.u-tokyo.ac.jp
Hae-Chang Rim
Det ) artment of Computer Science
Korea Ulfiversity
i5-CaAnam-Dong , Seongbuk-C~u
Seoul 136-701, Korea

Abstract
Since most previous works t br HMM-1 ) ased tagging consider only part-of sl ) eechint brmation in contexts , their models ( : minor utilize lexical in-forlnatiol ~ which is crucial tbrresolving some morphological tmfl  ) iguity . In this paper we introducemlifor mly lexicalized HMMs fin:i  ) art-of st ) eech tagging in 1 ) oth English and \] ( or e , an . 
The lexicalized models use a simplified backoff smoothing technique to overcome data Sl  ) arse-hess . In experiment ; s , lexi ( : alized models a ( : hieve higher accuracy than non-lexicifliz ( ~d models and thel ) ack-off smoothing metho ( lmitigates data sparseness 1 ) etter (  ; ban simple smoothing methods . 
1 Introduction 1) arl ; -Ofsl ) e ( : ( 'h ( POS ) tagging is al ) ro ( : essill which al ) rOl ) (' . r \])() S l ; ag is assigned to e a (: h w or ( lin raw tex ( ; s . Ev ( ' n though morl ) h ( )logi ( : all yam-l ) iguous words have more thnn one P ( )S tag , they l ) elong to just one tag in a colll ; ex(; . ' J ~ ore solve such ambiguity , tagger slmve to consult various som'ces of inibrmation such as lexica \] i  ) retbrences ( e . g . without consulting context , table is more probably an ( mn than a . ver ) or an adje (: t ; ive ) , tag ngram context ; s(e . g . after a non-1) ossessiv (: pronoun , table is more l ) robal ) ly a verb than a . nmmor an adjective . , as in th , eytable an amendment ) , word n-grain conl ; e . xl ; s(e . g . 
betbrelamp , table is more probal ) ly an adjective than ~ noun or ~ verb , as in I need a table lamp ) , and so on ( Lee et al ,  1 . 999) . 
However , most previous HMM-1 ) ased taggers consider only POS intbrmation in contexts  , and so they C~I ~ I l II ( )t capture lexical infi ) r-nm tion which is necessary for resolving some mort  ) hological alnbiguity . Some recent , workslm vere l ) orted thai ; tagging a ( ' curacy could l ) eiml ) roved 1 ) y using lexic Mint brnml ; ion in their models such as the transtbrmation -based patch rules  ( Brill ,  1994) , the ln~txinnunentropy model(lIatn~q ) arkhi ,  1996) , the statistical ex-ical ruh:s ( Lee et al ,  1999) , the IIMM considering multi-words ( Kim ,  1996) , the selectively lexicalized HMM ( Kim et al ,  1999) , and so on . 
In the l ) revious works ( Kim , 1996) ( Kim et al ,  1999) , however , their It MMs were lexicalized se-h : ctively and resl  ; rictively . 
\] n this l > al ) erw (' . prol ) ose a method of uniformly lcxicalizing the standard IIMM for part-of speech tagging in both English and Korean  . 
Because the slm rse-da . ta problem is more serious in lexicMized models ttl ~ llill the standard model  , a simplified version of the wellknown back-oil ' smoothing nml  ; hod is used to overcome the . 1) rol ) lem . For experiments , the Brown cor-pus ( Francis ,  1982 ) is used l brEnglish tagging and the KUNLP ( : or lms ( Lee ( ' tal . , 1999) is used for Kore , an tagging . TimeXl ) criln ( ; nl ; ~ t\]results show that lexicalized models l ) erform better than nonlexicalized models and the simplified backoff smoothing technique can mitigate data sparseness betl  ; erthans ilnple smoothing techniques . 
2 Tile " standard " HMM
We basically follow the not ~ ti ( m of ( Charniak et al , 1993) to describe Bayesian models . In this paper , we assume that wI,'w ~, . . . , w~0 is a set of words , tt , t'2, .   .   .   , t ; is a set of POS tags , a sequence of random variables l'lq , ,~ = l~q lazy . . . I'E ~ is a sentence of n words , and a sequence of random w ~ riables T1 , , , = 7 ~ T , 2 . . . TT ~ is a sequence of nPOS tags . Because each of random wtrb flfles W can take as its value any of the words in the vocabulary  , we denote the value of l'l ( i by wimMal mrticular sequence of w fluest brH ~ , j ( i < j ) by wi , j . In a similar w l . t y , we denote the value of Tibyl , i and a particular generality , terms wi , j and ti , j(i>j ) are defined as being empty . 
Tile purpose of Bayesian models for POS tagging is to find the most likely sequence of POS tags for a given sequence of ' words  , as follows := arglnaxPr(T ,   , n = - IW , , , , = w , ,  , dtl , n Because l'ef hrence to the random variables the lnselves can  1  ) eoulitted , the above equation be colnes : T (' w l , n ) = argmax Pr(tl , n\[wl , ,z ) (1) ~' l , ~ tNow , Eqn . 1 is transtbrnled into Eqn . 2 since
Pr(wl , n ) is constant for all tq,~,
Pr(l.j,n , wl , n)
T (*/ q,n)--argmax t, . . . . Pr('wl , n ) = arDnaxP , '(tj , ,~ , w , , , ,) (2) tl , n Then , tile prolmbility Pr(tL , z , wl , n ) is broken down into Eqn . 3 by using tile chain rule . 
fl(Pr(ti , t\],i-l , Wl , i-1))
Pr(tl , n , ~q , r , ,) = x Pr(/~i \[ tl , i , ~Vl , i-l )   ( 3 ) i = l Because it is difficult to compute Eqn .  3 , the standard It MM simplified itt ) 3 ; making a strict Markov assumption to get a more tract ~ d  ) let brm . 
Pr(t l , , , , W l , n ) ~ x P r ( w i I t d (4) i = l
I51 the standard HMM , the probability of the current tag ti depends oi5 only the previous K tags ti-K , i-1 and the t ) robability of ' the current word wi depends on only the current ag  1  . 
There ibre , this model cannot consider lexical information in contexts  . 
3 Lexicalized HMMs
In English POS tagging , the tagging unit is a word . On the contrary , Korean POS tagging prefers a morpheme 2 . 
1Usually , K is determined as 1 ( bigram as in ( Charniak et al , 1993)) or 2 ( trigram as in ( Merialdo ,  1991)) . 
2The main reason is that the mtmber of word-unit tags is not finite because I  ( orean words can be ti ' eely and newly formed l ) y agglutinating morphemes ( Le et al . , 1999) . 

Flies/NNS Flies/VBZ like/CS like/IN like/JJ like /VB a/A ~ a/IN a/NN t to wer/NN flower/VB  .  /  . 

Figure 1: A word-unit lattice ot ' " F lies like a \[ l ower  . " Figure 1 shows a word-unit lattice of an Eil-glish sentence  , " Flies like a flow c'r . " , where each node has a word and its word-unit tag . Figure 2 shows a morpheme-unit lattice of a Korean sentence  , " NcoNeunt Ial Suiss Da . " , where each node has a morphenm and its morI ) heme-unit tag . In case of Korean , transitions across a word boundary , which are depicted by a solid line , are distinguished fl ' om transitions within a word  , which are depicted by a dotted line . ill both cases , sequences connected by bold lines indicate the most likely sequences  . 
3.1 Word-unit models
Lexicalized HMMs fbr word-unitagging are defined 1  ) ymaking a less strict Markov assmnp-tion , as t bllows : A(T ( K , j ) , W ( I ; j )) ~ Pr(tl , ,~ , wl , n ) i=\]xPr(wiIti-L , i , wi-I , i-1) Ill models A(T ( K , j ) , 14/(Lj )) , the probability of the current tag ti depends on both tile previous If tags ti-K  , i-i and the previous d words wi-j , i-i and the probability of the current word ' wi depends on the current ag and the previous L tags ti_L  , i and the previous I words wi-l , i - ~ . 
So , they can consider lexieal inforination . In experiments , we set If as 1 or 2 , Jas 0 or K , Las1 or 2 , and 1 as 0 or L . If J and I are zero , the above models are nonlexicalized models . Otherwise , they are lexicalized models . 

Neo/NNI " Ncol/VV ?.4
No ' an ~ PXN cun/EFD
H~d/NNCCH d/NNBUH~(VV\]Ia/VX
S'a/NNCGSu/NNB Giss/\zJiss/VX
Da/EFFDa/EFC ?"' OO oo ,,, j~g_._.--"-./ss.

Figure 2: Amorl ) heme-unit latti(:(; of " N , oN , ' unllal S'ui . ssl ) a . "( = You (: and oit . ) rlf in a lexicalized model A(~/(9 , 2) , lI('J , 2)) , fin " ex-mnl ) lc , the t ) robal ) ility of a node " a/AT " of tlm most likely sequen ( : e in Figure 1 is calculate ( tast bllows : l'r ( AT'IN M&vILF li ( : , ~ , lit , : c ) ? tq?xPr(at:'1 ~ , NNS , VH , 1l ' ~ , c . s , lil , : c ) 3 . 2 Morphelne-unit models l) ; ~ yesian models for lnOrl ) heme-unit tagging t in ( t the most likelyse ( lueame of mor\] ) h ( mms and corresponding tags fi ) r ; ~ given sequence of words , as follows : ~' (11) , 1 , ,) = al'glll;XX Pr ( c l , v , , ?/~ , , , uI ' 1 , , , ,~) (6)
Cl ~ uflltl , , t , ra-axPr(c , , , , , .  ' ,,,,, ,,,) (7)

In the above equations , u ( _>' n ) denotes the lll Illl ) cr of morph ( mms in a Se ( ltlell (  ; e('or re-spending the given words equ('ncc , c denote samor l ) heme-mf it tag , ' m . denote samor l ) heme , aimp denotes a type of transition froln the pre -vious tag to the current ag  . p can have one of two values , "#" denoting a transition across a word bomldary and "+" denoting a transition within a word  . Be (- ause it is difficult to calculate Eqn .  6 , the word sequence term ' w ~ , , , is usually ignored as illEqn .  7 . Instead , we introduce p in
Eqn .7 to consider word-spacing 3.
Tile probability Pr(cj , ~ L , P2 , u , ' m , ~  , u ) is also broken down into Eqn . 8 t ) 3 r using the chain rule . 
Pr(c  ~ , , , , P2 , , ,   , ' m ,   ,   , , , ,) f l ( \]) r(ci , Pi\[cl , i-l , P2 , i-l , 'lnl , i-l )) ~- XP1"(1~'1 , i\[('d , i , I , 2 , i , 17tl , i_\])(8) i=1\]3 (' caus('Eqn . 8 is not easy to ( ; omlmte ~ it is sinll ) lified by making a Marker as smnt ) tion to get ; a more tractal ) le for lll . 
In a similar way to the case of word-unit ; tagging , lexicalize ( tHMMs form or l ) heme-mf it tagging are defined by making a less strict Markov as sunq  ) tion , ast blh ) ws : A(C\[ , q(K ,  . \]), AJ\[sI(L , 1))1 = Pr(c \],,,, p2,,,,'mq,~,)
I'r ( c\[ , pdI , , I , i- , Uc/--lC/-'(!))~=~ , xl'r(milcil , ,i\[ , >-L+l , ,i\] , 'mi-l , i--I ) In models A(C\[ . q(tc , ,I) , M\[q(L , Q ) , the 1 ) robal ) il-ity of the ( : urrent mori ) heme tag ci depends on l ) oth the 1 ) revious K : agsCi_K , i_1 ( oi ) tion-all y , th('ty l)eS of their transition Pi-K~1 , i -~) a . n(l the1) revious,\]morl ) hemesH ~, i_ . l , i_1 all ( 1 the probability of the current mort ) heine'm , i(t(>1) en(lson the current , tag and I : he previous L tags % l , ,i ( optional\]y , the typ('~s of their transition Pi-L-t-I , i ) and the 1) revious Imor l ) hemes ? lti--l , i-1 . ~() ~ t\]l(ly('&ll&lSO(-onsid( , rh;xi(-alinformation . 
In a lexicalized model A(C, . (~#) , M ( ~ , 2 ) ) whea:e word-spa ( : ing is considered only in the tag prob-al ) ilities , for example , the 1) rol ) al ) ility of a no d ( ; " S'u/NNBG " of the most likely sequence in Fig -urc  2 is calculated as follows:
Pr(NNBG , #\[ Vl 4 EFD ,  + , Ha , l)xPr(gu\[VV , EFD , NNBG , Ha , l ) 3 . 3 Parameter es t imat ion In supervised lcarning ~ the simplies t parameter estimation is the maximum likelihood  ( ML ) cs-timation ( Duda et al ,  1973 ) which lnaximizes the i ) robal ) ility ot ! a training set . The ML estimate of tag ( K + l ) -grami ) robal ) ility , PrML(f;i\[t , i-K , i-i ) , is calculated as follows : PPr(tilti_ir , i_j)__\]:q(ti-i ( , i ) (10)
MLF q ( ti-lGi-l ) a Most 1 ) rcvious HMM-bascd Korean tagg crs except ( Kim et al , 1998) did not consider word-spacing . 
483 where the flmction Fq ( x ) returns the fl : equency of x in the training set . When using the maximum likelihood estimation , data sparsenessimore serious in lexicalized models than in nonlexicalized models because the former has even more parameters than the latter  . 
In ( Chen ,  1996) , where various smoothing techniques was tested for a language model by using the perplexity measure  , a backoff smoothing ( Katz ,  1987 ) is said to perform better on a small traning set than other methods  . 
In the backoff smoothing , the smoothed probability of tag ( K + l ) -gram PrsBo ( ti\[ti-l ~ , i-l ) is calculated as t bllows:
Pr(ti\[ti-I ( , i -~) = , 5'1 ~20 dr PrML(ti\[ti-I ( , i-1) " if r > 0 (11) c~(ti-K , i-1) Prsso(ti\[ti-K+l , i-l ) if r = 0 where r = F q(ti_ , i ) , r *= ( r + 1) ' nr + l7 ~ , rr*(r + l . ) x ~% . + ldr~Flt l1-(r + l)xm . + lnln , . denotes then mn be r of ( K + l ) -gram whose frequency is r , and the coefficient dr is called the discount ratio  , which reflects the Good-~lhtring estimate ( Good ,  1953) 4 . Eqn . 11 means that Prxgo(ti\[ti-K , i-l ) is under-etimated by dr than its maximum likelihood estimate  , if r > 0 , or is backed off by its smoothing term Prsuo ( ti\[ti-K+j , i-l ) in proportion to the value of the flmction ( ~ ( ti-K , i-t ) of its condition altermti-K , i-1 , if r = 0 . 
However , because Eqn . 11 requires complicated computation in ~( ti-l ( , i-1) , we simI ) lify it to get a flmction of the frequency of a conditional term  , as t bllows : ct(F q(ti-K , i-1) = f ) = E\[Fq(ti-I ( , i-1) = f\]AxE7-oE\[Fq(ti-K , i-1)-=f\]whereA=1-~Pr(tglti-/c , i- , ) , 
SBO ti--K,i ~ r > O
E\[Fq(ti-g,i-1) = f\] =
SP\]to(ti\[ti-K+l , i-1) ti-K+Li , r=O , F q(ti-K , i-1) = f'(12) In Eqn . 12, the range of . f is bucketed into 74K at z said that d , . = i if r > 5 . 
regions such as f = 0 ,  1 ,  2 ,  3 ,  4 ,   5 and f > 6 since it is also difficult to compute this equation tbrall possible values off  . 
Using the formalism of our simplified backoff smoothing  , each of probabilities whose ML estimate is zero is backed off by its corresponding smoothing term  . In experiments , the smoothing terms of Prsl~o(ti\[ti-K , i-l , ~t)i- , l , i-l ) are determined as follows:
PI'sBo(ti\[ti-Ii+l , i-h ) if K > 1 , d > 1wi_j + ~ , i_~
Prsuo(ti if K >_1, d=1
Prs13 o(ti\[ti-K+Li-l ) if K > 1, J = 0
PrAD(ti ) if K = 0, J = 0
Also , the snloothing terms of ' Pl's\]~o(witi_L , i , Wi_l , i_1) are determined as follows : \[ Prst~o(wi
Prsuo(wi
Prs , o(wi

PrA . Oi)ti-L + ~ , i , ) if L_>1 , I > 1 i l ) i-I + l , i-Iti-L , i ) if L_>1 , I=1ti-L+Li ) if L >_1 , I = 0 if L = 0 , I--=0 il L = -1 , I = 0In Eqn . 13 and 14 , the smoothing term of a unigram probability is calculated by using an additive smoothing with  5  =  10   . 2 which is chosen through experiments . The equation for the additive smoothing ( Chen , 1996) is as t bllows:
Fq(ti-t(,i ) + 5
AD~tl(Fq(ti-lf , i ) + 5)
In a similar way , the smoothing terms of parameters in Eqn . 9 ~ redetermined . 
3 . 4 Model decoding h'om the viewpoint of the lattice structure  , the t ) roblem of POS tagging can be regarded as the problem of finding the most likely path ti'om the start node  ( $/$ ) to the end node ( $/$ )  . The Viterbi search algorithm ( Forney ,  1973) , which has been used for HMM decoding , can be effectively applied to this task just with slight modification  5  . 
4 Experiments 4.1 Environment
In experiments , the Brown corpus is used tbrEnglish POS tagging and the KUNLP corpus '% uch modification is explained in detail in  ( Lee ,  1999) . 





Brown KUNLP 167,115 53,885 15,211 826 51 . 64 3 . 4:1 61 . 54% 26 . 72%
NW Number of words . NS Number of sen-tcnccs . NT Numl ) ' . r of tags ( nl or pheme-unit tag for KUNLP ) . DA Degree of mn biguity(i . e . the number of tags per word ) . RUA1\] . atiof mlanlbiguous words . 
Table 1: Intbrmational ) out the Browneortms and the KUNLP to rtus
Inside-test () utside-;(;st
ML 95.57 94.97 = 1)-AD(a-\](-\])
AD(~=12T ) -
ADO ;--= a)
A \])(( ;=
AD(5 =
AD(5 =
AD(5=\]\]-~7) -
AD(5=93 . 92 93 . 02 95 . 02 94 . 79 95 . 42 95 . 08 95 . 55 95 . 05 95 . 57 94 . 98 95 . 57 94 . 94 : 95 . 57 94 . 91 95 . 57 94 . 89 95 . 57 94 . 87
SBO 95.55 95.25
ML Maximum likelihood estimate ( with simple smoothing )  . A \]) Add it iv(~smoothing . 
SBOS in lllitic d1) ack-off smootlfing.
lal)l ( , 2: lagging accura (: y(fA(C(\]:o) , M0:0)) for Kore~mPOS tagging . Table 1 shows some intbrmation M ) out 1 oth ( : ori ) or a ~ . Each of them was segmented into two parts , the training set of 90% and the test ; set of 10% , ill . the way that each sentence in the test set was extra'tc  , d\]i'()ln every1(senl ; ellce . A (: cording to Tabl(!1 , Ko-reml is said to 1 ) elllOre ( litli ( : ult to disambiguat (  ; tl\]ml English . 
We assmne " closed " w mabulary for English and " open " vocabulary for Korean since we do not h ~ ve any English morphological mmly zer consistent with the Brown corlms  . Therefore , for morphological mmlysis of English , we just a Note that some sent cnc (' . s , which have com losite tags ( such as " HV+TO " in " hafta " )   , "ILLEGAL " tag , or " NIL " tag ~ were remov ( Mfronlthe Browncorl ) us and tags with "*" ( not ) such as " BEZ *" were r ( ' , l ) la ( : ( ~ ( t1 ) y ( : of r~sto\]tt lingtas without "*" such as " BEZ " . 
2M 1.5M
IM (.5 M
IIII-ML
AD . x.-
SBO1 , 02  , 01  , 02  , 01  , 02  , 0  , ( 0  , 01  , ( 1  , 02  ,  ( 2  , 0\]'--II\[II . 99  . 98  . 97_I ~ L~~_?1 , 02  , 01  , 023 13 2 , 1 ( , 0  ,  1  , 01  , 1 2  , 02  , 0  . 98  . 97 2)6  . (,):, ? vii,-rJ--
AD '?-- . :)4 SBO-~---1 , 02 , 01 , 02 , (11 , 02 , 0 o , 00 , 01 , 01 , 02 , 02 , 0 1 . 

IIIII it ~
IIII Itt IIIII1 , 11 , 11 , 01 , 12 , 01 , 12 , 22 , 22 , 22 , 21 , 02 , 01 , 12 , 2 0 , 01 , 01 , 12 , 0 1 , 1 1 , 1 0 , 01 , 0 1 , 12 , 0 2 , 2 2 , 2 2 , 2 2 , 2(a )  #of paraln ; ters
M\],-D-
AD-?---
SB ( )
IIIIIIIIIII 1 ,  1 1 , l1 , 0 1 , 1 2 , 01 , 1 2 , 22 , 22 , 22 , 2 1  , ( 2 , 01 , 12 , 2 0 , 01 0 1 , 12 , 01 , 11 , 10 , 01 , 01 l 2 , 02 , 22 , 22 , 22 , 2 (1) Inside-test 1 , 11 , I 1 , 01 , 12 , 01 , I 2 , 22 , 22 , 22 , 21 , 02 , 01 , 12 , 2 0 01 , 01 , 12 , 01 , 11 , 10 , 01 , 01 , 12 , 02 , 22 , 22 , 22 , 2 ( c ) Ouiside-test 1 , 02 , 01 , 02 , 01 , 02 , 0 1 , 11 , 11 , 01 , 12 , 01 , 12 , 22 , 22 , 22 , 21 , 02 , 01 , 12 , 2 0 , 00 , 01 , 0 1 , 02 , 02 , 0 0 , 01 , 01 , l2 , 01 , 11 , 10 , (11 , 11 , I 2 , 02 , 22 , 22 , 22 , 2(d ) inside vs . outside-test in SBO
Figure 3: Results of English tagging corpus . In case of Korean , we have used a Korean morphological analyzer ( Lee , 1999) which is consistent with the KUNLP corpus . 
4.2 Results and evaluation
Table 2 shows the tagging accuracy of the simplest HMM , A(C(l:0) , M(0:0)) , for Korean tagging , according to various smoothing methods7 . Note that ML denotes a simple smoothing method where ML estimates with probability less than  10   -9 are smoothed and replaced by 109? Because , in the outside-test , AD ( d = 10-2 ) performs better than ML and kD ( a ? 102 )  , we use 5=10-2 in our additive smoothing . According to Table 2 , SBOI ) ertbrms well even in the simplest HMM . 
Figure 3 illustrates 4 graphs ' about the results of English tagging : ( a ) the number of parameters in each model ,   ( b ) the accuracy of each model t br the training set ,   ( c ) the accuracy of each model for the test set , and ( d ) the accuracy of each model with SBO t br both training and test set  . Here , labels in x-axis sI ) ecify models K ,   , 1 in the way that ~ denotes A(T ( \]; , j ) , W ( Lj )) . 
Therefore , the first 6 models are nonlexicalized models and tile others are lexicalized models  . 
Actually , SBO uses more parameters than others . The three smoothing methods , ML , AD , SBO , perform well for the training set ; since the inside-tests usually have little data sparseness  . On the other hand , t br the unseen test set , the simple methods , ML and AD , cannot mitigate the data sparseness problem , especially in sophisticated models . However , our method SBO can overcome the problem , as shown in Figure 3(c ) . Also , we can see in Figure 3 ( d ) that some lexicalized models achieve higher accuracy than nonlexicalized models  . We can say that the best lexicalized model , A(T (1 , ~) , W(1 , 1)) using SBO , improved the simple bigram model , A(T ( L0) , W(0 , 0)) us-?~0 mgSBO , from 97 . 19>/oto 97 . 87 ~( the error reduction ratio of 24 . 20%) . Interestingly , some lexicalized models ( such as A(T (1 , 1) , W-(0 , 0)) and A(T (1 , 1) , W (1 , o ))) , which have a relatively small number of paranmters  , perform better than nonlexicalized models in the case of outside-tests using SBO  . Untbrtunately , we cannot ex-r Inside-test means an exper iment on the training set itself and outs ide-test an experiment on the test set  . 
.96  . 94 ~ ?  .   . ~ uu . X ?"""~'" . ~1%~ ~  . 92  . 90
ML ~ k .88 AD . x.-
SBO . 86 IIIIIIIIII IIIf IIIIIII 1 , 02 , 01 , 02 , 01 , 02 , 0 1 , 11 , 11 , 01 , 12 , 01 , 12 , 22 , 22 , 22 , 21 , 02 , 01 , 12 , 2 0 , 00 , 01 , 01 , 02 , 02~0 0 , 01 , 01 , 12 , 01 , 11 , 10 , 01 , 01 , 12 , 02 , 22 , 22 , 22 , 2  ( a ) Outside-test ? 97 IIIII ~ IIId ~ IIItII - ~
C,M + ? . 966C ~, / l ~/+ . 9(;2 ~ . ~ , - ~ I~X\[\]+1 , 02 , 01 , 02 , 01102 , 0 1 , 11 , 11 , 01 , 12 , 01 , 12 , 22 , 22 , 22 , 21 , 02 , 01 , 12 , 2 0 , 00 , 01 , 01 , 02 , 02 , 0 0 , 01 , 01 , 12 , 01 , 11 , 10 , 01 , 01 , 12 , 02 , 22 , 22 , 22 , 2  ( b ) Considering word-spacing + x?x\[\]??
Illllll
Figure 4: Results of Korean tagging pect the result of outside-tests from that of inside-tests because there is no direct relation t  ) etween them Figm:e 4 includes 2 graphs about the results of Korean tagging : ( a ) the outside accuracy of each model A ( C ( K , j) , MiL , I ) ) and ( b ) the outside accnracy of each model A ( C\[s\] ( ~-g )  , M\[s\](L , 0 ) with/without considering word-spacing when using SBO  . Here , labels in K , J de-x-axis specify models in the way that , 7 , , notes A(C\[s\](K , j) , i~/I\[ . ~\]( Lj )) and , t brexample , C ,   , Min(b ) denotes k(C ~ ( , r , j ) , M(L , r)) . 
As shown in Figure 4 , the simple methods , ML and AD , cannot mitigate that sparse-data problem , t ) ut our method SBO can overcome it . Also , some lexicalized models per-tbrm better than nonlexicalized models  . On the other hand , considering word-spacingives good clues to the models sometimes  , but yet we cannot sw what is the best ww . From the experimental results , we can say that the best model , A(C (9 , 2) , M(2 , 2)) using SBO , improved the previous models , A(C (1 , 0) , M(o , o )) us-ML ( Kim et al ,  1998) , t ' ronl 94 . 97% and 95 . 05% to 96 . 98% ( the error reduction ratio of 39 . 95% mid 38 . 99%) respectively . 
5 Conclusion
We have 1 ) resented unit brmly lexicalized HMMs for POS tagging of English and Korean  . In the models , data sparseness was et lix : tively mit-igated by using our simplified ba  ( - k-ofl " smoothing . From the ext ) eriments , we have ol ) served that lexical intbrmation is use fiflfi ) rPOS tagging in HMMs , as is in other models , and ore " lexicalized models improved nonlexicalized models by the error reduction ratio of  24  . 20% ( in English tagging ) and 39 . 95% ( in Korean tagging ) . 
G(' . nerally , them fi form extension of models requires ral ) id increase of parameters , and hence suffers fl ' om largest or agea . nd sparse data . l  ~ . e-cently in many areas where HMMs are used , many eflbrts to extend models non-mf it brmly have been made  , sometimes resulting in noticeable improvement . For this reason ~ we are trying to transfbnnour uniform models into non-mliform models  , which may 1 ) e more effective in terms of both st ) ace ( : omt ) h ' ~ xity and relial ) le estimation of I ) areme ; ers , without loss of accuracy . 
References 12 . Brill .  1994 . Some Advances in ~ l?ansformation-Based Part of St  ) eech ~ Dtgging . In P~ve . of the 12th , Nat'l Cm ? . on Art'tficial hdelligencc(AAAI- . 9~), 722-727 . 
E . Charniak , C . Hendrickson , N . Jacobson , and M . Perkowitz .  1993 . l ~3 quations for Part-of Speech % ~ gging . In Proc , of the 11th , Nat'l CoT ~: f . on Artificial Int clligence ( AAAL93), 784-789 . 
S . F . Chen .  1996 . Building Probabilistic Models for Natural Language  . Doctoral Dissert ~ tion,
Harvard University , USA.
R . O . Duda and R . E . Hart .  1973 . PatternCI as-s'~fication and Scene Analysis . John Wiley . 
G.D . Forney . 1973. The Viterbi Algorithm . Ill
Proc . of the IEEE , 61:268-278.
W . N . Francis and H . Ku~era .  1982 . Frequency Analysis of English Usage : Lczicon and GT nmmar  . Hought on Mitlt in Coral ) any,
Boston , Massachusetts.
I . J . Good .  1953 . " The Population Frequencies of Species and the Estimation of Population Parameters  , " Ill Biometrika ,  40(3-4):237-264 . 
S . M . Katz .  1987 . Estimation of Probabilities fronl Sparse Data for the Language Model Component of a Speech Recognizer  . In IEEE Transactions on Acoustics , Speech , and Signali ' rocessing ( ASSl') ,  35(3):400-401 . 
J.-\]). Kim , S.-Z . Lee , and H.-C . Rim . 1998.
A Morpheme-Unit POS Tagging Model Considering Word -Spacing  . Ill Pwc . of th . e I0 th National CoT ~: fercnce on Koreanh ~: formation
PT veessing , 38.
J.-D . Kim , S.-Z . Lee , and H.-C . Rim . 1999.
HMM Specialization with Selective Lexicalization . In Pwe . of the joint SIGDATCo~l:h':rence on Empirical Methods in Nat-'aral Language Processing and Very La'qtc 
Co'rpora ( EMNLP-VLC-99), ld4-148.
J . -H . Kim .  1996 . Lcxieal Disambig ' aation with Error-Driven Learning  . Doctoral Dissertation , Korea Advanced Institute of Science and
Te . clmology ( KAIST ), Korea.
S . -H . Lee .  1995 . Korean POS Tagging System Considering Unknown Words  . Master Thesis , Korea Advanced Institute of Science and
Teclmology ( KAIST ), Korea.
S . -Z . Lee, . I . -D . Kim , W . -H . Ryu , and H . -C . Rim .  1999 . A Part-of Speech Tagging Model Using Lexicall / . ules Based on Corlms Statistics . In Pwc . of the International Conference on Computer \] ) 'lvcess in 9 of Oriental
Languages ( lCCPOL-99), 385-390.
S . -Z . Lee .  1999 . New Statistical Models for Automatic POS Tagging . Doctoral Dissertation , l(orea University , Korea . 
B . Merialdo .  1991 . Tagging Text with a Prol ) - a bilisl ; icModel . In P~vc . of the International Conference on Acoustic , Spccch and Signal
Processing ( ICASSP-91), 809-812.
A . Ratnap ~ rkhi .  1996 . A Maximum Entrol ) y Modelt br Part-of-Speech Tagging . In Proe . 
of the Empirical Methods in Natural Language P ~vcessi'ng Co'a:fercnce  ( EMNLP-9b' )  ,  133-142 . 

