Proceedings of the 22nd International Conference on Computational Linguistics ( Coling 2008), pages 273?280
Manchester , August 2008
Measuring Topic Homogeneity and its
Application to Dictionary-based Word Sense Disambiguation
Ann Gledson and John Keane
School of Computer Science , University of Manchester
Oxford Road , Manchester , UK M13 9PL
{ann.gledson,john.keane}@manchester.ac.uk

Abstract
The use of topical features is abundant in
Natural Language Processing ( NLP ), a major example being in dictionary-based
Word Sense Disambiguation ( WSD ). Yet previous research does not attempt to measure the level of topic cohesion in documents , despite assertions of its effects . This paper introduces a quantitative measure of Topic Homogeneity using a range of NLP resources and not requiring prior knowledge of correct senses . Evaluation is performed firstly by using the
WordNet::Domains package to create word-sets with varying levels of homogeneity and comparing our results with those expected . Additionally , to evaluate each measure?s potential value , the homogeneity results are correlated against those of 3 co-occurrence/dictionary-based WSD techniques , tested on 1040
Semcor and SENSEVAL subdocuments.
Many low-moderate correlations are found to exist with several in the moderate range ( above .40). These correlations surpass polysemy and sense-entropy , the 2 most cited factors affecting
WSD . Finally , a combined homogeneity measure achieves correlations of up to .52.
1 Introduction
Topical features in NLP consist of unordered bags of words , often the context of a target word or phrase . In WSD for example , the word bank in the sentence : ? If you?re OK being tied to one ? 2008. Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unported license ( http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved.
bank , you can get al your financial products there .? might be assigned its monetary sense , based on the occurrence of the term financial.
Often referred to as ? topical features ?, these are an important part of many NLP methods , such as WSD ( Yarowsky 1995) and Topic Area Detection ( TAD ) ( Hearst 1997). Furthermore , in the
SENSEVAL WSD competitions they are included in the highest performing systems.
We assert that the effectiveness of topical features in NLP depends upon the level of topic homogeneity in the text . To illustrate two extremes : the disambiguation of the word bank might be more difficult if occurring in i ) a work of fiction describing a series of activities which includes the phrases : ? stroll along the river ? and ? pick up her cheque book ?; than in ii ) a news report on a bank in financial difficulty ( a topically homogeneous text).
This paper contributes a set of unsupervised Topic Homogeneity measures requiring no knowledge of correct senses . A variety of NLP resources are utilized and a set of evaluation methods devised , providing useful results . The paper is structured as follows : Section 2 outlines related work ; Section 3 describes the experiments focusing on the resources used and their associated homogeneity measures ; in Section 4 three evaluation methods are described , including a WSD task-based evaluation ; Conclusions and future work are presented in Section 5.
2 Related Work
TAD research ( Hearst 1997) has revealed that word patterns within texts can be used to locate topic areas . Salton and Allan (1994) distinguish between homogenous texts , where the topic of the text might be ascertained from a small number of paragraphs , and heterogeneous texts , where topic areas change considerably . Unfortunately , this research only detects topic homogeneity at inter-paragraph level . We assert that of a text can vary from one text to another and that this is likely to affect the usefulness of topic features in NLP tasks . Caracciolo etal (2004) also indicate that documents can vary in topical structure , mentioning text homogeneity as an important feature but they do not evaluate these findings explicitly.
Lexical cohesion ( Halliday and Hasan 1976) is the analysis of the way the phrases and sentences of a text adhere to form a unified , comprehensible whole . Morris and Hirst (1991) describe a set of relationships between wordpairs , which include their likelihood to cooccur . These are used to create Lexical chains , which are sequences of related words in a document . The 2 main reasons for creating these chains are that they are ? an aid in the resolution of ambiguity ? and they determine ? coherence and discourse structure ?. We propose the use of lexical chains , alongside other methods , to measure document homogeneity.
Measuring the semantic relatedness between words is an important area in NLP , and has been used in areas such as WSD , Lexical Chaining and Malapropism Detection . Budanitsky & Hirst (2006) evaluate 5 such measures , based on lexically assigned similarities , and conclude that an area of future research should be the capture of ? non-classical ? relationships not found in dictionaries , such as distributional similarity . Weeds and Weir (2006) evaluate distributional similarities using document retrieval from the BNC.
Chen etal (2006) and Bollegala etal (2007) use web searchengine results . All of the reported similarity measures are between two words ( or texts ) only . We extend these to calculate the topic homogeneity of groups containing up to 10 words.
A small number of unsupervised WSD methods exist which take topic areas and/or document types into account ( eg Gliozzo etal 2004, McCarthy etal 2007), but these work at corpus level and do not measure the level of topical homogeneity in each text . An exception is the WSD work by Navigli and Velardi (2005) who report differing results for 3 types of text : unfocussed , mid-technical ( eg finance articles ) and overly technical ( eg computer networks).
3 Experiments
We propose the creation of a quantifiable measure of text homogeneity in order to predict the effectiveness of using cooccurrence features in WSD . In this initial set of experiments , documents are divided into simple ~50 content-word blocks , regardless of topic boundaries , to maximize the range of homogeneity levels in the texts and to nullify the effects of text length . The longterm objective is for documents to be divided into topic areas at the preprocessing stage , using a TAD algorithm . We also propose to take a single homogeneity measure of each entire subtext , as opposed to using a sliding window approach , as the latter would be overcomplex and comparable with a similarity-based WSD process.
3.1 Input Texts
The documents of Semcor2 and SENSEVAL (2 & 3) are used in the experiments . Each text is divided into subdocuments of approximately 50 content-words 3 . All documents are converted into Semcor 2.1 format.
From the resulting subdocuments , 1040 random Semcor texts and the entire set of 73 SENSEVAL 2 and 3 texts are used in the experiments.
3.2 Text PreProcessing
Non-topic words are found to have a negative effect on NLP methods using topic features , such as WSD ( eg Yarowsky 1993, Leacock etal 1998), and are therefore excluded from our topic homogeneity experiments . Unfortunately , no precise definition of non-topic words exists . Under ideal conditions , where correct senses are known , we define non-topic words as wordsenses appearing in over 25% of all Semcor texts and/or being marked as factotum in the WordNet Domains 3.2 package ( Magnini and Cavaglia 2000).
As the experiments described in this work assume no such prior knowledge , an approximation of the criteria used above is made . A J48 ( pruned ) decision tree is used to decide whether each word is nontopical . The input attributes were the PoS , sense-count , Semcor distribution ( SenseEntropy ) of its possible senses , whether all of the word?s senses are factotum , and the percent of Semcor documents containing that word.
The training and test data was the entire set of Semcor and SENSEVAL 2 and 3 English all-word task data and the minimum node size was set to 4000 instances to minimize the tree size and prevent overtraining . Using a 10-fold crossvalidation test mode the tree obtains 83% accura-2 Using the Brown-1 and Brown2 documents only 3 Splits are made as near as possible to the 50 content-word length whilst keeping sentences intact.
274 cy . The learned filter for selecting nontopical nouns and verbs is : ( All-Senses = Factotum ) || ( Corpus-Hit-Percent >25.0%) || (( Sense_Count > 1) && ( PoS = v )) || (( Sense_Count > 3) && ( SenseEntropy > 0.5668))
Upon entry into the system , each subdocument has all such words labeled as nontopical . The remaining words are labeled as top-ic-content . The confusion matrix output is shown in Table 1.

Classified As ? OTHER NONTOPIC
OTHER 15017 5768
NONTOPIC 4278 34292
Table 1: J48 Confusion Matrix
In addition , only nouns and verbs are considered in the experiments as these word types are considered most likely to contain topical information.
3.3 Homogeneity Measures
Five homogeneity measures have been created that cover a broad range of techniques for embodying topic-area information in natural language texts . This is to facilitate comparisons between different techniques and if such a variety of aspects is captured , it improves the likelihood of a successful combination of the methods to produce an optimised measure . Each takes a full preprocessed subdocument as input and outputs a single score.
Word Entropy
It is possible to capture topic homogeneity by using simple measures that require minimal reliance on external resources . Word entropy is considered as having the potential to reflect topical cohesion.
To measure WordEntropy , the frequency of each topic-content lemma of an input document d is obtained , and Entropy(d ) is measured using this set of frequencies , as follows : - ? i=1..n p(xi ) log2 p(xi ) [1] Where n is the number of different topic content lemmas in d , and p(xi ) is calculated as frequency(lemmai)/?j=1..nfrequency(lemmaj ) [2] As Entropy(d ) is affected by n and n varies from one document to another , Entropy(d ) is normalised by dividing it by the maximum possible Entropy calculation for d , that is if all lemmas had equal frequencies.
WordNet Similarities
WordNet::Similarities 1.04 ( Pedersen etal 2004) is publicly available software which uses aspects of WordNet to ? measure the semantic similarity and relatedness between a pair of concepts ?. The package can measure similarities between lemma pairs , where no knowledge of the correct sense or PoS is required . These similarities can be easily adapted to assist with the measurement of document homogeneity , by comparing similarities between sets of word-PoS pairs in the document.
Three WordNet Similarities homogeneity measures ( AvgSimMeasure ) are calculated for each document as follows : Step 1: Order the topic-content lemmas of the input text firstly in descending order of frequency , and then by first appearance in the text . Step 2: Take the first n lemmas from this list ( where n is all lemmas up to a maximum of 10) and add to FreqLemmas.
Step 3: Calculate the mean of all of the similarity measures between each pair of lemmas in Freq-
Lemmas . AvgSim can be defined as:
Mean(?i=1..n ? j=i+1..n SimMeasure(lemmai , lemmaj )) [3] Where SimMeasure(lemmai , lemmaj ) is the Word-Net::Similarity calculation between lemmai and lemmaj , where all allowable PoS combinations for the two lemmas when using the selected similarity measure are included.
The WordNet Similarity measures selected for use are Lesk , JCN4, and Lch ( see Pedersen etal (2004) and Patwardhan etal (2003) for details ), as each measure represents one of the 3 main algorithm types available : WordNet Gloss overlaps , information content of the least common subsumer and path lengths respectively.
Yahoo Internet Searches
The web as a corpus has been successfully used for many areas in NLP such as WSD ( Mihalcea and Moldovan 1999), obtaining frequencies for bigrams ( Keller and Lapata 2003) and measuring word similarity ( Bollegala etal 2007). Such reliance on Web searchengine results does come with caveats , the most important in this context being that reported hit counts are not always reliable , mostly due to the counting of duplicate documents . ( Kilgarriff 2007).
Using web-searches as part of the homogeneity measure is considered important to our experiments , as it provides up-to-date information on word cooccurrence frequencies in the largest available collection of English language docu-4 TheBNC information-content file is loaded.
275 ments . In addition , it is a measure that does not rely on WordNet . It is therefore necessary to produce a webbased homogeneity measure that limits the effects of inaccurate hit counts.
The SearchYahoo homogeneity measure is calculated for each document d as follows : Steps 1 and 2: Perform steps 1 and 2 described above ( WordNet Similarities ). Step 3: Using an internet searchengine , obtain the hit counts of each member of Freqlemmas . Step 4: Order the resulting Frequlemmas list of n lemma/hit-counts combinations in descending order of hitcounts and save this list to IndivHitsDesc . Step 5: For each lemma of IndivHitsDesc , save to CombiHitsDesc preserving the ordering . Step 6: For each member of CombiHitsDesc : CombiHitsDesci , obtain the hit counts of the associated lemma , along with the concatenated lemmas of all preceding list members of CombiHitsDesc ( CombiHitsDesc0 to CombiHitsDesc[i-1]). This list of lemmas are concatenated together using ? AND ? as the delimiter . Step 7: Calculate the gradients of the best-fit lines for the hitcounts of IndivHitsDesc and CombiHitsDesc : creating gradIndiv and gradCombi respectively . Step 8: SearchYahoo is calculated for d as gradIndiv minus gradCombi.
As SearchYahoo is taken as the difference between the two descending gradients , the measure is more likely to reveal the effects of the probability of the set of lemmas cooccurring in the same documents , rather than by influences such as duplicate documents . If the decline in hitcounts from IndivHitsDesc[i-1] to IndivHitsDesc[i ] is high , then the decline in the number of hits from CombiHitsDesc[i-1] to CombiHitsDesc[i ] is also expected to be higher , and the converse for lower drops is also expected . Deviations from these expectations are reflected in the final homogeneity measure and are assumed to be caused by the likelihood of lemmas cooccurring together in internet texts.
A webservice enabled searchengine was required to create a fully automated process . The Google searchengine hitcounts were less suitable , as they did not always decline as the number of query terms increased . This is perhaps because of the way in which Google combines the results of several searchengine hubs . The Yahoo web-services were therefore selected , as these produced the necessary declines for the measure to work.
Further evaluation of the Yahoo Internet searching homogeneity measure is presented in Gledson and Keane (2008), along with comparisons with similar methods using the Google and
Windows LiveSearch web-services.
WordNet Domains
Magnini etal (2002) describe the WordNet Do-mains5 ( Magnini and Cavaglia 2000) package as : ? an extension of WordNet in which synsets have been annotated with one or more domain labels , selected from a hierarchically organized set of about two hundred labels?.
( Magnini etal 2002 p.361)
They describe a domain ( eg ? Politics ?) as ? a set of words between which there are strong semantic relations ?. This resource is useful for measuring topic homogeneity , as it stores topic area information for wordsenses directly , and complements the other measures , thus contributing to a diverse set of measures.
Two WordNet Domains homogeneity measures are calculated : DomEntropy and DomTop3Percent . These are calculated for each input document d as follows : Step 1: Add each topic-content lemma of d to the list TopicContents . Step 2: for each WordNet sense of each topic-content lemma in TopicContents , find all associated domains using the Domains package , and add these to a DomainCounts list . This list contains each distinct domain dom present in the document , each with its associated count of the number of times it occurs in TopicContents : freq(domi ). Step 3: Calculate DomEntropy using the equation [1] above , where n is the number of items in DomainCounts , and p(xi ) = freq(domi ) / ? j=1..n freq(domj ) [4] Step 4: Calculate DomTop3Percent as follows : 100 (? i=1..3 freq(domi ) / ? j=1..n freq(domj )) [5]
Lexical Chaining ? Lexical chains are defined as clusters of semantically related words ? ( Doran etal 2004). These words are usually related by electronic dictionaries such as WordNet or Roget?s Thesaurus , and chains are created using a natural language text as input.
The lexical chaining method used in our experiments is a greedy version of the algorithm described in Ecran and Cicekli (2007). Their method uses the WordNet dictionary and calculates all possible chains in the text . We adopt a greedy approach to chaining , as it is only necessary to get an overall estimate of the levels of topic homogeneity within the text , rather than producing 5 We use version 3.2 released Feb 2007 LexChain homogeneity measure is calculated for the input document d as follows : Step 1: Add each topic-content noun occurrence in d to the list UnusedNouns . Step 2: For each item in UnusedNouns , find all other items in UnusedNouns that it is related to and add them to its corresponding RelatedNouns list . Each item of RelatedNouns is mapped to a score ( relScore ) using the following system ( Ercan and Cicekli 2007): 10 points are awarded if the wordsenses have identical lemmas or belong to the same WordNet 2.1 synset . 7 points are awarded if it is a hyponymy relationship and 4 points are awarded if it is a holonymy relationship . Step 3: Create chains : Iterate through UnusedNouns recursively , adding all related senses to the first chain , until no further linked nouns can be found.
As each new node ( UnusedNouns item ) is added to the chain , remove it from UnusedNouns . Continue creating further chains , until no more related nouns can be found . Step 4: Calculate the chainScore of each chain by adding together all of the relScores contained for each sense , at each node . Step 5: Set LexChain as the ChainScore of the highest scoring chain.
3.4 Adjusting for Polysemy and Skew
Each of the homogeneity measures ( except WordEntropy ) has the potential to be affected by the average polysemy and sense skews of the document . The effects are measured statistically using linear regression and the resulting line of best fit equation is used to reverse them.
To calculate the adjustments for each measure , the effects of polysemy and skew must be approximated . This is achieved by applying linear regression6 over the entire result set . The homogeneity measure is entered as the dependant variable and the appropriate7 average polysemy and skew measures ( per doc ) are input as independent variables . If the homogeneity measure is affected by either ( or both ) of the independent variables and the effect is statistically significant , a line of best fit equation is output representing the gradient of the effect caused by those varia-ble(s ). The appropriate homogeneity measure for each input document is adjusted by subtracting the coefficient of the gradient multiplied by the appropriate variable(s ): polysemy and/or skew.
6 Using SPSS 13.0 statistical software package.
7 Avg . document polysemy/skews are only calculated for lemmas incl . in homogeneity measures.
4 Evaluation and Discussion
It is anticipated that the main users of a set of topic homogeneity measures are other NLP techniques . They are , therefore , best measured in terms of the actual results of the processes they are intended to improve . Human judgments can be subjective ( Doran etal 2004) and are therefore deemed inappropriate for the evaluation of this task.
Three methods are used to evaluate the homogeneity measures . Firstly , each measure is compared with its equivalent where only correct senses are used . Secondly , the Word-Net::Domains ( version 3.2) hierarchy ( Magnini and Cavaglia 2000) is used to generate sets of words with varying levels of topic homogeneity.
Each set is then tested using the proposed measures , and the results compared with those expected . Finally , the usefulness of each measure is tested by evaluating their ability to indicate the likely outcome of several co-occurrence/dictionary-based WSD measures . In the WSD literature , the main non-topic related variables reported as affecting WSD results are polysemy and skew , so these two measures will be used as the baselines.
4.1 All Senses vs . Correct Senses
Table 2 show the set of measures correlated against their correct-sense complements , where only correct senses are utilized . Most of the results are in the moderate range (.40-.60) with AvgSimLESK and LexChains achieving correlations in the high and very-high ranges . Only the SENSEVAL DomEntropy result falls below the moderate level , indicating that homogeneity information is , in general , not negated by incorrect-sense noise.
Measure Correlation ( Pearson?s R)
Semcor SENSEVAL
AvgSimLESK 0.814** 0.834**
AvgSimJCN 0.491** 0.610**
AvgSimLCH 0.569** 0.474**
DomEntropy 0.575** 0.371**
DomTop3% 0.535** 0.444**
LexChain 0.659** 0.967** ** Significant at the .01 level (2-tailed ) Table 2: Correlation with correct-sense equivalents.
4.2 WordNet::Domains
The WordNet Domains package ( Magnini and Cavaglia 2000) assigns domains to each sense of each domain a relevant list of words can be extracted . The domains are arranged hierarchically , allowing sets of words with a varied degree of topic homogeneity to be selected . For example , for a highly heterogeneous set , 10 words can be selected from any domain , including factotum ( level-0: the non-topic related category ). For a slightly less heterogeneous set , words might be selected randomly from a level1 category ( eg ? Applied_Science ?), and any of the categories it subsumes ( eg Agriculture , Architecture , Buildings etc ). The levels range from level-0 ( factotum ) to level4; we merge levels 3 and 4 as level4 domains are relatively few and are viewed as similar to level3. This combined set is henceforth known as level3.
For our experiments , we have collected 2 random samples of 10 words for every WordNet domain (167 domains ) and then increased the number of sets from level-0 to level2 domains , to make the number of sets from each level more similar . The final level counts are : levels 0 to 2 have 100 word-sets each and level 3 has 192 word-sets . The sets contain 10 words each . We then assign an expected score to each set , equal to its domain level.
**Significant at the .01 level (2-tailed ) * Significant at the .05 level (2-tailed ) Table 3: Correlation with expected scores for
WordNet::Domains selected sets The first column of results on Table 3 represents the correlations with expected results for all 492 word-sets . The high WordNet::Domains results ( DomEntropy and DomTop3%) probably reflect the fact that they are produced using the same resource as the creation of the test sets . On the other hand , knowledge of correct senses is not required for the homogeneity measures and these scores indicate that they are capable nonetheless of capturing topic homogeneity . The SearchYahoo , AvgSimsJCN and LexChain methods all produce promising results with correlations in the moderate range (0.40 to 0.59) and again indicate that they can capture topic homogeneity.
To indicate whether the measures are more capable of distinguishing between extreme levels of homogeneity , we repeated the above tests , but included only those sets of level-0 and level3.
The results displayed in the final column of Table 3 and provide evidence that this might be the case for the WordNet::Domains measures and SearchYahoo , as the correlations are significantly higher for these more extreme test sets.
4.3 Dictionary-based WSD
The three WSD algorithms selected for evaluation are the technique described in Mihalcea and Moldovan (2000) and two WordNet Similarities measures : Lesk and JCN , adapted for WSD as described in Patwardhan etal (2004), in which they are found to achieve the best performances.
Each WSD technique uses the entire document as the context for each target word . The method of Mihalcea and Moldovan (2000) is included as it incorporates several techniques , all complementary to our overall set of evaluation methods . For our experiments it is split into three : Mih-ALL : covering all 8 procedures , including one that relies on colocation information ; Mih-4: utilising ? procedure-4?, which involves the use of noun cooccurrence and WordNet hyponymy data ; and Mih-5-8: using procedures 5 to 8, which involves synonymy and hyponymy . The results are adjusted to remove the effects of polysemy and
SenseEntropy ( section 3.4).
In Table 4, the finegrained WSD accuracy results ( for topic-content words ) are compared to those of the homogeneity measures , ( including the correct-sense measures which set the high-standard benchmark ). As a baseline , non-adjusted WSD accuracies are compared with the average polysemy and average Sense Entropy of each document.
All of the ? all-senses ? results , except Domai-nEntropy , are statistically significant and achieve at least low-moderate correlations with one or more of the WSD measures . All of the measures outperform the baseline correlations for most of the WSD algorithms displayed.
A COMBINED measure is calculated for the all-sense and the correct-sense sets of measures respectively . The measures included ( based on their individual performances and maintaining maximum diversity ) are AvgSimsJCN , WordEntropy , DomTop3Percent , LexChain and SearchYahoo . Each of these result sets are ordered by homogeneity score ( most homogeneous
Measure Correlation ( Pearson?s R)
All Extreme
SearchYahoo 0.46** 0.80**
AvgSimLESK 0.23** 0.15*
AvgSimJCN 0.47** 0.45**
AvgSimLCH 0.35** 0.25**
DomEntropy 0.70** 0.75**
DomTop3% 0.75** 0.83**
LexChain 0.42** 0.40** points at equal percentiles and numbering them from 5 down to 1 respectively . The combined measure for each document is the sum of all such scores . These measures often outperform all of the individual methods and achieve correlations of up to .52 in Semcor , the largest of the datasets.
5 Conclusions and Further Work
This paper presents a first attempt to measure Topic Homogeneity using a variety of NLP resources . A set of 5 unsupervised homogeneity measures are presented that require no prior knowledge of correct senses and which exhibit moderate to high degrees of correlation with their correct-sense-only equivalents . When used to measure word-sets created using the Word-Net::Domains package and which have varying levels of homogeneity , they are found to correlate well with expected results , further supporting our conjecture that they represent topical homogeneity information . Finally , when compared with WSD topic-content word accuracies , the effect of topic homogeneity is shown to surpass that of polysemy and sense-entropy , which have been recognized previously as having an influence on such results . By combining these measures , correlations are improved further , often outperforming the individual methods and achieving up to .52 for over 1000 random Semcor subdocuments , again indicating their potential importance . Correlations in SENSEVAL are often higher , but due to the lower number of documents , it is more difficult to obtain statistically significant results.
Our results provide evidence that improvements could be made to WSD and other NLP methods which utilise topic features , by adapting the algorithms used depending on the level of topic cohesion of the input text . For example , window-sizes for obtaining contextual data might be expanded or reduced , based on the homogeneity level of the target text . Furthermore , nontopical features such as collocation and grammatical cues might be given more emphasis when disambiguating heterogeneous documents.
Further work includes testing the measures on other NLP tasks . A machine learning approach might also be used to further optimize the combination of homogeneity measures . Finally , it is intended that our approach should eventually be combined with a TAD method to improve WSD results.
References
Bollegala , Danushka , Yutaka Matuo and Mitsuru Ishizuka , 2007. Measuring Semantic Similarity between Words Using Web Search Engines . In Procs World Wide Web Conference , Banff , Alberta.
Caracciolo , Caterina , Willem van Hage and Maarten de Rijke , 2004. Towards Topic Driven Access to Full Text Documents , in Research and Advanced SENSEVAL 2 & 3 SEMCOR
Mih
ALL
Mih 58
LESK JCN Mih
ALL
Mih 58
LESK JCN
ALL Senses
Word Entropy -.325 -.301 -.419 -.442 -.206 -.286 AvgSimLESK .500 .138 .132 .211 .121 .097 AvgSimJCN .286 .276 .357 .233 .255 .231 .284 .081 AvgSimLCH .271 .211 .224 .202 .254 .248 .285 .104 DomEntropy -.193 -.205 -.163 -.157 -.091 DomTop3% .308 .281 .224 .215 .145 .108 LexChain .344 .328 .345 .296 .095 SearchYahoo -.232 -.290 -.317 -.295 -.105 -.116 -.085 -.089 COMBINED .382 .421 .270 .243 .171 .521 .517 .256 .350
CORRECT
Senses
AvgSimLESK .461 .467 .630 .189 .186 .174 .237 .171 .115 AvgSimJCN .257 .272 .357 .325 .218 .162 .234 .210 AvgSimLCH .310 .483 .187 .198 .181 .247 .122 DomEntropy -.340 -.295 -.334 -.335 -.106 -.270 DomTop3% .376 .353 .214 .398 .393 .144 .316 LexChain .372 .297 .426 .440 .164 .112 .364 COMBINED .398 .363 .398 .379 .519 .494 .294 .434 Baseline AvgPolysemy -.100 -.030 -.234 (.252) (.113) (.146) (.169) -.004 .032 -.144 AvgSenseEntropy .031 -.044 .143 -.307 -.019 -.073 -.064 -.083 .041 -.141 Results in bold : significant at the 0.01 level (2-tailed ); Results in non-bold : significant at the 0.05 level (2-tailed ) Italicised results : not significant at the 0.05 level but considered to be of interest Bracketed results : inverse to expectations All WSD results are adjusted for polysemy / SenseEntropy , with the exception of where compared with baselines.
Table 4: Topic-content word WSD accuracy vs . Homogeneity : Correlations 495-500 Chen , Hsin-Hsi , Ming-Shun Lin and Yu-Chuan Wei , 2006. Novel Association Measures Using Web Search with Double Checking . In Proc.s 21st Intl.
Conference on Computational Linguistics and 44th Annual Meeting of the ACL , pp . 1009-1016 Doran , William , Nicola Stokes , Joe Carthy and John Dunnion , 2004. Assessing the Impact of Lexical Chain Scoring Methods and Sentence Extraction Schemes on Summarization , LNCS 2945, pp 627-635, CICLing 2004 Ercan , Gonenc and Ilyas Cicekli , 2007. Using Lexical Chains for Keyword extraction , Information Processing and Management , 43(2007), pp 1705-1714.
Gledson , Ann and John Keane , 2008. Using websearch results to measure word-group similarity . In Procs 22nd Intl Conference on Computational Linguistics ( COLING ), Manchester . ( To Appear ) Gliozzo , Alfio , Carlo Strapparava and Ido Dagan , 2004. Unsupervised and Supervised Exploitation of Semantic Domains in Lexical Disambiguation,
Computer Speech and Language
Halliday , Michael and Ruqaiya Hasan , 1976. Cohesion in English , Longman Group Hearst , Marti , 1997. Text Tiling : segmenting text into multiparagraph subtopic passages , Computational
Linguistics , 23(1), pp 3364
Keller , Frank and Mirella Lapata , 2003. Using the Web to Obtain Frequencies for Unseen Bigrams,
Computational Linguistics , 29(3)
Kilgarriff , Adam , 2007. Googleology is bad science , Computational Linguistics , 33(1), pp 147-151 Leacock , Claudia , Martin Choderow and George A.
Miller , 1998. Using Corpus Statistics and Wordnet Relations for Sense Identification , Computational
Linguistics , 24(1), pp 147-165
McCarthy , Diana , Rob Koeling , Julie Weeds and John Carroll , 2007. Unsupervised Acquisition of Predominant Word Senses , Computational Linguistics , 33(4), pp . 553-590.
Magnini , Bernardo and Gabriela Cavagli ?, 2000.
Integrating Subject Field Codes into WordNet.
In Procs LREC2000, Athens , Greece , 2000, pp 1413-1418.
Magnini , Bernardo , Carlo Strapparava , Giovanni Pezzulo and Alfio Gliozzo , 2002. The Role of Domain Information in Word Sense Disambiguation . Natural Language Engineering , 8(4), pp 359-373 Mihalcea , Rada and Dan Moldovan , 1999. A method for word sense disambiguation of unrestricted txt.
In Procs . 37th Meeting of ACL , pp 152-158 Mihalcea , Rada and Dan Moldovan , 2000. An Iterative Approach to Word Sense Disambiguation , In Proc . FLAIRS 2000, pp . 219-223, Orlando Morris , Jane and Graeme Hirst , 1991. Lexical Cohesion Computed by Thesaural Relations as an Indicator of the Structure of Text , Computational Linguistics , 17(1), pp . 2148.
Navigli , Roberto , Paola Velardi , 2005. Structural Semantic Interconnections : A Knowledge-Based Approach to Word Sense Disambiguation , IEEE Transactions on Pattern Analysis and Machine Intelligence , 27(7), pp . 1075-1086, July.
Patwardhan , Siddharth , Satanjeev Banerjee and Ted Pedersen , 2003. Using Measures of Semantic Relatedness for Word Sense Disambiguation , LNCS 2588, pp 241-257, CICLing 2003 Pedersen , Ted , Siddharth Patwardhan and Jason Michelizzi , 2004. WordNet::Similarity ? Measuring the Relatedness of Concepts , In Procs 19th National Conference on Artificial Intelligence.
Salton , Gerard and James Allan , 1994. Automatic text decomposition and structuring , In Procs . RIAO International Conference , New York.
Weeds , Julie and , David Weir , 2006, Cooccurrence Retreival : A Flexible Framework for Lexical Distributional Similarity . Computational Linguistics , 31(4), pp 433-475.
Yarowsky , David , (1995), Unsupervised word sense disambiguation rivaling supervised methods . In Procs 33rd Annual Meeting of the Association for Computational Linguistics ( ACL ) 1995, pp 189-
