Proceedings of the 22nd International Conference on Computational Linguistics ( Coling 2008), pages 1017?1024
Manchester , August 2008
Bayesian Semi-Supervised Chinese Word Segmentation
for Statistical Machine Translation
Jia Xu
?
, Jianfeng Gao
?
, Kristina Toutanova
?
, Hermann Ney
?
Computer Science 6
?
Microsoft Corporation
?
RWTH Aachen University One Microsoft Way D-52056 Aachen , Germany Redmond , WA 98052, USA { xujia,ney}@cs.rwth-aachen.de { jfgao,kristout}@microsoft.com
Abstract
Words in Chinese text are not naturally separated by delimiters , which poses a challenge to standard machine translation ( MT ) systems . In MT , the widely used approach is to apply a Chinese word segmenter trained from manually annotated data , using a fixed lexicon . Such word segmentation is not necessarily optimal for translation . We propose a Bayesian semisupervised Chinese word segmentation model which uses both monolingual and bilingual information to derive a segmentation suitable for MT . Experiments show that our method improves a state-of-the-art MT system in a small and a large data environment.
1 Introduction
Chinese sentences are written in the form of a sequence of Chinese characters , and words are not separated by white spaces . This is different from most European languages and poses difficulty in many natural language processing tasks , such as machine translation.
It is difficult to define ? correct ? Chinese word segmentation ( CWS ) and various definitions have been proposed . In this work , we explore the idea that the best segmentation depends on the task , and concentrate on developing a CWS method for MT , which leads to better translation performance.
The common solution in Chinese-to-English translation has been to segment the Chinese text using an off-the-shelf CWS method , and to apply a standard translation model given the fixed segmentation . The most widely applied method for MT is unigram segmentation , such as segmentation using the LDC ( LDC , 2003) tool , which requires a manual lexicon containing a list of Chinese words and their frequencies . The lexicon and c ? 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license ( http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
frequencies are obtained using manually annotated data . This method is suboptimal for MT . For example , ?( paper ) and ]( card ) can be two words or composed into one word ?]( cards ). Since ? ] does not exist in the manual lexicon , it cannot be generated by this method.
In addition to unigram segmentation , other methods have been proposed . For example , ( Gao et al , 2005) described an adaptive CWS system , and ( Andrew , 2006) employed a conditional random field model for sequence segmentation . However , these methods are not specifically developed for the MT application , and significant improvements in translation performance need to be shown.
In ( Xu et al , 2004) and ( Xu et al , 2005), word segmentations are integrated into MT systems during model training and translation . We refine the method in training using a Bayesian semisupervised CWS approach motivated by ( Goldwater et al , 2006). We describe a generative model which consists of a word model and two alignment models , representing the monolingual and bilingual information , respectively . In our methods , we first segment Chinese text using a unigram segmenter , and then learn new word types and word distributions , which are suitable for MT.
Our experiments on both large ( NIST ) and small ( IWSLT ) data tracks of Chinese-to-English translation show that our method improves the performance of a state-of-the-art machine translation system.
2 Review of the Baseline System 2.1 Word segmentation In statistical machine translation , we are given a
Chinese sentence in characters c
K
K which is to be translated into an English sentence e
I
I . In order to obtain a more adequate mapping between Chinese and English words , c
K
J
J in preprocessing.
In our baseline system , we apply the commonly tion . Given a manually compiled lexicon containing words and their relative frequencies P s ( f ? j ), the best segmentation f
J the joint probability of all words in the sentence , with the assumption that words are independent of each other f
J f ?
J ? ?
J ?
K ? argmax f ?
J ? ? ? j=1
P s ( f ? j ), where the maximization is taken over Chinese word sequences whose character sequence is c
K 2.2 Translation system
Once we have segmented the Chinese sentences into words , we train standard alignment models in both directions with GIZA ++ ( Och and Ney , 2002) using models of IBM1 ( Brown et al , 1993), HMM ( Vogel et al , 1996) and IBM4 ( Brown et al ., 1993).
Our MT system uses a phrasebased decoder and the loglinear model described in ( Zens and Ney , 2004). Features in the loglinear model include translation models in two directions , a language model , a distortion model and a sentence length penalty . The feature weights are tuned on the development set using a downhill simplex algorithm ( Press et al , 2002). The language model is a statistical ngram model estimated using modified Kneser-Ney smoothing.
3 Unigram Dirichlet Process Model for
CWS
The simplest version of our model is based on a unigram Dirichlet Process ( DP ) model , using only monolingual information . Different from a standard unigram model for CWS , our model can introduce new Chinese word types and learn word distributions automatically from unlabeled data.
According to this model , a corpus of Chinese words f m , . . . , f
M is generated via:
G |?, P f m | G ? G where G is a distribution over words drawn from a Dirichlet Process prior with base measure P concentration parameter ?.
We never explicitly estimate G but instead integrate over its possible values and perform Bayesian inference . It is easy to compute the symbol Pr (?) to denote general probability distributions with ( nearly ) no specific assumptions . In contrast , for model-based probability distributions , we use the generic symbol P (?).
probability of a Chinese word given a set of already generated words , while integrating over G.
This is done by casting Chinese word generation as a Chinese restaurant process ( CRP ) ( Aldous , 1985), i.e . a restaurant with an infinite number of tables ( approximately corresponding to Chinese word types ), each table with infinite number of seats ( approximately corresponding to Chinese word frequencies).
The Dirichlet Process model can be viewed intuitively as a cache model ( Goldwater et al , 2006).
Each word f j in the corpus is either retrieved from a cache or generated anew given the previously observed words f ? j :
P ( f j | f ? j ) =
N(f j ) + ? P j )
N + ? , (1) whereN(f j ) is the number of Chinese words f j in the previous context . N is the total number of Chinese words , P and ? influences the probability of introducing a new word at each step and controls the size of the lexicon . The probability of generating a word from the cache increases as more instances of that word are seen.
For the base distribution P generation of new words , we use the following distribution ( called the spelling model):
P = ?
L
L ! e ?? u
L (2) where is the number of characters in the document , i.e . character vocabulary size , and L is the number of Chinese characters of word f . We note that this is a Poisson distribution on word length and a unigram distribution on characters given the length . We used ? = 2 and ? = 0.3 in our experiments.
4 CWS Model for MT
As a solution to the problems with the conventional approach to CWS mentioned in Section 1, we propose a generative model for CWS in Section 4.1, and then extend the model to a more general but deficient model , similar to a maximum entropy model in which most features are derived from the submodels of the generative model.
4.1 Generative Model
The generative model assume that a corpus of parallel sentences ( c , e ) is generated along with a hidden sequence of Chinese words f and a hidden word alignment b for every sentence . The alignment indicates the aligned Chinese word f b i for each English word e i , where f cial null word as in the IBM models.
1018
Without assuming any special form for the probability of a sentence pair along with hidden variables , we can factor it into a monolingual Chinese sentence probability and a bilingual translation probability as follows:
Pr(c , e , f , b ) = Pr(c
K
J
I
I
J = Pr(f
J , c ) Pr(e
I
I
J where ?( f
J
K quence of words f are c , and to 0 otherwise . We can drop the conditioning on c in
Pr(e
I
I
J ministic given the words.
The joint probability of the observations ( c , e
I possible values of the hidden variables f
J
I
In Sections 4.1.1 and 4.1.2, we will describe the modeling assumptions behind the monolingual Chinese sentence model and the translation model , respectively.
4.1.1 Monolingual Chinese sentence model We use the Dirichlet Process unigram word model introduced in section 3. In this model , the parameters of a distribution over words G are first drawn from the Dirichlet prior DP (?, P are then independently generated according to G.
The probability of a sequence of Chinese words in a sentence is thus:
Pr(f
J
J ? j=1
P ( f j | G ) (3) 4.1.2 Translation model
We employ the Dirichlet Process inverse IBM model 1 to generate English words and alignment given the Chinese words . In this model , for every Chinese word f ( including the null word ), a distribution over English words G f is first drawn from a Dirichlet Process prior DP (?, P
P glish words in the parallel data . Then , given these parameters , the probability of an English sentence and alignment given a Chinese sentence ( sequence of words ) is given by:
P ( e
I
I
J f ) =
I ? i=1
P ( e i | G f b i )
This is the same model form as inverse IBM model 1, except we have placed Dirichlet Process priors on the Chinese-word specific distributions over English words.
2 b i is the Chinese word aligned to e i and G f b i is the distribution over English words conditioned on the word f b i .
Similarly , e a j is the English word aligned to f j in the other direction and G e a j is the distribution over Chinese words conditioned on e a j .
In practice , we observed that using a word-alignment model in one direction is not sufficient.
We then added a factor to our model which includes word alignment in the other direction , i.e . a Dirichlet Process IBM model 1. We ignore the detailed description here , because the calculation is the same as that of the inverse IBM model 1. According to this model , for every English word e ( including the null word ), a distribution over Chinese words G e is first drawn from a Dirichlet Process prior DP (?, P tion P the monolingual unigram Dirichlet Process prior.
The probability of a sequence of Chinese words f
J
J
English words e
I
P ( f
J
J
I e ) =
J ? j=1
P ( f j | G e a j ) 4.2 Final Model
We put the monolingual model and the translation models in both directions together into a single model , where each of the component models is weighted by a scaling factor . This is similar to a maximum entropy model . We fit the weights of the submodels on a development set by maximizing the BLEU score of the final translation.
P ( c
K
I
J
J
I ?
P ( f
J ?
I
I
J ?
J
J
I ? where Z is the normalization factor.
In practice we do not renormalize the probabilities and our model is thus deficient because it does not sum to 1 over valid observations . However , we found the model work very good in our experiments . Similar deficient models have been used very successfully before , for example , in the IBM models 3?6 and in the unsupervised grammar induction model of ( Klein and Manning , 2002).
5 Gibbs Sampling Training
It is generally impossible to find the most likely segmentation according to our Bayesian model using exact inference , because the hidden variables do not allow exact computation of the integrals.
Nonetheless , it is possible to define algorithms using Markov chain Monte Carlo ( MCMC ) that produce a stream of samples from the posterior distribution of the hidden variables given the observations . We applied the Gibbs sampler ( Geman and Geman , 1984) ? one of the simplest MCMC methods , in which transitions between states of the a boundary state , f to f ? f ?? .
Figure 2: Case II , transition from a boundary to a no-boundary state , f ? f ?? to f .
Markov chain result from sampling each component of the state conditioned on the current value of all other variables.
In our problem , the observations are D = ( d n , .., d
N ), where d n =( c
K
I bilingual sentence pair , the hidden variables are the word segmentations f
J directions a
J
I
To perform Gibbs sampling , we start with an initial word segmentation and initial word alignments , and iteratively resample the wordsegmentation and alignments according to our model of Equation 4.
Note that for efficiency , we only allow limited modifications to the initial word alignments . Thus we only use models derived from IBM1 ( instead of IBM4) for comparing different word segmentations . On the other hand , resampling the segmentation causes relinking alignment points to parts or groups of the original words.
Hence , we organize our sampling process around possible word boundaries . For each character c k in each sentence , we consider two alternative segmentations : c k + indicates the segmentation where there is a boundary after c k and c k ? indicates the segmentation where there is no boundary after c k , keeping all other boundaries fixed . Let f denote the single word spanning character c k when there is no boundary after it , and f ? , f ?? denote the two adjacent words resulting if there is a boundary : f ? includes c k and f ?? starts just to the right , with character c k+1 . The introduction of f ? and f ?? leads to M new possible alignments in the E-to-C direction b + k1 , . . . , b + kM , such as in Figure 1.
Together with the boundary vs no-boundary state at each character position , we resample a set of alignment links between English words and any of the Chinese words f , f ? , and f ?? , keeping all other word alignments in the sentence pair fixed . ( See
Figures 1 and 2.)
Table 1: General Algorithm of GS for CWS.
Input : D with an initial segmentation and alignments Output : D with sampled segmentation and alignments for n = 1 to ?
N for k = 1 to K that c k ? d n
Create M+1 candidates , cba + k,m and cba ? k , where cba + k,m : there is a word boundary after c k cba ? k : there is no word boundary after c k
Compute probabilities
P ( cba + k,m | dh nk ? )
P ( cba ? k | dh nk ? )
Sample boundary and relevant alignments
Update counts
Thus at each step in the Gibbs sampler , we consider a set of alternatives for the boundary after c k and relevant alignment links , keeping all other hidden variables fixed . At each step , we need to compute the probability of each of the alternatives , given the fixed values of the other hidden variables.
We introduce some notation to make the presentation easier . For every position k in sentence pair n , we denote by dh nk ? the observations and hidden variables for all sentences other than sentence n , and the observations and hidden variables inside sentence n , not involving character position c k . The fixed variables inside the sentence are the words not neighboring position k , and the alignments in both directions to these words.
In the process of sampling , we consider a set of alternatives : segmentation c k + along with the product space of relevant alignments in both directions b + k1 , . . . , b + kM , and a + k , and segmentation c ? k along with relevant alignments b k ? and a ? k . For brevity , we denote these alternatives by cba k,m + and cba k ? .
We describe how we derive the set of alternatives in section 5.2 and how we compute their probabilities in section 5.1.
Table 1 shows schematically one iteration of Gibbs sampling through the whole training corpus of parallel sentences , where ?
N is the number of parallel sentences.
5.1 Computing probabilities of alternatives For the Gibbs sampling algorithm in Table 1, we need to compute the probability of each alternative segmentation/alignments , given the fixed values of the rest of the data dh nk ? . The probability of the hidden variables in the alternatives is proportional to the joint probability of the hidden variables and observations , and thus it is sufficient to compute the probability of the latter . We compute these probabilities using the Chinese restaurant process sampling scheme for the Dirichlet Process , thus in-tributions G , G f and G e .
Let cba k denote an alternative hypothesis including boundary or no boundary at position k , and relevant alignments to English words in both directions of the one or two Chinese words resulting from the segmentation at k . The probability of this configuration given by our model is:
P ( cba k | dh nk ? ) ? P m ( cba k | dh nk ? ) ? ? P ef ( cba k | dh nk ? ) ? fe ( cba k | dh nk ? ) ? where P m ( cba k | dh nk ? ) is the monolingual word probability , and P fe ( cba k | dh nk ? ) and
P ef ( cba k | dh nk ? ) are the translation probabilities in the two directions.
We now describe the computations of each of the component probabilities.
5.1.1 Word model probability
The word model probability P m ( cab k | dh nk ? ) in Equation 5 is derived from Equations 3 and 1: There are two cases , depending on whether the hypothesis specifies that there is a boundary after character c k , in which case we need the probabilities of the two resulting words f ? , and f ?? , or there is no boundary , in which case we need the probability of the single word f . ( See the initial states in
Figures 1 and 2, respectively.)
Let N denote the total number of word tokens in the rest of the corpus dh nk ? , and N(f ) denote the number of instances of word f in dh nk ? . The probabilities in the two cases are
P m ( c + k | dh nk ? ) ?
N(f ? ) + ? P ? )
N + ? ?
N(f ?? ) + ? P ?? )
N + ?
P m ( c ? k | dh nk ? ) ?
N(f ) + ? P
N + ?
Here P 5.1.2 Translation model probability The translation model probabilities depend on whether or not there is a segmentation boundary at c k and which English words are aligned to the relevant Chinese words.
In the first case , assume that there is a word boundary in cab k , and that English words { e ? } are aligned to f ? and words { e ?? } are aligned to f ?? in the E-to-C direction according to the alignment b k , and that f ? is aligned to e ? ? and f ?? is aligned to e ? ?? in the C-to-E direction according to the alignment a k ( see the initial state in Figure 1). Here we overloaded notations and use b k and a k to indicate the alignments of the relevant Chinese words at position k to any English words . Let I denote the total number of English words in the sentence , and J+1 denote the number of Chinese words according to this segmentation . We also denote the total number of English words aligned to either f ? or f ?? in the E-to-C direction by P .
The translation model probability in the E-to-C direction is thus:
P ef ( c + k , b k , a k | dh nk ? ) ? )
P ? e ?
P ( e ? | f ? , dh nk ? ) ? e ??
P ( e ?? | f ?? , dh nk ? )
Here we compute P ( e|f , dh nk ? ) as:
P ( e|f , dh nk ? ) =
N(e , f ) + ? P
N(f ) + ? , where the counts are computed over the fixed assignments dh nk ? .
The translation probability in the other direction is similarly computed as:
P fe ( c + k , b k , a k | dh nk ? ) ? ( ) ? | e ? , dh nk ? ) P ( f ?? | e ? , dh nk ? )
And P ( f | e , dh nk ? ) is computed as:
P ( f | e , dh nk ? ) =
N(f , e ) + ? P
N(e ) + ? , where the counts are computed over the fixed assignments dh nk ? .
In the second case , if the hypothesis in evaluation does not have a word boundary at position k , the total number of Chinese words would be one less , i.e . J instead of J +1 in the equations above , and there would be a single set of English words aligned to the word f in the E-to-C direction , and a single word e ? aligned to f in the C-to-E direction ( see the initial state in Figure 2. The probability of this hypothesis is computed analogously.
5.2 Determining the set of alternative hypotheses As mentioned earlier , we consider alternative alignments which deviate minimally from the current alignments , and which satisfy the constraints of the IBM model 1 in both directions . In order to describe the set of alternatives , we consider two cases , depending on whether there is a boundary at the current character before sampling at position k.
Case 1. There was no boundary at c k in the previous state ( see Figure 1).
1021
If there is no boundary at c k , there is a single word f spanning that position . We denote by { e } the set of English words aligned to f at that state in the E-to-C direction and by e ? the English word aligned to f in the C-to-E direction.
Since every state we consider satisfies the IBM one-to-many constraints , there is exactly one English word aligned to f in the C-to-E direction and the words { e } have no other words aligned to them in the E-to-C direction.
In this case , we consider as hypothesis cba k ? the same segmentation and alignment as in the previous state . ( see Table 1 for an overview of the alternative hypotheses .) We consider M different hypotheses which include a boundary at k in this case , where M depends on the number of words { e } aligned to f in the previous state . Because we are breaking the word f into two words f ? and f ?? by placing a boundary at c k , we need to realign the words { e } to either f ? or f ?? . Additionally we need to align f ? and f ?? to English words in the C-to-E direction . The number of different hypotheses is equal to 2
P where P = |{ e }|. These alternatives arise by considering that each of the words in { e } needs to align to either f ? or f ?? , and there are 2
P combinations of these alignments . For example , if { e } = { e four possible alignments , illustrated in Figure 1:
I . ( f ? , e ?? , e ? , e ?? , e
III . ( f ? , e ? , e ?? , e ?? , e
For the alignment a k in the C-to-E direction , we consider only one option , in which both resulting words f ? and f ?? align to e ? . These alternatives form cba k,m + in Table 1.
Case 2. There was a boundary at c k in the previous state ( see Figure 2).
In this case , for the hypotheses c + k we consider only one alternative , which is exactly the same as the assignment of segmentation and alignments in the previous state . Thus we have M = 1 in Table 1.
Let f ? and f ?? denote the two words at position k in the previous state , { e ? } and { e ?? } denote the sets of English words aligned to them in the E-to-C direction , respectively , and e ? ? and e ? ?? denote the
English words aligned to f ? and f ?? in the C-to-E direction.
We consider only one hypothesis cba k ? where there is no boundary at c k . In this hypothesis , there is a single word f = f ? f ?? spanning position k , and all words { e ? } ? { e ?? } align to f in the E-to-C direction . For the C-to-E direction we consider the ? better ? of the alignments ( f , e ? ? ) and ( f , e ?? ? ) where the better alignment is defined as the one having higher probability according to the C-to-E word translation probabilities.
Table 2: Complete Algorithm of Gibbs Sampler for CWS including Alignment Models.
Input : D , F
T , F
T for t = 1 to T
Run GIZA ++ on ( D,F t?1 ) to obtain A t
Run GS on ( D,F t?1 , A t ) to obtain F t 5.3 Complete segmentation algorithm So far , we have described how we resample word segmentation and alignments according to our model , starting from an initial segmentation and alignments from GIZA ++. Putting these pieces together , the algorithm is summarized in Table 1.
We found that we can further improve performance by repeatedly aligning the corpus using GIZA ++, after deriving a new segmentation using our model . The complete algorithm which includes this step is shown in Table 2, where F t indicates the word segmentation at iteration t and A t denotes the GIZA ++ corpus alignment in both directions . The GS resegmentation step is done according to the algorithm in Table 1.
Using this algorithm , we obtain a new segmentation of the Chinese data and train the translation models using this segmentation as in the baseline MT system . To segment the test data for translation , we use a unigram model , trained with maximum likelihood estimation off of the final segmentation of the training corpus F
T .
6 Translation Experiments
We performed experiments using our models for CWS on a large and a small data track . We evaluated performance by measuring WER ( word error rate ), PER ( position-independent word error rate ), BLEU ( Papineni et al , 2002) and TER ( translation error rate ) ( Snover et al , 2006) using multiple references.
6.1 Translation Task : Large Track NIST
We first report the experiments using our monolingual unigram Dirichlet Process model for word segmentation on the NIST machine translation task ( NIST , 2005). Because of the computational requirements , we only employed the monolingual word model for this large data track , i.e . the feature weights were ? fore , no alignment information needs to be maintained in this case.
The bilingual training corpus is a superset of corpora in the news domain collected from different sources.
We took LDC ( LDC , 2003) as a baseline CWS method ( Base ). As shown in Table 3, the training corpus in each language contains more than two million sentences . There are 56 million Chinese
Data Sents . Words[K ] Voc.[K]
Cn . En . Cn . En.
Chars 2M 56M 49.5M 65.4 211
Base 39.2M 95.7
GS 40.5M 95.4 02 878 23.1 28.0 2.04 4.34 03 919 24.6 29.2 2.21 4.91 04 1788 49.8 60.7 2.61 6.71 05 1082 30.8 34.2 2.30 5.39 Table 4: Translation performance [% BLEU ] with the baseline(LDC ) and GS method on NIST.
MTeval LDC(Base ) GS 2005 32.85 33.26 2002 34.32 34.36 2003 33.41 33.75 2004 33.74 34.06 characters . The LDC and GS word segmentation methods generated 39.2 and 40.5 million running words , respectively.
The scaling factors of the translation models described in Section 2.2 were optimized on the development corpus , MTeval 05 with 1082 sentences.
The resulting systems were evaluated on the test corpora MTeval 02-04. For convenience , we only list the statistics of the first English reference.
Starting from the baseline LDC output as initial word segmentation , we performed Gibbs sampling ( GS ) of word segmentations using 30 iterations over the Chinese training corpus.
Since BLEU is the official NIST measure of translation performance , we show the translation results measured in BLEU score only . As shown in Table 4, on the development data MTeval 05, the BLEU score was improved by 0.4% absolute or more than 1% relative using GS . Similarly , the absolute BLEU scores are also improved on all other test sets , in the range of 0.04% to 0.4%.
We can see that even a monolingual semisupervised word segmentation method can outperform a supervised one in MT , probably because the training/test corpora contain many unknown words and words have different frequencies in our MT data from they do in the manually labeled CWS data.
6.2 Translation Task : Small Track IWSLT
We evaluate our full model , using both monolingual and bilingual information , on the IWSLT data.
As shown in Table 5, the Chinese training corpus was segmented using the unigram segmenter ( Base ) described in Section 2.1 and our GS method . Since the unigram segmenter performs better in our experiments , we took it as the baseline and the method for initialization in later experiments . We see that the vocabulary size of the Chinese training corpus was reduced more significantly by GS than by the baseline method , even Table 5: Statistics of corpora in task IWSLT.
Test Sents . Words[K ] Voc.
Cn . En . Cn . En.
Chars 42.9K 520 420 2780 9930
Base 394 8800
GS 398 6230
Dev2 500 3.74 3.82 1004 821
Dev3 506 4.01 3.90 980 820
Eval 489 3.39 3.72 904 810
Table 6: Translation performance with different
CWS methods on IWSLT[%].
Test Method WER PER BLEU TER
Dev2 Unigram ( Base ) 38.2 31.2 55.4 37.0
GS 36.8 30.0 56.6 35.5
Dev3 Unigram ( Base ) 33.5 27.5 60.4 32.1
GS 32.3 26.6 61.0 31.4
Eval Characters 49.3 41.8 35.4 47.5
LDC 46.2 40.0 39.2 45.0
ICT 45.9 40.4 40.1 44.9
Unigram ( Base ) 46.8 40.2 41.6 45.6 9gram 46.9 40.4 40.1 45.4
GS 45.9 40.0 41.6 44.8 though they resulted in a similar number of running words . This shows that the distribution of Chinese words is more concentrated when using
GS.
The parameter optimizations were performed on the Dev2 data with 500 sentences , and evaluations were done both on Dev3 and on Eval data , i.e . the evaluation corpus of ( IWSLT , 2007).
The model weights ? of GS from Section 5.1.2 were optimized using the Powell ( Press et al , 2002) algorithm with respect to the BLEU score.
We obtained ? optimal values and T = 4 as the optimal number of iterations of realignment with GIZA++.
For a fair comparison , we evaluated on various CWS methods including translation on characters , LDC ( LDC , 2003), ICT ( Zhang et al , 2003), unigram , 9gram and GS . Improvements using GS can be seen in Table 6. Under all test sets and evaluation criteria , GS outperforms the baseline method.
The absolute WER decreases with 1.2% on Dev3 and with 1.1% on Eval data over baseline.
We compared the translation outputs using GS with the baseline method . On the Eval data , 196 sentences are different out of 489 lines , where 64 sentences from GS are better , 33 sentences are worse , and the rests have similar translation qualities . Table 7 shows two examples from the Eval corpus . We list segmentations produced by the baseline and GS methods , as well as the translations corresponding to these segmentations . The GS method generates better translation results than the baseline method in these cases.
1023
Table 7: Segmentation and translation outputs with baseline and GS methods.
a ) Baseline ? ?4 m ? do you have a ?
GS ??4m ? do you have a shorter way ?
REF is there a shorter route ? b ) Baseline >???? please show me the in .
GS >???? please show me the total price .
REF can you tell me the total amount ? 7 Conclusion and future work We showed that it is possible to learn Chinese word boundaries such that the translation performance of Chinese-to-English MT systems is improved.
We presented a Bayesian generative model for parallel ChineseEnglish sentences which uses word segmentation and alignment as hidden variables , and incorporates both monolingual and bilingual information to derive a segmentation suitable for MT.
Starting with an initial word segmentation , our method learns both new Chinese words and distributions for these words . In a large and a small data environment , our method outperformed the standard Chinese word segmentation approach in terms of the Chinese to English translation quality.
In future work , we plan to enrich our monolingual and bilingual models to better represent the true distribution of the data.
8 Acknowledgments
Jia Xu conducted this research during her internship at Microsoft Research . This material is also partly based upon work supported by the Defense Advanced Research Projects Agency ( DARPA ) under Contract No . HR0011-06-C-0023.
References
Aldous , D . 1985. Exchangeability and related topics.
In ?
Ecole d??et?e de probabilit?es de Saint-Flour , XIII-1983, pages 1?198, Springer , Berlin.
Andrew , G . 2006. A hybrid markov/semi-markov conditional random field for sequence segmentation . In
Proceedings of EMNLP , Sydney , July.
Brown , P . F ., S . A . Della Pietra , V . J . Della Pietra , and R . L . Mercer . 1993. The mathematics of statistical machine translation : Parameter estimation . Computational Linguistics , 19(2):263?311, June.
Gao , J ., M . Li , A . Wu , and C . Huang . 2005. Chinese word segmentation and named entity recognition : A pragmatic approach . Computational Linguistics , 31(4).
Goldwater , S ., T . L . Griffiths , and M . Johnson . 2006.
Contextual dependencies in unsupervised word segmentation . In Proceedings of Coling/ACL , Sydney,
July.
IWSLT . 2007. International workshop on spoken language translation home page.
http://www.slt.atr.jp/IWSLT2007.
Klein , D . and C . D . Manning . 2002. A generative constituent-context model for improved grammar induction . In Proceedings of ACL , pages 128?135.
LDC . 2003. Linguistic data consortium Chinese resource home page.
http://www.ldc.upenn.edu/Projects/Chinese.
NIST . 2005. Machine translation home page.
http://www.nist.gov/speech/tests/mt/index.htm.
Och , F . J . and H . Ney . 2002. Discriminative training and maximum entropy models for statistical machine translation . In Proceedings of ACL , pages 295?302,
Philadelphia , PA , July.
Papineni , K . A ., S . Roukos , T . W ., and W . J . Zhu . 2002.
Bleu : a method for automatic evaluation of machine translation . In Proceedings of ACL , pages 311?318,
Philadelphia , July.
Press , W . H ., S . A . Teukolsky , W . T . Vetterling , and B . P . Flannery . 2002. Numerical Recipes in C++.
Cambridge University Press , Cambridge , UK.
Snover , M ., B . Dorr , R . Schwartz , L . Micciulla , and J . Makhoul . 2006. A study of translation edit rate with targeted human annotation . In Proceedings of AMTA , pages 223?231, Cambridge , MA , August.
Vogel , S ., H . Ney , and C . Tillmann . 1996. HMM-based word alignment in statistical translation . In Proceedings of COLING.
Xu , J ., R . Zens , and H . Ney . 2004. Do we need Chinese word segmentation for statistical machine translation ? In Proceedings of the SIGHAN Workshop on Chinese Language Learning , pages 122? 128, Barcelona , Spain , July.
Xu , J ., E . Matusov , R . Zens , and H . Ney . 2005. Integrated Chinese word segmentation in statistical machine translation . In Proceedings of IWSLT , pages 141?147, Pittsburgh , PA , October.
Zens , R . and H . Ney . 2004. Improvements in phrasebased statistical machine translation . In Proceedings of HLT/NAACL , Boston , MA , May.
Zhang , H ., H . Yu , D . Xiong , and Q . Liu . 2003.
HHMM-based Chinese lexical analyzer ICTCLAS.
In Proceedings of the Second SIGHAN Workshop on Chinese Language Learning , pages 184?187, July.
1024
