Motivations and Methods tbr Text Simplific at ion 
R . Chandrasekar * Christine Doran B . Srinivas
Institute for Research in l)el ) art m < ; nt of Deparl ; m cnt of
Cognitive , Science &(\] cntcrfor\[ , inguistics (\] Oml ) uter $?
the Advanced Study of hldia InlbrHlatioll Scienc ( ; University ot 7Pcnnsylwmia , lq filad clphia , PA 19104 ra?ckey c , doran , srin ? Ol ? nc . c is . upenn , edu
Abstract
Lottgall deolni ) licated selt teltces prov ( : to b ( : a . stumbling block for current systems relying on N \[  , input . These systenls stand to gaill froliln tethods that syntacti <: a Hy sim-plily su <: h sentences  . '\] b simplify a sen=tence , we nee<t an idea of t it ( . " structure of the sentence , to identify the <: omponent so be separated out . Obviously a parser couhl be used to obtain the complete structure of the sentence  . \]\[ owever , hill parsing is slow a + ndi ) rone to fa . ilure , especially on <: omph!x sentences . In this l ) a per , we consider two alternatives to fu\]l parsing which could be use < l for simplification  . The tirstal ) l ) roach uses a FiniteState Grammar ( FSG ) to pro-dn <: e noun and verb groups while the second uses a Superta  . gging model to i ) roduce dependency linkages . We discuss the impact of these two input representations on the simplification pro  ( :ess . 
1 Reasons for Text Simplification l , ong and <: oml ) licatcd sentences prove to be a stuml Jing block for < ' urrent systems which rely on natural language input  . ' l'lmsc systems stand to gain from metho<ls that preproces such sentences so as to make them simpler  . Consider , for exam-ph; , the following sentence : ( l )   7'he embattled Major government survived a crucial ' vole on coal pits closure as its last-minute concessions curbed the extent of ' l bry revolt over an issue that generated u ' a usual heat in the l \] ousc of Commons and brought the miners to London streets  . 
Such sentences are not uncommon in newswire texts .   ( \] ompare this with the multi-sentence version which has been manually simplified :  ( 2 ) The embatlled Major governmcnl survived a crucial vote o'u coal pits closure  . Its last : minute conccs sious curbed the cx lenlo\ ] "* Onle avefl ' om the National Centre for Soft  , wareTechno\]ogy , (lulmohar (? rossRoad No . 9, Juhu,
Bombay 4:0(/(149, India
To ry revolt over the coal-miue issue . Th . is issue generaled unusual heat in the ltousc of Commons  . II also brought the miners to
Londons treels.
If coml > lextext can be made simph'x , sen-ten (- es be con ae easier to process , both for In : O-grams and humans . Wc discuss a simplific a-tion process which identifies components of a sentence that may be separated out  , and transforms each of these into frec-sta , ding simpler sentences . 
(\] learly , somemmnees of meaning from the original text may be lost in the simplification process  . 
Simplitication is the retbre inappropriate for texts  ( such as legal docunlents ) where it is import a . nt not to lose any nuance . Iowew ; r , one c . ~tl\]C Oil-ceive of several areas of natural language processing where such simplitication would be of great use  . This is especially true in dolna in such as In a -chine translation  , which commonly have a manual postprocessing stage  , where semantic and prag-matic repairs may be <' arried out if ne <  ; essary . 
? Parsing : Syntactically <: omplex sentence's arc likely to generate a large number of parses  , and may cause parsers to fail altogether . Resolving ambiguities in attachment of con-st ituents is non-trivial  . This ambiguii , y is reduced for simpler sentences in <' e they involve fewer constituents  . ' Fhus simpler sentences lead to faster parsing and less parse aml  ) igu-ity . Once the i > arses for the simpler sentences are obtained  , the subparses can be assembled to form a full parse  , or left as is , depending on the application . 
? Machine Translation ( MT ) : As in the parsing case , simplification results in simplers cn-tential structures and reduced ambiguity  . As argued in ( Chandrasekar ,  1994) , this conld lead to improvements in the quality of machine translation  . 
? Information Retrieval : IR systems usually retrieve large segments of texts of which only a part n\]ayb creh ~' wml  ,  . Wit , simplified texts , it is possible to extract Sl > eCific phrases or simple sentences of relevance in response to queries  . 
1 041 ? Summarization : With the overload of information that people face today  , it would be very helpful to have text summarization tools that  ; reduce large bodies of text to the salient minimum  . Simplification can be used to weed out irrelevant ext with greater precision  , and thus aid in summarization . 
? Clarity of Text : Assembly/use/maintenance manuals must be clear and simple to follow  . 
Aircraft companies use a Simplified English for maintenance manuals precisely for this reason  ( Wojciket M . , 1993) . However , it is not easy to create text in such an artificially constrained language  . Automatic ( or semiautomatic ) simplification could be used to ensure that texts adhere to standards  . 
We view simplification as a twostage process.
The first stage provides a structural representation for a sentence on which the second stage applies a sequence of rules to identify and extract he components that can be simplified  . One could use a parser to obtain the complete structure of the sentence  . If all the constituents of the sentence along with the dependency relations are given  , simplification is straightforward , t to wever , full parsing is slow and prone to failure , especially on complex sentences . To overcome the limitations of full parsers , researchers have adopted FSG based approaches to parsing  ( Abney , 1994; Hobbs et al , 1992; Grishman ,  1995) . These parsers are fast and reasonably robust ; they produce sequences of noun and verb groups without any hierarchical structure  . Section 3 discusses an FSG based approach to simplification  . An alternative approach which is both fast and yields hierarchical structure is discussed in Section  4  . In Section 5 we compare the two approaches , and address some general concerns for the simplification task in Section  6  . 
2 The Basics of Simplification
Text simplification uses the f~ct that complex texts typically contains complex syntax  , some of which may be particular to specific domain of discourse  , such as newswire texts . We assume that the simplification system will process one sentence at a time  . Interactions across sentences will not bc considered  . Wcalso assume that sentences have to be maximally simplified  . 
2' o simplify sentences , wenced to know where we can split them . We define articulation-points to be those points at which sentences may be logically split  . Possible articulation points include the beginning s and ends of phrases  , punctuation marks , subordinating and coordinating conjunctions , and relative pronouns . These articulation points a regcner al , and should apply to arbitrary English texts . These may , however , be augmented with domain-specific articulation points  . We can use these articulation-points to define a set of rules which map froln given sentence patterns to simpler sentences patterns  . These rules are repeat-edly applied on each sentence until they do not apply any more  . For example , the sentence ( 3 ) with a relative clause can be simplified into two sentences  ( 4 )  . 
(3) Talwin der Singh , who masterminded the Kanishkacrash in 198 ~ , was killed in a fierce l wo-h on re ~ . connter . . . 
(4) Talwind cr Singh was killed in a . fierce two-hoarcn counler . . . Talwin der Siughmasterminded the Kanishkacrash in  198~  . 
3 FSG based Simplification ( Chandrase kar ,  1994 ) discusses an approach that uses a FSG for text simplification as part of a machineaided translation prototype named Vaakya  . In this approach , we consider sentences to be composed of sequence of word groups  , or chunks . Chunk boundaries are regarded as potential articulation-points  . Chunking allows us to define the syntax of a sentence and the structure of simplification rules at a coarser granularity  , since we need no longer be concerned with the internal structure of the chunks  . 
In this approach , we first tag each word with its part-of-speech . Chunks are then identified nsing a FSG . Each chunk is a word group consisting of a verb phrase or a noun phrase  , with some attached modifiers . The noun phrase recognizer also marks the number ( singular/plural ) of the phrase . The verb phrase recognizer provides some information on tense  , voice and aspect . Chunks identified by this mechanism include phrase such as the extent of Tory~evolt and have recently b cenfinal i z c d  . 
The chunked sentences are then simplified using a set of ordered simplification rules  . The order i~g of the rules is decided manually , to take care of more frequen transformations first  , and to avoid unproductive rule interaction . An example rule that simplifies sentences with a relative pronoun is shown in  ( 5 )  . 
(5) X:tiP , ReXPronY , Z--*X:tiPZ . X:tiPY.
The rule is interpreted as follows . If a sentence starts with a noun phrase ( X : tiP ) , and is followed by a phrase with a relative pronoun  , of the \[' or m ( , l % el Pron Y , ) followed by so Ine(Z ) , where Y and Z are arbitrary sequences of words , then the sentence may be simplified into two sentences  , namely the sequence ( X ) followed by ( Z ) , and ( X ) followed by ( Y ) . The resulting sen\] ; ences are then recursively simplified , to the extent possible . 
The system has been tested on news text , and performs well on certain classes of sentences  . See ( Chandrasekar and R , amani ,  1996 ) ibr details of quantitative valuation of the system  , including an evaluation of the acceptability of the resulting sentences  , was simplitied by the prototype system , resulting in 369 simplified sentences . 
I lowever , there are certain we Menesses in this system , caused mostly by the relatively simple mechanisms used to detect phrases and attach-meats  . Sentences which include long distance or crossed del  ) enden ( ' ies , and sentences which have malt plystacked appositives are not handled llrOl  ) erly ; nor are sentences with at n biguous or un-ch ' . arattach n wnts . Some of these prol ) \] oms can be hand h'dI ) y augmenting the ruh'set but what isi'eally require  ( I is ntorc syntactic firel ) ower . 
4 A Dependency-based model
A second a . I ) l ) roaeh to simplification is to user i ( : her syntactic in\[brmation , in terms of both constituency in lbrmation and dependency inf ' or ma-tion  . We use partial parsing and simple depen- . 
dency attachment techniques as an alternative to the FSGI  ) ased simpliiication . This ~ no ( M ( the I ) SM ) is based on as in q ) le dependency tel ) r ( > sentation provided l ) yI , exicalized Tree . Adjoining ( Ira . tmnar(I/FAG ) and uses the " SU l > er taggiug " l ; echniques described in ( Josh and Srinivas ,  1994) . 
4.1 BriefOvt ; rvlt ; w of LTAGs
The primitive element sel LTA (~ formalism are (' . l-(: lnentary trees . Elementary trees are of two types : initial frees and au  , iliary trees . Initial / ; rees are minimal linguistic structures that contain no recurson  , such assitnph ; sentences , NPs , l ) Ps etc . Auxiliary trees are recursives tru <- turcs which represent constituents that arc adjuncts to basic structure  ( e . g . relative clauses , sentential adjuncts , a(Iw ' . rbials ) . For a more R ) rmal and ( le-taile ( I ( lescription of l , ' l'A(\]s see ( Schabes et M . ,

4.2 SuI ) (* xl ; agging
Tl teelemmt tary trees of LTAG localize dependen- ( - ies , including hmg distance dependencies , by requiring that all and only the dependent elements be present within the same tree  . As a result of this localization , a lexical item may be ( and almost alwws is ) associated with more than one eL-ementary tree , We call these elementary trees su-pcr lags , since they conttdn more information ( such as sul ) categorization adagreement information ) than standard part-of speech tags . Henc . e , each word is associated with more than one supertag  . 
At the end of a complete l ) arse , each word is associated with just one supertag ( assuming there is no global ambiguity )  , and the supertags of all the words in a sentence are combined by sul  ) stitution and adjunct on . 
As in standard part-of-speech disambiguation , we can use local statistical information in the form of Ngram models based on the distribution of sn-l  ) ertags illa LTAG parsed corl ) us for disamhigua-tion . We . use a trigram model to disambiguateile supcr tags oas to assign one SU l  ) ertagt breach word , in a process termed supertagging . '\[' he trigram model of supcr tagging is very efficient  ( in linear time ) and robust ( Josh and Srinivas ,  \] 994) . 
' 1'o establish the dependency links among the words of the sentence  , we ( ' xph ) it the dei ) endency information present in the supertags . Each su-perl ; ag associated with a word allocate slots for the arguments  o1' the word . These slots have a polarity value re\[lecting their orientation wii  ; hre-Sl ) ect to the anchor o\['the SU l)ertag . Also asso- ( ' iated with a supertag is a list of internal nodes  ( hmluding the root node ) thai , appear in the supertag . Using I ; his information , a simple algo-rithnt may be used to annotate the sentence with d  (  , pe . ndency links . 
4 . 3 Simplification with DeI )   ( m den ( ' y linksTlte output provide ( \[ byt , he dellen cy analyzer not only contains depen ( hm cy links an nmg words but also in ( lical , cs the constituent strncture as cn-code ( I bysn per tags . The constituent information is used to identify whether a supertag contains a clausal constituent and the dependency links are used to identify the span of the clause  . Thus , embedded clauses can easily be identified and ex -tracte  ( t , a kmg with their arguments .  \] ) nnctuation can be used to identify constituents such as appositives which can also  1  ) esel ) arate ( Iont . As with the finite-state al ) l ) roach , the resulting segments may 1 ) e incomplete as in dellt'n detlt clauses . I\[' the segments are to I ) ereassemb h'd , no further processing need be done on them . 
l?igm'e1 shows a rule \[ brextracting relative ( ' lauses , in dependency notation . We tits identify the relative clause tree ( Z ) , and then extract the verb which anchors it along with all of its  ( te-pendents . The right hand side shows the two re-suiting trees  . The gap in the relative clause ( Y ) need only be tilled if the clauses are not going to b creconlbined  . Examples ( 6 ) and ( 7 ) show a sentence belbre and after this rule has applied  . 

Y : NP W
Z:RelClause=>y Z':S:NPY:NPW/
Figure 1: R , ule for extracting relative clauses (6) .   .   . an issue\[that generated unns nalheat in the I Iouse of Commons \]  .   .   . 
(7 ) An issne\[generated unusnal heat in the Ilouse of Commons \]  . The issue .   .   . 
1043 5 Evaluation
The objective of the evaluation is to examine the advantages of the DSM over the FSG-based model for simplification  . In the FSG approach since the input to the simplifier is a set of noun and verb groups  , the rules for the simplifier have to identify basic predicate argument relations to ensure that the right chunks remain together in the output  . 
The simplifier in the DSM has access to information about argument structure  , which makes it much easier to specify simplification patterns involving complete constituents  . Consider exam-pie8, (8) Th . ecreator of Air India , Mr . JRD 7hta , believes that the airline , which celebrated 60 years today , could return to its old days of glory . 
q'he FSG-based model fails to recognize the relative clause on the embedded subject the airline in example  ( 8 )  , because Rule 5 looks for matrix subject NPs . On the other hand , the DSM correctly identifies the relative clause using the rule shown in Figure  1  , which holds for relative clauses in all positions . 
Other differences are in the areas of modifier attachment and rule generality  . In contras to the /) SM approach , the FSG output does not have all modifiers attached  , so the bulk of attachment decisions must be made by the simplification rules  . 
The FSG approach is forced to enumerate all possible variants of the LHS of each simplification rule  ( eg . Subject versus Object relatives , singular versus plural NPs ) whereas in the DSM approach , the rules , encoded in supertags and the associated constituent types  , are more general . 
Preliminary results using the DSM model are very promising  . Using a corpus of newswire data , and only considering relative clause and appositive simplification  , we correctly recovered 25 out of 28 relative clauses and i4 of 14 appositives . We generated 1 spurious relative clause and 2 spurious appositives . A version of the FSG model on the same data recovered  17 relative clauses and 3 appositives . 
6 Discussion
Simplification can be used for two general ( : lasses of tasks . The first is as a preprocessor to a flfll parser so as to reduce \]  ; he parse ambiguity for the parser . Tile second class of tasks demands that the output of the simplifier be freestanding sentences  . Maintaining the coherence of the simplified text raises the fbllowing problems : ? Determining the relative order of the simplified sentences  , which impacts the choice of referring expressions to be used and the overall coherence of the text  . 
? Choosing referring expressions : For instance , when separating relative clauses fi'om the nouns they modify  , copying the head noun into the relative clause is simple  , but leads to quite awkward sounding texts . I Iowever , choosing an appropriate pronoun or choosing between definite and indefinite NPs involves knowledge of complex discourse information  . 
? Selecting the right tense when creating new sentences present similar problems  . 
? No matter how sophisticated the simplification heuristics  , the subtleties of meaning intended by the author may be diluted  , if not lost altogether . For many computer applications , this disadvantage is outweighed by the advantages of simplification  ( i . e . gains of speed and/or accuracy ) , or may be corrected with the use of human l ) ost-processiug . 

This work is partially supported by NSF grant NSF -STC SBR  8920230  , ARPA grant N00014-94 and All . () grant DAAH04-94-G0426 . 

Steven Abney .  1994 . I ) epcndency Grammars and ContextFree Grammars . Manuscript , University of Tubingen , March . 
R . Chandrasekar and S . Ramani .  1996 . Automatic Simplifica . tion of Natural Language Text . 
M~muscript , National Centre for So\[tware Technol : ogy , Bombay . 
R . Chandrasekar .  1994 . A Hybrid Approach to Machine Translation using Man Machine Communication  . Ph . D . thesis , ' Pata Institute of I " undamcnta \] Research /University of Bombay  , Bombay . 
Ralph Grishman .  1995 . Where's the Syntax ? The New York University M UC-6 System . \[ nP ~ occe ( gings of the Sixth Message Understanding Conference  , Columbia , Maryland . 
Jerry Hobbs , Doug Appelt , John Bear , \]) avid Israel , and W . Mary Tyson .  11992 . FAST ( IS : a system , for extracting information from natural anguage text  . 
Technical Report 519, SRI.
Aravind K . Joshi and B . Srinivas .  1994 . Disambiguation of SuperParts of Speech ( or Supertags ) : Almost Parsing . In Proceedings of the 17 th International Conference on Computational Linguistics  ( COLING'94 )  , Kyoto , Japan , August . 
Yves Schabes , Anne Abeilld , and Aravind K . Joshi . 
1988 . Parsing strategies with ' lexicMized'grammars : Application to Tree Adjoining Grammars  . 
In Proceedings of the 12 th International Co@renccon Computational Linguistics  ( COL1NG'88 )  , Budapest , Ilungm : y , August . 
Richard II . Wojcik , Philipt Iarrison , rod John Bremer . 
1993 . Using bracketed parses to evMuate a grain-mar checking  ; rpplication . In Proceedings of the 31'~t Conference of Association of Computational Linguistics  , Ohio State University , Columbus , Ohio . 

