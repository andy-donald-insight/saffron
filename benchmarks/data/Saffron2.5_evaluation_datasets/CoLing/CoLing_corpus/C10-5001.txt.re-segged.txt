Coling 2008: Kernel Engineering for Fast and Easy Design of Natural Language Applications?Tutorial notes , pages 1?91,
Beijing , August 2010
Kernel Engineering for Fast and Easy
Design of Natural Language Applications
Alessandro Moschitti
Department of Information Engineering and Computer Science
University of Trento
Email : moschitti@disi.unitn.it
The 23rd International Conference on Computational Linguistics August 22, 2010 Beijing , China
Schedule
? 14:00 - 15:30 First part
? 15:30 - 16:00 Coffee break
? 16:00 - 17:30 Second part
1
Outline (1)
? Motivation
? Kernel-Based Machines
? Perceptron
? Support Vector Machines
? Kernel Definition
? Kernel Trick
? Mercer?s conditions
? Kernel operators
? Basic Kernels
? Linear Kernel
? Polynomial Kernel
? Lexical Kernel
Outline (2)
? Structural Kernels
? String and Word Sequence Kernels
? Tree Kernels
? Subtree , Syntactic , Partial Tree Kernels ? Applied Examples of Structural Kernels ? Semantic Role Labeling ( SRL ) ? Question Classification ( QC ) ? SVM-Light-TK ? Experiments in classroom with SRL and QC ? Inspection of the input , output , and model files ? Kernel Engineering ? Structure Transformation ? Syntactic Semantic Tree kernels ? Kernel Combinations ? Kernels on Object Pairs ? Kernels for reranking ? Practical Question and Answer Classifier based on SVM-Light-TK ? Combining Kernels ? Conclusion and Future Work
Motivation (1) ? Feature design most difficult aspect in designing a learning system ? complex and difficult phase , e.g ., structural feature representation : ? deep knowledge and intuitions are required ? design problems when the phenomenon is described by many features ? Kernel methods alleviate such problems ? Structures represented in terms of substructures ? High dimensional feature spaces ? Implicit and abstract feature spaces ? Generate high number of features ? Support Vector Machines ? select ? the relevant features ? Automatic Feature engineering side-effect
Part I : Kernel Methods Theory
Sport Cn
Politic C1
Economic C2 . . . . . . . . . . .
Bush declares war
Wonderful
Totti
Yesterday match
Berlusconi acquires
Inzaghi before elections l i i i l ti
Text Classification Problem ? Given : ? a set of target categories : ? the set T of documents , define f : T ? 2C ? VSM ( Salton89?) ? Features are dimensions of a Vector Space.
? Documents and Categories are vectors of feature weights.
? d is assigned to if ? ? d ? ?
C i > th ?
C = C n { } i
C ? In Text Categorization documents are word vectors ? The dot product counts the number of features in common ? This provides a sort of similarity ? ?( d x ) = ? x = (0,..,1,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,1) buy acquisition stocks sell market zx ? ? ? ? ?( d z ) = ? z = (0,..,1,..,0,..,1,..,0,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,0) buy company stocks sell
Linear Classifier ? f ( ? x ) = ? x ? ? w + b = 0, ? x , ? w ? ? n , b ? ? ? The equation of a hyperplane is ? is the vector representing the classifying example ? is the gradient of the hyperplane ? The classification function is x ? w ? ( ) sign ( ( )) h x f x = x x x x o o o o
The main idea of Kernel Functions )( xx ?? ?? ) x (? ) x(?)x(?)x(?)(o ? )( o ? )( o ? )( o??
A mapping example ? Given two masses m1 and m2 , one is constrained ? Apply a force fa to the mass m1 ? Experiments ? Features m1 , m2 and fa ? We want to learn a classifier that tells when a mass m1 will get far away from m2 r mm
Crmmf = ? If we consider the Gravitational Newton Law ? we need to find when f(m1 , m2 , r ) < fa ))(),...,(()(),...,( nn ???? ??? =?= ? The gravitational law is not linear so we need to change space ) ln,ln,ln,(ln ),,,(),,,( aa =? zyxcrmmCrmmf 2ln2lnlnln),,(ln ( ln m1,ln m2,-2ln r )? ( x,y,z )- ln fa + ln C = 0, we can decide without error if the mass will get far away or not ? As 0lnln2lnlnln a ? We need the hyperplane A kernelbased Machine Perceptron training ? ? w ? 0 ; b 1? i ? l || ? x i || do for i = 1 to ? if y i ( ? w k ? ? x i + b k ) ? 0 then ? w k +1 = ? w k +? y i ? x i b k +1 = b k +? y i
R endif endfor while an error is found return k ,( ? w k , b k )
Novikoff?s Theorem
Let S be a nontrivial trainingset and let Let us suppose there is a vector and with ? > 0. Then the maximum number of errors of the perceptron is : * * , || || 1 = w w * * ( , ) , 1,..., , i i y b i l ?+ ? = w x
R t ?? ?= ? ? ? ? i i l
R x ? ? = ? So the classification function ? Note that data only appears in the scalar product Dual Representation for Classification ? ? w = ? j j=1..? ? y j ? x j ? sgn ( ? w ? ? x + b ) = sgn ? j j=1..? ? y j ? x j ? ? x + b ? ? ? ? ? ? ? ?
Dual Representation for Learning ? as well as the updating function ? The learning rate only affects the rescaling of the hyperplane , it does not affect the algorithm , so we can fix 1.? = ? ? if yi ( ? jj=1..?? y j ? x j ? ? x i + b ) ? 0 then ? i =? i +? ? As well as the updating function Dual Perceptron algorithm and Kernel functions ? h(x ) = sgn ( ? w ? ? ?(? x ) + b ? ) = sgn ( ? j j=1..? ? y j ?(? x j ) ? ?(? x ) + b ? ) = = sgn ( ? j i=1..? ? y j k ( ? x j , ? x ) + b ? ) ? if y i ? j j=1..? ? y j k ( ? x j , ? x i ) + b ?? ? ? ? ? ? ? ? ? 0 allora ? i =? i +?
Support Vector Machines ? Hard-margin SVMs ? Soft-margin SVMs
Classifier with a Maximum Margin
Var1
Var2
Margin
Margin
IDEA 1: Select the hyperplane with maximum margin
Var1
Var2
Margin
Support Vectors
Support Vector Machines
Var1
Var2 kbxw ?=+? ?? kbxw =+? ?? 0=+? bxw ?? k k w ?
The margin is equal to 2 k w
Var1
Var2 kbxw ?=+? ?? kbxw =+? ?? 0=+? bxw ?? k k w ?
The margin is equal to 2 k w
We need to solve ? max 2 k || ? w || ? w ? ? x + b ? + k , if ? x is positive ? w ? ? x + b ? ? k , if ? x is negative
Support Vector Machines
Var1
Var2 1w x b ? + = ?? ? 1w x b ? + = ? ? 0=+? bxw ?? w ?
There is a scale for which k=1.
The problem transforms in : ? max ? w || ? w ? ? x + b ? +1, if ? x is positive ? w ? ? x + b ? ?1, if ? x is negative ? ? ? max ? w || ? w ? ? x i + b ? +1, y i =1 ? w ? ? x i + b ? ?1, y i = -1 ? max ? w || y i ( ? w ? ? x i + b ) ?1 ? min || ? w || i ( ? w ? ? x i + b ) ?1 ? min || ? w || y i ( ? w ? ? x i + b ) ?1 ? ? ? ? ? ?
Optimization Problem ? Optimal Hyperplane : ? Minimize ? Subject to ? The dual problem is simpler libxwy ww ii ,...,1,1))(( )( = ?? ???
Dual Optimization Problem ? To solve the dual problem we need to evaluate : ? Given the Lagrangian associated with our problem ? Let us impose the derivatives to 0, with respect to w?
Dual Transformation ( cont?d ) ? and wrt b ? Then we substituted them in the objective function
Khun-Tucker Theorem ? Necessary and sufficient conditions to optimality ? Lagrange constraints : ? Karush-Kuhn-Tucker constraints ? Support Vectors have not null ? To evaluate b , we can apply the following equation ? a i i=1 l ? y i = 0, ? w = ? i i=1 l ? y i ? x i libwxy iii ,...,1,0]1)([ ==?+?? ??? i ?
Soft Margin SVMs
Var1
Var2 1w x b ? + = ?? ? 1w x b ? + = ? ? 0=+? bxw ?? w ? i ? slack variables are added Some errors are allowed but they should penalize the objective function i ?
Var1
Var2 1w x b ? + = ?? ? 1w x b ? + = ? ? 0=+? bxw ?? w ? i ? The new constraints are The objective function penalizes the incorrect classified examples C is the tradeoff between margin and the error ? y i ( ? w ? ? x i + b ) ?1?? i ? ? x i where ? i ? 0 ? min || ? w || i i ?
Dual formulation ? By deriving wrt ? ? w , ? ? and b Substitution in the objective function ? of Kronecker ij ?
Soft Margin Support Vector Machines ? The algorithm tries to keep ? i low and maximize the margin ? NB : The number of error is not directly minimized ( NP-complete problem ); the distances from the hyperplane are minimized ? If C ??, the solution tends to the one of the hard-margin algorithm ? Attention !!!: if C = 0 we get = 0, since ? If C increases the number of error decreases . When C tends to infinite the number of errors must be 0, i.e . the hard-margin formulation |||| w ? ? min || ? w || i i ? ? y i ( ? w ? ? x i + b ) ?1?? i ? ? x i ? i ? 0 ? y i b ?1?? i ? ? x i i ? Var1
Var2 0=+? bxw ?? ? i
Var1
Var2 0=+? bxw ??
Soft Margin SVM Hard Margin SVM
Kernels in Support Vector Machines ? In Soft Margin SVMs we maximize : ? By using kernel functions we rewrite the problem as : ? Kernels are the product of mapping functions such as ? ? x ? ? n , ? ? (? x ) = (? ? x ),? ? x ),...,? m ( ? x )) ? ? m
The Kernel Gram Matrix ? With KM-based learning , the sole information used from the training data set is the Kernel Gram Matrix ? If the kernel is valid , K is symmetric definite-positive .
25
Valid Kernels
Valid Kernels cont?d ? If the matrix is positive semidefinite then we can find a mapping ? implementing the kernel function ? Let us consider ?
K = K ( ? x i , ? x j ) ( ) i , j=1 n ? K symmetric ? ? V : for Takagi factorization of a complex-symmetric matrix , where : ? ? is the diagonal matrix of the eigenvalues ? t of K ? are the eigenvectors , i.e . the columns of V ? Let us assume lambda values nonnegative ?
K = V ? ?
V ? ? v t = v ti ( ) i =1 n ? ? : ? x i ? ? t v ti ( ) t =1 n ? ? n , i =1,..,n Mercer?s Theorem ( sufficient conditions ) ? ?( ? x i ) ? ?( ? x j ) = ? t v ti t=1 n ? v tj = V ? ?
V ( ) ij = K ij = K ( ? x i , ? x j ) ? Therefore , ? which implies that K is a kernel function ? ? z ? z ? ? z = ? ?
V ? v s ? ?
V ? v s = ? v s ' V ? ? ?
V ? v s = ? v s ' K ? v s = ? v s ' ? s ? v s = ? s ? v s ? Suppose we have negative eigenvalues ? s and eigenvectors the following point ? has the following norm : this contradicts the geometry of the space.
? ? v s ? ? z = v si ?( ? x i ) i=1 n ? = v si ? t v ti ( ) t = i=1 n ? ? ?
V ? v s
Is it a valid kernel ? ? It may not be a kernel so we can use M??M ? k(x,z ) = k1(x,z)+k2(x,z ) ? k(x,z ) = k1(x,z)*k2(x,z ) ? k(x,z ) = ? k1(x,z ) ? k(x,z ) = f(x)f(z ) ? k(x,z ) = k1(?(x),?(z )) ? k(x,z ) = x'Bz
Basic Kernels for unstructured data ? Linear Kernel ? Polynomial Kernel ? Lexical kernel ? String Kernel ? In Text Categorization documents are word vectors ? The dot product counts the number of features in common ? This provides a sort of similarity ? ?( d x ) = ? x = (0,..,1,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,1) buy acquisition stocks sell market zx ? ? ? ? ?( d z ) = ? z = (0,..,1,..,0,..,1,..,0,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,0) buy company stocks sell Feature Conjunction ( polynomial Kernel ) ? The initial vectors are mapped in a higher space ? More expressive , as encodes Stock+Market vs . Downtown+Market features ? We can smartly compute the scalar product as )1,2,2,2,,(),( ),()1()1( )()( zxKzxzxzx zxzxzzxxzxzx zzzzzzxxxxxx zx
Poly ? ? ? ? ? ? =+?=++= =+++++= =?= =??? )( industry telephone market company product
Doc 1 Doc 2
Lexical Semantic Kernel [ CoNLL 2005] ? The document similarity is the SK function : ? where s is any similarity function between words , e.g . WordNet [ Basili et al,2005] similarity or LSA [ Cristianini et al , 2002] ? Good results when training data is small ?
SK(d w zx ? ? ? ? ?(" bank ") = ? x = (0,..,1,..,0,..,1,..,0,......1,..,0,..,1,..,0,..,1,..,0) ? counts the number of common substrings bank ank bnk bk b ? ?(" rank ") = ? z = (1,..,0,..,0,..,1,..,0,......0,..,1,..,0,..,1,..,0,..,1) rank ank rnk rk r ? ? x ? ? z = ?(" bank ") ? ?(" rank ") = k("bank","rank")
String Kernel ? Given two strings , the number of matches between their substrings is evaluated ? E.g . Bank and Rank ? B , a , n , k , Ba , Ban , Bank , Bk , an , ank , nk,..
? R , a , n , k , Ra , Ran , Rank , Rk , an , ank , nk,..
? String kernel over sentences and texts ? Huge space but there are efficient algorithms , where , where i1 +1
Kernel between Bank and Rank
Efficient Evaluation ? Dynamic Programming technique ? Evaluate the spectrum string kernels ? Substrings of size p ? Sum the contribution of the different spectra
An example : SK(?Gatta?,?Cata ?) ? First , evaluate the SK with size p=1, i.e . ? a ?, ? a?,?t?,?t?,?a?,?a ? ? Store this in the table ? SK p=1 ? Evaluate the weight of the string of size p in case a character will be matched ? This is done by multiplying the double summation by the number of substrings of size p1 Evaluating the Predictive DP on strings of size 2 ( second row ) ? Let?s consider substrings of size 2 and suppose that : ? we have matched the first ? a ? ? we will match the next character that we will add to the two strings ? We compute the weights of matches above at different string positions with some not-yet known character ??? ? If the match occurs immediately after ? a ? the weight will be ?1+1 x ?1+1 = ?4 and we store just ?2 in the DP entry in [? a?,?a ?] ? If the match for ? gatta ? occurs after ? t ? the weight will be ?1+2 ( x ?2 = ?5) since the substring for it will be with ? a ??? ? We write such prediction in the entry [? a?,?t ?] ? Same rationale for a match after the second ? t ?: we have the substring ? a ???? ( matching with ? a ?? from ? catta ?) for a weight of ?3+1 ( x ?2) Evaluating the DP wrt different positions ( third row ) ? If the match occurs after ? t ? of ? cata ?, the weight will be ?2+1 ( x ?2 = ?5 ) since it will be with the string ? a ???, with a weight of ?3 ? If the match occurs after ? t ? of both ? gatta ? and ? cata ?, there are two ways to compose substring of size two : ? a ??? with weight ?4 or ? t ?? with weight ?2 ? the total is ?2+?4 ? The final case is a match after the last ? t ? of both ? cat ? and ? gatta ? ? There are three possible substrings of ? gatta ?: ? ? a ????, ? t ???, ? t ?? for ? gatta ? with weight ?3 , ?2 or ?, respectively.
? There are two possible substrings of ? cata ? ? ? a ???, ? t ?? with weight ?2 and ? ? Their match gives weights : ?5 , ?3, ?2 ? by summing : ?5 + ?3 + ?2
Evaluating SK of size 2 using DP2 ? The number ( weight ) of substrings of size 2 between ? gat ? and ? cat ? is ?4 = ?2 ([? a?,?a ?] entry of DP ) x ?2(cost of one character ), where a = ? t ? and b = ? t?.
? Between ? gatta ? and ? cata ? is ?7 + ?5 + ?4, i.e the matches of ? a??a ?, ? t?a ?, ? ta ? with ? a?a ? and ? ta ?. ? SK p = 2 ? Subtree , Subset Tree , Partial Tree kernels ? Efficient computation
Example of a parse tree ? ? John delivers a talk in Rome?
S ? N VP
VP ? V NP PP
PP ? IN N
N ? Rome
N
Rome
S
N
NP
D N
VP
V John in delivers a talk
PP
IN [ Collins and Duffy , 2002]
NP
D N
VP
V delivers a talk
The overall fragment set
NP
D
VP a Children are not divided
Explicit kernel space zx ? ? ? ? ?( T x ) = ? x = (0,..,1,..,0,..,1,..,0,..,1,..,0,..,1,..,0,..,1,..,0,..,1,..,0) ? counts the number of common substructures ? ?( T z ) = ? z = (1,..,0,..,0,..,1,..,0,..,1,..,0,..,1,..,0,..,0,..,1,..,0,..,0) ? ? x ? ? z = ?( T x ) ? ?( T z ) = K(T x , T z ) = = n x ? T x ? ?( n x , n z ) n z ? T z ? Efficient evaluation of the scalar product ? [ Collins and Duffy , ACL 2002] evaluate ? in O(n2): ? ?( n x , n z ) = 0, if the productions are different else ?( n x , n z ) =1, if pre - terminals else ?( n x , n z ) = (1+ ?( ch(n x , j),ch(n z , j ))) j=1 nc(n x ) ? ? ? x ? ? z = ?( T x ) ? ?( T z ) = K(T x , T z ) = = n x ? T x ? ?( n x , n z ) n z ? T z ? ? Normalization ? ?( n x , n z ) = ?, if pre - terminals else ?( n x , n z ) = ? (1+ ?( ch(n x , j),ch(n z , j ))) j=1 nc(n x ) ? ? ?
K ( T x , T z ) =
K(T x , T z )
K(T x , T x ) ? K(T z , T z ) ? Decay factor SubTree ( ST ) Kernel [ Vishwanathan and Smola , 2002]
NP
D
N a talk
D N a talk
NP
D N
VP
V delivers a talk
V delivers ? ?( n x , n z ) = 0, if the productions are different else ?( n x , n z ) =1, if pre - terminals else ?( n x , n z ) = (1+ ?( ch(n x , j),ch(n z , j ))) j=1 nc(n x ) ? ? Given the equation for the SST kernel
Evaluation ? ?( n x , n z ) = 0, if the productions are different else ?( n x , n z ) =1, if pre - terminals else ?( n x , n z ) = ?( ch(n x , j),ch(n z , j )) j=1 nc(n x ) ? ? Given the equation for the SST kernel where P(nx ) and P(nz ) are the production rules used at nodes nx and nz ?
K(T x , T z ) = ?( n x , n z ) n x , n z ? NP ?
NP = n x , n z ? T x ? T z :?( n x , n z ) ? 0 { } = = n x , n z ? T x ? T z : P(n x ) = P(n z ) { } ,
Algorithm ? We order the production rules used in Tx and Tz , at loading time ? At learning time we may evaluate NP in | Tx|+|Tz | running time ? If Tx and Tz are generated by only one production rule ? O(|Tx|?|Tz | )?
Observations ? We order the production rules used in Tx and Tz , at loading time ? At learning time we may evaluate NP in | Tx|+|Tz | running time ? If Tx and Tz are generated by only one production rule ? O(|Tx|?|Tz | )? Very Unlikely !!!!
NP
D N
VP
V gives a talk
NP
D N
VP
V a talk
NP
D N
VP a talk
NP
D N
VP a
NP
D
VP a
NP
D
VP
NP
N
VP
NP
N
NP NP
D N D
NP ?
VP ? SST satisfies the constraint ? remove 0 or all children at a time?.
? If we relax such constraint we get more general substructures [ Kashima and Koyanagi , 2002]
Weighting Problems ? Both matched pairs give the same contribution.
? Gap based weighting is needed.
? A novel efficient evaluation has to be defined
NP
D N
VP
V gives a talk
NP
D N
VP
V a talk
NP
D N
VP
V gives a talk gives
JJ good
NP
D N
VP
V gives a talk
JJ bad
NP
D N
VP
V brought a cat
NP
D N
VP
V a cat
NP
D N
VP a cat
NP
D N
VP a
NP
D
VP a
NP
D
VP
NP
N
VP
NP
N
NP NP
D N D
NP ?
VP ? SST + String Kernel with weighted gaps on
Nodes ? children
Partial Tree Kernel ? By adding two decay factors we obtain : ? In [ Taylor and Cristianini , 2004 book ], sequence kernels with weighted gaps are factorized with respect to different subsequence sizes.
? We treat children as sequences and apply the same theory
Dp
Efficient Evaluation (2) ? The complexity of finding the subsequences is ? Therefore the overall complexity is where ? is the maximum branching factor ( p = ?)
SVM-light-TK Software ? Encodes ST , SST and combination kernels in SVMlight [ Joachims , 1999] ? Available at http://dit.unitn.it/~moschitt / ? Tree forests , vector sets ? The new SVM-Light-TK toolkit will be released asap ? ? What does Html stand for ?? ? 1 | BT | ( SBARQ ( WHNP ( WP What))(SQ ( AUX does)(NP ( NNP S.O.S.))(VP ( VB stand)(PP ( IN for ))))(. ?)) | BT | ( BOW ( What *)( does *)( S.O.S . *)( stand *)( for *)(? *)) | BT | ( BOP ( WP *)( AUX *)( NNP *)( VB *)( IN *)(. *)) | BT | ( PAS ( ARG0 ( RA1 ( What *)))( ARG1 ( A1 ( S.O.S . NNP)))(ARG2 ( rel stand ))) | ET | 1:1 21:2.742439465642236E-4 23:1 30:1 36:1 39:1 41:1 46:1 49:1 66:1 152:1 274:1 333:1 | BV | 2:1 21:1.4421347148614654E-4 23:1 31:1 36:1 39:1 41:1 46:1 49:1 52:1 66:1 152:1 246:1 333:1 392:1 | EV|
Basic Commands ? Training and classification ? ./ svm_learn - t 5 - C T train.dat model ? ./ svm_classify test.dat model ? Learning with a vector sequence ? ./ svm_learn - t 5 - C V train.dat model ? Learning with the sum of vector and kernel sequences ? ./ svm_learn - t 5 - C + train.dat model
Kernel Engineering approaches ? Basic Combinations ? Canonical Mappings , e.g . object transformations ? Merging of Kernels ? Kernel Combinations : , pTree pTree
PTree p p
Tree
Tree
PTree pTreePTreepTreePTree
KK
KK
K
K
K
K
K
K
KKKKKK ? ? =+?= ?=+?= ?+ ?+ ?? kernel Tree featuresflat of kernel polynomial 3TreepKK Object Transformation [ Moschitti et al CLJ 2008] ? Canonical Mapping , ? M () ? object transformation , ? e . g . a syntactic parse tree into a verb subcategorization frame tree.
? Feature Extraction , ? E () ? maps the canonical structure in all its fragments ? different fragment spaces , e . g . ST , SST and PT.
),()()( ))(())(()()(),(
SSKSS
OOOOOOK
EEE
MEME =?= ?=?= ?? ?????? ? In an event : ? target words describe relation among different entities ? the participants are often seen as predicate's arguments.
? Example:
Paul gives a talk in Rome
Predicate Argument Classification ? In an event : ? target words describe relation among different entities ? the participants are often seen as predicate's arguments.
? Example : [ Arg0 Paul ] [ predicate gives ] [ Arg1 a talk ] [ ArgM in Rome ]
Given a sentence , a predicate p : 1.? Derive the sentence parse tree 2.? For each node pair < Np,Nx > a .? Extract a feature representation set F b .? If Nx exactly covers the Arg-i , F is one of its positive examples c .? F is a negative example otherwise Vector Representation for the linear kernel
Phrase Type
Predicate
Word
Head Word
Parse Tree
Path
Voice Active osition Right
PAT Kernel [ Moschitti , ACL 2004]
S
N
NP
D N
VP
V Paul in delivers a talk
PP
IN NP jj
Fv,arg.0 formal N style
Arg . 0 a ) S
N
NP
D N
VP
V Paul in delivers a talk
PP
IN NP jj formal N style
Fv,arg.1 b ) S
N
NP
D N
VP
V Paul in delivers a talk
PP
IN NP jj formal N style Arg . 1
Fv,arg.M c)
Arg.M ? These are Semantic Structures ? Given the sentence : [ Arg0 Paul ] [ predicate delivers ] [ Arg1 a talk ] [ ArgM in formal Style ]
NP
D N
VP
V delivers a talk
S
N
Paul in
PP
IN NP jj formal N style Arg . 1
SubCategorization Kernel ( SCF ) [ Moschitti , ACL 2004]
S
N
NP
D N
VP
V Paul in delivers a talk
PP
IN NP jj formal N style
Arg . 1
Arg . M
Arg . 0
Predicate ? PropBank and PennTree bank ? about 53,700 sentences ? Sections from 2 to 21 train ., 23 test ., 1 and 22 dev.
? Arguments from Arg0 to Arg5, ArgA and ArgM for a total of 122,774 and 7,359 ? FrameNet and Collins ? automatic trees ? 24,558 sentences from the 40 frames of Senseval 3 ? 18 roles ( same names are mapped together ) ? Only verbs ? 70% for training and 30% for testing Argument Classification with Poly Kernel Argument Classification on PAT using different Tree Fragment Extractor 0.75 0.78 0.80 0.83 0.85 0.88 0 10 20 30 40 50 60 70 80 90 100 % Training Data
A c c u r a c y ---
ST SST
Linear PT ? ProbBank arguments vs . Semantic Roles
Kernel Engineering : Node marking
Node Marking Effect
CMST
MMST
Experiments ? PropBank and PennTree bank ? about 53,700 sentences ? Charniak trees from CoNLL 2005 ? Boundary detection : ? Section 2 training ? Section 24 testing ? PAF and MPAF Predicate Argument Feature ( PAF ) vs . Marked PAF ( MPAF ) [ Moschitti et al ACL-ws-2005] ? Syntactic/Semantic Tree Kernel ? Kernel Combinations ? Experiments Merging of Kernels [ Bloehdorn & Moschitti , ECIR 2007 & CIKM 2007]
NP
D N
VP
V gives a talk
N good
NP
D N
VP
V gives a talk
N solid
Delta Evaluation is very simple ? Definition : What does HTML stand for ? ? Description : What's the final line in the Edgar Allan Poe poem " The Raven "? ? Entity : What foods can cause allergic reaction in people ? ? Human : Who won the Nobel Peace Prize in 1992? ? Location : Where is the Statue of Liberty ? ? Manner : How did Bob Marley die ? ? Numeric : When was Martin Luther King Jr . born ? ? Organization : What company makes Bentley cars ? Question Classifier based on Tree Kernels ? Question dataset ( http://l2r.cs.uiuc.edu/~cogcomp/Data/QA/QC /) [ Lin and Roth , 2005]) ? Distributed on 6 categories : Abbreviations , Descriptions , Entity , Human , Location , and Numeric.
? Fixed split 5500 training and 500 test questions ? Crossvalidation (10-folds ) ? Using the whole question parse trees ? Constituent parsing ? Example ? What is an offer of direct stock purchase plan ?? ? BOW , POS are obtained with a simple tree , e.g.
? PT ( parse tree ) ? PAS ( predicate argument structure ) ?
BOX is What an offer an * * * * *
Similarity based on WordNet
Multiple Kernel Combinations ? The classifier detects if a pair ( question and answer ) is correct or not ? A representation for the pair is needed ? The classifier can be used to rerank the output of a basic QA system
Dataset 2: TREC data ? 138 TREC 2001 test questions labeled as ? description ? ? 2,256 sentences , extracted from the best ranked paragraphs ( using a basic QA system based on
Lucene search engine on TREC dataset ) ? 216 of which labeled as correct by one annotator ? 138 TREC 2001 test questions labeled as ? description ? ? 2,256 sentences , extracted from the best ranked paragraphs ( using a basic QA system based on
Lucene search engine on TREC dataset ) ? 216 of which labeled as correct by one annotator A question is linked to many answers : all its derived pairs cannot be shared by training and test sets Bags of words ( BOW ) and POStags ( POS ) ? To save time , apply STK to these trees : ?
BOX is What an offer of * * * * * ?
BOX
VBZ WHNP DT NN IN * * * * * ? What is an offer of ?? ( word sequence , WSK ) ? What_is_offer ? What_is ? WHNP VBZ DT NN IN?(POS sequence , POSSK ) ? WHNP_VBZ_NN ? WHNP_NN_IN
Syntactic Parse Trees ( PT ) ? [ ARG1 Antigens ] were [ AM?TMP originally ] [ rel defined ] [ ARG2 as non-self molecules].
? [ ARG0 Researchers ] [ rel describe ] [ ARG1 antigens][ARG2 as foreign molecules ] [ ARGM?LOC in the body]
Kernels and Combinations ? Exploiting the property : k(x,z ) = k1(x,z)+k2(x,z ) ? BOW , POS , WSK , POSSK , PT , PASPTK ? BOW+POS , BOW+PT , PT+POS , ?
SK WSK PT
PAS_S
STK PAS_P
TK
BOW+
POS BOW+
PT
POS_
SK+P
T
WSK+
PT
POS_
SK+P
T+PAS _SST
K
POS_
SK+P
T+PAS _PTK
F1-me asu re
Kernel Type
Results on TREC Data (5 folds cross validation )
SK WSK PT
PAS_S
STK PAS_P
TK
BOW+
POS BOW+
PT
POS_
SK+P
T
WSK+
PT
POS_
SK+P
T+PAS _SST
K
POS_
SK+P
T+PAS _PTK
F1-me asu re
Kernel Type
SK WSK PT
PAS_S
STK PAS_P
TK
BOW+
POS BOW+
PT
POS_
SK+P
T
WSK+
PT
POS_
SK+P
T+PAS _SST
K
POS_
SK+P
T+PAS _PTK
F1-me asu re
Kernel Type
Results on TREC Data (5 folds cross validation )
SK WSK PT
PAS_S
STK PAS_P
TK
BOW+
POS BOW+
PT
POS_
SK+P
T
WSK+
PT
POS_
SK+P
T+PAS _SST
K
POS_
SK+P
T+PAS _PTK
F1-me asu re
Kernel Type
SK WSK PT
PAS_S
STK PAS_P
TK
BOW+
POS BOW+
PT
POS_
SK+P
T
WSK+
PT
POS_
SK+P
T+PAS _SST
K
POS_
SK+P
T+PAS _PTK
F1-me asu re
Kernel Type
Results on TREC Data (5 folds cross validation )
SK WSK PT
PAS_S
STK PAS_P
TK
BOW+
POS BOW+
PT
POS_
SK+P
T
WSK+
PT
POS_
SK+P
T+PAS _SST
K
POS_
SK+P
T+PAS _PTK
F1-me asu re
Kernel Type
SK WSK PT
PAS_S
STK PAS_P
TK
BOW+
POS BOW+
PT
POS_
SK+P
T
WSK+
PT
POS_
SK+P
T+PAS _SST
K
POS_
SK+P
T+PAS _PTK
F1-me asu re
Kernel Type
BOW ? 24 POSSK+STK+PAS_PTK ? 39 ?62 % of improvement
Kernels for Reranking ? Local classifier generates the most likely set of hypotheses.
? These are used to build annotation pairs , .
? positive instances if hi more correct than hj , ? A binary classifier decides if hi is more accurate than hj.
? Each candidate annotation hi is described by a structural representation ? h i , h j
Reranking framework
Local Model ? Pairs of parse trees ( Collins and Duffy , 2002)
Reranking concept labeling [ Dinarelli et al 2009] ? I have a problem with my monitor hi : I NULL have NULL a NULL problem PROBLEM-
B with NULL my NULL monitor HW-B hj : I NULL have NULL a NULL problem HW-B with NULL my NULL monitor
Multilevel Tree ? FST CER from 23.2 to 16.01 Reranking for NamedEntity Recognition [ Vien et al 2010] ? CRF F1 from 84.86 to 88.16 [ Moschitti et al CoNLL 2006] ? SVMs F1 from 75.89 to 77.25
Conclusions ? Kernel methods and SVMs are useful tools to design language applications ? Kernel design still requires some level of expertise ? Engineering approaches to tree kernels ? Basic Combinations ? Canonical Mappings , e.g.
? Node Marking ? Merging of kernels in more complex kernels ? Easy modeling produces state-of-the-art accuracy in many tasks , RTE , SRL , QC , NER , RE ? SVM-Light-TK efficient tool to use them ? Once we have found the right kernel , are we satisfied ? ? What about knowing the most relevant features ? ? Can we speed up learning/classification at real-application scenario level ? ? The answer is reverse kernel engineering : ? [ Pighin&Moschitti , CoNLL2009, EMNLP2009, CoNLL2010] ? Mine the most relevant fragments according to SVMs gradient ? Use the linear space ? Software for reverse kernel engineering available in the next months
Thank you ? Alessandro Moschitti and Silvia Quarteroni , Linguistic Kernels for Answer Reranking in Question Answering Systems , Information and Processing Management , ELSEVIER , 2010.
? Yashar Mehdad , Alessandro Moschitti and Fabio Massimo Zanzotto . Syntactic / Semantic Structures for Textual Entailment Recognition . Human Language Technology - North American chapter of the Association for Computational Linguistics ( HLT-
NAACL ), 2010, Los Angeles , Calfornia.
? Daniele Pighin and Alessandro Moschitti . On Reverse Feature Engineering of Syntactic Tree Kernels . In Proceedings of the 2010 Conference on Natural Language Learning , Upsala , Sweden , July 2010. Association for Computational Linguistics.
? Thi Truc Vien Nguyen , Alessandro Moschitti and Giuseppe Riccardi . Kernel-based Reranking for Entity Extraction . In proceedings of the 23rd International Conference on Computational Linguistics ( COLING ), August 2010, Beijing , China.
References ? Alessandro Moschitti . Syntactic and semantic kernels for short text pair categorization.
In Proceedings of the 12th Conference of the European Chapter of the ACL ( EACL 2009), pages 576?584, Athens , Greece , March 2009. Association for Computational
Linguistics.
? Truc-Vien Nguyen , Alessandro Moschitti , and Giuseppe Riccardi . Convolution kernels on constituent , dependency and sequential structures for relation extraction . In Proceedings of the Conference on Empirical Methods in Natural Language Processing , pages 1378?1387, Singapore , August 2009. Association for Computational Linguistics.
? Marco Dinarelli , Alessandro Moschitti , and Giuseppe Riccardi . Reranking models basedon small training data for spoken language understanding . In Proceedings of the Conference on Empirical Methods in Natural Language Processing , pages 1076?1085, Singapore , August 2009. Association for Computational Linguistics.
? Alessandra Giordani and Alessandro Moschitti . Syntactic Structural Kernels for Natural Language Interfaces to Databases . In ECML/PKDD , pages 391?406, Bled , Slovenia , 2009.
84
References ? Alessandro Moschitti , Daniele Pighin and Roberto Basili . Tree Kernels for Semantic Role Labeling , Special Issue on Semantic Role Labeling , Computational Linguistics
Journal . March 2008.
? Fabio Massimo Zanzotto , Marco Pennacchiotti and Alessandro Moschitti , A Machine Learning Approach to Textual Entailment Recognition , Special Issue on Textual Entailment Recognition , Natural Language Engineering , Cambridge University Press ., Arabic Language using Kernel Methods . In proceedings of the 46th Conference of the Association for Computational Linguistics ( ACL'08). Main Paper Section . Columbus,
OH , USA , June 2008.
? Alessandro Moschitti , Silvia Quarteroni , Kernels on Linguistic Structures for Answer Extraction . In proceedings of the 46th Conference of the Association for Computational Linguistics ( ACL'08). Short Paper Section . Columbus , OH , USA , June 2008.
References ? Yannick Versley , Simone Ponzetto , Massimo Poesio , Vladimir Eidelman , Alan Jern , Jason Smith , Xiaofeng Yang and Alessandro Moschitti , BART : A Modular Toolkit for Coreference Resolution , In Proceedings of the Conference on Language Resources and Evaluation , Marrakech , Marocco , 2008.
? Alessandro Moschitti , Kernel Methods , Syntax and Semantics for Relational Text Categorization . In proceeding of ACM 17th Conference on Information and Knowledge Management ( CIKM ). Napa Valley , California , 2008.
? Bonaventura Coppola , Alessandro Moschitti , and Giuseppe Riccardi . Shallow semantic parsing for spoken language understanding . In Proceedings of HLTNAACL Short Papers , pages 85?88, Boulder , Colorado , June 2009. Association for Computational
Linguistics.
? Alessandro Moschitti and Fabio Massimo Zanzotto , Fast and Effective Kernels for Relational Learning from Texts , Proceedings of The 24th Annual International Conference on Machine Learning ( ICML 2007).
85
References ? Alessandro Moschitti , Silvia Quarteroni , Roberto Basili and Suresh Manandhar , Exploiting Syntactic and Shallow Semantic Kernels for Question/Answer Classification , Proceedings of the 45th Conference of the Association for Computational Linguistics ( ACL ), Prague , June 2007.
? Alessandro Moschitti and Fabio Massimo Zanzotto , Fast and Effective Kernels for Relational Learning from Texts , Proceedings of The 24th Annual International Conference on Machine Learning ( ICML 2007), Corvallis , OR , USA.
? Daniele Pighin , Alessandro Moschitti and Roberto Basili , RTV : Tree Kernels for Thematic Role Classification , Proceedings of the 4th International Workshop on Semantic Evaluation ( SemEval-4), English Semantic Labeling , Prague , June 2007.
? Stephan Bloehdorn and Alessandro Moschitti , Combined Syntactic and Semanitc Kernels for Text Classification , to appear in the 29th European Conference on Information Retrieval ( ECIR ), April 2007, Rome , Italy.
? Fabio Aiolli , Giovanni Da San Martino , Alessandro Sperduti , and Alessandro Moschitti , Efficient Kernel-based Learning for Trees , to appear in the IEEE Symposium on Computational Intelligence and Data Mining ( CIDM ), Honolulu , Hawaii , 2007
References ? Alessandro Moschitti , Silvia Quarteroni , Roberto Basili and Suresh Manandhar , Exploiting Syntactic and Shallow Semantic Kernels for Question/Answer Classification , Proceedings of the 45th Conference of the Association for Computational Linguistics ( ACL ), Prague , June 2007.
? Alessandro Moschitti , Giuseppe Riccardi , Christian Raymond , Spoken Language Understanding with Kernels for Syntactic/Semantic Structures , Proceedings of IEEE Automatic Speech Recognition and Understanding Workshop ( ASRU2007), Kyoto,
Japan , December 2007 ? Stephan Bloehdorn and Alessandro Moschitti , Combined Syntactic and Semantic Kernels for Text Classification , to appear in the 29th European Conference on Information Retrieval ( ECIR ), April 2007, Rome , Italy.
? Stephan Bloehdorn , Alessandro Moschitti : Structure and semantics for expressive text kernels . In proceeding of ACM 16th Conference on Information and Knowledge Management ( CIKM-short paper ) 2007: 861-864, Portugal.
86
References ? Fabio Aiolli , Giovanni Da San Martino , Alessandro Sperduti , and Alessandro Moschitti , Efficient Kernel-based Learning for Trees , to appear in the IEEE Symposium on Computational Intelligence and Data Mining ( CIDM ), Honolulu , Hawaii , 2007.
? Alessandro Moschitti , Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees . In Proceedings of the 17th European Conference on Machine
Learning , Berlin , Germany , 2006.
? Fabio Aiolli , Giovanni Da San Martino , Alessandro Sperduti , and Alessandro Moschitti , Fast Online Kernel Learning for Trees , International Conference on Data Mining ( ICDM ) 2006 ( short paper).
? Stephan Bloehdorn , Roberto Basili , Marco Cammisa , Alessandro Moschitti , Semantic Kernels for Text Classification based on Topological Measures of Feature Similarity . In Proceedings of the 6th IEEE International Conference on Data Mining ( ICDM 06), Hong Kong , 1822 December 2006. ( short paper).
References ? Roberto Basili , Marco Cammisa and Alessandro Moschitti , A Semantic Kernel to classify texts with very few training examples , in Informatica , an international journal of
Computing and Informatics , 2006.
? Fabio Massimo Zanzotto and Alessandro Moschitti , Automatic learning of textual entailments with cross-pair similarities . In Proceedings of COLINGACL , Sydney,
Australia , 2006.
? AnaMaria Giuglea and Alessandro Moschitti , Semantic Role Labeling via FrameNet , VerbNet and PropBank . In Proceedings of COLINGACL , Sydney , Australia , 2006.
? Alessandro Moschitti , Making tree kernels practical for natural language learning . In Proceedings of the Eleventh International Conference on European Association for Computational Linguistics , Trento , Italy , 2006.
? Alessandro Moschitti , Daniele Pighin and Roberto Basili . Semantic Role Labeling via Tree Kernel joint inference . In Proceedings of the 10th Conference on Computational Natural Language Learning , New York , USA , 2006.
87
References ? Roberto Basili , Marco Cammisa and Alessandro Moschitti , Effective use of Wordnet semantics via kernelbased learning . In Proceedings of the 9th Conference on Computational Natural Language Learning ( CoNLL 2005), Ann Arbor ( MI ), USA , 2005 ? Alessandro Moschitti , A study on Convolution Kernel for Shallow Semantic Parsing . In proceedings of the 42th Conference on Association for Computational Linguistic ( ACL2004), Barcelona , Spain , 2004.
? Alessandro Moschitti and Cosmin Adrian Bejan , A Semantic Kernel for Predicate Argument Classification . In proceedings of the Eighth Conference on Computational Natural Language Learning ( CoNLL2004), Boston , MA , USA , 2004.
An introductory book on SVMs , Kernel methods and Text Categorization ? V . Vapnik . The Nature of Statistical Learning Theory . Springer , 1995.
? P . Bartlett and J . ShaweTaylor , 1998. Advances in Kernel Methods - Support Vector Learning , chapter Generalization Performance of Support Vector Machines and other Pattern Classifiers . MIT Press.
? David Haussler . 1999. Convolution kernels on discrete structures.
Technical report , Dept . of Computer Science , University of California at
Santa Cruz.
? Lodhi , Huma , Craig Saunders , John Shawe Taylor , Nello Cristianini , and Chris Watkins . Text classification using string kernels . JMLR,2000 ? Sch?lkopf , Bernhard and Alexander J . Smola . 2001. Learning with Kernels : Support Vector Machines , Regularization , Optimization , and Beyond . MIT Press , Cambridge , MA , USA.
Non-exhaustive reference list from other authors ? N . Cristianini and J . ShaweTaylor , An introduction to support vector machines ( and other kernelbased learning methods ) Cambridge
University Press , 2002 ? M . Collins and N . Duffy , New ranking algorithms for parsing and tagging : Kernels over discrete structures , and the voted perceptron . In
ACL02, 2002.
? Hisashi Kashima and Teruo Koyanagi . 2002. Kernels for semistructured data . In Proceedings of ICML?02.
? S.V.N . Vishwanathan and A.J . Smola . Fast kernels on strings and trees . In Proceedings of NIPS , 2002.
? Nicola Cancedda , Eric Gaussier , Cyril Goutte , and Jean Michel Renders . 2003. Word sequence kernels . Journal of Machine Learning Research , 3:1059?1082. D . Zelenko , C . Aone , and A . Richardella . Kernel methods for relation extraction . JMLR , 3:1083?1106, 2003.
89
Non-exhaustive reference list from other authors ? Taku Kudo and Yuji Matsumoto . 2003. Fast methods for kernelbased text analysis . In Proceedings of ACL?03.
? Dell Zhang and Wee Sun Lee . 2003. Question classification using support vector machines . In Proceedings of SIGIR?03, pages 26?32.
? Libin Shen , Anoop Sarkar , and Aravind k . Joshi . Using LTAG Based Features in Parse Reranking . In Proceedings of EMNLP?03, 2003 ? C . Cumby and D . Roth . Kernel Methods for Relational Learning . In Proceedings of ICML 2003, pages 107?114, Washington , DC , USA , 2003.
? J . ShaweTaylor and N . Cristianini . Kernel Methods for Pattern Analysis . Cambridge University Press , 2004.
? A . Culotta and J . Sorensen . Dependency tree kernels for relation extraction . In Proceedings of the 42nd Annual Meeting on ACL , Barcelona , Spain , 2004.
Non-exhaustive reference list from other authors ? Kristina Toutanova , Penka Markova , and Christopher Manning . The Leaf Path Projection View of Parse Trees : Exploring String Kernels for HPSG Parse Selection . In Proceedings of EMNLP 2004.
? Jun Suzuki and Hideki Isozaki . 2005. Sequence and Tree Kernels with Statistical Feature Mining . In Proceedings of NIPS?05.
? Taku Kudo , Jun Suzuki , and Hideki Isozaki . 2005. Boosting based parse reranking with subtree features . In Proceedings of ACL?05.
? R . C . Bunescu and R . J . Mooney . Subsequence kernels for relation extraction . In Proceedings of NIPS , 2005.
? R . C . Bunescu and R . J . Mooney . A shortest path dependency kernel for relation extraction . In Proceedings of EMNLP , pages 724?731, 2005.
? S . Zhao and R . Grishman . Extracting relations with integrated information using kernel methods . In Proceedings of the 43rd Meeting of the ACL , pages 419?426, Ann Arbor , Michigan , USA , 2005.
90
Non-exhaustive reference list from other authors ? J . Kazama and K . Torisawa . Speeding up Training with Tree Kernels for Node Relation Labeling . In Proceedings of EMNLP 2005, pages 137?144, Toronto , Canada , 2005.
? M . Zhang , J . Zhang , J . Su , , and G . Zhou . A composite kernel to extract relations between entities with both flat and structured features . In Proceedings of COLINGACL 2006, pages 825?832, 2006.
? M . Zhang , G . Zhou , and A . Aw . Exploring syntactic structured features over parse trees for relation extraction using kernel methods . Information Processing and Management , 44(2):825?832, 2006.
? G . Zhou , M . Zhang , D . Ji , and Q . Zhu . Tree kernelbased relation extraction with context-sensitive structured parse tree information . In Proceedings of EMNLPCoNLL 2007, pages 728?736, 2007.
Non-exhaustive reference list from other authors ? Ivan Titov and James Henderson . Porting statistical parsers with data-defined kernels . In Proceedings of CoNLLX , 2006 ? Min Zhang , Jie Zhang , and Jian Su . 2006. Exploring Syntactic Features for Relation Extraction using a Convolution tree kernel . In Proceedings of NAACL.
? M . Wang . A reexamination of dependency path kernels for relation extraction . In Proceedings of the 3rd International Joint Conference on Natural Language Processing-IJCNLP , 2008.
91
