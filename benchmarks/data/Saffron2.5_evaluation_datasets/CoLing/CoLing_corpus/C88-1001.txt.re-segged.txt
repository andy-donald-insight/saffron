FeaMble Learnability of Formal Grammars and
\[~? he Theory of Natm'al Language Acquisition
Naoki ABE
l ) epartment of Computer and Information Science
University of Pem~sylvania
Philadelphia , PA 19104-6389
Abst ; ract
We propo ; : e to apply a . complexity theoretic notion offeasible learnability called " polynomia learnability " to the evaluation of grammatical formalisms for linguistic de  . ~; criptiol ) . Polylm- . 
millh ; arnability was originally defined by Valiant in the context of bo  , llean concept t ( !arniiig and sul ) scquetltly generalized hyBlume cel , al . to i ~ ll in it a . cy domains . We give a clear , intuitive exposition el ' this notion ( /l'k ' arnabilityau ( l what characteristics of a collection of hmguages may or many not help feasible learn--ability under this paradigm  . In particular , we preset , t a novel , nontriv Jal : : on straint on the degree of " locality " of grammars which allows ari & class of mildly context sensitive languages to be feasibly learnable  . We discuss pos , ' ; ihle implications of this observati ( m to the theory of n at m ' a language acquisition . 
t . Introduct , ion
A central i ~ sueo\[linguistic theory is the " t ) ~' ojectio ~ lprohhml " , which was origblally prol ) osed by Noam Chomsky\[?\] and subsequ ( mtlyl . ? d to much of the development in modern linguistics  . 
This probh, . mpose ~ the question : " i \ [ ow is it poss lbk ~ for human infants to acquire the i  , ' native language on the basis of casual exposure to limited data in a short amount of t  , ime ? " The proposed solulion is that the human infantine ll\  ; ct " knows " what the natural anguage that it is trying to learn could possibly be  . Another way to look at it is that there is are . latively small set of possible grammars that it would be able to learn  , and its learmng stratergy , implicitly or explicitly , takes adwm tage of this a priori knowledge . The goal of linguistic theory , then , is to & aractedze this set of possible grammars , by specifiying the constraints , often cM led the " Uniwwsal ( I rammar " . Tile theory of induct iw ~' inference oilers a precise solution to this problem  , by characterizing exactly what collections of ( or its dual " constraints ou " ) language satisfytile requirement for being the set of possible grammars  , i e . are learnable ? A theory of " feasible " inference is particularly interesting because the language acqui-sitkm process of a human infant is feasible  , not to mention its relewmce to the technological counterpart of such ap wbh '  . m . 
In this paper , we investigate the learuability of formal grammars for linguistic description with respect to a complexity theoretic notion offeasible lea  . rnability called ' polynomial learnability ' . Polynomia learnabillty was originally developed by Valiant \[?\]  , \[?\] in the context of learning boolean coitcei ) t from examples , art d subsequently generalized by Illumer et al for arbitrary concepts \[?\]  . We apply this criterion of feasible l carnability to subclasses of formal grammars thai  , are of considerable linguistic interest . Specifically , we present a novel , nontrivial constraint on gramma , : scalled " k . locality " , whichen a \] ) k ~ sariche hlss of mildly context sensitive grammars called lank < ~ dNode Rewriting G ' rammars  ( RNI . (0 to be limsibly lear1 ~ able . \' Vc discuss possible implications of this result to th cL heory of natural In nguag cacqui : ~ition  . 
2 Polynomial Learnabil ity 2ol Formal Modeling of Learning What constitutes a good model of tile learning behavior ? Below we list tlve basic elements that any formal model of learning must con <  ,   . ( c . f .  \[13\]) 1 . Objects to be learned : l , ctuscall them ~ knacks ' for full generality . The question of learnability is asked of a collection of knacks  . 
2 . Environment : The way in whidl ' data ' are available to tile learner  . 
3 . I\[ypotheses : I ) escriptious t )) r'knacks' , usually CXl ) ressed in a certain language . 
4 .  / , earners : Ill general functions from data to hypotheses  . 
5 . Criterion of l , earning : \] ) efines precisely what is meant by the question ; When is a learner said to ' learn ' a giwm collection of ' knacks ' on the basis of data obtained through the enviromnent ? In most cases ' knacks ' can be thought of as subsets of some universe  ( set ) of objects , from which examples are drawn .  1  ( Such a set is often called the ' domain ' of the learning problem  . ) The obvions example is the definition of what a language is in the theory of natural language syntax  . Syntactically , the English language is nothing but the set of all grammatical sentences  , although this is subject to much philosophical controversy  . The corresponding mathematical notion of a formal language is one that is fi'ee of such a controversy  . A formal language is a subset of the set of all strings in  . E * for some alphabet E . Clearly E * is tile dom Mn . The characterization of a kna & as a subset of a universe is in fact a very general one  . For example , a boolean concept of n variables is a subset of the set of all assignments othosen variables  , often written 2' ~ . Positive examples in this case are assignments to then variables which's at is fy'the concept in question  . 
When the ' knacks ' under consideration can in fact be thought of as subsets of some domain  , the overall picture of a learning model looks like the one given in Figure  1  . 
2.2 Polynomial Learnability
Polynomial learnability departs from the classic paradigm of language learning  , ' idenitification in the limit ' , ~ in at least two important aspects , lilt enforces a higher demand oil tile time 1First order structures are an example in which lang tlages arc more than just subsets of some set  \[14\]  . 
2Identification i the limit w?~s originally proposed and studied by Gold  \[8\]  , and has subsequently been generalized in many difl brent ways  . See for example \[13\] for a comprehensive treatment of this and related paradigms  . 
The Knacks
The Domain
The Environmento
The Hypotheses
The Learner
The Crileriony
Figure 1: A Learning Model complexity by requiring that the learner converge in time polynomial  , but on the other hand relaxes the criterion of what constitutes a ' correct ' grammar by employing an approximate  , and probabilistic notion of correctness , or a ecraey to be ' precise . Furthermore , this notion of correctness is intricately tied to both the time complexity requirement and the way in which the environment presents examples to the learner  , Specifically , the environment is assumed to present othe learner examples from the domain with respect to an unknown  ( to the learner ) but fixed probability distribution , and the accuracy of a hypothesis is measured with respect to that same probability distribution  . 
This way , the learner is , so to speak , protected from ' bad'presentations of a knack . We now make these ideas precise by specifying the five essential parameters of this learning paradigm  . 
1 . Objects to be learned are languages or subsets of  ?2" for some fixed alphabet E . Although we do not specify a priori the language in which to express these grammars a  , for each collection of languages Z ; of which we ask the learnability , we fix a class of grammars G ( such that L ( ~ ) = ? where we write L ( ~ ) to mean L ( G ) IGE ~ ) with respect to which we will define the notion of ' complexity ' or ' size ' of a language  . We take the number of bits it takes to write down a grammar under a reasonable  4  , fixed encoding scheme to be the size of the grammar  . The size of a language is then defined as the size of a minimal grammar for it  . ( For a language L , we write size ( L ) for its size . ) 2 . The environment produces a string in E * with a time-invariant probability distribution unknown to the learner and pairs it with either  0 or 1 depending on whether the string is in the language in question or not  , gives it to the learner . It repeats this process indefinitely . 
3 . The hypotheses axe expressed as grammars . The class of grammars allowed as hypotheses , say " H , is not necessarily required to generate x actly the class Z  ; of languages to be learned . In general , when a collection ? can be learned by a learner which only outputs hypotheses from a class  7"/  , we say that ? is learnable by Tl , and in particular , when Z ;= L(~)) is learnable by ~ , the class of representations G is said to be properly learnable  . ( See\[6\] . ) 4 . Learners passively receive an infinite sequence of positive and negative xamples in the manner described above  , and a Potent Aally any ' l?urning program could be a hypothesis ~ By a reason bl cencoding  , we mean one which can represent nditrerent . 
grannnars using O(log * ~) bits.

at each initial ( finite ) segment of such a sequence , output a hypothesis . In other words , they are functions from finite sequences of positive and negative xamples  5 to grammars . 
A learning function is said to polynomially learn a collection of languages just in case it is computable in time polynomial ill the length of the input sample  , and for an arbitrary degrees of accuracy e and confidence  5  , its output on a sample produced by the environment by the manner described above for any language L in that collection  , will be an e-approximation of the unknown language L with confidence probability at least  1 -- a , no matter what the unknown distribution is , as long as the number of strings in the sample exceeds p  ( e - ~ ,  5 -~ , size(L )) for some fixed plynomial p . Here , grammar G is an e-approximation of language L , if the probability distribution over the symmetric difference  6 of L and I , ( G ) is at most e . 
2.3 Occam Algorithm
Blumer et al \[5\] have shown an extremely interesting result revealing a connection between reliable data compression and polynomial learnability  . Occam'sl ~ azor is a principle in the philosophy of science which stipulates that a shorter theory is to be preferred as long as it remains adequate  . B \] umel " el ; al . 
define a precise version of such a notion in the present context of learning which they call Occam Algorithm  , and establishes a relation between the existence of such an algorithm and poly-nomiM learnability : If there exists a polynomial time algorithm which reliably " compresses " any sample of any language in a given collection to a provably small consistent grammar for it  , then such an Mogor ithm polynomially learns that collection in the limit  . We state this theorem in a slightly weaker form . 
Definition 2 . 1 Let ? be a language collection with associated represenation ~ with size function " size "  .   ( Define a sequence of subclasses of ~ by 7~n = G e 7-\[ \] size ( G ) _ < n . ) Then A is an Occar ( ~ algorithm for ? with range size f ( m , ~z ) if and only if !

VSC graph(L ) if size(L ) = n and \] SI = m then
A(S ) is consistent with S and A(S )) e7~I ( , ~ , m ) and . A runs in time polynomial in the length of S . 
Theorem 2 . 1  ( Blumer et al ) If A is an Occam algorithm for f ~ with range size f  ( n , m ) = O(nk ~ ~) for some k >_;0 < c ~ < 1 then . 4 polynomially learns ? in the limit . 
We give below an intuitive explication of why an 0cesta Algo-rithm polynomiMly learns in the limit . Suppose A is an Occam Algorithm for ? , and let L~l : be the language to be learned , and nits size . Then for an arbitrary sample for L of an arbitrary size  , a minimal consistent language for it will never have size larger than size  ( L ) itself . Hence A's output on a sample of size m will always be one of the hypotheses in H\]  ( m , ~) , whose cardinality is at most 2\](~ , n ) . As the sample size m grows , its effect on the probability that any consistent hypothesis in  7~i   (  , ~ , , 0 is accurate will ( polynomially ) so on dominate that of the growth of the eardinality of the hypothesis class  , which is less than linear in the sample size . 
S in the sequel , we shall call them ' labeled samples ' S The symmetric difference between two sets A and B is  ( A-B ) U ( B-A )  . 
rF or any langugage L , ~ jraph(L ) = ( x , OIxC-:LUa : , I )\] a:~L . 
3 Rarjked Node Rewriting Grammars
In this section , we define l , hc class of nrihlly context sensitive grammars under consideration  , or Ranked Node Rewriting ( \] ram . -mars(RNR(~'s ) . \[ NR ( \]' s are based on the underlying ideas of Tree Adjoining Grammars  ( TArt's ) s and are also a specical case of contextfi'ee tree grammars  \[15\] in which unres ~ , ricted use of w ~ rial ) les for moving , copying and deleting , is not permitted , in other words each rewriting in this system replaces a " ranked " no clterminal node of say rankj with an " incomplete " tree containing exactly j edges that have no descendants  . If we define a hierarchy of languages generated by subclasses of RNRG's having nodes and rules with hounded rank j  ( RNRLj )  , then RNRL0 = CFL , and RNRLa::TAL . 9We formally define these grammars below . 
Definition ' LI ( Preliminaries ) 77 zefollowing definitions are necessar ! lJb ' , " the , ~ equel . 
( i ) The set of labeled directed trees over an alphabet E is denoted  7  ; > ( ii)r\['ll . eT a . ' ll . ' . of an " incomplete " tree is the number of outgoing edges with no descendents  . 
( iii ) The rarthoj ' a node is the . number of outgoing edges . 
( iv ) The ~ u & 4 ' a symbol is defined if the rank of any node labeled by it is always the same  , and equal ~ that rank . 
( v ) A ranked alphabet is one in which every symbol has a rank  . 
( vi)I , l ) r writ , ': rank(x ) for the rank of a ~ y thing x , if it is defined . 
Definition 3 . 2 ( Ranked Node Rewriting Grammars ) Aronl ; ednodt ; re'writingrammarCisaq'uinl , ph '>' , , v , E ' e ,  ~ , It ,  . , Re ; ) where : ( i ) EN is a ranked nonterminal alphabet . 
( ii )) ; ' r is a germinal alphabet di4o in tfi'omF~N . We let ~; =-; NU2T . 
( iii ) ~ is a distinguished symbol distinct from any member of E  , indicating " a'a outgoing edge with no descendent  "  , m(iv ) It ; is a finite set of labeled trees over E . We refer ~ oI ( ; as ~ he " initial trees " of the grammar . 
( v ) Ra is a finite set of rewriting rules : R < ~ C ( A , aIAeY , ' N&a CT~u . & rank(A ) = rank(re) . ( In the sequel , we write A-- . o for rewriting rule A , ce ) . )(vO , ' a ,, V(c ) = , ha ,, - ~, 4 . ( A ) IAeEN . 
We emphasize that the nonterm in M vs . terminal distinction above does not coiad de with the internal node vs  . frontier node distinction . ( See examples 2 . 1 - 2 . 3 . ) tiaving defined the notions of ' rewriting ' and ' derivation ' in the obvious manner  , the tree language of a grammar is then defiim das the set of trees over the terminal alphabet  , whid ~ can be derived fi ' om the grammar .   11 This is analogous to the way the string language of a rewriting grammar in the Chomsky hierarchy is defined  . 
Definition 3 . :"1  ( ' I Yee Languages and String Languages ) The tree language and string I ang ~ tag cof a RNRG G  , denoted s'\]?reeadjoit dn grammars were introduced a  . sa formalism for linguistic description by aoshi et al  \[10\]  ,  \[9\] . Various formal and computational properties of TAG's were studied in  \[17\]  . Its linguistic relevance was demon-s~rated in \[12\]  . 
9This hierar , : hy is different fi'om the hierarchy of " meta-TAL ' s " invented and studied exl  . ensively by Weir in \[20\] . 
l ? ln contextfreet . ree grammar siu\[15\] , variables are used in place of ~ J . 
' l ' hese variables can then be used in rewriting rules to move  , copy , orerase subtrees . . \[ t is i ; his restriction of avoiding such use of variables Hint keeps RNR  , G's within the class of etlicient , ly recognizable r writing systems called " Linear contextfi ' eere writing systems "  ( \[18\] )  . 
II ' Phisishowan " obligatory adjunction constraint " in the tree adjoining nunar formalism can be sintulated  . 
aSb9:
SaSd
Ij\[--.
b#c7:

IV ..
a8f
S $ b  #c d  #c derived : s as f as f s s b s c dseb  ) vcd ) ~e Figurc 2: a , fl , 7 and deriving ' a abbccd deeff'byG : ~ T (( ; ) and Leorepectively , are defined as follows ;/~( c')= . , ji ~ ld(~) t~~T ( O) . 
If we now define a hierarchy of languages generated by subclasses of RNRG's with bounded ranks  , contextfi ' ee languages ((' , FL ) and tree adjoining languages ( TAt ) constitute the first two members of the hierarchy . 
Definition 3 . 4 l ; breach j ~ NRNI~Gj = GIGCRNRG&rank(G ) < J . l ; breach j ~ N , INIU , j = L(C)IOe : antiC ;; Theorem 3 . 1 INI~Lo-CFL~tn . dl ~ NI ~\[ . 1:!I'AL . 
We now giw ; some examples of grammars in this laierarchy ,   J2 which also illustrate the way in which the weak generative capacity of different levels of this hierarchy increases progressively  . 

Example 3 . 1 . 1), =3%~\[n . CNCG l ' , is generated by the following l ? ~ N l ~ ( _7 o9 rammar ~ where o ' is shown in Figure 2 . 
6' , = ( s ,   , , a , b , L s ' ,   , 5'--~ ~ , ,~ + s (~)) Example 3 . 2I)2--a'W~c'~d'~\]nGNC-TALisocher , ted by the following \] ~ NI~G1 grammar , where / ~ is shown in Figure 2 . 
C ; ~=( S , s , a , b , e , d , ~ , ( S ( , ~) )  , S'- ,   , ' < S'+s(~)>Example 3 . 3  L3 = a'%'* c'~d'~e'~f'~In CN?TAL is generated by the following RNI ?  , G2 gr nm n ~ ar , where 7 is shown 5* t , ' igure 2 . 
C ;': ~= ( S ' , s , a , b , , . ' , d , c , f ,  ~ ,  ( , 5'( A , A )) ,  5'--  ,  7 ,   , 5'-~  , ~(~ , I 1 ) )  4 K-Local Grammars q'he notion of qocality ' of a grammar we define in this paper is a measure of how much global dependency there is within the grammar  . By global dependency within a gramnlar , we . mean the interactions that exist between different rules and nonterminals in the grammar  . As it is intuitively clear , allowing unbounded a mont of global interaction is a major  , though not only , cause of a combinatorial explosion in a search for a right grammar  . K-locality limits the amount of such interaction , by tSSimpler trees are represented as term struct . ures , whereas lnore involved trees are shown in the figure  . Also note that werise uppercase ltters for nonterminals and lowercase for terminals  . 
I a Some linguistic motiwltions of this extension of ' lDkG's are argagned for by the author in  \[1\]  . 
bounding the number of different rules that can participate in any slngle derivation  . 
Pormally , the notion of " k-locality " of a grammar is defined with respect o a formulation of derivations due originally to Vijay-Shankar  , Weir , and 3o shi(\[\[9\]) , which is a generalization of the notion of parse trees for CFO's  . In their formulation , a derivation is a tree recording the tfistory of rewritings  . The root of a derivation tree is labeled with an initial tree  , and the rest of the nodes with rewriting rules . Each edge corresponds to a rewriting ; the edge from a rule ( host rule ) to auothe rule ( applied rule ) is labeled with the address of the node in the host l  , ree at which the rewriting takes place . 
The degree of locality of a derivation is the number of distinct kinds of rewritings that appear in it  . In terms of a derivation tree , the degree of locality is the number of different kinds of edges in it  , where two edges are equivalent just in ease the two end nodes are labeled by the same rules  , and the edges themselves are labeled by the same node address  . 
Definition 4 . 1 Let 7 ) ( G ) denote the set of all derivation trees of G , and let r6D(G ) . Then , the degree of locality of t , written locality(r ) , is d 4 ned as follows , locality(r ) = card(p , q , , t ) I there is an edge in r from a node labeled with p to another labeled with q  , and is itself labeled with 77 The degree of locality of a gramm , ~ r is the maximum of those of all its derivations . 
Definition 4 . 2 a RNRG G is called k-local if max locality ( r ) \] re ~ ( C ) _ < k . 
We write k-Local-I~NRO- ( 7 IG ( 5RNRG and Gisk-Local and k-Local-t2Nl ~ L=L ( G ) IGC k-Local-i ~ NR ( :  , etc . .
Example 4 . 1L1 = a " bna " b ' ' In , mCN~/t-Local-RNRL osince all the derivations of G  , -( S , s , a , b ,  ~ , s(S , S ) , S-+sea , S , b ) , S--~A ) generating Lt have defl ree of locality at most 4 . l , brexample , the derivation for the string a3b3ab has degree of locality 4 as shown in Figure 8  . 
Because locality of a derivation is the number of distinct kinds of rewritings  , inclusive of the positions at which they takc place  , k-locality also puts a bound on the number of nonterminal occurrences in any rule  . In fact , had we defined the notion of k-locality by the two condit in s :  ( i ) at most k rules take part in any derivation , ( if ) each rule is k-bounded , t4 , the analogous learnability result would follow essentially by the same argument  . So , k-locality in effect forces a grammar to be an unbounded union of boundedly simple grammar  , with bounded number of rules each boundedly small  , with a bounded number of nonterminals . 
This fact is captured formally by the existence of the following normal form with only a polynomial expansion factor  . 
Lelnma 4 . 1  ( K-Local Normal Form ) For every k-Local-RN RGj G , if we let n = size(G ) , then there is a RNRGjG ' such that ~ . L(C ') = r , , ( a) . 
2 . c ' is in k-local normal form , i . c . O'=U1\]~IiC-rG , such that : ( a ) each lIi has a nonterminal set that is : disjoint from any other IIj  . 
( b ) each tI~isk-sire , pie , that is i . each Ili contains exactly i initial tree . 
1 4'K-bounded ' here means knontermine J occurrences in each rule  ,  \[4\] . 
For instance , a contextfree grammar in Chomsky Normall % rm has only  2-bounded rules . 
, /-: ss2
A2..?-s-*A.---s
SS a Sb
Iss2s2s- . /l ', , s - - - . ./1Xm aS b a S b a Sb locality (~-) = 4 s 2 s s s
As--*A Ams .. A
SSaSbSS a Sbssss-'/1",,s . oa SbaSba Sb Figure 3: Degree of locality of a derivation of a3b3ab by G1 if . each Hi contains at most k rules . 
iii . each II i contains at most k nonterminal occurrences  . 
s . ~ i ~ e(c ~") = o(~+').
Crucially , the constraint of k-locality on RNRG's is an interesting one because not only each k-local subclass is an exponential class containing infinitely many infinite languages  , but also k-local subclasses of the RNRG hierarchy become progressively more complex as we go higher in the hierarchy  . In particular , for each j , Il NP ~ Gj can " count up to "2 ( j + 1 ) and for each k > 2 , k-local-RN\[4Gj can also count up to 2 ( j + 1 ) )  5 We summarize these properties of k-loeal-RNRL's below  . 
Theorem 4 . 1 PbreverykEN , 1 . VjENU keNk-local-RNRLj = RNRLj . 
~ . Vj CNV k > 3 k-local-RNRLj+lis in comparable with
RNRL p3 . Vj , k ~ Nk-local : RNRL j is a p~o per subset of ( k + I ) -loeal-t~NRLj . 
4 . Vj Vk > 2 ENk-local-RNRLj contains in finitely many infinite languages  . 
hf formalt ' roof: 1 is obvious because for each grammar in RNRLj , the degree of locality o ~" the grannnar is finite  . 
As for 2 , we note that the sequence of the languages ( for the first three of which we gave example grammars  ) L ~= a ~* a ~ . . . a ~ Iu ~ N are each in 3-1ocal-RNRLI_I but not in RNRLi_2  . 
Toverii3 , we give the following sequence of languages Lj , k such that for each j and k , Lj , k is in k-local-RNRLj but not in ( k-1 ) -local-RNRL / . Intuitively this is because k-local-languages can have at most O  ( k ) mutually independent dependencies in a single sentence  . 
Example 4 . 2 For each j , k ~ N , let Lj , k = ~' ~2 , ~2 2~ , al . . . a20+1) al . . . a2(j + l)knkkn ~ . . . a1 . . . a2(j~t)\]nl,n2, . . . , nkeN . 
is obvious because Zoo=Uwe ~ . Lw where Lt ~= w "\] ne N are a subset of 2-1ocal-I~NRL0  , and hence is a subset of k : local-RNl ~ Lj for every j and k >_  2  . ??? clearly contains in if initely many infinite languages  .  \[\]  5 K-Local Languages Are Learnable It turns out that each k-loeal subclass of each RNRL jispoly -nomially lear ~ lable  . 
Theorem 5 . t For each j and k , k-local-RNRL j is polynomially

This theorem can be proved by exhibiting an Occam Algorithmi  ( c . f , for this class with size which is Subsection 2 . 3) , a rangel logarithmic in the sample size , and polynomial in the size of a minimal consistent grammar  . We ommit a detailed proof andigiw ~ an informal outline of the proof  .  : 1 . By the Normal Form Lemma , for any k-local-RNRGG , there is a language quivalent k-local-RNR . G Hink-local normal form whose size is only polynomially larger than the size of G  . 
t ~ A class of grammars G is said to be able to " count up to " j  , just in case a ? a ' ~ . . . a\]\]neNeL(G)\[G ( ~ Gbutai'a ' ~ . . . a ~+ 1\[neN?c(G ) Iae 6 . 
2 . The number of k-simple grammars with is a priori infinite  , but for a given positive sample , the number of such grammars that are ' relevant ' o that sample  ( i . e . which could have been used to derive any of the examples  ) is polynomially bounded in the length of the sample  . This follows essentially by the non-erasure and noncopying properties of RNRG's  . ( See\[3\] for detail . ) 3 . Out of the set of k-simple grammars in the normal form thus obtained  , the ones that are inconsistent with the negative sample are eliminated  . Such a filtering can be seen to be performable in polynomial time  , appealing to the result of Vijay-Shankar , Weir and Joshi \[18\] that Linear ContextFree Rewriting Systems ( LCFRS's ) are polynomial time recognizable . That R . NRG's are indeed LCFRS's follow also from the non -erasure and noncopying properties  . 
4 . What we have at this stage is a polynomially bounded set of k-simple grammars of varying sizes which are all consistent with the input sample  . The ' relevant ' part 10 of a minimal consistent grammar in k-local normal form is guaranteed to be a subset of this set of grammars  . What an Oceam algorithm needs to do , then , is to find some subset of this set of k-simple grammars that " covers " all the points in the positive sample  , and has a total size that is provably only polynomially larger than the minimal total size of a subse that covers the positive sample and is less than linear in the sample size  . 
5 . We formalize this as a variant of " Set Cover " problem which we call " Weighted Set Cover "  ( WSC )  , and prove ( in \[2 D the existence of an approximation algorithm with a performance guarantee which suffices to ensure that the output of  , 4 will be a basis set consistent with the sample which is provably only polynomially larger than a minimal one  , and is less than linear in the sample size . The algorithm runs in time polynomial in the size of a minimal consistent grammar and the sample length  . 
6 Discussion : Possible Implications to the Theory of Natural Language 
Acquisition
We have shown that a single , nontrivial constraint of ' k-locality ' allows a rich class of mildly context sensitive languages  , which are argued by some \[9\] to be an upper bound of weak generative capacity that may be needed by a hnguistic formalism  , to be learnable . Let us recall that k-locality puts a bound on the amount of global interactions between different parts  ( rules ) of a grammar . Although the most concise discription of natrual an-guage might require almost unbounded amount of such interactions  , it is conceivable that the actual grammar that is acquired by humans have a bounded degree of interactions  , and thus in some cases may involve some in efficiency and redundancy  . To illustrate the nature of inefficiecy introduced by ' forcing ' a gram-mar to be k -loeal  , consider the following . The syntactic ategory of a noun phrase seems to be essentially context independent in the sense that a noun phrase in a subject position and a noun phrase in an object position are more or less syntactically equivalent  . Such a ' generalization ' contributes to the ' global ' interaction in a grammar  . Thus , for a k-local grammar ( for some relatively small k ) to account for it , it may have to repeathe same set of noun phrase rules for different constructions  . 
t ? This , lotion is to be made precise.
As is stated in Section 4 , for each fixed k , there are clearly a lot of languages ( in a given class ) which could not be generated by a k-local grammar  . However , it is also the case that many languages , for which the most concise grammar is not a k -local grammar  , can be generated by a less concise ( and thus perhaps less explanatory ) grammar , which is k-locah In some sense , this is similar to the wellknown distinction of ' competence ' and ' performance '  . It is conceivable that performance grammars which are actually acquired by humans are in some sense much less efficient and less explanatory than a competence grammar for the same language  . After all when the ' projection problem ' asks : ' How is it possible for human infants to acquire their native languages  . . . ' , it does not seem necessary that it be asking the question with respect o'competence grammars '  , for what we know is that the set of ' performance grammars ' is feasibly learnable  . The possibility that we are suggesting here is that'k-locality ~ is not visible incompetence grammars  , however , it is implicitly the reso that the languages generated by the class of competence grammars  , which are not necessarily k-local , are indeed all k-local languages for some fixed ' k '  . 
7 Conclusions
We have investigated the use of complexity theory to the evaluation of grammatical systems as linguistic formalisms from the point of view of feasible learnability  . In particular , we have demonstrated that a single , natural and nontrivial constraint of " locality " on the grammars allows a rich class of mildly context sensitive languages to be feasibly learnable  , in a well-defined complexity theoretic sense . Our work differs from recent works on efficient learning of formal languages  , for example by Anglu in (\[4\]) , in that it uses only examples and no other powerful oracles  . We hope to have demonstrated that learning formal -- grammars need not be doomed to be necessarily computationally intractable  , and the investigation of alternative formulations of this problem is a worthwhile ndeav on r  . 
8 Acknowledgment
The research reported here in was in part supported by an IBM graduate fellowship awarded to the author  . The author gratefully acknowledges his advisor , Scott Weinstein , for his guidance and encouragement throughout this research  . He has also benefitted from valuable discussions with Aravind Joshi and David Weir  . Finally he wishes to thank Haim Levkowitz and Ethel Schuster for their kind help informatting this paper  . 
References\[1\]NaokiAbe . Generalization of tree adjunction as ranked node rewriting  .  1987 . Unpublished manuscript . 
\[2\] Naoki Abe . . Polynomial learnability and locality of formal grammars  . In 26th Meeting of A . C . L . , June 1988 . 
\[3\] Naoki Abe . Polynomially learnable subclasses of mildy context sensitive languages  .  1987 . Unpublished manuscript . 
\[4\] Dana Angluin . Leafing k-bounded contextfree grammars . 
Technical Report YALEU/DCS/TR-557 , Yale University , 
August 1987.
\[5\]A . Blumer , A . Ehrenfeucht , D . Haussler , and M . Warmuth . 
Classifying learnable geometric on cepts with the vapnik-cher vonen k is dimension  . In Proc . 18th ACM Syrup . on Theory of Computation , pages 243-282, 1986 . 
\[6\]A . Blumer , A . Ehrenfeueht , D . Hausslor , and M . Warmuth . Learnability and the Vapnik-Chervonenkis Dmen -sion  . Technical Report UCSCCI~L-87-20 , University of California at Santa Cruz , No vermber 1987 . 
\[7\] Noam Chomsky . Aspects of the Theory of Syntax . The MIT
Press , 1965.
\[8\]E . Mark Gold . Language identification i the limit . Information and Control , 10:447-474, 1967 . 
\[9\]A . K . Joshi . How much context sensitivity is necessary for characterizing structural description-tree adjoining ram-mars  . In D . Dowty , L . Karttunen , and A . Zwicky , editors , Natural Language Processing - Theoretical , Computational ~ and Psychological Perspectives , Cambridege University Press ,  1983 . 
\[10\]A ravin dK . Joshi , Leon Levy , and Masako Takahashi . Tree adjunct grammars . Journal of Computer and System Sciences ,  10:136-163 ,  1975 . 
\[11\]M . Kearns , M . Li , L . Pitt , and L . Valiant . On the learnability of boolean formulae . In Proc . 19th ACM Syrup . on Theory of Comoputation , pages 285-295, 1987 . 
\[12\]A . Kroch and A . K . Joshi . Linguistic relevance of tree adjoining grammars .  1989 . To appear in Linguistics and Philosophy . 
\[13\]Daniel N . Osherson , Michael Stob , and Scott Weinstein . 
Systems That Learn . The MIT Press , 1986.
\[14\]Daniel N . Osherson and Scott Weinstein . Identification i the limit of first order structures  . JouT " aal of Philosophical
Logic , 15:55-81, 1986.
\[15\]William C . Rounds . Context-free grammars on trees . In ACM Symposium on Theory of Computing , pages 143-148 ,  1969 . 
\[16\] Leslie G . Valiant . A theory of the learnable . Communications of A . C . M . , 27:1134-1142, 1984 . 
\[17\]K . Vijay-Shanker and A . K . Joshi . Some computational properties of tree adjoining grammars  . In 23rd Meeting of
A.C . L ., 1985.
\[18\]K . Vijay-Shanker , D . J . Weir , and A . K . Joshi . Characterizing structural descriptions produced by various grarmnat-ieal formalisms  . In 25th Meeting of A . C . L . , 1987 . 
\[19\]K . Vijay-Shanker , D . J . Weir , and A . K . Joshi . On the progression from context-freoto tree adjoining languages  . 
In A . Manaster-Ramer , editor , Mathematics of Language , 
John Benjamins , 1986.
\[20\] David J . Weir . From ContextFree Grammars to Tree Adjoining Grammars and Beyond-Adissertation proposal  . 
Technical Report MS-CIS-87-42 , University of Pennsylvania ,  1987 . 
