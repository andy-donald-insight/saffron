Learning Dependencies between Case Frame Slots
Hang Li and Naoki Abe
Theory NEC Laboratory , RWCP *
c/o ('. & C . Research Labora . tories , NEC.
4-1-1 Miyazaki Miyama . e-l~u , Kawasaki , 2116 Japan
lihang , abe(~.s bl.cl.nec.co.jp
Abstract
We address the problem of automati-
cally acquiring case frame patterns ( se-
lectional patterns ) from large corpus
data . In particular , wel ) ropose a method
of learning dependencies between case
frame slots . We view the problem of
learning case frame patterns as that
of learning a multidimensional discrete
joint distribution , where random vari-
ables represent case slots . We then for-
m Mize the dependencies between case
slots as the probabilislic dependencies
between these random variables . Since
the number of parameters in a multi-
dimensional joint distribution is exponential in general  , it is infeasible to accurately estimate them in practice  . To overcome this difficulty , we settle with approximating the target joint distribution by the product of low order component distributions  , based on corpus data . In particular we propose to employ an efficient learning algorithm based on the MDL principle to realize this task  . 
Our experimental results indicate that for certain classes of verbs  , the accuracy achieved in a disambiguation experiment is improved by using the acquired knowledge of dependencies  . 
1 Introduction
We address the problem of automatically acquiring case frame patterns  ( selectional patterns ) from large corpus data . The acquisition of case frame patterns normally involves the following three subproblems :  1  ) Extracting case fl ' ames from corpus data ,  2 ) Generalizing case frame slots wMf in these case frames  ,  3 ) Learning dependencies that exist between these generalized case frame slots  . 
In this paper , we propose a method of learning dependencies between case frame slots  . By * Real World Computing Partnership ' dependency ' is meant the relation that exists between case frame slots which constrains the possible values assumed by each of those slots  . As illustrative examples , consider tile following sentences . 
The girl will flyajet.
This a Mine company flies many jets.
The gM will fly Japan A Mines.
* The airline conlpany willfly , Japan Airlines.

We see that an ' airline company ' can be the subject of verb'fly '  ( the value of case slot ' argl ' )   , when the direct object ( the value of ease slot ' arg2' ) is an ' air plane ' but not when it is an ' airline company  '1  . These , examples indicate that the possible values of case slots depend in general on those of the other case slots : that is  , there exist ' dependencies ' between different case slots  . The knowledge of such dependencies i useflfl in various tasks in natural language processing  , especially in analysis of sentences involving multiple prepositional phrases  , such as The girl will flya jetfl'om Tokyoto Beijing  . 

Note in the above example that the case slot of ' from ' and that of ' to ' should be considered epen-dent and the attachments it  (  . " of one of the prepositional phrases ( case slots ) can be determined by that of the other with high accuracy and confidence  . 
There has been no method proposed to date , however , that learns dependencies between case frame slots in the natural anguage processing literature  . In the past research , the distributional pattern of each case slot is learned independently  ,   1 One may argue that'fly'has different word senses in these sentences and for each of these word senses there is no dependency between the case frames  . Word senses are in general difficult to define precisely  , however , and in language processing , they would have to be disambiguated Dora the context ~ nyway  , which is essentially equivalent to assuming that the dependencies between case slots exist  . Thus , our proposed method can in effect ' discover ' implicit word senses fi'om corpus data  . 
2 0 and methods of resolving ambiguity are also based on the assuml : ition th  . at case slots are independent ( llindle and Rooth ,  1991) , or dependencies lm-tween at most two case slots are considered  ( Brill and Resnik ,  1994) . Thus , provision of an efl'ec-tive method of learning de  , pendencies between (; as (; slots , as well as investigation of the usefulness of the acquired dependencies in disambiguation and other natural language processing tasks would be an in ll  ) ortant contributiot a to the fie . ld . 
In this paper , wc view the problem of learning ( ; as ( ? frame patterns as that of learning alnulti -dimensional discrete joint distribution  , where raw donivariables represent case slots . We then formalize the dependencies between case slots as the probabilistic dependencies betwee it theser all do iil variables  . Since the illl lllberOf dependencies that exist , in an mlti-dimension a . l joint disl . ribution is exponential if we allow nary dependencies in general  , it is int > asible to accurately esi . it llate them with high accuracy with a data size available in practice  . It is also clear that relatiw ; ly few of these ra . n dom variahles ( case slots ) are actually depeit-dent on each other with any signiticance  . Thus it is likely that the target joint distribution can be approximated reasonably well by the product of component distributions of low order  , drastically reducing then uniber ( : if paralneters / . hat need to be considered . ' Fhis is indeed the apl > roach we take in this lmper  . 
Now the probleniis how to approxilnal , e a , joint distribution by the product of lower or < ler com-pOllet it distributions  , llecently , ( Suzuki , 1993) l ) roposed a . ii algorithn l to approxhnal . clyleariial nulti-dimensional joint distribution exlwessible as a ' dendroid distribution '  , which is both efticient and tlworet , ica . il yso/lnd .  ~, . V employ Suzuki's algorithm 1 , olearn case fralim patterns ; is dendroid distributions . We conducted soll e experinlel its to automatically acquire case fi'alne patterns from the Penn'FreeBank bra  . cketed corpus . Our experimental results indicate that for seine class of verbs the accuracy achiew ? dillad is a  . nlbiguni . i on experinlent can be inl proved by using the acquired knowledge of dependencies between case slots  . 
2 Probability Models for Case
Frame Patterns
Suppose that we haw ? data given by ills ( antes of the case frame of a verb automatically extracted from a corpus  , using conventional techniques . As explained in Introduction , the l : i rol ~ lelu of learning case fraillel ) atteriisca . it be viewed as that of es-tilnating the unde ~rlying mulli-dimemsioltal joilll distribulioT ~ which giw~s rise to such data  . 111 this research , we assume that <' as ( . ' tame instances with the same head are generated by a joint distribution of type  , 
I ' ~ , (& , X ~ , .   .   .   , X , ,)  , (:3) where index Y stands for the head , and each of the rand onl variables Xi ,  / = 1  , 2  ,   .   .   . , n , represents a case slot . In this paper , we use ' case slots ' to mean re , face case slots , and we uniformly treat obligatory cases and optional cases  . ' rhusthemu N ) ern of the random variables is roughly equal to then unfl  ) er of prepositions in English ( and less than 100 )  . These models can be further classified into three types of probability models according to the type of values each random variable  . Xi assumes 2 . 
When Xi assumes a word or a special symbol '0' as its value , we refl : r to the corresponding model Pv(Xi ,   .  ? . , X , ) as a ' word-based model . ' Here '0' indicates the absence of the case slot in question  . 
When Xi assumes a . word class or '0' as its value , the corresponding model is called a ' classbased model  . ' When Xi takes on 1 or 0 as its value , we call the model a ' slot-based model . ' Here the value of ' l ' indicates the presence of the case slot in question  , and '0' al>sence . Suppose for simplicity that there are only 4 possible case slots ( random variables ) corresponding respectively to the subject , direct object , ' front'phrase , and ' to ' phrase . Then , l'flv(X . , . , at = girl , X . , . g2=jet,Xf , .   . . . . = 0 , X ~ o = O )   ( 4 ) is given a specific l ) robability value by a word-based model . In contrast , Ig , u(X < , , . ai = < person ), S . , . :, ~= ( airplane),
Xf, . o , , = O , X to = O )   ( a ) is given a specilic l ) robability by a classbased , node l , where ( l , e , ' son)alid ( airplane ) denote ~ word classes . Finally , l)tzy(X , , . , a ~= 1, X ~, . au = 1, X . r, . o , , , = O , X to = O )   ( o ) is assigned a specific probability by a slot -based model  . 
We then for lmllale the dependencies between case slots as the probabilislic dependencies between the rand onl variabh~s in each of these three tr to dc ls  . In the absence of any constraints , however , the number of parameters in each of the above three l nodels is exponential  ( even the slot-based model has 0 ( 2" ) parameters )  , and thus it is infeasible to accurately estimate them in practice  . A simplifying assumption that is often made to deal with this difficulty is that random variables  ( case slots ) are mutually independent . 
Sul ) pose for examl : ile that in the analysis of the set ltell Celsawagirl with at  . elescope , (7) two interpretatiolls are obtained . We wish to select . then lore appropriate of the two in ( eft : itera-tions . A heuristic word-based method for disambiguation , in which the slots arc assumed to be 2A representation of a probability distribution is usually called a probability model  , or simply a model . 
22 dependent , is to calculate tile following values of word -based likelihood and to select tile interpretation corresponding to the higher likelihood value  . 
Psee(Xa,', . 1 t="\[ , Xar 92 = girl , ) l ' ~ uith ~- telescope ) ( s)
P . ~ . ~(Xa, . al=I , Xa, . oe = girl ) (9) xl~li , . l(X ~ , , io , . = telescope ) If on the other hand we a . ssume that the random variables are independe ' ~ l , we only need to calculate and compare t ~ , :(X~ , iH , = telescope ) and Pgi , 't( . \' with = telescope ) ( c . f . ( Li and Abe . , 1995)) . The independence assumption can also be made in the case of a classbased model or a slot-based model  . For slot-based models , with tile independence assumption , P . ~( X , ~, ith = 1) and Ps, . l(X w itf l = 1) are to be compared ( c . f . ( Hindle and Rool: . h , 1991)) . 
Assuming that random variables ( case slots ) are mutually independent would drastically reduce tile number of parameters  . ( Note that . under the independence as suml ) tion tilen mn be r of parameters in a slot-based model becomes  0  ( ~ )  . ) As illustrated in Section 1, t . his assumption is not necessarily valid in practice  . What seems to be true in practice is that some case slots are ill fact dependent but overwhelming majority of t  . hema . reindependent , due partly to the fa . cl that usually only a few slots are obligatory and most others are optional  . : ~ Thus the target , joint distribution is likely to be a . pproximabie by the product of several component distributions of low order  , and thus have in fact a reasonably small number of parameters  . We are thus lead to the approach of approximating tiletal : getjoint distribution by such a simplified model  , based on corpus data . 
3 Approximation by Dendroid
Distribution
Without loss of generality , any n-din lensior laljoint distribution can be writl  . en asP ( xi,x . _,  .   .   .   .   . x , , ) = HP(x , , , IX . . . . .   .   .   .   . x %, _,) i=1 (1o ) for some pernnttation ( mq,m . _,  . . . . nb ~ ) of 1, 2 . . . . n , here we let P(X,~,I x . . . . . ) denote FIX , , , , ) . 
Apta . usib\[e assumption on I . he dependencies between random variables is intuitively that each variable dire etbj depends oil at most one other variable  .   ( Note that this assumptionistile simplest among those that relax the independence a  . s-sumption . ) For example , if a joint distribution P ( X1 , X , , , X : 3) over 3 random variables X1 , X2 , Xaa Optiona . 1 slots ~ trenotnecessarily independent , but if two optional slots are randomly selected , it is likely that they are indet ) endent of one a . nother . 
can be written ( approximated ) as follows , it ( al > proximately ) satisfies such an assumption . 
P( . z?1,-"k2,X3):(~,"~)P(-\'1)'/)(X2IX1) . P ( X: , IX\[ )   ( 11 ) Such distributions are referred to as ' dendroid distributions ' in tile literature  . A dendroid distribution can be represenled by a dependency forest  ( i . e . a set of dependency trees ) , whose nodes represent the random varia Mes , and whose directed arcs represent the dependencies that exist between these random w /riahles  , each labeled with a number of parameters specil ' ing the probabilistic dependency  .   ( A dendroid distribution can also be considered as are  . stricted form of the Bayesian Network ( Pearl ,  1988) . ) It is not difficult t . o see tha . t there are 7 and only 7 such representations for the joint distribution P ( X1 , X , 2 , X3) disregarding the actual nmnerical values of t . he probability parameters . 
Now we turn to the problem of how to select the best dendroid distribution fi:om among all possible ones to approximate a target joint distribution based on input data generated by it  . This problem has been in w?stiga . ted in the area of machine learning and related fields  . A classical method is Chow&Liu's algor Mnn for estimating an mlti-dimensional  . joint distribution as a dependency tree , ill away which is both el-~cient and theoretically sound  ( C . how and I , iu , 1968) . More re-cent . ly ( Suzuki ,  1993 ) extended their algorithm so that it estimates the target  , joint . distribution as a dependency Forest . or ' dendroid distrihution ' , allowing for the possibility of learning one group of random variables to be completely independent of another  . Since nlany of the random variables ( case slots ) in case flame patterns are esseutially independent  , his feature is crucial in our context , and we thus employ Suzuki's algorithm for learning our case frame patterns  . Figure 1 shows the detail of this Mgorithm , where ki denotes then un > her of possible values assumed by node  ( random variable ) Xi , N the input data size , and qog ' denotes the logarithm to the base 2 . It is easy to see that the nulnber of parameters in a dendroid distribution is of the order O  ( k2ne )  , where k is the maxin mni of all ki , and n is the . number of random variables , and the time complexity of the algorithm is of the same order  , as it is linear in the number of parameters . 
Suzuki's algorithm is derived from the Minimum Description Length  ( MDL ) principle ( liis-sanen ,  1989 ) which is a principle for statistical estimation in information theory  . It is known that as a . method of estimat . ion , MI ) L is guaranteed to be near opt in m . l4 . \[ napplying MDL , we usually assume that the given data are generated by a probability model t  . hat belongs to a certain class of models and selects a model within tile class which  4We reDr the interested reorder 1o   ( Li and Abe , 1995) for an introduction to MDL . 

I , et 7':=(/);(' . alculat . e1, hemuttmin\[ol:~nat . ionI(Xi , X5) for all uo(:t(~pairs( , Y / , Xj ); Sort . 1\]w node pairs ind(~scen(liugo\]'(h+r of l , and stor(~l . hent in t . oqm'ueQ;l,(;tV1)c/ . he set of X i , i = : 1, 2, . . . , ~\]: whih '+ The llla . xittltlltl vahw of linQsaris\[its\](& . , : v ~) > o(x ~ &) = (<: - t)(a , ~1) > ~"''2N dot )( ; ginI ~ muov ( " tlwnod(>l ) air ( , \7 i . ,\+ j)h;/vil~gth(,ni;/xi\]~mil+v ; t . ltw <) I'/\[' ro~tQ ; If " , \7 + amlAjI > ( , lot ~ g to di Il '(' r(mt , sclsIt + , ll ': + , in 1; Them Iel ) lac(>IVI a . n(lII + . , inl wilh H ' IUI1":,, and add edge (,\ i . A'j ) 10"\[': end Output . 7' as 1 . hose t . of ( xlg  cso1 ' the (' stitnal('(t model . 
l"ig . trcl : The hm . rtfing algoril . hulbest (' xplaii > l . hedal . a . I 1 . i . ( m(lsIoI~('llw('ascusua . Hyt ; hal , a simllh'r model has al ) oor (, rIllt . o1, hedal . a , a . H (/ anlore complexmo ( hqh+tsal + ( , i , l , ( q : filIlIOI ' ll ( )( la . t . a . Thust , h('l:eisnt . rad('-of I'I > ctw(>cnt , t > simplicity of amod ( qguml . h('go(>dn('ss of lit . to data . M 1 ) I , resolves I . his I . ra(h~-<)\[l'ina(lis('il>ti\[>dway:11 . s(eh , cl . saIllod (' l which isi'(msonably silu-I/l(>a . nd fits l . he data sal . isl"acl . orily as w ('\[ l . In our (' lil'I ; (' l/lprol ) l(ml , a:- ; ilt lI)\](?IHod('liil(: ; tl/S ; tIIIC ) (\[ (' I wil . hlessd(q ) (' l~(l(mcies , and thus Nil ) l , l ) rovi(l ( ,  . -, a(h(?or(q . ic ; dly sound way 1 . olearn ( ) Ill N I hosc & , pcq\]dcnciesthai , arcsl . al . isticMly signilicant in Ill (: given (\[ al ; a . Airesp ( ~ c\[;dlyiJll('t'( , s(iug\[~alur(~of MIll , isl\]lalit . incorl ) or at . esl:heil\]l ) tll , dala size in it . smodels oh > ct . ioncrit . crion . ' l'h is is rcfl~'('led,inour(u ~, s(>,int . hc < terival . i(>n()l'1, h('thr('sh(,hlO . 
Nol . el , haI , wh(mwc(lonot , \]l;iv(~enough data(i . e . 
\ [ or smfllN ) , the thr ( > shohls will b ( ' large and Ibwnodes Icn ( I 1 . o1) cI in lccd , rcsuliillgillasil\]l-piemod ( ' lill which most . o\[t , l > (' as (> tTr+m > slots arc , jtt(lgc'din(h':l)(m , ,hml . . This is r(uts(>na . lA (, since with a small data . size most cas , , slot > cam\]oi I )( , degermin(xli . oI ) cdep(m(h-\]tt with a . uy significance . 
4 Experimental Result ; s\~"oCOl\](\[/l (% . ' . "( I solt t("l)r ( , lindluu'y (' xp('ritn(qtts to lest . thei ; (, rl ' otulatlc (, o\[t . hcl ; l ', . ) lt()s(+(ltt+('th()(/as;\]m(,I . ho(Io1'+requiring('asol'r+uu('i ); tt ~ cru ~ , liti ); n+-1 . icular , wct . cs(('(lt . o see hox xcl\[(?('tiv(~th(>p ; tlt(q'usa . cquired by our nJ ( ' lhodar <' i \] ~ slructural disam-biguat ion  . \ V (' will dcs (: rib (' the resull . so 17 this ex-porin\]cntation it hissccl ; ion . 
4 . 1 . Experint t ; i ~ t t , 1:Slol;-basc , dMo delluotn ' tirsl , cxp(erim ( , nt , w ( , Iri('dioa('(luir('slot ); ~ s(~(Icasef'ra . tt\]epatt . ( u : us . Fil'sl . , W ('(' xl . r ; t('l . ('(\[18t , 250 case fra . ules from l , hcWallS1r ('(> t . l()urnal(WSJ ) I ) rackcted COl'IreSo/'l , \] tcI'enu'lrveI~ankast ; t:a . iniug data . . Thor ( > w ( ' t ' ( ~357 vcrl ) s\[or which '\[' al ) le1:Verbs and l:hoirl ) erptexity
Verb Indel ) ( mdentl ) ( m droid ndd 5 . g25 . 36 buy 5 . 0,11 4 . 98 find 2 . 07 1 . 92 ol ) ( m2 0 . 5(3 16 . 53 l ) rot . c('t . : L3!) 3 . 13 l/rovid (>, l . 46 4 . 13 r(?t)r ( , s(mt 1 . 2G 1 . 26 s(qld 3 . 20 3 . 29 su(:cc(+d'23) 72 . 57 tell 1 . 3(5 1 . 36 more ( , hmi 50 cas ( ~frame examph~s appeared in liral raining data  . 
lqrsl , wo acquit > dl , hcsloi-bascd case flame pal-iOI'IS for ; Illoft . he 357 verbs .  \ , ' V(~\[lll(~ii(~()t ~ ( l~\[((:to(l~I , cw fohl crossva\]idai , ionto cva\[uai , et lw % esI , data p(u:ph~xii , y ' of t , /w acquired case frame pat , terns , that is , w ? ~ used nine l , ( ml , ho\[the case flames % reach verb as train ing dat  , a ( saving what , rema . in sast , es ( , data ) , t , o acquire case flame pai , l , erns , and then calculal cdp Crl ) lexil . ? using the lesl , data . VV(>rel > C atc'd this procoss t . cn lim(~sa . nd calculated tlm ; tvcragcl ) Crl ) lexity . '\[' able I shows the average per-plex it . y ( ) btmm'd for some randomlys (' h'ctcd verbs . 
\ Veal so calculat . cd timav ( u : age perplexil . y of the q n d c p c n d e t t , slofn lodcls'acquired bas(~don1 . h('assumpt , iont , hal . ( ~ ach slofish Mepcmhml, . Ourexl ) crime nl , alrcsull , s shown in ' l ' able 1 indicate ( hal1 . he use o\[t . he +' ndroid models can achieve upt . o2() ~ . pcrpl ( ~ xil:y reduclion as CO mlmt ' ed ~ otheimb -\[ ) Old ( Hll , slot ll \]( , )(\ [ OIS . Its corns sail " lo sayl here \[' or e thai the dendroid utod clis mores ttitabl cI ' or rcp -rcscnl:ing the Ira+modelo\['caseflames than l  . \[ whMq ) emlcnls\]ol . lttOdO\[ . 
\Vc also used lheacquir (> d depend ( racy knowl-c + , gcillapl > at , lachmenl , disambiguai . ion experi-i \] lol\]l . , kV ( ' used the case h'an\]~s of ' all 357 verbs as o \] trt . raining dat . a . Wcused Chc cttl:irc + brack-et c < l corpus as I il 'a  . illillg dat . ait \] part : because w cwanl . edt . outilize as many t . raining data as possible . Weext . ract . (< l(c ~ rb ,   , ou ? q , prep ,  ? , ) t t ? ~2) or ( v ( , A , , t . ' cpt,~otml , prrp . 2, ~ ou ~\] 2) pat . terns\['rotlt the\VSJ tagged('orplts ; tsi , est . (lata , It Sillgpa . t-tc\]'n matching tccl!t\]iqucs . \ Vct . ook care to ensure ih a lot llyt , hc part . o\['l\[w(agg('d(non-l ) rackt , t . cll ) corlms which do (, snotov('rlap x it . h the I ) rack('l , ('( I corptlSist lSC(Ia , '< test . dai . a . ( The bracl , : ( , ted corlm slots overlap wii . hi)arl,o\[thet , ttgg~x:\[(orpus . )\ Vc acquired (' aso\[ratnepalt . crnsusing t , hc . raining da , ta . \ V ~ foundl : hait here were 266 v < wl>s , whose ' arg2' slot is ( tel ' ~ ( q tdc'lllOl1SOl\]tO . 
of i , hcot , lwr preposition slots . ' l'hm'v were 37 ( Se ~' exmr@es in ' l'al ) lc2 ) verbs whose depen- ( h > l \] cyI ) cl , w(>en;u : g2 and ol , hcr slots is posit Av ( , at l(l(~x (: o , , d . '-; a COl ; t . ailt threshold , i . e . Play92-l,pr+p=J ) 2 > 0 . 25 .  '1'11 ( > depend ( moles\[ound :1_3 by our method seem to agree with human intuition in most cases  . There were 93 examples in Table 2: Verbs and their dependent slots
Verb Dependent slots add blame buy climb compare convert defend explain file focus  arg2 to arg2 for arg2 for arg2 from arg2 with a . rg2 to arg2 against arg2 to arg2 against arg2 on
Table 3: Disambiguation results 1


Accuracy (%) 90/93 (96 . 8) 79/93(84 . 9) the test data (( verb , noun l , prc p , no ' an2) pattern ) in which tile two slots ' a . r g2' and prep of verb are determined to be positively dependent and their dependencies are stronger than tile threshold of  0  . 25 . We forcibly attached prep nou~t2 to verb for these 93 examples . For comparison , we also tested the disambiguation method based on the independence assumption proposed by  ( Li and Abe , 1995) on these examples . Table 3 shows the results of these experiments , where ' Dendroid ' stands for the former method and ' Independent ' the latter  . We see that using tile information on dependency we can significantly improve the disambiguation accuracy on this part of the data Since we can use existing methods to perform disambiguation for the rest of the data  , we can improve the disambiguation accuracy for the entire test data using this knowledge  . Furthermore , we found that there were 140 verbs having interdependent preposition slots . There were 22 ( See examples in Table 4 ) out of these 140 verbs such that their ease slot shawe positive dependency that exceeds a certain threshold  , i . e . 
P ( prepl = 1, prep2 = 1) > 0 . 25 . Again the dependencies found by our method seem to agree with human intuition  . In the test data ( which are of verb , prep:t , nount , prep ~ , nou ~ pattern ) , there were 21 examples that involw ? one of the above 22 verbs whose preposition slots show dependency exceeding  0  . 25 . We forcibly attached bot . hprep , no ' unl and prep2   noun2 to verb on these 21 examples , since the two slots prept and prep ~ are judged to be dependent  . Table 5 shows the results of this experimentation , where ' Den-droid ' and ' Independent ' respectively represent Table  4: Verbs and their dependent slots
Head Dependent slots acquire apply boost climb fall grow improve raises ell think  froII1 for for to from to from to from to fi'om to from to fl'om to to for 
Of as the method of using and not using the knowledge of dependencies  . Again , we found that for the part of the test data in which dependency is present  , the use of the dei ) endency knowledge can be used to improve the accuracy of a disambiguation method  , M though our experimental results are inconclusive at this stage  . 
Table 5: Disambiguation results 2

Dendroid 21./21 (100)
Independent 20/21 (95 . 2) 4 . 2 Experiment 2: Class-based Model We also used the 357 verbs and their case frames used in Experiment 1 to acquire classbase frame patterns using the proposed method  . We randomly selected 100 verbs among these 35r verbs and attempted to acquire their case frame patterns  . We generalized the case slots within each of these case frames using the method proposed by  ( Li and Abe , 1995) to obtain classbase slots , and then replaced the word-based case slots in the data with the obtained classbased case slots  . What resulted are classbased case frame examples . We used these data as input to the learning algorithm and acquired case frame patterns for each of ' the  100 verbs . We found iJmt no two case slots are determined as dependent in any of the case frame patterns  . This is because the number of parameters in a classbased model is very large compared to the size of the data we had available  . 
Our experimental result verifies the validity in practice of the assumption widely made in statistical natural language processing that classbased case slots  ( and also word-based case slots ) are mutually independent , at least when the data size available is that provided by the current version of the Penn Tree Bank  . This is an empirical finding that is worth noting  , since up to now the independence assumption was based so Myonhu -//  . . . ?  . . . . . . . . . . . . . . . 
///~'"" j : /../2.5"
Figure 2: ( a ) Number of dependencies versus data size and ( b ) KL distance versus data size manintuit , ion , to the best of our knowledge . To test how large a data size is required to eslimate a classbased model  , we conducted the following experiment . We defined an art if Malclassbased model and genera  . ted some data . according to its distribution . We then used the data to estimate a classbased model  ( dendroid distribution )  , and evaluated the estimated model by measuring the mlmber of dependencies  ( dependency arcs ) it has and the KL distance between the estimated model and the true model  . We repeatedly generate data and obserwed the learning ' curve '  , nan , ely the relationship between the number of dependencies in the estimated model and the data  . size used in estimation , and the relationship betweet t the KI , distance between the estimated and true modols and the data size  . We defined two other models and conducted the same experiments  . Figure 2 shows the results of these experiments for these three artificial models average dowert Otrials  . ( The number of parameters in Modell , Model 2 , and Model 3 are 18 ,  30 , and 44 respectiv (_' ly , while the number of dependencies are 1 ,  3 , aud 5 respectively . ) We see that to accurately estimate a model the data size required is as large as  100 times then mnber of parameters . Since a classbased mode\[tends to have more than  100 parameters usually , the current data size available in the Penn Tree Bank is not enough for accurate stimation of the dependencies wilh in case fi'antes of most verbs  . 
5 Conclusions
We conclude this paper with the following remarks.
1 . The primary contribution of research reported in this paper is that we ha  . veproposed a method of learning dependencies between case fi'ame slots  , which is theoretically somld and elficient , thus 1) roviding au effective tool for acquiriug ( ; as (' depend ( racy information . 
2 . For the sk)t-based to o(M , sometimes case slots are found to I ) edel ) endent . Experimeu-t . alresults demonstrate that using the dependency information  , when dependency does exist , structural disambignation results can be improved . 
3 . For the word-based or classbased models , case slots are judged independent , with the data size cm'renl , Iy available in the Penn TreeBank . This empirical finding verifies the independence assumption widely made in practice in statistical natural anguage processing  . 
We proposed to use dependency forests to represent case framepa ~ terns  . It is possible that more complicated probabilistic dependency graphs like Bayesian networks would be more appropriate for representing case frame patterns  . This would require even more data and thus the I ) roblenl of how to collect sufficient data would be . a crucial issue , in addition to the methodology ( ff learning case frame patterns as probabilistic dependency graphs  . Finally the problem of how to determine obligatory/optional cases based on dependencies  ( acquired fi'om data . ) should also be addressed . 

Eric Bril \] and Philip Resnik .  1994 . A rule-based approach to prepositional phrase attaclunent disant bignation  , lb ' occediT ~ gs of the 15 lh CO l , -\[ N(; , pages 1198-1204 . 
C, . K . Chow and C ', . N . Liu .  1968 . Approximat-ing discrete probability distributions with dependence trees  . \[NEE Transaclions on \[ n for-marion Theory , t4(3): , t 624 67 . 
Donald Hindle and Mats Rooth .  1991 . Structural ambiguity and lexical relations . Proceedings of the 29th ACL , pages 229-236 . 
Hang Li and Naoki Abe .  1995 . Generalizing case frames using a thesaurus att d the MDL principle  . Proceedings of Recent Advances in Nalural
Language Processing , pages 239--248.
Judea Pearl .  1988 . Probabilistic Reasoning in Intelligent Eyslems : Networks of Plausible Infer-euce  . Morgan Kauflnann Publishers Inc . 
Jorma Rissanen .  1989 . Slochastic Complexily in 5' talistical Inquiry . World Scientific Publisting

Joe Suzuki .  1993 . A construction of bayesian et-works fi'om databases based on an MDL principle  . Proceedings of Uncerlainty in A\['92 . 

