Evahmtion Metrics t'oi-Knowledge-Based Machine Translation 
Eric I 1. Nyberg , 3rd
Teruko Mitamura
Jaimc G . Carbonell
Ccnte , r for Machine Translation
Carnegie Mellon University
Pittsburgh , PA 15213
Topical Paper : machine translation

A methodology is presented for coml ) onent-l ) ase ( l machine translation ( MT ) evaluation through causal error analysis to complement existing lobal evaluation methods  . This methodology is particularly : q ) -propriate for knowledgc-I ) ased machine translation ( KBMT ) systems . After a discussion o\[M'I'evaluation criteria and the particular evahlatiou metrics proposed for KBMT  , we apply this methodology to a large scale application of the KANT  , nach inc translation system , and present some sample results . 
1 Introduction
Machine Translation ( MT ) is considered the paradigm task of Natural Language Processing  ( NLP ) hysome researchers because it combines almost all NLP research : treas : syntactic parsing  , semantic disambigt , ation , knowledgerel ) reseutation , language generation , lexical acquisition , and morphological analysis and synthesis . However , the evaluation methodologies for MT systems have here to forecentered on hlack box approaches  , where global properties of tile system are evaluated  , such as semantic fidelity of the translation or comprehensibility of the target langt  , age output . There is a long tradition of such black box MT evaluations  ( Van Slype , 1979; Nagao , 1985; JEIDA , 1989; Wilks ,  1991) , to the point that Yorick Wilks has stated : " MT Evaluation is better understood than MT "  ( Carbonell & Wilks .  1991 ) . While these valt , ,'ltions are extremely important , hey should be augmented with detailed error analyses and with component cvaluations in ord cr to produce causal analyses l  ) in pointing errors and the refm'e leading to system improvement  . In essence , we advocate both causal component analyses as well as gloi  ) albehavioral analyses , preferably when the latter is consistent with tile I or mervia composition of the component analyses  . 
Timadvent of Knowledge Based Machine Translation ( KBMT ) facilitates component evaluation and error attribution because of its modular nature  , though this ol ) servalion by no means excludes transfer-based systems from similar aualyses  . After reviewing the reasons at t ( I criteria for MT evaluation , this paper describes a specific evaluation methodology and its application to the KANT system  , developed at CMU's Center for Machine Translation  ( Mitamura , et al 1991) . The KANTKBMT architecture is particularly well -suited for detailed evaluation because of its relative simplicity ': ompared to other KBMT systems  , and because it has been scaled up to industrial -sized al  ) plications . 
2 Reasous for Evaluation
Machine Translation is evaluated for a number of different req sons  , and when possihle these should be kept clear and separate  , as diflerentypes of ev , ' duation are best suited to measure different aspects of an MT system  , l . etns review the reasons wily MT systems may be evaluated : ? Com /  ) arison with llumans . It is uselt d to establish a global comparison with hurmm-qu :  . dity ranslation as a function of task . For general-ptnl ) OSe accurate tralls-lation , most MT systelns have a long way to go . A behavioral black boxevahm tion is appropriate here  . 
? Decision to use or buy a particular MTs yMet ~ . t . This evahmliou is task dependent , audnmst take both quality of tralls lation as well as economics in R  ) accf ) nllt ( e . g . 
cost of purchase and of adapting the MT system to the task  , vs . hum : in translator cost ) . Behavioral black box evaluations arc appropriate here too  . 
, , Comparison of multiple MT'systems . The compariso ~ l may be to evahm te research progress  ; is iuthe ARPAMT evahmtions , or to determine which system should be considered for Imr chase and use  . If the system sem-l ) loy radically different MT paradigms , such ; is EBMT and KP , MT , only 1) lack-boxe vahm tions are meaningful , but if they employ similar methods , then I ) oth forms of evaluation tire appropriate . It can hevery informative to determine which system has the better parser  , or which is able to perform certain difficult ( lisaml ) iguatkms helter , at RlSOO 11 , wi1\[1; Illeye towards futt , resynthesis of the best ideasl , on l differeut systems . The Sl~CeCh-recognilion cmn munily has be nelited from such comparisons  . 
? Trackit , gtechnological progress . In order to determine how a system evolves over time it is very useful Ok now which components  , ' ue improving and which are not , as well tls their contribution Io overall MT1 ) erformance . 
Moreover , a phenomena-based evaluation is useful here : Which l  ) reviously problematic linguistic phenomena are being handled better and by having improved which module or knowledge source ? This is exactly the kind of information that other MT researchers would find extremely valu  , : thleto improve their own systems-much more so than a relalively empty glohal statement such as : " KANT is doing  5% better this month . ",, Improvement of a particular system . Ilere is where COlnponentan , ' llysis and error attribution are most vahl-able . Systcul engineers and ! linguistic knowledge source nlain iamers  ( uchtls lexicographers ) perfornihest when by-module performance metrics , are key , as well as an analysis of how each potentially problematic linguistic phenomenon is handled by each module  . 
Different communities will benefit from different evaluations  . For instance , the MT user community ( actual or potential ) will benefit most from global black box evaluations  , as their easons are most clearly aligned with the first three items above  . The funding community ( e . g . , EEC , ARPA , MITI ) , wants to improve the technological infrastructure and determine which approaches work best  . Thus , their interests are most clearly aligned with the third and fourth reasons above  , and consequently with both global and component evaluations  . The system developers and researchers need to know where to focus their efforts in order to improve system performance  , and thus are most interested in the last two items : the causal error analysis and component evaluation both for their own systems and for those of their colleagues  . In the latter case , researchers learn both from blame-assigmnent i error analysis of their own systems  , as well as fiom successes of specific mechanisms tested by their colleagues  , leading to importation and extension of specific ideas and methods that have worked well elsewhere  . 
3 MT Evaluation Criteria
There are three major criteria that we use to evaluate tile performance of a KBMT system : Completeness  , Correctness , and Stylistics . 
3.1 Completeness
A system is complete if it assign some output string to every input string it is given to translate  . There are three types of completeness which must be considered : ? Lexical Completeness  . A system is lexieally complete if it has source and target language lexicon entries for every word or phrase in the translation domain  . 
, , Grammatical Completeness . A system is grammatically complete if it can analyze of the grammatical structures encountered in the source language  , and it can generate all of the grammatical structures necessary in the target language translation  . Note that the notion of " grammatical structure " may be extended to include constructions like SGML tagging conventions  , etc . found in technical documentation . 
? Mapping Rule Completeness . A system is complete with respect omapping rules if it assigns an output structure to every input structure in the translation domain  , regardless of whether this mapping is director via an interlingua  . This implies completeness of either transfer rules in transfer systems or tile semantic inteq  ) retation rules and structure selection rules in interling t ta systems  . 
3.2 Correctness
A system is correct if it assigns a correct output string to every input string it is given to translate  . There are three types of correctness to consider : ? Lexical Correctness  . Each of the words selected in the target sentence is correctly chosen for the concept hat it is intended to realize  . 
? Syntactic Correctness . The grammatical structure of each target sentence should be completely correct  ( no grammatical errors )  ; ? Setnanlic Correctness . Senlanlic correctness presupposes lexical correctness  , but also requires that the corn-positional meaning of each target sentence should be equivalent totile meaning of the source sentence  . 
3.3 Stylistics
A correct OUt pU text must be ineaning invarial l\[ and untler-standable  . System evahmtion may go beyond correctness and test additional  , interrelated stylistic factors : ? Syntactic Style  . An output sentence may contain a grammatical structure which is correct  , but less appropriate for the context han another structure which was not chosen  . 
? Lexical Appropriateness . Each of the words chosen is not only a correct choice buttile most appropriate choice for the context  . 
, , Usage Appropriateness . The most conventional or natural expression should be chosen  , whether technical nomenclature or comlnou figures of speech  . 
? Oilier . l:orm'41ity , level of difficulty of the text , and othe , ' snch parameters shot lJd be preserved in the translation or appropriately selected when absent from the source  . 
4I ( BMTEvahmliou Criieria and Correctness
Met , - ics
In order to evahm tean inlerling nal KBMT system , we define the following KBMTevahmtion criteria , which are based on the general criteria discussed in the previou section : ? Analysis Coverage  ( AC )  . Tile percentage of test sentences for which tile analysis module produces all interlingua expression  . 
? Analysis Correctness ( AA ) . " File percentage of the inter-linguas produced which are complete and correct repre-senlatious of the meaning of tile input sentence  . 
? Generation Coverage ( GC ) . The percentage of coml ) lete and correctiuterling na expressions Rr which the generation module produces a target language sentence  . 
? Generation Correctness ( GA ) . The percentage of target language senlences which are complete and correct realizations of the given complete and correct interlingua expression  . 
More precise deliuitions of these Rnuquantities , as well as weighted ve , sions thereof , are preseuted ill Figure 11 . 
Given these four basic quantities , we can define translation corrccmess as follows : ? Translation Correctness  ( TA )  . This is tile percentage of the input sentences for which the system produces a complete and correct ot  , tput sentence , and call be c , ' ltcu-lated by mt , ltiplying together Analysis Coverage , Analysis Correctness , Generatiou Coverage , and Generation

TA = ACxAAxGCx(,'A(I)
For example , consider a test scenario where 100 sentences are given . 'Is input ; 90 sentences produce interliu-guas ; 85 of tile interlinguas are correct ; for 82 of these IA n additional quantity shown i ! n Figure  1 is the fluency of the target hmguage generation ( leA )  , which will not be discussed further in this paper . 

Criterion Formula
No . Sentences S
No . Sent . w/It , StL
No . Comp ./ Corr . IL 5' tL-CC
Analysis Coverage AC = S's#/'?
Analysis Accuracyil A = :, q'll . - . ('('/, q'#l . 
IL Error 1 Li
Weighted AAI/VAA = I-F ' > V , ( S , t.,)/, b'11,
No . TLl : ' roduced.q " rt.
No . Correct TL , b'T LC
No . Fluent TL , qru , '
Generation Coverage G'C:S~'I . / . S's L-cc ~ Generation Accuracy GA : , 5'7't , c/S's't , 
TL Corr . Error 7' Li
TL Fluency Error TLCI
Weighted GAW ( I/I = 1-EWi ( . b ' ~-, t,i)/,b ', #, t
Generation Flnency , S'<t'sm/,S'Tt , c
Weighted FAI'VI"A=1--)\]!'Vi ( , q'7 , t :<?) l__q"s'( . c Figure 1: l ) et in it lons and Ftlrinnlas for O < ileulating Strid and l grror-Weighted Fxaluation Measures in Analysis and  (  ; eneratinn Components interling nastile system produces French ot ut pt ~ t  ;   , ' lnd 80 of those culprit sentences fire correct . Then 90858280 rA~:l-\])~x , ~ x~?~(2) = . 90x . 94 x . 96 x . 98 =  . 80Of course , we can easily calctlltlte TAov cii . lll if we knowtile number of input sentences arid the numl  ) erel corrk ' ct output sentences for a given test suite  , but often n to d-ules are tested separately and it is us clul to comhine the analysis and generation ligures in this way  . It is also important to note that even if each module in tile system introduces only a small error  , the cuutuhttive ffect can be very substantial . 
All interlingua-based systems contain separate analysis and generation modules  , aud therefore all can be subjected to the style of eval nation preseuted in this paper  . Some systems , however , ft tr th cr modula rize the trausl . ' ttion process . KANT , for example , has two SeXluential analysis modules ( source text to syntactic fstructures ; fstructures to interlingua ) ( Mita-mnra , et al ,  1991) . I lencetile evahtation could be conducted at a finer-grained level  . Of course , for transfer-based systems the modular decomposition is analysis  , transfer and gorier-at ; on moclules , and for example-based MT ( Nagao , 1984) modnles are the tnatcher and the modifier . APl ~ ropriate met-ties for completeness and correctness can be detined for each MT paradigm hated on its modular decomposition  . 
5 P re l iminary Eva luat ion o f KANT In order to test a partict dar application of tile KANT system  , we identify a set of test suites which meet certain criteria : ? Grammar Test Suite  . This test suite contains enteuces which exemplify all of the grammatical constructions allowed in the controlled input text  , anti is int tended to test whether 1he system can trauslate all of them , ? Domain Lexicon Test Suite . This testsuite ctmtai~t sexts which exemplify all the ways in which general domaiutte  , ms ( especially verbs ) are used in different cort texts . It is intended to test whether the systent can translate  ; ill of the usage variants for general domaill ISills  . 
* Preselected hJput Texts . These testsuites cont , ' t in lexts from different parts of the domain ( e . g . , different ypes of nlanmtls for different pmducls  )  , selecled in advance . 
These are intended to demonstrate hat the system can transl  ; tte well in all parts of tilect~s to mer domain . 
, , & mdomlySelet:tcdIlq ) ttl Texts . These testsuites tire comprised of texts that are selected randomly by the evaluator  , and which have not been used to lest the system before  . These ztre inteuded to illustrate how well the system will do on text it has not seeu before  , which gives the l ) eslcnmpleteness-in-context measure . 
The first three types of test suite fire employed for regression testing as the system evolves  , where a stile latter type is ~ generated a new for each major evaluation  , l ) uring development , each successive version of the system is tested on the available test data to prodt ce~ggegate lil?ures for AC  , AA ,  (;(2 , and ( CA . 
5.1 Cnverage " lk'st lng
The coverage rcsults ( A CauclGC ) are ealct , lated at l to mat- ; tally by a program which cotmts output structt , resduring analysis and generation . During evaluatiou , the translation system is split into two halves : SotLrce-to-lnterlingua antiInterliulgua-to- 'lhr get  . l : or , Ij ; ivt ; utext , this allows us to , ' ltllo-matically count how many sell teuces l ) rO ducc dinlerling ttas , thus deriving AC . This also allows t , s to automatically count how ilia . lilyiuterling ti as prodtlce ( Iottt put sentences , tht zstie . -rivitlg (; C . 
5.2 Correctness Test in p,
The correctness results ( AAanti ( ; A ) are calcuhtted l ' of , ' l given text by a process of hunlan evaluation . Tiffs require stile effort of a humau evah ~ at or who is skilled in lhesource language  , target lauguage > , ' ttld translation domain . We have developed a method for calculating the correctness of the OUtl  ) Ut which involve stile following steps : 1 . The text to be evaluated is translated , and the input and out i ) ut Senlences are aligned illasop : i ratelilt for evalu-atiolt  . 
2 . A scoring program presenls each translation to the oval-uator  , lach transl , <ltimt is assigned a score fror fltile following so to f l  ) ossihilities : * C ( Ct/rrt ! cI )  . The OUtllul sentence is COml ) let ely correct ; it preserves the liieailiug of llie iUl ) tltseri-tencoconipletcly , is understandal ) le without difli-eillty , a ~ it l does liot violtlte any rules of gran/m ; ir . 
?1 ( Incorrect ) . The ?/ tllpUtseutencc is inconiple tc ( or einpty )  , or not easily und crsi ; iudable . 
? A ( Accq/table ) . The sentence is complete , ' ut deasily ull clerslalt dablo , I ) tlt is IlOtCOmlile tolygramm , ' lt-ical or violates some ~ q(iMl . lagging convention . 
3 . The score lor the whole text is calculated by tallying the different scores  . TIle overall correctlle SS of the trans-latioli is staled in terms of a range between the strictly correct  ( C ) aud the acceptahle ( C + A )   ( cf . Figure 2) 2 . 
2111 tilegerieral case , one Iyssigll a specific em ) r coeflicient to each citertype , and multiply that coeflicient I ) y lhen unlber of sell tel/ces exhibiting the error . The Stilnlllati Oll of these products across all the erroifulsell Lence si then used to lm~duce a we  ; pilled error rate . Tilts level of detailll as not yet proven lobe necessary in current KAN Tewiluatioi ~  . .qee Figure 1 I~r exain ples of lorlnul as weighted by elror . 
97 5.3 Causal Component Analysis
The scoring program used to present ranslations for evaluation also displays intermediate data structures  ( syntactic parse , interlingua , etc . ) if the evahmtor wishes to perform component analysis in tandem with correctness evaluation  . 
ht this case , the evaluator may assign different machine -readable rror codes to each sentence  , indicating the It ) cation of the error and its type , along with any comments that are appropriate . The machine-readable error codes allow all of the scored output obesorted and forwarded to maintainers of different modules  , while the unrestricted comntents capture more detailed information  . 
For example , in figure 2 , Sentence 2 is marked with the error codes ( : NAP : SEX )  , indicating that tile error is the selection of an incorrect target lexeme  ( ouvrez )  , occurring in the q , uget Language Mapper 3 . It is interesting to note that our evaluation method will assign a correctness score of  0%   ( strictly correct )  25%  ( acceptable ) to this small text , since no sentences are marked with " C" and only one sentence si markexl with " A "  . However , if we use the metric of " counting the percentage of words translated correctly " this text would score much higher  ( 37/44 , or 84%) . A sample set of error codes used for KANT evahmtion is shown in Figure  3  . 
1 . " Donotheat above the following temaperature : " "Nerdchauffez paslate mpdraturest  , ivanteau-dessus : "
Score:I ; Error::GEN:ORD2 . " Cut the bolt to a length of 203 . 2, ' am . "" Ouvrezleboulonfi unelongueur de 203, 2 nam . "
Score : 1; Error :: MAP:LEX3 . " Typicalo cation of the 3F0025 Bolts , which must be used on the 826C Compactors : " " Position typique des boulons 319025 surles compacteurs : "
Score:I ; Error::INT:IR ;: MAP:SNM 4 . "Uses pacers ( 2 ) evenly on both sides to eliminate side movement of the frame assembly  . "" Employezlesent retoises ( 2 ) sur les deux c6tds pour 61iminer jeu lat6ral de I ' ensemble tiebSti uniform6ment  . "
Score : A ; Error :: MAP : ORD
Figure 2: Sample Excerpt from Scoring She et 5 , 4 Current Results The process described above is performed for each of the test suites used to evaluate the system  . Then , an aggregate table is produced which derives AC , AA , GC , and GA for the system over all the test suites . 
At the time of this writing , we arc in the processor completing a largescale English-to-French application of KANT in the domain of heavy equipment documentation  . We have . used the process detailed in this section to evaluate tile system on a bi-wcckly basis during developm cnt  , using a randomly-selected sct of texts each time . An example containing , qg gre-gate results for a set of 17 randomly-selected texts is shown in Figure 4  . 
In the strict case , a correct sentence rcc civcs a vahle of l and a scntence containing any error receives a value of zero  . 
3 For brevity , the samplex cerpt dots not show the intermediate data structures that he evaluator would have exalnir led to make this decision  . 
Modt de Code Colnment : PAR : Lt-X Source lexicon , word missipg/incorrect : GRAUngrammatical sentence accel  ) ted , 
Grammatical sentence not accepted : INT : SNIF -structure slot  , tot interpreted : FNIF-structure feature not interpreted : IR Incorrect intedingua representation--MAP:LEX Target lexicon  , word missing/incorrect : SNM semantic role not , napped : FNM semantic feature not maapped--GEN : GRAUngrann`aatical sentence produced : ORD Incorrect constituent ordering : PARSyntactic Parser : INT Semantic Interpreter : MAP " l  , trget Language Mapper : GEN Target Language Generator Figure  3: Saml ) le Errm " Codes Used in KANT levahtati ( m
INAMES.5",'t./,"r;.c'GATAJ
Result 1608 5,167 -49 186 90% 7% 81%
Result 2608 546 467-519 . 4 6   86-95%   77-85% Figure 4: KANT Ev'4hiation Results , 17 R . 'mdnndy-
Selected Texts , 4/21/94
In tile weighted case , a sentence containing an error receives a partial score which is equal to the percentage of correctly-translated words  . When the weighted method is used , the percentages are considerably higher . For both Result1 and Result2 , then t , maber of correct target language sentences ( given as . 5" vrc ) is shown as ranging between comapletely correct ( C ) and acceptable ( C + A )  . 
We are still working to improve both coverage and accaracy of the heavy-equipment KANT application  . These numbers should , to t be taken as the upper bound for KANT accuracy  , since we are still in tile l ) r occs so f i , n proving the system . 
Nevertheless , our ongoing evahmtion results are useful , both to illustrate the evaluation methodology and also to focus the effort of the systemd cvelol  ) ers in increasing accur : lcy . 
6 Discussion
Our ongoing evalt , at it m of the lirst largescale KANT application Ires benefitted from the detailed error analysis presented here  . Following tile tabulation of error codes l ) rOduced during catlsal comp ( mcnt analysis , we can attril ) ute then tajority of the completeness problems to identiliable gaps in lexieal coverage  , : rod the majority of the accuracy prol ) lefns to areas of the domain n to del which are known Io be in colnplct cor insufiiciently general  . On the other hand , the grammars of both source and target language , as well a stile software modules , are relatively solid , as very few errors can be attributed there to . As lexieal coverage and domain model generalization reach completion  , the component and globale wlh , a tion of the KANT system will t ) ecome a more accurate rellection of the potential of the nnde  , lying technology in largescale apl ) lications . 
As illustr , 'tted in Figm-e 5 , traditional transfer-based MT system start with general coverage  , and gradt , ally seek to improve accuracy and later fluency . In contrast , the KBMT philosophy has been to start with high accuracy and gradually improve coverage and Iluen ~ay  . httile KANT system a , we combine both approaches by starting with coverage of a large specific dontain : rod achieving high accuracy and Iluency \]  0  ( )%
Cv2r , : ~ IO 100%
ACCH t'il C/~/
KB MT Traditional MT , , , . , o . ooo , oo .   .   .   .   .   .   .   .   .   .   .   .   .   . o , H , , Start : High Accuracy Start : lligh Covarago
Xmprove : Coverage , Improvo : Accuracy
Flusncy Fluency
Figure 5: Lnngltudln ' . dlm provemewl in Coverage , Accuracy and lque . cy within that domain . 
The evaluation methodolgy devtloped here is , no : mrtI ) eustd in conjunction with glnbal black box evaluation methods  , indt lendtnt of the course of develolment . The comlo-nt nt evaluations arc mean to provide insight for the sysltm devt lopers  , avid to identify prol ) ltmatic phenomena prior to system comlletion anl dt livefy  . In particular , the method lresented here c'm combine comlonent evalt tation and ! glla \] evaluation to support efficient system testing and nlaintenance beyond development  . 
7 A clmowledgements
We woul I like to thank Radha Rao , Told Kaufnlann , and all of our colleaguts on tile KANT project , includirig Jantts Altncher , Kathy Baktr , Alex Franz , Mildred Gahtrza , Sutllohn , Kathilannamico , Pare Jordan , Kevin Keck , Marion Kee , Sarah Law , John Leavitt , Danielal . on slale , Deryle Lonsdale , Jeanne Mier , Ve . nkatesh Narayan , Amalio Nieto , and Will Walker . We would also like to th:m kour Slms or sat Caterpillar  , Inc . and our colltagues at Carnegie GrOUl , luc . 
References\[11 Carbonell , J . , Mitamura , T . , and E . Nyberl ; (1993) . 
" Evahmting KBMT in thtI , arge , "Japan-US Workshop on Machine-Aided Translaliov , Nov . 2224, Washington , D . C . 
2\] Carbonell , J . and Y . Wilks (1991) . " Machint Transhl-tion : An In-Depth Tntorial , "  29th Annual Meeting of the Association for Compntational Linguistics  , Univerosity of Cali R ) rnia , Be , ' keley , CA , June 1821 , \[371 OooImml and Nirtnburg , eds .  (1991) . A Case Study in Knowledge-Based Machine Translation  , San Mateo , 
CA : Morgan Kaufmann.
\[4\]Isalmra , Sin-nou , Yamabana , Moriguchi and Nonmra ,  (1993) . " JEIDA'sl'roposed Method for l'valuating Machine "\[' ranslalion  ( Translation Quality )  , " l ' , oceed mgso /
SIGNLP 93-NL-96, July.
151 J\[tpll/Dlcctlonich ~ dustry 1 ) evclolmlent Association , A Japrmese View of Machine " l'ran . ' , ' h t t i o n t i l , ight of tile Considerations and Recommendations Reported by AL  . 
PAC , U . S . A . , JEIDA Machine Translation System P , t-search Commitlec , Tokyo . 
\[6\] King , M .  (1993) . " Panelon Evaluation : MT Summit IV . Introduction . " l ' roce edit ~ gs of MT Summit IV , July 2022 , Kobe , Japan . 
17\] Mitamura , " i , . , E . Nyberg " and J . Cmbonell (1991) . 
" An Efficient lnt crlinl . , , ua Translation System for Multi-livlglml Docunlent Production  , " Proceedingso /' Machine 7? anslatim ) Summit III , Washinglon , DC , July 2d . 
\[81 Nagao , M .  (1984)  . "AFtamework of a Mechanical Transl:ltion lct ween Japanese and Enp  , lish I ) y Analogy Princill C , " Artificial and Iluman Intelligence , Elithorn , A . and Bauerii , P, . ( eds . ), Elsevier Science Publishers,
B . V . 1984.
\[91 Nagao , M .  (1985)  . " Evaluation of tilt Quality of Machint -Transhtttd Sentences and the Control of Language  , "  . lournal ofh!formatioaProeessi,\]gSociety of . lapan , 26(1():1197-1202 . 
\[ ll Nakaiwa , Morimoto , Matsndaira , Na , ' itaanti Nomura ,  (1993) . " JF . IDA's Pmlosed Metho I for Evahmtinl ; Machine Traushltion (1) evClOltr's Guideliiles) , " I ' roceed-ings o/SIGNI . I '9 . '-NL-96, Jlily . 
\[111 Nomura , If .  (1093) . " l . ~v . 'lhmtion Method of Ma:hitm'\['r\[ll IsltttiOll:t't ' Ollltile Viewloit  , t of Natural hanguape Processing , " I ' roceedings of MT Summit IV , Jtlly 2022 , 
Kobt , Jalan.
\[12\] Nyberg , 12 . and T . Mitamura (1992) . " TimKANT Sys--tom:I:tlS\[ , Accurtlte , lligh-Quality Tratls Iatioll in Ptac-Ileal Domains  , " Proceedings of COEING 1992 , Nante . ,;,
France , July.
\[131 Rinscht , Adriane (1993) . " Towards a MT Evaluation Methodolop , y , " I ' roceedings of the t , ' , fth International Con/?rem:eo , ,* Theoretical and Methodological lssttes iH Machine  7  ) ' anslatiott , July 1416 , Kyolo , J . 2tltlll . 
( ld\]Rolling , I . . .  (1993) . " P:mel Contribution on MTE vahm-tion , " I ' roceed Mgso/MT Summil IV , July 2022 , Kobt , 

\[15\]Takzly , : una , l to h , Yagisawa , Mogi and Nomura (1993) , "JHDA's Proposed Method for l: . vahmtiug Machine Transhltio , ( End User System Selection ) , " l'r ~) ceedings of SIGNI , I'93-NL-96 , July . 
\[16 J Van Slype , G .  (1979) . " l: . valuation of the 1978 Version of tile SYSTRANF . uglish-French Automatic System or the Coml\ ] tissioll of the I ! tultan COmmuvlitics  , "7' he
Incorporated Linguist 18:86-89.
117\] Vasconcellos , M .  (1993) . " P:mtl Discussion : Evalu , ' ltion Method of Machine Translatim , " Proceedings of MT
Summit IV , July 2-22, Kobe , Japan.
\[18l Wilks , Y .  (1991) . " SYSTP , AN : It Obviously Works , but I low Much Canit behn provtd ? , " Teclnfical Rt to tt MC CS- . 91-215, (2 reputing Research 1, aboratn'y , New
MexicoStatt University , l . as Cruces.

