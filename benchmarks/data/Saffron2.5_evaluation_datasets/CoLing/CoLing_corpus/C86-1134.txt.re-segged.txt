Generating a Coherent Text Describing a
Traffic Scene
Hans-Joachim Noval P
Fachbeich Informatik , Universitgt Hamburg
D-2000 Hamburg 13, West-Germany
Abstract
If a system that embodies a reference semanl ; ic for motion verbs and prepositions is to generate a coherent text describing the recognized motions it needs it decision procedure  ~  , o select I he events . In NAOS event , selection is done by use of a specialization hierarchy of mell on verbs  . The st . rat-egy of anticipated visualization is used t br the selection of optional deep cases  , qJ he system exhibits low-level strategies which are based on verb inherent  , properties that allow the generation of a coherent descriptive 

tI ~ troduct lon
This contribution focuses on the verbalization component of t  ; he NAOS system ( the acronym stands for NAtural language description of Object movements in a traffic Scene  )  . NAOS is designed to explore the border area between computer vision and natural language processing  , especially the realm of recognizing and ver- -balizing motion concepts in image sequences  . 
NAOS goes all the way h'o mare presentation of a real ~ world traffic scene to a natural language text describing the scene  . 
The representation f the scene basically consists of its geometry  ( the retbre called geometric scene description ( GSD ) )+ ~ lk ) giw ~ an impression of the representation a GSD contains for each frame of the image sequence : o instance of timeovisible object so viewpo in to illumination o  31  ) shapeo surface characteristics ( color ) o class o identity 31 ) position and orientation in each flame ( fl ) ra detailed description , , f the GSD see \[ t6\]) . 
For event recognition we use event models (\[18\] ,  \[191 ) which define a reference semantic for motion verbs  . Int , he current implementation of the NAOS system about  35 motion verbs and (  , he prepositions beside , 1) y , in ~ fronl ; o of , near , and on may /> ereco-gnized by matching the event models agains the representation f the scene  . 
In this paper we are neither concerned with the representation ft  . he underlying scene data nor with the question of event recognition as t t  . . . . . . issues haw , bee , , put , list , ed elsewhere ( see\[10\][171\[20\]) . 
Instead , we fi ) cns on the generation of a coherent ; exl , describing their nage sequel ' lee . 
In the nexl , section we brielly describe the represent , a tion of the recognized events which f i ) rm the initial data for the verbalizatioue otll po +lerlt  , tl ) henI , +e overall strategy fnr ( : on+p ( mingacoher ( ! llt description is discussed . The fblk ) wing section i , ltrodnces apart , i alsolution to the selection problem which is based on the strategy of anticipated visualization  . Fourth , we show how some linguistic choiees like pass lve ~ restrictive relative elanses  , and negation I thank B . Neumamt who contributed several ideas to this article  . 
are natural consequences of the task of generating unambiguous referring expressions  . In the last section we relate our research with current work on language generation  . 
\]\[ nitia J ~\] Data
Verbalization starts when even treeognil , ion has been a clfiew ~ d . 
Besides complex event , slike overtalm and turn off , other predi--cares like in-front-of , I ) esi(les , move , etc . are also inst , antiated . 
Pleh ) w is a section of the database after event recognition has taken place  ( the original entries are ill German )  . 
1: ( MOVEPERSONI040) 2: ( WAI , KPERSONI 040 )  3:  ( RECE 1 ) EPERSONIFBI 2040 )  4:  ( OWm TAKEB MW~VWI 00:12 )   ( ~ , ~ 3~ , ) )  5:  ( MOVEB MW1 l04 0 )  6:  ( IN-FRONT-OFV WlTR AFFIC-LKHITI 2732 ) "\]' heabove entries are instantiations of event models containing symbolic identifiers for scene objects  ( e . g . BMWI ) . Tile last two elements of an instantiation denote the start and end time of the event  . 
We use the following notations to denote the event time : ~  .  (  .   .   .   . rbT e ) ~ .  (  .   .   .   . ( r < , , , T b .   .   .   . )(' r ~, , , , , r, . . . . . . )) a .  ( . . . .( rb ,, . , + Tb . . . . . . ) " r . ) 4 .  ( . . . . rb re .   .   .   .   .   . )) Tb , Te denote start and end t , ime of an event . The first notation is used for ( htratlve events ( e . g . move ) . A duratlve event , is also valid for each subinterval of ( TbTe ) . 
These colldt+oi , ation is tlsed for ilO\]l-dllrative evelltl  ; ( e . g . over take ) . Start and end time of such an event are Imth restrk : ted by lower and upper bounds  . Note , that nmt . -dnratlve events are not wdid for each subinterval of the event boundarie ~ L The third notation b  ; used for re . ( mltafiv events ( e . g . stop ) . 
The start time of ares nlt at lv event lies within an interwd whereas the end time is a time- -point  . 
Finally , the last notation is used for in choatlve events ( e . g . start moving , corresponding to the German verb loafah- . 
ten ) .   ) !nchoativ events have a well defined start t ime whereas the end time lies within an interval  . 
For the task of generating a coherent description of a tra\[fic scene NAOS first instantiates all event models and predicates which may be instantiated using the scene data  . This leads to the wellknown selection problem of natural language generation  . For one object , timre may be many instantiations with different ime intervals  , hence the task of the verbalization component , o choose what to say . In the next section we discuss the theoretical background on which our verbalization component is based  . 
3' I heoretmal Background
In general , language is not generated per sebut is M ways in . -tended for a hearer . Furthermore , language is used to fulfil certain hearer about certain facts  . 
In NAOS the generatio of a deseriptiou of the underlying image sequence aims at diminishing the discrepancy between tim : ~ ystem's knowledge of the scene and the heater's knowledge  ( the same mo-tivation is nsed in Davcy's program \[61  )  . Concernig the hearer we make the following assumptions :  1  . S/heknowstide static baekgrmmd of the scene , i . e . the streets , houses , traffic lights , etc . 
2 . S/he did not utter specific interests except : Describe ~ hescel\]e ! A description may be the result of snch diverse speech acts a a!NI?ORM  , PROMISE + PERSUADE , or CONVINCE . 
NAOS only generates the speech act INFORM.
q binform a hearer a botl L something means to tell her/his1 so-mething s/he has not known before , somethint , that is tr , le and new . In NAOS the definition of trueutt , erances buiht ~+ on the si-tuational semantics of Barwise and Perry  \[31  . ' rhcymt derstandt , he Iue an ill gOf an utterance as a relatiout ) etwcetl the tit\[clause alld the described sitmation  . The interpretatiou of an ut Lerance by a hearer usually consists of a set  , of possible situations with a meaning relation I ; otheut , terance . We now define an uta:rance to be true if the set of possible situations cooJ ~ ains the actually occn rred situation  . 
The requirement to generate true utterances has two consequell-ces for our verbalization component  , l , ' irst , the verbalization process nnlst take the bearer's lneanillg relations into account  . This coincides with the eommt mication rule to tune one's utterances to the heater's comprehensionability  . Second , assumiug that the speaker hastide same meatnng relations as the hearer  , the speaker can it lt-ticipate the hearer's interpretation of an IIL teraaee  , i e . the possible situations implied solely by the utterance can be generated without knowledge of the actual situation  . In the case of scene descriptions these situations are equivalenl  ; to the heater's visualization of an unknown scene . 
An utteraace must be new totile hearer in order I , o inform him . 
In the context of situational semantics we define an utterance to be new if its interpretation restricts the set of possible situations implied by previous utterances  . Thus new information additionally specifies described situations  . 
The task of a verbalization component is to choose utterances such that they inform in the above sense  . Therefore it is necessary to anticipate the hearer's understanding for judging whether a planned utterance carries new information  , The general principlet brhearer simulation is depicted in figure  1  . 
U1TERANCE............>UTTERANCE
CASEFRAMES CASEFffAMtS
T ~ __DEEPCASE - - ~ I " - - - SEMANflCS - - - - ~
EVENTSEVEN\[S
T , 5==: EVENTiNODELS = ; ~
GEOMETRICSCENEVISUAL\[ZEUSC h~\[
DESCRIPT\]ON DESCR\[P\[ION
SUEAKER iIERREU
Figure l : llearer simulation
On the side of the speaker the event recognition process leads by use of event models to instantiated event models  ( called events in the figure )  . Alirst selecl fion process chooses amoug the instantiati-ot is those which are to be verbalized  . As event models are associa-ted with verbs the appropriat  , ccase frame of the verb is available . 
A second selection process now chooses among the optional deep cases of the verb  . This is where the deep case semantics comes into play  . If , for instance , it it+decided that a locative expression should be generated it is necessary to know how the location of an object may be expressed in natural language as in the geometric scene descriptiou the location of an object is given by its x  , y , and z coordinates . ' l+le deep ease set nanties also eoilt ~ Lius information about the prepositions which may be used for expressing a specific deepease  . 
Assuming that the hearer has the same meaning relations as the speaker hebasically can use the speaker's processes in reverse order and reconstruct the underlying case fi'ame fror oI  . he utterance and thns build a visualized scene description  . 
Note , however , that we agree with Olsou \[21\] that the verbalization of a visuale wmtal ways leads to a loss of infkwmation  . In our cam ~ , for instance , we ca\[lllOtlSSlli'Ile/~llat the hearer knows the x  , y , audz coordinates of , an object when he hearst imphrase i ~ l\[f onto ' the del  ; arf , lne , q to  #comptJ terscience .  5 ; llcha\[ ) hl'ase\[~elleraLes a set of coordinates delining the regi  ( m which corresponds t ~ the preposition in - . front + of The act . a \] Io cat , iun ++ I ' the object which gave ri ~+ e to t , hege a eratiot l of f t , hepl n ' a selies somewhere within that region . Preset , tly , hearer modelings l , ~> l ) +; at I , holevel of ease frames aml the visualized scene i a a m  . ie ip a Led ( seexect . ion , 1 . 2) . 
As shown in figure I the case frame of a verb plays a cett tral role in our verbalization compoaent  . We adopt the view of Fillmore expressed in his scelles -+ and-fralness lnall Lic  \[71 I ; ll+t t case frames relates ceues to natural language x pressions  . 
4 The ~3 election Problem
Usually this ln ' oble misdivided in tc~the subtasks of deeiding what to mxy and how t  ,  . say it, . As mentioned above NAOS uses two selection processes  . First , it selects amoug the instantiated events and second  , it selects among the optional deep cases of the verb associated with the chose uevellt  . The first selection process corresponds to deciding what to say and the second one determines largely how to say it as will be shown later  . 
The selection processes are based on the representation of the ease semantics of an event model and on a specialization hierarchy of the verbs  . Below is the representation of the case semantics for tile event model iiber hole n  ( over take )   . 
Agent-Rest L:VEIti ( 0 LE
Deep--canen: ( VERBUEBERH flL )   ( UEBERI10LEN*SBJ 1*0BJ2*T1*T2 ) 
Obligatory : ( AGENTAGT- . .EXP ) ( ItEF AgT-EXP * OBJI ) ( TENSE TNS . ,I~XP )   ( TIME-REF'f NS-EXP*T1*T2 )   ( OBJECTIVE SBJF , XP ) ( REFOBJ-EXP*ONJ2)
Optional : ( LOCA'r IVEL OO--EXP ) ( LOC-'N . EFLUC-EXP*SBJ1*Ti *'1'2)
Combilm ~ ionlJ : NIL
Loc-prep~3: ( All AUI , ' BEIIIINTERI~lNEITEN
UE I\]ERUNTF,RVORZ~,'llSCttl ; ; tO
The first slot specifies the agent restriction . The deep < as us slot ration component and second the formal notation for an instantiation  . The obligatory cases must be generated but may be omitted in the surface string in case of elliptic utterances whereas optio-nal deep cases need not be generated at all  . In the combinations slot it is represented which deep cases may be generated together  ( e . g . for the verb fahren ( drive ) it is not allowed to generate a single SOURCE but instead SOURCE and GOAL must be generated  )  . The Lot-prep slot specifies the prepositions which may be used with the verb iiber holento generate locative expressions  . 
The case descriptions in the obligatory and optional slots consist of two parts : a declaration of an identifier for the case expression on the language side  , and a predicate ( in general a list of predicates ) relating the case expression to the scene data . The most important predicates are REF , TIME-REF , and LOC-REF . 
REF generates referring phrases for internal object descriptors like  BMW1  . TIME-REF generates the tense of the verb . As descriptions are usually given in present ense  , presently TIME-REF only generates this tensc . LOC-REF relates the abstract location of the object as given by its coordinates to a natural anguage x-pression for a reference object  . Note , that REF has to be used to generate a referring phrase for the reference object  . Consider the sixth entry of the database in section  2  . The instantiation only contains internal identifiers for objects  , like traflic-light l , for which referring phrases have to be generated ( see section 4 for further details on REF )  . 
In NAOS we use a specialization hierarchy for motion verbs  . 
This hierarchy is pragmatically motivated and is rooted in situational semantics  . It is no hierarchy of motion concepts as the one proposed in  \[23\]  . It connects general verbs witb more special ones . 
A situation which may be described using a special verb implies the application of all more general verbs Take for instance the verb iiber holen  ( over take )  . \[ t implies the use of the more general verbs vorueber fahren ~ vorl  ) ni fahren ( drive past )  , passieren(pass ) , naehern-r ( approach ) , entfernen-r ( recede ) , fahren ( drive , move ) , and be we gen-r(move ) . 
It shonld be intuitively plausible that such a hierarchy is also used for event recognition  . If , for instance , nonae her n-r ( approach ) can be instantiated the more special events need not be tested  . 
4.1 Event Selection
In NAOS the overall strategy for generating a descriptive text is as follows : * Group all moving objects according to their class membership  ; ? For each object in each group describe the motions of the object for the time interval during which it was visible in the scene  , Event selection for an object is done according to the following algorithm :  1  . Collect all events in the interval where the object was visible and where the object was the agent  ;  2 . determine for each time point during the object's visibility the most special event of the above collected ones  ;  3 . if two events have the same specificity then either take the one which started earlier and has the same or longer duration as the other one or take the one with longer duration  ;  4 . put the selected events on the verbalization list of the object in temporally consecutive order  . 
Consider the following example . All events which were found for PERSON 1 are : ( /ALKPERSONI04 0 )   ( OEHEI~PERSONI04 0 )   ( MOVE PERSONIO 40 )   ( BE~//EGEN-RPERSO Ill0 40 ) The above algorithm leads by use of the specialization hierarchy to the following verbalization list for  PERSON1:   ( ( ( ~ IALK PERSOI~I04 0 )   ( 0 20 ) )  ( ( RECEDE PERSON1 FBI 20 40 )   ( 20 40 ) ) )   ( The last entry in parenthesis of each selected event denotes the interval in which the event was the most special one  . ) 4 . 2 Select ion of Opt iona l Deep Cases This selection process is our first implementation f the strategy of anticipated visualization  . The underlying question is : Which optional deep cases should be selected to restrict the hearer's possibilities of placing the trajectory of an object in his internal model of the static background of the scene ? In NAOS the selection algorithm answering the above question is rather straightforward  . It is based on the manner of action of the verb , the verb type , and the heater's knowledge . The algorithm is graphically represented in figure  2  . 

NON-DURATIVE , INCHOATIVE

Tl , , o = EBAT , , , a = SE
Tb , ~= SB^T , , , d ? ~ SE
T ~, ~ ~ ~ - SBAT , , d = SE







DIR , STAT


DIR , REO





DEEP CASES

DIRECTION ?, LOCATIVE ?





DIRECTION ?, LOCATIVE ?





SOURCE ?, DIRECTION ?

REDNIL
T~,~~8B^T , , a ~ SEDIR , STATLOCATIVE ?
LOC SOURCE ?, GOAL7
Figure 2: Selection of Deep Cases
The abbreviations denote : Tb ~ , Ten , t : start , end time of the event ; SB , SE : scene begin and scene end ; I)IR , LOC , STAT , RED : directional ( turn off , return ) , locomotion ( walk , over take ) , and sta-tic ( stand , wait ) verbs , finally verbs whose recognition implies reference objects  ( reachs . th . , arrive at ) . 
The figure has to be read as follows . If an inchoative vent like losfahren ( start moving ) has to be verbalized which has the verb type locomotion  , then choose direction ? and locative ? as deep cases  . The question mark generally means , look into the partner model Lose e whether this deep case has already been generated fi  ) ranother event . If so , determine by use of the object's actual ocation ( represnnted in the scene representation ) whether it is still valid . If this is the cased on't generate a uatural anguage expression for this deep case  , otherwise do . 
Presently the partner model contains information about the sta-tic background of the scene and about what has been said so far in the same relational notation as was shown for instantiations in section  2  . It is being updated when an event is verbalized . 
Note , that for durative vents the decision is based on whether the start and end time of the event coincide with the beginning or ending of the image sequence  . Consider the first case for durative events as given in figure  2  . Right from the beginning of the sequence there is a car moving along a street until the sequence ends  . In such a case it is not possible to verbalize a source as the object may have started its motion anywhere  . To restric the hearer's visualization , direction and locative cases are verbalized , leading to a sentence like : The car moves on Schliiter street in direction of Ha Herplace  . 
Verbalizing a direction when the static background is known restricts the trajectory to being on one side of the road  . Basically , our direction case is a goal or source ease where only two prepositional phrases are allowed  , the German phrases in Richtungandaus Richtung ( in direction ~ from direction )  . These phrases do not imply that the motion ends at the goal location as do most prepositional phrases in German which have to be inaccusative surface case to denote a goal  . The English language is in this respect inherently ambiguous  . In the sentence The car moves behind the truck , the phrase behind the truck may denote a locative or goal deep case  . In German these eases arc distinguished at the surface  . 
\[" or locative the above sentence translates to DesAitt of ~ hrth inter dem LKW  , for the goal case , it translates to DesAut of ~ hrth interden LKW . 
We have to distinguish different verb types as e . g . the meaning of a directional phrase changes with the verl  ) type . Consider the sentences The car moves in direction of Haller place versus The car stands in direction of l\[aller place  ( in German both sentences arc wellformed )  . The first sentence denotes the direction of the motion whereas the second one denotes the orientation of I  , hccar . 
We thns distinguish between static ( STAT ) and h ) eomotion ( LOC ) verbs . The third verb type , directional ( I)IR ) , is used for verbs with a strong directional component like umke hren  ( return )  , abbie-gen ( turn off ) , etc . As they already imply a certain direction the additional verbalization of a direction using a prepositional phrase does usually not lead to acceptable sentences  . The fourth type ( REO ) is used t br verbs like erreichen ( reachs . th . ) having an obligatory locative case . 
The main result to note here is that the selection processes are low-level and verb oriented  . The only higher level goal is to inform the hearer and to convey as ranch information about an event as possible  . In the next section we show by differem ; verbalizations of the same scene how rather complex syntactic structures arise  . 
5 Generation
The general scheme for the generation process is as follows:  1  . Sort the objects according to their class membership  , vehicles first , then persons ; 2 . in the above partial order sort the objects according to their time of occurrence in the scene  , earliest first ; 3 . do for all elements in each verbalization list of each object  ( a ) if the current event has a precedent and its event time is included in the precedent's  , begin the sentence with dabei ( in the meantime ) ; goto(c ) ;   ( b ) if the current event has a precedent and its event time overlaps the precedent's  , begin the sentence with unter-dessen ( approx , in the meantime ) ; goto(c ) ;   ( c ) determine the optional deep cases and build a simple declarative sentence by using all chosen deep cases and applying the deep case semantics  . 
Two temporally consecutive events are not verbalized using a temporal adverb as in the cases of inclusion and overlapping  . This is due to the fact that from the linear order of the sentences the hearer usually infers consecutivity  . 
The result of the above algorithm is a formal representation f the surface sentence which  , rougidy , contains the w~rb'stem , gemls verbi , modality , and person , all deep cases in random order , and all stems of the \ [ exical entries which appear in the surface sentence  . 
This representation is taken as input by the system SUTRA  ( for further details on the formal represeutation ad the SUTRA system see  \[41  ) which then generates a correctly inflected German sentence  . 
Below is an example of the output of NAOS.
18., ausgabe text
DIESZENEEN ' rHAELT VIERBE WEG TEOBJEK TE : DREI
PKWSUNDEINENFUSSGAENGER.
The scene consists of four moving objects : three vehicles and a pede-strian  . 
EINGRUENER VWNAEHER TSICtt DEMGROSSEN FUSS GAENGERAUS RICHTUNG tIALLER PLATZ  . ERFAE-
IIRTAUFDERSCHLUETERSTRASSE.
Agreen VW approaches the tall pedestrian from the direction of flal-ter plaee  . It drives on Schlseter street . 
EINGEL BERV WFAE HRT VONDER ALTENPOST VORDIE AMPEL  . WAEHREN 1)I ) ESSENENTFERNTERSICH VON
DEMGRUENENVW.
A yellow VW drives from the old post of tice to the tra ~ clight  , h ~ the mean time it recedes from the green VW . 
EINSCHWARZERBM WFAEHRTINRICH TUNG ttAL -LERPLATZ  . DABEIUEBERIIOL TERDENG ELBENVW VORDEMI "ACIIBERI  , \]ICI\[IIN FOR MATIK , DERS Clt WAR ZEBMW
ENT FER NTS1 CI1 VONI ) EMGRUENEN VW.
A black BMW drives in the direction of Hallerpiace , During this time it overtakes the yellow VW in front of the department of computer science  . 
The black BMW recedes from the green VW.
DERGROSSEFUSSGAENGERGEHTINRICH TUNG
DAMM TOR AUFI ) EMSUED LICIIEN FUSS WEG WEST-LICHDER SCHLUETERSTRASSE  . WAEHRFND DESSENENT-FERNTERSICH VONDEM FACIIBEREICH INFOR MATIK  . 
The tall pedestrian walks hJ the direction of Dammtnron the southern side walk west of Sehlseter street  . h ~ the meantime here cedes from the department of compnter science  . 
19., logout
The first sentence above is a standard one having the same structure for all different scenes  . The remaining four paragraphs are motion descriptions for the tbur moving objects  . 
We now discuss tep ( c ) of the above algorithm in more detail as it cover some interesting phenomena  . 
Consider the third paragraph describing the motions of the yellow VW  . The verbalization list for this object is : ( ( ( DRIVE VW 1 10 20 )   ( 10 25 ) )  ( ( RECEDE VW1 ~2 25 32 )   ( 25 32 ) ) ) The beginning ( SB ) and ending of the sequence ( SE ) lie at points 0 and 40 , respectively . According to the selection algorithm ( figure 3 ) a SOURCE should be verbalized for a durative event with the above even time if the verb type is LOC  . The generation algorithm checks whether the chosen optional cases are allowed for the verb  , if so , it is further checked whether the combinations are allowed  . 
As a SOURCE may not be generated alone for a fahren  ( drive , move ) event , SOURCE and GOAL are generated . 
The fourth paragraph shows the outcome of a deep case selection in which the chosen case is not allowed for the verb  . The verbalization llst for the black BMW contains only ilber hole n  ( over take ) and entfer nen-r ( recede )  . 
( ( (OVERTAKE BMWI VWI ( 10 12 ) (12 32 )   ( 10 32 ) )  ( ( RECEDE Bl~qt ~/2 20 40 )   ( 32 40 ) ) ) According to event-and verh type DIRECTION is chosen as the appropriate deep case  . As this case may not be used with the verb overtake two sentences are generated  , one describing the direction second sentence begins with a temporal advert  ) specifying that both motions occur at the same time  . In order to generate the two sentences first the class membership of the agent of the verb which may not take the chosen deep case is determined  . Then the speeializa-tionhier are hy is used to go up to either fahren  ( driv % move ) org nhen ( walk ) as those verbs may take any deep case . Then the sentences are generated . 
Consider the following verbalization list : ( ( ( OVER TAKEBI~WIVW 1 ( 0 8 )   ( 12 18 )   (  0 18 ) )  ( ( DRIVE BI'~I 0 40 )   ( 18 40 ) )  ) Assuming the direction and location of the motion to be the same as before the algorithm presented sofat " would generate A black BMW drives in the direction of Haller place  . During this time it overtakes the yellow VW in front of the department of computer science  . The black BMW drives . 
According to the deep ease selection algorithm a DIRECTION and LOCATIVE should be generated for the second event above  . 
As both cases have already been generated with the first event and are still valid the sentence The black BMW drives is not generated because before generating a sentence it is checked whether the intbrmation is already known to the partner  . 
5.1 Referring Phrases
In this section some aspects of the referring phrase generator are discussed  . As can be seen from the example text objects are characterized by their properties  , introdueed with indefinite noun phrases when they are not single representatives of a class and they may also be pronominalized to add to the coherence of the text  . 
Therefore we use standard techniques as e . g . described in \[8\],\[9\] . 
We want to stress one aspect of our referring phrase generator  , namely its capability to generate restrictive relative clauses with motion verbs  . As it may be easily the ease that a scene contains two objects with similar properties the task arises to distinguish them and generate unequivocal referring expressions  . 
It is an interesting fact , that , we have several options to cope with this problem which each have their consequences  . 
One option is to adopt McDonald's scheme of generation without precisely knowing what to say next  \[13\]  . According to this scheme two similar objects are characterized in the following way in NAOS  . When the first one is introduced it is characterized by it's properties e  . g . a yellow VW . When the second one has to be introduced , REF notices that a yellow VW is already known to the partner and generates the phrase another yellow VW  . It starts get-ting interesting in subsequent reference  . The objects are then characterized by the events in which they were involved earlier whether as agent or in anothe role  . This leads to referring phrases like the yellow VW  , which receded from the pedestrian or the yellow VW  , which has been overtaken . Note , how passive relative clauses arise naturally from the task of generating referring phrases in this paradigm  . The same is also true for negation . Consider the case where the first yellow VW , say VWI , has passed an object and the second yellow VW , say VW2 , has overtaken an object and both event , s are already known to the partner . If REF has to generate again a referring phrase for VWI it notices that pass is a more general verb than overtake and may thus also be applied for the overtake event  . It therefore generates the phrase the yellow VW , which has not overtaken the other object to distinguish it unequivocally from 

Below is an example of this strategy in a texL for the same scene as above  . The difference to the th'st scene is that we replaced the green VW by a yellow one  . 
10., ausgabe text;
PKWSUNDEINENFUSS(~AENGER.
The scene consists of four moving objects : three vehicles and aped e  . .

EINGEL BERV WNAEIIER TSICIt DEMGROSSEN FUSS -GAENGERAUSRICIITUNG tIALI  , ERPI~ATZ . ERFAEHRT
AUFI ) ERSCHLUETERSTRASSE.
A yellow VW approaches the tallpe destrian from the direction of flaller place  . It drives on 3 chlueter street . 
EINANI ) ERERGEL BERV WFAE HRT VONDER AUPENPOST VORDIE AMPEL  . WAEtIREND DESSENENT FFRNTER S1Ctl VONDEMGIdLBE NVW , DERSICItI ) EMGROSSEN
FUS SGAEN GERGEN AEHER THAT.
Another yellow VW drives fi'om the old post office to the trall l clight  . 
\[ n the meantime it recedes from the yellow VW which approached the tall pedestrian  . 
\[! ; INSCHWARZERBM WFAEll RTINR1 CHTUNGII ALLER-PI , ATZ . I ) ABEIUEBEtHIOL TERDEN ANDERENCELBENVW , DF , RSICIIV ON 1) EMCELP , I ~ NVWENTFERNT flAT , VORDF MFACIIBFI-H ~ , ICtt INI , ' Oll MA'I'IK . DERSCH WAR ZEBMWENTI , 'ERNTSICItVON DEMGI!H , BI ~; NVW , DEINICIIT
UEBERIIOILTWORI)F,NIST.
A black BMW drives in direction of If aller phtce . Dewing this time it overtakes the other VW which receded fronltheyellow VW  , is ti ' oet of the department of computer science . Tile black BMW recedes fl ' om the yellow VW which was no to w ~ r taken  . 
I ) EIGROSSE FUSS ( . ~ AEN(\]Ie,R(IEIITINR 1 . CIITUNGI ) AMMTORAUFI)I , ,M SUEI)LICHI , 2NI , ' USS WEG WEST-LICHDERS CIILUI'~TIt ' , ISTRASSE . WAIi;IIIENDI)FSSI , ; NENT-FERNTE l1 . SICHVONI ) FMFACIIB h'J ~ . E\[Clt INFOR MATIK . 
"/' liet all pedestrian walks in direction of Dammtor on the southern side walk west of Schlueter street  . \[ n the meantime here cedes from the department of computer science  . 
11., logout
The consequences of this first option are rather complex syntactic structures whieh are not inotivated by higher level stylisl  . ic choices . 
1 , elus now look at a second opt , i on which has also been implemented . Experience with the above algorithm for dill % rent scenes showed  , that if more than two similar objects are in a scene the restrictive relative clauses become hardly mlder standable  . We ~ , hus determine how many similar objects there are in the scene before we start the generation process  . If there are more than two , REF generates names for them and introduces them as e  . g . the first yellow VW , the second yellow VW and so on and uses these phrases in subsequent references  . An example of this strategy would look like the first example text where the different vehicles arenan mdl  , he first . . . , the second . . . . Tbe rest of the text would remain the same . 
Taking this option implies leaving McDonald's scheme and approaching to a planning paradigm  . 
It should be noted here that there is a third optimt which has hardly been investigated  , namely to switch frmn contextual to co-textual reference as in phrases like the VWI mentioned last  . We need filrther researche for e we can use such techniques effectively  . 
6 Conc lus ion and Re la ted Research We have proposed the scheme of anticipated visualization to generate coherent exts describing reaL-wn rld events  ( visual data )  . 
The selection algorithms are based on low-level , verb inherent pro . -perties , and on a pragmatically motivated verb hierarchy . ' lk~gether with t , heverbalization componenthe NAOS system is now fully operational from event  , recognition to text generation in the domain of trafl'i escenes  . As this domain is rich enough to still pose a 1ol   ; of problems I , his opens upl , heol ) portunity t , oinl ; egral ; e hig-her levels l , rabel Jiesfore . g . combining sentences , selecting evengs , generating deie ~ ie expressions , el ; e . 
The main difference between NAOS and other systems for language generation is that  , we approach the verbalization problem from the visual side  . and thus are led to use basic selection algo-ril ; hms . Other systems like TAI , ESI'i N\[151 , KI)S\[12 J , TEXT\[1 , t , KAMI'\[l\] , and I1AM-ANS\[1() start their proees si , g wibh language whereas NAOS starts with images . In close emmection to our re-sea , < , is U , ewo , ' k , , f \[21 ,  1~ , 4 ,  1231 ,  \[?? , \] ,  ~ ,  . , , d\[,%'rhefi , . sti  V ) u , . 
authors deal wilJl questions of moqon recognition and with a re-  . 
ferell ccsenlant , ic forirl Ot ; i Orl verbs ) Lit ~ Ll'e I Iot . CoLleerlled witli , exL general ~ ion . They showed that case frames can I wused to generate single ut l  , erancem Conklin and Ivh : l ) on ald use the notion of salience to deal wil , hg heseleel , ion problem in the task of describing a single image of anal  ) uraloul , door scene . 
TALE SPIN exemplifies ~ ha ~ ; plans and goals of an actor may form the underlying sl  , rueture of narratives and may I ; hus be motivation for l ; ext generation , hiKI ) S are presental , ion of wha ~ to do in ea ~( . , of fire a la rm is transformed into a natural language  . 
text . As the initial represent a 1 , i on already contains lexiealeni , ries and primitive l ) roposil fions the task is to organize tJds information a new so that i ~ may be expressed ill an English text  . Matll / and Mooreprol ) ose rules for ( : oml ) ining l ) ropositiol m and re ,  . ediL the texte onl , inuously to produce l , he final version . TEXT gem . ' rate ~; pars . 
gr ~ tplls as ai is wel's ~ oqtlestiolls a\[ ) ollt da\[ , a base Stl ' llCtl/Fe . \[~ e/cl(ef)wI1 lasiden l ; ified discoursestra ( . e , gie , ~ for fulfilling three (; ( mmmlaie ~ fl . ive goals : detine , compare , aud describe . Theses l , rategi(~sg , d de , the t , ; e-aeration l ) ro(:e . < lsill deciding what ; to say \] lext . M e ( , e o w l I l s e s 1 , he qucsl ; i onto deteemine tile eom munh : al . ivegoal that the text should fldfil . Research of IJ fisk in disvery important o clarify ~ he relation between l  , he\[orln of (- z text and il ; s underly in goals . 
( ) ue of I ; he domains of IIAM . .ANS is the Mad of I ; raflic scene which is also used in NAOS . /nthis domain IAM-ANS deals with primarily with answering questions about ~ he tool  , iota ; o\[ol@~cts and wi~h over answering yes/no que , % ions\[25I . The dialogue (: or e-i ) onent , of IIAM-ANS may be commcted to NA () ~ gI ; o also allow quest , ions of the user if ' tm generated text was nots M \ [ ieienlfi  ) rhisunderst ; and ing . An evalual ; ion of the kind of question being asked by a user may help in devising bel  , tergeneration strategies . 
( AMP is a , system tbrplamfing natural languago ubteranees ia the domain of task oriented dialogues  . The 1) lant finlgal to rith mi ; akes 1 ; he knowledge and I ) elief ' a of the hearer into account ,  . '\[' hisy . -stem shows low a priori beliefs of 1 ; he hearer may a \] L ; o be integrated in NAOS to gener at ; eappropria / ; e referring phrases . 
It would be interesting to use a phrasing componen ~ for NAOS which would firs /  , determine all deep ease suecessary ~ o maximally restrict \ [  , hevisualized t , ra . jeet ; ory of a nobjeet's mot , ion sequence and then try to distribute I ; he cases to the di\[ferent verbs u . sed in the descripl ; ion in order to general ; esmooth text . 
.~fl l ography \[1\] Appelt , \]) . E . , iPlamfi*Jtg Nat m'al-Lal:G'tmge Utt ; era:nee ~: to ~ hd ; i . ~; fyM ' ul Liple Goah ~ . SRI lntecn , xt lonal , Technical Note 259 , Menlo Park , 
CA . , 19822 Bad \] or , N . I . , ' . l_~m ~ po:ral . ~ . Ice , teAnalysis:(\]oxtceptnal\]i)e . , : (: ril ) tio L ~ of Object Movemenl : mleport TR- . 80, l ) ept , of CS , University of
Toronto , 197513\] Barwise, . I . , Perry , J . , ~ il ; L~atio~s and Attibnde . <; . Bradford Books,
MIT Press , 198314\] Busemann , S . , ~ qurlhc . eTrans li ) rmations dm ' ing the Generation of Written German Senteneea  . hcllolc , L . ted . ), Natural , anguage Generation Systems . Springer , Berlin , 1984\[51 Conldin , E . J . , McDomdd ~\]) . D . , , qallenee : The . Keyt ; othe Se-leeti(mProl ) leminNat:m'alLanl , mage Generation . COI , ING-82, 129-13516\] Davey , A . , l ) i ae our ~ e Produel ; i,m . A Computer Model of gome Aspeel : ~ of a Spealter . Edinburt ; hUniwn'siWPress , 1978\[71 Filhnm'e , C . ,\] . , , qeenes-and . -fl'amesgemantle, . , ILL:Zampolli , A . ( ed . ), fAngui . stieStructures Processing . No Lth-lloll and , Amsterdam ,  1977 , 55-81\[8\] Goldman , N . M . , Coneeplaml (-' eneration . In : Scha . lqIt . C , ( ed . ), Concept . alhffolmation Processhlg , Noi~h . I\]olland , 1973, 289~371191y onllahn , W . , Hoeppner , W . , , lame . son , A . , Wahlster , % i . , ' . Fhe AnaLomy of the Natural Langm~gert lialogueb ; ysl:emJ(\]AM-~LPIVi . In : Bole , i , ( eLl . ), Natm'al \], anig, . age \] lased (1o mimter . Systems . 
Ilanser/McMillan , Miinchen , 1980, 11!) . 253\[101l\[o eppner , W . , (' hristalhw , T . , Marburger , II . , Morik , K . , Nobel , \[I , O ' Leary , M . , Wah Mmr , W . , lhWmld . k , nah ; o . ~ndepend cnee : Ex-perlenee with the Develol muml ; of a(-\]eri La\]l ) #~ lglla ~2A ?: ee!i\[i , ~ iysl ; emtot ( ighly Diverse gaek grmmdgy . 'nh . 'r, . q , lJCAI . .83,588 . .594 Ill \] Jamesoa , A . , WM dster , W . , / lser Modelli~lg in/~nlflmra Gener-al ; it nl : lgl lipt dt la ~ tdl ) ei ~ nlte )) oser ip ( . i a ~ . ECAI-82:222-227\[12\]Mami,W . C . , Moore , J . , ConLlmter GeneratiwL of Mnli ; iparn-graph ~' exl ;  . AJC , 711), 1981, 1% 29 ( J3/McDonald , I) . l ) . , Nat;m'al L . ' nL gnage ( ~ enera LJ on as a Com-prd . a : ional ) ~ rol ) ! enl : a ~ l ~' n ; lod ' netion . in : lh'ady , M . , Bmwick , H . C . ( eds . ), Computai Aonal Models of Discourse . M\]T Press , Cambridl ; e,
Ms : Is . , 1983, ?,09- . 265114\] McI(eown,\[CII . , ) iL ; eom ' ~ m ~ . ; I ; ratx wje ~ for (- ~ eneral ; i, . ~;N : qmral-iLaxiff , mlge Text . Artificial hd . elligenee 27, 1985, 1-, tl115\] Meehan , J . , ' YA . LIn . . . qPIN . h < S chank , I . C . , lli , , d : , eck , C . K . ( e&; . ), hmicte Conllmterl . Indel-st ~ Lnding : Vive PioF , rarlu ~ plus Miniat . ures . I , EA , llills dMe , New Jersey , 198 i , i97-258\[161 Nemmmn , ll . , Naturall , angm~ge Descrii ) l;im ~ of ' Yi : ~ bm . SVaryi : , ~ g ~ 3 ee : nea . ln : Walt , z , l) . ted . ), Advances in Natm'MI , : mge . agel ' rocesses . 
Vohlme1 ( in press ); also as I , ' II\[-\[Ilt . .\]L . 105/8, t , l /' ach be reichl format , ik,
Unlversit littlamburg , 1984117 Neumann , ll . , On Natm'al Language Aecem ; to \] ( mage 5h > quence t ; : Ewml ; \] Reeognition a ~ d Verbalization . Proc . Firsl Confer--trice oil Altilici all nl . elllgence Apl ) lications ( CAIA-8 , 1) , Denver , Celorz ~ clo , JVJ(odelsfi ~ rl in agef:leqnence\[:al ; erpretat ; ion : The u : J ~ lles .  (' . SRG q'echn . Note // ad , University of ' l ;) ronto , 1983\[191 Neumann , It , Nov M ? , I \ [ . . . J . , F , ~ entModels Rn'Recognition and Na;m'al . ', angnage Description of I'Ivenl; . , tinli ~ . ea\] . ' World~mage ~; eque neem IJCAI . .sa , 724-726 \[20J Nowdq II . .J . , B1  ~ . elatitmal Matching Strategy tb '_ , ' tremlmral Event li ' . eeognitioy L . ILL:l , aubsclh . / . ( ed . ), CWAI-84 . \]nformaeik Fach-berichte10 a , Springer ~1985 , 109-118121/OlsonI) . R . , Lang , Ual ; eUsefor(dommmd cat : i ~ G , -\[ ntd:muq ; inI >; mud ' . l Phlnklng . In : I ' ? eedle , l . O . , Carroll, . I . \[ L(eds . ) , I , an? , uage Con > prehension and l , he Acquisition of Knowledge . Washington , 1972\[221 Okada , N . , Concepl ; ual Taxonomy of Japanese-Ve : < bsfo:e ( )' n-dertLI ; and lngNatnrall\[ , a : nguage and i qeture Pat , retire . COI,\[NG-80, 127-135\[23\]il'sog:~os,J . K , A . Frameworlu : Cot Visaal Motion " crnders La ~ Lding . 
CSttC"I?R . -114 , University of Toronto , 198012/!'l'suji , S . , Km'oda , S . , Morizono , A . , "(\] . a der sl ; and hW ; aSh ~ JpleG ' arl ; o on Film by a Compul ; er Vishm System . lJCAI-77, 6(\]9-610\[2\[;\]Wahlat . er , W . , Mi~rburger , H . , Jameson , A . , Bnsemann , S . , Overm ~- e ~ werh ~ g . ' - . eLi-No-Qneatioim:\]t'\]xtended\]t / . esponses ia a N , i *_ d:er-face f , oagi0ionSyi*tem . IJCAI . .sa , 6,13-646
