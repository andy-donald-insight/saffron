PROBABILISTICTAGGING WITH FEATURI~STR , UCTUR , I;3S
ntrcI(empe
University of Stuttgart , Institute for (? omputational Linguistics , 
Azenbergstrage 12 , 70174 Stuttgart , (lerniany , kellipe ~_) ims . uni-stuttgart . de

The described tagger is b , ' used on a hidden Markov model and uses tags composed of feature such as part-of speech  , gender , etc . ' l ? he contextual probability of a tag ( state transition probability ) is deduced from the contextual probabilities of its feature -- value-pairs  . 
This approach is advantageous when the available training corpus is small and the tagset large  , which can be the case with morphologically rich languages  . 
1 INTRODUCTION ' l'he present article describes it probabillstic tagger based on a hidden Marl  ( or model ( IIMM )   ( Rabiner , 1990) and employs tags which are fe , ' iture structures . 
Their features concern part-of-speech ( POS ) , gel , der , number , etc . and t lave only atouiie vahles . 
Usually , the contextual probability of a tag ( state transition probability ) is estimate dividing a trigra infrequency by a bigram frequency  ( second order IIMM )  . 
With a large tagset resulting frointire fact that the tags colitain besides or the POS a lot of l norphologi-cal information  , and with only as ln all training corpus available , most of these frequencies are too low for an exact estimation of contextual probabilities  . 
Our feature structure taggerest hnates these probabilities by connecting contextual probabilities of the single feal vre-w due-pai  , 's ( rv-pairs ) of the tags ( cf . sec . 

Starting point for the iulph ; nientation of the\['ea-ture structure tagger was a second-order-li'Ivl M tagger  ( trigrams ) b~med on a modilied version of the Viterbi algorithm  ( Viterbi , 1967; Chllrch ,  1988 ) which we had earlier implemented in C ( l ( empe , 1994) . ' Flier : we modified tim calculus of the contextual probabilities of the tags in the above -described way  ( cf see .  4) . 
A test of both tatters under the sanle conditions Olia French corpus  1 has shown that tile feature structure tagger is clearly better when tim available training col pus is small and the tagset is large but the tags are decomlmsable into relatively few fv -pairs  . ' l ' he hitter can be the case with morphologically rich languages when the tags contain a lot of morphological infornia-tion  ( cf . see .  5) . 
11 in llnm chobliged to Achim Stein and Leo W , tuner , lto-nl ~ . UC ~: l ) ept . , Univ . Stuttglirt , Gel'lll & liy , for t ~ rovidlng the cor-ptl S and it dictionary . 
2 MATHE MATICAL BACK-

In order to ~ Ls sign tags to a word sequence , a IIMM can be used where tim tagger selects among all possible tag sequence stile most probable one  ( Garside , Leech and Saulpson , 1987; ( Tlnlrch , 1988; Browne . tal . , 1989; Rabiner , 1990) . The joint probability of a tag sequence l--I0 .   .   . tN_1 given a word sequencel g . , : ~ v0 . . . lON_-lish i the case of a second order IIMM :*' ( l ,   , Z , ) := ~ t , ,  , , ? p ( , v0 lZ0) . J , ( ivllZ ,)'
N1l-(p(,.,I',),(l,I(1) i=2
The term r qot , stands for tim initial slate probability , i . e . the probability that the sequence begins with the first two tags  . Nistimn unlber of words in the sequence , i . e . the corpus size . " Phetermp ( w?\]ll ) is the probability of a word w ? in the context of the assigned tagtl  . it is called observation symbol prolmbility ( lexical probability ) and can be estimated by : f ( wlll ) t , ( , , , ~ lt ~ ) -- f ( t  ~ )   ( 2 ) The second order state transition probability ( contextual probability )  1 , ( t ~ It ~-2 re- . t ) in formula ( l ) expresses how probable it ; is that the tag tl appears in the context of its two preceding tags li -'  , all ( \] ti-\] . It is usually esthnate . d as the ratio of the frequency of the trigram ( ll -'2 , t ~ - l , t ; ) in a given training corpus to the . I ' requency of the higram ( li_2 , li ~ lill , lies allie corpll S : f(ti- . ~ ti-~ti ) With a large tagset and a relatively small handtagged training corpus for inula  ( 3 ) has an iin l ) ortant disadvantage : The mai or il , y of transition probabilities cannot be estimated exactly because most of the possible trigrams  ( sequences of three consecutive tags ) will not appear at all or only a few tilnes a . 
I10 llrex a rrlp le we have a 1 , ' renclitraining corplls of 10 , 0 00 words tagged with a set of 386 different tags wh Ml could for rn a8a a = 57  , 512 , 450 different trigrams , but because of the corpus size no more than 10 , 000-2 trigranrs can appear . Actually , their nuinber was only 4, 8\[5, i . e .  0 . 0 08 % of all possible '2 A detaih M descrillt lon of pro\]ileli/S egnlsed by sniall and  ,  . 4el'O frequencies was given by Clah ~ and Church ( 1989 )   ( table 1 )  . 
frequency number and percentage range of trigrams in the range >  128   1   ( 0 . 021%) 64 - 127 2 (0 . 042%) 32-6313 (0a6%) 16-3143 (0 . 89 %) 8- 15 119 (2 . 5 %) 47 282 (5 . 9 %) 2~3 860 (18 %) 1 3 , 495 (73%) sum 4 , 815  ( 100 % ) Table 1: Trigram count from a French training corpus of 10  , 000 words When we divide e . g . a trigram frequency 1 by a bigram frequency 2 according to formula ( 3 ) we gelt be probability p = 0 . 5 but we cannot trust it to be exact because the frequencies it is based on are too small  . 
We can take advantage of the fact that the 386 tags are constituted by only 57 different fv-pairs concerning POS , gender , number , etc . If we consider probabilistic relations between single fv-pairs then we get higher frequencies  ( fig . 2) and t be resulting probabilities are more exact . 
From the equations n--\[1(t ,= e , 0 nc , , . . . he , , .  _  , =/ N ~ , ~  ( 4 ) \] k k=0 ) where tl means a tag and the elk symbolize its D -pairs and  ( \ k=0 I/\~=0 ( c d . p(e , olC , ), p(~,IC~ne , 0) . 
.  .   .   . p e ~ .   .   .   . Ielk ( 5 )   k=0 whe ~ Ci means the context of /~ and contains tile tags t  ; _ . ~ and ti1 follows p ( t i l  C i ) = p ( clol Ci ) " ~\ [ pelkCi 0 elj ( 6 ) k = t\I j=0 Tire latter formula 3 describes the relation between the contextual probability of a tag and the contextual probabilities of its fv-pairs  . 
The unification of morphological features inside a noun phrase is accomplished indirectly  , hra given context of D-pairs the correct fv-pair obtains the probability p = l and therefore will not influence t improbability of the tag to which it belongs  ( e . g . 
p ~ (0 num : SG\[ . . . ) = 1 in fig .  2) . A wrong fv-pair would obtain p=0 and make the whole tag impossible . 
a suggested bYM ats Rooth , IMS , Unlv . Stuttgart , Germany 3 TRAINING ALGORITHM In the training process we are not interested in analysing and storing the contextual probabilities  ( state transition probabilities ) of whole tags but of single fv-pairs . We note them in terms of probabilistic feature relations  ( PFI : ~ ) : Vr ' l~: ( e , Ic , '" ~; p(~ , Ic ~" ~)) (7) which later , in the Lagging process , will be combined in order to obtain the contextual tag probabilities  . 
The term el in formula (7) is afv-pair . G ~" ~ is a reduced context which contains only a subset of the fv-pairs of a really appearing context Ci  ( fig .  1) . C/~is obtained from Ci by eliminating all fv -pairs which do not influence the relative frequency of e  , ' , according to the condition : P ( e , ' lC~'"b)/p(e , lC ~) C\[1-e ,  1 + ~\]  ( 8 ) The considered D-pair has nearly 4 the same probability in the complete and in the reduced contexts  , i . e . Ci does not supply more information abont the probability of el than C/~''b does  . 
ti--2 ti--1 tl2typ:l ) l'3FIgen:FEMOgen:FEM2 geu:FEM hmm:S ( l0m~m:SG2 nu In : SG (  , , ) Figure 1: ( a ) Complete context Ci and ( b ) reduced context C/'"b of the feature-value-pair el = Ogen : FEM In the example  ( fig . la ) we consider tile fv-pair Ogen : l,'EM . Within the given training corpus , its probability illtile complete context Ci , i . e . in the context of all tile other fv-pairs of figure la  , is p~=44/44 = I(of . p ~ in fig .  2) . 
The presence of inum : SG in tag ti1 does not influence the probability of Ogen : FEM in tagIi  . Therefore lnum : SGeaube eliminated . Only fv-pairs which really have an influence remain in the context  . The reduced context C ~" b with less D-pairs , which we obtain this way , is more general ( fig . lb ) . 
In the given training corpus , the probability of Ogen : FEM in the context CZ " b is  p0=170/174=0  . 997 ( el . P0 in PFR0 infig . 2), which is near top ~= l . The reduced context C~''~ is used to form a PFR which will be stored  . 
4 A small change in the probabil ity caused by the elimination of fv-pairs from the context is admitted if it does not exceed a defined sman percentage e  . ( We used ~--3% . ) complete ones two advantages : ( 1 ) A great number of complete contexts containing many fv-pairs can lead after eliminatim  , of irrelevant fv-pairs to the same PFR , which makes then mnber of all possible PF lks much smaller than the number of all possible trigrams  ( cf . sec .  2) . 
(2 ) "\[' he probability of afv-pair can be estimated more exactly in a reduced context than in a complete one because of the higher frequencies in the first case  . 
The Generation of Pl ., ' ls
In the training process we first extract from a training corpus a set of trigrams where the tags are split up into their fv-pairs  . From these trigrams a set of PFILs is generated separately \[' or every f v q mlrei  . We examined four difl'erent methods for this procedure : Method  13: For every trigram we generate all possible subsets of its fv-pairs  . Many trigrams , e . g . 
if they dill k ' . r in only one fv-pair , have most of their subsets of fv-pairs in coil , IliOn . Both the complete trigrams and the subsets , constitute together the set , of contexts and subcontexts ( Ci and C/'''~ ) where in a fv-pair couhl appear . To generate Pl : lLs for at giw ' . nfv-pair , we preselect and mark those ( sub- ) contexts which are supposed to have an intluence on the contextual probability of the  . fv-pair . A ( sub- ) context will not be preselected if its frequency is smaller than a defined threshold  . We used ilferent ways for the pres-election : Melhod  1: A ( sub- ) context will be preseleeted if the considered D -pair itself or all fv-p  ; drl ) etong-ing to the same feature type ew ' . r appears in this ( sul ) -) context . E . g . , if gen:MAS appears in a certain ( sub- ) context the , , this ( sub-)context will l , e preselected for gen:l : EM too . Furthermore , it is possible to impose special conditions on the preselection  , e . g . 
that a ( sub- ) context can only be preselected if it contains a POS feature in tagt l and  ti1   ( cf . lit . l ; t :
Opos and Ipos).
Method 2: In order to preselect ( sub- ) contexts for an fv-pair , we generate a decision treer ' ( Quinlan , I983) where the feature of the fv-pair , e . g . ten , humel . e , serves to classify all existing ( sub-) contexts . E . g . , hum prodt , cesthree classes of contexts : those containing the fw pair Onum : SG  , those with Onum : PL and those without a Onum feature  . We assign to tile tree nodes other features than this upon which the cl~ussification is based  . The root node is labeled with the feature from which we expect most informational  ) out the probability of the currently considered feature  . The values of the rout node feature are assigned to the I  ) ranches starting at the root node . ~, h . ~continue the . branching until there remain no features will , an expected information gain and a frequency higher than defined S suggested lwl Iehnut Schmld  , \[ MS , Univ . Stuttgart , Ger-Ilk , lilly , lear reasolls of space we explain only how we etn ploy decision trees for our purposes  . For details about the automatic generation of such trees see Quinhm  ( 1983 )  . 
threshohls . To ever ) , leaf of the tree corresponds a ( sul > ) context which will be marked and thus prese-letted for further analysis  . 
Method 3: For each f v q ) air concerning POS we preselect every ( sub- ) context containing only I'OS features ht tagt l-2 ; t , ,d ti1 ( classical I'OS trigram ) , e . g . 
2 pos:PREP lpos:DETt brOpos:NOUN . For the other fv-p ; tirs we mark every ( sub-)conl ; ext containing any fv-pair of the same type in the previous tag  ti1 and ally POS features in tag li_ 1 alld Ii , e . g . lpos : DET
Igen : FL'MO pos:NOUN for@en:I:EM.
Witl , the methods 13 , we next eliminate froll lev~cry preselected ( sul > ) context all fv-pairs which in the above described sense do not intluenee the relative frequency of the currently considered fv-pair  ( eq .  8) . 
Method 4: l : ronl the set of trigrams extracted from a training corpus we generate separately for every fv-pair  , a binary d > ranched decision tree which shall tie -scribewtrious contextual probabilities of this fv-pair  . 
The tree is generated on a modi\[ied version of the II  ) 3 algorithm ( Quildan , 1983) and is similar to the one desr . rlbed by Schmid (1994) . 
We start with a binary classification of all trigrams based on the considered D-palr  . l '\] . g . , a classification for : len:l"EM will divide the set of trigrams in two subsets  , one where the trigrams contain Ogen : l " EM in the tag I i and one where they do not  . 
\[Igen:MAS\]--F~g e , ,:l:EM ycs ~- ~ y e s / / Figure 3: l)ecision tre . e for the fv-pair Ogen : l , ' EM ( Every number is a probability of Ogeu : l " lt M in the context described by the path from the root node to the node labeled with them unl>er  . ) The tree is built up recursiw ~ ly ( fig .  3) . At each step , i . e . with the construction of each node , we test which one of the other D-pairs delivers most inf ofmatioi ! concerning the abow > described chmsill cation  . 
The current node will be labeled with this fv -pair  . One of its two branches concerns the trigrams which con ~  2gen:FEM   2num:SG   2pos:DET   2typ:DEF   )  = 44/298 = 0 . 148 p ~ (   0gen:FEM  \[  0num:SG   0pos:ADJ lgen : FEM hmm:SGlpos:NOUN 2gen:FEM   2num:SG   2pos:DET   2typ:DEF   )  = 44/44 = 1 . 0 PFRo?(0gen:l " EM\]0 pos:A1) Jlgen:FE'M ; p0 = 170/174 = 0 . 977 ) p ~' ( 0 num : SG\[0gen:FEM0 pos : Al ) Jlgen:FEMl num : SGIpos:NOUN 2gen:FEM   2num:SG   2pos:l  ) ET2 typ:DEF )  = 44/44 = 1 . 0 PF llq: ( 0 num : SG\[0pos : Al ) Jlnurn : SG 2pos:l ) ET ; p ~ = 90/96 = 1 . 0 ) p  ~ (   0pos:ADJ \[lgen:FEMl num:SGlpos:NOUN 2gen:PEM   2num:SG   2pos:DET   2typ:DEF   )  = 44/298 --= 0 . 148 PFR ~: ( 0 pos:ADJ\[lgen:FEM liras:NOUN2 pos:DET ; p2 = 69/465 = 0 . 148 ) The position index at the beginning of every feature-v '  , due-pair indicates the tag to which it belongs ; e . g . Ogen : FEM belongs to t~tgli and 2num : SG toll-2 . 
Figure 2: Decomposition amt reconstruction of a contextual tag probability  ( state transition probability ) using probabilislic feature relations ( PFH , ) ta in the D-pair , the other branch concernstim trigrams which do not contain it  . The recursive x pan-sion of the tree stops if either the information gained by consulting further fv-pairs or the frequencies upon which the calculus is based are smaller than defined thresholds  . 
4 TAGGING ALGORITHM
Starting point for the implementation of a feature structure tagger was a  second-0rdcr-IIMM tagger ( trigrams ) based on a modified version of the Viterbi algorithm  ( Viterbi , 1967; Church ,  1988 ) which we had earlier implemented in C ( Kempe , 1994) . There we replaced the function which estimated the contextual probability of a tag  ( state transition probability ) hy dividing a trigram frequency by a bigram frequency  ( eq .  3 ) with a flmction which accomplished this calculus either using  PF1Ls in the above-described way ( eq . s 6, 7) or by consulting a decision tree ( fig .  3) . 
To estimate the contextual probability of a tag we have to know the contextual probabilities of its fv-pairs in order to multiply them  ( eq .  6) . 
Using PFRs generated by roof : hod1 or 2, when e . glooking for the probability p ~ (0 pos : ADJI . . . ) from Ilgure 2 , we may find in the list of PFRs , instead of a PFR , which would directly correspond ( but is not stored )  , the two PFRs ( 0 pos:ADJ\[lgen:FEM lpos:NOUN2 pos:Dl ; ;T ; 
Pl-----0 . 148 )   ( 0 pos:ADJ\[0num:SGlllln:S ( ~ lsyn:NOUN2 syn:l ) ET ; p ~ = 0 . 4 14> Both of them contain subsets of tile fv-pairs of the required complete context and could therefore both be applied  . In such c ; * sewelaced to know how to combine Pl and p2 in order to gel ; p(=p . ~ infig .  2) . 
As there exists no mathematical relation between these three probabilities  , we simply average Pt and P2 to get pl ) ecause this gives as good tagging results as a nmn ber of other more complicated approaches which we examined  . 
PFRs generated by method 3 do not create this problem . For every complete context only one PFIL is stored  . 
When we use the set of decision trees generated by method  4  , we obtain for every fv-palr in every possible context only one probability by going down on the relevant branches until a probability information is reached  . 
In opposition totile PFRs oftile other methods , the decisiou trees also contain negative informational  ) ont the contex L of an fv-l ) air , i . c . not only which fv-llairs have to be in the context but also which ones nmstb cabsent  . 
5 TAGGING RESULTS
In tile training arm tagging process we experimented with different values for parameters like : minimal admitted frequency for preselection  , admitted percentua \] difference c between probabilities considered to b c equal  , etc . ( cf . see .  3) . 
The feature structure tagger was trained on the French  10  , 000 words corpus already mentioned ill table 1 , with the fonr differen training methods ( see .  3) . 
When tagging a 6 , 000 words corpus 6 with an average ambiguity of 2 . 63 tags per word ( after the dictionary S No overlap betWeell train ing and test corpora  . 
164 lookup ) 88.89% ( table 2).
t'ag-training corpus tagset of words I guage 2 , 000 , 000 English 47--2~00 , 000 English 47'--t'l'10 , 000 French 386 57 tT 10 , 000 French 38657 lpT1 0 , 000 French 386 fsT 110 , 000 French 386 fsT 210 , 000 French 386 fsT 310 , 000 French 386 fsT 410 , 0 00 l ~ rench 386 we obtained in the best case an accuracy of
IIMM tagging order accuracy 194 . 93 % 2 96 . 16 % 1 56 . 39 % 2 83  . 23 % 57 2 83  . 81% 57 --- 88 . 53 % 57 - - \] 88  . 89 % 57 - - ~ ~ 57 --- 188  . 14 ( Z ) tT--4"traditional " tlMM-tagger , IpT--+" Tagger " considering ~ nly lexical prohahilitles  , \] sTl . . 4 ---* feature structure tagger trMned with method 1  . .,1,
HMM order I~blgrams , 2~trigrams
Table 2: Comparison of the tagging accuracy with different aggers  , corpora , tagsets and IIMM orders Comparatively , we used a " traditional " II/V lM-tagger ( cf . see .  4 ) on the same training and test corpora and got an accuracy of  83  . 23% 7, i . e . the error rate was about 50 % higher than with the feature structure tagger ( table 2 )  . 
When we used a tool which always selects the lexi -tally most probable tag without considering the context we obtained an accuracy of  83  . 81% , which is even better than with the " traditional "IIMM-tagger  . 
Provided with enough training data and working on a small tagset  , our " traditional " tagger got an accuracy of 96 . 16% ( Kempe , 1994) , which is usual intiffs case ( Cuttingeta1 . ,1992) . The English test cori ) us we used here had an average amt ) iguity of 2 . 6 1 tags per word which is amazingly similar to the aml  ) iguity o\["the French corpus . 
The feature structure tagger is clearly bel , l . er when the available training corpus is small and the tagset large but the tags are decomposal  ) leint of e w f v-pairs . 
6 FURTHEI~RESEARCH
We intend to search for other similar models while keeping in mind the basic idea described above : Split-ringupatag into D-pairs and deducing it  , s contextual probability from the contextual probabilities of its fv-pairs  . 
Furthermore , it may be preferable to split up the tags only when tim frequencies are too smalls  . 
7 For a similar experiment for Qerman (20 , 000 words training corpus , 689 tags , trigrams ) an accuracy of 72 . 5% has been reported ( Wothke et al , 1993, p .  21) . 
Ssuggest cd by ' riscoe , Rank Xerox Research Centre , 
Grenoble , France

Brown , P . F . et al (1989) . A Statistical Approach to Machine q ~ ranslation . Technical / . epo , ' t,I/ . C 14773 (~//-66226) 7/17/89 , IBM Research l ) ivision . 
Church , K . W .  (1988) . A Stochastic Parts Program and Noun Phra . se Parser for Unrestricted Text . In Proc . 2rid Conference on Applied Natural Language
Processing , ACL ., pp . I36-143.
Cutting , I) . et al (1992) . A Practical Part-of-Sl ) eech Tagger . In Pwc .   3rd Conference on Applied Natural Language Processing  , ACL . Trento , Italy . 
Gale , W . A . and Church , K . W .  (1989) . What's Wrong with Adding One ? . Statistical Research Reports , No . 
90, AT&T Bell Laboratories , Murray Ilill.
Garside , IL , Leecll , CI . and Sampson , ( I .  (1987) . The . 
Computational Analysis of English : A Corpusbased
Approach . London : I , ongman.
Kempe , A .  (1994) . A Prol ) a bilistic Tagger audan Analysis of Tagging Errors  . Research Report . IMS,
Univ . of Stuttgart.
Quinlan , J . R .  (1983) . Learning Efficient Classi\[ica-tion Procedures and Their Application to Chess End Qames  . In Michalski , R . , Carbonell , J . and Mitchell T . ( Eds . ) Machine Learning : An arlificial inlelli-gence approach  , pp .  463-482 . San Mateo , California:
Morganl ( aufmann.
Rablner , L . R . . (1990) . A ' Bltorial onllidden Markov Models and Selected Applications in Speech Recognition  . In Waibel , A and Lee , K . F . ( Eds . ) Readings in Speech Recognition . San Mateo , California : Mofg as Kanfinann . 
Schmid , 1\[ .  (19!14) . l ' robabilistic Part-of-Sp each Taggingll sing\] ) eeision Trees . Research I/ . eport . IMS,
Univ . of Stuttgart.
Viterbi , A . J .  (1967) . Error Bounds for Convolutional Codes and an Asymptotieal Optimal '  ) ecoding Algorithm . In Proceedings of lEEE , vo\] . 61, pp .  268-278 . 
Wothke , K . et al (1993) . Statistically Based Auto-marie Tagging of German Text Corpora with Parts-of-Speech-Some Experinm nl  , s . Researchll , eport , Doe . No . TR 75 . 93 . 02 . Ileidelberg Scientific Center,
IBM Germany.

