Unsupervised Discovery of Phonological Categories through 
Supervised Learning of Morphological Rules
Walter Daelemans *
CL & AI , Tilburg Uniw ' xsity
P . (). Box 90153, 5000 LE Tilburg
The Netherlands
walter , dae-Iemans@kub , nl

We describe a case study in tit (' , application of symbolic machinclearning techniques for the discow  ; ry of linguistic rules and categories . A supervised rule induction algorithm is used to learn to predict the  . correct dimilmtive suffix given the phonological representation f Dutch nouns  . The system produces rules which are comparable , to rules proposed by linguists , l , Slrthermore , in the process of learning this morphological task  , the phonemes used are grouped into phonologically relevant categories  . We discuss the relevance of our method for linguistics attd language technology  . 
1 Introduction
This paper shows how machine lem'ning techniques can be used to induce linguistically relevant rules and categories fl'om data  . Statistical , connectionist , and machine learning induction ( data-oriented approaches ) are current lynsed mainly in language , engineering at ) t ) lications in order to alleviate the . linguistic knowledge acquisition bottleneck ( the fact that lexical an ( t grammatical knowledge usually has to be reformulated t'i '  ( )iii scratch whenever a new application has to be built or an existing application ported to a new domain  )  , and to solve problems with robustness and coverage inherent in knowledge-based  ( the . ory-oriente . d , handcrafting ) approaches . Linguistic relevance . 
or inspectability of the induced knowledge is usually not an issue in this type of research  . \]n linguistics , on the other hand , it is usually agreed that while computer modeling is a useful  ( or essential ) tool for enforcing internal consistency , completeness , and empirical validity of the linguistic theory being modeled  , its role in formulating or evaluating linguistic theories is minimal  . 
In this paper , we argue that machine learning techniques can also assist in linguistic theory for -* Visiting fl '  . llow at NIAS ( Netherlands Instituee for Advanced Studies )  , Wassenaar , The . Netherlands . 
Peter Berck and Steven Gillis
Linguistics , University of Antwerp Unive . rs ite its ple in 1 , 2610 WiMjkl ~ elgiumst even , gillis@uia , u  a . ac . bepeter , berck@uia , ua . ac . bemation by providing a new tool for the evaluation of linguistic hypotheses  , for the extraction of rules front corpora , and for the discovery ( if usefllllinguistic categories . As a case . study , we apply Quinlan's C4 . 5 inductive machine learning me . thod ( Quinlan ,  1993 ) to a particular linguistic task ( diminutive fi ) rmation in Dutch ) and show that it ; can be use ( l ( i ) to test linguistic hypotheses about this process ,   ( ii ) to discover interesting morphological rules , and ( iii ) discover interesting phonological categories . Nothing hinges on our choic . e of (\] 4 . 5 as a rule induction mechanism . Wc chose it because it is an easily available and sophisticated instance of the class of rule induction algorithms  . 
A second focus of this paper is the interaction between supervised and unsulmrvised machine learning me  . thods in linguistic discovery , in supervised learning , the . learner is presented a set of examples ( the experience of the system )  . These examples consist of an in Imtouttmt association  ( in our case , e . g . , a representation of a llotln as input , and the corresponding dimilm tive sul\[ix as output  )  . Unsupervised learning methods do not 1 ) rovide the h' , m'ner within for l natioil at ) out the out f ) ut to be generated ; only the inputs ar ( ; I ) re-sented to the learner as experience , not the target outputs . 
Unsupervised learning is necessarily more limited t  , hm~supervised learning ; the only information it has to construct categories i the similarity between inputs  . Unsupervised learning has been successflfly applied e  . g . for the discovery of syntactic categories from corpora on the basis of distributional inforlnation about words  ( Finch and Chalet 1992 , tIughes 1994 , Schiitze 1995) . We will show that it , is possible and useful to make use of unsupervised learning relative to a particular task which is being learned in a supervised way  . 
In our exper in mnt , phonological categories are discovered in an unsupervised way  , as a side-effect of the supervised learning of a morphological problem  . We will also show that this raises interesl ; ing questions about , the . task-dependence of linguistic category systems . 
952 Supervised Rule Induction with

For the experiments , we used C4 . 5 ( Quinlan , 1993) . Although several decision tree and rule induction variants have been proposed  , we chose this program because it is widely available and reasonably well tested  . C4 , 5 is a TDIDT ( Top Down Induction of Decision Trees ) decision tree learning algorithm which constructs a decision tree on the basis of a set of examples  ( t i t ( ' . training set ) . This decision tree has tests ( feature names ) as nodes , and feature values as branches between nodes . The leaf nodes are labeled with a category name and constitute the output of the system  . A decision tree constructed on the basis of examples is used after training to assign a class to patterns  . 
To test whether the tree has actually learned the problem  , and has not just memorized the items it was trained on  , the 9eneralization accuracy is measured by testing the learned tree on a part of the dataset not used in training  . 
The algorithm for the construction of a C4 . 5 decision tree can be easily stated . Given are a training set T ( a collection of examples )  , and a finite number of classes C1 . . . C  ~ . 
1 . If T contains one or more cases all belonging to the same class Cj  , then the decision tree for 5/" is a leaf node with category Cj . 
2 . If T is empty , a category has to be found on the basis of other information  ( e . g . domain knowledge ) . The heuristic used here is that the most frequent class in the initial training set is used  . 
3 . If T contains different classes then ( at Choose a test ( feature ) with a finite number of outcomes ( values )  , and partition T into subsets of examples that have the same outcome for tim test chosen  . The decision tree . consists of a root node containing the test , and a branch for each outcome , each bt ' anch leading to a subset of the original set  . 
( b ) Apply the procedure recursively to subsets created this way  . 
In this algorithm , it is not specitied which test to choose to split a node into sut  ) trees at some point . Taking one at random will usually result in large decision trees with poor generalization per -formanee  , as uninformative tests may be chosen . 
Considering all possible trees consistent ; with the data is computationally intractable , so a reliable heuristic test selection method has to be found  . 
The method used in C4 . 5 is based on the concept of mutual information ( or information gain )  . 
Whenever a test has to be selected , the feature is chosen with the highest information gain  . This is the feature that reduces the information entropy of the training  ( sub ) set on average most , when its value would be known . For the computation of information gain , see Quinlan (1993) . 
Decision trees can be easily and automatically transformed into sets of if-then rules  ( production rules )  , which are in general easier to understand by domain experts  ( linguists in our case )  . In C4 . 5 this tree-to-ruh ; transformation ivolves additional statistical evaluation resulting sometimes in a rule set more understandable at t  (  . l accurate than the corresponding decision tree . 
The C4 . 5 algorithm also contains a value grouping method which  , on the basis of statistical information , collapses different values for a feature into the same category  . That way , more concise decision trees and rules can be produced  ( instead of sew ' ~ ral different branches or rule conditions for each w flue  , only one branch or condition has to be detined , making reference to a (; lass of values ) . 
The algorithm works as a heuristic search of the search space of all possible partitionings of the wd-ues of a particular t bature into sets  , with the for-Ination of homogeneous nodes ( nodes representing examples with predominantly the same category  ) as a heuristic guide . See Quinlan (1993) for more information . 
3 Diminutive Formation in Dutch
In the remainder of thist ) ape . r , we will describe a case study of using C4 . 5 to test linguistic hy-1 ) otheses attd to discover regularities and categories  . Tit ( , . case study concerns allomorphy in Dutch diminutive formation  , " one of the more vexed probleins of l ) ut chi , honology ( . . . ) \[ and \] one of the most spectacular phenomena of modern Dutch morphophonemics "  ( Trommelen 1983 )  . 
Diminutive for l nation is a productive morphological rule in Dutch  . Diminutives are formed by attaching a form of the German icsntfix-tjetot  ; hesingular base form of a noun . The suffix shows allomorphic variation ( Table 1) . 
Noun huts ( house ) man ( man ) raam ( window ) woning ( house ) baan ( job ) 
Form hu is jemannet jera aln pje wo nink jebaant je
Suffix-jc-gtj (;- pj('~-tie
Table 1: Allomorphic variation in Dutch diminu-tives . 
The fi'equency distribution of the different categories is given in Table  2  . We distinguish between database frequency ( frequency of a suffix in a list , of 3900 diminutive forms of nouns we took from the CELEX lexical database  1  ) and corpus ~ Developed by tile Center for Lexical Ilfforma-tion  , Nijmegen . l ) is tributed by tile Linguistic Data

96 frequency ( frequency of ~ sutfix in the text corpus on which the word list was based  )  . 
1) . , , al , as . ' X , LC ( , , p-s\]~t . i7-----~897--> ig . 7 ~, T3 b . ,~%-1
U/ar . a'x,/i ;, ? / 70 . 9% tj 10 42 . 7' x , 14 . o % 1i ? io J 77 20% 38%/1 , , _ : ? ' l'abh ~ 2: Lexicoil an(l (: O , l)US\[requ(!n (: y of alh /- morphs . 
l list or icnlly , dilh ; rcnta . nalyses of diminutive for-real ; iontav(~takena(lifferenl , view of tile rules thai ; goveruthe(:hoi(:(' , of 1; he diminutiv(~sullix , and ot ! the , linguistic con(:el ) l ; splaying a role in these rules ( see , e . g . T (; Winkel 11866, Kruizing a 1( . t15 , Cohen 11958 , and l'ef('~t'ellcesill#OItlillt' , lell 1983) . Int ; ho , lal ; 1; er , i l ; ix argued l ; hal ; ( limimll ; iveformation ix a local 1) recess , in which coll Cel ) l ; s such as word stress and morphological st , r l l ( ; l , llre(proposed illl ; he earlier analyses ) (1() not play ar()le . The rhyme of the last syllabic oftim noun is necessary and sutlicienl  ; t(/predictI ; m col'l'(~cl ; a/lomort ) h . The , nal ; uraJ (: a tegorics ( or feal , ures ) w l f i ( : harehyllo the sised in her rules in ( :lu ( h ' , obst , r ' u-ents ,   . so nor wnl , . % alld the ( : lass of bimoraic vowels ( consisting of long vowels , diphtongs and schwa ) . 
Diminutive formation is a . Slna\[llinguisl ; i (: ( lo-main for which different CO mlmting l , hcories have men pr ( /t ) os ( ' xl ~ & ll ( \[fol'whi ( : ll ( liff ( ' , r(~nt generaliza-l ; ions ( in terms of rules and linguistic categories ) have been proposed . What we will show tw , x  ~ ; is how machine learning techniquest llay t ) (~ l lSed I ; O(i ) test competing h y i ) o the so~s , ( ii ) discovc , rgene , r-alizations in the data whi ( ; hctIlI ; ll(IIt ) ecomt/are (1 to the gener Mizal ; ions formulated ) y linguists , aim ( iii ) discover phonologi ( : al categories in mlun supervised way by supervised learning of diminutive suttix prediction  . 
4: Experiments li ' orea (: hofl , he 3900 nouns we coll(!cted , th (! following information was kept . 
1 . The phoneme transcription describing the syllable structure  ( in terms of onset , nucleus , and coda ) of l ; he last three syllables of the word . Missing slots are imlicate xl with = . 
2 . D ) reach of l ; hesel ; hree last syllables the , pres-el ICe()I ' abse . llce of Sl ; l'O , ss . 
3 . The (: or reslionding dimitmtive allomorph , abbreviated to E(-etjc ) , T (- tie , ) ,   . /l-j(;),K(-Me ), and I'(-pie ) . This is the '' (: al , egory ' of the word to be learned by the learner . 
Some examples are given below ( l ; he word itself and its gloss are provided for convenience and were not used in the exllerimenl  , s ) . 
-bi=-z@=+mhntJbiezenmand(basket) .   .   .   .   .   .   .   . + bIxE big(pig ) .   .   .   . +bK =- banTbijbaan(sidejob) .   .   .   . + bK = ~ b@iT bij bel(bible ) 4 . 1 Experimental Method The , ext ) ( wim(ml ; alse t-ut ) use ( tinalle Xl ) Crin/ ( ml : s consisted of a ten-lb hl cross-wflid ; ttione Xl ) erim cnt ( Weiss & Kulikowski 1991) . In this setup , the database is partitioned l ; entime ~ s , each with ; tdiL\['orelll . 101 ~/ ( If lll ( ~ dal ; asel ; as the tesl prot , mid the remaining 9/1% as training parL . For each ?) fl tel , (' , n simulations in our exp('~riine lll ; s , I ; h(~l ; esl ; p;u't , was used to to , stgo , ueralization perforn uu , : e . The success rate of an algoril ; hm is obtained I ) y cah : u-lat ; ingIhcav(ua , r ( , , aCClll '/ lCy ( ll l l ltl l)(!l ' O \[: l;(~SI , t ) nt , -I , ern categories correctly predit:ted ) over the l : entest sets in the tenfold crossvalidation eXlmri-lilO  . n; . 
4.2 Learnal fility
The , exp(~rim(mts show thai ; the diminutive li~-marion 1 ) roblem is learn M flein a data- ( /ri ( ml ; ( ~( lway(i . e .  1 ) y extraction of regularities \[' rein ( ! x-amlflCs , without , any a priori knowledge ahout ~ the domain ") . The average accuracy on unseent x~st data of 98 . 4% should be compared to bast ; -linel ) cr for lnan (: emeasures baso , dontnolm bilit~y-based guessing . This baseline would t ) e an accu-ra (: y of a . l ) out4() ~ for this prol)h;m . This shows t ; hat the tn'()l ) h'm is a . lm(/stt ) (; rlh(:tlyh ' , a ruabl (! I ) y induction , It , shouhl 1) enoted that CI '; I , I , ; X contains a numl )( ~ r ()\ [ coding (' , trots , so that some ( ff lhe ~ wrong ' all ( mlOrl ) hs\] ) r ( ' , ( li(:ted by the ma(:lfine h ; arning system were actually (: ( II'I'(~( ; L , Wq did not correct for this . 
\] It the next ; three secl ; ions , we will describe L here sull ; sofl ; he(~xImrim ( ; nts ; tirst on the 1 ; ask of (: Olll-paring conlli(:tingl ; he(/reti(:alhypotheses , the nondiscover itt glinguistic gen (; raliza Lions , and flintily ( munsul ) (~rvis ( ~ ( ldis ( : overy of l/h ( /nologica . l cat (> gories . 
5 Linguistic Hypothesis Testing ( ) n the basis of the analysis of I ) utchdiminutive formation by Tronune Jen ( 1983 )  , discussed brietly in SecLion 3 , Lhe following hypotheses ( among others ) can be \[ brnmlated . 
1 . Only informatioil about the last , syllable is re , levant in predicting the , correct allomorph . 
2 . \[ nlormation about l ; he onset of the last sylla-bi ( , is irrelevant in predicting the , correct allomorph . 
3 . Stress is irrelevant in predicting l ; he correct allomorph . 
: ~ lCxcepl ; syllM destru(:tm-e , of the last syllable of a noun is necessary and sufficient opredict the correct allomorph of the diminutive suffix  . To test these hypotheses , we performed four experiments , training and testing the C4 . 5 machine learning algorithm with fore ' different corpora  . These corpora contained the following information . 
1 . All information ( stress , on set , nucleus , cod a ) about the three last syllables ( 3-SYLL corpus )  . 
2 . All information about the last syllable ( SONC corpus )  . 
3 . Information about the last syllable without stress  ( ONC corpus )  . 
4 . Information about the last syllable without stress and onset  ( NC corpus )  . 
5.1 Results
Table 3 lists the learnability results . The generalization error is given for each allomorph for the four different  ; training corpora . 
s '' mXal

I-kjy\[-p3e
Errors and Error percentale . s3SYLI , SONCONCNC 611 . 6 79 2 . 0 80 2 . 0 77 2 . 0 13 0 . 7 16 1 . 1 26 7 . 3 4 5 . 2 2 1 . 9 13 0 . 7 15 1 . 0 49 13 . 7 0 0 2 1 . 9 14 0 . 7 16 1 . 1 48 13 . 5 0 0 2 1 . 9 14 0 . 7 14 1 . 0 44 12 . 3 0 0 5 4 . 8 Table 3: Error of C4 . 5 on the different corpora . 
The overall best results are achieved with the most elaborate corpus  ( containing all information about ; the three last syllables ) , suggesting that , eontra Trommelen , important information is lost by restricting attention to only the last syllable  . 
As far as the different encodings of the last syllable are concerned  , however , the learnability experiment coroborates Trommelen 's claim that stress and onset are not necessary to predict the correct diminutive allomorph  . When we look at the error rates for individual allomorphs  , a more complex picture emerges . The error rate on-etje dramatically increases ( from 7% to 14% ) when restricting information to the last syllable  . The-k ~ eallomorph , on the other hand , is learned perfectly on the basis of the last syllable alone  . What has happened here is that the learning method has overgeneralized a rule predicting - kjeafter the velarn as al  , because the data do not contain enough information to correctly handle the notoriously difficult opposition between words like leerling  ( pupil , takes-etje ) and koning(king , takes-kje ) . Purther-more , the error rate on-pje is doubled when onset information is left out from the corpus  . 
We can conch ; de from these experiments that although the broad lines of the analysis by Trom-melen  ( 1983 ) are correct , the learnability results point at a number of problems with it  ( notably with-kjevers us-et je and with-pje )  . We will move now to the use of inductive learning algorithms as a generator of generalizations about the domain  , and compare these generalizations to the analysis of Trommelen  . 
6 Supervised Learning of
Linguistic Generalizations
When looking only at the rhyme of the last syllable  ( the NC corpus )  , the decision tree generated by C4 . 5 looks as follows:
Decision Tree : codain rk , nt , lt , rt , p , k , t , st , s , ts , rs , rp , f , x , ik , Nk , mp , xt , rst , ns , nst , rx , k t , f t , if , mr , Ip , ks , is , kst , ix : J codain n , = , l , j , r , m , N , rn , rm , w , lm : nucleus in I , A , ,O , E : co da in n , l , r , m : E codain = , j , rn : T codain rm , lm : Pcoda = N:1 nucleus = I : K
I nucleus in A , O , E : E nucleus in K , a , e , u , M , @ , y , o , i , L , ) , I , < :\[ co dain n , = , l , j , r , rn , w:T\[coda = m:P Notice that the phoneme representation used by CELEX  ( called DISC ) is shown here instead of the more standard IPA font  , and that the value grouping mechanism of C4 . 5 has created a mnnber of phonological categories by collapsing different phonemes into sets indicated by curly brackets  . 
This decision tree should be read as follows : first check the coda  ( of the last syllable )  . If it ends in an obstruent , the allomorphis-jc . If not , checktile nucleus . If it is bimoraic , and the coda is / m/ , decide-pj e , if the coda is not/m/ , decide-tj e . When the coda is not an obstruent , the nucleus is short and the codais/ng/ , we have to look at the nucleus again to decide between-kj e and-etje  ( this is where the overgeneralization to-kje for words in-ing occurs  )  . Finally , the coda ( nasa-liquid or not ) helps us distinguish between-et je and-pje for those cases where the nucleus is short  . It should be clear that this tree can easily be formulated as a set of rules without loss of accuracy  . 
An interesting problem is that the-et jeversus -kjeproblem for words ending in-ing couhthot be solved by referring only to the last syllable  ( C4 . 5 and any other statistically based induction algorithm over generalize to-kj c  )  . The following is the knowledge derived by C4 . 5 t ' r of ll the flfll corpus , with all information about the three last syllables  ( the 3SYLL corpus )  . We provide the rule version of the inferred knowledge this time  . 

Default class is-tj e1 . IF codalastis/lm/or/rm/
THEN-pj e2 . IF nucleus last is\[+bimoraic\]codalastis / m / 
THEN-pje 3. IF codalast is/N/
THENIF nucleus penultimate is empty ( monosyllabic word ) or schwa
THEN-etje
ELSE-kje4 . IF nucleus last is\[+short\]codalastis\[+ n as \] or \[+ liq\] 
THEN-et je 5. IF codalast is\[+obstruent\]
THEN-je
The default class is-tjc , which is the allomorph chosen when none of the other rules apply  . This explains why this ruleset looks simi)h' . r thant it (; decision tree earlier . 
The first thing which is interesting in this rule set  , is that only tlu'ee of the twelve presented features  ( codaan ( 1 nll clelts of (  ; lielast syllable , nll-cleus of the i ) emlltimate syllal ) le ) m'e used in the rules . Contrary to the hyi ) oth(;sis of Trommelen , apart from the rhyme of the last sylbfl)le , them > ( :\[ eus of the pemfltimate sylhd ) leistakento\] ) ere . levant ; ~ swell . 
The induced rules roughly correspond to the previous decision tree  , but ; in ad ( lition a solution is provided to the-et jeversus -kjeproblem for words ending in-  in9   ( rule 3 ) making use of information about the nucleus of the  . t)emfltiInate syllabi(; . Rule 3 states that words ending in/ng/get-et jeas ( liminutive alloin or l ) h when they are monosyllables ( nucleus of the penultimate syllable is empty ) or when they have aschwaast ) (multi - mate rainless , and - kj cothe , rwise . As froas we now , this generalization has not been prot ) osed in this form in the lmblished literature on diminutive formation  . 
We conclude from this part of the experiment that the Inae hine learning in ethod has suc  ( :ee ( led in extracting a sophistieate ( l set of linguistic rules from the examph ' . s in a purely data-oriented way , an ( l that these rules are formulated at a level that makes their use in the development of linguistic theories possible  . 
7 Discovery of Phonological
Categories
To structure the phoneme inventory of a language , linguists define features . '\[' he seek LIl be interpreted as sets of st ) ee ( : h sounds ( categories ) : e . g . the category ( or feature ) labial groups those speech sounds that involve the lips as an a  ( : tiveart\[c-ulator . Speech sounds behm g to different categories , i . e . , are defined by ditferent\['e ~ tures . l , ', . g . 
t is voiceless , a coronal , and a stop . Categoriest ) roposed in phonology are inspired by articulatory , acoustic ortmreeptual phonetic ditferences between speech sounds  . They are also proposed to allow an optimally concise or elegant formulation of rules for the description of phonological or mot'phological processes  . E . g . , the so-calleA major ( : lass features ( obstruents , nasals , liquids , glides , vowels ) efficiently explain syllable structure eomput ; ation , lint are of little use in the definition of rules describing assimilation  . For as s\[re\[In\[ion , placu of mti ( : ulation f ( ~atllr ( ~ sarct ) estilse ( l . This situation has led to the t ) roposa . l of many dillhren C phonoh ) gieal category systems . 
Whih ; constructing the decision tre . e ( seep rey\[-Oils section ) , several t ) honologically relevant cat-( ; gories are ' discovered ' by the value grouping mechanism in  C4  . 5 , including the nasals , the liquids , the obstruents , the short vowels , mtd the bimoraic vowels . This last category corresponds completely with the  ( then new ) category hypothesis e , d by Trommelen and containing the long vowels , t i t ( ; diphtongs at t ( l the s('h w a , in oth (; r words , the learning a . lgorithm has discovered this set of phonemes to 1 ) e a useful category in solving the ( i minut ; iveformation problem t)yt)rovi(lingmle?-I , e . nsional detinition of it(alisl ; of timinst ; ulees ()\[:
I ; heea . tegory).
This raises the question of the task-dependence of linguistic categories  . Similar experiments in Dutcht ) lural formation , for examt ) le , fail to produce th ( '  ( : a tcg ory of bimoraic vowels , and for some tasks , categori ( :s show ut ) which hi~vc no ontological status in linguistics . In other words , making category formation del ) endentoil the task to t ) e learned , unde . rmitms the . tratlition a linguistic ideas about absolute , task-indel ) endent ( and even 1; mguage-in(h ' , t ) ende itt ) categories . We present he I'e&lI (! ~ , v methodology with which this flltl ( lDo-mental issue in linguistics cant ) ( ; investigated : category system sext ; racted for difl'eren tasks in different languages can be studied to see which categories  ( if any ) truely have a universal status . 
This is subject brfll r therresem'ch . It wouhlal sol)e use . rid to stu ( ly the indu ( : ed categories when intensional descriptions ( feature represeutations ) are used as input instead of extensional descrit ) -lions ( phoitetnes )  . 
We also experimented with a siml ) h ; r alternative to the computationally complex heuristicategory\[orma  . tion algorithm used by (; 4 . 5 . This method is inspire ( 1 by machine learning work on wflue di f ference metrics  ( Stanfill & Waltz , 1986; Cost & Salzberg ,  :1993) . Starting fl'om the training set of the sut ) ervised learning exl ) erinlent ( the set ( ) f input ouq ) ut mappings used by the system to extract rules )  , we selc (: ta particular feature ( e . g . the coda of the last syllable ) , and comt ) uteatable as - the number of times the pattern in which it  , occurs was assigned to each different category ( in this case , each of the the five allomorphs ) . This produces a table with for each value a distribution over categories  . This table is then used in standard clustering approaches to derive categories of values  ( in this case consonmlts )  . The following is one of these clustering results . The example shows that this computationally simple approach also succeeds in discovering categories in an unsupervised way on tile basis of data for supervised learning  . 
......> l
I .   .   .   .   .   .   . > r-I .   .   .   .   .   .   .   .   .   .   .   . > nI---I .   .   .   .   . I .   .   .   .   . > t
III . . . . . I . . . . > k
I .......... II ....> s
I ..... I-->p
I .... II-->f
I ..... I ....> m
I .... I ....> N
I .... I-->x
I__1->j
I -> w
Several categories , relevant for diminutive formation , such as liquids , nasals , the velar nasal , semivowels , fi'icatives etc . , are reflected in this hierarchical clustering . 
8 Conclusion
We have shown by example that machine learning technique  , scan profitably be used in linguistics as a tool for the comparison of linguistic theories and hypotheses or for the discovery of new linguistic theories in the form of linguistic rules or categories  . 
The case study we presented concerns dimiml-live formation in Dutch  , for which we showed that ( i ) machine learning techniques can be used to corroborate and falsify some of the existing theories about the phenomenon  , and ( ii ) machine learning techniques can be used to ( re ) discover interesting linguistic rules ( e . g . the rule solving the-etj cversus-kjeproblem ) and categories ( e . g . the category of bimoraic vowels ) . 
The extracted system can of course also be used in language technology as a data-oriented system for solving particular linguistic tasks  ( in this case diminutive format ! on )  . In order to test the usability of the approach for this application  , we compared the performance of the extracted rule system totile performance of the handcrafted rule system proposed by Trommelen  . Table 4 shows for each allomorph the number of errors by the  C4  . 5 rules ( trained using corpus NC , i . e . only the rhyme of the last syllable ) as opposed to an implementation of the rules suggested by ~ l ? ommelen  . 
One problem with the latter is thai ; they often suggest more than one allomorph ( the rules are not mutually exclusive )  . In those cases where more than one rule applies , a choice was made at random . 
Suffix Trommelen C4 . 5 - tje 53   11 - jc 12   12 - eric 28   39 - ~ iie 38 o-pj e 21   4 
Total 15266
Table 4: Comparison of accuracy between hand-crafl ; ed and induced rules . 
The comparison shows that C4 . 5 did a good job of finding an elegant and accurate rule-based e-scription of the problem  . This ruleset is useful both in linguistics ( for evaluation , refinement , and discovery of theories ) and in language technology . 

Cohen , A . Het Nederlands ediminutie fsuilix ; een morfologische I ) roeve . DeNic~t we Taalgids , 51, 4045, 1958 . 
Cost , S . and Salzberg , S . ' A weight e , d nearest neight ) or algorithm to r learning with symbolic features . ' Mach , ine Learning , l(/,5778, 1993 . 
Finch , S . & N . Chater . ' Bootstrapping Syntactic Categories Using Statistical Methods '  , in : W . Daelemans & 1) . Powers ( eds . ), Backgrov , r ~, d and E . ,cperirnent . s in Machine Lear ' nb ~ , g of N at v , -ral Language , Tilburg University , ITK ,  1992 . 
Itughes , J . ' Automatically Acquiring a Classification of Words '  , Phi ) dissertation , University of Leeds , School of Computer Studies , 1994 Kruisinga , E . Devorm vail verk Mnwoorden . De
Nieuwc ~ lhalgids , 9, 9697, 1915.
Quinlan , J . It , . C  ~ . 5 Programs for machine learning 1993 . 
Schitze , H . , Ambiguity in Language Learning : Computational nd Cognitive Models  , PhD dissertation , Stanford University , Department of
Linguistics , 1995.
Stanfill , C . and Waltz , D . L . ' Toward Memory-based Reasoning ' . Communications of the ACM 29, 1986, 12131228 . 
\]? onunelen , M . T . G . Th csyllable in Dutch , with special reference to diminutive formation . Foris,
Dordrecht , 1983.
Weiss , S . and Kulikowski , C .  (1991) . Computer systems that learn . Morgan Kaufmann , San

Winkel , L.A . Te . Over deverkl cin wo or den . De
Taalgids 4: 81-116.

