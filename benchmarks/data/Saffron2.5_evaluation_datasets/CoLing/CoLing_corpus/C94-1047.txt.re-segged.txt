LOGIC COMPRESSION OF DICTION ARIES FOR
MULTILINGUAL SPELLING CHECKERS
Boubaker MEDI ) EIIII AMRO UNI
GETA , IMAG-campus ( UJF&CNRS)
BP 53 , I ; -38041 Grenoble Cedcx09 , FRANCE Boubaker . Meddeb-namrouni@imag . t7r &
WinSoftSA.
34, Bd.del'Esplana de1:-38000 Grenoble , FRANCF

To provide practical spelling checkers on micro -com-puters  , good compression algorithms , ' ~' cessenlial . Cut Teut techniques used to compress lexicons for indo-Fm ropean languages provide efficient spelling checker  . Applying the . ~ une methods to languages which have a different morphological system  ( Arabic , Turkish ,  . . . ) gives insufficient re-suits . To get bette results , we apply other " logical " compression mechanisms based on tile structure of the language itself  . Experiments with muir ) lingual dictionaries show a significant reduction rate attributable to our logic compression alone and even betteres nlls when using our method in conjunction with existing methods  . 
KEYWORDS : Spelling checkers , Multilinguism , Compression , Dictionary , Finite state machines . 

Since the first work in 1957 by Glantz\[611 , a great deal of timer ) zing and reseltrch as taken place on the subject of spelling verificatiou and correction  . Many commercial products ( word processors , desktop resentation ,  . . . ) in-elude efficient spelling checkers on mic , ' o-computers . The classical methods , used arc generally based on a morphological analyzer  . This is sufficient o provide a robust monolingual spelling checker  , but using morphological amdyzers can become unrealistic when w c want to develop an univers ~ d solution  . In fact , tile analyzers built for each language use various linguistic models and engines  , and it is impossible to convert a morphoh ) gical nalyzer from one formalism to another . Furthermore , using flmse classical mcthods would lead to combining into the host application as many of grammars and parsers as languages  , which would increase the codesize and I he mainten:mcc problem of rules and data  . The method presented in this paper is based on building a dictionary of all surface forms for each language  , which is sufficient for spelling checkers applications  . " llle dictionary built with the existing genera- ) or scan bce ~ ily updated manually bt , tmayl ) e huge , especially for some agglutinative language ( Arabic , Turkish ,  . . . ) . A compression process on the muir ) lingual dictionaries in eeess , ' u ' y to obtain a reduced size . The exist-ins compression methods generally used are physical and provide good results for indo -European languages  . 
Applying the sane techniques to other languages ( Arabic , Tnrkish ,  . . . ) shows their limits . For this reason we introduce a new kind of compression techniques that we called " logic compression "  . This new technique requires ap , ' imi-tire morphological knowledge during tile compression process and requires less storage space than prevkms methods  . It , also has the advantage of being an universal lnelhod applicable to all languages  , Seclion 1 contains an overview of existing methods for building spellcheckers and the limits of such system wh cll we take into account new constra int such as lnnlti-lingual  ) sin . Section 2 outline stile first two steps of our work : we adapt an existing method to Arabic  , then make a first extension hy introducing a new kind of compression called " logic compression "  . Section 3 introduces ill detail the logic compression with its application to other lan-gtmges  , ll , ld shows the improvcinents obtained when using logic compression ill conjunction with existing methods  . 
Section 4 outlines the architecture of our l nullilhlgual spelling checker system and some future projects  . 
1. OVERVIE WOFEXISTING
MET llOI ) S1 . 1 .   ( ~rammar-hased approach These methods were used in the beginning on early computers when storage space was expensive  . It consists in building a small lexicon contaiuing roots and affixes  , a grammar of rules that express tile morphographemic alternations  , and all engiue that use stile grammar and the lexi  . .
conto see if an input word belongs to the lauguage or not  . 
If the process of recognition fails , some operations ( substitution , insertion ,  . . . ) are performed on the misspelled word to provide a list of candidale words that helps the user 
Io selectile correct form.
Even though , it is a great accomplishment to design a powcrful cng in c  \[3\]   \[8\] and to cxprcss rules in a pseudo natural way \[9\] even for different languages \[1\]   \[2\]   \[11\]  , these systems present some limits : - Multilinguism : This methods does not support all languages  . To offer arot , It ) lingual solution for n languages you have to store n grammars and n lexicons  , and generally n different engines in lotile host application  . 
-Cosl of retrievak For some languages , the retriewd of words may belong . For instance , a vocalized Arabic spellchecker nlust accept non -vocalized or partially vocalized words which require more lime to be accepted than fully vocalized words  . 
-Cost of guessing alternalives for a misspelled word : To guess a correct word when a misspelled word is found  , we have to modify the misspelled word by all possible ope  , ' a-tions(substitution , insertion , suppression ,  . . . ) for 1 or 2 characters and then try to check them . This matter can take a lot of time before displaying the correct form slot end-users  . 
-Maintaining file grammars and data : The grammars and lexicon require conti  , nlous updating . You need to fiud a muir ) lingual computational linguist who knows the linguistic theory and tileft  ) rmalism to easily update data and rules\[811 . 
-Ergonomic fcalures : In some languages , end users want to have some options that let I hem choose how tile spellchecker will accept words  . In Arabic , for example , different regions have slightly different orthographical conventions  . 
292 1.2. Lexicai-fiased appro.'t ch:
Lexical-based approach appear after the first methods described above  , when storage space become less expensive . The first step is to build complete list of surface forms belonging to the language using morphological generators  , SI , LP ( Specialized l + anguagcs for Linguistic Progr , ' uns ) , etc . and then compresses the large word-dictionary . They are generally used for office a pl ) lications such as word processors , desktop resentation , etc . Their main advantage is that they cover a complete language since all the forms can be fouud in the initial lisl  . Also , they allow efficient rctric valand guessing of misspelled words  \[4\]  . I lowever , some limits exist in such systems : - Multilinguism : The compression process give a good ratio for languages with a wcak inflexion factor  ( English ,  . . . ) where the compression nteehanism give up to 150 KB of storage fi'om around 3 MB of a fifll list \[4\]  . The compression technologies arc still powcrf if l for languages with a medium iuf lexion factor  ( Russian ,  . . . ) . For example , a list of all surface Russian words of between 10 and 15 MB of size can be reduced to 700 KB \[41  . For hmguagcs with a high inflexion factor ( Arabic , I ' in nish , llungarian ,  . .+) , it won't be easy to find compression technologies that give practical results  \[4\]  . For instance , a full list of completed vocalized woMs in Arabic h:m  300 MB in size antithe current compression mefll ( x ls are it n praclical . 
-No morphological knowledge : These methods arc neutral with respect othe text language  , the efficiency of compression techniques + nay be improved by using specific properties of the language  \[41  . 
I 1. AFIRSTAPPROACII : ADAPTING
ANEXISTING METItOD FOR ARABIC
ILl . Using an existing method
As a first step , we take an eflicieut method used to compress dictionaries for F+uropean  ( l : nglish , l : rench ,  . . . ) spelling checkers 11411 and try t it apply it to Arabic . The first step of our work cousists in building a full list of surface fin+ms usiug a morphological generator  151 anticompleted by all irregular fonns and existing corpus  . The final large word dictionary which coversuou -vocalizcd Arabic has a size of  75 MB . The comprcs sioup rocess yickls 18M Biua or ) repressed fi ) rmat . I : or . ' midea of the compression process readers can refer to  \[10\]  . Table 1 gives some results of the compression process for a few Europeaulan-guagcs to see the efficiency of thc method auditsitm/le-quacy for the Arabic language  . 
wordforms l ) anish 448.000
German 40 3.000
Arabic 7 milllous , " E ~' tglish 88 . 0 00 size siz cun comprcs sed compressed 5689 KB 725 KB 5297 KB 866 KB 75 i ' ~ lB '18 MB 84l KB " 224 KB
Table 1
The result f l ) rArabic is impractical for small computers . 
We must titan find other techniques that produce a smaller dictionary or extend this method  ; to get an exploit'dale solution . 
11.2. Extension of the method:
The initial idea is applied to the morphological system of Aral  ) ic . While most of the fully inflect c/l forms words in Arabicm c built by adding to a stem prefixes and suffixes wcl  ) roposc replacing some words with only one form beginning by a special code that represents it family of prefixes and finishing by another special code which represents a family of suffixes  . For this purpesc , w c wrote a program in MPW-C that processes a full list of inflected/brms and  ( +sing an existing decomposition of affixes into subsets already established  , give the reduced lcxi couwhere many for , ns are replaced by only ot to representation ( PSistemSSj ) where PSi ( with r c spec to SS j ) is the set i ( with respecto j ) of prefixes ( with respecto suffixes )  . 
Note that the reduced lexicon reprcs cnts faithfully theiui-tial list without any silence  ( missing words ) or noise ( incorrect words )  . Only compressed words are replaced , and the rest remain in the reduced list . The figure 1 gives an example of words , an example it1" a decomposition sluld the obtained result . 
Decmn+osilion
Fulls ttrfa?c for lm~7-Reduced list
J we AXl~l~\]S~II!!s\[qeAX !) dw
Iw ~ Xl ~ , us\]xyAXl)~us
Fig . I:Ikaml@ , of the compression process The next crucial problem to resolve is lit find the best dc -composilion that provide the best retluced lcxicon  . Theme + hill must t ~ automatic , It must process the large word-dictionary , and rcgar ( ling an initial list of prcl\]xes and sill L fixes  , must give as oul put the best dccompositiou and the optimal reduced dictionary  . But , hclk ) rc studying the implementation of such an algorithm  , we began , titsee how much space we couklgain by this teChlfique starting from alt  ) an ual decomposition . 
~ ~ t l d R c l h ~ d ; _Starting front a different fifll lists for each category of words  ( transitive verbs , nouns ,  . . . ) , we choose different decompositions and processed the full list with the coml  ) rcssion tool . The best decomposition kept\[or each category was lh c decomposition which eliminated the maxiluum forms  . This method gave mauy candidate decompositious depending ou Ih cgrammalio'tl calcgory of ihc word  . To choose I hebest global one we took into account the fi'equency of dictionary et lIries  . This method was test it + differeut Arabic word lists and some results : Ire described here  , Re:tders cat + refer to 1101 or fill for luore itf formation . To see some dc coln positiou , consider the following sets : lil :\[ wa , f a \] , I ~ ll ; +2 = la , sa ,  / , 3 ~/ l ; , 3= ha , at ,  1  , 3i / . . . . . . . 
F1~tom , ttnuna , ta , Uma , /~3 ~.../
F2: ya ,; din , yimt , + ulna , /, 31, 31, 3~ .  /  . . . . . .
I ; 6= ha , haft , ya , ka , kern , k t ) um a , kent , l ) . om , houma , bona , haft , F7-~F6\ya , i + la S 4 . hi , 1:9~=( wa .   .   .   .   .   . 
l : . i ( with respecto Fj ) is a set of prefixes ( with respecto suffixes )  . We uotc the quantity I(i . Ej ( wilh respecto FI . Fi ) all strings built by a collcal cnation of each clcmcut of l~+i  ( with rcsl~ecl to F it with each clement of l ( j ( with rcspect
Iol : j).
l'~xaml ) leof3 class ( from 6 ) of I heprefix class : 29+3 ellvii /'-"'-'"-'-"% lkSl
CI'"o--,vv''o-o---(
Fig , 2: Initial automaton
Pl = El-P2 = E4.
P3 = E3+~-E3+El . E2.E3
Ex , ' unple of 4 class ( from 13 ) of I he suffix class : St = F1 . S2 = F2 .  $7 =177  . S8 = F9 . F7 . 
? pirs ~ r ? , ~ , ! 1~8: case of Arabic : With all the classes already found for Arabic  ( 6 classes of prefixes , 13 cl , ' tsscs of suffixes ; each class containing an average of 8 affixes ) , we processed a collection of non-vocalized Arabic dictionaries  ( 17MB )  , the rest llt gave a reduction lexicon of 254KB . 
Used this in combination with the compression process described in ?  1  . 2, tile final result is 121KB . Note also that part of this work was implemented in a commercial multilingual word processor  ( W in Text ? ) to offer Arabic spellchecking . 
II1LOGICCOMPRESSION :
III . 1. Theoretical aspects :
Let V be a finite set and V * the set of words built on 
V including null strings noted ~.
WEV *. W = WiW > .. Wn . Wie V .
ic\[1..n \]. Let V+=V*-~l.
Let Y be a subset of V that contain vowels.
1. Prefix ( W ). V WcV+.
We call order i prefix the quantity :
Pi = WlW2 . . . (1_<i_<n-I).
2. Suffix ( W ). V WeV+.
We c,'fll order j suffix Ib equantity:
Sj = WjWj + t . . . W . . (1_<j_<n).
3. Voe Pat ( W)g WeV+.
We call vocalic pattern of W the set :
Vy = Wi , Wj , ... Wk , Wi < Y.
card(Vy )__. leugfll(W ) 4. Root(W ). V WeV+.
We call root the quantity:
R = Wp . . . Wq . (1_<p < q_<n ), card(R)_<q-p+l . 
5 . Pi : Prefixes class . P i = ~, F ' i l , P i > . . . l:Ji ', : ?
Pij is a prefix . 1_<j_<k
Card(P i ) = k + 1. if k >__1.
= 1 . if l'i = 0, 6 . Sj : Suffixes class . Sj = ~, Sjl , Sj2, . . . Si~: . 
Sji is a suffix . 1_<i_<k
Card(Sj ) = k + l.ifk_>1.
= 1. if Sj = tZi .
7. Vl : Vowel class.
Vk = ? J,VYkI , VYk2, . . . VYtk
V y i i is a vocalic pattern . 1<_i <_ k
Card(Vv .) = k + 1. if k_>.1.
= 1. if Vk = ~.
Ill . 2 . Imgic Conlpression : Wllatisit ? Let's take the following automal a that represent some surface w  ) calized words ( fig 2 ) 
Pij is a prefix . 1_<_j <_n .
Sji is a suffix . 1_<i_<n.
Ci are tile consonal ltS of the vocabtilary.
1_<i_<k .
VijiS the vowel attached to the conson aut Cj.
l ~< i_<q and l_<j_<_k.
? J is the null string.
This automata recognizes all words beginning from an initial state  ( marked by * ) and finishing in a final state ( marked by a double circle ) The utunher of arcs of such an aulofual a is: 11 II ~_ . ~ length ( l'ik ) ++ ZI ength ( Sjk ) 2q ( k-1 ) k = lk = lIf we consider , for example , that affixes have a single chm '- acter , then mn be r of a , cs is equal to 2(n+1)+2q(k-1) . 
The logic compression consist in supplying the class of prefixes  , suffixes and vowels and replaces each set by only one arc that represent a family of prefixes  , suffixes or vowels . 
Starting from the following sets already es lablished : Pi = ~  , Pil , Pi2 , - . .l~i ,~\] a class of prefixes slored as x . 
Sj = ~' J,Sjl,Sj2, . . . Sjnachiss of suffixes stored ; is y . 
Vk = Vll, . . . Vlk,V21, . . . V2~: . . . .  Vql, . . . Vql ?) a class Of v(K'alic paller nslorc dasz . 
The logic compression reduces the initial automal OU to this new one: 
Fig , .3:P.cduced automata
The number of arcs kept in the automata is equal to  3 + k . 
The SOlVt : contains a sub-sclofk vowels which must be applied to the last k characlers  . 
Ill . 3. Experiments :
The logic compression with only an affix decomposition  , built by the manual meflmd cx plaiued above , has been tested on various list of words that represent collec-lions of multilingual dicl ionaries  ( a list of inflected forms )  . Three languages are tesm d:non-vocalized Arabic which has a great inllexion lactor  , French which has a 2 . 94
Arabic French Russian
Size of uncompressed list ( MB )
Ratio from it complete dictionary
Number of inflected forms
Class decomlx 3sition(Ih'efixes )? . ( suffixes ) 1 . 980 . 280  . .~1- l'hysical compression -- , 56602 - Morphg-physica . 1 comp .  +=  , l 22 l 3 -FSM compression 88   4  +  l~8ic compression 253  . 686 4 + 1 145 . 086 2 . 636 1 80 16 247 . 406 892 . 646 311 . 593 201 . 216 480 . 770 207 . 376 4+2 121 . 500 1114 . 665 ~ , , 44-3 57  . 214 150 . 321 75 . 234 348 . 636 109,418 48 . 78 163 . 202 56 . 784 37 . 74 36 . 717 lble2 weak inf lexion factor , Russian which has a medium inflex-toll factor , l ; . xtm rimenls arc do lie in two ways . First by using our logic compression alone anti , the l , in conjitction with other methods by supplying the reduced lexicon  ( lisl of compressed words in text format ) obtained with our method as input to existing methods  . The three other methods tested a , e I he following : o Physical compression : Using a commercial physical process  ( Stuff it )  . 
-Morpho-physical coin prcssion : This method was used to compress dictionaries used to bui Ma spellchecker  1411  . 
It combines morphological proprieties by taking inloaccount the suffixes of the language  , but wilh out any link between I hem . It also contain soniephysical features 171 . 
? FSM ( FiniteState Machine ) Compression : Using file Lexc ( FiniteState Lexicon Compiler ) which allows the conversion of a list of surface forms in loat ransducer which is then minimized  \[81  . 
Rest tlls are described in table 2.
11 1.4. Interpretations :
Then lost interesting observed on this table is the improvement obtained when we combine our method with a previotls one  . These resulls show that the existing methods are not optimal and can be improved by our logical compression in its first step  . These important results in storage spaces houhl not hide others aspects of Slmll checker systems  ( retrieval and guessing )  . It would be interesting if the results given in the table were followed by oilier results showing impmven mnts in the etrieval and guessing of words  . 
IV . APROPESEI ) ARCIIITECTURI , ; OFAUNIVERSALSI'ELLING CHECKI , ' R : Figure 3 shows the architecture of our proposed universal spelling checker  . Our method is inspired from previous methods (?1 . 2) , but present some new original aspects that allow it to be considered a truly multilingual solution  . In summary , our system has the following l'ea+t tlles : ? Multilinguism : lhism clhod will insure the multilingual constraint By using different tools  , specific to each langtage , to create a list of all surface lk ) , 'ms . 
? Storage space : by introducing the logic compression into the compression process  , we will be able to get a reduced lexicon for whalever langu'lg c we have to use  . One task that still remains is to improve the logic comp  , ' ession by making the lask of finding the best decomposilion more automatic  . This problem is coiibilatorial ; well lllSl discover how to apply the optimization algorithms  ( genetic algorithll , stochastic algorithm ,  . . . ) in each case Io find an optimal reduced lexicon starling from I he large word-dictiol mry and primilive morphological km  ) wledge ( list of affixes and w wets )  . 
? Retrieval/guessing : even lllollgh we have ll ' l any conc'ele  , -esults now , the firsl exper in lenl show I hat the process of checking words in an I  ; SM formalisl n is faster\[halto the rexisling methods  , l ' url her more , we are exploring paths Io introduce functions ( similarily key ,  . . . ) into the final obtained lexicon to make a rapkl guessing of replacements for misslx flled words  . 
CONCI , USION ( ) ill " approach 1o spellchecking differs from previous in ethods by faking in tollccolm\[aliew paraneler which is ? in --#  .   . < ,  .   .   .   .   .   .   .   .   .   .   .   . . . . . . . . - -  .   .   .   .   .   .   .   .   . 
\] xe~:~st , re~e\[_~Machine(Psm)\[-~('-l ! . y ~! ca!1ComiSressit;n '', ~ . _4 ~ Reduced lexicon . " lor , n , l , S, . " J "
Fig . 3: Universal spelling checker 29 . 5 file multilinguism . The system proposed tries to give solutions for the three main problems : Multilinguism  , de-teclion/guessiug and storage size . 
The first results , although using a manual method to find the decomposition i this first step  , show that the previous methods to store dictionaries  , are not opthnal and can be improved by exploring other techniques from the language itself  . Another interesting experiment is to find m ~ original opfimiz ~ a tion algorithm to find the optimal reduced lexicon that represents faithfully the initi'd list without any silence  ( missing words ) or noise ( incorrect words )  . Yet another project is to build a more robust method for the two other problems  ( detection and guess-iug ) from the reduced lexicon . 
ACKNOWLEDGMENTS qlie author would like to thank Prof  . Christian BOI'I'I-~'I " for his constant support , ' uld encouragement . I am also very grateful to Mr . Kenneth BEES LEY ( Rank Xerox , Grenoble ) for his fruitful discussions and Mr . Lauri KARTTUNEN ( Rank Xerox , Grenoble ) for his help to realize some experiments . 
REFERENCES\[1\]BeesleyK . R . , Bukwalter T . , (:1989) Two-level , FiniteState Analysis of Arabic Morphology . 
Proceedings of the Seminar on Bilingual Computing in Arabic and English  , 67 Sept .  1989 . Camhridge , England : The Literary and Linguistic Computing Center & The 
Center for Middle Eastern Studies.
\[2\] Beesley K . R . ,  ( 1990 ) Finite-state description of Arabic Morphology , iuthe Pr ( vceediug of the Second Cambridge Conference on Bilingual Computing in Arabic and English  , Cambridge , England , 67 September 1989 . No pagination . 
\[3\] Benllamadou A . ,  ( 1986 ) A Compression technique for Arabic Dictionaries : The affix Analysis  , in the Proceeding of COLING-86 , Boml 1986 , pp .  286-289 . 
\[41 Circle Noetic Services (1989) Passwd , Reference Manual , MIT Branch Office , Boston , pp .  16 . 
\[5\] Circle Noetic Services (1989) Conjugate tool , Reference Manual , MIT Branch Office , Boston , pp . 

\[6\]Glantz11 . ,  ( 1957 ) On the recognition of in-for nultion with a digital computer  , J . ACM , Vol . 4, No . 

\[7\] lluffman D . A . ,  ( 1951 ) A method for the construction of minimum redundancy odes  , Proc . IRE40(1951), 1098-1101 . 
\[8\] Karttunen L .  (1993) , FiniteState Lexicon Compiler , Xerox P , ' do Alto Research Center , April 1993 ,  135 . 
\[9\] Koskeniemmi K ., (1983) Two level
Morphology , Publicationo .  11 , Department of Geucral Linguistics , University of llel sinki , pp .  18 . 
\[101 Meddebll . B . ,  ( 1993 ) lnt dgration d'une com-posante morphologique pour la compression d ' undictionnaire arabe  , in Proc . Langue Arabec . t Technologies Infonnatiques Avancfes , C , -t ~ a blanca , pp . 14 . 
\[11\] Meddeh II . R . ,  ( 1994 ) Logic Compression of Multilingual dictionaries , in Proe . of ICE MCO-94 , International Confcreuce and Fxhibition on Multilingual Computing  , University of C~unbridge , Center of Middle Eastern Studies , London , April-1994 , pp .  14 . 
\[12\] Oflazer K , Solak A ,   ( 1992 ) Parsing agglutinative word structures and its application to spelling checking for Turkish  , Proc . of COI~ING-92, Nantes , Aug . 
2328, Vol . 1, pp . 3945.

