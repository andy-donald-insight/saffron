Exploiting a Probabilistic Hierarchical Mode l for Generation 
Srinivas Bangalore and Owen Rambow
AT&T Labs Research
180 Park Avenue
Florham Park , NJ 07932
srin ?, rambow@research , art.com
Abstract
Previous stochastic approaches to generation do not include a tree-based representation of syntax  . While this may be adequate or even advantageous for some applications  , other applications profit from using as much syntactic knowledge as is available  , leaving to a stochastic model only those issues that are not determined by the grammar  . We present initial re-suits showing that a tree -based model derived from a tree-annotated corpus improves on a tree model derived from an unannotated corpus  , and that a tree-based stochastic model with a handcrafted grammar outpert brms both  . 
1 Introduction
For many apt ) lications in natural anguage gen~eration ( NLG )  , the range of linguistic expressions that must be generated is quite restricted  , and a grammar t br generation can be fltlly specified by hand  . Moreover , in maW cases it ; is very important not to deviate from certain linguistic standards in generation  , in which case handcrafted grammars give excellent control  . However , in other applications t br NLG the variety of the output is much bigger  , and the demands on the quality of the output somewhat less stringent  . A typical example is NLG in the context of ( interlingua-or transthr-based ) machine translation . Another reason for reb ~ xing the quality of the output may be that not enough time is available to develop a flfll grammar t branew target language in NLG  . In all these cases , stochastic ( " empiricist " ) methods provide an alternative to handcrafted ( " rational-ist " ) approaches to NLG . To our knowledge , the first to use stochastic techniques in NLG were Langkilde and Knight  ( 1998a ) and ( 1998b )  . In this paper , we present FERGUS ( Flexible Em-piricist/Rationalist Generation Using Syntax  )  . 
FErt GUS follows Langkilde and Knight's seminal work in using an ngram language model  , but ; we augment it with a tree-based stochastic model and a traditional tree-based syntactic grammar  . 
More recent work on aspects of stochastic generation include  ( Langkilde and Knight ,  2000) , ( Malouf , 1999) and ( Ratnaparkhi ,  2000) . 
Betbre we describe in more detail how we use stochastic models in NLG  , we recall the basic tasks in NLG ( Rainbow and Korelsky , 1992; Reiter ,  1994) . During text planning , content and structure of the target text ; are determined to achieve the overall communicative goal  . During sentence planning , linguistic means-in particular , lexical and syntactic means are determined to convey smaller pieces of meaning  . 
l ) uring realization , the specification chosen in sentence planning is transtbrmed into a surface string  , by line ~ rizing and intlecting words in the sentence  ( and typically , adding function words ) . 
As in the work by Langkilde and Knight , our work ignores the text planning stage , but it ; does address the sentence , planning and the realization stages . 
The structure of the paper is as t bllows . In Section 2 , we present he underly in grammatical tbrmalism , lexicalized tree-adjoining grammar ( LTAG ) . In Section 3 , we describe the architecture of the system , and some of the modules . In Section 4 we discuss three experiments . 
In Section 5 we coln pare our work to that of Langkilde and Knight  ( 1998a )  . We conclude with a summary of ongoing work . 
2 Modeling Syntax
In order to model syntax , we use an existing wide-coverage grammar of English  , the XTAG grammar developed at the University of Peru > sylvania  ( XTAG-Gronp ,  1999) . XTAG is a tree ~ adjoining grammar ( TAG )   ( Joshi , 1987a ) . In Trees used in derivation "- ~ , - " " 7 / / " '  , PA , // iP -- , 7
NAux1)N/Nl'NI'~/l)AN
IIIII" . -JII I there was n cost estimate i ' of the second phase  1   '3   71   Y2    z2   74   71   75   ( z 1 Other supertags for the loxem cs found in the training corpus: 
IIOI 1CZ 4Z1 ~ I ( z 1    z4 z 5 z 2   7   2 z 5   5 more 11 more 4 more I ( more
Il OIlez 3 z 2 z 1   3'   2   5 IIIO rC 2 Ill We Figure 1: An excerl t from the Xq'AG gr~um\]lm"t ( derive Th , " r(' . wa . su , oto . st:stim , , tc . f i ) r the . second phase . ; dotted lines show t ) oss it leal jun ( ' tims that were not made a TAG , the elementary structures are \] ) hrase-structure trees which are comt ) osed using two oter ~ tions , sutstitui , ion(w \] fich ali ; nts one tree ~1 ; the fl : ontier of another ) mtda ( tjnlmtio \] t ( which in s ; rts one tree into them illl ' , of im-o ; her ) . Ingral ) hi:alrei ) reselltal ; ill , \] lotes & I ; which substitul ; ion can take 1 ) lac'~are\]uarked withdow\]>arrows . In linguis I ; icuses ( fTAG , we asso'ial ; e one lexical item ( its anchor ) with each tree , and heor ( typically ) more trees with each lexical ire\]n ; its a result we obtain a lexicalized TAG or LTAG . Since ea'h lexi:al item is associated with a whole tree  ( rather than just a phrase-stru'tureule , tbrexa\]nl)le ) , we cm\]st ) e ( : i\[y to th the t ) relicate-argument structure of the lexeme ( tyincludillg nodes at which its arguments must sutstitute  ) and morl ) h-syntactic on straint such as sutje ( ' t-verb agree-men within the sl ; rucl ; ure associated with the lexeme . This property is retbrred to as TAG's cztcndcd domain of locality  . N ) l ; e that in an LTAG , I ; here is no distinction betw:en lexicon nnd grammar  . A Smnl ) le grammar is shown in
Figure 1.
-We depart fl : omXTAG in our treatment of ; reest bradjuncts ( such as adverls ) , ant instead t bllow McDonMd and Pusteiovsky (1985) . 
While in XTAG the elementary tree for a nad- . iuncl ; conl ; ains 1) hrases l ; ru:i ; ure ; hat atta:hesl , headjmmt toll ( tesin another tree with the stag anchored by 71  \] ) et72N7:~Aux7 . tPro , l0T1', ~ A(ljadjoins to direction


S , VP
NP , VP

N right right right h : f tright right
Figure 2: Adjmmtion tablet brgraamnar frag-lUelltsl ) ecitie ( 1 label ( say , VP ) from the specified direction ( say , fronltheleft ) , in our systenl the trees for adjunct simply express their active valency  , trot 11 o1\[ ; how they connect to the lexical item they modi\ [ y  . This ilf l ' ormal ; ion is kept in the adjunct on table which is associated with the  . 
grammar ; an excerpt is shown in Figure 2 . Treest ; hat can adjo into other trees ( and have entries in the adjunct on table )   ; ~ recalled gamma-trees , the other trees ( which can only t ) esubstituted into other trees ) are alpha-trees . 
Note that we can refer to a tree by a combination of its name  , called its supertag , and its anchor . N ) r example ,   ( q is the supertag of an all ) ha-tree anchored 1 ) y a noun that projects up to NP , wM le 72 is ; liesuperi ; agofit gamma tree anchored by a noun that only t ) rojects 1 ; ) N ( we the adjunction table shows , can right-adjo into an N . So that estimate ~ is a particular tree in our LTAG grammar  . Another tree that a supertag can be associated with is ~ t ~  , which represents the predicative use of a noun . 1 Not all nouns are associated with all nominal supertags : the expletive there is only an cq  . 
When we derive a sentence using an LTAG , we combine elementary trees fl ' om the grmn mar using adjunction and substitution  . For extort-pie , to derive the sentence There was no cost estimate for the second phase from the grammar in Figure  1  , we substitute the tree t br there into the tree t brestimate  . We then adjoin in the trees t br the auxiliary was  , the determiner no , and the modit ) ing noun cost . Note that these adjunctions occur at different nodes : at VP  , NP ~ and N , respectively . We then adjoin in the preposition , into which we substitute ph , as e , into which we adjo in the and second . Note that all adjunctions are by gamma trees , and all substitution by alpha trees . 
If we want to represent this derivation graphically  , we can do so in a derivation tree , which we obtain as follows : whelm verwead join or substitute a tree t ~ into a tree  t2  , we add a new daughter labeled t ~ to the node labeled tg  . As explained above , the name of each tree used is the lexeme along with the supertag  .   ( We omit the address at which substitution or adjunction takes place  . ) The derivation tree t br our derivation is shown in Figure  3  . As can be seen , this structure is a dependency tree and resembles a representation of lexical argument structure  . 
aoshi ( 1987b ) claims that TAG's properties make it particularly suited as a syntactic representation tbr generation  . Specifically , its extended domain of locality is useflfl in generation tbrlocalizing syntactic properties  ( including word order as well as agreement and other morphological processes  )  , and lexicalization is useful tbr providing an interfime from semantics  ( the deriw ~ tion tree represent the sentence's predicate-argument structure  )  . Indeed , LTAG has been used extensively in generation , starting with ( McDonald and Pustejovsky ,  1985) . 
1Sentences such as Peter is a doctor can be analyzed with with be as the head  , as is more usual , or with doctor as the head , as is done in XTAG1 ) eeause the be really behaves like an auxiliary , not like a flfll verb . 
estimate there was no cost for phase c ~ the second  71   75 Figure 3: Derivation treet brLTAG deriw ~ tion of There was no cost estimate for the second phase  3 System Overview FERGUS is composed of three modules : the  2?ee Chooser , the Unraveler , and the Linear Precedence ( LP ) Chooser . The input to the system is a dependency tree as shown in Figm'e  4  . Note that the nodes are labeled only with lexemes  , not with supertags .   2 The Tree Chooser then uses a stochastic tree model to choose TAG trees fbr the nodes in the input structure  . This step can be seen as analogous to " supertagging "  ( Bangalore and Joshi ,  1999) , except that now supertags ( i . e . , names of trees ) must be fbundt br words in a tree rather than tbr words in a linear sequence  . The Unraveler then uses the XTAG grammar to produce a lattice of all possible linearizations that arc compatible with the supertagged tree and the XTAG  . The LPC hooser then chooses the most likely traversal of this lattice  , given a language model . We discuss the three components in more detail . 
The Tree Chooser draws on a tree model , which is a representation of XTAG derivation tbr 1  , 000 , 000 words of the Wall Street Journal . a The ~ I Yee Chooser makes the simplifying as-2In the system that we used in the experiments described in Section  4  , all words ( including flmction words ) need to be present in tt , einlmt representation , flflly inflected . This is of course unrealistic for applications . In this paper , we only aim to show that the use of a %' ee Model improves performance of a stochastic generator  . 
See Section 6 for further discussion.
3This was constructed from the Penn ~ lS"ee Bank using some heuristics  , since the Pemt~IYeeBank does not contain hill head-dependent infornlation  ; as a result of the use of heuristics , the Tree Model is not flflly correct . 
44 estimate there was no cost for phase the second
Figure 4: Inlmt to FEII . GUS
Smnl ) tions that the (: hoice of n tree . tbr~tnode dei ) ends only on its daughter nodes , thus allowing \]' or a to t ) - ( lown dynamicl ) rogrmnlning algo-ril ; hln . St ) ccifically , a node ' q in the intml ; si ; ru(:-ture is assigned ~ tsui)e , rt ; ~ gssoth ; tt the 1) rol ): t-)ilil ; y off in ( ling the treelet ( ; ()m \]) ose(t of ~1 with superta X , ~  ; rod : dl of its ( l ; mght( ; rs(as foun(t in I ; heini ) uts l ; rucl ; ure ) is m ; rximiz( ; d , and such l ; ha , t . ' ~ is (: Oml ) a , tit)le with ' q'~sm other ~ tll(lhersut)e , r tag . sin . Here , "(' omt ) atible " l:nem is ; hat ; the tree ret ) resclf l ; ed by . ' ~ can 1) eadjoined or substii ; uted into the tree ret ) resented by , %~ , : m-(' or ( ling to the XTAG gra , nmmr . For our exmn-t)lesenl ; en(:(; , the , in i ) ui ; 1; othesy sl , e , misthet ; ree shown in Figured , and theoul ; 1) ul ; fi'oml ; he ~ . l~ee ( ~ hooser is the , tree . ; ts shown in \], ' igure .  3 . No (; c that while a(le , riw ~ tion tree in TAGfullySl ) (: ( :- iiies a derivation and thus : tsmTth , (: e , s(mte . n(:e , the oul ; lmtfl:om the ~ l~-ee Chooser ( loes not ; . 
There are two reasons . \] ? irstly , asexi ) laine . dat ; the end of Section 2 , fin : us trees ( : or responding to adjuncts are underspe . (-itied with rest ) ect to the adjunction site a at ( t/or I ; h ( ; a(ljmwl ; ion direction ( from left ; or fl ' Oln right ) in the tree of the mother node , or theyn my 1) em~or de . re(l with respc (: to other ad . iun('ts(tbrex~nni)l(; , the fmnous adjective ordering t ) roblem ) . Secondly , Sul)ert ; agsnl ~ yh ~ vebeen (: hose . n incorre(:l ; lyornotat ; ill . 
The Unr ; ~veler takes ; ~s input the senti-specitied derivation tree , ( Figure 3) ml(l1) ro-duces a word lattice . Each node , in the deriw > tion tree consisl ; sof~t lexi (: alitemm ~ da supertag . The linear order () f the d mlg hte . rs with rest ) cottol ; hehe ; td1) osil ; ion of ; tsut)ertngisst)ecilied in the Xrl'AG grmnmar . This information is ( : on sulted to order the ( laughter nodes

TAGI ) eliwtlion Tree wilh t , ut , SIIpel tags " l'l CC(?h ? ~) scl/J -=- Tree\\h

One siaglese all is pecified \\' I'A Gl ) cdvalion \] lees\
I\[(h'alll , llill'\]
W ( nd l . altice

Figure 5: Ar(:hii ; e ( : ture of FERGUS with rcsl ) e ( : t to the head at each le . vel of the ( terival ; ion tree . in cases where ~ daughter node C&ll\] ) ( I ntta ( ' hed at more thin 1 ( ) lie t ) lace in the head SUl ) er tag ( as is the ( : ; ~sein our exmnt)le for " was and for ) , n disjunction of M1 these , positions are . assigned to the dmlghter node . A botton > up algorithm the . n constructs ~ lattice that ell -( ; odes the strings rei ) re . sented 1) y( ; ~(:1 ~ level of th (! derivation tr(x ' . . The latti('e ~ at the . root of the ( teriwttion tr(w . is the result o 171;\]mUm '; tveler . 
' Fhe resulting l ~ t t t i (:( ; for the (' . Xaml)h ' . s(ml ; e . nce is shown in Figure 6 . 
The\]~t ; ti (' . e . OUtlmt from the . Unra . veh'a " encodes all t ) ossible word sequences l ) erniitted 1 ) y the derivations trueial re . We rm lk these . 
word sequen ( : es in the order of their likeli-hoo ( l1 ) y composing the lattice with a finite-state machine rel  ) rese . nting ~ trigrmnbmgu ~ Ge1no(tel . This mo ( M has 1) e e . n('onstructed froln1 , 000 , 0000 words of W~dlStre , et Journal (: or pus . 
We 1) i(:k the 1) est path through the lattice , resulting from the comt ) osition using the Viterl ) i algorithm ,   ; m ( t this to I ) ranking word sequence is the outt ) ut of the LP Chooser . 
4 Experiments and Results
In order l : oshow ; ll~tl ; L hell SO , of ~ ttl:celIlode\]troda , grmmnar doe . sindeed hell ) pe , r formmme , wepe . r forme , d three experiments : Figure 6: Word lattice t brexample sentence after Tree Chooser and Unraveler using the supertag-based model ? For the baseline experiment  , we impose a random tree structure i breach sentence of the cortms and build a Tree Model whose parameters consist of whether a lexemel ~ t precede sort bllows her mother lexemelm  . 
We call this the Baseline LeftRight ( LR )
Model . This model generates There was estimate for phase the second no cost  . for our example input . 
? In the second experiment , we derive the parmneters t br the LR model fl ' om an annotated corpus  , in particular , the XTAG derivation tree cortms . This model generates Th , crcno estimate J ' or the second phase was cost . t brour example input . 
? In the third experiment , as described in Section 3 , we employ the supertag-based tree model whose parameters consist of whether a lexemeld with supertagS diszt dependent of Im with supertag sin  . Fm'-thermore we use the supertag in ~ brmation provided by the XTAG grammar to order the dependents  . This model generates Th crc was no cost estimate for the second phase  . t brour example input , which is indeed the sentence ibund in the WSJ . 
As in the case of machine translation , evaluation in generation is a complex issue . We use two metrics suggested in the MT literature  ( A1-shawl et al ,  1 . 998) based on string edit ; distance t ) etween the outtmt of the generation system and the reference corpus string front the WSJ  . 
These metrics , simple accuracy and generation accuracy , allow us to evaluate without human intervention , automatically and objectively . 4 Simple accuracy is them nn ber of insertion ( I )  , deletion ( D ) and substitutions ( S ) errors between the target language strings in the test corpus and the strings produced by the generation model  . The metric is summarized in Equation (1) . R is the number of tokens in the target string . This metric is similar to the string distance metric used for measuring speech recognition accuracy  . 
I+D+.q
SimplcAccuracy = (1----)(1)
R 4\~7c do not address the issue of whether these metrics can be used for comparative valuation of other generation systems  . 



Simt)le Go , ner ~ rtion
Ac(:ura (' y Accuracy
Average time perscnten(:(;
Baseline LR Model 41 . 2% 56 . 2% 186 ms ~ l ?(; cbank derived LI/ . Model 52 . 9% 66 . 8% 129 msSut ) ertag-bascd Model 58 . !)% 72 . 4%517 msTabl( ; 1: Performance results front the thre (' , tree models . 
Unlikes l ) eech recognition , the task of generation involves reordering of tokens  . The simple accuracy metric , however , penalizes a mist ) lacc . d token twice , as a deletion from its c . xpo , ct (' . d position and insertion at at different l ) osition . W cll SO ~ second metric , Generation A (: (: ura('y , shown in Eqm ~ tion (2) , which treats ( hiltion of ~ token ~ ttOIIC location in 1 ; 11( ; string ~ mdth ( ; insertion of the same to k ( m~tanoth ( ' a " location in tim string as one single mov ( 'an ( mt (  ; trot ( M ) . This is in addition to the rem~fining insertions ( 1t ) and deletions ( Dl )  . 
Ge'n(~'rationAcc' , ,racy = (1 -54 + I I + 1)' - t - , q ) (2) The siml ) lc , a (: cura (' y , g(merntiona('(:ur ; my a , n(ltimav(n:ag(~time , ti ) rgo am ration of (; a , (: hl ; cst ; s(~ , u-t(m('c for timt in ' o , (' , ( Xl ) crinmnts ; ~ r(~tabul~m , xlin % d)le1 . The test set consist ( xl of 1O0r~m ( to n fly ( : hoscnWS . Is(mt(m(:( ; with ml ~ w(n:agelngt ; h of 16 words . As can be seen , timsut ) crtng-1 ) asedmo ( Mrot ) roves over the LR model derived from mmotated at a ~md both models improv  (  ; over the baseline LR mod(:l . 
Sul ) ertngs in corl ) or ~ tericher infbrmations to has argunmnt mida ( tjunci:disl ; in (: tion , and n m nb cr and types of argunm nts . Y Vecxt ) ( ; (:t to iml ) rove the performance of the supcr tag-bas (  ; d model by taking these features into a (:(: ount . 
In ongoing work , we h~vc developed tree-based metrics in addition to the string-l  ) ased presented here , in order to ewfluates to ( : hastic gener ~ tion models . We h~vcal so attempted to correlate these quantitative metrics with human  ( tualitativ ( ~ judgcnl ( mts . Ado , tail ( ~d dis ( : ussion of these experiments and results is t ) r ( ' , s(mto , din(Bangalore(' , ; al . , 2000) . 
5 Comparison with Langkild e8z
Knight
Langkild c and Knight ( 1998a ) use a hand- ( : rafted grmmmu : that maps semantic representations to sequences of words with lino  , arization constraints . ACOml ) lex semantics t , ructur ( ~ , is trnnsl~ted to ~ L lattice , , mid a bigrmn langunge mode , 1t ; hell (: hoost ~ , s&lltOllg ; lo , l ) ossiblo , surface , strings ( moo(led in the l~ttice . 
The system of Langkild c8 ~ Knight , Nitrogen , is similar to FERGUS in that generation is divided into two phases  , the first of which results in a lattice fl'om which as ur Nccsi  ; ring is chosen during the , s( ; condt ) has ( ; using a language model ( in our case a trigram model , in Nitrogen's case a . 1) igr~ml1no(M ) . I h)w (' , ver , (; t1(; first t ) hases nr (' , quit (' , ditf (; r(mt . In FEI ( . GUS , wesI ; m : i ; with a lex-i (: ~ dpr (' , dit:at( ; -argulnent st ; ru( ; l ; ur ( ~ while in Nitrogen , a more s0 , mantic in tmt is used . FEII . GUS (: ould (' , asily )( ; augm ( ; nt( ; d with a t ) r ( ; t)ro(:cssorl ; h ~ d ; maps as o , m ; mti (: rc , t ) ro , s(mtal ; iont ; oore:syn-ta (: ti (: inl ) ut ; this is not the focus of our r(~sc~u'ch . 
\[Iowev(' , r , the r(~are two more imt ) or l , mfl ; differ-(m('es . First ,   ; t1( ; h~m(t-crafl ; edgrmmnar in Nitrogen mapsdir ( ; (: tly from semantics to a linear r(~l)r ( ; sentation , skipping tho , nr ) or ( ; s(:(mtrcI ) rc-sentation usually f ~ vore(t br the , rod)r (' , s(mtn ; i on of syntax . There is no stochastic tree model , since , the , re , ~tr (' , no trees . In FEIGUS , intied (' hoices arc , ma ( t c stochastically t ) ascdontim treercl ) rcs cntation in the " I?ce Chooser . This allows us to capture stochastically certain long-  ( tisl ; ancecfli ' , (: ts which n-grmns camlot , such as sct ) ~ ration of p ; n'ts of a collocations ( such as peT : form an ope~ution ) through interl ) osing adjuncts ( John peT : formed along ,   . somewhate-dious , and quite frustrating opcration on hi , sborder collie ) . Second , tim hand-('rafl ; cdgram-ln ; trllSCd in FEll . (- IUS was crafted in del ) end cntlyfl ' om then ( ; (' xl forgent , rat ; ion and is a imrcly(l ( ; (: larativercl ) rcs(mtation of English syntax . Asfects such as agreement , which cannot in general be clone by an ngram model and which are  , at ; the same time , descriptively straightforward and which are handled by all non-stochastic generation modules  . 
6 Conclusion and Outlook
We have presented empirical evidence that using a tree model in addition to a language model can improve stochastic NLG  . 
FERGUS aSpresented in this paper is not ready to be used as a module in applications  . 
Specifically , we will add a morphological component , a component that handles flmction words ( auxiliaries , determiners ) , and a component that handle simnctuation . In all three cases , we will provide both knowledge-based and stochastic components  , with the aim of comparing their behaviors , and using one type as a backupt br the other type  . Finally , we will explore FI ; R-OUS when applied to a language t br which a much more limited XTAG grammar is available  ( for example , specit\[ying only the basic sentence word order as  , sw , SVO , and speci(ying subject verb agreement ) . In the long run , we intend FEI/OUS to become a flexible system which will use handcrafted knowledge as much as possible and stochastic models as much as necessary  . 

Hiyan Alshawi , Srinivas Bangalore , and Shona Douglas .  1998 . Automatic acquisition of hierarchical transduction models tbr machine translation  . In Proceedings of the 36th Annual Meeting Association for Computational Linguistics  , Montreal , Canada . 
Srinivas Bangalore and Aravind Joshi . 1999.
Supertagging : An approach to a hnost parsing . Computational Linguistics , 25(2) . 
Sriniw ~ sBangalore , Owen Rainbow , and Steve Whittaker .  2000 . Ewfluation Metrics for Generation . In Proceedings of International Cor ~: fere nee on Natural Language Generation  , 
Mitzpe Ramon . Isreal.
Aravind K . Joshi . 1987a . An introduction to Tree Adjoining Grammars . In A . Manaster-Ramer , editor , Mathematics of Language , pages 87-115 . John Benjamins , Amsterdam . 
Aravind K . Joshi . 1987b . Tlm relevance of tree adjoining grammar to generation  . In Gerard Kempeu , editor , Natural Language Generation : New Results in Artificial In-te Uigence  , Psychology and Linguistics , pages 233252 . Kluwer Academic Publishers , Dordrecht/Boston/Lancaster . 
Irene Langkilde and Kevin Knight . 1998a . Generation that exploits corpus-based statistical knowledge  . In 36th Meeting of the Association . for Computational Linguistics and 17th International Cor ~: \[' crcnce on Computational Linguistics  ( COLING-ACL'98 )  , pages 704-710 , Montrdal , Canada . 
Irene Langkilde and Kevin Knight . 1998b.
The practical value of ngrams in generation . In Proceedings of the Ninth International Natural Language Generation Workshop  ( INLG'98 )  , Niagara-on-the-Lake , Ontario . 
Irene Langkilde and Kevin Knight . 2000.
Forestbased statistical sentence generation.
In Proceedings of First North American ACL,
Seattle , USA , May.
Robert Malouf .  1999 . Two methods tbr 1 ) re-dieting the order of prenonfinal t~djectives in english  . In Pwceedings of CLINg 9 . 
David D . McDon Md and James D . Pusteiovsky.
1985 . % ~ gs as a grammatical formalism t br generation . In 23rd Meeting of the Association for Computational Linguistics  ( ACL'85 )  , pages 94103 , Chicago , IL . 
Owenl:\[ambow and Tany ~ Korelsky .  1992 . Applied text generation . In Third Conference on Applied Natural Language Processing  , pages 4047 , % ento , Italy . 
Adwaitt / . atllaparkhi .  2000 . Trainable methods for surface natural language generation  . In Proceedings of First North American ACL,
Seattle , USA , May.
Ehud Reiter .  1994 . Has a consensus NL generation architecture appeared  , and is it psycholinguistically plausible ? In Proceedings of the  7th International Workshop on Natural Language Generation  , pages 163-170 , Maine . 
The XTAG-Group .  1999 . A lexicalized %' ee Adjoining Grammar for English  . Technical Report http://w~rw . c is . upen n , edu/~xtag/tech-report/tech-report . htral , The Institute for Research in Cognitive Science , University of Pennsylvania . 

