Extracting the Names of Genes and Gene Products with a 
Hidden Markov Model
Nigel Collier , Chikashi Nobata and Jun-ichi Tsujii
l ) el ) artm(mt of Information Science
( h'aduate School of Science
University of Tokyo , Hongo-7-3-1
Bunkyoku , Tokyo 113, . Japan
E-maihnigel , nova , tsuj ? ?@? s . s . u-tokyo , ac . jp
Abstract
\ ~ e report the results of a study into the use of a linear interpolating hidden Marker model  ( HMM ) for the task of extra . (' tinglxw\]mi (: al ; er-minologyfl:om MEDLINEal ) stra('ts and texl ; s in the molecular-bioh ) gy domain . T iffs is the first stage is la . system that will exl ; ra('l ; even l ; information for automatically ut ) da . ting1) ioh ) gy databases . We trained the HMM entirely with 1 ) igrams based ( m lexical and character features in a relatively small corpus of  100 MEDLINE abstract ; s that were ma . rked-ul ) l)y ( lo-main experts wil ; hterm (: lassesu (: hast ) rol ; eins and DNA . I . Jsing crossvalidation methods wea(:\]fieved a , n \] . e-score of 0 . 73 and we (' , xm nine the ( ' ontrilmtion made by each 1 ) art of the interl ) o-lation model to overconfing ( la . taSl)arsen (' . ss . 
1 Introduction
Ill the last few ye~trs there hast ) een a great investment in molecula . r-l ) iology resear (: h . This has yielded many results l ; \] la . 1; ,  1 ; ogel ; her wil ; hamigration of m : c\]fivalmal ; erial to the internet , has resulted in a nexl ) losion in l ; tmnuns-\] ) el 7 of research tmbli ( ' ation saa ~ ail at ) le in online databases . The results in these 1 ) al ) ershowever arc not available illa structured for nmt and have to  1  ) e extracted and synthesized mammlly . 
Updating databases such as Swiss Prot ( Bairochmid Apweiler ,  1 . 997) this way is time (: on smning and nmansl ; h~t the resull ; s are not accessible so conveniently to he 11 ) researchers in their work . 
Our research is aimed at autonmti ( : ally ex-tra ( : ting facts Kern scientific abstracts and flfll papers ill the molecular-biology domain and using these to update databases  . As the tirst stage in achieving this goal we have exl  ) lored th (  ; use of a generalisable , supervised training method based on hidden Markov models  ( It MMs )   ( Rabiner and . \] uang ,  1986 ) f brtim identification mid classitieation of technical expressions ill these texts  . This task can 1 ) e considered to be similar to the named c . ntity task in the MUC evaluation exercises ( MUC ,  1995) . 
In our current work we are using abstracts available fl : om PubMed's MEDLINE  ( MED-\] , INE ,  1999) . The MEDLINE ( lnta . l ) as e is an online collection of al ) straets for pul ) lished journal articles in biology midmedicine and contains more than nine million articles  . 
With the rapid growth in the mlmbcr of tmb-\]ished l  ) al ) ers in the field of moh ; (' ular-biolog 3 , there has been growing interest in the at ) pli-cation of inform a . tion extra (: tion , ( Sekimizu et al . , 1998) ( Collier et al , 1999) ( Thomas et al , 1999) ( Craven and Kmnlien ,  1999) , to help solve so use ( sf the t ) robhmss that are associated with information overload  . 
In the remainder of this i ) a per we will first of all ( ratline the t ) ack ground to the task and then d ( ~s ( 'ril ) et ; hc basics of ItMMs and the f i ) r-real model w c are using . The following sections give an outline of a . lse\vtagged (' orlms ( Ohta et al . , 1999 ) thn tour team has deve h ) i ) ed using al ) -stra ( ' ts taken from a subdomain of MEDLINF , and the results of our experinmnts on this cor -lmS  . 
2 Background
I leeent studies into the use of SU l ) ervised learning - t ) ased models for the n~mled entity task in the miero-l sio h  ) gy domain have . shown that l nodels based on HMMs and decision trees such as  ( Nol ) al ; ~t et al ,  1999) ~ , r ( ; much more gener-alisable and adaptable to slew classes of words than systems based on traditional hand-lmilt  1  ) at texns a . nd domain specific heuristic rules such as ( Fukuda et al ,  1998) , overcoming the 1 ) rol ) lems associated with data sparseness with the help of sophisticated smoothing algorithms HMMs can be considered to be stochastic finite state machines and have enjoyed success in a number of felds including speech recognition and part-of-speech tagging  ( Kupiec ,  1992) . 
It has been natural therefore that these models have been adapted tbruse in other word class prediction tasks such as the atoned-entity task in IE  . Such models are often based on ngrams . Although the assumption that a word's part of speech or name class can be predicted by the previous  n1 words and their classes is counterintuitive to our understanding of linguistic structures and long distance dependencies  , this simple method does seem to be highly effective ill I  ) ractice . Nymble ( Bikel et al ,  1997) , a system which uses HMMs is one of the most successfl fl such systems and trains on a corpus of marked up text  , using only character features in addition to word bigrams  . 
Although it is still early days for the use of HMMs for IE  , we can see a number of trends in the research . Systems can be divided into those which use one state per class such as Nymble  ( at the top level of their backoff model ) and those which automatically earn about the model'structure such as  ( Seymor et al . , 1999) . 
Additionally , there is a distinction to be made in the source of the knowledge for estimating transition t  ) robabilities between models which are built by hand such as  ( Freitag and McCal-lure ,  1999 ) and those which learn fl ' om tagged corpora in the same domain such as the model presented in this paper  , word lists and corpora in different domains - so called distantly-labeled data  ( Seymor et al . , 1999) . 
2 . 1 Challenges of name finding in molecu lar - b io logy texts The names that we are trying to extract fall into a number of categories that are often wider than the definitions used for the traditional named-entity task used in MUC and may be considered to share many characteristics of term recognition  . 
The particular difficulties with ident it ) dng and elassit ~ qng terms in the molecular-biology domain are all open vocabulary and irrgeular naming conventions as well as extensive cross over in vocabulary between classes  . Their regular naming arises in part because of the number of researchers from difli  ; rent fields who are TI-Activation of < PROTEIN > JAKkinases </ PROTEIN > and < PROTEIN > STAT pT vteins </ PR  , OTEIN > by < PROTEIN > interlcukin- 2 </ PROTEIN > and < PROTEIN > in tc~fc~vn alph , a </ PROTEIN > , but not the < PROTEIN > T cell antigen receptor < ~ PROTEIN >  , in < SOURCE . ct > h , uman Tlymphoeytes </ SOURCE . e t > . 
AB The activation of < PROTEIN > Janus proteint, . flvsinek in ascs</PROTEIN > ( < PROTEIN > JAI ( s </ PROTEIN > ) and < PROTEIN > signal transducer and ac-tivator of transcription </ PROTEIN >  ( < PROTEIN>STAT</PROTEIN > ) pro-reins by < PROTEIN > int crIcuk in ( IL ) 2 </ PROTEIN > , thc < PROTEIN > T cell antigen receptor </ PROTEIN >  ( < PROTEIN > TCR </ PROTEIN > ) and < PROTEIN > in tc~fcrvn ( IFN ) alpha</PROTEIN > was czplor cdin < SOURCE . ct > human periph , cralblood-derived Tcclls</SOURCE . et > and the < SOURCE . el > leukemic T cell line K it 225 </ SOURCE . el > . 
Figure 1: Example MEDLINE sentence marked up in XML for l fiochemical named-entities  . 
working on the same knowledge discovery area as well as the large number of substances that need to be named  . Despite the best , etforts of major journals to standardise the terminology  , there is also a significant problem with synonymy so that often an entity has more tlm  . none name that is widely used . The class cross over of terms arises because nlal lyprot  ( : in s are named after DNA or RNA with which they react  . 
All of the names which we markup must belong to only one of the name classes listed in Table  1  . We determined that all of these name classes were of interesto domain experts and were essential to our domain model for event extraction  . Example sentences from an mrkedut ) abstract are given in Figure 1 . 
We decided not to use separate states ibr pre- and post-class words as had been used in some other systems  , e . g . ( Freitag and McCal-lure , 1999) . Contrary to our expectations , we observed that our training data provided very poor maximum-likelihood probabilities for these words as class predictors  . 
We found that protein predictor words had the only significant evidence and even this was quite weak  , except in tlm case of post-class words which included ammfi  ) er of head nouns such as " molecules " or " heterodimers "  . In our
P1K ) TEIN 21 . 25  . MKki'n , a . se\])NA 358 IL2\] rlvmotcr\]NA 30771 I ? , S()UICF ,  . cl93 le'ukemic T cell line K it 225 S()UI \], CE . (: t417h,'wm , an Tlymphocytes SOURCE . too 21, % hizosacch , aromyces pomb c
S()URCE.mu64 mice
SOURCE.vi90ItJV-1
S()UICE . sl77 membrane
S()UICE.ti37 central'ner , vo'ussystem
UNK t , y ~ vs in eph , osphovy lal , iont ) rox fiils ~ protein groups , families ~ cOral ) loxes and Slll ) Sl;I'll CI ; lll'eS . 
I ) NAs I ) NA groups , regions and genes
RNAsI~NA groups , regions and genescell line ( : ell type lllOll ( )-organism multiorganism viruses sublocat ; iont issuel mckground words Tablel : Named ( mtilsy ( : lasses . ~/: indi (: at ( tstsfic ~ mmt ) crofXMI , tagged terms in our (: or pus of 100 abstracts . 
early experiments using IIMMs that in ( : or po-rated pro-and 1 ) ost-c\]as states we\[imnd tha . tpcr for lnance was signiticantly worse than wil ; h-Oll t ; sll ( ; hsi ; at ; csan(lst)w (' . formulated the ~ uod clasg , ~ iv cllilS(;(;\[;iOll:/ . 
~, . f(Qi, . .~,l < _,Ffi , . .~ . , ,  > )  +  ( 1 ) and for all other words and their name classes as t bllows :  3 Mx . ' tzhodThelm rl ) os cof our mod ( ; 1 is Iolindt ; hcn , osl : likely so ( tilth , liCe of name classes ( C ) l bra given se ( tucn cc of w or ( ls ( W )  . The set of name ( ' lasses in chutcs the ' Unk ' name ( : lass whi ( ' h we use li ) r1 ) ack grom M words not 1 ) elonging to all y ( )\[ the interesting name classes given in Tal ) lc1 and t ; hc given st ; qu(m (: e of words which w(~ , >(' . spans a single s(,Jd ; cn('c . The task is th crc for (~1 ( max-intize Pr((TIH :) . \ ? ciml ) lem ( mta I\]MM to es-timate this using th ( ' . Markov as suml ) tion that Pr ( CII ? ) canbet ' ( mnd from t ) igrams of ha . me classes . 
Int h(' . following model we ( : on sid ( u " words to 1 ) cordered pairs consisting of a . surface word , W , and a . word t bature , 1", given as < W,F > . 
The word features thcms (' Jvcs arc discussed in
Section 3.1.
As is common practice , we need to ( : alculat c the 1 ) rol ) abilities for a word sequence for the first ; word's name class and every other word difl brently since we have no initial naln t > class to make a transition froll l  . Accordingly we use l ; heR ) llowing equation to ( : alculat c the ilfitial name ( : lass probability ,  ~ , , J'(Cz ,  . .~,,I < wi ~, . .~, 19~, . ,~,, >) +
I ', , .( G ) ~ o.1'(G
A , . /'( G ; v ~ . f(G5: I . /'( G ), ~ . /'( G ),, ~ . I(G ) < Wt , l ,  > , < llS , _ , , l , i ~> , GJ ):-<1'15 . , I ~ , , >, < I,V ~ _ ~ , l )_j >, G . - ~) + <_, l'i >, <115_l,Ft,-~>,Ct- . , ) + < 115 , Fi > ,  < _ , P  ~ , _~ > , G ~) + <_ , l , ) > ,  <  . _ ,  1% ~ > , C ~__~) + (2) whc , : cf ( I ) is ( ' alculat cd with n mx in luln-likelihood estimates from counts on training data  , so that tbrexample ,   . f(G , I < 1 , ~5 , 1 , i > , < I , t , ~_ , , F~_~ > , G-~)-T (< Il S ,  1 , ~ > , G . , <1'15_,,1 ~_~>, G . -@ , T (< l'lZj , ,l~J > , <\[' Vt-l , Ft-I > , Ct-l )  ~3 ) Where T ( ) has been found from counting the events in thc training cortms  . In our current sysl ; oln\vcSC\[ ; t ; tlcC ( ) llSt ; & lltS ~ il Jldo-i \] ) y halld all ( llet ~ ai = 1 . 0, ~ Ai = 1 . 0, a0> alkO-2,A0>AI .   .   . _>As . Tile current name class Ct is conditioned oil the current word and fea-t  ; llrc ~ thcI ) rcviolls name class , ~* t--l : and t ) rc-vious word an ( tt batur c . 
Equations 1 and 2 implement a linear-interpolating HMM that incorporate sammfl  ) crA coefficients ) designed to reduce the effects of data sparseness  . While we hope to have enough training data to provide estimates tbrall model parameters  , in reality we expect to encounter highly fl ' agmented probability distributions  . In the worst case , when even a name class pair has not been observed be ibrein training  , the model defaults at A5 to an estimate of name class unigrams . We note here that the bigram language model has a nonzero probability associated with each bigram over the entire vocal  ) -ulary . 
Our model differs to a back of formulation because we tbund that this model tended to suffer fl'om the data sparseness problem on our small training set  . Bikel et al for example considers each backoff model to be separate models  , starting at the top level ( corresl ) onding approximately to our Ao model ) and then falling back to a lower level model when there not enough evidence  . In contrast , we have combined these within a single 1 ) robability calculation tbr state ( class ) transitions . Moreover , we consider that where direct bigram counts of 6 or more occur in the training set , we can use these directly to estimate the state transition probability and wense just the  , ~0 model in this case . For counts of less than 6 we smooth using Equation 2  ; this can be thought of as a simt ) le form of q ) nck-eting ' . The HMM models one state per name ( : lass as well as two special states t br the start and end of a sentence  . 
Once the state transition l ) rol ) abilities have been calcnlated according to Equations  1 and 2  , the Viterbi algorithm ( Viterbi ,  1967 ) is used to search the state space of 1 ) ossible name class assignments . This is done in linear time , O ( MN2 ) for 54 then unfl ) er of words to be classified and N the number of states  , to find the highest probability path , i . e . to max infise Pr(W,C ) . In our exl ) eriments 5/i is the length of a test sentence . 
The final stage of our algorithm that is used after name class tagging is complete is to use ~ cleanup module called Unity  . This creates a frequency list of words and name classest bradocmnent and then retags the document using the most frequently nsed name class assigned by the HMM  . We have generally tbund that this improves Fscore performance by al  ) out 2 . 3%, both t brretagging spuriously tagged words and
Word Feature Exmnl ) le
Digit Nmnber 15
Single Cap M
Greek Letter alpha
Caps And Digits I2
Two CapsRalGDS
Letters And Digits p52 hfitCapInterleukin
Low Capska , t ) pa B
Lowercasek in ases

Backslash /
OpenSquare\[
Close Square \]


Percent %
Oi)enParen (
Close Paren )


Deternliner the
Conjmmtion and
Other *+
Table 2: Wordt batures with examples tbr finding untagged words in ml known contexts that had been correctly tagged elsewhere in the text  . 
3.1 Word features
Table 2 shows the character t'eatnres that we used which are based on those given for Nymble and extended to give high pertbrmance in both molecular -biology and newswire domains  . The intnition is that such features provide evidence that helps to distinguish nmne classes of words  . 
Moreoverwehyt ) othesize that such featn res will help the model to finds in filarities between known words that were tbnnd in the training set and unknown words  ( of zero frequency in the training set ) and so overcome the unknown word t ) rol ) lem . To give a simple example : if we know that LMP- 1 is a member of PROTEIN and we encounter AP- 1 for the first time in testing , we can make a fairly good guess about the category of the unknown word ' LMP' based on its sharing the same feature Two Caps with the known word ' AP ' and ' AP's known relationship with ' -   1'  . 
Such unknown word evidence is captured in submodels  A1 through )  , 3 in Equation 2 . \ ? e more mealfing flll distinctions between name (  ; \] asses than for exam I ) le part-of-speech ( POS ) , since POS will 1 ) redominmltly 1 ) enounfi ) rall name class words . The t'catures were chosen to be as domain independent as poss it  ) le , with the exception of Ilyphon and Greel , : Letter which have t ) articular signitieance for the terminology in this dolnain  . 
4 Experiments 4 . 1 Tra in ing and tes t ing set The training set we used in our experiments  ( ' on sisted of 100 MEI ) II , INI~al ) stra(:ts , marked Ul ) illXS/\[Ll ) ya ( lonminext ) ert for the name ( ' lasses given in Tal ) le1 . The mmf l ) er of NEs that were marked u1 ) by class are also given in Tfl ) le 1 and the total lmmber of words in the corlms is 299/\]:0  . The al ) stracts were chosen from a sul ) (lomain of moleeular-1 ) iology that we formulated by s ( ' , ar ( ; hing under the terms h/um an , blood cell , trav ,  . scription , /' actor in the 1) utiMed data l ) as c , This yiel(l (' . ( tal)t ) roximately 33 (10 al/-stracts . 
4.2 Results
The results are given as Fscores , a ( ; Olllll ( ) ll measurement for a (: (: ura(:y in tlw , MUC conferences that e on f l ) in es r ( ; (: all and 1) re(:ision . 
These are eah : ulated using a standard MUC tool ( Chinchor ,  1995) . Fscore is d (' . iin(~das'2xlS"(eci . sionxl ~ cc , ll
F - . ~ cor . = (4) l ) ' rccisio ~ ,  + \]? , cc ( dl The tirst set ot7 experiments we did shows the effectiveness of the mode  . 1 for all name ( : lasses and is smnmarized in Table 3 . We see that data sparseness does have a net fe ( ' t ~ with 1 ) rote in s-the most mlmerous (  ; lass in training-getting the best result and I/ , NA-thesn mllc , st training (: lass-getting the worst result . The tal ) le also shows the ett ' eetiveness of the character feature set  , whi (' hin general adds 10 . 6% to the Fscore . 
This is mainly due to at ) ositive effect on words in the 1 ) R , OTEIN and DNA elases , but we also see that memt ) ers of all SOURCE sul ) - ( ' lassessufl'er from featurization . 
We have atteml ) ted to in corl ) or at e generalisation through character t ' eatm : es and linear in-teri  ) olation , which has generally \]) een quites u(:-cessful . Nevertheless we were (: urious to see just
Class Basellase-l'eatures
PROTEIN 0.759 0.670 (-11.7%)
DNA 0 . 472 0 . 376 (-20 . 3%)\] ~ NA 0 . 025 0 . OOO(-leo . o%)
SOURCE(all ) 0.685 0.697 (+1.8%)
S()UICE . cl 0.478 0.503 (+5.2%)
SOURCE . el 0.708 0.752 (+6.2%)
SOURCE.me 0.200 0.311(+55.5%)
SOURCE . mu 0.396 0.402 (+1.5%)
SOURCE.vi 0.676 0.713 (+5.5%)
S()URCI,Lsl 0.540 0.549 (+1.7%)
SOURCE.ti 0.206 0.216 (+4.9%)
All classes 0 . 728 0 . 651 (-10 . 6% ) q ) d ) le3:Named entity acquisition results using 5-fi ) ld crossvalidation on 100 XML tagged MEI ) I~INE al/stra ( : ts , 80 for training and 20 fin . 
testing , l\]ase-J '(', at ' urc . sues no character feature inibrmation . 
) ~ Mode\[No.
 #Texts 0 1 2 3 4 5 0 . 06 0 . 19 0 . 10 0 . 63 0 . 94 1 . 0 () . ()  ~ l0 . 15 0 . 09 0 . 59 0 . 89 1 . 0 0 . 03 0 . 12 0 . 08 0 . 52 0 . 83 1 . 0 0 . 02 0 . 09 0 . 06 0 . 41 0 . 68 1 . 0 Tal)le 4: M(' , anlmml ) er of successfll calls to sul)-m(i(t ( ; ls during testing as a fl ' aetion of total mnn-1 ) er ( If stale transitions in the Viterl ) i latti ( : e , g/:T ( ! x is indicates the mmfl ) er of al ) stra ( : ts used ill training . 
whi ( : ht ) arts of the model were contributing to the bigram s  ( : or es . Table 4 shows the l ) ercent-age of bigranls which could be mat ( ' hed against training t ) igrams . The result indicate tha ~ a high 1 ) ereentage of dire ( : t bigrams in the test e or l ) uS neveral ) t ) ( ; ar in the training (: of t ) us and shows tha , tour HMM model is highly depel > ( l ( mton smoothing through models ~ kl and ) ~:~ . 
\? e can take another view of the training data 1 ) y's alalni-slieing ' the model so that only evi- ( tenee from 1 ) art of the model is used . Results are shown in Tat ) le5 and support the eonchl-sion that models Al , A2 and Aaare . crucial at this sir , (; of training data , although we would expect their relative iln portance to fifil as we have more  ( tircct observations of bigrams with larger training datasets  . 
Tal ) le 6 shows the rolmstness of the model \[ Fscore ( all classes )  0 . 728 0 . 722 0 . 644 0 . 572 0 . 5 76 \] Table 5: Fscores using different n fixtures of models tested on  100 abstracts , 80 training and 20 testing . 
I  #Texts 80 40 20105\]
IF score 0.728 0.705 0.647 0.594 0.534\]
Table 6: training stracts).
Fscore for all classes a gMnst size of corpus ( in number of MEDLINE ab-for data sparseness , so that even with only 10 training texts the model can still make sensible decisions about term identification and classification  . As we would expect; , the table ; flso clearly shows that more training data is better  , and we have not yet reached a peak in pertbr-i nance  . 
5 Conclusion
HMMs are proving their worth for various tasks in inibrmation extraction and the results here show that this good performance can be achieved across domains  , i . e . in molecular-biology as well as rising newspaper reports  . The task itself ' , while being similar to named entity in MUC , is we believe more challenging due to the large nunfl  ) er of terms which are not proper nouns , such as those in the source subclasses as well as the large lexieal overlap between classes such as PROTEIN and DNA  . A usef if lline of work in the future would be to find empirical methods for comparing difficulties of domains  . 
Unlike traditional dictionary-based lnethods , the method we have shown has the advantage of being portable and no handmade patterns were used  . Additiolmlly , since the charactert batures are quite powerful , yet very general , there is little need for intervention to create domain specific features  , although other types of features could be added within the interpolation framework  . Indeed the only thing that is required is a quite small corpus of text containing entities tagged by a domain expert  . 
Currently we have opt in fized the , k constants by hand but clearly a better way would be to do this antomatically  . An obvious strategy to use would be to use some iterative learning method such as Expectation Maximization  ( Dempster et al ,  1977) . 
The model still has limitations , most obviously when it needs to identity , term boundaries for phrases containing potentially ambiguous local structure such as coordination and pa  . ren-theses . For such cases we will need to add postprocessing rules  . 
There are of course many NF , models that are not based on HMMs that have had success in the NE task at the MUC conferences  . 
Our main requirement in implementing a model for the domain of molecular-biology has been ease of development  , accuracy and portability to other subdomains since molecular-biology itself is a wide field  . HMMs seemed to be the most favourable option at this time  . Alternatives that have also had considerable success are decision trees  , e . g . ( Nobata et al , 1 . 999) and maximum-entropy . The maximum entropy model shown in ( Borthwick et al ,  1998 ) in particular seems a promising approach because of its ability to handle overlapping and large feature sets within n well founded nm then mtical ti'a mework  . However this implementation of the method seems to incorporate a number of handcoded domain specitic lexical Datures and dictionary lists that reduce portability  . 
Undoubtedly we could incorporate richert ba-tures into our model and based on the evidence of others we would like to add head nouns as one type of feature in the future  . 

We would like to express our gratitude to Yuka Tateishi and Tomoko Ohta of the Tsujiilaboratory for their efforts to produce the tagged cor-tins used in these experiments and to Sang-Zoo Lee also of the Tsujiilaboratory tbrhis comments regarding HMMs  . We would also like to thank the anonymous retire est br their help flfl comments  . 
206\] ~, eferences
A . Bairoch and R . Apweiler .  1997 . The SWISS-PF\[OT1 ) r ) t ~ in sequence databank and its new SU l ) l ) lement 15: EMBL . Nucleic Acids Research , 25:31-36 . 
D . Bikel , S . Miller , I:L Schwartz , and
R . We sichedel .  1997 . Nymble : a high-t ) ertbrmane elarning \] mlne-tin ( ler . In Proceedings of the Fifth Co ~@ rcrcnccon Applied Natural  Langua9 e\] ) ~ vcessi'n , g , pages 194201 . 
A . Borthwick , J . Sterling , E . Agichtein , and ll , . Grishman .  1998 . Extliting div ( :rse knowledges our ( : esviallla Xillllll ( mtroly in named entity recognit in . In P'mccc dings of the Worlcshop on Very Lar . qcCorpora ( WVLC'98) . 
S . Chert and J . Goodman .  1996 . An empirical study of smoothing te:hm fiquest br language motleling  . 3/tst Annual Meeting of tlt , (: Association of Computational Linguistics , Calffofnia , USA ,  2427  . hme . 
N . Chin:hr . 1995. MUC-5ew duatimetrics.
In In Pwcecdings of th , ci " ffl , h , Mc . ss(u . le Un-dcr stand in 9Cou:fe'rencc(MUC-5) , Baltimore , ,
Maryland , USA ., 1) ages 6978.
N . Collier , It . S . Park , N . Ogata , Y . Tateishi , C . Nolata , ' F . Ohta , T . Sekimizu , H . \]mai , and J . Tsujii .  1999 . The GENIA 1r ) je ( : t:corlms-1 ) ascdkn ( wlcdge acquisitio \] , and in-forlnal , i on extra (' tionf\]'Olll genomer ' , sear (: ht)al)ers , in Proccediu , fl . s of the An ' , ,'aal M(' , eting of the Europeanch , apt cr of the Association for Computational Lingu 'istic  , s(EA(/\]3'99) , 3 uuc . 
M . Craven and 3, Kumlien .  1999 . Constructing biohgical know h ; tg ; tases t ) y extracting information from texts our ( : es . In \] ~ v c (:( , Aings of the 7th , hl , t cr national CoT ff ( : rence on Intelligent Systcmps for Molecular Biology  ( ISMB99 )  , Heidellmrg , Germmly , August 610 . 
A.P . Dempster , N.M . Laird , and D . B . Rubins.
1977 . Maxim mn likelihood from incoml ) leted at a via the EM algorithm .   , \] ou'rnal of the Royal Statistical Society ( B) ,  39:1-38 . 
l ) . Freitag and A . McCMlum .  1999 . Intbrma-tion extraction with HMMs and shrinkage . 
In Proceedings of the AAAl'99 Worl ~ . ~ h , opou , Machine Learning for IT~:formation Extraction , Orlando , Florida , July 19th . 
K . Fuku(la , T . Tsunoda , A.2) mmra , and
T . Takagi .  1998 .  ~12 ) ward int brmation extraction : identifying l ) rote in names from biologi-eal papers . IllPT vcccdings of thcPac'lificSymposium on Biocomp'uting'98   ( PSB'98 )  ,   . Jan-1u AYy . 
.1 . Kupiec .  1992 . l/o bust Imrt-of speech tagging using a hidden markov model  . Computer
Speech and Lang'aagc , 6:225-242.
MEI ) LINE .  1999 . The PubMed datal ) as e can be t ' ( mnd at : . 

DAIIPA .  1995 . lroceeding . so . flth , cSixth , Message Understanding Cou : fcrcnce ( MUC-6) , Cohmdfia , MI ) , USA , Nove , nfler . Morgan
Nail\['\]ll allll.
C . Nobata , N . Collier , and J . Tsu . iii .  1999 . Automatic term identification and classification in  1iology texts . In Proceeding . s " of the Nat-u'ral Lang , lmgc PacificRimSymposium(NL-
PRS'2000), November.
Y . Ohta , Y . Tateishi , N . Collie'r , C . No-1) at a , K . IIushi , and J . Tsujii .  1999 . As enmntieally annotated cort ) us from MED-L\]\[NE al ) sl ; ra:l ; s . Inl'rocccd , bu . ls of th . c ~: nth . 
Workshop on Go'home I~f formatics . Universal A : a demy Press , Inc . , 1415 Deccntl)er . 
l  ~ . l labiner and B . .\]uang .  1!)86 . An introtu:-ti(m to hidden Markov too ( Ms . H'2EE ASSP
Magazi ', , (',, 1ages d16, Jammry.
T . Sekilnizu , H . Park , and J . ' l'sujii .  1998 . 
Ilenti\[yingl ; he interaction 1 ) etween genes an 1g Olleiro ( lucts\]ase ( lonf\]'e ( lue \] My seen verbs in n \] et lineal ) si ; rael ; s . Ill~(:'li,()?ll,('~\]~f for'm,al,ics' . 
Univ crsa , 1Academy Press , Inc.
K . Seymore , A . MeCallum , and l . Ioscnfeld.
1999 . Learning hidden Markovestrucl : ure for informatim  (  , xtraction . In \] ) wcc dings of the AAAl'99 Workshop on Macfli'n , (: Lcarni'n9 for l ' , fo'rmationE : draction , Orland , Florita . ,
July 19th.
.J . Thomas , D . Milward , C . Ouzounis , S . Pulman , and M . Carroll .  1999 . Automatic extraction of 1 ) rote in interactions fl'oms'ien-tific abstracts . In Proceedings of the Iac'll/ic Symposium on Biocomputing'99   ( PSB'99 )  , 
Hawaii , USA , Jmmary 49.
A .  3 . Vit(;rbi .  1967 . Error l ) mnds for : on volu-tion seles and an asyml ) totically optimum deco ( ling algorithm . IEEE Tran , s'actiou, . s ' on
I ~ formation Theory , IT-13(2):260269.

