Local context templates for Chinese constituent boundary 

Qiang Zhou
The State Key Laboratory of Intelligent Technology and Systems 
Dept . o1' Computer Science and technology,
Tsinghua University , Beijing 100084
zhouq@s1000e.cs.tsinghua.edu.cn

in this paper , we proposed a shallow
syntactic knowledge description :
constituent boundary representation adits simple and efficient prediction algorithm  , based on different local context templates learned fiom the annotated corpus  . An open test on 2780 Chinese real text sentences showed the satisfying results :  94%  ( 92% ) precision for the words with multiple ( single ) boundary tag output . 
llt simplified the complex constituent levels in parse trees and only kept the boundary information of every word in different constituents  . Then , we developed a simple and efficient constituent boundary prediction algorithm  , based on different local context templates learned flom the annotated corpus  . An open test on 2780 Chinese real text sentences showed the satisfying results :  94%  ( 92% ) precision for lhew ords with multiple ( single ) boundary lag output . 
I . Introduction
Research on syntactic parsing has been a focus in natural anguage processing for a long lime  . As the develop lnent of corpus linguistics , many statistics-based parsers were proposed , such as Magerman ( 1995 ) 's statistical decision tree parser , Collins (1996)'s bigram dependency model parser ,  1 ; /atnaparkhi (1997)'s maximum entropy model parser . All of lhem fried to get the complete parse trees of the input sentences  , based on the statistical data extracted l'roman annotated corpus  . 
The besl parsing accuracy of these parsers was about  87%  . 
Realizing the difficulties o1'complete parsing , many researches turned to explore the partial parsing techniques  . Church ( 1988 ) proposed a silnple stochastic technique for lecognizing the nonrecursive base noun phrases in English  . 
\ ; outilaimen ( 1993 ) designed an English noun phrase recognition tool - - ~ NPT bol  . Abney ( 1997 ) applied both rule-based and statistics-based approaches for parsing chunks in English  . Due to the advantages of simplicity and robustness  , these systems can be acted as good preprocessors for the further colnplete parsing  . 
In tiffspaper , we will introduce our partial parsing a Pl ) roach for the Chinese language . We first proposed a shallow syntactic knowledge description : constituent boundary representation  . 
2. Constituent boundary description
The constituent boundary representation comes fl ' om the simplification of the complete parse Irees of the senlences  . It omits the constituenfl levels in parse trees and only keeps the boundary information of every word in different constituents  , i . e . it is at the left boundary , right boundary or middle position of a constituent  . 
l~ , vidently , if the input sentence has only one parselre e , i . e . without syntactic ambiguity , the constituent boundary position of every word in the sentence is clear and definite  . In the sense , the constituent boundary tag indicates the basic syntactic structure information in the sentence  . 
Separating them froll l the constituent smlcture tree and assigning them Io every word in the sentence  , we can form a special syntactic unit : word botm dao  , block ( WBB ) . 
Definition : A word boum la O , block is lhe combination o1' the word ( including part-of-speech information ) and its constituent boundary tag , i . e . 
wbb ~= <% , b ~ > , where % is the ith word in the sentence , b ~ can value 0 , 1 , 2 , which means % is at Illereafler , ' constiluent'represents all internal or root nodes in a parse tree  , i . e . phrase oF sentence tags . In our syslem , each consliluen ( must consist of two or more words leaf node in parser tree  )  . 
975 the middle , leftmost , or right--most position of a constituent respectively  . 
In the view of syntactic description capability , the WBBs defined above , the chunks defined by Abney ( 1991 ) and the phrases ( i . e , constituents ) defined in a parse tree have the following tealtions : WBBs < chunks < phrases 
Here is an example : ? The input sentence ( 10 words ) :  ( My brother gives him a book . ) ? It sparse tree representation (7 phrases ): 1 , , ,  \[ , ,~ \[ , ,~ @~ t'('J ~ ' \] \[ , ,4 l , ,~ ~( t T \] 41J ~ \[1'6 \[ , '~--$\]-I ~ \]1\] O\]?lts chunk representation ( 5 chunks ) : ? Its constituent boundary tep resentation ( 10 WBBs ) : <~ f ~ , l > < l'l < J , 0> <~' ~' A\] , 2> < )(\] , 1> < T , 2> < 41 g , 0> <-  , 1> </ l < , 2> < :1~ , 2> < o , 2>
The goal of the constituent boundary prediction is to assign a suitable boundary tag for every word in the sentence  . It can provide basic information for further syntactic parsing research  . 
The following lists some application examples : ? To develop a statistics-based Chinese parser  ( Zhou 1997 ) based on tile bracket matching principle ( Zhou and Huang ,  1997) . 
? To develop a Chinese maxinmm noun phrase identifier  ( Zhou , Sunad Huang ,  1999) . 
? The automatic inference of Chinese probabilistic context-five grammar  ( PCFG )   ( Zhou and throng 1998 )  . 
3. Local context templates
The linguistic intuition stellus that many local contexts may be useful for constituent boundary prediction  . For example , many function words in Chinese have their certain constituent boundary position in the sentences  , such as , most prepositions are at the left boundaries , and the aspectual particles (" le " , " zhe " , " guo ") at eat the right boundaries . Moreover , seine content words also show their pteferential constituent boundary positions in a special ocal context  , such as most adjectives arc at the right boundary in local context : " adverb+adjective "  . 
A tentative idea is how to use such simple local context information  ( including the part-of-speech ( POS ) tags and the number of Chinese characters ( CN ) ) todevelop an elTicient automatic boundary prediction algorithm  . Therefore , we defined the following local context templates ( LCTs ) : 1 ) Unigram POS template : t ~ , BPFL , 2) Bigram POS templates: , Left restriction : t ~_ , t  ~ , BI'I ; L~?Right restriction : Lt ~+ , , BPFL ~ 3) Trigram POS template : t~_~t~t ~+~ , BPFL , 4) Trigram POS+CN template : t ~_~+ cn~_~t ~+ cn , ~t ~+ ~+ cn ~+ ~ , BPFL ~ In tile above LCTs , t ~ is tile POS tag of the ith word in the sentence  , cn  ~ is its character number , and BPFL ~ is the frequency distribution list of its diffetent BP  ( boundary prediction ) value ( 0 , 1 , 2) under the local context restrictions ( I . CR ) ( the left and right word ) . 
Table 1 Some examples of the local context templates
I~Token
Unigram p , 39849476 .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 
bigram an , (left ) 5164 2007 bigram an , ( right ) 420 1216 0

A preposition is prior to at the constituent left boundary in Chinese  . 
A noun is prior to at the right boundary if its previous word is an adjective  . 
An adjective is prior to at llle left boundary if its nexl word is a noun  . 
Trigranlnnu . Jl ) l ' ~, A noun is prior loattile
POS 1   18   1496 right boundary if its previous word is a norm and its next one is a  .   .   .   .   .   .   .   .   .   . /ra_r_tial(De ) . 
Table shows some examples of LCTs . All these templates can be easily acquired fiom the Chinese treebanks or  (  . hme : e ~" s corpus annotated with constituent boundary tags  . 
Among these templates , some special ones have the following properties : a  ) TIV ~= ~ BPI ; L , \[ bl ) , \]>( z , b ) 3 bl ) , e10 , 21 , P ( bPALCR ) = BPFL , \[1%117 T ,  >  f3 where the total frequency threshok lo ~ and the BP probability threshold f ~ are set to  3 and 0  . 95, respectively . They are called the proiect cd templates ( PTs )   ( i . e . the local context template with a projecting BP w flue  )  . 
Based on the different PTs , we can design at htee-stage training procedure to overcome tile problem of data sparseness : tClll plales Oil the whole in slall COS in annolated COlm S  . 
Slago 2: I , carnlh cIrigral nP ( )S lonlphltos Oil the nonq~rojocled unigraili and bigraiilill Slall CCs  ( see i1o ? lsection for ill Ol ' Od clailod )  . 
Slago 3:\[ , carlll holfig faill PO , q-I-CN Io in philes oil I ho non-proj c clt ; dIrigraniP () ~ i liSlanccs . 
Therefore , only the useful trigranl templates
Call be learled.
4 . Automatic prediction algoritln nAfter getting lhcLCTs  , the auloillatic prediction algorithm becomes very simple:  1  ) to sot the protecting BPs based on the projected LCTs  ,  2 ) to select hebest l : IPs based on tilelion -procoted LCTs  . Sonic detailed in l~rmation will be discussed in the  12fllowing sections . 
4.1 Settile projecting lips
In this stage , tile refer cllceseqlielice loth eLCTs is: unigfanl ~ I  ) igralil ~ higranl POS tiigran lI > OS + CN , i . e . l : ronl therough roslriclion L crrstotile tight restriction L  ( \] Ts . This sequence is same with the LCT training procedure  . 
The detailed algoritln n is as follows:
Input : the position of the/ill word in the sentence  . 
Background : the LCTs learned'1 o111 corpus.
Output : the procoting BP of the word-if'fo / lnd  ; -1-otherwise . 
Procedure : ? Gellhclocal context of the iih word  . 
Ill If its unigran \] tonal ) late is a PT , thor return its projecting BP . 
? If its left and right bigram template satisfy the following conditions : > rE  , + TF , ~- Z SmFL , Ijl+ZBP FL , Ijl > a > p0 , / , ,I Lc#O : ( BPFL , Ijl+nS'FL . Ijl)/(77: ,  + 77= , <  ) > I~thorreturll this combined projecting l ~P  ( l ) l@?If its trigram POS template is a PT , then retul ' u its protecting BP . 
? If its trigram POS+CN template is a PT , then return its projecting BP . 
4.2 Select the best liPs
In this stage , tile reference sequence to the LCTs is : trigram POS+CN --> trigram POS "-> bigram  ---7 unigram . It's a backing-off model ( K . at z ,  1987) , just like the approach of Collins and Brooks ( 1995 ) for the prepositional phrase all achine ilt plot ) loin in English . The detailed alger finn is as follows : Input : the position of the ith word in lho sentence  . 
Background : lho LCTs learned from corpus.
Output : tile best BP of the word.
Procedure : ? Get the local context of tile ith word  . 
oFortile kth nlatched lrigram POS+CN to nl platos , if 77 , ' x > CZ , lhen rolH rllSelectBest ll >

? Sor the ruthnia tchodloft bigran l and nth matched righl : bigrain  , ) ~ Gellho Combined BI'FL = Blqq , + IHq < L , , lJ " TFc< , , , ,I , i , , , ' , l , <' , , , H . . . . . > 0, then rol ! . lril
Select lJest BP ( C < mibined Bl qqO.
I For lhekthn latched unigram templates , if 7"/~>0 , lhen relurn Select lJest Bl ~ (17 PlrLk ) . 
I17, olurnl(dol\]niltisat the loftt ) oundary).
The internal function Select Besl BP ( ) tries to select the best BP based on the fi ' equency distribution list of different BP wllue in LCTs  . It has two output modes : I ) single-output mode : only output the best t3t > with the highest fiequency in the LCT ;  2 ) nmltiple-outlmt mode : outpul the BPs salisfying the conditions : \[/'  , , , , , -P t , , . , ,\[<7, where 7= 0 . 2 5 . Experimental results 5 . 1 Training and test data The training data were extracted fl ' Olll two ditTerent parts of annotated Chinese corpus:  1  ) The small Chinese treebank developed in Peking University  ( Zhou , 1996b ) , which consists of the sentences extracted fiom two parts of Chinese texts :  ( a ) test set for
Chinese-Englisla machine transhltion systems , (b ) Singapore priiaary school text books . 
2 ) The test suite treebank being developed in Tsinghua University  ( Zhou and Sun ,  1999) , which consists of about 10 , 0 00 representative Chinese sentences extracted from a largescale Chinese bahmced corpus with about  2  , 000 , 000
Chinese characters.
The test data were extracted from the articles of People's Daily and manually annotated with divided into two parts :  1  ) The ordinary sentences . 
2 ) The sentences with keywords for conjunction structures  ( such as the conjunctions or special punctuation ' Dun Hao '  )  . They can be used to test the performance of our prediction algorithm on complex conjunction structures  . 
Table 2 shows some basic statistics of these training and test data  . Only the sentences with more than one word were used for training and testing  . 
Table 2 The basic statistics of training and test data . ( ASL = Average sentence length)




Sent . Word Char . ASL
Num . Num . Num . ( w/s ) 557 3644 2689 49211 . 56 7774 108542 173334 13 . 96 2780 68986 108218 24 . 82 1071 32358 51169 30 . 21 5 . 2 The learned templates After the three-stage l arning procedure  , we got four kinds of local context emplates . Table 3 shows their different distribution data , where the section ' Type ' lists the distribution of different kinds of LCTs and the section ' Token ' lists the distribution of total words  ( i . e , tokens ) covered by the LCTs . In the colm nn ' PTs ' and ' Ratio ' , the slash'/' was used to separate the PTs with total frequency threshold  0 and 3  . 
More than 66% words in the training corpus can be covered by the unigram and bigram POS projected templates  . Then only about 1/3 tokens will be used for training the trigram templates  . 
Although the type distribution of the trigram template shows the tendency of data sparseness  ( more than 70% trigram projected templates with total fi'equency less than  3  )  , the useful trigram templates ( TF > 3 ) still covers about 70% tokens learned . Therefore , we can expect hat them can play an important role during constituent boundary prediction in open test set  . 
5.3 Prediction results
In order to evaluate the performance of the constituent boundary prediction algorithm  , the followiug measures were used : 1 ) The cost time ( CT ) of the kernal functions ( CPU : Celeron TM3 66 , RAM : 64M ) . 
2 ) Prediction precision ( PP ) = number of words with correct BPs ( Cort BP ) total word number ( TWN ) For the words with single BP output , the correct condition is:
Annotated BP = Predicted BP
For the words with nmltiple BP outputs , the correct conditiou is:
Annotated BP ~ Predicted BP set
Tile prediction results of the two test sets were shown in Table  4 and Table 5  , whose first columns list the different emplate combinations using in the algorithm  . In the columns ' CortBP ' and ' PP' , the slash '/' was used to list the different results of the single and multiple BP outputs  . 
After analyzing the experimental results , we found : 1 ) The POS information in local context is very important for constituent boundary prediction  . After using the bigram and trigram POS templates , the prediction accuracy was increased by about 9% and 3% respectively . 
But the chmacter number information shows lower boundary restriction capability  . Their application only results in a slight increase of precision in single-output mode but a slight decrease in l nultiple-output l node  . 
Table 3 Distribution data of different learned LCTs
LCTsl-gram2-gram ( Left ) 2-gram ( Right ) 3gram ( POS ) 3-graln ( P+CN ) 



Ratio(tz=0/3) 40.68 71.13/40.81



Ratio 0x = 0/3) 31 . 41 50 . 68 / 50 . 28 1440 1008/567 70 . 00/39 . 38 171705 99443/98754 57 . 92/57 . 51 3105 2324 ! 713 74 . 85 / 22 . 96 50333 24280 / 21982 48 . 24 / 43 . 07 2553 1677 / 287 65 . 69/I 1 . 24 19098 5978 / 4079 31 . 30 / 21 . 36
Set the Projecting BPs templates used
I-gram 22408+2gram 46167
TWN\[CortBP
PP (%) 98.82 98.09 97.56 97.40
TWNCortBP 465 553 2876/
POS + 3gram 57 36 0

Selec the best BPs
PP (%) 70 . 62/ 73 . 84 71 . 01/ 77 . 55 72 . 90/ 80 . 53 70 . 40/ 77 . 18


CortBPPP (%) CT 81 . 95 89 . 14/ 11/15 91 . 30 92 . 68/13/I 194 . 19 68963 64034/ 92 . 85/ 64821 93 . 99 11/14 2 ) Most of the prediction errors can be attributed to the special structures in the sentences  , uchascol ! iunction structures ( CSs ) or collocation structures . I ) ue to the long distance dependencies among them , it's very difficult to assign the conect botm dary lags to the words in these structures only according to the local context emplates  . The lower overall precision of the test set 2 ( about 2% lower than tesl set 1 ) also indicates the boundary prediction difficulties of the conjunction structures  , because there are more CSs in test set 2 than in test set I . 
3 ) The accuracy of the multiple outlml results is about  2% better than the single OUtlmt results . But the words with multiple boundary tags constitute only about  10% of the tolal words predicted . Therefore , the multil ) le-output mode shows a good tradeoff between precision and redundancy  . It can be used as the best preprocessing data for the further syntactic parser  . 
4 ) The maximal ratio of the words set by prqjected templates can reach  80%  . It guarantees the higher overall pl'ecisioi L Table  5 Experimental results of the test set 2   5  ) Tile algoritlm l shows high efficiency . It can process about 6 , 000 words per second ( CPU : Celeron TM3 66 , RAM : 64M ) . 
5.4 Compare with other work
Zhou ( 1996 ) proposed a constituent boundary prediction algorithm based on hidden Marcov model  ( HMM )  . The Viterbi algorithm was used to find the best boundary path B ': 
B ' = argmaxP(W,T\[B)P ( B)
II = argmaxI'(CT , Ib , ) P ( t , iII , ,- , ) i - I where the local POS probability l ' ( C ~\[ b ) was computed by backing-off model and the bigram parameters :  . /(/~, t ,, b ) and . l(b~,t ,,\[ i + l)"
To compare its 19 erformance with our algorithm , the trigram ( POS and POS+CN ) information was added up to its backing-off model  . Table 6 and Table 7 show the prediction results of lhcHMM-based algorithm  , based on the same parameters learned from training set  1 and 2  . 
Table 6. Prediction results of the HMM-based __I
Templates Settile Project i ~
Used LTWN CoriBP
I-gram 9873+2gram20454+3-D'am24079
POS + 3gram 248 66

PP (%) 98.57 97.00 96.42
Selectile best BPs fWN CortgPI ) P ( % ) II 22342 15737/170 . 44/ 6593 74 . 27 11273 7856/ 70 . 44/ 8607 74 . 27 7384 5225/ 70 . 76/ 5777 78 . 24 6519 4525/ 69 . 40/ 4958 76 . 05 96 . 23


Cort BPI 25610/79 . t5/81 . 79 87 . 49/ 89 . 81 90 . 56/ 92 . 27 90 . 83/ 92 . 17\[\[CT6/54/43/68/6
TWNCortBPPP (%) C time 2gram 6896 360 90888 . 3 2144+3g-POS 6896 363 397 91 . 93 138+3g-P+CN 6896 3636 4992 . 29139 Table 7 . Prediction results of the HMM-based algorithm ( test set 2 ) + 2gram+3g-POS+3g-P+CN
TWNCortBPPP (%) 32358 2779285 . 89 32358 28918 89 . 37 32358 29030 89 . 72
C time surpassed the HMM-based algorithm in accuracy  ( about 1% ) and efficiency ( about 10 times )  . 
Another similar work is Sun (1999) . The difference lies in the definition of the constituent boundary tags : he defined them between word pair : w  ; /_ ? w ; ~ ;  , not for the word . By using the HMM and Viterbi model , his algorithm showed the similar performance with Zhou  ( 1996 )   ( using bigram POS parameters ) : ? Training data: 3051 sentences extracted from People's Daily . 
? Test data : I000 sentences.
? Best precision : 86.3% 6. Conclusions
The paper proposed a constituent boundary prediction algorithm based on local context templates  . Its characteristics can be summarized as follows : ? The simple definition of the local context templates made the training procedure very easy  . 
? The three-stage training procedure guarantees that only the useful trigram templates can be learned  . Thus , the data sparseness problem was partially overcome  . 
? The high coverage of differentypes of projected templates assures a higher overall prediction accuracy  . 
? The multiple output mode provides the possibility to describe different boundary ambiguities  . 
? The algorithm runs very fast , surpasses the HMM-based algorithm in accuracy and efficiency  . 
There are a few possible improvement which may raise performance flwther  . Firstly , some lexical-based templates , such as prepositions as left restriction , may improve performance further-this needs to be investigated  . The introduction of the automatic identifiers for some special structures  , such as conjunction structures or collocation structures  , may reduce the prediction errors due to the long distance dependency problem  . Finally , more training data is a hnost certain to improve results  . 

The research was supported by National
Natural Science Foundation of China ( NSFC )   ( Grant No .  69903007) . 

Abney S .  (1991) . " Parsing by Chunks " , In Robert Berwick , Steven Abney and Carol Tenny ( eds . ) Principle-Based Parsing , Kluwer Academic

Abney S .  (1997) . " Part-of-speech Tagging and Partial Parsing " , Ill Young S . Bloothooft G . ( eds . ) Corpusbased ntethods in language and speech processings  ,  118-136 . 
Collins M . and Brooks J .   ( 1995 ) " Prepositional Phrase Attachment through a Backing-OIT Model "  , In David Yarowsky & Ken Church ( eds . ) Proceedings of the third workshop on very large corpora  , MIT . 

Church K .  (1988) . " A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text  . " In : Proceedings of Second Conference on Applied Natural Language Processing  , Austin , Texas ,  136143 . 
Collins M . J .  (1996) . " A New Statistical Parser Based on Bigram Lexical Dependencies  . " In Proc . of ACL-34, 184-191 . 
Katz S .  (1987) . " Estimation of Probabilities from sparse data for the language model component of a speech recogniser "  . IEEE Transactions on ASSP,
Vol . 35, No.3.
Magerman . D . M .  (1995) . " Statistical Decision-Tree Models for Parsing " , In Proc . o1' ACL-95, 276-303 . 
Ratnaparkhi A . (1997) . '% linear observed time statistical parser based on maximum entropy models "  . In Claire Cardie and Ralph Weischedel ( eds . ) , Second Conference on EmpMcal Methods in Natural Language Proeessing  ( EMNLP-2 )  , Somerset , New Jersey , ACL . 
Sun H . L . , Lu Q . and Yu S . W . (1999) . " Two-level shallow parser for t , n restricted Chinese text " , In Proceedings of Computalional linguistics , Beijing:
Tsinghua University press , 280-286.
Votllilan mnA .  (1993) . " NP Tool , a delector of English Noun Phrases . "In : Ken Church ( ed . ) t ' roceedings of lhe Workshop on Very La , ge Corpora : Academic and lnduslrial Per speelives . Columbus , Ohio , USA , 4857 . 
Zhou Q . (1996a ) . '% Model for Automalic Prediction of ( ; hinesel ' hrase Boundary I , ocation " , Zhou Q . (1996b ) . Phrase Bracketing and Annotating on Chinesel , anguage Corpus . l~h . l ), l ) is sertalion,
Peking University.
Zhou Q . (1997) " A Statistics-Based Chinese Parser " , Inl ; roc , of lhe Fiflh Wol'kshol ~ on Very I , arge
Corpora , 415.
Zhou Q . and l\]u ang C . N .   ( 1997 ) "A Chincse syntaclic parser based on bracket malehing principle "  , 
Communication fCOId PS , 7(2), #97008.
Zhotl Q . all dHt lang C . N .  (1998) . " Anhll'crence Approach for Chinese Probabilistic Con/exl-FreeGramnmr "  , Chinese . Iournal of Computers , 21(5), 385-392 . 
Zhou Q . and Sun M . S .  (1999) . " P , ui Ma Chinese Trecbank as the lest suite for Chinese parser "  , In
Key-Sun Choi & Young-Soog Chae ( cds.)
Proceedings of the workshop MAI , '99, Beijing .  32-37 . 
Zhou ( , ) . , Sun M . S . and ltuallg C . N . (1999 ) " Attlonmlically Identify Chinese Maximal Noun Phrases "  , Technical Report 99001 , Slate Keyl ~ ab . o1' Intelligent Technology and Syslcm , % l ) epl , of Comlmter Science and Technology , Tsinghua


