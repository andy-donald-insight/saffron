Automatic Thesaurus Generation through Multiple Filtering 
KyoKAGEURAt , Keita TSUJI *, and Akiko , N . AIZAWA *
? National Institute of hffonnatics
2-1 . -2 Hitotsubashi , Chiyoda-ku , Tokyo , 101-8430 Japan
E-Mail : kyo , akiko@nii.ac.jp
t Graduate School of Education , University of Tokyo , 7-3-1 Hongo , Bunkyoku , Tokyo , 113 JapanE-Mail : i 34 188 hn-unix . cc . u-tokyo . ac . jp
Abstract ; 11 , this paper , we propose a method of gen-(' , rating bilingual keywordeh . lsters or thesauri from parallel or comi . m , able bilingual corpora . 
The method combines nmr phologicalnd lexical processing  , bilingual word aligmnent , and graph-theoretic cluster generation . An experiment shows that the method is promising . 
1 Introduction
In this paper , we propose a method of auto-matte bilingual thesaurus generation by a combination of methods or multiple tiltering  . The procedure consists of three modules : ( i ) a morphological and lexical processing module , ( it ) a translation pair extraction module , and ( iii ) a cluster generation module . The method takes parallel or comparable corpora as input  , and produces as outlmt bilingual keyword clusters with a reasonable computational cost  . 
Our aim is to construct domain-oriented bilingual thesauri  , which are much in need both for cross-language IR and t br technical tr~msla-tors  . We assume domain-dependent parallel or compar M ) le corpora as a source of inibrmation , which are . abundant in case of Japanese and English . 
The techniques used in each module are reasonably well developed  , including statistical word alignment methods . I to we ver , there remain at le . ast three problems : ( i ) ambiguity of multiple h Nmx combinations illan aligmnent  , which cannot be resolved by purely statistical methods  , ( it ) syntagmatie unit mismatches , especially in such cases as English and Jal ) anese , and ( iii ) difficulty ill final cleaning-up1 . 
In this paper , we show that the proper combination of the above modules can be useful especially for resolving the cleaning-u problem and can produce good results in bilingual ellis-ter or thesaurus generation  . 
2 Method
The procedure for thesaurus generation consists of the following three main nlodules  . 
(1 ) Morphological and lexical processing module : keyword milts  2 for English and Japanese are extracted separately . 
(2 ) Translation pair extraction module : statistical weighting is applie  . d to a corpus which has been through the morlhological nd lexical processing module  . The ailn of this stage is not to determinem dque translation pairs  , but to restrict translation candidates to a reasonable extent  . 
(3 ) Cleaning and cluster generation module : a bilingual keyword graph is constructed on the basis of " the pairs extracted at translation pair extraction module  , and a graph-theoretic method is applied to the keyword graph  , to generate proper keyword clusters by removing erroneous links  . 
If we want to obtain a clean lexicon , minor translation variations tend to be omitted , while many errors would be included if we want to retain minor variations  . 
2 The word ' keyword ' implies words that are impof tant with respect to documents or domains  . In this paper , we use the word for convenience , roughly in the 81une se~lseas " content-bearing words " . If necessary , a module of keyword orter in weighting ( e . g . Fl'antzi $? Ananiadou 1995 ; Nakagawa & Mort 1998) can be incorporated easily . 
397 2 . 1 Morphological gz lexical processing At this stage  , basic lexical units or keyword candidates are extracted  . We separately extract minimum or shortest units and max in nlmor longest complex units as syntagmatic units for keyword candidates  . So two outputs are pro-duccd from this module , i . e . a bilingual keyword corpora of minimum units and another of maximum units  . 
The processing proceeds as follows: ( a ) Morphological analysis First , the cortms is morphologically analysed and POS -tagged  . Currently , JUMAN3 . 5  ( Kurohashi ~ z Nagao 1998 ) is used for Japanese and LT_POS/LT_CHUNK ( Mikheev 1996 ) is used for English . 
( bl ) Extra et lon of minimum units
Minimum units in English are simply defined as non -flmctional simple words extracted from the output of LT_POS  . Minimum meaningful units in Japanese are defined as : C_Prefix *  ( C_A dv lC_A djl N ) CSuffix * where C_ indicates that the unit should consist of either Chinese characters or Katakana  3   . 
( b2) Extraction of maximum units
Maximum complex units for English are the units extracted by LT_CHUNK  , with some adhoc modifications . 
Maximum complex units fin ' Japanese are defined by the following basic pattern  , ^ C_Adj * ( C_AffixlC_tdvlC_Adj\[N ) + where ^ C means that the unit should begin with either Chinese character or Katakana  . The pattern remains deliberately coarse , to absorb errors by JUMAN . Coarse patterns with simple character type restrictions produce better results than grammatically well-defined syntagmatic patterns  . A separate stop word list for affixes is also prepared together with an exceptional treatment routine  , to make the Japanese units better corresl ) ond to English units 4 . 
After these processes , two corpora , one consisting of minimum units and the other of max-3 In addition , we have made a few adhoc rules to screen out some consistent errors produced by the morphological analysers  . 
4 For instance , the Japanese suffix ' Ill ' is eliminated because it corresponds in most cases to the English word ' for '  , which tends to be excluded fi'om chunks made by

imum units , are created.
Intermediate constituent units are not extracted , because their inter-lingnal unit correspondence is less reliable  . Also , many important intermediate units of longer complex units appear themselves as an independent complex unit in a large domain-specific corpus  , and , even if they do not , intermediate units can be extracted on the basis of minimum and maximum translation pairs if necessary  . 
2 . 2 Extraction of translation candidates The module for extracting translation candidate pairs consists of statistical weighting and postprocessing  . These are applied to the data of n finimum units and maximum units separately  . 
After that , the two data are merged to make input for the cluster generation module  . 
( a ) Statistical weighting
Many methods of extracting lexical translation pairs have been proposed  ( Daille , Gaussier & Langd 1994 ; Eijkt 993 ; Fung 1995 ; Gale ? ~ Church 1991 ; Hiemstra 1996 ; ltull 1998 ; Kupiec 1993 ; Melamed 1996 ; Smadja , McKeown & Hatzivmssiloglou 1996) . Though it , is ditficult to evaluate the performance of existing methods as they used it ferent corpora for evaluation  5   , the performance does not seem to be radically different  . We adopted loglikelihood ratio ( Dan-ning 1993) , which gave the best pert brmance among crude non -iterative methods in our test experiments  6   . 
( b ) Postproeessing filter
As the output of statistical weighting is simply a weighted list of all English and Japanese cooccurring pairs  , it ; is necessary to restrict translation candidate so that they can beef-t + ctively used in the graph -theoretic cluster generation module  . In addition to restricting possible translation pairs  , it is necessary to determine unique translation pairs for hapax legomena  . We use both macro - and micro-filtering heuristics to restrict translation candidates  . 
' ~ A common test bed exists for French-English alignment  ( Veronis 1996-99 ) but not for JapaneseEnglish . 
6At the time of writing this paper , we have finished a preliminary comparative xperiments of various methods  , among which the method proposed by Melamed ( 1996 ) gave by far the best result . We are thus planning to replace this module with the method proposed by Melamed  ( 1996 )  . 

Two macro heuristics , applied to the overall list of pairs , are defined , i . e .   ( i ) a proper translation should have a statistical score higher than the threshold X s  , and ( ii ) a keyword should have maximally Xc translations or X px token frequer t cy when the frequency is less than X c  . 
Microheuristics uses the information within each alignment  ; we assume that a keyword in one language only has one translation within an alignine ntr  . Selecting unique pairs in each alignment is achieved by recursively taking a pair with tile highest score within an alignment  , ead ~ time deleting other pairs which have the same English or Japanese elements  8   . 
After this process , the data of nlininmm units and maximum units are merged  , which constitutes input for the , next stage . 
2 . 3 Graph - theoret i c c lus ter generat ion Up to this stage  , the cooccurrence in for lna-tion used to extract pairs has the depth of only one  . In order to eliminate erroneous translations , we re-organise the extracted pairs into graph and use multilevel topological information by applying tile graph-theoretic method  . 
For exi ) lanation , let us assume that we obtained the data in Table 1 fi'om the previous module ~ us an input to this module  . 
Firstly , the initial ke!j word graph is constructed , where each node represents an English or JaI ) an ese keyword , and a link or edge repre-se . nts the pairwise relation of corresponding keywords  . W ( ' define the capacit ~ jor strength of a link by the frequency of occurrence of the pair in the corpus  , i . e . . the nmnber of al ignments in which the pair occurs  9   . Figure 1 shows the This is not true for longer alignment units such as full texts  . However , this will apply to parallel titles and abstracts which are readily available  . Many lexical alignment method starting fi'om sentence-levd aligmnents assume this or some variations of this  . 
Many maximum unit pairs in fact have the same score  . We used the arithmetic mean of the constituent minimum units to resolve aligmnent ambiguity  . 
9 The score of likelihood ratio is a possible alternative for linke ai  ) a city , but the result of a preliminary experiment was no better  . In addition , after selecting pairs by threshold , whether a pair constitutes a proper translation or not is not a matter of weight  , because threshold setting implies that all pairs above that are regarded as correct  . So we adopt simple frequency as the link capacity  . I to we ver , we notice a lack of at finity / ) etween the Japanese keywords English keywords frequency-U--b ~ information retrieval  1 g---q--b ? keyword 39   5-  --~  . 7 . b ~ . .~ information retrieval 1 5- 4 ~ X b ~, . ' ~' ~ text retrieval 65-4-XI-~'~ . ~ text search 3
J ~> 1 t ~ . ~ rll/-~iiii " keyword1 ~ J . "~ . l ' ~ ~ . ~ . " ~" ~ information retrieval 1 '\] ~ ifi ' ~ information gathering 4 ' l'~N ~ information retreival ti ' ~~' ~ information retrieval  320   '1~   N~'5  . ' ~ information search 5 t ~ ~ fl\[?~J Sinibrmation gathering 6  ' \]  '~11~ information retrieval 1 ~ i ~ t ; ~' ~ bibliographic search 1 ~ iJ ~ t t ~ document retrieval 11  ~ ~: ~ , ~ "- . 4~ document retrieval 19 ~' ~ text retrieval 1 Table 1  . Input exanlple for cluster generation i if itial keyword graph made from t  , he data in Table 1 . The task now is to detect and remove erroneous links to generate independent graphs or clusters consisting of I  ) roper translation pairs 1? . 
in for nlalion galhcrillg O , ihlie , t , llq phicr , ' uie ~ td ) Figure I . Example of initial keyword graph The detection algorithm is based on the simple principle that sets of links  , which decompose a connected keyword cluster into disjoint subclusters when they are removed fronl tile original cluster  , are candidates for improt ) er translations . In graph theory , such a link set is called an edge cut and the edge cut with the minimal total capacity is called am  , inimum edge cut . A minimum edge cut does not necessarily imply a single translation error  . An efficiental-statistical alignment method we used here and the deft-nition of link cat  ) a city , which is currently under examination and will be iml  ) roved by renewing the alignment module . 
m This approach is radically different fl'om statistically oriented word clustering  ( Deerwestert . al 1990 ; Finch 1993 ; Grefenstette 1994 ; Schiitze & Pedersen 1997 ; Strzalkowski 1994) ; this is why we use the word ' ( : luster generation ' instead of ' clustering ' . 

I ~ e w o r U t 2 ~ , f % 1 : k y e w o ~ r d . ~)/;/#o ; ' . ' , ' iI ~ ,   , / ~0 to xt search = F-'/--4"~J ""~" z/ , t ,   ,   ,   ,   ,   ,   , ~D // : r ~ z , k t ~? ~ .   .   .   . coreclI~tev ~ . ~I(le'tl relR~i(ID~Y'v " tg'~l,~t#t ~' .   . ~; J *\/\\ x .   .  ~', . J . Y ~??; t,'(Ivld . '-, r ?, kJI ~ ln foll ~ latlon rot r~lva/?)Id ~ . ~u . lent in Jorln ( ttion . I ~ .  ~20~ .  \  . A ~1 ~ ; ' etrt~:val retrievt dl/Vxl\~f ~ l~\19 r . / i , , , ~ . ~", ~ u ~ nlrot fiova , x \ 6 ~ i foah . /, II\Ithihhor . pldcTl~iz't ~ fl*:':- . . .  "6 . ./ .  \  . ", f : . infon ' hz diongal h'el~ng ~\~/ / ~ ~ ~ ~ ~ ~~ bibl ~graphic sear-ch'u !  ( a )   ( b ) Figure 2 . Steps of graph-theoretic cluster generation ( c ) gorithm exists for minimum edge cut detection ( Nagamochi 1993 )  . 
Our procedure first checks links that should not be eliminated  , using the conditions : ( i ) the frequency is no less than Na ,   ( ii ) the Japanese and English notations are identical , or ( iii ) either of the Japanese or English expressions have only one corresponding translation  ( Figure 2 ( a )  ; it is assumed that N~=N/~=Ne=3) . 
Secondly , core keywords whose fi'equency is no less than NZ are checked  ( Figure 2 ( b ) ) . This is used for the restriction that each cluster should include at le~tone core keyword  . Lastly , edge cuts with a total capacity of less than Ne are detected and removed  ( Figure 2 ( c ) ) . This procedure is repeated recursive ty until no fllrther application is possible  . Figure 3 shows the state after these steps are applied . 

I .
" ' "  .   .   .   .   .   .   .   .   .   . : L .   .   .   .   .  ~  .   .   .   .   .   .   .   .   .   .   .  2  .   .   .   .   .   .   .   .   .  ~'-
Figure 3 . Generated clusters 3 Experiment 3 . 1 Settings and procedures
We applied the method to Japanese and
English bilingual parallel corpus consisting of 25534 title pairs in the field of computer science . Table 2 shows the basic quantitative information after morphological nd lexical filtering was applied  . 
Mininmmunits
Japanese Token:178091 Type : 14938
English Token : 154554 Type : 12634
Maximum units
Japanese Token : 89742 Type : 38813
English Token:80018 Type : 41693
Table 2. Basic quantity of the data
In the pair extraction module , the threshold Xs'was set to 1011 . The parameter X cw~s set to 10 and X p to 0 . 5 . As a result , 28905 translation candidate pairs were obtained , with 24855 Japanese and 23430 English keywords . 
Of these ,   20071 pairs occurred only once and 3581 only twice . The most frequent pair occurred 3196 times in the corpus .  8242 (28 . 5%) were minimum unit pairs , and 20663 (71 . 5%) were maximum unit pairs . 
Table 3 shows the number of keywords which had N translations  . On average , a Japanese keyword had 1 . 16 English translations , while an English keyword had 1 . 23 Japanese translations . 
NJap . Eng . N , lap . En.

Table 3 . Number of translation six This is purely heuristic  . Minimum units and maximum units are given ditferent scores  . But only 3 pairs below this threshold were proper translation pairs in  100 random samples of minimum unit pairs , and 5 in 100 samples of max in mn~units . 

Evaluating recall and precision on the basis of 100 rand on fly selected title pairs , which consisted of 778 keyword token pairs , the precision token wise was 84 . 06%  ( 654 correct translations ) and the recall was 87 . 08% (654 of 751 correct pairs) . Typewise precision was 81 . 65% (543 correct of 665 pairs) . 
The initial keyword graph generated fi'om these 28905 translation candidates consisted of 19527 independent subgraphs , with the largest cluster containing 2701 pairs ( i . e .  9 . 3% of all the pairs ) . The cluster generation method was applied with parameters Na =:  4  , Ne = 10 and N/~=1)2 . As a result , 893 translation pairs were removed , and 20357 bilingual clusters were generated . The maximum cluster now contained only 64 pairs . Table 4 : shows the number of clusters by size given by number of pairs  . 
size no . of clusters size no . of ( : lusters 1   16693   59   322   2   2354   1019   52   3   504   20-64   22   4   410 
Table 4 . Number of chlsters by size 3 . 2 Overal l evaluation The result was manually evaluated fi'om two points of view  , i . e . consistency of clusters and cocrectness of link removal  ~3   . 
(1) ribcheck the internal consistency , clusters were classified into three groul ) Sbysize , and were separately evaluated . 2000' small'clusters , consisting of only one pair , were randomly sampled and evaluated as ' correct '   ( c )  , ' more or less correct '( m ) or ~ wrong'(w ) .   4t0 medimn size clusters consisting of 29 pairs and all the 74 large clusters consisting of 10 or more pairs were evaluated as ' consistent ' ( c : consisting only of closely related keywords )  , ' mostly consistent ' ( Ill : consisting mostly of related keywords )  , ' hybrid ' (   t1: consisting of two or more different keyword groups :  11  ) or q ) ad ' ( w )  . Table 5 shows the result of the evaluation . The general performance is very good , with more or less 80% of the clusters being meaning flfl . 
12This is again determined heuristically . For an examination of the effect of parameters , ee Aizawa & Kageura ( toapl ) ear ) . 
~3 The evaluation was done by the first author . Currently no crosschecking has been carried out . 
For small clusters , the performance was separately evaluated for minimuln and maximuln refit pairs  . Note that the ratio of maximum unit pairs is comparatively higher in the small cluster than the overall average  . Most pairs ewfluated as partially correct , as well as some wrong pairs , suffered from mismatch of the syntagmatic units . 
cmw total
Small 1389 370 241 2000 (69 . 5%) (18 . 5%) (12 . 1%) (100%) milfimum 2882 669383 (75 . 2%) (6 . 8%) (18 . 0%) 19 . 2% maximum 1101 344 172 1617 (68 . 1%) (21 . 3%) (10 . 6%) 80 . 9% cm hw
Medium 116 1483 2104 (29 . 0%) (37 . 0%) (8 . 0%) (26 . 0%)
Large 818435 ( lo . 8%) (24 . 3%) (58 . 1%) (&8%) Table 5 . Evaluation of internal consistency 73% of tile medium sized clusters were ' cor-reel )  , ' mostly correct ' or ' hybrid ' . Among the ' l nostly con'ect ' and ' hybrid ' clusters  ,  97  ( 91 and 6 respectively ) were mainly caused by the mismatch of the units . For instance , in the case : Kid , i ~ iN'fL , ~ i ~@ , optimization , optimal , optimisation , optimum , network optimization , the last English keyword has the excess unit ' network '  . Other ' mostly correct ' and ' hybrid'chtsters were due to the l  ) roblem of corpus frequencies . 
Among the large clusters , more than half were qlybrid'14 . Among the h nostly correct ' and qly brid ' large ( ; lusters , only 8(3--t-5) were due to unit mismatch , while 53 ( 15+38 ) were due to quantitative factors . This shows a striking contras to the medium sized clusters  . Large hybrid clusters tended to include ln any common word pairs which occur fi'equently  . For instance , in the largest chlster , ') x ? . z , system ' (3196) , ' lJ~l ~ development ' (1097) , ' ~ tki ~\] design ' (1073) , and ' NiLen viromnent ' ( 890 ) are included due to indirect associations . The tbllowing are two examples of hybrid clusters  , whose hybridness comes fi : om quantitative factors and unit mismatches respectively : Example  1:  ~  fg2:/~  . C6/~tJ/4 ) -x "' ) /overview/outline/summary/smmnarization /overall  14 And most of the subclusters in these hybrid clusters are ' mostly correct '  . 
4 01 / pattern/patterns/patten/patterm matching In the first case  , the ' overall ' group and the ' summary ' group are mixed up  . In the second case , the mismatch of syntagmatic units is caused by borrowed words  . In fact , many errors caused by the mismatch of syntagmatic units involve borrowed words written in Katakana  . 
(2 ) To look at the perfbrmance of graph-theoretic cluster generation  , we exanfined the removed pairs fl ' om two points of view  , i . e . the correctness of link removal and the internal consistency of clusters generated by link remowf l  . 
For the former , we introduced three categories for evaluation : mismatched pairs correctly removed  ( c )  , proper translation pairs wrongly removed ( w ) , and pairs of related meaning removed ( p ) . The consistency of newly generated clusters were evaluated in the same manner as above  . 
cpw total cc 90 (10 . 1) 53 (5 . 9) 39 (4 . 4) 182 (20 . 4) cm 148 (16 . 6) 56 (6 . 6) 32 (3 . 6) 236 (26 . 4) ch 96 (10 . 8) 20 (2 . 2) 6 (0 . 7) 122 (13 . 7) mm 44 (4 . 9) 29 (3 . 3) 30 (3 . 4) 103 ( tl . 5) mh52 (5 . 8) lS(1 . 5) 5 (0 . 6) 70 (7 . 8) hh 30 (3 . 4) 3 (0 . 3) 3 (0 . 3) 36 (4 . 0) xc42 (4 . 7) 9 (1 . 0) 9 (1 . 0) 60 (6 . 7) xm28 (3 . t ) 8 (0 . 9) 20 (2 . 2) 56 (6 . 3) xh8 (0 . 9) 2 (0 . 3) 5 (0 . 6) 15 (1 . 7) xx4 (0 . 5) 1 (0 . 1) 8 (0 . 9) 13 (1 . 5) all 542 (60 . 7) 194 (21 . 7) 157 (17 . 6) 893 (100)
Table 6. Evaluation of removed links
Table 6 shows the result of evaluation of all the 893 removed pairs . ' c''p'and'w ' in the top row indicate types of removed links  , and ' cc' , ' cm'etc . in the leftmost column indicate internal consistencies of two clusters generated by link removal  . A total of 157 (17 . 6% ) of the removed links were correct links wrongly removed  , but among them , 115 links did not produce'bad'clusters . If we consider them to be tolerable , only 42 removals (4 . 7%) were fatal errors . 
By exanfining the renloved links , wc found that the links removed at the higher edge capacity included more wrongly removed pairs  . For instance , among 142 edges removed at capacity 4 ( which is the maximum deletable values et by N , ~) , 41 or 28 . 9% were wrongly removed correct translations , while among 288 links removed at capacity l , only 15 or 5 . 2% were correct translations . 
4 Discussion
From the experiment , we have found some factors that affect performance  . 
(1 ) Many errors were produced at the stage of extracting keyword milts  , by syntagmatic mismatch . A substantial nmnber of them involved Japanese Katakanakey words  . There ibre , in addition to the general refinement of the morphological processing module  , the perfbrmance will be improved if we use string prox in fity information to determine syntagmatic units  15   . 
(2 ) We expect that some errors produced by statistical weighting and filtering could be removed by applying stemming and orthographic normalisations  , which are not flflly exploited in the current implementation  . Looking back from the cluster generation stage , frequently occurring keywords tend to cause problems due to indirect associations  . At the time of writing , we are radically changing the statistical alignment module based on Melamed  ( 1996 ) and incorporating iterative alignment anchoring routine so that the method can be applied not only to titles but also to abstracts  , etc . Used in conjunction with string proximity and stemming inforina-tion  , we might be able to retainn finor va . riations properly . 
(3) At the cluster generation stage , we observed that correct links tend to be wrongly removed for higher capacities of edge cut  . In the current implementation , the parameter values remain the same for all the clusters  . Performance will be improved by introducing a method of dynamically changing the parameter w -dues according to the cluster size and the frequencies of their constituent pairs  . 
5 Conclusion
We have proposed a method of constructing bilingual thesauri automatically  , fl ' omparallel or comparable corpora . The experiment showed that the performance is fairly good  . We are currently improving the method further , along the lines discussed in the previou section . Further experiments are currently being carried out  , using the data of narrower domains ( e . g . artificialls This can also be used for resolving hapax ambiguity  . 
402 intelligence ) as well as abstracts instead of titles . 
At the next stag ( . ' , we are 1 ) lanning to evaluate the method fi'om the point of view of performance of generated clusters in practical applications  . We are currently planning to apply the generated clusters to query expansion and user navigation in crosslingual Il  . , as well as to online dictionary lookup systems used as translation aids  . 

This research is a part of the research project " A Study on Ubiquitous Information Systems tbr Utilization of Highly Distributed Information F Lesources "  , fimded by the Japan
Society for the Promotion of Science.
References\[1\]Aizawa , A . N . and Kageura , K .   ( to appear ) " Agrai ) h-/ ) as edal ) proach to the autoinatic generation of multilingual keyword clusters  . " In : Bouligmflt , D . , Jacquemin , C . and l't Iomme , MC . ( eds . ) Recent Advances in Computational 7 ~ rminology . Amsterdam : John Benjan fins . 
\[2\] Dagan , I . and Church , K .   ( 1994 ) " Termight : Identifying and translating technical terminology  . " Prec . of the Fourth ANLP . p . 34 40 . 
\[3\] Daille , B . , Gaussier , E . and Langd , J . M .   ( t994 ) " Towards automatic extraction of monolingual and bilingual terminology  . " COLING'9 ~ . p . 

\[4\] Deerwester , S . , Dumais , S . T . , Furnas , G . W . , Landauer , T . K . and Harshman , R . (1990) " Indexing by latent semantic analysis . " JASIS . 
41(6), p . 391407.
\[5\] Dunning , T . (1993) " Accuratereel , hods for the statistics of surprise and coincidence  . " Computational Lin . quistics . 19(1), p .  61 74 . 
\[6\]Eijk , van der P .   ( 1993 ) " Automating the acquisition of bilingual terminology  . " Prec . of the 6th
EACL . p . 11.3-119.
\[7\]Finch , S . P . (1993) Finding Structure in Lan-9ua . q e . PhD Thesis . Edinbourgh : University of

\[8\] Frantzi , K . T . and Ananiadou , S .   ( 1995 ) " Statistical measures for terminological extraction  . " Proc . of 3rd Int'l Conf . on Statistical Analysis of Textual Data . p .  297-308 . 
\[9\] Fung , P .   ( t995 ) " At ) attcrn matching method for finding noun and proper noun translations fi'om noisy parallel cort  ) ora . ." Proe . of 33rd
ACL . p . 233236.
\[10\]Gale , W . A . and Church , K . W .  (1991 . ) " Idem tifying word correspondences in parallel texts  . " Proc . of DARPA & ~ eech . and Natural Lan . quwe
Workshop . p . 152-157.
\[11\]G refenstette , G .   ( 1994 ) Explorations in Automatic Thesaurus Discovery . Boston : Kluwer

\[12\]tfiemstra , D .   ( 1996 ) Using Statistical Methods to Creata Bilingual Dictionary  . MSc Thesis,
Twcnte University.
\[13\]Ifull , D . A .   ( 1998 ) " A practical approach to terminology aligmnent . " Computerm'98 . p .  1---7 . 
\[14\] Kitamura , M . and Matsumoto , Y .   ( 1997 ) " Automatic Extraction of Translation Patterns in Parallel Corpora  . " Transactions of IPSJ . 38(4), p .  727- 735 . 
\[15\]Kupiec , J .   ( 1993 ) " An algorithm for finding noun phrase correspondences in bilingual colpora  . "15" oc . of 31st ACL . p . 17--22 . 
\[16\] Kurohashi , S . and Nagao , M . (1998) Japanese Morphological Analysis System . luman versio T~3 . 5 User's Mawaal . Kyoto : Kyoto University . "\[17\] Melamed , I . D .   ( 1996 ) " Automatic onstruction of clean broad coverage translation lexicons  . "2nd Conference of the Association for Mach , in e Translation in the Americas . p .  125-134 . 
\[18\] Mikheev , A .   ( 1996 ) '% earning pro:t-of-speech guessing rules from lexicon  . " COLING'96, p . 

\[19\] Nagamochi , H . (1993) " Minimum cut , in a graph . "In : Fujisige , S . ( ed . ) Discrete Structure and Algorithms H ( Chapter 4) . Tokyo:

\[20\] Nal ~ Gawa , H . and Mori , T .   ( 1998 ) "Nested collocation and COml ) ound noun for term extraction . " Computerm'98 . p 64 70 . 
\[21\] Schitze , It . and Pedersen , J . O .   ( 1997 ) "A cooccurrence-based thesaurus and two applications to information retrieval  . " Information Processing and Management . 33(3), I) . 307-318 . 
\[22\]S madj a,F . , MeKeown , K . R . and Hatzivassiloglou , V .   ( 1996 ) " Translating collocations for bilingua lexicons : A statistical apt  ) roach . " Computational Linguistics . 22(1), p .  \]-38 . 
\[23\] Strzalkowski , T .   ( 1994 ) " Building a lexicM domain map from text corpora . " COLING'94, t) . 604-610 . 
\[24\] Veronis , J .   ( 1996- ) " ARCADE : Evaluation of parallel text alignment systems  . " httl ): // www . lpl . univ-aix . fi'/projects/arcade/\[25\]Yonezawa , K . and Matsumoto , Y .   ( 1998 ) " Zosh intekitaiouz ukeniy or utaiyakute kisuto lm rano hol?ya kuhyou geno cyu syutu  . " Proe of the \] #h , Annual Meeting of th . eAssociation for NLP . p .  576-579 . 

