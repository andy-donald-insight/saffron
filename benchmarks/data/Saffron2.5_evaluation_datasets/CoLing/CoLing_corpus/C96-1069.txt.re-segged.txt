An Automatic Clustering of Articles Using Dictionary 
Definitions
Fumiyo FUKU MOTO Yoshimi SUZUKI ~
Dept . of Electrical Engineering a , nd Computer Science , Yalnanashi University
4-3-11T ~ keda , K of u 4(10, b ~ pan
fukumoto Oskye , ysuzuki ~ suwaj . esi . yamanashi , ac . jp

In this paper , we propose a statistical approach for clustering of art Mesusing online dictionary definitions  . One of the characteristics of our approach is that every sense of word in artMes is automatically disambiguated using dictionary definitions  . The other is that in order to cope with the problem of a phrasal lexicon  , linking which links words with their semantically similar words in articles is introduced in our method  . The results of experiments demonstrate the effectiveness of the proposed method  . 
1 Introduction
There has been quite a lot of research concerned with automatic clustering of articles or autonmtic identification of selnantically similar articles  ( Walker ,  1986) , ( Guthrie ,  1994) , ( Yuasa ,  1995) . Most of these works deal with entirely dif ferent articles  . 
In general , the 1) rot ) l(m : that the same word ca: , be used differently in different sul ) jeet domains is less problematic in entirely ditferent art Mes  , such as ' weather forecasts ' , ' medical rel ) or ts ' , and ' computer manuals ' . Because these articles are characterised by a larger number of different words than that of the same words  . However , in texts from a restricte domain such as financial art Mes  , e . g Wall Street Journal ( WS , I in short ) ( Libernmn ,  1990) , one encounters quite a large number of pop ysemous words  . Therefore , polyseinous words of_ten hamper the I ) recise cla ~ ssification of art Mes , each of which belongs to the restricted subject do-nlai II  . 
In this paper , we report an experimental study for clustering of articles by using online dictionary definitions attd show how dictionary-definition can use effectively to classify articles  , each of which belongs to the restricted subject domain  . We first describe a method for disambiguating word senses in articles based on dictionary definitions  . Then , we present a method for classifying articles and finally  , we rel ) or t some ext ) eriments in order to show the effect of the method . 
2 Related Work
One of major approaches in automatic lustering of articles is based on statistical information of words in  , ~ rticles . Every article is characterised by a vector , each dimension of which is associated with a specific word in articles  , and every coordinate of the art Me is represented bytern : weighting  .   Tern1 weighting methods have been widely studied in iu fornmtion retrieval research  ( Salton ,  1983) , ( Jones ,  1972 ) and some of then : are used in an automatic clustering of articles  . Guthrie and Yuasa used word frequencies for weighting  ( Guthrie ,  1994) , ( Yuasa ,  1995) , and Tok unaga used weighted inverse document frequency which is a word frequency within the document divided by its fl'equency throughouthe entire document collection  ( Tokunaga ,  1994) . The results of these methods when al ) plied to articles ' c bussification task , seem to show its etfectiveness . However , these works do not seriously deal with the 1 ) roblem of polysemy . 
The alternative al ) l ) roach is based on dictio-nary's in fl ) rlnation as a thesaurus . One of major problems using thesaurus ( ' a tegories a . ssense rep-rese :: tation is a statistical sparseness for thesaurus words  , since they are nmstly rather uncommon words ( Niwa ,  1995) . Yu as a reported the experimental results when using word frequencies for weighting within large documents were better re-suits in clustering  ( lo ( ' unmnts as those when EDR electronic dictionary as a thesaurus  ( Yuasa ,  1995) . 
The technique developed by Walker also used ( lietionary's infornmtion and seems to cope with the discrimination of polysemy  ( Walker ,  1986) . 
He used the semantic odes of the Longmau Dictionary of Contemporary English in order to determine the subject donm in for a set of texts  . For a given text , each word is checked agains the dictionary to determine the semantic codes associ-ate  ( l with it . By accumulating the frequencies for these senses and then ordering the list  , of categories in terms of frequency , the subject matter of a phrasal lexicon , such as Atlantic Seaboard , New England gives a negative influence for clustering  , since it cannot be regarded ~ us units , i . e . each word which is the element of a 1 ) hrasa lexicon is assigned to each semantic ode . 
The approach proposed in this paper focuses on these l  ) roblems , i . e . 1) olysemy and a phrasal lexicon . Like Guthrie and Yuasa's methods , our approach adopts a vector representation , i . e . every article is characterised by a vector . However ~ while their ~ p proaehes assign each ( : oor ( linate of a vector to each word in artMes , we use a word ( noun ) of wt fich sense is disambiguated . Our disambiguation method of word senses is based on Niwa's method whMt use  ( l the similar it ; y1)et ween two sentences , i . e . a senteve e which contains a polysen mus noun and a sevtenee of dictionary-definition  . In order to cope with Walker's l ) rob-lem , for the results of disand ) iguation technique , semantic relativeness of words are cMeulated , and semantically related words are grout ) ed together . 
We used WSJ corpus as test artich , s in the experiments in order to see how our metho ( l can effectively classify art Mes , eacl ,  < ) f whi < : h be h ) ngste the restricted subject domain , i . e . WS . I . 
3 Framework 3 . 1 Word-Sense D isambiguat ion Every sense of words in art Mes which should be  ( : lustered is automatically disambiguated in advance  . Word-sense dism nl ) iguation ( WSD in short ) is a serious problem for NLP , and awlri ( ' tyofal ) l ) roaches have been 1 ) roposed for solving it ( Ih ' own ,  1991) , ( Yarowsky ,  1992) . 
Our disaln biguation method is based on Niwa's method which used the similarity  1  ) etween a sen-tenee containing at ) olysemous noun and a sen=tence of dictionary -definition  . Let x beat ) olyse-mous noun and a sentence X be X"???~ 3 :- n ~??? ~ a'-i ~ ~1:~   ~1:1   , ?"" ~ ilYn ~""'
The vector representation of X is
V(X ) = ~ V(xi ) where V(xi ) is
V(xi ) = ( Mu(xi , o ~), ..., Mu(xi , om))
Here , Mu(x , y ) is the v ' , due of mutual information proposed by ( Church ,  1991) . oj , . . . ,om ( We call them basic words ) are selected the 1000th most frequent words in the reference Collins English 
Dictionary ( Lil ) erman , 1990).
Let word x have senses sl , s2, . . . ,sp and the dictionary-definition of si be Ysi : "  "   , Y-n ,  ' "  , Y-I , Y , Yt , " " '  , Yn , """ The similarity of X and ~ i is measured t ) y the imterl ) roduct of their normalised vectors and is detined as follows : v  ( x ) ?vo ;   ) = Iv ( x ) IIvo % dl ( 1 ) We infer that the sense of word x in X is si if Hi're  ( X , ; i ) is maximnmal nongt ' ~ ,  . . . , ~ p . 
Giw : nm larticle , the procedure for WSD is applied to each word ( noun ) in an article , i . e . the sense of each noun is estimated using formula  ( 1 ) and the word is rel ) laced 1 ) y its sense . Tat ) le1 shows sam I ) le of the results of our disambiguation nn'thod . 
Tabh ~1: The results of the WSD l nethod
Input Amunber of major a Mines adopted continental a Mines '  . - . 
Output A number5 of major airlines ladopted cont inental2   airlines2   .   .   . _In Tal)le I , underline signifies polysenmus nolln . 
'() utlmt . ' shows that e a ( ' hnoun is rel ) laced l ) yasyml ) ol word which corresl ) onds to each sense of a word . We call ' Inlmt ' and ~( ) utput ' in Table 1 , mt ( rrigin alart Me and a new art Me , respectively . 
Tabh , 2: The definition of ' nnnt be r'~W\] . : mmd wr 2:
Iltllllber3;nltHl)er4:llllllt\])erS :
Every mmd ) eroccupies a unique position iv a sequence . 
He was lieu ( HIe of Ollrnlllll ) er.
Atelel ) hOn Cnuml ) er.
~h (" was nu Inberse Veltill tit (, ra(,c.
A largen mnber of people.
Table 2 shows the definition of hmml ) er ' in the Collins English , Dictionary . ' numl ) erl ' ~' nunt-l ) er 5' are symbol words and show different senses
Of ' llunlber'.
3.2 Linking Nouns with their
Semantically Similar Nouns
Our method for classification of articles uses the results of dismnbiguation method  . The problems here are : 1 . The frequency of ewwy disambiguated noun in new articles is lower than that of every polysemous noun in oriqinal articles  . For exaln-ple , the frequency of ' nulnber5' in Table 1 is lower than that of ' number ' t . Furthermore , some nouns in articles may be semantically similar with each other  . For example ,  '  num-ber5' in Table 2 and ' sum4' in Table 3 arc ah nost the saine sense . 
2 . A phr~s a lexicon which Walker suggested in his method gives a negatiw ~ influence for classification  . 
1If all ' mlmber ' are used ~ s ~ nunJ ) er 5' sense , the flequency of ' number ' is the same as ' numl ) erS' . 

Table 3: The definition of ' sum ' in the dictionarys tun1: 
Sllll 2:
Slll li3: sun l4:s unlS :
The result of the addition of num-~?l'S , lie or inoreeo hntlnsor rows of numbers to be added  . 
The limit of the first n terms of a converging in finite series as  , ~ tends to infinity . 
He borrow sell or lnolt S sluns.
The essence or gist of a matter.
Table 4: Pairs of nouns with Dis(vl , v2) w flues
BBK 0 . 125 share1 company 10 . 140 giorgio di 0 . 215 shares 2 share 20 . 262 share 2 corot ) any 10 . 345 new 3 yorkl In order to cope with these prol ) lems , we linked nouns in new articles with their semantically sim-ilar nouns  . The procedm'es for linking are the following five stages  . 
Stage One : Calculating Mu
The first stage for linking nouns with their se -mantic Mlys in filar nouns is to calculate Mu between noun pMrx and y in new articles  . In order to get a reliable statistical data , we merged every new article into one and used it to calculate Mu  . 
The results are used in the following stages.
Stage Two : Representing every noun as a vector The goal of this stage is to rel  ) resent every noun in a new article as a vector . Using ~ tterm weighting method , nouns iI a new article would be represented by vector of the form v =  ( 2 ) where wl is the element of a new art i ( : le and corresponds to the weight of the noun wl . In our method , the weight of wi is the w due of Mu between v and wi which is calculated in Stage One  . 
Stage Three : Measuring similarity between vectors Given a vector representation of nouns in new articles ~ sill for lnula  ( 2 )  , a dissimilarity between two words ( noun ) v l ,   v2 in an article would be obtMned by using formula ( 3 )  . A dissimilarity measure is the degree of deviat ion of the grout  ) in an n-dimension MEuclidean space , where ' n is the number of nouns which cooccur with t ~  1 and ' U 2   . 
Dis(vl , v2) = E ~ = , Ej ' ~= , (vlj-ffj )'2(a ) ~0 = ( f h ,  " '  ,   . q ; , ) is the centre of gravity and I . qI  is the length of it . A group with a smMler value of ( 3 ) is considered semantically less deviant . 
Stage Four : Clustering method
For a set of nouns Wl~'W2~"'' , w , ~ of a new article , we calculate the semantic devi ~ t tion value of all possible pairs of nouns  . 
Table 4 shows sample of the results of nouns with their semantic deviation values  . 
Iu Table 4 , ' BBK' shows the topic of the article which is tagging in the WSJ  , i . e . ' Buybacks ' . 
The value of Table 4 shows the semantic deviation vMue of two nol lnS  2  . 
The . clustering algorithm is applied to the sets shown in Table  4 and produced a set of semantic clusters , which are . ordered in the as ( ' ending order of their semantic deviation w dues . We adopted non-overlal ) ping , group average method in our clustering technique ( Jardine ,  1991) . The sample results of clustering is shown in Table  5  . 
Table 5: Chtstering results of ' BBK'0 . 125\[share1 company1\] 0 . 140 \[ giorgio di \] 0 . 215\[shares2shm'e2\]0 . 2 51 \[ sharel comp ~ myl stmre2   shares2\] The w due of T aMe 5 shows the selnantied eviation value of the cluster  . 
Stage Five : Linking nouns with their semantically similar nouns We selecte different  49 art Mes from 1988  , 1989 WSJ , and applied to Stage One ~ Four . From these results , we manuMly selected ( : lusters which are judged to be semantically similar  . For the selected chtsters , if there is a noun which belongs to several clusters  , these clusters are grouped together . As a result , each cluster is added to a sequential number . The sample of the results are shown in Tal)le 6 . 
Table 6: The results of Stage Five
Se(l . nlllil word l:"lUol'd2:
Iuol ~ d3: word 4: word 5:
Semantic Mly similar nouns bank 3 , banks 3emia da 3 , emm da4
Amerieanl , expresslco ., corp ., eompanyl ???
August , June , July , Sept . Oct.-..
new 2 york 2 eIn Table 4 , the rem'c some nouns which are not added to the number  ,  '1' ~ , , '5' , e . g . ' giorgio ',' di ' . 
This shows that for these words , there is only one meaning in the dictionary . 
408' Seq . hum ' in Table 6 shows a sequential numt ) er , ' wordl ' ,   . . . ,' word , ,' whi ( : h are added to the grou 1 ) of semantically similar nouns 3 . Tal)le 6 shows , for examl)le , ' new 2' and ' york2'arexemanti ( ' ally similar and for maphn~sal lexicon . 
3.3 Clustering of Articles
According to Table 6 , freqllen ( ' y of every word in new art Mesix counted , i . e . if a word in an e , w article t ) ehmgs to the gronl ) shown ill Tal ) h'6 , the word is rel ) laced t ) yits rel ) resentative mmfl ) ('r ' wordi ' and th ( ' fre ( luency of ' word /' is count ( 'd . 
For ( , xalnlJe , ' l ) ank 3' and ' banks 3' in a new ~ Lrti-clearer el ) laced by ' wordi ' , aud the frequen ( ' y of ' word i ' equals to the total nuln l ) er of fr ( ' quency of ' bank 3' and q ) anks3' . 
Using a term weighting method , articles wouhlbe represented 1 ) y vectors of the form
A = ( w t , w . , '", w .   )   ( 4 ) where W i ( : or resl ) on ( ls to the weight of the noun i . The weight is used to the fr('(lu(mcy of noun . 
Given the vector rel ) resentations of articles as in formula ( 4 )  , a similarity between Ai and Aj are caJculated using formula  ( 1 )  . The greater the wtlue of Sim(Ai , Aj ) is , then to rexinfilar these two articles are . The ( ' lustering Mgorithm whh:h is described in Stage Four is apptic d to each  1  ) a h ' of articles , and t ) roduces a set of ( ' lusters whh'h are ordered in the des ( : ending order of ~ heir semantic similarity w dues . 
4 Experiments
We have conducted flmr (' xl ) eriments , i . e . q !' req ' , ' Dis ' , ' Link ' , and ' Method ' in order to exanline how WSD me , thod and linking words with their semantically similar words  ( linking method in short ) atfect the clustering results . ' Fl'eq'isfl'equency-t)a~sedexlmriment , i . e . we use word frequency for weighting and do not use WSD and linking methods  . ' Dis'iscon ( : erned with disambiguation d ) ased experim ( mt , i . e . the ( : lustering algorithm is applied to new art Mes . ' lAnk'ix con/:erned with linking-l ) ~ used experiment , i . e . we applied linking method to original artMes . ' Method's hows our proposed method . 
4.1 Data
The training tort ) us we have used ix the 1988 , 1!)89 WSJ ill ACL/DCICD-IOM whi ( . h('onsists of al ) out 280 , 0001) art-of-spee('htagged sentences ( Brill ,  1992) . From thise or lmx , we seh , cted at random 49 ( lifferent articles for test data , each of which (- onsixts of 3 , 500 sentences and has different tel ) it il all lew lfich is tagging in the WS , I . 
We classified 49 artMes into eight categories , e , g . 
S in our experiments , m equals to 238.
' market news ', ' food . restaurant ', etc . The di ( ' tio-nary we have used is Collins English Dictionary in ACL/DCICDROM  . 
in WSD nwthod , the ( : o-occurrence of x and yf ' or cah : ulating Mu is that the two words  ( x , y ) al ) -pear in the training ( : or l ) uS in this order in a window of 100 words , i . e . a : is folh ) wed by y within a 100-word distance . This is because , the larger w in ( h ) w sizes might be ( ' on sidered to be useful for extra ( ' tings ( unanti ( ' relations hil ) s between ltOl tllS . 
Basic words are sele ( 'te ( l the lO00 th most fre ( luent words in the reference Collins English , Dictionary . 
'\[' he length of a sell tetl (: ( , ~" which contains a 1 ) ol-yxemons n ( mn and the h'ng th of a sentence of d i ( ' tionary-defilfition are maximuln 20 words . Forea('ht ) olysemous nmm , we selected the tirst top 5 definitions in the ( lictionary . 
In linking m ( , thod , a window size of the c(>o('('urren('e of . candy for ( ' ah'ulating Mu is the same as that in WSD method , i . e . ~L window of 100 words . W (' xeh , cted 969~9128 different ( noun , nomt ) pairs for each article ,   377  ~  1259 tilt % r-elllOllll S Oil condition that frequ (  , ncies and Mu ~ , , ,  . . or 1,,w ( f ( . , , : j)_>5,M , , ( .   , v)_>a)t , , permit a relial)le statistical analysis 4 . As a result of Stage Four , wenlanually selected ( : lusters wl fich are judged to 1 ) esemanti ( : ally similar . As a result , w ( ' sele ( 'te ( l clusters on ( : ondition that the threshold value for similarity wax  0  . 475 . For the seh'cted (' lusters , if there ix a noun which belongs to xev- ( ' ralehtsters , the x ( , chlsters are grouped together . 
As ar ( , sult , we obta hwd 238 clusters in all.
4.2 Resnlts of the experiments
The results are show it in TM ) le 7.
'\[ h\])ie7: The results of the experiments
Ar;i('h , NumFreqLink D is Method 5   10   4   4   5   8   \[0   10   4   6   6   9   15   10   7   7   7   8   20 l . O 66 66
Total 4021 2324 31(%)(-)(52 . 5) (57 . 5) (60 . 0) (77 . 5) In Tal)le 7 , ' Article ' means them unber of articles which are sele  ( : ted from test data . ~ Nltnl'ii IP alls the , nunlber for each ' Article ' , i . e . we selected 1 ( I sets for each ' Article ' . ' Freq ' , ' Link ' , ' Dis ' , and < Method's how the nulnlm r of sets which are clustered  ( : orrectly in ea ( : h experiment . 
The samph'results of ' Article = 20'fl ) reach ( , xperiment is shown in Figure 1 ,  2 ,  3 , and 4 . 
In Figure 1 ,  2 ,  3 , mid 4 , the X-axis is the similarity w due . A 1 ) l ) reviation words in each Figure and categories are shown in Talile  8  . 
4 Her (' , f(x , y ) is them unl ) er of total cooccurrences of words : e and y in this order inst window size of  100 words . 
409 0.9 0.705 X : slmllarity value
II 0 . 6 38 I*X\['--BBK ~_~420 TNM market ~ STK 2'~~1 newsL  ~ 260 metal-~CS retailing-RET ~\ [ oodr-RFDO . 141 restauran U-FOD
AR Oeem cat_MTC
Figure 1: Tile results of ' Freq ' experiment news t . _DIV ~\[0 . 890

CMD . -~981~843 a762
ARC)_:=:_==_.a
TNM 0.956 ~ CS~382
BVG _____1 ~_1~871
CEOPRO ~_~.84 10,651
FOD ~~'814~204
ENV ' I\[-'--
HEAII
IMTC


Figure 2: The results of ' Link'experiment 0 . 7 0 . 5 0 . 3II--I *, X .   .   .   . 0,679 V ~ ~ . 53o market lT~N ' ~ l ~ --- I~4 13 news/DI ~ ____~~_ 0 . 263 metal-PCS~I
HI055--0263\[restaurant\]REC~~~\]
IFOP--,\[\[It .. RFD'II
BON-0 , 1'~ I environment - ENV 0~7N k science-ARO~\[_J\[farm ~ CMD 0~35   ~172 \[ chemical \[- HEA- , ~ , oo- , -- . ~ jt-MTC-J0 . 0 73 Figure 3: The results of ' Dis'experiment 5 Discussion 1  . WSD method According to Table 7 , there , are 24 sets which could be ( : lustered correctly in ' D is ' , while 21 sets in ' Freq ' . Examining the results shown in Figure 3 , ' BVG ' and ' HRD ' are correctly classified into ' food ? restaurant ' and ' market news '  , respectively . However , the results of ' Freq ' ( Figure 1 ) shows that they are classified in corre <' tly . Table 0 . 90T7r~l~h~JL'9~4969015 ? XBB1-\[DIVJ ~ . 9 23   1 STK-~~ L922 marketITNM 0~72~L~913 newsI ~\ [ L863 
I ~ ~ tAs 2st-HRD ~\] 10 . 819 science-ARO-~metal--PCS 0 . 893'L,\[-BVG~58~-756
IFOD ~ Itlfood IPRO~\]II . -'" restaurant\[hE~-~7~J ~" a"t . _RF ~ I ) retailing - - RRF ! l) , ' ~~0:845 lI0"5~73 , environment--ENV ehemical \[- MTC 0 farm - ~ M ~- Figure 4: The results of ' Method ' experiment
Table 8: Topic : and category name
Category market news science metalfood restaurant

BBK : Buybacks
BON:Bond Market News
CEO : Dow Jones interview
DIV : divid ends
ERN : em'nings
HI/D : Hem'd on the street
STK : stockmm'ket
TNM:tender offers
ARO : aero space
PCS : precious metals , stones , gold
BVG : beverages
FOD : food products
PRO : corporate profile
REC : recreation , entertainment
RFD : restaurant , supermarket retailing RET : retailing environment chemical farm 
ENV : environment
HEA : health care providers , medicine
MTC:medicM and biotechnology
CMD:commodity news , farm products 9 shows different senses of word ill ' BVG ' , and ' HRD ' which could be discriminated in ' D is '  . 
In Table 9 , for example , ' security ' is high freqtlen-ties and used ill ' being secure ' sense ill ' BVG ' ar-tMe  , while ' security ' is ' certificate of creditors hiI  ) ' sense in ' HRD ' . One possible cause that the results of ' Freq ' is worse than ' Dis ' is that these polyselnous words which are high-frequencies are not recognised polysemy in ' Freq '  . 
2. Linking method
As shown in Table 7 , there are 23 sets which could be clustered correctly in ' Link  '  , while 21 sets ill ' Freq ' . For example , ' ERN ' and ' HRD ' are both concerned with ' market news '  . In Figure 2 , they are clustered with high similarity wflue (0 . 943), while in Figure 1, they are not (0 . 260) . 
Exalnilfing the results , there are 811 nouns in ' ERN ' article , and 714 nouns in ' HRD' , and security rate sale stock
BVG the state of being secure a quantity in relation the exchange of goods total goods 
HRD certificate of creditorship l ) rice of charge the all lOll ltt of sold stock market of these  , ' shares ' , ' stock ' , and ' share ' which are semantically similar ~ re included  . In linking method , there are 251 nm n in ' ERN ' and 492 nouns in ' HRD ' whi ( ' h~trere pl~tccd for representative words . However , in ' Freq ' , each noun corresponds different coordinate , and regards to different meaning . As a result , these to l ) ics are clustered with low similarity w due . 
3. Our method
Tit (' . results of ' Method's how tha , t31 out of 40 sets are cbLs sified correctly , att ( I the per (' entage attained was 77 . 5% , while ' Freq ' , ' Link ' , and ' Din'ext ) eriment at t , ~ tined 52 . 5%, 57 . 5%, 6() . 0%, renl)e (:- tively . This shows the effe(-tivelms so four method . 
In Figure 4 , the ~ u ' ticles , tre judged to (' l , tssify into eight categories . Examining ' ERN ' , ' CEO ' and ' CMD ' in Figure 1 , 'CE()'and'CM1)' are grouped together , while they have ( lifferent c ~ , t-egories with each other . On the other hand , in Figure 3 , ' ERN ' and ' CE()'ar ( , groul ) ed together corre('tly . Examining the nouns which arc 1 ) elonging to ' ERN ' mid ' CE ( )'  , ' plant'(factory and food senses ) , ' oil'(petrohmnl and food ) , ' or-der'(colmn~nd ; md dema . nd ) , and ' interent ' ( del ) t and curiosity ) whi ( : h are high frequencies ~ recorrectly dismnbiguated  . Furthermore , in Figure 4 , ' ERN'mM'CEO ' are classified into ' market news '  , and ' CMD ' are cb~ssilied into ' fm:m' , correctly . For example , ' plant'which is used in ' factory ' sense in linked with semanti  ( ' ~ llysilnib ~ r words , ' ntanuf ; w-turing ' , ' factory ' , ' production ' , or ' job ' et (' . . In a simibtr way , ' i ) bmt ' which in uned in flood ' sense is linked with ' environmeltt '  , ' forest ' . As a result , the articles are classified correctly . 
As shown in Table 7 , the rear c9 nets which could not 1 ) e clustered correctly in our method . A possible improwmmnt is that we use all definitions of words in the dictionary  . Wes ( qeeted the first top 5 definitions in the dictionary for each noun and used the ln in the cxperilnent  . However , there are some words of which the memfings are not included these selected definitions  . Thin ( : ~ mses the fact the ft it is hard to get a higher percentage of correct clustering  . Another interesting t ) ossibil-ity into use mlalter mt tive weighting policy  , such a , s the widf ( weigh . te , d in vcr . scdocwmcntfre , qucncy ) ( Tokunaga , 1994) . The widf is reported to have a marked ~ ulw mtage over the idf  ( invers ~ . document frequency ) for the text categoris ~ L tiontank . 
6 Conclusion \ Vehaverei ) orted a nexl ) eriment adstudy for clustering of ~ rticles by using online  ( lictiom ~ rydeft-nitions mid showed how dictionary -definitiol tcamuse effectively to classify articles  , e a ( ' h of which belongs to the restricted sul ) ject domain . In order to Col ) e with the relnain iug i ) rol ) lems in entioned in section 5 and apply thin work to practical use , we will conduct further e?perilnents . 

P . F . Brown et al , 1991 . V ~ ror ( 1-Sense Disambiguation Using Statisti ( ' al Methods . In Proc . of the 291 hAn-w~tal Meeting of the ACL , pl) .  264-270 . 
E . Brill , 1992 . A siml ) le rule-I)~Lsed1) art of speech tagger . In Proc . of th , c 3nd conference on applied natural language procc , ssiug , ACL , pp .  152-155 . Trcnto,
Italy , 1992.
K . W . Church ( , tal . , 1991 . Using Statistics in Lexi-eal Analysis , Lexical acquisition : Exploiting online re . qource ~ to build a lea:icon . ( Zernik Uri(ed . )), pp . 
115-164, London , Lawrence Erlbaum Associates.
L . Guthric and E . Walker , "DOCUMENT CLASSI-FICATION BY MACHINE : Theory and Practice "  , h , Proc . of the 15th , International Confercnccon Computational Linguistics  , Kyot % Japan ,  1994 , l ) P . 

N . .lardine and 11 . Sibson , 1968 . The construction of hierarchic and non-hierarchic classifications  . In
Computer , lourrtal , i ) p . 177-1.84.
K . S . Jones , 1973 . A statistical interl ) retation of terms p ( ~cificity and its apl ) lieation in retrieval . Journal of Documentation , 28(1973)1, pp .  1121 . 
M . IAberman , editor .  1991 . CDROMI , Association for Comlmtational Linguistics Data Collection Initiative  , University of Pennsylvania . 
Y . Niwa and Y . Nitta , 1995 . Statistical Word Sense Disaln biguation Using Dictionary Definitions In Proc  . of the Natural Language Processing Pacific Rim Sympoaium  '95  , Seoul , Korea , pp .  665-670 . 
G . Salton and M . a . M('Gill , 1983 . Introduction to Modern hfformation Retrieval . McGraw Hill , 1983 . 
T . Tokunag and M . Iwayalna , 1994 . Text Categori-s ~ fl ; ioll based on Weighted Inverse Doenment FI'e -quency IPS  . \]SIGl ~ . cl ) orts , 94-NL-1f ) 0, 1994 . 
I ) . Yarowsky , " Word sense ( lismnbiguation using statistical models of I/og et 's categories trained on large corl  ) or a " , In Proc . of the 14th International Confc > e'nccon Computational Linguistics  , Nantes , France ,  1992 , l ) P . 454-46) N . Yuasa (' tal . , 1995 . Cb~ssifying Art Mes Using Lexical Cooccurrence in Large Document Databmses In TTu'na  . of Infl~rmation Processing Society Japan , pp .  1819-1827, 36 (1995) 8 . 
1), Walker and I/ . Amsler , 1986 . The Use of Machine-l-leadable Dietionm : ies in Sublanguage m  , Mysis , Analyzing Language in Restricte domains , ( Grishmana im Kittredge ( cd . )), pp . 69-84, Lawrence Erlbaum , ltillsdale , NJ .  11987) 2 . 

