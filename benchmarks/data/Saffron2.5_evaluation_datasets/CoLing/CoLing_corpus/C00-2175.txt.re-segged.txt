Comparing two trainable grammatical relat ionsfinders 
Alexander Yeh
Mitre Corp.
202 Burlington Rd.
Bedford , MA 01730


Abstract
Grammatical relationships ( Glls ) form an important level of natural language processing  , but different sets of ORs are useflfl for different purposes  . The retbre , one may often only have time to obtain a small training corpus with the desired  GI1  . annotations . Onsu & a small training corpus , we compare two systems . They used if l'erent learning tedmiques , but we find that this difference by itself only has a minor effect  . 
A larger factor is that iLL English , a different GI / . 
length measure appears better suited for finding simplem : gumentGIs than ~ br finding modifier GRs  . We also find that partitioning the datamaWhelp memory-based learning  . 
1 Introduction
Grmnnmtical relationships ( GRs ) , whidl include arguments ( e . g . , subject and object ) and modifiers , form an important level of natural language processing  . Glls in the sentence Yesterday , my catate th , e food in the bowl . 
include a te having tile subject my cat , the object the food and the time modifier Y cstcr' -day  , ~ mdt , hc . food having the location modifier in ( the bowl ) . 
However , different sets of GRs are useful for dii % rent purposes  . For exmnple , Ferro et al ( 1999 ) is interested in semantic interpretation , and needs to differentiate between time , location and other modifiers . The SPARKLE project ( Carroll et al ,  1997) , on the other lmnd , * This paper reports on work performed at , the MITRE Corporation under the support of the MITRE Sponsored Research Program  . Marc Vilain , Lynette Hirsehman and Warren Greiff have helped make this work happen  . 
Christinel ) or an and John Henderson provided help flflediting . Copyright@2000 The MITRE Corporation . All rights reserved . 
does not differentiate between these types of modifiers  . As has been mentioned by John Carroll ( personal communication )  , this is fine for infbrmation retrieval . Also , having less differentiation of tile modifiers can make it  , easier to find them ( Ferro et al ,  1999) . 
Unless the desired set of GRs matches the set already annotated in some large training col pus  ( e . g . , the Buchholzel ; al .   ( 1999 ) GR finder used the GRs annotated in the Penn 3~'eelmnk   ( Marcusel ; al . , 1993)) , one will have to either manually write rules to findtile GIs or mmo-tate a training corpus tbr the desired set  . Manually writing rules is expensive , as is annotating a large corpus . We have performed experiments on learning to find ORs with just a small annotated training set  . Our starting point is the work described in l ?erro et al  ( 1999 )  , which used a faMy smM1 training set . 
This paper reports on a comparison between the transforination-based rror-driven learner described in Ferro et al  ( 1999 ) and the lnemory-based learner for GRs described in Buchholz et M  .  (1 . 999 ) on finding GIlst overbs 1 by retraining the memory-based learner with tile data used in Ferro et al  ( 1999 )  . We find that the transformation versus memory -based difference only seems to cause a small difference in the results  . Most of the result differences seem to instead be caused by differences in tile representations and information used by tile learners  . An example is that different GR length measures are used  . In English , one measure seems better f brrecovering simple argument ORs  , while another measure seems better ibr modifier GI l  . s . We also find that partitioning the data sometimes helps melnory-based  learn-1That is , ORs that have a verb as the relation target . 
For example , in Catseat . , there is a " subject " relation that has cat as the target and Cats as the source  . 
1146 ing.
2 Differences Between the Two

For root al .  (\] . ( ) 00 ) alld Buchholz et al ( 1999 ) both describe learning systems to find GRs . 
rl ' he former ( TI ) uses transformation based error-driven learning ( Brill and Resnik ,  1994 ) aim the latter ( MB ) uses lnemory-bascd learning ( l ) a elemans et al ,  1999) . 
In addition , there are other difl'erences . The TR system includes several types of inibrm a -tion not used in the MB system  ( some because memory-based systems have a harder time handling set-wdued attributes  ) : possible syntactic ( Comlex ) and semantic ( Wordnet ) classes of ac\]11111 k head word ,  1;11(' , stem(s ) and named-entity category ( e . g . , person , h ) cation ) , if any , of ac\]mnk head word , lcxemes in a clmn k besides the head word , pp-attachment estimate and cerl ; a in verb chunk properties ( e . g . , passive , infinitive ) . 
Some lexemes ( e . g . , coordinating COlljllllC-tions an ( 1 lmnctuation ) are usually outside of any clmnk . The T12 , system will store these in an attribute of the nearest chunk to the left  ; and to the right of such a \] eX ellle . r . l'lleMB system represent such lexemes as if the , yarc Olle word chunks . TimMB system cm motuse 1; 11(; TI syste , m method of storage , l ) ecaus ( ' ~ melnory-based systelns have difficulties with set-v ~ ducdal  , l , ribtttes ( value is 0 or \] now ~ lexemes) . 
'\[' lieN,iB systellt(all(lllOtthe ' . \['1 syStelll ) a \] so exalnines the areal ) or of commas an ( tverb ( : hunks crossed by a potential G12 . .
Thesi ) acc of l ) ossible GlTls searched 1 ) y the two systems is slightly different . The TI , system searches fbrGl~s of length three clmnks or less  . 
The MB system set ~ r ( -hes for GRs which cross at lllOSt either zero ( target to the source's left ) or one ( to the right ) verb ( : l nulks . 
Also , slightly different are the chunks ex-mnined relative to a potential GR  . Both systems will examine the target and source chunks  , plus the source's immediate neighboring chunks . 
The MB systeln also examines the source'se ( ' ~ endneighl ) or to the left ;  . The Tll , system instead also exm nines the target's immediate height  ) ors and all the clmnks between the source and target  . 
The T12 , system has more data partitioning than the MB system  . With the TI syst : em , possible Gls that have a diit'erent source chunk tyl  ) e ( e . g . , noun versus verl )) , o1" a different relationship type ( co . subiect versus ol ) ie ct ) or \" , %" ,   , direction or length ( in chunks ) are alwws considered separately and will be afl ' ected by differ-eat rules  . The MB system will note such differences , but lilW decide to ignore some or all of them . 
3 Comparing the Two Systems 3 . 1 Experiment Set-UpOne cannot directly coral ) are the two systems from the descriptions given in Ferro et al  ( 1999 ) and Buchholz et al ( 1999 )  , as the re-suits in the descril)tions we , rebased oll different ( tatt ~ sets and on different assumptions of what is known and what nee  , ds to be fbund . 
It erewetest how well the systems 1 ) er form using the same snm\]lannotated training set  , the a2 . ~) . 0 words of elementary school reading comprehension test bodies used in t  , brro et al (1999) .   2 We are mainly interested in comparing the parts of the system that takes in syntax  ( noun , verb , etc . ) chunks ( also known as groups ) and tinds the G12 . s between those chunks . So for the exl ) eriment , we used the general'I'i MBLsysi ; eln(l ) aelemans et al , 1999) to just reconst ; ruct the part of the MB syst can that takes in ( : hmlksan ( tfinds G12s . Th (' , input to 1 ) o t\]1 this reconstructed part and the T\]-syst onlis data that has been manually al Hlotate  ( t for syntax chunks and ORs , a hmg with automatic lexeme and sentence segmentation alt d t  ) art-of-sl ) eech tagging . In addition , the '\] . '12 . system has nlmltlaln allie(t-e\]ltity allllOt3 . tioll ~ all ( 1 alltO-matic estimations for verb properties and in el  ) o-sition and sul ) or dilm te conjmlction attachments ( l~k ' , rroel ; al . , 1999) . Because the MB system was originally desigued to handle Gll  . s attached to verbs ( and not noun to 1101111O12S , etc . ) , We 17311 the reconstructed part to only find Glistow ; rbs , and ignored other types of GRs wheneomt ) aring the reconstructed part with the T12 . 
system . The test set is the 1151 word test set used in Ferro et al ( t999 )  . Only G12s to verbs were examined , so the elt'eetive training set GR count fellti ' om  1963 to 1298 and test set C12  . 
' ) Note that if w chad been trying to compare the two systems on a largemmotated training set  , the , M\]~system would do better by default just lmcause the TR system wotlld take too long to l  ) roecs s a large trailling set . 
1147 (: ount from 748 to 500.
3.2 Initial Results
In looking at the test set results , it is useful to divide up the Gils into the following sub-tyl  ) es:1 . Simplem ' guments : ubject , object , indirect object , copula subject and object , expletive subject ( e . g . , " It " in " It mined today .  ") . 
2 . Modifiers : time , location and other modifiers . 
3 . Not so simple arguments : arguments that syntactically resemble modifiers  . These are location objects , and also sut ) jects , objects and indirect objects that are attached via a preposition  . 
Neither system produces a spurious response for tyl  ) e3G ils , but neither sysl ; emrecalls many of the test keys either . The reconstructed MB system recalls 6 of the 27 test key instances ( 22% )  , the TR system recalls 7(26%) . A possible ext ) lanation t br these low performances is the lack of training data  . Only 58 ( 3% ) of the training data GR instances are of this type  . 
The type 2GRs are another story . There are 103 instances of such Glls in the tess set key . 
Tile results are
Type 2 C4 II . s
System Recall Precision F-s (: or e
MB 47 (46%) 4!)% 47%
TR25(24%) 64% 35%
Recall is the number ( and percentage ) of the keys that m'ere called . Precision is the number of correctly recalled keys divided by them unber of ORs the system claims to exist  . Fscore is the harmonic mean of recall ( r ) and precision ( 1 ) ) percentages . It equals 2pr/(p + r) . Here , the differences in r , p and F score are all statistically significant , a The MB system performs better as measured by tile Fscore  . But a tradeoff is involved . The MB system has both a higher recall and a lower precision  . 
Tilet ) ulk (370 or 74%) of tile 500 Gil , key in stmmes in tile test set are of type 1 and most 3When comparing differences in this paper , the statistical signiticance of the higher score I  ) eing better than the lower score is tested with a one-sided test  . Differ-cnccs deemed statistically significant m'e significant at the  5% level , l ) ifferences deemed non-statistically signifleant are not significant at the  10% level . 
of these are either subjects or objects . Witll type JGRs , the results are
Type 1GRs
System Recall Precision l?-score
MB 23I . (62%) 66% 64%
Til . 284 (77%) 82% 79%
With these GRs , the TR system I ) erforms considerably better both in terms of recall and precision  . The ditferences in all three scores are statistically significant  . 
Because 74% of the GI-/ . test key instances are of tyt ) e1 , where the TR system performs better , this system peribrlns better when looldng at the results for all the test Glscoml  ) in ed . Again , all three score diff fere nees are statistically significant : 
Combined Results
System Recall l ? recision Fscore
MB 284 (57%) 63% 60%
Til , 31((G3%) so %
Later , we tried some extensions of the reconstructed MB systeln to try t  ; ol int ) rove its overall result . We eould improve the overall result by a combination of using the  I\]71 search algorithm ( instead of IG27~EE ) in TiMBL , restricting the t ) oten tial Gils to those that crossed no verb chunks  , adding estimates on prepo , si-tion and complement attachments ( as was done in TR ) and adding in fbrnlat , io nonverb chunks about 1)eing passive . , an infinitive or an uncon-jugated present 1 ) articit ) le . The overall Fscore rose to 65% (63% recall , 67% precision ) . This is an improvement , but the Til . system is still better . The differences between these scores and the other MB and Til  , confl ) ined scores are statistically significant . 
3 . 3 Exploring the Result Differences 3 . 3 . 1 Type 2 GRs : modifiers The reconstructed MB system performs better at type  2 Gil , s . How can we account t br this result difl'erence ? Letting the TR system find longer GRs  ( beyond 3 chunks in length ) does not hell ) nmch . 
It only finds one more type 2 Oil , in the test set ( adds 1% to recall and 1% or less to precision )  . 
Rerumfing the TR system rule learning with an information organization closer to the MB system produces the stone  47% Fscore as the high (  ; r ) . S1) c<:ifi(:a . lly , we ~ , ; ( ) I : 1; his rc . sull ; when I ; \] IC ~ I'\] , sys ; CIII WaSl ' Crllll wil ; h1 lo informal ; ion <) n l ) p - - al ; l ; a . (: hnl('nl ; s , v(;rl ) chunl(1) r < ) l > ( , r l ; i ( ; , ~( e . g . ; l)aS,~dv (' . , ilflinil ; ivc ), nam ( ;d-cm ; il ; ylal ) cls <) rhc . a . d-r1"1 wor(tsLems . Also , l;\]m . Il ~ .   , % ml ; cmHow(;xmn-in(; , q(;h<;<:hmtkscxamin(;(l1) yl ; h<'<) rJj , ,ina \] M \ ] I s2/sl;('an : garg (' . ; , ' ~ our ccand,~our(:(;' . ~n('Jj , ihl ) <) rs . 
Illa ( l<lil ; ion , insl ; c , ad of 6 al ) s <> lul ; eh ; ngl , hca . l ; ('- gories(l ; arg ; cLi , ~;3<:\] ranks1;()l ; hclcf; , 2(:\[mnks , \] clmnk , and similarly for I ; 11(' . righl ;), 1; he(\]l . sc,m,~i<l('xc , lnowjusl ; fall Jill ; t);/ . 11 < ta . re\])m'i ; it ; i(m <' xt in L <) 3 re\]at ; iv ( ; cat ; ( ; gorics : i ; arg cl ; isl ; h(!tirsl ; verb chunk 1 ; o Lhclol L ,   , ~ dmila . rly /; <) L h <' . righl ; and l ; a . r-gel ; isi;\]m:-~c . (:(m<\[verb(:hun\]~1;<)l:\]~crig\]~I; . ~l'h(' . 
MI\]SNS(i (' . III(;;~11(\[isgin ?; ui ~' , hl ) (' k we (' ~ n (;\] w + ; e sam(; r (' , \]al;iv(;<:a , 1;('gori(' , ~ . 
I h ' . <loingl ; hi,s'l . ' l . sy , %<; ~ nr <; i'm ~' wil . /m ' l + , l , < : hiu ~ l ~ h(' . a . (lwor<l ~; ynLa(;l;i < :(> r,~(' . mmg;\](: las ~; (' , s\])r <>-< lu < : csz~d(i(/>l:-s(:or <'~ . If in a < h l i i ; i (/ n , ( , h ( , : l ) l ) - al ; L a . (: linmnl ; , v ( , . rl ) < :\] ulnk1) rol ) (' . rl ; y , nam <; < l--cHl;il ; ytal)el and\]mad word , ql ; <' . n ~ int brnml ; i <) n are a d < l(:<lI)a(;l(in , i ; t ~('~ i:-s (: or (' , a clamlly ( h'ol ) , ~1;()/J3<~) .  ' \[ . '\]1(;<lifl'(;r<',nc <' . ," , t )(; 1; w < : cnl ; hc , ~(; d'\[(/) , d(i (/> mMd3</)rc . -r~m , ~c <) r(; , ~: u : ('~ H()I; , % a , i ; isl;i (: alls ~:; ig ~ fiti (' , anl; . 
H <) wit ; hI ; yt ) <;2(~1C4,MII~y ~; l ; (' . m ', ~; l)c(;lcr\])(' . r-\['()rman(:(~>c (' . n~s1 ; <) 1)( ; l~tainly(lu(:il ~' ; al ) ilil ; y\[ ; () ~ litl'er ( ,  . nl;ia . I ; elh <; l ) <>( ;(, nl ; ial CI:I,~I ) 3~1,11(' . \[' ca . l ; ur < ~<) t : l ; \] t ( ; ~ mml ) < ; r <> fv < ! rt ) (: \] mld ;  , ~; < : r(>s >( ; < l1) 3: aC , I . Inl ) a . rl ; : i < : uhu ', makin ~, ~ l;hi,~:/ . l ~ < lafe w <> l ; hcr < : hm ~ , ; csI ; ( ) I ; Im'l'l , %' , ~l ; <; nl in (: r ( ,  . m :; csil ; :; i: . ,~ ; < :() r <~ l <)\[ ; 11( ; MB sysl ; ( ; m's\]i'-~4<x>re , a \] l < t1;\]1(' . <) L h < u ' < : m , ug< ; s(r ('- moviug(:cri ; a in iniormal ; i <) n ) < w ( ) llmv('a ,   , ,~ i , " - ~ nifica , ni ; (' Ke . cl ; . Sou~dngl ; h(;rip , hi : lb . a , l ; ~ u (' + ~ can(ll ~\ [ cr(m(:(' . make a , la , rgo '' F or I ; hes<~I ; yl ) e2(~'\] L~(luoditicr ~ , ) in English , i l , <lotss('~( unLlm ; l ; hc . nmnb(u "()\ [ verb cJ mnks(:ro , ~ , s(;<t is a , l ) (' l ; l ; (; r was ~ i ; o ~ rou 1) l ) OS--si \])\] c , m () < lifi(;r , qLhanI ; 11<: al ) solul ; (~ clmn\](l<'~ngl ; h . 
AncXaml )\] (: is comt > aring\] . /lyo'a~l~ac . sday . mM l . /1!/k,o'm , c . fl'o'm , k , (", ' co':~,\[l~u , (' . , sd(~?/ . in 1) oi ; hs (', n-l ; ( ; ll (: (', s , on T . ,<'sday is a , l ; imcm(>(liticr <) fJly and on (: ro , ~; s('~s nov ca'l) , ~;1:<) reach . fly ( onal ; l ; ach (' ~ sl ; <) l ; hctirsl ; verb1 ; ( ) ii ; sleft ; ) . \]htl ; inl ; h(;fir , ~; I ; s(;nt(;ncco ' ~ , is ncxl ; to Jly , while in l ; h ( ; s(~(:<) n<l SCllt ; ( ~ llC(~1 ; lmr ( ~ arcl ; \] ll "( x  ~ , chllil \] r ~ , q , ~c \]); l , r;i . , ill~o71 ~ aud . fly . 
a . a . ~ Type1 (-11 . .s : s imple arguments \]~ ( / r Ly\] ) e\] ( l \ ] s , l ; lmT\] , sy , % cmt )(; r forms 1) ('% l ; ( u ' . 
How can w <' a , (:(:<) unl ; for t ; his ' ?
Much < If 1; h(;cxt,rainf()rnml ; ion1 ; 11(;' . \['\]  . , ~; ysl ; (' . mcxami Hc , ~( comt)ar ( ; d to 1 ; he MB sysl x ; m ) do ( ; snoI ; seem 1 ; o have much of an c t k , cI ; . Whenth (-' . 1 . '\]-, sysl;ClllWa , qrerun with no in \[' or ma . l ; io non\]ma(lwol'd synl ; acLic or seman Lic classes , nmnt'~<l-cnl ; ity labels or h(;ad words Lems , 1; h(' ,  \] . :- scc > r('~in--creased to m 79% l ; o 80% . Anol ; lmrr (: ; r l l l l l t ; l l i ~ ; in addi Lionhad no information ( mt)\])-~d , l;a(:hm(ufl ; c , sl ; ima , l;<; . so rany of 1; he non-hca(lwor < t , sintim (: lmnks also ha < lanF score of 80% . A1; hir < llCrlllt , hal ; fllrl ; h( ; rlllor ( ; had no il ~ tin : nmLiollOll verb (' . hunk1) ro \]) crl ; ics(c . .g . , lms sivc . , infinil ; ivc ) had an Fscore of 78% . In Lifts set of F -, ~ t:ore . s , only l ; hediff c . rences bc . Lw cen the 8()~) scor(' . s and I ; hc78(X ) s(:or carcsl ; at ; isti (: ally signiiica Jfl ;  . 
Some M\]/sysl ; cnircrun , qsliow cdt ) ~(' l ; or sthai ; S(;CI lICdl ; oltta , l ; l ; (; r11101"(; . hi ; \] l C\[\]1"s ; 17 (~1' Ull ~ w ( ; l ) arl ; il ; ion( ; ( lt ; h ( ; ( la . La \]) yt ) ot ;(; n/; ial (11 , ~ om'(:<~<:hlud~i ; yt)c(c . .g , l loun versus vcrl )) and ra . llaSCl ) aral ; (' . m<mlory-l ) a . s(~(ll ; railfiUt ~ and l ; (' . ~;i ;\[ or ca(:ht)arl ; it ; ion .  ' . \[' hc . Colnl ) ill(~d\]:-s(:<)rcin<:rcav;(' . dfl'om 64% t;o(i9(fi ~ . At ' L cr w m d s , wcmadear ('-> ml ; ha Lres <; nfl)l ( , dI ; h <; TI . sysl ; cm run wil ; hl : hc , 78 (/0\]' Ls(:t>rc , ( ex < : ('=\]) l ; l ; \]mi ; m(unory-l ) asc(\]l <' m'n-ing was use < l ): only C , I . / s <) flcngt~h3 < : mnk , ~; < n " less were (: <) nsid(;rc < l ,   ; h < ; data was l > arl ; il ; i<m< ; <l(in:~<\](lil ; ionI ; <> s()urcc , < :\] mnkI ; y \]) c ) y (111 lengl ; \] lau <\[( lir(' . < : Li <> n((,,,'I ; arg(; l ; isl ; w < >(' . \] mnksI ; ( ) l ; h < ~ l ( ; fL ) an ( lalsot ) yr <'\] al ; i (> nl ; yl )( ; (: ~ c1) ~ , ral ; crmIsti ) l <; a(:h1 , yl ) (' . ), l ; hcc <) 11 11 1 ~/ .  ~/ . \] l(V(l')<:hunk(:ro , q:;-ing (:() unt ; swcr(;n<)l:(:on~d<h' . rc < t , aal(tl , ; < : lmuk > normally cx a . min(~<l1) yl;h(;'11 ~ ysl ; <' mwe recx- . 
mni,l(, . ( t . 'l'\]fisfm'Lll('~riJmrcm ce<tLh( ;  \]:- > ; (:<> r (' . lx ) 75% . Inl ; his s < ; l ; of F-scor('~s , alll ; h('~ditl'ercncc > ares La . l ; isl;i (: ally , ~ ignilica . nl ; and al\]I;\]ml -->; (: or c . s in Lifts seA ; arc . slal ; isl ; ica . llysign if i (: a . nt ; lyd iff crcnl;J'l'Olll(;lit','i'll . SySi ; Cll\[l'llllSwil ; hl ; h<"78</) and 7!) </>\])'- s(:orcs . 
Froml ; hes Lal ; ist ; ica . lly , ~; ign it it : ant ; scow . < li if <' a-(; n(x's , iLs ( ; < : msl ; \] lal ; t ) arl ; il ; ion in /(\ [ al ; a1) yl )()- l ; ent ; i a \] CI/ . s(>m'(:e <: hunkl ; yp < ', hell ) s(in(:rc . a . q < ' ~ from(id:(/>1; o(i9%), as doesLh <' . resL <> fl ; hc . 
imrl ; ii ; ioning l ) (: rf < > rm(;dm Mrim . king some slighl ; clmngcs in what is (' , xamincd ( increase 1; o 75%) , using ; rmls form a Lion-lmscd learning in sl ; ca(t of m cln <) ry-bas (' , tlearning ( increase to 78 (/ ~) an ( lusing v < ' , r l ) chunk l ) rot )(; r ty informat , ion(in('r <' , ase : o 80%) . 
In the . original MBsysl , cm run , L h c s o m c e c l m n k L y l ) (' = m i d l ; h('=1) oi ; ('ag ; ial (: , 11\]CltgLhan(t(lir('=(:l ; Jonwcxea \] rca(lyde Lcrminc(ll)yI ; hcm (' , m(/ry-bas('d\]carn(;rl ; ol ) el ; h(' , mosLiml ) or l ; mfl ; ing the data and runs by the values of these attributes be of extra help ? A possible answer is that for different values  , the relative order of importance of the other attributes  ( as deter-nfined by the menlory-based learner ) changes . 
For example , when the som'ce chunk type is a noun , the second most inlportant attribute is the sourced lunk's head word when the target is one to the right  , but is the source chunk's right neighbor's head word when tile target is one to the left  ;  . Partitioning the data and runs lets these different relative orders be used  . Having one combine dataset and rUlLineal LS that only one relative order is used  . Note that while this partitioning may 11 ot ; be the standard way of using memory-based learning  , it ; is consistent with the central idea in memory -based learning of storing all the training instances and trying to find tile " lmarest " training instance to a test case  . 
Another question is why using transtbrl natiol > based  ( rule ) learning seems to be slightly better than nmmory-1  ) ased learning for these type 1GF ls . Memory-based learning keeps all of the training instances and does not try to find generalizations such as rules  ( Daelenlansel ; al . , 1999, Ch .  4) . However , with type 1Gl~s , a few simt ) legeneralizations can account for many of the , instances . In then lanner of Stevenson (1998) , we wrote a set of six simple rules that when run on the test set type  1 ORs produces an Fscore of 77%  . 
This is better than what our reconstructed MB system originally achieved and is  ( : lose to the TII . system's original results ( close enough not to be statistically significantly different  )  . An example of these six rules : IF ( 1 ) the center d rank is a verb chunk and ( 2 ) is not considered as possibly passive and ( 3 ) its head word is not some fbrm of to be and ( 4 ) the right neighbor is a noun or ve , rb chunk , THEN consider that chunk to the right as 1 ) eing an object of the centere huuk . 
4 Discussion
ORs are important , but different sets of GR , s are useflfl for different imrposes . We have been looking at ways of ilni ) roving automatic Oil , finders when one has only a small amount of data with tile desired Oil  , mmotations . In this paper , we compared a transformation rule-based systeln with a menlory-based system  oi1 a small training corpus . We found that oll GIls that point to verbs , most of the result ditfer-ences can be accounted fbr by ditferences in the representations and information used  . The type of GR determines which information is more important  . The rule versus memorp based difference itself only see in stop roduce a small result difference  . We also find that partitioning the data m whell ) melnory-based learning . 
References
E . Brill and P . Resnik .  1994 . A rule-based approach to prepositional phrase attachlnent disambiguation  . In lath , lVn , te:r'national Cot@on Computational Linguistics ( COLING )  . 
S . Buchholz , J . Veenstra , and W . Daelelnans.
1999 . Cascaded grmmnatical relation assignment . In Joi'n , tSIGDAT ' Cor@renee on Em-pir'ical Methods in NLP and Very Larqe Colpora  ( EMNLP/VLC'99 )  . cs . CL/9906004 . 
J . Carroll , T . Briscoe , N . Calzolari , S . Federiei , S . Montemagni , V . Pirrelli , G . Grefenstette , A . Sanfilippo , G . Carroll , and M . F\[o oth . 
19!)7 . Sparkle work package 1 , specification of phrasalimi'sing , final report . Available at ; http://www,ilc . pi , cnr . it/sparkle/-sparkle , htm , Now ; tuber . 
\? . Daelemans , J . Zavrel , K . van der Sloot , and A . van den Bosch .  1999 . Tilnbl : Tilburg memory based learner , version 2 . 0, reference guide . ILK Technicallle port ILK99-01 . 
Available fl ' om http://ilk . kub . nl/~ilk /- papers/ilk9901, ps . gz . 
L . Ferro , M . Vilain , and A . Yeh .  1999 . Lem'n-ing transtbrmation rules to find graln matie at relations  . In Computa , tional naturalla'n , guage lear'n in 9 ( CONL?-99) , pages 4352 . EACL'99 workshop , cs . CL/9906015 . 
M . Marcus , B . Santorini , and M . Mareinkiewiez . 
1993 . Building a large mmotated corpus of english : tile penn treebank  . ComI ) utational
Linguistics , 19(2).
M . Stevenson .  1998 . Extracting syntactic relations using heuristics . In I . Kruijff Korbayov S , editor , Prec . of the 3'rd ESSLLI
Student Scssion . Chapter 19.

