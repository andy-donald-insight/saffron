Toward , q Automatic Extraction of Monolingual and Bilingual 

B(atriccI)AILLI';*-l::,'icOAUSSIEll-J( . ' ali-Mal (; LANCI;;
(*) TALANAU niv . Pa , ' is 7k IBM-France , See 3099 , 75592 Paris Cedcx 12
Emaihjml ( (.0 vnet.ibm.conl

In this paper , we make use of linguistic knowledge to identify certain noun phrases  , both in English and French , which are likely to be terms . We then test and cmnl ) are ( lifl'e . r-ent statistical scores to select the " good " ones among tile candidate terms  , and finally propose a statistical method to build correspondences of multiwords units across languages  . 

Most of this work was carried out under project EUII  . OTP ~ AET-10/63 , co-sponsored by the European Economic Conmmnity . 
1 Introduction
A technical lexicon consists of simple as well as complex lexical units  . Complex lexical units are mainly compounds , commonly characterized by a set of morphosyntactic and semantic criteria  ( see for example \[ llen veniste , 1966\] for Dench and \[ tlatcher , 1960\] for English ) . The constitution of a list of technical terms from a specific domain is an important issue in Nat-urM Language Processing  , for example in applications such as Machine Translation  . Several methods for the extraction of terms have been proposed  , relying on linguistic or statistical approaches . To build a monolingual terminology bank , \[Bourigault ,  1 ) 9 2\] has implemented a software package which provides it list of likely terminologic M units  , based on linguistic specifications . Different statistical methods have Msobeen tested to extract collocations from large corpora  , as \[ Church and lianks ,  1990 , Smadjaaim McKeown ,  1990\] . Our work makes use of both linguistic and statistical knowledge  . 
The outline of this paper is the following : lirst  , given linguistic specifh : ations of English and l ; ' rencliterms , we select noun phrases which are likely tabe terms  , and which we will call candidate terms . To build a monolingual terminology bank , we then apply different statistical scores to select the good ones among the candidate terms  . Finally , we . test statistical in eth-ods to obt Mn ; ~list of bilingual terms , using aligned corpora in l " cench and Ex@ish . All work and tests are carried out on a parallel bilingual corpus that is tagged by the words ' part-of-speech and lemma  . This corpus consists of Mmut 20 ( I , 0 00 words for each language in the tieht of telecommunications  . Simil , ~rwork can be found in \[ Van der Eijk ,  1993\] . 
2 Linguistic specifications of
French and English terms 2 . 1 Basic mul t i - words un i ts From an investigation of the cori  ) us , and from the tenninologists ' point of view ( for exam-pie\[Nkwenti-Azeh ,  1992\]) , it appears that most of tile terms encountered in technical tiehls are noun phrases corresponding to ~ t limited num-Ier of syntactic patterns  . These patterns detine the morphosyntactic structures of multiwords units  ( MWU ) which are likely to be terms . We detine the length of a MWU as the number of main items it contains  . A main item is either a noun , an adjective , a verb or an adverb ; thus , neither deternliners nor prepositions are considered main items  . MWU of length 1 often correspond to words connected with a hyphen or an apostrophe  , and are easy to identify . We will not deal with them in this paper . Nor are we concerned with mono-word terms ( e . g . telematics ) , the extraction of which is a different-possibly snore complex-problem  . We present below the patterns retained for MWU of length  2  , which we will refer to as base MWU . For each pattern we give an example followed by its translation in parentheses  . The interpretation of the abbreviations is strMgth forward  . 
French ? NAdj or biteg do stationnaire ( geostationary or bit ) ? N1N2 dio detunnel ( tunnel diode ) ? N1 de ( det ) N2 bande de fr dq ue ne e ( frequency b and ) ? N1 prep ( det ) N2 assignation e ' tla deman de ( demand-assignment ) 
English ? Adj N multiple access ( ace , smultiple ) ? N1N2 data transmission ( transmission de do nn des )  2 . 2 Operat ions on base MWU Although MWU of length greater than  2 do exist , they are most of the time built from base MWU by one of the following operations  , encountered both in French and English ( the base
MWU is bracketed in the examples ) : ? overcomposition : rdgdngration des\[lobes latdraux\]\[sidelobe\]regrowth ? modification : \[ station terrienne \] brouille use interfering \ [ earth  ( - ) station \] . coordination : assemblage et dgs assemblaged paquets packet assembly/disassembly  0vercomposition and modification do not always preserve the base -MWU structure : in French  , an adjective modifier is often inserted inside a base-MWU of Igl prop/  q2 structure ; for example , national ( domestic ) cast be inserted insider dseaufisatellites ( satellite network ) to producer'd seaunation alit satellites ( domestics at ellite network )   ( notice that in English , the adjective precedes the base MWU most of the time  )  ; in l ' ; nglish , the Adj Ig structure is altered when two base -MWU  , one of Adj N1 structure and one of N2 N : t structure , are overcom-posed : geostation nary satellite and telecommu-nication satellite yield geostation nary telecommunication satellite  . Coordination , which requires two base-MWU , always breaks the structure of one of them . These operations are not used with the same frequency  , composition and modification being more frequent than coordination  . 
From the preceding considerations , it seems natural to focus on base MWU . But , we tat~e into accoun the cases where base-MWU structure is broken ms well as their variants  . Variants of base-MWU are mainly graphical , orthographical and morphosyntactic ( for more details see\[l ) aille ,  1994\]) . In English , we have also ac-eel ) ted the transformation of a base-MWU of N2 lql structure into a N : I . of lg2 structure . 
2.3 Extraction of base MWU
To extract and count the occurrences of base MWU ( under their various forms ) we use their morphosyntactic structures . II appily enough , our corpus is tagged : we can rely on tile sequences of part-of-speech to extract the relevant candidates  . For this purpose , we have implemented finite automatassociated to regular expressions covering most occurrences of morphosyntactic structures  , including modilications clue to the afore mentionned operations  . The output of the program is a list of pairs composed of two lemmas  . Under each pair are stored all occurrences of the base MWU extracted from the corpus : for example  , for the pair ( interference , level ) , we get the following sequences : interference l vel  , interference levels , level of interference , levels of interference ; for the pair ( circuit , num 6 rique ) , we get : circuit num drique , circuits num driques , ciT vuitenti ~ rement num drique , circuits analogique setnum driques . 
Unfortunately , the pairs obtained through this method are not always " good " terms  ; we therefore have to introduce additional filters : statistical scores which use the number of occurrences of the pairs as input  . 
Statistical scores to select good monol ingual can-didates Since the MWU extracted after the linguistic specifications still contain noise  , we use statistical scores as an additional tilter in ofder to choose the ~ good " ones among the candidate base MWU  ( or candidates for short )  . 
These scores apply on candi & ~ tclcmma pairs for a given morphosyntactic pattern  . We con > puted different scores : frequencies , associa . tiol ~ criteria , Shafl nondiversity and distance mea ~ sures . We discuss frequencies and association criteria below  . Informations provided by Shannon diversity and distance measures are presented in \[ Daille  ,  1994\] . Most of the association criteria can be found in the classic literature  , such as \[ Clifford and Stephenson ,  1975\] , and are based on the socalled " contingency tables "  . In the field of eomputation a . 1 linguistics , mutual information \[ Brown et al ,  1988\] , ?2\[Church and Hanks ,  1990\] , or a likelihood ratio test\[Dunning , 199 a \] are suggested . 
Our testing method consists in comparing our result list  , sorted according to a specified score , with a reference list containing only valid terms  . In order to l ) uild the reference list , we augmented an existing terminology database ( EURODICAUTOM ) with handwork : we selected those candidates for which at least two judges out of three agreed on their goodness  , and included them in the reference llst . A candidate will be considered " good " when it is found in this reference list  . 
The program for the extraction of candidates was run on a  240  , 000 words French corpus , divided into 9 , 541 sentences . It yiehled 2 , 4 00 candidates that appear at least three times in the text  . After sorting the candidates according to the particular score examined  , we build a graphic representation of the prol ) ortion of " good " candidates per range of score w ~lues  , in the form of a histogram . Figure 1 shows the histograms obt Mned on the French Iqt de  1~2 candidates with two different scores : the likelihood ratio and mutuM information  . The vertical axis , which represents the proportion of good candio dates  , is bounded between 0 and 1 . llut , since we do not pretend to have built an exhaustive reference list  , we can't actually expect values of more than 0 . 8 . The horizont Maxis grows with the score . The likelihood ratio test ( LOG )   ( the use of a likelihood ratio test leads to a log likelihood statistics  , which explainstile LOG abbre-vi:ttion ) shows a curve that grows slowly at tirst , and then decidedly . There is a true ol ) position ill the behaviours of small and big v ~ Llues  , the representativity of good candidates approaching  0  . 8 in that case . Ontile contrary , mutual infor-mat , ion(MI ) is quite uniforme iy distributed , and the representativity never exceeds 0 . 5 . Thus , if one had to choose between these two scores , the first one would undoubtedly be selected . 
The outcome of tile study of tile histograms is that very few searing methods will meet our objective  . The most signilicant seem to be t it (' . 
likelihood ratio test , and the number of occur-fences of the pair . If it were for the sole histograms , one could be tempted to keep only the frequency of the pair  , but due to its definition unfrequent terms won't stand out  ; therefore , we have to keep several methods . Anyhow , the performance of our scoring functions indicate tile best method to follow : start any task  ( human postediting for example ) with the highest frequency values on monolingual candidates previously identified via linguistic patterns  , thus maximizing the odds that the candidates will indeed be " good " candidates  . 
Is the best score a combination of s ( : or es ? An extra data analysis method To complete the evaluation and comparison of the different scores used to select actual ternls from our candidates list  , we have applied to these scores specific data analysis methods  . In our case , tile techniques of data an Mysis can provide at least two things : a probabill stic study of the correlations between scores  , in order to select tile relevant ones , and the possibility of coml ) ining scores , in order to improve the results . 
We carried out Principal Component Analysis ( PCA ) with a set of fifteen variables , i . e . tile scores , on a set of 1, 230 individuals , i . e . the can -0 . 6 0  . 4 0 . 2 0  . 4 0 . 5 0 . 2 0 . 10 t . . . .
Figure 1: It is to grams corresponding to the loglikelihood ( left )  , and tomntnal information ( right ) didates . The first objective of PCA is to reduce the number of variables with as little loss of information as possible  . Doing so shouhlenable us to visualize our data more easily  . From the study of the correlation matrix of variables and their part in the principal axis  , it appeared that variables could be divided into four classes  . In each class , we wanted to retain the most significant variable with regard to our task  . This was done by selecting in each class the variable with the best score-histogram  . All other w ~ riables can be confidently connected to one of these  . 
With the remaining four variables we tried to investigate the set of candidates  . The difficulty of this task comes from the high nuln ber of candidates  , owe decided to work on smaller sets . We divided the whole set of candidates into six subsets depending on the number of occurrences of the candidate  ( respectively with a number of occurrences in the text equal to  2  ,  3 ,  4 ,  5 , between 6 and 10 , and over 10) . This division was used to counterbalance the fact that most scores tend to privilege low frequencies  . 
We then visualized the candidates graphically ( with different tick marks distinguishing terms and nonterms  ) on several two-dimensional diagrams , the axis of which are linear combinations of the different variables  . The results show series of marks overlapping each other  , without any clear separation between terms and nonterms  . 
The PCA allowed us to go further in the study of the statistical scores  , and confirmed that only a few are relevant o our task  ; moreover , we found no satisfactory combination of variables that could account for term hood  . 
4 Bilingual candidate term alignment
The problenl is no wone of finding correspondences between candidates across languages  ( between English and I " rench in our case )  . As it has been shown in the previou section , it is difll cult to restrict the set of candidates to the " good " ones  . l ' ~ urthermore , if a candidate is really a term , it shouht be easy to find the equivalent in other languages  , or more precisely , in our case , in aligned sentences ( supposing that terms are always translated into the same lexical unit  )  . 
For these reasons , we tried to pick out associations between the whole sets of candidates  , both l ? rench and English . Sticking to our general philosophy of harmonizing linguistic and statistical contributions  , we made experiments with two methods . 
4 . 1 A simple count method , and a possible improvement This method basically counts how often a source candidate and a target candidate occur in aligned sentences  ; we shall cMl the resulting score bilingual count . For a given source language candidate , the most frequently aligned target candidates are sorted according to the score  ; one can then apply the criterion defined in \[ Gaussier et al  ,  1992\] , which consists in extracting a bilingual pair only when the target candidate has no stronger association with any other source candidate  . Such a naive frequency collection gives satisfactory results  , providing a sorted list where the top half candidates are that result by taking adv  , ' tntage of available linguistic in for If fation , namely the part-of-speech patterns of the candidates  . We estimated , by counting pattern alignments on a bilingual corpus  , pattern aJ finity , which we define as the probability that the pattern of the source candidate gets translated into the pattern of the target candidate  ( e . g . terms with pattern NdcN in French get translated to terms with pattern NN in English in  81% of the eases )  . Now , instead of simple counts of the occurrences of the source and target candidates  , we use counts that are weighted with a function of the pattern aft in-ity of candidates  , as explained in the following formula : p , ,o: , ~ . ~( p , t7, . , ) x c(t)C(s , t ) = ~, .  ~ . p , . ol , ~4>1>) xc (>) where C ( ) means " count of " , and p " pattern of " ; probes stands for the probabilities wees-t imated  , and sent represents the set of aligned sentences in which both the source candidate  , s , and the target candidate , t , appear . Results are discussed below . 
4 . 2 Another method : us ing word a l ignment scores This method consists essentially in aligning terms using bilingual associations of single words  , that have been previously cmnputed through statistical methods  ( see\[Gaussier et al ,  1992\]) . Let word cnl and worden2 denote the two words composing an English candidate an  ( l , similarly , wordfrl and wordfr2 those for a given French candidate . 
Each time a source and target candidate occur in aligned sentences  , we define the association score between the two candidates as the sum of all the possible single -word-associations between the words composing the candidates : 
Assoc(wordenl , wordfrj )

This method relics on the assumption that if two terms are translation of one another  , then word frl is likely to be the translation of , or at least associated to , either word cnlor word cn2 , and so is word fr > For example , l ) oth French words station and terrienne belong to the lists of single-word translation candidates associated to the English words earth and station  . 
4.3 It\]valuation and results
To evaluate the results , we manually constructed a reference list which contains  1  , 2 38 correspondences of MWU2 we believe to be terms ( a . ppearing at least twice ) . For any list of candidate pairs to be tested ( extracted and sorted according to some scores )  , we define its precision as the ratio of the number of elements found in the reference list to the total numl  ) er of elements , and its recall as the ratio of the number of dements found in the reference list to the total number of elements in the reference list  . In so far as these parameters ; u'e affected by the length of the list , we divided each list into units of the same length  , considering the first 100 elements , the lirst 2 (0 elements and so on , till we reach the end of one list . For each unit , we calcula . tedits recall a . nd its precision . The scores can then be compared in graphs showing the evolution of precision and recall with the number of candidates considered  . Using such graphs , we found out that adding pattern probabilities gives only slightly better or equal results than the silnple frequency method  , which does not entirely satisfy our expectations as to their role on the quality of term alignment  . 
The second method presented above , which does not make use of pattern probabilities , behaves better for candidates of rank 1 , 00 ( and more , hut worse for the first 100 best candidates . The examination of the top 100 candidates for both methods brings another surprise : only  6 term pairs are cmnm onto the two metLods . Of course this could be due to the restriction to the very hest candidates  ; but , suspecting that there is nmret o it , we combined the candidates from both methods and thus obtained an improvement of precision in all the range of candidates  . The comparison between this combination of methods and the method involving pattern at linities is shown in figure  2  . The plain line in figure 2 shows precision and recall for the first method involving pattern aftinitles  ( which is referred to as normal on the figure )  . We think that such results could be even better if we chose the right combination of two-or  , possibly , more than two-scoring methods . 
All in all , our method provides 70% precision for a list of more than 1  , 00 ( candidates , or 80% if one restricts to the topmost 500 . it gives a good starting point for terminologists  ~-   60   , q . o " ~40,1"" . . . . J " \[__~_ normal method \] / .   . ~ a " j ~"\[_ , . combined melh0ds J,I,\],I, . I25050875010001250
N " of candidates
Figure 2: Precision and Recall when using first method vs . combined methods . 520 seeking to extract bilingual terms from texts . 
It has to be noted that the remaining " wrong " alignments are generally somehow relevant  , but either incomplete because one of the candidates has a length of more than  2  , or trivial because we also extract pairs such as following formula/formules uivante  . 
5 Conclusion
For monolingual terminology extraction , we have first defined the linguistic specifications of multiword unit terms  . These specifications are used to extract candidate pairs from the corpora  . I ~ om these pairs and using frequency counts , we have applied a range of statistic ~ d scores , in order to find the score that would best discriminate between terms and nonterms among the candidates  . The simple frequency count of the pair turns out to be the best score  . We then carried out bilingual terminology extraction using results from then m no lin-gum step  . Two scoring methods augmented by the use of prior knowledge about the bilingual correspondences of linguistic patterns were applied to bilingual pairs of monolingua\[candidate pairs  . In this case , the best way to proceed is derived from the combination of the two methods  . 
With regard to the future , we will focus on monolingual extraction of MWU of lengtha and more  ( based on the identified base MWU and linguistic specifications on term composition  )  , and on the improvement of term alignment in order to deal with terms of heterogeneous length  . 
References\[Benveniste , 1966\]Benveniste B .  (1966) . 
Formes nouvelles del~t composition omi-nale . \[ n Gallimard Ed . , Probldmes delin-gulstique gdndrale , pp .  163-173 . 
\[Bourigault , 1992\]BourigaultD .  (1992) . Surface Grammatical Analysis for the Extraction of Terminologic MNoun Phrases  . Proceedings of the l~lh International Conference on Computational Linguistics  . 
\ [ Brown et el . , 1988\] Brown , P . F . , Co&e , J . , I ) ella Pietra , S . A . , Della Pietra , V . J . , 3t-linek , F . , Mercer , R . L . , and Roossin , P . S . 
(1988) . A statistical approach to language translation . Proceedings of the 12th In-ternationol Conference an Computational

\[Church and llanks , 1990\] Church K . W . and I lanks P . , 1990\] (1990) Word Association Norms , Mutual Information , And Lexicography . Computational Linguistics , Vol .  16,
Number I.
\[ Olifford and Stephenson , 1975\]Clifford II . T . and Stephenson W .   ( 1975 ) An Introduction to Numerical Classification , Academic Press . 
\[Daille , 1994\]Daille B .  (1994) . Study and fin-plementation of combined techniques for Automatic Extraction of Terminology  . In
The Balancing Act : Combining Symbolic and Statistical Approaches to Language  . 
Workshop at the 32nd Annual Meeting of the ACL.
\[ Dunning , 1993\] Dunning T .  (1993) . Accur at , , Methods for the Statistics of Surprise attd Coincidence  . Computational \], inguis-tics , Vol . 19, N~m ~ ber1 . 
\[G , ~ ussiertal . , 1992\] Gaussier E . , Langd J . -M . 
and Meunier F . (1992). Towards Bilingual
Terminology . Proceedings of ALLC/ACH
CO@FCT"tCe,\[Ilatcher , 1960\] Hatcher A . J .  (1960) . An introduction to the analysis of English noun compounds  . In Word , i 6, 356-373 . 
\[J , ~ cquemin , 1991\]J acquem in C .  (1991) . Transformation desnoms composds . Th~sededoctoraten Informatique Fondamentale,
Universitd Paris Z\[Mathieu-Colas , 1988\] M , ~thieu-Colas M . 
(1988). Typologie des noms composds.
Technical l ~. eport of the " Programme de
Reeherches Coordonn ~ esInformatique Lin-guistiquc  '  , C . N . R . S . 
\[Nkwenti-Azeh , 1992\]Nkwenti-Azeh B . (1992).
Positional and Combinational Characteristics of S ~tellite Communications Terms  . In-terim Report , CCL-UMIST . 
\[Smadj and McKeown , 1990\] Slnadja I ". A.
and McKeown K .  11~ .  (1990) . Automatically Extracting And Representing Collocations For Language Generation  . Proceedings of the 28th annual Meeting of th cACL . 
\[ Vander Eijk , 1993\] V~m der Eijk P .   ( I903 ) Automating the Acquisition of Bilingu M Terminology  . Proceedings of Em'opean ACL . 

Computational Linguistics
