BackTransliteration from Japanese to English
Using Target English Context
Isao Goto ? , Naoto Kato ?? , Terumasa Ehara ??? , and Hideki Tanaka ?
? NHK Science and Technical
Research Laboratories
1-11-10 Kinuta , Setagaya,
Tokyo , 157-8510, Japan


? ? ATR Spoken Language Trans-
lation Research Laboratories
2-2-2 Hikaridai , Keihanna
Science City , Kyoto , 619-0288,


? ? ? Tokyo University of
Science , Suwa
5000-1, Toyohira , Chino,
Nagano , 391-0292, Japan




This paper proposes a method of automatic back transliteration of proper nouns  , in which a Japanese transliterated-word is restored to the original English word  . The English words are created from a sequence of letters  ; thus our method can create new English words that are not registered in dictionaries or English word lists  . When a katakana character is converted into English letters  , there are various candidates of alphabetic characters  . To ensure adequate conversion , the proposed method uses a target English context to calculate the probability of an English character or string corresponding to a Japanese katakana character or string  . We confirmed the effectiveness of using the target English context by an experiment of personal-name back transliteration  . 
1 Introduction
In transliteration , a word in one language is converted into a character string of another language expressing how it is pronounced  . In the case of transliteration into Japanese , special characters called katakana are used to show how a word is pronounced  . For example , a personal name and its transliterated word are shown below  . 
Cunningham ????? ( kaningamu ) [ Transliteration]Here , the italic alphabets are romanized Japanese katakana characters  . 
New transliterated words such as personal names or technical terms in katakana are not always listed in dictionaries  . It would be useful for cross-language information retrieval if these words could be automatically restored to the original English words  . 
Backtransliteration is the process of restoring transliterated words to the original English words  . 
Here is a problem of back transliteration.
? ?????????  ( English word )   ( kuracchifi?rudo ) [ Backtransliteration ] There are many ambiguities to restoring a transliterated katakana word to its original English word  . For example , should " a " in " kuracchifi ? rudo " be converted into the English letter of " a " or " u " or some other letter or string ? Trying to resolve the ambiguity is a difficult problem  , which means that back transliteration to the correct English word is also difficult  . 
Using the pronunciation of a dictionary or limiting output English words to a particular English word list prepared in advance can simplify the problem of back transliteration  . However , these methods cannot produce a new English word that is not registered in a dictionary or an English word list  . Transliterated words are mainly proper nouns and technical terms  , and such words are often not registered . Thus , a back transliteration framework for creating new words would be very useful  . 
A number of backtransliteration methods for selecting English words from an English pronunciation dictionary have been proposed  . They include Japanese-to-English ( Knight and Graehl ,  1998) 1  , Arabic-to-English ( Stalls and Knight ,   1 Their English letter-to-sound WFST does not convert English words that are not registered in a pronunciation dictionary  . 
1998) , and Korean-to-English ( Lin and Chen ,  2002) . 
There are also methods that select English words from an English word list  , e . g . , Japanese-to-English ( Fujii and Ishikawa , 2001) and Chi-nese-to-English ( Chen et al ,  1998) . 
Moreover , there are back transliteration methods capable of generating new words  , there are some methods for back transliteration from Koreanto English  ( Jeong et al , 1999; Kang and
Choi , 2000).
These previous works did not take the target English context into account for calculating the plausibility of matching target characters with the source characters  . 
This paper presents a method of taking the target English context into account to generate an English word from a Japanese katakana word  . 
Our character-based method can produce new English words that are not listed in the learning corpus  . 
This paper is organized as follows . Section 2 describes our method . Section 3 describes the experimental setup and results . Section 4 discusses the performance of our method based on the experimental results  . Section 5 concludes our research . 
2 Proposed Method 2 . 1 Advantage of using English context First we explain the difficulty of back transliteration without a pronunciation dictionary  . Next , we clarify the reason for the difficulty . Finally , we clarify the effect using English context in back transliteration  . 
In back transliteration , an English letter or string is chosen to correspond to a katakana character or string  . However , this decision is difficult . 
For example , there are cases that an English letter " u " corresponds to " a " of katakana  , and there are cases that the same English letter " u " does not correspond to the same " a " of katakana  . " u " in Cunningham corresponds to " a " in katakana and " u " in Bush does not correspond to " a " in katakana  . It is difficult to resolve this ambiguity without the pronunciation registered in a dictionary  . 
The difference in correspondence mainly comes from the difference of the letters around the English letter " u  . " The correspondence of an English letter or string to a katakana character or string varies depending on the surrounding characters  , i . e . , on its English context . 
Thus , our back transliteration method uses the target English context to calculate the probability of English letters corresponding to a katakana character or string  . 
2 . 2 Notation and conversion-candidate lattice We formulate the word conversion process as a unit conversion process for treating new words  . 
Here , the unit is one or more characters that form a part of characters of the word  . 
A katakana word , K , is expressed by equation 2 . 1 with "^" and "$" added to its start and end , respectively . 

mmkkkk ++== K(2 . 1) 0^k =, 1$mk += (2 . 2 ) where jk is the jth character in the katakana word  , and m is the number of characters except for "^" and "$" and  10 mk + is a character string from 0k to 1mk  +  . 
We use katakana units constructed of one or more katakana characters  . We denote a katakana unit ask u . For any ku , many English units , eu , could be corresponded as conversion-candidates . 
The ku's and eu's are generated using a learning corpus in which bilingual words are separated into units and every kuunit is related a neu unit  . 
EL denotes the lattice of all eu's corresponding toku's covering a Japanese word  . Every eu is a node of the lattice and each node is connected with next nodes  . EL has a lattice structure starting from "^" and ending at "$  . " Figure 1 shows an example of EL corresponding to a katakana word "?????????  ( kirushushutain )  . " In the figure , each circle represents one eu . 
A character string linking individual character units in the paths  1   2  (   ,   ,  . . ,   ) dqpppp ? between "^" and "$" in EL becomes a conversion candidate  , where q is the number of paths between "^" and "$ " in EL  . 
We get English word candidates by joining eu's from "^" to "$" in EL  . We select a certain path , pd , in EL . The number of character unit scchi ? ( k i ) chicicki cykkekhiki kiekiikyquichche chouchuss cs chs chush ?  ( ru )  ?? ( shu )  ?? ( shu )  ? ( ta )  ? ( i )  ? ( n ) tatad tagteter that i to t tatue h i ij iy yeh yimmon mpnnengnghninnnnent nwt ??  ( tai ) lld leles le wlll ellul oul urr cr drerg roor our rrrer tlusheshus usys zch chechou chusscschschush sheshus usys z tajtay tey titietyty e ? ? ? ? ? ? ? ? ? ? ? ? ? ? ??????^$ Figure  1: Example of lattice EL of conversion candidates units  . 
except for "^" and "$" in pd is expressed as (   ) dnp . 
The character units in pd are numbered from start to end  . 
The English word , E , resulting from the conversion of a katakana word , K , for pd is expressed as follows : m m k k k k + += = K  (   )  1 0 0 1  (   )  1 . .
ddnpnp + += ku = kukuku , (2 . 3) ( ) 1 0 0 1 ( ) 1 . .
ddl plpee e e ++ == E ( ) 100 1() 1..
ddnpnp + += eu = eueu , (2 . 4) 0000^ke ==== kueu ,  1  (   )  1  (   )  1  (   ) 1 $ d d d m l p n p n p k e + + + + = = == k u e u ,  (2 . 5 ) where ej is the jth character in the English word . 
(  ) dlp is the number of characters except for "^" and "$" in the English word  .   (   ) 10 dnp + eu for each pd in EL in equation 2 . 4 becomes the candidate
English word . ( ) 10 dnp + k u in equation 2 . 3 shows the sequence of katakana units . 
2 . 3 Probability models using target English context To determine the corresponding English word for a katakana word  , the following equation 2 . 6 must be calculated : ? argmax ( ) P

E = EK . (2.6)
Here , E ? represents an output result.
To use the English context for calculating the matching of an English unit with a katakana unit  , the above equation is transformed into Equation 2 . 7 by using Bayes ? theorem . 
? argmax ( )( ) PP =

EEKE (2.7)
Equation 2 . 7 contains a translation model in which an English word is a condition and katakana is a result  . 
The word in the translation model (    ) PKE in Equation 2 . 7 is broken down into character units by using equations  2  . 3 and 2 . 4 . 
   (   )  1  (   )  1 0 0  (   )  1  (   )  1 0 0  (   )  1  (   )  1 0 0  (   )  1  (   )  1 0 0  (   )  1  (   )  1  (   ) 1000 ? argmax (   )   (   ,   , ) argmax ( )( ,   ,  ) (   ,   )   (    ) d d n p n p d d d n p n p d d d n p n p n p n pnp np np 




PP+++++++++++=?=??????
Eeuku
Eeuku
EE
Kkueu E

Kkueu Ekueu Eeu E(2 . 8 )   (   ) 1 affected by (   ) 1 rewritten as follows: (   )  1  (   )  1 0 0  (   )  1 0 0 0  . 
? argmax (   )   (    )   (    )   (    ) d n p n p d d d n p n p n p n p


PP++++++??????=????
Eeuku
EE
Kkukueueu E(2 . 9 )   (   )  1 0 (    ) dnpP+Kk u is 1 when the string of K and (   )  1  (   ) 1 of the K is the same . Thus , ()10() dnpP+Kk u is always 1 . 
We approximate the sum of paths by selecting the maximum path  . 
( ) 1 ( ) 100 ( ) 1 ( ) d d d n p n p n p
PP
P + + + ? ?

EEkueueu E(2.10)
We show an instance of each probability model with a concrete value as follows:  (   )   ( ^ C rutch field $ ) 


E , ()10()(^^/)()(/////) dnpP
Pkuracchi firu do kuracchi firu do + ??
Kku ?????????$?/?/???/?/?/$ ,   (   )  1  (   )  1 0 0 (    )   ( ^/^/ C/ru/tch/fie/l/d /$ )   (  / / / / /  ) ddnpnpP
Pkuracchifirudo + + ? kueu ?/?/? ?/???/?/?/ $   ,   (   )  1 0 (    )   ( ^/ C/ru/tch/fie/ld /$^ Crutch field $ ) dnpP
P + eu E.

We broke down the language model (   ) PE in equation 2 . 10 into letters . 
( ) 1 ( )( ) d l p j j j a j
PeeP + ? ? = ? ? E (2.11)
Here , a is a constant . Equation 2 . 11 is an ( a + 1)-gram model of English letters . 
Next , we approximate the translation model (   )  1  (   )  1 0 0 (    ) ddnpnpP + + kueu and the chunking model (   )  1 0 (    ) dnpP + euE . For this , we use our previously proposed approximation technique  ( Goto et al ,  2003) . The outline of the technique is shown as follows . 
(  )  1  (   )  1 0 0 (    ) ddnpnpP + + kueu is approximated by reducing the condition  . 
( ) 1 ( ) 1 0 0 ( ) 1 ( ) 11 0 0 (   , ) d d d n p n p n p n p i i i

P++++?==?kueukukueu (   )  1  (   )  1  (   )   (   ) 1 dnp start ii startibiendii
Pee+??+=??kueu (2 . 12 ) where start ( i ) is the first position of the ith character unit eui  , while end ( i ) is the last position of the ith character unit eui  ; and b is a constant . 
Equation 2 . 12 takes English context (   )  1 (   ) starti startibe ?? and (   ) 1 endie + into account . 
Next , the chunking model (   )  10 (    ) dnpP + euE is transformed . All chunking patterns of (   ) 10 dlpe += E into (   ) 10 dnp+eu are denoted by each l ( pd ) +1 point between l ( pd ) + 2 characters that serve or do not serve as delimiters  . eu0 and ()1 dnp+eu are determined in advance . l(pd)-1 points remain ambiguous . We represent the value that is delimiter or is non-delimiter between ej and  ej+1 by zj . We call the zj delimiter distinction . delimiter non-delimiter jz = (2 . 13) Here , we show an example of English units using zj . 
( e1 e2 e3 e4 e5 e6 e7 e8 e9 e11)
Crutch field ( z1 z2 z3 z4 z5 z6 z7 z8 z9 z10 )  / / / / 1 0 1 0 0 1 0 0 1 1

Values of zj:/
In this example , a delimiter of zj is represented by 1 and a non-delimiter is represented by 0  . 
The chunking model is transformed into a processing per character by using zj  . And we reduce the condition . 
( ) 1 0 0 ( ) 1 ( ) 11 0 0 (  ) (   , ) d d d d n p l p l p l p l p j j j

Pze
Pzze+?+?+?===?euE()111 ( , ) d l p j j j j c j c j
Pzze ? ? + ? ? ? = ? ? (2.14)
The conditional information of the English 1j j ce + ? is as many asc characters and 1 character before and after zj , respectively . The conditional information of 1   1 jj cz ??? is as many as c+1 delimiter distinctions before zj . 
By using equation 2 . 11, 2 . 12, and 2 . 14, equation 2 . 10 becomes as follows: (   )  1  (   )   (   )  1  (   )   (   ) 111 ? argmax (    )   (    ,   ,  ) (   ,  ) . 
d d d l p j j j a j n p s t art i i startibiend i i l p j j j j c j c j 
Pee
Pee
P z z e + ? ? = ? ? + = ? ? + ? ? ? ? ? ? ? ?

Ekueu (2.15)
Equation 2 . 1 5 is the equation of our back transliteration method  . 
2 . 4 Beam search solution for context sensitive grammar Equation  2  . 15 includes context-sensitive grammar . As such , it cannot be carried out efficiently . 
In decoding from the head of a word to the tail , e end(i)+1 in equation 2 . 15 becomes context-sensitive . Thus we try to get approximate results by using a beamsearch solution  . To get the results , we use dynamic programming . Every node of eu in the lattice keeps the Nbest results evaluated by using a letter of e end  ( i ) + 1 that gives the maximum probability in the next letters  . When the results of next node are evaluated for selecting the Nbest  , the accurate probabilities from the previous nodes are used  . 
2 . 5 Learning probability models based on the maximum entropy method The probability models are learned based on the maximum entropy method  . This makes it possible to prevent data sparseness relating to the model as well as to efficiently utilize many conditions  , such as context , simultaneously . We use the Gaussian Prior ( Chen and Rosenfeld , 1999) smoothing method for the language model . We use one Gaussian variance . We use the value of the Gaussian variance that minimizes the test set's perplexity  . 
The feature functions of the models based on the maximum entropy method are defined as combinations of letters  . In addition , we use vowel , consonant , and semivowel classes for the translation model . We manually define the combinations of the letter positions such as ej and  ej-1  . 
The feature functions consist of the letter combinations that meet the combinations of the letter positions and are observed at least once in the learning data  . 
2.6 Corpus for learning
A JapaneseEnglish word list aligned by unit was used for learning the translation model and the chunking model and for generating the lattice of conversion candidates  . The alignment was done by semiautomatically . A romanized katakana character usually corresponds to one or several English letters or strings  . For example , aromanized katakana character " k " usually corresponds to an English letter " c  , "" k , "" ch , " or " q . " With such heuristic rules , the JapaneseEnglish word corpus could be aligned by unit and the alignment errors were corrected manually  . 
3 Experiment 3 . 1 Learning data and test data We conducted an experiment on back transliteration using English personal names  . The learning data used in the experiment are described below  . 
The Dictionary of Western Names of 80 , 0 00   People2 was used as the source of the JapaneseEnglish word corpus  . We chose the names in alphabet from A to Z and their corresponding katakana  . The number of distinct words was 39 , 830 for English words and 39 , 562 for katakana words . 
The number of English-katakana pairs was 83,0573 . We related the alphabet and katakana character units in those words by using the method described in section  2  . 6 . We then used the corpus to make the translation and the chunking models and to generate a lattice of conversion candidates  . 
The learning of the language model was carried out using a word list that was created by merging two word lists : an American  personal-2 Published by Nichigai Associates in Japan in 1994  . 
3 This corpus includes many identical English -katakana word pairs  . 
name list 4 , and English head words of the Dictionary of Western Names of  80  , 000 people . The American name list contains frequency information for each name  ; we also used the frequency data for the learning of the language model  . A test set for evaluating the value of the Gaussian variance was created using the American name list  . The list was split 9:1 , and we used the larger data for learning and the smaller data for evaluating the parameter value  . 
The test data is as follows . The test data contained 333 katakana name words of American Cabin et officials  , and other high-ranking officials , as well as high-ranking governmental officials of Canada  , the United Kingdom , Australia , and New Zealand ( listed in the World Yearbook 2002 published by Kyodo News in Japan )  . The English name words that were listed along with the corresponding katakana names were used as answer words  . Words that included characters other than the letters A toZ were excluded from the test data  . Family names and First names were not distinguished  . 
3.2 Experimental models
We used the following methods to test the individual effects of each factor of our method  . 
? Method A
Used a model that did not take English context into account  . The plausibility is expressed as follows : (   ) dnpiii
P == ?

Eeuku . (3.1) ? Method B
Used our language model and a translation model that did not consider English context  . The constant a = 3 in the language model . The plausibility is expressed as follows : (   )  1  (   ) 11 ? argmax (    )   (    ) d d l p n p j j j i i j i
P e e P + ? ? = = = ? ?

Ekueu.
(3.2) ? Method C
Applied our chunking model to method B , with c = 3 in the chunking model . The plausibility is expressed as follows : 4 Prepared from the 1990 Census conducted by the U . S . 
Department of Commerce . Available at http://www . census . gov/genealogy/names / . The list includes 91, 910 distinct words . 
(  )  1  (   ) 11 ? argmax (    )   (    ) d d l p n p j j j i i j i
P e e P + ? ? = = = ? ?

E k u e u ( ) 1 1 4 4 3 d l p j j j j j j
P z z e ? ? + ? ? = ? ? (3 . 3 ) ? Method D Used our translation model that considered English context  , but not the chunking model . b = 3 in the translation model . The plausibility is expressed as follows : (   )   (   )  1  (   )  1  (   )  3  (   )  1   ,   ,   . 
ddlpjj jj np start ii startii end ii
Pee
P e e + ? ? = ? ? + = = ? ? ?

Ekueu (3.4) ? Method E
Used our language model , translation model , and chunking model . The plausibility is expressed as follows : (   )   (   )  1  (   )  1  (   )  3  (   )  1 1 4 4 3   ,   ,  (   ,  ) . 
d d d l p j j j j n p s t art i i start i i end ii lp j j j j j j 
Pee
Pee
P z z e + ? ? = ? ? + = ? ? + ? ? = = ? ? ? ? ?

Ekueu (3.5) 3.3 Results
Table 1 shows the results of the experiment5 on back transliteration from Japanese katakana to English  . The conversion was determined to be successful if the generated English word agreed perfectly with the English word in the test data  . 
Table 2 shows examples of back transliterated words . 
Method A B C D E
Top1 23.7 57.4 61.6 63.1 66.4
Top2 34.8 69.1 72.4 71.8 74.2
Top3 42.9 73.6 76.6 75.4 79.3
Top5 54.1 77.5 79.9 80.8 83.5
Top10 63.4 82.0 85.3 86.5 87.7
Table 1: Ratio ( % ) of including the answer word in high-ranking words  . 
5 For model D and E , we used N = 50 for the beam search solution . In addition , we kept paths that represented parts of words existing in the learning data  . 
Japanese katakana ( romanized katakana )
Created English ??????? ( ashukuro futo)
A shcr of t ????????? ( kiru shus hutain )
Kirschstein ????? ( supensa-)
Spencer ???? ( paueru )
Powell ????? ( purinshipi )

Table 2: Example of English words produced.
4 Discussion
The correct-match ratio of the method E for the first-ranked words was  66%  . Its correct-match ratio for words up to the 10th rank was 87%  . 
Regarding the top1 ranked words , method B that used a language model increase the ratio  33-points from method A that did not use a language model . This demonstrates the effectiveness of the language model  . 
Also for the top1 ranked words , method C which adopted the chunking model increase the ratio  4-points from method B that did not adopt the chunking model in the top  1 ranked words . 
This indicates the effectiveness of the chunking model  . 
Method D that used a translation model taking English context into account had a ratio  5-points higher in top 1 ranked words than that of method B that used a translation model not taking English context into account  . This demonstrates the effectiveness of the language model  . 
Method E gave the best ratio . Its ratio for the top 1 ranked word was 42-points higher than method A's . 
These results demonstrate the effectiveness of using English context for back transliteration  . 
5 Conclusion
This paper described a method for Japanese to English back transliteration  . Unlike conventional methods , our method uses a target English context to calculate the plausibility of matching between English and katakana  . Our method can treat English words that do not exist in learning data  . We confirmed the effectiveness of our method in an experiment using personal names  . 
We will apply this technique to cross-language information retrieval  . 

Hsin-Hsi Chen , Sheng-Jie Huang , Yung-Wei Ding , and Shih-Chung Tsai .  1998 . Proper Name Translation in CrossLanguage Information Retrieval  .   36th Annual Meeting of the Association for Computational Linguistics and  17th International Conference on Computational Linguistics  , pp . 232-236 . 
Stanley F . Chen , Ronald Rosenfeld .  1999 . A Gaussian Prior for Smoothing Maximum Entropy Models  . 
Technical Report CMU-CS-99-108 , Carnegie Mellon University . 
Bonnie Glover Stalls and Kevin Knight .  1998 . Translating Names and Technical Terms in Arabic Text  . 
COLING/ACL Workshop on Computational Approaches to Semitic Languages  . 
Isao Goto , Naoto Kato , Noriyoshi Uratani , and Teru-masa Ehara .  2003 . Transliteration Considering Context Information based on the Maximum Entropy Method  . Machine Translation Summit IX , pp . 125-132 . 
Kil Soon Jeong , Sung Hyun Myaeng , Jae Sung Lee , and Key-Sun Choi .  1999 . Automatic Identification and Back-Transliteration of Foreign Words for Information Retrieval  . Information Processing and
Management , Vol . 35, No . 4, pp . 523-540.
Byung-Ju Kang and Key-Sun Choi .  2000 . Automatic Transliteration and Back -Transliteration by Decision Tree Learning  . International Conference on Language Resources and Evaluation  . pp . 1135-1411 . 
Kevin Knight and Jonathan Graehl .  1998 . Machine Transliteration . Computational Linguistics , Vol . 24,
No . 4, pp . 599-612.
Wei-Hao Lin and Hsin-Hsi Chen .  2002 . Backward Machine Transliteration by Learning Phonetic Similarity  . 6th Conference on Natural Language
Learning , pp . 139-145.
Atsushi Fujii and Tetsuya Ishikawa .  2001 . Japa-nese/English CrossLanguage Information Retrieval : Exploration of Query Translation and Transliteration  . Computers and the Humanities,
Vol . 35, No . 4, pp . 389-420.
