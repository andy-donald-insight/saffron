ORANGE : a Method for Evaluating Automatic Evaluation Metrics 
for Machine Translation
Chin-Yew Lin and Franz Josef Och
Information Sciences Institute
University of Southern California
4676 Admiralty Way
Marina del Rey , CA 90292, USA



Comparisons of automatic evaluation metrics for machine translation are usually conducted on corpus level using correlation statistics such as Pearson?s product moment correlation coefficient or Spearman?s rank order correlation coefficient between human scores and automatic scores  . However , such comparisons rely on human judgments of translation qualities such as adequacy and fluency  . Unfortunately , these judgments are often inconsistent and very expensive to acquire  . In this paper , we introduce a new evaluation method , ORANGE , for evaluating automatic machine translation evaluation metrics automatically without extra human involvement other than using a set of reference translations  . We also show the results of comparing several existing automatic metrics and three new automatic metrics using 

1 Introduction
To automatically evaluate machine translations , the machine translation community recently adopted an ngram cooccurrence scoring procedure BLEU  ( Papineni et al 2001 )  . A similar metric , NIST , used by NIST ( NIST 2002 ) in a couple of machine translation evaluations in the past two years is based on BLEU  . The main idea of BLEU is to measure the translation closeness between a candidate translation and a set of reference translations with a numerical metric  . 
Although the idea of using objective functions to automatically evaluate machine translation quality is not new  ( Su et al 1992 )  , the success of BLEU prompts a lot of interests in developing better automatic evaluation metrics  . For example , Akiba et al ( 2001 ) proposed a metric called RED based on edit distances over a set of multiple references  . 
Nie?en et al ( 2000 ) calculated the length-normalized edit distance , called word error rate ( WER ) , between a candidate and multiple reference translations  . Leusch et al ( 2003 ) proposed a related measure called position -independent word error rate  ( PER ) that did not consider word position , i . e . using bag-of-words instead . Turian et al ( 2003 ) introduced General Text Matcher ( GTM ) based on accuracy measures such as recall , precision , and Fmeasure . 
With so many different automatic metrics available  , it is necessary to have a common and objective way to evaluate these metrics  . 
Comparison of automatic evaluation metrics are usually conducted on corpus level using correlation analysis between human scores and automatic scores such as BLEU  , NIST , WER , and PER . 
However , the performance of automatic metrics in terms of human vs  . system correlation analysis is not stable across different evaluation settings  . For example , Table 1 shows the Pearson?s linear correlation coefficient analysis of  8 machine translation systems from 2003 NIST ChineseEnglish machine translation evaluation  . The Pearson ? correlation coefficients are computed according to different automatic evaluation methods vs  . human assigned adequacy and fluency . BLEU 1 ,  4 , and 12 are BLEU with maximum ngram lengths of 1 ,  4 , and 12 respectively . GTM 10, 20, and 30 are GTM with exponents of 1 . 0, 2 . 0, and 3 . 0 respectively .   95% confidence intervals are estimated using bootstrap resampling  ( Davison and Hinkley 1997 )  . From the BLEU group , we found that shorter BLEU has better adequacy correlation while longer BLEU has better fluency correlation  . GTM with smaller exponent has better adequacy correlation and GTM with larger exponent has better fluency correlation  . 
NIST is very good in adequacy correlation but not as good as  GTM30 influency correlation . Based on these observations , we are not able to conclude which metric is the best because it depends on the manual evaluation criteria  . This results also indicate that high correlation between human and automatic scores in both adequacy and fluency cannot always been achieved at the same time  . 
The best performing metrics in fluency according to Table  1 are BLEU12 and GTM30   ( dark/greencells )  . However , many metrics are statistically equivalent ( gray cells ) to them when we factor in the 95% confidence intervals . For example , even PER is as good as BLEU 12 inadequacy . One reason for this might be due to data sparseness since only  8 systems are available . 
The other potential problem for correlation analysis of human vs  . automatic framework is that high corpus-level correlation might not translate to high sentence -level correlation  . However , high sentence-level correlation is often an important property that machine translation researchers look for  . For example , candidate translations shorter than 12 words would have zero BLEU12 score but BLEU12 has the best correlation with human judgment in fluency as shown in Table  1  . 
In order to evaluate the ever increasing number of automatic evaluation metrics for machine translation objectively  , efficiently , and reliably , we introduce a new evaluation method : ORANGE . We describe ORANGE in details in Section 2 and briefly introduce three new automatic metrics that will be used in comparisons in Section  3  . The results of comparing several existing automatic metrics and the three new automatic metrics using ORANGE will be presented in Section  4  . We conclude this paper and discuss future directions in 
Section 5.
2 ORANGE
Intuitively a good evaluation metric should give higher score to a good translation than a bad one  . 
Therefore , a good translation should be ranked higher than a bad translation based their scores  . 
One basic assumption of all automatic evaluation metrics for machine translation is that reference translations are good translations and the more a machine translation is similar to its reference translations the better  . We adopt this assumption and add one more assumption that automatic translations are usually worst than their reference translations  . Therefore , reference translations should be ranked higher than machine translations on average if a good automatic evaluation metric is used  . Based on these assumptions , we propose a new automatic evaluation method for evaluation of automatic machine translation metrics as follows: 
Given a source sentence , its machine translations , and its reference translations , we compute the average rank of the reference translations within the combined machine and reference translation list  . For example , a statistical machine translation system such as ISI?s AlTemp SMT system  ( Och 2003 ) can generate a list of nbest alternative translations given a source sentence  . We compute the automatic scores for the nbest translations and their reference translations  . We then rank these translations , calculate the average rank of the references in the nbest list  , and compute the ratio of the average reference rank to the length of the nbest list  . We call this ratio ? ORANGE ? ( Oracle 1 Ranking for Gisting Evaluation ) and the smaller the ratio is , the better the automatic metric is . 

There are several advantages of the proposed
ORANGE evaluation method : ? No extra human involvement ? ORANGE uses the existing human references but not human evaluations  . 
? Applicable on sentence-level ? Diagnostic error analysis on sentence-level is naturally provided  . This is a feature that many machine translation researchers look for  . 
? Many existing data points ? E very sentence is a data point instead of every system  ( corpus-level )  . For example , there are 919 sentences vs . 8 systems in the 2003 NIST
ChineseEnglish machine translation evaluation.
? Only one objective function to optimize ? Minimize a single ORANGE score instead of maximize Pearson?s correlation coefficients between automatic scores and human judgments in adequacy  , fluency , or other quality metrics . 
? A natural fit to the existing statistical machine translation framework ? A metric that ranks a good translation high in an nbest list could be easily integrated in a minimal error rate statistical machine translation training framework  ( Och 2003 )  . 
The overall system performance in terms of 1 Oracles refer to the reference translations used in the evaluation procedure  . 
Method Pearson 95%L 95% UPears on 95%L 95% U
BLEU 10.86 0.83 0.89 0.81 0.75 0.86
BLEU 40.7 70.7 20.8 10.8 60.8 10.90
BLEU 120.66 0.60 0.72 0.87 0.76 0.93
NIST 0.89 0.86 0.92 0.81 0.75 0.87
WER 0.47 0.41 0.53 0.69 0.62 0.75
PER 0.67 0.62 0.72 0.79 0.74 0.85
GTM10 0.82 0.79 0.85 0.73 0.66 0.79
GTM20 0.77 0.73 0.81 0.86 0.81 0.90
GTM30 0.74 0.70 0.78 0.87 0.81 0.91
Adequacy Fluency
Table 1 . Pearson's correlation analysis of 8 machine translation systems in 2003 NIST
ChineseEnglish machine translation evaluation.
generating more humanlike translations should also be improved  . 
Before we demonstrate how to use ORANGE to evaluate automatic metrics  , we briefly introduce three new metrics in the next section  . 
3 Three New Metrics
ROUGEL and ROUGES are described in details in Lin and Och  ( 2004 )  . Since these two metrics are relatively new , we provide short summaries of them in Section 3 . 1 and Section 3 . 3 respectively . 
ROUGEW , an extension of ROUGEL , is new and is explained in details in Section 3 . 2 . 
3 . 1 ROUGEL : Longest Common Subsequence Given two sequences X and Y  , the longest common subsequence ( LCS ) of X and Y is a common subsequence with maximum length  ( Cormen et al 1989 )  . To apply LCS in machine translation evaluation , we view a translation as a sequence of words . The intuition is that the longer the LCS of two translations is  , the more similar the two translations are . We propose using LCS-based Fmeasure to estimate the similarity between two translations X of lengthm and Y of length n  , assuming X is a reference translation and Y is a candidate translation  , as follows:
Rlcsm
YXLCS ), (= (1)
Plcsn
YXLCS ), (= (2)
F l c s l c s l c s l c s l c s

PR ??+ += (3)
Where LCS(X , Y ) is the length of a longest common subsequence of X and Y  , and ? = Plcs/Rlcs when ? Flcs/?Rlcs_=_?Flcs/ ? Plcs  . We call the LCS-based Fmeasure , i . e . Equation 3, ROUGEL . 
Notice that ROUGEL is 1 when X = Y since LCS(X , Y ) = morn ; while ROUGEL is zero when LCS(X , Y ) = 0 , i . e . there is nothing in common between X and Y . 
One advantage of using LCS is that it does not require consecutive matches but in-sequence matches that reflect sentence level word order as ngrams  . The other advantage is that it automatically includes longest in-sequence common ngrams  , therefore no predefined ngram length is necessary . 
By only awarding credit to in-sequence unigram matches  , ROUGEL also captures sentence level structure in a natural way  . Consider the following example :
S 1.police killed the gunman
S 2.police kill the gunman
S3. the gunman kill police
Using S1 as the reference translation , S2 has a ROUGEL score of 3/4 = 0 . 75 and S3 has a ROUGEL score of 2/4 = 0 . 5, with ? = 1 . Therefore S2 is better than S3 according to ROUGEL . This example illustrated that ROUGEL can work reliably at sentence level  . However , LCS suffers one disadvantage : it only counts the main in-sequence words  ; therefore , other alternative LCSes and shorter sequences are not reflected in the final score  . In the next section , we introduce ROUGEW . 
3.2 ROUGEW : Weighted Longest Common

LCS has many nice properties as we have described in the previous sections  . Unfortunately , the basic LCS also has a problem that it does not differentiate LCSes of different spatial relations within their embedding sequences  . For example , given a reference sequence X and two candidate sequences  Y1 and Y2 as follows:
X : [ ABC DEF G ]
Y1:[ABCDHIK]
Y2:[AHBKCID]
Y1 and Y2 have the same ROUGEL score.
However , in this case ,   Y1 should be the better choice than Y2 because Y1 has consecutive matches . 
To improve the basic LCS method , we can simply remember the length of consecutive matches encountered so far to a regular two dimensional dynamic program table computing LCS  . We call this weighted LCS ( WLCS ) and use k to indicate the length of the current consecutive matches ending at words xi and yj  . Given two sentences X and Y , the recurrent relations can be written as follows  :   ( 1 ) If xi = yj Then // the length of consecutive matches at // position  i1 and j1 k = w ( i-1 , j-1) c(i , j ) = c(i-1 , j-1 ) + f ( k+1 ) ? f ( k ) // remember the length of consecutive // matches at position i  , j w ( i , j ) = k + 1 (2) Otherwise
If c(i-1 , j ) > c(i , j-1) Then c(i , j ) = c(i-1 , j)w(i , j ) = 0 // no match at i , j
Else c(i , j ) = c(i , j-1) w(i , j ) = 0 // no match at i , j (3) WLCS(X , Y ) = c(m , n ) Where c is the dynamic programming table , 0 <= i <= m , 0 <= j <= n , w is the table storing the length of consecutive matches ended atctable position i and j  , and f is a function of consecutive matches at the table position  , c(i , j ) . Notice that by providing different weighting function f  , we can parameterize the WLCS algorithm to assign different credit to consecutive in-sequence matches  . 
The weighting function fmust have the property that f  ( x + y ) > f ( x ) + f ( y ) for any positive integers x and y . In other words , consecutive matches are awarded more scores than nonconsecutive matches  . For example , f(k)-=-?k ? ? when k >= 0 , and ? ,  ? > 0 . This function charges a gap penalty of ?? for each nonconsecutive ngram sequences  . 
Another possible function family is the polynomial family of the form k ? where -? >  1  . However , in order to normalize the final ROUGEW score , we also prefer to have a function that has a close form inverse function  . For example , f ( k ) -=-k2 has a close form inverse function f-1 ( k ) -=-k1/2 . Fmeasure based on WLCS can be computed as follows  , given two sequences X of lengthm and Y of length n : 
R w l c s ? ? ? ? ? ? ? ? ? = ? )( ), (1 m f
YXWLCSf(4)
P w l c s ? ? ? ? ? ? ? ? = ? )( ), ( 1 n f
YXWLCSf(5)
F w l c s w l c s w l c s w l c s w l c s

PR ??+ += ( 6 ) f-1 is the inverse function of f . We call the WLCS-based Fmeasure , i . e . Equation 6, ROUGEW . Using Equation 6 and f ( k ) -=-k2 as the weighting function , the ROUGEW scores for sequences Y1 and Y2 are 0 . 571 and 0 . 286 respectively . Therefore , Y 1 would be ranked higher than Y 2 using WLCS . We use the polynomial function of the form k ? in the experiments described in Section  4 with the weighting factor ? varying from 1  . 1 to 2 . 0 with 0 . 1 increment . ROUGEW is the same as ROUGEL when ? is set to 1 . 
In the next section , we introduce the skip-bigram cooccurrence statistics  . 
3.3 ROUGES : Skip Bigram Co-Occurrence

Skip-bigram is any pair of words in their sentence order  , allowing for arbitrary gaps . Skip-bigram cooccurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations  . Using the example given in Section 3 . 1:
S 1.police killed the gunman
S 2.police kill the gunman
S3. the gunman kill police
S4 . the gunman police killed each sentence has C (4 , 2)2 = 6 skip-bigrams . For example , S1 has the following skip-bigrams : ( ? police killed ? , ? police the ? , ? police gunman ? , ? killed the ? , ? killed gunman ? , ? the gunman ? ) Given translations X of lengthm and Y of length n  , assuming X is a reference translation and Y is a candidate translation  , we compute skip-bigram-based Fmeasure as follows :
R skip2)2,(),(2mC
YXSKIP = (7)
P skip2)2,(),(2nC
YXSKIP = (8)
F skip 22)1 ( skip skip skip skip

PR ??+ += (9)
Where SKIP2(X , Y ) is the number of skip-bigram matches between X and Y  ,  ? =  Pskip2/Rskip2 when ? Fskip2/?Rskip2_=_?Fskip2/?Pskip2  , and C is the combination function . We call the skip-bigram-based Fmeasure , i . e . Equation 9, ROUGES . Using Equation 9 with ? = 1 and S1 as the reference , S2?s ROUGES score is 0 . 5, S3 is 0 . 167, and S4 is 0 . 333 . 
Therefore , S2 is better than S3 and S4 , and S4 is better than S3 . 
One advantage of skip-bigram vs . BLEU is that it does not require consecutive matches but is still sensitive to word order  . Comparing skip-bigram with LCS , skip-bigram counts all in order matching word pairs while LCS only counts one longest common subsequence  . We can limit the maximum skip distance , between two in order words to control the admission of a skip-bigram  . 
We use skip distances of 1 to 9 with increment of 1   ( ROUGE-S1 to 9 ) and without any skip distance constraint ( ROUGES * )  . 
In the next section , we present the evaluations of BLEU , NIST , PER , WER , ROUGEL , ROUGEW , and ROUGES using the ORANGE evaluation method described in Section  2  . 
2 Combinations : C(4, 2) = 4!/(2!*2!) = 6.
4 Experiments
Comparing automatic evaluation metrics using the ORANGE evaluation method is straightforward  . 
To simulate realworld scenario , we use nbest lists from ISI?s state-of-the-art statistical machine translation system  , AlTemp ( Och 2003) , and the 2002 NIST ChineseEnglish evaluation corpus as the test corpus  . There are 878 source sentences in Chinese and 4 sets of reference translations provided by LDC3  . For exploration study , we generate 1024-best list using AlT emp for 872 source sentences . Al Temp generates less than 1024 alternative translations for 6 out of the 878 source 3 Linguistic Data Consortium prepared these manual translations as part of the DARPA?s TIDES project  . 
sentences . These 6 source sentences are excluded from the 1024-best set . In order to compute BLEU at sentence level , we apply the following smoothing technique : Add one count to the ngram hit and total ngram count for n >  1  . Therefore , for candidate translations with less than n words  , they can still get a positive smoothed BLEU score from shorter ngram matches  ; however if nothing matches then they will get zero scores  . 
We call the smoothed BLEU : BLEUS . For each candidate translation in the 1024-best list and each reference , we compute the following scores : 1 . BLEUS1 to 92 . NIST , PER , and WER 3 . ROUGEL4 . ROUGEW with weight ranging from 1 . 1 to 2 . 0 with increment of 0 . 1 5 . ROUGES with maximum skip distance ranging from 0 to 9   ( ROUGE-S0 to S9 ) and without any skip distance limit ( ROUGES * ) We compute the average score of the references and then rank the candidate translations and the references according to these automatic scores  . 
The ORANGE score for each metric is calculated as the average rank of the average reference  ( oracle ) score over the whole corpus ( 872 sentences ) divided by the length of the nbest list plus 1 . 
Assuming the length of the nbest list is N and the size of the corpus is S  ( in number of sentences )  , we compute O range as follows : ORANGE = ) 1 (   ) ( ??? ? ??? ?? =


Sii (10)
Rank ( Oraclei ) is the average rank of source sentence i?s reference translations in nbest list i  . 
Table 2 shows the results for BLEUS1 to 9 . To assess the reliability of the results , 95% confidence intervals ( 95%-CI-L for lower bound and CI-U for upper bound ) of average rank of the oracles are Method ORANGE AvgRank  95%-CI-L   95%-CI-U 
BLEUS 135.39% 3633 37387
BLEUS 225.51% 2612 39283
BLEUS 323.74% 2432 21267
BLEUS 423.13% 2372 15258
BLEUS 523.13% 2372 15260
BLEUS 62 2.91% 2352 11257
BLEUS 72 2.98% 2362 13258
BLEUS 823.20% 2382 14261
BLEUS 923.56% 2412 18265
Table 2. ORANGE scores for BLEUS1 to 9.
Method Pearson 95%L 95% UPears on 95%L 95% U
BLEUS 10.87 0.84 0.90 0.83 0.77 0.88
BLEUS 20.84 0.81 0.87 0.85 0.80 0.90
BLEUS 30.80 0.76 0.84 0.87 0.82 0.91
BLEUS 40.76 0.72 0.80 0.88 0.83 0.92
BLEUS 50.73 0.69 0.78 0.88 0.83 0.91
BLEUS 60.70 0.65 0.75 0.87 0.82 0.91
BLEUS 70.65 0.60 0.70 0.85 0.80 0.89
BLEUS 80.58 0.52 0.64 0.82 0.76 0.86
BLEUS 90.50 0.44 0.57 0.76 0.70 0.82
Adequacy Fluency
Table 3. Pearson's correlation analysis
BLEU S1 to 9 vs . adequacy and fluency of 8 machine translation systems in 2003 NIST
ChineseEnglish machine translation evaluation.
Method ORANGE AvgRank 95%-CI-L95%-CI-U
ROUGEL 20.56% 2111 90234
ROUGE-W-1.12 0.45% 210 189232
ROUGE-W-1.22 0.47% 2 10 186 230
ROUGE-W-1.32 0.69% 212 188 234
ROUGE-W-1.42 0.91% 214 191238
ROUGE-W-1.52 1.17% 217 1962 41
ROUGE-W -1.62 1.47% 220 1992 42
ROUGE-W -1.72 1.72% 223 200 245
ROUGE-W -1.82 1.88% 224 2042 46
ROUGE-W -1.92 2.04% 226 203 249
ROUGE-W -2.02 2.25% 228 206 250
Table 4. ORANGE scores for ROUGEL and
ROUGE-W-1.1 to 2.0.
Method ORANGE AvgRank 95%-CI-L95%-CI-U
ROUGE-S02 5.15% 2582 34280
ROUGE-S 122.44% 230 209253
ROUGE-S 220.38% 209186231
ROUGE-S 319.81% 203183226
ROUGE-S 419.66% 20217 7224
ROUGE-S 519.95% 204184226
ROUGE-S 62 0.32% 208 187 230
ROUGE-S 72 0.77% 213 1912 36
ROUGE-S 821.42% 220198 242
ROUGE-S 921.92% 225 2042 47
ROUGES * 27.43% 281259 304
Table 5 . ORANGE scores for ROUGE-S1 to 9 and ROUGES* . 
estimated using bootstrap resampling ( Davison and Hinkley )  . According to Table 2 , BLEUS6 ( dark/greencell ) is the best performer among all BLEUSes , but it is statistically equivalent to BLEUS 3 ,  4 ,  5 ,  7 ,  8 , and 9 with 95% of confidence . 
Table 3 shows Pearson?s correlation coefficient for BLEUS1 to 9 over 8 participants in 2003 NIST ChineseEnglish machine translation evaluation  . 
According to Table 3 , we find that shorter BLEUS has better correlation with adequacy  . However , correlation with fluency increases when longer ngram is considered but decreases after  BLEUS5  . 
There is no consensus winner that achieves best correlation with adequacy and fluency at the same time  . So which version of BLEUS should we use ? A reasonable answer is that if we would like to optimize for adequacy then choose  BLEUS1  ; however , if we would like to optimize for fluency then choose  BLEUS4 or BLEUS5  . According to Table 2 , we know that BLEUS6 on average places reference translations at rank 235 in a 1024-best list machine translations that is significantly better than  BLEUS1 and BLEUS2  . Therefore , we have better chance of finding more humanlike translations on the top of an nbest list by choosing  BLEUS6 instead of BLEUS2  . To design automatic metrics better than BLEUS6 , we can carry out error analysis over the machine translations that are ranked higher than their references  . Based on the results of error analysis , promising modifications can be identified . This indicates that the ORANGE evaluation method provides a natural automatic evaluation metric development cycle  . 
Table 4 shows the ORANGE scores for ROUGEL and ROUGE-W-1  . 1 to 2 . 0 . ROUGEW 1 . 1 does have better ORANGE score but it is equivalent to other ROUGEW variants and ROUGEL  . Table 5 lists performance of different ROUGES variants . 
R OUGE-S4 is the best performer but is only significantly better than  ROUGE-S0   ( bigram )  , 
ROUGE-S1, ROUGE-S9 and ROUGES* . The relatively worse performance of ROUGES * might to due to spurious matches such as ? the the ? or ? the of ?  . 
Table 6 summarizes the performance of 7 different metrics . ROUGE-S4 ( dark/greencell ) is the best with an ORANGE score of 19 . 6 6% that is statistically equivalent to ROUGEL and ROUGE-W-1  . 1  ( gray cells ) and is significantly better than BLEUS6 , NIST , PER , and WER . Among them
PER is the worst.
To examine the length effect of nbest lists on the relative performance of automatic metrics  , we use the AlTemp SMT system to generate a 16384-best list and compute ORANGE scores for BLEUS4  , 
PER , WER , ROUGEL , ROUGE-W-1.2, and
ROUGE-S4 . Only 474 source sentences that have more than 16384 alternative translations are used in this experiment  . Table 7 shows the results . It confirms that when we extend the length of the nbest list to  16 times the size of the 1024-best   , the relative performance of each automatic evaluation metric group stays the same  . ROUGE-S4 is still the best performer . Figure 1 shows the trend of ORANGE scores for these metrics over Nbest list of N from  1 to 16384 with length increment of 64  . 
It is clear that relative performance of these metrics stay the same over the entire range  . 
5 Conclusion
In this paper we introduce a new automatic evaluation method  , ORANGE , to evaluate automatic evaluation metrics for machine translations  . We showed that the new method can be easily implemented and integrated with existing statistical machine translation frameworks  . 
ORANGE assumes a good automatic evaluation metric should assign high scores to good translations and assign low scores to bad translations  . Using reference translations as examples of good translations  , we measure the quality of an automatic evaluation metric based on the average rank of the references within a list of alternative machine translations  . Comparing with traditional approaches that require human judgments on adequacy or fluency  , ORANGE requires no extra human involvement other than the availability of reference translations  . It also streamlines the process of design and error analysis for developing new automatic metrics  . Using ORANGE , we have only one parameter , i . e . 
ORANGE itself , to optimize vs . two in correlation analysis using human assigned adequacy and fluency  . By examining the rank position of the Method ORANGE AvgRank  95%-CI-L   95%-CI-U 
BLEUS 62 2.91% 2352 11257
NIST 29.70% 304280328
PER 36.84% 378 350 403
WER 23.90% 2452 22268
ROUGEL 20.56% 2111 90234
ROUGE-W-1.12 0.45% 210 189232
ROUGE-S 419.66% 20217 7224
Table 6 . Summary of ORANGE scores for 7 automatic evaluation metrics . 
Method ORANGE AvgRank 95%-CI-L95%-CI-U
BLEUS 418.27% 2993 2607 3474
PER 28.95% 4744 4245 5292
WER 19.36% 3172 2748 3639
ROUGEL 16.22% 26572 2593072
ROUGE-W-1.21 5.87% 2600 2216 2989
ROUGE-S 414.92% 2444 2028 2860
Table 7 . Summary of ORANGE scores for 6 automatic evaluation metrics ( 16384-best list )  . 
references , we can easily identify the confusion set of the references and propose new features to improve automatic metrics  . 
One caveat of the ORANGE method is that what if machine translations are as good as reference translations ? To rule out this scenario  , we can sample instances where machine translations are ranked higher than human translations  . We then check the portion of the cases where machine translations are as good as the human translations  . 
If the portion is small then the ORANGE method can be confidently applied  . We conjecture that this is the case for the currently available machine translation systems  . However , we plan to conduct the sampling procedure to verify this is indeed the case  . 

Akiba , Y . , K . Imamura , and E . Sumita .  2001 . Using Multiple Edit Distances to Automatically Rank Machine Translation Output  . In Proceedings of the MT Summit VIII , Santiago de Compostela , Spain . 
Cormen , T . R . , C . E . Leiserson , and R . L . Rivest .  1989 . 
Introduction to Algorithms . The MIT Press.
Davison , A . C . and D . V . Hinkley .  1997 . Bootstrap Methods and Their Application . Cambridge
University Press.
Leusch , G . , N . Ueffing , and H . Ney .  2003 . A Novel String-to-String Distance Measure with Applications to Machine Translation Evaluation  . In Proceedings of
MT Summit IX , New Orleans , U.S.A.
Lin , CY . and F . J . Och .  2004 . Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and SkipBigram Statistics  . 

Nie?en S . , F . J . Och , G , Leusch , H . Ney .  2000 . An Evaluation Tool for Machine Translation : Fast Evaluation for MT Research  . In Proceedings of the 2nd International Conference on Language Resources and Evaluation  , Athens , Greece . 
NIST .  2002 . Automatic Evaluation of Machine Translation Quality using Ngram Co-Occurrence 
Statistics . AAAAAAAAA http://www . nist . gov/speech/tests/mt/doc Franz Josef Och .  2003 . Minimum Error Rate Training for Statistical Machine Translation  . In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics  ( ACL 2003 )  , Sapporo , 

Papineni , K . , S . Roukos , T . Ward , and W . -J . Zhu .  2001 . 
Bleu : a Method for Automatic Evaluation of Machine Translation  . IBM Research Report RC22176 ( W0109-022) . 
Su , K . -Y . , M . -W . Wu , and J . -S . Chang .  1992 . A New Quantitative Quality Measure for Machine Translation System  . In Proceedings of COLING-92,
Nantes , France.
Turian , J . P . , L . Shen , and I . D . Melamed .  2003 . 
Evaluation of Machine Translation and its Evaluation  . In Proceedings of MT Summit IX , New
Orleans , U.S.A.
2000 4000 6000 8000 10000 12000 14000 16000 0 . 1 0 . 15 0 . 2 0 . 25 0 . 3 0 . 35 0 . 4ORANGE at Different NBEST Cutoff Length avg
NBEST CutoffLength=1 to 163 84










Figure 1 . ORANGE scores for 6 metrics vs . length of nbest list from 1 to 16384 with increment of 64  . 
