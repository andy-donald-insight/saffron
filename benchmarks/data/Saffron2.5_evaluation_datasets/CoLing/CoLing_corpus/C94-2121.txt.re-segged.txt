Word Sensei ) is mnliguati <) nad Text Setmentation
Bas(qonI , c?ical(+ohcslOll
() KUMUIA Manabu , IIONI ) A Takeo
School of \[ n forma , tion Science,
Japan Advanced Institute of Science a . nd Technology('l'al . sunokuchi , lshikawa 923-12 Japan)c-nmil : oku , honda?~jaist . ac . ji~
Abstract
In this paper , we describe i how word sense am = biguity can be resolw '  . d with the aid of lexical eo-hesion . By checking \] exical coheshm between the current word and lexical chains in the order of the salience  , in tandem with getm ration of lexica\]chains ~ we realize incretnental word sense disam biguation based on contextual infl  ) rmation that lexical chains , reveah Next; , we < le ~< : ribe how set men < boundaries of a text can be determined with the aid of lexical cohesion  . Wc can measure the plausibility of each point in the text as a segment boundary by computing a degree of agreement of the start and endpoints of lexical chaihs  . 
1 Introduction
A text is not a mere set of unrelated sentences.
Rather , sentences in a text are about the same thing and connected to each other \[ l  ( )\] . Cohesion and cohere ' nee are said to contribute to such connection of the sentences  . While coherence is a semantic relationship and needs computationally expensive processing for identification  , cohesion is a surface relationship among words i u a text and more accessible than coherence  . Cohesion is roughly class it led into reference t , co'r@tnction , and lezical coh , esion 2 . 
Except conjmwtion that explicitly indicates l ; he relationship between sentences , l ; he other two <: lasses are considered to t > e similar in that the relationship hetweer ~ sentences i in < licated by two semantically same  ( orelated ) words . But lexical 1Reference by pronouns and ellipsis in Halliday and Hasan's classification\[3\] are included here . 
2 Reference by flfll NPs , substitution mtd lcxical cohe- . 
sion in Ilalll day and Hasan's classill cation a . reincluded here . 
cohesion is far easier to iden lAfy than reference because  1  ) oth words in lexical cohesion relation appear in a text while one word in reference relation is a pr < mom  , or elided and has less information to infer the other word in the relation automatically  . 
Based on this observation , we use lexical cohesion as a linguistic device for discourse analysis  . 
We call a sequence of words which are in lexieal cohesion relation with each other a Icxical chain like  \[10\]  . l , exical chains tend to indicate portions of a text ; that form a semantic utt it . Andsovari . -ous lexical chains tend to appear in a text corre . 
spou(ling to the change of the topic . Therefore , I . lexical chains provide a local context to aid in the resolution of word sense ambiguity  ;  2 . lexical <' hains provide a <' lue for the determination of segnlent boundaries of the  text\[10\]  . 
\] n this paper , we first describe how word sense ambiguity cant ) ere solved with the aid of lexical cohesion . During the process of generating lex-i<'al chains incrementally  , they are recorded in a register in the order of the salience  . The salie'ncc of lexical chains is based on their recency and length  . Since the more salient lexical chain represents the near by local context  , by checking lexi:ca\[cohesion between the current word and lexieal chains in the order of tile salience  , in tandem with generatiou of lexical chains , we realize incremen . 
talword sense disambiguation based on contextual information that lexical chains reveal  . 
Next ; , we describe how segment boundaries of a text can be determined with the aid of lexical cohesion  . Since the start and endpoints of lexical chains it  , the text tend to indicate the start and end points of the segment  , we can measure the plausibility o\['each point in the text as a segment boundary by computing a degree of agreement of the sta  . rt and endpoints of lexical chains . 

Morris and Itirst\[10\] pointed out the above two importance of lexical cohesion for discourse analysis and presented a way of computing lexical chains by using Roger's International  Thesaurus\[15\]  . I Iowever , in spite of their mention to the importance , they did not present he way of word sense disambiguation based on lexical cohesion and they only showed the correspondences between lexical chains and segment boundaries by their intuitive analysis  . 
McRoy's work\[8\] can be considered as the one that uses the information of lexical cohesion for word sense disambiguation  , but her method does not ; take into account the necessity to arrange lexical chains dynamically  . Moreover , her word sense disambignation method based on lexical cohesion is not evaluated fully  . 
In section two we outline what lexical cohesion is  . In section three we explain the way of incremental generation of lexical chains in t and em with word sense disambiguation ad describe the result of the evaluation of our disambiguation method  . In section four we explain the measure of the plausibility of segment boundaries and describe the result of the evaluation of our measure  . 
2 Lexical Cohesion
Consider the following example , which is the English translation of the fragment of one of Japanese texts that we use for the experiment later  . 
In the universe that continues expancbing , a number of stars have appeared amld is appeared again and again  . And aboutten billion years aftertile birth of the universe  , in the same way as the other stars , a primitive galaxy was formed with the primitive sun as the center  . 
Words nniverse , star , universe , star , galaxy , sun seem to be semantically same or related to each other and they are included in the same category in Roget's International Thesaurus  . Like Morris and tIirst , we compute such sequences of related words ( lexical chains ) by using a thesaurus as the knowledge base to take into account not only the repetition of the same word but the use of superordinates  , subordinates , and synonyms . 
We . use a Japanese thesaurus ' Bnnrui-goihyo'\[1\] . Bunrui-goihyo has a similar organization to Roger ' s : it consists of  798 categories and has a hierarchical structure above this level  . 
For each word , a list of category numbers which corresponds to its multiple word senses is given  . 
We count a sequence of words which are included in the same category as a lexical chain  . It might be ( : lear that this task is computationally trivial . 
Note that we regard only a sequence of words in the same category as a lexical chain  , rather than using the complete Morris and Hirst's framework with five types of thesaural relations  . 
The word sense of a word can be determined in its context  . For example , in the context universe , star , universe , star , galaxy , sun , the word ' earth'hasa'planet'sense , not a ' ground ' one . As clear from this example , lexical chains ( ' an be used as a contextual aid to resolve word sense  ambiguity\[10\]  . In the generation process of lexical chains , by choosing the lexical chain that the current word is added to  , its word sense is determined . Thus , we regard word sense disambiguation as selecting the most likely category number of the thesaurus  , as similar to \[16\] . 
l' ; arlier we proposed incremental disambiguation method that uses intrasentential information  , such as selectional restrictions and case frames \ [ l  2\]  . In the next section , we describe incremental disambiguation method that uses lexical chains as intersentential  ( contextual ) information . 
3 Generation of Lexical Chains
In the last section , we showed that lexical chains carl play a role of local context  , t\]o wever , multiple lexical chains might cooccur in portions of a text and they might vary in their plausibility as local context  . For this reason , for lexical chains to function truly as local context  , it is necessary to arrange them in the order of the salience that indicates the degree of tile plausibility  . We base the salience on the following two factors : the recency and the length  . The more recently updated chains are considered to be the more activated context in the neighborhood and are given more salience  . The longer chains are considered to be more about the topic in the neighborhood and are given more salience  . 
By checking lexical cohesion between the cu > rent word and lexical chains in the order of the salience  , the lexical chain that is selected to add the current word determines its word sense and plays a role of local context  . 
Based on this idea , incremental generation of ambiguation using contextual information that lexical chains reveal  . During the generation of lexical chains , their salience is also incrementally updated . We think incremental disambiguation\[9\] is a better strategy , because a combinatorial explosion of the number of total ambiguities rnight occur if ambiguity in not resolved as early as possible during the analytical process  . Moreover , incremental word sense disarn biguation is in dist ) ensable during the gemeration of lexical chains if lexical chains are used for incremental nalysis  , because tile word sense ambiguity might cause many undesirable lxical chains and they might degrade the performance  , of the analysis ( in this case , the disambignation itself ) . 
3.1 The Algorithm
First of all , a & ~ pauese text is automatically seg--mented into a sequence of words  1  ) y the morphological analysis \[ l1\] . Ih-omtile result of the nor-phological analysis  , candidate words are selected to inch . l de in lexical chains . We consider only nouns , verbs , and adjectives , with sonte exceplions such as nouns in adverbial use and verbs in postpositional use  . 
Next lexical chains are formed . Lexical cohesion among candidate words inside a sentence is first  ; checked by using the thesaurus . Ilere the word sense of the current w/ ) rd might be determined . This preference for lexica . 1 cohesion inside a sentence over the intersentential one retlects our observation that the former n fight be tighter  . 
After the analysis htside a sentence , i : audidate words are tried to be added to one of the lexi-eal chains that are recorded in the register in the order of the above salience  . The ih'st chain that the current word hastile lexica \] cohesion relation is selected  . The salience of the selected lexical chain gets higher and then the arrangement in the register is updated  . 
Here not ( ) lily the word sense amt ) iguity of the current word is resolved but the word sense of the amt  ) iguous words in the selected\]exica\[chain cau also be determined  . Because the lexical chain gets higher salience , other word senses of them nhiguous words in the lexic~dchain whi/-h correspond to other lexical chains can here jected  . Therefore , , lcxica \] chains can be used riot only a . sprior context but , also later context for wordseuse disambiguation . 
If a candidate word cannot be added to the existing lexical chain  , new lexieal chains for each word sense are recorded in the register  . 
As clear fl ' omtile algorithm , rather than the truly incremental method where the register of lexical chains is updated word by word in a sen-tenee  , we adopt the incremental method where updates are performed at the end of each sentence because we regard intrasentential nformation as more iml  ) ortant . 
The process of word sense disambiguation using lexical chains is illustrated in Figure  1  . The most salient lexical chain is located at the top in the register  . In the initial state the word W1 re--utain saml ) iguous . When tile current unambiguous word W2 is added , tile chain b is selected ( top left ) . The chain bt ) ecomes the most salient ( top-right ) . Ilere the word sense ambiguity of the word W\[ in the chain b is resolved  ( bottom-left )  . If the word to be added is ambiguous ( W3) , tile word sense corresponding to the more salient \] exieal chaln  ( 1D21 ) in seh ; eted(l)ottom-right) . 
3.2 The Ewfluation
W capply L heal go dth n ~ to five texts . Tal ) lel shows the system's performance . 
The ' correctness ' of the disambiguation is judge  , d by one of the authors . The system's performance is continted as the quotient of the number of correctly disambiguated words by the numher of ambiguous words mi uus then mn ber of wrongly segmented words  ( morphological attalysis ergors )  3 , Words that relnaill ambiguous are those that ( 1o llOt \[' orin any lexical chains with other words  . 
F , x cept t ) y the errors in the n tor phologieal naly-sis , most of the errors in the disambiguation are caused by being dragged into the wrong context  . 
The average performance is 63 . 4 % . We think the system's l?er formam:e is promising for the following reasous : I  . l , exical cohesion is not the only knowledges our ( ' elbr word sense disatn biguation and\[ ) roves to be use fill at least as a sources up -plernentary to our earlier framework that used cane  frmnes\[12\]  . 
2 . In fact , higher performance is reported in \[16\] , thai ; uses bro~der context acquired by at , Ilie accuramy ot'thein or phological analysis will be im-l  ) r ( wed by adding new word entries or the like . 
757 W2,\[ID2\])/~'(~, p~nb wlDmq ~ . haincWI\[ID , 2! ( mind )))) ~ is to fa/~oiguous wordsk(Wl , ID11 , II\]121) ( . . . ,\[ . . . , . . . \])  . . . . . . . .
min  bW2\[ID2\]W1\[ID11\] ) aina D < chain c ) ~ aind )   )   . , :: i . 4!:!:i:x:i:!$>"f::+- . 

(,,,*,, bw~I,o2jw ? ~ eino) . . . . !:!: i :?::, ~-----~ ( Wl,\[ID11,ID12\]) ( . . . ,\[ . . . , . , . \])  . . . . . . . .
( W3,\[ID3\] . , ID32\])//raina , , ,) c~incl )) ~ J . stofm r biguous words ~ zst of ambiguous words ( . . . \[: . ., . .,\])  . . . . . . . . ) ) ~,~ ( . : . \[ . . . , . . . 1)  . . . . . . . .
Figure 1: The process of word sense disambiguation number of candidate words number of ambiguous words text number of sentences 
No . 141481166
No . 22619771.
No . 32421257
No . 438433123
No . 5  24   163   82 number of words that remain ambiguous number of correctly disambiguated words performance  ( % )  87 . 5 51 . 6 64 . 2 60 . 1 53 . 8 Table 1: The performance for the disambigm~tion can attain such tolerab\[elvel of performance without any training  . 
However , our salience of lexical chains is , of course , rather naive and must be refined by using other kinds of inibrmation  , such as Japanese topic Mmarker'wa ' . 
chains start ~ end ( i - 24 )   (  4 - 13 )   ( 14-is )   (  8 - 9 )   ( 14 - 18 ) text i 2 cal Chains The second importance of lexic ~ d chains is that they provide a clue for the deternfination of segment boundaries  .   ( Jertain spans of sentences in a text form selnantic units and are usually called segments  . It is crucial to identify the segment boundaries as a first step to construct the structure of a  text\[2\]  . 
4 . 1 The Measure for Segnmnt t ioun<l - a r ies When a portion of a text forms a semantic unit  , there is a tendency for related words to be used . 
Therefore , if lexical chains can be found , they will ten ( t to indicate the segment boundaries of the text . Whenal . exical chain ends , there is a tendency for a segmen to end . \[ fa , llew chain begins , this might be an indication thai ; a new segment has begun\[l0\] . Taking into account iffs correspondence of \[ exieal chain boundaries to segment boundaries  , we measure the plausibilit ; yeleach point ; in the text ; as ~ segment hotm dary : t breach point between sentences nan  ( l ' nkI ( where it ranges fl ' om1 to them nlt ) erel ' sentences in the text minus 1 )  , compute the stun of the numl ) er of lexical chains that en ( lat the sentence ? z and the number of lexical chains that begin at the sentence n +  1  . We call this naive measure of a degree of agreement of the start and endpoints of lexicM chains w  ( n , n + l ) boundary strength like \[14\] . The points ill the text are selected in the order of boundary strength as candidates of segment boundaries  . 
Consider for example the livelexieal chains in the imaginary text that consists of  24 sentences in Figure 2  . In this text , the boundary strength can be computed as follows : w  ( a , 4) = 1 , ,  . , (7, s)-1, w(9, 10) ~-1, w (13, 14) --3, .   .   .   . 
Figure 2: l , exieal chains in the text 4 . 2 The Evahmtion We , try to segnient the texts ill section 3 . 2 and apply the above measure to the lexical chains that were tbluned  . We pick out three texts ( No . 3 , 4 , 5) , which are fi : om the exam questions of the Japanese language  , that ask us to par-tiglon the texts into a given number of segments  . 
The system's perform mwe is judged by the com.
p ~ rison with segment boundaries marked as an attaehe  ( l model answer . Two more texts ( No . 6 , 7 ) \[' rom the questions are also tried to be segtnented  . 
Here we do not t:M~e into accounthe intbrmation of paragraph l mundaries  , uc has the inden ration , at all in the following rea , sons : ?\] ce all se OllF texts a Feh'o in the exam questions  , nla , ny ( ) f them have no I\]T ta , rks of paragraph I ) oundaries ; ? ill ? case of , laps . nose , it is pointed out that paragraph and segment boundaries do not always coincide with each other \[ l  3\]  . 
Table 2 shows the t ) cr formanee in case where the system generates the given number of segment bot m  ( laries 4 in the order el " the strength . From Table 2 , we can compute the system's marks as an exanline e in timLest that consists of these five ques Liolm  . Tal- ) le 3 shows the performance in case where segment boundm:ies are generate down to half of the maximum strength  . ' l ' he metrics that we . use for the ewduation are as follows : Recall is the quotient of ' the intuber of correctly identified boundaries by the total mmlber of correct boundaries  . Precision is the quotient of then mn be r of ( : or re ( : t\[y identifie ( lI ) ounda , ries by the tnllnl ) er of generated boundaries . 
We think the poor result for the text No . 5 might be caused by the difficulty of tile text ~ The number of boundaries to be given is the mtm ber of segments given in the question minus  1  . 
759 text




No . 7 given number of boundaries number of correct boundaries  1   1   6   3   1   0 Table 2: The performance for the segmenta-tion ( l ) text
I __




No . 7 number of generated boundaries number of correct boundaries  3   1   10   3   7   3   5   1 rec . prec . 
I 10 . 3 H 0 . -- T-~0~-%o1o . 7 - - - ~ o . 4~\] _ 0 . aK0 . 2 0 A Table 3: The performance for the segmenta-tion ( 2 ) itself because it is written by one of the most difficult writers in Japan  , KOBAYASH\[Hideo . Table 2 shows that our system gets 8 ( 1+3+3+1 )  /15 ( 1+6+1+4F3 ) = 5% in the test . From Table 3 , the average recall and precision rates are 0 . 52 and 0 . 25 respectively . Of course these results are unsatisfactory , but we think this measure for segment boundaries is promising and useful as a preliminary one  . 
Since lexical chains are considered to be different in their degree of contribution to segment boundaries  , we arc now refining the measure by taking into account heir importance  . We base the importance of lexical chains on the following two factors :  1  . The lexical chains that include more words with topical marker ' wa ' get more importance  . 
2 . The longer lexical chains tend to represent a semantic unit and get more importance  . 
The start and endpoints of the more important lexical chains can get the more boundary strength  . This refinement of the measure is in the process and yields a certain extent of improvement of the system's performance  . 
Moreover , this ewduation method is not necessarily adequate since partitioning into a larger number of smaller segments might be possible and be necessary for the given texts  . And so we will have to consider the evaluation method that the agreement with hmnan subjects is tested in future  . I lowever , since human subjects do not always agree with each other on  segmentation\[6  ,  4 ,  14\] , our evaluation method using the texts in the questions with model answers is considered to be a good simplification  . 
Several other methods to text segmentation have been proposed  .   Kozima\[7\] and Youmans\[17\] proposed statistical measures ( they are named LCP and VMP respectively )  , which indicate the plausibility of text points as a segment boundary  . Their hills or valleys tend to indicate segment boundaries  . However , they only showed the correlation between their measures and segment boundaries by their intuil  , ive analysis of few sample texts , and so we cannot compare our system's and their performance precisely  . 
l tearst\[5\] independently proposes a similar measure for text segmentation adevaluates the performance o\[ her method with precision and recall rates  . However , her segmentation method depends heavily on the information of paragraph boundaries and always partitions a text at the points of paragraph boundaries  . 
5 Conclusion
We showed that lexical cohesion can be used as a knowledge source for word sense disambiguation and texts egrnent at in n  . We think our method is promising , although only partially successful results can be obtained in the experiments so far  . 
Here we reported some preliminary positive results and made some suggestions for how to improve the method in future  . The improvement of the method is now underway . 
In addition , because computation of lexical chains depends completely on the thesaurus used  , we think the comparison among the results by different hesauri would be insightful and are now planning  . \[tit also necessary to incorporate other textual information  , such as clue words , which can be computationally accessible to improve the performance  . 

References\[1\]Bunrui-Goihyo . Shuei Shuppan . , :1964 . in

\[2\]B . J . Grosz and C . L . Sidner . Attention , intentions , and the structure of discourse . 
Coraputationol Li?iguistics , 12(3):175204, 1986.
\[3\]It . A . K . ftalliday and R . Hassan . Cohesion in English . Longman , 1976 . 
\[4\] M . A . l tearst . Texttiling : A quantitative approach to discourse segmentation  . Technical Report 93/24, University of California,
Berkeley , 1993.
\[5\] M . A . Hearst . Multiparagraph segment wtion of expository texts . Technical Report 94/790 , Uniw ~ rsity of California , Berkeley ,  1994: . 
\[6\] J . ttirschberg and B . C , rosz . lnt : onational features of local and global discourse structure  . 
In Proc . of the Darpa Workshop on Speech and Natu ~ vd Language  , pages , 141- 446 ,  1992 . 
\[7\] H . Kozima . Text segmentation based on similarity between words  . In Proc . of the 31st Ann . lLalMeeting of the Association for Computational Linguistics  , pages 286288 ,  1993 . 
\[8\]S . W . Mcll~oy . Using multiple knowledge sources for word sense discrimination  . Computational Linguistics , 18(1):130, 1992 . 
\[9\] C . S . Mellish . Computer Interpretation of Natural Language Descriptions  . Ellis Hor--wood , 1985 . 
\[10\]J . Morris and G . tlirst . Lexical cohesion computed by thesaural relations as an indicator of the structure of text  . Computational
Linguistics , 17(1.):21-48, 1991.
\[11\]Nagao Lab ., Kyoto University .,\] apancs c
Morphological Analysis System , \] UMAN
Manual Versionl . O , 1993. in , lapanese.
\[12\]M . Okumura and H . Tanaka . Towards incremental disambiguation with a generalized discrimination etwork  . In Proc . of the 8th National Conference on Arti\]icial Intelligence , pages 990995 ,  1990 . 
\[13\]T . Ookuma . Gengotan ' i to shiten obun ~ shou . Nihongogaku , 11(4):20-25, 1992 . in

\[14\]R . J . Passonneau . Intention-based segment a . -tion : Human reliability and correlation with linguistic cues  . In Proc . of the 31st Annual Meeting of the Association for Computational Linguistics  , pages 148-155 ,  1993 . 
\[15\]P . Ioget . Roget's International Thesaurus , Fourth Edition . Harper and Row Publishers
Inc ., 1977.
\[16\]D . Yarowsky . Word-sense disambiguation using statistical models of roget's categories trained on large corpora  . In Proc . of the ldth . 
\[' nt cr national Co ~@ re~nce on Computational
Linguistics , pages 454--460, 1992.
\[17\]G . Youmans . A new tool for discourse analysis : The vocabulary-management profile  . 
Language , 67:763--789, 1991.

