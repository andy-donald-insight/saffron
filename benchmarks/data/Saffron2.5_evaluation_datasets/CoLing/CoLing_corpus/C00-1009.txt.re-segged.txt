COMBINATION OF NGRAMS ANDS TO CHASTIC
CONTEXT-FREEGRAM MARS FOR LANG UAGE MODELING *
Jos 6-Miguel Benedf and Joan-Andreu SSnchez
De , i ) art mnento de Sistemas Informgti ( : osyComl ) utaci6n
Universidad Polil ; (! Clfi Ca(te Valencia
Cm ni node\Seras/n , d 60 22 Valencia ( Slmin ) email: . jt)enedi,j and reu(@dsic . ul)v . es
Abstract
This t ) al)t ; r de , scribes a hybrid prol ) os alto combine ngrams and Stochastic ContextFree Grammars  ( SCFGs ) t br language modeling . A classical ngram model is used to cat ) lure the local relations between words , while a stochastic grammatical in odel is considered to represent the hmg-term relations between syntacticals tru  ( : tm'es . In order to define this granmlatical model , which will 1 ) e used on large-vo ( : almlary comph ' ~ x tasks , a eategory-t ) ased SCFG and aprol ) a bilisti ( " model of ' word ( tistrilmtion in the categories have been 1 ) rol ) osed . Methods for leanfing these stochastic models tTor complex tasks are described  , and algorithms for con > puting the word transition probal  ) ilities are also 1 ) resented . Filmily , ext ) erilnents using the Penn Treel ) ank corpus improved by 30% the test ; set ; l ) erph~xity with regard to the classical ngram models  . 
1 Introduction
Language modeling is an important as l ) e ( ' t to consider in large-vocabulary Sl ) eeeh recognition systenls ( Bahl et al ,  1983;  , leline k ,  1998) . The n--grain models are the most widely-used for a wide range of domains  ( Bahl et al ,  1983) . The ngrams are simple and robust models and adequately capture the local restrictions between words  . Moreover , it is wellknown how to estimate the parameters of the lnode t and how to integrate them in a speech recognition system  . However , the n-grmn models cannot adequately characterize the long term constraints of the sentences of the tasks  . 
On the other hand , Stochastic ContextFree Grammars ( SCI ) 'Gs ) allow us a better model-*This work has been partially SU l  ) l ) < ) rted by the S1 ) an ish CICYT under contract ( TIC98/0423-C ( 16 )  . 
ing of long term relations and work well onlh nited -domain tasks of low perplexity  . However , SCFGs work poorly for large-vocabulary , general-purpose tasks because learning SCFGs and the  ( : Olnlmtation of word transition 1 ) roba-bilities present serious 1 ) roblenlst Lr ( ' olnplex real tasks . 
In the literature , a nuln ber of works have proposed ways to generalize the ngram models  (  . le-line k , 1998; Siu and Ostendorf ,  2000 ) or com-1 ) ining with other structural models ( Bellegarda , 1998; Gilet and Ward , 1998; Chellm and Jelinek ,  1998) . 
In this iml ) er , we present a confl ) ined language model defined as a linear combination of ngrams  , whk ' harellse ( t to capture the local relations between words , and astoehasti ( : gram-matieal model whi ( : h is used to represent heglottal relation 1 ) etw ( x ' dl synl : ae tiest rll (  ; tllrt ~ s , hior(terto(:at)turc , these lollg-terl tl relations an ( t to solve the main 1 ) rol flems derived Dora the large-vocabulary complex tasks  , we 1) l'Ol ) ose here to detine : a eategory -- ba , ~ed SCFG and a prolm bilis-tic model of word distrilmt  ; ion in the categories . 
Taking into a (: count his proposal , we also describe here how to solve the learning of these stochastic models and their integrati  ( mprol > 1CIlIS . 
With regard to the learning problem , several algorithms that learn SCFGs by means of estimation algorithms have been  1  ) reposed ( Lari and Young , 1990; Pereira and Schal)es , 1992; Sfinehez and Benedi ,  1998) , and pronfising re-suits have been achieved with category-based SCFGs on real tasks  ( Sfi . nchez and Benedi , \]999) . 
In relation to the integration problem , wel ) resent wo algorithms that compute the word transition  1  ) robability : the first algorithm is based on the l ~efl  ; -to-ll , ight Inside algorithl nond is based on an application of a Viterbi scheme to the LRI algorithm  ( the VLRI algorithm )   ( S~nehez and Benedf ,  1997) . 
Finally , in order to evaluate the behavior of this proposal  , experiments with a part of the Wall Street Journal processed in the Penn Treebank project were carried out and significant improvements with regard to the classical ngram models were achieved  . 
2 The language model
An important problem related to language modeling is the evaluation of Pr  ( wkIwl .   .   . wk-1) . 
In order to compute this probability , we propose a hybrid language model defined as a sire-ple linear combination of ngram models and a stochastic grammatical model G ~: Pr  ( wkl wl .   .   . w~_l ) = c ~ Pr(~klwk-n . .  . wt~-l ) + (1-c ~) P "( wklw ~ . . . wk- , , G ~) ,   ( 1 ) where 0 < c ~ < 1 is a weight factor which depends on the task . 
The expression Pr(w / ~ lwk_n . . . wk- , ) is the word probability of occurrence of w /~ given by the ngram model  . The parameters of this model can be easily estinmted  , and the expression Pr(w l ~ l W l ~_ n . . . wtc-1) can be efficiently computed ( Bahl et al , 1983; Jelinek ,  1998) . 
In order to define the stochastic grammatical model G ~ of the expression Pr  ( wk\]w ~ .   .   . wk_j , G ~) for large-vocabulary complex tasks , we propose a combination of two different stochastic models : a category-based SCFG  ( G  ~ )  , that allows us to represent the long term relations between these syntactical structures and a probabilistic model of word distribution into categories  ( Cw )  . 
This proposal introduces two imlmrtant as-peels , which are the estimation of the parameters of the stochastic models  , Gc and Cw , and the computation of the following expression :
Pr(~klWl . . . Wk-1, ac , Cw ) = Pr(wl .   .   . wk . .  . la ~, c ~) (2)
IC , C , , ) 3 Training of the models
The parameters of the described model are estimated fi'om a training sample  , that is , from a set of sentences . Each word of the sentence has a part of speech tag  ( POS tag ) associated to it . These POS tags are considered as word categories and are the terminal symbols of the SCFG  . h'om this training sample , the parameters of G ~ and C ~ can be estimated as t bllows  . 
First , tile parameters of Cw , represented by
Pr(w\[c ), are computed as :=
E , o , (3) where N(w , c ) is the number of times that the word w has been labeled with the POS tag c  . It is important onote that a word w can belong to different categories  . In addition , it may hapt ) en that a word in a test set does not appear in the training set  , and therefore some smoothing technique has to be carried out  . 
With regard to the estimation of the category -based SCFGs  , one of the most widely-known methods is the Inside-Outside  ( IO ) algo-rithln ( Lari and Young ,  1990) . The application of this algorithm presents important problems which are accentuated in real tasks : the time complexity per iteration and the large number of iterations that are necessary to converge  . An alternative to the IO algorithm is a . n algorithm based on the Viterbi score ( VS algorithm )   ( Ney ,  1992) . The convergence of the VS algorithm is faster than the IO algorithm  . However , the SCFGs obtained are , in genera . l , not as well learned ( Simchez et al , 1996) . 
Another possibility for estimating SCFCs , which is somewhere between the IO and VS algorithms  , has recently been proposed . This approach considers only a certain subset of derivations in the estimation process  . In order to select this subset of derivations , two alternatives have been considered : froln structural information content in a bracketed corpus  ( Pereira and Schabes , 1992; Amaya et al ,  1999) , and from statistical information content in the kbest derivations  ( Sgmchez and Benedl ,  1998) . 
In the first alternative , the IOb and VSb algorithms which learn SCFGs from partially bracketed corpora were defined  ( Pereira and Schabes , 1992; Amaya et al ,  1999) . In the second alternative , the kVS algorithm for the estimation of the probability distributions of a SCFG fl'om the kbest derivations was proposed  ( Shnchez and
Benedi , 1998).

All of these algorithms have at ilne ( : omi ) lexity O ( 'n , a\[PI ) , where ' n is the length of the inputs t ; ring , and\[1)1 is the size . of the SCFG . 
These algorithms have been tested in real tasks fl ) restimating cat (  , gory-1) ased SCFOs ( Sfinchez and Benedf ,  1999 ) and the results obtained justify their applicatiol  , in complex real tasks . 
4 Integration of the model l ? rome xl ) ression ( 2 )  , it can beese ( m that in order to integrate the too ( M , it is necessary to e f l i ( : iently ( ' oml ) ute the expression :
P ~0,, ~...', ,, k . . . ., < , , ). (4)
In order to describo how this computation ( : anl ) em~de , we tirst introduce some notation . 
A Court : el-Free , Grammar G is a four-tul)le(N , E , P , S ) , wher (; N is the t in it ( ; set of nont (; r-minals , )2 is the tinite sol ; of terminals ( N~-/E = 0) , S~N is the axiom or initial symbol and 1' is the finite set of t ) rodu ( : tions or ruh ; s of the tbrmA -+ it , where AcNa . ndc~C(NUE ) + ( only grmmmtrs with non ( ; mt ) ty rules ar ( ; considered ) . FOI " siml ) li (' ity ( but without loss of g (' . n-erality ) only (: on text-i Yee grammars in Ch , om . s'ky Normal Form are . consider e(l , that is , grammars with rules of the form A-+HC or A->v wh  ( n' ( :
A , B , CCN and v ~) 2.
A Stoch , a . stic Contcxt-l '; rc . cU'raw,w,w l " G . ~ is a pair ( G , p ), where G is a (: on text-fr (, . (; grain-mar and p:P-+\]0 , 1\] is a 1) robal ) ility tim(:- ; ion of rule ai)l ) li('al ; ionsu(:h that VA~N : ~ , c(Nu > ~) + p(A - - + , ~) -- i . 
Now , wepr ( : sent two algorithms ill order to compute the word transition  1  ) rol ) at ) ility . The first algorithm is based on the Ll/i algorithm  , a . nd the second is based on an apt ) li ( ' atiou of a Viterbis ( : heme to the LRI algorithln ( the VLI/\]a . lgorithm) . 
Probability of generating an initial subst ring The CO mlmtation of  ( 4 ) isl ) as ( ' . donan algorithm which is a modith : ation of the I , RI algorithm ( aelinek and Lafl'erty ,  1991) . This new algorithln is based on the detlnition of Pr  ( A << i , j )) = Pr(A ~ wi .   .   . wj .   .   . Ic .   . c , , , ) asth ( ,  1 ) robability that A generates the initial sul ) string wi .   .   . wj .   .   . given Gc and C . ,,, . This can l ) e computed with the following ( lynamic 1 ) rogrmmnings ( : henl (  ; : c + ~ Q(A~D ) p(D ~ ~) P "(~" ~ l ~)) , 
D . i1
Pr(d << i , j ) = E ~ Q(A ~ BC)
B , CGNl = ipl . ( J ~ < ~ : , 1>) p , .( c < <1 + 1, j).
::: this way , Pr(~,~ .   .   . ~, k .   .   . IG , : , 6', , , ) =
Pr(S < < l , k).
In this cxi ) ression , Q ( A  ~ D ) is the probability that D is the leftmol : terminal in all sentential fOHllS which are derived from A  . The vahu ; Q(A ~ BC ) is the probability that BC is th ( ; initial substring of all sential forms de-riv ( ; d from i \ . Pr(H < i , l >) is th ; probability that the substring " wi . . . wz is generated from/~given G , : and C . ,,, . Its contlmt ; ation will be defined\]ater . 
It shouh : l be noted that th ( ; combination of the models G ,  . and C ~ , , in carried out in the vah:e P'r(A << i , i ) . This is the lnain difl : ' crcnce with resp ( wt the \] A/I algorithm . 
Probability of the best derivation gener at ing an initial substring An algorithm whi  ( :\] l is similar to the previous ( > he ( -m  ~ l ) e ( l ( ~fin ( ~ dt ) ased on the \ ; iterl ) i , ~(: lmme . 
In this way , it isl ) on sil ) le to obtain the \]) cst ; parsing of an initial sul ) string . This new algorithm is also related to the \/' Lll . I algol'ithni ( Shn (: hez and B('aw , d i , 1997) and is 1) ased on the ( lci inition of P , ~'( A <<' , : , J )) = P , ~'( A ~" , , i .   .   . ',, j .   .   . IG ~: , Cw ) as the probability of the most probal ) le1 ) arsing which generates wi .   .   . wj .   . , from A given G , : and C , , . This can 1)(i (: omput cd as follows : p : ( A < < i ,   , :) = m : ? x(p(A ~ c)l) , (~ , , ~ l ,  . '),m ~ Lx(Q(A => D ) ' p(D ~ c)Pr('wi\[c ))),

PI~'(A << i , j ) = max : mix(Q(A ~\] 3C)
H,CcNl=i . . . j-I

Pr(\]3<i,l >) Pr(C << l+1,j)).
AA
Thorcfo , o > 4", ~ . . . ' wk . . . \[ a , : , < , ) - - p , . ( s << 1, k) . 
In this expression , Q ( A  ~ D ) is the t ) rob-al ) ility that D is the leftmost nontermin a , linrived ti ' omd . The value Q ( A  ~ B C ) is the probability that BC is the initial substring of most the probable sential form derived from A  . Pr(B < i ,  1 > ) is the probability of the most probable parse which generates wi ? ? ? wl  froli1 B . 
Probability of generating a string
The w flue Pr(A < i , j >) = Pr(A d > w i . . . 'u;jlG~ , Go ) is defined as the probability that the substring wi  . . . wj is generated from A given G ~ and C,~ . To calculate this probability a modification of the wellknown Inside algorithm  ( Lari and Young , 1990) is proposed . 
This computation is carried out by using the following dynamic progral mning scheme : 
Pr(A < i , i >) --~ p(A-~c)Pr(wilc) , c j -- I
Pr(A < i,j >) : EE p(A-+BC)
B , CcNl = ipr(B < i , l >) Pr(C < 1+1, j >).
In this way , Pr(w ~ . . . whiGs , C , ,) = Pr(S < 1, n >) . 
As we have commented above , the combination of the two parts of the grammatical model is carried out in the value Pr  ( A < i , i >) . 
Probability of the best derivation generating a string Thet  ) rol ) a bitity of the best derivation that genel ' ~- gt es a string  , Pr('u , 1  .   .   . ~ t/2 , ~l  ~ c , 6' w ) , can be evaluated using a Viterbi-like scheme ( Ney ,  1992) . As in the previous case , the computation of this probability is based on the definition of p  . ( A < g , j >) = pU-(A < , , ) as the probability of the best derivation that generates the substring w i  . . . wj fi'om A given Gc and Cw . Similarly:
Pr(A < i , i >) =
Pr(A < i , j >) =
Therefore , 1, n >).
in ax\])(a-9C)I)I'('//)ilC ) , C max n , ax-+Be )
B , CCN ...
Pr(B < i , l >) Pr(C < 1 + 1, j >).
nla , C , , o ) = P-(X <
Finally , the time complexity of these algorithms is the same as the algorithms they are related to  , there % re the time colnplexity is O(k:alrl ) , where tc is the length of the input string and IPI is the size of the SCFG  . 
5 Experiments with the Penn
Treebank Corpus
The corpus used in the experiments was the part of the Wall Street Journal which had been processed in the Petal % ' eebank project  1   ( Marcusel : al . , 1993) . This corpus consists of English texts collected from the Wall Street Journal from editions of the late eighties  . It contains approximately one million words . This corpus was automatically labelled , analyzed and manually checked as described in ( Marcuset 31 . , 1993) . 
There are two kinds of labelling : a POS tag labelling and a syntactic labelling  . The size of the vocal mlary is greater than 25 , 000 diil'erent words , the POS tag vocabulary is composed of 45 labels 2 and the syntactic vocabulary is composed of 14 labels . 
The corpus was divided into sentences according to the bracketing  . In this way , we obtained a corpus whose main characteristics are shown in Table  1  . 
Table 1: Characteristics of the Petal Treebank corpus once it  ; was divided into sentences . 
No . of Av . Std . Min . Max.
senten , length deviation length length 49, 20723 . 61 11 . 1 3   1   249 We took advantage of the category-based SCFGs estimated in a previous work  ( Simchez and Benedf ,  1998) . These SCFGs were estimated with sentences which had less than  15 words . Therefore , in this work , we assumed such restriction . The vocabulary size of the new corpus was 6 , 333 different words . For the experiments , the corpus was divided into a training corpus ( directories 00 to 1 . 9) and a test corpus ( directories 20 to 24) . The characteristics of these sets can be seen in Table  2  . The part of the 1Release   2 of this dataset can be obtained t'rmn the Linguistic Data Consortium with Catalogue number  LDC94T4B   ( http://www . ldc . upenn . edu/ldc/nofranm . html ) 2 There are 48 labels defined in ( Marcus et al ,  1993) , however , three of ttmm do not appear in the corpus . 
58  ( -orlmslal ) (:led with l ) ( ) Stags was used to ( : st ; i-mate the p~wameters of tlm grammati (: alme(M , while the non-lad)e\](;(lpart was u , s(' , d i ; oestimate th ( ; parameters ( it " then-grmnlnod c . l . \?(~ now des (' ri be the estinmtion l ) roec , ,~s in ( l('%ail . 
Table 2: Chm ' acteristics of th (: data . s('A ; s(h ~ iined for the eXl ) eriments wh (' , n the senl : en(:(~swi ( ; h more l ; lmn15 l ) OS l ; ags were r ( ; moved . 
Da . ta\[No . of IAv . Std . \]l ( , nglh deviation S(~,J ; Sell I ; ell .  \ ] _  . 
~l.i ; st .2,295 1.1~3.55
The 1) a . rmn(%er , q of a 3-grmn to o(l ( ; 1 were ( ' ~ stimatcd with the softw ~ retooldes ( 'rit ) (' . (1 in ( l /, osen fehl , 1995): t . W ( ~ u , qedtlm linearini ; (', rl/o-la . tion ~ qmootht('~(:lmiqu(~SUpl ) orted by , ; hi , ~ tool . 
Th('~o1:l ; -of-v ( / ( :al ) l flary words w cr ( ' ~ groul/e ( l in the same ( :\] as , ~ and w(u'e used in th(~('omt ) ula-1 ; ionofi ; \]~(' ~ perl)h ~' xity .  ~ . l'h ( , . I : (~ sl , ~(: Il ) ( ~ rl ) l('~xity with t : hismo(lel was 180 . 4 . 
T\]w , values () f(' . x t ) r ( ~ , qsi()n (3) wure(:()ml ) ut(!(t from the t ~: . gged and l:on-l:agg(:(ti ) alq ; O1'\[; lie , l : raining corpus . In or ( h'a' to avoid mill values , them ~ seen ( ~' vents wer ( ' ~ lal ) ele ( 1 with ~ Sl ) C-cial symbol'w'wl fich did not apl ) ear in (  ; hei , s . ,:h -? 0 , VcC(/ , whtu'(~(/wasI:h('~set()f(:at(' . g()ri(~s . 
That is , all th(:('at(' , gori(~s could g(' , n (: rat (' , i ;\] : (: uu - - , q('x'~n even I , This l ) rolml ) ility took a . v(~rysmallvalu(;(s(',v (' . ral()rd(;rsofmagnii;u(l('~h ' ~ , q s t l m n m i n w ~ v , c(:cl)r('wlc) , where V was the \ . ' o('alm-lary of the tra . ining corpus ), and ( liffer(mI ; vahte . ~ of this i ) robability did not chang ( ' ~ tlmr ( ~sults . 
Thei ) aramet ( ' ~ rs of an initialer godic SCFG were estimated with each one of the estimation methods mentioned in Se  ( ' tion 3 . This SCFG had 3 , 374 rules ,   ( : omt ) osed fl'om 45 terminal syml ) ols ( the numl ) er of l ) ( ) Stags ) and \] d nonterminal symbols ( then mn be r of sy nl ; a('l ; i (: labels ) . The prol ) z~l ) ilitics were rmi dolnly gem'a'-ated midt ; hree different seeds were tested , lint only one of them is reported given that the re-suits were very similar  . The training ( : or lms was the label e ( l part of the des ( 'ril ) ed ( : or lm , q . The 1) erl ) lexity of the labeled part of the test ; s(:t for all . clc as (~2 . 04 is availal ) leathtl ; l ): // svr-www . cng . cmn . ac . uk / ~1) rcl 4/ toolkit . html . 
diti'(n'('~nt(~stimation algorithms (; a . n l ) c . , ~ cenin
Tal)le 3.
\[I . ' abl(~3:PCxl ) lexity of the labeled 1) ? tl't ; Of I ; \] lO test set with the SCFC , estimated with the methods mentioned in Section 3  . 
71/'~vsla,\sl,ot,I\sbI
Once we had estimated the lmrame i ; ers of the defined model , we applied expression (1) 1) y using the IAI , \] algorithmm ~ d the VI . \[/, I algorithm in ('~ xt)w,ssion(d) . Th ( ; test set lWa'l ) lexitly that wasol)I ; ained inflmctiont ) f(t % rdifl't'a'(:nt ; esti-nm ~ tion algorithms ( V S , kVS , lObmid VS b ) can be seen in \] rig .  1 . 
In the best case , the tn ' ot ) osed l ~ m guage model el ) rained more than a . 30% in lI)rOVellle:l;OV erre , ~ ultsol ) tain cd1)y the 3gramlmlguagcmotM(s(w . ~l'at)led ) . This result wa , qol)t ; ainc . d wh (' ~ nth (: SCF Gusl ; imat ( ~ d with the lOb algorit ; hm wa , ~ u , ~(;(1 . The SCFGs (' . stimat (' . (l with () the ral-g ( ) ril ; hms also ( ) l ) tain (' . diml ) or tanl ; ilnlirovt, . nw . lfl ;, ~(: Oinl ) ar(;dl;o\[; he3-grain . In ~( t(lition , ii ; can be oliser v (' . di ; hatt ) othth(, . LI-( . I algorithm and the VIA/I algoril ; hm obtained good results . 
Tal)led :\] 3( ~ , sttx~stlW , r l ) lexity for d if l '( ~ , rem ; SCFG (: . % imation algorithins , and I ; h(~\])er (: cntage of im-i ) rovt ' . mc ~ lll ; with resi ) ( wl ; I ; oi ; hc 3gram model . 
VS mk VS lOb VSI)\]All 133 . 6 130 . 3 124 . 6 136 . 3i % improv .  25 . 9% 27 . 8% 30 . 9% 24 . 5%\[V\]~l:lI~137 . 2I13Z4IV , 9 . 7I \[% iml ) rov .  20 . 5% 23 . 0% 26 . 6% 17 . 0% An important aspect to note is ( ; hat the weight ; of the grmmnatical part was approxi-mat ; ely 50% , which means that this part provided iml ) or i ; mlI ; inform ~ tion to the language model . 
6 Conclusions
Anc'w language model has been introduced.
This new language model is detined as a ~ linear ( ' olnl ) in ~ ttion of an ngram which repre-s ( mts relations betwe ( ' ~ n words , and a stochastic > " ~17 ( 1 14 ( \] 13 ( \] 19 ( \] 14 ( 1  ;  , , / l:j ~ 3-ffrgtn:
VS bk VS VS
IOb0304 0.5 0.6 0.1 0.2 ~/ 3 ~ gt ' anl

VSk VS lOb 0 . 7 0,8 0 . 9 / 0 . 1 0 2 0 . 3 0 . 4 0 . 5 0 . 6 0 7 0 . 8 0 . 9 0  ( t Figure 1: Test set perplexity obtained with the proposed language models in function of gamma  . Different curves correspond to SCFGs estimated with different algorithms  . The upper graphic correst ) onds to the results obtained when the LRI algorithm was used in the language models  , and the lower graphic corresponds to the results obtained with the VLRI algorithm  . 
grammatical model which is used to represent the global relation between syntactic structures  . 
The stochastic graminatical model is composed of a category-based SCFG and a probabilistic model of word distribution in the categories  . 
Several algorithms have been described to estimate the parameters of the model flom a the smnple  . In addition , efficient algorithms tbr solving the problem of the interpretation with this model have been presented  . 
The proposed model has been tested on the part of Wall Street  . Journal processed in the Penn Treebank project , and the results obtained improved by more tlmn 30% the test set ; perplexity over results obtained by a simple 3-grain model . 
References
F . A maya , J.M . Benedi , and J . A . Shuchez.
1999 . Learning of stochastic contextfree grammars from bracketed corpora by means of reestimation algorithms  . In M . I . Torres and A . Sanfeliu , editors , Proc . VIII Spanish Symposium on Pattern Recognition and Image Analysis  , pages 119126 , Bilbao , Est)afia , 
May . AERFAI.
L . R . Bahl , F . Jelinek , and ILL . Mercer .  1983 . 
A maximmn likelihood approach to continuous speech recognition  . IEEE Trans . Pattern Analysis and Machine Intelligence , PAMI-5(2):179190 . 
J . R . Bellegarda .  1998 . A multispan language modeling frmnework tbr large vocabulary speech recognition  . IEEE Trans . Speech and
Audio Processing , 6(5):456-476.
C . Chelba and F . Jelinek .  1998 . Exploiting syntactic structure for lm~guage modeling  . In Proc . COLING , Montreal , Canada . University of Montreal . 
J . Gilet and W . Ward .  1998 . A language model combining trigrams and stochastic on text-fl'ee grammars  . In In 5th International Con- . fercnce on Spoken Language Processing , pages 23192322 , Sidney , Australia . 
F . Jelinek and J . D . Lafferty .  1991 . Coml ) uta-tion of the probability of initial substring en-eration by stochastic on text-free grammars  . 
Computational Linguistics , 17(3):315323.
F . Jelinek .  1998 . Statistical Meth , ods for Speech
Recognition . MIT Press.
K . Lari and S . J . Young .  1990 . The estimation of stochastic on text-fl'ee grmn mars using the insideoutside algorithm  . Computer , Speech and Language , 4:3556 . 
M.P . Marcus , B . Santorini , and M.A.
Marcinkiewicz .  1993 . Buikting a large annotated corpus of english : the penn treebank  . 
Computational Linguistics , 19(2):313-330.
H . Ney .  1992 . Stochastic grmmnars and pattern recognition . In P . Laface and R . De Mori , editors , Speech Recognition and Understanding . 
Recent Advances , pages 319344. Springer-

F . Pereira and Y . Schabes .  1992 . Inside-outside reestimation from partially brack - n ' u al  A4ectin9 of the As , ~ociatiou for Comp ' uta-tional Linguistics , 1) ages 128135 . University of l ) el awarc . 
ll , . Roscnfl'hl .  1 . 995 . Th('cmusta . tisl:i ( : a lan-guage mo ( Ming toolkit and its use in the 1994 art ) acsr evaluation . In ARPA Spoken Language Technology Workshop , Austin , Texas , 

M . Siu and M . Osto . n d or f .  2000 . \; al'i ablc ngrams mid(;xl ;( ~ . n , siol Ls for conv crsatiom tl speech langu ~ g('mo(h ; ling . IEEI ~' Tm , ', , s . on Speech and A'udio P'roc(;s . sing , 8(1):6375 . 
.I . A . Sfi . nch(;z and , J . M . B(;ncdf .  11997 .   ( ) Olnl ) llta-tion of the probability of the best ( hwiw~t ; ion of an initial substring fi'om a stochastic ( : on tcxt-fl'ec ; grammar . In A . Smffeliu, . \] . . l . Villa . mle Va , and . J . \ ; itri ; t , editors , Prm: . VII Spanish , Sym-posi'um , on Pattern Rccogu , itio ' n and image Analysi . Lpages 181-186, Barco . hm a . , Eslmfia,
April . AERFAI.
J . A . SSn(:h ?; zmid J . M . Bcn('(lf .  1998 . Estimation of the l ) robability di , ~tributions of sto(:ha , sti(:(;oni ; (' ~ xt-frc(;grammar , s from th(;\] , :-1) (; st derivations . Inb~,5th,b ~, ter'nationalCo'n- . f('r (: nc(: on Spoke ' , , Langua9(: l)'ro(:c , ssing , i m g c , s2d 952498 , Sidney , Australia . 
.J . A . Sgmchczmid , \] . M . Bcn(;(li .  1999 . lx ~ rn-ing of , % ochast , iccont(~xt-fl'(~'cgrammm's by memos of (' stima . tion ~ flgol'ithms . In 1) ' roe . EU-H,OSl ) EECl\]'99, volume 4, lmges 17991802,
Budal ) cSt , Hungary.
J . A . S~nchez , J . M . B(;nedf , and F . Casacuberta .  1996 . Comparisonlmi , ween the insi ( h~-outside algorithm mid the vitcrl ) i a . lgorithm for stochastic context-fl'ee grammars . In P . Perncr , P . Wang , amd A . Iosenfe . ld , editors , Advances in Str'uct'm'al and Syntactical Pattcrn ll  , ccogu , ition , pages 5059 . Springer-


