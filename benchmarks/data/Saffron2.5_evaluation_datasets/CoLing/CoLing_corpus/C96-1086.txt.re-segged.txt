Inherited Feature-based Similarity Measure Based on Large 
Semantic Hierarchy and Large Text Corpus
Hideki Hirakawa
Toshiba I . & D Center
1Komukai Toshiba-cho , Saiwai-ku,
Kawasaki210, JAPAN
hirakawa ? ee:l ., rdc.toshiba , co.jp
Zhonghui Xu , Kenneth Haase
MIT Media Laboratory
20 Ames Street
Cambridge , MA 02139 USA
xu , haase@media.mit.edu

We describe a similarity calculation model called IFSM  ( Inherited Feature
Similarity Measure ) between objects ( words/concepts ) based on their common and distinctive features . We propose an implementation method for obtaining features based on abstracted triples extracted fi ' omalarge texte or pusutilizing taxonomical knowledge  . This model represents an integration of traditional methods  , i . e , . relation b~useds in > it arity measure and distribution based similarity measure  . An experiment , using our new concept abstraction method which we <' all the fiat probability grouping method  , over 80 , 000 surface triples , shows that the abstraction level of 3000 is a good basis for feature description . 
'1 Introduction
Determination of semantic similarity between words is an important component of linguistic tasks ranging from text retrieval and filtering  , word sense disambiguation or text matching . In the past five years , this work has evolved in conjunction with the availability of powerful computers and large linguistic resource such as WordNet  ( Miller , 90) , the EDR concept dictionary ( EDR , 93) , and large text corpora . 
Similarity methods can be broadly divided into " relation based " methods which use relations in an ontology to determine similarity and " distribution based " methods which use statistical analysis as the basis of similarity judgements  . This article describes a new method of similarity nmtehing  , inherited feature based similarity matching ( IFSM ) which integrates these two approaches . 
Relation based methods include both depth based and pathbased measures of similarity  . 
The Most Specific Common Abstraction ( MSCA ) method compares two concepts based on the taxonomic depth of their common parent  ; for example , " dolphin " and " human " are more similar than " oak " and " human " because the common concept " mammal " is deeper in the taxonomy than " living thing "  . 
Path-length similarity methods are based on counting the links between nodes in a semantic network  . ( Rada , 89 ) is a widely adopted approach to such matching and  ( Sussna , 93 ) combines it with WordNet to do semantic disambiguation  . 
The chief problems with relation-b~sed similarity methods lie in their sensitivity to artifacts in the coding of the ontology  , l ; or instance , MSCA algorithms are sensitive to the relative deplh and detail of different parts of the concept taxonomy  . If one conceptual domain ( say plants ) is sketchily represented while another conceptual domain  ( say , , animals ) is richly represented , similarity comparisons within the two domains will be in-commensurable  . A similar problem plagues path-length based algorithms  , causing nodes in richly structured parts of the ontology to be consistently judged less similm " to one another than nodes in shallower or hess complete parts of the ontology  . 
Distribution-based methods are based on the idea that the similarity of words can be derived fror n the similarity of the contexts in which they occur  . These methods difl'ermost significantly in the way they characterize contexts and the similarity of contexts  . WordSpace(Schutze , 93 ) uses letter 4grams to characterize both words and the contexts in which they appear  . Similarity is based on 4grams in common between the contexts . Church and tlanks ( '89 ) uses a word window of set size to characterize the context of a word based on the immediately adjacent words  . 
Other methods include the use of expensive-to -derive feature such as subject-verb-object  ( SVO ) relations ( Hindle , 90) or other grammatical relations ( Grefenstette , 94) . These choices are not simply iml ) lemelltational but imply ditferent similarity judgements  . The chief problem with distribution based methods is that they only permit the formation of first -order concepts definable directly in terms of the original text  . Distribution based methods can acquire concepts b ~sed on recurring patterns of words but not on recurring patterns of concepts  .  \[ , ' or instance , a distributional system could easily identify that an article involves lawyers based on recurring instances of words like " sue " or " court "  . But it could not use the oc~currence of these concepts as conceptual cues for in connection with the " lawyer " e o n c e l  ) t . 
One . notable integration of relation t ) ased and distrilmtional methods is llesnik's annotation of a relational ontology wilh distributional infornla-lion  ( lesnik , 95a , 95b ) . \] lesnik in L roduees a " class probability " associated with nodes  ( synset . s ) in WoMNet and uses these to determiue similarity . 
Given these probabilities , hee Oltlpt ttestile simi-larit . y of concepts I + ) ased on the " inl ' on nation " that wou\] ( l be necessary to distinguish them , tneasured ttsing iMbrmalion-theoretie calculations + 
The Feature-based Similarity

The Inherited Feature Similarity Measure ( IFSM ) is another integrated approach to measuring simi -la  . rity . It uses a semantic knowledge base where concepts are annotated wit\]\]disli<qlli  . sbiW \] fi'a-ltu'es and i ) ases similarity on ( : otnl > aril ~ . g these sels of feal ; ures . In our exl ) erime\]t ts , we deriw > d the feature sets I ) 3 , a distJ ' ilmtion a \] analysis of + t larget : Ol : l  ) tiN . 
Most existing relation-hase ( l similarity methods directly usel , here lat : iotl ~ Ol ) O/ogy of the semantic network to derive similarity  , either by strategies like link counting ( f ~ a(la , 89 ) or tim determination of the depth < ) f <: otnmonal ) slra <: lions ( Kolod : net , g9) . \[FSM , ineontrasl . , uses the I:Ol ) O\] Ogy to derive ( leseril ) lions whose ( : omparisotly ields a similarity measure . Inl ) arti(:ular , it a SS lllnesart Otl-
I : o\[ogywhere:
I . Each (: once \]) l ; has a set of features 2 . Each concept inherits Features from its get > er Mizations  ( hypernyms )  3 .  \]! ; ; u:h concept has one or more " ( listinctiw ~ features " which are not inherite ( lft : omits hy-\] ) el:nylllS . 
Note that we neidter claim nor require t : hat the feature seon q > let ely chara elerize their  ( : ( m cepts or lhat in h <' . ritan <: e offealm : es is sound . We only required latthere I ) e some set of feal ; ul : es we use for similarity judg cmettts . For instance , a similarity . iud genle 31t betwe ( +mapenguin and arot ) in will t ) epartially based on the fe++ture " ean-\[ly " assigned to the concel  ) t bird , ewm though it ( toes not a pl ) lyit ~ dividually to t ) et\]guins . 
Fig I shows a Siml ) leex at np leo\['a fragment of a ( : once l ~ ttud taxonomy wiLl ~ associated featttres . 
Inherited features are initalh : while disliuctive llal cnl  ( < h~vu-chiid :> ) falhcl ( < male > <\] lave child > ) if lothel ( <female > < hove-chihl > 1 Fig . 1 Fragment ot'c ( mccptual taxonomy \ [ Salutes are in bold . In our model , features have a weight l ) ased otl the importance o1' the feature to the eolleel ) t . 
We\[laV(~chosel\]to all tOlIla , tieally gel\]e rate'ea-tures ( list ril ) utionally by analyzing a large e Orl ) US . 
We ( leseribelids geueration process below , but we will \[ irst ttlrt l to the e\qthlgti ( )tl of similarity based on feat ural analysis . 
2 . 1At ) i ) roaehes to Featm'e Matel fing'l'here area variety of similarity measures a wu\]-able for sets of \[  ; ~ atm'es , biLL all make their eom-l ) arisonst ) ase ( lonsome combination of shared\['etltlH ; es , disLille L\['ealttres , alt d share ctttl ) sellLl'ea- . 
tures ( e . g . , neither X or Y is red ) . For example , Tversky ( '77 ) proposes a mode \] ( based on huntan similarity judgements ) where similarity is a linear combination of shared and distinct features where each f  ( ' at m'e is weighted 1 ) ased on its it nl ) or tatme+'l'w>rsky's experiment showed the highesle or rela-lion with hunmn subjects ~ feelings when weighted shared and dislinet features are taken into consi  ( l-eration . 
HI~X'I'ANT((~reii:nstette , 94 ) introduce ( 1 the \ Veighted 3aeeard Measure which combitms the Jaeeard Measure with weights derive  ( l froth an in h ) rmation theoreti <: an Mysis of % ature occur -fences+'\]'he we : ight of a feature is com\ [mte  ( l from a global weight ( based on then muber of glohal occurrences of the , wor ( lor concept ) and a\[ ( ) ( : at weight ( based Oilthe\['re ( lllellcyOftlt +> Features at-laehed to the word )  . 
\] nour (: urrent work . we have a dol)te(t the Weighted . laeeard Measure for prelimit , ary ewJ-tmti ( mofotu " al ) lJroaeh . ' l'heclistine tiw " feature of our apl ) : roach is the rise of the ontology I . o ( e+rive features rather than assuming atomic sets of 

2 . 2 P roper t ies o f I FSM / u this section we compare IFSM's similarity judgements to those generated by other tneth-  ( )<Is . In our diselts siou , we will consider the simple netwoH ? o ~' Fig 2 . We will use 1 he expression sim . ( ci , cj ' ) to denote the similarity of e oncel ) ts ( haride 2 . 
Given lhesituation of Fig 2 , both MS(L , \ an ( tt lesnik's MISM ( Most Informative Sul>stmtor Method ) asse , ' t . s'im(Ct,C2) = sirn(C2, C3) . 
MSCA makes thesit nilarit . y thesatile because they have the sante ( nearest ) eotmn on abstraction CO . 
MISM holds the similarity I obe the same 1 ) eeause ( : ll ( ' l (  :2 ~" ( "3I " ig . 2I " xanal ) le of a hierardtical strttctul'eFig . 3 l son lo l - ph ic suhst rac tures in h igher / lower levels of hierarchy the assertion of  C2 adds no information given the assertion of C3  . Path-length methods , in contrast , assertsire ( C1 , C2) < sire(C2 , C3 ) since the number of links between the concepts is quite different  . 
Because IFSM depends on the features derived from the network rather than on the network itself  , judgements of similarity depend on the exact features assigned to  C1  , C2 , and C3 . Because IFSM assumes that some distinctive features exist for  C3  , sire(el , 62) and sire(el , C3) are unlikely to be identical . In fact , unless the distinctive features of C3 significantly overlap the distinctive feature of C1  , it will be the case that si , ~(C1 , C2) < si , ~(C2 , C3) . 
IFSM differs from the path length model because it is sensitive to depth  . If we assume a relatively uniform distribution of features  , the total number of features increases with depth in the hierarchy  . This means that sim(C0 , C1 ) located in higher part of the hierarchy is expected to be less than sim  ( C2 , C3) located in lower part of the hierarchy . 
3 Components of IFSM model
IFSM consists of a hierarchical conceptual thesaurus  , a set of distinctive features assigned to each object and weightings of the features  . We can use , for example , WordNet or the EDR concept dictionary as a hierarchical conceptual thesaurus  . Currently , there are no explicit methods to determine sets of distinctive features and their weightings of each object  ( word or concept )  . 
Here we adopt an automatic extraction of features and their weightings from a large text corpus  . This is the same approach as that of the dis -tribdted semantic models  . However , in contrast to those models , here we hope to make the level of the representation of features high enough to capture semantic behaviors of objects  . 
For example , if one relation and one object can be said to describe the features of object  , we can define one feature Of " human " as " agent of walking "  . If more context is allowed , we can define a feature of " human " as " agent of utilizing fire "  . 
A wider context gives a precision to the contents of the features  . However , a wider context exponentially increases the possible number of features which will exceed current limitations of computational resources  . In consideration of these factors , we adopts triple relation such as " dog chase cat  "  , " cut paper with scissors " obtained from the cot -" k dog chases a cat "" k hound chases a cat "" A dog chases a kitty "  ( " chase " " dog " " cat " )   ( *' chase " " hound " " cat " )   ( " chase " " dog " " kitty " )   . . . .  . . . . . . . . . . . . . . o ' o . . . . . . . . . . . . . . o / . . . . . . . o . . . . . . . . . . . 
'"1 I// . ( so ~~, , . 0 . J ~ 0, lo //-< so ~, ~ h,~ . ~ki , , . 51  . . . .  . . . . . . . . . . . . . . . . . . . . . i , , , , , il . . . . . . . . . . . . . . i . . . . . . . . . . . . . . . W . . . . . . . . . . . . i . . . . . . . . . . O-iso ~,,,,~ c Jo ~ ?,~, -, ) 0 0 (9 0 i . -  . e % ed .   . r , ooeTri , ls . . . . . . . . . . . . N . . . . . . . . . . . . . . N . . . . . . . . . . . . . .  . . . . . . . 
i O O O?O O . O O O O O O
L . Deep Triples . ~ ~ ..\ [.... ~..
////( SOv228 n5n9 . . .  5 . 332 (" dll , Se"nmafter " ) ("dog " " hound"~"cat " ki ty " ) ) :~0 0 O " 0 0 0 0 0 i ,   ,   , II 11 ? Abstracted Triples .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . III
I Filtering Heuristics I00 , , , Filtered Abstracted Triples .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 
Fig . 4 Abstracted triple extraction from corpus pus as a resource of features  , and apply classbased abstraction ( Resnik 95 a ) to triples to reduce the size of the possible feature space  . 
As mentioned above , features extracted fi'om the corpus will be represented using synsets/concepts in IFSM  . Since no largescale corpus data with semantic tags is available  , the current implementation of IFSM has a word sense disambiguation problem in obtaining class probabilities  . Our current basic strategy to this problem is similar to  ( Resnik , 95 a ) in the sense that synsets associated with one word are assigned uniform frc quency or " credit " when that word appears in the corpus  . 
We call this strategy the " brute-force " approach  , like Resnik . On top of this strategy , we introduce filtering heuristics which sort out unreliable flata using heuristics based on the statistical properties of the data  . 
4 The feature extraction process
This section describes the feature extraction procedure  . If a sentence " a dog chase dacat " appears in the corpus  , features representing " chase cat " and " dog chase " may be attached to " dog " and " cat " respectively  . Fig 4 shows the overall process used to obtain a set of abstracted triples which a resources of feature and weighting sets for synsets  . 
4 . 1 Ext ract ion of surface typed tr ip les f rom the corpus Typed surface triples are triples of surface words holding some fixed linguistic relations  ( Hereafter call this simply " surface triples " )  . The current implementation has one type " SO " which represents surface triples are extracted from a corpus with their frequencies  . 
Surface triple set ( TYPE VERBNOUN 1 NOUN 2FREQUENCY ) 
Fx . ( SO " ch ~ se " < log " " cat " 10) 4 . 2 Expans ion of sin-face tr ip les to deep triples Surface triples are expanded to corresponding deep triples  ( triples of synset IDs ) by expanding each surface word to its corresponding synsets  . 
The frequency of the surface triples is divided by the number of generate deep triples and it is assigned to each deep triple  . The frequency is also preserved ~ it is as an occurrence count  . Surface words are also reserved for later processings  . 
Deep triple collection ( TYPEV-SYNSE'FN 1-SYNSETN2-SYNSEq'FREQENCY
OCC UttRENCEV-WORDNI-WORDN 2-WORI))
Ex . ( SOv123n5n 90 . 210 " chase "" < log " " cat " )  "  v123" and " n5" are synset IDs corresponding to word " chase " and " dog " respectively  , These deep triples are sorted and merged . The frequencies and the occurrence counts are summed up respectively  . The surface words are merged into surface word lists as the following example shows  . 
Deep triple set ( TYPEV-SYNSET N1-SYNSEq  '  N2-SYNSET FREQUENCY OCCURRENCEV-WOttDS N1-WORDS   N2-WORDS   ) gx . ( SOv123n5n 90 . 7 15  ( " chmsc " ) (" dog " " hou nd " )   ( " cat " ) ) In this example , " dog " and " hound " have same synset ID " n 9" . 
4.3 Synset abstraction method
The purpose of the following phases is to extract featm : esets for each synset in an abstracted form  . 
In an abstracted form , the size of each lhature space becomes tractable . 
Abstraction of asy use t can be done by divid ~ ing whole synsets into the appropriate number of synset groups and determining a representative of each group to which each member is abstracted  . 
There are several methods to decide a set of synset groups using a hierarchical structure  . One of the simplest methods is to make groups by cutting the hierarchy structure at some depth from the root  . 
We call this the flat-depth grouping method . Another method tries to make then mnber of synsets in a group constant  , i . e . , the upper/lower bound for a number of concepts is given as a criteria  ( ttearst , 93) . We call this the flat-size grouping method . In our implementation , we introduce a new grouping method called the flat-probability grouping method in which synset groups are specified such that every group has the same class probabilities  . One of the advantages of this method is that it is expected to give a grouping based on the quantity of information which will be suitable for the target task  , i . e . , semantic abstraction of triples . The degree of abstraction , i . e . , the number of groups , is one of the principal factors in deciding the size of the feature space and the preciseness of the features  ( power of description )  . 
4.4 Deep triple abstraction
Each synset of deep triples is abstracted based on the flat-probability grouping method  . These abstracted triples are sorted and merged . Original synset IDs are maintained in this processing for feature extraction process  . The result is called the abstracted eep triple set  . 
Abstracted deep triple set ( TYPEV-ABS-SYNSET NJ-ABS-SYNSET N2-ABS-SYNSFT V-SYNSE q'-LIS q' N1-SYNSE'r-LIS'f   N2-SYNSET-LIST 
SYN-FREQUENCY OCCURRENCE
V-WORDSNI-WORDSN2-WORDS)
Ex . ( SOv28 n5n9(v123v224) ( n5) ( n9n8)5 . 332 Cc ! . . . . " " ru n "' after" ) C dog " " hound " ) C cat " " kitty " ) ) Synset " v28" is an abstraction of synset " v123" and synset " v224" which corresponds to " chase " and " run_after " respectively  . Synset"ng"con:e-sponding to " cat " is an abstraction of synset " nS " corresponding to " kitty "  . 
4 . 5 Filtering abstracted triples by heuristics Since the current implementation adepts the " brute -force " approach  , almost all massively generated deep triples are fake triples  . The filtering process reduces the number of abstracted triples using heuristics based on statistical data attached to the abstracted triples  . There are three types of statistical data available  ; i . e . , estimated frequency , estimated occurrences of abstracted triples and lists of surface words  . 
\[ ler % the length of a surface word list associated with an abstracted synset is called a surface support of the abstracted synset  . A heuristics rule using some fixed frequency threshold and a surface support bound are adopted in the current implementation  . 
4 . 6 Common feature ext rac t ion f rom abstracted triple set This section describes a method for obtaining features of each synset  . Basically a feature is typed binary relation extracted from an abstracted triple  . From the example triple , (SOv 28115n9(v12 av224) ( , ,5) ( , ,9 ns ) , ~ , a a ~ ( " chase " " run " ' after " ) (" dog " " hound " )   ( " cat " " kitty " ) ) the following features are extracted for three of the synsets contained in the above data  . 
n5(ovv28n95 . 3 32  ( " chase " " run " ' after " ) ("cat " " kitty " ) ) i19 ( svv2Sn 55 . 3 32  ( " chase " " run " ' after " ) (" dog " " hound " ) ) n8 ( svv28n55 . 3 32  ( " chase " " run " ' after " ) ( ' dog " " hound " ) ) An abstracted triple represents a set of ex -mnples in the text corpus and each sentence in the corpus usually describe some specific event  . 
This means that the content of each abstracted sally true  . For example , even if a sentence " aman bitadog " exists in the corpus  , we cannot declare that " biting dogs " is a general property of " man "  . Metaphorical expressions are typical examples . Of course , the distributional semantics approach assumes that such kind of errors or noise are hidden by the accumulation of a large number of examples  . 
However , we think it might be a more serious problem because many uses of nouns seem to have an anaphoric aspect  , i . e . , the synset which best fits the real world object is not included in the set of synsets of the noun which is used to refer to the realworld object  . " The man " can be used to express any descendant of the concept " man "  . We call this problem the word-referent disambiguation problem  . Our approach to this problem will be described else whc're  . 
Preliminary experiments on feature extraction using  1010 corpus In this section , our preliminary experiments of the feature extraction process are described  . In these experiments , we examine the proper granularity of abstracted concepts  . We also discuss a criteria for evaluating filtering heuristics  . WordNet 1 . 4 ,   1010 corpus and Brown corpus are utilized through the exI  ) eriments . The 1010 corpus is a multiqayered structured corpus constructed on top of the FRAME IX-D knowledge representation language  . More than 10 million words of news articles have been parsed using a multiscale parser and stored in the corpus with mutual references to news article sources  , parsed sentence structures , words and WordNet synsets . 
5 . 1 Exper iment on f ia t - p robab i l i ty group ing To examine the appropriate number of abstracted synsets  , we calculated three levels of abstracted synset sets using the fiat probability grouping method  . Class probabilities for noun and verb synsets are calculated using the brute force method based on  280K nouns and 167K verbs extracted fl ' om the Browneortms ( 1 million words )  . 
We selected 500 ,  1500 ,   3000 synset groups for candidates of feature description level  . The 500 node level is considered to be a lowest boundary and the  3000 node level is expected to be the tar-
I ) epth 1 2 3 4 5 6 78
Synsets 6111 22966 2949 5745 122938 3847 408
Depth 910 1112 1314 1516
Synsets 519 13068 1781 23149 4366
Table 1. Depth/Noun_Synsels in WordNet 1.4
Level 500 ( 518 synsets )  1  ( structure construction \] ( 72\]9 . 474): a thing constructed ; a con ~ . plexeons true tio I ' lor entity 2 time_period period period_of_tilne\] ( 6934 3 ) : a lengthef time ; " government services began during the colonial period "  3 organization \] ( 6469 . 94 4 ) : a group of people who work together 4 action ( 6370 . 549): something done ; 5 natural_object (6277 . 263): an object occurring naturally ;
Level 3000 ( 3001 synsets ) 1 natural language tongue mother tongue \] ( 678 . 7 6 ) : the language of a community ~ 2 we apon arm weap on_system \] ( 673 . 7 ( ~ 6 ) : used in fighting or hunting 3 head chief top_dog ( 671 . 55): 4 capitalist (669 . 45  ) : a person who believes in the capitalistic system  5 point point_in_~ime ( 669 . 298): aparti . cular clock time ; Table 2: Synsets I ) y flat-probal ) illtygrouping metho ( 1 get abstraction level . This expectation is based on the observation that  3000 node granularity is empirically sulficient for deseribing the translation patterns for selecting the proper target Fmglish verb for one Japanese verb  ( lkehara , 93) . 
Table 1 shows the average synset node depth and the distribution of synset node depth of  Word-Net1  . 4 . Table 2 lists the top five noun synsets in the fiat probability groupings of  500 and 3000 synsets . "" shows synset . The first and the second number in "0" shows the class frequency and the depth of synset respectively  . 
Level 500 grout ) ings contain a very absracted level of synsets such as " action "  , " time_period " and " natural_object " . This level seems to be too general for describing the features of objects  . 
In contrast , the level 3000 groupings contains " natural_language " , " weapotf ' , " head , chief ' , and " point_in_time " which seems to be a reasonable basis for feature description  . 
There is a relatively big depth gap between synsets in the abstracted synset group  . F , ven in the 500 level synset group , there is a two-depth gap . In the 3000 level synset group , there is 4depth gap between " capitalist " ( depth 4: ) and " point_in_time " ( depth 8 )  . The interesting point here is that " point_in_time " seems to be more at  )  . 
stract than " capitalist , " inluitively speaking.
The actual synset numbers of each level of synset groups are  518  ,  15%8 , and 3001 . ' fhus the fiat probability grouping method can precisely control the lew ' J of abstraction  . Considering the possible abstraction levels available by the fiat-depth method  , i . e . , depth 2 (122 synsets ) , depth 3 (966 synsets ) , depth 4 (2949 synsets ) , this is a great advantage over the flat probability grouping  . 
5 . 2 Exper iment : Abst rac ted t r ip les f rom 1010 corpus A preliminary experiment for obtaining abstract triples as a basis of features of synsets was conducted  .  82 , 703 surfaces vo triples are extracted from the 101 . 0 corpus . Polarities of abstracted triple sets for 500 ,  1500 , 3000 level abstraction are 1 . 20M , 2 . 03M and 2 . 30M respectively . Each 1 organization talks p , : a kuttermouth verbulize vurbi ~ vorganization\ ]   ( 70 , 4 . 24 , 1)8)2 organization ) talkspcal ~ utt < rinottth v , rl > aliz < ver/- , if y action (5(; , 'La5 , 112 )   3 organization changettn dergo ~ a_change becozlLe_  ( liitk:rent l > ossession ( 60 , 2 . 83 , 188 )   4 torgauization talks peak uttermouthvez'lmlizu vcrbi ~  , \]   . . . . . . . ~ t ) (48 , 175 , a4) 5cn'ganiza*ion move displac . . . . . . . k e . . . . . . . . . action (5( ), ~ . s4,82)
L , wel 3000
Ic ? pcrL greetz ' (: cogniz ~ . :) /" cxpr < sagrc(ring:;up(mnlcc!ing . " due_process due_process_of_law 2 jmT/"a body of citi'z cns sworn to give a true verdict  . " ~ l > ronoun (: eIM ) el judge ) /') t ' Ol ' ~ Otll ~(' c , j t t d g m , nton " capit ~ a list (4 , 11 . 09 , 4 ) 3 police police_force constal ) ulary law allegea versay /" lt ( alleged lt lat w ( was the victim " female fen : ~ alc_pel's on ( 4 , \] . , 3)4 assc'nfl)ly"abody that holds formal n~(x . lings " refusel't ~ jccLl) , '/sS_tll3Ltlrll_(Iown(h<:\[in(!/"rcfllsOI ; 0 ~ t(:('c\])t ; " r , : quest petition solicitation (( ; , 0 25 , G ) 5 animal animate_b < : ing /) ~ ; mlI ) r ttt0 Cl'c~tlttl'uft~llllH/wingaitL/'win somvthiug dlrough one's  ;   ,  . ll\~vls " contest comt ) etition \] (5, 0 . .19,6) "()"  . + I . . . . . . (# ,, f s,,,,r . . . . . . . . . s , , pl , , , ~+ ~, l', . ,,q,,, . , , y : o .   .   .   .   .   .   .   . i , , , , . . . . . . . ) TaMe 3: ~ xamph ! of abstracted triple . '+ abstract triple holds ft:equeu (: y , oc < : llJ ' lX ) llO ( , lltl/ll- . 
be G and w of ullist , which mq q~(a't . seach of thce(~al ) stra(:ted synsel:s . 
A liltering heuristic that el in ~ htates al:+sll'a < : ttrit  , les whose stlr\['ac(;Sul)pOrl is three ( i . e . , supported ) y only one sm ' face\] ) ~I ~ LC\]:II ) iSal > plicd to each set of al ) sLracLed Iril ) les , ; 111 (1l ; ( . ','- . ;tl\[I . siuthe R ) llowing sizes of at ) stract : ed triple sets in the 379K ( level 500 )  , 150b : ( level 1500) and 561 , :(\] ewq3000) respectively . F , ach triple is assiglted a evaluation score which is as nt  , of m ) rnmlized surface SUl ) l ) (~ rL score (: : Sll . l:f3e (' . Sllt ) l ) or l ; s < : ore/tl-taXil Htlll , qlll'I ' ~ ~ / ce SUl ) l ) or L score ) ; + timnormalized\[\]:e(luet ~( ; y(~fre+(luency/nmxi/unmfi'equency ) . ' l ' at ) le3 shows the top\[beabstra <' tedtril ) les with respect odw+i rew dua Liot ~ scores , ltetns in the talJe shows subject sylls eL , vert ) SyllSel; , ohj 0 el ; synseL , sttrfa <: esup-l ) Orl; , fr <' qtleltcy ~ tll(\]oc (' . ll \]) re\]lC(~IlllIlI \])0 I!S . 
All the sul<iccl ; s in the top five al ) sL ract triples of level 500 are " organizal ; iotf ' . This seems to be . 
r0 as on al)le bee ; rose the COlll ; eltl ; s of the 10\]0 corpus are news articles ~ t t ( tl : hese triples seem to show some highly abstract  , briefing of the cont . ent , s of the corpus . 
The clfc clAveness of the filtering ; re(l/or scoring hcuri6dcsca , nbctl:l(~a , ,stlr(;(l ttsilt ~ tv ?(~ ch)scly re- . 
lated criteria . One measm : esthel ) laus it fility o\['al ) stract . ed triple , si . e . , the r(x'all and l)cecision ' a -+ I ; io of the l )\] ausible at ) straeted Lriples . ' l ' he other criteria shows the correctness of the mappings of die surface t  ; riple I ) at Lerns to abstracted tril ) les . 
varsity\[htiled_Nationst:ealn subsidiary State state staffs o+vier school l'o lit buropolice patrol partypalml Organization OI '  ( \[ CI " operationlle !' v VS l ) ~/ , )Vl ' li issioll Ministry lll ( :II21 ) t ' Flll\[tg\[~-zine lin ( : law_firm law hind3u:~tice_l ) epartmcnt jury industry hOLS ( ? h<~ztd ( tual't ,  . ' I'S govut'illI/tHltg?tllgI:tlA division CO lllq  , : ' Oll H-try co/lllcil Collf ( ! l'eltc ( ! <: ( Jllll ) tlly ( ' ommitlct ( : ollcge ( :  ht\]2 Cabi-net business board associ ~ tion Association airline Table  4  . SII rflk(?(~ . Snplmrt , s of " orgmdzation " This is measured I ) y counting thee on : ect surface supports of each absl  . racted triple , l " or example , considering a set of sm : l '; u : e words sut ~ port . ing " o>ganization " of L he I of level , 500 shown in table 4 , the word " panel " rnight loeused as " panel board "  . 
' l'h is abilily is also measm:ed by developing the word sense dismnbiguator whic  . hinputs the sur-fa ( : etril ) le and select : slhemost l ~\ [ ausil ) le deep Iril ) lebased ou abstracted triple scores matched with the deep triple  , ' Flm surface SUlh ~ octs iu ' l' ; t--hie 4 show the intuit Netendency that a suftlcient number of triple data will generate solid results  . 
6 Conclusions
This paper described a simil + ~ rity calculaik ) n model between ol )  , je+cl . s based on commoz ~ and dis-linctiwe feal , ures ; -mclprol ) oses an hnplementation l > rocedure \[ brobtaining feat  ; ures based on al>stract lriples extracted loma large text <: or pus  ( 1010 corpu , ~) utilizing taxonomical km ) wle(lge(WordNet) . The exl ) erittettL , which used around 801 SL lr faee triples , shows l , lal ; t , he abstraction level 3000 l ) rovi(le . sa good basis for \[' eal ; ttre . (les<-zit)-lion . Afeal ; m ' e extra ( : tioneXl ) erhnent based ( m large trii ) \] c ( \[ a  ~ a is ( ) tic next . goal+leferences\](~:nneth(ThurchmidPal ; rick Hanks . t98 , 9 , l/Vo~dass oc2-at ~ on norms , ~ nnt ~ alinj ' or m , ttio . n , and h ? . rlcogvaphy , \] n Pro-ccc dings of th ( . '27th Annual Me+~tittg of A(:I,EI ) IL 1995 .  >' . umma ~?/ for the I ' ; I ) \] ~ Ele <: tronwl ) ict'~o ? ~ argVersion lTechnical<:'lz~de El ) If T1~2-005 , Japan I '; \] ectronic I ) ictionary II . cs ~ arch Iiml : ilul %' l'o kyo . 
( h'egory Grctbnstettc 1994 . I ' ; xp to vations in A~ztotnatic7'k+saurusl )'~ scovc*y , /( luwur Academic Publishers . 
Marti . A . Ilcarst and Him ' lobS < hutze .  \]9 . q ' l . (: ~ zs~om+znl9 aLc:!:~contoHettcr . % tztaComl ) ut at'wnal Task . Proco . dmgs of the ACf , SIGI , I . ; X Workshop , ( hJmnbus , Ohio . 
\]) cmaldll in dh . 19 q 0 . No ~ z*t class zJi cativn ~( r ; ;m pred'u:ale i~<q~t?ncn ~ st?ltCttZ?t : . % Proceedings of life 28th Anntt~d Meeting of A (  :1 , S . Ikthm '; t,M . Miyazaki , A . Yokoo .  1993 . (' lass ~ firat ' ~ o ~ of J ) a ~ . jtitt~je\[(llo2ulrtdfJ(!fo~'Mca'lr*ny . Anahjs'ls JnA . hz(h . zneTtams Jat ~ on , Transactions of h/lk)z'mal ; iont ' roces Mng Society of Jal > aP ,   , Vol 34 , No . 8, pl > s .  \]692-1704 . 
J . Kolodner and C . H . ies beck . Cast > l\]ased Rt as ott ~ n ~ h tutorial t . xt book of \] lth IJCAI . 
( ; , org ( A . Miller , ILichar dileck with , Christian et " el\[bmtm , I ) tl ' C\]((\]I'ONSIK ad~crine Millet < 19!)0 . l " ~ ve Papers on Word-N , t , ( k)gni livcSciunce\] , M ) or ; ~ tory I Leport 43 , Princetont Jni-vcrsity . 
lloy Rada , Hafeclh Mill , \] " llen Bicknell , Maria Bh'ttner . 
1989 . Z ) eu alvpme'ntalz dApph cat + oa of a Metric on 5rc ; tlalt ltcN+ts , \] I' ; I' ; EqYansacl : ionson Systems ~ MmG : rod('yberncti , ' s ,  \%1 . tg , No . I . 
Phiiptt : , n nik . l ! 10, g z t . ? i , ' sZnClinfo?~nat+o~t('o ~ ttcrlttol " , val-tizzY +> ' cm~u ~ ticSzmilarityi ~ ta Taxonomy  , Pz ' occedings of 1J (: A\[-95 . 
Philiplies nik . 1!)!151)I)'lsamb~guatingNo~t ~(,' roul . ulgsw~th\]es pccto I/V ordNet 5' e'as+~s , Proceedings of Annual
M(.cting of ACt , .
lIinz'ichS chutze .  1993 . Adva ' ~ wesin Neurall Tz for n ~ at ~ on t bocessing , <' gste*ns5 , Stephen , / . \[\[ anson , Jack D . Cowan , (: . i , ce Files editors , Morgmt Kaufmmm , San Marco(?A . 
Michael SklSSll/t .  1993 . Wo~d : ' ensck ) ' ~ sambiguation for t " rce-temthzdccqnqUsing a Massive Semantic Network  , P\['o-cc dings of the S , ,?(:on (\[ \[ nterlmtional Conf~rcn(:e , :) n\[n for nla-don and l(zzowl ,  . dge Managument ( CIKM-93) Amos Twrsky 1977 . \[" aat+tresOf5'im Ha~ity . l ' sychological t-view , Vol . 84, Number 4 . 

