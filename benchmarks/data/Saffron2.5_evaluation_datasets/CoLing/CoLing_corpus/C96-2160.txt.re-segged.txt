Computing Phrasal-signs in HPSG prior to Parsing 
Kentaro Torisawa andaun'ichi Tsujii
l)ci ) artmenl ; of Informat , i on S (: ic ncc , University of Tokyo , 
ltongo7-3-1, Bunkyoku , Tokyo , 113, Japan
torisawa , tsujii0 is . o . u-'cokyo . ac . jp

This t ) ai ) er deseril ) es techniques to compile lexical entries in I IPSG  ( Pollard and Sag ,  1 . 987; Poll ; ml and Sag ,  1993 ) -style grammar into a set of finite state automata  . The states in automat ~ L are possible signs derived fl ' omh '  , xical entries and contail information raised fl'om the lexical entries  . The automatt ~ are augmented with feature structures use  ( l by a partial unification routine and de-layed /frozen definite el  ; rose programs . 
1 introduction
Our aim is to 1) uil dane , fli(:ient and robust\]tl)SG-based parser . IIPSG has 1) ( ; eure , gar(led as a sophisticated but fl : ; t gile and inettieient ff~mw work . 
However , its principle-basedal ' ( : hitecturenables a parser to handle realworld texts only by giving concise core grammar  , including principles and templates for lexicM entries  , default lexical en-tries ( Horiguchi et al ,  1995) . The architecture is different fl'om those of e onvelltional unification-l  ) t~sedff ) rm Misms which require hundreds of CFG skelet ; on stot ) arse realworld texl ; s . 
However , tiles (' , design prin ( :il ) les of llI'SG have drawbacks in parsing cost . That is , signs/feature structures corresponding ~ on on -termillal symbols ill CFG become vi  , sible , only after applying t ) l ' in ci-pies all d ~ t t ) & rs0r has to Cl'e~4te ~ (  ; & tlIl ; Cstl'IlCtllleS ( ) lie by one using unification . \] in addition , identity checking of non-terln in M symbols used to eliminate spurious signs must be replaced with subsumption checking  , which flH ther detcrior ~ tesef-fi (' ien(:y . 
Our grammar eompih ', rCOlnpute skeleta . l1 ) ~rt of possible phrasal-signs froln individual h ! xicalenl  ; l ' ies prior to parsing , and generates a set of finite state automata from h ' ~ xical entries to  ; tvoid the above draw-I ) acks . We call this operation Oil-lineraising and an automaton thus generated is called a LexiealEnt ryAutomaton  ( LA )  . its states corresponds to 1) art of sigl , s and each transition between stales ( ; or respon(lstoat ) plication of a rule schema , which is a nonqexical comt ) onent of grammar . 
Our parsing algorithm adopts a two-i ) hased l/arsing method . 
Phase1\]iottom-up ( : hart-like 1 ) ~rsing with LAs . 
Ilewriting lt , ule:
MOTIIEI(\[1\]) , IIEAI)-I ) TR(\[81) NON-IIEAt)-I)TI~(\[S\])
FS : sign\['......'tt
Syllb;II)C ~) a , content\[4\]scmiIKlices\[3\] ,   . . . . . . ~  .  \] 1  . . . . . . l-dr ,, \[ s \] '~ Y " \[ . ~, l , , : ~, :(, ~ r l >
S ( ! lll indices indices \] J goals arg2   2   arg2 fl ' ( ! t!z ( ~ Figur ( '1:AlLe . xalnph ; of a rules (: hema . 
Phase 2 Computing part of feature , structures which cannot l ) e (: omput cdat ( ; oml ) i le-t in m . 
We call tilet bature structures that are represented as states in automa  , t ; ~ mtd are CO ml ) uted at conlpih > time Core-structures , and the fca-tl lrestrll Ctlll'es whi ( ; hare t ; ol)e (: Olnl ) ut , ed in Phase 2 , Sub-structures . Inl ) h~sc1 parsing , t ~ (: ore-si , ruct me . (: or r (, spond to a state in an \], A . The cost of comt ) uting substructures at Phase 2 is in in-imized by Dependency Analysism MPart i al 
Unification.
Tile next section describes rules chcmtm ~ , centrale ompouents of the . formalism , and gives ~ definition of Definite Clause Programs  . Section 3 describes how to obtain LAs h'om lexical entries and how to perform the Phaseip  ; ~ rsing . Section 4 ex-pl Mns the Phase 2Parsing algorithm . A parsing exmnple is ln'es(;iLted in Section 5 . The effectiveness of our method is exeinplified with a series  ( 117 eXl ) eriments in Section 6 . 
2 Rule Sch(:nmta and Definite
Claus ( ; l ) rograms
Our fonm dism has only one type of compolmntg-tS llOll-10xic&l   (  ; olll ) Ollellt , q of ~ rallll nar , i . e . , rule schemata . IAn example is showl in Figure 1 . Aruh's (: henl ; ~ consists of the following two items . 
I h l ( ) I l l ' c i l r r ( ! i l t , sysLeil/~rill(' , s(;h(2mltt ~ l'(' . golt c . r-i~t(' . (1 froul principh, . s and rewriting rules ~ m (: ording ( ; c ) ; LSlm Cifical , ion given by ~ progr ~ mmter . 
949 rule ( R ) a rewriting rule without specific syntactic categories  ; fs(R ) a feature structure . 
A characteristic of HPSG is in the flexibility of principles which demands complex operations  , such as append or subtraction of list-value feature structures  . In our formalism , those operations are treated by a Definite Clause Program  . ADCP can be seen as a logic program language whose arguments are feature structures  . An auxiliary term , a query to a DCP augmenting a rule schema , is embedded in a feature structure of a rule schema as the value of goals  . The rule schema in the exam-pieh as an auxiliary term  , append (\[1\] ,  [2\] ,  \[3\] ) . 
The bottom-up application of the rule schema R is carried out as follows  . First , two daughter signs are substituted to the HEAD -DTP ~ position and NOR-HEAD-DTI~position of the rewriting rule rule  ( R )  . Then , the signs are unified with the head-dtr value and the non-head-dtr value of the feature structure of the schema  , fs(R ) . Finally , the auxiliary term for DCPs given in the schema is evaluated ? Our definition of a DCP has a more operational flavor than that given by Carpenter  ( Carpenter ,  1992 ) ? The definition is crucial to capture the correctness of our method  . 2 Definition 1 ( DCP ) A definite clause program ( DCP ) is a finite set of feature structures , each of which has the following form . 
goals ( HI113) Jnext-steps\[goaSs(1~o , B 1 ,  ' ' '  ,  ~  , \[1\])\] a where 0 <_n and H , Bo , " ? ? , B , ~ are feature structures . 
A feature structure of the above form corresponds to a clause in Prolog  . H , B0, .   .   . , B  ~ corresponds to literals in Prolog . H is the head and Bo , " '  , B , ~ are literals in the body of a clause . 
Definition 2 ( Execution of DCP ) Execution of a DCPP for the query , 
C2~e~y =\ [ go~Ss(qo , q  ~ , '" qz )\] is a sequence of unification , 
Query Url Ur 2 U . . . Urn where ri =\[( next-steps ) i1Ci\] , C , ? PorCi =\ [ goals0\] . /f the execution is terminated , C ~ must be unifiable with\[goals()\] . In this case , we call the sequence ( r l ,  ' "  , r , ~ a resolu-tion sequence . 
2 Though , through the rest of the paper , we treat the definition as if it were used in an actual implementation  , the actual implementation uses a more efficient method whose output is equivalent with the result obtained by the defiifition  . 
a(H0, . - '  , H  ~ ,  \[1\] ) is an abbreviation of rest "' rest\[1\]
E21\[?N ~, b cat 0
My colleague\[sig,!~d\]
S2maJ subcat('qlms . ) V subcat ( \[2\] NP ) ln~isutJ cat ( ~maj V subcat ( \[ lJNI' , \[2\] NP ) wrote a good paper
Figure 3: A parsing example ( next-steps ) i1\[goals of QueryHrlHr2H . ? ? Hri represents the goals which are to be solved in the steps following the ith step  . The goals are instantiated by the steps fi'om the first one to it hone  , through structure sharings . The result of execution in a Prolog-like sense appears in the query  , Figure 2 is an example of execution for the query append ( \[ a\] , \[ b \] , X ) , whose definition is based on a standard definition of append in Prolog  . 
Given this definition of DCPs , an application of a rule schema to two ( laughter signs D1 and D2 can be expressed in the following form , where @1 , r2 ,   .  "  , r , ~ is a resolution sequence : M =\[ head -dtr non_head_dtr  D~D2 \] Ufs ( R ) U' , 'tUr . 2U . . . Ur , ~  3 LexicalEntry Automata This section presents a Lexical Entry Automaton  ( LA )  . The ineifieiency of parsing in HPSG is due to the fact that what kind of constituents phrasal-signs would become is invisible until the whole sequence of applications of rule schemata is completed  . Consider the parse tree in Figure 3 . The phrasal-signs $1 and $2 are invisible until a parser creates the feature structures describing them  , using expensive unification . 
Our parsing method avoids this online construction of phrasal-signs by computing skeletal part of parse trees prior to parsing  . \ [ n Figure 3 , our compiler generates $1 and $2 only from the lexical entry " wrote , " without specifying the nonhead daughters indicated by the triangles in Figure  3  . Since the nonhead aughters are token-identical with subcat values of the lexical entry for " wrote "  , the obtained skeletal parse tree contains the information that St takes a noun phrase as object and  $2 selects another noun phrase . 
Then unifying those nonhead daughters with actual signs constructed from input  , parsing can be done . An LA expresses a set of such skeletal parse trees  . A state in an LA corresponds to a phrasal-sign such as Sj and  $2  . They are called core-structures . A transition arc is a domination link between a phrasal-sign and its head daughter  , and its condition for transition on input is a nonhead 
Pl " Ogt'i Ull:
I , ' ~ xecutiot:
Q 10(J2 = :
C ; I = : goal sllext-steps goals 1.....
/i ~ l'g , 1(/lll'g ~\[2\] I\[\]\],
L , , , . ga\[21m , , , l . ~\[ J\]\]next-steps .   .   .   .   .   . 
.-,, . ~ a\[ ( ! ~' q\]\[l\[~i ) \] argl4\[e-list\]arg'36& rgl ( : goals arg2 ( 21 II (   ; 2 U\[nca:t- . sL,ep . sCI \]=: ( goals ('2 = next-steps l\[7\]\[(((((((((((((((((((( , -list1\[;~ , \[\[!\[) elist\])\[(231\[8\]\](\[q\[ , , )) a , ' g2r ,   ( f : ~ t\[\4\] ) \[\[7\] gl'ga ( \[3Jl\[~ ) goals arg24 i\[r \] re'g3
I\[r)\[e-list\]) next-steps / , u ~ J4\[ , < i . ~t\]goals/arg25b . . . . xt-steps goals <) \]
I\[r\]\[,!-li,~t\])\]
Figure 2: An examl ) le of DCP's execution daughter , such assigns tagged\[1\] and\[2\] in Figure 3 . Kasperc . tal .  1 ) resented an idea similar to this@lineraising in their work on HPSG-TAG compiler  ( Kasper et al ,  1995) . The difference , is that our algorithm is based ou substitution , not adjoining , Furthermore , it is not clear in their work how offline raising is used to improve ef\[i-cicn cy of parsing  . 
Before giving the definition of LAs , we detine the notion of a quasi-sign , which is part of a sign and constitutes l ~ As . 
Definition 3 ( quasi-sign ( n )   ) For a given integer n , afcatu , estructure S is a q'a asi-sign ( n ) if it has some of tile following four attributes : syn  , sem , head-dtr , non-head-dtr and does not/Lave values for the paths  ( head-dtr+non-head-dtr ) "" . 
Aqua, . si-sign ( ' n ) cannot rel ) resent a parse tree whose height is in ore than n , while a sign can express a parse tree with any height  . Tlm ) ugh the rest of this 1) aper , we often extract a quasi-sig"n . ( n ) S from a sign or a quasi-sig ' , ,( , n /) S ' where ' . , < n ' . This operation is denote ( l by S ' = c'x(S ' , , n) . 
This means that 5' is equivMent to S'except ff ) r the attributes head-dtrm M non-head-dtr whose root is the  ( head-dtr+non-head-dtr ) ' ~ value in S ' . Note that S and S ' are completely different entities  . In other words , S and S ' pose different scopes on structure sharing tags  , in addition , we also extract a feature structure Freached by a path or an attribute  1  ) in a feature structure IP ' . 
We denote this by F = val(F' , p ) and regard F and F ' as different entities . 
Definition 4 ( LexicalEntry Auton , at on ( LA )   ) A Lezical Entry Automaton is at uplc ( Q , A , qowhel'e ~ Q : a set of states , where a . state is a quasi-sign(O ) . 
A : a , set of transition arcs between states , where a transition arc is a tuple ( q d , q . . . . N , D , R ) where qd , q , . 6 Q , N is a quasi-sign(O ) , Disaquasi-sign(I ) and R is a rule schema . 
q o : tile initial state , which corresponds to a lezi-cale rL try . 
In a transition : - t t '(; < q d , q . . . . . N , D ,  1~  , q , ~ denotes the destination of the transition arc , and q d is the root of the arc . The N is a nonhead daughter of al ) hrasal-sign , i . e . , the destination state of the transition , and expresses the input condition for the transition  . The D is used to represe , nt : the dependency 1 ) etween then n ) ther sign and the daughters through structure sharings  . This is called a Dependency Feature Strueture ( DFS ) of the transition arc , the role of which will be discussed in Section 4 . 1~, is the rule schema used to create this arc . 
An LA is generated fl'omalexieal entry l by the following recursive pro  ( : edure : 1 . Let ;, ~; 1) e /, A be a neml ) ty set and s d =/2 . Forea (: h rule , schema1 ~ , and for each of its ea ( : h resolution sequence ( rl ,   .   .   . , ' r,~obtain,1)-\[head-dtr,Sd\]uf . s(l ~) ur , u . - . or , ~ and ifl ) is a feature structure , obtains , , = ex(D , O ) and
N = ex(w ~ l(D , non-head-dtr ), 0).
a . If Disat ~ ature structure , ? If the , re is a state s ~ , ~6 S such that s ' , ~  , s . . . . 4 let s ,~ be s ~,~ . Otherwise , adds , , ~ to 5" . 
* If there is no T'r = \'/~" d ,  '~ , ,~"' , N " , D " ,  1~)
A such that . % ~ ~ s ' , ~ , s , z~sSl , N4 For ~ my feature structures f~mdf' , f~f'ifffEf ' ~ mdf'Ef assume = (1 , r , S , Dep ) return SU sub-structure ( e ) sub-structure ( e : edge )  ; assmnee=(l , r , S , Dep )
If Dep = 4) then return sub(S ) , else for each ( D , eh , e  ~ , R ) C Dep , assume that el ~= ( lh , rh , Sh , Deph ) and e ~: ( In , r  ~ , Sn , Dep , ~)
Sh := sub-structure ( eh),
S,~:=SnU sub-strueture(e ~)
If neither of Sh and Sn is nil , s % bo:~fv(dep(D ) usub(fs(R )) , \[ head-dtrSh\]non-head-dtrSn'rs ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ( A ) for each resolution sequence , rd , sub := . sub0LIT1I ~? ? ? UTi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ( B )
If sub is not a feature structure or either of Sh or S~isnil  , then return nilelse return subFigure 4: A recursive procedure for tile Phase 2 
N " and D~D " , then , add the tuple ( s , t , s , ,~ , N , D , R ) to A . 
4 . If the new quasi-sign(O ) ( s , ~) was added to S in the previou step , lets dbes , ~ and go to
Step 2.
When this terminates , ( S , A , l ) is the LA for 1 . 
The major difference of Step 2 and the normal application of a rule schema is that non-head-dtr values are not specified in Step  2  . 
In spite of this underspecification , certain parts of the non-head-dtrare instantiated because they are token-identic M with certain values of the head-d % r domain  . By unifying non-head-dtr values with actual signs to be constructed fl'om input sentences  , a parser can obtain parsing results . 
For more intuitive explanation , see ( Torisawa and
Tsujii , 1996).
However , this simple LA generation algorithm has a termination problem  . There are two potential causes of non -termination  . The first is the generative capacity of a feature structure of a rule schema  , i . e . , a rule schema can generate infinite variety of signs  . The second is non-termination of the execution of DCP in Step  2 because of lack of concrete nonhead daughters . 
For the first case , consider a rule schema with the following feature structure  . 
head-dtr syn\[counter\[1\]\]\]
Then , this can generate an infinite sequence of signs , each of which contains a part , \[ counter < bar , ba , r ,   .   .   .   , bar ) l and is not equivalent to any previously generated sign  . In order to resolve this difficulty , we apply tim restriction ( Shieber , 1985) to a rule schemat and a lexical entry , and split the feature structure F = fs ( R ) of a rule schema R or a lexical entry F = l , into two , namely , core ( F ) and sub ( F ) such that F = core ( F ) U sub ( F )  . The definition of the restriction here is given as follows  . 
Definition 5 ( paths ) For arty node n in a feature structure F , paths ( n , F ) is a set of all the paths that reaches n from the root of F  . 
Definition 6 ( Restriction Schema ) A restriction schemars is a set of paths . 
Definition 7( Res ) F ' = Res(F , rs ) is a ma . ~ ; i-real feature structure such that each node n in F ~ satisfies the following conditions  . 
? The ~ is a node no inf : such that paths ( no , F ) = path . s(n , F ') and type (' n ) = t ? tpe(no) . 
? For any pC paths (' n , F ') , there is no path p , , 6 rs which prefixes p . 
Reseliminates the feature structure nodes which is specified by a restriction schema  . For a certMn given restriction schemars , eore(fs(l ~ , ))-=Res(fs(R ) , rs ) and sub ( fs ( R ) ) is a mini-mM feature structure such that core ( fs ( R )   ) U sub ( fs ( R ) ) = fs ( R )   . Tile nodes eliminated by Res must appear in sub ( fs ( R ) ) . Intile example , if we add ( syn , counter to a restriction schema and replace fs ( R ) with e or c ( fs (  . R )) in the Mgo-rithm for generating LAs , the termination prob-lenl does not occur because LAs can contain a loop and equiv Ment signs are reduced to one state in LAs  . The sub(fs(R )) contains the synl counter , and the value is treated at Phase 2 . 
The other problem , i . e . , termination of DCPs , often occurs because of underspecification f the nor k-head-dtr wines  . Consider the rule schema in Figure 1 . The append does not terminate at Phase 2 because the indices value of nonhead ( laughters is \[?\] . ( Consider the case of executing append ( X , ( b ) , Y ) in Prolog . ) We introduce the . freezeNnctor in Prolog which delays the evaluation of the second argument of the functors if the first arguruent is not instantiated  . For instance , freeze(X , append(X , \[ b \] , Z )   ) means to delay the ewfluation of append until X is instan-tinted  . We introduce the functor in the following for ln . 
goals arg2 ( flarg3\[~ freeze\]
This means the resolution of this query is not performed if  \[1\] is\[?\] . The delayed evaluation is considered later when tile non-head-dtr values are instantiated by an actual sign  . Note that this change does not affect the discussion on the correctness of our parsing method  , because the difference can be seen as only changes of order of unification  . 
Now , tile two phases of our parsing algorithm can be described in more detail  . 
Phase 1 : Enumerate possible parses or edges in a chart only with unifiability checking in a bottom-up chart-parsing like manner  . 

Phase 2: For comt ) leted parse trees , compute substructures by DFSs ,   , sub ( fs ( R ) ) for each schema R and frozen 1 ) C1 ) programs . 
Note that , in\['has(;1 , unification is replaced with nnifiability checking  , which is more efficient than unification in terlns of space an  ( l time . The intended side effect by unification , such as building up logical forms in sereva lues  , is CO mlntted at Phase 2 only for the parse trees covering the whole input . 
a.1 Phase 1 Parsing
The Phase ~ 1parsing algorithm is quite similar to a bottom-up chart parsing for CFG  . The Mgorithm has a chart and edges . 
Definition 8 ( edge ) An edge is at upla (1 , r , S , l ) ep ) where , ?1 and rarc . vertexes in the chart . 
? S is a slate of an LA.
?  . l)epi . sa . set of tuples in the form of ( D , eh , c , , , l l w h , e , rc . eha7%dCnaTY ; (: dges,\])i . saquasi- . sign(I ) and R is a rule . schema . 
The intuition behind this definition is , ??' l ) lays the role of a non-/termimd in CFG , though it is actually a quasi-sign(O ) . 
? ch an de , ~ denote a head daughter edge and a nonhead daughter edge  , respectively . 
? Dep represents the dependency of an edge and its daughter edges  . Where ( D , eh , c , ~ , l ~ E D c p , Disa DIeS of a transition arc . Basi (: all y , Phase 1 parsing creates these tuples , and \]) hase2 parsing uses them . 
The Phase 1 parsing ( : on sists of the folh ) wing steps . Assume that a word in input \] n~s a lexical entry L ~ and that an LA  ( Q , ; , A , ,q ~ ) generated fi'om Li is attached to the word : 1 . Create an edge li -= ( j . i , ji+1 , q  ~ , ( )) in the chart for each Li , for at ) propriate . ji . 
2 . For an edge e . 1 whose state is q ~ in the chart , pickut ) an edge e2 which is adjacent to el and whose state is q ~ . 
3 . For a transition arc ( ql , q , N , D , ll ) , check if
N is unifiable with q2.
4 . If the unifiability check is successful , find an edge ( l = (' m , d , 'n , d , q , Depd ) strictly covering el and e2 . 
5 . if there is , replaced with a new edge ( m , , , 'na , q , Dep , zU(D , c ,   , eu , B )) it ) the . 

6 . Otherwise , create a new edge ( Tn , n , q , ( D , el , e2 , R )) strictly covering el and e2 . 
7. Gotoste I ) 2.
4 Phase 2 Parsing
The algorithn l of Phase 2 parsing is given in Figure 4  . The procedure sub- . structure is a recursive 1 ) rocedure which takes an edge as input and builds Ul  ) substructures , which is dif-fer'ential feature structures representing modifications to core-structures  , in a bottoln-U1) n lanner . 
The obtained substructures are unified with core -structures when  1  ) the input edge covers a whole input or 2 ) the edge is a nonhead daughtered geofs on mother edge  . Note that the . ~ ub-struet ' are treats sub(fs(R )) , a feature structure eliminated l ) y the restriction in the generation of LAs ,   ( the ( A )  1 ) art in Figure 4 ) and frozen goals of DCPs , by addition a lew duation of DCPs . ( the ( B ) part ) Here , we use two techniques : ( ) tie is dependency analysis which is eml ) odied by the function dep in Figure 4 . The other is a partiM unification routine expressed by p_nnify in the figure  . 
The del ) endency analysis is represented with the function , dep(F , ' rs ) , where F is a DFS and rs is a restriction schema used in generation of 

Definition 9 ( dep ) For a feature structure \["' and the . restriction schemar . s,F = dep(lc ~, r , s ) is a maximal fc . at u ' re ~ structure such O ~ , at any ' node ' n in F sati , ~ fies the conjunction of th , e . following two conditions : t . There is a node n ' inf i '', such , that v(tm . +,  . , P ) -~),, m . , + ,, ', F')a, . Zt : , mc . (7, 0:=typc(n') . 
2 . Where A ) ha . = ' nor B ) n , t is a descend an ? of n , pa , ths(n , z , F ) contains a path . prefixed by one of ( head-dtr ) , ( non-head-dtr ) and < goa:ts > . 
3 . The diajune tion of the following three conditions is satisfied where A  ) n , t = norB ) ' n(t is a descendant of n . 
? For . some pGpa , th , s(7t  ~ l,F ), there i . sapath , p , , . E'rswh , ieh prefix esp . 
? Some p ~ p . , th,@n,t,F ) is prefixed by (~ m . ,ls) . 
? 7' here is no node ' n . in F . ~' uchth , at i ) there is paths P i , 7) ' 2 ~ paths (' n < ,  . , f ; ' ) such that Pi is prefixed by ( syn )  07'  ( sere ) aTtdP2 is'p'r'efi ; Le . d by ( head-dtr ) Or ( non-head-dtr > , and i /) for a ~ typG paths ( rid , F ) there is p , ~ E path . .s(n ,~, F ) which prefixes p . 
Roughly , depeliminates 1 ) the descendant nodes of the node which a pl ) ears both in syn/sem domains and head-dtr/non -head-dtr domains and  2  ) the nodes at ) peering only in syn/sem domains , excet ) t for the node whichel ) pears in s'ab ( fs ( \]? ) ) or goa ls domains . In other words , it removes the feature structures that have I ) een already raised to core-structures or other DFSs  , ex (: ept for the structure sharings , and leaves those which will be required by DCPs or xub  ( fs ( R )   )   . 
p_unify(F l , F . 2  , rs ) is a partial unification routine where Fl and F2 are feature structures , and rs is a restriction schema used in generation of LAs  . loughly , it performs unification of F , and l'12 only for common part of Ft , F . 2 , and it produces unified results only for the node ' ninFlifs ' nj is ~ t descendant of '  n2 in a feaiure structur ~ l , ' ill'nt#n2 , and the . r (' . ~ u:e paths Pl6 path , s(~ , , l , \[") ~ HldI)'2 Epa , th , s(n . 2, l "), nnd p2l)r (' . fixespl . 
953 phon " wrote " syn , or v,\] . . . .  . . . . subcat < NI'\[1\] , NP\[2\] ) reinwrotes ere content agent object indices 0
Figure 5: A lexical entry for " wrote " $2 ? A State
PbT:NT2: NS1 A Transition Arc
TI:NP ( N denotes
Lanon-head-dtr.)
Figure 6: The LA derived from " wrote " n has a counter part in F ~  . More precisely , it produces the unification results for a nod ( ; n in Fj such that ? there is a path p ~ paths ( n , I ~ ) such that the node reached by 1 ) is also defined in F2 , or ? there is a path p ~ paths ( n , F1) prefixed by some p , , C rs or ( goals) . 
Note that a node is unified if its structure -shared part has a counter-I  ) art in F2 . Intuitively , the routing produces unified results for the part of Fi instantiated by  /7'2  . The other part , that is not produced by p_unify , is not required at Phase 2 because it is already computed in a state or DFSs in LAs when the LAs are generated  . Then , a sign can be obtained by unifying a substructure and the corresponding core-structure  . 
5 Example
This section describes the parsing process of the sentence " My colleague wrote a good paper  . " The LA generated fronlthe lexical entry for " wrote " in Figure  5 is given in Figure 6  . The transition arc T1 between the states L and S1 is generated by the rule schema in Figure 1  . Note thai ; the query to DCP , freeze(\[1\] , append ( Ill ,  \[2\] ,  \[3\])) , is used to obtain union of indices values of daughters and the result is written to the indices values of the mother sign  . During the generation of the transition arc , since the first argument of the query is \[?\] , it is frozen . The core-structures arid the dependency-analyzed DFSs that augmen the LA are shown in Figure  7  . We assume that we do not use any restriction , i . e . , for any lexical entry l and rule schenaata 2~ , s , bb(1)~-\[?1 and sub(fs(I )) =\[ ?1 . 
Note that , in the DFSs , the already raised feature structures are eliminated and  , that the DFS of the transition arcT contains the frozen query as the goals  . 
Assmne that the noun phrases " My colleague " and '% good paper " are already recognized by a parser  . At phase 1 , they are checked if they are unifiable to the condition of transition arcs  T1 and T2  , i . e . , the NPs which are nonhead daughters $2 synsenl 5'1 synhead\[ . . . . . ior V \]\] subcat 0 \] rehlwrote content agent ? object ? indices ? head\[major V\]\]subcat  ( NP\[2\] ) reinwrotes enl content agent \] object indices A _ 
The dependency-an MyzedDFS of T2syn\[~ . . . . . l\[s\]\]seln indices\[a\]-synSign1\] . . . . \] h~ad-dtr \[ a \] s~beat . \[ qN ;'\[ r \])
Z\[tflllajor . . . . . synsubcttt ) non-head-dtr \[5\] sere indices goals argl arg2   arg2 
The dependency-analyzed DFS of T1 head 8\] synsubcat /10\]?\[9\] content \[6\] agents ere object indices \[3\] ? signs y 1   . . . . . I-dtr\[3\]9\]) . . . . . \[ic'::~itceet:t\[2t ( , \] syn\[tsubcat ) non-head-dtr\[5\]sere indices 1 . . . . . \]> freeze1 goals argla,-~2 () arg2


Figure 7: States and DFSs intimLA in Figure 6
The substructure for $2 content agen ~ 1\]my_colleaque    . . . . . \[ obje,:t\[2\]good_p . vJ4'J indices\[llmy_collea . o .   . . \[2\]good_paper)
The substructure for S1 sere\[object \[1\]good_paper \] indices ( \[1\] good_paper ) The goals , head-dtr , non-head-dtrv Mues are omitted . 
Figure 8: The substructures obtained in the parsing
Phase 1 only
Phe use 1 & Phase 2
Phase 1 & Phase 2 ~ f - ~ naive application of rules che , mata naive application of rules <: hentata ~\[' ylTe  . -6 fsent ; ences (  #of sentences ) glT70----<ufly successful ~7 ~ ) enD . s ~ s  ~
Ass ) on Ty~fuT---_(~)_.......

A v3n < e&1 . 25 ~ L 121_3 . 00 ~1 . 65) 85 . 09 1093 . 22 ~2 . 1 ~ A bracketed time indicates non- ( ~ ( \] execution tim ( <'\]' heXl ) erimeuts was l ) (nformed on SparcStat ion 20 with 128 MI ) IIAM Figure 9: Ext > eriments on a Japanese newsl ) aper ( Asahi Shinl ) un )  < ) f$1 and $2 . Since alll ; heu , dfial ) ility < : lwx:k-ings , '/ . 1 " o successful , Phase 1 parsing produ ( : es the parse tree whose form is presented in Figure  3  . 
The Phase 21 ) arsing produces the substructures in Figure 8 . Note that the frozen goals are evaluated and the indices w dues have al  ) prot ) riate values . Al ) arsing result is obtainel by unifying the substructure for  5"2 with tim correspon<ling core-structllre . 
The amount of the features tru < : ture nodes gen -erate  ( 1 during t ) arsing arer ( ~<lu ( :e ( 1:<m~t > are ( l to the case of the naive at ) l ) lication of rule schemata presented in Section 2 . The important point is that they cont Mn only either the partiuthe DFSs that was instantiated by head daughters ' substructures  , and nonhead daughters ' core-structures and substructures  , or the part that contributes to the DCP's exaluation  . The feature structure that does not al ) pear i , a substructure appears in the corresponding core -structure  . Se , e Figure 7 . Because of these 1) rot > erties , the correctness of our parsing nmthod is guaranteed  . (' lbri-sawa and Tsujii , 1996) . 
7 Conclusion
We have lu'e sented a two-phased t ) arsing n lethod t or HPSG . In the first l ) hase , , our 1 ) arser produces parse trees using LexicalEntry Automnt a compilc x l from lexical entries  , in the second phase , only the feature structures whi <: hluust\] ) e ( : ompute ( \[ dynamically are ( : omputed . As a re-suit , amount of the fl ; ature structures unifie < lat 1 ) arsing-time is reduce . d . We also showed the el'-feet of our opt infization te  ( : hniques by a series of exl ) erin wats < mare alworld text . 
\] t can l ) enoticed that ea <: h transition arc of tim cOral ) ileall , As can be seen as a rewriting rule in CFG ( or adott ; ednotation in a chart parser . ) We belie . vethis can Ol ) en the way to integrate severa Jn , et ; hods deveh)l > ed for CI , 'G , including the insideoutside algorithm to tgrmmnar learning or disambiguation  , into an HPSC , framework . We also 1) e-lieve that , by pursuing this direction for optimizing ttl ) SG parsers , we can reach the point whe . regrammar learning from corl ) or a can be done with concise , and linguistically well-defined (: or e grant-

6 ExI ) eriments
We have implenmnted our parsing metho<lin Common Lisp Ol  ) je <: tSysten ~ . hnproven mnt by our method has / ) een measured on 70 ra . n donfly selected Japanese sentences from a news l ) at ) er ( Asahi Shinbun )  . The used grammar (' , on sists of just 5 rule schemata , which are generated fl ' omprinciples and rewriting rules  , aim 55 default lexical entries given for each part of speech  , with 44 manually tailored lexical entries . The total number of states in the LAs compiled fl ' oln them was  1490  . The grammar does not have a semantic part . The results arc . l ) resented in Figure 9 . Our grammar produ <: edl > ossil ) le parse trees for 43 sen-ten <' . es (61 . 4%) . We compared the . execution time of our I ) arsing method and a more naive algorithm , which l ) erforms Phase 1 parsing with LAs and al ) -plys rules ( : hematato ( : olnph ' . ted pars < ; trees in the naive way described in Se <: tion 2 . As the . naive algorithm caused thrashing for storage in GC  , it is pointless to compare those tigures simply . However , it is obvious that our method is much fi~ster than the naive one  . We could not measure the execution time for a totally naive algorithm which t  ) uilds parse trees without LAs because of U wash-ing . 

Bob Carl > enter .  1992 . The Looi <: of "/\] qpedF . a,t . ., . +' . 
Str'ucturcs . Cambridge University Press.
Keiko Horiguchi , Kentaro Torisawa , and Jun'ichi Tsujii .  1995 . Automatic acquisition of cont ( ; nt words using an IIPSG . -based parser . In NL-1"1~S'95 . 
Robert Kasper , Bernd Kiefer , Klaus Netter , and K . Vijay-Shanker .  1995 . Compilation of IIPSG to TAG . In ACL95 . 
Carl Pollard and Ivan A . Sag .  1987 . h ~, fovmatio , ,- Based Syntaz and Semau , ties Vol .  1 . CSLIlecture notes 11 o . 1 3 . 
(,' arl Pollard and Ivan A . Sag .  1993 . lh . .a . d-Driv<' . n Phrase Structure Grammar . University of Chicago Pressan ( lCSLIl ) ul ) li<:ations . 
Stuart C . Shieber .  1985 . Using restri <: tion to extend I ) arsing algorithms for conq ) lex feature based formalisms . In ACL85 . 
Kentaro Torisawa and Jun'ichi Tsujii .  1996 . () if-lineraising , dei ) endency analysis anll > artial unifie at ; ion . In Third Iu , ternational Conference on HPSG . In the pr <) ceedings of TALN'96 . 

