An Experiment On Incremental Analysis
Using Robust Parsing Techniques
Kilian Foth and Wolfgang Menzel and Horia F . Pop and IngoSchr 6der
foth Imenzel Ihf pop Is chroeder@nats . infonnatik . mfi-hamburg . de
Fachbereich Informatik , Universitgt Hamburg Vogt-K 611 n-Strage 30 , 22527 Hamburg , Germany
Abstract
The results of an experiment are presented in which an approach for robust parsing has been applied incrementally  . They confirm that due to the robust nature of the underlying technology an arbitrary prefix of a sentence can be analysed into an intermediate structural description which is able to direct the further analysis with a high degree of reliability  . 
Most notably , this result can be achieved without adapting the gramrnar or the parsing algorithms to the case of increinental processing  . The resulting incremental parsing procedure is significantly faster if compared to a nonincremental bestfirst search  . 
Additionally it turns out that longer sentences benefit most from this acceleration  . 
1 Introduction
Natural language utterances usually unfold over time  , i . e . , both listening and reading are carried out in an incremental left-to-right manner  . Modeling a similar type of behaviour in computer -based solutions is a challenging aim particularly interesting task for a number of quite different reasons that are most relevant in the context of spoken language systems : ? Without any external signals about the end of an utterance  , incremental analysis is the only means to segment the incoming stream of speech input  . 
? An incremental analysis mode provides for a lnore natural  ( mixed-initiative ) dialogue behaviour because partial results are already available well before the end of an utterance  . 
? Parsing may already take place in concurrency to sentence production  . Therefore the speaking time becomes available as computing time  . 
? Dynmnic expectations about the upcoming parts of the utterance might be derived right in time to provide guiding hints for other processing components  , e . g . , predictions about likely word forms for a speech recognizer  ( Hauenstein and Weber ,  1994) . 
In principle , two alternative strategies can be pursued when designing all incremental parsing procedure :  1  . Tokeel ) open all necessary structural hypotheses required to accomodate every possible continuation of an utterance  . This is tile strategy usually adopted in an incremental chart parser  ( WirSn ,  1992) . 
2 . To commit to one or a limited number of interpretations where  ( a ) either this commitment is made rather early and a mechanisnl for partial reanalysis is provided  ( Lombardo ,  1992 ) or ( b ) it is delayed until sufficient information is eventually available to take an ultimate decision  ( Marcus ,  1987) . 
The apparent efficiency of human language understanding is usually attributed to a nearly commitment strategy  . 
Our approach , in fact , represents an attempto combine these two strategies : On the one hand  , it keeps many of the available building blocks for the initial part of an utterance and passes an  ( ' updated ) search space to the following processing step . Additionally , the optimal structural description for the data already available is determined  . This not only makes an actual interpretation ( together with expectations for possible continuations  ) available to subsequent processing components , but also opens up the possibility to use this information to effectively constrain the set of new structural hypotheses  . 
Determining the optimal interpretation for a yet incomplete sentence  , however , requires a parsing approach robust enough to analyse an arbitrary sen-tencc prefix into a meaningful structure  . Therefore two closely related questions need to be raised :  1  . Can the necessary degree of robustness be achieved ?  2  . Is the information contained in the currently optimal structure useful to guide the subsequent analysis ? tential for search space reductions against a  1  ) ossible loss of accuracy , a series of experiments has been conducted . Sections 2 and 3 introduce and motivate the framework of the exl ) eriments . Section 4 describes a number of heuristics for reuse of previous solutions and Section  5   1  ) resents the results . 
2 Robust Parsing in a Dependency

Our grammar models utterances as depert de Tz cyt ryes  , which consist of pairs of words so that one depends directly on the other  . This subordination relation can be qualified by a label  ( e . g . to distinguish conq ) lements fl : ommodifiers ) . Since each word cml only depend on one other word  , a labeled tree is formed , usually with the finite verb as its root . 
The decision on which stru ( : ture to 1 ) ostulate for an utterance is guided 1 ) y explicit constrai ~ , ts , which are rel ) resented asmfiversally quantified logical formulas about features of word tbrms and l  ) artial trees . 
t ~ r instance , one constraint might t ) ostulate that a relation labeled as ' Subject ' can only occur between a noun and a finite verb to its right  , or that two different del ) endencies of the same verb may not both be labeled ' Sul  ) ject ' . For efficiency reasons , these formulas may constrain individual del ) endency edges or pairs of edges only . The al ) plication of constraints Call I ) eg in assoou as the first word of an utterance is read  ; no global information about the utterance is required for analysis of its beginning  . 
Since natm : allanguage in l ) ut will often exhibitir regularities such as restarts  , repairs , hesitations and other gr~tmmatical errors , individual errors should not make further analysis impossible  . Instead , a robust 1 ) arser should continue to build a structure t br the utterance  . Ideally , this structure should be close to that of a similar  , but grammatical utterance . 
This goal is attained by annotating the constraints that constitute the grammar with scores ranging from  0 to 1  . A structure that violates one or more constraints i annotated with the product of the col responding scores  , and the structure with the highest combined score is defined as the solution of the pars-lug problem  . In general , the higher the scores of the constraints , the more irregular constructions can 1 ) e analysed . Parsing an utterance with mmotated or soft constraints thus m nounts to multidimensional optimization  . Both complete and heuristic search methods can be eml  ) loyed to solve such at ) roblem . 
Our robust al ) proach also provides m ~ easy way to implement partial parsing  . If necessary , e . g . , an isolated noun labeled as ' Subject ' may form the root of a del  ) endency tree , although this would violate the first constraint lnentioned above  . If a finite verl ) is available , however , subordinating the noun under the verb will avoid the error and thus produce a better structure  . This capability is crucial for the analysis of incomplete utterances  . 
Different l cv cls of analysis can be defined to model syntactic as well as semantic structures  . A depel > dency tree is constructed for each of these levels  . 
Since constraints can relate the edges in parallel dependency trees to each other  , having several trees contributes to the robustness of the approach  . Altogether , the gramlnar used in the experiments described comprises  \]2 levels of analysis and 490 constraints ( SchrSderel , al . , 2000) . 
3 Prefix Parsing with Weighted
Constraints
In general , dependency analysis is well-suited for incremental analysis  . Since subordinations always concern two words rather than full constituents  , each word can be integrated into the analysis as so on as it is read  , although not necessarily in the ol ) timal way . 
Also , the 1 ) re-colnl ) uted dependency links can easily 1 ) ere used in subsequentierations . Therefore , de-1 ) endency grammar allows a finegrained incremental analysis  ( boxn bardo ,  1992) . 
mrc / "\ , r2"-modo"?\o?daal ; lassensieuns doch
Ihenlatyouusz " c1., . Ij " ~0 mod ///.
rlocheinen Ten'nil ; < pa~t > yet a meeting ( a ) -\c"~mod\\~o00/~\' , danal as sensie uns doch
Then let youus < part >
Let's appoint yet another meeting then.
O"cJ -": iIB " , , , o a ~~ i ( b ) o ~ ~ no cheinoll Terrainaus machenye la meeting appoint l Pigure  1: An example for a prefix analysis When assigning a det  ) endency structure to incomplete utterances , the problem arises how to analyse words whose governors or complements still lie beyond the time horizon  . Two distinct alternatives are 1) os sible : 1 . The parser can establish a dependency between tilt word and a special node representing aim -tative word that is assmned to follow in the remaining input  . This explicitly models the expectations that would be raised by the prefix  . 
l lowever , unifying new words with these underspecified nodes is difficult  , particularly when many constraints cannot be meaningfully applied to words with unknown features  . 
2 . An incomplete prefix can be analyzed directly if a grammar is robust enough to allow partial parsing as discussed in the previous section : If the constraint that forbids multiple trees receives a severe but nonzero penalty  , missing governors or complements are acceptable as long as no better structure is possible  . 
Experiments in prefix parsing using a dependency grammar of German have shown that even complex utterances with nested subclauses can be analysed in the second way  . Figure la provides an example of this : Because the infinitive verb ' ausmachen ' is not yet visible  , its coinplement ~ Terinin ' is analysed as an isolated subtree  , and the main verb'lassen'is lacking a comI ) lement . After the missing verb has been read , two additional dependency edges suffice to build the correct structure from the partial parse  . 
This method allows direct comparison between incremental and nonincremental parser runs  , since both methods use tim same grammar . Therefore , we will follow upon the second alternative only and construct extended structures guided by the structures of prefixes  , without explicitly modeling missing words . 
4 Re-Use of Partial Results
While a prefix analysis can produce partial parses and diagnoses  , so farthis inforlnation has not been used in subsequentierations  . In fact , after a new word has been read , another search is conducted on all words already available  . To reduce this duplication of work , we wish to narrow down the problem space for these words  . Therefore , at each iteration , the set of hypotheses has to be updated : ? By deciding which old dependency hypotheses should be kept  . 
? By deciding which new dependency hypotheses should be added to the search space in order to accomodate the incoming word  . 
For that purpose , several heuristics have been devised , based on the following principles : Predict ion strength  . Restrict the search space as much as possible , while maintaining correctness . 
Economy . Keep as nmch of the previous structure as possible  . 
Rightmost attachment . Attach the incoming word to the most recent words . 
The heuristics are presented herein increasing order of the size of the problem space they produce : A  . Keep all dependency edges from the previous optimal solution  . Add all dependency edges where the incoming word modifies  , or is modified by , another word . 
B . As A , but also keep all links that differ h'om the previous optimal solution only in their lexical readings  . 
C . As B , but also keep all links that differ fl ' om the previous optimal solution only in the subordination of its last word  . 
D . As C , but also keep all links that differ from the previous optimal solution only in the subordination of all the words lying on the path h'om the last word to the root of the solution tree  . 
E . As D , but for all trees in the previous solution . 
5 Results
In order to evaluate the potential of the heuristics described above  , we have conducted a series of experiments using a grammar that was designed for nonincremental  , robust parsing . We tested the in-crelnental against a nonincremental parser using  222 utterances taken from the VERBMOBIL domain ( Wahlster ,  1993) . 
V:\]-o-.\[=\]1r;
ABCDE

Figure 2: Solution quality and processing time for different heuristics Figure  2 compares the five heuristics with respect totile following criteria :  1 Accuracy . The accuracy ( graybar ) describes how many edges of the solutions are correct  . 
correct edges accuracy = ~ edges found tNote that the heuristics provide at most one solution and may fail to find any solution  . 

Weak recall . We base our recall measure-given as the black bar-on the number of solutions found non -incrementally  ( which is less than 100% ) because we want focus on tile impact of our heuristics  , not the coverage of tile grammar . 
weak recall =@ correct edges@edges fouud non -incrementally Relative run-time  . The runtime required by the incremental procedure as a percentage of the time required by the non -iucremental search algorithm is given as the white bar  . 
The difference between tile gray and the black bar is due to errors of the heuristic method  , i . e . , either because of its incal ) ability to find the correct subordination or due to excessive resource demands  ( which lead to process abortion )  . 
2N voobservations can be made : First , all buttt , elast heuristics needless time than the nonincremental algorithm to complete while maintaining a relative high degree of quality  . Second , the more elaborate the heuristics are , the longer they need to run ( as expected ) and the better are the results for the accuracy measure  . However , the lmuris-tics D and E could not complete to parse all sentences because in some cases a predefined time limit was exceeded  ; this leads to the observed decrease in weak recall when compared to heuristics C  . As expected , a tradeoff between computing time and quality can be found  . Overall , heuristics C seems to be a good choice because it achieves all accuracy of up to  93  . 7% in only one fifth of the runtime . 
250%:200%t 50% ~00% 50%

Figure heuristics ~ Time for incremental compared to nonincremental method 
DAb solute time for incremental (16 . . . 20 set to 100% ) ? The graybar present stile normalized time with the time for sentence length between  16 and 20 set to 100%  . 
Tile results show that the speedup observed in Figure  2 is not evenly distributed . While the incremental analysis of the short sentences takes longer  ( 2 . 5 times slower ) than the nonincrement algo-rithm , the opposite is true for longer sentences ( 10 times faster )  . However , this is welcome behavior : The incremental procedure takes longer only in those cases that are solved very fast anyway  ; tim problematic cases are parsed more quickly . This behavior is a first hint that the incremental nalys is with reuse of partial results is a step that alleviates the combinatorial explosion of resource demands  . 
- ? ~ ~ m ~ cd100% " ci , , 80%.

O % 1 .   .   . 5 6  .   .   . 10 11  .   .   . 15 16  .   .   . 20 overall
Sentence Length 3: Processing time vs . sentence length 3 coral ) ares the time requirements of
C for different sentence lengths.
? The relative runtime ( as in Figure 2 ) is given as the wl fite bar . 
0%, 1...56 ...1011 ...1516 ...20 over all
Sentence Length
Figure 4: Accuracy vs . sentence length ( colors have tile same meaning as ill Figure 2 ) Finally , Figure 4 compares the quality resulting from heuristics C for different sentence lengths  . It turns out that , although a slight decrease is observable , the accuracy is relatively independent of sentence length  . 
II ? ~ r-q~~I ~ . 1 I~~6 Conclusions coc , ~m 6 ~ Anapl ) roach to the incremental parsing of natural language utterances has been presented  , which is based on tlle idea to use robust parsing techniques to deal with incomplete sentences  . It determines a structural description for arbitrary sentence prefixes by searching for the optimal combination of local hypotheses  . This search is conducted in a problem space which is repeatedly narrowed down according to the optimal solution found in tile preceding step of analysis  . 

The results available so far confirm the initial expectation that the grammar used is robust enough to reliably carry out such a prefix analysis  , although it has originally been developed for the nonincremental case  . The optimal structure as determined by the parser obviously contains relevant information about the sentence prefix  , so that even very simple and cheap heuristics can achieve a considerable level of accuracy  . Therefore , large parts of the search space can be excluded fi'oln repeated reanalysis  , which eventually makes it even faster than its nonincremental counterpart  . Most importantly , the observed speedup grows with the length of the utterance  . 
On the other hand , none of the used structure-based heuristics produces a significantiml  ) rovement of quality even if a large amount of computational resources is spent  . Quite a number of cases can be identified where even the most expensive of our heuristics is not strong enough  , e . g . , the German sentence with a topicalized irect object : 
Die NOM , A Ce Frausie ht der NOM Mama.
The woman see stile man .
The woman , the man sees.
Here , when analysing the subsentence die Frausie ht , the parser will wrongly consider die Frau as the subject  , because it appears to have the right case and there is a clear preference to do so  . Later , when the next word comes in , there is no way to allow for dicFrauto change its structural interpretation  , because this is not licensed by any of the given heuristics  . 
Therefore , substantially more i ) roblenl-oriellted heuristics are required , which should take into account not only the ol ) timal structure , but also the conflicts caused by it . Using a weak but cheap heuristics , a fast al ) proximation of the optimal structure can be obtained within a very restricted search space  , and then refined by subsequent structural transformations  ( Foth et al ,  2000) . To a certain degree this resembles the idea of applying reason maintenance tchniques for conflict resolution in incremental parsing  ( Wir6n ,  1990) . In deciding which strategy is good enough to find the necessary first approximation the results of this paper might play a crucial role  , since the I ) ossible contribution of individual heuristics in such all extended fi'a mework can be precisely estimated  . 

This research as been partly fimded by the German Research Foundation " Deutsche Forschungsgemeinschaft " under grant no  . Me 147 2/1-2 . 

Kilian Foth , Wolfgang Menzel , and Ingo Schr Sder.
2000 . A transformation based parsing technique with anytime property  . In Procecdings of the International Workshop on Parsing Technologies  ( IWPT-2000 )  , pages 89-100 , Trento , Italy . 
Andreas Hauenstein and Hans Weber .  1994 . An investigation of tightly coupled time synchronous speech language interfaces using a unification grammar  . In Proceedings of the i2th National Conference on Artificial Intelligence : Workshop on the Integration of Natural Language and Speech Processing  , pages 42-49 , Seattle , Washington . 
Johannes Heinecke , J/irgen Kunze , Wolfgang Menzel , and Ingo Schr Sder .  1998 . Eliminative parsing with graded constraints . In Proceedings of the Joint Conference COLING/ACL-98  , Mon-trial , Canada . 
Vincenzo Lombardo .  1992 . Incremental dependency parsing . In P~vceedings of the Annual Meeting of the ACL , Delaware , Newark , USA . 
Mitchell P . Marcus .  1987 . Deterministic parsing and description theory . In Peter Whitclock , Mary McGee Wood , Harold Seiners , Rod Johnson , and Paul Bennett , editors , Linguistic Theory and Computer Applications ' , pages 69-112 . Academic Press , London , England . 
Wolfgang Menzelandingo Schr Sder .  1998 . Decision procedures for dependency parsing using graded constraints  . In Sylvain Kahane and Alain Pol-gu~re , editors , Proceedings of the Joint Conference COLING/ACL-98 Workshop : Processing of Dependency-based Grammars  , pages 78-87 , Mon-tr Sal , Canada . 
Ingo Schr Sder , Wolfgang Menzel , Kilian Foth , and Michael Schulz .  2000 . Modeling dependency grammar with restricted constraints  . International Journal J ?' a item cnt Automatique des Langues : Grammaires de d@endance  ,  41(1) . 
Wolfgang Wahlster .  1993 . Verbmobih Translation of face-to-fawogs . In Proceedings of the 3rd European Conference on @ eech Communication and Technology  , pages 2938 , Berlin , Germany . 
It ans Weber .  1995 . LR-inkrementelles proba-bilistisches Chart parsing von Worthypo these n-graphenmit Unifikations grammatiken : Eineenge Kopplung von Sucheund Analyse  . Verbmobil-Report 52 , Universit At Erlangen-Niirnberg . 
Mats Wir6n .  1992 . Studies in Incremental Natural Language Analysis . Ph . D . tlms is , Department of Computer and Information Science , Link Sping
University , Link Sping , Sweden.
Mats Wir Sn .  1990 . Incremental parsing and reason maintenance . In Proceedings of the i3th International Conference on Computational Linguistics  ( COLING90 )  , pages 287-292 , Helsinki , Finland . 

