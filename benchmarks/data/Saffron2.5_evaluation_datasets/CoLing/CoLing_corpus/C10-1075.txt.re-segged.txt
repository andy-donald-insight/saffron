Proceedings of the 23rd International Conference on Computational Linguistics ( Coling 2010), pages 662?670,
Beijing , August 2010
Adaptive Development Data Selection for Loglinear Model
in Statistical Machine Translation
Mu Li
Microsoft Research Asia
muli@microsoft.com
Yinggong Zhao?
Nanjing University
zhaoyg@nlp.nju.edu.cn
Dongdong Zhang
Microsoft Research Asia
dozhang@microsoft.com
Ming Zhou
Microsoft Research Asia
mingzhou@microsoft.com
Abstract
This paper addresses the problem of dy-
namic model parameter selection for log-
linear model based statistical machine
translation ( SMT ) systems . In this work,
we propose a principled method for this
task by transforming it to a test data de-
pendent development set selection prob-
lem . We present two algorithms for au-
tomatic development set construction , and
evaluated our method on several NIST
data sets for the ChineseEnglish trans-
lation task . Experimental results show
that our method can effectively adapt
loglinear model parameters to different test data , and consistently achieves good translation performance compared with conventional methods that use a fixed model parameter setting across different data sets.
1 Introduction
In recent years , loglinear model ( Och and Ney , 2002) has been a mainstream method to formulate statistical models for machine translation . Using this formulation , various kinds of relevant properties and data statistics used in the translation process , either on the monolingual-side or on the bilingual-side , are encoded and used as real-valued feature functions , thus it provides an effective mathematical framework to accommodate a large variety of SMT formalisms with different computational linguistic motivations.
?This work was done while the author was visiting Microsoft Research Asia.
Formally , in a loglinear SMT model , given a source sentence f , we are to find a translation e ? with largest posterior probability among all possible translations : e ? = argmax e
Pr(e|f ) and the posterior probability distribution Pr(e|f ) is directly approximated by a loglinear formulation:
Pr(e|f ) = p?(e|f ) = exp ( ? M m=1 ? mhm(e , f ))? e ? exp ( ? M m=1 ? mhm(e ?, f )) (1) in which hm?s are feature functions and ? = (?1, . . . , ? M ) are model parameters ( feature weights).
For a successful practical loglinear SMT model , it is usually a combined result of the several efforts : ? Construction of well-motivated SMT models ? Accurate estimation of feature functions ? Appropriate scaling of loglinear model features ( feature weight tuning).
In this paper , we focus on the last mentioned issue ? parameter tuning for loglinear model.
In general , loglinear model parameters are optimized on a heldout development data set . Using this method , similarly to many machine learning tasks , the model parameters are solely tuned based on the development data , and the optimality of obtained model on unseen test data relies on the assumption that both development and test data observe identical probabilistic distribution , goal of this paper is to investigate novel methods for test data dependent model parameter selection . We begin with discussing the principle of parameter learning for loglinear SMT models , and explain the rationale of task transformation from parameter selection to development data selection . We describe two algorithms for automatic development set construction , and evaluated our method on several NIST MT evaluation data sets.
Experimental results show that our method can effectively adapt loglinear model parameters to different test data and achieves consistent good translation performance compared with conventional methods that use a group of fixed model parameters across different data sets.
2 Model Learning for SMT with
Loglinear Models
Model learning refers to the task to estimate a group of suitable loglinear model parameters ? = (?1, . . . , ? M ) for use in Equation 1, which is often formulated as an optimization problem that finds the parameters maximizing certain goodness of the translations generated by the learnt model on a development corpus D . The goodness can be measured with either the translations ? likelihood or specific machine translation evaluation metrics such as TER or BLEU.
More specifically , let e ? be the most probable translation of D with respect to model parameters ?, and E(e ?,?, D ) be a score function indicating the goodness of translation e ?, then a parameter estimation algorithm will try to find the ? which satisfies : ?? = argmax ?
E(e ?,?, D ) (2)
Note when the goodness scoring function E (?) is specified , the parameter learning criterion in Equation 2 indicates that the derivation of model parameters ?? only depends on development data D , and does not require any knowledge of test data T . The underlying rationale for this rule is that if the test data T observes the same distribution as D , ?? will be optimal for both of them.
On the other side , however , when there are mismatches between development and test data , the translation performance on test data will be suboptimal , which is very common for realworld data . Due to the difference between data sets , generally there is no such ?? that is optimal for multiple data sets at the same time . Table 1 shows some empirical evidences when two data sets are mutually used as development and test data . In this setting , we used a hierarchical phrase based decoder and 2 years ? evaluation data of NIST Chinese-to-English machine translation task ( for the year 2008 only the newswire subset was used because we want to limit both data sets within the same domain to show that data mismatch also exists even if there is no domain difference ), and report results using BLEU scores . Model parameters were tuned using the MERT algorithm ( Och , 2003) optimized for BLEU metric.
Dev data MT05 MT08nw
MT05 0.402 0.306
MT08nw 0.372 0.343
Table 1: Translation performance of cross devel-opment/test on two NIST evaluation data sets.
In our work , we present a solution to this problem by using test data dependent model parameters for test data translation . As discussed above , since model parameters are solely determined by development dataD , selection of loglinear model parameters is basically equivalent to selecting a set of development data D.
However , automatic development data selection in current SMT research remains a relatively open issue . Manual selection based on human experience and observation is still a common practice.
3 Adaptive Model Parameter Selection
An important heuristic behind manual development data selection is to use the dataset which is as similar to test set as possible in order to work around the data mismatch problem to maximal extent . There are also empirical evidences supporting this heuristics . For instance , it is generally perceived that data set MT03 is more similar to MT05, while MT06-nw is closer to MT08nw.
Table 2 shows experimental results using model parameters induced from MT03 and MT06-nw as ble 1. As expected , MT06-nw is far more suitable than MT03 as the development data for MT08nw ; yet for test set MT05, the situation is just the opposite.
Dev data MT05 MT08nw
MT03 0.397 0.306
MT06-nw 0.381 0.337
Table 2: Translation performance on different test sets of using different development sets.
In this work , this heuristic is further exploited for automatic development data selection when there is no prior knowledge of the test data available . In the following discussion , we assume the availability of a set of candidate source sentences together with translation references that are qualified for the loglinear model parameter learning task . Let DF be the full candidate set , given a test set T , the task of selecting a set of development data which can optimize the translation quality on T can be transformed to searching for a suitable subset of DF which is most similar to T :
D ? = argmax
D?DF
Sim(D,T )
To achieve this goal , we need to address the following key issues : ? How to define and compute Sim(D,T ), the similarity between different data sets ; ? How to extract development data sets from a full candidate set for unseen test data.
3.1 Dataset Similarity
Computing document similarity is a classical task in many research areas such as information retrieval and document classification . However , typical methods for computing document similarity may not be suitable for our purpose . The reasons are twofold : 1. The sizes of both development and test data are small in usual circumstances , and using similarity measures such as cosine or dice coefficient based on term vectors will suffer from severe data sparseness problems . As a result , the obtained similarity measure will not be statistically reliable.
2. More importantly , what we care about here is not the surface string similarity . Instead , we need a method to measure how similar two data sets are from the view of a loglinear
SMT model.
Next we start with discussing the similarity between sentences . Given a source sentence f , we denote its possible translation space with H(f ). In a loglinear SMT model , every translation e ? H(f ) is essentially a feature vector h(e ) = ( h1, . . . , hM ). Accordingly , the similarity between two sentences f1 and f2 should be defined on the feature space of the model in use . Let V ( f ) = { h(e ) : e ? H(f )} be the set of feature vectors for all translations inH(f ), we have
Sim(f1, f2) = Sim (
V ( f1),V ( f2) ) (3)
Because it is not practical to compute Equation 3 directly by enumerating all translations inH(f1) and H(f2) due to the huge search space in SMT tasks , we need to resort to some approximations.
A viable solution to this is that if we can use a single feature vector h?(f ) to represent V ( f ), then Equation 3 can be simply computed using existing vector similarity measures.
One reasonable method to derive h?(f ) is to use a feature vector based on the average principle ? each dimension of the vector is set to the expectation of its corresponding feature value over all translations : h?(f ) = ? e?H(f)
P ( e|f)h(e ) (4)
An alternative and much simpler way to compute h?(f ) is to employ the max principle in which we just use the feature vector of the best translation inH(f ): h?(f ) = h(e ?) (5) where e ? = argmaxe P ( e|f).
Note that in both Equation 4 and Equation 5 we make use of e?s posterior probability P ( e|f).
664
Since the true distribution is unknown , a pre-learnt modelM has to be used to assign approximate probabilities to translations , which indicates that the obtained similarity depends on a specific model . As a convention , we use SimM(f1, f2) to denote the similarity between f1 and f2 based on M , and callM the reference model of the computed similarity . To avoid unexpected bias caused by a single reference model , multiple reference models can be simultaneously used , and the similarity is defined to be the maximum of all model-dependent similarity values:
Sim(f1, f2) = maxM SimM(f1, f2) (6) where M belongs to { M1, . . . , Mn }, which is the set of reference models under consideration.
To generalize this method to data set level , we compute the vector h?(S ) for a data set S = ( f1, . . . , f|S |) as follows : h?(S ) = | S |? i=1 h?(fi ) (7) 3.2 Development Sets Pre-construction In the following , we sketch a method for automatically building a set of development data based on the full candidate set DF before seeing any test data.
Theoretically , a subset of DF containing randomly sampled sentences from DF will not meet our requirement well because it is very probable that it will observe a distribution similar to DF .
What we expect is that the prebuilt development sets can approximate as many as possible typical data distributions that can be estimated from subsets of DF . Our solution is based on the assumption that DF can be depicted by some mixture models , hence we can use classical clustering methods such as kmeans to partition DF into subsets with different distributions.
Let SF be the set of extracted development data from DF . The construction of SDF proceeds as following : 1. Train a loglinear model MF using DF as development data ; 2. Compute a feature vector h?(d)1 for each sentence d ? DF usingMF as reference model ; 3. Cluster sentences in DF using h?(d)/|d | as feature vectors ; 4. Add obtained sentence clusters to SDF as candidate development sets.
In the third step , since the feature vector h?(d ) is defined at sentence level , it is averaged by the number of words in d so that it is irrelevant to the length of a sentence . Considering the outputs of unsupervised data clustering methods are usually sensitive to initial conditions , we include in SDF sentence clusters based on different initialization configurations to remove related random effects.
An initialization configuration for sentence clustering in our work includes starting point for each cluster and total number of clusters . In fact , the inclusion of more sentence clusters increases the diversity of the resulted SDF as well.
At decoding time , when a test set T is presented , we compute the similarity between T and each development set D ? SDF , and choose the one with largest similarity score as the development set for T :
D ? = argmax
D?SDF
Sim(T,D ) (8)
When a single reference model is used to compute Sim(T,D ), MF is a natural choice . In the multimodel setting as shown in Equation 6, models learnt from the development sets in SDF can serve this purpose.
Note in this method model learning is not required for every new test set because the model parameters for each development set in SDF can also be pre-learnt and ready to be used for decoding.
3.3 Dynamic Development Set Construction In the previous method , test data T is only involved in the process of choosing a development set from a list of candidates but not in process of development set construction . Next we present a 1Throughout this paper , a development sentence d generally refers to the source part of it if there is no extra explanation.
665 method for building a development set on demand based on test data T .
Let DF = ( d1, . . . , dn ) be the data set containing all candidate sentences for development data selection . The method is iterative process in which development data and learnt model are alternatively updated . Detailed steps are illustrated as follows : 1. Let i = 0, D0 = DF ; 2. Train a modelMi based on Di ; 3. For each dk ? DF , compute the similarity score SimMi(T , dk ) between T and dk based on modelMi ; 4. Select top n candidate sentences with highest similarity scores from DF to form Di+1; 5. Repeat step 2 to step 4 until the similarity between T and latest selected development data converges ( the increase in similarity measure is less than a specified threshold compared to last round ) or the specified iteration limit is reached.
In step 4, Di+1 is greedily extracted from DF , and there is no guarantee that SimMi(T,Di+1) will increase or decrease after a new sentence is added to Di+1. Thereby the number of selected sentences n needs to be empirically determined.
If n is too small , neither the selected data nor the learnt model parameters will be statistically reliable ; while if n is too large , we may have to include some sentences that are not suitable for test data in the development data , and miss the opportunity to extract the most desirable development set.
One drawback of this method is the relatively high computational cost because it requires multiple parameter training passes when any test set is presented to the system for translation.
4 Experiments 4.1 Data
Experiments were conducted on the data sets used for NIST ChineseEnglish machine translation evaluation tasks . MT03 and MT06 data sets , which contain 919 and 1,664 sentences respectively , were used for development data in various settings . MT04, MT05 and MT08 data sets were used for test purpose . In some settings , we also used a test set MT0x , which containing 1,000 sentences randomly sampled from the above 3 data sets . All the translation performance results were measured in terms of case-insensitive BLEU scores.
For all experiments , all parallel corpora available to the constrained track of NIST 2008 ChineseEnglish MT evaluation task were used for translation model training , which consist of around 5.1M bilingual sentence pairs . GIZA ++ was used for word alignment in both directions , which was further refined with the intersec-diag-grow heuristics.
We used a 5gram language model which was trained from the Xinhua portion of English Gigaword corpus version 3.0 from LDC and the English part of parallel corpora.
4.2 Machine Translation System
We used an inhouse implementation of the hierarchical phrasebased decoder as described in Chiang (2005). In addtion to the standard features used in Chiang (2005), we also used a lexicon feature indicating how many word paris in the translation found in a conventional ChineseEnglish lexicon . Phrasal rules were extracted from all the parallel data , but hierarchical rules were only extracted from the FBIS part of the parallel data which contains around 128,000 sentence pairs.
For all the development data , feature weights of the decoder were tuned using the MERT algorithm ( Och , 2003).
4.3 Results of Development Data
Pre-construction
In the following we first present some overall results using the method of development data pre-construction , then dive into more detailed settings of the experiments.
Table 3 shows the results using 3 different data sets for loglinear model parameter tuning . Elements in the first column indicate the data sets used for parameter tuning , and other columns contain evaluation results on different test sets . In the MT03 0.399 / 0.392 0.395 / 0.390 0.241 / 0.258 0.319 / 0.322 MT06 0.381 / 0.388 0.382 / 0.391 0.275 / 0.283 0.343 / 0.342 MT03+MT06 0.391 / 0.401 0.392 / 0.397 0.265 / 0.281 0.336 / 0.345
Oracle cluster 0.401 0.398 0.293 0.345
Self-training 0.406 0.402 0.298 0.351
Table 3: Translation performance using different methods and data sets for parameter tuning.
third row of the table , MT03+MT06 means combining the data sets of MT03 and MT06 together to form a larger tuning set . The first number in each cell denotes the BLEU score using the tuning set as standard development setD , and the second for using the tuning set as a candidate set DF .
For all experiment settings in the table , we used cosine value between feature vectors to measure similarity between data sets , and feature vectors were computed according to Equation 5 and Equation 7 using a reference model which is trained on the corresponding candidate set DF as development set.2 We adopted the kmeans algorithm for data clustering with the number of clusters iterating from 2 to 5. In each iteration , we ran 4 passes of clustering using different initial values.
Therefore , in total there are 56 sentence clusters generated in each SDF .3 From the table it can be seen that given the same set of sentences ( MT03, MT06 and MT03+MT06), when they are used as the candidate set DF for the development set pre-construction method , the translation performance is generally better than when they are just used as development sets as a whole . Using MT03 data set as DF is an exception : there is slight performance drop on test sets MT04 and MT05, but it also helps reduce the performance see-saw problem on different test sets as shown in Table 1.
Meanwhile , in the other two settings of DF , we observed significant BLEU score increase on all test sets but MT0x ( on which the performance almost kept unchanged ). In addition , the fact that using MT03+MT06 as DF achieves best ( or al-2For example , in all the experiments in the row of MT03 as DF , we use the same reference model trained with MT03 as development set.
3Sometimes some clusters are empty or contain too few sentences , so the actual number may be smaller.
most best ) performance on all test sets implies that it should be a better choice to include as diverse data as possible in DF .
We also appended two oracle BLEU numbers for each test set in Table 3 for reference . One is denoted with oracle cluster , which is the highest possible BLEU that can be achieved on the test set when the development set must be chosen from the sentence clusters in SMT03+MT06.
The other is labeled as self-training , which is the BLEU score that can be obtained when the test data itself is used as development data . This number can serve as actual performance upper bound on the test set.
Next we investigated the impact of using different ways to compute feature vectors presented in Section 3.1. We reran some previous experiments on test sets MT04, MT05 and MT08 using MT03+MT06 as DF . Most settings were kept unchanged except that the feature vector of each sentence was computed according to Equation 4.
A 20-best translation list was used to approximate H(f ). The results are shown in Table 4.
Test set average max
MT04 0.397 0.401
MT05 0.393 0.397
MT08 0.286 0.281
Table 4: Translation performance when using averaged feature values for similarity computation.
The numbers in the second column are based on Equation 4. Numbers based on Equation 5 are also listed in the third column for comparison . In all the experiment settings we did not observe consistent or significant advantage when using Equation 4 over using Equation 5. Since Equation 5 practice . So did we conduct all following experiments based on Equation 5.
We are also interested in the correlation between two measures : the similarity between development and test data and the actual translation performance on test data.
First we would like to echo the motivating experiment presented in Section 3. Table 5 shows the similarity between the data sets used in the experiment withMMT03+MT06 as reference model.
Obviously the results in Table 2 and Table 5 fit each other very well.
Dev data MT05 MT08nw
MT03 0.99988 0.99012
MT06-nw 0.99004 0.99728
Table 5: Similarity between NIST data sets.
Figure 1 shows the results of a set of more comprehensive experiments on MT05 data set concerning the similarity between development and test sets.
0.28 0.3 0.32 0.34 0.36 0.38 0.4 10 20 30 40 50 60
BL
EU
Rank of development set
Multiple
MT03+MT06
MT06
Figure 1: Correlation between similarity and
BLEU on MT05 data set
In the figure , every data line shows how BLEU score changes when different prebuilt development set in SMT03+MT06 is used for model learning . The data points in each line are sorted by the rank of similarity between the development set in use and the MT05 data set . We also compared results based on 3 reference model settings . In the first one ( multiple ), the similarity was computed using Equation 6, and the reference model set contains all models learnt from the development sets in SMT03+MT06. The other two settings use reference models learnt from MT06 and MT03+MT06 data sets respectively.
We can observe from the figure that the correlation between BLEU scores and data set similarity can only be identified on macro scales for all the three similarity settings . Although using data similarity may not be able to select the perfect development data set from SDF , by picking a development set with highest similarity score , we can usually ( almost always ) get good enough BLEU scores in our experiments.
4.4 Results of Development Data Dynamic
Generation
We ran two sets of experiments for the method of development data dynamic construction.
The first one was designed to investigate how the size of extracted development data affects the translation performance . Using MT05 and MT08 as test sets and MT03+MT06 as DF , we ran experiments for the algorithm presented in Section 3.3 with n = 200 to n = 1, 000. In this experiment we did not observe significant enough changes in BLEU scores ? the difference between the highest and lowest numbers is generally less than 0.005.
The second one aimed at examining how BLEU numbers changes when the extracted development data were iteratively updated . Figure 2 shows one set of results on test sets MT05 and MT08 using MT03+MT06 data set as DF and n set to 400.
0.365 0.37 0.375 0.38 0.385 0.39 0.395 0.4 0.405 1 2 3 4 5 6 7 0.265 0.27 0.275 0.28 0.285 0.29 0.295 0.3 0.305
M
T0
EU
M
T0
EU
Iteration
MT05
MT08
Figure 2: BLEU score as function of iteration in dynamic development data extraction.
The similarity usually converged after 2 to 3 it-scores on test sets . However , in all our experimental settings , we did not observe any results significantly better than using the development set pre-construction method.
5 Discussions
Some of the previous work related to building adaptive SMT systems were discussed in the domain adaptation context , in which one fundamental idea is to estimate a more suitable domain-specific translation model or language model.
When the target domain is already known , adding a small amount of domain data ( both monolingual and bilingual ) to the existing training corpora has been shown to be very effective in practice . But model adaptation is required in more scenarios other than explicitly defined domains . As shown by the results in Table 2, even for the data from the same domain , distribution mismatch can also be a problem.
There are also considerable efforts made to deal with the unknown distribution of text to be translated , and the research topics were still focused on translation and language model adaptation . Typical methods used in this direction include dynamic data selection ( Lu ? et al , 2007; Zhao et al , 2004; Hildebrand et al , 1995) and data weighting ( Foster and Kuhn , 2007; Matsoukas et al , 2009).
All the mentioned methods use information retrieval techniques to identify relevant training data from the entire training corpora.
Our work presented here also makes no assumption about the distribution of test data , but it differs from the previous methods significantly from a loglinear model?s perspective . Adjusting translation and language models based on test data can be viewed as adaptation of feature values , while our method is essentially adaptation of feature weights . This difference makes these two kinds of methods complementary to each other ? it is possible to make further improvement by using both of them in one task.
To our knowledge , there is no dedicated discussion on principled methods to perform development data selection in previous research . In Lu ? et al (2007), loglinear model parameters can also be adjusted at decoding time . But in their approach , the adjustment was based on heuristic rules and reweighted training data distribution.
In addition , compared with training data selection , the computational cost of development data selection is much smaller.
From machine learning perspective , both proposed methods can be viewed as certain form of transductive learning applied to the SMT task ( Ueffing et al , 2007). But our methods do not rely on surface similarities between training and training/development sentences , and develop-ment/test sentences are not used to retrain SMT submodels.
6 Conclusions and Future Work
In this paper , we addressed the data mismatch issue between training and decoding time of loglinear SMT models , and presented principled methods for dynamically inferring test data dependent model parameters with development set selection . We describe two algorithms for this task , development set pre-construction and dynamic construction , and evaluated our method on the NIST data sets for the ChineseEnglish translation task . Experimental results show that our methods are capable of consistently achieving good translation performance on multiple test sets with different data distributions without manual tweaking of loglinear model parameters.
Though theoretically using the dynamic construction method could bring better results , the pre-construction method performs comparably well in our experimental settings . Considering the fact that the pre-consruction method is computationally cheaper , it should be a better choice in practice.
In the future , we are interested in two directions . One is to explore the possibility to perform data clustering on test set as well and choosing suitable model parameters for each cluster separately . The other involves dynamic SMT model selection ? for example , some parts of the test data fit the phrasebased model better while other parts can be better translated using a syntaxbased model.
669
References
David Chiang . 2005. A hierarchical phrasebased model for statistical machine translation . In Proc.
of the 43th Annual Meeting of the Association for Computational Linguistic ( ACL ). Ann Arbor,
Michigan.
George Foster and Roland Kuhn . 2007. Mixture-Model Adaptation for SMT . In Proc . of the Second ACL Workshop on Statistical Machine Translation..
Prague , Czech Republic.
Michel Galley , Mark Hopkins , Kevin Knight and Daniel Marcu . 2004. What?s in a translation rule ? In Proc . of the Human Language Technology Conf.
(HLTNAACL ). Boston , Massachusetts.
Almut Hildebrand , Matthias Eck , Stephan Vogel , and Alex Waibel . 1995. Adaptation of the Translation Model for Statistical Machine translation Based on Information Retrieval . In Proc . of EAMT . Budapest , Hungary.
Philipp Koehn , Franz Och and Daniel Marcu . 2003.
Statistical Phrase-Based Translation . In Proc . of the Human Language Technology Conf . ( HLTNAACL).
Edmonton , Canada.
Yang Liu , Qun Liu , and Shouxun Lin . 2007. Tree-to-string alignment template for statistical machine translation . In Proc . of the 45th Annual Meeting of the Association for Computational Linguistic ( ACL ). Prague , Czech Republic.
Yajuan Lu ?, Jin Huang and Qun Liu . 2007. Improving Statistical Machine Translation Performance by Training Data Selection and Optimization . In Proc.
of the Conference on Empirical Methods in Natural Language Processing . Prague , Czech Republic.
Spyros Matsoukas , Antti-Veikko I . Rosti and Bing Zhang . 2009. Discriminative Corpus Weight Estimation for Machine Translation . In Proc . of the Conference on Empirical Methods in Natural Language Processing . Singapore.
Franz Och and Hermann Ney . 2002. Discriminative Training and Maximum Entropy Models for Statistical Machine Translation . In Proc . of the 40th Annual Meeting of the Association for Computational
Linguistic ( ACL ). Philadelphia , PA.
Franz Och . 2003. Minimum Error Rate Training in Statistical Machine Translation . In Proc . of the 41th Annual Meeting of the Association for Computational Linguistic ( ACL ). Sapporo , Japan.
Nicola Ueffing , Gholamreza Haffari and Anoop Sarkar . 2007. Transductive learning for statistical machine translation . In Proc . of the Annual Meeting of the Association for Computational Linguistics . Prague , Czech Republic.
Bing Zhao , Matthias Eck , and Stephan Vogel . 2004.
Language Model Adaptation for Statistical Machine Translation with Structured Query Models . In Proc.
of COLING . Geneva , Switzerland.
670
