CONTENT CHARACTERIZATIONUSING WORDSH A TOKENS
Penelope Sibun and David S . Farrar
Fuji Xerox Palo Alto Laboratory , 3400 Hill view Avenue , Palo Alto , CA 94304
sibun@pal.xerox.com , farrar@pal.xerox.com

By quickly classifying character images into character shape categories  , il is possible to automatically extract syntactic information from the text of document images without optical character recognition  . Using word shape tokens composed of these character shape codes  , a properly mrned text tagger can extract part-of . speech information fronls canne document images . Later components of a document processing system can then use this information to locate topics  , characterize document style , and assistill inlormation rctriewll . 
extract noun phrases and other content characteristics using only word shape tokens that have been tagged with their parts of speech  . Using this approach , we can process document images quickly to determine whether OCP  , is warranted , t brexample , when a text is a likely match for keywords in a database query  . 
In the next two sections , we describe how word shape tokens are derived ; in section four , we discuss part-of-speech tagging ; in the following fonr sections , wc describe in detail parl-of-speech tagging nsing word shape tokens  ; in sections nine and ten we discuss our results . 
1 INTRODUCTION
There arenlany text processing tasks that we would like to accomplish  , such as document classification , text database structuring , matching documents with queries , and topic characterization . The field of computatiom d linguistics has developed a variety of techniques for accomplishing these tasks for text & vuments represented by character codes  ( e . g . , ASCII) . l lowever , many documents for which we would like to use ot n automated techniques arc not stored online in character-coded\[or nla\[  , but instead exist only on paper . Optical character recognition ( OCR ) is a technique for converting scanned document images into character codes  . By using ( ) CR , document images can \[ y , 2 converted into a form amenable to existing text processing techniques  , t to w c v c r , OCR is expensive , slow , and o\[\[cnill accn rate . Because of these drawbacks , we would like to avoid OCR it we can , c . rat the least , postpone using OCR until we are confident that a document wammts detailed processing  . In other words , we would like a high-band width document processing system that is sensitive nough to detect desire document 

Our document understanding goals at the Fuji Xerox Pale Alto Laboratory include latlgaage determination  ( Nakayama and Spitz , 1993; Sibun and Spitz , forthcoming ) , (: otllettl('hara ( Terizalion , and style charucteri = alion . Toward these goals , we are developing it set of methods for extracting in lk  ) rmation from document images which do not depend on OCR  . We have been working toward our goal of inexpensive content characterization by adapting a part of -  . v ) eech tagger to process word shape tokens rather than character coded words  . Part-el-speech tagging is a technique that has been developed and refined over the past several years  , and it provides an inexpensive , last , and reliable source of inlormation for recognizing noun phlases and other syntax-related text features which help characterize a doeun len\ [ rscontent  . 
In this paper , we describe how we combine our technology for determining word shape tokens with text-tagging technology  . We are developing systems that can 2 WORDSHAPE TOKENC REATION In this section we briefly describe our system that constructs character shape codes and word shape tokens from a document linage  ( for more detail , see Nakayama and Spitz , 1993; Sibun and Spitz , forthcoming ) . To recognize character shape codex from an image , S Onle transforn latit nl salc first nla de\[o correct for various scanning artifacts such as skewangle and text line cn rvature  . On each text line , four horizontal lines define three significant zones : the area between the baseline and the top of characters such as " x " is the xcone  ; the area above the x-height level is the ascender , ~ one ; the are a below the x-zone is the descender zone ( figure 1 )  . Tim text line is furthcr divided into character cells by vertical bonnda  , ics which delineate the connected components of each character image  . 
~top x-height baseline bottom
Figure I : The text line parameter positions.
The majority of characters can easily be mapped to a small numher of distinct ccMes  ( \ [ igure 2 )  .   1 Cllaracters which are contained entirely in the x -zone map to shape codex  ; characters which extend \ [ rom the baseline to alx we the x-height line map to shape code A : and those which extend from below the baseline to the xq might line map to shape code g  . Characters which map to A , x , org are composed o1 a single connected component . Some characters conain Fnorcth an one connected component : an x-height character with a single diacritical mark in the ascender zone maps to i  ; a character with a descender and a single diacritical mark maps to j  . Most common punctuation marks map to unique shape codes  ; however , I If this nmppmg can bcd one from docmn cnt images  , it can more trivially b caCCOlnplished frmn character coded docmn cnts  , sllch as . ,\ St ' . \[I text ( providing , of course , that lhc method of encoding is known ) . 
6 86 some are mapped into shape codes shared with alphabetic characters  ( e . g . , "& " maps to shape code A) . 
Shape Oxtc
A ..... Ctmtacter
A-Zbdfhklt0-9#$&/(-:'1
Xace Irl I1 Ors UVt AX7 , i Iaitae e e 1 I I o o o u t r t l n i g g P q Y ~ :
JJl : igurc 2: Character shape codes.
3 SttAPECONVERSION
In general , our approach to docmnent processing finesses the problems il tllerent in mapping from an imagc to a character coded representation : wenlap instead froll t the imag c to a shal  ) ebased r ~ Tn'esentalion . This technique can transformevella degraded ocument illage it l to a representation which provides useful abstractions about the text of a document  . The shape-based representation that we construct is proving to be a relnarkably rich source  o1 information . While our initial goal has beel l to , use it lor language identification in support of downstrean lOCRpr  ( x ; esses , we are finding that this representation lnay be a sufficient source of information for document content characterization  , such as that supported by part-of-speech lagging . 
In our tagging work , we have used characters hape c~?ted text derived froth normal character-c ~  , led txt . This is simply because wed c , tlOt have access to enough in lage documents on which to train a taggef  . We call the process of creating a shape-Ntsed version ol the dtx xt tnent lroln the character eeriebased version shape conver  . viot LF or the purl x ~ se of text tagging , then , we cltn think oI the words hatx : token representation as an approximation of the representation composed of words  . We can think about the relationship between words and word shape tokens its a mapping from a word to its corresponding word shape token  . For example , the word " apple " maps to tile word shape token x g g A x  , and tile word " apples " maps to the word shape token x g g Axx  . 
hid(?;uments , words exist its sur/ace . fi~rms , not its morphological systems ; thus " apple " and " apples " are different words  . Therefore , it is of nousetous to have a lexicon organized in terms of stems and suffixes  ; i + rstcad , our lexicon is conlposed of stlrfaee forms like " apple " and " apples "  . Throughout the rest of this paper , when we say " words " , we rllean words as Sillface ftwll\]S . 
4 I'ART-OF-SI+EE CliT AGGING
A part of speech tagger is at system that uses context to assign parts of speech to words  . Part-of-speech information facilitates higher -level analysis  , such as recognizing nOUll phrases and other patterns ill text ? Several different approaches have been used for building text taggers  . A particular for nl of Markov model has been widely used that assumes thai a word d cpends probabilistically on just its part-of-slx ~ eeh category  , which m turn depend solely on the categories of the plecedmg two words  . Training the trlodeliss on let inles doue by means of a large lagged corpus  , but this is not necessary . 
The I~autn-Welch algorithm ( Baum ,  1972) , also know ttits the t ; or ward-l ~ , ackwardlg or ithm , carl be used . In this ease , the model is called a hidden Markovn lodel ( IIMM )  , since state transitic ms(i . e . , part- . of-speech ategories ) are assunled to be unobseuvable . 
l : or this work , we use an 11MM-based text tagger that is publicly available from X erox PAP  , C . As described in Cutting el al .  (1902) , the PAR ( 2 tagger is efficient and highly flexible . It is particularly important that the tagger can be trained on any e or ptlsel text  , using ally lexicon . 
This flexibility allows us to shape-convert our training corpus and lexicon  , its described in section 5 , without needing to modify the tagger itself . Below we outline tile basic operation of tire PARC tagger  ; please refer to Cutting el al . (1902) for further detail . 
1 . Text destined for tire tagger first encotlllters a tokenizer  , whose duty is to eolt Vel+text ( a sequence l characters ) into a sequence of tokens . Each sentence boundary is also identified by the tokenizer  , and is passed its a special token . 
2 . The tokenizer passes tokens t ? + the lexicon , where tokens are matched with a set of surface fofms  , each annotated with a Imrt-of-speech tag . These teltags constitutes an ambiguily class . The lexicon passes along a stream of ( . ~' llrfilce . fi)rnt , ambigltily class ) pairs . 
3a . In training mode , the tagger takes long sequences of ambiguity classes as input  . It uses the BaumWelch algorithm to produce a trained IIMM  , which is used its input in tagging I node . Training is performed on some corpus of interest ; this corpt l Sl nay be of broadcoverage or may be genre specific  . 
3b . Illlagging mode , tile tagger buflers sequence sel ambiguity classes between sentence bound mies  . '\[' hesc sequences are disambiguated by computing tile lnaximal path through the IIMM with the Viterbi algorithm  ( lO 67 )  . 
Operating at sentence granuhuity does l lot sacrifiee accuracy  , since sentence boundaries are unambiguous . 
Output consists of pairs of surface forms and tags . 
5 THELEXICON
The word shape tagging in our work follow stile t  IMM4  ) ased process described above . Both word shape tagging at ld standard word tagging require a lexicon  . 
5.1 Constructing tile Lexicon
A word shape lexicon can be derived from a standard lexicon of words  . The lexicon used with the standard text tagger contains a list of all the distinct surface forms likely to be encountered m the hmguage  . Associated with each surface form is a list of the possible parts of sIx ' ech that the ~ ttr face form can hit ve  .  \] ; or exalllple : ijp ~ le noun ~ LP ~ i ) hual noun eatver beats third person singular verb t ~ l noun  , adjective f . lledeterminer (   ) liCe We have a lexicon which consists of st tr face fonns  , we can use it to create a lexicOlt of word shape tokens for consists of the following steps :  1  . Shape convert the surface forms to th , ir corresponding word shape tokens . 
2 . Sort the lexicon by surface form word shape . At this stage there may be duplicate word shape tokens  . 
3 . Eliminate duplicatentries in the lexicon : collect all parts of speech behind one word shape token  ( combine their ambiguity classes )  . At this stage each word shape token should be unique  . 
4 . Eliminate duplicate parts of speech behind each word shape token  . At this stage each part of speech should be unique within each mn biguity class  . 
The lexicon fragment above would be converted to : xggAx noun x ggAx xplural norm xxA verb  , noun , adjective xxAx third person singular verb
A Ax det el mine r5 . 2 Analysis of the Lexicon For this work , we use a lexicon provided by Xerox PARC . This lexicon is organized so that there is an entry for each of roughly  150  , 000 surface forms , l : or word shape tagging , we shape converted this lexicon . As can be seen in the table , shape conversion results ill about 50 , 000 distinct word shape surlace forms . This suggests that , on average , each word shape token is a mapping of three surl acc forms  . However , about 30 , 000 of the word shape tokens arc unique , that is , correspond to a single surface form . 
Surface Forms Count % Total
Standard Lexicon 148,703 I0()+0
Sh~.tpc-eonverted Lexicon 47,1()2 31.7
Shape-converted Unique .28, 949 19.5
Thus , the word shape lexicon is approximately one-third the size of the standard lexicon  . Clearly , information has been lost , but not as much as one might think . In fact , the 20% of the word shape tokens that are unique carry exactly as much reformation as their corresponding character-coded words  . While some surface forms that map to unique word shape tokens are long and infrequent  ( like " flibbertigibbet " , AAiAAxxAigiAAxA) , many are short , Ct/lnlylOn words : xggAxx
A AigA thirst XA Aix x Aglife like Ai AxAi Axgx Axx g g x g A x g g x g A x g x While word shape tokens that are unique have the salne parts of speech as their corresponding surface forms  , the others will tend on average to have many more parts <  ) l speech than an averages tn Tace form . This defx znds somewhat on the tagset ( see section 6 )  . In general , word shape tokens frequently have as many as 10 to 15 parts of speech , where a standard surlace forms rarely have more than  4 or 5  . 
6 DEVISING THE TAGSET
The lagset is implicit in the lexicon : it includes all parts of speech listed in any entry of the lexicon  ; it also includes a small set of tags for punctuation  , such as comma , hyphen , and sentence boundary . Although the tagset is not explicitly defined , we can modify it by mapping from selected tags fonnd in the lexicon to other tags of our choosing  . For example , the lexicon distinguishes between verb tenses and has separate tags for different combinations of verb tense  , person , and number : presenl tense verb , paxllense verb , third pets ' on singular present verb , etc . If we preferred , we could map all these different verb forms to a single verb tag  . However , we typically prefer to maintain such distinctions  , as the text tagg cr can take advantage of differences in the surface forms of verbs with different enses in order to uniquely identify their parts of speech  . 
Shapecom , ersion collapses different surface lorms onto one word shape and merges their ambiguity classes  . The result is that them tend to betcwer distract surface forms  , and that each surface form has , on average , a larger ambiguity class . If this ambiguity is problematic , one way to reduce it may be to reduce the size of the tagset  . 
t : or example , we may choose to have one undifferentiated verb tag rather than a set which differentiates tense  , person , and namber . With fewer possible parts of speech to choose from  , the HMM may find the part-of-speech selection more constrained  . This in turn may improve its accuracy at selecting one of the tags that are available  . 
The uninteresting case , of COtll'Se , is where every word shape has the same tag , that is , a tagset of one . This situation yields no useful syntaclic inforlnation from the doct lnlent  . Since the use of word shape tokens does reduce the mnount of information that is mailable to the tagger  , it may rexluce the number of different tags it can accurately assign  . The proper size of the tagset becomes conshained on one hand by the anloun\[oJ syntactic ill\ [ Ormation we wish to extract  ( more in lk~rmation with a larger tagset ) and on the other by the size of the ambiguity classes of the word shape tokens  ( more ambiguity with a larger tagset )  . 
Its proper size is thus an empirical question . For our tests we used tagsets vd thap proximately 30parts of speech . 
7 TIlETRAINING PROCESS
Just as the hidd cn Markov model fc , r standard t cxt tagging requires a large corpus of text to tram on  , the word shape HMM requires a large corpns of text that has been converted to word shape tokens  . We used at least 3 . 5 megabytes of ASCII text for our standard text laggcr's corpus  ; we then shape converted this text to create the corpus for the word shape tagger  . This corpus consisted of a variety of different writing styles  ( from colloquial to professional ) and difficulty levels ( from casual Io erudite )  . 
\[-'2 x amplcs includes says by huulorists , proposals lor new government lx~lieies , and classic works o\[literature . 
6888 TtlETA(;GIN(;PRO(~ENS
With the word Shal ) Clexicon in place and t inadequatcly trained 1 \[ MM , word shape tagging works just as stmldmd text tagging does  . 111 part ( el . liar , words impetagging consists of the following steps : I  . A stie an it of tc xl is tokenized in ( ( ) a streani of w (  , ird shap0 tok0ns segnlented it l to S0 lltellces . 
2 . The slml ) c-e onv crted lexicon assigns an ambiguity class to caeh words l  , iape to kcll . Thcucsultisi/Stl Ci ( l l l ( ) l " sentence ++ composed of ( word shape Ioke . , am hig . ilv clas . v ) pairs . 
3 . The lagg cruses thctrained hidden Mark ( , iv model to comt mt c the highest probability part <11 speech for each word shape t ( ~keninasen ( cute . The rcsultix a stream of ( wordshape loken , parto/speech ) pairs , ~ hich are grouped accordiilg to senletice bOUlMaties  , W0 can lim x us 0 the r0 sulting l , it/l'ts O ( speeclit to illl OlM ( ) thor se~+ ( litleltts of i , it OetllTI oi qlull delSlill ldillg : ; ystell . The word shape ix ut--ol-sp cechlagg crtiros accepts w  , ind shal ) e tokens grouped by solltei , ieebll tuldaries ; within those boundaries , it assigi is the in l ) sl likely part of speech t ( ~ ca ( hi words hat ~ c to k0n . 
9 RI?SULTS
In th lissection , we introduce i , i tool whichet in recognize noun phrases in sentences  , and we use this tool to conipme the performal it cc  ( 1 1 the standard tagg cr and timwo ix_l shape tagger . We exemplily the comparison with tWO texts : one on which the staitidard tagger perfoims very wel l  , al , idoiticoit it which it does rehitti+elyp (+ oity . While the word shape tagger doesh : ss well in each case  , its behavit/r tracks that ( fl ' the standard tagger , exhibiting siinihu " successes aild faihlrcs , l : or the partieuhu task ( /Ii indiititg simple not lnpiuases , the word shape tagger's pei'l'01l/lililee is less than t Jilit of the standard tlll ~ gcl's , billahu'ge!+l ; aetion of the litOtliit phrase still are found . 
Wchave Lits . ystcnltht , it ( : tillieeogiitiies Jlnpl clie ( ill phrases wh cn givei , i its input the seq , ileilce of tags Iot a SO l , licit ( co+tach of these phrases conlpriscs a contigtlous sequence  o1 tags that satisfies as trut+h:gral , illilar , l " or example , aII ( , it llpluase eltil besimi ) lya plonoullt ~ igor ( , in all : litlaly set it lell ce(: , Ilie ( It 1 lindad . iective lags , pmsibly preceded by adctell , i Hilerlag and possibly with till embedded possessive lag  . 2 The hlnges l possible S , i leh sequences it r0I + ot md . (\] oi , ij , ili , iction saleloile c ~> gliized ( is piut of all OUll p in as e , llOlisprcp ( +sithmal Dhirase : alhle hnient perf ( )rii , icd . We can bce onlident of finding Ill ( lilyshnple no ( it/phmtses b0cause the t~old " thC'hl as I hetnlique word shape / % Ax  . 3I , ~ ccognilioneli1 ( 1 ( 1 11 phrases is a iirst sicp in topic id cntilie at ion : the topie  ( it ad (  , iCUlilel , it is likely t < l be indicaled t ) 3 its l no slhequent tie ( in phrases . 
li , i0 vahialing . , the hit(gel e l le l rnle , w crises 0 veralli itea S , illes ( s0 ctables ) . We calculate lhcpcle enlagcof Iola J error ~ , thc percentage of Irh , Talerror & and the porcel , illlge ? . Thei ) osscs sivct a pistls c(I for "' s"el'r , , as in " the cat's l)ajanias ' striF , es " 3 Another I , inglish xvc ) l'd , " lhl , " also mapsIoAAx;I ' ollilllalcly , ill III+ . )SI Ct ) lllL ' XlSI his word is lllC ( ~1l ~ ernicious error ~ ( there met it fewei T ( )lS that do not fall in either of the latter categories  )  . Tagging ' lalaMning " ill " what the advocates a , efinding a huming " astit present t xuticipk : rather than as an adje clivc is an exampl c/f la trivial error  . Pernicious errors typically in voh , ere ( stagging nouns as verbs or verbs as nouns ( in l ' ; nglish , the re~tl Cilially stlrtitce IOIIDS that can be either l  , i ( ) lll Hlal ( , il verbal ) . These latter el'l"()i+se0 . 11 seprobh : ms in h , it crpl + oe essiitlg , such las dote . ( ring simple it it OU it l phrases , sitice they May ( Ib Nctll'l : 1101111 phl + a + ~ esorill h + od tl ce spurious ( /lies . 
We compatct hest and at+d tagger and the word shape tagg cr by counting the real  ( hem in the str cat ns of output tags . We do not demand strict matches , but ms ( cad allow the rags to belong to pertinent equivalence classes  . I , ' or exarn ple , the standat+d tagger labels the noun " monitors " as a plural noun  , at , id the word shape tagger la\[xelsxxxiAxxx simply  ( is a litOut , i . We c() it its ider this a match , Sill Ceitl , i ( Itllit\[ttitdaplkitit'ltlitit ( 3tllit iltl'e equal ly wel l recognized as pttrt o1 it lit ) till ( phrase , Ahl , ioslall instances , ( , it niismate hesrcs tllt from the standard tagger being right and the word shape lagger being wrong  . Very occasionally the situatiotitix the reverse , but this ix to be expected as within the normal range of probabilities  . More interesting is the observation that almost every pernicious error made by the standard tagger ix repeated by I he word shape tagger  . W c take this a + s c ( , in firtnal tion of tim word shape tagger's ability to appmx in tate he standard tagger's pcr to imat  , iee . 
The first COl/ll ) ariso II of tagger peMormance involves a 30/!---w (  , ird excell , it I + l + Oll it govorl/I , i lell l , doct liititent . The standard lagger's I ) eitT ( )itmance is better than 95c ) ~ correct .   ( it bett cr than 97% if trivial errors are disregarded . The word shape tagger's perl ( irnuuite e is a 59% match ( 11 the standaFd tagger's ( or 51% if only exact matches are considered )  . 
The noun phrase recogni/ . er\[outld 113 sinlple limln phrases in the standard tagger's (  , it ltptlti it litd 77((~b ; %) o1 these in the word shape tagg cr's OUtl ) Ut . 
Si and al'd Tagger Erl ' or s
Matching Outpu ! ol
Standard Tagger and Word Shape Tagger l ) is regard mg In eludiil gall \] t_'lh trial Misnultehes Mismatches  1   59%  -  .   .   .   .   .   .   -5"~ ill'lellt\[Nonscnscl~l "/ <: 38~ INounPhr:tses Recognized from Tagger Outpul The second comparison is of lags  , big a 14+ I word pieceel IIO It SeIISC VCI'SC . The stiill diild t +: . lg . gcr'si ) etf + . )rnlilncc is shape tagger's perfornmnce is a 47% match ( or 38% considering only exact matches )  . The noun phrase recognizer found 45 simple noun phrases in the sl and m'd tagger's output and  17   ( 38% ) of these in the word shapelagger's output . 
\[: urther study is needed to determ in exactly how reliable word shape part-of-speech tagging and simple noun phrase recognition will be in finding the topic or topics in a document image  , One means of improving this reliability may be our technique for grammatical function assignment which uses only the output of the part-of-speech tagger and phrase recognizers  ( Sibun 1991 )  . 
However , we can a beadynse part-of-speech lagging and simple noun phrase recognition as a tool for discerning something about the content of the document by discovering at least some of its noun phrases  , Since our document rceognition technology allows us to use word shape tokens to index directly into the document image  , we can also identify parts of the image as promising candidates for OCR  , 10I ) ISCUSSION Although the word shape tagger-tleals wilh greater ambiguity  , it can still extract significant information from a text  . The increase in ambiguity is not as high as might be expected : a large number of word shapes remain unambiguous after the lexicon has been shape converted  . 
As noted above , the creation of the word shape lexicon from the standard lexicon reduces the number of distinct entries to approximately one-third  . Vor example , distinct words such as " cat " and " rat " map onto the same word shape token xxA  . Nevertheless , the complexity of English spelling still allows a large proportion of surface forms to be distinguished merely by their word shapes  . 
Severalinl provements on our technique remain to be fully implemented  . We do not yet have a principled way to determine the optimal tagset for a given corpus of texl  . 
As noted alxwe , there is a tension between the size of the tagset and the amount of syntactic information that is available in the word shape tokens  . 
We are also investigating computationally inexpensive ways of making finer distinctions between characte  , s that map to the character shape codes x and A . Initially , parentheses and brackets were always classified as A antidistorted any word shape they were adjacent to : for example  , "( USA ) " woukl be shape converted to AAAAA . 
Recently we have made progress m recognizing these nor a alphabetic characters as wnrd shape token delimiters  , rather than parts of the word shape tokens I hemselves  . It may also be useful to distinguish more alphabetic haracter elasses by mapping scanned character bnages to a larger set of ch macter shape codes  . We canext , ' act more useful in lknmation by distinguishing uppercase letters from lower case letters  , such as " h " and " k " , which malt to the character shape code A . A larger number of character shape codes gives us more information about the word shape tokens  , and helps Io reduce ambiguity , l lowever , we must be careful to choose character shape features which can bceasily dctccted in the image and quickly class if led by a character shapect x  . le . 
In keeping with VnjiXerox's multiqingual document emphasis  , we are also exploring ways in which this method may be applied to other Roman-alphabet languages  , uchas French , German , Dutch , and Spanish . 
The technique will need to be evaluated separately for each language  , however , to better understand how each hmguage's typographic conventions may be reflected in its word shape  . 
11 CONCLI ! SION
We have presented a new technique for the understanding of English document images without optical charactere cognition  . By scanning and categorizing character shapes , it is possible to extract word shapes from the document text  ; these word shapes tokens can then be used as input to a tagger which determines part-of-speech reformation  . This part-of-speech inlormation can then be used to in form other document understanding techniques  , including noun phrase recognition and topic identification  . 
The lack of OCR means we cannot extract all of the information contained in the scanned ??: tnnent's image  ; nevertheless , the information from the word shape tokens allows us to characterize the document's content with significant accuracy  , and more quickly than if we performed O(;\[ , I . 

We thank Larry Spitz and MasaOzaki for their useful comments  . 

Baum , 1, . E . " An inequality antiassociated maximization technique in statistical estimation for pm babilistic functions of a Markov process  . "lue qualilies , 3:l--g , 1972 . 
Cutting , Doug , Julian Kupiec , Jan Pedersen , and Penelope Sibun . "A Practical Part-of-Speech Tagger . " In Proceedings of the Third Cotference on Applied Natural Language Processing  ( ACIJ , pp 133-140 , Trento , Italy ,  1992 . Also Report SSL-924) l/P92-00(X)I , Xerox Palo Alto Research Center ,  1992 . 
Nakayama , Takehiro and A . L . Spitz . " European Language Determination from Image . " Inl ~ roeeedings of the SeColld hllernational ( ~ . onference on D ( K3tlnlerl\[Amflysis and Recognition , pp 159-162 , Tsukuba
Science City , Japan , 1993.
Sibun , Penelope . " Grammatical Function Assignment in Unrestricted Text  . "Inte , nal Report , Xerox Palo Alto
Research Center , 1991.
Sibun , Penelope and A . l , a wrence Spitz . " Language Determination : Naturall , anguage Processing frolu Scannedl ) ocnnmnt Images . " l:orthcoming . 
Viterbi , A . J . " Error Ix ) unds\[k ) r convolution codes and ctn asymptotically optimal decoding algorithm  . "llqffs'Transactions onlt/brmalion Theory . pit 260-269 . 
April 1967.

