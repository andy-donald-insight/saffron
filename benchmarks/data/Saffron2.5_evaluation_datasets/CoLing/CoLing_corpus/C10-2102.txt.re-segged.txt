Coling 2010: Poster Volume , pages 885?893,
Beijing , August 2010
A Learnable Constraint-based Grammar Formalism
Smaranda Muresan
School of Communication and Information
Rutgers University
smuresan@rci.rutgers.edu
Abstract
Lexicalized Well-Founded Grammar ( LWFG ) is a recently developed syntactic-semantic grammar formalism for deep language understanding , which balances expressiveness with provable learnability results . The learnability result for LWFGs assumes that the semantic composition constraints are learnable . In this paper , we show what are the properties and principles the semantic representation and grammar formalism require , in order to be able to learn these constraints from examples , and give a learning algorithm.
We also introduce a LWFG parser as a deductive system , used as an inference engine during LWFG induction . An example for learning a grammar for noun compounds is given.
1 Introduction
Recently , several machine learning approaches have been proposed for mapping sentences to their formal meaning representations ( Ge and Mooney , 2005; Zettlemoyer and Collins , 2005; Muresan , 2008; Wong and Mooney , 2007; Zettlemoyer and Collins , 2009). However , only few of them integrate the semantic representation with a grammar formalism : ?- expressions and Combinatory Categorial Grammars ( CCGs ) ( Steedman , 1996) are used by Zettlemoyer and Collins (2005;2009), and ontology-based representations and Lexicalized Well-Founded Grammars ( LWFGs ) ( Muresan and Rambow , 2007) are used by Muresan (2008).
An advantage of the LWFG formalism , compared to most constraint-based grammar formalisms developed for deep language understanding , is that it is accompanied by a learnability guarantee , the search space for LWFG induction being a complete grammar lattice ( Muresan and Rambow , 2007). Like other constraint-based grammar formalisms , the semantic structures in LWFG are composed by constraint solving , semantic composition being realized through constraints at the grammar rule level . Moreover , semantic interpretation is also realized through constraints at the grammar rule level , providing access to meaning during parsing.
However , the learnability result given by Muresan and Rambow (2007) assumed that the grammar constraints were learnable . In this paper we present the properties and principles of the semantic representation and grammar formalism that allow us to learn the semantic composition constraints . These constraints are a simplified version of ? path equations ? ( Shieber et al , 1983), and we present an algorithm for learning these constraints from examples ( Section 5). We also present a LWFG parser as a deductive system ( Shieber et al ., 1995) ( Section 3). The LWFG parser is used as an innate inference engine during LWFG learning , and we present an algorithm for learning LWFGs from examples ( Section 4). A discussion and an example of learning a grammar for noun compounds are given is Section 6.
2 Lexicalized Well-Founded Grammars
Lexicalized Well-Founded Grammar ( LWFG ) is a recently developed formalism that balances expressiveness with provable learnability results ( Muresan and Rambow , 2007). LWFGs are a type of Definite Clause Grammars ( Pereira and Warren , 1980) in which (1) the contextfree backbone is extended by introducing a partial ordering relation among nonterminals , 2) grammar nonterminals are augmented with strings and their syntactic-semantic representations , called semantic molecules , and (3) grammar rules can have a . ( w1, ` h1b1 ?)= ( laser,0
B
B
B
B
B @ head X1 mod X2
X1.isa = laser , X2.P1=X1
E
C
C
C
C
A ) b . ( w2, ` h2b2 ?)=( printer,0
B
B
B
B
B @ nr sg head X3
X3.isa = printer
E
C
C
C
C
A ) 2. Syntagmas containing a derived semantic molecule ( w , ` hb ?)=( laser printer,0
B
B
B
B
B @ nr sg head X
X1.isa = laser , X . P1=X1, X . isa=printer
E
C
C
C
C
A ) 3. Constraint Grammar Rule
NC(w , ` h b ? ) ? Noun(w1, ` h1 b1 ? ), Noun(w2, ` h2 b2 ? ) : ? c(h , h1, h2),?onto(b ) ? c(h , h1, h2) = { h.cat = nc , h1.cat = noun , h2.cat = noun , h.head = h1.mod , h.head = h2.head , h.nr = h2.nr } ? onto(b ) returns ? X1.isa = laser,X.instr = X1,X.isa = printer ? Figure 1: Syntagmas containing elementary semantic molecules (1) and a derived semantic molecule (2); A constraint grammar rule together with the semantic composition and ontology-based interpretation constraints , ? c and ? onto (3) two types of constraints , one for semantic composition and one for semantic interpretation . The first property allows LWFG learning from a small set of examples . The last two properties make LWFGs a type of syntactic-semantic grammars.
Definition 1. A semantic molecule associated with a natural language string w , is a syntactic-semantic representation , w ? = ( hb ), where h ( head ) encodes compositional information , while b ( body ) is the actual semantic representation of the string w.
Grammar nonterminals are augmented with pairs of strings and their semantic molecules.
These pairs are called syntagmas , and are denoted by ? = ( w,w ?) = ( w , ( hb ) ).
Examples of semantic molecules for the nouns laser and printer and the noun-noun compound laser printer are given in Figure 1. When associated with lexical items , semantic molecules are called elementary semantic molecules . When semantic molecules are built by the combination of others , they are called derived semantic molecules . Formally , the semantic molecule head , h , is a one-level feature structure ( i.e ., values are atomic ), while the semantic molecule body , b , is a logical form built as a conjunction of atomic predicates ? concept?.?attr ? = ? concept ?, where variables are either concept or slot identifiers in an on-tology.1 1The body of a semantic molecule is called OntoSeR and Muresan and Rambow (2007) formally defined LWFGs , and we present here a slight modification of their definition.
Definition 2. A Lexicalized Well-Founded Grammar ( LWFG ) is a 7-tuple , G = ??,??, NG , , PG , P ?, S ?, where : 1. ? is a finite set of terminal symbols.
2. ?? is a finite set of elementary semantic molecules corresponding to the terminal symbols.
3. NG is a finite set of nonterminal symbols.
NG ?? = ?. We denote pre(NG ) ? NG , the set of preterminals ( a.k.a , parts of speech ) 4.  is a partial ordering relation among nonterminals.
5. PG is the set of constraint grammar rules . A constraint grammar rule is written A (?) ? B1(?1), . . . , Bn(?n ) : ?(??), where A,Bi ? NG , ?? = (?, ?1, ..., ? n ) such that ? = ( w,w ?), ? i = ( wi , wi ?), 1 ? i ? n,w = w1 ? ? ? wn , w ? = w?1 ? ? ? ? ? w?n , and ? is the composition operator for semantic molecules ( more details about the composition operator are given in Section 5). For brevity , we denote a rule by A ? ? : ?, where A ? NG , ? ? N+G . P ? is the set of constraint grammar rules whose lefthand side are preterminals , A (?) ?, A ? pre(NG).
is a flat ontology-based semantic representation.
886
We use the notation A ? ? for this grammar rules . In LWFG due to partial ordering among nonterminals we can have ordered constraint grammar rules and non-ordered constraint grammar rules ( both types can be recursive or nonrecursive ). A grammar rule A (?) ? B1(?1), . . . , Bn(?n ) : ?(??), is an ordered rule , if for all Bi , we have A  Bi.
In LWFGs , each nonterminal symbol is a lefthand side in at least one ordered nonrecursive rule and the empty string cannot be derived from any nonterminal symbol.
6. S ? NG is the start nonterminal symbol , and ? A ? NG , S  A ( we use the same notation for the reflexive , transitive closure of ).
The partial ordering relationmakes the set of nonterminals well-founded2 , which allows the ordering of the grammar rules , as well as the ordering of the syntagmas generated by LWFGs . This ordering allow LWFG learning from a small set of representative examples ( Muresan and Rambow , 2007) ( P ? is not learned).
An example of a LWFG rule is given in Figure 1(3). Nonterminals are augmented with syntagmas . Moreover , in LWFG the semantic composition and interpretation are realized via constraints at the grammar rule level (?(??) in Definition 2). More precisely , syntagma composition means string concatenation ( w = w1w2) and semantic molecule composition (( hb ) = ( h1 b1 ) ? ( h2 b2 )) ?- where the bodies of semantic molecules are concatenated through logical conjunction ( b = ( b1, b2)?, where ? is a variable substitution ? = { X2/X,X3/X }), while the semantic molecules heads are composed through compositional constraints ? c(h , h1, h2), which are a simplified version of ? path equations ? ( Shieber et al , 1983) ( see Figure 1(3)). During LWFG learning , compositional constraints ? c are learned together with the grammar rules . Semantic interpretation , which is ontology-based in LWFG , is also encoded as constraints at the grammar rule level ? ? onto ? providing access to meaning during parsing.
?onto(b ) constraints are applied to the body of the semantic molecule corresponding to the syn-2 should not be confused with information ordering derived from flat feature structures tagma associated with the lefthand side nonterminal . The ontology-based constraints are not learned ; rather , ? onto is a general predicate that succeed or fail as a result of querying an ontology ? when it succeeds , it instantiates the variables of the semantic representation with concepts/slots in the ontology ( see the example in Figure 1(3)).
2.1 Derivation in LWFG
The derivation in LWFG is called ground syntagma derivation , and it can be seen as the bottom up counterpart of the usual derivation.
Given a LWFG , G , the ground syntagma derivation relation , ? G ?, is defined as : A??
A?G ?? ( if ? = ( w,w ?), w ? ?, w ? ? ??, i.e ., A ? pre(NG , ), and Bi?G??i , i=1,...,n , A(?)?B1(?1),...,Bn(?n ) : ?(??)
A?G ?? .
The set of all syntagmas generated by a grammar G is L?(G ) = {?|? = ( w,w ?), w ? ?+, ? A ? NG , A ? G ? ?}. Given a LWFG G , E ? ? L?(G ) is called a sublanguage of G . Extending the notation , given a LWFG G , the set of syntagmas generated by a rule ( A ? ? : ?) ? PG is L?(A ? ? : ?) = {?|? = ( w,w ?), w ? ?+, ( A ? ? : ?) ? G ? ?}, where ( A ? ? : ?) ? G ? ? denotes the ground derivation A ? G ? ? obtained using the rule A ? ? : ? in the last derivation step.
3 LWFG Parsing as Deduction
Following Shieber (1995), we present the Lexicalized Well-Founded Grammar parser as a deductive proof system in Table 1. The items of the logic are of the form [ i , j , ? ij , A ? ? ? ?? A ], where A ? ?? : ? A is a grammar rule , ? A ? the constraints corresponding to the grammar rule whose lefthand side nonterminal is A ? can be true , ? shows how much of the righthand side of the rule has been recognized so far , i points to the parent node where the rule was invoked , and j points to the position in the input that the recognition has reached . We use the following notations : ? Rij = ( wRij , ( hRij bRij ) ) are syntagmas corresponding to the partially parsed righthand side of a rule ; ? Lij = ( wLij , ( hLij bLij ) ) are ground-derived syntagmas ( i.e ., they are augmenting the lefthand side non-the ? A constraint can be true Axioms [ i , i + 1, ? Lii+1, Bi ? ?] 1 ? i ? n,Bi ? pre(NG ), Bi ? ? Lii+1 ? P ? Goals [ i , j , ? Lij , A ? ?? A ?] 1 ? i , j ? n + 1, A ? NG , ? ? N+G
Inference Rules
Prediction [ i,j,?Lij , B???B?][i,i,?Rii,A??B??A ] ? A ? B ? : ?
A ? ( A ? B ? : ? A ) ? PG ? Rii = ?? ( i.e ., wRii = , bRii = true and hRii = ?) Completion [ i,j,?Rij , A ?? ? B ? ? A ] [ j,k,?Ljk,B ?? ? B ?][ i,k,?Rik,A ?? B ? ? ? A ] ?
R ik = ? Rij ? ? Ljk , where wRik = wRijwLjk , bRik = bRijbLjk , hRik = hRij ? hLjk Constraint [ i,j,?Rij , A????A][i,j,?Lij , A???A ?] ??
A is satisfiable ? ? Lij = ?(? Rij)
Table 1: LWFG parsing as deductive system terminal of a LWFG rule ). The goal items are of the form [ i , j , ? Lij , A ? ?? A ?], where ? Lij is ground-derived from the rule A ? ? : ? A.
Compared to the deductive system in ( Shieber et al , 1995), the LWFG parser has the following characteristics : each item is augmented with a syntagma ; the Constraint rule is a new inference rule , and the goal items are associated to every nonterminal in the grammar , not only to the start symbol ( i.e ., LWFG parser is a robust parser ). The Constraint inference rule is the only one that obtains an inactive edge3, from an active edge by executing the grammar constraint ? A ( the ? is shifted across the constraint ). By applying the Constraint rule as the last inference rule we obtain the ground-derived syntagmas ? Lij . Thus , the goal items are obtained only after the Constraint rule is applied . During this inference rule we have that ? Lij = ?(? Rij ), where ? is defined by : wLij = wRij , bLij = bRij?ij , and hLij = ?( hRij ). The substitution ? ij and the function ? are implicitly contained in the grammar constraint ? Ac ( hLij , hRij ) ( see Section 5 for details ) Definition 3 ( Robust parsing provability ). Robust parsing provability corresponds to reaching the goal item : ` rp A(?Lij ) iff [ i , j , ? Lij , A ? ?? A?].
Thus , we can notice that the ground syntagma derivation is equivalent to robust parsing provability , i.e ., A ? G ? ? iff G ` rp A(?).
3We use Kay?s terminology : items are edges , where the axioms and goals are inactive edges having ? at the end , while the rest are active edges ( Kay , 1986).
4 Learning LWFGs
The theoretical learning model for LWFG induction , Grammar Approximation by Representative Sublanguage ( GARS ), together with a learnability theorem was introduced in ( Muresan and Rambow , 2007). LWFG?s learning framework characterizes the ? importance ? of substructures in the model not simply by frequency , but rather linguistically , by defining a notion of ? representative examples ? that drives the acquisition process.
Informally , representative examples are ? building blocks ? from which larger structures can be inferred via reference to a larger generalization corpus referred to as representative sublanguage in ( Muresan and Rambow , 2007). The GARS model uses a polynomial algorithm for LWFG learning that take advantage of the building blocks nature of representative examples.
The LWFG induction algorithm belongs to the class of Inductive Logic Programming methods ( ILP ), based on entailment ( Muggleton , 1995; Dzeroski , 2007). At each step a new constraint grammar rule is learned from the current representative example , ?. Then this rule is added to the grammar rule set . The process continues until all the representative examples are covered . We describe below the process of learning a grammar rule from the current representative example : 1. Most Specific Grammar Rule Generation.
In the first step , the most specific grammar rule is generated from the current representative example ?. The category annotated
STEP 2 ( Grammar Rule Generalization ) ( laser printer,
CANDIDATE GRAMMAR RULES laser printer
Performance CriteriaBEST RULE (( laser printer ) manual)(desktop ( laser printer))
K ? Background KnowledgeLexicon ( laser , )
Previously learned grammar rules cat nc ) ( printer , ) ? = ( w , ( hb )) - Current representative example a ) chunks={[NA(laser ), Noun(laser )], [ NC(printer),Noun(printer )]} rg1 NC ? Noun Noun:?c4 ( score=1)rg2 NC ? NA Noun:?c5 ( score=2) b ) r : NC(w , ( hb )) ? Noun(w1, ( h1b1)) Noun(w2, ( h2b2)):?c4(h , h1, h2)?c4(h , h1, h2) = { h.cat = nc , h1.cat = noun , h2.cat = noun,
E ? - Representative Sublanguage
NC ? NA NC:?c7 rg4 NC ? NA NC:?c7 ( score=3)rg3 NC ? Noun NC:?c6 ( score=2)
Noun ? cat nounhead X1mod X2?X1.isa = laser,X2.Y = X1?cat noun ? X3.isa = printer?
NA ? Noun:?c1 ? B.isa = laser , A.P1 = B,A.isa = printer?
NA ? NA NA:?c2NC ? Noun:?c3 nr sghead A head X3nr sgNoun ? h.head = h1.mod , h.head = h2.head , h.nr = h2.nr } Figure 2: Example of Grammar Rule Learning in the representative example gives the left-hand-side nonterminal , while a robust parser returns the minimum number of chunks covering the representative example.
The categories of the chunks give the nonterminals of the righthand side of the most specific rule . For example , in Figure 2, given the representative example laser printer annotated with its semantic molecule , and the background knowledge containing the already learned rules NA ? Noun : ? c1 ,
NA ? NA NA : ? c2 , NC ? Noun : ? c3 the robust parser generates the chunks corresponding to the noun laser and the noun printer : [ NA(laser),Noun(laser )] and [ NC(printer),Noun(printer )], respectively . The most specific rule is
NC ? Noun Noun : ? c4 , where the lefthand side nonterminal is given by the category of the representative example , in this case nc . Compositional constraints ? c4 are learned as well . In section 5 we give the algorithm for learning these constraints , and several properties and principles that are needed in order for these constraints to be learnable.
2. Grammar Rule Generalization . In the second step , this most specific rule is generalized , obtaining a set of candidate grammar rules ( the generalization step is the inverse of the derivation step used to define the complete grammar lattice search space in ( Muresan and Rambow , 2007)). The performance criterion in choosing the best grammar rule among these candidate hypotheses is the number of examples in the representative sublanguage E ? ( generalization corpus ) that can be parsed using the candidate grammar rule , rgi in the last ground derivation step , together with the previous learned rules , i.e ., | E??L?(rgi )|. In Figure 2 given the representative sublanguage E ?={ laser printer , laser printer manual , desktop laser printer } the learner will generalize to the recursive rule NC ? NA NC : ?7, since only this rule can parse all the examples in E?.
5 Learnable Composition Constraints
In LWFG , the semantic structures are composed by constraint solving , rather than functional application ( with lambda expressions and lambda reduction ). This section presents the properties and principles that guarantee the learnability of the compositional constraints,?c , and presents an algorithm to generate these constraints from examples , which is a key result for LWFG learnability.
The information for semantic composition is encoded in the head of semantic molecules . There are three types of attributes that belong to the semantic molecule head h : category attributes Ach , variable attributes Avh , and feature attributes Afh.
Thus , Ah = Ach ? Avh ? Afh and Ach,Avh,Afh arepairwise disjoint . For example , in Figure 1 for the noun-noun compound laser printer , we have that Ach = { cat }, Afh = { nr }, and Avh = { head},while for the noun laser we have that Ach1 = { cat }, Afh1 = ?, andAvh1 = { head,mod } ( nounscan be modifiers of other nouns , so their representation is similar to that of an adjective).
We describe in turn each of these types of attributes and their corresponding principles . All principles , except the first and the last mirror principles in other constraint-based linguistic formalisms , such as HPSG ( Pollard and Sag , 1994).
The category attributes Ach are state attributes , and their value set gives the category of the semantic molecule . There is one attribute , cat ? Ach , which is mandatory and whose value is the name of the category ( e.g ., h.cat = nc in Figure given by : 1) the cat attribute alone , or 2) the cat attribute together with other state attributes in Ach which are syntactic-semantic markers.
Principle 1 ( Category Name Principle ). The category name h.cat of a syntagma ? = ( w , ( hb ) ) is the same as the grammar nonterminal augmented with syntagma ?.
When learning a LWFG rule from an example ?, the above principle allows us to determine the nonterminal in the lefthand side of the grammar rule . For example , when learning the LWFG rule from the syntagma corresponding to laser printer in Figure 2, the nonterminal in the lefthand side of the LWFG rule is NC since h.cat = nc.
The variable attributes Avh are attributes whose values are logical variables and represent the semantic valence of the molecule , which allows the binding of the semantic representations.
These logical variables appear in the semantic molecule body as well . For example , in Figure 1(2) for the noun-noun compound laser printer , the value of the variable attribute head ? Avh is a variable X , which appears also in the body of the semantic molecule ? X1.isa = laser,X.P1 = X1, X.isa = printer ?. It can be noticed that the semantic molecule body contains other variables as well ( X1, P1). However , only the variables present in the semantic molecule head as well ( X ) will participate in further composition.
Principle 2 ( Semantic Representation Binding Principle ). All the logical variables that the body b of a semantic molecule corresponding to a syntagma ? = ( w , ( hb ) ), share with other syntagmas , are at the same time values of the variable attributes ( Avh ) of the semantic molecule head.
There is one variable attribute , head ? Avh that represents the head of a syntagma , giving the following principle : Principle 3 ( Semantic Head Principle ). Given a syntagma ? = ( w , ( hb ) ) ground derived from a grammar rule , r , there exists one and only one syntagma ? i = ( wi , ( hi bi ) ) corresponding to a nonterminal Bi in rule r?s righthand side , which has the same value of the attribute head , i.e ., h.head = hi.head.
The feature attributes Afh are the attributeswhose values express the specific properties of the semantic molecules ( e.g ., number , person).
Principle 4 ( Feature Inheritance Principle ). If ? i = ( wi , ( hi bi ) ) is the semantic head of a ground-derived syntagma ? = ( w , ( hb ) ), then all feature attributes of ? inherit the values of the corresponding attributes that belong to the semantic head ? i . That is , if h.head = hi.head , then h.f = hi.f , ? f ? Afh ? Afhi .
Besides this principle , the feature attributes are used for category agreement . The categories that enter in agreement are maximum projection categories . This linguistic knowledge about agreement is used in the form of the following principle : Principle 5 ( Feature Agreement Principle ). The agreeing categories and the agreement features are apriori given based on linguistic knowledge , and are applied only at the semantic head level.
Given all the above principles , we can now formulate the general Composition Principle : Principle 6 ( Composition Principle ). A syntagma ? = ( w,w ?) corresponding to the lefthand side nonterminal of a grammar rule is obtained by string concatenation ( w = w1 . . . wn ) and the composition of semantic molecules corresponding to the nonterminals from the rule righthand side : w ? = ( h b ) = ( w1 ? ? ? wn )? = w?1 ? ? ? ? ? w?n = ( h1 b1 ) ? ? ? ? ? ( hn bn ) = ( h1 ? ? ? ? ? hn ? b1, . . . , bn ?? ) The composition of the semantic molecule bodies is realized through conjunction after the application of a variable substitution ?. The body variable specialization substitution ? is the most general unifier ( mgu ) of b and b1, . . . , bn , s.t b = ( b1, . . . , bn )?. It is a particular form of the commonly used substitution ( Lloyd , 2003), i.e ., a finite set of the form { X1/Y1, . . . , Xm/Ym }, whereX1, . . . , Xm , Y1, . . . , Ym are variables , and
X1, . . . , Xm are distinct.
The composition of the semantic molecule heads is realized by a set of constraints ? c(h , h1..., hn ), which is a system of equations van Noord , 1993), but applied to flat feature structures : ? ? ? hi.c = ct hi.vi = hj . vj hi.f = ct or hi.f = hj . f ? ? ? where 0 ? i , j ? n , i 6= j c ? Achi vi ? Avhi , vj ? A v hj f ? Afhi , f ? A f hj When learning a LWFG rule from a representative example ? as in Figure 2, the robust parser returns the minimum number of chunks , n , covering ?. The body variable substitution ? is fully determined by the representative example as mgu of b and b1, . . . , bn , and the compositional constraints ? c(h , h1, . . . , hn ) are learned using Alg 1. For example , in Figure 2, when learning from the representative example corresponding to the string laser printer , we have that ? = { X1/B,X2/A,X3/A , Y/P1}.
In Alg 1 we use the notation ?0 = ( w0, ( h0 b0 ) ) to denote the representative example ?.
Alg 1: Learn Constraints(?0, ?1, . . . , ? n ) ? i = ( wi , ` hi bi ? ), 0 ? i ? n ? c ? ? ? ? mgu(b0, ( b1, . . . , bn )) foreach 0 ? i ? n ? c ? Achi do1 if hi.c = c1 then ? c ? ? c ? { hi.c = c1} foreach 0 ? i , j ? n ? i 6= j ? X/Y ? ??2 vi ? Avhi ? vj ? Avhj doif hi.vi = X ? hj . vj = Y then ? c ? ? c ? { hi.vi = hj . vj } if hs.head = h0.head , 1 ? s ? n then3 foreach f ? Afh0 ? Afhs doif h0.f = c1 ? hs.f = c1 then ? c ? ? c ? { h0.f = hs.f } if hs.cat = cs ? hi.cat = ci ? agr(cs , ci ), 1 ? i ? n then foreach f ? agrFeatures(cs , ci ) do if hs.f = c1 ? hi.f = c1 then ? c ? ? c ? { hs.f = hi.f } for all other f ? Afhi , 0 ? i ? n do4 /* i.e ., if we are not in case 3 */ if hi.f = c1 then ? c ? ? c ? { hi.f = c1} return ? c /* i.e ., ? c(h0, h1, . . . , hn ) */ In the first step , the constraints corresponding to category attributes are fully determined by the values of these attributes that appear in the semantic molecule heads of ?0, . . . ? n . In Figure 2, when learning the most specific rule r from the representative example laser printer , the set of constraints { h.cat = nc , h1.cat = noun , h2 = noun } ? ? c4 are the constraints corresponding to category attributes . In the second step , the constraints corresponding to variable attributes are fully determined by the variables in the substitution ? that also appear as values of variable attributes hi.vi , hj . vj , where 0 ? i , j ? n and i 6= j . In Figure 2, only { X2/A,X3/A } ? ? will be used , generating the set of constraints { h.head = h1.mod , h.head = h2.head } ? ? c4 .
In the third step , the values of the feature attributes which obey Principles 4 and 5 are generalized ? agr(cs , ci ) is the predicate which gives us the agreement between the categories cs and ci ( e.g ., the subject agrees with the verb ), and agrFeatures(cs , ci ) gives us the set of feature attributes that participate in agreement ( e.g ., nr , pers , case ). In Figure 2, the set of constraints { h.nr = h2.nr } ? ? c4 represents the generalization of the feature attribute values for nr , using Principle 4 . For all features attributes besides the ones that obey the above two principles , the generated constraints keep the particular values of these attributes ( step 4 of Alg 1).
6 Examples
The LWFG formalism allows us to learn grammars for deep language understanding from examples . Instead of writing syntactic-semantic grammar by hand ( both rules and constraints ), we need to provide only a small set of representative examples ? strings and their semantic molecules . Qualitative experiments on learning LWFGs showed that complex linguistic constructions can be learned and covered , such as complex noun phrases , relative clauses and reduced relative clauses , finite and nonfinite verbal constructions ( including , tense , aspect , negation , and subjectverb agreement ), and raising and control constructions ( Muresan and Rambow , 2007). In Figure 3 we show an example of learning a LWFG grammar for noun-noun compounds . The first four examples (14) are representative examples , while the last four examples are used for gener-1. ( laser,0
B
B
B
B
B @ head A mod B
A.isa = laser , B.P1=A
E
C
C
C
C
A ) 5. ( laser printer manual,0
B
B
B
B
B @ head A mod B
C.isa = laser , D.P1=C , D.isa=printer,A.P2=D , A.isa=manual , B.P3=A
E
C
C
C
C
A ) 2. ( laser printer,0
B
B
B
B
B @ head A mod B
C.isa = laser , A.P1=C , A.isa=printer , B.P2=A
E
C
C
C
C
A ) 6. ( desktop laser printer,0
B
B
B
B
B @ head A mod B
C.isa = desktop , A.P1=C , D.isa=laser,A.P2=D , A.isa=printer , B.P3=A
E
C
C
C
C
A ) 3. ( printer,0
B
B
B
B
B @ nr sg head A
A.isa = printer
E
C
C
C
C
A ) 7. ( laser printer manual,0
B
B
B
B
B @ nr sg head A
B.isa = laser , C.P1=B , C.isa=printer , A.P2=C , A.isa=manual
E
C
C
C
C
A ) 4. ( laser printer,0
B
B
B
B
B @ nr sg head A
B.isa = laser , A.P1=B , A.isa=printer
E
C
C
C
C
A ) 8. ( desktop laser printer,0
B
B
B
B
B @ nr sg head A
B.isa = desktop , A.P1=B , C.isa=laser , A.P2=C , A.isa=printer
E
C
C
C
C
A )
B . Learned LWFG Rules:
NA(w , ? h b ? )? Noun(w1 , ? h1 b1 ? ) : ? c1 ( h , h1) , where ? c1 ( h , h1) = < > : h.cat = na h1.cat = noun h.head = h1.head h.mod = h1.mod = > ;
NA(w , ? h b ? )? NA(w1, ? h1 b1 ? ), NA(w2, ? h2 b2 ? ) : ? c2 ( h , h1, h2) where ? c2 ( h , h1, h2) = > >< > > >: h.cat = na h1.cat = na h2.cat = na h.head = h1.mod h.head = h2.head h.mod = h2.mod > >= > > >;
NC(w , ? h b ? )? Noun(w1, ? h1 b1 ? ) : ? c3 ( h , h1) , where ? c3 ( h , h1) = < > : h.cat = nc h1.cat = noun h.head = h1.head h.nr = h1.nr = > ;
NC(w , ? h b ? )? NA(w1, ? h1 b1 ? ), NC(w2, ? h2 b2 ? ) : ? c4 ( h , h1, h2) where ? c4 ( h , h1, h2) = > >< > > >: h.cat = nc h1.cat = na h2.cat = nc h.head = h1.mod h.head = h2.head h.nr = h2.nr > >= > > >; Figure 3: Learning LWFG Rules for Noun-Noun Compounds alization (58). The learned grammar rules , including the learned composition constraints are also shown . The first two LWFG rules ground derive syntagmas for noun adjuncts , while the last two rules ground derive syntagmas for noun compounds . For example , ? desktop laser printer ? can be either a fully-formed noun compound ( category nc ), or it can be further combined with the noun ? invoice ? to obtain ? desktop laser printer invoice ?, case in which it is a noun adjunct ( category na ). The learned rule for noun adjuncts is both left and right recursive , accounting for both left and right-branching noun compounds . Even though we can obtain overgeneralization in syntax , the ontology-based interpretation constraint at the rule level will prune some erroneous parses.
Preliminary results in the medical domain show that ? onto can help remove erroneous parses even when using just a weak ontological model ( semantic roles of verbs , prepositions , attributes of adjectives and adverbs , but no synonymy , or hierarchy of concepts or roles ). However , more experiments need to be run for reporting quantitative results.
7 Conclusions
We have presented the properties and principles that the semantic representation integrated in LWFG requires so that the semantic compositional constraints are learnable from examples.
These properties together with Alg 1 give a theoretical result that in conjunction with the learnability result of Muresan and Rambow (2007) show that LWFG is a learnable constraint-based grammar formalism that can be used for deep language understanding . Instead of writing grammar rules and constraints by hand , one needs to provide only a small set of annotated examples.4 4The author acknowledges the support of the NSF ( SGER grant IIS-0838801). Any opinions , findings , or conclusions are those of the author , and do not necessarily reflect the views of the funding organization.
892
References
Dzeroski , Saso . 2007. Inductive logic programming in a nutshell . In Getoor , Lise and Ben Taskar , editors , Introduction to Statistical Relational Learning . The
MIT Press.
Ge , Ruifang and Raymond J . Mooney . 2005. A statistical semantic parser that integrates syntax and semantics . In Proceedings of CoNLL2005.
Kay , M . 1986. Algorithm schemata and data structures in syntactic processing . In Readings in natural language processing , pages 35?70. Morgan Kaufmann Publishers Inc ., San Francisco , CA , USA.
Lloyd , John W . 2003. Logic for Learning : Learning Comprehensible Theories from Structured Data.
Springer , Cognitive Technologies Series.
Muggleton , Stephen . 1995. Inverse Entailment and Progol . New Generation Computing , Special Issue on Inductive Logic Programming , 13(3-4):245?286.
Muresan , Smaranda and Owen Rambow . 2007. Grammar approximation by representative sublanguage : A new model for language learning . In Proceedings of the 45th Annual Meeting of the Association for
Computational Linguistics ( ACL).
Muresan , Smaranda . 2008. Learning to map text to graphbased meaning representations via grammar induction . In Coling 2008: Proceedings of the 3rd Textgraphs workshop on Graphbased Algorithms for Natural Language Processing , pages 9?16, Manchester , UK , August . Coling 2008 Organizing Committee.
Neumann , Gu?nter and Gertjan van Noord . 1994. Reversibility and self-monitoring in natural language generation . In Strzalkowski , Tomek , editor , Reversible Grammar in Natural Language Processing , pages 59?96. Kluwer Academic Publishers , Boston.
Pollard , Carl and Ivan Sag . 1994. Head-Driven Phrase Structure Grammar . University of Chicago
Press , Chicago , Illinois.
Shieber , Stuart , Hans Uszkoreit , Fernando Pereira , Jane Robinson , and Mabry Tyson . 1983. The formalism and implementation of PATR-II . In Grosz , Barbara J . and Mark Stickel , editors , Research on Interactive Acquisition and Use of Knowledge , pages 39?79. SRI International , Menlo Park,
CA , November.
Shieber , Stuart , Yves Schabes , and Fernando Pereira.
1995. Principles and implementation of deductive parsing . Journal of Logic Programming , 24(1-2):3? 36.
Steedman , Mark . 1996. Surface Structure and Interpretation . The MIT Press.
van Noord , Gertjan . 1993. Reversibility in Natural Language Processing . Ph.D . thesis , University of
Utrecht.
Wong , Yuk Wah and Raymond Mooney . 2007. Learning synchronous grammars for semantic parsing with lambda calculus . In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics ( ACL2007).
Zettlemoyer , Luke S . and Michael Collins . 2005.
Learning to map sentences to logical form : Structured classification with probabilistic categorial grammars . In Proceedings of UAI-05.
Zettlemoyer , Luke and Michael Collins . 2009. Learning context-dependent mappings from sentences to logical form . In Proceedings of the Association for
Computational Linguistics ( ACL?09).
893
