Coling 2010: Poster Volume , pages 828?836,
Beijing , August 2010
A Power Mean Based Algorithm for Combining Multiple
Alignment Tables
Sameer Maskey , Steven J . Rennie , Bowen Zhou
IBM T.J . Watson Research Center
{smaskey , sjrennie , zhou}@us.ibm.com
Abstract
Most existing techniques for combining multiple alignment tables can combine only two alignment tables at a time , and are based on heuristics ( Och and Ney , 2003), ( Koehn et al , 2003). In this paper , we propose a novel mathematical formulation for combining an arbitrary number of alignment tables using their power mean . The method frames the combination task as an optimization problem , and finds the optimal alignment lying between the intersection and union of multiple alignment tables by optimizing the parameter p : the affinely extended real number defining the order of the power mean function . The combination approach produces better alignment tables in terms of both Fmeasure and BLEU scores.
1 Introduction
Machine Translation ( MT ) systems are trained on bitext parallel corpora . One of the first steps involved in training a MT system is obtaining alignments between words of source and target languages . This is typically done using some form of Expectation Maximization ( EM ) algorithm ( Brown et al , 1993), ( Och and Ney , 2003), ( Vogel et al , 1996). These unsupervised algorithms provide alignment links between english words ei and the foreign words fj for a given e?f sentence pair . The alignment pairs are then used to extract phrases tables ( Koehn et al , 2003), hierarchical rules ( Chiang , 2005), or tree-to-string mappings ( Yamada and Knight , 2001). Thus , the accuracy of these alignment links has a significant impact in overall MT accuracy.
One of the commonly used techniques to improve the alignment accuracy is combining alignment tables obtained for source to target ( e2f ) and target to source ( f2e ) directions ( Och and Ney , 2003). This combining technique involves obtaining two sets of alignment tables A1 and A2 for the same sentence pair e ? f , and producing a new set based on union A ? = A1 ? A2 or intersection A ? = A1 ? A2 or some optimal combination Ao such that it is subset of A1 ? A2 but a superset of A1 ? A2. How to find this optimal Ao is a key question . A ? has high precision but low recall producing fewer alignments and A ? has high recall but low precision.
2 Related Work
Most existing methods for alignment combination ( symmetrization ) rely on heuristics to identify reliable links ( Och and Ney , 2003), ( Koehn et al , 2003). The method proposed in ( Och and Ney , 2003), for example , interpolates the intersection and union of two asymmetric alignment tables by adding links that are adjacent to intersection links , and connect at least one previously unaligned word . Another example is the method in ( Koehn et al , 2003), which adds links to the intersection of two alignment tables that are the diagonal neighbors of existing links , optionally requiring that any added links connect two previously unaligned words.
Other methods try to combine the tables during alignment training . In ( Liang et al , 2006), asymmetric models are jointly trained to maximize the similarity of their alignments , by opti-agreement heuristics . In ( Ayan et al , 2004), the authors present a technique for combining alignments based on various linguistic resources such as parts of speech , dependency parses , or bilingual dictionaries , and use machine learning techniques to do alignment combination . One of the main disadvantages of ( Ayan et al , 2004)?s method , however , is that the algorithm is a supervised learning method , and so requires human-annotated data.
Recently , ( Xiang et al , 2010) proposed a method that can handle multiple alignments with soft links which are defined by confidence scores of alignment links . ( Matusov et al , 2004) on the other hand , frame symmetrization as finding a set with minimal cost using use a graph based algorithm where costs are associated with local alignment probabilities.
In summary , most existing alignment combination methods try to find an optimal alignment set Ao that lies between A ? and A ? using heuristics.
The main problems with methods based on heuristics are : 1. they may not generalize well across language pairs 2. they typically do not have any parameters to optimize 3. most methods can combine only 2 alignments at a time 4. most approaches are adhoc and are not mathematically well defined In this paper we address these issues by proposing a novel mathematical formulation for combining an arbitrary number of alignment tables.
The method frames the combination task as an optimization problem , and finds the optimal alignment lying between the intersection and union of multiple alignment tables by optimizing the parameter p of the power mean function.
3 Alignment combination using the power mean Given an english-foreign sentence pair ( eI1, fJ1 ) the alignment problem is to determine the presence of absence of alignment links aij between the words ei and fj , where i ? I and j ? J . In this paper we will use the convention that when aij = 1, words ei and fj are linked , otherwise aij = 0. Let us define the alignment tables we obtain for two translation directions as A1 and A2, respectively . The union of these two alignment tables A ? contain all of the links in A1 and A2, and the intersection A ? contain only the common links . Definitions 1 and 2 below define A ? and A ? more formally . Our goal is to find an alignment set Ao such that | A ?| ? | Ao | ? | A ?| that maximizes some objective function . We now describe the power mean ( PM ) and show how the PM can represent both the union and intersection of alignment tables using the same formula.
The power mean:
The power mean is defined by equation 1 below , where p is a real number in (??,?) and an is a positive real number.
Sp(a1, a2, ..., an ) = ( n ? k=1 apk )
The power mean , also known as the generalized mean , has several interesting properties that are relevant to our alignment combination problem.
In particular , the power mean is equivalent to the geometric mean G when p ? 0 as shown in equation 2 below:
G(a1, a2, ..., an ) = ( n ? i=1 ai ) = lim p?0 ( 1n n ? k=1 apk ) The power mean , furthermore , is equivalent to the maximum function M when p ??: M(a1, a2, ..., an ) = max(a1, a2, ..., an ) = lim p ??( n ? k=1 apk ) Importantly , the PM Sp is a nondecreasing function of p . This means that Sp is lower bounded by G and upper-bounded by M for p ? [0, ?]:
G < Sp < M , 0 < p <?. (4) section when combining multiple alignment tables.
They key insight underpinning our mathematical formulation of the alignment combination problem is that the geometric mean of multiple alignment tables is equivalent to their intersection , while the maximum of multiple alignment tables is equivalent to their union.
Let Aq be an alignment with elements aqij such that aqij = 1 if words ei and fj are linked , and aqij = 0 otherwise . The union and intersection of a set of n alignment tables can then be formally defined as follows:
Definition 1: The union of alignments
A1, A2, ..., An is a set A ? with a?ij = 1 if aqij = 1 for any q ? {1, 2, ..., n}.
Definition 2: The intersection of alignments A1, A2, ..., An is a set A ? with a?ij = 1 if aqij = 1 for all q ? {1, 2, ..., n}.
Figure 1 depicts a simple example of the alignment combination problem for the common case of alignment symmetrization . Two alignments tables , Ae?f and Af?e ( one-to-many alignments ), need to be combined . The result of taking the union A ? and intersection A ? of the tables is shown . A ? can be computed by taking the elementwise maximum of Ae?f and Af?e , which in turn is equal to the power mean Ap of the elements of these tables in the limit as p??.
The intersection of the two tables , A ?, can similarly be computed by taking the geometric mean of the elements of Ae?f and Af?e , which is equal to the power mean Ap of the elements of these tables in the limit as p ? 0. For p ? (0,?), equation 4 implies that Ap has elements with values between A ? and A ?. We now provide formal proofs for these results when combining an arbitrary number of alignment tables.
3.1 The intersection of alignment tables
A1..An is equivalent to their elementwise geometric mean
G(A1, A2, ..., An ), as defined in (2).
Proof : Let A ? be the intersection of all Aq where q ? {1, 2, .., n }. As per our definition of intersection ? between alignment tables , A ? contains links where aqij = 1 ? q.
Let Ag be the set that contains the elements metric mean of the elements aqij where q ? {1, 2, .., n }, as defined in equation 2, that is , agij = (? nq=1 agij ) 1 ? q and zero otherwise , since aqij ? {0, 1} ? q.
Hence Ag = A ?. Q.E.D.
3.2 The union of alignment tables A1..An is equivalent to their elementwise maximum M(A1, A2, ..., An ), as defined in (3).
Proof : Let A ? be the union of all Aq for q ? {1, 2, .., n }. As per our definition of the union between alignments A ? has links where aqij = 1 for some q.
Let Am be the set that contain the elements of M(A1, A2, ..., An ). Let amij be the maximum of the elements aqij where q ? {1, 2, .., n }, as defined in equation (3). The max function is equal to 1 iff aqij = 1 for some q and zero otherwise , since aqij ? {0, 1} ? q . Hence Am = A ?. Q.E.D.
3.3 The elementwise power mean
Sp(A1, A2, ..., An ) of alignment tables
A1..An has entries that are lower-bounded by the intersection of these tables , and upper-bounded by their union for p ? [0, ?].
Proof : We have already shown that the union and intersection of a set of alignment tables are equivalent to the maximum and geometric mean of these tables , respectively . Therefore given that the result in equation 4 is true ( we will not prove it here ), the relation holds . In this sense , the power mean can be used to interpolate between the intersection and union of multiple alignment tables.
Q.E.D.
4 Data
We evaluate the proposed method using an English-Pashto translation task , as defined by the DARPA TransTac program . The training data for this task consists of slightly more than 100K parallel sentences . The Transtac task was designed to evaluate speech-to-speech translation systems , so all training sentences are conversational in nature.
The sentence length of these utterances varies greatly , ranging from a single word to more than
Method Fmeasure
I 0.5979
H 0.6891
GDF 0.6712
PM 0.6984
PMn 0.7276
U 0.6589
Table 1: Fmeasure Based on Various Alignment
Combination Methods 50 words . 2026 sentences were randomly sampled from this training data to prepare held out development set . The held out Transtac test set consists of 1019 parallel sentences.
5 Experiments and Discussion
We have shown in the previous sections that union and intersection of alignments can be mathematically formulated using the power mean . Since both combination operations can be represented with the same mathematical expression , we can search the combination space ? between ? the intersection and union of alignment tables by optimizing p w.r.t . any chosen objective function.
In these experiments , we define the optimal alignment as the one that maximizes the objective function f({aijt }, { a?ijt }, p ), where f is standard Fmeasure , { a?ijt } is the set of all estimated alignment entries on some dataset , { aijt } is the set of all corresponding human-annotated alignment entries , and p is the order of the power mean function . Instead of attempting to optimize the Fmeasure using heuristics , we can now optimize it by finding the appropriate power order p using any suitable numerical optimization algorithm . In our experiments we used the general simplex algorithm of amoeba search ( Nelder and Mead , 1965), which attempts to find the optimal set of parameters by evolving a simplex of evaluated points in the direction that the Fmeasure is increasing.
In order to test our alignment combination formulation empirically we performed experiments on English-Pashto language with data described in Section 4. We first trained two sets of alignments , the e2f and f2e directions , based on GIZA ++ ( Och and Ney , 2003) algorithm . We then combined these alignments by performing intersec-0.5979 for intersection ( I ), 0.6589 for union ( U).
For intersection the Fmeasure is lower presumably because many alignments are not shared by the input alignment tables so the number of links is underestimated . We then also reproduced the two commonly used combination heuristic methods that are based on growing the alignment diagonally ( GDF ) ( Koehn et al , 2003), and adding links based on refined heuristics ( H ) ( Och and Ney , 2003), respectively . We obtained Fmeasure of 0.6891 for H , and 0.6712 for GDF as shown in
Table 1.
We then used our power mean formulation for combination to maximize the Fmeasure function with the aforementioned simplex algorithm for tuning the power parameter p , where Fmeasure is computed with respect to the hand aligned development data , which contains 150 sentences.
This hand aligned development set is different than the development set for training MT models.
While doing so we also optimized table weights
Wq ? (0, 1), ? q Wq = 1, which were applied to the alignment tables before combining them using the PM . The Wq allow the algorithm to weight the two directions differently . We found that the Fmeasure function had many local minima so the simplex algorithm was initialized at several values of p and { Wq } to find the globally optimal
Fmeasure.
After obtaining power mean outputs for the alignment entries , they need to be converted into binary valued alignment links , that is , Sp(a1ij , a2ij , ... anij ) needs to be converted into a binary table . There are many ways to do this conversion such as simple thresholding or keeping best N % of the links . In our experiments we used the following simple selection method , which appears to perform better than thresholding . First we sorted links by PM value and then added the links from the top of the sorted list such that ei and fj are linked if ei?1 and ei+1 are connected to fj , or fj?1 and fj+1 is linked to ei , or both ei and fj are not connected . After tuning power mean parameter and the alignment weights the best parameter gave an Fmeasure of 0.6984 which is higher than commonly used GDF by 2.272% and H by 0.93% absolute respectively . We observe in Figure 2 that even though PM has higher Fmeasure compared with GDF it has significantly fewer number of alignment links suggesting that PM has improved precision on the finding the alignment links . The presented PM based alignment combination can be tuned to optimize any chosen objective , so it is not surprising that we can improve upon previous results based on heuristics.
One of the main advantages of the combining alignment tables using the PM is that our statements are valid for any number of input tables , whereas most heuristic approaches can only process two alignment tables at a time . The presented power mean algorithm , in contrast , can be used to combine any number of alignments in a single step , which , importantly , makes it possible to jointly optimize all of the parameters of the combination process.
In the second set of experiments the PM approach , which we call PMn , is applied simultaneously to more than two alignments . We obtained four more sets of alignments from the Berkeley aligner ( BA ) ( Liang et al , 2006), the HMM aligner ( HA ) ( Vogel et al , 1996), the alignment based on partial words ( PA ), and alignment based on dependency based reordering ( DA ) ( Xu et al , 2009). Alignment I was obtained by using Berkeley aligner as an off-the-shelf alignment tool . We built the HMM aligner based on ( Vogel et al , 1996) and use the HMM aligner for producing Alignment II . Producing different sets of alignments using different algorithms could be useful because some alignments that are pruned by one algorithm may be kept by another giving us a bigger pool of possible links to chose from.
We produced Alignment III based on partial words . Pashto is morphologically rich language with many prefixes and suffixes . In lack of a morphological segmenter it has been suggested that keeping only first ? n ? characters of a word can effectively reduce the vocabulary size and may produce better alignments . ( Chiang et al , 2009) used partial words for alignment training in English and Urdu . We trained such alignments using using GIZA ++ on parallel data with partial words for
Pashto sentences.
The fourth type of alignment we produced , Alignment IV , was motivated by the ( Xu et al , ferent Combination Types 2009). ( Xu et al , 2009) showed that translation between subject-verb-object ( English ) and subject-object-verb ( Pashto ) languages can be improved by reordering the source side of the parallel data . They obtained dependency tree of the source side and used high level human generated rules to reorder source side using precedence-based movement of dependency subtrees . The rules were particularly useful in reordering of verbs that moved to the end of the sentence . Making the ordering of source and target side more similar may produce better alignments for language pairs which differ in verb ordering , as many alignment algorithms penalize or fail to consider alignments that link words that differ greatly in sentence position . A Pashto language expert was hired to produce similar precedence-based rules for the English-Pashto language pair . Using the rules and algorithm described in ( Xu et al , 2009) we reordered all of the source side and used
GIZA ++ to align the sentences.
The four additional alignment sets just described , including our baseline alignment , Alignment V , were combined using the presented PMn combination algorithm , where n signifies the number of tables being combined . As seen on Table 1, we obtained an Fmeasure of 0.7276 which is 12.97% absolute better than intersection and 6.87% better than union . Furthermore PMn , which in these experiments utilizes 5 alignments , is better than PM by 2.92% absolute . This is an encouraging result because this not only shows that we are finding better alignments than intersection and union , but also that combining more than two alignments is useful . We note that PMn performed 3.85% absolute better than H ( Och and Ney , 2003), and 5.64% better than GDF heuristics.
In the above experiments the parameters of the power mean combination method were tuned on development data to optimize alignment Fmeasure , and the performance of several alignment combination techniques were compared in terms of Fmeasure . However , it is not clear how correlated alignment Fmeasures are with BLEU scores , as explained in ( Fraser and Marcu , 2007).
While there is no mathematical problem with optimizing the parameters of the presented PM-based combination algorithm w.r.t . BLEU scores , computationally it is not practical to do so because each iteration would require a complete training phase . To further evaluate the quality of the alignments methods being compared in this paper , we built several MT models based on them and compared the resulting BLEU scores.
E2F Dev Test
I 0.1064 0.0941
H 0.1028 0.0894
GDF 0.1256 0.1091
PM 0.1214 0.1094
PMn 0.1378 0.1209
U 0.1062 0.0897
Table 2: E2F BLEU : PM Alignment Combination
Based MT Model Comparision
We built a standard phrasebased translation system ( Koehn et al , 2003) that utilizes a stack-based decoder based on an A ? search . Based on the combined alignments , we extracted phrase tables with a maximum phrase length of 6 for English and 8 for Pashto , respectively . We then trained the lexicalized reordering model that produced distortion costs based on the number of words that are skipped on the target side , in a manner similar to ( Al-Onaizan and Papineni , 2006). Our training sentences are a compilation of sentences from various domains collected by DARPA , and hence we were able to build interpolated language model which weights the domains differently . We built an interpolated LM for both icantly more monolingual sentences (1.4 million in total ) compared to slightly more than 100K sentences for Pashto . We tuned our MT model using minimum error rate ( Och , 2003) training.
F2E Dev Test
I 0.1145 0.1101
H 0.1262 0.1193
GDF 0.1115 0.1204
PM 0.1201 0.1155
PMn 0.1198 0.1196
U 0.1111 0.1155
Table 3: F2E BLEU : PM Alignment Combination Based MT Model Comparision We built five different MT models based on Intersection ( I ), Union ( U ), ( Koehn et al , 2003) Grow Diagonal Final ( GDF ), ( Och and Ney , 2003) H refined heuristics and Power Mean ( PMn ) alignment sets where n = 5. We obtained BLEU ( Papineni et al , 2002) scores for E2F direction as shown in Table 2. As expected MT model based on I alignment has the low BLEU score of 0.1064 on the dev set and 0.0941 on the test set on E2F direction . Intersection , though , has higher precision , but throws away many alignments , so the overall number of alignments is too small to produce a good phrase translation table . Similarly the U alignment also has low scores (0.1062 and 0.0897) on the dev and test sets , respectively . The best scores for E2F direction for both dev and test set is obtained using the model based on PMn algorithm . We obtained BLEU scores of 0.1378 on the dev set and 0.1209 on the test set which is better than all heuristic based methods . It is better by 1.22 absolute BLEU score on the dev set and 1.18 on a test compared to commonly used GDF ( Koehn et al , 2003) heuristics . The above BLEU scores were all computed based on 1 reference.
Note that for the e2f direction PM , which combines only 2 alignments , is not worse than any of the heuristic based methods . Also note that the difference in the BLEU score of PM and PMn is quite large , which indicates that combining more than two alignments using the power mean leads to substantial gains in performance.
Although we saw significant gains on E2F di-
Type PT Size (100K)
I 182.17
H 30.73
GDF 27.65
PM 60.87
PMn 25.67
U 24.54
Table 4: E2F Phrase Table Size rection we did not see similar gains on F2E direction unfortunately . Matching our expectation Intersection ( I ) produced the worse results with BLEU scores of 0.1145 and 0.1101 on the dev and test set respectively , as shown in Table 3. Our PMn algorithm obtained BLEU score of 0.1198 on the dev set and 0.1196 on test set which is better by 0.83 absolute in dev set over GDF . On the test set though performance between PMn and GDF is only slightly different with 0.1196 for PMn and 0.1204 for GDF . The standard deviation on test set BLEU scores for F2E direction is only 0.0042 which is one third of the standard deviation in E2F direction at 0.013 signifying that the alignment seems to make less difference in F2E direction for our models . One possible explanation for such results is that the Pashto LM for the E2F direction is trained on a small set of sentences available from training corpus while English LM for F2E direction was trained on 1.4 million sentences . Therefore the English LM , which is trained on significantly more data , is probably more robust to translation model errors.
Type PT Size (100K)
I 139.98
H 56.76
GDF 22.96
PM 47.50
PMn 21.24
U 20.33
Table 5: F2E Phrase Table Size
Note that different alignments lead to different phrase table ( PT ) sizes ( Figure 2). The intersection ( I ) method has the least number of alignment links , and tends to produce the largest phrase tables , because there are less restrictions on the on the other hand , tends to produce the least number of phrases , because the phrase extraction algorithm has more constraints to satisfy . We observe that PT produced by intersection is significantly larger than others as seen in Tables 4 and 5. The PT size produced by PMn as shown in Table 4 is between I and U and is significantly smaller than the other heuristic based methods . It is 7.1% smaller than GDF heuristic based phrase table . Similarly in F2E direction as well ( Table 5) we see the similar trend where PMn PT size is smaller than GDF by 4.2%. The decrease in phrase table size and increase in BLEU scores for most of the dev and test sets show that our PM based combined alignments are helping to produce better MT models.
6 Conclusion and Future Work
We have presented a mathematical formulation for combining alignment tables based on their power mean . The presented framework allows us to find the optimal alignment between intersection and union by finding the best power mean parameter between 0 and ?, which correspond to intersection and union operations , respectively . We evaluated the proposed method empirically by computing BLEU scores in English-Pashto translation task and also by computing an Fmeasure with respect to human alignments . We showed that the approach is more effective than intersection , union , the heuristics of ( Och and Ney , 2003), and the grow diagonal final ( GDF ) algorithm of ( Koehn et al , 2003). We also showed that our algorithm is not limited to two tables , which makes it possible to jointly optimize the combination of multiple alignment tables to further increase performance.
In future work we would like to address two particular issues . First , in this work we converted power mean outputs to binary alignment links by simple selection process . We are currently investigating ways to integrate the binary constraint into the PM-based optimization algorithm . Second , we do not have to limit ourselves to alignments tables that are binary . PM based algorithm can combine alignments that are not binary , which makes it easier to integrate other sources of information such as posterior probability of word translation into the alignment combination framework.
7 Acknowledgment
This work is partially supported by the DARPA TRANSTAC program under the contract number of NBCH2030007. Any opinions , findings , and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA.
References
Al-Onaizan , Yaser and Kishore Papineni . 2006. Distortion models for statistical machine translation . In
Proceedings of ACL.
Ayan , Necip , Bonnie J . Dorr , , and Nizar Habash.
2004. Multi-align : Combining linguistic and statistical techniques to improve alignments for adaptable mt . In Proceedings of the 6th Conference of the Association for Machine Translation in the Americas.
Brown , P ., V . Della Pietra , S . Della Pietra , and R . Mercer . 1993. The mathematics of statistical machine translation : parameter estimation . Computational
Linguistics , 19(2):263?311.
Chiang , David , Kevin Knight , and Samad Echihabi.
2009. In Presentation at NIST MT 2009 Workshop,
August.
Chiang , David . 2005. A hierarchical phrasebased model for statistical machine translation . In Proceedings of ACL.
Fraser , Alexander and Daniel Marcu . 2007. Measuring word alignment quality for statistical machine translation . Comput . Linguist ., 33(3):293?303.
Koehn , Philipp , Franz Josef Och , and Daniel Marcu.
2003. Statistical phrasebased translation . In Proceedings of HLT/NAACL.
Liang , Percy , Ben Taskar , and Dan Klein . 2006.
Alignment by agreement . In Proceedings of ACL.
Matusov , Evgeny , Richard Zens , and Hermann Ney.
2004. Symmetric word alignments for statistical machine translation . In Proceedings of COLING , page 219, Morristown , NJ , USA.
Nelder , JA and R Mead . 1965. A simplex method for function minimization . The Computer Journal 7: 308-313.
Och , F . J . and H . Ney . 2003. A systematic comparison of various statistical alignment models . Computational Linguistics , 29(1):19?51.
835
Och , Franz J . 2003. Minimum error rate training in statistical machine . In Proceedings of ACL.
Papineni , Kishore , Salim Roukos , Todd Ward , and Wei jing Zhu . 2002. Bleu : A method for automatic evaluation of machine translation . In In Proceedings of
ACL , pages 311?318.
Vogel , Stephan , Hermann Ney , and Christoph Tillmann . 1996. Hmm-based word alignment in statistical translation . In COLING 96: The 16th Int . Conf.
on Computational Linguistics , pages 836?841.
Xiang , Bing , Yonggang Deng , and Bowen Zhou . 2010.
Diversify and combine : Improving word alignment for machine translation on low-resource languages.
In Proceedings of ACL.
Xu , Peng , Jaeho Kang , Michael Ringgaard , and Franz Och . 2009. Using a dependency parser to improve smt for subject-object-verb languages . In NAACL , pages 245?253, Morristown , NJ , USA.
Yamada , Kenji and Kevin Knight . 2001. A syntaxbased statistical translation model . In Proceedings of ACL , pages 523?530, Toulouse , France , July.
ACL.
836
