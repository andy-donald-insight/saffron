ANEXPERIMENTON"""Lb,\.RNINCAPPROPRIATE
SELECTION ALRESTRICTIONS I?IOM APARSED CORPUS
1, ' r ~ mccscl\[ibasl?ramis *
Departament de Llenguatges i Sist ; emes Inform ~ ttics , Universitat ; Polit Scnica de Catalunya
Pau Gargallo 5, 08082 Barcelona , SPAIN . eq naihribas  Qlsi . upc . es

We present a methodology to extract Selectional Restrictions at a variable level of abstraction from phrasally analyzed corpora  . The method relays ia the use of a wide-coverage noun taxonomy and a statistical measure of the cooccurrence of linguistic items  . 
Some experimental results about the performance of the method are provided  . 
Keywords : large text corpora , computational lexicons 1 INTI~ODUCTION These last years there has been a common agreement in the natural anguage processing research community on the importance of having an extensive coverage of the surface lexical semantics of the domain to work with  , (specially , typical contexts of use ) . ' iF his knowledge may be expressed at different levels of abstraction depending on the phenomena involved : selec Lional re~strictions  ( SR  ~ )  , lexical preferences , eel-locations , etc . 
We are specially interested on SIhs , which can be ex- . 
pressed as semantic type constraints t/tata word se~seim poses on the words with which it combines in the process of semantic interpretation  . SR snmstiachl cle information on the syntactic position of the words thai  ; are being restricted semantically . For instance , one of the senses of the verb drink restricts its subject ~ o be an animal and its object to be a liq ' uid  . 
SILs may help a parser to prefer some parses among several grammatical ones \[  WFB90\]  . Furthermore , S\]s may help the parser when deciding tilts ( -mantic role played by a syntactic complement . Lexicography is also interested in the acquisition of SlCs  . On the one hand , SRs are an interesting inform ~ xtion to be im : Imied in dictionaries  ( defining in co~dezt approach )  . Oath (; other band , ; m\[ClI90\] remark , the e\[lbrt involved ht all -* This research as been supported by a grant conceded by the Generalitat de Catahmya  , 91-1) OGC-1 , 191 . Much of the work reported here wa-n carried out , during a visit , at . the Cl ) tllptt\[el ' , ab-oratory , University of Cambridge . 11 amgrate fult , oTed Briscoe and t\[oracloRod riguez by their vMuab\[eeo~tHlleltt  . s . 
alyzing and cl~ussifying all tile linguistic material provided by concordances of use of a word can be extremely labor-intensiw  .  ~ . If it was possible to represent roughly the Sits of the word being studied  , it could be possible to clmssify roughly the concordances automatically in the different word uses before the lexicographer analysis  . 
The possible sources of Sits are : introspection by lexicographers  , machine-readable dictionaries , ~ n do n -- line corpora . ' l ' he main advantage of the latter is that they provide experimental evidence of words uses  . tl . e--cently , several approaches on acquiring different kinds of lexical information from corpora have been developed\[  BPV92  , CG\[III91 , CH90 , Res92\] . This paper is interested in exploring the a menability of using a method f  ( ~ rextracting SI ~ from textual data , in the line of these works . The aim of the proposed technique is to learn the Sl  ~  , s that a word is imposing , from the analysis of the examples of use of that word con-taim'd in the corpus  . An illustration of such a learning is shown in Figurel  , where the system , departing from the three examples of use , and knowing that prosec'utor , buyer att dlaw make rare nouns belonging to the semantic lass < pera-m ~  , individual > , and that i ~ z dict me ~ d , assura ~ zce and legislation are members of < legal_in  . strttmeu Z >, should induce that the verb see/ . ' imposes SILs that constrain the subject to be a nmm-  . 
bet of the semantic type < pera on , individval > , and the object to be a kind of < legaLius trurnent  >  . Con-eluding , the system should extract for each word ( with contpleme ut ,  . .~ ) having enough number occurrences of use in the corpus and for each of its syntactice omple -melttS  , ali: ; t of the alternative Sl~s that this word is imposing  . 
In order to detect the SRs that a word imposes in i l  ; scout exthy means of statistical techniques two distiuct approaches haw ~  , been proposed : word-based\[CC , HIIgl\] , and class . .based \[ Bpv92, ~ tes92\] . Word- . 
based al~proach infers SRs as the collection of words that cooccur significantly in the syntactic context of the studi  , ' . dword . The clms s--based techniques gather the ; dillhrene nouns by means of semantic l , ' uqses . The advantages of the latter are clear . On the one hand , sl . atist . ically meaningful data can be gathered From ( tel--verb seek from three examples of use ? Three examples of use prosecutors may so on seek an indictment on rack-eteering and securities fraud charges  . 
In the recent past , bondbuyers didn't seek such assurance . 
Some law makers may seek legislation to limit overly restrictive insurance policies  . 
? The extracted SILs ( seek , subject , < person , individual >) ( seek , object , < legal_instrument >) atively ) small corpora , and not only for the most frequent words . On the other hand , SRs are generalized to new examples not present in the training set  . Fi-nMly , the acquired SRs are more independent of the lexical choices made in the training corpus  . 
We have developed and implemented a method for automatically extracting classbased SRs from online corpora  . In section 2 we describe it while discussing other approaches . In section 3 we analyze some data about the performance of an experiment run in a Unix machine  , on a corpus of 800 , 000 words . Finally , in section 4 we discuss the performance achieved , and suggest further refinements of the technique in order to solve some remaining problems  . 
2 THE METHOD OF ACQUISITION
SRs have been used to express semantic constraints holding in different syntactic and functional configurations  . However , in this paper we focus only in selectional restrictions holding between verbs and their complements  . The method can be easily exported to other configurations  . We won't distinguish the SiTs imposed by verbs on arguments and adjuncts  . We believe that few adjuncts are going to provide enough evidence in the corpus for creating SRs  . In the following paragraphs we describe the functional specification of the system  . 
Training set The input to the learning process is a list of cooccurrence triples codifying the cooccurrence of verbs and complement heads in the corpus:  ( verb , syntaclic relationship , noun ) . Verb and noun are the lemrn as of the inflected forms appearing in text  . Syntactic relationship codes the kind of complement :  0subject   , I object , or preposition in case it is a PP . A method to draw the cooccurrence triples from corpus is proposed in subsection  2  . 1 . 
Output The result of the learning process is a set of syntactic SP as  , ( verb , syntactic relationship , semantic class . Semantic classes are represented extensionally as sets of nouns  . SR a are only acquired if there are enough cases in the corpus as to gather statistical evidence  . As long as distinct uses of the same verb can have different SRs  , we permit to extract more than one class for the same syntactic position  . Nevertheless , they must be umtually disjoint , i . e . not related by hyperonymy . 
Previous knowledge used In the process of learning SRs  , the system needs to know how words are clustered in semantic classes  , and how semantic classes are hierarchically organized  . Ambiguous words must be represented ms having different hy-peronym classes  . In subsection 2 . 2 we defend the use of ab'road-coverage taxonomy . 
Learning process The computational process is divided in three stages :  ( 1 ) Guessing the possible semantic lasses , i . e . creation of the space of candidates . In principle , all the hyperonyms ( at all levels ) of the nouns appearing in the training set are candidates  .   ( 2 ) Evaluation of the appropriate ~ hess of the candidates  . In order to compare the different candidates , art statistical measure sum-ma . rizing the relevance of the occurrence of each of the candidate classes is used  .   ( 3 ) Selection of the most appropriate subset of the candidate space to convey the SILs  , taking into account hat the final classes must be mutually disjoint  . While in subsection 2 . 3 an statistical measure to flflfill stage 2 is presented , stages 1 and 3 are discussed in 2 . 4 thoroughly . 
2 . 1 Ext rac t ing Cooccur rence Tr ip les In any process of learning from examples the accuracy of the training set is the base for the system to make correct predictions  . In our case , where the semantic classes are hypothesized not univoquely from the ex~staples  , accuracy becomes fundamental . 
Different approaches to obtain lexical co-occurren -ces have been proposed in the literature \[  BPV92  , CGHH 91 , CH90\] . These approache seem inappropriate for tackling our needs  , either because they detect only local co-occurrences\[CGHtI9i   , CtI90\] , or because they extract many spurious cooccurrence triples\[  BPV92  , Clt90\] . On the one hand , our system in tends to learn SRs on any kind of verb's complements  . 
On the other hand , the fact that these approaches extract cooccurrences without reliability on being verb -complements violates accuracy requirements  . 
However , if the cooccurrences were extracted from a corpus annotated with structural syntactic information  ( i . e . , part of speech and " skeletal " trees ) , the results would have considerably higher degrees of accu-to detect all tile relationships between verb and con > plements  , and few nonrelated cooccurrences would be extracted  . ' rile most serions objection to this approach is that the task of producing syntactic analyzed corpora is very expensive  . Nevertheless , lately there has been a growing interesto produce skeletally analyzed corpora  1 A parser , with some simple heuristics , won I ( 1 be enough to meet the requirements of representativeness and accuracy introduced above ? On the other band  , it could be useful to represent he cooccurrence triples as holding between lemmas  , in order to gather ~ much evidence as possible . A simple morphological nalyzer that could get the lemma for a big percentage of tile words appearing in the corpus would suffice  . 
2.2 Semantic Knowledge Used
Of the two classbased approaches presented in section  1  ,  \[  Res92\]'s technique uses a wide-coverage semantic taxonomy , whereas \[ BPV92\] consists in hand-tagging with a fixed set of semantic labels  . The advantages and drawbacks of both approaches are diverse  . On the one hand , in\[BPV92\] approach , semantic lassea relevant o the domain are chosen , and consequently , the adjustment of the classes to the corpus is quite nice  . 
Nevertheless ,  \[  I'~es92\]'s sytem is less constr~ined and is able to induce a most appropriatelvel for the SRs  . On the other hand , while \[ BPV92\] implies hand-coding all the relevant words with semantic tags  ,  \[\[ . es92\] needs a broad semantic taxonomy . I Iowever , there is already an available taxonomy , WordNet 2 . We take ires92\] approach because of the better results obtained , and the lower cost involved . 
2  . 3 C lass appropr ia teness : the Assoc i - a t ion Score When trying to choose a measure of the appropriate -  . 
hess of a semantic lass , we have to consider the features of the problem :   ( 1 ) robustness in front of noise , and ( 2 ) conservatism in order to be able to generalize only front positive examples  , without having the tendency to overgeneralize . 
Several statistical measures that accomplish these requirements have been proposed in the literature  \[   BPV92  , CGIItI91 , les92\] . We adopt \[ Res92\]'s approach , which qnantifies tile statistical association X For instance  , Penn Treebank Corpus , which is belug collected and anayzed by the University of Penl ~ sylwmia  ( sec\[MSM93\] )  . The material is available on request , from the l , in-guistic Data Consortium , ( email ) ldc@unagi . cis . upenn . ed , tt = WordNet is a lexieal data b . ~ e developed with psycholitt-guist l caims . it represents lexiea \[ scnlantics in fl ) t ' In at lonabout nouns , verbs , adjectives and adverbs such as hypevonyms , meronyms ,   . . . it presently contains information on about 83 , 000 lemt n , ' ~ q . See\[MBF+ 90\] between verbs and classes of nouns from their co -occurrenee  . I Iowever we adapt it taking into account tile syntactic position of the relationship  . Let v = , , .   .   .   .   . ~,,, : ?--, u .   .   .   .   . , , , , , , a = 0, 1, , 0, ~ t .   .   .   .  , ~,  . tc = ~1 ? cN betim sets of all verbs , nouns , syntactic positions , and possible noun classes , respectively . Given vEV , sE '; and cGC , Association Score , Assoc , between van de in a syntactic positions is defined to be A ~  . ~ oc(~, . , ~) =_ P(e / ,   ,   , ~) ~(~; ~/~) = 1"(cI , , , s ) log = e( , , , c/s ) Where conditional probabilities are estimated by counting the number of observations of tile joint event and dividing by the frequency of the given event  , e . g  ~ , , ece o , . ~ t ( . , s ,   , , ) The two terms of Assoctry to capture different properties of the SI expressed by the candidate class  . Mutual information , \[( v ; c/s) , measures the strength of the statistical association between the given verb v and the candidate class c in the given syntactic position s  . If there is a real relationship , then hopefully l(v , c/s ) > > 0 . On the other hand , the conditional probability , P ( e/v , s ) , favors those classes that have more occurrences of norms  . 
2.4 Selecting the best classes
The existence of noise in the training set introduces classes in tile candidate space that can't be considered as expressing SILs  . A common technique used for ignoring , as farms possible this noise is to consider only those events that have a higher mnnber of occurrences than a certain threshold  . I lowever , some erroneous classes may persist because they exceed the threshold  . However , if candidate classes were ordered by the significance of their Assoc with the verb  , it is likely that less appropriate classes ( introduced by noise ) would be ranked in the last positions of the candidate llst  . 
'\]' he algorithm to learn SIts is based in a search through all the ckmses with more instances in the training set than the given threshold  . In different iterations over ~ hese candidate classes  , two operations are per-fol:med : first , the class , c , having the best Assoc ( best class ) , is extracted for titile final result ; and second , the remaining candidate classes are filtered from classes being hyper/hyponyms to the best class  . This last step is made be canse the definitive classes must be mutually disjoint  . The iterations are repeated until the candidate space has been runotlt  . 

Table 1: SRs acquired for the subject of seek
Acquired SR-~soc#n#s < cognition > Senses I  -0  . 04 < activity > SensesI ~0 . 01 < status > Senses 0 . 087 < social_control > Senses 0 . 111 < administrative_district > Senses 0 . 14 < city > Senses 0 . 15 < radical > Senses 0 . 16 < person , individual > Ok 0 . 23 < legal_action>Ok 0 . 28 < gro~p > ~ Abs .  0 . 35 < suit > Senses 0 . 40 < suit_of_clothes > Senses 0 . 41 < suit , suing > Senses 0 . 41 5 1 6 1 5 0 6 0 36 0 36 0 5 0 61 38 7 6 64 46 7 0 7 0 7 0
Examples of nouns in Treebank concern , leadership , provision , science administration , leadership , provision government , leadership administration , government proper_name proper_name group advocate  , buyer , carrier , client , company ,   . . . 
suit administration , agency , bank, . . . , group, . . . 
suitsuitsuit\[ Res92\] performed a similar learning process , but while he was only looking for the preferred class of object nouns  , we are interested in all the possible closes ( SRs )  . He performed a bestfirst search on the candidate space  . I to we ver , if tile function to maximize doesn't have a monotone behavior  ( as it is the c~e of Assoc ) the bestfirst search doesn't guarantee global optimals  , but only local ones . This fact made us to decide for a global search , specially because the candidate space is not so big  . 
3EXPERIMENT ALIESULTS
In order to experiment the methodology presented , we implemented a system in a Unix machine . The corpus used for extracting cooccurrence triples is a fragment of parsed material from the Penn Treebank Corpus  ( about 880 , 000 words and 35 , 000 sentences ) , consisting of articles of the Wall Street Journal  , that has been tagged and parsed . We used Wordnet ~ the verb and noun lexicons for the lemmatizer  , and also as the semantic taxonomy for clustering nouns in semantic classes  . In this section we evaluate the performance of the methodology implemented :  ( 1 ) looking at the performance of the techniques used for extracting triples  ,   ( 2 ) considering the coverage of the WordNet taxonomy regarding the noun senses appearing in Treebank  , and ( 3 ) analyzing the performance of the learning process . 
Tile total number of cooccurrence triples extracted amounts to  190  , 766 . Many of these triples (68, 800, 36 . 1% ) were discarded before tile lemmatizing pro~tess because the surface NP head was n't a noun  . 
The remaining 121 , 966 triples were processed through the lemmatizer .  113,583 (93 . 1% ) could be correctly mapped into their corresponding lemma\[\  ) rm . 
in addition , we analyzed manually the results obtained for a subset of tile extracted triples  , looking at the sentences in the corpus where they occurred  . 
The subset contains 2 , 6 58 examples of four average common verbs in the Treebank : rise  , report , seek and present ( from now on , tile testing sample ) . On the one hand , 235(8 . 8% ) of these triples were considered to be extracted erroneously because of the parser  , and 51 (1 . 9%) because of the lemmatizer . Summarizing , 2, 372 (89 . 2% ) of the triples in the testing set were considered to be correctly extracted and lemmatized  . 
When analyzing the coverage of WordNet taxonomy a we considered two different ratios  . On the one hand , how many of the noun occurrences have one or more senses included in the taxonomy :  113  , 583 of the 117 , 215 extracted triples (96 . 9%) . On the other hand , how many of the noun occurrences in the testing sample have the correct sense introduced in the taxonomy :  2  , 615 of the 2372 well-extracted triples (8 . 7%) . These figures give a positive evaluation of the coverage of 

In order to evaluate the performance of the learning process we inspected manually the SRs acquired on the testing-sample  , a . ssessing if they corresponded to the actual Sits imposed  . A first way of evaluation is by means of meazuring precision and recall ratios in the testing sample  . Inoure ~ e , we define precision as the proportion of triples appearing in syntactic positions with acquired SRs  , which effectively fififill one of those SRs . Precision amounts to 79 . 2% . The remaining 20 . 8% triples didn't belong to any of the classes induced for their syntactic positions  . Some of them because they didn't have the correct sense included in the WordNet taxonomy  , and others because tile correct class had not been induced because there was n't  3The information of proper nouns in WordNet is poor . For this reason we assign four predel\]ned classes to them : < person  , individual > , < organization : > , < adm't ~ i a trgtive_di . strict 2 > etll(I < : city : > . 
772 enough evidence . On the other hand , we dellne recall as the proportion of triples which fnlfill one of tile SRs acquired for their corresponding syntactic positions  . Recall an rounts to 75 . 7% . 
A second way of evaluating the performance of t , heabstraction process is to manually diagnose the reasons that have made the system to deduce the SRs obtained  . 
Table 1 shows the SILs corresponding to the subject position of the verb seek  . Type indicates the diagnostic about the class appropriateness  . Assoc , the value of the association score . " #n ' , tile number of nouns appearing in the corpus that are contained in the clmss  . 
Finally , " ~ s "" indicates the number of actual noun senses used in the corpus which are coutained in the class  . In this table we can see some examples of the five types of manual diagnostic : Ok The acquired SR  . is correct according to the noun senses contained in the corpus  . 
~ Abs The best level for stating the SI is not the one induced  , but a lower one . It happens becauser-roneous senses , irletolly Inies ,   . . . jaccumulate vi-dence for the higher class . 
gAbsSome of the Slks could be best gathered in a unique class  . We didn't lind any such case . 
Senses The class has cropped up because it accumulates enough evidence  , provide . d by erroneous senses . 
Noise The class accumulates enough evidence provided by erroneously extracted triples  . 
Table 2 shows the incidence of the diagnostic types in the testing sample  . Each row shows : the type of diagnostic , the num her and l ) ercert tage of classes that accomplish it , and then mnl ) er and percentage of noun occurrences contained by these classes in tile testing sample  4  . Aualyzing the results obtained from the testing sample  ( some of which are shown in tables 1 and 2 ) we draw some positive ( a , e ) and some negative conclusions ( b , c , d and /): a . Almost one correct semantic lass t breach syntac . 
ticposition in the sample is acquired . The technique aehicves a good coverage , even with few co--occurrence triples . 
b . Although many of the classes acquired result Dora tile accumulation of incorrect senses  ( 73 . 3%), it ; seems that their size . tends to be smaller than cl~usses in other categories  , an they only contain a 51 , 4% of tim senses . 
' lthlstotal doesn't equal the numher of tr iples in the testing sample because tile same  1t  ( 2 , 1 11 i ilg tybelong to IllOFQ thaliolll class in the f in  ; dSIls
Table 2: Summary of the Sits acquired
Diagnostic  #Clas ~' es ~ - - - - ~- n ~ - - - - ~- -
Ok 45 lAbs 7 liAbs0
Senses 176
Noise 12
Term 2402 . 9362 I6 . 8 0 . 00 I 0 . 0 73 . 3 2,7401 51 . 4 5 . 0130 I2 . 4\[ 00 . 0e . There doesn't seem to be a clear co-relation between As set and the manual diagnostic  . Specifically , the classes considered to be correct sometimes aren't ranked in t  ; he higher positions of tile Asset ( e . g . ,
Tablel).
( t . Tim overgeneralization seems to be produced because of little difference in the nouns included in the rival closes  . Nevertheless this situation is rare . 
e . The impact of noise provided by erroneous e?trac -tion of cc~-occurrence triples  , iu the acquisition of wrong semantic lasses , seems to be very moderoate . 
( . Since difl hrcnt verb senses occur in the corpus , the
SI~acquired appear mixed.
4\]?'Uf\[THEKV~r0Pd(
Although perfornmnce of thc technique presented is pretty good  , some of the detected problems could poses ibly be solved  . Specifically , there are various ways to explore illorder to re . dacetile problem stated in points b and c above :   1  . 

To measure the Assoe by means of Mutuallnt br -marion between the pair v-s and c  . In this way , tim syntactic position also wouhl provide iutbrma -lion  ( statistical evidence ) for measuring the most appropriate classes . 
To modify timAs set in such a way that it was based in a likelihood ratio test\[  Dun93\]  . It seems that this kind of tests have a better performance than nmtual in l'ormation when the counts are sma  . ll , ~ mit is the case . 
3 . To estimate the probabilities of classes , not di+rectly from the frequencies of their liouumen > bets  , but correcting this evidence by the number of senses of those nouns  , e . g#~ertJes ~, t ~ Cl '( ~ ls ) ~ . ) -'" coco . ~, uv , . ~, m  ~, .  + -,- . v , J - - - " - - ' ~ a probability distribution , and more interesting , nouns would provid evidence on the occurrence of their hyperonyms  , inversely proportional to their degree of ambiguity  . 
4 . To collect a bigger number of examples for each verbal complement  , projecting the complements in the internal arguments  , using diathesisub-categorization rules . Hopefully , Assoc would have a better performance if it was estimated on a bigger population  . On the other hand , in this way it Would be possible to detecthe SRs holding on internal arguments  . 
In order to solve point d above , we have fore seen two possibilities : 1 . To take into consideration the statistical significance of the alternatives involved  , before do ing a generalization step , climbing upwards ,  2 . To use the PPs that in the corpus are attached to other complements and not to the main verb as a source of " implicit negative xamples "  , in such a way that they would constrain the over generalization  . 
Finally , It would be interesting to investigate he solution to point flOne possible way would be to disambiguate the senses of the verbs appering in the corpus  , using the SRs already acquired and gathering evidence of the patterns corresponding to each sense by means of a technique similar to that used by \[  Yar92\]  . Therefore , once disambiguated the verb senses it would be possible to split the set of SRs acquired  . 
\[BPV92\]\[CGHH91\]\[cH9o\]\[Dun93\]

P ~. Basili , . M.T . Pazienza , and P . Velardi.
Computational lexicons : the neat examples and the odd exemplars  . In Proc . of lhe3rd
ANLP , 1992.
K.W . Church , W . Gale , P . Hanks , and
D . It indle . Using statistics in lexicai analysis . In U . Zernik , editor , Lexicat Acquisition : Exploiting OnLine Resources to Build a Lexicon  . Lawrence Erlbaum , 1991 . 
K . W . Church and P . Hanks . Word association norms , mutual information and lexicography . Computational Linguistics , 16(1), 1990 . 
T . Dunning . Accurate methods for the statistics of surprise and coincidence  . Computational Linguistics , 19(1), 1993 . 
\ [MBF+90\]  \[  MSM93\]   \[1%es92\]  \[  WFB90\]  \[  Yar92\] 
G . Miller , R . Beckwith , C . Fellbaum,
D . Gross , and K . Miller . Five papers on wordnet . Technical report , CSL , Princeton
University , 1990.
Mitchell P . Marcus , Beatrice Santorini , and Mary Ann Marcinkiewicz . Building a large annotated corpus of english : the 
Penn Treebank . Computational Linguistics , 19(2), 1993 . 
P . Resnik . Wordnet and distributional analysis : A classbased approach to lexical discovery  . In Proc . of AAAI Workshop on Statistical Methods in NLP ,  1992 . 
G . Whittemore , K . Ferrara , and II . Brun-net . Empirical study of predictive pow-ers of simple attachment schemes for postmodifier prepositional phrases  . In Proc . of the 28th ACL , 1990 . 
David Yarowsky . Word-sense disambiguation using statistical models of Roger's categories trained on large corpora  . In Proceedings of COL\[NG-92 , Nantes , France ,  1992 . 

