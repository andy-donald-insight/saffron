Robust parsing of severely corrupted spoken ut terances 
Egidio P . Giachin Claudio Rullent
CSELT-Centro Studie Laboratori Telecomunicazioni 
Via Reiss Romoli274, Torino , Italy-Ph .  439-11-21691

This paper describes a technique for enabling a speech understanding system to deal with sentences for which some monosyllabic words are not recognized  . Such words are supposed to act as mere syntactic markers within the system linguistic domain  . This result is achieved by combining a modified case frame approach to linguistic knowledge representation with a parsing strategy able to integrate expectations from the language model and predictions from words  . Experimental results show that the proposed technique permits to greatly increase the quota of corrupted sentences correctly understandable without sensibly decreasing parsing efficiency  . 
1 Introduction
The problem addressed by this paper is how to make a speech understanding system deal wlth sentences for which some types of words are not recognized  . 
The continuou speech understanding system under development a CSEL Tlaboratories \[ Fissore  88\] is part of a question-answer lng system allowing to extract information from a database using voice messages with high syntactic freedom  . The system is composed of a recognition stage \[ Laface  87\] followed in cascade by an understanding stage . The recognition stage analyze speech using acoustic-phonetic knowledge  . Since utterances are spoken without pauses between words  , it is not possible to uniquely locate words without using syntactic and semantic constraints  . Thus the actual output of the recognition stage is a set : of word hypotheses  , usually called lattice in the literature . A word hypothesis is characterized by its begin and end times  , corresponding to the portion of the utterance in which it has been located  , by a score representing its belief degree , and by the lexeme itself . The understanding stage has the task of analyzing the word lattice using linguistic knowledge and producing a representation of the meaning of the most likely consistent word sequence  . 
A twostage approach to speech understanding offers several advantages and is the most widely followed in the current research  . A serious difficulty , however , lles in the fact that often some short words that were actually uttered are not detected by the recognltion level and hence they are missing from the lattice  . To cope with this problem the understandlng stage must adopt a language representation ada parsing strategy which  1  ) whenever possible , do not rely on such words to understand a sentence  , and 2 ) keep parsing efficiency comparable with the case in which no word is missing  . This paper describes a technique for obtaining such results  . The following is divided into four sections . The next one focuses on the various implications of word undetectlon on the linguistic processing  . Then the linguistic knowledge bases of the understanding system and the parsing strategy are outlined  ( assuming that all words are present in the lattice  )  . Next the technique for coping with missing words is introduced  . Finally , experimental results are discussed , showing that the proposed technique permits to greatly increase the quota of corrupted sentences correctly understandable without sensibly decreasing parsing efficiency  . 
A discussion is also provided relating our results to other works addressing slmilar problems  . 
2 A closer examination of the problem 2 . 1 F rom the acoust i ca l v iewpo int The phenomenology of word undetectlon at the recognition level is somewhat complex but mainly depends on word length  . The dependency on length penalize short words over long ones  ; i it is partly intrinsic to the signal-processing techniques used for recognition  , and also hear ? ily enhanced by coarticulatlon events  . The consequence is that short words are frequently undetected or are given unreliable scores  ; then a standard parsing either would not work or would encounter heavy inefficiencies  . 
There is also an additional problem for continuous  1By ' short ' word we mean a word described by one or two phonetic units  . Phonetic units can be viewed approximately as phonemes \[ Laface  87\]  . 
This work hoJ been partially supported by the " EEC wl bh in the Esprit project  ~6  . 
196 speech . Often short words are erroneously detected and assigned a good score  . That happens frequently when their phonetic representation is also part of a longer word that was actually uttered  . For this reason the efficiency of a traditional parser would be reduce due to the necessity of taking into consideration such nonexistent words  . 
2.2 From the understanding viewpoint
Short words span the wide strange of lexical categories and have various degrees of ' significance '  ( take this term informally )  . Some cannot be eluded and , if they are miss-ingj it is necessary to understand the rest of the sen-fence and to initiate an additional interaction wlth the recognition level trying to figure out the most plausible words among a very limited set glven by the parser  ; if no accept ~ ble word is found , a dialogue with the user may be t ~ tarted , aimed at eliciting the essential information . Both are time-consuming operations ; the lat-terp more over , requires carefuler gonomi considerations\[ Kaplan  87  , \] . However , there are words for which the situation is 1Lot so drastic . This is the case of determiners , prepositions , and auxiliary verbs . 
The ~ ; reatment of words of these categories follow two main guidelines in the literature  . In the former ~ such words act mainly as syntactic markers for multiword semantic on stituents  , without providing an intrinsic semantic ontribution  . This philosophy includes casebased \[ Fillmore 68\] and conceptual-dependency based approaches to natural anguage understanding \[ Schank  75\]  . 
In the latter guideline , such words play an independent role as semantic units and contribute compositionally wlth other words to the global meaning  , with equal dignity \[ Hinrlchs 86 , Lesmo85\] . Clearly , given the specific problem we are addressing , it is mandatory to follow the former guideline . Happily , this commitment is coherent with the preference granted to case framebased parsing coming from different and independent reasons inherent in speech understanding  ( see\[Hayes 86\] for an excellent discussion )  . The peculiar case framebased approach sum-marlzed in the next section provides in most cases the ability of understanding a sentence without relying on such word ~  . 
3 The standard parsing strat-egy 3 . 1 Linguistic knowledge representation Linguistic knowledge representation is based on the notion of cas ~ frame \ [ Fillmore  68\] and is described in detail in \[ Poeslo 871  . Case frames offer a number of advantages h* speech parsing  , hence their popularity in many recent speech understanding systems \[ Hayes  86  , Brietzmann 86\] , but cause two main difficulties . 
First , the analysis cannot be driven by case markers , as is the case with written language , since often case mark-erear ejustt ! lose kinds of short words that are unreliably recognized or undetected at all  . The standard approach is to assign to case headers the leading role  , that is to instantlate case frames using word hypotheses to fill their header slot and subsequently to try to expand the case slots  . This strategy induces parsing to proceed in a topdown fashion  , and works satisfactorily when headers are among the best-scored lexical hypotheses  . However , it can be shown \[ Gemello 87\] to cause severe problems if there is a bad-scored but correct header word  , because the corresponding case frame in et antiation will not be resumed until all of the caseffames having better-scored but false header words have been processed  . The situation of headers with bad scores happens quite frequently  , especially when the uttered sentences suffer from strong local corruption due to coartlculatlon phenomena or environmental noise  . Moreover , the standard strategy does not exploit the fact , dual to the one previously outlined , that some word hypotheses , though not being headers , have a good and reliable score . An integrated top-down/bottom-up strategy , able to exploit the predictive power of non -header words  , is mandatory in such situations . 
A second difficulty is given by the integration of case frames and syntax  . This is due to two conflicting requirements . From one side , syntax should be defined and developed as a declar at lve knowledge base independently from case frames  , ince this permits to exploit syntactic formalisms ~t the best and in s  , i resease of maintenance when the linguistic domain has to be expanded or changed  . On the other hand , syntactic on straints should be used together with semantic ones during parsing  , because this reduces the size of the inferential activity  . 
To overcome these problems ~ case frames and syntactic rules are pro-compiled into structures called Knowledge Source e  ( KSs )  . Each KS owns the syntactic and semantic competence necessary to perform a wellformed interpretation of a fragment of the input  . Fig . 1 shows a simple case frame , represented via Conceptual Graphs \[ Sown84\] , and a simplified view of the resulting KS obtained by combining it with two rules of a Dependency Grammar \] Hays  64\]  . The dependency rules are augmented wlth information about the functional role of the immediate constituents  ; this information id used by the offline compiler as a mapping between syntax and semantics necessary to automatically generate the KS  . The KS accounts for sentences like " Daquale monte haste  i1 Te-vere ? " ( " From which mount does the Tevere originate ?" )  . 
The Composition part represents a way of grouping a phrase having a MOUNT type header satisfying the Activation Condition and a phrase having a RIVER type header  . The Constraints part contains checks to be performed whenever the KS is operating  . The Meaning part allows to generate the meaning representation starting\[TO-HAVE-SOURCE\]-- *   ( AGNT : Oompulsory ) --+ RIVER -- ~ ( LOC : Compulsory ) -- ~\[ MOUNT\]

VERB ( prop ) = NOUN ( interr-ind lr-loe ) <GOVERNOR>NOUN ( subJ )   ; ; Features and Agreements < GOVERNOR > ( MOOD ind )   ( TENSE pres )   ( NUMBER_x )   . . . .
NOUN-I ...
NOUN-2 ( NUMBER_x )....

VERB ( prop ) = NOUN ( interr-indir-loc ) <GOVERNOR > PROP-NOUN ( ~ubJ ) 
DeIKSKS-2 4.12;; Composition
TO-HAVE-SOURCE=MOUNT < HEADER>RIVER ; ; Constraints < HEADER>-MOUNT ( ( H-cat VERB )   ( S-eat NOUN )   ( H-feat MOOD in d TENSE pre . . . . )  . . . ) < HEADER>-RIVER . . . . . . . . . . . . . . . 
; ; Header Activation Condition
ACTION ( TO-HAVE-SOURCE) ; ; Meaning ( TO-HAVE-SOURCEI * agntilos 0 ) Figure 1: A case frame ( expressed in CG notation )  , two dependency rules and a corresponding KS . 
rfype:TO-HAVE-SOURCE
Header . " NASCK "
Left : MOUNTRight:RIVER /
Type : MOUNT
Header:"MONTE"?JOE/Right:none# ( to be solved ) 
Type : RIVER
Header : " TEVEFIE "
Left : JOLLY Right : none-IL-\[missing \]
Figure 2: An example of DI.
from the meaning of the component phrases.
3.2 Parsing
Each of the phrase hypotheses generated by KSs during parsing relates to an utterance fragment and is called Deduction/natance  ( DI )  . D is are an extension of the island concept in the HWIM system\[Woods  82\]  . ADI is supported by word hypotheses and has a tree structure reflecting the compositional constraints of the KSs that built it  . It has a score computed by combining the score of the word hypotheses supporting it  . A simplified view of a DI is shown in Fig .  2 . That DI refers to the sentence " D a quale monte nasceil ' revere ? "  ( " From which mount does the Tevere originate ?" )  ; its root has been built by the KS of Fig .  1 , and two more KSs were required to build the rest of it  . The tree structure of the DI reflects the compositional structure of the KSs  . The bottomleft part of the picture shows that there are two types  ( SPEC and JOLLY ) that correspond to phrases that have still to be detected  . Such'empty ' nodes are called goa/a . SPEG will account for the phrase " Quale " ( " Which " )  ; JOLLY represents he need of a preposition that might be missing from the lattice  ( this aspect is discussed later )  . 
Parsing is accomplished by selecting the best -scored DI or word hypothesis in the lattice and letting it to be accreted by all of the KSs that can do the job  . Such opportunistic score-guided search results in topdown  , ' expectation-based'actions that are dynamically mixed with bottom-up  , ' predictive'actions . The actions of KSs on D is are described by operators  . 
Top-down actions consist in starting from a DI having a goal  , and : 1 . if it is a header slot , solve it with a word hypothesis ( VERIFY operator )  ;  2 . if it is a case-filler slot , ? solve it with already existing complete D is ( MERGE )  , or ? decompose it according to a KS knowledge contents  ( SUBGOALING )  . 
Bottom-up actions consist in creating a new DI start'ing either  1  . from a word hypothesis , which will occupy the header slot of the new DI ( ACTIVATION )  , or 2 . from a complete DI , which will occupy a case-filler ( PREDICTION ) . 

Such a strategy is opportunistic , since the element on which the KSs will work is selected according to its score  , and the actions to be performed on it are determined solely by its characteristics  . 
The activity of the operators is mainly concerned with the p:eop agation of constraints to the goal nodes of each newly-created DLC on straints are propagated from a father to a sonor vice versa ccording to the current parsing direction  . They consist in : Time intervals , in the form of start and end ranges ; Morphological information , used to check agree-men~s inside the DI ; Fun (: tional information , used to verify the correctness of the grammatical relations that are being established within the DI  ; Semantic type information . This information is used when , unlike the case of Fig .  1 , more than one case frame are represented by a single KS  ( the offih ~ e compiler may decide to do this if the case frames are similar and the consequent estimated reduction of redundancy appears ufficiently great  )  . 
In such a situation compliance with the single case-flames may have to be checked  , hence the reason for this type of information . 
4 Dealing with missing short words
As was pointed out , there are many different kinds of words thai . are short . In general , their semantic relevance depends on the linguistic representation ad on the chosen domain  . If the words are determiners , prepositions or auxiliary verbs , however , the integration of syntax and semantics outlined above makes them irrelevant in most cases  , as very often it allows to infer them from the other words of the sentence  . Such an inference may result not possible ( mainly when prepositions are concerned )  , or the word may belong to other categories , uchas connectives (" and " , " or ") or proper nouns , which are short but whose semantic relevance is out of question  ; in these cases the system must react exactly as to the lack of a ' normal ~ word  . 
Let us call ' jollies ' the types of word for which only a functlonal role is acknowledged  . Jollies are considered merely as ~ yntact lc markers for constituents to which they do not offer a meaning contribution per se  . The pursued goal is twofold : 1 .   Par~3ing must be enabled to proceed without them in most cases  ;  2 . However ~ whenever possible and useful , one wish to exploit their contribution i terms of time constraint and score  ( remember that there are also ~ long ' jollies , much more reliable than short ones ) . 
The general philosophy is ' ignore a jolly unless there is substantial reasons to consider it '  . The proposed solution is as follows : 1 . Jollies are represented as terminal slots in the compositional part of a KS  , like headers . There can be syntactic and even semantic on straints on them  , but they do not enter into the rule describing the meaning representation  . 
2 . Since we assume that jollies have no semantic predictive power  , all of the operators are inhibited to operate on them  . 
3 . Another topdown operator , JVERIFY , is added to solve jolly slots , acting only when a DI has enough support from other ' significant ' word hypotheses  . 
Fig .   3 shows a KS deriving from the same caseffame of Fig  . 1 but from a different dependency rule . Such a KS treats sentences like " D a quale monte siorlginail Tenvere ? "  ( " From which mount does the Tevere originate ?" )  , in which the word " si " is a marker for verb reflexivity  . 
The way JVERIFY operates depends on the result of a predicate  , JOLLY-TYPE , applied on the jolly slot . JOLLY-TYPE has three possible values : SttORT-OR -UNESSENTIAL  , LONG-OR-ESSENTIAL , and UNKNOWN that depend on various factors , including tlle lexical category assigned to the jolly slot  , the temporal , morphologl c and semantic on straints imposed on that slot by other word hypotheses  , and the availability of such data . if the returned value is LONG-OR-ESSENTIAL , then the jolly must be found in the lattice , and it ~ loss causes parsing to react in a way exactly similar as to the loss of any other ' normal ' word  . Conversely , if the value is SHORT-OR-UNESSENTIAL , the jolly is ignored by placing a suitable temporal ~ hole ~ in the slot pf the DI  . 
The hole has relaxed temporal boundaries so as not to impose too strict a constraint  o11 the position of words that can fill adjacent slots  ; thresholds are used for this purpose . Finally , if the value is UNKNOWN , an action like the previous one is done , followed by a limited search in the lattice , looking for words exceeding the maximum width of the ' hole '  . Such a search is necessary because it insures that parsing does not fail when tim correct word is a jolly larger than the ' hole '  . JVERIFY is submitted to the standard scheduling just as the other operators are  . 
5 Experimental results
The above ideas have been implemented in a parser called SYNAPSIS  ( from SYN tax-Aided Parser for Semantic Interpretation of Speech  )  . SYNAPSIS is an evolution of the parser included in the SUSY system for understanding speech and described in \[ Poesio  87\]  . SYNAPSIS has been implemented in Common Lisp and relies on about  150 KSs , able to handle a 1011-word lexicon on a restricted semantic domain . An idea of the lingulstic overage is given by the equivalent branching factor  , which is VEP ~ B ( prop ) = NOUN ( interr-indir-loc ) REFLEX < GOVERNOR , > NOUN ( subj) ; ; Features and Agreements < GOVERNOR > ( MOOD ind )   ( TENSE pres )   ( NUMBER . x ) . . . .
NOUN-1....
I % EFLEX nil
NOUN-2 ( NUMBER ..x )....
DefKSKS-2 4.13;; Composition
TO-HAVE-SOURCE=MOUNT < JOLLY > < HEADER>RIVER ;   ; Meaning ( TO-HAVE-SOURCE !* agnt 1iocO)
Figure 3: AKS with a jolly field.
III ", 00, 1111 sentences present , 1, 3 . 1\[18\[0\[O\]ii0b , lattices pre * snt152 36665'l"l\]11\[
Table 1: Jolly word detection.
miss'ng jolly words per sentence n . of sentences successfully parsed averagen . of generated D is 1   2   3   40   18   4   35   15   3   318   440   563 about 35  . The system has been tested with 150 word lattices generated by processing as many sentence suttered in continuou speech with natural intonation in a normal office environment  . The overall performance results in about 80% correct sentence understanding \[ Fissore 88\]  . 
The thresholds for JVERIFY have been experimentally determined to minimize the computational load  , represented by the average number of D is generate dur-ing each parsing  . Tab .   1 shows the number of jolly words that have been skipped by the parser vs  . the number of jollies actually missing in the corresponding lattices  . 
The former figures are higher than the latter , indicating that many words , albeit present , have been discarded by JVERIFY because of their bad acoustical scores or their scarce contribution to contraint propagation  . 
The most apparent advantage of the above technique is the increase in the number of sentences that can be analyzed without querying the user for lacking information  . 
Tab . 2 displays the number of lattices , corresponding to the sentences containing at least one word of jolly type  , in which some of such words are missing . It is seen that about 75~ of them have been successfully understood . 
This figure does not change substantially as the number of missing jollies per sentence increases  , and hence indicates robustness . The computational load , given by the number of generated D is , is somewhat affected by the number of missing jollies  . However , this is mainly due to the fact that sentences with many jollies are also longer 
Table 2: Successful parsing and syntactically complex . The actual efficiency can be better estimated from Fig  .  4 , where the average number of generated Disis plot as a function of the threshold on the width of the jolly temporal ' hole '  . The figure displays also the amount of parsing failures related to jolly problems  ( failures due to other reasons have been ignored for simplicity  )  . The curve indicates that raising the threshold does not change much the number generated D is  ( the relative oscillations of the values are small  )  . This means that the relaxation of constraints during the application of JVERIFY is not a source of inefficiency  . Moreover , there is a large range of values for which the parsing failure remains low  . 
The curve also shows that relaxing constraints may even speed up the parsing  . This can be easily explained . When the threshold is low , no jolly is skipped , and failure occurs when jollies are missing from the lattice  . When the threshold is raised , skipping begins to work : good-scored false jollies are no more a source of disturbance  , and correct but bad-scored jollies are skipped thus avoiding to delay the parsing  ; as a consequence the overall number of D is decreases  . Further enlarging the threshold reverts this tendency  , since the too-much-relaxed constraints allow the aggregation of words that would have been discarded with stricter constraints  ; failures occur when one of such aggregations makes up a complete parse scoring of generated Dis  ( relative ut ? its ) k 0 . 9 0 . 85 --
Percentage of illc . Orl'Cct\[)Ismld crstanding % N . IY ) ~-_ .  ( '  . ~% 25% y : k--0 . . . .  .   . 0 f-I-0%_Lk .   .   .   .   .   . t - . - - t - - - - k .   .   .   .   .   .   .   . v . + - - - h -~ ~1   10   15   20   211   3i   35 Threshold on hole width time li'ames ) Figure 4: Performance vs . width threshold . 
better th ~ a the correc ~ one.
6  ( Jonclnsmns and links with current research Experimer  , ts show that the presence of jolly slots solvable as described above  , beside permitting to successfully an-aly ~ ea much greater quota of word lattices  , also speeds up parsing preventing it from being misled by false jollies  . 
This well : : ompensate e for the growth of the inferential ~ t ctivity dlte to the relaxed tempor M constraints in the Discont Mning ~ holes  '  . As a consequence it is possible ~ ouse KS having chains of two or even three adjacent jolly slots without compromising excessively the global perform ai  , ,:es . This is a novel improvement over systems that , to our knowledge , only admit one single skippable word and use a more rigid linguistic knowledge representation \[ Tomita  87\] or recognize any configuration of missing words but do not distinguish cases in which the information content of an absent word cart be ignored\[Goerz  83\]  . 
An attracting feature of the present parsing technique is th  ; ~t the KS activities are modularized into a set of operators  . Consequently , it remains open to ' local ' improvement ~ , on single operators as well as to overall heuristic adjustments on the score-guided control strategy  . As a a exampi ~ , the response of the predicate JOLLY-TYPE of the oper ~ to r  JVER3FY may be rendered more ' intelligent ' by exploiting further information  , such as estimates of the expected word length ~ that has not been kept into consideration in the present implementation  . 
A diff , ! rent philosophyarising in very recent speech understanding research development sentrusts the problem of solving trouble some portions of the utterance  ( including those were jollies were not found ) to a deeper , % e oustl cal analysis guided by linguistic expectation \[ Niedermair  87\]  . Our approach is not in conflict , but rather , complementary to it . We believe that corn-bining the two approaches would lead to a research area that should turn very fruitful in producing robust speech parsing  . 
The authors wi~h to ezpress their gratitude to their colleague  , the late Dr . SuBs , for Id8 contribution to the develotnnent of the system . 
References\[Briet ~ . mann 86\]A . Brietzmann , U . Ehrlich , " The role of semantic processing illan automatic speech understanding system "  , ProsCOL 1NG 86 , Bonn . 
\[ Fillmore 68\] C . J . Fillmore , " The case for case " , in Bach ~ Harris ( eds . ) , Universals in Linguistic Theoryl Itolt , Rine-hart , and Winston , New York ,  1968 . 
\[ Fissore88\]L . Fissore , E . Giachin , P . Laface , G . Micca , R . 
Pieraccini , C . Rullent , " Experimental results on large-vocabulary speech recognition and understanding "  , Proc . 
ICASS " P88, New York.
\ [ Gemello87\]R . Gemello , E . Giachln ~ C . Rullent , " A knowledge-based framework for effective probabilistic control strategies in signal understanding "  , Prvc . GWAI87, Springer Verlaged . 
\[Goerv . 83\]G . Goerz , C . Beckstein , " How to parse gaps in spoken utterances " , Proc . 1 ~ tConf . Europ . CnapLACL . 
\[ Hayes 86\]P . J . tIayes , A . G . Hauptmann , J . G . Carbonell , M . 
Tomita ~" Parsing spoken language : a semantic ase frame approach "  , Prec . COL1NG86, Bonn . 
\[Itays 64\]D . G . Hays , "Dependency theory : a formalism and some observations "  , Memorandum RM 4087 P . R . , The
R and Corporation.
\[ Hinrichs86\]E . W . t t in richs , "A compositional semantics for directional modifiers "  , Proc . COLING86, Bonn . 
\[ Laface87\]P . Laface , G . Micca , R . Pieraceini , " Experimental results on a large lexicon access task "  , Proc . ICASSP 87,

\[Lesmo85\]L . Lesmo , P . To rasso , "Weighted interaction of syntax and semantics in natural language analysis "  , Pro ? . 
IJCAI85, Los Angeles.
\[ Kaplan82\]S . J . Kaplan , " Cooperative responses from a portable natural language query system "  , Artificial Intelligence 19 ,  1982 . 
\[Niedermair87\]G . T . Niedermair , "Merging acoustics and linguistics in speech understanding "  , NATOASI-Conference ~
Bad Windsheim.
\[ Poesio87\] M . Poesio , C . Rullent , " Modified case frame parsing for speech understanding systems "  , Proc . IJCAI 87, Milano . 
\[Schank75\]R . Schank , Conceptual Information Processing,
North-ttol I and , New York , 1975.
\[Sows84\]J . F . Sowa , Conceptual Structures , Addison Wesley,
Reading ( MA ), 1984.
\[ Tomita87\]M . Tomita , " An efficient augmented-context-free parsing algorithm "  , Computational Linguistic $ , Vol . 13, n . 1-2, Jan-June 1987 . 
\[Woods82\]W . A . Woods , " Optimal search strategies for speech understanding control "  , Artificial Intelligence 18 ,  1982 . 

