Handling Sparse Data by Successive Abstraction
Christer Samuelsson
Universit?t des Saarlandes , FR8 . 7 , Comput cr linguistik Postfach 11 50 , D-66041 Saarbrfick cn , Germany
Internet : christer ? coli.uni-sb , de

A general , practical method for handling sparse data that avoids heldout data and iterative reestimation is derived from first principles  . It has been tested on a part-of-speech tagging task and outperformed  ( deleted ) interpolation with context-independent weights , even when the latter used a globally optimal parameter setting determined a posteriori  . 
1 Introduction
Sparse data is a perennial problem when applying statistical techniques to natural anguage processing  . The fundamental problem is that there is often not enough data to estimate the required statistical parameters  , i . e . , the probabilities , directly from the relative frequencies . This problem is accentuated by the fact that in the search for more accurate probabilistic language models  , more and more contextual information is added , resulting in more and more complex conditionings of the corresponding conditional probabilities  . This in turn means that the number of observations tends to be quite small for such contexts  . Over the years , a number of techniques have been proposed to handle this problem  . 
One of two different main ideas behind these techniques i that complex contexts can be gene -ralized  , and data from more general contexts can be used to improve the probability estimates for more specific contexts  . This idea is usually referred to as backoff smoothing  , see ( Katz 1987) . 
These techniques typically require that a separate portion of the training data be held out from the parameter-estimation phase and saved for determining appropriate backoff weights  . Further ~ more , determining the backoff weights usually requires resorting to a time-consuming iterative ree -stimation procedure  . A typical example of such a technique is " deleted interpolation "  , which is described in ' Section 5 . 1 below . 
The other main idea is concerned with improving the estimates of low-frequency  , or no-frequency , outcomes apparently without trying to generalize the conditioning s  . Instead , these techniques are based on considerations of how population frequencies in general tend to behave  . Examples of this are expected likelihood estimation  ( ELE )  , see Section 5 . 2 below , and GoodTuring estimation , see ( Good 1953) . 
We will here derive from first principles a pract i -cal method for handling sparse data that does not need separate training data for determining the backoff weights and which lends itself to direct calculation  , thus avoiding time-consuming reestimation procedures  . 
2 Linear Successive Abstraction
Assume that we want to estimate the conditional probability P  ( xIC ) of tile outcome x given a context C from the number of times N ~ it occurs in N = ICI trials  , but that this data is sparse . 
Assume further that there is abundant data in a more general context CtDC that we want to use to get a better estimate of P  ( xIC )  . The idea is to let the probability estimate/5 ( xIC ) in context C be a flmctiong of the relative frequency f  ( xIC ) of the outcome x in context C and the probability estimate P  ( x\[C ' ) ill context C':
IV ) = g(f(IIV '))
Let us generalize this scenario slightly to the si -tuation we rew chave a sequence of increasingly more general contexts CmC  Urn-1 C . . . CC1 , i . e . , where there is a linear order of the various contexts Ck  . We can then build the estimate of P ( xICk ) on the relative frequency f ( xICk ) in context Ck and the previously established estimate of P  ( xICk-1 )  . Wc call this method linear successive abstraction  . A simple example is estimating the probability P ( xI/n-j+l ? . . . , In ) of word class x given l,-j+l, . . . , ln , tile last j letters of a wordll, .   .   . , l , . In this case , the estimate will be based on the relative frequencies f  ( xIl , ,_~+ , , . . . , l , ,), . . . , f(x\[In ), f(x ) . 
We will here consider the special case when the flmction g is a weighted sum of the relative frequency and the previous estimate  , appropriately f ( xI+0P ( xIP ( xICk )  =  1+0 We want the weight 0 to depend on the context Ck , and in particular be proportional to some measure of how spread out the relative frequencies of the various outcomes in context Ck are from the statistical mean  . The variance is the quadratic moment w . r . t , the mean , and is thus such a measure . However , we want the weight to have the same dimension as the statistical mean  , and the dimension of the variance is obviously the square of the dimension of the mean  . The square root of the variance , which is the standard deviation , should thus be a suitable quantity . For this reason we will use the standard deviation in Ck as a weight  , i . e . , 0 = ~ r(Ck ) . One could of course multiply this quantity with any reasonable real constant  , but we will arbitrarily set this constant to one , i . e . , use ~ r(Ck ) itself . 
In linguistic applications , the outcomes are usually not real numbers , but pieces of lingui-stic structure such as words  , part-of-speech tags , gramma rules , bits of semantic tissue , etc . This means that it is not quite obvious what the standard deviation  , or the statistical mean for that matter , actually should be . To put it a bit more abstractly , we need to calculate the standard deviation of a non-numerical random variable  . 
2 . 1 Der iv ing the S tandard Dev ia t ion So how do we find the standard deviation of a non-numerical random variable ? One way is to construct an equivalent numerical random variable and use the standard deviation of the latter  . 
This can be done in several different ways . The one we will use is to construct a numerical random variable with a uniform distribution that has the same entropy as the non-numerical one  . Whether we use a discrete or continuous random variable is  , as we shall see , of no importance . 
We will first factor out the dependence on the context size  . Quite in general , if ~ N is the sample mean of N independent observations of any numerical random variable  ( with variance a02 , i . e . , - N = ~- , i = 1(i , then ~2=Var\[~N\]=1N1N ~---- Var\['~-~ ( i \] = ~~ V ar\[ ( i \] - - - - i=1 i=1 In our case , the number of observations N is simply the size of the context Ck  , by which we mean the number of times Ck occurred in the training data  , i . e . , the frequency count of Ck , which we will denote \] Ck\[ . Since the standard deviation is the square root of the variance  , we have o-(cn = Vic , ,I Here ~ r0 does not depend on the number of observations in ' coff text Ck  , only on the underlying probability distribution conditional on context Ck  . 
To estimate cr0(Ck ) , we assume that we have eit-her a discrete uniform distribution on  1  ,   .   .   .   , Mora continuous uniform distribution on \[0 , M \] that is as hard to predict as the one in Ck in the sense that the entropy is the same  . The entropy H\[~\] of a random variable ~ is the expectation value of the logarithm of P  ( ( )   . In the discrete case we thus have H\[ ( \] = E\[-lnP (   (   ) \]:~-~- P ( xi ) lnP ( xi ) it Iere P ( xi ) is the probability of the random variable ( taking the value xi , which is ~ for all possible outcomes xi and zero otherwise  . Thus , the entropy is In M:

E-P ( xi ) lnP ( xi ) = E-~-lnM------=lnMi i=1 The continuous case is similar . We thus have that lnM = H\[Ck\]or M = eIIICk \] The variance of these uniform distributions is M  2   1--T in the continuous case and ~ in the discrete case  . We thus have
M11 cr ? ( Ck ) = X/~x/r ~ M-1-xff2e-H\[ckl Unfortunately , the entropy It\[Ck\] depends on the probability distribution of context Ck and thus on Cro  ( Ck )  . Since we want to avoid trying to solve highly nonlinear equations  , and since we have access to an estimate of the probability distribution of context  Ck-1  , we will make the following approximation :
O ' 0 ( Ck-1) 1~(Ck ) ~
It is starting to look sensible to specify ~ r- 1 instead of ~ , i . e . , instead of ~ we will write l q - o " ' c~-I q1  "  2  . 2 The F ina l Recur rence Formula We have thus established a recurrence formula for the estimate of the probability distribution in context Ck given the estimate of the probability distribution in context  Ck-1 and the relative frequencies in context Ck :
P ( xICk ) = (1) , r ( Ck ) -ly ( xC  ~ ) + p ( xIC ~- l )  ~ ( Ck ) -~+1 and ( cn = We will start by estimating the probability distribution in the most general context  C1  , if necessary is the most general context , this will be the context with the most training data  . Thus it stands the best chances of the relative frequencies being acceptably accurate stimates  . This will allow us to calculate an estimate of the probability distribution in context  C2  , which in turn will allowns to calculate an estimate of the probability distribution in context Ca  , etc . We can thus calculate estimates of the probability distributions in all contexts  C1  ,   .   .   . , Cm . 
We will next consider some examples from part-of -speech tagging  . 
3 Examples from PoS Tagging
Part-of-speech ( PoS ) tagging consists in assigning to each word of an input text a  ( set of ) tag ( s ) from a finite set of possible tags , a tag palette or a tagset . The reason that this is a research issue is that a word can in general be assigned different tags depending on context  . In statistical tagging , the relevant information is extracted from a training text and fitted into a statistical language model  , which is then used to assign the most likely tag to each word in the input text  . 
The statistical language model usually consists of lexical probabilities  , which determine the probability of a particular tag conditional on the particular word  , and contextual probabilities , which determine the probability of a particular tag conditional on the surrounding tags  . The latter conditioning is usually on the tags of the neighbouring words  , and very often on the N-1 previous tags , so called ( tag ) Ngram statistics . These probabilities can bcestimated either from a pretagged training corpus or from untagged text  , a lexicon and an initial bias . We will here consider the former case . 
Statistical taggers usually work as follows : First  , each word in the input word string 1471 ,  ?  . . , W , is assigned all possible tags according to the lexicon  , thereby creating a lattice . A dyna-mic programming technique is then used to find tag the sequence  5/\]   ,   .   .   . , ~, that maximizes
P(T1, . . . , TnIW l , .   . ., Wn ) = t t = I IP ( Tk T1, . .  . , Tk-1;Wl, .   .   . , Wn)k = l1:=1
P ( TkTk-N+I , .   .   . , Tk-1;VIZk ) ?7'" . P ( T ~ wk)P ( Tk~k-N + l, .   .   . , k-l)\[k = lflP(Tk

Tk-N+~,...,~-~)"P(WkITk)~:~P(Wk)
Since the maximum does not depend on the factors P ( Wk )  , these can be omitted , yielding the standard statistical PoS tagging task : max\]-\[P  ( TkIU ~-~ V + ~ ,  . . . ,Tk-J . P ( WkJT ~)
TI , ..., T ~, t ~ l =
This is well-described in for example ( DeRose 1988 )  . 
We thus have to estimate the two following sets of probabilities : ? Lexical probabilities : The probability of each tag Ti conditional on the word W that is to be tagged  , p(r'II wr!i
Often the converse probabilities P ( W are given instead , but we will for reason soout o become apparent use the former formulation  . 
? Tag Ngrams :
The probability of tagTi at position k in the input string  , denoted T ~ , given that tags 7~ . -N+1T , .   .   .   ,   k1 have been assigned to the previous N - 1 words ? Often N is set to two or three , and thus bigralns or trigrams are employed . When using trigram statistics , this quantity is P ( T ~\] 7' k - ~ , Tk-1) . 
3.1 Ngram Back-off Smoothing
We will first consider estimating the Ngram probabilities P  ( T ~\] Tk-N+I ,  . . . ,Tk-1) . IIer e , there is an obvious sequence of generalizations of the context  5/~-N+1  ,  . . . , 7~-1 with a linear order , na-mely ~/ ~-- N+I ~ CT k-N+2 ,   , Tk-1C ,   .   .   . , k-1 .   .   . 
? cT ? . k1 Cf l , where f ~ means " no information " , corresponding to then nigram probabilities . Tiros we will repeatedly strip off the tag furthest from the current word and use the estimate of the probability distribution in this generalized context o improve the estimate in the current context  . This means that when estimating the ( j + 1 ) -gram probabilities , we back off to the estimate of the j-gram probabilities  . 
7' So when estimating P ( T\[ITk-j , .   .   .   ,  ~-~) , we simply strip off the tag 5~_j and apply Eq . (1): ~-~(~' ~\[ Tk-j, .   .   . , Tk-i ) = --1, ir , f(%I : tk-j, . . . , rk-~)=++P ( T and ~-1+1~-~+1, . . . , Tk-O~-1+1? . , 7~_1\[e-ll\[Tk_j+l . . . . . T  ~ . _ 11 3 . 2 Hand l ing Unknown Words We will next consider improving the probability estimates for unknown words  , i . e . , words that do not occur in tile training corpus , and for which we therefore have no lexical probabilities  , The same technique could actually be used for improving the estimates of the lexical probabilities of words that thaF there is a substantial amount of information in the word suffixes  , especially for languages with a richer morphological structure than English  . For this reason , we will estimate the probability distribution conditional on an unknown word from the statistical data available for words that end with the same sequence of letters  . Assume that the word consists of the letters I1, . ??, I , ~ . We want to know the probabilities P ( Ti Ill, .   .   . , ln ) for the various tags Ti . 1 Since the word is unknown , this data is not available . However , if we look at the sequence of generalizations of " ending with same last j letters "  , here denoted ln-j+l ,  ? ? . , In , we rea-lize that sooner or later , there will be observations available , in the worst case looking at the last zero letters  , i . e . , at the unigram probabilities . 
So when estimating P ( Ti IIn-j+l , . . . , ln ) , we simply omit the jth last letter In-j+l and apply 
Eq . (1):
P ( TI == er-lf(TiIl,-j + l, .   .   . , ln ) + ~-1+1
P ( TiIIn-i+2, . . . , 1, ,) + a-l + lander-1 = ln-j + l, .   .   . , l , le-tl\[1"-j += . . . . . MT his data can be collected from the words in the training corpus with frequencies below some threshold  , e . g . , words that occur less than say ten times , and can be indexed in a tree on reversed suffixes for quick access  . 
4 Par t ia l Success ive Abst ract ion If there is only a partial order of the various gene -ralizations  , the scheme is still viable . For example , consider generalizing symmetric trigram statistics  , i . e . , statistics of the form P ( TITz , Tr) . Here , both Tt , the tag of the word to the left , and Tr , the tag of the word to the right , are one-step generalizations of the context 7 , Tr , and both have in turn the common generalization ~   ( " no information " )  . 
We modify Eq . (1) accordingly:
D(TIT t , T ~) = cr(T t , T ~) - ~ f(TIT z , T ~) o'(r / , Tr ) -1q-1-~
IP(TIT0+P(TIT~)+-2~(T ,, T~)-~+i and
P ( TIT ~)
P ( TIT , ) a ( Ti ) -1 f ( TIT ~ ) + P ( T ) a ( T z ) -1 + 1 ~ r ( Tr ) -1 f ( TITs ) + P ( T )  ~ ( T  ~ ) -I+11 Orreally , P ( TiI1o ,  11 ,  . . . , ln ) where lo is a special symbol indicating the beginning  o1 the word . 
We call this partial successive abstraction . Since we really want to estimate cr in the more specific context  , and since the standard deviation ( with the dependence on context size factored out ) will most likely not increase when we specialize the context  , we will use : In the general case , where we have Mone-step generalizations C~of C  , we arrive at the equation
P ( xIc )  = -1/ ( xIc ) + EY = II el ) and +1-1=v , iT-nt caBy calculating the estimates of the probability distributions in such an order that whenever estimating the probability distribution in some particular context  , the probability distributions in all more general contexts have already been estima-ted  , we can guarantee that all quantities necessary for the calculations are available  . 
5 Relationship to Other Methods
We will next compare the proposed method to , in turn , deleted interpolation , expected likelihood estimation and Katz's backoff scheme  . 
5.1 Deleted Interpolation
Interpolation requires that the training corpus is divided into one part used to estimate the relative frequencies  , and a separate held-back part used to cope with sparse data through backoff smoo-thing  . For example , tag trigram probabilities carl be estimated as follows: 
P ( Tj\[Tk-2 , Tk-1 ) ~ Alf ( 7~ ) ++ Auf ( TikITk_1 ) +A af ( TikITk-2 , T ~_ I ) Since the probability estimate is a linear combination of the various observed relative frequencies  , this is called linear interpolation . The weights 13 . 
may depend on the conditionings , but are required to be nonnegative and to sum to one over j  . An enhancement is to partition the training set inton parts and in turn perform linear interpolation with each of then parts held out to determine the backoff weights and use the remaining n-  1 parts for parameter estimation . The various backoff weights are combined in the process  . This is usually referred to as deleted interpolation  . 
The weights Aj are determined by maximizing the probability of the heldout part of the training data  , see ( Jelinek & Mercer 1980) . A locally Welch reestimation , see ( Baum 1972) . BaumWelch reestimation is however prohibitively time-consuming for complex contexts if the weights are allowed to depend on the contexts  , while successive abstraction is clearly tractable  ; the latter effectively determines these weights directly from the same data as the relative frequcncies  . 
5 . 2 Expected L ike l ihood Es t imat ion Expected likelihood estimation  ( ELE ) consists in assigning a next rahalfa count to all outcomes  . 
Thus , an outcome that didn't occur in the training data receives half a count  , an outcome that occurred once receives three half counts  . This is equivalento assigning a count of one to the occurring  , and one third to the non-occurring outcomes . To give an indication of how successive abstraction is related to ELE  , consider the following special case : If we indeed have a uniform distribution with M outcomes of probability M ! in context  Ck-1 and there is but one observation of one single outcome in context Ck  , then Eq .   ( 1 ) will assign to this outcome the probability v q h + l and v q h + M to the other  , non-occurring , outcomes 1 Sov ~+ m ' if we had used 2 instead of vq2 in Eq .  (1) , this would have been equivalen to assigning a count of one to the outcome that occurred  , antia count of one third to the ones that didn ' t  . As it is , the latter outcomes are assigned a count of 1   4i5+~"   5  . 3 Katz ' s Back - Of f Scheme The proposed method is identical to Katz's backoff method  ( Katz 1987 ) up to the point of sugge-sting a , in the general case nonlinear , retreat to more general contexts :
P (= Ic ) = g(f(=IIt'))
Blending the involved distributions f ( x\]C ) and /5 ( xIC' )  , rather than only backing of t " to C'iff ( x\]C ) is zero , and in particular , instantiating the flmction g(f , P ) to a weighted sum , distinguishes the two approaches . 
6 Experiments
A standard statistical trigram tagger has been implemented that uses linear successive abstraction for smoothing the trigram and bigram probabilities  , as described in Section 3 . 1 , and that handles unknown words using a reversed suffix tree  , as described in Section 3 . 2 , again using linear successive abstraction to improve the probability estimates  . This tagger was tested on the Susanne Corpus , ( Sampson 1995) , using a reduced tagset of 62 tags . The size of the training corpus A was almost 130 , 000 words . There were three separate test corpora B , C and D consisting of approxima-tely10 , 000 words each . 
Test corpus

Error rate (%)-tagomissions-unknown words
Unknown words
Error rate (%)
B bigram trigram HMM 4 . 41 4,36 4 . 49 0 . 67 1 . 36 1 . 20 1 . 52 6 . 18 22 . 1 19 . 4 24,5
Test corpus C bigram HMM trigram Tagger
Error rate (%)-tagomissions-unknown words
Unknown words
Error rate (%) 4 . 26 3 . 93 4 . 03 0 . 68 1 . 43 1  . 30 1 . 34 7 . 78 18 . 3 16 . 8 17 . 3
Test corpus Dbigram HMM Tagger
Error rate (%)-tagomissions-unknown words
Unknown words
Error rate (%) trigram 5 . 14 4 . 81 5 . 13 0 . 94 1 . 80 1 . 63 2 . 02 8 . 06 22 . 3 20 . 2 25 . 0 Figure 1: Results on the Susanne Corpus Tile performance of the tagger was compared with that of ant lMM -based trigram tagger that uses linear interpolation for Ngram smoothing  , but where the backoff weights do not depend on thee on ditionings  . An optimal weight , setting was determined for each test corpus individually  , and used in the experiments . Incidentally , this setting varied considerably from corpus to corpus  . Thus , this represented the best possible setting of backoff weights obtainable by linear interpolation  , and in particular by linear deleted interpolation  , when these are not allowed to depend on the context  . 
In contrast , the successive abstraction scheme determined the backoff weights automatically from the training corpus alone  , and the same weight setting wasnsed for all test corpora  , yiel-ding results that were at least on par with those obtained using linear interpolation with a globally optimal setting of contcxt-independent backoff weights determined a posteriori  . Both taggers handled unknown words by inspecting the suffi-xes  , but the HMM-based tagger did not smooth the probability distributions  . 
The experimental results are shown in Figure 1.
Note that the absolute performance of the trigram tagger is around  96 % accuracy in two cases and distinctly above 95 % accuracy in all cases , which is clearly state-of-the-art results . Since each test corpus consisted of about 10 , 000 words , and the error rates are between 4 and 5% , the 5 percent significance level for differences in error rate is between  0  . 39 and 0 . 43% depending on the error rate , and the 10 percent significance level is between 0  . 32 and 0,36% . 

We see that the trigram tagger is better than the bigram tagger in all three cases and significantly better at significance lvel  10 percent , but not at 5 percent , in case C . So at this significance level , we can conclude that smoothed trigram statistics improve on bigram statistics alone  . 
The trigram tagger performed better than the HMM -based one in all three cases  , but not significantly better at any significance level below  10 percent . This indicates that the successive abstraction scheme yields backoff weights that are at least as good as the best ones obtainable through linear deleted interpolation with context -independent backoff weights  . 
7 Summary and Fur ther D i rec t ions In this paper  , we derived a general , practical method for handling sparse data from first principles that avoids heldout data and iterative reestimation  . It was tested on a part-of-speech tagging task and outperformed linear interpolation with context-independent weights  , even when the latter used a globally optimal parameter setting determined a posteriori  . 
Informal experiments indicate that it is possible to achieve slightly better performance by replacing the expression for ~ ro ~  ( Ck ) with a fixed global con1 stant ( while retaining the factor I~kl'which is most likely a quite accurate model of the dependence on context size  )  . However , the optimal value for this parameter varied more than an order of magnitude  , and the improvements in performance were not very large  . Furthermore , suboptimal choices of this parameter tended to degrade performance  , rather than improve it . This indicates that the proposed formula is doing a pretty good job of approximating an optimal parameter choice  . It would nonetheless be interesting to see if the formula could be improved on  , especially seeing that it was theoretically derived  , and then directly applied to the tagging task , immediately yielding the quoted results . 

The work presented in this article was funded by the  N3 " Bidirektionale Linguistische Deduktion ( BiLD ) " project in the Sonderforschungsbereich 314 Kiinstliche Intelligeuz--Wissens basierteSy -steme  . 
I wish to thank greatly Thorsten Brants , Slava Katz , Khalil Sima'an , the audiences of seminars at the University of Pennsylvania and the University of Sussex  , in particular Mark Liberman , and the anonymous reviewers of Coling and ACL for pointing out inaccuracies and supplying useful comments and suggestions to improvements  . 

L . E . Baum .  1972 . " An inequality and associated maximization technique in statistical estimation for probabilistic functions of Markov processes "  . In equalities IIh 18 . 
Steven J . DeRose .  1988 . " Grammatical Category Disambiguation by Statistical Optimization "  . Computational Linguistics , 14(1):31-39 . 
I . J . Good .  1953 . " The population frequencies of species and the estimation of population parameters "  . Biometrika , 40:237-264 . 
Frederick Jelinek and Robert L . Mercer . 1980.
" Interpolated Estimation of Markov Source Pa -ramenters from Sparse Data "  . Pattern Recognition in Practice : 381-397 . North Holland . 
Slava M . Katz .  1987 . " Estimation of Probabi-lities from Sparse Data for the Language Mo-del Component of a Speech Recognizer "  . IEEE Transactions on Acoustics , Speech , and Signal
Processing , 35(3): 400-401.
Geoffrey Sampson .  1995 . English for the Compu-ter . Oxford University Press . 

