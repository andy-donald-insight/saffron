Proceedings of the 23rd International Conference on Computational Linguistics ( Coling 2010), pages 134?142,
Beijing , August 2010
Simultaneous Ranking and Clustering of Sentences : A Reinforcement
Approach to Multi-Document Summarization
1Xiaoyan Cai , 1Wenjie Li , 1You Ouyang , 2Hong Yan
1Department of Computing , The Hong Kong Polytechnic University
{csxcai,cswjli,csyouyang}@comp.polyu.edu.hk
2Department of Logistics and Maritime Studies , The Hong Kong Polytechnic University
lgthyan@polyu.edu.hk


Abstract
Multidocument summarization aims to produce a concise summary that contains salient information from a set of source documents . In this field , sentence ranking has hitherto been the issue of most concern.
Since documents often cover a number of topic themes with each theme represented by a cluster of highly related sentences , sentence clustering was recently explored in the literature in order to provide more informative summaries . Existing cluster-based ranking approaches applied clustering and ranking in isolation . As a result , the ranking performance will be inevitably influenced by the clustering result . In this paper , we propose a reinforcement approach that tightly integrates ranking and clustering by mutually and simultaneously updating each other so that the performance of both can be improved . Experimental results on the DUC datasets demonstrate its effectiveness and robustness.
1 Introduction
Automatic multidocument summarization has drawn increasing attention in the past with the rapid growth of the Internet and information explosion . It aims to condense the original text into its essential content and to assist in filtering and selection of necessary information.
So far extractive summarization that directly extracts sentences from documents to compose summaries is still the mainstream in this field.
Under this framework , sentence ranking is the issue of most concern.
Though traditional feature-based ranking approaches and graphbased approaches employed quite different techniques to rank sentences , they have at least one point in common , i.e ., all of them focused on sentences only , but ignored the information beyond the sentence level ( referring to Figure 1(a)).
Actually , in a given document set , there usually exist a number of themes ( or topics ) with each theme represented by a cluster of highly related sentences ( Harabagiu and Lacatusu , 2005; Hardy et al , 2002). These theme clusters are of different size and especially different importance to assist users in understanding the content in the whole document set . The cluster level information is supposed to have foreseeable influence on sentence ranking.

Figure 1. Ranking vs . Clustering
In order to enhance the performance of summarization , recently cluster-based ranking approaches were explored in the literature ( Wan and Yang , 2006; Sun et al 2007; Wang et al 2008a,b ; Qazvinian and Radev , 2008).
Normally these approaches applied a clustering algorithm to obtain the theme clusters first and then ranked the sentences within each cluster or by exploring the interaction between sentences and obtained clusters ( referring to Figure 1(b )). In other words , clustering and ranking are regarded as two independent processes in these approaches although the cluster-level information has been incorporated into the sentence ranking process . As a result,
Ranking Ranking
Clustering
Ranking
Clustering ( a ) ( b ) ( c ) influenced by the clustering result.
To help alleviate this problem , we argue in this paper that the quality of ranking and clustering can be both improved when the two processes are mutually enhanced ( referring to Figure 1(c )). Based on it , we propose a reinforcement approach that updates ranking and clustering interactively and iteratively to multidocument summarization . The main contributions of the paper are threefold : (1) Three different ranking functions are defined in a bi-type document graph constructed from the given document set , namely global , withincluster and conditional rankings , respectively.
(2) A reinforcement approach is proposed to tightly integrate ranking and clustering of sentences by exploring term rank distributions over the clusters . (3) Thorough experimental studies are conducted to verify the effectiveness and robustness of the proposed approach.
The rest of this paper is organized as follows.
Section 2 reviews related work in cluster-based ranking . Section 3 defines ranking functions and explains reinforced ranking and clustering process and its application in multidocument summarization . Section 4 presents experiments and evaluations . Section 5 concludes the paper.
2 Related Work
Clustering has become an increasingly important topic with the explosion of information available via the Internet . It is an important tool in text mining and knowledge discovery . Its ability to automatically group similar textual objects together enables one to discover hidden similarity and key concepts , as well as to summarize a large amount of text into a small number of groups ( Karypis et al , 2000).
To summarize a scientific paper , Qazvinian and Radev (2008) presented two sentence selection strategies based on the clusters which were generated by a hierarchical agglomeration algorithm applied in the citation summary network . One was called CRR , which started with the largest cluster and extracted the first sentence from each cluster in the order they appeared until the summary length limit was reached . The other was called C-LexRank , which was similar to CRR but adopted LexRank to rank the sentences within each cluster and chose the most salient one.
Meanwhile , Wan and Yang (2008) proposed two models to incorporate the cluster-level information into the process of sentence ranking for generic summarization . While the Cluster-based Conditional Markov Random Walk model ( ClusterCMRW ) incorporated the cluster-level information into the text graph and manipulated clusters and sentences equally , the Cluster-based HITS model ( ClusterHITS ) treated clusters and sentences as hubs and authorities in the HITS algorithm.
Besides , Wang et al (2008) proposed a language model to simultaneously cluster and summarize documents . Nonnegative factorization was performed on the term-document matrix using the term-sentence matrix as the base so that the document-topic and sentence-topic matrices could be constructed , from which the document clusters and the corresponding summary sentences were generated simultaneously.
3 A Reinforcement Approach to
Multidocument Summarization 3.1 Document Bi-type Graph First of all , let?s introduce the sentence-term bi-type graph model for a set of given documents D , based on which the algorithm of reinforced ranking and clustering is developed.
Let >=< WEVG ,, , where V is the set of vertices that consists of the sentence set },,,{ 21 nsssS ?= and the term set },,{ 21 mtttT ,?= , i.e ., TSV ?= , E is the set of edges that connect the vertices , i.e ., },|,{ VvvvvE jiji ?><= . W is the adjacency matrix in which the element ijw represents the weight of the edge connecting iv and jv .
Formally , W can be decomposed into four blocks , i.e ., SSW , STW , TSW and TTW , each representing a subgraph of the textual objects indicated by the subscripts . W can be written as ??? ? ??? ?=
TTTS
STSS
WW
WW
W , where ),( jiWST is the number of times the term jt appears in the sentence is . )( i,jWSS is is and js . TSW is equal to
T
STW as the relationships between terms and sentences are symmetric . For simplification , in this study we assume there is no direct relationships between terms , i.e ., 0=TTW . In the future , we will explore effective ways to integrate term semantic relationships into the model.
3.2 Basic Ranking Functions
Recall that our ultimate goal is sentence ranking . As an indispensable part of the approach , the basic ranking functions need to be defined first.
3.2.1 Global Ranking ( without Clustering ) Let )( isr ( i=1, 2, ?, n ) and )( jtr ( j=1, 2, ?, m ) denote the ranking scores of the sentence is and the term jt in the whole document set , respectively . Based on the assumptions that ? Highly ranked terms appear in highly ranked sentences , while highly ranked sentences contain highly ranked terms . Moreover , a sentence is ranked higher if it contains many terms that appear in many other highly ranked sentences .? we define )(),()1()(),()( n j
SS m j jSTi srjiWtrjiWsr ?? == ???+??= ?? (1) and )(),()( n i
TSj srijWtr ? = ?= . (2)
For calculation purpose , )( isr and )( jtr are normalized by ? = ? n i i i i sr sr sr 1' ' )( )( )( and ? = ? m j j j j tr tr tr 1' ' )( )( )( .
Equations (1) and (2) can be rewritten using the matrix form , i.e ., ??? ??? ? ? ?= ? ???+? ??= ||)(|| )( )( ||)(|| )( )1( ||)(|| )( )(
SrW
SrW
Tr
SrW
SrW
TrW
TrW
Sr
TS
TS
SS
SS
ST
ST ?? . (3)
We call )( Sr and )( Tr the ? global ranking functions ?, because at this moment sentence clustering is not yet involved and all the sentences/terms in the whole document set are ranked together.
Theorem : The solution to )( Sr and )( Tr given by Equation (3) is the primary eigenvector of SSTSST WWW ??+?? )1( ?? and STSSTS WWIW ????? ?1))1(( ?? , respectively.
Proof : Combine Equations (1) and (2), we get ||)(|| )( )1( ||)(|| )( ||)(|| )( )1( || ||)(|| )( || ||)(|| )(
SrW
SrW
SrWW
SrWW
SrW
SrW
SrW
SrW
W
SrW
SrW
W
Sr
SS
SS
TSST
TSST
SS
SS
TS
TS
ST
TS
TS
ST ? ???+?? ???= ? ???+ ? ?? ? ?? ?= ?? ??)( As the iterative process is a power method , it is guaranteed that )( Sr converges to the primary eigenvector of +?? TSST WW ? SSW ?? )1( ? . Similarly , )( Tr is guaranteed to converge to the primary eigenvector of STSSTS WWIW ????? ?1))1(( ?? . ? 3.2.2 Local Ranking ( within Clusters)
Assume now K theme clusters have been generated by certain clustering algorithm , denoted as },,,{ 21 KCCCC ?= where kC ( k=1, 2, ?, K ) represents a cluster of highly related sentences )( kC CS k ? which contain the terms )( kC CT k ? . The sentences and terms within the cluster kC form a cluster bi-type graph with the adjacency matrix kCW . Let )( kk CC Sr and )( kk CC Tr denote the ranking scores of kCS and kCT within kC . They are calculated by an equation similar to Equation (3) by replacing the document level adjacency matrix W with the cluster level adjacency matrix kCW . We call )( kk CC Sr and )( kk CC Tr the ? withincluster ranking functions ? with respect to the cluster kC . They are the local ranking functions , in contrast to )( Sr and )( Tr that rank all the sentences and terms in the whole document set D . We believe that it will benefit sentence overall ranking when knowing more details about the ranking results at the finer granularity of theme clusters , instead of at the coarse granularity of the whole document set.
136 3.2.3 Conditional Ranking ( across Clusters ) To facilitate the discovery of rank distributions of terms and sentences over all the theme clusters , we further define two ? conditional ranking functions ? )|( kCSr and )|( kCTr .
These rank distributions are necessary for the parameter estimation during the reinforcement process introduced later . The conditional ranking score of the term jt on the cluster kC , i.e ., )|( kCTr is directly derived from kCT , i.e ., =)|( kj Ctr )( jC tr k if kj Ct ? , and 0)|( = kj Ctr otherwise . It is further normalized as ? = = m j kj kj kj
Ctr
Ctr
Ctr )|( )|( . (4)
Then the conditional ranking score of the sentence is on the cluster kC is deduced from the terms that are included in is , i.e ., ? ? ? = = = ? ? = n i m j kjST m j kjST ki
CtrjiW
CtrjiW
Csr 1 1 )|(),( )|( . (5)
Equation (5) can be interpreted as that the conditional rank of is on kC is higher if many terms in is are ranked higher in kC . Now we have sentence and term conditional ranks over all the theme clusters and are ready to introduce the reinforcement process.
3.3 Reinforcement between Within-
Cluster Ranking and Clustering
The conditional ranks of the term jt across the K theme clusters can be viewed as a rank distribution . Then the rank distribution of the sentence is can be considered as a mixture model over K conditional rank distributions of the terms contained in the sentence is . And the sentence is can be represented as a K-dimensional vector in the new measure space , in which the vectors can be used to guide the sentence clustering update . Next , we will explain the mixture model of sentence and use EM algorithm ( Bilmes , 1997) to get the component coefficients of the model . Then , we will present the similarity measure between sentence and cluster , which is used to adjust the clusters that the sentences belong to and in turn modify withincluster ranking for the sentences in the updated clusters.
3.3.1 Sentence Mixture Model
For each sentence is , we assume that it follows the distribution )|( isTr to generate the relationship between the sentence is and the term set T . This distribution can be considered as a mixture model over K component distributions , i.e . the term conditional rank distributions across K theme clusters . We use ki ,? to denote the probability that is belongs to kC , then )|( isTr can be modeled as : ? = ?=
K k kki CTrsTr = =
K k k ki ,? can be explained as )|( ik sCp and calculated by the Bayesian equation ?? )|()|( kiik CspsCp )( kCp , where )|( ki Csp is assumed to be )|( ki Csr obtained from the conditional rank of is on kC as introduced before and )( kCp is the prior probability.
3.3.2 Parameter Estimation
We use EM algorithm to estimate the component coefficients ki ,? along with )}({ kCp . A hidden variable zC , },,2,1{ Kz ?? is used to denote the cluster label that a sentence term pair ),( ji ts are from . In addition , we make the independent assumption that the probability of is belonging to kC and the probability of jt belonging to kC are independent , i.e ., ?= )|()|,( kikji CspCtsp )|( kj Ctp , where )|,( kji Ctsp is the probability of is and jt both belonging to kC . Similarly , )|( kj Ctp is assumed to be )|( kj Ctr .
Let ? be the parameter matrix , which is a Kn ? matrix }{ , kiKn ?=? ? ;,,1( ni ?= ),,1 Kk ?= . The best ? is estimated from the relationships observed in the document bi-type graph , i.e ., STW and SSW . The likelihood of generating all the relationships under the parameter ? can be calculated as : ???? = == = ???= ???=? n i n j jiW ji n i m j jiW ji
SSSTSSST
SSST ssptsp
WpWpWWL 1 1 ),( 1 1 ),( ' )|,()|,( )|()|(),|( and jt both belong to the same cluster , given the current parameter . As )|,( ? ji ssp does not contain variables from ? , we only need to consider maximizing the first part of the likelihood in order to get the best estimation of ? . Let )|( STWL ? be the first part of likelihood.
Taking into account the hidden variable zC , the complete loglikelihood can be written as ( ) ( ) ( )?? ?? ?? = = = = = = ???= ???= ?=? n i m j zjiZST n i m j zzji n i m j jiW zjiZST
CptspjiW
CpCtsp
CtspCWL jiSTW
ST 1 1 1 1 1 1 ),( )|(),( log ),( )|(),|,( log )|,,( log),|(log ),( .
In the E-step , given the initial parameter 0? , which is set to Kki expectation of loglikelihood under the current distribution of ZC is : ??? ??? = = = = = = ? ?=??=? +?=??= ?=?? n i
K k m j jikzkzST
K k n i m j jikzjikST
ZSTWCf tsCCpCCpjiW tsCCptspjiW
CWLEQ
STZ 1 1 1 ),,|()),( log (),( ),|(( log ),( 0 The conditional distribution in the above equation , i.e ., ),,|( 0?= jikz tsCCp , can be calculated using the Bayesian rule as follows : )()|()|( )|(),|,( ),,|( kzkzji jikz
CCpCtpCsp
CCpCCtsp tsCCp =? ?=?=? ?= . (7)
In the M-Step , we first get the estimation of )( kz CCp = by maximizing the expectation ),( 0??Q . By introducing a Lagrange multiplier ? , we get the equation below.
?=?=+??=? ? ? = 0)]1)((),([ )( 1 k kz kz
CCpQ
CCp ? ?? = = =+?== n i m j jikz kz
ST tsCCpCCp jiW 1 1 0 0),,|( )(
Thus , the estimation of )( kz CCp = given previous 0? is ?? ?? = = = = ?= == n i m j
ST n i m j jikzST kz jiW tsCCpjiW
CCp 1 1 1 1 ),,|(),( )( . (8)
Then , the parameters ki ,? can be calculated with the Bayesian rule as ? = = ==
K l lzli kzki ki
CCpCsp
CCpCsp )()|( )()|(? . (9)
By setting ?=?0 , the whole process can be repeated . The updating rules provided in Equations (7)-(9) are applied at each iteration.
Finally ? will converge to a local maximum.
A similar estimation process has been adopted in ( Sun et al , 2009), which was used to estimate the component coefficients for author-conference networks.
3.3.3 Similarity Measure
After we get the estimations of the component coefficients ki ,? for is , is will be represented as a K dimensional vector ,,,( 2,1, ? iiis ??= ), Ki ? . The center of each cluster can thus be calculated accordingly , which is the mean of is for all is in the same cluster , i.e ., || k
Cs i
C
C s
Center kik ? ?= , where || kC is the size of kC .
Then the similarity between each sentence and each cluster can be calculated as the cosine similarity between them , i.e ., ?? ? == ==
K l C
K l i
K l Ci ki lCenterls lCenterls
Cssim k k )()( ),( . (10)
Finally , each sentence is reassigned to a cluster that is the most similar to the sentence.
Based on the updated clusters , withincluster ranking is updated accordingly , which triggers the next round of clustering refinement . It is expected that the quality of clusters should be improved during this iterative update process since the similar sentences under new attributes will be grouped together , and meanwhile the quality of ranking will be improved along with the better clusters and clustering.
3.4 Ensemble Ranking
The overall sentence ranking function f is defined as the ensemble of all the sentence conditional ranking scores on the K clusters.
? = ?=
K k kiki Csrsf where k ? is a coefficient evaluating the importance of kC . It can be formulated as the normalized cosine similarity between a theme cluster and the whole document set for generic summarization , or between a theme cluster and a given query for querybased summarization.
]1,0[?k ? and ? = =
K k k
Figure 2 below summarizes the whole process that determines the overall sentence ensemble ranking scores.
Input : The bi-type document graph >=< WETSG ,,? , ranking functions , the cluster number K , 1=? , 001.0=Tre , 10=IterNum .
Output : sentence final ensemble ranking vector )( Sf .
1. 0?t ; 2. Get the initial partition for S , i.e . tkC , Kk ?,2,1= , calculate cluster centers t kC
Center accordingly.
3. For ( t=1; t<IterNum && Tre >? ; t ++) 4. Calculate the withincluster ranking )( kk CC Tr , )( kCkC Sr and the conditional ranking )|( ki Csr ; 5. Get new attribute is for each sentence is , and new attribute t kCCenter for each cluster t kC ; 6. For each sentence is in S 7. For k=1 to K 8. Calculate similarity value ),( tki Cssim 9. End For 10. Assign is to 10 + t kC , ),( maxarg0 t kik Cssimk = 11. End For 12. || max 1 t kC t kCk
CenterCenter ?= +? 13. 1+? tt 14. End For 15. For each sentence is in S 16. For k=1 to K 17. ? = ?=
K k kiki Csrsf 18. End For 19. End For Figure 2. The Overall Sentence Ranking Algorithm 3.5 Summary Generation In multidocument summarization , the number of documents to be summarized can be very large . This makes information redundancy appears to be more serious in multidocument summarization than in single-document summarization . Redundancy control is necessary . We apply a simple yet effective way to choose summary sentences . Each time , we compare the current candidate sentence to the sentences already included in the summary.
Only the sentence that is not too similar to any sentence in the summary ( i.e ., the cosine similarity between them is lower than a threshold ) is selected into the summary . The iteration is repeated until the length of the sentences in the summary reaches the length limitation . In this paper , the threshold is set to 0.7 as always in our past work.
4 Experiments and Evaluations
We conduct the experiments on the DUC 2004 generic multidocument summarization dataset and the DUC 2006 querybased multidocument summarization dataset . According to task definitions , systems are required to produce a concise summary for each document set ( without or with a given query description ) and the length of summaries is limited to 665 bytes in DUC 2004 and 250 words in DUC 2006.
A well-recognized automatic evaluation toolkit ROUGE ( Lin and Hovy , 2003) is used in evaluation . It measures summary quality by counting overlapping units between system-generated summaries and human-written reference summaries . We report two common ROUGE scores in this paper , namely ROUGE1 and ROUGE2, which base on Unigram match and Bigram match , respectively.
Documents and queries are preprocessed by segmenting sentences and splitting words . Stop words are removed and the remaining words are stemmed using Porter stemmer.
4.1 Evaluation of Performance
In order to evaluate the performance of reinforced clustering and ranking approach , we compare it with the other three ranking approaches : (1) Global-Rank , which does not apply clustering and simply relies on the summary sentences ; (2) Local-Rank , which clusters sentences first and then rank sentences within each cluster . A summary is generated in the same way as presented in ( Qazvinian and Radev , 2008). The clusters are ordered by decreasing size ; (3) ClusterHITS , which also clusters sentences first , but then regards clusters as hubs and sentences as authorities in the HITS algorithm and uses the obtained authority scores to rank and select sentences.
The classical clustering algorithm Kmeans is used where necessary . For querybased summarization , the additional query-relevance ( i.e . the cosine similarity between sentences and query ) is involved to rerank the candidate sentences chosen by the ranking approaches for generic summarization.
Note that Kmeans requires a predefined cluster number K . To avoid exhaustive search for a proper cluster number for each document set , we employ the spectra approach introduced in ( Li et al , 2007) to predict the number of the expected clusters . Based on the sentence similarity matrix using the normalized 1-norm , for its eigenvalues i ? ( i=1,2, ?, n ), the ratio )1(/ 21 ?= + ???? ii is defined . If 05.01 >? + ii ?? and i ? is still close to 1, then set K=i+1. Tables 1 and 2 below compare the performance of the four approaches on DUC 2004 and 2006 according to the calculated K.
DUC 2004 ROUGE1 ROUGE2
Reinforced 0.37082 0.08351
ClusterHITS 0.36463 0.07632
Local-Rank 0.36294 0.07351
Global-Rank 0.35729 0.06893
Table 1. Results on the DUC 2004 dataset
DUC 2006 ROUGE1 ROUGE2
Reinforced 0.39531 0.08957
ClusterHITS 0.38315 0.08632
Local-Rank 0.38104 0.08841
Global-Rank 0.37478 0.08531
Table 2. Results on the DUC 2006 dataset It is not surprised to find that ? Global-Rank ? shows the poorest performance , when it utilizes the sentence level information only whereas the other three approaches all integrate the additional cluster level information in various ways . In addition , as results illustrate , the performance of ? ClusterHITS ? is better than the performance of ? Local-Rank ?. This can be mainly credited to the ability of ? ClusterHITS ? to consider not only the cluster-level information , but also the sentence-to-cluster relationships , which are ignored in ? Local-Rank ?. It is happy to see that the proposed reinforcement approach , which simultaneously updates clustering and ranking of sentences , consistently outperforms the other three approaches.
4.2 Analysis of Cluster Quality
Our original intention to propose the reinforcement approach is to hope to generate more accurate clusters and ranking results by mutually refining withincluster ranking and clustering . In order to check and monitor the variation trend of the cluster quality during the iterations , we define the following measure ? ?= ?= ?? ?=
K k
K kll ji
CsCs ki
Cs sssim
Cssim quan ljki ki ) ),( min ),( min ( , (12) where ),( min ki
Cs
Cssim ki ? denotes the distance between the cluster center and the border sentence in a cluster that is the farthest away from the center . The larger it is , the more compact the cluster is . ),( min , ji
CsCs sssim ljki ?? , on the other hand , denotes the distance between the most distant pair of sentences , one from each cluster . The smaller it is , the more separated the two clusters are . The distance is measured by cosine similarity . As a whole , the larger quan means the better cluster quality.
Figure 3 below plots the values of quan in each iteration on the DUC 2004 and 2006 datasets.
Note that the algorithm converges in less than 6 rounds and 5 rounds on the DUC 2004 and 2006 datasets , respectively . The curves clearly show the increasment of quan and thus the improved cluster quality.
0 0.5 1 2 3 4 5 6IterNum
Q ua n
DUC2004 DUC2006
Figure 3. Cluster Quality on DUC 2004 and 2006 the generated clusters , we are also quite interested in whether the improved clusters quality can further enhance the quality of sentence ranking and thus consequently raise the performance of summarization . Therefore , we evaluate the ROUGEs in each iteration as well . Figure 4 below illustrates the changes of
ROUGE1 and ROUGE2 result on the DUC 2004 and 2006 datasets , respectively . Now , we have come to the positive conclusion.
0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4 1 2 3 4 5 6
IterNum
R
O
U
G
E -1
DUC2004 DUC2006 0.045 0.05 0.055 0.06 0.065 0.07 0.075 0.08 0.085 0.09 0.095 1 2 3 4 5 6
IterNum
R
O
U
G
E-
Figure 4. ROUGEs on DUC 2004 and 2006 4.3 Impact of Cluster Numbers In previous experiments , the cluster number is predicted through the eigenvalues of 1-norm normalized sentence similarity matrix . This number is just the estimated number . The actual number is hard to predict accurately . To further examine how the cluster number influences summarization , we conduct the following additional experiments by varying the cluster number . Given a document set , we let S denote the sentence set in the document set , and set K in the following way : || SK ?= ? , (13) where )1,0(?? is a ratio controlling the expected cluster number . The larger ? is , the more clusters will be produced . ? ranges from 0.1 to 0.9 in the experiments . Due to page limitation , we only provide the ROUGE1 and ROUGE2 results of the proposed approach , ? ClusterHITS ? and ? Local-Rank ? on the DUC 2004 dataset in Figure 5. The similar curves are also observed on the 2006 dataset.
0.355 0.36 0.365 0.37 0.375 0.38 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
R
O
U
G
E -1
ClusterHITS Local Rank Reinforced ? 0.072 0.075 0.078 0.081 0.084 0.087 0.09 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
R
O
U
G
E -2 ?
Figure 5. ROUGEs vs .? on DUC 2004
It is shown that (1) the proposed approach outperforms ? ClusterHITS ? and ? Local-Rank ? in almost all the cases no matter how the cluster number is set ; (2) the performances of ? ClusterHITS ? and ? Local-Rank ? are more sensitive to the cluster number and a large number of clusters appears to deteriorate the performances of both . This is reasonable.
Actually when ? getting close to 1, ? Local-Rank ? approaches to ? Global-Rank ?. These results demonstrate the robustness of the proposed approach.
5 Conclusion
In this paper , we present a reinforcement approach that tightly integrates ranking and clustering together by mutually and simultaneously updating each other.
Experimental results demonstrate the effectiveness and the robustness of the proposed approach . In the future , we will explore how to integrate term semantic relationships to further improve the performance of summarization.
Acknowledgement
The work described in this paper was supported by an internal grant from the Hong
Kong Polytechnic University ( G-YG80).

J . Bilmes . 1997. A Gentle Tutorial on the em Algorithm and Its Application to Parameter Wstimation for Gaussian Mixture and Hidden Markov Models . Technical Report ICSI-TR-97-02, University of Berkeley.
Brin , S ., and Page , L . 1998. The Anatomy of a Large-scale Hypertextual Web Search Engine . In
Proceedings of WWW1998..
Harabagiu S . and Lacatusu F . 2005. Topic Themes for Multi-Document Summarization . In
Proceedings of SIGIR2005.
Hardy H ., Shimizu N ., Strzalkowski T ., Ting L.,
Wise G . B ., and Zhang X . 2002. Cross-
Document Summarization by Concept
Classification . In Proceedings of SIGIR2002.
Jon M . Kleinberg . 1999. Authoritative Sources in a Hyperlinked Environment . In Proceedings of the 9th ACM-SIAM Symposium on Discrete
Algorithms.
Karypis , George , Vipin Kumar and Michael Steinbach . 2000. A Comparison of Document Clustering Techniques . KDD workshop on Text
Mining.
Lin , C . Y . and Hovy , E . 2000. The Automated Acquisition of Topic Signature for Text Summarization . In Proceedings of COLING2000.
Li W.Y ., Ng W.K ., Liu Y . and Ong K.L . 2007.
Enhancing the Effectiveness of Clustering with Spectra Analysis . IEEE Transactions on Knowledge and Data Engineering ( TKDE).
19(7): 887-902.
Li , F ., Tang , Y ., Huang , M ., Zhu , X . 2009.
Answering Opinion Questions with Random
Walks on Graphs . In Proceedings of ACL2009.
Otterbacher J ., Erkan G . and Radev D . 2005. Using RandomWalks for Question-focused Sentence Retrieval . In Proceedings of HLT/EMNLP 2005.
Qazvinian V . and Radev D . R . 2008. Scientific paper summarization using citation summary networks . In Proceedings of COLING2008.
Sun P ., Lee J.H ., Kim D.H ., and Ahn C.M . 2007.
Multi-Document Using Weighted Similarity Between Topic and Clustering-Based Nonnegative Semantic Feature . APWeb/WAIM 2007.
Sun Y ., Han J ., Zhao P ., Yin Z ., Cheng H ., and Wu T . 2009. Rankclus : Integrating Clustering with Ranking for Heterogenous Information Network Analysis . In Proceedings of EDBT 2009.
Wang D.D ., Li T ., Zhu S.H ., Ding Chris . 2008a Multi-Document Summarization via Sentence-Level Semantic Analysis and Symmetric Matrix Factorization . In Proceedings of SIGIR2008.
Wang D.D ., Zhu S.H ., Li T ., Chi Y ., and Gong Y.H.
2008b . Integrating Clustering and Multi-Document Summarization to Improve Document Understanding . In Proceedings of CIKM 2008.
Wan X . and Yang J . 2006. Improved Affinity Graph based Multi-Document Summarization . In
Proceedings of HLT-NAACL2006.
Zha H . 2002. Generic Summarization and Key Phrase Extraction using Mutual Reinforcement
Principle and Sentence Clustering . In
Proceedings of SIGIR2002.
142
