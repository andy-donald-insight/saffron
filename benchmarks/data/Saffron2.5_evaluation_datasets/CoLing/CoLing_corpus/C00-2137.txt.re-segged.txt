More accurate tests Ibr the statisticals ignificance of result 
differences *
Alexander Yeh
Mitre Corp.
202 Burlillgl ; on Rd.
Bedford , MA 01730

asy ~ mitrc.org

Statisti(:a , 1 signiticance testing of ( litl'e relmeS in v ; ~hl(`-s of metri(:s like recall , i ) rccision and bat-au ( : ( ~ ( lF-s ( : ( )rc is a ne ( : ( `- ssaryt ) art of eml ) iricalual ; ural language 1) ro (: essing . Unfortunately , we lind in a set of ( ; Xl ) erin lc\]d ; s( ; hal ; many (: ore-inertly used tesl ; softe , n underestimate t . he significanc can ( l so are less likely to detect differences that exist  1  ) el ; we end it l ' ercnt techniques . This undel'esi ; imation comes from an in(let ) end cn ('( ~ a , -; SU lnl ) tion that is often violated . \~fel ) oint out some usefull ; e , % s(;hal ; ( lonol ; make this assuml ) - lion , including computationally--int cnsiveran-d()mizat , ion1; cs ; s . 
1 Introdu(-tion
In Clnl ) irical natural \] al~gUag(~l ) rocessing , on (' , is ot't cal : ('~ st ; ing whether some new technique 1 ) ro ( lu ( ' esim\] ) rove ( ll ' esull ; s(asm casur(xl\])yone()1'1110 17(`-IIICI ; I'\]CS ) Oitson Ici ; esl ; (lai ; ~L set ; \ V \] l(`-ll ( ; Olll\])a I'e(lli ( ) sollle( ; llrrellt(l ) as clilm ) l ; c(:\]lnique . 
\5/\] lell , \] lelCslll ; s are better with the new tcch-ni ( lUe , a question arises as t ( ) wh (' , l ; h( ; r these l : (`-- sult ;   ( litl'eren ( : esaredu et ( ) the new technique a ( :t ; ually 1) eingl ) cl ; t('x or just ; due 1 ; o(:han(:e . Un-t ' or tmmtely , one usually Callll()t ) directly answer the qnesl ; ion " what is the 1) robat fility that 1 ; 11( ; now l ; ( x:hni(luC , ist ) el ; lx ~ r givelll ; he results on the t(' , sl , dal ; as ol ; ": I ) (new technique is better \ [ test set results ) \] ~ ul ; with statistics , one cml answer the following proxy question : if the new technique was a  ( > tually noditt ' eren than the old t ( ' , (' hnique(( ; he * This paper reports on work l ) erfonncd at the MITR1 , ; Corporation under the SU l ) porl : of the MIT Il J , ;  , qponsored Research l ) rogrmn . Warren Grcit\[ , l , ynette Ilirschlnmb Christilml ) or all , Johnllen ( lerson , Kelmeth Church , Tedl ) unning , Wessel Kraaij , Milch Marcus and an anonymous reviewer l ) rovided hell ) rid suggestions . Copyright@2000 The MITRE Corl ) oration . All rights r(~s(n'vcd . 
null hyl ) othesis ) , wh ~ tt is 1:11( ;  1 ) robat ) ility that the results on the test set would l ) e at least this skewed in the new technique's favor  ( Boxeta \] . , 1978, So(: .  2 . 3) ? Thai ; is , what is P ( tests e , t results at least this skew ( ' A in the new techn i ( lue's favor I new technique is no ( liffer cnt than the old ) If the i ) robtfl ) ility is small enough ( 5% off ; on is used as the threshold ) , then one will r qiect the millhy i ) otheMs and say that the differences in 1 ; he results are :' sta . t isl ; i cally sigl filicant " aI ; that thrt , shold level . 
This 1 ) al ) (n " examines some of th ( `- 1 ) ossil ) leme ? hods for trying to detect statistically sign if '- leant difl'el'enc  ( `- s in three commonly used met-l'i ( : s:tel'all , 1) re('ision and balanced Fscore . 
Many of these met ; Ire(Isarc foun(t to be i ) rol ) lem-a . ti (" illa , so , t ; of eXl ) er in w , nts that are performed . 
The s(~methods have a , tendency toull de resti-mat(`-th (' , signili(:ance , of the results , which tends t()1 hake one , 1) elieve thai ; some new techni ( tucis no 1)el ; l ; erl ; lmn the (: urrent technique even when il ; is . 
This mtder estimate comes fl ' om these lnch-ells assuming l  ; hat ; the te ( :hlfi ( tues being con > lmr cd produce indepen ( lc , nt results when in our eXl ) eriments , the techniques 1 ) eing COml ) ared tend to 1 ) reducel ) ositively corr ( `-lated results . 
To handle this problem , we , point out some st ~ t tistical tests , like the lnatche ( t-pair t , sign and Wilcoxon tests ( Harnett ,  1982 , See .  8 . 7 and 15 . 5), which do not make this assuln ption . OneCall ITS (' , l ; llcse testsOllI ; hcrecalln lel ; ric , but l ; he precision an ( l1 ) alanced Fscore metric have too COml ) lex at brm for these tests . For such com-1) lexlne ; ri( ; s ~ wellSea coln plll ; e-in ; Clisiv ( ~ randomization test ( Cohen ,  1995 , Sec .  5 . 3) , which also ~ tvoids this indet ) en ( lence assmnption . 

The next section describes many of the standard tests used and their problem of assuming certain forms of independence  . The first subsec-rio11 describes tests where this assumption appears in estimating the standard deviation of the difference between the techniques'results  . 
The second subsection describes using contingency tables and the X  2 test . Following this is a section on methods that do not  1hake this independence assumption . Subsections in turn describe some analytical tests  , how they can apply to recall but not precision or the Fscore  , and how to use randomization tests to test precision and Fscore  . We conclude with a discussion of dependencies within a test set's instances  , a topic that we have yet to deal with . 
2 Tests that assume independence between compared results  2  . 1 F ind ing and using the variance of a result difference For each metric  , after determining how well a new and current techniquet  ) efforms on solne test set according to that metric  , one takes the difl brence between those results and asks " is that difference significant ? " A way to test this is to expect  11o difference in the results ( the null hypothesis ) and to ask , assuming this expectation , howmmsual are these results ? One way to answer this question is to assulne that the diff b  , rence has a normal ort distribution ( Box et al ,  1978 , Sec .  2 . 4) . The none calculates the following : ( d-Z\[4 ) / s d = d / , ~ , ~  ( 1 ) where d = xl- x2 is the difference found between xl and x2  , the results for the new and current echniques , respectively . E\[d\] is the expected difference ( which is 0 under the null hypothesis ) and Sd is an estimate of the standard deviation of d  . Standardeviation is the square root of the variance  , a measure of how much a random variable is expected to vary  . The results of equation 1 are compared to tables ( c . f . in Box et al (1978 , Appendix ) ) to find out what the chances are of equaling or exceeding the equation  1 results if the null hypothesis were true . 
The larger the equation 1 results , the more unusual it would be under the null hypothesis  . 
A complication of using equation 1 is that one usually does not have Sd , but only stands 2 , where Sl is the estimate for Xl'S standard deviation and similarly for  s2  . I low does one get the former fi ' om the latter ? It turns out that  ( Box et al ,  1978 , Ch .  3 ) o -2   o-12 + a~d = -- 2p12a10-2 where cri is the true standardeviation ( instead of the estimates i ) and pl'2 is the correlation coefficient between xl and : c2  . Analogously , it turns out that 2 zSd 82 - t - 82  - -  2   r128182   ( 2 ) where r12 is an estimate for P12 . So not only does crd ( and Sd ) depend on the properties of xl and x2 in isolation , it also depends on how Xl and . ~'2 interact , as measured by P12 ( and ' rr)) . When Xl and x2 are independent , p12 = 0 , and then ( Td = ~-+ c7~ and analogously , Sd = ~+ s ~ . When P~2 is positive ,   ; 1 ;   1 and x2 are positively correlated : arise in xl or x2 tends to be accompanied by arise in the other result  . When P12 is negative , : cl and x2 are negatively correlated : arise in : clor x9 tends to be accompmfied by a decline in the other result  . -1 < P 12 <1 ( Larsen and Marx , 1986,
Sec . 10.2).
The assuln ption of ' independence is often used in for nlnl as to determine the statistical significance of the difference d =  . ~: 1-x2 . But how accurate is this assumption ? Onenfight expects onic positive correlation from both results coming from the same test set  ;  . One may also expect some positive correlation when either both techniques are just variations of each other  1 or both techniques are trained on the same set of training data  ( and so are missing the same examples relative to the test set  )  . 
This assumption was tested during some experiments for finding granunatical relations  ( subject , object , various types of nxodifiers , etc . ) . The metric used was the fraction of the relations of interest in the test set that were recalled  ( tbund ) by some technique . The relations of interest were w ~ riou subsets of the  748 relation instances in that test set . An example subset is all the modifie relations . Another subset is just that of all the time modifie relations  . 
1 These var ia t ions are often des igned to usual ly behave ill the stone way and only differ in just a few cases  . 

First , two difl'erent e(:hniques , oneltle lllory-t ) ased and the other tl ' ansti ) rlnation-rule based , wei " e trained on the same training set , and then both test e(1 on that ticst set ; . l  ~ . e (: alle onlt ) a . risons we , remadet brten subsets of tim relations and the r12 was found for each cOral ) arisen . From
Box et al (1978, Ch . 3) "'12 = ~( I , Jlk--Yl )   ( ~2k--~2 )  /  ( ' glt~2 ( 71--l .   )   ) k where Yil ~=\] if the ith technique recalls the tcl  ; h relation and = 0 if not . ' lz , is then mnl ) cr of relations in the subset . !\] i and s i are mean and stmJ(lard de , vial , ionestimate . s(based on the Yik'S ) , rest ) ectively , fl ) r the ith technique . 
For the ten subsets , only ClioCOlnl ) arison hada'r12 ( :: lose to 0 ( It was -0 . 05) . The other nine c()ml ) arisons had'r12's1) etw(x' , n0 . 29 and 0 . 53 . 
The tencoral ) arise ninedian value was 0.38.
Next ; , the transformatiol > rulct ) ased t . cch-nique was rUll with difl'erent sets of start ing conditions and/or different  , but overlapl ) ing , subsets of the training set . Recall comparisons were ma ( le on the same test ( lata . set 1) etweenl ; he diff crent variations . Many of the comparisons were , of how well two wu : iations recalled a particular subset of the relations  . A total of 40 comparisons were made . . The ' r\]2's on all d0 were 1)osi-tire . 3 of the ' r , 2' sw (' ~ reill the 0 . 20-0 . 30 range . 
24 of the rj2' swore in the 0 . 50--0 . 79 range . 13 of the ' r \] 2' s were in the 0 . 80-1 . 0 () range . 
So in our ext)er in ~ ents , we were usually eom-t ) aring 1 ) ositivcly correlated results . How much error is introdu ( :e ( tt ) y assuming independence ? An easy--to-analyze case is when the standard devial  , ions for the results being eoml ) are da:t'c the same . ? ~' Jhen equation 2 reduces to s ,   , - sV/2 ( l-r12) , where s = sl = , s'2 . If one , assumes there . sultsm ' eind cpel:dent ( ~/ S Slllller , 2 = 0) , then sd :- ~ . sv / 22 . Call this w flue sd-i , 7, g . 
As flu increases in value , Sd decreases : \[() . 38 d 0 . T87 ( sd_i , , d)1 . 27\[p . aoI1 . 41\[O . 80 J 0 . 447(Sd . __i . ,, . d ) 2 . 2 4 ' l'herightmost cohunn above indicates the magnitude by which erroneously assuming indepen-> \[ lifts is actually roughly true in the coml  ) arisons nm de , and is assumed to be true in many of the standard
Wsts for statistical significance.
(lence ( using 8d_indill1 ) lace of sd ) will increase the standardeviation estimate . In equation 1 , sd forms the denominator of the ratiod / . sd . So erroneously assmning independence will mean that the mmmrator d  , the difference between the two results : will nee ( t to increase by that same factor in order f ( )r equation 1 to have the same w tlue as without the indel ) endence as smnt ) tion . 
Since the value of that equation indicates the statistical significance of d  , assunfing independence will mean that e1 will have to be larger than without the . assumption to achieve the same al ) parent level of statistical significance . 
l ? rolntile tal)le above , when r12 = 0 . 50, (1 will need to 1) cabout 41% larger . Another way to look at this is that assuming indei  ) en ( lenee will make the same . v ~ due , of d appear less statist ; i-cally signifieal tt . 
The common tests of statistical significance use this assumt  ) tion . The , tesl ; klloWlt as the 1, ( Boxet ; al . , 1978, Sec .  4 . 1) or two-saml ) let ( Harnett , 1982, See .  8 . 7) test does . This test uses equation 1 and then compares the resulting va . lue against het ; distribution tal)les . This test has a ( : Oml ) licated form for sdl ) eeause:1 . : c ! and : c2cant ) e1 ) ased on ( tiffering num-1 ) ersofs aml ) les . Call these retail ) or s'n ~ and ' n2r ( ; sl)ectivcly . 
2 . 111 lthist (; st , the zi's are each an nisam-pie average , of alt other varial ) le ((: allityi ) . 
'\[' his is important because the si's in this test are standm'd deviation estimates tor the yi's  , not the xi's . The relationship between them is that si for Jli is the same as  ( for : , : , : . 
3 . The test itself assumes that !11 and Y2 have the same standardeviation ( call this common values )  . The denominator estimates , s using a weight e(1 average of 81 and s2 . 
The weighting is b~sed on nl and r7,2.
From Harnett (1982, Scc . 8.7), the denominator
Sd~-nl+n2-2 711-br ~ , 2)' i7 , 177 , 2When'nl='n2(call this common value'n ) , ' ~1 and s2 will be given equal weight , and Sdsiml ) li-fie . sto ~+, s'~)/n . Making the substitution described above of si v/57 t brsi leads to an S d of independence assumption . 
Another test that both makes this assulnt ) -tion and uses at brm of equation 1 is a test t br bin onlial data ( Harnett ,  1982 , Sec .  8 . 1 . 1 ) which uses the " t'aet " that binomial distributions tend to approximate normal distributions  . In this test , the zi's being compared are the fraction of the items of interest that are recovered by the ith technique  . In this test , the denominators d of equation 1also has a complicated fbrm , both due to the reasons mentioned for the t , test above and to the fact that with a binomial distribution  , the standardeviation is a flmction of the number of samples and the mean wflue  . 
2 . 2 Using cont ingency tab les and X 2 to test precision A test that does not use equation  1 but still makes an assunlption of independence l ) etweena : landa : u is that of using contingency tables with the chi-squared  0  , 52) distribution ( Box et al . , 1978, Sec .  5 . 7) . When tile assmnption is valid , this test is good for comparing differences ill the pr'ecision metric  . Precision is the fraction of the items " Ibund " 1  ) y some technique that are actually of interest . Precision = l ~ , /( I  ~ , + S ) , where R is the number of items that are of interest and m'e Recalled  ( fbund ) by tile technique , and S is them unber of items that are found by tile technique that turn out to be Spurious  ( not of interest )  . One can test whether the precision results from two techniques are different by using a  2 x 2 contingency table to test whether the ratio R/S is different for the two techniques  . 
One make stile latter test , by seeing if tile assumption that the ratios for the two techniques are the same  ( the null hypothesis ) leads to a statistically significant result when using a X  2 distribution with one degree of freedom . A2x2 table has 4 cells . The top 2 cells are filled with the R and S of one technique and the bottom  2 cells get the R and S of the other technique . In this test , the valu c in each cell is assumed to have a Poisson distribution  . When the cell values are not too small , these Poisson distributions are approximately Normal  ( Gaussiml )  . As a result , when the cell values are independent , smnming tlle normalized squares of the difference between each cell and its expected value leads to aX  2 distribution ( Boxel ; al . , 1978, Sec .  2 . 5-2 . 6) . 
How well does this test work in our experiments ? Precision is a nonlinear time  ( ion of two random wu'iables R and S , so we did not try to estimate the correlation coefficient \]' or precision  . 
However , we can easily estimate the correlation coefficients for the R's  . They are ther 12's found in section 2 . 1 . As that section mentions , the r12's fbund are just about always positive . So at least in our experiments , the R's are not ill-dependent , but are positively correlated , which violates the assumptions of the test . 
An example of how this test behaves is the following comparison of the precision of two different methods at finding the modifier elations using tiles tone training and test set  . The corm-lation coefficient estilnate to rR is 0 . 35 mid the data is
Method 17 , 5' t ? recision 147484 ! ) % 225 1464% Placing thel ~ , and S values into a 2 x 2 table leads to a X 2 value of 2  . 38 . a Att degree of freedom , tile X 2 tables indicate that if the null hypothesis were true  , there would 1 ) e a 10% to 20% chance of producing a X 2 value at least this large . So according to this test , this nnlch of an observed difference in precision wouht not be unusual if no actual differ  (  , ncc in the precision exists between the two nw , thods . 
This test assumes independence bt ween the/~ , wdues . When we use a 22 ( I ( =1048576 ) trial approxima term l domization test ( section 3 . 3) , which makes no such assumptions , then we find that this latter test indicates that under the null hypothesis  , there is less than a 4% chance of producing a difference in precision results as large as the one observed  . So this latter test indicates that this nmch of an observe difference in precision would be mmsual if no actual difference ill the precision exists between the two methods  . 
It should be mentioned that the manner of testing here is slightly different hant he manner in the rest of this paper  . The X 2 test looks at the square of the difference of two results  , and rejects them ill hylm thesis ( the compared techniques are the same ) when this square is a \ Ve do not use Yate's adjustment to compensate l br the numbers in the table being integers  . 1) oing so would lmve made the results even worse . 
950 large , whel ; he , rl ; lm largeness is (: a used l ) y t ; he new t ; eehn i(luet ) l " o(lucing'a , much l ) ( fl ; l ; erresult ; titanl ; he current , l ; e(:hlfique or vice versa . So 1 , ol ) efair , we eoln l ) are dl ; he X2 resull ; s with al ; wo-sided version of l ; hcrml don~iz~fl ; iont ; esl , : es-l;inm ; e , l ; he likelihood glu ~ l ; l ; he obsea'ved magni-l ; u(leoft , here sull ;   ( lifl'eren ( : e would 1 ) c matched or exceeded ( regardless of ' which l ; echnique produced l ; hebetl ; erresull ; ) raider them illhyl ) oth-es is . A one-sided version of the test ; , which is colnt ) aral ) let ; o what we use in l ; here st of the t ) a-per , esl ; inml ; esl ; he likelihood of a ( tifferenl ; oul ; -come undert ; henull hyt ) o Chesis : that of m~l : cll-ing or exceeding t ; he ( lit\['erence of howlllchl )? , l ; teri ; he new ( possibly 1) ett , er ) l ; e(:lmi(lue'soh-s('a'ved result is than l ; he currenl ; l ; e('hnique'so)-serve , (1l'esull; , htt ; heahoy (; scenario , a one-sided t(;sl ; t)rodu(:es~3(~ , tigure insl ; ead of s~d : % figure . 
3 Tests without that independence assumption a . 1 Tests for matched pairs At ; l ; his point , one may wonder it ' all st ; al ; isl ; icalt ; CSt ; Slllil\](eSllC\]ls/ , \] lintle peal denct ~ as Sllltl\])l ; ioll . 
Th (' , miswer is no , lml ; t ; \] l ( ) selesl:sl ; hal ; ( tonol ; lltC~Slll ; e , howlll lllch ; wel ; e(;\]l ni ( lllesilll ; (' , ra(:l ; ( toneedi ; olm tke , some as smnpl ; io Hal ) oul ; t ; \] m ; ill-I ; ( ; r~t('l ; ionmid l ; yl)it:a . llEl ; \] ml ; assuml ) l ; iollisin(te-t ) ( ~\] ldell ( ; e . ' Fhose I ; esl ; sI ; ll ~ H ; Hol ; i( ; c in S ( ) lll(~\\r ; ~ Br how much l ; wotc(:hniqucshm ; ra(:l ;  ( ; ~1 , 11 lib ( ; ~ h ( ) seol ) servations in sl ; ead of relying on assumt ) l:ions . 
Onew ' , ~ yt ; omeasure how 1 ; wel ; e(:lmi(lucsin-i ; erac ( ; is 1 ; ocomtm . re\]to wsimilarly (; he , t;wot ; ecl > ni ( tuestea . el ; 1; ovarious l ) arl ; s()f1; hel ; (; s\[; seA; . 
'_l . " his is done in the mal ; t:hed-lm . ir 1, I ; esl ; ( Hm'-nctl ;, 1982, Se(: .  8 . 7) . This l ; csI ; tin(ls the dith'a'-once bet ; we , n h o w t ; eclmiques 1 and 2 l ) eribrmone ~ t (:: hl ; esl ; set , Saml ) le . The / , dist ; ri)ul ; ion and a fOI'l no feqm~l ; ionlm:e used . The nullly l ) ol ; h-esis is st ; i l l l ; \]l~tl ; ~ hemt meral ; ord\]ms ~ t0 me , m , butel is now l ; hest unof these difference values ( divided 1) yt ; he number of Smnl ) les ) , instead of being : r ~-: re . Similm ' ly , the ( lenomim d ; or . sd is no wesl ; in ml ; ingl ; hesi ; a . ndm'd(leviation of l ; hesedifl'e reneew dues , instead of being a funcl ; ion of s : l and su .  ' . Flf is means for example , (; hal ; even if t ; lm values fl ' oml , eclmiques l and 2 vary on ( lii-ti:rent ; test ; Smnl ) les , Sd will now 1) ('0 if on each tesI ; sm nl ) le , l ; echnique \]1) reduces a . value l ; lmt is the ssulle C()llS ; all I ; tHI1 Olll Ii ; lllOl'et ; hanl ; heva , \] uefl'omt , echnique 2 . 
Twool ; h(' , r tests foreomlm ring how ( ; we tech-ni ( lueS1) ert '() rm1) 3 , comtmring how well l ; heyper form on each I ; estSmnl ) learc the sign mid Wilcoxon tests ( Harnel ; t ;  ,  1!)82 , See .  15 . 5) . Unlike , t;\]lenl~tl ; ched-tmirt:t ; esI ; ~ neither of t , hesel ; wo I ; CSI ; 5 slSSlllltet ; ln~l ; I h c sum of l ; he ( litl'crences has a normal ( Gaussian )   ( list ribul ; ion . Thei ; wo tests are , so-calh ~ dnonl ) a . rmut % ri(:l ; esl ; s , which ( lo not ; make assuml ) l , ionsa . 1) out ; howl , herc sull ; saxe dis-ln'il ) ut , ed ( thrnel , l , , 1982 , Ch .  15) . 
il ' hesign ; est is I ; he simplier of lJmI ; wo . It uses a 1) inomial dist , rilm ; i onto examine them unber of l ; esl ; smni ) les where t ; e(:hlfi(lUe\]1) cr forms \]) el ; -l ; ert ; ha . nl ; e(:hnique2ve , rsusl ; hemunl ) er where 1; he Ol ) posite occurs . The null hyl ) ol ; hesis is l ; h ~ d ;  1 ; het ; wot ; eclmiques 1) ert ' or mequally well . 
Unlike the signt ; esl; , t ; he V filcoxon ; esl ; also uses in lbl ' nlal ; ion on how large a difference x is l ; s1)el ; ween t , hcl ; wol ; echniques'r ( , ,sull;s on each of l;hc l;csl ; smnpl(;s . 
3 . 2 Us ing the tes ts for matched-pa i rs All three of l  ; hcma . l , (: he (1-tmirt , sign and Wilcoxont ; csl ; scan 1) ea . pl ) liedt ; ot ; hcre , call metric , whicll is the fl'act ; ion of ; heil ; emsofinl ; crcsl ; in ~ , hel:csl ; sol;l ; lml ; a , I ; e , ehniquc recalls ( finds ) . 
Eachil ; emofinl , eresi ; in ; hel ; esl ; ( la ~; a serves as a . l ; cst sainlfl U . \ ? euset ; hesign l ; esl ; b(' , causciI ; 11 Htkcsfcwel " assumi ) i ; ions1 ; harti ; henml ; chcd-l ) air 1:I ; est and is simplier l ; hanthe Wih'ox on I ; esi ;  . 111 add it ; ion , the fro:i ; glml ; t ~ hesign l ; e , stignores l ; he size of 1 ; he result ; difl'erence one a clll ; esl ; Smnl)le(tocsllOI ; nml ; terhere . \? i I : h I ; here call met ; rio , eachs a . mpleofint ; eresl ; is either found or nol ; by a . t ; eehnique . There are no interlned bttevalues . 
While 1; he1; hreel ; esl ;s described in sccl ; ion 3 . 1 can be used on there (: ~ dlmctxic ,  1 ; hey Callll O ; bc ""'' used onell ; lint t ; hc precision or slamg hfforwardly 1 ) abmced Fscore met ; rics . This is because both precision and Fscore ~ tre more coml  ) licated non-linem'flmci ; ions of rml ( lom varial ) lcs than recall . 
Infst(:tbol ; h can bel ; hought of as non-linem " flm(:l ; ions involving recall . As described in Section 2 . 2, precision = 1 ~ . /(1~+ S ) , where I ~ isi ; henmnl ) erofiW mst ; lmt ; are of inl : eresl ; that ; are '/' c'-called by a W , chnique midSisl ; hemmfl ) er of it ; e , ms(fi ) und 1) ys ~ technique ) that ; are nol ; of interest ;  . The 1) ~ dmmed Fscore = 2ab/(a + b) , where a is recall and b is precision . 
951 3 . 3 Using randomizat ion fbr precision and F - score A class of technique that e an h and k e all l dnds of flmetions of random variables without the above problenls is the computationally-intell sive randomization tests  ( Noreen ,  1989 , Ch . 2) ( Cohen , 1995, Sec .  5 . 3) . These tests have previously used on such flmctions during the " message understanding "  ( MUC ) evaluations ( Chinchor et al . , 1993) . The randomization test we use is like a randomization version of the paired sample  ( matched-1 ) air ) ttest ( Cohen ,  1995 , Sec .  5 . 3 . 2) . 
This is a type of stratified shuffling ( Noreen ,  198!) , Sec .  2 . 7) . Wheneomt ) aring two techniques , we gather-uI ) all the responses ( whether actually of interest or not ) produced by one of the two techniques when examining the test data  , but not both techniques . Under the 111111 hyl ) othesis , the two techniques are not really different , so any resl ) onse produced by one of the teehniques e onld have just as likely come fl ' om the other  . So we shuffle these responses , reassign each response to one of the two techniques  ( equally likely to either technique ) and see how likely such as huffle1 ) roduces a difference ( new techniqueln in usold technique ) in the metric ( s ) of interest@1 our ease , precision and l?-score ) that is at least ; as large as the difference observed when using the two techniques on the test data  . 
' n responses to shuttle and assign 4 leads to 2 ~' difl ' erent w ~\ ys to shuffle and assign I ; hose responses . So when'n . is small , one can try each of the different shuttles once and produce an exact randomization  . V ~; henn gets large , the mmfl ) er of differents hutttes gets too large to be exhaustively evaluated  . ~ J?hen one performs a . u approximate randomization where each shuffle is perforn md with randoln assignments  . 
Forus , when n < 20(2'" . <_ . 1048576), we use an exact randomization . For n > 20 , we use an approximate randomization with 1048576 shufties . Because an approximate randomization uses random nmnbers  , which both lead to oc~casional unusual results and may involve using a not-so-good pseudorandom  1111111\]  ) ( ; I " genera-to l " ~ , we perf brm the following cheeks : 4Note that responses produced by both or neither techniques do not need to be shulIled and  , ~s signed . 
5One exam I ) le is the RANDU routine on the IBM 360 ( Forsyth et al . , 1977, See .  10 . 1) . 
? We run the 1048576 shuttles a see on d time and colnp are the two sets of results  . 
? We also use tile same shutttes to calculate the statistical significance for the recall metric  , and compare this significance value with the significance value found for recall analytically by the sign test  . 
An example of using randomization is to compare two different methods on finding modifier relations ill the same test set  ,  . The results on the test ; set , are :
Method ~ ~ Precision Fscore ti_l_556 t'I49 . 5% 47 . 5%
Zl : 64.1% 35.2%
Two questions being tested are whether the apparent ilnt  ) rovement in reca . ll and Fscore f!romusing method I is significant . Also being tested is whether the apparent imt ) roven mnt ; in pl'eci-sionfl'om using method Ii is significant  . 
In this example , there are 10"1 relations that should be found ( are of interest )  . Of these , 19 are recalled by both methods , 28 are recalled by method I but not ; II , and 6 are recalled by II but not I . The correlation coeificient estilnate between the methods ' recalls is  0  . 35 . In addition , 5 stm rious ( not of interest ) relations arc found by both methods , with method If inding an additional 43 Sl ) uriolls relationships ( not found by method II ) and me?hod II finding an additional 9 relationships . 
There are a total of 28+6+43+9=86 relations that are found ( whether of interest oi ' not ) by one method , but not the other . This is too many to t ) er for nlan exact randolnizg tion , so a 1048576 trial apt ) roximate randomization is perforn md . 
In 96 of these trials , method I's recall is greater than method iI's recall by at  , least (45 . 6%-24 . 3%) . Similarly , in 14794 of the trials , the Fscore difference is at least (47 . 5%-35 . 2%) . In 25770 of the trials , method II's precision is greater than method I's precision by at least  ;  (64 . 1%-49 . 5%) . N : om(Noreen , 1989, Sec . aA . a ) , the significance level ( probability under the null hypothesis ) is at most (  . , e+1)/(,~t+1), where ',, . : is the nul~lt/er of trials that meet the criterion all d  1t   , t is the number of trials . S of br recall , the significance level is at most ( 96+1 ) / ( 1048576+1 )  =0 . 00009 . 

Similarly , for Fscore , the significance level is at most 0 . ( ) 1 d : and for l ) re(:ision , the level is at lllOSt0 . 025 . A secon ( l1048576 trialt ) ro ( luce similar results , as does a sign test on recall . ' l'hus , we see that all three dit\[ere . n(:es are statistically sig-lfi Ii ca . nt . 
4 The future . : handling inter-smnple dependencies An assmnption made by all I  ; he methods mentioned in this I ) ~tl ) erist tmt then lenl bcrs of the Lest set are all independent of one a not hex  . Tlmt is , knowing how a method l ) ('rforms on one test sot sanlple should not give any information on how that method \]  ) el ' for lll sonother test set samples . This assulnl ) tJon is not always true . 
Church and Mercer ( 1993 ) give some exaln-ples of dependence bct we . entest set insl ; ancesill naturalla . llguage . Onetyt ) e of dc i ) endence is that of a lexeme's part of speech on the l  ) m'l ; s of speech of neighl ) oring lexenm , ~( th ( , ir section 2 . 1) . Sinfilar is the concept of collo-ca , t;ion , where the prolm l ) ility of a lexeme'sal>l ) earance is influenced by the . lexemes ai ) pea . rin : ~ i1 ~ nearby positions ( their section 3) . A type of ( tet ) en ( lence that is less local is that often , a . con--tent word'sal)pe . a rance in a piece of text gr ( ; atly increases the cha . n('es of th ~ t ts ; ulle wor ( 1 ~ q ) l ) ear-illg b~ter in that 1 ) iece of texl ; ( theirse(:l ; ion 2 . ;/) . 
What ~ tr (' . the effects when SOllled : t ) endency exists ? The expected ( average ) value of ' the in-stall C ( ~ results will stay the , same . However , the ( ' lmnees of getting an llllllSllal reslllt (  ; a , ltc\]la . ll~re . 
As an eXmnl)le , take five flips of a Nitcoin.
When no dependen (: ies exist 1)et ween the tlil ) s , the clmnces of the extreme result tha . t all the flit ) sl:md on : ~ particular side is faMy small ( (1/2 ) 5--i\[/32 )  . When the ttil)s are positively correlated , the sechmices increase . When the first fliplands on that side , the chances of the other four tlil ) s do ing the same are no wea . ch greater tlm n1/2 . 
Since statistical significance testing involves finding the chances of getting an mmsmd  ( skcwe ( 1 ) result under some nullhy t ) othesis , one needs to determine those del ) endencies in order to accurately determine those dmnces  , l ) eter-mining the etk's : t of these dependencies is something that is yet to l  ) ed one ,  . 
5 Conclusions
In elnpirical natural language processing , one is often CO ml ) aring differences in values of metrics like recall  , precision and balanced Fscore . 
Many of the statistics tests commonly used to make such comparisons assume the independence between the results being compared  . \? eran ~ set of m ~ tural language processing experiments and tbund that this assuml  ) tion is often violated in . ~uch awayast , ounder state the sta-l , istical significance of the difli ; rences between the results . We point out some analyt ; ica . 1 statistics tests likelnatched-l ) airt , , sign mid Wilcoxon tests , which do not midge this assmnption and show that they  (  ; tl , ll \]) ellsed Oll all letric like recall , l ? br more complicated 1nettles like precision and balanced Fscore , wcuse a compute--intensive rand on fization test , which also avoids this assumption . A next topic to address is that of possible dependencies l  ) etween test set samples . 
References
G . Box , W . Hunter , and J . Hmlter . 1978.
, gta , i is l . ics for " < rpc . ri'm . ent , er . ~ . John Wiley and

N . Chinchor , L . Hirschman , and l) . Lewis .  \] 9!)3 . 
\] ~ vahmtillg message understanding systems : an analysis of the  , third message understanding confer c . nce(muc- . 3) . Co'll ~, p'ltt(tt'io'llglLi ~,- gui,stic . s , 1!) (3) . 
K . Churchmid 171 . . Mercer .  1993 . Introduction 1 ; othesl ) ecial issue on computational linguistics using large corpora  . Cornp ' u , tational Lin-guistic . s , 1!) (1 . ):1 24 . 
P . Cohen .  1!)95 . Empirical Meth , ods for Ar'tifi-cial Intelligence . . MIT Press , MA , USA . 
G . Forsythe , M . M~dcolm , and C . Moler . 1977.
Com , put c ' r methods for ~ nath cm , atical comp'u-l . ati~m, . s . Prentice-lI~dl ~ N,J , USA . 
D . Harnett . 1982. Statistical Methods.
Addison-X Yesley Publishing Co ., 3rd edition.
R . Larsen and M . Marx .  1986 . An Introduction to Ma , th , cmatical Statistics and Its Applications . Prentice-Hall , NJ , USA , 2nd edition . 
E . Noreen .  1989 . Computer-intensive met ; hods . \[ brtesting h , y poth , cscs : an in t , r ' od't tction . Jolm
Wiley and Sons , Inc.

