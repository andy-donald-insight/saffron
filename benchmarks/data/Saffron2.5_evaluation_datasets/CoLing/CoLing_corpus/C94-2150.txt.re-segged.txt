\ ? cfldy 17 . c , trlct , c . dSt,()chasti(:(;;rammars
: Ricksop den Akker ~ nd Itugot cr Doest
University o :\[ Tw (' ntc , l ) epartment of Compu Ser Science
P . O . Box 217, 7, 500A\]i ', Ensch(;de , The Netherlands
Keywords : stochastic languages , grammars , grammar


A new type of stochatstic grammars i introduced to rinw~s-tigation : weakly restricted stochastic grammars  . In this p~L per we will concentrate oil the consist cl  , cyprol ) lcnt . ~1' of ind conditions for stoch~stl cgramma . rs to be consist m~t , the theory of multitype Galton-Watson branciting processes and gelm rating functions is of central ilnporl  , ~utce . 
The unrestricted stochastic grammar formalism generates the s~tmeclass of langu~gcs as the wc ~ klyrcstricl  . cd formal is ln , q'te inside outside algorithm is adapt . cd for use with weakly restricted granlmars . 
1 Introduction l\['W ( COilSJdcr ~ naturallttnguages as as Lr ~ , lcLtirelno ( l-elled by a formal grammar we do not consider il any more as a language thal  . is used . Formal ( contextfree ) grammars are often advocated as a model lbrl , he " linguistic competence " of an idealtal ; ural ~ m-guage user . It is also 1 , oticed that this matmmatical concept is far froln at suIicieut model for describing all aspects of the language  . What cannot be expressed by this model is tile fact thai  . some sentences or phrases are more likeley to occur than others  . '\]' his notion of occurrence refers to the use of language and thereR  ) reconsidering this kind of statistical knowledge about language has to do with the pragmatics of language laid down in a corpus of the language  . With a particular context of use in minda syntactically ambiguous sentence will often have a most  , likely meaning and hence a most likely mlalys is . Some of the shortcomings of the pure ( contextfree ) grammar model can may be be solved by stochastic gralmnars  , a model that makes it possible to incorporate certain statistical facts about the languagense into a model of the possible structures of sentences as we conceive them from a mathematical  , formal , point of view . Natural languages are now seen as stochastic ; a user of a lan-gnage as a stochastic source producing sentences  . A stochastic language over some alphabet E is simply a formal language L over  i3 together with a probability function 05 assigning to each string a : in the language a real mn-nber  05  ( a , ) in\[0 ,  1\] . Since05 ( a: ) is interpreted as the chance that tile event x , or the event that a language-source produces x , will occur , it will be clear that the sum of 4 ) ( x ) where x ranges over all possible sentences i equal to one  . Tile stochastic language is called contextfree if tile language L is contextfree  . 
The usual grammatical model for a stochastic contextfree language is a contextfree gramm ~ u ' I  , o-g c ther with a probability function f that assigns a real mnn be rin  \[0  , 1\] to each of the productions of the grammar . The Ineaning of this Nnction is the follow-in t . A step in a derivation of a sententia \] form , in which a nonterminal A is rewritten using production p has chance f  ( p ) to occur , independent of which A is rewri , ten in the sentential form and in depelident of the history of the procesl  , hat produced the sentential form . The probability of a derivation ( -tree ) is the product of the probabilities of bile derivation steps that produces the tree  . The probability of a sentence generated by t in , gramnm . r is the sum of the probabilities of all the . trees of ' a sentence . So given a stochastic grammar we can compute the probabilities of all its sentences  . The distribution language generated by a stochastic grammar G  , I ) L(G ) , is defiued as these to \[' all deriw ~ tion trees with their probabilities  . ' Chestochast , ic language generated by a stochastic grammar (7 , HL(G ) , is defined as the set of all sentences generated by the grammar with their probabilities  . 
A stochastic gr~mmlar G is an adequate model of a language L if on its basis we can correctly compute tile probabilities of the sentences in the hmgnage L  . 
Of course this assumes a statistical analysis of a language corpus  . A stochastic grammar that generates a stochastic language is called consistent  . 
Definition 1 . 1 A stochastic grammar G is called consistent if for the probability measure preduced b ? l  (  ; onto the laT~g ~ agcenerated by its underlying ram - Otherwise the grammar is called inconsistent  . 
Not all stochastic grammars generate a stochastic language  . Even proper , and reduced grammars 1 do not 1A granlmed is culled proper if for all nontcrminals A  , talcs t ln l of the probabilities assigned to the rules for A is  1  . A grammar is c~dled reduced if all nont . erminals are reachable and can produce a terminal s t  , ring . 
929 necessarily generate a stochastic language . This is illustrated in the following example . " Example 1 . 1 Consider the stochastic grammar G with nonterminal set VN =  S  , terminal set V'r = a . The productions with their probabilities are given by: 
S ~ S S
S---'a
Following the technique presented in \[2\] we find that the production generating function is given by gl  ( St ) = q s~+1-q , and that , the first moment matrix/2 is given by \[2q\] . We can conclude that the grammar is consistent if and only if q_<  1/2  . For details we refer to \[5\] . No ~ ice thai , all the differentrees of stringar ~ have Lhe same probability  . Hence , they cannot be distinguished according to their probabilities  . \[\] It has been noticed that the usual model of a stoch-astic grammar as presented above  , and which we from now on call the unrestricted stochastic gram  . mar model , has some disadvantages for modelling " real " languages  . In this paper we present a more adequate model , the weakly restricted stochastic grammar model . We give necessary and sufficient conditions to test in an efficient way whether such a grammar defines a stochastic language  . Moreover , we will show that these grammars can be transformed into an equivalent model of the usual type  . The nicething about the new model is that it models " context-dependent " probabilities of production-rules directly in terms of the grammar specification of the language and not in terms of some particular implementation of the grammar as a parser  . The latter is done by Briscoe and Carroll \[3\] by assigning probabilities to the transitions of the LR-parser constructed for the grammar  . In section 2 weakly restricted grammars are introduced , in section 3 conditions for their consistency are investigated  ; in section 4 it is proven that weakly restricted grammars and unrestricted grammars generate the same class of stochastic languages and section  5 presents the insideoutside algorithm for weakly restricted grammars  . 
2 Weakly Restricted Stochastic
Grammars ~ Ib add context sensitivity to the assignment of probabilities to the application of production rules  , we take into account ( and distinguish ) the occurrences of the nonterminals . Then , for each nonterminal occurrence distinct probabilities can be given for the production rules that can be used to rewrite the nonterminal  . 
This way of assigning probabilities to the application  2Although we found in \[7\] by Jelinek and Lafferty the ( false ) statement that a stochastic grammar is cons is tent if and only if it is proper  , given that the underly in grammar is reduced . 
The example gives a clear counterexample of their statement  . 
of production rules seems unknown in literature , although we found some other for nlalisms that we redesigned to add context sensitivity to the assignment of probabilities  . For instance , the definition of stoch-astic grammars by Saloma a in  \[8\] is somewhat different from the definition we gave in our introduction : the probability of a production to be applied is here dependent on the production that was last applied  . 
To escape the bootstrap problem ( when a derivations is started , there is no last applied production ) an initial stochastic vector is added to the grammar  . 
Weakly restricted stochastic grammars are introduced in  \[1\]  . In the following definition Ca , denotes the set of productions for Ai and l ~ ( Ai ) denotes the number of right-hand side occurrences of nonterminal A ~  . 
Definition 2 . 1 A weakly restricted stochastic gram-mat " Gzv is a pair "  ( Cc , A ) , where Cc = ( VN ,  ~ , , , P , X ) is a conlex  #freef lrammar and A is a set of functions 
A = p~lA~CVN where , if jE1 . . . t~(Ai ) and kE1 . . . \[CA , I , pi(j , k ) = Pij ~" G\[0 ,   1\] The set of productions P contains e zacily one produclion for start symbol S  . 
In words , Plj ~ stands for the probability that the kth production with left ~ hand side Ai is used for rewriting the j-th righthand side-occurrence of nonterminal Ai  . The usefullness of this context-dependency can be seen immediately fi'om the following unrestricted stochastic grammar  , which is taken ( in part ) from the example grammar in \[3\] ( p .  29):
S ~ NP VP
VP o ~ V t NP
NPo ~\[5, NP
NPQD dN
NP ~ NP PP
Unrestricted stochastic grammars cannot model context dependent use of productions  . For example , an NP is more likely to be expanded as a pronoun in subject position than elsewhere  . Exactly this dependence on where a nonterminal was introduced can be modeled by using a weakly restricted stochastic grammar  . 
Since in a weakly restricted stochastic grammar the probabilities of applying a production are dependent on the particular occurrence of a nonterminal in the right hand side of a production  , it is usefl fl to require that there is only one start production  . 
The characteristic grammar of a weakly restricted grammar is the underlying contextfree grammar  . 
The next step is to compute probabilities for strings with respect to weakly restricted stochas ~aic grammars  . For this purpose a tree is written in terms of its subtrees  ( trees with a nonterminal as root ) asq\[tiljl , f . i2j ~ , ? . . , ti , ~(q)j , ( q)\] , in which q is a production , n ( q ) is the number of nonterminals in the righthand side of q and tij denotes a  ( sub ) tree with the j-th which n ( q ) ---0 is written as \[\] . 
Definition 2 . 2 The probabilily of a derivation tree t wi~h respectoo weakly reslric lcds~ochastie grammar is deflated r'ecurs wely a  , ~( q ) m . ~, nm3,,,) rn = l where 1 < k , , ~<\ [ CA , , .  \] . 
The probability of a string is deiined as the sun : of the probabilities of all distinct derivation trees ~ hat yield this string  . 
l ) efinition 2 . 373 e probability of a string xZ ~ l
L(G ~) is defined as
The distribution langnage \] ) L ( Ow ) and stochastic language , 5'/;((\]~) of a wealdy restrieted gramuxar((;~ , A ) are detined an a \] oguous to: ; l~e distribution language and stochastic language of an nn restricted granllilar  . 
3 Consistency
In this section consistency of we~kly rest , rict , ed stoch-astic grammars will be considered . The theory of nmltiw peh ranching processes will be used to come to a similar theorem as is given in  \[2\] for unrestricted stochastic grammars . 
Definition 3 . 1 l~or the jth occurrence of ~ ontermi-hal Ai ~ VN the production generating fnnetA on \] or weakly restricted stochaslic grammars is defined as:  9i~  ( ~1 , 1 ,   . . . , sk,It(A ~)) =
ICA ilkt ~( Am ) u = lm~:\]n : . : l where r , m(k ) is 1 if nonlerminal-occurrence A . . . . appears in the righ1-hand side of the k-lh production rule wilh nonIerminal A i as left hand side and  0 otherwise . 
Note that for each right hand side nonterminal occurrence a dummy-variable is introduced : sij corresponds to the j-th occurrence of nonterminal Ai  . A special variable is Sl , :: it corresponds to the s~art symbol which is the right hand side of the start productions ~ P of the form Z~S  , The genera-~ing function for nonterminal occn rrenee  A0 entails for each production for Ai a term . If 91j has a term of the form
O~Si : Si2 .   .   . Si ,   , then we know thai it corresponds to a prodnetion for 
Ai of the fbrm
Ai-+:t : i : Ail Xi2gi ~ .   .   . zi,~Ai ,, : ci , , + ~ where the . ~: ij ~1 . 4\[ . The production has , if it is used for rewriting oceurrence Aij , probability ct of being applied . In Example 3 it will be illustrated how the terms of t ~ he genera  . tAngtim ctions correspond to the productions of the grammar  . 
Theorem 3 . 1Let Aq=~ct , " thus ~ hej thoecurre ~ . cc of nonlerminal Ai is re . written using ezactly one pro-dTt clion . 73 c probability tha ~( ~ contains lhe , . . th occurrence of nontermznal Am is given by c ~gii  ( Sl , t .   .   .   .   . s t ' , tt ( A k ) ) Proof h ~ general ~ he generating function can be written as  9 ij ( SIA ,   .   .   . , Sk , I ~( A ~)) = : 91j ( St , 1, .   .   . , Sk ~/( A ~)) - Ieli where glj(slj , , .   .   , s  ~ , Jt(a ~)) only contains terms dependent on slj ,   .   .   .   , s k , la(A ~) and where eij is a (; Oll-stant ~ term . The terms dependent on S : , l, .   .   . , S  ~ . ,R ( A k ) come from productions for A i that contain nontermi  . .
nals in their right-h~md sides and the constant ermsfl ' omproduetAons for A i that only contain terminals in their right hand sides  . When partial derivatives are taken from 9ij we can just as well consider . qlj , since the constanterm will become zero . Wck now thal . the term sing ~ j do not contain any powers higher than  1  . of the variables in it . This leads us to the insight , that taking the mn-th partial derivative of . q ij results in at most one term consisting of the form po  , f(si ,  :  .   .   .   . , s#,/~(Ak )) where f does not depend on s . . . . . and Pij , is one of the probabilities resulting from applying Pi to j and some h in  1   .   .   . \]('~ fA,\] . If we substi- . 
lutei for all remaining variables in the partial deriw ~- live we find as value for eijmn the probability that the j-th occurrence of nor  , terminal Ai is rewritten by the production that contains in its righthand side nonterminal occur rence A  , 7 m . D The first-moment matrix for weakly restricted grammars is defined just like the first-moment matrix for unrestricted grammars : Definit ion  3  . 2 The f irst-moment matrix E associated wzth the weakly restricted grammar G is /  ; =\ [ ~ u , ,-d We order the set of eigenvalues of the first -moment matrix from the largest one to the smallest  , such that
P : presenl , s the maximum.
Theorem 3 . 2 A proper weakly restric~ed grammar is consistenl if pl <  1 a ' nd is nol cons is Zcnl if pj > 1 of the related theorem in \[2\] and we will not trea . tithere ( see\[5\] for a proof ) . 
Example 3 . 1 Consider the weakly restricted stoch-astic grammar  ( G  ~ , A ) where G ~= ( VN , VT , P , Z ) = (( Z , S , ( a , P , Z ) and P ~ mdA are as follows : z-~s(p , 1-p)s-~ss(q , ~- q ) ( r , 1-r ) For a reason at . the of the example to become clear , we assume that p ? 0 . The production generating functions are given by gll  ( s11 , s t2 , sla ) = ps12 s13+1-pg12 ( S11 ~ Sl2 ~813 ) " ~ -- qs 128 13@1-qg 13 ( S11 ,  812 , 813 )  ---~  rs12813  +  1 -r The first-moment matrix E is given t ) y 0 q q 0   7' r The characteristic equation is given by ? ( x ) = x ( (x - q ) (q -- r ) -qr ) = x2 ( x - ( q+r ) ) = 0 . Thus , the eigenvalues of the matrix are 0 and x = q+r . According to Theorem 3 . 2 the grammar is consistent if q + r < 1 and inconsistent if q+r > I . If q+r = 1 the theo- . 
rein does not decidetile consistency of the grammar  . 
From the characteristic equation it follows that the value of p does not influence the consistency of the grannnar  . However , looking at the gramn rar we find that it is consistent if p =  0  , regardless of probabilities q and r . Therefore , before Theorer n3 . 2 can be used for checking the consistency of tt~e grammar  , the grammar must be stripped of productions having for each nonterminal occurrence probability zero of being applied  . \[\] Definition 3 . 3 A final class C ofnonterminal occurrences is a subset of tile set of all nontcrminal occur -fences having tile property that any occurrence in C has probability  1 of producing , when rewritten using one production rule , exactly one occurrence also in

Theorem 3 . 3 A weakly restricted s~ochastic grammar is consistent if and only if Pl <_  1 and there are no . final classes . 
For the proof of Theorem 3 . 3 we refer to \[5\] . Applying this theorem to the example learns us that if q+r =  1  , the grammar is consistent if and only if there is no final class of nonterminals  . Looking at the grammar we see that there is a final class of occurrences if q =  1 or r = 1   ( or both )  ; the final classes then are S2 , Ss and $2 ,  $3 , respectively ; if in addition p = 1 , then the final classes are S1 , S2 , S1 , $3 and $1 , $2 ,  $3 , respectively . Hence , the grammar is consistent if and only if q+r < 1 Aq ? 1 Ar?i 1  . 
Notice that if q 7~ r then all trees of a ~ have difi ~ rent probabilities  . 
4 Equivalence
In this section we will show that a weakly restricted stochastic grammar can be transformed into an equivalent unrestricted grammar  . We define two grammars G and H to be equivalent if DL  ( G ) = DL ( II )  . 
The transformation is pertbrmed as follows . With each nonterminal occurrence Aij in the right hand side of a production rule associate a new unique nonterminal Aij  ; for each new nonterminal Aij copy the set of production rules with nonterminal Ai as lefthand side  , replace the lefthand sides with Aij and replace in the right hand sides each nonterminal with its new  ( associated ) nonterminal ; assign probability Pijk to the kth production rule with lefthand side Aij  . We formalized this in the following algorithm . 
Algorithm 4.1
Associate with the jth occurrence of nonterminal Ai in the right hand sides of tile production rules a  ( new ) unique nonterminal Aij ( clearly j ~ I ,   .   .   . , R(A ~)) . \[\[' he set of nonterminals for the rewritten grammar C'is denoted by V/~ and is the set of associated nonterminals plus the start symbol S from the we~fldy restricted grammar G  . 
2 This step is given in pseudo-pascah for i := 1 to IVNI do for j := \] to t ~ ( Ai ) do\[":=_P'UCA , ( j)odod
Klwhere CA , ( j ) is the set of productions CA , with left hand sides Ai replaced by Aij and the nonterminals in tile righthand sides of the production rules replaced by their associated nonterminals  . 
The probabilities to be assigned to tim production rules in CA  , ( j ) are deduced from the Ply-~(Pijl ,  . . ", PijlCAil ): the \] cth production rule in
CA , ( j ) is assigned probability pij ; : .
Theorem 4 . 1 For every weakly restricted stoch-astic grammar there is an unrestricted stochastic grammar which  , is distributively equivalent . 
Proof We can prove the theorem by proving that the algorithm finds for every weakly restricted grammar an unrestricted grammar that is distributively equivalent  . From the algorithm it immediately follows that the languages  ( without the probabilities ) generated by the weakly restricted grammar and the unrestricted grammar generated by the algorithm are equal  . The production rules introduced by the algorithm in the unrestricted grammar cannot generate any other strings than the string generated by l\[rom tile algorithm that the unrestricted grammar associates the same probabilities with its strings as tt ~ eun restricted grammar  . II ence , the theorem holds . \[\]A corollary of this theorem is that % reach weak \] y restrict  , ed grammar there exists an unrestricted grammar that is stochaslically equivalent  . 
q ' he time-complexity of the algorithm can easily be found  . Y We obserw ~ that , if we denote the number of nonterminals in the weakly restricted grammar by k  , each step can be done in in O(k ) steps . Then the total time complexity is O(k ) . We , deiine the size of a grannriar to be tile product of the number of nonterminals and then mn be r of productions  . The size of the newly created grammar c~albe found to he polynomial in the size of tile weakly restricted graln m ~ u '  . 
5 Inference
The insideoutside algorithm is originally a reestiln a-Lion procedure for the rule probabilities of an unrestricted stochastic grammar in Chomksy Normal Form  ( CNF )  \[4\] . It , takes as input an initial m~re-stricted stochastic grammar  (  ; in CNF and a sam-piese t b7 of strings and it it cralJvely reestimates rule probabilities to ma ~ ximize the probability that the grammar would produce the samt  ) leset ,  . 
The basic idea of " the insideoutside algorithm is l  , otlse the cllrrent rl . tle probabilities to c stirnate from the sample set the expected frequencies of certain derivation steps  , and them compute new rule probability estimates as appropriate frequency rates  . Therefore , each iteration of the algorithm starts by c ~deulating the inside and outside probabilities for all strings in the sample set  . These probabilities are . ill fact . probability functions which haw ~ as arguments a string w from the sample set  , indexe ~ which inclicate what substring of w is to be considered  , and an occurrence of a nonterminal , say A . Withi ; hese arguments , the inside proi ~ a biliw now is the probability that the occurrence of A derives the substring of w  ; the oulside probability is the probability that the occurrence of nonterminal A appears in the intermediate string of some deriw ~ tion of string w  . 
In what follows , we will take I@ , V7 , as tixed n = Iv ~ l , ~- I Vrl , and as s . n , e that VN:-z--A~,,,S'=A1,A2, .   .   . , A , ~ and l/!t , = a\] .   .   .   . , ct  ~ . By definition it is required t , hat the grammar has one production for start , symbol Z:Z-+?' . Parallel to the definition of generating fnnctions for weakly restricted grammars  , we have to distinguish all nonterminal occurrences in right-hand sides of productions  ; we remind that the probahility of each production depends on the par:ticular nonterminal occurrence to be rewritten  . The inside and outside t > rohabilities now have to hespec : ilied for ea  . ch nonterminal occurrence seperately . As already stated in the introduction , the insideoutside algorithm is designed only for contextfree grammars in CNIi'  . Using this fact we can sirnplify the way nonterminal occurrences are indexed : A  , ffp . , . )( A , . ( vq . )) denotes the occurrence of . /lq(At ) ill the production Ap--*AqA, . ; for this production also the notation ( pqr ) is used and for the production Ap---~aq ( pq )  . 
Similarly the probability of occurrence Aq(p . r ) to be rewritten using rule ( q s t ) is denotes by Pq ( p . r)(q~t) . 
For the start production a special provision has to he taken : the norlterminal occurrence in its righthand side is denoted by Z~l  ( 0  .   .  )  . A stochastic grammar in CNF over these sets can then be specified by tg  ( Adllf liprobabilities . Since wc require stochastic gr~mmars to he proper  , we know the ft for p , q , r = l ,   .   .   .  , ~, ~_ . p,~(>, . )(q~ . t)t-)_~PqO, . ,)(, . ) = 1  . s , ts If we want to use the inside-outside a lgorithm for grammar inf'erence  , then the . grammar prohabilities haw ; to meet the above condition in order for titile rees-timation to make sense  . 
If string w-w\]'w2 . . . wl~d , then 1 . tvj ~0 ~ i < j < IW l denotes the substring wi + i .   .   . wj . The inside probahili , yP q , ( i , j ) estimates the likelihood
P\[C-r) .   . , that occurrence Av ( , i . , . ) derives iwj , wlnle the outside probability , O  ~ l , (q . , , ) ~ i , j ) estimates the likelihood of de--riving otl ~ iAp ( q , ,)j~t)lw I \[ roln the start symbol S . Them sideq ~ robability for st , ring w and nonterm in ~ d occur-rellceA p(q . , , ) is defined by the recurrent relation l ~, q . , . )( i -- t,i ) = p,,(q . , . )(p ~), where a , = wlw(,(~, . , . )(~, a : ) :=) w ? . to s , ti < j < k Similarly , the outside q ~ robabilities f ' or shorter spans of w can he computed from the inside probabilities and the outside probabilities for longer spans by the following recurrence : wo  , ,( , , , )(0 , I , ~1) = \] , if q--1 tuo , ,(~ , o(o , I , ~1) = 0 , otherwise o ; , % ,   , )( i , = i--1~w'w'EEOq(s . t ) ( J '~')\], . ( qp . ) (3, i ) p m a . t)(qpr ) ~ , tj = O The second equation above is somewhat simpler than the corresponding one For unrestricted stoch-astic grammars  , because the occurrence Ap(q . r ) for which the outside probability O~(q , . )(/ , k ) is computed specifics the production use ( ~ ~ rceating it and consequently the prohability for Ap  ( v ,  . ) to generate cl ' wiAp(q . , . ) j'w\[wI  is the sum of lnnchless possibilities . 
Once the inside and outside probabilities are con -tinted for each string in the sample set E  , the reestimated probability of binary rules , ~ Kf . , . )0 , ~ t ) , and tile computed using the following reestimation formulae:/Sv  ( q . , . )0, . , ,   ) = 1\[Pv ( q'" ) (w* ) I~ ( q't ) (i'J ) \] weeO<i < J < k <\[ wl--I ~ . )(J'k)O~v(q . , ")( i , k)-Zweel)v(q:,')(v ~) = pp(q . r ) ( p,QO pW(q . , , )( i --1 , i ) wEE where P ~ is the probability assigned by the current model to string w 
P ~= I 7<0) (0 , I , ol ) and P ~ is the probability assigned by the current model to the set of derivations involving some instance of Ap '~=\[~ i ' ~ i '  0_<i<j_<\[wl The denominator of the estimates /3p  ( q . , . )(p ~ , ) and p(q . r ) (ps ) estimates the probability that a derivation of a string wCE will involve at least one expansion of the nonterminal occurrence Ap  ( q . ~) . The numerator of \]) p(q . r ) (pa ~ ) estimates the probability that a derivation of a string wCE will involve rule A  , ~ ~ Aq Ar , while the numerator of 7~ p(q . , , ) ( pa ) estimates the probability that a derivation of a string w ~ E will rewrite Ap to a a  . Thus Dp(q . ,' ) (pst ) estimates the probability that a rewrite of Ap ( q . r ) in a string from E will use rule Ap--+A ~ A , , and Dp(q . ~' ) (ps ) estimates the probability that occurrence Av ( q . ~) in a string from E will be rewritten to a , . Clearly , these are the best current estimates for the binary and unary ruie probabilities  . The process is then repeated with the reestimated probabilities until the increase in the estimated probability of the sample set given the model becomes negligible  . We presented the inside , outside and ( estimated ) production probabilities only for the nonterminal occurrences of the form Ap  ( q . r ); for occurrences Ap(qr . ) these can simply be found by adapting the equations we have given for them  . 
'\] ? here estimation algorithm can be used both to refine the current estimated probabilities of a stoch-astic grammar and to infer a stochastic grammar from scratch  . The former application can be said to be incremental  . In the latter case , the initial weakly restricted grammar for the insideoutside algorithm consists of all possible CNF rules over the given sets VN of nonterminals and liT of terminals  , with suitable nonzero probabilities assigned to the nontm'minal occurrences  . 
6 Conclusions
In this paper we have investigated consistency of weakly restricted stochastic grammars and presented an adapted version of the insideoutside algorithm  . 
Other issues concerning stochastic grammars and especially weakly restricted grammars that are being investigated at the moment are stochastic grammatical inference and parsing using weakly restricted grammars  . By stochastic grammatical inference we mean grammatical inference whereby the production probabilities are computed simultaneously  . Consistency of stochastic grammars and stochastic inference will be treated in full in the master thesis of H  . W . L . terDoest , which is to appear in 1994\[5\] . 
Acknowledgement We are grateful to Jorma Tarhio , presently at the University of Berkeley , California , for stimulating discnssions . 
References\[1\]R . op den Akker . Stochastic Gram . mars : fl , eory and applications . University of Twente , Department of Computer Science , Memoranda Informatica 93-19 ,  1993 . 
\[2\]T . L . Booth , R . A . Thompson . Applying Probability Measures to Abstract Languages  . In : IEEE Transactions on Computers Voh C-22, No . 5, May 1973 . 
\[3\]T . Briscoe , J . Carroll . Generalized Probabilistic Lt ~ Parsing of Natural Language  ( Corpora ) with Unification-Based Grammars . hr : Computational
Linguistics , Vol . 19, No . 1.
\[4\]K . Lari and S . J . Young . Applications of Stoch-astic Context-free Grammars Using the Inside-Outside Algorithm  . In : Computer Speech and Lan-guage , Vol . 5, pp .  237-257,1991 . 
\[5\]II . W . L . ter Doest . Stochastic Grammars : Consistency and Inference . M . Sc . Thesis , University of Twente , Enschede , in preparation , The Netherlands . 
\[6\]T . E . Harris . Th . e Theory of Branching Processes . 
Springer-Verlag ( Berlin and New York ), 1963.
\[7\]F . aelinek , J . D . Lafferty . Computation of the Probability of Initial Substring Generation by Stochastic ContextFree Grammars  . In : Computational Linguistics , Vol . 17, No .  3 . 
\[8\]A . Salomaa . Probabilistic and Weighted Grammars . In : Information and Sciences , Vol . 15(1969), pp .  529-544 . 
C . S . We therell . Probabilistic Languages : A Review and Some Open Questions  . In : Computing Surveys , Vol . 12, No . 4, pp . 361-379, December 1980 . 

