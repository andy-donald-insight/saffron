A New Mel ; hod of ' Ngram S ( ; al : is tics for
Lmge Nulril .) er of ' n
all (\] Autorna , tJc\]: . xl ; ra , cton of Wordsml (\] Phra , ses
from Largeicxl ; Datao\[', Ja , pa , nes(;
M,~d , : otc , Nagao , Shilisul ( eMori
l )(; pa , rl ; illolit0\["l !,\] o,(' . trl(:al\]!AIII~II\]C(H'IIIg
Kyo (, o University
Abstract
In the process of establishing the it , formation theory , C . F , . Shannon prol ) ose . d the Markov I ) ro ( : ess as a good model to characterize ~ tnatural la  . nguage . 
The core or this ide . aist ; ocah : ula . tethe\['re ( lU ( ' Ii-des of strings compose ( l of ' n characters ( ' ngrams )  , but this statistical analysis of large text . ( lata a . ,id for a large n lilts l lever be ( HI carried ( ) t i t ) eca . /ise of the memory limitation of ( : omputer and the shortage of text data . Taking advantage of the recent powerful computers we developed a  . new a Jgorithm of ngrams of large text data for arbitr ~ try hu'ge'na  , nd (: alculated successl ' ully , within , ' el a , tiv ( . ly short thlle ~ ngrams of some Japa , nese text ( la , t ~ t containing between two an ( l thirty million chara , (:ters . 
From this exl ) eriment it 1) ecame (: loa , rt\]l & t the automatic extraction or detern , i , tation of words , (: om-l ) ound words and ( ; ol\]o cations i possible by mutually comparing ngram statistics for dill ' etcht values of lt  . 
category : topical pa , per ~ quantitative linguisth : s , large text corpora , text t ) rocess hlg1 Introduction Claude E . Shannone stal ) lished tile in foH . atio , theory in 19d8\[1\] . Ir is theory included the co , lcept tlutt ~ hmgnage could be a , pproximated by an nth order Markov model by ntol ) e extended t ( ~ infinity . Since his proposal there were ma . nytri-~tlstoea . h : ulate ngrams ( statistics of ' nc , ara . (: ter strings of a language ) l brabig text data , of a , la . -guage , l\[o we vercomputers u1) to tim present o . hinot ca . h : ulate them for a large n1 ) e ( : a use the cah : u-ation require ( 1 hug (  ; amount of memory space au(\[time . For example the I'r ( , quen (' yea , h;ulati(m of 10-grams of English requires a , tleast 2( ; winl0s ~ I()(~gig a word memory space . Therefore tile ca , l cuh ~ tiou was do ne at most for n :: d ,  . o5 wil ; h modest text qua , ntit ; , , We developed a new method of calcula , ting wgra . msFor large'n's . We (1o not In ' el ) are a table for a n'n . gra . m . Our methods consists of two stages . 
The first stage per h ) rmsI ; hesorting of sul ) strings of a text , aimfin ( Is outtile lenl Kth of t : he prefix parts which axe the same forth  (  ; a , dja(:entsul ) stritGsiN thest ) rtedta , ble . The second sta , ge is the (: a , lcuh~ , tion of an ' ngram when it is a al ( ed for as l ) ecific n . Only the existing ' n , chara , cter combinations require the ta , hleentrie , gt'm'( , liel ' requen (: y count , so that we . eed not r ( , serve a , bigsi ) ace for ' ngram table . The progranl we ha , vedevelol ) e(1 requires 71 bytes for anl cha , ra , cter text of two byte (: ode such as Japa , llcse and Chinese texts and 6! bytes for an l character text o1' English and other F ' , uropean la , nguages . By the present program ' . , ca , n be extended up to 255 . 
The program canl)e(:hant ~; edvery easily for la , rger ' . , if it is required . 
Wel ) erf . rme(l ' . ,- l , ; ramIrequen(:y(:a , h ' ulations for three (11fl ' (; rent text data . We were not somu (: hinterested in tile . entropy w due of a \] a . nguage \]) ut were illter('ste(I in the extra . or ion of varieti ( , sollangua ~ ei)rOl ) ( , rties , su(:\]laswor(ls , (: olnl ) oun(I words , (: o\[-\]o ca . l . ions and so on . The cah : ula . tion off re(lue lu:y of o(:(:urren(:es of clul . ra(' . t( , rsl , ringsist ) articularly ilnlmrt all t to ( leterlnilm what is ; ~ wor(lin such la , nguage . sas3al ) all e . ~ea , nd Chinese where there is nosl ) a . (: esbetween words a . nd the determinath ) n of word bound arh ~ . ~ is not so easy . In this l ) a per we will explain some of our results on 1 , hose probh ; nls . 
2 Calculation of ' ngrams for an arbit , rary large number of ' n It was w ! ry difficult to calculate ' ngrams for a large number  o1" ' n  because of the . memory limitation of a computer . For examph . ', Ja , panese langua , geha . sm~t'et halld000 di/l'ere . t characters a , nd if we want we must reserve 4000 l ? entries , which exceed 10 aa . 
Therefore only 3 or 4grams were calculated so far.
A new method we developed can calculate n_grams for an arbitrary large number of n with a reasonable memory size in a reasonable calcula  . tion time . It consists of two stages . The first stage is to get a table of alphabetically sorted substrings of a text string and to get the value of coincidence number of prefix characters of adjacently sorted strings  . 
The second stage is to calculate the fl'equency of ' ngrams for  M1 the existing ? z character strings from the sorted strings for a specific number of n  . 
2 . 1 First stage ( 1 ) When a text is given it is stored in a computer a . so nelong character string . It may include sentence boundaries , paragraph boundaries and so oil if they are regarded as components of text  , When a text is composed of I characters it occupies  2I hytemernory because a Japanese character is encoded by  16 bit code . We prepare another table of the same size ( I ) , each entry of which keeps the pointer to a substriug of the text string  . This is illustrated in l " igure 1 . 
text string (/ characters : 21 bytes)
V ''""' t"'?r ~.......\[ lli4 bytes
Figure 1: Text string and tile pointer table to substrings . 
A substring pointed by i1 is defined as compose , d of the characters fi ' om the /- t it position to the end of the text string  ( see Figure 1 )  . We call this substring a word . The first word is the text string itself , a . nd the second word is the string which starts fi ' om the second character and ends at the final ch ~ u ' a cter of the text string  . Similarly the last word is the final character of the text string  . 
As the text size is I characters al ) o interimls t have at least p bits where 27'_>_l . In our program we set p = 32 bits so that we can accept the text size tipto 2   a2 ~d giga . characters . The . pointer table represents a set of l words . 
We apply the dictionary sorting operation to this set of/words  . It is performed by utilizing the pointers in the pointer t ~ d  ) le . We used comb sort\[2\] which is an improved version of bubbles or t . The sorting thne is the order of O(llogl ) . When the sorting is completed the result is the change of pointer posi-tlons in the pointer table  . , and there , is no replacement of actual words . As we are iuterested in ngrams of ' n less than 255  , actual sorting of woMs is performed for the lertmost  255 or less cha . ra . cters of words . 
(2 ) Next we compare two adjacent words in tile l ) o intert ~ dt le , and count the length of tile prefix parts which are the s~tmeill the two words  . For example when " extension to the left side . . . " and " extension to the right side . . . " are two words placed adjacent , the nutrlber is 17 . This is stored intile t : d ) leof coincidence nuln ber of prefix characters . 
' l ? lils is shown hil , ' igure 2 . As we ; ti'e interested ill1 <' n < 255 , one byte is given to an el/tryOf this table ,  . The total lnemory space required to this first stag  (  , , operation is 214-4I-I-I = 7I bytes . For example when a text size is 10 mega Japa . nese the . ratters , 70 meyahytememory In tist be reserved . This is not difficult by the preseut-dag conipnl ; ers . 
table of coincidence
I ' ~ tlrn borecl ~ a racters pointer table 1byte   4bytes text string ( /characters : 2 lbytes ) ~ ZZ~ZE~Z22 ~ I:1\[_ Figure 2: Sorted poh/terta . ble and t ~ ble of coincidence nuii llter of cha . r:-i . cte . rs We developed two software versions , one by using main memory alone , and tile other by using a ( list " memory where the software hastile a , d ditional op-eral ; ions of discmerges or t . lilly the disc version we can ha . nd leatext of more than 100 meg ~ character Japanese text . The . software wasiml > lelnented on ~2 . 2 Second s tage Tile second stage is the calculation of n-gra  . m frequency table . This is done by using the pointer table and the table of coincidence number of prefix characters  . Let us tixn to a certMn number . We first read out the tirst n characters of the first word in the pointer table  , and see the number in the table of coincidence number of prefix char ~ tcters  . If this is equal to or larger than n it means that the second word has at least the same n prefix characters with the first word  . Then we see the next entry of the coincidence number of I  ) refix characters a , ndchecl ( whether it is equal to or larger than n or not . We continue this operation until we meet the condition that the number is smMler than n  . The number of words checked up to this is the frequency of then prefix characters of the first word  . At this stage the tirst n prefix characters of the next word is different  , and so the same operation as the th's tn characters is performed from here  , that is , to che . ck the number in tile coincidence number of prefix characters to see whether it is equal to or larger than  7z or not , and so on . In this way we get the frequency of the second n prefix characters  . Wel , e , ' form this process until the last entry of the table  . These operations give the ngram table of the glve . n text . We do not need any extra memory space in this opera-  , i on when we print out every ngram string and its fl ' equency when they  ;  , reobtained . 
We calculated ngrams for some diflhrent Japanese texts which were available in electronic form in our  1Mmratory   . These were the followings . 
1 . Encyclopedicl ) ictionary of Coml > uter Science ( a . 7M bytes ) 2 . Journ Mistic essays from Asahi Newsl ) al)er (8
Mbytes ) 3 . Miscellaneous texts avail M ) le in our laboratory ( 59M bytes ) The first two texts were not large and could \[ ) ( , managed in the main memory . ' l ' he third one was processed by using a disc memory l  ) ya . pi)lyi , lgamer gesort progn unthreethnes . ' l'hellrst two texts were processed within one . ~md two hourshya , standard SUN SPAR . C Station for the first stage mentioned above . The thh'd text required about twentyt bur hours . Calculation of ngram frequency ( the second stage ) too \] ( less than an hour including i ) rint-out . 
Extraction of useful linguistic information fl ' om ngram fl'e-quency data  3  . 1 Ent ropy Everybody is interested in the entropy w due of  ; ~ language . Shannon's theory s ~ tyst lmt the . entropy is cah : ula . tedhy the formula\[3\]
H , , (: . ) = r ( .  ,  .   ) where l ' ( w ) is the prol ) ability of occurrence , of w , and the suInma . tionist b , " a . ll the different string s ' w of ~ z characters appea  . ringhi ~ Ll ~ mguage_The entropy of a langua . ge L is : : (: , ) = , ira We cah ' ulated . II , L ( L ) for the text : smentioned in Section 2 for ~ ,  .  = 1,2,3, . . . The results is shown in Figure 3 . Unlike our hfitia . lexpecta . tion tha . t the entropy will converge to a certain constant value  be-1  . ween 0 . G and 1 . 3\vllich C . E . Shtul nones thrutte . d for English , it cotk timted to decrease to zero . We checked in detail whether our method had something wrong  , but there was nothing doubtful . Our conclusion for this strange phenomenon was that the text quantity of a few mega characters were too small to get a mean h  , g ful statistics for a . large '/ Z be . cause\v (! have 11 lo , ' ethan , 1000 different char-a . cters in the . lal ~; mese language , l , ' or English and ma . nyother l " tlrope . a , n\]allgtla . g ; es which hawe alpha : betic sets of less than fifty cha  . racte . rs the situation may he better . I lut still the text quantity of a few giga . by lesor more will be necess a . rytogel : a . n , ean-ingfulel , tropy value for ~ t = 10 or more . 
II , ~? l : ', a\[ .   . , O-~~- . -- I . . . . . . . . . . . . ~  . . . . . . . . . . . . i . . . . . . . . . . .  !  . . . . . . . . . . . . i . . . . . . . . . . . . !  . . . . . . . . . . . . . t . . . . . . . . . . . .
Ai . .  .   .   .   . ii .   .   .   .  !  .   .   .   .   .   .   .   . '2 ii .   .   .   .   .   .   .   .   .   .   .   . 
ii 117111 iii ; i ! ii 71111', b; .   .   .   .  ' "-*+"+  .   .   .   .   . ; a .   .   .   . i .   .   .   .   . 
I : i4,*~*T . . . . .   , ----*  , 0 is ~0 a ~10 a ~ ,   40 \] " igure : l : EI ~ tropy curve by ngram word l ? rom the ngram frequency table we can get many interesting information  . When we have a string w ( length n ) of high fl ' equency as shown in Figure 4 , we can try to find out the longest string w* which includes w by the following process by using the ngram frequency table  . 
2' ,~ . . . .~, W : . '-~' x , frequency .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .  _ _ : " "< . 
. . . . . . . . i .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 
I-4- . :- t -----! . . . . . . . . :  . . . . . . . . . . . . . . . . . . . . . . i " . , 2ZiK2ZZ2Z ',, iX ~ a . . . . . . . . . . . . . . . . . . . . . ii .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .  ~ .   . . . .
Figure 4: Obtaining the longest word w ~ from a high f i ' equency word fragment w  ( 1 ) extension to the left : We cutoff the last character of w and add a character a : to the left of w  . We call this a cut-and-pasted word . 
We look for the character x which will give the maximum frequency to the cut-and-pasted word  . Repeat the same operation step by step to the left and draw a frequency curve for these words  . This operation will be stopped wheu the frequency curve drops to a certain w due  . 
This process is performed by seeing the ' u-grain  #equeney table alone  . 
(2 ) extension to the right : The same ol ) eratiot ~ a . s ( 1 ) is performed by cutting the left character and adding a character to the right  . 
(3 ) extraction of high frequency part : From the fi ' equency curve as shown in Figure  , 1 we can easily extract a high fl'equency part a . s the longest string . An example is shown in Pig-ure 5 The strings extracted in this way are very often compound words of postpositions in Japanese  . 
Postposition M phrases are usually composed of one to three words  , and are used as if they are compound postpositions  . Some extracted examples are , partial strings freqtmncies ~9-5c101
O-~u~k 168 95K~h~:1310
C~\]fi-'("784
E\]fi'("+78,1:b~C + ~70770
Figure 5: Frequencies of partial strings and ing the longest word "  9   -5 C ~ ~ ) ~- e ~5" obtain - ( must do . . . ) ( it is known that . . . )( cando . . . )( can ask . .  . ) 3 . 3 Word ext rac t ion After getting high frequency chat ' ~ cter strings by the  . above method we can nmke . consultations with diction a . ries for these strings . Then we find out many strings which are not included in tim dictionaries  . 
Somea . rephrases ( colloca , tions , idiomatic expres-slons ) , some others are terminology words , and unknown ( new ) words . From the text data of Pm cyclo-pedicl ) ict kmary of Computer Science we extracted many term hm logica  . 1 words . In general the frequencies of ngrams become smaller as n becomes larger  . 
But we had sometimes relatively high frequency w due s in ngrams of large n's  . These were very oil ten terminological words or terminological phrases  . 
We extracted such terminological phrases as , ?( .   .   . ) ~ iiili-e ~ J ~/ J , ~; k 7" ,   ,   , / ~ a(p , 'or; , a , , , sw , ' iu . e , , I , y ( .   .   . ) l ' ,   ,   ,   , g ,   ,   ,   , ? e ) ( i ) roble , msolving in a , rLificial intelligence )   ( page replace merit algorithm )   ( partial correctness of programs )  3 . 4 Compound word We can get more . interest hlgln form a , tion when we compare data of different ; n's . When we have a character string ( length'n ) of high frequency , which we may be al ) le to de , fine as a word (' iv ) , we are recommended to check whether tWO substr\ ] ngs  ( W1 and w2 ) o1'tile length'ula , nd'n2('hi-I-'n2='n ) as , ,~ E ~'~;~''~''~' ,  . at ~, e, . (280) = f i ! l ~ . ~ J ! ( 166 ) = ~ II~I ~? ( \] ss ) = ' Pal ) h:1: l ) etermination of compound word proper segmentation iml  ) roper segmentation ~ iE ~'~1545"f , ~(205s ) .  ~21! (2(\] . ,)s ) fi:;~t~L_ ( tSS ) , ~t/~\]:~l\[(:(;S ) ,  @~ . , ~(\](; S ) ~ J . ~ g(((2,12) . fin(1:~50) ~ h'/l'l(\]Ss ) , ~i'/\[i , IN(lSS ) , ~ J/lf , \](188)():\[reque . ncy in li ; ncyclol ) edic Dictionary of Computer Schmce .   . I .   , Figure 6: Possible segmentation of ; ~ word into two components sltown in l , ' igure 6 h : we high frequency al ) pea . ra . nceinn . t-gramaild n ~- gram tables . If we can find out such a situation by she . rising n ~ ( and ' n ~ ) we cm~conclude that the original character string'w is a e Olnliollnd Wol'd of W  1 and 102  . ~Olll(:e X hlll ) lesill'e shown in TM ) le1 . 
3.5 Collocation
We can see whether a particular word w has strong colloeatioilal relatioils with some other words from the ngram fi'equency results  . We cnn get an ' ngram tM ) lew here n is sufficiently la . ,'ge , w is the prefix of tltese ngrams , and some words ( w ~ , ' w " ,   ,   .   . ) may appear in relatively high freqnency . This is shown in Figure 7 , We can find out easily that " tO--'t Otaild ? o--'~o Ital'et % voal locational expressions fl ' om this ligurc '  . \] for example we have \[ j ; ~ ~_1 ( effect ) and llnd out that \[ j ;  , ~9 , 1!~"~U5i ( reoceive effect ) and Fj ; ~ N~'-~j:2 . 5J ( give eflbct ) have relatively high frequencies and there are no oth Cw significant combinations in the ngramt M  ) le with rJ~gNI as the prefix , l-)kN ~'( I(ill;I . ll ( l ( ) lit Ims-pital ) b ~ weah no stall the time I-~@b ~9- . I(repeat ) as the Mlowiilg phrase , and so we will be able to judge that \[ ~ , J ~\ [?~' ~' ~ , )/) b-_1 i ~ . an idiom a . ticexpression . 
4 Conclusions
We developed a new method and software for ' . ,- gram frequency ealcula , tion for nup to 255 , and cah : ulated ngrams for some hu'ge text da , ta of Jaq ) ~ ilese , From these da . ta we could derive words , compound words and collocations automatically . 
WB''i-----+-~--+i ~-~----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ii  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . !  . . . . . . . i ~; . . . . . . . . . . . . . . ii ; ;~ . . . . . . . . . . . . . . i Figure 7: l"indilql colloca . 1 fonal word pairs " w -' w ' all ( 1' to ?-' u/tWethinl < tha , t this method is equMly use full brhm-guages lil , : o Chiu (' . sew h (, rethere is no word spaces in a . sentence , a . nd for EUrO l )  u  ~ lang na ? ; es a . swell , al \]( l ; I \] so foFsl ) eec\[lp\]lOl\]<qlles( ; q t tell (-( ~ s to ~ et ll ) ol'e de ta . iled IIMM models . 
Another possil ) ility is that when we get a large text data wil , hpart-speech tags , we can extract high frequency pa . rt-of . .sl)eeeh sequences by this n-gra , m calcula , tiou , Jverth(~pa . rt . .ol: . speech data . The sema . y be regarded as grammar rules of the primary level  , llytel ) hieing these pa , rt-of speechs e , qllellces bysis ~ gle ~ lou-terminal symbols we ca . n cah : ula . tenew'n . --grams , a . nd will be able to get hit '; her lew dgra . mm ; ~ r rules . The see ? a . mph's indical , e that lar . gete?t data , with wLriel , ies of annotations ; trevery illl-imrt aut and valual ) leh ) t " the extra . orion of littguistic in forul : Lti(mI>yc,dcula . tit~g'n-gralus for la . rgerva . lue
O\['1 l..
References\[I ) ( , ' . l '\] .   , ~ hallllOll " A mathematical theory of cornm unicatiou  , Bell System Teci ~ . #) . , Vol . 27, i ) 1) . 379-423, pp . ( ~2:b(;56, (19, 18) . 
\[2\]SI ; ephen Laeey , ll . lchard Box : Nikkei BYTE,
November , pp .: 105-312, (1991).
\[3\]N . AI ) rams on : tllfo)'m : L tion theory a . ndco(\[-iug , McGraw 11111, (1963) . 

