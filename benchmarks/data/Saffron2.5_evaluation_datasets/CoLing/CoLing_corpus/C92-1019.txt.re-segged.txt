WORDIDENTIFICATION FOR MANDARINCIl INESE SENTENCES 

Keh-Jiann Chen Shing-lluan Liu
Institute of lnfl~rmation Science
Academia Sinica
Chinese sentences are composed with string of characters without blanks to mark words  . However the basic unit for sentence parsing and understanding is word  . Therefore the first step of processing Chinese sentences i to identify the words  . The difficulties of identifying words include ( l ) the identification of complex words , such as Determinative-Measure , redupli-cations , derived words etc . , (2) the identification of proper names , (3) resolving the ambiguous segmentations . In this paper , we propose the possible solutions for the above difficulties  . We adopt a matching algorithm with 6 different heuristic rules to resolve the ambiguities and achieve an  99  . 77% of the success rate . 
The statistical data supports that the maximal matching algorithm is the most effective heuristics  . 
1. Introduction
Chinese sentences arc cx ) mposed with string of characters without blanks to mark words  . However the basic unit for sentence parsing and understanding is word  . Therefore the first step of processing Chinese sentences is to identify the words  ( i . e . segment the character strings of the sentences into word strings  )  . 
Most of the current Chinese natural language processing systems include a processor for word identification  . Also there are many word segmentation techniques been developed  . Usually they use a lexicon with a large set of entries to match input sentences  \[2  , 10 , 12 , 13 , 14 , 21\] . It is very often that there are many l ~ ) ssible different successful matchings . Therefore the major focus for word identification were on thc resolution of ambiguities  . However many other important aspects , such as what should be done , in what depth and what are considered to be the correct identifications were totally ignored  . High identification rates are claimed to be achieved  , but none of them were measured under equal bases . There is no agreement in what extend words are considered to be correctly identified  . For instance , compounds occur very often in Chinese text , but none of the existing systems except ours pay much attention to identify them  . Proper name is another type of words which cannot be listed exhaustively in the lexicon  . Therefore simple matching algorithms cannot successfully identify either compounds or proper names  . In this paper , we like to raise the ptx ~ blems and the difficulties in identifying words and suggest the possible solutions  . 
2 . Difficulties in the Identification of Words As we mentioned in the prevkms chapter  , the ba-sictechnique to identify the words is by matching algorithms  . It requires a well prepared lexicon with sufficient amount of lexical entries which covers all of the Chinese words  . I qowc versuch a large lexicon is never existing nor will be composed  , since the set of words is open ended . Not only because the new words will be generated but because there are unlimited number of compounds  . Most of the word identification systems deliberately ignore the problem of compounds and leave the problem unsolved until the stage of parsing  . 
We don't agree their viewpoints and believe that different type of Compounds should be handled by the different meth ~ Ms at different stages  . Some types of the compounds had better to be handled before parsing  , for they require different grammatical representations and id cntificalion strategies compared with the parsing of phrase structures  . On the camtrary , if the lnorphological rules \[ or compounds have the ~ me representation as the phrase structure rules  , it is better to be identified at parsing stage . We will di~uss this issue in more details in the later sections  . 
The other problem is that ambiguous segmentations frequently tv  . : curduring thc processing of word matching . It is because that very often a multisyllabic word contains mono syllabic words as its components  . 
We have to try the different strategies to re~flve such ambiguities  . 
Many problems need to be ~ lved , but first of all a lexicon shotdd be composed for the matching algorithm  . 
2.1 What are the Set of Words
According to Liang's\[14 , 15\] definition , word is a smallest , meaningful , and freely used unit . It is the basic processing unit fur Chinese natural anguage processing  . Since there is no morphological features as word segmentation marks  , we have to adopt such a wtgue definition of the Chinese word  . Liang \[151 also propose a word segmentation standard . However some of his viewpoints are debatable and self contradictory  . In fact it is almost impossible to define a standard for COr-rect identification  . Th crefl ) reinstead of proposing a AcrEs DECOLING-92 . NAm'as , 2328 Aot ) r1992101 PRoe . ov COL1NG-92, NANTES , AUG .  2328 , 1992 standard , we propose a criterion which should be followed by a good word segmentation algorithm  . It is that a good segmentation algorithm should be able to produce a result which is suitable and sufficient for the propose of later processing  , such as parsing and understanding . 
The set of words is open ended . ~lherefore the existing lexicons contain lexical entries which vary from  40 to 120 thousands . A large lexicon usually includes many compounds as well as many proper names  , for it is hard to distinguish a word and a compound  . Fbr systems with a small lexicou , they might not perform worse than systems with a large lexicon  , if they incorporate algorithms to identify compounds and proper names  . However on the other hand there is no harm to have a large lexicon  , once there is a way of handling the ambiguities , since statistically a large lexicon has the better chance to match words as well as producing ambiguou segmentations  . Therefore we have the follow principle to collect he word set for the purpose of word segmentation  . 
Principle for composing a lexicon : qqm lexicon should contamas many as possible words  . If there is a doubt whether a string of characters is a word or a compound  , you can just collect it as an entry . 
Currently we have a lexicon of around 90 thousands entries , and keep updating for new words . A lexicon with such as ~ e of course would still leave out many compounds and proper names  . We use this lexicon to match the Chinese text , the result of the algorithm is a sequence of words defined in the lexicon  . (1) is an instance of result . 
(1) a . jieshuoyuan Jau Shian-Tingyind autamen interpreter JauShian-Tinguide them'q'he interpreter Shian-Ting Jauguided them  . " b . jieshuo-yuan-Jau-Shian-Ting-yindau-ta-men However we can see that ~ me of the compounds and proper names are not identified as shown in  ( 1lb . 
They were segmented into words or characters . Therefore at later stage those pieces of segments should be regrouped into compounds and proper names  . We will discusstile issue at next two sections . 
2 . 2 Compmmds 1here are many differentype of compounds in Chinese and should be handle differently  \[3  ,  6 ,  7 ,  11 ,  17 ,  191 . 
a . determinative-measure compounds ( DM )
A determinative-measure compound is composed of one or more determinatives  , together with an optional mcasm'e . 
(2 ) jesanbent his three CL " these three " It is used to determine the reference or the quantity of the noun phrase that co-~w  . curs with it . De~ite the fact that I ~ ) th categories of determinatives and measures are closed  , the comhinations of them are not . 
However the set of DMs is a regular language which can be expressed by regular expressions and reCOg -nized by finite automata  \[19\]  . Mo\[191 also point out that the structure of I ) Ms are exocentric . They are hardly similar to other phrase structures which are endocentric and contextfree and can bcanalyzed by headdriven parsing strategies  . Therefore we suggest hattile identification of DMs should be done in parallel with the identification of common words  . There are 76 rules for DMs which covers a hnost all of the DMs \[19\]  . 
Dorword identification , those rules function as a supplement for the lexicon  , which works as ff the lexicon contains all of the DMs  . We will show the test result in the section 3 . 3 . 
b . Reduplications
In Chinese many verhs can be reduplicated to denote all additional meaning offlying the actions gently and relaxedly  ( 3 ) \[7\] . 
(3 ) tiautiau wu jump jump dance " dance a little " This kind of molphological construction will m  ) t change the argument structure of the verbs , but do change theirs 3mt actic behavior . For instance , the re-duplications cannot cooccur with the adjuncts of postverb allocation  , aspect marker , duration , and quantifier\[171 . In \[17\] , they derived 12 different reduplication rules which cover the reduplication construction of verbs  . Ill addition , there are 3 rules for the reduplication of DMs and 5 rules for A-not-A questions formation . 
The identification of the reduplication construction should be done after the words have been identified  , since it is better to secthe words and then check whether part of the words has been reduplicated  . It is a kind of context dependent process , so a separated process other than the process for DMs or Phrase structures  , should be incorporated . 
A(ZlT . SI ) ECOLING-92 , NArCrl~s , 2328 Ao(rr 1992102 PROC . OFCOLING-92, NANTES , AUG . 2328, 1992c . A-not-A construction A-not-A constructions are commonly used in Chinese to form a questiou  \[13  , 71 . As we mentioned before , for A--not ~ A construction , there are 5 different rules for reduplicating part of tile verbs and cover bs  \[17\]  . 
(4 ) fang-bu-fangshinput not stop-worry " stop worrying or not " A-n of  . A constructien is a kind of reduplication constrution  . Therefore the technique for the idemifi-cation of reduplication is aim applicable for the identification of the A-not--A construction  . 
d . Derived Words
A derived word is at compound which ( x ) mposed with a word of stem and a prefix or a suffix\[  11  \] . I ) erivative affixes are very productive in Mandarin Chinese  . 
(5 ) difang-shing place quality " locality r " Those affixes usually are bound morphemes  . In \[11\] , we collect asct of most frcqucntly occurred affixes and study their morphological behaviors  . We found that there are syntactic and semantic restrictions bct ween modifiers and heads  . Such an rorphok ) gical patterns can be reprcsented in terms of lnfonnat ion-  . 
based Case Grammar\[5\] , which is also the grammatical formalism adopted for representing Chinese phrase structures in our parsing  system\[5  , ll \] . Following is an example of representation . 
(6) shing
Semantic : meaning : equivalent of "- NESS " , "- lq~(" , for cxprcs sing abstract notation
Syntactic : a tegory:Nad feature:bound ; I+N , -- V \] constraints : MR : Vhl , V\[+transitivc\[ , N <<* Since the grammatical representation f derived words is the same as the representation f phrase structures  , wc suggest that the identification of the dcrivcd words are better to be done attile parsing stage  . Furthermore , the Ixoundaries of the derived word save syJt -tactically ambigu  ( ms . They can no the identified without checking the contextual information  . 
2.3 Proper Names
Proper names ( w . curvery frequently in all kinds of articles . ~\[ lte identification of proper n an lcs become one of the most difficult problems in Chinese natural language processing  , since we cannot exhaustly list all t , f the ptT opername in the lcxice u . Also there is no mor-phologieal nor punctuation makers to denote a proper name  . Besides that a proper name may contain a suh -string of common words  . It makes the identification of the proper names even harder  . The only clue might be usetul in identifying proper names is the occurrences nf Ix  ) and nu ) ll ) heules . Usually each Chinese character is a meaningful nit  . Some of them can I ; e used freely as a word . Some are not ; they have to be combined with other characters to form a word  . Such characters cannot be used freely as words , are named bound nmr- . 
phemes . If boundmoq flmmes occur after word matching process  , it means that there are derived words or proper names occurred iu the text and have not been identified  . The semantic lassification of morphemes can be utilized to identify the different type of proper names  . For instance , in\[11 , the set of surnames were used its a clue to identify people's names and titles  . 
There is no general solution so far to handle the different types of proper names  . The only suggestion is that mark the proper names before identification pro-cuss or treat the unknown strings as proper names  . 
2.4 Ambiguities
For Chiuese character strings , they might have many different well fl~rmed segmentations  , butust , -ally there is tndy one grammatically aud semantically sound segmemation fur~lch sentence  ( 7 )  . 
(7 ) yijing jeuglic hujiegnoaheady arrangc ~ out result ' q'he result has come out  . " yijiug-\[jengli-e hu\]-jiegnoyijing-ljeng qichu\ ]-jiegun Therefore many algorithms were proposed and heuristic or statistical pretcrence rules were adopted for remlving ambiguities  . However none of those rules has been thoruughly tested and provided their success rates  , ht the next section , we will state our algorithm as well a stile heuristic rules and also provides the experiomer it results in section  3  . 2 to show the success rate of each individual tall:  . 
3. Wnvd Identification Algnrithm
According to the discussion of the chapter 2 , the picture of the word identificatinn algoxithm should be clearly its Icit \ [ ows  . 

In fact trot all of the above processes were thoroughly studies  , but moreovless some of them were studied and have successful results  \[2  ,  8 ,  12 ,  13 ,  14 ,  16 ,  19 ,  20 , 211 . Our word identification system adoptt l , e alnlve sequence of algorithms , lint we defer the second last prlycess of finding derived words until parsing stage and tile last In ' ocess of finding proper names is tempo+Acrt ! sDE  COLING-92  , NANIES , 2328 Aot'n'1992103 PROC . OVCOLING92, NANIES , A ~, IG .  23 . -28, 1992
Words and DMs ql " ~ Lexicon and matching
I Find compounds of ~1 . redupli ~ tion 2 , A-not-Arary ignored for not having a feasible identification algorithm  . 
3 . 1 Matching algorithm and disambiguation rules The first two steps of word identification algorithm are the word matching and disambiguation  . 
These two processes were performed in parallel . Once an ambiguous match occurs , the disambiguation process is invoked immediately  . The algorithm reads the input sentences from left to right  . Then match the input character string with lexemes as well as DMs rules  , If an ambiguous segmentation do occur , then the matching algorithm looks ahead two more words  , then apply the disambiguation rules for those three word chunks  . For instance , ha (9) , the first matched word could be ' wan ' or ' wancheng '  . Then the algorithm will lookahead to take all of the possible combinations of three word chunks  , as shown in (10) , into consideration . 
(9 ) wanchengji and ing haugau complete authenticater port " complete the report about authenticating "   ( 10 ) wan ~: heng-ji and ing wancheng-ji anding-bau wancheng -jianding-baugau The disambiguation algorithm will selec the first word of the most plausible chunk as the solution  . In this case , it is the word ' wancheng' . The algorithm then proceeds to process the next word until all the input text been processed  . 
'/' he most powerful and commonly used disambiguation rule is the heuristic of maximal matching  \[12  ,  13 ,  14 ,  21\] . There are a few variations of the sense of maximal matching  , but after we have done the experiments with each of different variations  , we adopt the following maximal matching rules . 
Heuristic rule 1:
The most plausible segmentation is the three word sequence with the maximal length  . 
This heuristic rules achieves as high as 99 . 69% accuracy and 93 . 21% of the ambiguities were resolved by this rule . We will see the detail statistics in the next section  . However there are still about 6 . 7 9% of ambiguitie still cannot be re~lved by the maximal matching rule  . Therefore we adopt the next heuristic rule . 
Heuristic rule 2:
Pick the three word chunk which has the smallest standard eviation in the word length  . This is equivalent to find the chunk with the minimal value on  ( L ( W1 ) - Mean )  **2 +  ( L ( W2 ) -Mean ) **2 +  ( 14'W3 ) -Mean ) **2  , where W l , W2 , and W3 are three words in a chunk ; Mean is the average length of W l , W2 . , and W3;L(W ) denotes the length of the word W . 
Heuristic rule 2 simply says that the word length are usually evenly distributed  . For instance in (11) , the segmentation f(lla ) has the value 0 , but (1 lb ) has value 2 . Therefore according to the heuristic rule number 2  , the ( lla ) will be the selected solution and it is the correct segmentation  . 
(11 ) yianjious hengmin chiyuan research life origin " to investigate he origin of life " a  . \[ yianjiou-shengminl-chiyuanb . \[ yianjiousheng-min\]-chiyuan However it may happen that there are more than two chunks with the same length and variance  , we need a further esolution . 
Heuristic rule 3:
Pick the chunk with fewer bound morphemes.
Heuristic rule 4:
Pick the chunk with fewer characters in DMs.
That is to say the normal words get higher priority than the bound morphemes and DMs  . For instances examples (12 , 13 ) were resolved by the rule 3 and 4 respectively . (12a ) and (13a ) are right choices . 
(12 ) shietiau shangshoushiu jiauma fan negotiate up procedure more trouble some AcrEs DE  COLING-92  , NANTES . 2328 hOt'q"1992 104 PROC . OFCOLING-92, Nhr?rEs , AUG .  2328 , 1992 " Innegotiation , the process is more complicated . " a . shietiau-\[shang-shoushiu \]- jiau-mafanb . shietiau-\[shangshou-shiu\]-ii au-mafan ( 13 ) tabenrenheserf " he himself " a . ta-benren b . taben-ren The heuristic rules 2 , 3 , and 4 only resolve 1 . 7 1% of the ambiguities as shown in the table 2 of the next section . After we observe the remaining ambiguities we found that many ambiguities were occurre due to the occurrences of monosyllabic words  . For instances , the character string in ( 14 a ) can be segmented as ( 14 b ) or ( 14 c )  , but none of the above resolution rules Can resolve this case  . 
(14 ) ganran dechiuanshrrenshushieshial ai infect DE real number write down " write down the precise number of the infected " ganran-\[de-chiuanshr\] -renshu-shieshial aigan ran-\[dechiuan-shr\]-renshu -shieshial ai If we compare the correct segmentations with the incorrect segmentations  , we find out that almost all of the monosyllabic word in the correct answer are function words  , such as prepositions , con\]unctions , as well as a few high frequent adverbs . And that the monosyllabic words in the incorrect segmentations are lower frequency words  . The set of such frequently occurred monosyllabic words are shown in appendix  1  . 
We then have the following heuristic rule.
Heuristic rule 5:
Pick the chunk with the high frequently occurred monosyllabic words  . 
This rule contributes 3 . 46% of the success of the ambiguity resolution . The remaining unsolved ambiguities are about 1 . 62% of the total input words . They usually should be resolved by applying real wurld knowledge or by checking grammatical validity  . However it is almost impossible to apply real world knowledge nor to check the grammatical validity at this stage  , so applying Markovm ( v . .lel is a possible solution\[21\] . 
The other solution is much simpler , i . e . to pick the chunk with the highest accumulated frequency of  words\[221  . It requires the frequency counts for each words only instead of word bigram or trigram which required by the Markov model  . 
Heuristic rule 6:
Pick the chunk with the highest probability value.
q~e prohability value of the sequence of words ' W  1   W2 
W3' can be estimated by either a ) Markov model with the bigram approximation
P~P ( W01Wl ) * P ( WI\[W2)*P(W21W3)*
P ( W3); or b ) Word probability accumulation
P = PfWl ) + t,(w2) + P ( W3)
Heuristic rule 6 a might not be feasible , since it requires word bigram a matrix of size in the order of  10"'10  . But heuristic 6b ) might not produce a satisfactory resolution . According to our experiment the success rate for 6b   ) is less than 70% . qlaere forethe other solution is to retain the ambiguities and resolve at the parsing stage  ,  3 . 2 Experiment Results We designed a word identification system to test the matching algorithm and the abovementioned heuristic rules  . The lexicon for our system has about 90 thousands lexical entries plus mf limited amount of the DMs generated from  76 regular expressions . The 90 thousands lexemes form a word tree data structure in order to speed up the word matching  \[4  , 10\] . For the same reasons , DM rules are compiled first to produce a Chomsky Normal Form like parsing table  . The parsing table will then he interpreted during the word matching  stage\[19\]  . ' l ~ vosets of test data are randomly selected from a Chinese corpus of  20 million characters . We summarize the testing result in " lh hl e1 . qhble 2 shows the success rates and applied rates for each heuristic rule  . 
qhere call rate and recognition rate in the above table are defined as follows  . Let NI = the number of words m the input text ,   N2 = the number of words segmented by the system for the input text  . 
N3= the number of words were correctly identified . 
Then the recall ratess defined to bc N3/NI and the precision rateks N3/N2  . " lhcdclmlta ) n of the other statistical result are t ~ v lous J ) I ' olln wed the conven-tion . The above testing algorithm do not include the process of handling derived  ~4 wds . "lh crefore the above statistics do not ta ) unthemt~lakcs occurre due to the existence of derived word  , , , or proper names . 
We can sect hat the maximal matching algorithm is the most effective h cunst ~ t ~  , lbcrcare 10311 number of ambiguities out of 17404 occurrences of the seg-ACRES DECOL\]NG-92  , NANTES . 2328 AO(;r1992l05 PROC . OFCOLING-92, NANTES , Aua .  2328 . 1992 mentations . It counts 58 . 94% of the total segmentations and 93 . 2 1% of ambiguities were resolved by this heuristics . 
4. Discussions and Concluding Remarks
From the statistical result shown in table 2 , it is clear that them a . ,dmal matching algorithm is the most useful heuristic method  . Most of the mistakes caused by this heuristic are due to the occurrences of the words which are composed by two subwords  . Those words are needed to have further investigations  . If we want to further improve our system's performance  , it seems that employing lexically dependent rules is unavoidable  . 
The errors caused by the heuristic rule 2are due to the cases of a three character word followed by a monosyllabic word and which can he divided into two hisyl-labic words  , for instance (15) . 
(15 ) tzaishia sanjou at down three week " in the followlugthree week "\[ tzai-shia sanjou\]\[ tzaishia-sanjou \] Such mistakes can be avoided by giving the second bisyllabic words a lexically dependent marker which denotes that a low priority is given to this word when the heuristic rule  2 is applied . 
Table 1. Testm results
Sample 1 Sample 2 Total  #of sentences 833   1968   2801  #of characters 8455   20879   29334  #of v ~ ords 5085   12409   17494  +  ,  #of words identified 5076   12399   17475 by the system  #of correct 5O64   12370   17434 identifications . , recall rate 99 . 58% 99 . 69% 99 . 66% precision rate 99 . 76% 99 . 77% 99 . 77% guLe2 ~ e4



Table 2. The success rates of the heuristic rules
Sample 1 S + mple 2 Total  #of laent di cattom  #of error ~ succe ~ ts rate $ of mentilio aiomi of erron ~ u coms rate i rofklentificlt lor  #of errors suCCX~s rate  2875   13   99  . 58% 6938 17 99,75% 9813 30 99 . 09% 36 4 8~ . 89% 74 3 95 . 98% 110 7 93 . 64% 00 100% 50 100% 80 100% 18O 100% 321 96 . 86% 50 1 98 . 00% 104 2 98,08%  . 238 12 94 . 96% 342 14 95 90% 48 15 6878% 109 36 66,97% 157 51 67 . 52% ACr F~DECOL 1NG-92 , NANTES , 2328 nOt3T1992106 PROC . OFCOLING-92, NANTES , AUG .  2328 ,   1992 ' FILe heuristic rules #3 and #4 are the most reliable disambiguation rules . However they only contribute 0 . 53% of the disambiguation processes . 
The heuristic rule #5 is ugeful , but the priority values for each high frequent monosyllabic word has to be carefully rearranged in order to reduce possible mistakes  \[181  . 
The heuristic title #6 needs to be further studied . 
It will be much more easier to use the bigram or trigram based ongt-ammatical categories instead of the word bigram or the simple accumulation of the word frequencies  . It will be the future study . 
About the identification of the proper names , it requires a further investigation on the results of the proper names ' after segmentation algorithm is applied  . 
5 . Referenees\[1\]J . S . Chang , " A Multiple-Corpus Approach to Identication nf Chinese Surname-Names  . " Proc . 
of Natural Language Processing Pacific Rim Symposium  , Singapore ,  1991 \[2\] 3" . S . Chang , J . I . Chang and S . D . Chert , " A Meth-?yd of Constraint Satisfaction and Statistical Optimization for Chinese Word Segmentation  , " Proc . 
of the 1~1R.O . CComputational Linguistics
Conference , Taiwan , 1991\[3\]Y . R . Chad , A Grammar of Spoken Chinese , Uni~versity of California Press , California , 1968\[4\]K . J . Chen , C . J . Chert and L .  3" . Lee , " Analysis and Research in Chinese Sentences -- Segmentation and Construction  , " Technical Report , TR-864 X ) 4 , 
Nankang , Academia Sinica , 1986\[51K .  3 . Chen and C . R . Huang , "lutonnation-based Case Grammar , " COLING-90 , Vol 2 , p . 54-p . 59\[61K . J . Chen et al " Compounds and I ~ arsing in Mau ~ dar in Chinese  , " Plot . of National Computer Symposium , 1987\[71G . Y . Chen , " A-not-A Questions in Chinese , " manuscript , CKIP group , Academia Sinica , ' Pulp-el , 1991\[8\]C . K . Fan and W . H . q . t ; ai , " Automatic Word Identification in Chinese Sentences by the Relaxation "/ ~ chniqne  , " Computer Processing of Chinese and Oriental Languages  , Vol . 4, No . l , November 1988\[9\] R . Garside , G . Leech and (; . Sampson , " The Computational Analysis of English--a Corpusbased Approach  , " lamgmanGroupUKLimited , Words , " Master Thesis , National Taiwan Institute of qi : chnology , " Paipei , " lhiwan , 1983\[I1\]W . M . Hong , C . R . Huaug , T . Z . qhng and K . J . 
Chen , " The Morphological Rules of Chinese De-rivative Words  , " ribbe presented at the 1991 International Conference on " l Eaching Chinese as a Second Language  , December ,  1991 , 'Ihipei\[12\]C . Y . Jie , Y . Lin and N . Y . Liang , " On Methods of Chinese Automatic Segmentation , " Journal of Chinese hfformation Processing , VoL 3 , No . l , real Matching Automatic Chinese Word Segmentation Algorithm Using Corpus " lhgging for Ambbguity Resolution  , " Proc . of the 1991 R . O . C Computational Linguistics Conference , Taiwan , 1991\[14\]N . Y . Liang , " Automatic Chinese Text Word Segmentation System -- CI  ) WS " , Journal of Chinese Information Processing , VoL 1 , No . 2,1987\[151N . Y . Liang , " Contemporary Chinese Language Word Segmentation Standard Used for Information Processing  , " 1989 , a draft proposal \[16\]N . Y . Liang , " Fhe Knowledge of Chinese Words Segmentation , " Journal of Chinese Information
Processing , Vol . 4, No . 2 . , 1990\[17\]M . L . Lin , " The Grammaticalnd Semantic Properties of Reduplications  , " manuscript , CKIP group , Academia Sinica , 1991\[18\]I . M . Liu , C . Z . Chang and S . C . Wang , " Frequency Count of Frequently Used Chinese Words  , "' Pulp-el , qM wan , Lucky Bcmk Co . , 1975\[19\]R . EMo , Y . J . Yang , K . J . Chen and C . R . Huang , "Determinative-Measure Compounds in Manda-rin Chinese : Their Formation Rules and Parser Implementation  , " Proc . of the 1991 R . O . C Computational Linguistics Conference , qhiwan , 199 l\[20\]R . Sproat and C . Shih , " A Statistical Method for Finding Word Boundaries in Chinese ' lext  , " Computer Processing of Chinese and Oriental Languages  , Vol . 4, No . 4, March 1990\[21\]C . L . Yeh and H . J . Lee , " Rulebased Word Identification fi3r Mandarin Chinese Sentences -- A Unification Approach  , " Computer Processing of Chinese and Oriental Languages  , Vol . 5, No . 2,
March 1991
Append ~ ~ ~ ~ ~ ~ ~ T ~ b ~ ~
AcEs DECOLING-92 , NAbn'Es , 2328 Ao~rr 1992 107 PP . oc . OFCOLING-92, NAI ~ rES , Auo .  2328, 1992
