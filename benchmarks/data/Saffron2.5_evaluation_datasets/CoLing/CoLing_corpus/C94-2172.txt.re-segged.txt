DOCUMENT CLASSIFICATION BY
MACHINE : Theory and Practice
Louise Guthrie
Elbert Walker
New Mexico State University
Las Cruces , New Mexico 88001
Joe Guthrie
University of ' lb.xasatE1Paso
E1Paso,'l>xa~s 799 68

In this note , we present results concerning the theory and practice of determining for a given document which of several categories it best fits  . We describe a mathematical model of classification schemes and the one scheme which can be proved optimal among all those based on word frequencies  . Finally , we report the results of an experiment which illustrates the efficacy of this classification method  . 
TOPICAL PAPER,
Subject Area : TEXT PROCESSING 1 Introduction A problem of considerable interest in Computational Linguistics is thai  ; of classifying documents via computer processing \[ lIayes  , 1992; Lewis 1992; Walker and Amsler ,  1986\] . Siml > lyput , it is this : a document is one of several types , and a machine processing of the document is to determine of w bicll type  . 
In this note , we present results concerning the theory and practice of classification schemes t  ) ased on word frequencies . The theoretical results are about matlt . 
ematical models of classification schemes , and apply ' to any document classitication problem to tile extent that the model represents faithfully that problem  . One must cimosca model that not only provides a mathematical description of the problem atimnd  , but one in which the desired calculations can be made  . For example , in document classificatiou , it would bcnice to be able to calcn latc the probability that a document on subject i will be classified as on subject i  . Further , it would be comforting to know that there is no better scheme than the ouc being used  . Our models have these characteristics . They are siml ) lc , the calculations of probabilities of correct document classification are straightforward  , and we imve proved that there are no schemes using tiles a meinformation that have better success rates  . In an experiment the scheme was u~d to classify two types of documents  , and was found to work very well indeed . 
2 The Descr ip t ion of a Classifi-cat ion Scheme Suppose that we must classify a document into one of k types  . These types arc known . Here , k is any positive integer at least 2 , and a typical value might be anywhere from 2 to 10  . I ) enote these types T1,7~, .   .   . , 7l'k . 
The set of words in tile language is broken into m disjoint subsets  W1  , W2 ,   .   .   . , W , , . Now from a host of doc-umeuts , or a large body of literature , on subject ~/ ~ , tile frequencies Pij of words in Wi are determined  . So with subject ~ we have associated the vector of frequencies  ( pil , P i2 ,   .   . ?, Pim ), and of cour ~ pil+pi2+ .   .   . + Pim = 1 . 
Now , given a document on one of the possible k subjects  , it is classified as follows . The document has n words in it , nl words from I4/1 , n2 words from W ~ ,  .   .   . , and nm words from W in . Based on this information , a calculation is made to determine from which subject the document is most likely to have come  , and is so classified . This calculation is key : there arc many possible calculations on which a classification can be made  , but some are better t it anothers . We will prove that in this situation , there is a best one . 
We elaborate on a specific case which ~ emsto hold promise  . The idea is that the frequencies ( Pil , Pi2, ? .   .   , pi , , , ) will be ditferent enough from i to i to distinguish between types of documents  . From a document of word length n , let nj be the number of words in Wj . Titus the vector of word frequencies for that particular document is  ( hi/n , n2/n ,   .   .   . , nm/n ) . The word frequencies g from a document of type i should resemble the frequencies  ( Pil , Plm .   .   .   , Pim ) , and indeed , the classification scheme is to decl are the documeutohe of type Ti if its freq nencies " most closely resemble " the frequencies  ( Pll , P i2 ,  ? ?  . , Pi , , ) . Intuitively , if two of tile vectors are ( Pil , P i2 ,   .   .   .   , Pim ) very nearly equal , then it will be difficult to distinguish documents of those two types  . Thus the success of cla . ssification depends critically on the vectors ( pil , pi ' ~ ,   .   .   . , pim ) of frequencies . Equivalently , the sets Wj are critical , and must be chosen with great care . The particular situation we have in mind is this . Faeh of the types of document sicialized vocabulary  . The Language is broken in to k + 1 disjoint sets W1  , Wu ,   .   .   . , Wk+l of words . For i < k , the words in W/are " specific " to subject i , and Wk+l consists of tire remaining words in the language  . Now from a host of documents , or a large body of literature , on the subject T/ , we determine the frequencies pij of words in W/ . But first , the word sets W i are needed , and it , is also from such bodies of text that they will be determined  . Doing this in a manner that is optimal for our models is a difficult problem  , but doing it in such a way that our models are very effective seems quiter out in e  . 
So with subject Ti we have associated the vector of frequencies  ( Pil , P i2 ,   .   .   .   , Pim ) , the vector being of length one more than the number of types of documents  . Since the words in Wi are specific to documents of type  7\]  , these vectors of frequencies should be quite dissimilar and allow a sharp demarkation between documentypes  . This particular scheme has the added advantage that missmall  , being k + l , only one more than the number of documentypes . Further , our scheme will involve only a few hundred words , those that appear in W l , W2 ,   .   .   . , Wk , with the remainder appearing in Wk+l . This makes is possible to calculate the probabilities of correct classification of documents of each particular type  . Such calculations are intractable for large m , even on fast machines . There are classification schemes being used with m in the thousands  , making an exact mathematical calculation of probabilities of correct classification ext to impossible  . But with k and m small , say no more than 10 , such calculations are possible . 
3 The Mathematical Model
A mathematical description of the situation just described is this  . We are given k multinomial populations , with the ith having frequencies ( plr , pi  ~ ,  . .  . , Pi , ~) . The ith population may been visioned to be an in finite set consisting of m types of elements  , with the proportion of type j being Pij . We are given a random sample of size n from one of the populations  , and are asked to determine from which of the populations it came  . If the sample came from population i , then the probability that it has n j elements of type j is given by the formulan m  ( n !/ , , ,  !  , ~! . .  . n . ~!)( P ; ~' PT ~""' P, . ) ' This is an elementary probabilistic fact . If a sample to be classified has nj elements of type j  , we simply make this calculation for each i , and judge the sample to be from population i if the largest of the results was for the ith population  . Thus , the sample is judged to be from the ith population if the probability of getting the particularn /' s that were gotten is the largest for that population  . 
To determine which of ( n!/nl!n2! .   . nIX\[,~'~,," .   .   .   . p  ~ , ~ , ) ? rn "\] k Yil ~ i2 is the largest , it is only necessary to determine which of n ln ~ r '  . m the ( PlrPi2"""Pin , ) is largest , and that is an easy machine calculation . All numbers are known beforehand except then i ' s  , which are counted from the sample . 
Before illustrating success rates with some calculations  , some comments on our modeling of this docn-meat classification scheme are in order  . The ith multinomial population represents text of type  7~  . This text consists of m types of things , namely words from each of the W i . The frequencies ( pit , Pi ~, . .  .   , pin , ) give the proportion of words from the classes W1 , W ' 2 ,  .   .   . , Wm in text of type 7~ . A random sample of size n represents a document ' of word length n  . This last representation is arguable : a document of length n is not a random sample of n words from its type of text  . 
It is a structured sequence of such words ? The validity of the model proposed depends on a document reflecting the properties of a random sample in the frequencies of its words of each type  . Intuitively , long documents will do that ? Short ones may not . The success of any implementation will hinge on the frequencies  ( Pit , P i2 ,   .   .   . , Pim ) . These frequencies must differ enough from documentype to documentype so that documents  ( ' an be distinguished on the basis of them . 
4 Some Calculations
We now illustrate with some calculations for a simple case : there arc two kinds of documents  , T1 and 7~ , and three kinds of words . We have in mind here that Wj consists of words specific to documents of type Tz  , W : 2 specific to T 2 , and that W a consists of the remaining words in the language  . So we have the frequencies ( pu , pr2 , w3) and ( m ~ , m2 , m3) . Of course vi . ~ = ~- Pll-Pi 2 . Now we are given a document hat we know is either of type  711 or of type 7~  , and wenms t discern which type it is on the basis of its word frequencies  . 
Suppose it has n j words of type j , j = 1, 2, 3 . We calculate the numbers nl n2   r~3 ti = Pil Pi2   Pi3 for i = 1  ,  2 , and decl are the document to be of type 7~ if t i is the larger of the two . Now what is the probability of success ? tere is the calculation  . If a document of size n is drawn from a trinomial population with parameters  ( p11 , P12 , pla ) , the probability of getting nl words of type l , n2 words of type 2 , and n3 words of type 3 is n!~lInll ' ~! nl ~ na (   . /r .  2 .  3 . )(PllP12P13) . 
Thus to calculate the probability of classifying sue-cess fidly a document of type  7'1 ms being of that type , we must add these expressions over all those triples  ( nl , n2 , n3) for which tl is larger than t2 . This is a a host of different p's and n's . Table I contains results of some of these calculations  . 
Table i gives the probability of classifying a document of type  T1 as of type 7~  , and of classifying a document of type 7~ as of type '/ ~ . These probabilities are labeled Prob ( f ) and Prob ( 2 )  , respcctively . Of course , here we get for free the probability that a document of type  7'1 will be classified ms of type 7~  , namely 1-Prob(1) . Similarly , 1 ~- l ' rob ( 2 ) is tile probability that a document of type 7 . ) will be classified as of type 7\] . The Plj are the frequencies of words from Wj for documents of type '/ ~  , and n is the muuber of words in the document . 
Table 1
Ptj .08 .04 .88
P2j .03 .06 .91 n 50 100 200 400
Prob (1) .76 0.87 1.95 1.991
Prob(2) . 842  . 899  . 959  . 992 \[ ~,, . o ~ (2) . 10  . 03  . 87  . 02  . 05  . 93 50 100 200 400  . 894  . 963  . 995  . 999  . 920  . 975  . 997  . 999
IPlj .08 .04 .88 p ~2.0 7.0 4.89
In 50 100 200 400 Prob(1) . 575  . 553  . 595  . 638 Prob(2) . 533  . 598  . 617  . 6 58 There are several things worth noting in Table 1  . 
The frequencies used intile table were chosen to illustrate the behavior of the scheme  , att ( l not necessarily to reflect document claqsification reality  , l\[o wev cr , consider the first set of l ? equeneies ( . 08,  . ()4,  . 88) and ( . 03,  . 06,  . 91) . This represents a circnmstan ( -c where documents of type T1 have eight percent of their words specific to that subjcct  , and four percent specific to the other subject . Documents of type 7 . ) have six percent of their words specific to its subject  , and three percent specific to the other sutlject . These percentages seem to be easily attainable . Our scheme correctly classifies a document of length  200 and of type q'l 95  . 1 percent of the time , and a docmneut of length 40099 . 1 percent of the time . The last set of frequencies , ( . 08,  . 04,  . 88) and ( . 07,  . 04,  . 89) arc Mrnostalike , and as the table shows , do not serve to classify documents correctly with high probability  . In general , the probabilities of success arc remarkably high , even for relatively small n , and in the experiment reported on in Section 6 , it was easy to find words cts with satisfatory frequencies  . 
It is a fact that the probability of success can be made as close to  1 as desired by taking n large enough , assuming that ( Ptt , Pv  ~ , Pro ) is not identical to ( P'21 , P2~ , P23) . l lowever , since for reasonable frequencies , tile probabilities of success are high for n just a few hundred  , this snggests that long documents would not have to he completely tabulated in ord cr to be classified correctly with high probability  . One could just use . a random sample of appropriate size from tile document  . 
The following table give some success rates for the case where thcreare three kinds of documents and four word cla  . , ~ qes . The rates are surprisingly high . 
Table 2
PU .05 .03 .02 .90
P2j .01 .06 .01 .92
P~L ._.04 .02 .08 .86 n50 1O0 200 400
Prob(1).70 3.87 1.96 6.997
Prob(2).8 84.93 8.98 5.999
Prob (3) .826 .922 .981 .998
Plj .05 .03 .02 .90
P ' zj .01 .05 .0 l.93
P3j . 03  . 02  . 05  . 90 n 50 100 200 400 l'r ob(l) . 651  . 784  . 906  . 978 l ' rob(2) . 826  . 917  . 977  . 9981 " rob(3) . 697  . 815  . 916  . 9785 Theoretical Results In this section , we prove our optilnality result . But first we must give it a precise mathematical formulation  . ' lb say that there is no better claqsification schcme than some given one  , wcnms t know not only what " better " means , we must kuow precisely what a classitication schenu ~ is  . The setup is as in Sec--tion 3 . We have k multinomial populations with frequencies  ( Pil , Plu ,  . . . , plm ), i = 1, 2, .   .   . , k . We , are given a random sample of size n from one of the populations and are forced to assert from wM chone it cam c  . T be infi ) rmation at our disposal , besides the set of frequencies ( pit , pin ,  . . . ,pim ) , is , for each j , th c number n j of elements of type jill the sam -pie  . So the inlormation L from the sample is the tu -pie  ( hi , n = , ,  .   .   . , n , , ) . Our scheme for specifying front which population it came is to say that it came ? t\/I ' llt l : ~* t ~ m front population i if  ( n !/ nl ! n ~ ! . . n . ~ . ) tpi ~ Piu "' Pi , , , ) is maximnm over the i's . This then , determines which ( n ~, nu .   .   .   . , n . , ) re . nits in which cla . ~sification . 
( ) ur scheme partitions the sample space , that is , the set of all the tuples ( nl , n2 .   .   .   . , n . , ), into k pieces , for which ( n!/nl!nz ! . " n , , , ! ) ( p : ? l~p .   .   .   .   . P ~ r ~) is maximum . For a given sample ( or document ) size n , this leads to the definition of a scheme as any partition  A1  , A  ~ .   .   .   . , Ak of the set of tuples ( nl , n ~, .   .   . , nm ) for which ~ in i = n into k pieces . The procedure then is to classify a sample as coming from the ith population if the tuple  ( hi , n2 ,   .   .   . , am ) gotten from the sample is in Ai . It doesn't matter how this partition is arrived at  . Our method is via the probabilities rlmqi(nl , nu ,   .   .   . , nm ) = ( n!/nl!n2! . " nm !) ( P~?P ~? "" Pi , n) . 
There are many ways we could define optimality.
A definition that has particular charm is to define a scheme to be optimal if no other scheme has an higher overall probability of correct classification  . But in this setup , we have no way of knowing the overall rate of correct classification because we do not know what proportion of samples come from what populations  . So we cannot use that definition . An alternate definition that makes sense is to define a scheme to be optimal if no other scheme has  , for each population , a higher probability of correct classification of samples from that population  . But our scheme is optimal in a much stronger sense  . We define a scheme A1, A2, .   .   .   , A k to be optimal if for any other scheme B1 , B2 ,  ? ? . , B~,~_~P ( AdTi ) >_~ P ( BiIT ~) . 
Proofs of the theorems ill this note will be given elsewhere  ,   . .
Theorem 1 Let T1, T2, .   .   .   , Tk be multinomial populations with the i-th population having frequencies  ( Pil , P i2 .   .   .   .   . Pim ) . For a random sample of size n from one of these populations  , let n j be the number of elements of type j . Let tlmqi(nl , n2 .   .   .   . , nm ) = ~: nI/n . /1 . 1n ^ l "' z . nm"\]~vill~('~n~vi2"~n .   .   .   . Pim ) "
Then space by the partition of the sample !( nl , n2 ,   .   .   .   , n ~) : n j > 0 , Ejnj = n given Ai ~ -( nl , n2 .   .   .   .   . am ): qi(nl,n2 .   .   .   .   . am )) > qj(nl,n2, .   .   .   , Urn ) for i  #j  is an optimal schente for determining from which of the populations a sample of size n came  . 
An interesting feature of Table 1 is that for all frequencies Prob ( 1 ) + Prob ( 2 ) is greater for sample size 100 than for sample size 50  . This supports our intuition that larger sample sizes should yield better results  . This is indeed a fact . 
Theorem 2 The following inequality holds , with equality only in the trivial case that Pik ~ - - - P j k for all i  , j , and k , m~x ((( ,  + 1 ) ! / (  ,   , ! n ~! ? ? ? n . , !) pT:p~? . . . P , "2) -> n-t-1max((n !/( nl!n ~! .   .   .  - , ,  .  ,  . i ,   , ' is " " Pi ,   ,  1 , n where ~ n + l means to sum over those tuples ( hi , n  ~ ,  ? ?  .   , nm ) whose sum is n+1 , and ~ n means to sum over those tuples ( nl , n2 ,  ?  .   . , n  ~) whose sum is n . 
6 Practical Results
Our theoretical results assure us that documents can be classified correctly if we have appropriate sets of words  . We have algorithms which compute the probability of classifying document types correctly given the document size and the probability of some specialized sets of words appearing in the two documentypes  . 
Tables 1 and 2 show some sample outputs from that program . Intuitively , we need sets of words whicb appear much more often in one text type than tile other  , but the words do not need to appear in either text type very often  . Below we describe an experiment with two document collections that indicates that appropriate word sets can be chosen easily  . Moreover , in our sample experiment , the word sets were chosen automatically and the classification scheme worked perfectly  , as predicted by our theoretical results . 
Two appropriate collections of text were available at the Computing Research Laboratory  . The first was made up of 1000 texts on busine ~ ( joint ventures )  / , from the DAR . PATIPSTER project and the second collection consisted of  1100 texts from the Message Understanding Conference ( MUC ) \[ Sundheim ,   1991\] describing terrorist incidents in South America . The business texts were all newspaper articles , whereas the MUC texts were transmitted by teletype and came from various sources  , such as excerpts from newspaper articles , radio reports , or tape rccorded messages . 
The collections were prepared by human analysts who judged the relevance of the documents in the collections  . Each collection contained about half a million words  . 
We removed any dates , annotations , or header information from the documents which uniquely identified it as being of one text type or another  . We divided each collection of texts in half to form two training sets and two test sets of documents  , yielding four collections of about a quarter of a million words each  . We treated each of the training sets as one huge text and obtained frequency counts for each of the words in the text  . Words were not stemmed and no stoplist was used . Thc result was two lists of words with their corresponding frequencies  , one for the TIPSTER training set and one for the MUC training set  . 
Our goal at this point was to choose two sets of words  , which we call TIP-SET and MUC-SET , that could be used to distinguish the documents . We knew from the results of TABLE 1 that if we could identify one set of words ( TIP-SET ) that appeared in the TIP-documents with low probability  ( say . 03 or less ) and another set ( MUC-SET ) that appeared with probability . 1 in the MUC documents and a low probability ( say . 03 or less ) in the TIPSTER documents , that we could achieve perfect or nearly perfect classification  . 
We used a simple heuristic in our initial tests : choose the TIP-SET by choosing words which were among the  300 most frequent in the TIPSTER training set and not in the  500 most frequent in the MUC training set . 
We intended to vary the 300 and 500 to see if we could choose good sets . However , this algorithm yielded a set of words that appeared with probability  . 1 3 in the TIPSTER training set and with probability  . 01 in the MUC training set . Note that even though no stoplist was used when the frequency counts were taken  , this procedur effectively creates a stoplist automatically  . 
The same algorithm was used to create the MUC-SET : choose words from among the  300 most frequent in the MUC training set if they did not appear in tile  500 most frequent in timTIPSTER~training set . 
Our theoretical results implied that we could classify each documentype correctly  99  . 9 9% or the time if we had documents with at least 200 words . Our average document size in the two collections was  500 words . We then tested the classification scheme on the remaining half  ( those not used for training ) of each document set . 
Only one document was classitied ifferently from the truman classification  . 
When we read the text in question , it was our opinion that the original document classification by a human was incorrect  . If we change the classification of this text , then our document classitication scheme worked perfectly on  700 documents . It should be noted that the two document collections that were available to us were on very different subject matter  , so the choice of the word sets was extremely easy  . We expect that differentiating texts which are on related subject areas will be much more difficult and we are developing retinements for this task  . 
\[ Walker and Amsler , 1986\]D . Walker and R . Amsler , The Use of Machine-Readable Dictionaries in Sublanguage Analysis  , Analyzing Language in l~eslricled Domains , Grishman and Kittredge , eds . , Lawrence Erlbaum , I\[illsdale , NJ . 
7 References \[ Hayes , 1992\] Philip Hayes , Intelligent tligh-Volume qhxt Processing Using Shallow  , Domain Specific Techniques , 7 ~ xt-Based Intelligent Systems , P . Jacobs , cd . , Lawrence Erlbaum , Ilillsdale , NJ , pp .  227- 241 . 
\[ Lewis , 1992\] David Lewis , Feature Selection and Feature Extraction for Text Categorization  , Proceedings Speech and Natural Language Workshop , Morgan Kaufman , San Mateo , CA , February 1992 , pp .  212-217 . 
\[ Sundheim , 1991\] Beth Sundheim , editor . Proceedings of the Third Message Understanding Evaluation and Conference  , Morgan Kaufman , Los Altos , CA , May 1991 . 

