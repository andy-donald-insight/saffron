Experiments in Automated Lexicon Building for Text Searching 
Barry Schiffman and Kathleen R . McKeown
Department of Computer Science
Columbia University
New York , NY 10027, USA
bschiff , kathy@)cs.columbia.edu

This paper describes experiment's in the automat ' ic construction of lexicons that would be use flfl in searching large document collect ' ionstot text frag~ment stinct address a specific inibrmation eed  , such as an answer to a quest ' ion . 
1 Introduction
In develot ) ing a syst'em to find answers in text to user questions  , wem movered a major obstacle : Doe-mnent sentences t'hat contained answers dkl not of_ten use the same expressions as the question  . Whilean : ; wers in documents and questiol tsllse terms that ' are relat'e  ( l to each other , a system that sear ( : hes for answers based on the quesl : i on wording will often fail  .  3 . b address t ' h is problel n , we develol ) ed techniques to al , tomatically build a lexicon of associated terms t ' hat can be used to hell  ) lindal ) lIrol/riate bext'seglllent , s . 
The mismatch ) et'ween ( tuestion an ( l d o c t l t t l e n t w o r d i n g w a s I ) rought home to us in an analysis of a test bed of question/answer l/airs  . \ ~ Ze had a collection of newswire articles about the Clinton impeach-ment t '  ( ) use as a small-scale corl ) uS fin ' development of ; _t system . V ~ Z easked several ) eol ) let o1 ) ose questions about this wellknownt'opic , but we ( lid not make the corpus availal ) le to our cont'ril ) utors . \ ~ Ze wanted to avoid quest ' ions that tracked t ' he terminology in t'he corlms too  ( : losely to sinnllate quest'ions t'o a realworld syst'em  . The result was a set of questions that used language that ' rarely nmtchedt'hephrasing in the  . corl ) us .  \ , Ve had expected t'hat'we would be able to make most of these lexical connections with the hel l  ) of V ~ rordnet ( Miller ,  1990) . 
For example , consider a simple quest ' ional ) out test imony : " Did Secret Service agents give testimony about ' Bill Clinton ? " There is no reason t ' o expect that ' the answer would appear  1  ) aldlyst'ated as " SecretService . agents dkltesti(y . . . "What we need to know is what'testimon y is about '  , where it : occurs , who gives it . The answer would lie likely to be found in a passagement ' ioning juries  , or 1) rose eut ' or s , like the set bund in our Clinton corl ) uS : Starr immediately brought Secret Service employees beforetim grandjury for questioning  . 
Prosecutors repeat'edly asked SecretSer-vie e1 ) ersonnel to rel ) eat ' gossil ) they may have heard . 
Yet , tile V ~ ordnet synsets fbr " test in lony " offer : " evidence  , assertion , a vermentalia as severation , " not a very hell ) t iff selection here . -Wordnet hypernyms become general quickly : " declar at ' ion  , " indi-cat'ion " and " inforlnation " are only onest  , eliu 1) in t'lle hierarehy . Following these does not lead us into a courtroom . 
We asked our cont'ril ) ut ' ors for a second round of questions , but this time made the corpus available to them , exl ) laining t ' hat we wanted to be sure the answers were contained in t'he collection of articles  . 
' J'he result was a set of questions that'mueh more closely matched t'he wording in the corpus  . This was ~ int ' a et , what'the1999 DARPA question-answering ( : oml ) et'ition did in order t'o ensure that their questions couhl be answered  ( Singhal ,  1!199) . The sec-trod question-answering conference adopted a new approach to gathering questions and verifying separately that ' they a  . reans werable . 
Our intuition is t'hat if we can lind the tyl ) ical lexical neighbor hoods of concept's , we can efficiently locate a concept described in a query or a question without needing to know the precise way the answer is phrased and without relying on a cost ' ly  , handbuilt concept ' hierarchy . 
The example above illustrat'es the 1) oint . Tes-timony is given 1) 3 , wit'nesses , defendant's , eyewit-nesses . It is so licited by 1) rose cutors , counsels , lawyers . It is heard by judges , juries at trials , hearings , and recorded in depositions and transcripts . 
What'we wanted was a complete description of t ' he world of test imony-the who  , what , when and where of the word . Or , in other words , the " meta-aboutness " of terms . 
To this end , we exl ) erimented/tSitlg shallow lin-guist . k : techniquest'ogat'her and analyze word cooccurrence data in various configurat ' ions  . Unlike previous collocation research , we were in t ' erested in an expansive set ' of relationships between words we felt that the information we needed could be derived from an analysis that crossed clause and sentence boundaries  . We hyl ) othesized that news articles would be coherent so that the sequences of sentences and clauses would be linked conceptually  . 
We exanfined the nouns in a number of configurations - paragraphs  , entences , clauses and sequences of clauses - and obtained tile strongest results from configurations that count cooccurrences across the surface subjects of sequences of two to six clauses  . 
Exl ) eriments with multi-clause configurations were generally more accurate in a variety of experiments  . 
In the next section , we briefly review related research . In section 3 we describe our experiments . 
In section 4 , we discuss the problem of evaluation , and lookahead to future directions in the concluding sections  . 
2 Related Work
There has been a large body of work ill the collection of cooccurrence data from a broad spectrum of perspectives  , fi'om information retrieval to the devel-opnlent of statistical methods for investigating word similarity and classification  . Our efforts fall somewhere in tile middle . 
Compared with document retrieval tasks , we are more closely focused on the words themselves and on specific concepts than on document " aboutness  . " Jing and Croft ( 1994 ) exanfined words and phrases in paragraph units , and found that the association data improves retrieval performance  . Callan ( 1994 ) compared paragraph units and fixed windows of text in examining passage-level retrieval  . 
In the question-answering context , Morton ( 1999 ) collected document cooccurrence statistics to uncover  1  ) art-whole and synonymy relationships to use in a question-answering system  . The key difference here was that cooccurrence was considered on a whole-docmnent basis  . Harabagiu and Maiorano ( 1999 ) argued that indexing in question answering should be based on  1  ) aragraphs . 
One recental ) proach to automatic lexicon building has used seed words tolmild up larger sets of semmltically similar words in one or nlore categories  ( Riloff and Shepherd ,  1997) . In addition , Strzalkowski and Wang ( 1996 ) used a bootstrapping technique to identify types of references  , and Riloff and Jones ( 1999 ) adapted bootstrapping techniques to lexicon building targeted to information extraction  . 
In the same vein , researchers at Brown University ( Caraballo and Charniak , 1999) ~ ( Berland and Charniak ,  1999) , ( Caraballo , 1999) and ( Roark and Charniak , 1998) focused on target constructions , in particular complex nount ) hrases , and searched for information ot only on identifying classes of nouns  , lint also hypernyms , noun specificity and meronymy . 
We have a difl brent perspective than these lines of inquiry  . They were specifying various semantic relationships and seeking ways to collect similar pairs  . 
We . have a less restrictive focus and are relying on surface syntactic information about clauses  . 
For more than a decade , a variety of statistical techniques have been developed and refilled  . Tile focus of much of this work was to develop the methods themselves  . Church and Hanks ( 1989 ) explored tile use of mutual information statistics in ranking cooccurrences within five -word windows  . 
Smadja ( 1992 ) gathered cooccurrences within five-word windows to find collocations  , particularly in specific domains . Hindle ( 1990 ) classified nouns on the basis of cooccurring patterns of subjectverb and verb-object pairs  . Hatzivassiloglou and Me Keown ( 1993 ) clustered adjectives into semantic classes , and Pereira et al ( 1993 ) clustered nouns on their appearance ill verb -object pairs  . We are trying to be less restrictive in learning multiple salient relations hil  ) s between words rather than seeldng a particular elationship  . 
Illaway , our idea is the mirror image of Barzilay and Elhadad  ( 1997 )  , who used Wordnet to identify lexical chains that would coincide with cohesive text segments  . We assunmd that documents are cohesive and that cooccurrence l  ) atterns call uncover word relationships . 
3 Experiments
Tile focus of onr experiment was on units of text in which the constituents must fit together in order for the discourse to be coherent  . We made the assumption that the documents in our corpus were coherent and reasoned that if we had enough text  , covering a broad range of topics , we could pick out domain-independent associations  . For example , testimony can be about virtually anything , since anything can wind up in a court dispute . But over a large enough collection of text , the terms that directly relate totile " who , "" what " and " where " of testimony per se should appear in segments with testimony more frequently than chance  . 
These associations do not necessarily appear in a dictionary or thesaurus  . When huntans explain all unfamiliar word , they often use scenarios and analogies . 
We divided the experiments in two groups : one group that looks at cooccurrences within a single unit  , and another that looks at a sequence of units . 
In the first group of experinmnts , we considered paragraphs , sentences and clauses , each with and without prepositional phrases . 
? Single paragraphs with/without PP?Single sentences with/without PP?Single clauses with /without PP and sequences of subject  110un phrases from two to six chmses . Ill this group , we had: , , Two clauses with/without Pl ) , , A sequence of subject NPs fl'onl 2 clauses A sequence of subject NPs Dora 3 clauses , , A sequence of subject NPs from 4 clauses ? A sequence of subject NPs fi'om 5 clauses , , A sequence of subject NPs from 6 clauses The intuition for the second groul ) is that a topic flows from one granmm . tical unit to another so that the salient nouns , l ) articularly the surface subjects , in successive clauses should reveal the associations we are seeldng  . 
'\ [ loillustrate the method , consider the three-clause configuration : Say that ~ vordiapl  ) ears in claus c , ~ . 
We maintain a table of all word pairs and increment the entries for O  , ,o , '( h ,  ' , , , o , ' d ~) , where ,   , 0  , % is a subject noun in cla'usc , ~ , clauscn + ~ , or ell ' use , +2 . No effort was made to resolve pronomial references  , and these were skipped . 
We used nolln SO lfly ' because l ) reliminary tests showed that pairings between ouns seemed to stand out  . V ~ Te included tokens that were tagged as 1 ) ropernallle S when they also have have conlnlon nleanings  . 
For example , consider the Linguistic Data Consor-l ; ium at the University of Pennsylvania . Data , Consortium and University wou M be ontile list used to build the table of nmt chul  ) s with other nouns , \]) litl)eml sylvania would not . V~To also collected noun modifiers as well as head nouns as they can carry more information than the surface heads  , such as " business group " ,  '" . science class " or " crinm scene . " The corpus consisted of all tile general -interest articles from the New York Tinms newswire in  1996 in the North American News Corlms , and ( lid not include either st ) or ts or l ) usiness news . We tirst removed dul ) licate articles . The data fl'om 1996 was too slmrse for the sequence-of-subjects ontigura-lions  . '\]' ol ) alance the expcrinmnts better , we added another year's worth of newswire articles  , from 1995 , t br the sequence-of subject configurations so that we had more than one million match ups for each configuration  ( Table 1 )  . 
The I ) roeess is flflly automatic , requiring no su-1) ervision or training examples . The corpus was tagged with a decision-tree tagger  ( Schmid ,  1994 ) and parsed with a finite-state parser ( Abney ,  1996 ) using a specially written context-fi'e e-grannnar that focused on locating clause boundaries  . The grammar also identified extended noun l ) hrases in tile subject position , verb l ) hrases and other noun l ) hrases and prepositional 1 ) hrases . The nouns in the tagged , parsed corl ) uS were reduced to their syntactic roots ( removing l ) lurals from nouns ) with a lookup table created t'romWordnet ( Miller , 1990) and CELEX (1995) . We . performed this last step mainly to address the sparse data problem  . There were a substantial nunfl ) er of paMngs that occurred only once . 
We el infinated from consider at ; i on all such singletons , although it did not al ) peal to have much etfect on the overall outcome . 
Confi . qMatchups
Para+pp 6.5 million
Sent 1.7 million
Sent+pp 4 million 1Clause 1 . 1 million 1Clause + pp2 . 8 million 2Clause 1 . 9 million 2Clause+I ) P5n fillion
Subj 2Clause 1.1 million *
Subj 3Clause 1.6 million *
Subj 4Clause 2.1 million *
Subj 5Clause 2.6 m ~
Subj 6Clause 3 . 1 million*'lhble1:N mnl)er ofmatchut ) sibund ; tile "*" denotes the inclusion of 1995 data There were about 1  . 2 million paragraphs , 2 . 2 million sentences and 3 . 4 million clauses in the selected portions of the 1996 COl'pus . The total number of words was 57 million . Table 2 shows then mnl ) er of distinct nouns . 
IIAll Extracted
Nol ) ps 74,500
W/pps 91,700
Subjs 51,000
Counts > 1.

Td)le2:DistinctNouns , 1996 Data
To score then mt chups in our initial exlmriments , we used the Dice Coeliicient , which l ) roduces values i ' ronl 0 to 1 , to measure the association between pairs of words and then produced an ordered association list fl ' om the cooccurrence table  , ranked according to the scores of the entries . 
2 ? f , ' ~ q(wo , . ,h n , oo , %) score , , = frcq ( wordi ) + frcq ( word j ) One 1 ) roblem was immediately al ) parent : The quality of tile association lists wxried greatly  . Tile scoring was doing an acceptable job in ranking the words within each list  , buttile scores varied greatly from one list to another  . Our initial strategy was to choose a cutoff , which we set at 21 t breach list , and we tried several alternatives to weed out weak associations  . 

In one method , we filtered the association lists by cross -referencing  , removing from the association list for word i any word j that failed to reciprocate and to give a high rank to word i on its association list  . Another similar approach was to try to con > bine evidence fl ' om different experiments by taking the results fl ' om two configurations into consideration  . A third strategy was to calculate the mutual information between the target word and the other words on its association list  . 
scorc , ,i = p ( x  y ) * log\p ( z ) p ( y )   (   ( x  y )   ) Using the mutual information computation provided an way of using a single measure that was able to compare match ups across lists  . We set a threshold of l x l . 0-6 for all match ups . Thus these association lists vary in length , depending on the distributions for the words , allowing them to grow up to 40 , while some ended up with only one or two words . 
4 Evaluation
The evaluation of a system like ours is problematic  . 
The judgments we made to determine correctness were not only highly subjective but time -consunfing  . 
We had 12 large lexicons fl ' om the different configurations  . We had chosen a random sample of 10 percent of the 2  , 7 00 words that occurred at least 100 times in tile corpus , and manually constructed an answer key , which ended up with a hnost 30 , 000 entries . 
From the resulting 270 words , we discarded 15 of those that coincided with common names of people  , such as " carter , " which could refer to the former American president  , Chris Carter ( creator of tile television show " X-Files " )  , among others . We thought it better to delay making decisions on how to handle such cases  , especially since it would require distinguishing one Carterfl'om another  . Such words presented several difficulties . Unless the individuals involved were wellknown , it was often impossible to distinguish whether the system was making errors or whether the resulting descriptive terms were in-t brmative  . 
Tables 3 and 4 show an example from the answer keyt br the word " faculty  . " The overall results from the first stage of the process  , before the cross-referencing filter are shown in Table  5  , ranging from 73% to 80% correct . The configurations that included prepositional phrases and those that used sequences of subject noun phrases outperformed the configurations that relied on suh-jects and objects in a single grammatical unit  . These differences were statistically significant , with p < 0 . 01 in all eases . 
The overall results after cross-referencing , in Table 6 , showed improvements of 5 to 10 percentage enrollment hiring adn finistrator journal is malumnus student school union math engineering curriculum trusteegroup seminar the sistenure stair department mathematician edu cat or memberivy arts college chancellor report senate activism universit yel  , airman professor teaching law regent doctorate mt ministration academic committees emester board cam I  ) us undergraduates a lary council research president adviser mathematics course advisor sociology dean study science teacher can nonprovost vote 
Table 3: Answer Key for Faculty : OK load tratll cway unrest architecture diversity hurdles hield minority revision disburse percent woman clement Table  4: Answer Keyff ) rF a culty : Wrong points , while the effect of the number of match ups was diminished  . Here , the subject-sequence on fig-urations showed a distinct advantage  . While more noise might be expected when a large segment of text  ; is considered , these results support the notion that the nnderlying coherence of a discourse can be recovered with the prol  ) er selection of linguistic features . 
The improvements in each configuration over the corresponding configuration in the first stage were all statistically significant  , with p < 0 . 01 . Likewise , the edge the sequence-of subjects configurations had overtile other configurations  , was also statistically significant . 
The results fl ' om combining the evidence from different configurations  , in Table 7 , showed a much higher accn rae > but a sharp drop in the total n nm-ber of associated words found  . The most fl ' uitful pairs of experiments were those that combined distinct approaches  , for example , tile five-subject configuration with either fifll paragraphs or with sentences with prepositional phrases  . It will remain unclear until we conduct a task -based evaluation whether the smaller number of associations will be harnfful  . 
The final experiment , computing the mutual information statistict br the matchul  ) so fake y word with cooccurring words was perhaps the most ill-teresting because it gave us the ability to apply a 
Para+l/l ) 3832 105478, qent 3773 1270 75
Sent+Pl )   3973   1070   79 \] Clause 3652   1371   73 \] Clauses q-l ) l )   3935   1108   78 "! Clauses 3695   1328   74 "! Clauses-t-l ) l )  3983 1018 80
Subj2CI 38771 13977
Sul)j3CI 3899 111778
Subj 4 CI 3 !) (/5:108278
Sul ) j 5C 1390d 107678
Sul)j6CI 390910667!)
Table 5: Results 13 efore Cross I lofe rencing
Contig () K Wrong Pet () K
Paraq-Pl ) 365 173/183
Sent 33287 4282
Sent-bpp 3751   8:18   82   :1 Clause 3067   748   80   1 Clauses +1  ) I / 3659   826   82   2 Cbmses 3048   55d   85   2 Clauses + pp 3232   60d   8d 
Subj2CI 291045087 t-;ul~j3CI30204d()87
Subj 4CI 3050d 2888l~tll) . j5(J\];1:12 t3 d d2 88
Subj 6   C1   3237   dd9   88 ' lhble 6: lesults After Cross Referencing single threshold across different keywords  , saving the effort of performing the cross -retbrencing calm > lations and providing a deeper assorl:ment in SOllle C~lSeS  . lilt l nost of the configurations , lltlltllPlill fOr-mat . ion gave 118 lllore\Vol'ds , and greater ln ' ecision at ; the sanle time , but n lost of all , gave us a reasonable threshold to apply throughout he exlicrinlent  . 
While the accuracies in most of the configurations were close to one another  , those that used only sin-g\]e units tended to be weaker than the multi-clause units  . Note that the paragraI ) h contiguration was tested with far more data than any of the others  . 
Our systemma D~s no eth ) rt to a eCOllnt for lexical aml ) iguil ; y . The uses we intend for our lexicon should provide some insulation from the ett'ects of polysemy  , since searches will be conducted on an un>l ) er of terms , which should converge to one meaning . 
It is clear that in lists for keywords with mult i-plesenses  , the donfinant sense where there is one , al ) pears much lnore frequently , such as " faculty , " where the meaning of " teacher " is more t :' re ( tuent than the meaning of " al ) ility . " Figure \] shows the top 21 words in the sequence-otC six subjects , beibre the cross-referencing i ilterwas applied . Twenty of the 21 . entries were scored aeceptal ) le . 
After the cross-referencing is applied , doctorate , , education and revision were elinfinated . 
Contig OK\?rong Pcl ; OKl ~ ara 200318392
Sent 1962 22290
Sent-t- 2033   213   91   1 Clause 1791   218   89   1 Clause + 2004   198   91   2 Clause 2028   277   88   2 Clause + 2:129   24  , 190 Tal)le 7: Results of coml ) ining evidence ; all configu-rations were combined with the sequence of six subjects 
Conlig OK " Wrong Pet OK
Para+pp 492380786
Sent 519 3990 84
Sent+Pl ) 4876775861 Claus( ;  52! ) !/1233811 Clauses-t-l ) l )   5047   878   85   2 Clauses 5025   928   84   2 Clauses-I-Pl ) d 66 87 28 87
Subj2C15 2299 3985
Subj 3C 15 18 78 60 85
Subj ~1C 151 19808 86
Subj5C1500"76d 87
Subj 6 CI 4 !) 80 73 687
Table 8: llesults with mutual information The results from the single clause configuration  ( Figure 2 ) were almost as strong , with three erroFs , and a fair amount of overlap between the two . 
The word " admiral " was more difficult % r the ex -\]  ) erill lelltil Siligthel ) ice coefficient . The . list shows some of l . he confusion arising from our strate . ~ yOll prot ) er nouns . Admiral would be expected to occur with many properll ~ tnles  , ill cluding some that axest ) elled liD ; common 11 o 11 11 . q , bill the list h ) r the single clause qpp configur at km presented a lmzzling list  ( Figure 3 )  . 
The sparseness of the data is also al ) lmrent , but it was the dogre Dxenees that al ) peared quite strange at a ghulce : Inspection of the  . articles showed that they callle froln all a . rticle on the pets of famous people . Note that the dogs did not al ) l ) ear in top ranks of the sequence of subjects configuration in the Dice experiment  ( Figure 4 )  , nor were they in the results t'rom the experiments with cross-referencing  , combining evidence and mutual information . 
After cross-reR ; rencing , the much-shorter list for the Sub j-6 configuration had " aviator " , " breakup " , '; commander " , " decoration " , " equal-ot ) portunity " , " tleet " , " merino " , " navf ' , " pearl " , " promotion " , " rear " ~ alia " short " . 
' l'he combined-evidence list contained only eight words : " navy "  , " short " , " aviator " , " merino " , " dis-honor " , " decoration " , " sul ) " and " break-ul ) " . 
Using the mutual in t brlnation scoring , the list in the Subj-6 configuration t bradmiral had only college ( ll3 )  0 . 034; member (369) 0 . 028; profes-sor(102) 0 . 028; university (203) 0 . 027; student (206) 0 . 025; regent (19) 0 . 025; tenure (15) 0 . 025; ctmncel-lor (28) 0 . 023; administrator (34) 0 . 023; provost (12) 0 . 023; dean (27) 0 . 021; ahmmus (13) 0 . 021; math (12) 0 . 017; revision (8) 0 . 013; salary (13) 0 . 013; so-ciology (7) 0 . 013; educator ( ll ) 0 . 012; doctorate (6) 0 . 011 . ; teaching (9) 0 . 011 ; Figure 1: Tile top-ranked match ups for " faculty " from the  Subj-6-Clause configuration before cross-referencing . Then mnbers in parentheses are the number of match ups and there alumnbers following are the scores  . Errors are in bold faculty trustee (31) 0 . 033; meml)er(266) 0 . 025; ad-nfinistrator (31) 0 . 023; college (42) 0 . 012; dean (15) 0 . 012; tenure (8) 0 . 011; ivy (6) 0 . 011; staff ( a3) 0 . 01; semester (6) 0 . 01; regent (7) 0 . 01; salary (12) 0 . 01; math (7) 0 . 008; professor(a1) 0 . 008; load (6) 0 . 007; curricuh un(5) 0 . 006; revision (4) 0 . 006; minority(ll ) 0 . 006 ; Figure 2: The top-ranked matchups for " faculty " under the single clause confign ration  . Errors are in bold . 
nine words : " navy " , " general " , " commander " , " vice " , " promotion " , " officer " , " fleet " , " military " and " smith . " Finally , the even-sparser mutual information list for the paragraph configuration lists only " navy " and " suicide  . " 5 Conclusion Our results are encouraging . We were able to decipher a broad type of word association  , and showed that our method of searching sequences of subjects outperformed the snore traditional approaches in finding collocations  . We believe we can use tiffs technique to build a largescale lxicon to help in difficult information retrieval and information extraction tasks like question answering  . 
The most interesting aspect of " this work lies in the system's ability to look across several clauses and strengthentile connections between associated words  . We are able to deal with input that contains numerous errors from the tagging and shallow parsing processes  . Local context has been studied extensively in recent years with sophisticated statistical tools and the availability of enormous amounts of text in digital form  . Perhaps we can expand this perspective to look at a window of perhap several sentences by extracting the correct linguistic units in order to explore a large range of language processing problems  . 
admiral-navy ( all ) 0 . 027; ayMon(d ) 0 . 024; cheat-ing (5) 0 . 02; gall antry (3) 0 . 016; chow (4) 0 . 015; ser-vice , nan(d ) 0 . 013; short (3) 0 . 013; ward room(2) 0 . 012; american(2) 0 . 012; enos(2) 0 . 012; self-assessment (2) 0 . (/11; merino(2) 0 . 011; ocelot (2) 0 . 011; wolfhound (2) 0 . 011; igloo(2) 0 . 011; pa-prika(2) 0 . 011; spaniel(2) 0 . 01; medal(8) 0 . 01; awe(a ) 0 . 01; pedigree(2) 0 . 009; te , ' rier(2) 0 . 009 ; Figure 3: Top-ranked match ups for " adnfiral " under the clause + pp configuration  . 
admiral-navy (88) 0 . 071; short (7) 0 . 03; promo-tion(ll ) 0 . 027; hal)l ) in ess (8) 0 . 026; fleet ( ll ) 0 . 024; aviator (5) 0 . 022; mnbition(8) 0 . 019; merino (3) 0 . 019; dishonor(3) 0 . 018; rear (4) 0 . 018; deco-ration(4) 0 . 015; sub(a ) 0 . 013; airman (3) 0 . 013; graveses (2) 0 . 012; submariner(2) 0 . 012; equal-opportunity (2) 0 . 012; break-up(2) 0 . 012; comman-der (18) 0 . 012; pearl (7) 0 . 012; l ) rophccy(d ) 0 . 01 . 2; torture r(2) 0 . 012 ; Figure 4: The list for admiral fi'om the Sub j-6 contiguration . 
6 Future Work ? We will have the scoring key itself evaluated by people who are not involved in tile research  . 
? ~ reare planning to conduct ask-based evaluation in question answering  . 
? We are considering deploying a named entity module to provides onic classification of which proper nouns should be counted and which should not  . 
? We 1 ) lanto experiment with ways to incorporate using examining verbs and making use of surface objects in the configurations with sequences of clauses  , as well as strengthen the finite state grammar . 
? We will explore using tile system to extract biographic information  . 

This material is based upon work supported by tile National Science Foundation under grants Nos  . IIS-96-19124 and IRI-96-18797 , and work jointly supported by the National Science Foundation and the National Library of Medicine under grant No  . IIS-98-17434 . Any opinions , findings , and conclusions or recmmnendations expressed in this material are those of tile authors and do not necessarily reflect the views of the National Science Foundation  . 


Steven Abney .  1996 . Partial parsing via finite-state cascades . In Proceed in 9 softh , eESSLLI'95 Robust
Pars in 9Workshop.
Regina Barzilay and Michael Elhadad .  1997 . Using lexical chainst br text smm narization . In Pwcced-ings of the Ntelligent Scalable Text b ' ummariza-tion Workshop  . ACL . 
Matthew Berland and Eugene Charniak . 1999.
Finding parts in very large corpora . ' l . bchnical Report TRCS99-02, Brown University . 
James P . Callan .  1994 . Passage-level vidence in document retrieval . In Proceedin9s of the Seventeenth Annual Intcunational ACM SIGIR Conference  , Dublin , Ireland . ACM . 
Sharon Caraballo and Eugene Charniak .  1999 . Determining the speciticity of nouns from text . In P~vceedinfls of Co~@rcnce on E , mpi ~ eal Methods in Nat'u'ral Langua9e Processing . 
Sharon Caraballo .  1999 . Automatic acquisition of a hylmrnym-labeled noun hierarchy from text  . In Pwceedings of th , e 37th Annual Meeting of the Association for Comp ' utational Linguistics  , June . 
CELEX , 19!)5 . Tit (; CELEX lezical database Dutch , , English , Ge . rntan . Centr for Lexical hffor-mation , Max Planck Institute for Psycholinguis-ties , Nijmegen . 
Kenneth W . Church and Patrk : kItanks .  1989 . Word association or ms , mutual infornmtion and lexicography . In Proceedings of th . e27th . nteet in 9 of the ACL . 
S&nda ~/\[ . Ilara , bagiuantiS ; even J . Maiorano .  1999 . 
Finding answers in large collect kms of texts : Paragraph in dexiltg-t-adductive inference  . In Q ' aes-tion Answering Systema' . AAAI , November . 
Vasileios Hatziw ~ ssiloglou and Kathleen R . McKeown .  1993 . ' lbwards the automatic identification of adjectival scales : Clustering adjectives according to meaning  . In P~vceed in 9 softh , c31st Annual
Meeting of th , eACL.
Donald Hindle .  1990 . Noun classitication ti'om predicate-argument structures  . In PTvceedin9s of the 28th Annual Meeting of the ACL . 
Yufcng Jing and W . Bruce Croft .  1994 . An association thesaurus for information retrieval  , tech . rep . 
no 94-17 . 2 bchnical report , Amherst : University of Massachusetts , Center for Intelligent hfforma-tion Retrieval . 
G . Millet ' .  1990 . Wordnet : An online lexical database . International . our nal of Lezicoqraphy . 
Thomas S . Morton .  1999 . Using coreibrence tor question answering . In P~vccedings of thcWorkshop on Coreference and Its Applications  , l ) ages 85-89 , College Park , Maryland , June . Association for Computational Linguisties , Association for Computation Linguistics . 
Fernando Pereira , Naffali Tishby , and Lillian Lee . 
1993. Distributional clustering of english words.
In Pwcecdings of the 31st Annual Meeting of the

Ellen Rilotf and Pmsie Jones .  1999 . Learning die-tionarics for intbrmation extraction by multilevel bootstral  ) ping . In Proceedings of the Sixteenth Na , tional Co ~@ renccon Artificial Intelligence . AAAI . 
Ellen Iilotf and Jessica Shepherd .  1997 . A corpus-based approach for building semantic lexicons  . In Proceedings of the Second Conference on Empir ~ i-calMeth  , ods in Natural Langua 9c Processing . 
Brianlloark and Eugene Charniak .  1998 . Noun-phrasae cooccurrence statistics for semiautomatic semant k : lexicon construction  . In P~vcccdings of thc 36th Annual Meetin9 of the Association for Computational Linguistics and the  17th htternational Conference on Computation Linguistics  . 
Hehnut Schmid .  1994 . Probabilistic part of speech tagging using decision trees  . In Proceedings of the International Cor@rence on New Methods in  Lan-9ua  . qe Proecss in 9 . 
Amit Singhal .  1999 . Question and answer track home page . WWW . 
Frank Smadja .  1992 . Retrieving collocations fi'om text : Xtract . Comp'll , tational Linguistics , Special

Tomek Strzalkowski and Jin V ~ rang .  1996 . A self-learning universal concepts potter . In lhvceed in fls of th , e International 6'm@renee on Computational
Linfluisties ( Colin 9199@.

