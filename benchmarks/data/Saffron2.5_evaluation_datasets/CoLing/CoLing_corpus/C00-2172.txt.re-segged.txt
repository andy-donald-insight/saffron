Construction of a Hierarchical \]? anslat i on Memory 
S . Vogel , H . Ney
Lehrstuhlflit Intbrmatik VI , Computer Science Department
11~7~111 Aach (', nUniversity of T ( ~ chnology
1)-52056 Aachen , Gerinm ~ y
Elnaihvogel@informatik , rwth-aachen , de
Abstract
_q : anslation memories are t ) ronfising devi ( ' es for art t ; omati (- translation . Their main weakuess , however , is poor coverage , onlllSeell ; ex; . \] ill this t ) at ) er , l ; he use of a hierarchical ; ransla-tion memory , (: on sisting of a (' as (: a de of finites i ; ~d ; etransducers , is t ) rot ) os ( ; d . Ammfl ) er of tr~nsdu (: e , rs is al ) l)\]ied to ( ; on verl ; s( ; ni ; enee 1 ) airs fl : omat ) ilingual cortms into translation patterns , which are then used as a translation me , m-or y . Pr(; liminary results on the (\] erman English
VERBMOIIIL (' or l ) usa , regiven.
1 Introduction
In reeenl ; years , exa , in t)le-1) ased t ; l"ansl~l ; i <) l ~ has been 1) rol ) osed as an efli < : i ent ~ n ( ; t ; l lo < l for auto-m ~ d ; i (: translation ( Sal ; o and Nag ; to , 1990; Ki-tan () , 1993; Brown , \]99(i ) . ' lli'anslations are sl ; ()l ; edill atralls lai ; i ( ) l l l l l e . lllory tloll ( tllso , d t ; ocoi1-SI ; Yll CI ; trauslations for newsealten (: e . s . In itss in > 1) lest version , examl ) le-1 ) ased translation boils downtoll Sillga ( tat ; fl ) ase of SOllrcesell ( ; ell ( ; es with their l ; rml slations . For many translat ; ion tasks , esl ) eeially in coml ) ul ; erassisl ; c d ; ransla-tion , this at ) l ) roa (: h works with greal ; success . 
For flfllyaul ; onlat ; i("l ; ranslal ; i on the maint ) rot ) -\] emist ) oor COV el ' a ~ eoi1lleW data . To overcol Ile this weakness , it hierar ( :hi ( : altranslation llleln-ory is prot ) osed . Al ) plying a cascade of tinites ti % te ; ra , ils ( hl ( ' ers ~ a ~ S Ollrce Sell tell ce is ; ralis-laW , d into the t in : get language . 
2 The Transducers 2.1 Overview
A translat ; ionlnemory is siml ) lyae olle(: ; ion of source-l ; arge3 ; string i ) airs . As a tirst Stel ) ~ these translation examt ) les ( : all be ( : onverted in l ; o translation 1) at l ; ( . q:nst ) ylilt ; reducing category \] abels , e . g . tbrprol ) ernmnes or numbers . 
3 . ' 0 make the translation patterns even more useful , not only single words but comph ; x phrases can be replace . d by category labels . Which phrases t ; o select for categorization depends on the apl flication  , l , brexample , the corpus lls0 . d for this si ; udycoal ; alas many time and date expressions . Therefore , a specialized ; ransduce , r was constructed to recognize and translal ; esuche , xl ) ressions . 
Each transducer is ase ~ of quadrut ) les of the tbrm : label#source pal ; t ; er n  #l ; arget ; t ) at l ; ern  #score , Som'cel ) al ; terns and target patterns may contain category labels  . We call su(:hl ) atterns ~(: ompomld L ~ . l . ~: ansdue ea's working only on the word level are ( : ailed'simple ' . \] if ala:ans-dll(:e , rcoal ; alas recursive p:tl ; terns , e . g .  \] ) ATE#\] ) NPE lind\] ) ATE#I ) AS?I'~and\] ) ATI'3#-3 . 0, it ; has ; obe . a pl ) liedre . cursively t ; ot ; he input ; . 
The scores a , t ; a ( : hed to the translation t ) at-terns can be viewed ns translation scores . They are llse , d to bi~ts towards 1 ; he selection of lollg ( ; rpart ; eras and towards lliore likely translations in I ; hose cases where several targol ; patterns are associated with () lie SOllrcet ) a , l;i;ern . 
' . l ' he transducers can be applied in 1 ) oth directions , i . e . for a given language pair , each language can be viewed as source language . 
Thcrel ) y , bilingual abeling is possilf le . This can l ) e applied to convert a bilingual corlms into a selection of translation l  ) atterns which are . formulated in terms of words and ( ' ategory lal ) els . 
2 . 2 Const ruct ion o f the Transducers The transducers should t  ) eselected in such a way amto minimize l ; helle , edt br recursive ap-t ) li(:al ; ion in order l ; ol int ) rove efficiency . The re-t bre ,   ; 11('l ) at l ; erns to search tbrarel ) artitioned to for lna ( ' as ( : a deoft ; rans ducers . Sonic trans ( luc-ers analys ( , ' l ) arts of the senten ( : e and rel ) la ( : e it step by another transducer . The labeling of the days of the week or the names of the months is a prerequisite to apply more complex patterns for date expressions  . The transducers currently used are listed in Table  1  . 
' Fable 1: List of transducers.
1 . names ( persons , towns , places , events , etc ) 2 . spelling ( e . g . ' DA double L ') 3 . numbers ( ordinal , cardinal , fractions , etc ) 4 . time and date expressions 5 . parts of speech ( tbr certain word classes ) 6 . grammar ( noun phrases , verb phrases ) Some transducers are general in scope , e . g . 
the transducers for numbers , part of speech tags and grammar . Others are costumized towards the domain tbr which the translation system is developed  . Intile VERB MOBIL corpus , which is used for the experiments , time and date expressions are very prominent . To recognize these expressions , a small grammar has been developed and coded as finite state transducer  . Actually , two transducers are used . On the first level , words are replaced by labels , like DAY-OFWEEK = Montag , Dienstag ,   . . . . On the second level , these labels are used to t'orm complex time and date expressions  . This second transducer works recursively , as simpler expressions are used to build more complex expressions  . 
Finally , a small grammar based on POS ( part of speech ) tags has been crafted mamlally . The purpose of this grammar is to recognize simple noun phrases  . Extensions to handle the different word ordering in the verb phrases arc under development  . 
2.3 Scoring
The scores attached to the translation patterns can be viewed as a kind of translation scores  . 
In the current implementation a rather crude heuristic together with some manual tuning in the grammar transducer is applied  . The idea is to give preference to longer translation patterns as they take more context into account and encode word reordering in an explicit manner  . Thus , fbr simple and compound translation patterns the score is exponential to the length of the source pattern  . Tile scores are negative by convention : not translating a word gives zero cost  , translating it gives a benefit , i . e . negative costs . In future , scoring will be refined by using corpus statistics to assign probabilities to the translation patterns  . 
2.4 Bilingual Labeling
The sentence pairs ill the bilingual training corpus can be segmented into shorter segments with the help of an alignment progrmn  ( Och et al . , 1999) . This collection of segments could be used directly as a translation memory  . However , to improve the coverage on unseen data , these segnmnts are labeled . Applying the transducers as given in Table 1 transfbrms these segments into compound t ) hrases . 
The procedure is as follows : 1 . For each transducer taken from the complete cascade-as given in Table  1 ap-lilY the transducer to both , the source and tlm target sentences of the bilingual training cortms  . 
2 . Find those sentence pairs which contain equal number and types of category labels t br both sentences  . 
3 . For sentence pairs which do not match in mmflmr and type of the category labels keep the original sentence pair  . 
Table 2 shows examples of some translation patterns which resulted flom bilingual a beling  . 
3 Applying the Transducers
The working of the transducers ibest described as tile construction of a translation graph  . That is to say , the sentence to be translated is viewed as a graph which is traversed fi'om left to right  . 
For each matching source pattern , as encoded in the transducers , a new edge is added to the graph . The edge is labeled with the category label of the translation pattern  . The translation and the translation score are attached to the edge  . In this way a translation graph is constructed . In those cases , where a source pattern has several translations , one edge t breach translation is added to the graph  . 
Timleft right search on the graph is organized in such a way that all paths are traversed 
CTP ~ DATE_DAY gingees wied cr
CTP ~ SURNAME amA1)i ) ~ rat
CTP ~ NP dauert DATE
CTP@nehmenPPERNPDATE@DATE_DAY it is possible again : ~  -4  . 6 ~/: this is SURNAMEst ) caking@-3 . 3
NP takes DATE : ~-3 . 3 let PPER take NPDATE@-4 . 6 in parallel and tile patterns l ; ored in the transducer are matched synchronously . For each ~ lo(ten and each edge e leading to n , all patterns in tile transducer starting with the label of earc attached to n  . This give sammlber of hypotheses describing partially matching patterns  . Already started hypotheses are expanded with tile lal  ) el of the edge running ti ' om the l ) revious node to the current node . This procedure is shown in l ~' i gul'e1 . For a selection of t ; rmmlation patterns from the siml ) le , word-1 ) ased translation memory the hyt ) othesest br1 ) artially matching patterns generate during the left--right traversal are shown as well as the resulting new edges  . 
The result of applying all transducers is a graph where each path is a  ( partial ) translation of the source sentence . The 1 ) at h with the best overall score is used to construct the final translation  . For good result ; s , not ; only the scores from t ; he transducer shoul ( l1 ) e used in selecting the best t ) ath , but a language model of the target language should l  ) einchlde ( l . 
1 llIIl#al , on , at the 9 Montag  #Monday 17 waere essomoeglich  #would that be poss il flc 18 wicist cs bell hncn  #how about you 19 wiewaerces  #howal ) out 20 wiew a crccs denn  #how about 21 wiewaerees dennam Montag  #how about Monday 22 wiewacrce sam Montag#Imwabout Monday Figure 1: Ext ) ansion of Pattern Hypotheses 3 . 1 Error Tolerant Match To improve tile coverage on unseen test data  , it may be a vantageous to allow tbrap proxima-tivc matching  . The idea is , to apply longer segment st br syntactically better translations without loosing to much as far as tile content of the sentences i concerned  . 
We us (; weighted edit distance , i . e . each error ( insertion , deletion , substitution ) is assici-ated with an individual score . Thereby , the deletion or insertion of typical filler words can be allowed  , whereas the deletion or insertion of content words is avoided  . 
3.2 Translation on Word Lattices
The approach described so far can be used for a tight integration of speech recognition and translation  . Speech recognition systems typically 1 ) ro ( luce wor ( l lattices which encode the most likely word sequences in an e  . flicient lllall-net . A direct translation on the lattice has , compared to transforming the lattice , into an nbest list ; , translating each word sequence , mM selecting the overall best translation , an uln ber of advantages : ? all the paths can be covered  , whereas in an nbest approach typically only a small fraction of tile paths is considered  ; ? partial translation hypotheses are reused ; ? acoustic scores can be taken into account when calculating an overall score for each translation hypothesis  . 
4 Experiments and Results
In this section , we will report on first expert-ments and results obtained with the cascaded transducer approach  . Experiments were per-t brmed on the VERB MOBIL corpus  . This corpus consists of spontaneously spoken dialogs in the appointment scheduling domain  ( Wahlster ,  1993) . The vocabulary comprises 7335 German of 147 sentences with a total of 1   968 words was used to test the coverage of tile transducers and to run preliminary translation experiments  . 
In Table 3 the sizes of the transducers are given.
Table 3: Number of translation t ) atterns of tile transducers . 
Transducer Patterns




POS Tags ~ raln nlar 671.4
In a first series of experiments , the coverage of the cascaded transducers was tested  . TILe sentences pairs Dora the training corpus were segmented into shorter segments  . This resulted in 43609 bilingual phrases running from 1 word up to 82 words in length . The longest phrases were discarded as it is very unlikely that they will match other sentences  . Thus , for the experiments only 40000 sentence pairs were used , the longest sentences containing sixteen source words  . 
Starting fi'om those simple phrases , successively more transducers were applied 1 lt ) to the flll cascade . In Table 4 the coverage for each level is shown . As expected , the coverage increases and nearly flfll coverage on the test sentences is reached  . In tile final step , the POS transducer and the grammer transducer are both applied  . 
The first cohnnn shows which transducers have been applied  . In each step , one additional transducer is applied tbr bilingual labeling and tbr translation  . Bilingual labeling reduces the number of distinct patterns in the translation memory  , whereas the immber of compound patterns increases  . The last column shows the number of words in the test sentences not covered by the patterns ill tile translation mmory  . 
As can be seen , the coverage increases which each step . The large improvement in the final Table 4: Efl'ect of selected transducers oi1 coverage on test corpus . 
% ' ansdncers Patterns Coln-not pound covered

Name+Spell+Number+Date+Gramnlar1 . 2 59 step results froln applying tile POS tag transducer whidl coveres a large part of the vocabulary  . 
4.2 Translation
First experiments have been performed to test tile approach tbr translation  . So far , no language model t br the target language is applied to score the different ranslations  . 
For the sentence ' Samstag und Februars in dgut , a berder siebze hntew are besser ' the best t ) at h through the resnlting translation graph gives a structure as shown in Figure  2  . IlL Table 5 , some translation examples for test sentences not seen ill the training corpus arc given  . 
Table 5: Three translations generated t'rom the hierarchical translation memory  . 
Ich wer delnit dem Fhlgzeugkolnnmn.
I will come with the plane.
Ja , wunderbar . Machenwird as so , und d an nt refl bnwi runs daimill Hamburg . 
Vielen Dankundauf Wiederh Sren.
Well , excellent . Shall we fix this , and then we will meet then in Hanfl ) urg . 
Thank you very much good by e.
Daskannichnicht einrichten . Ich habeeine Chanceab dreimld z wanzigsten
Oktober . Istesdabei Ihnenm 6glich ?
It can I not arrange . I have a chance froln twenty-third of
October . Is it as for you possible ? the fourth would be better  -7  . 4 ~ DATE\]
Saturday and February-4, 2
DATEDATE
Saturday February -0.6 -0.6

Saturday -0.5
Samstag I

February -0.5
Februar 4
IDATE the fourth-4 . 1 ~ DATEDAY the fourth -4 . 0 areood but-21-01 ~ a_ere ~ A
Figure 2: \[\[~' m~slation example 5 Sun'nnary and conclusions In this t ) npcra translation at ) pronch 1 ) as exlon cascaded t in restate l ; ra , nsducers has l ) een pre-se next . Amm ~ l\]mm~l)er of simplel ; rmlsdut'-( ; rs is handcrafted and then used to convert ; n bilingual cortms in ; o a translation memory consisting of som : (: cl ) al ; tcrn target ; i ) a , l ; l ; (; rnp ~ tirs , which in chuh ; category lnlmls . Trmlslni ; ion is the nlmr form cd by applying l ; he comtflel ; ecas-ca(leofl ; rans(luce . rs . 
First (; xl ) e . rim(mtsha , v ( ; shownl ; lm \]) ot , cnl ; i ; J of this ai ) l ) ro ~ u : h form ~ t chine l ; rans la , tion . Good coverag ( ~ on mlse , (mtest data (' ould 1)eol ) l;aine(l . 
The . main ditficulty in this nt ) l ) roach is to ( te-lhea ( : on sistenl ; scorings (' heme thr the ( litt'e , r-ent transdu (:(' rs . Especially , ~ goodl ) M~m (' et ) (;- tween the grammm : and th (' , word-t ) as (; d , ransb > lionm (' mory is n (' , c(;ssary . ' Phis will t)eth('main focus for futur(' , work . 
As Mrea(ty mentioned ,   ; ~l ~ tngmtge modal forth ( ; tnrgetl ~ mguag ( ; has to bc integrated into t ; h ( , scoring of the translation hyl ) othes ( , s . Fi-mflly , the l , rmm du ( ' erbasedal ) t ) roadl to translation will 1 ) etested on word lattice . sasi ) rodu(:ed by spee , (: h recognition systeans . 
Acknowledgement . This work was partly
SUl ) t ) orted l ) y the German Fede . ral Ministry of E(tuc ~ ttion , S(:ie . n(:e , ll . es(;m:chmM3b . (: hnoh)gy under the . Contract Nulnl ) er01IV 701T d(vlmvonu,) . 

R .  1) . Brown . i\[996 . Exmut ) lc-1) as e , dmachine translation in the pan gloss system , l " rocc , cd-ings of the 16th , international Co ~@ rencc , on Computational Linguistics ,  169-174 , Copcn-tm , ge , n , l)emnark , August . 
It . l(itm Jo .  1993 . ACOml ) rehensivem Mprn ( > ti ( -M model of memory-ha , seal machine trmls-la . tion,l~mcccdi,ng . ~ of the 13th , hzl , c'r , natio'nal Joint Co'nfere , nce , o ' nA rt/icialbl , t cl lig c , ' n , ce , vohm m2 .  1276 1282 . Morgmt Ka . ufmmm . 
F . .\] . Och , C . Tillmmm , mMH . Ney .  1999 . lm-prove , d aligmnent models for statistical ma-chilw , I ; ranslation . Procceding , s of the Joint SIGDAT Co ~@ rcnccon Empirical Meth , ods in Na , t , wral Language PTwccs . s in 9 and Very Large , Corpora ,  20 28 , University of Mm:y~land , College Park , MD , USA , June . 
S . Sato and M . Nagao .  1990 . To wmd memory-based tnmslation . P ' rocc , edings of the 13th International Cm@rcnce on Computational Lingui , ~ tics , vol . 3, 24:7~252, Hcl sinki , Finland . 
W . Wahlster .  1993 . Vert ) mobil : %' anslation of t'a(:c-to-fac ( ; dialogs . Proceedings of th , eMT
Summit IV , 1.27135, Kol)e , Jal)mL
