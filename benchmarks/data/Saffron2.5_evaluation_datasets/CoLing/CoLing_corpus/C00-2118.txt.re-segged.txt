Automatic Lexical Acquisition Based on Statistical Distributions * 
Suzanne Stevenson
Deparl ; ment of Coml ) uter Science
Uldv crsity of Toronto
6 King's Collegel ( oa , d
'.l.'or onto,ON Ca,lla,d~t M5S 311 115
suzanne ~ cs , toronto , edu

Wea , ui ; omatically cla , ssi(yverbs into lexica , 1 semantic classes , 1) ased on distributions of indices-tots of verb a . lterna . tions , extra . cCed froln a very la . rgea . nnota . ted corpus . We~ddressa . prol ) lem which is pa . rti cuhtrly difficult 1) eca . use the verl ) classes , a . l though sema . ntica . lly different , shows im-ila . r surface syntactic 1) e h a . vior , Fivegra . m,na . tica . 1 fea . l ; tlres~u:esu\[\[icient to reduce errori:ate by more tha  . n 50% over cha . nc ( , : we a . chieve almost 70% a . ceura . cy in a 1; ask whose baseline perl'ornmn(:e is 34% , and whose exl ) ert-I ) as edUl ) l ) er bound we ( ' aJ-culated a . t86 . 5% . We concludel ; ha . 1; corl ) us-drivenexl ; racl ; ion of gramma . 1; ical\['eaJ ; uresia . promising lnethodology for line-grained verb classilica  . tion . 
1 Introduction \]) et a . i leal hf form a . tiona . I ) outverbs is critical to a . 
broadra . nge of NI , I ) and lit1; asks , yet ; ilsmauus . 1(lel ; (' rmin a . tion for la . rgenuml ) erso\['verl ) s is difficult a ml resource intensive . I . esea . rcho , Itima . ul , ( ) matic , a (- quisil ; iono\['verb-I ) as as edk , owl(~(Ig ( , has succededing leaning sylH . a . (' l ; icl ) rol ) erties o\['verl ) s such assul ) ca . tegoriza . tion frames from el > line resources ( Irent , \]9!)3; lh'iscoea . ndC , a . rroll , 1997; \]) err , 1997; Ma . nning ,  \]993) , ll , ecently , researchers have investiga?ed statistica . l corpus-ba . sed methods for lexica . lsema . ntic classitica . tion from synta . ctic prol ) erties of verl ) usage ( A one a . ndMcKee , \]996; l , a . pa . ta , and Brew , \]999; Schulte im Wa . lde ,: 1998; Stevensona . nd Merle . 1999; Steve . n-son et al . , \]999; McCarthy , 2000) . 
C , or l ) us-based al ) pro~m hes to lexica . lsema . ntic classitic ~ tion in pa , rticular ha . vedra . wn on Levin's hypothesis ( I , ev in ,  1993 ) that verbs can be classi-lied according to the dia  . thesis a Jterna . tions(a . lter-nations in the syntactic expressions o\["a . rguments ) ill which they f ) articil ) a . tel ' or exa . mple , whether a . 
* This research was partly sponsored 1 ) yUSNSI " grants #9702331 and #9818322 , Swiss NSI " Mlowshlp 82 10-d 65 (; 9 , Information Sciences ( . Ollll(il of Hurters University and Ill CS , U . of Peimsylwmia . ' l'h is research was conducted wldle the tirst author was atllutgers University  . 
Paola Merle
LAT\],-\])cpa.rtment of l,inguisCics
Univcrsil ; y of ( leneva2 rue de C ' ~ m dolle121\]Gent ; red- . Su is semer Zo ? lettres , un?ge , ch verb occurs in the dative/preposition a . l phrase al-tern a . tion in l '; nglish . One diagnostic for dis . thesis a . lternations i the sulx ' a . tegorization aJternatives of a . verb . l to wew ~, r , some classes exhibit the same subca . tegoriza . tkm possibilities but differ in their a . rgument structures , i . e . tim content el ' the the-real ; it roles assigned to the arguments of l ; he verb . 
rF his gyl ) e of situation consl ; it utes a . pa . rticula . rly difficult case R ) r corpus-based classification methods . 
In this paper , we apply corpus-based lexica . la . cquisition methodology 1 ; odistinguish classes of verbs which allow the same subca  . tegoriza ? ions , but differ in tlte Jna . tic roles . We first assume tha . tone ca . na . ut;oma . tiea . lly restrict l ; he choice o\['(' lasses to those that ; paxl ; icil)a . 1; e in the relewu~t subcate-gorizations ( c\[' . ( l , a . p~ta . and Brew , \]999)) . Our prOl ) OS a . 1 is lhen to uses t . a . tistic sovel:di ~ Ll ; hesis alterna . nl , sasa , wa . y to \[' urther distinguish those verl ) swld challow 1 ; he same sul ) ca . tegoriza . tions ; achievi , g finegrained cla . ssifica . tion within that S('l , .   ( ) UI ' work \[' O ( tll SeSoll determining tile 1 ) eslse-ma . nl , ic class for a verl ) lgpc - the set of usages o1" a . verl ) across a . document or corpus rather t ; ha . nf l ) rasing lever bI , ok cn in ~ single local context . 
In this way , we c~u/exploit the broad beha . vioro1'theverba . cross 1 ; he corl ) uS to determine its most likely class overall . 
We investiga . te the proposed a . l ) l ) rOa Chinanin-del ) th case studyo\[' the three major classes of o1> tiona . llyinlra , nsitive w , r l ) s in English : ullergative , unaccusa , tive , and ol ) ject-drop . More specifically , according to l , evin's classifica J ; ion(l , ev in ,  1993) , the unerga . tives are ma . nner of motion verbs , such as jump and march ; the una . ccusa . tives are verl ) s of cha . nge of state , , such a . sopen and explode ; the object-drop verbs a . reun expressed object a . lCerna . -Lion verl ) s , such as played a . nd painted . These classes a . llsupl ) or t1) othtr~msitive and intra . nsi-1, i resul)cal , egoriza . tions , I ) ut a . redistinguished by the pal ; tern of them a . ticrole assignments i , osub-jecCa . nd object position . Wea . utomatica . lly cla . s-si(y these verbs on the basis of sta . tistical a , p-lying argunlent structures , using numerical features collected from a large syntactically annotated  ( tagged or parsed ) corpus . We apply machine learning techniques to determine whether the fi'equency distribn tions of the features  , individually or in combination , support automatic classification of the verbs . Top review our results , we demonstrate that combining only five numerical indicators is sufficient o reduce the er-ror rate in this classification task by more than  50% over chance . Specifically , we achieve a hnost 7 ( 1% accuracy in a task whose baseline ( chance ) per\[brmance is 34% , and whose expert-based upper bound is calculated at  86  . 5% . We conclude that a distribution-based method for lexical semantic verb classification is a promising avenue of research  . 
2 The Argument Structures
Our approach rests on tile hypothesis that , even in cases where verb classes cannot be distinguished by subcategorizations  , the frequency distributions of syntactic indicators can hold clues to the underlying thematic role differences  . We start here then with a description of the subca  . tegorizations and thematic role assignments for each of l  . he three verb classes under investigation . 
As optionally intransitive verbs , each of the three classes participates in the transi-tive/intransitive Mternation : 
Uuergative ( la ) The horse raced past the barn.
(1b ) The jockey raced the horse past tile barn.
Unaccnsative (2a ) The buttermelted in the pan.
(2b ) The cook melted the butter in the pan.
Object-drop(3a ) The boy washed the hall.
(3b ) The boy washed.
Unergatives are intransitive action verbs , as in (1) , whose transitive form can be the causative counterpart of the intransitive form  . In the causative use , the semantic argument hat appears as the subject of the intransitive  , as in ( la ) , surfaces as the object of the transitive , as in ( lb ) ( Ilale and Keyser ,  1993) . Unaccusatives are intransitive change of state verbs  , as in (2a ) ; the transitive counterpart for these verbs exhibits the causative alternation  , as in (2b ) . Object-drop verbs , as in (3) , have a noncausative transitive/intransitive alternation  , in which the object is simply optional . 
Subj of
Classes Trans
Unergative Causal Agent
Unaccusative Causal Agent
Object-drop Agent
Obj of Subj of ' r Prans In trans
Agent Agent ' ? hemer I ' heme
Theme Agent
Table 1: Summary of Thematic Alternations.
Each class is distinguished by the content of tile thematic roles assigned by tile verb  . For object-drop verbs , tile subject is all Agent and the optional object is a Theme  , yielding tile thematic assignments ( Agent , Tlmme ) and ( Agent ) for the transitive and intransitive alternants respectively  . 
Unergatives and uuaccusatives differ \['1"o111 object-drop verbs in participating in the causative alternation  , and also differ from each other in their core thematic argument  . In an intransitive unerga-live , the subject is an Agent , and in an intransitive unaccusative , the subject is a Theme . In the causative transitive form of each , this core semantic argument is expressed as the direct object  , with the addition of a Causal Agent ( the causer of the action ) as subject in bol ; h cases . The thematic roles assigned , and their mapping to syntactic position , are summarized in Ta . ble 1 . 
3 The features for C lass i f i ca t ion The key to any automatic lassification task is to determine a set of ' useful fea  . tures for discriminating the itenls to be classitied  . In what follows , we refer to the cohnnns of Table 1 to explain \] to w we expect the thematic distinctions to yield distributional features whose frequencies discriminate among the classes ~th and  . 
Considering column one of Table 1 , only unergative and unaccusa . tiverbs assign ~ Causal Agent to the subject of the transitive  . We hy-1 ) othesize that the causative construction is linguistically more complex than the simple argument optionality of object-drop verbs  ( Stevenson and Merlo ,  1 . 997) . We expect then that object-drop verbs will be more fi:equent in the transitive than the other two classes  . Furthernmre , the object of an unergative verb receives the Agent role  ( see the second column of Table 1 . ) , a linguistically marked transitive construction ( Stevenson and Merlo ,  1997) . We therefore xpectuner ga-tives to be quite rare in the transitive  , leading to a three-way distinction intransitive usage among the three classes  . 
Second , due to the causative alternation of TrmlsitiviLy Unaccusativ cs and unergativ cs have ~  , causative transitive , hence lower transitive use . Fur-l ; hc ' rl nor c , unerga . tivcs ha . rea . nagent . ire object , hence very low transitive use . 
Pa . ssivcVoice Passive implies transitive use , hence correlated with transitive feature . 
VBN Tag Passive implies past pa . rt ; iciple use(VBN ) , hence correlated with transitive ( and passive ) . 
Causativity ( ) l ) ject-drop verbs do not have a . causal agent , hence low " ca . usative " use . Unergatives are rare in the transitive , hence low cmlsative use . 
Animacy Unaccusatives have a Theme subject in the intransitive  , hence lower use of animal , esubjects . 
unergatives and nnaccusatives , the l , hematic role of the subjec ~ of the intransitiw , ~ is identical to that of the objecl of the transitiw  ;  , as shown in columns two and three of Table 1 . C , iven the identity of thematic rolemal ) ped to subject and object positions , we expect to observe thesa . menoun occurring at times a . s subject of the verb , and at other times as object of the verb . In con-trast , for object-tirol ) verbs , Cite then m . ticroleo\['thesul ) jecto17 the intransitive is identical to l ; ha , of the sul ) ject of the transitive , not the object of the transitive . Thus , we expect that it will be less common for the same noun to occur in subject and object position of the same object-drop verb  . We hypothesize that this pattern of thematic role assignments will be retlected in difl'e rential amount of u ~' ~ age across the classes of the same nouns as subjects and ol  ) jects for a given verb . Furthermore , since the causative is a transitive use , a . nd the 1, ra . nsitive use of unerga . gives i oxpocl ; ed to be rare . , this overlapo (' subjects and ob . iects should primarily distinguish unaccusatives ( predicted to have high overlap of subjects and objects  ) from the other two classes . 
Finally , considering columns one and three of Tal)le 1 , we note that unergative and objecl ; -drop verbs assign all agentive role to their subject in both the transitive and intra  . nsitive , while unac-cusatives assign an agentive role to their subject only in the tr~msil  , ive . Under the assutn pLion that the intransitive use of ' unaccusatives in ot rare  , 1 we then expect thai , unaccusatives will occur less often overall with an agentive subject than the other two verb classes  . On the flu:ther assumption that Agents tend to be animate entities more so than Themes  , we expect that unaccusatives will occur less freqnently with an animate subject compared to unergative and object-drop verbs  . 
Note the importance of our use of frequency distributions : the claim is not that only Agents can ~ This as sumpl  , ion is based on the linguistic conlplexity of the causative  , and borne out in our corpus analysis . 
be animate , but rather that nouns that receive an Agent role will more often beanimate than nouns that receive a Theme  , ' ole . 
The above interactions between thematic roles and the syntactic expressions of arguments thus lead to three features whose distrit  ) utional properties appear promising for distinguishing the verb classes : transitivity  , causativity , and an imaey of subject . We also investigate two addition M syntactic l'ea . l , ures , the passive voice and tile past pa . r-ticiple POS tag(VI3N ) . These features are related to the transitive /intransitive Mternal  ; ion , since a passive use implies a transitive use of the verb  , and then se of passive in turn implies the use of the past participle  . Our hyl ) ol ; hesis is that these five features will exhibit distributional differences in the observed usages of the verbs  , which can be used for classifica . tion . The features and their expected relevance are summarized in  '13ble   2  . 
4 Da ~ a Collection and Analysis
We chose a set of 20 verbs from each of three classes . The complete list of verbs is reported in Appendix A  . Recall that our goal is to achieve a finegrained classification of verbs that exhibit the same subcategorization frames  ; thus , tile verbs were chosen because they do not generally show massive del  ) artures from the intended verb sense ( and usage ) in the corpus . 2 In order to simplify tile counting procedure , we included only tile regular ( "- ed " ) simple past/past participle form of tile verb , assuming that this would approximate the distribution of tile features across all forms of the verb  . Finally , as far as we were able given the preceding constraints  , we selected verbs that could occur in the transitive and in the passive  . 
We counted the occurrences of each verb token in a transitive or intransitive use  ( 3' RANS )  , illa 2~l~hough note that there are only 19 unaccusatives because ripped was excluded fl'om the analysis as it occurred mostly in a very different use  ( ripped off ) in the corpus from the intended diange of state usage  . 
817 passive or active use ( PASS) , in a past participle or simple past use ( VBN ) , in a causative or noncausative use(tAgS ) , and with an animate subject or not ( ANIM ) , as described below . The first three counts ( TRANS , I'ASS ~ VBN ) were performed on the LDC's 65-million word tagged ACL/DCI corpus ( Brown , and Wall Street Journal 1987-1989) . 
The last two counts ( CAUS and ANIM ) were performed on a 29-million word parsed corpus ( \gall Street Journal 1988 , provided by Michael Collins ( Collins ,  1997)) . The features were counted as follows : TaANS : The closest noun following a verb was considered a potential object  . A verb immediately \[ bllowed by a potential object was counted as transitive  , otherwise as intransitive . 
pass : A token tagged VBD ( the tag for simple past ) was counted as active . A token tagged VBN ( the tag for past participle ) was counted as active if the closest preceding auxiliary was have  , and aspassive if the closest preceding auxiliary was be  . 
VBN : The counts tbr VBN/VBI ) were based on the POS label in the tagged corl ) us . 
Each of the above counts was normalized over all occurrences of tim "- ed " form of the verb  , yielding a single relative fi : equency measure \[' or each verb for that feature  . 
tags : For each verl ) token , the subject and object ( it ' there was one ) were extracted from the parsed corpus , and the proportion of overlap between subject and object nouns across all tokens of a verb was calculated  . 
ANIM : To approximate animacy without reference to a resource external to the corpus  ( such as WordNet )  , we count pronouns ( other than it ) in subject position ( cf . (Aone and McKee , 1996)) . 
The aSSUlnption is that the words I , we , you, . ~' tze , he , and theft most often refer to animate entities . 
We automatically extracted all subject/verb tuples  , and computed the ratio of occurrences of pronoun subjects to all subjects for each verb  . 
The aggregate means by class resulting from the counts above are shown in Table  3  . The distributions of each feature are indeed roughly as expected according to the description in Section  3  . 
Unergative show a very low relative fi'equency of the TRANS feature  , followed by unaccusatives , then object-drop verbs . Unaccusative verbs show a high frequency of the CAUS feature and a low frequency of the ANIM feature compared to the other classes  . Although expected to be a redundant indicator of transitivity  , pass and VBN do Ta . ble 3: Aggregated Relative Frequency Data \[' ortile Five Features  . E = unergatives , A = unac-cusatives , O = object-drol ) s . 




NMEANI~ELATIVE F REQUENCY
TR , ANS PASS VBNCAUS ANIM 200 . 23 0 . 07 0 . 21 0 . 00 0 . 25 19 0 . 40 0 . 33 0 . 65 0 . 12 0 . 07 20 0 . 62 0 . 31 0 . 65 0 . 04 0 . 15 not distinguish t ) etween unaccusative and object-drop verbs , indicating that their distributions are sensitive to factors we have not yet investigated  , a 5 Experiments in Classification The frequency distributions of our features yield a vector for each verb that represents the relative frequency w dues for the verb one a cln dimension :\ [ verb  , TRANS , PASS , VBN ~ CAUS , ANIM , class\]Example:\[opened ,   . 69,  . 09,  . 21,  . 16,  . 36 , unaec\]\?euse the resulting 59 vectors to train an automatic classifier to determine  , given a verb that exhibits transitive ~ intransitives ttb categorization frames  , which of the three major lexical semantic classes of English optionally intransitive verbs it belongs to  . Note that the baseline ( chance ) per-Ibrmance in this task is 33 . 9% , since there are 59 vectors and 3 possible classes , with the most coin-men class having 20 verbs . 
We used the C5 . 0 machine learning system ( tnttp://www . rulequest . com ), a newer version of C4 . 5 ( Quinlan ,  1992) , which generates decision trees and corresponding rule sets from a training set of known classifications  . We found little to no difference in performance between the trees and rule sets  , and report only the ruleset results . \? e report here on experiments using a single holdout training and testing methodology  . In this approach , we hold out a single verb vector as the test case  , and train the system on the remaining 58 cases . We then test the resulting classifier on tile single hold out case  , recording tile assigned class for that verb . This is then repeated for each of the 59 verbs . This technique has the benefit of yielding both an overall accuracy rate  ( when the results are averaged across all 59 trials )  , as well as providing tile data necessary tbr determining accuracy for each verb class  ( because we have the classification of each verb when it is the test case  )  . This allows us to evaluate tile contribution a These observations have been confirmed by t -test  . s between feature values for each pair of classes . 
818% d ) le < 1:: Percent Accuracy of Verb Clas-sifica , l , i on Task Using \] , ' eatures in Combination . T = TllANS ; \]) = PASS ; \/= VBN ; C = CAUS ; An = ANIM . E = unergatives , A = unaccusatives,
O=:ol ) ject-drops
Percent Accuracy by Class
AlllEIaI0\],' ca . I , 1llJes1 .  ' . I ' PVC An 69 . 5 85 . 0 2 . PVC An 64: . 4: 80 . 0 3 . TVCA n 71 . 2 80 . 0 4: . 51 ' PCA n 61 . 0 65 . 0 5 . TP VA n 62 . 7 70 . 0 6 . 51 ' PVC 61 . 0 80 . 0 63 . 2dT . d 73 . 7 68 . 4 63 . 2 42 . 1 60 . 0 65 . 0 60 . 0 50 . 0 55 . 0 60 . 0 of individual feal:ures with respect to their effect on the perfornlance of individual classes  . 
We performed experiments on the \[' ullsel , of features , as well a . seach subsel , offea . l , ures wil , ha . single f~ture remow ; d , as reported in Tabled . Consider l ; he first column in the ta . ble . The first line shows that the overall ~ ccuracy for all live features is  69  . 5%, a reduction in tile error ra . te of more than 50% above the baseline . The removalo\["the PASS lea . lure appears to improve performance ( row 3 of Ta . ble 4) . However , it should be noted that this increase in performance results h:oln a single addition a J verb being classified correctly  . There-ma . in in grows show thal no feal , ure is superflous or hm'mfl dasl , he removal of any I ' ealure has a . 58% negative elfect on l ) erR ) rmance . Coral ) arable . 
accuracies have been demonsl ; rated vsing a more thorough crossvalidation methodology a  . nd using reel ; hods that are , in principle , better a , t taking adva . nl , age of correlated lea , lures ( Stevenson and Merle , 1999; Stevensonel . al . , 1999) . 
q ' he single hold out prol , ocol provides new data , f branalysing the performal meon individual verbs and classes  . The class-by-class accuracies a . reshown in the remaining columns of Ta . ble 4 . \? e can see clearly thal , using all five features , l , he unergatives are classified with much greater accuracy  ( 85% ) than l , he UlmCCU satives and object-drop verbs (63 . 2% and 60 . 0% respectively ), as shown in the first row . Therema . in in grows show that this l ) al , tern generally holds \[' or l , he subsel , s of features as well , with tire excel ) lion of lined . 
\? hileful , ure work on our verb classificalion task will need lo focus on deterlnining features thal bel  , terdiscriminate unaccusative a . nd object-drop verbs , we can ah : eady exclude an explanation of the resull  , sbased simply onl , he wwbs ' or tile classes ' frequency . Unergatives have tile lowest average ( log ) frequency ( 1 . 3) , but are the best classified , while unaccusatives and object-drops are comparable  ( a . verage log fi'equency = 2) . If we group verbs by frequency , the proportion of errors to lhetotal number of verbs remains fairly similar  ( freq 1: 7 errors/23 verbs ; fi:eq . 2:6 errors/24 verbs ; freq . 3:4 errors/10 verbs ) . The only verb of frequency 0 is correctly classified , while lhe only one with log frequency 4 is not . In sum , we do not find that more frequent classes or verbs are more accurately classitied  . 
lml mrtantly , the experiments also enable us to see whether the fealures indeed contribute to discriminating the classes in the manner predicted in Seclion  3  . The single holdout results allow us to do 1; his , by comparing the individual class labels assigned using the full sol  , of five features ( TIIANS , PASS , VBN , CAUS , ANIM ) to the class labels assigned using each size four subset of features  . This comparison indicates 1 ; he changes in class labels l , hat we can a . l , tribul , et ol , headded feature ingoing fi'oma size four subset to the full set of features  . 
( The individual class labels supporling our a . naly-s is below a . rea . vailable from the authors . )\? e con-cent ; rate on tile three main features : CAUS , ANIM , TRANS . \? efilial thai , the behaviour of lhese fea-l , ures genera Jly does conform to our predicl ; ions . 
We expected that TRANS would help make a . three-way distinction among the verb classes . While unergatives are ah : eady accurately classified with-Ollt TRANS  , inspection of lhechange in class la . -bels reveals that the addition of TRANS tOtire sel  ; improves performance on unaccusatives by helping to distinguish  1  ; hem from object-drol ) s , llow-ever , in this case , we also observe a loss in precision of unerga . lives , ince some object-drops are now classitied a . sunergatives . Moreover , we expected CAUS and ANIMtO be parl , icularly help fidinidenti\['yingunaccus~l , ives , and this is also borne out in our analysis of individual la  . bels . We note that the increased accuracy from CAUS is primarily due to bel  , terdisl , inguishing unergatives from unaccusatives , and l , he increased accura . cy from AN1M is primarily due go better distinguishing un -accusatives from objecl  , -drops . \? e conclude tha . t the feal , ures we have devised are successful in clas-siting optionally  1  , ra . nsil ; ive verbs because they ca . p-lure predicted if l ' erences in underlying argument struct lrre  .   4   4 Matters are more cmn plex with the other two features and we arc still interpreting tile results  . Our prediction Kappa Statistics ) of Three Experts ( El , E2 , h ; 3 ) Compared to a Gold Standard ( Levin ) and to the Classifier ( Prog )  . Numbers in parentheses are percentage of verbs on which judges agree  . 
PltOGF.I.E2E3
El . 36 (59)
E 2.50(68).59 (75)
E 3.49(66).53 (70).66 (77)
LEWN . 54 (69 . 5)  . 56 (71 . )  . 80 (86 . 5)  . 74  ( 83 )   6 Comparison to Experts In order to evaluate the performance of the algorithm in practice  , we need to compare it to the accuracy of classification performed by an expert  , which gives a realistic upper bound for the task . 
In ( Merle and Stevenson ,  2000 ) we report the re-suits of an experiment that measures experts per-t brmance and agreement on a classification task very similar : to the program we have described here  . The results summarised in Table 5 illustrate the performance of the progra , m . On the one hand , the algorithm does not perform at expert level , as indicated by the fact that , for all experts , the lowest agreement score is with the program . On the other : hand , the accuracy achieved by the program of 69 . 5% is only 1 . 5% less than one of the human experts in comparison to tire gold standard  . In fact , if we take the best performance achieved by an expert in this task  86  . 5%--as the maximum achievable accuracy in classification  , our algorithm then reduces the error rate over ; chance by approximately 68% , a very respectable result . 
7 Discussion
The work here contributes both to general and technical issues in automatic lexical acquisition  . 
Firstly , our results confirm the primary role of argument structure in verb classification  . Our experimental focus is particularly clear in this regard because we deM with verbs that are ~ Illilli-was that VBN and PASS would be have similarly to TRANS  . 
In fact , PASS is at best unhelpful in classification . VBN does appear to make the expected I . hree-way distinction . 
The change ill class labels shows that the improvement in performance with VBN results from better distinguishing unergatives fi'om object -drops  , and object-drops from un-accusatives . The latter is surprising , since analysis of the data found that the VnN feature values are statistically in-distinc ~ for the object-drop and unaccusative classes as a whole  . 
mal pairs " with respecto argument structure .   13y class if ~ ying verbs that show the same subcatego -rizations into different classes  , we are able to eliminate one of the confounds in classification work created by the fact that subcategorization adargument structure : M'e largely co-variant  . We can infer that the accuracy in our classification is due to argument structure information  , a . s subcategorization is the same for : all verbs , confirming that the con , tent of thematic roles is crucial for classification  . Secondly , our results further support the assumption that thematic differences such as these are apparent not only in differences in subcategorization frames  , but also in differences in (; heir frequencies . We thus join the many recent results that all seem to converge in SUl  ) porting the view that the relation between lexical syntax and semantics can be usefully exploited  ( Aone and McKee , 1996; l ) or r , 1997; Dorr and Jones , 1996; Lapata and Brew , 1999; Schulte im Walde , 1998; Siegel ,  1998) , especially in a statistical franmwork . 
Finally , we observe that this information is detectable in a corpus and can be learned automatically  . Thus we view corpora , especially if annotated wil ; h currently available tools , a . s useful repositories of implicit grammars . 
Technically , our N ) proach extends existing corpus-based learning techniques  1  ; o a more complex lea . ruing problem , in severa J dimensions . Our statistical apl ) roach , which does not require explicit negative xamples , extends ai ) l ) roaehes that encodel ~ evin's alternations directly  , as symbolic properties of a verb ( Dorr et al , 1995; l ) or r and J ones , 1996; l ) or r ,  1997) . We also extend work using surface indicators to approximate underlying properties  . ( Oishi and Matsumoto ,  1997 ) use case marking particles to approximate graim nat-ical functions  , such as subject and object . We improve on this approach by learning argument structure properties  , which , unlike grammatical functions , are not marked l nor phologically . Others have tackled the problem of lexical semantic classification  , as we have , but using only snbeate-gorization frequencies as input data  ( Lapata and Brew ,  1 . 999; Sehulte im Walde , 1998) . By contrast , we explicitly address the definition of features that cantap directly into thematic role differences that are not reflected in " subcategorization distinctions  . Finally , when learning of thematic role assignment has been the explicit goal  , the text has been semantically annotated ( Webster and Marcus ,  1989) , or external semantic re-19!)6) . We extend these results by showing that them ; ~tic in form a , tion can 1) einducexl from corpus

The exl ) erimental results show that our method is l ) owerful , and suited to Cite classitica . l;i(m of lex-ica . 1 items . However , we have not yet addressed the problem of verbs that can h  ; ~ve multiple classifications . We think tha . t many eases of a m-1 ) iguous classification of verb types can 1 ) e addressed with the notion of intersective sets in-troduced by  ( D a . ng et al , 71998) . This is an im-t ) ortant concept tha , tl)rOl ) OSestha , t"i'egula , r " a . m-biguity in classifica . tion-i . e . , sets of v (; rbs that ha . ve the same multi-way classitications a ~ccording to  ( l , ev in , 1993) can be captured with a . liner-grained notion of lexical semantic classes  . I~x-tending our work to exploit this idea . requires only to define the classes a . pl ) ropriately ; the basic a . 1) t ) roac\]l will remain the same . When we turn to consider ambiguity , we must a . l so address the l ) robleml ; ha . t individual insta . nces of verl ) s may come from diffel : ent classes . In future research wet ) lant o extend our method to the ( '\] a . ssificagion fa . m biguous tokens , by experimenting with a . func-tics that combines severa J sources of information : a biast br the verb type  ( using the cross-corpus sta . l ; istics we collect ), as well as \[ ~ a . tures o\["the usage of the insta . nce being class iiiod ( cf . ( l , apa . taa,n<l I~rew , t999; Siegel , 199, q)) . 
References
Cllinatsu Aolm and Dot@as Mcl(ec .  1990 . Acquiring predicate-argument mapping information in multilingual texts  . In Branimirl \]oguraev and Jamesl ) ustetjovsky , editors , Cou ) us \]) rocess in q . \[ orl , cxieal Acq ~ tisition , pages 191-202 . MIq ' Press . 
Michac'\]lh'ent .  1993 . l " rom grammar to lexicon : Un SU l ) er-vised learning o\[lexical syntax . Compatational Linguistics , 1912):243262 . 
' lk:dBriscoe and . / ohn Cm'roll .  1997 . Automatic extraction of subcategorization from corpora  . In IS " ocs of the I " ~ Hh
ANLPCo , @ rence , pages 356-363.
Michael John Collins .  1997 . Three generative , lexicallsed models for statistical pro'sing . InlS " ocs of ACL'97, pages 1623 . Madrid , Spain . 
Hot Trang Dang , Karin Kipper , Mm'thal ) almer , and . lose phlose nz weig .  1998 . Investigating regular sense exl , ensions 1 oased on intersective 1 , evin classes . \[ n Procs of COI , ING-ACL'98 , pages 293299 , Montreal , 

B Oltnie 1) or rat tdl ) ( n , gJ ones .  1996 . Role of word sense disambiguatloni lexical acquisition : lhedieting semantics from syntactic ues  . In 15"oc . of UOL1 NG'96 , pages 3; 22327 , Col ) enhage , l , ) en mark . 
Boluiiel ) or r , Joet ~ larlliall , and Amy V Veinberg .  1995 . 
\] ~ l : Oln syntactic encodillgs to thematic roes : luilding lexical entries for interlingual MT  . Journalo \ [ Machine ! l'ran . ? lation , 9(3):71-100 . 
\]\] onniel ) or r .  1997 . I , m-ge-scale dictionary construction for foreign language tutoring and inter\]ingual machine  . 
translation . Machine Translation , 12:155.
K(' . nl\[ale and Jay \] ( eyser .  1993 . On argument structure and the lexical representation fsyntactic relations  . In I ( . \] tale and . \] . l(eyser , editors , The Viewfl'oml Juilding 20 , pages 53-110 . MIT Press . 
Maria Lapata and Chris Brew .  11999 . Using subcategorization t , ore solve verb class ambiguity . In Frocs of Joint , 5' IGDATC on\[erence on Empirical Mett , ods in Natural
Langaage , College Park , M\]).
Beth Lcvin .  1993 . English Verb Classes and Alternations . 
University of Chicago Press , Chicago , I\],.
Christopher 1) . Manning .  :1993 . Automatic acquisition of a large subcategorlzation dictionary l > om corpora  . Inl ) rocs of AUL'93, pages 235-242 . Ohio State University . 
Diana McCarthy .  2000 . Using semantic l ) referenee Loid cn-tit'y verb participation in role switchitlg alternations  . In 15" oes of NAAC1,-2000, Seattle , Washington . 
l ) aola Merlo and Sllzantle Stevenson . 2000:-F , stablishing the upper bound and interjudge a grem cnt in a verb classification task  . Inl ) rocs of LI~EC-2000, pages 16591G64 . Athens , Greece . 
Akira Oishi and Yuji Matsumoto .  1997 . l ) etecting the organization of semantic subclasses of Japanese verbs  . International Journal of Corpus Linguistics ,  2(1):65 89 . 
,/ . \] toss Quinlan .  1992 .  (?4 . 5: l ~ rograms . for Maehi , ze Learning . Morganl ( aufmamt , San Mateo , CA . 
Sabine Sehulte im Walde .  1998 . Automatic semanl . i clas-sification of verbs according to their alternation behaviour  . AIMSl\]eport4(3), IMS , Universit St Stuttgart . 
I ", eicV . Siegel .  1998 . Linguistic Indicators . for L(mouaffcUnd crsland in 9 l)h . I ), thesis , I ) ept . of Comput(w Science , ( Johml)ia University . 
Suzalllle Steve Hsollalldl ) aola Merlo .  1997 . l ~ exica \] s t . ru (:- turc and \]) ro (: essing complexity . Lan  uag cand(7o\[pzilivc \]) rocc:,~se . ,% 12(1-2):3 . '19 399 . 
Suzanne Stevenson and \]) aola Mer\]o .  1999 . Verb classification using distributions of gt ' ammatical features  . Inl ) rocs of 1?, 4CL'99 . Bergen , Norway . 
Suzmme Stevenson , l ) aola Merlo , Natalia Kariaeva , and l(amin Whitehouse .  1999 . Supervised learning of lexical semantic verb classes using fl'equency distributions  . \[ nI ) rocs of Si . qLex '99, College Park , Maryland . 
Mort Webster and Mirth Mm'cus .  1989 . Automatic acquisition of the lexical semanl , ics of verbs fl ' om sentence frames . In Procs of ACL'89, pages 177-184, Vancouver,

Appendix A
Unergatives : floated , galloped , glided , hiked , hopped , hur-ried , jogged , jumped , leaped , marched , paraded , raced , rush cd , seo otcd , scurricd , skipped , tiptoed , t ~ vttcd , va , d t c d , wandered . Unaccusativ cs : boiled , changcd , cleared , collapsed , cooled , cracked , dissolved , divided , exploded , flooded ,   . folded , fractured , hard cned , melted , opc ncd , sim-mcred , solidified , stabilized , widened . Object-drops : bofrowed , e all c d , earv c d , clca~cd , danced , inheritcd , kiekcd , knittcd , or  an is cd , pack c d , paint c d , playcd , reaped , rcnted , skcle he . d , studied , swallowed , typed , washcd , ycllcd . 

