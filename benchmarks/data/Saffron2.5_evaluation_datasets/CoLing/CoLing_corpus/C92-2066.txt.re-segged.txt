Grammaires Stochastiques Lexical is desd'Arbres Adjoints 
Rdsumddu papier
Stochastic Lexicalized Tree-adjoining Grammars
Yves Schabes

Les technique stoch & stiques b4n6ficient aujourd'lmid ' unregain de popularit4  . Cependant , les modules stochastiques utilis ~ sont clairement in addquats pour l ' analyse syntaxique des langues naturelles  . Les for-malismes probabilistes quiout dr6   propos4s dans le do ~ maine de la th4orie delacommunication ( process us de Markovet n-grammes )   ( Pratt , 1942; Shannon , 1948; Shannon ,  1951 ) on t~terapidement r6fut6sen linguistique . Eneffet , cesmodules ont incapables de d$crire la syntaxe demani ~ re  hi4rarchique   ( sous for in td ' arbre )  . 
Deplus , les ph6nomSnes portant surdel on gues distances nepeuvent pas fit reprisencompte parces for-malismes  . Les grammaires stochastique shors coutext e ( Booth ,  1969 ) permettent d % la bore rune description hi4rarchique dela syntaxc . C cpendant , aucune approche utili santles grammaires stoct lastique shors con-text e  ( Lari and Young , 1990; Jelinek , Lafferty , and Mercer ,  1990 ) esten pratique aussiefl lcacequeles processus de Markovoules n-grammes  . Eueffet , lesr Sgleshors contextene sont p as directement sensibles aumotet done ? une distribution de mots  . 
Grammaires Stochastiques Lexi-calis ~ esd ' Arbres Adjoints Les grammaires lexical is desd'arbres adjoiuts consistent d ' unensembled ' arbres  , chacuna . ssoci4?unmot . Elles permettent de localiser laplup art des contraiutes syn-taxiques  ( pare x emple , sujet-verbe , verbe-objet ) to utendd crivant lasyntaxes ousformed ' arbres . 
Dansccpapicr , lanotion de derivation des grammaires lexical is desd ' arbres adjoints  ( tree-adjoining grammars ) est modifi6e auc as dederivatious stochas-tiques . Lenouve au formalisme , lesgrammaires stochas-tiques lexicalisdes d ' arbres adjoints  ( stochastic lexicalized tree-adjoining grammars ou SLTAG  )   , a despro-pridtdsuniques caril maintient la notion de distribution cnt rc mottouten manipulant lasyntaxe demaniSre  hi6rarchique  . 

Un algorithme pour calculer la probabilit d'une phraseest  pr4senter dansle papier . 
Ensuite , unalgorithm equiper met de r4estimer lesparam~tresd'une grammaire stochastiquel xicalisded ' arbres adjointsestdd crit  . Cette algorithme per-met der ~ estimer les param ~tresdefa ~ on  5  . aug-menter apr~schaque it6ration la probabilit6 du corpus . Cette algorithme peut 6tre   utilis6 comme algo-rithmed ' apprent issage . Lagrammai reinitialed'entr de g4n~re to usles roots derout esles faq ons possibles . 
L ' algorithme perm cten suite d'inf4rer uncgrammaire b . partirdu corpus . 
Evaluation Expdrimentale
Nous avons test dl ' algorithme der $ estimation sur uncorpus artificiel  ( Figure 1 ) et aussi sur les sequences de parties du discours  ( Figure 2 ) du corpus ' ATIS ' ( Hemphill , Godfrey , and Doddington ,  1990) . Dansles deux cas , l ' algorithme pour les grammaire stochas-tiques lexical is ~ esd'arbres adjoints converge plus rapi -dement que celui pour les grammaire shors context e  ( Baker ,  1979) . Cesexp driences confirment le fait queles grammaires stochastiques lexicalisdes d ' arbresad -joints permettent demod ~ liser des distributions entreroots queles grammaires stochastique shors contextene peuvent pase x primer  . 

SLTAG--\\SCFG . . . . . -"\\\ tttIIIII 2   3   4   5   6   7   8   9   i0 iteration Figure 1: Convergence avecun corpus ( lephr ~ qesdu language a " b " ln > 0 i \[ l
SLTAG--
SCFG .....
5 i0152025 itoration
Figure 2: Convergence surle ATIS Corpus A Che SDr- ; COLING-92 . NANTES , 2328 AOUT 1992425 PROC . OFCOLING-92, NANTES , AUG .  2328 ,   1992 Stochastic Lexicalized Tree-Adjoining Grammars * 
Yves Schabes
Dept . of Computer & Information Science
University of Pennsylvania
Philadelphia , PA 19104-6389 , USAs chabes@unagi , c is . upenn , edu

The notion of stochastic lexicalized tree -adjoining grammar  ( SLTAG ) is formally defined . The parameters of a SLTAG correspond to the probability of combining two structures each one associated with a word  . The characteristics of SLTAG are unique and novel since it is lexieally sensitive  ( as Ngram models or Hidden Markov Models ) and yet hierarchical ( as stochastic on text-free grammars )  . 
Then , two basic algorithms for SLTAG arc introduced : an algorithm for computing the probability of a sentence generated by a SLTAG and an inside -outside-like iterative algorithm for estimating the parameters of a SLTAG given a training corpus  . 
Finally , we should how SLTAG enables to define a lexicalized version of stochastic on text-free grammars and we report preliminary experiments showing some of the advantages of SLTAG over stochastic on text-free grammars  . 
1 Motivations
Although stochastic techniques applied to syntax modeling have recently regained popularity  , current lazl-guage model suffer from obvious inherent inadequacies  . 
Early proposal such as Markov Models , Ngram models ( Pratt , 1942; Shannon , 1948; Shannon ,  1951 ) and tlidden Markov Models were very quickly shown to be linguistically not appropriate for natural language  ( e . g . 
Chomsky (1964 , pages 1318 ) ) since they are unable to capture long distance dependencies or to describe hierarchically the syntax of natural anguages  . Stochastic contextfree granunar ( Booth ,  1969 ) is a hierarchical model more appropriate for natural languages  , however none of such proposals ( Lari and Young , 1990; Jelinek , Lafferty , and Mercer ,  1990 ) perform as well as the simpler Markov Models because of the difficulty of capturing lexical information  . The parameters of a stochastic contextfree grammar do not correspondirectly to a distribution over words since distributional phenomena over words that are embodied by the application of * This work was partially supported by DARPA Grant  N0014-90-31863  , ARO Grant DAAL03-89-C-0031 and NSF Grant 1RI90-16592  . We thank Aravind Joshi for suggesting the use of TAGs for statistical nalys is during a private discussion that followed a presentation by bS'ed J dine k during the June  1990 meeting of the DARPA Speech and Natural Language Workshop  . We are also grateful to Peter Braun , F Yed Jelinek , Mark Liberman , Mitch Marcus , Robert Mercer , Fernando Pereira said Stuart Shieber for providing vMu ~ ble comments  . 
more than one contextfree rule cannot be captured under the context-freeness a sumption  . This leads to the difficulty of maintaining a standard hierarchical model while capturing lexieal dependencies  . 
This fact prompted researchers in natural language processing to give up hierarchical language models in the favor of nonhierarchical statistical models over words  ( such as word Ngrams models )  . Probably for lack of a better language model , it has also been argued that the phenomena that such devices cannot capture occur relatively infrequently  . Such argumentation is linguistically not sound . 
Lexicalized tree-adjoining grammars ( LTAG ) t combine hierarchical structures while being hxie any sensitive and are therefore more appropriate for statistical analysis of language  . In fact , LTAGs are the simplest hierarchical formalism which can serve as the basis for lexicalizing contextfree grammar  ( Schabes , 1990; Joshi and Sehabes ,  1991) . 
LTAG is a tree-rewriting system that combines trees of large domain with adjoining and substitution  . The trees found in a TAG take advantage of the available x-tended domain of locality by localizing syntactic dependencies  ( such as finer-gap , subject verb , verb-objeet ) and most semantic dependencies ( uch as predicate-argument relationship )  . For example , the following trees can be found in a LTAG lexicon : 
S/k
NP , LVIP VP

VNP INP NP VP * ADV
LII Iuts J ~ np ~ nutJ hungrily
Since the elementary trees of a LTAG are minimal syntactic and semantic units  , distributional analysis of the combination of these elementary trees based on a training corpus will in form us about relevant statistical aspects of the language such as the classes of words appearing as arguments of a predicative lement  , the distribution of the adverbs licensed by a specific verb  , or the adjectives licensed by a specific noun . 
This kind of statistical analysis as independently suggested in  ( Resnik ,  1991 ) can be made with LTAGs because of their extended omain of locality but also because of their lexiealized property  . 
l We att all nle familiarity throughout the paper with TAGs and its lexicallzed variant  , See , for instance , ( Joehl ,  1987) , ( Schabes , Abeill ~ , and Joehi ,  1988) , ( Schabes , 1990) or ( Joslfi and Schabes ,  1~1) . 
ACTESDECOLING-92 . NANTES , 2328 AOUT 1992426 PROC . OFCOLING-92, NANTES , AUG .  2328 , 1992 In this paper , this intuition is made formally precise by defining the notion of a stochastic lexicalized tree-adjoinin grammar  ( SLTAG )  . We present an algorithm for computing the probability of a sentence generated by a SLTAG  , and finally we introduce an iterative algo-rithm for estimathlg the parameters of a SLTAG given a training corpus of text  . This algorithm can either be used for refining the parameters of a SLTAG or for inferring a tree -adjoining grammar frmn a training corpus  . We also report preliminary experiments with this algorithm  . 
Due to the lack of space , in this paper tim algorithms are described succinctly without proofs of correctness and more attention is given totile concepts and techniques used for SLTAG  . 
2 SLTAG hfformally speaking , SLTAGs are defined by assigning a probability to tile event that an elementary tree is combined  ( by adjunction or substitution ) on a specific node of another elementary tree . These events of combination are the stochastic processes considered  . 
Since SLTAG are defined on the basis of the derivation and since TAG allows for a notion of derivation independent from the trees that are derived  , a precise mathematical definition of the SLTAG derivation must be given  . For this purpose , we use stochastic linear indexed grammars ( SLIG ) to formally express SLTAGs derivations . 
Linear Indexed grammar ( LIG ) ( Alto , 1968; Gazdar ,  1985 ) is a rewriting system in which the nonterminal symbols are augmented with a stack  , in addition to rewriting nonterminals , the rules of the grammar can have the effect of pushing or popping symbols on top of tile stacks that are associated with each nonterminal symbol  . A specific rule is triggered by the non-term lnal on the lefthand side of the rule and the top element of its associated stack  . 
The productions of a LIG are restricted to copy the stack corresponding to tile nonterminal being rewritten to at most one stack associated with a nonterminal symbol on tile righthand side of the production ? In tile following  ,  \[ . . p \] refers to a possibly unbounded stack whose top element is p and whose remaining part is schematically written as '  . .' . \[$\] represents a stack whose only element is the bottom of the stack  . While it is possible to define SLIGs in general , we define them for the particular case where the rules are binary branching and where tile left hand sides are always in comparable  . 
A stochastic linear indexed grammar , G , is denoted by ( VN , VT , VI , S , Prod ) , where VN is a finite set of nonterminal symbols ; VT is a finite set of terminal symbols ; VI is a finite set of stack symbols ; SEVN is the start symbol ; Prod is a finite set of productions of the form :
Xo\[$po\]--*a
Xo\[ . .po \] -- . x ~\[ . .m \] x~\[$p ~\] x0\[ . .po \] -~ Xl\[$pd x~\[- . p~\]
Xo\[$Po\]--~Xl\[$pl\] X2\[$p2\] where XkEV jv , a EVT and po ~ . VI , Pl , P2EV\[;P , a probability distribution which assigns a probability  , 0 < P ( X\[ . .z \] ~ A ) < 1, to a rule , X\[ . .x \] -* A ~ . Prod such 2LIGs have been shown to be weakly eqt fivalent to " Ibee-Adjoining Graramars  ( V ~ jay-Shanker ,  1987) . 
that t be sum of the probabilities of all the rules that can be applied to any nonterminal nnotated with a stack is equal to one  . More precisely if , VXEVN , VpEVI : ~ p(xt . .pl -~ A ) = 1

P ( X\[ . . p \] --* A ) should be interpreted as the probability that X\[ . .p \] is rewritten as A . 
A derivation starts from S associated with the empty stack  ( S\[$\] ) and each level of the derivation must be validated by a production rule  . The language of a SLIG is defined as follows : L = wEVT ~\[ S\[$\]~w  . 
The probability of a derivation is defined as the product of tile probabilities of all individual rules involved  ( counting repetition ) in the derivation , the derivation being validated by a correct configuration of the stack at each level  . The probability of a sentence is then computed as the sum of the probabilities of all derivations of tile sentence  . 
Following tile construction described in ( Vijay-Shanker and Weir ,  1991) , given a LTAG , Glaa , we construct an equivalent LIG , G , u  a . Tile constructed LIG generate stile same language as Gtag and each derivation of Gtaa corresponds to a unique LIG derivation corresponds to a unique derivation in G  , ua ( and conversely ) . In addition , a probability is assigned to each production of the LIG  . For simplicity of explanation and without loss of generality we assume that each node in an elementary tree inGt  , 9 is either a leaf node ( i . e . 
either a foot node or a nonempty terminal node ) or binary branching , a The construction of the equivalent
SLIG follows.
The nonterminal symbols of Gs ti a are the two symbols ' top '  ( t ) and ' bottom ' ( b )  , tile set of terminal symbols is the same as the one of  Gta9  , the set of stack symbols is the set of nodes ( not node labels ) found in the elementary trees of Gla ~ augmented with the bottom of tile stack  ( $ )  , and tile start symbol is ' top'(t ) . 
For " all root nodes ~10 of an initial tree whose root is labeled by S , the following starting rules are added : t\[$\] ~ t\[$  , t0\] ( 1 ) These rules state that a derivation must start from the top of the root node of some initial tree  . P is the probability that a derivation starts from the initial tree associated with a lexical item and rooted by %  . 
Then , for all node '/ in an elementary tree , the following rules are generated . 
? If rhT/2 arettle 2 children of a node r/sucb that r/2 is on the spine ( i . e . subsume stile foot node ), include : b\[ . .~l ~&' tI$n , lt\[- . , ~ l ( 2 ) Since ( 2 ) encodes an immediate domination link defined by the tree-adjoining rammar  , its associated probability is one . 
? Similarly , if th T / ~ are the 2 children of a node r/such that rhison the spine  ( i . e . subsumes the foot node ), include : b\[ . .rt \] P =-*~ t\["rl~\]t \[$~\] ( 3 ) Since ( 3 ) encodes a ~ t immediate domination link defined by the tree-adjoining rammar  , its associated probability is one . 
a The algorlthnmex plained ill this paper cart be generalized to lexicadized tree-adjoining granunars that need not be in Chottmky Normal Form using techniqu ? ~ similar the one found in  ( Schabet ,  1991) . 
ACIES DECOLING-92 , NANTES , 2328 AO~rf 1992427 P~oc . OFCOLING-92, NANTES , AUG .  2328 ,   1992 * If ~/ tT/2 are the 2 children of a node q such that none of them is on the spine  , include : b\[$~\]p~l\]~\[$I~1\]t\[$i~2\] ( 4 ) Since ( 4 ) also encodes an immediate domination link defined by the tree-adjoining grammar  , its associated probability is one . 
? If 7? is a node labeled by a nonterminal symbol and if it does not have an obligatory adjoining constraint  , then we need to consider the case that adjunetion might not take place  . In this ease , include : t\[ . .~\] L b \[ . .~\]  ( 5 ) The probability of rule ( 5 ) corresponds to the probability that no adjunetion takes place at node q  . 
o If t / is an node on which the auxiliary treefl can be adjoined  , the adjunetiou of fl can be predicted , therefore ( assuming that ~ tr is the root node of f l ) include : t\["0\]Lt\[ . .rl , ,\]  ( 6 ) The probability of rule ( 6 ) corresponds to the probability of adjoining the auxiliary tree whose root node is ~/ ~  , say/3 , on the node 0 belonging to some elementary tree , say a . 4 ? If r ) ! is tim foot node of an auxiliary tree fl that has been adjoined  , then the derivation of the node below q\]must resume  . In this case , include : b\["0l\],~1b\[ . .\]  ( 7 ) The above stochastic production is included with probability one since the decision of adjunction has already been made in rules of the form  ( 6 )  . 
? Finally , if r h is the root node of an initial tree that can be substituted on a node marked for substitution r  )  , include : t\[$~\]Lt\[S ~ t\](g ) Here , p is the probability that the initial tree rooted by ~/ ~ is substituted at node q  . It corresponds to the probability of substituting the lexicalized initial tree whose root node is  71  , say 6 , at the node q of a lexicalized elementary tree , say a .   5 The SLIG constructed as above is well defined if the following equalities hold for all nodes ~ l : P  ( t\[ . .~/\] ---* b \[ . .~/\]) + E P(t \[ . .~/\] --* t\[ . .q0~\] ) = 1 (9)
P ( t\[$~/\]---*t\[$Ol\])----1(10)
EP ( t\[$\]-~t\[$O0\] )  = 1  ( 11 ) 4 Since the granmmr is lexicalized , both trees a and /3 are a ~ sociated with lexicaliter ~ s , mad the site node for adjtmction ~ correuponds to some syntactic modification  . Such lldeen capsulates Smodifiers ( e . g . s ~ tential adverbs as in " apparently John left ") , VP modifiers ( e . g . verb phr ~ ead verbs as in " John left abruptly " , NP modifiers ( e . g . relative clauses as in " The man who left was happy "  )  , N modifiers ( e . g . adtieetive ~ as in " prelty woman ") , or even sentent iM complements ( e . g . John think 8 that
Harry is sick).
sAmong other cases , the probability of thi ~ rule corr ~ ponds to the probability of filling some argument p  ( ~ ition by a lexiealized tree . It will encapsulate he distribution for Belectional restriction since the position of substitution is taken into account  . 
A gramular satisfying (12) is called consistent .  6
EP ( t\[$\]~w ) = 1 (12) wEZ *
Beside the distributional phenomena that we mentioned earlier  , SLTAG also captures the effect of adjoining constraints  ( selective , obligatory or null adjoining ) which are required for tree-adjoin in grammar . 73 Algorithm for Computing the
Probability of a Sentence
We now define an bottom-up algorithm for SLTAG which computes the probability of an input string  . The algorithm is an extension of the CKY-type parser for tree-adjoining grammar  ( Vijay-Shanker ,  1987) . The extended algorithm parses all spans of the input string and also computest belr probability in a bottom-up fashion  . 
Since the string on the frontier of an auxiliary is broken up into two substrings by the foot node  , for the purpose of computing the probability of the sentence  , we will consider the probability that a node derives two substrings of the input string  . This entity will be called the inside probability  . Its exact definition is given below . 
We will refer to the subsequenee of the input string w = ax " " aN from position i to j  , w ' . It is defined as follows : w~/'~f ai+t " . uj , if i >_j ' if / < j Given a string w = at .   .   . a N and a SLTAG rewritten as in ( 18 ) the inside probability , F ( pos ,  71 , i , j , k , l ) , is defined for all nodes 7/ contained in an elementary tree and for posEt , b , and for all indices 0 < i < j < k < I < N as follows: ( i ) If the node 7/ does not subsume the foot node of ( ~  ( if there is one )  , then j and k are un-bound and : l ~( pos ,  ~ , i ,  -  ,  -  , I ) d ~= lP ( pos\[$@~w ~ )   ( it ) If the nodey/subsumes the foot node 7/! of e , then : l ~( pos , rLi , j , k , l ) a ~ lP ( pos\[$@~wb\[$ollw ~ ) In ( ii )  , only the top element of the stack matter since as a consequence of the eonstrnction of the SLIG  , we have that if pos\[$tl\]~w ~ b\[$rll\]w ~ then for all string  7 eV/~we also have pos\[$Tr/\]~ w~b\[$7~l\]w~  . S Initially , all inside probabilities are set to zero . Then , the computation goes bottom-up starting from the productions introducing lexieal items : if r / is a node such that  b\[$7/\] -- ~ a , then : 1 if l = i + lAa = w ~+ t ( 1 ~ IW ( b'T l'i'-'-'l ) = 0 otherwise . 
Then , the inside probabilities of larger substrings are computed bottom-up relying on the recurrence qua- ~ We will not investigate im conditions under which  ( 12 ) holds . 
We conjecture that the techniques used for dmcking the eolmis-tency of stochastic contextfree grammars  ( Booth and Thomp6 on , 1973) can be adapted to SLTAG . 
rFor example , for a given node 0 setting to zero the probability o\[all rules of the forts  ( 6 ) ht ~ the effect of blocking adjunction . 
8Thls can be seen by obae ~ . ing that for any node on the path from the root node to the foot node of an auxiliary tree  , the stack remains unchanged . 
ACRESDECOLING-92, NANTES . 2328 AOt ~ T1992428 PROC . OFCOLING-92, NANTES . AUG . 2328,1992 lions stated in Appendix A . This computation takes in the worst case O ( IGl~N6 ) -time and O ( IGINa ) -space for a sentence of lengt bN . 
Once the inside probabilities c mnputed , we obtain the probability of the sentence flu follows : P  ( w ) aJP ( t\[$\]~ , ~) = Z~(t ,  $ ,  0  ,  -  ,  -  , I w l )   ( 14 ) Wc now consider the problem of reestimating a

4 Ins ide - Ous ide A lgor i thm for 1%eest imating a SLTAG Given a set of positive example sentences  , W = wt ' " wK , we would like to compute the probability of each rule of a given SLTAG in order to maximize thc probability that the corpus were generated by this SLTAG  . An algorithm solving this problem can be used in two different ways  . 
The first use is as a reestimation algorithm . In t tf is approach , the input SI , '1' A ( ~ derives structures that arc reasonable according to some criteria  ( such as a linguistic theory and some a priori kuowledge of the corpus  ) and the intended use of the algorithm is to refine the probability of each rule  . 
The second use is as a learning algorithm . At the first iteration , a SLTAG which generates all possible structures over a given set of nodes and terminal symbols is used  . Initially the probability of each rule is randomly assigned and then tile algorithm will reestimate tbese probabilities  . 
Informally speaking , given a first estimate of the parameters of a SLTAG  , the algorithm reestimates these parameters on the basis of the parses of each sentence in a training corpus obtained by a CKY-tyt  ) e parser . The algorithm is designed to derive a new estimate after each iteration such that the probability of the corpus is increased or equivalently such that tile crossentropy estimate  ( negative log probability ) is decreased : log ~ ( e ( r0 ) ) l t ( W , G ) - weW ( 15 ) wEWIn order to derive a new estimate , the algorithm needs to compute for all seutences in W the inside probabilities and the outside probabilities  . Given a string w = al .   .   . aN , t be outside probability , 0 ~( pos ,  ~ , i , j , k , It , is defined for all nodes rI contained in an elementary tree a and for posEt  , b , and for all indices 0 < i < j < k < l < N as follows: ( it If the node r/does not subsume the foot node of a  ( if there is one )  , then j and k axe un-bound as ld : . . de \] O '? ( P os , O , i ,  -  ,  -  , t)-P ( B"/CV~s . t . t\[$\]=~Wiopos\[$Ttl\]w ~ )   ( ii ) If the node ~/ doe subsume the foot node ~/! of a then :  0  '~  ( pos , O , i , j , k , l ) a eJ -/' (37 ~ V ~* * s . t . 
t\[$\]~Wlopos\[$Trl\]w ~ and b\[$7~ll\]~w\] ) Once the inside probabilities computed , the outside probabilities can be computed topdown by considering smaller spans of the input string starting with O "  ( t , $  , O ,  -  ,  -  , N ) = 1 ( by definition ) . This is done by computing the recurrence quations tated in Appendix B  . 
In the following , we assume that rI subsumes the foot node r/l within a same elementary tree  , and also that tll subsumes the foot node ~111 ( within a same elementary tree )  . The other cases are handled similarly . Table 1 shows the reestimation formulae for the adjoining rules  ( 16 ) and the null adjoining rules ( 17 )  . 
(16 ) corresponds to the average number of time that tl  .   .   .   . le L\[ . .T1\] . -* t\[ . .yqv \] is used , and (17) to th .   .   .   .   .   . 
age number of times no adjunction occn rred on T/ . The denominators of ( 16 ) and of ( 17 ) estimate the average number of times that a derivation involves tlLe expansion of t\[-  . ~/\] . The numerator of ( 16 ) estimates the average number of times that a derivation involves the rule t\[  . -7/\]-~t\[ . .Tirfl \] . Therefore , for example ,   ( 16 ) estimates the probability of using the rule/\['-~7\] ~l\["rplt\] . 
The algorittun reiterates until H(W , G ) is unchanged ( within some epsilon ) between two iterations . Each iteration of the algoritbm requires at most O  ( IGINe ) time for each sentence of length N . 
5 Grammar Inference with

The reestimation algorithm explained in Section 4 can be used botll to reestimate the paramcters for a SI  , TAG derived by some other mean or to infer a grammar from scratch  . Ill the following , we investigate grammar Inference from scratch . 
The initial grammar for the reestimation algoritiim consists of all SLIG rules for the tressill Lexicalized Normal I~brm  ( ill short LNF ) over a given set = aill . < i_<T of terminal symbols , with suitably assigned nonzero probability : 9
S0$4 sht~ai
The above normal form is capable not only to derive any lexicalized tree-adjoining language  , but also to impose ally binary bracketing over the strings of the language  . The latter property is important as we would like to be able to use bracketing information in the il L-put corpus as in  ( Pereira and Schabes ,  1992) . 
The worst case complexity of tim reestimation algo -rithm given iu Section  4 with respect o the length of the input string ( O ( NS ) ) makes this approach in general impractical for LNF grammars  . 
However , if only trees of the form fita ' and a ~" ( or only of tile form / ~' and a ~ )   , the language generated is a contextfree language and can be handled more efficiently by the reestimation algorithn L  9Adjoining constraints can be u~dintiffs normal form , They will be reflected in the SLI Geq ~ vaient grammar  . Indices have been added on S nodes in order to be able to uniquely refer to each node in the granunar  . 
AcrEsOECOLING-92, NANTES . 2328 AOOT1992429 DROC . OFCOLING-92, NANTES , AUG . 2328,1992 wwPW)xQW(t\[ . .~/\] ~ t\[ . -r/rp\])P ( t\[- . t/\]---, t\[ . .~Tt/t \] )  = 1  ( 16 ) ~ wp---~x\[R  ~0/ ) + ~_ ~ O ' ~ ( t\[ . .O \] --, t\[ . .~/r / , \] ) \] Ot ? ( t\["r/\]~t\["r/rY\] ) = ZP ( t\["O\]--*t\["O~Y\] ) ?Iw ( t'o /' i'r's'l ) xlW ( b'o'r'j'k's ) xOW ( t ' ~ l'i'j'k'l )   ( 18 ) i ) r , j~k , t)l/~w(r /) = ~ P ( t\[ . .r /\] ~ b\[ . .r /\]) x l~(t , o , i , j , k , l)xO~?(b , )l , i , j , k , l ) (19) i , j , k , I Table 1: Ke estimation of adjoining rules ( 16 ) and null adjoining rules ( 17 ) It can be shown that if , only trees of the form ~ a ~ and ~ a ~ are considered  , the reestimation algorithm requires in the worst case O  ( Na ) -time ) ? The system consisting of trees of the form ~' and c~can be seen as a stochastic lexicalized conle ~: t-free grammars since it generates exactly contextfree languages while being lexically sensitive  . 
In the following , due to the lack of space , we report only few experiments on grammar inference using these restricted forms of SLTAG and the reestimation algorithm given in Section  4  . We compare the results of the TAG insideoutside algorithm with the results of the insideoutside algorithm for contextfree grammars  ( Baker ,  1979) . 
These preliminary experiments suggest that SLTAG achieves faster convergence  ( and also to a better solution ) than stochastic on text-free grmn mars . 
5 . 1 In ferr ing the Language  a"b"\]n > 0 We consider first an artificial language . The training corpus consists of 100 sentences in the language L = a " b ' ~ ln > 0 randomly generated by a stochastic contextfree grammar  . 
The initial grammar consists of the trees ~' , fl  ~ , c ~ a and ab with random probability of adjoining and null adjoining  . 
The inferred grammar models correctly the language L  . Its rules of the form ( I ) ,   ( 5 ) or ( f i ) with high probability follow ( any excluded rule of the same form has probability at least  l0  -  a3 times lower than the rules given below )  . The structural rules of the form (2) ,  (3) ,   ( 4 ) or ( 7 ) are not shown since their probability always remain  1  . 
Z ? This can be Been by ol~erving that , for exaan pleinl(posji , i , j , k , I ) , it i ~ nece ~ y theea ~ that k = l , nnd also by noting that k is superfluous . 
t\[$ , Tg\]s : ~4t\[S , lg , 78\]t\[$og\]o_~t\[$ , lg , lg\]t\[ . -t/~\]z_~, ob\[, . ~7 ~\] t\[~\], . .~ o b \[,~\] t \[ . .~\] ~,? b \ [~\ ] t\[ . .o ~\] 1~0 b\[ . .o ~\] In the above grammar , a node S'k in a treec ~ a or / ~ associated with the symbol a is referred as t / ~  , and a node S ~ in a tree associated with basr / ~ . 
We also conducted a similar experiment with the insideoutside algorithm for contextfree grammar  ( Baker ,  1979) , starting with all pc~sible Chomsky Normal Form rules over  4 nonterminals and the set of terminal symbols a , b (72 rules ) . The inferred grammar does not quite correctly model the language L  . Furthermore , the algorithm does not converge as fast as in the case of SLTAG  ( See Figure 1 )  . 

IIIIIII
SLTAG--
SCFG .   .   .   .   .  " \  2   3   4   5   6   7   8   9   1   0 iteration Figure 1: Convergence for the Language anb " ln > 0   5  . 2 Exper iments on the ATIS Corpus We consider the part-of-speech sequences of the spoken-language transcriptions in the Texas Instruments sub-ACT ~ B E  COIANG-92  . NANTES , 2328AO~'1992430 PROC . OFCOLING-92, NANTES , AUG .  2328 , 1992 set of the Air Travel hfformation System ( ATIS ) corpus ( Hemphill , Godfrey , and Doddington ,  1990) . This corpus is of interest since it has been used for infcrring stochastic on text-free grammars from partially bracketed corpora  ( Pereira and Sehabes ,  1992) . We use the data given by Pereira and Schabes ( 1992 ) on raw text and compare with an inferred SLTAG . 
The initial grammar consists of all trees ( 96 ) of the form f l ~ , a ~ for all 48 terminal symbols for part-of-speech . As shown in Figure 2 , the grannnar converges very rapidly to a lower value of the log probability than the stochastic on text-free grammar eported by 
Pereira and Schabes (1992).
16 i 0
SCFG . . . . " ii it 510 1520 25 iteration
Figure 2: Convergence for ATIS Corpus 6 Conclusion A novel statistical language model and fundamental-gorithms for this model have been presented  . 
SLTAGs provide a stochastic model both hierarchical and sensitive to lexical information  . They combiae the advantages of purely exical models such ms Ngram distributions or Ilidden Markov Models and the one of ifier archical modes as stochastic on text-free grammars without their in hercnt limitations  . The parameters of a SLTAG correspond to the probability of combining two structures each one associated with a word and therefore capture linguistically relevant distributions over words  . 
An algorithm for computing the probability of a sentence generated by a SLTAG was presented as well as an iterative algorithm for estimating the parameters of a SLTAG given a training corpus of raw text  . Similarly to its contextfree counterpart , here estimation algorithm can be extended to handle partially parsed corpora  ( Pereira and Schabes ,  1992) . 
Preliminary experiments with a contextfree subset of SLTAG confirms that SLTAG enables faster convergence than stochastic on text-free grammars  ( SCFG )  . 
This is the case since SCFG are unable to represent lexieal influences on distribution except by a statistically and eomputationally impractical proliferation of nonterminal symbols  , whereas SLTAG allows for a lexi-eally sensitive distributional mmlys is while maintaining a hierarchical structure  . 
Furthermore , the techniques explained in this paper apply to other grammatical formalism such as combinatory categorial grammars and modified head grammars since they have been proven to be equivalent to tree-adjoining grammars and linear indexed grmnmars  ( Joshi , Vijay-Shanker , and Weir ,  1991) . 
Due to the lack of space , only few experiments with SLTAG were reported . A full version of tile paper will be available by tile time of the meeting and more experimental details will be reporte during the presentation of the paper  . 
In collaboration with Aravind Joshi , Fernando Pereira and Stuart Slfieber , we are currently investigating additional algorithn Ls and applications for SLTAG  , methods for lexical clustering and autonratic on struc-tion of a SLTAG from a large training corpus  . 

Aho , A . V .  1968 . lndexed grammars - An extension to contextfree grammars  . JACM , 15:647-671 . 
Baker , J . K .  1979 . Trainable grammars tbr speech recognition . In Jared J . Wolf and Dennis H . Klatt , editors , Speech communication papers present acd at the 97 ~h Meeting of the Acoustical Society of America , MIT , Cambridge , MA , June . 
llooth , Taylor R . and Richard A . Thoml ) son .  1973 . 
Applying probability measures to abstract languages  . 
IEEE7)' aasactions on Computers , C-22(5):442-450,

Booth , T .  1969 . Probabilistic representation f formal languages . In Tenth Annual IEEE Symposium on Switching and Automata Theory  , October . 
Chomsky , N . , 1964 . Syntactic Structures , chapter 23, pages 1318 . Mouton . 
Gazdar , G .  1985 . Applicability of indexed gr , ' unmars to natural anguages . Technical Report CSLI-85-34 , Center for Study of Language and Information . 
tlempt till , Charles T . , John J . Godfrey , and George IL Doddington .  1990 . The ATIS spoken language systems pilot corpus . In DARPA Speech and Natural Laaguage Workshop , Hidden Valley , Pennsylvania , 

Jelinek , F . , J . D . Lafferty , and R . L . Mercer .  1990 . Basic methods of probabilistic on textfree grammars  . 
Technical Report RC16374 (72684), IBM , Yorktown
Heights , New York 10598.
Joshi , Aravind K . and Yves Schabes .  1991 . Tree-adjoiuing grammars and lexiealized grammars . In Maurice Nivat and Andreas Podelski , editors , Defin-ability and Recognizability of Sets of Trees  . Elsevier . 

Joshi , Aravind K . , K . Vijay-Simnker , and David Weir . 
1991 . The convergence of mildly context-sensitive gramnmtical formalisms  , in Peter Sells , Stuart Shieber , and Tom Wasow , editors , Foundational Issues in Natural Language Processing  . MIT Press,
Cambridge MA.
Joshi , Aravind K .  1987 . An Introduction to Tree Adjoining Grammars . In A . Manaster-Ramer , editor , Mathematics of Language . John Beujamins , Amster-dana . 
Lari , K . and S . J . Young .  1990 . The estimation of stochastic on text-free grmn mars using the Inside-Outside algorithm  . Computer Speech and Language , 4:35-56 . 
ACRESDECOL 1NG-92 , NANTES , 2328 AO~r1992431 PROr' . . OI : COLING-92, NANTES , AUG . 2328,1992 Pereira , Fernando and Yves Schabes .  1992 . Inside-outside reestimation from partially bracketed corpora  . In 20 th Meeting of the Association for Computational Linguistics  ( ACL'9 ~ )  , Newark , Delaware . 
Pratt , Fletcher .  1942 . Secret and urgent , the story of codes and ciphers . Blue Ribbon Books . 
Resnik , Philip .  1991 . Lexicalized tree-adjoining ram-mar for distr ibutional analysis  . In Penn Review of
Linguistics , Spring.
Schabes , Yves , Anne Abeill ~, and Aravind K . Joshi . 
1988 . Parsing strategies with ' lexicalized ' grarn mars : Application to tree adjoining gra ~mnars  . In Proceedings of the 1~ lh International Conference on Computational Linguistics  ( COLING'88 , Budapest , Hungary , August . 
Sehabes , Yves .  1990 . Mathematical nd Computational Aspects of Lexicalized Grammars  . Ph . D . thesis , University of Pennsylvania , Philadelphia , PA , August . 
Available as technical report ( MS-CIS-90-48 , LINCLAB 179 ) from the Department of Computer Science . 
Schabes , Yves .  1991 . An insideoutside algorithm for estimating the parameters of a hidden stochastic contextfree grammar based on Earley's algorithm  . 

Shannon , C . E .  1948 . A mathematical theory of communication . The Bell System Technical Journal , 27(3):379-423 . 
Shannon , C . E .  1951 . Prediction and entropy of printed english . The Bell System Technical Journal , 30:50-64 . 
Vijay-Shanker , K . and David J . Weir .  1991 . Parsing constrained grammar formalisms . In preparation . 
Vijay-Shanker , K .  1987 . A Study of ? lbee Adjoining Grammars . Ph . D . thesis , Department of Computer and Information Science , University of Pennsylvmfia . 
A Computing the Inside Prob-abilities In the following  , the inside and outside probabilities are re\ ] ative to the input string w  . 3t " stands for the the set of foot nodes , S for the set of nodes on which substitution can occur  , ~ for the set of root nodes of initial trees , and , 4 for the set of nonterminal nodes of auxiliary trees  . The inside probability can be computed bottom-up with the following recurrence quations  . For all node v/found in an elementary tree , it can be shown that : 1 . If b\[$r/\]~a , I(b , 7 , i ,  -  ,  -  , I ) = dl if /= i+1 and if a = w~+1 , 0 otherwise . 
2 . \] f 71 E3c , l(b , 7/ , i , j , k , t ) = l if i = j and if k = l , 0 otherwise . 
3 . If b\[ . .7\] ~ t\[ . .Talt\[$7~\]: l(b , 7, i,j,k,I)=
El(t , 7j , i , j , k , m)xl(t , 7~, m , --,-, t)m = k4 . If b\[ . .7\] - - t\[$oa\]t\[ . .7z \] , l(b ,  7 , i , j , k , I ) = ~ I(t ,  71 , i ,  -  ,  -  , m ) xl(t , 72 , m , j , k , I ) m~i + l ~ . ffb\[$t~\]~t\[$~dt\[$7~\] , ( b ,  7 , i ,  -  ,  -  , 0 = El(t'T t'i'-'--'m ) xl(t ,  7~ , m , - , - , I ) m~i + l6 . For all node 7 on which adjunction can be performed : l ( t ,   ,   , i , j , k , 0=1 ( b , , , i , j , k , t ) ? P ( t\[ . .7\] ~ b\[ . .,l \]) + ? l(b , 7,r,j,k,s ) ? e(t\[ . .7\] - t\[- . , , id ) 7 . For all node 7ES : l(t ,  7 , i ,  -  ,  -  , l ) = Zl ( t'T l'i'--'--'l ) ? P ( t\[$7\]~t\[$Ta\] ) ' h8 . I(t , $ , i ,  -  ,  -  , l ) = EI(t , 7 , i , - , - , I ) ? P ( t\[$\]~t\[$0\])) l
B Computing the Outside
Probabilities
The outside probabilities can be computed topdown recursively over smaller spans of the input string once the inside probabilities have been computed  . First , by definition we have : O(t ,  $ ,  0  ,  -  ,  -  , N ) = 1 . The following recurrence equations hold for all node y found in an elementary tree  . 
1 . If 7E " g , O(t ,  7 ,  0 ,  -  ,  -  , N ) = e(t\[$\]~t\[$7\]) . 
And for all ( i , j ) ~ (0 , N ) , O(t , ~ , i ,  -  ,  -  , j ) = o(t ,   , 10 , i ,  -  ,  -  , j ) ? P (@%\]~@) ~\]) 2 . If 7 is an interior node which subsumes the foot node of the elementary tree it belongs to  , O(t ,  ~ , i , j , k , l ) = ~ O(b , % , i , j , k , q )) ? l(t ,  7~ ,  1 , - ,  - , q)q = t + , ? P ( b\["70\]~t\["Tlt\[$7~\])i1O(b , qo , p , j , k , l )) + Z ? l(t'71'P'-'-'i)~=0xP(b\[ . -70\]~t\[$7, lt\[ . .7\]) 3 . If T/is an interior node which does not subsume the foot node of the elementary tree it belongs to  , we have : o(t , 7  , i ,  -  ,  -  , t ) = vO(b , )lo , i , - , - , q ))
E ? l ( t ' ) h'l'-'-'q ) q = lq-i ? P ( b\[$70\]~t\[$7\]t\[$72\] ) + ? I ( t , 7~ , p , - , - , Q?P ( b\[$7 ol~t\[$7 , \] t\[$7\]) + ~ O(b'7?'i'j'Lq ) ? I(t ,  72 , l , j , k , q ) , = ,  ~= , + ,   . = . ? P ( b\[ . m ) ~@ 71 t\[ . .Td ) + ? I(t , 71,p,j,k,i ) ? P(b \[ . .%\] ~ t\[ . .7#\[$7\]) 4 . If T/E . 4 , then : O(t , 7 , i , j , k , l ) = k-l ~ ( O ( t '' l?'i'p'q'l )   ) ~ o ~? l ( t , 7o , j , p , q , k ) p = jq = ~+ , ? P ( t\["7o \]~ t\[- . % rl \]) ~ f ~% ( o(t , % , i , - , - , t )) + ? l(t , ) lo , j , -  , -  , k ) ? P ( t\[$%\]~t\[$%7\])5 . If 7 is a node which subsumes the foot node of the elementary tree it belongs to  , we have : O(b ,  7 , i , j , k , I ) = O(t ,  7 , i , j , k , l ) ? e(t\["7\]~b\[ . .~/\]) + ? l(t , 7o , p , i , l , q ) % p=oq=*\xP(t\["7o\]-t\["7o ) ?\]) 6 . And finally , if ) 1 is a node which does not subsume the foot node of the elementary tree it belongs to: 
O(b ,  7 , i ,  -  ,  -  , t ) = o(t ,  7 , i ,  -  ,  -  , t)xP ( t\[$7\]~b\[$7\]) + ? l(t , % , p , i , l , q ) 70 p = oq = ~\ xP ( t\[$7 o\]~t\[$7 oY/\] ) ACRESDE COLING-92 , NA me , s . 2328^o ~ rr 1992432 Paoc . OFCOLING-92, NANTES . AUG .  2328, 1992
