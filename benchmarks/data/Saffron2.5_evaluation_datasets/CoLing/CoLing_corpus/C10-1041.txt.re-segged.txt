Proceedings of the 23rd International Conference on Computational Linguistics ( Coling 2010), pages 358?366,
Beijing , August 2010
A Large Scale Ranker-Based System
for Search Query Spelling Correction

Jianfeng Gao
Microsoft Research , Redmond
jfgao@microsoft.com
Xiaolong Li
Microsoft Corporation
xiaolong.li@microsoft.com
Daniel Micol
Microsoft Corporation
danielmi@microsoft.com
Chris Quirk
Microsoft Research , Redmond
chrisq@microsoft.com
Xu Sun
University of Tokyo
xusun@mist.i.u-tokyo.ac.jp


Abstract
This paper makes three significant extensions to a noisy channel speller designed for standard written text to target the challenging domain of search queries . First , the noisy channel model is subsumed by a more general ranker , which allows a variety of features to be easily incorporated . Second , a distributed infrastructure is proposed for training and applying Web scale ngram language models . Third , a new phrasebased error model is presented . This model places a probability distribution over transformations between multiword phrases , and is estimated using large amounts of query-correction pairs derived from search logs.
Experiments show that each of these extensions leads to significant improvements over the state-of-the-art baseline methods.
1 Introduction
Search queries present a particular challenge for traditional spelling correction methods . New search queries emerge constantly . As a result , many queries contain valid search terms , such as proper nouns and names , which are not well established in the language . Therefore , recent research has focused on the use of Web corpora and search logs , rather than human-compiled lexicons , to infer knowledge about spellings and word usages in search queries ( e.g ., Whitelaw et al ., 2009; Cucerzan and Brill , 2004).
The spelling correction problem is typically formulated under the framework of the noisy channel model . Given an input query , we want to find the best spelling correction among all candidates : (1)
Applying Bayes ' Rule , we have (2) where the error model models the transformation probability from C to Q , and the language model ( LM ) models the likelihood that C is a correctly spelled query.
This paper extends a noisy channel speller designed for regular text to search queries in three ways : using a ranker ( Section 3), using Web scale LMs ( Section 4), and using phrasebased error models ( Section 5).
First of all , we propose a ranker-based speller that covers the noisy channel model as a special case . Given an input query , the system first generates a short list of candidate corrections using the noisy channel model . Then a feature vector is computed for each query and candidate correction pair . Finally , a ranker maps the feature vector to a real-valued score , indicating the likelihood that this candidate is a desirable correction.
We will demonstrate that ranking provides a flexible modeling framework for incorporating a wide variety of features that would be difficult to model under the noisy channel framework.
Second , we explore the use of Web scale LMs for query spelling correction . While traditional LM research focuses on how to make the model ? smarter ? via how to better estimate the probability of unseen words ( Chen and Goodman , 1999); and how to model the grammatical structure of language ( e.g ., Charniak , 2001), recent studies show that significant improvements can be achieved using ? stupid ? ngram models trained on very large corpora ( e.g ., Brants et al , 2007).
We adopt the latter strategy in this study . We present a distributed infrastructure to efficiently train and apply Web scale LMs . In addition , we observe that search queries are composed in a language style different from that of regular text . We thus train multiple LMs using different texts associated with Web corpora and search queries.
Third , we propose a phrasebased error model that captures the probability of transforming one Compared to traditional error models that account for transformation probabilities between single characters or substrings ( e.g ., Kernighan et al , 1990; Brill and Moore , 2000), the phrasebased error model is more effective in that it captures inter-term dependencies crucial for correcting real-word errors , prevalent in search queries . We also present a novel method of extracting large amounts of query-correction pairs from search logs . These pairs , implicitly judged by millions of users , are used for training the error models.
Experiments show that each of the extensions leads to significant improvements over its baseline methods that were state-of-the-art until this work , and that the combined method yields a system which outperforms the noisy channel speller by a large margin : a 6.3% increase in accuracy on a human-labeled query set.
2 Related Work
Prior research on spelling correction for regular text can be grouped into two categories : correcting nonword errors and real-word errors . The former focuses on the development of error models based on different edit distance functions ( e.g ., Kucich , 1992; Kernighan et al , 1990; Brill and Moore , 2000; Toutanova and Moore , 2002). Brill and Moore?s substring-based error model , considered to be state-of-the-art among these models , acts as the baseline against which we compare our models . On the other hand , real-word spelling correction tries to detect incorrect usages of a valid word based on its context , such as " peace " and " piece " in the context " a _ of cake ". Ngram LMs and na?ve Bayes classifiers are commonly used models ( e.g ., Golding and Roth , 1996; Mangu and Brill , 1997; Church et al , 2007).
While almost all of the spellers mentioned above are based on a predefined dictionary ( either a lexicon against which the edit distance is computed , or a set of real-word confusion pairs ), recent research on query spelling correction focuses on exploiting noisy Web corpora and query logs to infer knowledge about spellings and word usag in queries ( Cucerzan and Brill 2004; Ahmad and Kondrak , 2005; Li et al , 2006; Whitelaw et al ., 2009). Like those spellers designed for regular text , most of these query spelling systems are also based on the noisy channel framework.
3 A Ranker-Based Speller
The noisy channel model of Equation (2) does not have the flexibility to incorporate a wide variety of features useful for spelling correction , e.g ., whether a candidate appears as a Wikipedia document title . We thus generalize the speller to a ranker-based system . Let f be a feature vector of a query and candidate correction pair ( Q , C).
The ranker maps f to a real value y that indicates how likely C is a desired correction . For example , a linear ranker maps f to y with a weight vector w such as , where w is optimized for accuracy on human-labeled pairs . Since the logarithms of the LM and error model probabilities can be included as features , the ranker covers the noisy channel model as a special case.
For efficiency , our speller operates in two distinct stages : candidate generation and reranking.
In candidate generation , an input query is first tokenized into a sequence of terms . For each term q , we consult a lexicon to identify a list of spelling suggestions c whose edit distance from q is lower than some threshold . Our lexicon contains around 430,000 high frequency query unigram and bigrams collected from 1 year of query logs . These suggestions are stored in a lattice.
We then use a decoder to identify the 20-best candidates from the lattice according to Equation (2), where the LM is a backoff bigram model trained on 1 year of query logs , and the error model is approximated by weighted edit distance : (3) The decoder uses a standard two-pass algorithm.
The first pass uses the Viterbi algorithm to find the best C according to the model of Equations (2) and (3). The second pass uses the A-star algorithm to find the 20-best corrections , using the Viterbi scores computed at each state in the first pass as heuristics.
The core component in the second stage is a ranker , which reranks the 20-best candidate corrections using a set of features extracted from . If the top C after reranking is different from Q , C is proposed as the correction . We use 96 features in this study . In addition to the two features derived from the noisy channel model , the rest of the features can be grouped into the following 5 categories.
1. Surface-form similarity features , which check whether C and Q differ in certain patterns , an apostrophe , or by adding a stop word at the beginning or end of Q.
2. Phonetic-form similarity features , which check whether the edit distance between the metaphones ( Philips , 1990) of a query term and its correction candidate is below some thresholds.
3. Entity features , which check whether the original query is likely to be a proper noun based on an inhouse named entity recognizer.
4. Dictionary features , which check whether a query term or a candidate correction are in one or more human-compiled dictionaries , such as the extracted Wiki , MSDN , and ODP dictionaries.
5. Frequency features , which check whether the frequency of a query term or a candidate correction is above certain thresholds in different datasets , such as query logs and Web documents.
4 Web Scale Language Models
An ngram LM assigns a probability to a word string according to ? ( | ) ? ( | ) (4) where the approximation is based on a Markov assumption that each word depends only upon the immediately preceding n1 words . In a speller , the log of ngram LM probabilities of an original query and its candidate corrections are used as features in the ranker.
While recent research reports the benefits of large LMs trained on Web corpora on a variety of applications ( e.g . Zhang et al , 2006; Brants et al , 2007), it is also clear that search queries are composed in a language style different from that of the body or title of a Web document . Thus , in this study we developed a set of large LMs from different text streams of Web documents and query logs . Below , we first describe the ngram LM collection used in this study , and then present a distributed ngram LM platform based on which these LMs are built and served for the speller.
4.1 Web Scale Language Models
Table 1 summarizes the data sets and Web scale ngram LMs used in this study . The collection is built from high quality English Web documents containing trillions of tokens , served by a popular commercial search engine . The collection consists of several data sets built from different Web sources , including the different text fields from the Web documents ( i.e ., body , title , and anchor texts ) and search query logs . The raw texts extracted from these different sources were preprocessed in the following manner : texts are tokenized based on whitespace and upper case letters are converted to lower case . Numbers are retained , and no stemming/inflection is performed . The ngram LMs are word-based backoff models , where the ngram probabilities are estimated using Maximum Likelihood Estimation with smoothing . Specifically , for a trigram model , the smoothed probability is computed as (5) { ( ) where is the count of the ngram in the training corpus and is a normalization factor.
is a discount function for smoothing . We use modified absolute discounting ( Gao et al , 2001), whose parameters can be efficiently estimated and performance converges to that of more elaborate state-of-the-art techniques like Kneser-Ney smoothing in large data ( Nguyen et al 2007).
4.2 Distributed Ngram LM Platform
The platform is developed on a distributed computing system designed for storing and analyzing massive data sets , running on large clusters consisting of hundreds of commodity servers connected via high-bandwidth network.
We use the SCOPE ( Structured Computations Optimized for Parallel Execution ) programming model ( Chaiken et al , 2008) to train the Web scale ngram LMs shown in Table 1. The SCOPE scripting language resembles SQL which many programmers are familiar with . It also supports
Dataset Body Anchor Title Query
Total tokens 1.3T 11.0B 257.2B 28.1B
Unigrams 1.2B 60.3M 150M 251.5M
Bigrams 11.7B 464.1M 1.1B 1.3B
Trigrams 60.0B 1.4B 3.1B 3.1B 4grams 148.5B 2.3B 5.1B 4.6B Size on disk # 12.8TB 183GB 395GB 393GB # Ngram entries as well as other model parameters are stored.
Table 1: Statistics of the Web ngram LMs collection ( count cutoff = 0 for all models ). These models will be accessible at
Microsoft (2010).
360
C # expressions so that users can easily plugin customized C # classes . SCOPE supports writing a program using a series of simple data transformations so that users can simply write a script to process data in a serial manner without wondering how to achieve parallelism while the SCOPE compiler and optimizer are responsible for translating the script into an efficient , parallel execution plan . We illustrate the usage of SCOPE for building LMs using the following example of counting 5grams from the body text of English Web pages . The flowchart is shown in Figure 1.
The program is written in SCOPE as a step-by - step of computation , where a command takes the output of the previous command as its input.
ParsedDoc=SELECT docId , TokenizedDoc
FROM @?/ shares/?/EN_Body.txt?
USING DefaultTextExtractor;
NGram=PROCESS ParsedDoc
PRODUCE NGram , NGcount
USING NGramCountProcessor(-stream
TokenizedDoc - order 5 ? bufferSize 20000000);
NGramCount=REDUCE NGram
ON NGram
PRODUCE NGram , NGcount
USING NGramCountReducer;
OUTPUT TO @? Body-5-gram-count.txt?;
The first SCOPE command is a SELECT statement that extracts parsed Wed body text . The second command uses a build-in Processor ( NGramCountProcessor ) to map the parsed documents into separate ngrams together with their counts . It generates a local hash at each node ( i.e ., a core in a multicore server ) to store the ( ngram , count ) pairs . The third command ( REDUCE ) aggregates counts from different nodes according to the key ( ngram string ). The final command ( OUTPUT ) writes out the resulting to a data file.
The smoothing method can be implemented similarly by the customized smoothing Proces-sor/Reducer . They can be imported from the existing C # codes ( e.g ., developed for building LMs in a single machine ) with minor changes.
It is straightforward to apply the built LMs for the ranker in the speller . The ngram platform provides a DLL for ngram batch lookup . In the server , an ngram LM is stored in the form of multiple lists of key-value pairs , where the key is the hash of an ngram string and the value is either the ngram probability or backoff parameter.
5 Phrase-Based Error Models
The goal of an error model is to transform a correctly spelled query C into a misspelled query Q.
Rather than replacing single words in isolation , the phrasebased error model replaces sequences of words with sequences of words , thus incorporating contextual information . The training procedure closely follows Sun et al (2010). For instance , we might learn that ? theme part ? can be replaced by ? theme park ? with relatively high probability , even though ? part ? is not a misspelled word . We use this generative story : first the correctly spelled query C is broken into K nonempty word sequences c1, ?, ck , then each is replaced with a new nonempty word sequence q1, ?, qk , finally these phrases are permuted and concatenated to form the misspelled Q . Here , c and q denote consecutive sequences of words.
To formalize this generative process , let S denote the segmentation of C into K phrases c1?cK , and let T denote the K replacement phrases q1?qK ? we refer to these ( ci , qi ) pairs as biphrases . Finally , let M denote a permutation of K elements representing the final reordering step.
Figure 2 demonstrates the generative procedure.
Next let us place a probability distribution over rewrite pairs . Let B(C , Q ) denote the set of S , T , M triples that transform C into Q . Assuming a uniform probability over segmentations , the phrasebased probability can be defined as:
Recursive
Reducer
Node 1 Node 2 Node N?...
?...
Output
Web Pages
Parsing
Counting
Local
Hash
Tokenize
Web Pages
Parsing
Counting
Local
Hash
Tokenize
Web Pages
Parsing
Counting
Local
Hash
Tokenize
Figure 1. Distributed 5gram counting.
C : ? disney theme park ? correct query
S : [? disney ?, ? theme park ?] segmentation T : [? disnee ?, ? theme part ?] translation
M : (1 ? 2, 2? 1) permutation
Q : ? theme part disnee ? misspelled query
Figure 2: Example demonstrating the generative procedure behind the phrasebased error model.
361 ? (6)
As is common practice in SMT , we use the maximum approximation to the sum : (7) 5.1 Forced Alignments Although we have defined a generative model for transforming queries , our goal is not to propose new queries , but rather to provide scores over existing Q and C pairs that will act as features for the ranker . Furthermore , the word-level alignments between Q and C can most often be identified with little ambiguity . Thus we restrict our attention to those phrase transformations consistent with a good word-level alignment.
Let J be the length of Q , L be the length of C , and A = a1?aJ be a hidden variable representing the word alignment between them . Each ai takes on a value ranging from 1 to L indicating its corresponding word position in C , or 0 if the ith word in Q is unaligned . The cost of assigning k to ai is equal to the Levenshtein edit distance ( Levenshtein , 1966) between the ith word in Q and the kth word in C , and the cost of assigning 0 to ai is equal to the length of the i th word in Q.
The least cost alignment A * between Q and C is computed efficiently using the A-star algorithm.
When scoring a given candidate pair , we further restrict our attention to those S , T , M triples that are consistent with the word alignment , which we denote as B(C , Q , A *). Here , consistency requires that if two words are aligned in A *, then they must appear in the same biphrase ( ci , qi ). Once the word alignment is fixed , the final permutation is uniquely determined , so we can safely discard that factor . Thus we have : (8) For the sole remaining factor P(T|C , S ), we make the assumption that a segmented query T = q1? qK is generated from left to right by transforming each phrase c1?cK independently : ? , (9) where is a phrase transformation probability , the estimation of which will be described in Section 5.2.
To find the maximum probability assignment efficiently , we use a dynamic programming approach , similar to the monotone decoding algorithm described in Och (2002).
5.2 Training the Error Model
Given a set of ( Q , C ) pairs as training data , we follow a method commonly used in SMT ( Och and Ney , 2004) to extract bi - phrases and estimate their replacement probabilities . A detailed description is discussed in Sun et al (2010).
We now describe how ( Q , C ) pairs are generated automatically from massive query reformulation sessions of a commercial Web browser.
A query reformulation session contains a list of URLs that record user behaviors that relate to the query reformulation functions , provided by a Web search engine . For example , most commercial search engines offer the " did you mean " function , suggesting a possible alternate interpretation or spelling of a user-issued query . Figure 3 shows a sample of the query reformulation sessions that record the " did you mean " sessions from three of the most popular search engines.
These sessions encode the same user behavior : A user first queries for " harrypotter sheme part",
Google : http://www.google.com/search ? hl=en&source=hp & q=harrypotter+sheme+part&aq=f&oq=&aqi = http://www.google.com/search ? hl=en&ei=rnNAS8-oKsWe_AaB2eHlCA & sa=X&oi=spell&resnum=0&ct = result&cd=1&ved=0CA4QBSgA & q=harry+potter+theme+park&spell=1
Yahoo : http://search.yahoo.com/search ; _ylt=A0geu6ywckBL_XIBSDtXNyoA ? p=harrypotter+sheme+part & fr2=sb-top&fr=yfp-t-701&sao=1 http://search.yahoo.com/search ? ei=UTF-8&fr=yfp-t-701& p=harry+potter+theme+park & SpellState=n-2672070758_q-tsI55N6srhZa.
qORA0MuawAAAA%40%40&fr2=sp-top
Bing : http://www.bing.com/search ? q=harrypotter+sheme+part&form=QBRE&qs=n http://www.bing.com/search ? q=harry+potter+theme+park&FORM=SSRE Figure 3. A sample of query reformulation sessions from 3 popular search engines . These sessions show that a user first issues the query " harrypotter sheme part ", and then clicks on the resulting spell suggestion " harry potter theme park".
362 and then clicks on the resulting spelling suggestion " harry potter theme park ". We can " reverse-engineer " the parameters from the URLs of these sessions , and deduce how each search engine encodes both a query and the fact that a user arrived at a URL by clicking on the spelling suggestion of the query ? an strong indication that the spelling suggestion is desired . In this study , from 1 year of sessions , we extracted ~120 million pairs . We found the data set very clean because these spelling corrections are actually clicked , and thus judged implicitly , by many users.
In addition to the " did you mean " functionality , recently some search engines have introduced two new spelling suggestion functions . One is the " autocorrection " function , where the search engine is confident enough to automatically apply the spelling correction to the query and execute it to produce search results . The other is the " split pane " result page , where one half portion of the search results are produced using the original query , while the other half , usually visually separate portion of results , are produced using the autocorrected query.
In neither of these functions does the user ever receive an opportunity to approve or disapprove of the correction . Since our extraction approach focuses on user-approved spelling suggestions , we ignore the query reformulation sessions recording either of the two functions . Although by doing so we could miss some basic , obvious spelling corrections , our experiments show that the negative impact on error model training is negligible . One possible reason is that our baseline system , which does not use any error model learned from the session data , is already able to correct these basic , obvious spelling mistakes.
Thus , including these data for training is unlikely to bring any further improvement.
We found that the error models trained using the data directly extracted from the query reformulation sessions suffer from the problem of underestimating the self-transformation probability of a query P(Q2=Q1|Q1), because we only included in the training data the pairs where the query is different from the correction . To deal with this problem , we augmented the training data by including correctly spelled queries , i.e ., the pairs ( Q1, Q2) where Q1 = Q2. First , we extracted a set of queries from the sessions where no spell suggestion is presented or clicked on . Second , we removed from the set those queries that were recognized as being autocorrected by a search engine . We do so by running a sanity check of the queries against our baseline noisy channel speller , which will be described in Section 6. If the system consider a query misspelled , we assumed it an obvious misspelling , and removed it.
The remaining queries were assumed to be correctly spelled and were added to the training data.
6 Experiments
We perform the evaluation using a manually annotated data set containing 24,172 queries sampled from one year?s query logs from a commercial search engine . The spelling of each query is manually corrected by four independent annotators . The average length of queries in the data sets is 2.7 words . We divided the data set into non-overlapped training and test data sets . The training data contain 8,515 pairs , among which 1,743 queries are misspelled ( i.e . ).
The test data contain 15,657 pairs , among which 2,960 queries are misspelled.
The speller systems we developed in this study are evaluated using the following metrics.
? Accuracy : The number of correct outputs generated by the system divided by the total number of queries in the test set.
? Precision : The number of correct spelling corrections for misspelled queries generated by the system divided by the total number of corrections generated by the system.
? Recall : The number of correct spelling corrections for misspelled queries generated by the system divided by the total number of misspelled queries in the test set.
We also perform a significance test , a ttest with a significance level of 0.05.
In our experiments , all the speller systems are ranker-based . Unless otherwise stated , the ranker is a two-layer neural net with 5 hidden nodes.
The free parameters of the neural net are trained to optimize accuracy on the training data using the back propagation algorithm ( Burges et al , 2005) .
6.1 System Results
Table 1 summarizes the main results of different spelling systems . Row 1 is the baseline speller where the noisy channel model of Equations (2) weighted edit distance function and the LM is a backoff bigram model trained on 1 year of query logs , with count cutoff 30. Row 2 is the speller using a linear ranker to incorporate all ranking features described in Section 3. The weights of the linear ranker are optimized using the Averaged Perceptron algorithm ( Freund and Schapire , 1999). Row 3 is the speller where a nonlinear ranker ( i.e ., 2-layer neural net ) is trained atop the features . Rows 4, 5 and 6 are systems that incorporate the additional features derived from the phrasebased error model ( PBEM ) described in Section 5 and the four Web scale LMs ( WLMs ) listed in Table 1.
The results show that (1) the ranker is a very flexible modeling framework where a variety of finegrained features can be easily incorporated , and a ranker-based speller outperforms significantly ( p < 0.01) the traditional system based on the noisy channel model ( Row 2 vs . Row 1); (2) the speller accuracy can be further improved by using more sophisticated rankers and learning algorithms ( Row 3 vs . Row 2); (3) both WLMs and PBEM bring significant improvements ( Rows 4 and 5 vs . Row 3); and (4) interestingly , the gains from WLMs and PBEM are additive and the combined leads to a significantly better speller ( Row 6 vs . Rows 4 and 5) than that of using either of them individually.
In what follows , we investigate in detail how the WLMs and PBEM trained on massive Web content and search logs improve the accuracy of the speller system . We will compare our models with the state-of-the-art models proposed previously . From now on , the system listed in Row 3 of Table 1 will be used as baseline.
6.2 Language Models
The quality of ngram LMs depends on the order of the model , the size of the training data , and how well the training data match the test data.
Figure 4 illustrates the perplexity results of the four LMs trained on different data sources tested on a random sample of 733,147 queries . The results show that (1) higher order LMs produce lower perplexities , especially when moving beyond unigram models ; (2) as expected , the query LMs are most predictive for the test queries , though they are from independent query log snapshots ; (3) although the body LMs are trained on much larger amounts of data than the title and anchor LMs , the former lead to much higher perplexity values , indicating that both title and anchor texts are quantitatively much more similar to queries than body texts.
Table 2 summarizes the spelling results using different LMs . For comparison , we also built a 4gram LM using the Google 1T web 5gram corpus ( Brants and Franz , 2006). This model is referred to as the G1T model , and is trained using the ? stupid backoff ? smoothing method ( Brants et al ., 2007). Due to the high count cutoff applied by the Google corpus ( i.e ., ngrams must appear at least 40 times to be included in the corpus ), we found the G1T model results to a higher OOV rate ( i.e ., 6.5%) on our test data than that of the 4
Web scale LMs ( i.e ., less than 1%).
The results in Table 2 are more or less consistent with the perplexity results : the query LM is the best performer ; there is no significant difference among the body , title and anchor LMs though the body LM is trained on a much larger amount of data ; and all the 4 Web scale LMs outperform the G1T model substantially due to the significantly lower OOV rates.
6.3 Error Models
This section compares the phrasebased error model ( PBEM ) described in Section 5, with one of the state-of-the-art error models , proposed by Brill and Moore (2000), henceforth referred to as # System Accuracy Precision Recall 1 Noisy channel 85.3 72.1 35.9 2 Linear ranker 88.0 74.0 42.8 3 Nonlinear ranker 89.0 74.1 49.6 4 3 + PBEM 90.7 78.7 58.2 5 3 + WLMs 90.4 75.1 58.7 6 3 + PBEM + WLMs 91.6 79.1 63.9 Table 1. Summary of spelling correction results.

Figure 4. Perplexity results on test queries , using ngram LMs with different orders , derived from different data sources.
el . It estimates as ? (10) where R is a partitioning of correction term c into adjacent substrings , and T is a partitioning of query term q , such that | T|=|R |. The partitions are thus in one-to-one alignment . To train the B&M model , we extracted 1 billion term-correction pairs from the set of 120 million query-correction pairs , derived from the search logs as described in Section 5.2.
Table 3 summarizes the comparison results.
Rows 1 and 2 are our ranker-based baseline systems with and without the error model ( EM ) feature . The error model is based on weighted edit distance of Eq . (3), where the weights are learned on some manually annotated word-correction pairs ( which is not used in this study ). Rows 3 and 4 are the B&M models using different maximum substring lengths , specified by L . L=1 reduces B&M to the weighted edit distance model in Row 2. Rows 5 and 6 are PBEMs with different maximum phrase lengths . L=1 reduces PBEM to a word-based error model . The results show the benefits of capturing context information in error models . In particular , the significant improvements resulting from PBEM demonstrate that the dependencies between words are far more effective than that between characters ( within a word ) for spelling correction . This is largely due to the fact that there are many real-word spelling errors in search queries . We also notice that PBEM is a more powerful model than # # of word pairs Accuracy Precision Recall 1 Baseline w/o EM 88.55 71.95 46.97 2 1M 89.15 73.71 50.74 3 10M 89.22 74.11 50.92 4 100M 89.20 73.60 51.06 5 1B 89.21 73.72 50.99 Table 4. The performance of B&M error model ( L=3) as a function of the size of training data (# of word pairs).
# # of ( Q , C ) pairs Accuracy Precision Recall 1 Baseline w/o EM 88.55 71.95 46.97 2 5M 89.59 77.01 52.34 3 15M 90.23 77.87 56.67 4 45M 90.45 78.56 57.02 5 120M 90.70 78.49 58.12 Table 5. The performance of PBEM ( L=3) as a function of the size of training data (# of ( Q , C ) pairs).
B&M in that it can benefit more from increasingly larger training data . As shown in Tables 4 and 5, whilst the performance of B&M saturates quickly with the increase of training data , the performance of PBEM does not appear to have peaked ? further improvements are likely given a larger data set.
7 Conclusions and Future Work
This paper explores the use of massive Web corpora and search logs for improving a ranker-based search query speller . We show significant improvements over a noisy channel speller using finegrained features , Web scale LMs , and a phrasebased error model that captures intern-word dependencies . There are several techniques we are exploring to make further improvements.
First , since a query speller is developed for improving the Web search results , it is natural to use features from search results in ranking , as studied in Chen et al (2007). The challenge is efficiency.
Second , in addition to query reformulation sessions , we are exploring other search logs from which we might extract more pairs for error model training . One promising data source is clickthrough data ( e.g ., Agichtein et al 2006; Gao et al , 2009). For instance , we might try to learn a transformation from the title or anchor text of a document to the query that led to a click on that document . Finally , the phrasebased error model is inspired by phrasebased SMT systems.
We are introducing more SMT techniques such as alignment and translation rule exaction . In a broad sense , spelling correction can be viewed as a monolingual MT problem where we translate bad English queries into good ones.
# System Accuracy Precision Recall 1 Baseline 89.0 74.1 49.6 2 1+ query 4gram 90.1 75.6 56.3 3 1 + body 4gram 89.9 75.7 54.4 4 1 + title 4gram 89.8 75.4 54.7 5 1 + anchor 4gram 89.9 75.1 55.6 6 1 + G1T 4gram 89.4 75.1 51.5 Table 2. Spelling correction results using different LMs trained on different data sources.
# System Accuracy Precision Recall 1 Baseline w/o EM 88.6 72.0 47.0 2 Baseline 89.0 74.1 49.6 3 1 + B&M , L=1 89.0 73.3 50.1 4 1 + B&M , L=3 89.2 73.7 51.0 5 1 + PBEM , L=1 90.1 76.7 55.6 6 1 + PBEM , L=3 90.7 78.5 58.1 Table 3. Spelling correction results using different error models.
365
Acknowledgments
The authors would like to thank Andreas Bode , Mei Li , Chenyu Yan and Kuansan Wang for the very helpful discussions and collaboration . The work was done when Xu Sun was visiting Microsoft Research Redmond.
References
Agichtein , E ., Brill , E . and Dumais , S . 2006. Improving web search ranking by incorporating user behavior information . In SIGIR , pp . 1926.
Ahmad , F ., and Kondrak , G . 2005. Learning a spelling error model from search query logs . In HLT-
EMNLP , pp . 955-962.
Brants , T ., and Franz , A . 2006. Web 1T 5gram corpus version 1.1. Technical report , Google Research.
Brants , T ., Popat , A . C ., Xu , P ., Och , F . J ., and Dean , J.
2007. Large language models in machine translation.
In EMNLPCoNLL , pp . 858 - 867.
Brill , E ., and Moore , R . C . 2000. An improved error model for noisy channel spelling correction . In ACL , pp . 286-293.
Burges , C ., Shaked , T ., Renshaw , E ., Lazier , A ., Deeds , M ., Hamilton , and Hullender , G . 2005.
Learning to rank using gradient descent . In ICML , pp . 8996.
Chaiken , R ., Jenkins , B ., Larson , P ., Ramsey , B ., Shakib , D ., Weaver , S ., and Zhou , J . 2008. SCOPE : easy and efficient parallel processing f massive data sets . In Proceedings of the VLDB Endowment , pp.
1265-1276.
Charniak , E . 2001. Immediate-head parsing for language models . In ACL/EACL , pp . 124-131.
Chen , S . F ., and Goodman , J . 1999. An empirical study of smoothing techniques for language modeling . Computer Speech and Language , 13(10):359-394.
Chen , Q ., Li , M ., and Zhou , M . 2007. Improving query spelling correction using web search results . In
EMNLPCoNLL , pp . 181-189.
Church , K ., Hard , T ., and Gao , J . 2007. Compressing trigram language models with Golomb coding . In
EMNLPCoNLL , pp . 199-207.
Cucerzan , S ., and Brill , E . 2004. Spelling correction as an iterative process that exploits the collective knowledge of web users . In EMNLP , pp . 293-300.
Freund , Y . and Schapire , R . E . 1999. Large margin classification using the perceptron algorithm . In
Machine Learning , 37(3): 277-296.
Gao , J ., Goodman , J ., and Miao , J . 2001. The use of clustering techniques for language modeling - application to Asian languages . Computational Linguistics and Chinese Language Processing , 6(1):27?60, 2001.
Gao , J ., Yuan , W ., Li , X ., Deng , K ., and Nie , JY.
2009. Smoothing clickthrough data for web search ranking . In SIGIR , pp . 355-362.
Golding , A . R ., and Roth , D . 1996. Applying winnow to context-sensitive spelling correction . In ICML , pp.
182-190.
Joachims , T . 2002. Optimizing search engines using clickthrough data . In SIGKDD , pp . 133-142.
Kernighan , M . D ., Church , K . W ., and Gale , W . A.
1990. A spelling correction program based on a noisy channel model . In COLING , pp . 205-210.
Koehn , P ., Och , F ., and Marcu , D . 2003. Statistical phrasebased translation . In HLT/NAACL , pp . 127-133.
Kucich , K . 1992. Techniques for automatically correcting words in text . ACM Computing Surveys , 24(4):377-439.
Levenshtein , V . I . 1966. Binary codes capable of correcting deletions , insertions and reversals . Soviet
Physics Doklady , 10(8):707-710.
Li , M ., Zhu , M ., Zhang , Y ., and Zhou , M . 2006. Exploring distributional similarity based models for query spelling correction . In ACL , pp . 10251032.
Mangu , L ., and Brill , E . 1997. Automatic rule acquisition for spelling correction . In ICML , pp . 187-194.
Microsoft Microsoft web ngram services . 2010.
http://research.microsoft.com/web-ngram
Nguyen , P ., Gao , J ., and Mahajan , M . 2007. MSRLM : a scalable language modeling toolkit . Technical report TR-2007-144, Microsoft Research.
Och , F . 2002. Statistical machine translation : from single-word models to alignment templates . PhD thesis , RWTH Aachen.
Och , F ., and Ney , H . 2004. The alignment template approach to statistical machine translation.
Computational Linguistics , 30(4): 417-449.
Philips , L . 1990. Hanging on the metaphone . Computer Language Magazine , 7(12):38-44.
Sun , X ., Gao , J ., Micol , D ., and Quirk , C . 2010.
Learning phrasebased spelling error models from clickthrough data . In ACL.
Toutanova , K ., and Moore , R . 2002. Pronunciation modeling for improved spelling correction . In ACL , pp . 144-151.
Whitelaw , C ., Hutchinson , B ., Chung , G . Y ., and Ellis , G . 2009. Using the web for language independent spellchecking and autocorrection . In EMNLP , pp.
890-899.
Zhang , Y ., Hildebrand , Al . S ., and Vogel , S . 2006.
Distributed language modeling for nbest list reranking . In EMNLP , pp . 216-233.
366
