Using Discourse Predictions for Ambiguity Resolution 
Yan Qu , Carolyn P . Ros 6 and Barbara Di Eugenio
Computational Linguistics Program
Department of Philosophy
Carnegie Mellon University
Pittsburgh , PA 15213
yqu , cprose (6) cs.cmu.edu , dieugeni ~ cmu.cdu
Abstract
In this paper we discuss how we all-
ply discourse predictions along with non
context-based predictions to the prob-
lem of parse disambiguation i Enthusi-
ast , a Spanish-to-English translation sys-
tem(Woszcyna et al , 1993; Snhmetal,
1994; Levinel ; al ., 1995). We discuss
extensions to our plan-based discourse
processor in order to make this possi-
ble . We evaluate those extensions and
demonstrate the advantage of exploiting context -based predictions over a purely noncontext-based approach  . 
1 Introduction
A system which processes poken language must address all of the ambiguities arising when processing written language  , plus other ambiguities specitie to the speech processing task  . These include ambiguities derived from speech disfluencies  , speech recognition errors , and the lack of clearly marked sentence boundaries  . Because a large flexible grammar is necessary to handle these features of spoken language  , as a side-effect the number of ambiguities increases  . In this paper , we discuss how we apply discourse predictions along with noncontext-based predictions to the problem of parse disambiguation  . This work has been carried out in the context of Enthusi-ast  , a Spanish-to-English speech-to-speech translation system  ( Woszcyna et al , 1993; Suhm et al , 1994; Levin et al ,  1995) , which currently translates spontaneous dialogues between two people trying to schedule a meeting time  . 
A key feature of our approach is that it allows multiple hypotheses to be processed through the system in parallel  , and uses context to disambiguate among alternatives in the linal stage of the process  , where knowledge can be exploited to the fullest extent  . In our system , numerical predictions based on the more local utterance level are generated by tile parser  . The larger discourse context is processed and maintained by a plan-based discourse processor  , which also produces context-based predictions for ambiguities  . Our goal was to combine the predictions from the context-based discourse processing approach with those from the noncontext-based parser approach  . 
In developing our discourse processor for disambiguation we needed to address three major issues  . 
First , most plan-based or finite state automaton based discourse processors  ( Allen and Schubert , 1991; Smith , Hipp , and Biermann , 1995; Lambert , 1993; Reithinger and Maim: ,  1995) , including tile one we initially developed ( l ~ . osd et al ,  1995) , only take one semantic representation as input at a time : thus  , we had to extend the discourse processor so thai ; it can handle multiple hypotheses as input . Secondly , we needed to quantify the disambiguating predictions made by the plan-base dis-course processor in order to combine these predictions with the noncontext-based ones  . Finally , we needed a method for combining context-based and noncontext-based predictions in such a way as to reflect not only which factors are important  , but also to what extent they are important , and under what circumstances . We assume that knowledge from different sources provides different perspectives on the disambiguation task  , each specializing in different ypes of ambiguities  . 
In this paper , we concentrate on the first two issues which are imperative to integrate a traditional plan-based is course processor into the disambiguation module of a whole system  . The third issue is very important for successful confl  ) ination of predictions from different knowledge sources  . 
We address this issue elsewhere in ( Rosd and Qu ,  1995) . 
The paper is organized as follows : Fh'st , we briefly introduce the Enthusiast speech translation system and discuss the ambiguity problem in Enthusiast  . Then we discuss our discourse processor , focusing on those characteristics needed to generate predictions lbr disambiguation  . Finally , we evaluate our performance , and demonstrate that tile use of discourse context improves performance on disambiguation tasks over a purely noncontext-based approach in the absence of cumulative error  . 
358 2 System Description
The main modules of our system include speech recognition  , parsing , discourse processing , and generation . Processing begins with tim speech input in the source language  . The top best hypothesis of the speaker's utterance is then passed to L he parser  . The GLR * parser ( Lavie , 1995) produces a set of interlinguate xts , or ILFs , for a given sentence . For robustness , the . (- ILI , * parser can skil ) words in the inpu / , sentence in order to find a partial parse for a sentence which otherwise would not be parsable  . An 11: I' is a frame-based language , independent meaning reprcsen ration of a sentence  . The main components of an 11:1' are the sl ) eechact ( e . g . , suggest , accept , reject ) , the sentence type ( e . g . , state , query-J . g , fragraent ), and the main semantic frame ( e . g . , Iree , busy) . 
An example of an IUI ' is shown in Figure 1 . The parser may produce many Ilfl's for a single sen-tence j sometimes as many as one hundred or lnore  . 
( ( when ( ( fi'ame * simple-time )   ( day-of-week wednesday )   ( t i n , e-or-day , noruing ) ) )   ( a-speech-act ( * multiple ** suggest * accept ) )  ( who ( ( frame * i ) ) )   ( frame * free )   ( sentence-type * state ) ) ) Sentence : 1 couhld oit ; Wednesday niol'ning too . 
Figure 1: An Example ILT '\[' he resulting set of llTs is then sent to the discourse processor  . The discourse l ) rocessor , based on I , ambert's work (\[ , ambert and Carberry , 1992; I , ambert ,  1993) , disaml ) iguates tiles l ) ee chact of e ~( ; h sentence ~ normalizes temporal expressions ( ? o in context , and incorl ) or at es the selt t , enee into tile discourse context represented by a plan tree  . 
The discourse l ) roees sor also updal ; es a calendar which keeps track of what the speakers h ~ we said M  ) out their schedules . We will discuss the discourse i > rocessor and how we extended it for the disambiguation task in Sectiou  4  . 
3 Ambiguity it , Enthusias (;
Because t ; hespontalleous sche(\[lding dialogues3 , reun restricted , ambiguity is a major problenl in En-thusiast . We gange ambiguities in terms of differences between members of the set of ILTs produced by the parse  , r for the sail ~ e source sentence . 
As we mentione de , arlier , the disaan biguation task benelits from both non ( -ontexL - and context-l ) ~sed methods . We observed that some classes of ambiguities can be more l  ) erspieuously dealt within one way or the other . 
3 . 1 Non Context - Based D isambiguat ion When the parser produces more than one IlJl ' for a single sentence  , it scores these ambiguities according to three diti'e  . rent noncontext-base dis-aml ) iguation in ethods . The first method , based on ( Carroll and Briscoc ,  1993) , assigns probal ) il-ities to actions in the (~ I , R , * l ) arser's 1) arse table . 
The probabilities of the parse actions inducest , a-tistical scores on alternative parse trees , which are then used for parse disambiguation . The re-suiting score is called the slalislical score  . The second method the parser uses to score the II /l's makes use of penalties mammlly assigned to different rules in the l  ) arsing grammar , rl ' he resulting score from this method is called the gr'am-marpr'cfercucc score  . The third score , called the parser score , is a heuristic combination of the previous two scores ldUS other information such as the number of words skil  ) ped . These threellOll context-based scores will be referred to later when we discuss comt  ) in ing none on text-I ) as edl ) redic-t , ions with context-based ones . 
Error analysis of parser disambiguation output shows that the C  , IA * parser handles well ambiguities which are not strongly dependent upon the context for a reasonable interpretation  , laBrex--ample , the Sl ) anish word uua can mean either ou cora , as an indefinite reference . The parser always chooses the indelinite reference meaning since the vast  , majority of training examples use this sense of the word  . Moreover , since in this case incorrect disambiguation does not adversely affect translation quality  , it ; ramies sense to handle this ambiguity in a purely noncontext-based manner  . 
3.2 Context-Based Disambiguation
While a broad range of ambiguities can I ) ehal > died well in ~ noncontext-basel\] manner  , some ambiguities must be treated in a contexl , s e , nsitive manner in order to be translated correctly  . 
Table 1 lists some examples of these tyt ) es of at n--biguities . Each type of ambiguity is categorized by CO ml ) aring either difl'erent slots in alternativell ; l's or dilt'eren L values in ambiguous II2F slol . s given \[; he same input utteran ( ; e . 
For example , one . typeo1"ambiguity l ) es that > dh ' d with ~ context d ) ase ( I approact l is the day vs hour ~ md ) iguity , exen q ) lified by timphrase do sacua&v . It can mean either Ihc second alJ'o'a % lhc second loth eJbur lhorlwog of our  . Out of conte . x . , it is iml ) ossil ) le to tell which is the I ) cst intert ) retation . ( ~ on textua . linlk ) rmation makes il ; possible to choose the correct interpre Lal , ion . I ? or (; xaml)le , if l , h ( : sl ) eakers are trying to estal ) lish a dab : when they can meet , , then the sccoud to the Jourlh is t ; hcmost liD ~ lyit d ; erl ) retatiot J . Itowcver , dayvs hour a temporal expression can be recogn ized as a  ( layor all hour state vs qaery-lfambigui ty between sentence type state or query-if speaker reference ambiguity between pro-drop pronouns tense ambiguity between past tense and present tense how vs greet ambiguity between f ramehow and greet when vs where ambiguity between when slot and where slot 
Exaln ples do sacuatro second at four or second to fourth or two to  , fourest ~ bien
It's OKor\[sitOK?tambidnpodr\[aese d\[a also i could that day or also you could that day  d6nde no sencontramos where are we meeting or where were we meet in qqu ~ tal 
How are you ? or
How is that ? s ? badoquince
Saturday the fifteenth or
Saturday building 15
Table 1: Examples of Context-Sensitive Ambiguities if the speakers have already chosen a date and are negotiating the exact time of the meeting  , then only the meaning two to four makes sense . 
Some sentence type ambiguities are also context -based  . For example , l'S std bien can be either the statement It is good or the question Is it good ?  . This is an example of what we call the state vs query-i : fambiguity : in Spanish  , it is impossible to tell out of context , and without information about intonation , whether a sentence is a statement or a yes/no question  . However , if the same speaker has just made a suggestion , then it is more likely that the speaker is requesting a response from the other speaker by posing a question  . ht contrast , if the previous speaker has just made a suggestion  , then it is more likely that the current speaker is responding with an accepting statement than posing a question  . 
In gener M , we base our context-based predictions for disambiguation on turntaking information  , the stage of negotiation , and the speakers ' cMendar information . This information is encoded in a set of context -based scores produced by the discourse processor for each ILT  . 
4 Discourse Processing and

Context-based ranking of ambiguities is performed by the plan-base discourse processor described in  ( Rosdeta L . , 1995) which is based on ( Lambert and Carberry , 1992; Lambert ,  1993) . 
Origin Mly , our discourse processor took as its input the single best parse returned by the parser  . 
q'he main task of the discourse processor was to relate that representation to the context  , i . e . , to the plan tree . In genera L , plan inference starts from the surface \[ brms of sentences  . Then speech-acts are inferred . Multiple speech-acts can be inferred for one ILT . A separate inference chain is created for each potential speech act performed by the associated ILT  . Preferences for picking one inference chain over another were determined by the focusing heuristics  , which provide ordered expectations of discourse actions given the existing plan tree  . Our focusing heuristics , described in detail in ( los 6 et al ,  1995) , arc an extension of those described in ( Lambert ,  1993) . In determining how the inference chain attaches to the plan tree  , the speech act is recognized , since each inference chain is associated with a single speech act  . 
As mentioned in the introduction , for a plan-based disconrse processor to deal with ambiguities  , three issues need to be addressed : 1 . The discourse processor must be able to deal with more than one semantic representation as input at a time  . Note that simply extending the discourse processor to accept multiple ILTs is not the whole solution to the disambiguation problem : finer distinctions must be made in terms of coherence with the context in order to produce predictions detailed enough to distinguish between alternative LLTs  . 
2 . Before context-based predictions can be combined with quantitative noncontext-based predictions  , they must be quantified , it was necessary to add a mechanism to produce more detailed quantifiable predictions than those produced by the original focusing heuristics described in  ( Ros 6 et al ,  1995) . 
3 . Finally , context-based predictions must be combined successfully with non-context-based ones  . The discourse processor must be able to weigh these various predictions in ofder to determine which ones to believe in specific circumstances  . 
Thus , we extended our original discourse processor as follows  . It takes multiple ambiguous lI , Tsfi'om the parser and computes three quantified discourse scores for each ambiguity  . The discourse scores are derived by taking into accotmt reflected by two kinds of focusing scores  , and , he score returned by the . qradcd conslrain ls , a new type of constraint we introduced . Then for each ambiguity the discourse processor combines these three kinds of context-based scores with the noncontext-based scores l  ) roduced by other modules of the system to make tire final choice  , and returns the chosen IUI' . As in the first version of the discourse processor  , the chosen II , T is attached to the plan tree and a speech act is assigned to it  . We discuss now how the discourse scores are derived  . 
Note that lower w dues for all scores are preferred . 
4.1 Focusing scores
The focusing scores are derived from focusing heuristics based Ott  ( Sidner , 198 l ; l , ambert , 199:f ; Rosd et al ,  1995) . The focusing heuristics identify the most coherent relationship between a new inference chain and the discourse  ) In ntree . Atl , achmeat preferences by the Focusing heuristics are translated into numerical preference scores based on attachment positions and the length of the in--ference chains  . The assignment of focusing scores reflects the assumption thai  , then tost coherent move in a di Mogue is to continue the most salient focused actions  , namely , the ones on the right f l , ost frontier of the plan tree . The first feet ( sing score is a boolean focusing fla ( l . It returns 0 if the inference chain for the associated 11  , '1' attachest , otheright most fl ' outier of the plan tree ,   1 if it either attaches to the tree buttrot to t i t  (  . ', right frontier or doesn't attach to the tree . The second focusing score , the J'o cusing score i ) roper , assigns a score between 0 and t indicating \[ tow far up the rightmost frontier the inference chain attaches  . The maximal score is assigned in the case that the inference chain does not attach  . 
4.2 Graded constraints
Once the . discourse processor was extended to accept multiple ILTs as input  , it became clear that Ibr most ambignous parses the original focusing heuristics did not provide enough information to distinguish among the alternatives  . Our so httion was to modity the discourse processor's constraint processing mechanism  , making it possible to bring more domain knowledge to bear on the disambiguation task  . In the original discourse processor , all of the constraints on plan operators , which we (: all elimination constraints , were used solely \ [ or the purpose of binding w ~riables and eliminating certain inference possibilities  . Their purpose was to eliminate provably wrong inferences  , and it , this way to give the focusing heuristics a higher likelihood of selecting the torte  . c(inference chain from the remaining set . 
We introduced a different type of constraint , graded conslraints , inspired by the concept of graded unification discuss edit  , ( Kim ,  1994) . Or , -like elimination constraints , they neither bind variables not " eliminate any inferences  . Graded constraints always return true , so they cannot eliminate inferences . However , they assign numerical penalties or preferences to inference chains based on domain specific information  . This information is then used to rank the set of possible inferences Left after the elimination constraints are 

For example , consider the day versus hour ambiguity we discussed earlier  . In most cases inference chains for Ilfl's with this ambiguity have t it  (  ; same focusing scores . We introduce the possible-time constrMnt to ( he . ok whether the temporal constraints conflict with the dynamical endaror the recorded dialogue  ( late when the inference chains are built . If the temporal information represented in an II , T is in conflict with the dialogue record date ( e . g . , scheduling a time before the record date ) or with the temporal constraints already in the calendar  ( e . g . , propose a time that is ah'eady rqiected ) , a penalty score is assigned to that inference chain  ; otherwise , a default value ( i . e . no penalty ) is returned . Several graded constraints may be fired in one inference chain  . Penalties or preferences for all graded constraints in the inference chain are summed together  . ' Phe result is the graded constraint score for that ambiguity  . 
Introducing raded constraints has two adwm--tages over adding more elimination constraints  . 
As far a stile systetning e , neral is COlmerned , graded constraints only give preferences , they do not rule out inferencing and attachment possibilities : thtls  , introducing new constraints will not damage the broad coverage of the system  . As far as the discourse processor is concerned , it ; would be possible to achieve the same effect by adding more elimination constraints  , but this wouht make it , necessary to introduce more fine-tuned plan operators geared towards specilic cases  . By introducing graded constraints we avoid expanding the search space among the plan operators  . 
4.3 Combining Predict , ions
Once the information from the graded constraints and the focusing scores is awdlable  , the challenging problem of combining these context-based predictions with tile noncontext -based ones arises  . 
We experimented with two methods of automat- -really learning functions for combining our six scores into one composite score  , namely a genetic progranmfing approach and a neural net approach  . The basic assumption of our disambigua ~ tion approach is that the context-based attd non context-based scores provide different perspectives on the disambiguation task  . They act together , each specializing in differentypes of cases , to constrain the final result . Thus , we want our learning approach to learn not only which factors are important  , but also to what extent they are genetic progranlming and neural net approaches are ideal in this respect  . 
Genetic programming ( Koza , 1992; Koza ,  1994 ) is a method for " evolving " a program to accomplish a particular task  , in this case a flmction for computing a composite score  . This technique can learn functions which are efficient and humanly understandable and editable  . Moreover , because this technique samples different parts of the search space in parallel  , it avoids to some extent he problem of selecting locally optimal solutions which are not globally optimal  . 
Connectionist approaches have been widely used\[' or spoken language processing and other areas of computational linguistics  , e . g . , ( Wermpter , 1994; Miikkulainen , 1993) to name only a few . 
Connectionist approaches are able to learn the structure inherent in the input data  , to make fine distinctions between input patterns in the presence of noise  , and to integrated if l'erent information sources . 
We refer the reader to ( losd and Qu ,  1995 ) for fall details about the motivations underlying the choice of these two methods as well as the advantages and disadvantages of each  . 
both kinds of testing are the same be canse cumulative error is only an issue for context -based approaches  . 
Our results show that the discourse processor is indeed making nsefld predictions for disambiguation : when we abstract away the problem of cumulative error  , we can achieve an improvement of 13% with the genetic programming approach and of 2  . 5% with the neural net approach over the parser's non-context based statistical disam-biguatiou technique  . For example , we were able to achieve almost perfect performance on the state vs query-if ambiguity  , missing only one case with the genetic programming approach  ; thus , for this ambiguity , we can trust the discourse proces-sor's prediction  . 
However , our results also indicate that we have not solved the whole problem of combining noncontext-and context-based predictions for disambiguation  . \[ n the face of cumulativ error , both of the two discourse combination approaches uffer fl ' omperformance degradation  , though to a different extent . Our current direction is to seek a solution to the cumulative rror problem  . Some preliminary results in this regard are discussed in  ( Qu et al ,  1996) . 
5 Evaluation
Both combination methods , the genetic programming approach and the neural net approach  , were trained on a set of 15 Spanish scheduling dialogues . They were both tested on a set of five previously unseen dialogues  . Only sentences with multiple ILTs , at least one of which was correct , were used as training and testing data . Altogether 115 sentences were used for training and 76 for testing . 
We evaluated the performance of our two methods by comparing them to two noncontext-based ones : a baseline method of selecting a parse randomly  , and a Statistical Parse Disambiguation method . The Statistical Parse Disambiguation method makes use of the three noncontext-based scores described in Section  3  . The two context-based approaches combine the three noncontext-based scores as well as the three context-based scores  , namely the focusing flag , the focusing score , and the graded constraint score . 
Table 2 reports the percentages of ambiguous sentences correctly disambiguated by each method  . We present two types of performance statistics on the testing set : without cumulative error Testing without CE and with cumulative r-ror Testing with CE  . Cumulative error builds up when an incorrect hypothesis is chosen and incorporated into the discourse context  , causing future predictions based on discourse context to be inaccurate  . Notice that for the two noncontext-based approaches  , the performance figures for 6 Conclusions In this article we have discussed how we apply predictions from our plan-based is course processor to the problem of disambiguation  . Our evaluation demonstrates the advantage of incorporating context-based predictions into a purely noncontext-based approach  . While our results indicate that we have not solved the whole problem of combining noncontext - and context-based predictions for disambiguation  , they show that the discourse processor is making usefld predictions and that we have combined this information suc-cessflllll ll lly with the noncontext -based predictors  . 
Our current efforts are aimed at solving the cumulative rror problem in using discourse context  . 
We noticed that cumulative rror is especially a problem in spontaneous speech systems where unexpected in pnt  , disfluencies , out-of-domain sentences and missing information cause the deterio-:ration of the quality of context  . One possibility is to reassess and reestablish the context state when a conflict is detected between context and other predictions  . A second proposal is to keep the nbest hypotheses and to choose one only after having processed a sequence of inputs  . Preliminary experiments show that both t ) roposals help reduce the adverseffect of the cumulative rror problem  . 
Our results also suggest another possible avenue of future development  . Instead of trying to learn a general function for combining various information sources  , we could decide which source of information to trust in a particular case and classify 
II , lindo in
Statistical Parse B is lunl ) iguation \ [ D\]P genelle Progrtllnlliil lgDPNoltlcalN  ( ! t'l ~ ainixlg\]Testing without CE 32% I45% 76 . 5~ / 76 3% 91 . (; % l 89 . 5% s 5,2%_\[78 . 8%
Testing with CE 45% 76 . 3% 60% 71  . a %' l'~tble2:Disanfliiguation ( if All hnll liguous S (  ; nten cos the type of ambiguity at ti ; md with the best ap-1 ) ro~tch forth L < s ~ mil ) iguity . This could be ace , om-plished , for exa3nl ) h; , with a decision treele ~ trning ~1) preach . 

The authors would like to thank I , or iLev in , Alert I , ~ vielind Alex Waibel for COllllllellt SOi ~ l the work reported here a  , tidth ; mk the two ~ tnoilyiii OllS reviewers for COllillie AltSelithe  , earlier version of t , he1)~q ) er . The work is supl ) or Lodin pa , l'l ; by ~ Lgi':~iilt\['1'OI11 the l)epa , rtll lell to fI)e\['ellse . 
l ? eferences
Allen , J . F . and 1, K . Schubert .  1991 . 7' he Trains Project . Ph . I ) . thesis , University of Rochester ,   , q chool of (7 o Hipllter Science . 
( J ; ~ rroll , 3 . ~ tti(\[T . Ilriscoe .  1993 . ( Jenera , l-ized probabilistic 1 , 1 , ptu'siiig of natured lal > e , u  a , ge((:orpol'a0 with unlflc~d ; hm-bascdgraAu-lIl ; % l'S . (,* o?lt pnlalioltal Lingnislics , 19(1) . 
Sidner , C .  \[, .  1981 . Focusing for\[nterl ) retationf\])rono/lllS . dlmcrican Journal of Computational
Linguistics , 7(4):21723I.
Kim , A .  1994 . Graded unitieation : AI ) ' amework for interactive processing . In Proceedings of the Association for Computational Linguistics  . 
Koza, . 1 .  1992 . Geneticl ' ~ v gramming : On the I " tv gramming of Computers by Means of Nalu-'ral  , qeleclion . MI'I'Press . 
Koza , a . 1994. Gc.netie Programming 1I . MIT

l , ~ mibert , I, .  1993 . Recognizing Complex Discourse Acts : A Tripa ; <> titc Plan-Based Model of Dialogue . Ph . D . thesis , Department of Coin-purer Science , University of l ) elaw ~- ~ rc . 
Lambert , L . and S . ( Tarl ) erry .  1992 . Modeling negotiation subdialogues , hiProceedings@the

Lavie , A .  19)5 . A Grammar Based Robnsll ' arser / , br 5' ponlaneous Speech . I'h . 1) . thesis , School of Computer Science , Carnegie Mellon University . 
Levin , 1, . , O . Glickman , Y . Qu , D . Gates , A . I , avie , C . P . losd , 17 VanEssDykema , and A . Waibe \] .  1995 . Using context in machine translation of spoken l ~ mgmtge  . In Theoretical and Methodological Issues in Machine Translation  . 
Miikkulainen , R, .  1993 . , Sub . symbolic Natural Language P~vcessing : An Intcgrated Model of  , % ripls , Lexicon , and Memory . The MI'FI ) ress . 
Qu , Y . , B . l ) iF , uge , nio , A . I , avie , I, . S . l , evin , ~ uid C , P . los6 .  1996 . Minimizing Cumulative Error in I ) is course ( km text . To appear in ICUAI Workshop Proceedings on Dialoguef ' rocessing in  , S'poken Language Systems . 
lh ; ithinger , N . ~ mdE . Meier .  1995 . Utilizing st~-tistic Mdi Mogueact , t ) rocessing in Verbmobil . In
Proceedings of the ACL.
Rosd , C . P . , B . l)i Igugenio , L . S . l , evin , mid(J . Vl-%ll\['\]ss-l ) ykema .  1995 .  / ) is eourse processing of dialogues with multiph " threads  , lit Proceedings of the ACL . 
l , os 6, C . P . ~ mdY . Qu .  1995 . Automatically Learning to Use Discourse Information l'b rI  ) is ambiguation . Center for M~chine'l'r misl~-tion , (;arnegie Mellon University . '\[' echnical R , e-port . 
, qiiiith , R . W . , l ) . II, . Hipp , iliid A . W . l Jierin ~ liti . 
1995 . An architecture for voice dialogue sys-tel\[iS based on prolog-style theoreli+i proving  . 
() omputalional Lin(luislics , 21(3):218320.
Suhm , B . , 1, . Levin , N . Coccaro , J . Carbone U , K . Iloriguehi , R, . Isotani , A . \[mvie , L . Maylield , C .  ) . los ( , C . Van-EssDykcma , ~ mdA . W ~ d bel . 
1994 . Speech-hm guage integration in a multilingual speech translation systen i  . In Proceedings of the AAAI Workshop onln leg  #nlion of Nalnral Language and  , qpeech Processing . 
Werml)ter , S .  1994 . (7 on nection is l , learning of flat synt~mti can , ~lysis for speech/language systems . 
In Proceedings of the International ConJerence on Artificial Neural Networks  . 
Woszeyna , M . , N . Coeearo , A . Eisele , A . Lavie , A . McNair , '\[' . Polzin , I . Regime , C . P . 1 ? os 6, T . Slobod ~, M . ' Pomita , a . Tsutsumi , N . Waibel , A . Waibel , and W . W ~ r d .  1993 . R , ecent ~ d-vances in JANUS : a speech translation system  . 
Inl'roceedings of the ARI'Alluman Languages 7'ethnology Workshop . 

