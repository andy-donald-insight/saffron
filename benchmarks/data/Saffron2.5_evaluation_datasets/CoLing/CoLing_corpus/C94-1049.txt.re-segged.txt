CO-OCCURRENCE VECTORS FROM CORPORAVS.
DISTANCE VECTORS FROMDICTIONARIES
Yoshiki Niwa ~ tnd Yoshihiko Nitta
Advanced Research l ~ aboratory , llitachi , l , td . 
Hatoyama , Saitam ~ t350-03,.)ap;m
ni wa2,nitta ) ~ harl.hitachi.co.jp

A comparison W~LS made of vectors derived by using ordinary cooccurrence statistics from large text corpora and of vectors derived by measuring the interword distances in dictionary definitions  . The precision of word sense disambiguation by using cooccurrence vectors frorn the  1987 Wall Street Journal ( 20M total words ) was higher than that by using distance vectors from the Collins English l  ) ictionary ( 60K head words+1 . 6M definition words ) , l lowever , other experimen--tal results suggest hat distance vectors contain some different semantic information from cooccurrence vectors  . 
1 Introduction
Word vectors reflecting word meanings are expected to enable numerical approaches to semantics  . Some early attempts at vector representation iI ) sycholinguistics were the semantic d ( O'erential approach ( Osgood et al .  1957 ) and the associative distribution apl ) roach ( Deese 1962 )  . l lowever , they were derived manually through psychological experiments  . An early attempt at automation was made I ) y Wilksela L ( t990 ) us- . 
ing cooccurrence statistics . Since then , there haw " been some promising results from using cooccurrence vectors  , such as word sense disambiguation ( Schiitze\[993 )  , and word clustering ( Pereirae Lal .  1993) . 
l lowever , using the cooccurrence statistics requires a huge corpus that covers even most rare words  . 
We recently developed word vectors that are derived from an ordinary dictionary by measuring the interword distances in the word definitions  ( Niwa and Nitta 1993 )  . ' this method , by its nature , h~s no prol ) lom handling rare words . In this paper we examine then sefldness of these distance vectors as semantic reWresentations by comparing them with co-occur  , ' encevectors . 
2 Distance Vectors
A reference network of the words in a dictionary ( Fig . 
1) is used to measure the distance between words , q'he network is a graph that shows which words are used in the  . definition of each word ( Nitta 1988) . The network shown in Fig .   1 is for a w ~ ry small portion of the reference network for the Collins English  1  ) ictionary ( 1979 edition ) in the CI ) -IOMI ( Liberman 1991 )  , with 60K head words-b1 . 6M definition words . 
writing unit ( Or ) \/ word comnm nieation / ~ alph Mmtl cal\L_\/l anguag  , - - - - - - dictionaryo , /\(:) p ,  . ~ ople ~, ook(O ~) Fig .  1 . Portion of a reference network . 
For example , tile delinition for diclionargis % bookill which the words of a language are listed alphabetically  .   .   .   . " The word diclion a ~ d is thus linked to the words book  , word , language , and alphabelical . 
A word w ~ et or is defined its the list of distances from a word to a certain sew of selected words  , which we call origins . The words in Fig . 1 marked with Oi ( unit , book , and people ) m'e assumed to be origin words . In principle , origin words can be freoly chosen . 
In our exl~eriments we used mi ( Idlefi'equency words : the 51st to 1050th most frequent words in the reference Collins English I  ) ictiotmry ( CI ' ; D ) , The distance w ~ c to r f l ) rdi clionary is deriw Mit '* foblOWS : ~ )   . . . disti~uc ,, (( ticl . , 01) dictionary ~1 .   .   . distance ( dict . , 0'2) 2  .   .   . distance ( dicL , Oa ) Thei-4 , helement is the distance ( the length of the shortest path ) between diclionary and the ith origin , O i . To begin , we assume every link has a constant lengtho\[' 1  . The actual definition for link length will be given later  . 
If word A is used in the definition of word B , t . he , m words are expected to be strongly related . This is the basis of our hypothesis that the distances in the refi~r-ence network reflect the associative distances between words  ( Nitta 1933 )  . 

Use(if Refe . renee Networks lefi , rence networks have been successfully used its neural networks  ( by Vdronis and Ide ( 1990 ) for word sense disain l ) igua-tion ) and as fields for artificial association , such its spreading activation ( by Kojiina and l:urugori ( 1993 ) for context-coherence measurement )  . The distance vector of a word can be considered to be a list  , of the activation strengths at the origin nodes when the word node is activated  . Therefore , distance w ~ ctors can be expected to convey almost the santo information as the entire network  , and clearly they are Ili~i clieasier to handle . 
Dependence on Dietiol nlrles As a seinant c representation of words  , distltll Cew ~ ctors are expected to depend very weakly on the particular source dictionary  . We eolil pared two sets of distance vectors , one from I , I ) OCE ( Procter 1978 ) and the other from COBUILD ( Sinclair 1987 )  , and verified that their difference is at least snlaller than the difDrence of the word definitions themselves  ( Niwa and Nitta 1993 )  . 
We will now describe some technical details al ) Oll t the derivation of distance vectors . 
Lhlk Length Distance measurenient in a reference network depends on the detinition of link length  . 
Previously , we assumed for si inplicity that every link has a construct length  . I lowever , this shnph ; definitions eernstlnn atllral because it does not relh '  . ct word frequency . Because tt path through low-fi'equency words ( rare words ) implies a strong relation , it should be ineasnred ms a shorter path . Therefore , we use the following definition of link length , which takes accotltlt of word frequency . 
length(Wi,W2)d,'I:---log(7N-77' ~ . , ) n ' This shows the length of the links between words W i  ( i = 1 , 2) ill Fig ,  2 , where Ni denotes the total mini-bet of links front and to Vi and n denotes the uul nlmr of direct links bt  . ' tween these two words . 
Fig . 2Links between two words.
Normalization l ) is tance vectors ; iren or rial-ized by first changing each coordinal  , e into its deviation in the coord in ; tLe:v--: ( ' vi ) -~+ v '= vi--ai where ai and oi are the average and the standa Mdeviation of the distances fi ' om the ith origin  . Next , each coordinal . e is changed hire its deviation in thc ~ vector : where t ? and cd are tile average  . ~_llldi , hestandard deviation of v(i = I . . . . ) . 
3 Cooccurro.ric (; Vectors
We use ordinary co-o (: Clll'rl ; ll Ce statistics ; tll dillell Sllre the co-occurrei/ce likelihood betwee i it wo words  , X and Y , hy the Inutua \] hi for lna Lioii estilnate . ((\] hurch and ll~uiks1989)' . 
l(X,V ) = i < , giP ( xIV )
P ( X ) ' where P ( X ) is the oCcilr reilce , density of word X hiw hole corllus , and the conditional probability l ' ( xIv ) is the density of X in an eight > or hood of word Y , llere then eighl ) or hood is defined as 50 words lie . foreor afters . i i y appearance of word Y .   ( There is a variety of neighborhood definitions Sllchas  "100 sllrrollllding words " ( Yarowsky 1992 ) and " within a distance of no more thall 3 word sigllor h / g filnction words " ( I ) a garlel , al . 

The logarithm with '- t -' is dellned to be ( ) for an ar-g ; ument less than 1 . Negative stimates were neglected because they are mostly accidental except when X and Y are frequent enough  ( Chnrch and l Ianl , : s1989) . 
A cooccurence vector of a word is defined as the list of co-occt lrrellce likelihood of the word with a cer-tahise to \[' orighi words  . We tlsed the salnese to foright words ; is for the distance vectors . 
I(w , ?30 l(w , %)
CV\[w =
I(w , 0, , , )
C(~-oeelll'l'elle(~,V(~t'tol'.
When the frequency of X or Y is zero , we cannot measure their co-c , ccurence likelihood , and such cruses are not exceptional . This sparseness problem is wellknown and serious in the cooccurrences Latis C\[cs  . We used as ~ corpus the 1!)87 Wall Street ; Journ M in the CI ) - I ~ . OMi (1991), which has a total of 20M words . 
'\]' hen Ulliber of words which appeared al , least OIlCe , was about 50% of the total 62I ( head words of CEI )  , and tile . percentageOf " tile word-origin pairs which appeared title astonce was about  16% of total 62K  ?  1K   ( = 62M ) pairs . When the cooccurrence likelihood CallliOt Im in easurc ~ d > I  , he vahle I(X , Y ) was set to 0 . 
305 4 Experimental R , esults
We compared the two vector representations by using them for the following two semantic tmsks  . The first is word sense disambiguation ( WSD ) based on the similarity of context vectors ; the second is the learning of positive or negative meanings from example words  . 
With WSD , the precision by using cooccurrence vectors from a  20M words corpus was higher than by using distance vectors from the CEIL  4   . 1 Word Sense D isambiguat ion Word sense disambiguation is a serious semantic prob-lena  . A variety of approaches have been proposed for solving it  . For example , V ( ! ronis and Ide ( 1990 ) used reference networks as neural networks , l learst ( 1991 ) used ( shallow ) syntactic similarity between contexts , Cowie el al .   ( 1992 ) used simulated annealing for quick parallel disambignation  , and Yarowsky ( 1992 ) used cooccurrence statistics between words and thesaurus categories  . 
Our disambiguation method is based on the shn -ilarity of context vectors  , which was originated by Wilks el al .  (1990) . In this method , a context vector is the sum of its constituent word vectors  ( except the target word itself )  . That is , tile context vector for context , C: .   .   . W_N .   .   . W_l W W l . . . WN , . . . ~ isNtv(c ) = ~ V(w ~) . 
i = - N
The similarity of contexts is measured by the angle of their vectors  ( or actually the inner product of their normalized vectors  )  . 
V ( CI)V(C2) sim(C  ~, C . ~) = lv(C ~) l IV ( C2) l ' Let word w have senses l , s2 ,   . . . , sll I  a  ' ld each sells ( ; have the following context examples . 
Sense Context Examplessl Cll , C12,, . . Cln , s2 C~l , C22 .   .   .   . C ~, ~:
SmCml , Cm2, . . . Cmn , , ,
We infer that the sense of word w in an arhitrary context C is si if for some j the similarity  , sire(C , Cij ) , is maximum among all tile context examples . 
Another possible way to infer the sense is to choose sense si such that the average of sim  ( C , Cij ) over j = 1 , 2 ,  . . . , hi is maximum . We selected the first method because a peculiarly similar example is more important hant heaverage similarity  . 
Figure 3 ( next page ) shows the disamhiguation precision for 9 words . For each word , we selected two sense shown over each graph . These senses were chosen because they are clearly different and we could collect sufficient nmnber  ( more than 20 ) of context examples . The names of senses were chosen from the category names in Roger's International Thesaurus  , except or gan's . 
The results using distance vectors are shown by clots  (  .  ?  . ) , and using cooccurrence vectors from the 1987 vsa ( 20M words ) by cir ,  . tes(ooo ) . 
A context size ( x-axis ) of , for example ,   10 means 10 words before tile target word and 10 words after tile target word . Wc used 20 examples per sense ; they were taken from tlle 1988 WSJ . Tile test contexts were from the 1987 WSJ : Then mn be r of test contexts varies from word to word  ( 100 to 1000 )  . The precision is the simple average of the respective precisions for the two senses  . 
The results of Fig .   3 show that the precision by using cooccurrence vectors are higher than that by using distance vectors except two cases  , interest and customs . And we have not yet found a case where the distance vectors give higher precision  . Therefore we conclude that cooccurrence vectors are advantageous over distance vectors to WSD based on the context similarity  . 
The sl ) arseness problem for cooccurrence vectors is not serious in this case because a ch context consists of plural words  . 
4  . 2 Learn ing o f pos i t ivc-or - ne#al ive Another experiment using the same two vector representations was done to measure tile learning of positive or negative meanings  .  1 , ' igure 4 show stile changes in the precision ( the percentage of agreement with the authors ' combined judgement  )  . The x-axis indicate stile nunll ) er of example words for each positive or ~ teg -alive pair  . Judgement w~s again done by using the nearest example  . The example and test words are shown in Tables 1 and 2  , respectively . 
In this case , the distance vectors were advantageous . The precision by using distance vectors increased to about  80% and then leveled off , while the precision by using cooccurrence vectors tayed arouud  60%  . We can therefore conclude that the property of positive-or-negative is reflected in distance vectors more strongly than ill cooccurrence vectors  . Tiles parseness l ) roblem is supposed to be a major factor in this case  . 
306% suit ( CLDTIIING/LAWSUIT ) ooo ^0 ~ ? ? ? o o o o 100 e $ 4 ) 4 ) o ? ? ? 4 )  ?  ( listance vector jiA ,   ,   , llJ , Jl ,   , 5 10 20 30 40 50 context size organ ( BODY/MUSIC ) 
O Ooooooa * oooo ? oo 0 oeee ,   ,   , it , JILJi ,  5 10 20 311 4' ( 50 is sll (  :  ( EMERGENCE/TOPIC ) 1O ( l ~ 0 0 0 0 e QO O 0
Ill , , I . ); 0 a ;; 050 % lOOtOO ? oo ? ? ? ? ? ~ . . ~ ~ ~0 eO00 ?
Oo5(I0CO-(R ; . V ? . C\[ , Or a distance vector 5 i  Cl 20   30   40   50 context size oo ~ oe ~ ooooou o Oo 000 OeO???x_t_~t . ~ ~' l'O'3 t()~--a520'I050
I00 e . 8 ? Oo ??? .   .   .   .   .   .   . ';,\['0' 5 l 0 203 50%
I00 o ~? ooooo?go ooe , U , ~ eeo ??( , ?0? ocO-oc , vector ? distance vector ,   ,   ,   ,   ,   ,   ,   ,   ,   , jloio : ; o ; 05 ocon text size (' . llS ; OIIIS ( IIAIIIT(pl . ) / SEll . VICE ) hl ~ . eres(;(CURIOSITY/DEBT ) 100I0(1 , 5\[)o Oo " . ? . o . ' ? o ? ' .   . ? oo ??0? ? 5I02 ( ) 30 40 50 5 ( I ~ ooeooo ~ O ? ? ooo~?o 5   10   20   30   40   50 Fig .   3 Disambiguation of 9 words hyusing ro-ot:rm'rence vectors ( ooo ) m , lhy using distance w . * ctors(--,) . ( The number of examples is 10\[' or each sense . ) 50%  .   .   .   .   .   .   .   .   .   .   .  ?  .   .   .   .   .   .   . t ~ JJ JJ J J~2 ?????? o?????o?o ??? ? o .   .   .   .   .  ~_990  .   .   .   .   .   .   .   .   .   .   .   .   .   .   . ~ ooco-oc , vector ( 20M ) ? distance vector iitliit lIttllItl trill ItttIl  l~0   10   20 number of example pairs Fig . 4 Learning of posiff ve-or-negative . 
Table 1 Example pairs.
positive negative positive negative 1 true false 16 l ) roperly crime 2 new wrong 17 succeed ( lie 3 better disease 18 worth violent , l clear angry 19 friendly hurt 5 pleasure noise 20 useful punishment ,   6 correct pain 21 success poor 7 pleasant lose 22 in t crest lng badly 8 snltable destroy 23 active fail 9 clean dangerous 2  , 1 polite suffering 10 advantage harm 25 wine nemy 11 love kill 26 improve rude 12 best fear 27 favour danger attractive ill 29 happy waste powerful foolish 30 praise doubt
Table 2Test words.
positive ( 20 words ) balance de labor at eelation eligible en joy fluent honorary Imnourable hopeful hopefully influential interested legible lust renormal recreation replete resilient restorative sincere negative  ( 30 words ) conflmion cuckold dally daumation dull ferocious flaw hesitate hostage huddle in attentive liverlsh lowly mock neglect que errape ridiculous avage scanty sceptical schizophrenias coffscrnffy ship wreck superstitions y cophant trouble wicked worthless  4  . 3 Supplementary Data In the experiments discussed above  , the corpus size for cooccurrence vctors was set to  20M words ( '87 WSJ ) and the vector dimension for both cooccurrence and distance vectors wins set to  1000  . llere we show some supplementary data that suppor these parameter settings  . 
a . Corpus size ( for co-occurrence vectors ) Figure 5 shows the change in disambiguation pre-eision as the corpus size for cooccurrence statistics increases from  200 words to 20M words . ( The words are suit , issue and race , the context size is 10 , and the number of examples per sense is 10 . ) These three graphs level off after around IM words  . Therefore , a corpus size of 20M words is not too small . 
\]00% 50% * ooo Oo * 000*000 ?00 , ****** o?oog ~? o 0 Ooo * suit 0 lSSlle 0 facelf l 3   104   10  ~  1M   10M e or p t l s s i z e ( wor ( I ) Fig .   5 Dependence of the disambiguation precision on the corpus size for c  . o-occurrence vctors . 
context size : 10 , number of examples : 10/sense , vector dimension : 1000 . 
l ). Vector Dimension
Figure 6 ( next page ) shows the dependence of disambiguation precision on the vector dimension for  ( i ) cooccurrence and ( ii ) distance vectors . As for cooccurrence vectors , the precision levels off near a dimension of 100 . Therefore , a dimension size of 1000 is suflicient or cvcn redumlant . I Iowever , in the distance vector's case , it is not clear whether the precision is leveling or still increasing around  1000 dimension . 
5 Conclus ion ? A comparison was nlade of co -occnrrence vctors from large text corpora and of distance vectors from dictionary delinitions  . 
? Fortile word sense disambiguation based on the context simih trity  , cooccurrence vectors fl ' om tile 1987 Wall Street Journal ( 20M total words ) was advantageous over distance vectors from the Collinsl  , ;nglish Dictionary (60K head words+1 . 6M definition words ) . 
? For learning positive or negalive meanings from example words  , distance vectors gave remarkably higher precision than cooccurrence vctors  . 
This suggests , though further investigation is required , that distance w : ctors contain some different semantic information from cooccurrence vectors  . 
308 lOO % s0% 100% 5o %
Fig . 6 ( i ) by co-o e , vectors *** *** 0   0   0   0   0   0   0   0   0   0   0 * sult 0   18S lle 0 l'Jlee=__u__L_a~t ~ 10   100   1000 vector dlll lension ( ii ) by distance vectors ~ 0   0   0   8 ooo #*  o ? QO
Oo OO 8?, suitoissue.
Orace 10100 lOOO vector diln(msion
I ) ependence on vector dimension for ( i ) cooccurrence veetors and ( ii ) distance vectors . 
context size : 10 , examples : 10/sense , corpus size for co-o e , vectors : 20M word . 

Kenneth W . Church and Patrickllanks .  1989 . Word association or ms , mutual information , and lexicography . In Proceedings of lhe 27th Annual Meeting of the Association for Computalional Ling  , is-tics , pages 76-83 , Vancouver , Canada . 
Jim Cowie , Joe Guthrie , and Louise Guthrie .  1992 . 
Lexieal disambiguation using simulated . : mtwal-ing . In Proceedings of COI , ING-92, pages 1/59-365,
Nantes , France.
Ido Dagan , Shaul Marcus , and Shaul Markovitch.
1993 . Contextual word similarity and estimation from sparse data  . In Proceedings of Ihe 31st Annual Meeting of the Association for Compulational Linguist & s  , pages 164-171 , Columbus , Ohio . 
James Deese .  1962 . On the structure of associative meaning . Psychological Review , 69(3):16F175 . 
Marti A . II earst .  1991 . Noun homograph disambigna-tion using local context in large texteorl  ) ora . In Proceedings of lhe 71h Annum Confercncc of Ihe Universily of Walerloo Center for lhc New OEI  ) and Text Research , pages 122 , Oxford . 
llideki Kozima and Teiji Furugori .  1993 . Similarity between words computed by spreading actiw ~ tion on an english dictionary  . In Proceedings of I'7 ACL-93 , pages 232--239 , Utrecht , the Netherlands . 
Mark Liberman , editor .  1991 . CDROML Associa-rio , , for Comlmtational I , inguistics Data Collection Initiative , University of Pennsylvania . 
Yoshihiko Nitta .  1988 . The referential structure , of the word definitions in ordinary dictionaries , h , Proceedings of lhe Workshop on rite Aspects of Lexicon for Natural Language Processing  , LNL88-8 , JSSST ' , pages I-21 , Fukuoka University , Japan . 
( i ", 1 a panese).
Yoshihiko Nitta .  1993 . Refi . ' rential structure . -anmchanism forgiving word-delinition i ordinary lexicons  . In C . Lee and II . Kant , editors , Language , Information and Computation , pages 99-1t0 . Thaehaksa , Seoul . 
Yoshiki Niwa and Yoshihiko Nitta .  1993 . Distance vector representation \[' words , derived from refe . r-encenetworksi , tordinary dictionaries . MCCS 93-253, (; Oml)l , tingll . esearch I , aboratory , New Mexico State University , l , as Cruces . 
C .  1'; . Osgood , ( l . F . Such , and P . II . Tantmnl ) an ln . 
1957 . 7' he Measurement of Meaning . University of Illinois Press , Urlmna . 
Fernando Pereira , Naftali Tishby , and IAIlian Lee . 
1993. l ) is tributional clustering of english words.
lit Proceedings of the 31st Annval Meeting of the Association for Computational Lin : luislics  , pages
I8;I190, Colmnlms , Ohio.
I'aulProcter,e<lit . or .  1978 . Longman Dictionary of Contemporaryl Cnglish ( LI ) OCE )  . Long\]nan , liar-low , Essex , tirst edition . 
llinrich Sch/itze .  1993 . Word space . % J . D . Cowan , q . J . llans on an ( IC . L . C , iles , editors , Advances in Neural Information lb ' ocessing ?' ystems  , pages 8!)5902 . Morgan Kaufinann , San Mateo , Calif of , lia . 
John Sinclair , editor .  1987 . Collins COBUILD English Language l ) iclionary . Collins and t . he Uni-w ~ rslty of llirming ham , London . 
Jean Ve'ro , fis and Nancy M . \[ de .  1990 . Word sense disambiguation with very large neural networks extracted from machine readable dictionaries  . In Proceedings of COLING-90 , pages 389-394 , llels in ki . 
Yorick Wilks , I ) a , , Fass , Chengmint Guo , James 1" . 
Me Dolmhl , Tony Plate , and Ilrian M . Slator .  1990 . 
Providing machine tractable dictionary tools . Machine Translation , 5(2):99154 . 
l ) avid Yarowsky .  1992 . Word-sense disambigua-lion using statistieal models of roget's categories trained on large corpora  . In Proceedings of COLING-92 , pages 454-460 , Nantes , France . 

