Semantic Similarity Applied to Spoken Dialogue Summarization 
Iryna Gurevych and Michael Strube
EML Research gGmbH
Schloss-Wolfsbrunnenweg 33
69118 Heidelberg , Germany


We present a novel approach to spoken dialogue summarization  . Our system employs a set of semantic similarity metrics using the noun portion of WordNet as a knowledge source  . So far , the noun senses have been disambiguated manually . The algorithm aims to extract utterances carrying the essential content of dialogues  . We evaluate the system on 20 Switchboard dialogues . The results show that our system outperforms LEAD  , RANDOM and TF*IDF baselines . 
1 Introduction
Research in automatic text summarization began in the late  1950s and has been receiving more attention again over the last decade  . The maturity of this research area is indicated by recent largescale evaluation efforts  ( Radev et al ,  2003) . In comparison , speech summarization is a rather new research area which emerged only a few years ago  . However , the demand for speech summarization is growing because of the increasing availability of  ( digitally encoded ) speech databases ( e . g . spoken news , political speeches ) . 
Our research is concerned with the development of a system for automatically generating summaries of conversational speech  . As a potential application we envision the automatic generation of meeting minutes  . The approach to spoken dialogue summarization presented here in unifies corpus-and knowledge -based approaches to summarization  , i . e . 
we develop a shallow knowledge-based approach.
Our system employs a set of semantic similarity metrics which utilize WordNet as a knowledge source  . We claim that semantic similarity between a given utterance and the dialogue as a whole is an appropriate criterion for the selection of utterances which carry the essential content of the dialogue  , i . e . 
relevant utterances . ? In order to study the performance of semantic similarity methods  , we remove the noise from the preprocessing modules by manually disambiguating lexical noun senses  . 
In Section 2 , we briefly describe research on summarization and how spoken dialogue summarization differs from text summarization  . Section 3 gives the semantic similarity metrics we use and describes how they are applied to the summarization problem  . 
Section 4 provides information about the data used in our experiments  , while Section 5 describes the experiments and the results together with their statistical significance  . 
2 Text , Speech and Dialogue

Most research on automatic summarization dealt with written text  . This work was based either on corpus-based , statistical methods or on knowledge-based techniques  ( for an overview over both strands of research see Mani & Maybury  ( 1999 ) ) . Recent advances in text summarization are mostly due to statistical techniques with some additional usage of linguistic knowledge  , e . g . ( Marcu , 2000; Teufel & Moens ,  2002) , which can be applied to unrestricted input . 
Research on speech summarization focused mainly on single-speaker  , written-to-be-spoken text ( e . g . spoken news , political speeches , etc . ) . The methods were mostly derived from work on text summarization  , but extended it by exploiting particular characteristics of spoken language  , e . g . acoustic confidence scores or intonation . Difficulties arise because speech recognition systems are not perfect  . 
Therefore , spoken dialogue summarization systems have to deal with errors in the input  . There are no sentence boundaries in spoken language either  . 
Work on spoken dialogue summarization is still in its infancy  ( Reithinger et al , 2000; Zechner ,  2002) . Multiparty dialogue is much more difficult to process than written text  . In addition to the difficulties speech summarization hast of ace  , spoken dialogue contain saw hole range of dialogue phenomena as disfluencies  , hesitations , interruptions , etc . Also , the information to be summarized may be contributed by different speakers  ( e . g . in question-answer pairs ) . Finally , the language used in spoken dialogue differs from language used in texts  . Because discourse participants are able to immediately clarify misunderstandings  , the language used does not have to be that explicit  . 
3 Semantic Similarity 3 . 1 Semantic Similarity Metrics Experiments reported here employed Ted Peder-sen?s  ( 2002 ) semantic similarity package . We applied five of the metrics , which rely on WordNet as a knowledge base and were developed in the context of work on word sense disambiguation  . The first measure is Leacock and Chodorow?s ( 1998 ) Normalized Path Length ( we will refer to it as lch )  . Semantic similarity sim between words w1 and w2 is defined as given in Equation 1:   simc1  , c2=?loglen(c1 , c2 ) 2 ? D ( 1 ) c1 and c2 are concepts corresponding to w1 and w2 . 1 len(c1 , c2 ) is the length of the shortest path between them . D is the maximum depth of the taxonomy . 
The following measures incorporate an additional , qualitatively different knowledge source based on some kind of corpus analysis  . The extended gloss overlaps measure introduced by Banerjee & Pedersen  ( 2003 )   ( referred to as lesk in the following ) is based on the number of shared words ( overlaps ) in the WordNet definitions ( glosses ) of the respective concepts . It also extends the glosses to include the definitions of concepts related to the concept under consideration based on the WordNet hierarchy  . Formally , semantic relatedness sim between words w1 and w2 is defined by the following equation : simc1  , c2=? score(R1(c1) , R2 ( c2 ) )  ( 2 ) where R is a set of semantic relations , score ( ) is a function accepting two glosses as input , finding overlaps between them , and returning a corresponding relatedness score . 
The remaining three methods require an additional knowledge source  , an information content file ( ICF ) . This file contains information content values for WordNet concepts  , which are needed for computing the semantic similarity score for two concepts  . Information content values are based on the frequency counts for respective concepts  . Resnik ( 1995 )   ( res for short ) calculates the information content of the concept that subsumes the given two  1This also refers to the rest of the methods . 
concepts in the taxonomy ( see Equation 3): simc1 , c2=maxc?S(c1 , c2) [ ? log p(c )] (3) where S(c1 , c2 ) is the set of concepts which subsume both c1 and c2 and ? log p ( c ) is the negative loglikelihood ( information content )  . The probability p is computed as the relative frequency of the concept  . Resnik?s measure is based on the intuition that the semantic similarity between concepts may be quantified on the basis of information shared between them  . In this case the WordNet hierarchy is used to determine the closest superordinate of a pair of concepts  . 
Jiang & Conrath ( 1997 ) proposed to combine edge-and node-based techniques in counting the edges and enhancing it by the node-based calculation of the information content as introduced by Resnik  ( 1995 )   ( the method is abbreviated as jcn )  . 
The distance between two concepts c1 and c2 is formalized as given in Equation 4:   distc1  , c2=IC(c1)+IC(c2) ?2 ? IC(lso(c1 , c2 ) )  ( 4 ) where IC is the information content value of the concept  , and lso(c1 , c2) is the closest subsumer of the two concepts . 
The last method is that of L in ( 1998 )   ( we call this metric lin )  . He defined semantic similarity using a formula derived from information theory  . This measure is sometimes called a universal semantic similarity measure as it is supposed to be application -  , domain - , and resource independent . According to this method , the similarity is given in Equation 5: simc1 , c2 = 2 ? log p(lso(c1 , c2)) log p(c1) + log p(c2) (5) 3 . 2 Semantic Similarity in Summarization The process of automatic dialogue summarization  , as defined in the context of this work , means to extract the most relevant utterances from the dialogue  . 
We restate this as a classification problem , which is similar to the definition given by Kupiec et al  ( 1995 )  . This means that utterances are classified as relevant or irrelevant for the summary of a specific dialogue  . By relevant utterances we mean those carrying the most essential parts of the dialogue?s content  . The summarization task is , then , to extract the set of utterances from the transcript  , which a human would use to make a dialogue summary  . 
The key idea behind the algorithm presented here is to quantify the degree of semantic similarity between a given utterance and the whole dialogue  . We argue that semantic similarity between an utterance 
A.: Utt1 Okay.
Utt 2Tell me about your home.
B .: Utt3Well , it?s an older home.
Utt 4It was made back in the early sixties.
Utt 5It?s a pier beam house.
A.: Utt 6 Huh-uh.
B .: Utt 7Got three bedrooms , one bath
Utt 8 and that just makes mescream.
A.: Utt 9That?s pretty tough.
Utt10 What area do you live in ?
B .: Utt 11Ilive in Houston.
Table 1: Switchboard dialogue fragment
Number Concepts Sense Number
CRUtt1 ? ?
CRUtt 2 home 2
CRUtt 3 home 2
CRUtt 4 sixties 1
CRUtt5 pier , beam , house 2, 2, 1
CRUtt 6? ?
CRUtt 7 bedrooms , bath 1, 5
CRUtt 8? ?
CRUtt 9? ?
CRUtt10 area 1
CRUtt 11 Houston 1
Table 2: Utterances mapped to WordNet concepts and the dialogue as a whole represents an appropriate criterion for the selection of relevant utterances  . 
We describe each of the processing steps , employing the example dialogue D from Table 1 . This example consists of the set of utterances Utt1  ,  . . . ,

3 . 2 . 1 Creating conceptual representations The semantic similarity algorithms introduced in Section  3  . 1 operate on the noun portion of WordNet . 
Our approach to dialogue summarization , as previously stated , is to compute semantic similarity for a given pair Uttn  , D . In order to do that , we require a WordNet-based conceptual representation of both Uttn  , i . e . CRUttn , and D , i . e . CRD , and compare them using the semantic similarity measures  . 
Therefore , we map the nouns contained in the utterances to their respective WordNet senses and operate on these representations in the subsequent steps  . 
The results of this operation are shown in Table 2 . 
The number in the last column indicates the disambiguated WordNet sense  . 
The resulting dialogue representation CRD will be the set of concepts resulting from adding individual utterance representations  , i . e . CRD = home , home , sixties , pier , beam , house , bedrooms , bath , area , Houston . 
3 . 2 . 2 Computing average semantic similarity For each utterance Uttn  , we create a two-dimensional matrix C with the dimensions  (  #CRD ?  #CRU ttn )  , where  #denotes the number of elements in the set . 
C = ( cij)i = 1, . . . ,#CRD,j=1, . . . ,  #CRUttn , see Table 3 . Then , we compute the semantic similarity SS score ( i , j ) employing any of the semantic similarity metrics described above for each pair of concepts  . The semantic similarity score SS final for CRU ttn and CRD is then defined as the average pairwise semantic similarity between all concepts in CRUttn and CRD: 
SS final=?#CRU ttni=1 ? #CRD j=1 SS score(i , j )  #CRUttn ?  #CRD Computing SS final results in a list of utterances with scores from the respective scoring methods  , Table 4 . Note that the absolute utterance scores are taken from the real data  , i . e . they have been normalized w . r . t . the conceptual representation for the whole dialogue  , and not for the dialogue fragment given in Table 1  . The rankings were produced for this specific example to make it more illustrative  . 
3.2.3 Extracting relevant utterances
In order to produce a summary of the dialogue , the utterances first have to be sorted numerically  , i . e . 
ranked on the basis of their scores , see Table 4 for the results of the ranking procedure . 2 Given a compression rate COMP with the range [1 , 100] , the number of utterances classified as relevant by an individual scoring method PN r is a function of the total number of utterances in the dialogue : PN r =  ( COMP/100 ) ? Number total . 3 Then , given a specific compression rate COMP , the top-ranked PN rutterances will be automatically classified as relevant  . ? Returning to the example in Table 1 , we obtain the summaries given in Table 5 . 
COMP Selected Utterances 20% I live in Houston.
Got three bedrooms , one bath.
35% I live in Houston.
Got three bedrooms , one bath.
Tell me about your home.
Well , it?s an older home.
Table 5: Summaries based on Resnik?s measure 2If two or more utterances get an equal score , they are ranked according to the order of their occurrence  . 
3Note that this number must be rounded to a natural number  . 
homehome sixties pier beam house bedrooms bath are a Houston bedrooms  3  . 8021 3 . 8021 0 2 . 5158 2 . 5158 3 . 8021 9 . 3157 5 . 8706 0 . 8287 0 . 8287 bath 3 . 8021 3 . 8021 0 2 . 5158 2 . 5158 3 . 8021 5 . 8706 10 . 7821 0 . 8287 0 . 8 287 Table 3: Concept matrix C for Utt7 from Table 1 based on Resnik?s measure
Number Utterance Resnik?s scoreRank
Utt1 Okay . ?8
Utt 2Tell me about your home .  1 . 4181 106 40937 23 Utt 3Well , it?s an older home .  1 . 4 181106409372   4   Utt4 It was made back in the early sixties .  0 . 55 18309 1499 572 17 Utt 5It?s a pier beam house .  1 . 18821772523631 6
Utt 6 Huh-uh . ?9
Utt 7Got three bedrooms , one bath 1 . 5 0689651387565   2   Utt8 and that just makes mescream .  ? 10
Utt 9That?s pretty tough . ?11
Utt10 What area do you live in ?1 . 25186984433606 5 Utt 11Ilive in Houston .  1 . 5 1301080520959   1 Table 4: Utterance scores based on Resnik?s measure
Tokens Utterances Turns
Total 348 30327 51852
Average 174 1.51 63.75 92.6
Table 6: Descriptive corpus statistics 4 Data The data used in the experiments are 20 randomly chosen Switchboard dialogues ( Greenberg ,  1996) . 
These dialogues contain two-sided telephone conversations among American speakers of at least  10 minutes duration . The callers were given a certain topic for discussion  . The recordings of spontaneous speech were , then , transcribed . Statistical data about the corpus , i . e . total numbers and averages for separate dialogues  , are given in Table 6 . Tokens are defined as running words and punctuation  . 
An utterance is a complete unit of speech spoken by a single speaker  , while a turn is a joint sequence of utterances produced by one speaker  . 
In the annotation experiments , we tested whether humans could reliably determine the utterances conveying the overall meaning of the dialogue  . Therefore , each utterance is assumed to be a markable , i . e . the expression to be annotated resulting in a total of  3275 markables in the corpus . Three annotators were instructed to select the most important utterances  . They were supposed to first read the dialogue und then to mark about  10% of all utterances in the dialogue as being relevant  . Then , we produced two kinds of Gold Standards from these data  . 
Gold Standard 1 included the utterances which were marked by all three annotators as being relevant  . 
Gold Standard 2 included the utterances which were selected by at least two annotators  . 
Table 7 shows the results of these experiments.
We present the absolute number of markables selected as relevant by separate annotators and in two Gold Standards  . Also , we indicate the percentage , given the total number of markables 3275 . As the table shows , Gold Standard 1 includes only 3 . 69% of all markables . Therefore , we used Gold Standard 2 in the evaluation reported in Section 5  . The Kappa coefficient for interannotator agreement varied from  0  . 1808 to 0 . 6057 for individual dialogues . 
An examination of the particular dialogue with the very low K apparate showed that this was one of the shortest ones  . It did not have a well-defined topical structure , resulting in a low agreement rate between annotators  . For the whole corpus , the Kappa coefficient yielded 0 . 4309 . While this is not a high agreement rate on a general scale  , it is comparable to what has been reported concerning the task of summarization and in particular dialogue summarization  . 
5 Evaluation 5 . 1 Evaluation Metrics and Baselines We reformulated the problem in terms of standard information retrieval evaluation metrics : Precision = PP /PN r  , Recall = PP/NP , and Fmeasure = 2 ? Prec ? Rec/(Prec+Rec) . PP is the number of cases where the individual scoring method and the Gold Standard agree  . PN r is computed according to the definition given in Section  3  . 
Annotator 1 Annotator 2 Annotator 3 Gold Standard 1 Gold Standard 2  ?  417   12  . 73% 350 10 . 69% 347 10 . 6% 121 3 . 69% 310 9 . 4 7% Table 7: Number of markables labeled as relevant NP is the total number of utterances marked as relevant in the Gold Standard  . For comparison , three baseline systems were implemented . The first system is the RANDOM baseline , where relevant utterances ( depending on the compression rate ) were selected by chance . The second baseline system is based on the TF*IDF scoring metric  . A large corpus is required to make this method fully powerful  . Therefore , we computed TF*IDF scores for every word on the basis of  2431 Switchboard dialogues ( ca .  19 . 3MB of ASCII text ) . Then , an average TF*IDF score for each utterance of the  20 dialogues in our corpus was computed by adding the individual scores for all words in the utterance and normalizing by the number of words  . The LEAD baseline is based on the intuition that the most important utterances tend to occur at the beginning of the discourse  . While this observation is true for the domain of news  , the LEAD baseline is not necessarily efficient for the genre of spontaneous dialogues  . 
However , given the Switchboard experimental data collection setup  , the dialogues usually directly start with the discussions of the topic  . This hypothesis was supported by evidence from our own annotation experiments  , too . 
5.2 Results
Experiments were performed using the semantic similarity package  V0  . 05 ( Pedersen , 2002) and WordNet 1 . 7 . 1 . We employed Gold Standard 2 ( see Section 4) . Three of the methods , namely res , lin , jcn , require the information content file ( ICF ) . 
A method for computing the information content of concepts from large corpora of text is given in Resnik  ( 1995 )  . ICF contains a list of synsets along with their part of speech and frequency count  . We compare the results obtained with 2 different ICFs : ? a WordNet-based ICF , provided at the time of the installation of the similarity package with precomputed frequency values on the basis of 
WordNet ( WDICF ); ? an ICF , generated specifically on the basis of 2431 Switchboard dialogues with the help of utilities distributed together with the similarity package  ( SWICF )  . 
Figures 1 and 2 indicate the performance of all methods in terms of Fmeasure  . The results of the 0 . 1 0 . 15 0 . 2 0 . 25 0 . 3 0 . 35 0 . 4 0 5 10 15 20 25 30 35 40
Fmeasure
Compression rate in % reslinj cn lesk

TF*ID Flch

Figure 1: Results based on WordNet ICF0 . 1 0 . 15 0 . 2 0 . 25 0 . 3 0 . 35 0 . 4 0 5 10 15 20 25 30 35 40
Fmeasure
Compression rate in % reslinj cn lesk

TF*ID Flch

Figure 2: Results based on Switchboard ICF semantic similarity methods making use of the information content file generally improve when the Switchboard-based ICF is used  . The improvements are especially significant for the jcn and lin measures  , while this does not seem to be the case for the resmeasure  ( depending on a specific compression rate )  . 
The summarization methods perform best for the compression rates in the interval  [20  , 30] . Given these rates and the Switchboard-based ICF , the competing methods display the following performance  ( in descending order ) : jcn , res , lin , lesk , lch , tf*idf , lead , random . For the default ICF the picture is slightly different : res  , jcn and lesk , lch , lin , tf*idf , lead , random ( see Table 8) . lch relying on WordNet structure only performs worse than the rest of similarity metrics incorporating some corpus evidence  . 
A direct comparison of our evaluation with alternative results  , e . g . , Zechner?s (2002) is problematic . 
Though Zechner?s results are based on Switchboard , too , he employs a different evaluation scheme . The evaluation is broken down to the word level . The results are compared with multiple human annotations instead of a Gold Standard  . 
5 . 3 Statistical Significance and Error Analysis For determining whether there is a significant difference between the summarization approaches pairwise  , we use a paired related ttest ( as the parent distribution is unknown )  . The null hypothesis states there is no difference between the two distributions  . 
On consulting the ttest tables , we obtain the significance values presented in Table  9  , given the compression rate 25%4 and the Switchboard ICF . These results indicate that there is no statistically significant difference in the performance between theres  , lin , j cn and lesk methods . However , all of them significantly outperform the LEAD , TF*IDF and
RANDOM baselines.
The maximum Recall of the semantic similarity -based summarization methods in the current implementation is limited to about  90%  , given COMP=100% . This means that if the system compiled a 100% ? summary ? , it would miss 10% of all utterances marked as relevant . The reason lies in the fact that the algorithm operates on the concepts created by mapping nouns to their WordNet senses  . Thus , the relevant utterances which do not have nouns on the surface  , but contain for example anaphorical expressions realized as pronouns  , are missed in the input . Resolving anaphorical expressions in the preprocessing stage may eliminate this error source  . 
6 Concluding Remarks
We introduced a new approach to spoken dialogue summarization  . Our approach combines statistical , i . e . corpus-based , and knowledge-based techniques . 
It utilizes the knowledge encoded in the noun part of WordNet and applies a set of semantic similarity metrics to dialogue summarization  . All semantic similarity-based summarization methods outperform RANDOM  , LEAD and TF*IDF baseline systems . In the following , we discuss some remaining challenges and future research  . 
More sophisticated data preprocessing . We plan 4Roughly speaking , the differences are most evident for compression rates between  20% and 30%  . 
to incorporate the preprocessing components used by Zechner  ( 2002 ) and evaluate their contribution to our task . Including an anaphora resolution component would also result in better Recall  . 
Automatic word sense disambiguation . Switchboard conversational speech is highly ambiguous  . 
Automatic disambiguation of noun senses to WordNet concepts is important in order to integrate our approach into real-life summarization systems  . 
Investigating other types of information in parallel  . 
A clear desideratum will be assessing the overall coherence of the discourse  , speaker info , turn type , information about nonnouns . 
Application to text and speech summarization . Our approach can be applied to written-to-be -spoken speech and text summarization  . It will be interesting to investigate whether conceptual structures of texts  ( the input to our system ) are comparable to the conceptual structures found in dialogues  . 
Readability , coherence , and usability of the summaries produced . A close examination of summaries based on human comprehension will be interesting  . It may be necessary to introduce filtering or other postprocessing techniques improving the quality of summaries  . 
Even without very sophisticated preprocessing of the dialogue data  , our algorithm yields promising results . It was evaluated on the Switchboard data , which is a challenging evaluation corpus . Our vision is to adopt the summarization approach presented here in a system used for the automatic production of meeting minutes  . 

This work has been funded by the Klaus Tschira Foundation  . We thank Christoph Zwirello for his valuable contributions  , the annotators Tatjana Medve deva , Vanessa Michelli and Iryna Zhmaka , and Ted Pederson and colleagues for their software  . 

Banerjee , S . & T . Pedersen (2003) . Extended gloss overlap as a measure of semantic relatedness  . In Proceedings of the 18th International Joint Conference on Artificial Intelligence  , Acapulco , Mexico , 9?15 August ,  2003 , pp .  805?810 . 
Greenberg , S .  (1996) . The Switchboard transcription project . In Proceedings of the Large Vocabulary Continuous Speech Recognition Summer Research Workshop  , Baltimore , Maryland , USA , April 1996 . 
Jiang , J . J . & D . W . Conrath (1997) . Semantic similarity based on corpus statistics and lexical taxonomy  . In Proceedings of the 10th International Conference on Research in Computational Linguistics  ( ROCLING )  , pp .  19?33 . Tapei , Taiwan . 
RANDOMLE ADT F*IDF Res LinJcn Lesk Lch
Precision 10% WDICF . 08563  . 23242  . 21101  . 25076 SWICF . 07645  . 18654  . 17125  . 23853  . 22936  . 26606  . 22936  . 25076 20% WDICF . 1026  . 23933  . 23018  . 24390 SWICF . 07963  . 16159  . 19512  . 23780  . 24085  . 24695  . 23780  . 2378030% WDICF . 10429  . 22380  . 21261  . 21668 SWICF . 09407  . 14140  . 17192  . 22075  . 22482  . 23093  . 22584  . 21567 Recall 10% WDICF . 09032  . 24516  . 22258  . 26452 SWICF . 08065  . 19677  . 18065  . 25161  . 24194  . 28065  . 24194  . 2645220% WDICF . 21613  . 50645  . 48710  . 516 13 SWICF . 16774  . 34194  . 41290  . 50323  . 50968  . 52258  . 50323  . 5032330% WDICF . 32903  . 70968  . 67419  . 687 10 SWICF . 29677  . 44839  . 54516  . 70000  . 71290  . 73226  . 71613  . 68387 Fmeasure 10% WDICF . 08791  . 23862  . 21664  . 25746 SWICF . 07849  . 19152  . 17582  . 24490  . 23548  . 27316  . 23548  . 2574620% WDICF . 13915  . 32505  . 31263  . 33126 SWICF . 108  . 21946  . 26501  . 32298  . 32712  . 33540  . 32298  . 3229830% WDICF . 15839  . 34029  . 32328  . 32947 SWICF . 14286  . 21500  . 26141  . 33565  . 34184  . 35112  . 34339  . 32792 Table 8: Precision , Recall and Fmeasure for 10% ,   20% and 30% and two ICFs reslinj cn lesk lead tf*idfl ch random res X X X  p>0  . 05 p > 0 . 05 p > 0 . 05 p < 0 . 01 p < 0 . 01 p > 0 . 05 p < 0 . 01 linXXX p > 0 . 05 p > 0 . 05 p < 0 . 01 p < 0 . 05 p > 0 . 05 p < 0 . 01 j cnXXX p > 0 . 05 p < 0 . 01 p < 0 . 01 p < 0 . 05 p < 0 . 01 lesk X XX p < 0 . 01 p < 0 . 05 p > 0 . 05 p < 0 . 01 lead X X X p > 0 . 05 p < 0 . 01 p < 0 . 01 tf*idf X XX p > 0 . 05 p < 0 . 01 l ch X X X p < 0 . 0 1 random X X X Table 9: Statistical significance of results at COMP=25% and based on SWICF Kupiec , J . , J . O . Pedersen & F . Chen (1995) . A trainable document summarizer . In Research and Development in Information Retrieval  , pp .  68?73 . 
Leacock , C . & M . Chodorow (1998) . Combining local context and WordNet similarity for word sense identification  . In C . Fellbaum ( Ed . ), WordNet : An Electronic Lexical Database , pp .  265?283 . Cambridge:
MIT Press.
Lin , D .  (1998) . An information-theoretic definition of similarity  . In Proceedings of the 15th International Conference on Machine Learning , pp .  296?304 . Morgan Kaufmann , San Francisco , CA . 
Mani , I . & M . T . Maybury ( Eds . ) (1999) . Advances in Automatic Text Summarization . Cambridge/MA , Lon-don/England : MIT Press . 
Marcu , D .  (2000) . The Theory and Practice of Discourse Parsing and Summarization  . Cambridge/MA :
The MIT Press.
Pedersen , T .  (2002) . Semantic Similarity Package . 

Radev , D . R . , S . Teufel , H . Saggion , W . Lam , J . Blitzer , H . Qi , A . Celebi , D . Liu & E . Drabek (2003) . Evaluation challenges in largescale document summarization  . In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics  , Sapporo , 
Japan , 7?12 July 2003, pp . 375?382.
Reithinger , N . , M . Kipp , R . Engel & J . Alexandersson (2000) . Summarizing multilingual spoken negotiation dialogues  . In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics  , Hong Kong , 1?8 August 2000 , pp .  310?317 . 
Resnik , P .  (1995) . Using information content to evaluate semantic similarity in a taxonomy  . In Proceedings of the 14th International Joint Conference on Artificial Intelligence  , Montre?al , Canada ,  1995 , Vol . 1, pp .  448? 453 . 
Teufel , S . & M . Moens (2002) . Summarizing scientific articles : Experiments with relevance and rhetorical status  . Computational Linguistics , 28(4):409?445 . 
Zechner , K .  (2002) . Automatic summarization of open-domain multiparty dialogues in diverse genres  . Computational Linguistics , 28(4):447?485 . 
