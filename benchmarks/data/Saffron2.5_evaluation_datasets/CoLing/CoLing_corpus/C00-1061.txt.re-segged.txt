English-to-Korean Transliteration using Multiple Unbounded 
Overlapping Phoneme Chunks
In-Ho Kang and Gil Chang Kim
Department of Computer Science
Korea Advanced Institute of Science and Technology 
Abstract
We present in this paper the method of
English-to-Korean ( E-K ) transliteration and back-transliteration . In Korean technical documents , many English words are transliterated into Korean words in w~rious tbrms in diverse ways  . As English words and Korean transliterations are usually technical terms and proper nouns  , it ; is hard to find a transliteration and its variations in a dictionary  . The retbre an automatic transliteration system is needed to find the transliterations of English words without manual intervention  . 
3? . o explain EK transliteration t ) he nomen a , we use phoneme chunks that do not have a length limit  . By a I ) plying phoneme chunks , we combine different length intbrmation with easy  . The EK transliteration method h~m three stet)s . In the first , we make at ) hone me network that shows all possible transliterations of the given word  . In the second step , we apply t ) hone me chunks , extracted fl ' om training data , to calculate the reliability of each possible transliteration  . Then we obtain probable transliterations of the given English word  . 
1 Introduction
In Korean technical documents , many English words are used in their original forms  . But sometimes they are transliterated into Korean in different forms  . Ex .  1 ,   2 show the examples of w ~ rious transliterations in KTSET  2  . 0 ( Park et al , 1996) . 
(1) data ( a ) l : ~\] o\]>\]-(teyitha)\[1 , 033\] 1  ( b ) r ~ lo \] >\] ( teyithe ) \[527\]1 the frequency in KTSET ( 2 ) digital ( a ) ~\] x\]~- ( ticit hul )  \[254\]  ( b ) qN~- ( tichith M ) \[7\]  ( c ) ~1 z \] ~ ( ticit hel )   \[6\] These various transliterations are not negligible t br natural angnage processing  , especially ill information retrieval . Because same words are treated as different ones , the calculation based ( mtile frequency of word would produce misleading results  . An experiment shows that the effectiveness of infbrmation retrieval increases when various tbrms including English words are treated eqnivMently  ( Jeong et al ,  1997) . 
We may use a dictionary , to tind a correct transliter at km and its variations  . But it is not fhasible because transliterated words are usually technical terms and proper nouns that have rich productivity  . Therefore an automatic transliteration system is needed to find transliterations without manual intervention  . 
There have been some studies on EK transliteration  . They tried to explain transliteration as tfl lone me-lmr-phoneme or alphabet-per-phonenm classification problem  . They restricted the information length to two or three units be ibre and behind an input unit  . Intact , ninny linguistic phenomena involved in the EK transliteration are expressed in terms of units that exceed a phoneme and an alphabet  . For example , ' a'in'ace ' is transliterated into % 11?l ( ey0"lintin ' acetic ' , " ot ( eft and ill ' acetone ' , " O(a ) " . If we restrict the information length to two alphabets  , then we cmmot explain these phenomena . Three word sge ~ the same result ~() r ~ a ' . 
(3) a ceotlo\]~(eyisu ) (4) acetic cq . qvI(esithik ) In this t ) at ) er , wet ) rot ) ose/ ; he EK transliter-al ; i on model t ) ased onl ) hone me chunks that do not have a length limit and can explain transliter  ; ~ tionl ) he nolnem , in SOllle degree of reliability . Notaalphal ) et-per-all ) habet but a chunk-i ) er-chunk classification 1 ) roblem . 
This paper is organized as t bllows . In section 2, we survey an EK transliteration . \] 111 section 3 , we propose , phonenm chunks 1 ) a sexl transliteration and back-transliteration . In Seel ; ion 4 , the lesults of ext ) erilnents are presented . Finally , the con(:hlsion follows in section 5 . 
2 Eng l i sh - to - Korean t rans l i te ra t ion EK transliteration models are  ( : lassitied in two methods : thel ) ivot method and the direct method . In the pivot method , transliteration is done in two steps : ( : onverting English words ill ; ( ) pronunciation symbols and then ( : onverting these symbols into Kore~mwor ( ts by using the Korean stm ~ ( tard conversion rule . In the direct method , English words are directly converted to Korean words without interlnediate stct  ) s . An exl ) eriment shows that the direct method is better than the pivot method in t in  ( lingw triations of a transliteration ( Lee and ( ~ hoi ,  1998) . Statis-ti (: alinformation , neural network and de ( : ision tree were used to imt ) lelneld ; the direct method . 
2 . 1 S ta t i s t iea l T rans l i te ra t ion method An English word is divided into phoneme sequence or alphal  ) et sequence as ( 21~ ( 22~ . . . ~ en . 
Then a corresponding Korean word is rel ) -resented ask l , k2 ,  .   .   . , t~:n . If n corresponding Korean character ( hi ) does not exist , we fill the blank with '-' . For example , an English word " dressing " and a Korean word "> N\]zg  ( tuley sing ) " are represented as Fig .  1 . The ut ) per one in Fig .   1 is divided into an English phonemer efit and the lower one is divided into an alphabet mlit  . 
dressh , g : ---~ ~1~ d/~+r/a+e/ 4l + ss/x+i/I+ng/old/=---+r/e + e/41 + s/~ . + s /-+ i/I + n/o + g /- Figure 1: An EK transliteration exault ) le Thet ) roblem in statistical transliteration reel ; hod is to lind out the . lllOSt probable transliteration fbragiven word . Let p ( K ) be the 1 ) tel ) -ability of a Korean word K , then , for a given English word E , the transliteration probal ) ility of a word KC all be written as P ( KIE )  . By using the Bayes'theorem , we can rewrite the transliteration i ) rol ) lemas follows: , a ' . qmazp(KIE ) = a, . .q ma :,: p(K)p(~IK ) (1)
KK
With the Markov Independent Assulni ) tion , weapl ) roximate p ( K ) and p ( EIK ) as t bllows : 7~ i=2 i=1 As we do not know the t ) rommciation of a given word , we consider all possible tfllonelne sequences , l ? or exanlple , ' data ' hast bllowing possible t ) holmme sequences , ' d-a-t-a , d-at-a , data ,   .   .   .  '  . 
As the history length is lengthened , we . can get more discrimination . But long history in-fornlation c~mses a datasl ) arseness prol ) lenl . In order to solve , a Sl ) arseness t ) rol ) len ~, Ma . ximmn Entropy Model , Backoff , and Linear intert ) ola-tion methods are used . They combine different st ~ tistical estimators . ( Tae-il Kim ,  2000 ) useut ) to five phonemes in feature finlction ( Berger et a , l . , 1996) . Nine % ature flmctions are combined with Maximum Entrot  ) y Method . 
2 . 2 Neura l Network and Dec is ion Tree Methods based  011 neural network and decision tree detenninistically decide a Korean character for a given English input  . These methods take two or three alphabet sort ) honemes as an input and generate a Korean alphabet or phoneme as an output  . ( Jung- . \] ae Kim , 1 . 999 ) proposed a neural network method that uses two surrom ~ dingt  ) holm mes as an in tmt . 
( Kang ,  1999 ) t ) roposed a decision tree method that uses six surrounding alphabets  . If all in l ) ut does not cover the phenomena of prol ) er transliterations , we cammt gel ; a correct answer . 

Even though we use combining methods to solve the data sparseness problem  , the increase of an intbrmation length would double the complexity and the time cost of a problem  . It is not easy to increase the intbrmation length . 
To avoid these difficulties , previous studies does not use previous outputs ( ki_z )  . But it loses good information of target language . 
Our proposed method is based on the direct method to extract the transliteration and its variations  . Unlike other methods that determine a certain input unit's output with history information  , we increase the reliability of a certain transliteration  , with known EK transliteration t ) he nonmna ( phoneme chunks )  . 
3 Trans l i te ra t ion us ing Mul t ip le unbounded overlapping phoneme chunks For unknown data  , we can estimate a Korean transliteration ti'onl handwritten rules  . We can also predict a Korean transliteration with experimental intbrmation  . With known English and Korean transliteration pairs  , we can assume possible transliterations without linguistic knowledge  . For example , ' scalar " has common part with's calc:~sqlN ( suhhcyil ) ' , ' casino J\[xl(t:hacino ) ' , ' t:o ala:e-l-&:ho alla ) ' , and ' car : ~ l-(kh . a )' ( Fig .  2) . We can assume possible transliteration with these words and their transliterations  . From'scale ' and its transliteration l'-~\]~ ( sukheyil )  , the'sc'in'scalar'can be transliterated as ' ~:- J  ( sukh ) ' . From a ' casino ' example , the ' c' has n lore evidence that can be transliterated as ' v  ( kh ) ' . We assume that we can get a correct Korean transliteration  , if we get useful exper in lental information and their proper weight that represents reliability  . 
3 . 1 The a l ignment of an Engl ish word with a Korean word We can align an English word with its transliteration in alphabet unit or in phoneme unit  . 
Korean vowels are usually aligned with English vowels and Korean consonants are aligned with English consonants  . For example , a Korean consonant ,  '1~  ( p ) ' can be aligned with English consonants ' b ' , ' p ' , and ' v' . With this heuristic we can align an English word with its transliteration in an alphabet unit and at  ) hone Ine unit with the accuracy of 99 . 4% ( Kang , 1999) . 
sca1ars ca1ek oa1 no
IL ?
Car

Figure 2: the transliteration of ' scalar : ~~\]- ( sukhalla ) ' 3 . 2 Ext ract ion o f Phoneme Chunks From aligned training data  , we extract phoneme clumks . Weemmw . rate all possible subsets of the given English -Korean aligned pair  . During enumerating subsets , we add start and end position infbrmation . From an aligned data " dressing " and " ~ et lN ( tuley sing ) " , we can get subsets as Table 12 . 
Table 1: The extraction of phoneme chunks
Context Output dr .  # ? , _ ) dr ' cd /= -- ( d ) + r/~- ( r ' ) + e/ql ( ey ) The context stands t bragiven English alphabets , and the output stands for its transliteration . We assign a proper weight to each phoneme chunk with Equation  4  . 
C ( output ) wei . q h , t ( contcxt : output ) -C ( contcxt )   ( 4 ) C ( x ) mean stile frequency of z in training data . 
Equation 4 shows that the ambiguous phenomenon gets the less evidence  . The clnmk weight is transmitted to each phoneme symbol  . 
To compensate for the length of phoneme , we multiply the length of phoneme to the weight of the phoneme chunk  ( Fig .  3) . 
2@ means the start and end position of a word 4 ,  4 ,  4 ,  4 ,  4 , o  ~ 2a aa 2o ~\] ? igure 3: The weight of a clmnk and at ) honeme This chunk weight does not mean the . relia-t ) ility of a given transliteration i ) henomenon . 
We know real reliM ) itity , after all overlapping phonenm chunks are applied . The chunk that has some common part with other chunks gives a context information to them  . Therefore a chunk is not only an in t ) ut unit but also a means to ( -Mculate the reliability of other dmnks . 
We also e , xl ; ra (: t the connection information.
From Migned training ( b : ~ ta , we obtain M1 possible combinations of Korem~characters and English chara  ( : ters . With this commction in-tbrmation , we exclude iml ) ossit ) h ; connections of Korean characters ~ md English phon  (  ; nte sequences . We can gel ; t ; he following ( : ommction information from " dressing " examph ' . (~12 fl ) le2) . 
2 ? fl ) le 2: Conne(:tionInformation
Enffli . sh , Kore . a ', . 1\]lql't\[righ,tIIZ(?ltl , . ,: . , 1,, t/a , .  ( ,9 , (' .  09 3 . 3 A Trans l i te ra t ion Network For a given word  , we get alt ) ossil ) h~t ) honemes and make a Korean transliteration etwork . 
Each node in an et ; work has an Englisht ) honent ( ; and a (' or rcspondillg Korean character . Nodes are comm(:ted with sequence order . For example , ' scalar ' has the Kore , an transliteration et work as Fig .  4 . In this network , we dis ( ' ommct some no ( les with extracted ( : onne ( ' tion in for nla-tion . 
After drawing the Koreantr~msl iteration network , we apply all possible phone , me , chunks to the . network . Each node increases its own weight with the weight of t  ) hone me symbol in a phoneme chunks ( Fig .  5) . By overlapping the weight , nodes in the longer clmnksget ; more evidence . Then we get the best t ) at h that has the Figure 4: Korean Transliteration Network for'scalar' highest sum of weights  , with the Viterbi algo-ril , hm . The Tree . -Trcll is Mgorithm is used to gel ; the variations ( Soong and Huang ,  1991) . 
Figure 5: Weight apt flication examt ) le 4 E-K back-transliteration EK back transliteration is a more difficult prot  ) -lemt htntF , -Ktrmlsliteration . During the EK trm~slit ; cra ; ion ~ ( lifli ' xent alphabets are treated cquiw ~ h' . ntly . \], ~) rexmnph ' . , ~ f , t/mM~v ~ b'spectively and the long sound and the short str and are also treated equivalently  . The refim ' , the number of possible English phone , rues per a Korean character is bigger than the number of Korean characters per an English phoneme  . 
The ambiguity is increased . In EK back-transliteration , Korean1) honemes and English phoneme , sswitch their roles . Just switching the position . A Korean wordix Migned with an English word in a phoneme unit or a character refit  ( Fig .  6) . 
\[~---~ l~:dressing\]
F/d+--/-+~/r + 41/e + ~-/ ss+I/i + o/n 9 , ~ Figure 6: EK back-transliteration examt ) le Experiments were done in two points of view : the accuracy test and the variation coverage test  . 
5.1 Test Sets
We use two datasets for an accuracy test . Test Set I is consists of 1 . , 650 English and Korean word pairs that aligned in a phoneme unit  . It was made by ( Lee and Choi , 1998) and tested by many methods . To compare our method with other methods , we use this dataset . We use same training data (1 , 500 words ) and test data (150 words ) . Test Set II is consists of 7 , 185 English and Korean word paii's . We use Test Set H to show the relation between the size of training data and the accuracy  . We use 90% of total size as training data and 10% as test data . For a variation coverage test , we use TestSet III that is extracted from KTSET  2  . 0 . Test Set HI is consists of 2 , 391 English words and their transliterations . An English word has 1 . 14 various transliterations in average . 
5.2 Evaluation functions
Accuracy was measured by the percentage of the number of correct transliterations divided by the number of generated transliterations  . We (: all it as word accuracy ( W . A . ) . We use one more measure , called character accuracy ( C . A . ) that measures the character edit distance between a correct word and a generated word  . 
no . of correct words
W.A . = no.o.f . qenerated words (5)
C . A . = L ( 6 ) where L is the length of the original string , and i , d , mids are the number of insertion , deletion and substitution respectively . If the dividend is negative ( when L < ( i+d+s ) ) , we consider it as zero ( Hall and Dowling ,  1980) . 
For the real usage test , we used variation coverage ( V . C . ) that considers various usages . We evaluated both t br the term frequency ( tf ) and document frequency ( dJ )  , where tf is the number of term appearance in the documents and df is the number of documents that contain the term  . 
If we set the usage tf(or d . / ) of the transliterations to 1 t breach transliteration , we can calculate the transliteration coverage tbr the unique word types  , single . frequency ( . sf) . 
V . C . = if , df , sf of found words (7) t . f , 4f , < f of , se do , ' d s 5 . 3 Accuracy tests We compare our result \[ PCa , PUp \] a with the simple statistical intbrmation based model  ( Lee and Choi , 1998)\[ST\] , the Maxin mmEntropy based model ( Tae-il Kim , 2000)\[MEM\] , the Neural Network model ( Jung-Jae Kim ,  1999 ) INN\] and the Decision % ' eebased model ( Kang , 1999)\[DT\] . Table 3 shows the result of EK transliteration and back -transliteration test with Test  , get L Table 3: C . A . and W . A . with Test Set I
EK trans.
method C . A . IW . A.
ST 69.3% 40.7% 4
MEM 72.3% 43.3%
NN 79.0% 35.1%
DT 78.1% 37.6%
Pep 86.5% 55.3%
PCa 85.3% 46.7%
EK backtrans.
C.A.\[W.A.
60 . 5% 77 . 1% 31 . 0% 81 . 4% 34 . 7% 79 . 3% 32 . 6% method with the size of training data , Test Set II . We compare our result with the decision tree based method  . 
~----~ I~C'A'PC ? Ii--~-w . ADTI+W . A . POp ~ W , A . BC a
JJJ ~ Ji 1000   2000   3000   4000   5000   6000 Figure 7: EK transliteration results with Test
Set HaPC stands for phoneme chunks based method and a and b stands for aligned by an alphabet unit and a  1  ) honeme unit respectively 4with   20 higher rank results 80-5O   20 t 1000   2000   3000   4000   5000   6000 "--?-- C . A,DT---U--C . A . PCp
C.A.PCaI !-- x-~A . Dri
I .__144A.POp\]
I . ~ l ~ W , A . PO a
Figure 8: EK back-transliteration results with
Test Set H
With 7' c , s'tSc , tH~we(:m~gett ; 15( ; fi ) llowing result ( Table ,  4) . 
Tabled : C.A . and W.A . with the Test Set H
EKtr~ms . EK backtr~ms.
method C . A.14 LA.C.A.IW.A.
PUp\[~9.5% 57.2% 84.9% 40.9%
PCa\[19 o . 6% 58 . 3% s 4 . 8% 4(/ . 8% 5 . 4 Variation coverage tests To ( : oml ) ~reour result ( PCp ) with ( Lee and ( ) hoi ,  1998) , we tr~finc dourl nethods with the training data of Test Set LIn ST  , ( Leemid Choi , 1998) use 20 high rank results , but wejt lst ll Se5 results . TM ) le5 shows the ( : overage of ore : i ) rol ) osed me . thod . 
Table 5: w ~ riatione over ~ ge with Tc . ~ tSetIII method tfd . f , ~ f
ST 76.0% 73.9% 47.1%
PCp 84.0% 84.0% 64.0%
Fig . 9 shows the increase of ( : overage with the number of outputs . 
5.5 Discussion
We summarize the , information length ~ md the kind of infonnation ( Tnble 6 )  . The results of experimenLs and information usage show the ft MEM combines w ~ rious in formal  ; i on better than DT and NN . ST does not list & previous in lmt ( el-l ) but use ~ previous output ( t , :i_~ ) to calculate the current outlml?s probability like  5O !-m-elf ,   ,   , ~ sf 1 2 3 4 5 6 7 8 9 10
Figure 9: The 17 . C . result q ~ , l ) le 6: Int brmation Usage previous output
ST20 Y
MEM 22N
NN\]1N
DT 33N
PCY
Part-ofSt ) eee hrl'~gging probleln . But ST gets the lowest aecm'acy . It means that surrmmding alphal ) ei ; s give more informed ; ionthant ) revious outlmL . In other words , EK trmls lii ; e . ration is not the all ) h ~ bet-per-alphabet or phonenle-per-t ) hone me ( : lassific ~ tion problem . A previous out I ) ut does not give , enough information for cllrrent ltnit's dismnbiguat  ; ion . An input mill midanOUtlm tunits houhtbe exl :ende  ( t . EK transliteration is a ( : hunk-l ) er-chunk classification prot ) lenL Werestri ( : t the length of infiwm ~ tion , to see the influence of ' phoneme-chunk size . Pig . 10 shows the results . 
i9oi ~ ~ ~_~", ~
F706 oi 5040/~# . ~---- C . A . 7 bstSotf30//I-~-c . A . z  ~ . ~ so ,, I20 t ~/ I~WA To, . tSet/i
Io-?-~iL ;, L ,,! x ~
Figure 10: the result of ~ length limit test get the higher C  . A . and W . A . than other methods . It means previous outputs give good information and our chunk-based nmthod is a good combining method  . It also suggests that we can restric the max size of chunk in a permissible size  . 
PC agets a higher accuracy than PCp . It is clue to the number of possible phoneme sequences  . A transliteration network that consists of phonemenn it has more nodes than a transliteration network that consists of alphabet unit  . With small training data , despite of the loss due to the phoneme sequences ambiguity a phoneme gives more intbrmation than an alphabet  . When the infbrmation is enough,
PCa outpert brms Pep.
6 Conclusions
We propose the method of English-to-Korean transliteration and back-transliteration with multiple mfl  ) ounded overlapping phoneme chunks . We showed that EK transliteration and back -transliteration are not at  ) hone me-per-phoneme and alphabet-per-alphabet classification problem  . So we use phoneme chunks that do not have a length limit and can explain EK transliteration phenomena  . 
We get the reliability of a given transliteration phenomenon by applying overlapt  ) ing phoneme chunks . Our method is simple and does not need a complex combining method t brw  , rious length of information . The change of an intbrmation length does not affect the internal representation f the problem  . Our chunk-based method can be used to other classification problems and can give a simple combining method  . 

Tae-il Kim .  2000 . English to Korean transliteration model using maxinmm entropy model for cross language information retrieval  . Mas-ter's thesis , Seogang University ( in Korean ) . 
Kil Soonaeong , Sllng Hyun Myaeng , Jae Sung Lee , and Key-Sun Choi .  1999 . Automatic identification and back-transliteration of foreign words t brinformation retrieval  , b ~: for-mation Processing and Management . 
Key-Sun Choi Jung-Jae Kim , Jae Sung Lee.
1999 . Pronunciation unit based automatic English-Korean transliteration model using neural network  . In Pwceedings of Korea Cognitive Science Association  ( in Korean )  . 
Byung-Ju Kang . 1999. Automatic Korean-
English back-transliteration . IP wecedings of th , c11th , Conference on Iiangul and Korean Language Information Prvcessing  ( in Ko-

Jae Sung Lee and Key-Sun Choi .  1998 . English to Korean statistical transliteration for information retrieval  . Computer PT vcess in 9 of
Oriental Languages.
K . Jeong , Y . Kwon , and S . H . Myaeng . 1997.
The effect of a proper handling of foreign and English words in retrieving Korean text  . In Proceedings of the 2nd Irdernational Workshop on lrt for rnation Retrieval with Asian 

K . Knight and J . Graehl .  1997 . Machine transliteration . In Proceedings o . f the 35th Annual Meeting of the Association J ' or Computational Linguistics  . 
Adam L . Berger , Stephen A . Della Pietra , and Vincent J . Della Pietra .  1996 . A maximum entro I ) y approach to natural language processing . Computational Linguistics . 
Y.C . Park , K . Choi , J . Kim , and Y . Kim.
1996. Development of the data collection ver.
2 . 0ktset2 . 0 tbrKore an intbrmation retrieval studies . In Artificial Intelligence Spring Conference . Korea Intbrmation Science Society ( in Korean ) . 
Frank K . Soong and Eng-Fong Huang . 1991.
A tree-trellisbased Nst search for tinding then best sentence hypotheses in e on tim musspeech recognition  . In IEEE International Conference on Acoustic Speech and Signal 
Pwcessing , pages 546-549.
P . Hall and G . Dowling .  1980 . Approximate string matching . Computing Surveys . 

