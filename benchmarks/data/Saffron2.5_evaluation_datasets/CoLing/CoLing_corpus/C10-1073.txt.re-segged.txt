Proceedings of the 23rd International Conference on Computational Linguistics ( Coling 2010), pages 644?652,
Beijing , August 2010
Improving Corpus Comparability for Bilingual Lexicon Extraction from
Comparable Corpora
Bo Li , Eric Gaussier
Laboratoire d?Informatique de Grenoble ( LIG)
Universite ? de Grenoble
firstname.lastname@imag.fr
Abstract
Previous work on bilingual lexicon extraction from comparable corpora aimed at finding a good representation for the usage patterns of source and target words and at comparing these patterns efficiently . In this paper , we try to work it out in another way : improving the quality of the comparable corpus from which the bilingual lexicon has to be extracted . To do so , we propose a measure of comparability and a strategy to improve the quality of a given corpus through an iterative construction process . Our approach , being general , can be used with any existing bilingual lexicon extraction method . We show here that it leads to a significant improvement over standard bilingual lexicon extraction methods.
1 Introduction
Bilingual dictionaries are an essential resource in many multilingual natural language processing ( NLP ) tasks such as machine translation ( Och and Ney , 2003) and cross-language information retrieval ( CLIR ) ( Ballesteros and Croft , 1997).
Handcoded dictionaries are of high quality , but expensive to build and researchers have tried , since the end of the 1980s , to automatically extract bilingual lexicons from parallel corpora ( see ( Chen , 1993; Kay and Ro?scheisen , 1993; Melamed , 1997a ; Melamed , 1997b ) for early work ). Parallel corpora are however difficult to get at in several domains , and the majority of bilingual collections are comparable and not parallel . Due to their low cost of acquisition , several researchers have tried to exploit such comparable corpora for bilingual lexicon extraction ( Fung and McKeown , 1997; Fung and Yee , 1998; Rapp , 1999; De?jean et al , 2002; Gaussier et al , 2004; Robitaille et al , 2006; Morin et al , 2007; Yu and Tsujii , 2009). The notion of comparability is however a loose one , and comparable corpora range from lowly comparable ones to highly comparable ones and parallel ones . For datadriven NLP techniques , using better corpora often leads to better results , a fact which should be true for the task of bilingual lexicon extraction . This point has largely been ignored in previous work on the subject . In this paper , we develop a well-founded strategy to improve the quality of a comparable corpus , so as to improve in turn the quality of the bilingual lexicon extracted . To do so , we first propose a measure of comparability which we then use in a method to improve the quality of the existing corpus.
The remainder of the paper is organized as follows : Section 2 introduces the experimental materials used for the different evaluations ; comparability measures are then presented and evaluated in Section 3; in Section 4, we detail and evaluate a strategy to improve the quality of a given corpus while preserving its vocabulary ; the method used for bilingual lexicon extraction is then described and evaluated in Section 5. Section 6 is then devoted to a discussion , prior to the conclusion given in Section 7.
2 Experimental Materials
For the experiments reported here , several corpora were used : the parallel English-French Europarl corpus ( Koehn , 2005), the TREC ( AP , English ) and the corpora used in the multilingual track of CLEF ( http://www.clef-campaign.org ) which includes the Los Angeles Times ( LAT94, English ), Glasgow Herald ( GH95, English ), Le Monde ( MON94, French ), SDA French 94 ( SDA94, French ) and SDA French 95 ( SDA95, French ). In addition to these existing corpora , two monolingual corpora from the Wikipedia dump1 were built . For English , all the articles below the root category Society with a depth less than 42 were retained . For French , all the articles with a depth less than 7 below the category Socie?te ? are extracted . As a result , the English corpus Wiki-En consists of 367,918 documents and the French one Wiki-Fr consists of 378,297 documents.
The bilingual dictionary used in our experiments is constructed from an online dictionary.
It consists of 33,372 distinct English words and 27,733 distinct French words , which constitutes 75,845 translation pairs . Standard preprocessing steps : tokenization , POStagging and lemmatization are performed on all the linguistic resources.
We will directly work on lemmatized forms of content words ( nouns , verbs , adjectives , adverbs).
3 Measuring Comparability
As far as we can tell , there are no practical measures with which we can judge the degree of comparability of a bilingual corpus . In this paper , we propose a comparability measure based on the expectation of finding the translation for each word in the corpus . The measure is light-weighted and does not depend on complex resources like the machine translation system . For convenience , the following discussions will be made in the context of the English-French comparable corpus.
3.1 The Comparability Measure
For the comparable corpus C , if we consider the translation process from the English part Ce to the 1The Wikipedia dump files can be downloaded at http://download.wikimedia.org . In this paper , we use the English dump file on July 13, 2009 and the French dump file on
July 7, 2009.
2There are several cycles in the category tree of Wikipedia . It is thus necessary to define a threshold on the depth to make the iterative process feasible.
French part Cf , a comparability measure Mef can be defined on the basis of the expectation of finding , for each English word we in the vocabulary Cve of Ce , its translation in the vocabulary Cvf of Cf .
Let ? be a function indicating whether a translation from the translation set Tw of w is found in the vocabulary Cv of a corpus C , i.e .: ?( w , Cv ) = { 1 iff Tw ? Cv 6= ? 0 else
Mef is then defined as:
Mef ( Ce , Cf ) = E(?(w , Cvf )| w ? Cve ) = ? w?Cve ?( w , Cvf ) ? Pr(w ? Cve )? ?? ?
Aw = | C v e | | Cve ? Dve | ? w?Cve?Dve
Aw where Dve is the English part of a given , independent bilingual dictionaryD , and where the last equality is based on the fact that , the comparable corpus and the bilingual dictionary being independent of one another , the probability of finding the translation in Cvf of a word w is the same for w is in Cve ? Dve and in Cve\Dve 3. Furthermore , the presence of common words suggests that one should rely on a presence/absence criterion rather than on the number of occurrences to avoid a bias towards common words . Given the natural language text , our evaluation will show that the simple presence/absence criterion can perform very well . This leads to Pr(w ? Cve ) = 1/|Cve |, and finally to:
Mef ( Ce , Cf ) = ? w?Cve?Dve ?( w , Cvf )
This formula shows that Mef is actually the proportion of English words translated in the French part of the comparable corpus . Similarly , the counterpart of Mef , Mfe , is defined as:
Mfe(Ce , Cf ) = ? w?Cvf?Dvf ?( w , Cve ) 3The fact can be reliable only when a substantial part of the corpus vocabulary is covered by the dictionary . Fortunately , the constraint is satisfied in most applications where the common but not the specialized corpora like the medical corpora are involved.
645 and measures the proportion of French words in Cvf translated in the English part of the comparable corpus . A symmetric version of these measures is obtained by considering the proportion of the words ( both English and French ) for which a translation can be found in the corpus:
M(Ce , Cf ) = ? w?Cve?Dve ?( w , C v f ) + ? w?Cvf?Dvf ?( w , C v e ) | Cve ? Dve |+ | Cvf ? Dvf | We now present an evaluation of these measures on artificial test corpora.
3.2 Validation
In order to test the comparability measures , we developed goldstandard comparability scores from the Europarl and AP corpora . We start from the parallel corpus , Europarl , of which we degrade the comparability by gradually importing some documents from either Europarl or AP . Three groups ( Ga , Gb , Gc ) of comparable corpora are built in this fashion . Each group consists of test corpora with a goldstandard comparability ranging , arbitrarily , from 0 to 1 and corresponding to the proportion of documents in ? parallel ? translation . The first group Ga is built from Europarl only . First , the Europarl corpus is split into 10 equal parts , leading to 10 parallel corpora ( P1, P2, . . . , P10) with a goldstandard comparability arbitrarily set to 1. Then for each parallel corpus , e.g.
Pi , we replace a certain proportion p of the English part with documents of the same size from another parallel corpus Pj(j 6= i ), producing the new corpus P ? i with less comparability which is the goldstandard comparability 1 ? p . For each Pi , as p increases , we obtain several comparable corpora with a decreasing goldstandard comparability score . All the Pi and their descendant corpora constitute the group Ga . The only difference betweenGb andGa is that , inGb , the replacement in Pi is done with documents from the AP corpus and not from Europarl . In Gc , we start with 10 final , comparable corpora P ? i from Ga . These corpora have a goldstandard comparability of 0 in Ga , and of 1 in Gc . Then each P ? i is further degraded by replacing certain portions with documents from the AP corpus.
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.83 0.84 0.85 0.86 0.87 0.88
M
Goldstandard comparability scores
Scores from the comparability metric
Figure 1: Evolution of M wrt goldstandard on the corpus group Gc ( x-axis : goldstandard scores , y-axis : M scores ) We then computed , for each comparable corpus in each group , its comparability according to one of the comparability measures . Figure 1 plots the measure M for ten comparable corpora and their descendants from Gc , according to their goldstandard comparability scores . As one can note , the measure M is able to capture almost all the differences in comparability and is strongly correlated with the goldstandard . The correlation between the different measures and the goldstandard is finally computed with Pearson correlation coefficient . The results obtained are listed in Table 1. As one can note , Mfe performs worst among the three measures , the reason being that the process to construct Gb and Gc yields unbalanced bilingual corpora , the English section being larger than the French one . Translations of French words are still likely to be found in the English corpus , even though the corpora are not comparable . On all the 3 groups,M performs best and correlates very well with the gold standard , meaning that M was able to capture all the differences in comparability artificially introduced in the degradation process we have considered . This is the measure we will retain in the following parts.
Mef Mfe M
Ga 0.897 0.770 0.936
Gb 0.955 0.190 0.979
Gc 0.940 -0.595 0.960
Table 1: Correlation scores for the different comparability measures on the 3 groups of corpora comparability of bilingual corpora , we now turn to the problem of improving the quality of comparable corpora.
4 Improving Corpus Quality
We here try to improve the quality of a given corpus C , which we will refer to as the base corpus , by extracting the highly comparable subpart CH which is above a certain degree of comparability ? ( Step 1), and by enriching the lowly comparable part CL with texts from other sources ( Step 2). As we are interested in extracting information related to the vocabulary of the base corpus , we want the newly built corpus to contain a substantial part of the base corpus . This can be achieved by preserving in Step 1 as many documents from the base corpus as possible ( e.g . by considering low values of ?), and by using in step 2 sources close to the base corpus.
4.1 Step 1: Extracting CH
The strategy consisting of building all the possible subcorpora of a given size from a given comparable corpora is not realistic as soon as the number of documents making up the corpora is larger than a few thousands . In such cases , better ways for extracting subparts have to be designed . The strategy we have adopted here aims at efficiently extracting a subpart of C above a certain degree of comparability and is based on the following property.
Property 1. Let d1e and d2e ( resp . d1f and d2f ) be two English ( resp . French ) documents from a bilingual corpus C . We consider , as before , that the bilingual dictionary D is independent from C.
Let ( d1e ?, d1f ?) be such that : d1e ? ? d1e , d1f ? ? d1f , which means d1e ? is a subpart of d1e and d1f ? is a subpart of d1f .
We assume : ( i ) | d1e?d2e||d2e | = | d1f?d2f | | d2f | ( ii ) Mef ( d1e ?, d1f ) ? Mef ( d2e , d2f )
Mfe(d1e , d1f ?) ? Mfe(d2e , d2f )
Then:
M(d2e , d2f ) ? M(d1e ? d2e , d1f ? d2f )
Proof [ sketch ]: Let B = ( d1e ? d2e ) ? Dve )\( d2e ? Dve ). One can show , by exploiting condition ( ii ), that : ? w?B ?( w , d1f ? d2f ) ? | B|Mef ( d2e , d2f ) and similarly that : ? w?d2e?Dve ?( w , d1f ? d2f ) ? | d2e ? Dve | Mef ( d2e , d2f ) Then exploiting condition ( i ), and the independence between the corpus and the dictionary , one arrives at : ? w?(d1e?d2e)?Dve ?( w , d |( d1e ? d2e ) ? Dve |+ |( d1f ? d2f ) ? Dvf | ? | d2e ? Dve | Mef ( d2e , d2f ) | d2e ? Dve |+ | d2f ? Dvf | The same development on Mfe completes the proof . 2 Property 1 shows that one can incrementally extract from a bilingual corpus a subpart with a guaranteed minimum degree of comparability ? by iteratively adding new elements , provided ( a ) that the new elements have a degree of comparability of at least ? and ( b ) that they are less comparable than the currently extracted subpart ( conditions ( ii )). This strategy is described in Algorithm 1.
Since the degree of comparability is always above a certain threshold and since the new documents selected ( d2e , d2f ) are the most comparable among the remaining documents , condition ( i ) is likely to be satisfied , as this condition states that the increase in the vocabulary from the second documents to the union of the two is the same in both languages . Similarly , considering new elements by decreasing comparability scores is a necessary step for the satisfaction of condition ( ii ), which states that the current subpart should be uniformly more comparable than the element to be added.
Hence , the conditions for property 1 to hold are met in Algorithm 1, which finally yields a corpus with a degree of comparability of at least ?.
4.2 Step 2: Enriching CL
This step tries to absorb knowledge from other resources , which will be called external corpus ,
Input:
English document set Cde of C
French document set Cdf of C
Threshold ?
Output:
CH , consisting of the English document set Se and the French document set Sf 1: Initialize Se = ?, Sf = ?, temp = 0; 2: repeat 3: ( de , df ) = argmax de?Cde , df?Cdf
M(de , df ); 4: temp = max de?Cde , df?Cdf
M(de , df ); 5: if temp ? ? then 6: Add de into Se and add df into Sf ; 7: Cde = Cde\de , Cdf = Cdf\df ; 8: end if 9: until Cde = ? or Cdf = ? or temp < ? 10: return CH ; to enrich the lowly comparable part CL which is the left part in C during the creation of CH . One choice for obtaining the external corpus CT is to fetch documents which are likely to be comparable from the Internet . In this case , we first extract representative words for each document in CL , translate them using the bilingual dictionary and retrieve associated documents via a search engine . An alternative approach is of course to use existing bilingual corpora . Once CT has been constructed , the lowly comparable part CL can be enriched in exactly the same way as in section 4.1: First , Algorithm 1 is used on the English part of CL and the French part of CT to get the high-quality document pairs . Then the French part of CL is enriched with the English part of CT by the same algorithm . All the high-quality document pairs are then added to CH to constitute the final result.
4.3 Validation
We use here GH95 and SDA95 as the base corpus C0. In order to illustrate that the efficiency of the proposed algorithm is not confined to a specific external resource , we consider two external resources : ( a ) C1T made of LAT94, MON94 and SDA94, and ( b ) C2T consisting of Wiki-En and Wiki-Fr . The number of documents in all the corpora after elimination of short documents (< 30 words ) is listed in Table 2.
C0 C1T C2T
English 55,989 109,476 367,918
French 42,463 87,086 378,297
Table 2: The size of the corpora in the experiments For the extraction of the highly comparable part CH from the base corpus C0, we set ? to 0.3 so as to extract a substantial subpart of C0. After this step , corresponding to Algorithm 1, we have 20,124 English-French document pairs in CH . The second step is to enrich the lowly comparable part CL of the base corpus documents from the external resources C1T and C2T . The final corpora we obtain consist of 46,996 document pairs for C1 ( with C1T ) and of 54,402 document pairs for C2 ( with C2T ), size similar to the one of C0. The proportion of documents ( columns ? De ? and ? Df ?), sentences ( columns ? Se ? and ? Sf ?) and vocabulary ( columns ? Ve ? and ? Vf ?) of C0 found in C1 and C2 is given in Table 3. As one can note , the final corpora obtained through the method presented above preserve most of the information from the base corpus . Especially for the vocabulary , the final corpora cover nearly all the vocabulary of the base corpus . Considering the comparability scores , the comparability of C1 is 0.912 and the one of C2 is 0.916. Both of them are more comparable than the base corpus of which the comparability is 0.882.
From these results of the intrinsic evaluation , one can conclude that the strategy developed to improve the corpus quality while preserving most of its information is efficient : The corpora obtained here , C1 and C2, are more comparable than the base corpus C0 and preserve most of its information . We now turn to the problem of extracting bilingual lexicons from these corpora.
5 Bilingual Lexicon Extraction
Following standard practice in bilingual lexicon extraction from comparable corpora , we rely on the approach proposed by Fung and Yee (1998).
In this approach , each word w is represented as a
C1 0.669 0.698 0.821 0.805 0.937 0.981
C2 0.785 0.719 0.893 0.807 0.968 0.987
Table 3: Proportion of documents , sentences and vocabulary of C0 covered by the result corpora context vector consisting of the weight a(wc ) of each context word wc , the context being extracted from a window running through the corpus . Once context vectors for English and French words have been constructed , a general bilingual dictionaryD can be used to bridge them by accumulating the contributions from words that are translation of each other . Standard similarity measures , as the cosine or the Jaccard coefficient , can then be applied to compute the similarity between vectors.
For example , the cosine leads to : sc(we , wf ) = ? ( wce,wcf )? D a(w c e)a(wcf ) ??? we ? ? ??? wf ? (1) 5.1 Using Algorithm 1 pseudo-Alignments The process we have defined in the previous section to improve the quality of a given corpus while preserving its vocabulary makes use of highly comparable document pairs , and thus provides some loose alignments between the two corpora.
One can thus try to leverage the above approach to bilingual lexicon extraction by reweighting sc(we , wf ) by a quantity which is large if we and wf appear in many document pairs with a high comparability score , and small otherwise . In this section , we can not use the alignments in algorithm 1 directly because the alignments in the comparable corpus should not be 1 to 1 and we did not try to find the precise 1 to 1 alignments in algorithm 1.
Let ? be the threshold used in algorithm 1 to construct the improved corpus and let ?( de , df ) be defined as : ?( de , df ) = { 1 iff M(de , df ) ? ? 0 else Let He ( resp . Hf ) be the set of documents containing word we ( resp . wf ). We define the joint probability of we and wf as being proportional to the number of comparable document pairs they belong to , where two documents are comparable if their comparability score is above ?, that is : p(we , wf ) ? ? de?He,df?Hf ?( de , df ) The marginal probability p(we ) can then be written as : p(we )? ? wf?Cvf p(we , wf ) ? ? de?He ? df?Cdf | df | ? ?( de , df ) Assuming that all df in Cdf have roughly the same vocabulary size and all de have the same number of comparable counterparts in Cdf , then the marginal probability can be simplified as : p(we ) ? | He |. By resorting to the exponential of the pointwise mutual information , one finally obtains the following weight : pi(we , wf ) = p(we , wf ) p(we ) ? p(wf ) ? 1|He | ? | Hf | ? de?He,df?Hf ?( de , df ) which has the desired property : It is large if the two words appear in comparable document pairs more often than chance would predict , and small otherwise . We thus obtain the revised similarity score for we and wf : scr(we , wf ) = sc(we , wf ) ? pi(we , wf ) (2) 5.2 Validation In order to measure the performance of the bilingual lexicon extraction method presented above , we divided the original dictionary into 2 parts : 10% of the English words (3,338 words ) together with their translations are randomly chosen and used as the evaluation set , the remaining words (30,034 words ) being used to compute context vectors and similarity between them . In this study , the weight a(wc ) used in the context vectors ( see above ) are taken to be the tfidf score of wc : a(wc ) = tf-idf(wc ). English words not cluded from the evaluation set . For each English word in the evaluation set , all the French words in Cvf are then ranked according to their similarity with the English word ( using either equation 1 or 2). To evaluate the quality of the lexicons extracted , we first retain for each English word its N first translations , and then measure the precision of the lists obtained , which amounts in this case to the proportion of lists containing the correct translation ( in case of multiple translations , a list is deemed to contain the correct translation as soon as one of the possible translations is present).
This evaluation procedure has been used in previous work ( e.g . ( Gaussier et al , 2004)) and is now standard for the evaluation of lexicons extracted from comparable corpora . In this study , N is set to 20. Furthermore , several studies have shown that it is easier to find the correct translations for frequent words than for infrequent ones ( Pekar et al , 2006). To take this fact into account , we distinguished different frequency ranges to assess the validity of our approach for all frequency ranges . Words with frequency less than 100 are defined as low-frequency words ( WL ), whereas words with frequency larger than 400 are high-frequency words ( WH ), and words with frequency in between are medium-frequency words ( WM ).
We then tested the standard method based on the cosine similarity ( equation 1) on the corpora C0, CH , C?H , C1 and C2. The results obtained are displayed in Table 4, and correspond to columns 26. They show that the standard approach performs significantly better on the improved corpora C1/C2 than on the base corpus C0. The overall precision is increased by 5.3% on C1 ( corresponding to a relative increase of 26%) and 9.5% on C2 ( corresponding to a relative increase of 51%), even though the low-frequency words , which dominate the overall precision , account for a higher proportion in C1 (61.3%) and C2 (61.3%) than in C0 (56.2%). For the medium and high frequency words , the precision is increased by over 11% on C1 and 16% on C2. As pointed out in other studies , the performance for the low-frequency words is usually bad due to the lack of context information . This explains the relatively small improvement obtained here ( only 2.2% on C1 and 6.7% on C2). It should also be noticed that the performance of the standard approach is better on C2 than on C1, which may be due to the fact that C2 is slightly larger than C1 and thus provides more information or to the actual content of these corpora . Lastly , if we consider the results on the corpus CH which is produced by only choosing the highly comparable part from C0, the overall precision is increased by only 1.9%, which might come from the fact that the size of CH is less than half the size of C0. We also notice the better results on CH than on C?H of the same size which consists of randomly choosing documents from C0.
The results obtained with the refined approach making use of the comparable document pairs found in the improved corpus ( equation 2) are also displayed in Table 4 ( columns ? C1 new ? and ? C2 new ?). From these results , one can see that the overall precision is further improved by 2.0% on C1 and 2.3% on C2, compared with the standard approach . For all the low , medium and high-frequency words , the precision has been improved , which demonstrates that the information obtained through the corpus enrichment process contributes to improve the quality of the extracted bilingual lexicons . Compared with the original base corpus C0, the overall improvement of the precision on both C1 and C2 with the refined approach is significant and important ( respectively corresponding to a relative improvement of 35% and 62%), which also demonstrates that the efficiency of the refined approach is not confined to a specific external corpus.
6 Discussion
It is in a way useless to deploy bilingual lexicon extraction techniques if translation equivalents are not present in the corpus . This simple fact is at the basis of our approach which consists in constructing comparable corpora close to the original corpus and which are more likely to contain translation equivalents as they have a guaranteed degree of comparability . The pseudo-alignments identified in the construction process are then used to leverage state-of-the-art bilingual lexicon extraction methods . This approach to bilingual lexicon extraction from comparable corpora radically differs , to our knowledge , from previous approaches WL 0.114 0.144 0.125 0.136 0.181 0.156 2.0%, 4.2% 0.205 2.4%, 9.1% WM 0.233 0.313 0.270 0.345 0.401 0.369 2.4%, 3.6% 0.433 3.2%, 20.0% WH 0.417 0.456 0.377 0.568 0.633 0.581 1.3%, 16.4% 0.643 1.0%, 22.6% All 0.205 0.224 0.189 0.258 0.310 0.278 2.0%, 7.3% 0.333 2.3%, 12.8% Table 4: Precision of the different approaches on different corpora which are mainly variants of the standard method proposed in ( Fung and Yee , 1998) and ( Rapp , 1999). For example , the method developed in ( De?jean et al , 2002) and ( Chiao and Zweigenbaum , 2002) involves a representation of dictionary entries with context vectors onto which new words are mapped . Pekar et al (2006) smooth the context vectors used in the standard approach in order to better deal with low frequency words.
A nice geometric interpretation of these processes is proposed in ( Gaussier et al , 2004), which furthermore introduces variants based on Fisher kernels , Canonical Correlation Analysis and a combination of them , leading to an improvement of the F1score of 2% ( from 0.14 to 0.16) when considering the top 20 candidates . In contrast , the approach we have developed yields an improvement of 7% ( from 0.13 to 0.20) of the F1 score on C2, again considering the top 20 candidates . More important , however , is the fact that the approach we have developed can be used in conjunction with any existing bilingual extraction method , as the strategies for improving the corpus quality and the reweighting formula ( equation 2) are general . We will assess in the future whether substantial gains are also attained with other methods.
Some studies have tried to extract subparts of comparable corpora to complement existing parallel corpora . Munteanu (2004) thus developed a maximum entropy classifier aiming at extracting those sentence pairs which can be deemed parallel . The step for choosing similar document pairs in this work resembles some of our steps . However their work focuses on high quality and specific documents pairs , as opposed to the entire corpus of guaranteed quality we want to build . In this latter case , the cross-interaction between documents impacts the overall comparability score , and new methods , as the one we have introduced , need to be proposed . Similarly , Munteanu and Marcu (2006) propose a method to extract subsentential fragments from nonparallel corpora.
Again , the targeted elements are very specific ( parallel sentences or subsentences ) and limited , and the focus is put on a few sentences which can be considered parallel . As already mentioned , we rather focus here on building a new corpus which preserves most of the information in the original corpus . The construction process we have presented is theoretically justified and allows one to preserve ca . 95% of the original vocabulary.
7 Conclusion
We have first introduced in this paper a comparability measure based on the expectation of finding translation word pairs in the corpus . We have then designed a strategy to construct an improved comparable corpus by ( a ) extracting a subpart of the original corpus with a guaranteed comparability level , and ( b ) by completing the remaining subpart with external resources , in our case other existing bilingual corpora . We have then shown how the information obtained during the construction process could be used to improve state-of-the-art bilingual lexicon extraction methods . We have furthermore assessed the various steps of our approach and shown : ( a ) that the comparability measure we introduced captures variations in the degree of comparability between corpora , ( b ) that the construction process we introduced leads to an improved corpus preserving most of the original vocabulary , and ( c ) that the use of pseudo-alignments through simple reweighting yields bilingual lexicons of higher quality.
Acknowledgements
This work was supported by the French National
Research Agency grant ANR-08-CORD-009.
651
References
Ballesteros , Lisa and W . Bruce Croft . 1997. Phrasal translation and query expansion techniques for cross-language information retrieval . In Proceedings of the 20th ACM SIGIR , pages 84?91, Philadelphia , Pennsylvania , USA.
Chen , Stanley F . 1993. Aligning sentences in bilingual corpora using lexical information . In Proceedings of the 31st Annual Conference of the Association for Computational Linguistics , pages 9?16, Columbus,
Ohio , USA.
Chiao , Yun-Chuang and Pierre Zweigenbaum . 2002.
Looking for candidate translational equivalents in specialized , comparable corpora . In Proceedings of the 19th International Conference on Computational Linguistics , pages 1?7, Taipei , Taiwan.
De?jean , Herve ?, Eric Gaussier , and Fatia Sadat . 2002.
An approach based on multilingual thesauri and model combination for bilingual lexicon extraction.
In Proceedings of the 19th International Conference on Computational Linguistics , pages 1?7, Taipei,
Taiwan.
Fung , Pascale and Kathleen McKeown . 1997. Finding terminology translations from nonparallel corpora.
In Proceedings of the 5th Annual Workshop on Very Large Corpora , pages 192?202, Hong Kong.
Fung , Pascale and Lo Yuen Yee . 1998. An IR approach for translating new words from nonparallel , comparable texts . In Proceedings of the 17th international conference on Computational linguistics , pages 414?420, Montreal , Quebec , Canada.
Gaussier , E ., J.-M . Renders , I . Matveeva , C . Goutte , and H . De?jean . 2004. A geometric view on bilingual lexicon extraction from comparable corpora.
In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics , pages 526?533, Barcelona , Spain.
Kay , Martin and Martin Ro?scheisen . 1993. Text-translation alignment . Computational Linguistics , 19(1):121?142.
Koehn , Philipp . 2005. Europarl : A parallel corpus for statistical machine translation . In Proceedings of MT Summit 2005.
Melamed , I . Dan . 1997a . A portable algorithm for mapping bitext correspondence . In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and the 8th Conference of the European Chapter of the Association for Computational Linguistics , pages 305?312, Madrid,
Spain.
Melamed , I . Dan . 1997b . A word-to-word model of translational equivalence . In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and the 8th Conference of the European Chapter of the Association for Computational Linguistics , pages 490?497, Madrid , Spain.
Morin , Emmanuel , Be?atrice Daille , Koichi Takeuchi , and Kyo Kageura . 2007. Bilingual terminology mining - using brain , not brawn comparable corpora . In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics , pages 664?671, Prague , Czech Republic.
Munteanu , Dragos Stefan and Daniel Marcu . 2006.
Extracting parallel subsentential fragments from nonparallel corpora . In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics , pages 81?88, Sydney , Australia.
Munteanu , Dragos Stefan , Alexander Fraser , and Daniel Marcu . 2004. Improved machine translation performance via parallel sentence extraction from comparable corpora . In Proceedings of the HLTNAACL 2004, pages 265?272, Boston , MA ., USA.
Och , Franz Josef and Hermann Ney . 2003. A systematic comparison of various statistical alignment models . Computational Linguistics , 29(1):19?51.
Pekar , Viktor , Ruslan Mitkov , Dimitar Blagoev , and Andrea Mulloni . 2006. Finding translations for low-frequency words in comparable corpora . Machine Translation , 20(4):247?266.
Rapp , Reinhard . 1999. Automatic identification of word translations from unrelated English and German corpora . In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics , pages 519?526, College Park , Maryland,
USA.
Robitaille , Xavier , Yasuhiro Sasaki , Masatsugu Tonoike , Satoshi Sato , and Takehito Utsuro . 2006.
Compiling French-Japanese terminologies from the web . In Proceedings of the 11st Conference of the European Chapter of the Association for Computational Linguistics , pages 225?232, Trento , Italy.
Yu , Kun and Junichi Tsujii . 2009. Extracting bilingual dictionary from comparable corpora with dependency heterogeneity . In Proceedings of HLTNAACL 2009, pages 121?124, Boulder , Colorado,
USA.
652
