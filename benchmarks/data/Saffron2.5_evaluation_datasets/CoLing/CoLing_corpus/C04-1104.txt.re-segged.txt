Subcategorization Acquisition and Evaluation for Chinese Verbs 
Xiwu Han , Tiejun Zhao , Haoliang Qi , Hao Yu
Department of Computer Science,
Harbin Institute of Technology , 150001 Harbin , China
hxw , tjzhao , qhl , yh@mtlab.hit.edu.cn


This paper describes the technology and an experiment of subcategorization acquisition for Chinese verbs  . The SCF hypotheses are generated by means of linguistic heuristic information and filtered via statistical methods  . Evaluation on the acquisition of 20 multi-pattern verbs shows that our experiment achieved the similar precision and recall with former researches  . Besides , simple application of the acquired lexicon to a PCFG parser indicates great potentialities of subcategorization information in the fields of 


This research is sponsored by National Natural Science Foundation  ( Grant No . 60373101 and 60375019) , and HighTech Research and Development Program ( Grant No . 2002AA117010-09) . 

Since ( Brent 1991 ) there have been a considerable amount of researches focusing on verb lexicons with respective subcategorization information specified both in the field of traditional linguistics and that of computational linguistics  . As for the former , subcategory theories illustrating the syntactic behaviors of verbal predicates are now much more systemically improved  , e . g . 
( Korhonen 2001) . And for auto-acquisition and relevant application  , researchers have made great achievements not only in English  , e . g . ( Briscoe and Carroll 1997) , ( Korhonen 2003) , but also in many other languages , such as Germany ( Schulte im Walde 2002) , Czech ( Sarkar and Zeman 2000) , and Portuguese ( Gamallo et . al 2002) . 
However , relevant theoretical researches on Chinese verbs are generally limited to case grammar  , valency , some semantic computation theories , and a few papers on manual acquisition or prescriptive designment of syntactic patterns  . 
Due to irrelevant initial motivations , syntactic and semantic generalizabilities of the consequent outputs are not in such a harmony that satisfies the description granularity for SCF  ( Han and Zhao 2004 )  . The only auto-acquisition work for Chinese SCF made by  ( Han and Zhao 2004 ) describes the predefinition of 152 general frames for all verbs in Chinese , but that experiment is not based on real corpus . After observing and analyzing quantity of subcategory phenomena in real Chinese corpus in the People?s Daily  ( Jan . ~ June ,  1998) , we removed from Han & Zhao?s predefinition 15 SCFs that are actually similar derivants of others  , and then with this foundation and linguistic rules from  ( Zhao 2002 ) as heuristic information we generated SCF hypotheses from the corpus of People?s Daily  ( Jan . ~ June ,  1998) , and statistically filtered the hypotheses into a Chinese verb SCF lexicon  . As far as we know , this is the first attempt of Chinese SCF auto -acquisition based on real corpus  . 
In the rest of this paper , the second section describes a comprehensive system that builds verb SCF lexicons from large real corpus  , the respective operating principles , and the knowledge coded in our SCF . The third section analyzed the acquired lexicon with two experiments : one evaluated the acquisition results of  20 verbs with multisyntactic patterns against manual gold standard  ; the other checked the performance of the lexicon when applied in a PCFG parser  . The forth section compares and contrasts this research with related works done by others  . And at last , Section 5 concludes our present achievements , disadvantages and possible future focuses . 
1 SCF Acquisition 1 . 1 The Acquisition Method There are generally 4 steps in the process of our auto-acquisition experiment  . First , the corpus is processed with a cascaded HMM parser  ; second , every possible local patterns for verbs are abstracted  ; and then , the verb patterns are classified into SCF hypotheses according to the predefined set  ; at last , hypotheses are filtered statistically and the respective frequencies are also recorded  . 
The actual application program consists of 6 parts as shown in the following paragraphs . 
a . Segmenting and tagging : The raw corpus is segmented into words and tagged with POS by the comprehensive segmenting and tagging processor developed by MTLAB of Computer Department in Harbin Institute of Technology  . The advantage of the POS definition is that it describes some subsets of nouns and verbs in Chinese  . 
b . Parsing : The tagged sentences are parsed with a cascaded HMM  parser1  , developed by MTLAB of HIT , but only the intermediate parsing results are used  . 
The training set of the parser is 20 , 000 sentences in the Chinese TreeBank 2 of ( Zhao 2002 )  . 
c . Error-driven correction : Some key errors occurring in the former two parts are corrected according to manually obtained error-driven rules  , which are generally about words or POS in the corpus  . 
d . Pattern abstraction : Verbs with largest governing ranges are regarded as predicates  , then local patterns , previous phrases and respective syntactic tags are abstracted  , and isolated parts are combined , generalized or omitted according to basic phrase rules in  ( Zhao 2002 )  . 
e . Hypothesis generation : Based on linguistic restraining rules  , e . g . no more than two NP?s occurring in a series and no more than three in one pattern  , and no PPT PMP occurring with NP before any predicates  ( Han and Zhao 2004 )  , the patterns are coordinated and classified into the predefined SCF groups  . In this part , about 5% unclassifiable patterns are removed . 
1 When evaluated on auto-tagged open corpus , the parser?s phrase precision if 62 . 3%, and phrase recall is 60 . 9% ( Meng , 2003) . 
2 A sample of the tree bank or relevant introduction could be found at http://mtlab  . hit . edu . cn . 
f . Hypothesis filtering : According to the statistical reliability of each type of the SCF hypotheses and the linguistic principle that arguments occur more frequently with predicates than adjuncts do  , the hypotheses are filtered by means of statistical methods  , in this paper which are binomial hypotheses testing  ( BHT ) and maximum likelihood estimation ( MLE )  . 

Table 1: An Example of Auto-acquisition
No Actions Results a ) Input ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ) Tag and parse
BNP [ BMP [? / m ? / q]?/ng ] ?/ pNDE[???/r ? /usde]BVP [ ??/ vg?/vq ] BVP [??/ vg ? /ut]NP [??/ nc?/usde??/ng]?/wjc  ) Correct errors
BNP [ BMP [?/ m?/q]?/ng]?/pNDE[???/r?/usde ? ?/vg?/vq]BVP [??/ vg?/LE]NP [??/ nc?/usde ??/ ng ]? / wjd  ) Abstract patterns
BNP PPB VP [ vg LE]NP e ) Generate hypothesis
NP v NP?01000?f ) Filter hypotheses
NP v NP 01 11 13
In Table 1 , for example , when acquiring SCF information for ???? ( prove ) and a related sentence in the corpus is a )  , our tagger and parser will return b ) , and error-driven correction will return c ) with errors of NDE and the 1st BVP cor-rected 4 . Since the governing range of ???? is larger than that of ????  ( ask )  , the other verb in this sentence , the program abstracts its local pattern BVP [ vg LE ] and previous phrase BNP  ,   gen-3   01000 projects to the Chinese syntactic morphemes ????? ????  ,   1 means the SCF may occur with the respective morpheme  , while 0 may not ( Han & Zhao ,  2004) . 
4 Note that not all errors in this example have been corrected  , but this doesn?t affect further procession . Also , for definitions of NDE and BVP see ( Zhao ,  2002) . 
eralizes BNP and NDE as NP , combines the second NP with isolated part ??/ p ? into PP  , and returns d ) . Then the hypothesis generator returns e ) as the possible SCF in which the verb may occurs . Actually in the corpus there are 621 hypothesis tokens generated , and among them 92 ones are of same arguments with e )  , and thuse ) can pass the hypothesis testing ( See also Section 1 . 2), so we obtain one SCF for ???? as f ) . 
1.2 Filtering Methods
In researches of subcategorization acquisition , statistical methods for hypothesis filtering mainly include the BHT  , the LogLikelihood Ratio ( LLR ) , the T-test and the MLE , and the most popular one is the BHT . Since ( Brent 1993) began to use the method , most researchers have agreed that the BHT results in better precision and recall with SCF hypotheses of high  , medium and low frequencies . Only ( Korhonen 2001) reports 11 . 9% total performance of the MLE better than the BHT  . Therefore , we applied the two statistical methods in our present experiment  . This subsection chiefly illustrates the expressions of our methods and definitions of parameters in them  , while performance comparison of the two will be introduced in Section  3  . 
When applying the BHT method , it is necessary to determine the probability of the primitive event  . As for SCF acquisition , the cooccurrence of one predefined SCFs c fi with one verb v is the relevant primitive event  , and the concerned probability is p(vscfi ) here . However , the aim of filtering is to rule out those unreliable hypotheses  , so it is the probability that one primitive event doesn't occur that is often used for SCF hypothesis testing  , i . e . the error probability:pe(vscfi ) = 1p(vscfi ) .   ( Brent 1993 ) estimated peaccording to the acquisition system ?s performance  , while ( Briscoe and Carroll 1997 ) calculated pe from the distribution of SCF types in ANLT and SCF tokens in Susanne as shown in the following equation  . 

Brent?s method mainly depends on the related corpus and processing program  , which may cause intolerable errors . Briscoe and Carroll?s method draws on both linguistic and statistical information thus leading to comparatively stable estimation  , and therefore has been used by many latter researches  , e . g . ( Korhonen 2001) . But there is no MRD proper for Chinese SCF description so we estimated pe from the  1  , 7 75 common verbs and SCF tokens in the related corpus of  43  , 000 sentences used by ( Han and Zhao 2004) . We formed the equation as follows : Then the number of all hypotheses about verb vj is recorded as n  , and the number of those for scfiasm . According to Bernoulli theory , the probability P that an event with probability pexactly happens m times out of n such trials is : And the probability that the event happens mor more times is : In turn  , P ( m + , n , p e ) is the probability that scfi wrongly occurs mor more times with a verb that doesn't match it  . Therefore , a threshold of 0 . 0 5 on this probability will yield a 95% confidence that a high enough proportion of hypotheses for scfi have been observed for the verb legitimately to be assigned scfi  ( Korhonen 2001 )  . 
The MLE method is closely related to the general performance of the concerned SCF acquisition system  . First , we randomly draw from the applied corpus a training set  , which is large enough so as to ensure similar SCF frequency distribution  . Then , the frequency of scfi occurring with a verb vj is recorded and used to estimate the actual probability p  ( scfivj )  . Thirdly , an empirical threshold is determined , such that it ensures maximum value of Fmeasure on the training set  . 
Finally , the threshold is used to filter out those SCF hypotheses with low frequencies from the total set  . 
2 Experimental Evaluation 2 . 1 Acquisition Performance Using the previously described theory and technology we have acquired an SCF lexicon for  3  , 5 58 common Chinese verbs from the corpus of People?s Daily  ( Jan . ~ June , 1998) . In the lexicon the minimum number of SCF tokens for a verb is  30  , and the maximum is 20 , 000 . In order to check the acquisition performance of the used system  , we evaluated a part of the lexicon against a manual gold standard  . The testing set includes 20 verbs of multisyntactic patterns , and for each verb there are 503~2 , 000 SCF tokens with the total number of 18 , 316 ( See Table 2) . Table 3 gives the evaluation results for different filtering methods  , including non-filtering 5 , BHT , and MLE with thresholds of 0 . 001, 0 . 005, 0 . 008 and 0 . 01 . We calculated the type precision and recall by the following expressions as  ( Korhonen 2001 ) did : In here , true positives are correct SCF types proposed by the system  , false positives are incorrect SCF types proposed by system  , and false negatives are correct SCF types not proposed by the system  . 

Table 2: Verbs in the Testing Set6
Verbs English Tokens Verbs English Tokens ? Read 503 ? ? Hope 620 ? ? Find 529 ? See 645 ? ? Reckon 543 ? ? Invest 679 ? Pull 544 ? ? Know 722 ? ? Report 612 ? Send 800 ? ? Develop 1  , 006?? Setup 1 , 186 ?? Behave 1 , 007 ? ? Insist 1 , 200? ? Decide 1 , 038 ? Think 1 , 200?? End 1 , 140 ? ? Require 1 , 200 ? ? Begin 1142 ? Write 2 , 000 According to Table 3 , all other filtering methods outperform non -filtering  , and MLE is better than BHT . Among the four MLE thresholds , 0 . 0 08 achieves the best comprehensive performance but its Fmeasure is only  0  . 74 larger than that of 0 . 01 while its precision drops by 2 . 4 percent . Hence , we chose 0 . 0 1 as the threshold for the whole experiment with purpose to meet the practical requirement of high precision and to avoid possible overfit phenomena  . Finally , with a confidence of 95% we can estimate the general performance of the acquisition system with precision of  60  . 6% +/- 2 . 39%, and recall of 51 . 3%+/-2 . 45% . 
5 Non-filtering means filtering with a zero threshold or not filtering at all  . This method is used as baseline here . 
6 The English meanings given here are not intended to cover the whole semantic range of the respective verbs  , on the contrary they are just for readers ? reference  . 

Table 3: System Performance for Different Filtering Methods Measures 
Methods Precision Recall Fmeasure
Non-filtering 37.43% 85.9% 52.14
BHT 50% 57 . 2% 53 . 36 0 . 001 39 . 2% 85 . 9% 53 . 83 0 . 005 40 . 3% 83 . 33% 54 . 33 0 . 008 58 . 2% 54 . 5% 56 . 3 MLE 0 . 01 60 . 6% 51 . 3% 55 . 56 2 . 2 Task-oriented Evaluation In order to further analyze the practicability of the previously described technology  , we performed a simple task-oriented evaluation , applying the acquired SCF lexicon in a PCFG parser helping to choose from the nbest parsing results  . 
The concerned parser was trained from 10 , 000 manually parsed Chinese sentences7 . In this experiment there are 664 verbs and their SCF information involved . The open testing set consists of 1 , 500 sentences , for each of which the PCFG parser outputs 5best parsing results . Then SCF hypotheses are generated for each result by means of the formerly mentioned technology  . 
Finally , the maximum likelihood between hypotheses and those SCF types for the related verb in the lexicon is calculated in the following way : where i ?  5  , hi is one of the hypotheses generated for the parsing results  , and scfj is the jth SCF type for the concerned verb  . This calculation keeps the likelihood between 0 and 1  . The parsing result 7 These sentences and the testing corpus mentioned latter are all taken from the Chinese TreeBank developed by MTLAB of HIT  , and a sample may be downloaded at http://mtlab . hit . edu . cn . 
with maximum likely hood is then regarded as the final choice  . When two or more hypotheses hold the same likelihood  , the one with larger or largest
PCFG probability will be chosen.
Table 4 shows the phrasebased and sentence-based evaluation results for the parser without and with SCF heuristic information  . There are three cased included : a ) The output is one-best ; b ) The output is 5best and the best evaluation result is recorded ; c ) The 5best output is checked again for the best syntactic tree by means of SCF information  . The phrased-based evaluation follows the popular method for evaluating a parser  , while the sentence-based depends on the intersection of the parsed trees and those in the gold standard  . 
Since the PCFG parser output at least one syntactic tree for every sentence in our testing corpus  , the sentence-based precision and recall are equal to each other  . 

Table 4: Parsing Evaluation
Phrase-based Sentence-based


Precision Recall Precision = Recall
One-best 57 . 5% 55% 13 . 64% 5best 65 . 28% 64 . 59% 26 . 2%
With SCF 62.86% 62.1% 21.66%
Table 4 shows that SCF information remarka-bly improved the performance of the PCFG parser : the phrasebased precision increased by  5  . 36% and recall by 7 . 1% , while the sentence-based precision and recall both increased by  8  . 04% . However , this doesn?t reach the upper limit of the 5best . The possible reasons are : a ) the our present SCF lexicon remains to be improved  ; b ) our method of applying SCF information to the parser is too simple  , e . g . probabilities of PCFG parsing results haven?t been exploited thoroughly  . 
3 Related Works
As far as we know , this is the first attempt to automatically acquire SCF information from real Chinese corpus and the first trial to apply SCF lexicon to a Chinese parser  . Our research draws a lot on related works from international researches  , and for the purpose of crosslingual processing , our research is kept inconsistency with SCF conventions as much as possible  . 
Due to linguistic differences , nevertheless , not all theories , methods or experiences could adapt to Chinese . Generally , there are four aspects that our research differs from those of other languages  . First , the SCF formalization of most former researches follows the Levin style  , in which most SCF somit NP before predicates , while Chinese SCFs need to depict arguments occurring before verbs  . Second , except ( Sarkar and Zeman 2000) , most former researches are based on manual SCF predefinition  , while our predefined SCF set is statistically acquired  ( See Han and Zhao 2004 )  . Third , involved parsers of former researches are mostly better than Chinese parsers to some degree  . Forth , our SCF information also includes 5 syntactic morphemes ( See also Section 1 . 1) . 
Meanwhile , the basic purpose for Chinese SCF acquisition is also to determine the subcategory features for a verb via its argument distributions and then apply the lexicon to NLP tasks  . Therefore , under similar cases the respective evaluations are comparable  . And Table 5 gives the comparison between our research and the best English results without semantic backoff  8 in ( Korhonen 2001 )  . 

Table 5: Performance Comparison Between
Chinese and English Researches Filtering
Measures NonBHT MLE
Ours 37 . 43% 50% 58 . 2% Precision Korhonen 24 . 3% 50 . 3% 74 . 8% Ours 85 . 9% 57 . 2% 54 . 5% Recall Korhonen 83 . 5% 56 . 6% 57 . 8%
Ours 52 . 14 53 . 36 56 . 3 F-measure Korhonen 37 . 6 53 . 3 65 . 2 The comparison shows that our nonfiltering result is better than Korhonen?s  , both BHT results are similar , while our MLE result is much worse 8 Semantic backoff is a method of generating SCF hypotheses according to the semantic classification of the concerned verb  . Note that this paper doesn?t involve verb meanings for generating hypotheses  . Besides , though the evaluation for English SCF acquisition is the best  , it?s not the newest . For the newest , please refer to ( Korhonen 2003) , in which the precision is 71 . 8% and recall is 34 . 5% . 
than Korhonen?s . That means our hypothesis generator performs well but our filtering method remains to be improved  . According to the analysis of relevant corpus , we found the main cause might be that low frequency SCF types account for  32% in our corpus while those in ( Korhonen 2001 ) sum to nearly 21% . 
Furthermore ,   ( Briscoe and Carroll 1997 ) applied their acquired English SCF lexicon to an intermediate parser  , and reported a 7% improvement of both phrasebased precision and recall  . 
Our application of SCF lexicon to a PCFG parser leads to  5  . 36% improvement for phrasebased precision , 7 . 1% for recall , and 8 . 04% for sen-tence-based precision and recall . 
4 Conclusion
This paper for the first time describes a largescale experiment of automatically acquiring SCF lexicon from real Chinese corpus  . Performance evaluation shows that our technology and acquiring program have achieved similar performance compared with former researches of other languages  . And the application of the acquired lexicon to a PCFG parser indicates great potentialities of SCF information in the field of NLP  . 
However , there is still a large gap between Chinese subcategorization works and those of o-ther languages  . Our future work will focus on the optimization of linguistic heuristic information and filtering methods  , the application of semantic backoff , and the exploitation of SCF lexicon for other NLP tasks  . 

Brent , M . R .  1991 . Automatic acquisition of subcategorization frames from untagged text  . In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics  , Berkeley , CA .  209-214 . 
Brent , M .  1993 . From Grammar to Lexicon : unsupervised learning of lexical syntax  . Computational Linguistics 19 . 3 .  243-262 . 
Briscoe , Ted and John Carroll , 1997 . Automatic extraction of subcategorization from corpora  . In Proceedings of the 5th ACL Conference on Applied Natural Language Processing  , Washington , DC . 
Dorr , B . J . Gina-Anne Levow , Dekang Lin , and Scott Thomas ,  2000 . ChineseEnglish Semantic Resource Construction ,   2nd International Conference on Language Resources and Evaluation  ( LREC 2000 )  , 
Athens , Greece , pp . 757--760.
Gamallo , P . , Agustini , A . and Lopes Gabriel P . , 2002 . 
Using Co-Composition for Acquiring Syntactic and Semantic Subcategorisation  , ACL02 . 
Han , Xiwu , Tiejun Zhao , 2004 . FML-Based SCF Pre-definition Learning for Chinese Verbs  . International Joint Conference of NLP 2004 . 
Jin , Guangjin , 2001 . Semantic Computations for Modern Chinese Verbs . Beijing University Press , Beijing . ( in Chinese ) Korhonen , Anna , 2001 . Subcategorization Acquistion , Dissertation for Ph . D , Trinity Hall University of
Cambridge . 29-77.
Korhonen , Anna , 2003 . Clustering Polysemic Subcategorization Frame Distributions Semantically  . 
Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics  , pp .  6471 . 
Meng , Yao , 2003 . Research on Global Chinese Parsing Model and Algorithm Based on Maximum Entropy  . Dissertation for Ph . D . Computer Department,
HIT . 3334.
Sabine Shulteim Walde , 2002 . Inducing German Semantic Verb Classes from Purely Syntactic Subcategorization Information  . Proceedings of the 40st
ACL , pp . 223-230.
Sarkar , A . and Zeman , D .  2000 . Automatic Extraction of Subcategorization Frames for Czech  . In Proceedings of the 19th International Conference on Computational Linguistics  , aarbrucken , Germany . 
Zhan Weidong , 2000 . Valence Based Chinese Semantic Dictionary , Language and Character Applications , Volume 1 . ( in Chinese ) Zhao Tiejun , 2002 . Knowledge Engineering Report for MTS 2000 . 

