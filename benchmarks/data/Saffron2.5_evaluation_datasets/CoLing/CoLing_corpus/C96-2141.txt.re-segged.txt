HMM-Based Word Alignment in Statistical Translation 
Stephan Vogel Hermann Ney Christoph Til lmann 
Lehrstuhlffir Informatik V , RWTH Aachen
D-52056 Aachen , Germany
vogel , ney , tillmann@informatik . rwth-aachen , de

In this paper , we describe a new model for word alignment in statistical translation and present experimental results  . 
The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions  . 
To achieve this goal , the approach uses a first-order Hidden Markov model  ( HMM ) for the word alignment problem as they are used successfully in speech recognition for the time alignment problem  . The difference to the time alignment HMM is that there is no monotony constraint for the possible word orderings  . We describe the details of the model and test the model on several bilingual corpora  . 
1 Introduction
In this paper , we address the problem of word alignments for a bilingual corpus  . In the recent years , there have been a number of papers considering this or similar problems :  ( Brown et al ,  1990) , ( Dagan et al ,  1993) , ( Kay et al ,  1993) , ( Fung et al ,  1993) . 
In our approach , we use a first-order Hidden Markov model ( HMM )   ( aeline k ,  1976) , which is similar , but not identical to those used in speech recognition  . The key component of this approach is to make the alignment probabilities dependent not on the absolute position of the word alignment  , but on its relative position ; i . e . we consider the differences in the index of the word positions rather than the index itself  . 
The organization of the paper is as follows.
After reviewing the statistical approach to machine translation  , we first describe the conventional model ( mixture model )  . We then present our first-order HMM approach in lull detail  . Finally we present some experimental results and compare our model with the conventional model  . 
2 Review : Translation Model
The goal is the translation of a text given in some language F into a target language E  . For convenience , we choose for the following exposition as language pair French and English  , i . e . we are given a French string f ~= fx . . . fj . . . fJ , which is to be translated into an English string e /= el  . . . ei . . . cl . 
Among all possible English strings , we will choose the one with the highest probability which is given by Bayes ' decision rule : a = argmaxP  ,  . ( clAa)q = argmaxPr(ejt ) . l'r ( fle\[ ) el ~ Pr ( e ) is the language model of the target language , whereas Pr ( fJ le ) is the string translation model . The argmax operation denotes the search problem . In this paper , we address the problem of introducing structures into the probabilistic dependencies in order to model the string translation probability Pr  ( f ~ le )  . 
3 Alignment Models
A key issnein modeling the string translation probability Pr  ( J ' ~ leI ) is the question of how we define the correspondence bt ween the words of the English sentence and the words of the French sentence  . In typical cases , we can assume a sort of pairwise dependence by considering all word pairs  ( fj , ei ) for a given sentence pair I . -/1\[~'J' , elqlj ' We further constrain this model by assigning each French word to exactly one English word  . Models describing these types of dependencies are referred to as alignment models  . 
In this section , we describe two models for word alignrnent in detail :  ,  . a mixture-based alignment model , which was introduced in ( Brown et al , 1990); ? an HMM-based alignment model . 
In this paper , we address the question of how to define specific models for the alignment probabilities  . The notational convention will be as follows . We use the symbol Pr ( . ) to denote general as SUml ) tions . In contrast , for modcl-t)ased prol ) -- ability distributions , we use the generic symbol v ( . ) . 
3 . 1 Al ignment w i th M ix ture D is t r i  mt ion Here  , we describe the mixture-based alignment model in a fornmlation which is different fronlthe original formulation ill  ( Brownel , a \ [ . , 1990) . 
We will , is ( : this model as reference t br the IIMM-based alignments to lie  1  ) resented later . 
The model is based on a decomposition of the joint probability \[ br  , l ' ~ into a product over the probabilities for each word J  ) : aj=l wheFe ~ fo\['norll-la\]iz ; i , to n 17 ( ~/ SOllS ~ the 8 elltC\]\[ce length probability p ( J \] l ) has been included . The next step now is to assutne a sort O\['l , airwise interact , ion between tim French word fjan ( leach , F , n-glish word ci , i = 1 ,   . . . l . These dep ( ' ndencies are captured in the lbrmofarnixtnre distritmtion :  i=1 
I = ~_~p(ilj , l ). p(fjle ~) i=1
Putting everything together , we have the following mixture-based n to del :
Jlr,'(fi!l~I ) = p(JIO'H~_~\[~,(ilJ , l) .  ~ , ( j ) led\] ( 1 ) j = li = t with the following ingredients : ? sentence length prob~d  ) ility : P ( Jll )  ; ? mixture alignment probability : p(ilj , I ); ? translation probM ) ility : p(f\[e) . 
Assuming at mi for nl ~ flignment prol ) ability we arrive at the lh'st model proposed t ) y ( Brown et al ,  1990) . This model will be referred to as
IBM1 model.
To train the translation probabilities p(J'fc ) , we use a bilingual ( ; or pus consisting of sentence pairs \[:/' ; 4"1 : ' ,   .   , s Using the , , laxin , ul , likelihood criterion , weol ) ta in the following iterative La equation ( Brown et al , 1990):/)(fie ) = ~- will ,  $'
A(f , e ) = ~2~5 ( f , J ). ~) ~ a(e , e ~. ~)
For unil brm alignment probabilities , it can be shown ( Brown et al ,  1990) , that there is only one opt in nnn and therefore the I  , ' , M algorithm ( Baum , 1!)72) always tinds the global optimum . 
For mixture alignment model with nonunilbrm alignment probabilities  ( subsequently referred to as IBM2 model )  , there ~ treto ( ) many alignrnent parameters Pill j , I ) to be estimated for smMl colpora . Therefore , a specific model t brtile Mign-ment in : obabilities i used : r  ( i-j~- )   ( ~ ) p ( i l j , 1) = l . I
Ei ': l "( it --" JJ-)
This model assumes that the position distance relative to the diagonaline of the  ( j , i ) plane is the dominating factor ( see Fig .  1) . ' lb train this model , we use the , naximut n likelihood criterion in the socalled ulaxim mnal  ) proximation , i . e . the likelihood criterion covers only tile most lik  ( - . lyalign:inch , rather than the set of all alignm ( , nts : dP , '(f(I , : I ) ~ II ~ " IUHO , ~) v(JI , : ~)\] ( a)j=l In training , this criterion amounts to a sequence of iterations  , each of which consists of two steps : * posil i on alignm cnl :  ( riven the model parameters , de Lerlni im the mosL likely position align-\]lient . 
? paramc , lcr cstimalion : Given the position alignment , i . e . goiug along the alignment paths for all sentence pairs  , perform maxi-tnulu likelihood estimation of the model parameters  ; for model-De (' distributions , these estimates result in relative frequencies . 
l ) ue to the natnre of tilen fixture tnod(:l , there is no interaction between djacent word positions  . 
The retbre , the optimal position i for each position j can be determined in  ( lependently of the neighbouring positions . Thus l . he resulting training procedure is straightforward . 
a . 2 Alignment with HMM
We now propose all HMM-based alignment model.
'\[' hemotivation is that typic Mly we have a strong localization effect in aligning the words in parallel texts  ( for language pairs fi:om\]n do european languages  ) : the words are not distrilmted arbitrarily over the senteuce \]  ) ositions , but tend to form clusters . Fig .   1 illustrates this effect for the language pair German-  15'nglish  . 
Each word of the German sentence is assigned to a word of the English sentence  . The alignments have a strong tendency to preserve the local neighborhood when going from the one langnage to the other language  . In mm , y cases , although notal ~ ways , there is an even stronger restriction : the differeuce in the position index is smMler than  3  . 













WELL + + + + + + + + + j ~ + + + + + + + + + ~ J ~ + + +++++++/+?+  .  -  . 
+++++++/++++++++++~ x~+++++ +++++/+ D+++++++++~+++++++++ +_ ~+ + + + + + + + + + + ~ + + + + + + + + + + jg + + + + + + + + + + ~ + + + + + + + + + + g + + + + + + + + ++ + zaa Figure  1: Word alignment for a German-English sentence pair  . 
To describe these word-by-word aligmnents , we introduce the mapping j---+ aj , which assigns a word fj in position j to a word el in position = aj  . The concept of these alignments i similar to the ones introduced by  ( Brown et al ,  1990) , but we wilt use another type of dependence in the probability distributions  . Looking at such alignments produced by a hm nan expert  , it is evident that the mathematical model should try to capture the strong dependence of a j on the previous aligmnent  . Therefore the probability of alignment aj for position j should have a dependence on the previous alignment aj_  1 : p ( ajiaj_l , i ) , where we have inchided the conditioning on the total length\[of the English sentence for normalization reasons  . As in filar approach as been chosen by ( Da . gan et al , 1993) . Thus the problem formulation is similar to that of the time alignment problem in speech recognition  , where the socalled II idden Markov models have been successfully used for a long time  ( Jelinek ,  1976) . Using the same basic principles , we can rewrite the probability by introducing the ' hidden ' alignments af := al  .   .   . aj .   .   . a a for a sentence pair If , a ; e\]:
Pr(f  ~ ales ) = ~_ , Vr(fal , a T\[eI't , a7 , 1=~1-IP"(k , " stfT -' , " -* , e / ) a Ij = l So fart here has been no basic restriction of the approach  . We now assume a first-order dependence on the alignments ajonly : 
Vr(fj , as lf-~ , J -* aI , el ) where , in addition , we have assmned that tile translation probability del  ) ends only oil aj and not oil aj-:l . Putting everything together , we have the ibllowing ll MM-based model : a Pr ( f : i ' le ) = ~ II\[p ( a j laj -' , l ) . p(Y ) lea , )\] (4) af J = , with the following ingredients : ? Il MM alignment probability : p  ( i \] i ' , I ) or p(ajlaj_l , I ); ? translation probabflity : p(f\]e) . 
In addition , we assume that the tMM alignment probabilities p ( i \[ i' , \[) depend only on the jump width ( i-i') . Using a set of nonnegative parameters s ( i - i ' )  , we can write the IIMM alignment probabilities in the form :  4 i - i ' )   ( 5 ) p ( i li ' , i ) = E's ( 1-i ' )   1=1 This form ensures that for each word position i ' , i ' = 1 ,   . . . , I , the It MM alignment probabilities satisfy the norm Mization constraint  . 
Note the similarity between Equations (2) and (5) . The mixt m ; e model can be interpreted as a zeroth-order model in contrast to the first-order tlMM model  . 
As with the IBM2 model , we use again the maximum approximation :

Pr(fiSle ~) " ~ max\]--\[\[p(asl <* j-1 , z ) p(fjl < ~ , )\] (6) a '/ . ll . j , , , j = l In this case , the task of finding the optimal alignment is more involved than in the case of the mixture model  ( lBM2 )  . There ibre , we have to resort to dynainic programming for which we have the following typical reeursion formula : Q  ( i , j ) = p(fjlel) , nvax\[p(ili' ,  1)  . Q(i ', j-1)\]i = l, .   ,   , I Here , Q(i , j ) is a sort of partial probability as in time alignment for speech recognition  ( Jelinek ,  197@ . 
4 Experimental Results 4 . 1 The Task and the Corpus The models were tested on several tasks : ? the Avalanche Bulletins published by the 
Swiss Federal Institute for Snow and
Avalanche Research ( SHSAR ) in Davos,
Switzerland and made awtilable by the Eu-p " qI ropean Corpus Initiative  ( I , CI/MCI ,  1994) ; ? the Verbmobil Corpus consisting of spontaneously spoken dialogs in the domain of appointment scheduling  ( Wahlster , 1993); phrases from the tourists and t . ravel do cnain . 
( Eu Trans , 1996).
' l'able\]gives the details on the size of tit  <  ; corpora a , udt;\]t <' it ' vocal > ulary . It shottld I > e noted that in a . ll the s(; three ca . sestheratio el ' vocal ) t,-\]ary size a . ml num l)er of running words is not very faw ) rable . 
Tall)le , I : (, orpol : L(,o~ptsl , angua . ge Words Voc . Size
AvalancJte\]A\[\[rails

Froltch(~('~llall

I , ; nglish ( le11 an
English--1:77@-25 , \]27 dO172`\]/13For several years1)et ; weeu 83 and !) 2 , the Avalanche Bulletins are awdlabte for I > oth Get-ntan and I !' ren  (  ; \] l . The following is at yl ) i calsen--t < ; ncet > airfS ; onl the < ; or : IreS : Beizu (' . rstrechtholnm , Sl ) ~ i . tevtM ' eren'l'em-l ) eraJ , uren sind vou Samsta . ghis1) iens tag tn of gett auf < l <'~ t ; All > ennor(ls <' . iteun </ amAll > en- . 
ha . uptkanm lober halb2000m60 his80cm
Neuschneegel ' aJlen.
l ) ardestem p &' a tures d ' a borddlevdes , puisplusbasses , 60 h8 (1 cm deneige sent to mbs desamedih . mardimatins ur leversant her del ; laeft're des Alpesau-dessus de 2000 l\[1 . 
Anexa , nq ) lefi'om the Vet % mobil corpus is given in Figure 1 . 
4 . 2 Training and ILesults l , ; a ch of the three CO rl Jora . we rettsed to train 1) oth alignnmnt models , the mixture-I>ased alignment model in Eq . (1) and the ll MM-base < la . lignntent mod('l in Eq . ( d ) . ltere , we will consider the ex-p <' . rimenta . ltesl ; sont it <' . Avalanche corpus in more detail . The traii , ing procedure consiste ( l of the following steps : ? , Initialization training : IBMI model trahted for  t0 iterations of the i ' ; M algorithm . 
, , l , efinement traiuiug : The translation pcoba-1 ) ilities Dotn the initialization training wet ' (  ; use+d to initialize both the IBM2 model and the IIMM-based nligntnent mo < t < '+ l IBM2 Model: 5 iteratious using Lit ( " max-il numa . I ) proximatiol t ( Eq+ ( 3 ) ) I IMM Model : 5 iterations usiug l le max- . 
imumal ) l ) roximation ( F q . (6)) ' l'h ( , resulting perl > h : ' ~ xity(inverse g < ~ olu(;l . ricav-era , ge of the likelihoods ) for the dilferent l no ( lel save given iutim Tal>\[es 2 and 3 for the Awdanehe <: < ) rims . In adclitiout ; othe totali > erl > lexity , whi < ' . his the ' globa . l optimization criterion , the tables also show the perplexities of the translation probabilities and of the alignment probabilities  . The last line in Table 2 gives the perplexity measures wh ( ma . lJplying the rt laxiln unapproximation and CO ml > uting the perph'~xityint  ; \] l is approximation . 
These values are equal to the ones after initializing the  IBM2 and HMM models , as they should be . 
From Ta , ble 3, we can see . that the mixture alignment gives slightly better perplexity values for the translation l  ) roba . 1) ilities , whereas the IIMM model produces a smaller perplexity for the alignment l > rohal  ) ilities . In the calculatiot , of the , perplexities , th <' seld ; en( ; elength probal ) ility was not in = eluded . 
Tahle 2: IBMI : Translation , a , ligmnent and total pert ) h'~xil . y as a . fimction of ' the iteration . 
IterationTra , nslatiotl . Alignrnent Total 3 . 72 2 . 67 t . 87 1 . 86 20 . 07 20 . 07 20 . 07 20 . 07 20 . 07 1994 . 00 7/1 . 57 53 . 62 37 . 55 37 . 36
Max .  3 . 88 20 . 07 77 . !) 5'l'able3:'1 rans\]~+tion , aligmnent and totaJ perplexity as a function of the it cra  . tion for the IBM2(A ) and the IIMM model (13)
Iter . Tratmlat ; i(m
A0 l,\]
Aligni N . elJt3 . 88- 20 . 07 3 . 17 10 . 82 3 . 25 10 . 15 3 . 22 10 . 10 3 . 20 \] 0 . 06 3 . 18 10 . 05 3 . 88 20 . 07 3 . 37 7 . 99 3 . 46 6 . 17 ; . /17 5 . 90 " Ld 65 . 85 3 . `\]5 5 . 8, \]' l'otal 77 . 95 34 . 27 33 . 03 32 . 48 32 . 18 32 . 00 77 . 95 26 . 982 t . 36 20 . 48 20 . 2/1 20 . 18 Anoth < 2 rinl ; crc : sting question is whether the IIMM alignntent model helps in finding good and sharply fo  ( ' uss cd word + to-word ( -orres\]Jondences . 
Asan (; xamf , 1o , Table 4 gives a CO mlm+rison of the translatio J ~ probabil ities p  ( fle ) bct weet t the mixture and the IIMM align nw+nt model For the  (  , e , u + l word Alpen si id hang . The counts of the words a . regiven in brackets . The , reisvir Luallyno , : lilf c~rc~nce between the translation l . al > les for the two nn ) dels (1BM2 and IIMM ) . But + itt general , the tlMM model seems to giw ' . slightly better re-suits in the cases of (; , ttnatCOml+olmd words like Alpcus'i id ha'n , (Iv cr sant sud des Alpes which require \[' u , tc tion words in the trattslation . 

Table 4: Alpens/id hang.
IBM1 Alpes (684) 0 . 171 des (1968) 0 . 035 le (1419) 0 . 039 sud(416) 0 . 427 sur (769) 0 . 040 versant (431) 0 . 284
IBM2 Alpes (684) 0 . 276 sud (41 . 6) 0 . 371 versant (431) 0 . 356
HMM Alpes (684) 0 . 284 des (1968) 0 . 028 sud(416) 0 . 354 versant (431) 0 . 3 33 This is a result of the smoother position alignments produced by the HMM model  . A pronounced example is given in Figure 2 . ' She problem of the absolute position alignment can he demonstrated at the positions  ( a ) and ( c ) : both Schnee bretlge fahr und Schneever frachtungen have a high probability on neige  . The IBM2 models chooses the position near the diagonal , as this is the one with the higher probability . Again , Schnee brettge fahr generates de which explains the wrong alignment near the diagonal in  ( c )  . 
However , this strength of the HMM model can also be a weakness as in the case of est developpeist  . . . ent standen ( see ( b ) in Figure 2 . The required two large jumps are correctly found by the mixture model  , but not by the HMM model . These cases suggest an extention to the HMM model  . In general , there are only a small number of big jumps in the position alignments in a given sentence pair  . Therefore a model could be useful that distinguishes between local and big jumps  . 
The models have also been tested on the Verb mobil Translation Corpus as well as on a small Corpus used in the EuTrans project  . The sentences in the Eu Trans corpus are in general short phrases with simple grammatical structures  . 
However , the training corpus is very small and the produced alignments are generally of poor quality  . 
There is no marked difference for the two alignment models  . 
Table 5: Perplexity results ( b ) Verbmobil Corpus . 
for ( a ) Eu Trans and
Model Iter . Transl . Align . Total
IBM 110 2.610 6.233 16.267
IBM25 2.443 4.003 9.781
HMM 52.46 13.93 49.686
IBM 1104.373 10.674 46.672
IBM25 4.696 6.5383 0.706
It MM 54.85 95.45 226.495
The Verbmobil Corpus consists of spontaneous-ly spoken dialogs in the domain of appointment scheduling  . The assumption that every word in the source language is aligned to a word in the target language breaks down for many sentence pairs  , resulting in poor alignment . This in turn affects the quality of the translation probabilities  . 
Several extensions to the current IIMM based model could be used to tackle these problems : * The results presented here did not use the concept of the empty word  . For the HMM-based model this , however , requires a second-order rather than a first-order model  . 
. We could allow for multiword phrases in both languages  . 
? In addition to the absolute or relative alignment positions  , the alignment probabilities can be assumed to depend on part of speech tags or on the words themselves  . ( confer model 4 in ( Brown et al , 1990)) . 
5 Conclusion
In this paper , we have presented an it MM-based approach for rnodelling word aligmnents in parallel texts  . The characteristic feature of this approach is to make the alignment probabilities explicitly dependent on the alignment position of the previous word  . We have tested the model successfully on real data  . The HMM-based approach produces translation probabilities comparable to the mixture alignment model  . When looking at the position alignments those generated by the ItMM model are in general much smoother  . This could be especially helpful for languages uchas German  , where compound words are matched to several words in the source language  . On the other hand , large jumps due to different word orderings in the two languages are successfully modeled  . 
We are presently studying and testing an mltilevel HMM model that allows only a small number of large jumps  . The ultimate test of the different alignment and translation models can only be carried out in the framework of a fully operational translation system  . 
6 Acknowledgement
This research was partly supported by the ( \]er-man Federal Ministery of Education , Science , te-search and Technology under the Contract Number  01 IV 601 A ( Verbmobil ) and under the Esprit
Research Project 20268' Eu Trans).

L . E . Baum .  11972 . An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process  . In equalities , 3:1-8 . 






M2O00





















WALLIS + + + + + + + + + + + + + + + + + + + + + +  + + + ++ ++ + + + + + + + + + + + + + + + +++ + +  + + + ++ + + + + + + + + + + + + ++ + ++ +~ + + +  + + + + + + +/~ + + + + + + + + + +   t44  + ++++++++ ; / ; /#+++++++ j , +?++- I-~+++++~+++++++++~++ + + + + + +   .  ~ + ++ + + + + + +
O + + + + + + + + + + + + + + + + + + + + + + + + + + I + + + + + + + + + + + + - + + Mixture + j + + + +  + + + + + +   , + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +  + + + +  ( b ) + + + + + + + + + + + + + ++ ++ + + + + + + +++ + + + + + + ++ + + + + + + + ++ ++ + ++ + + + ++ + + + + + + + ++ ++ + + ++ + +++ + + + + + + + + + + + + ++ + + + + + + +  4  -+ + + + + + + + + + ++ + + + ++ + + + + +~ + + +  + + + + + + + + + + + ~ + ~ + + + + + + + + ~ + + + + + + + + + + + + + + / l ~ - Q-g + + + ~ + + + + +  + + +~ +   ++++++++1/+++++++++++1/++++_12/   ,   , + + + + + + + + + + + / + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + ++/--'+++ HMM ++++++++ I ++++ ++++++++ O--I~'-g-g +++ +++ +  + + ~ + + + + + + + + + + + + + + + + + + + + + + + ~- t ~ t ~ - ~ + + + + + + + + + + + + + + + + + + + + + + + + ~/~ + + + + + + + + + + + + + ? ? + +  ? ? + + + + + + + + ~ J + + + + + + + + + + + + + + + + + + + + + + + + + + + + ~ + + + + + + + + +  + + + + + + + + + + + + + + + + + + + + + + + +++  + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + 

Figure 2: Alignments generated by the IBM2 and the HMM model . 
Peter F . Brown , Vincent J . Della Piet , ra , Stephen A . I ) ella Pietra , and Robert1, Mercer . 
\]993 . The Mat , hema . tics of Statistical Machine'lS : unslalfion : Parameter Estimatiom  ( \] omputa-tional Linguistics ,  19(2):26")--31 1 . 
hlo Dagan , Ken Ctmreh , and William A . Gale.
1993 . l , obust 13 ilingual Word Alignment , for Machine Aided'l'rm~sl~ttionl ' rocecdings of the Workshop on Very Largc Corpora  , C , oluml ) us , 
Ohio , 18
ECI/MC\[: The European Corpus Initiative Mul-.
tilingual Corpus1 .  \[!)94 . Association for Com-pul ; ational binguistics . 
Eu Trans . ' l'he I ) et in i do no fa M'I''\['ask . ' l)eh-nieal Report , I , ~ f\]'ransProject 1996 ( I , 'orth-conuni , g ) , l ) ep to , de Sistemas Informaticos y Computacion ( DSIC ) , Universidad Politecnica de Valencia . 
Pascale I " ung , and Kenneth Ward Church . 11994.
K-vet : A flew N ) proach\[braligning par Mlel texts . Proceedings of COLING94, 1096-ll02,
Kyoto , Japan.
Frederik Jelinek .  1976 . Speech Recognition by Statistical Met ; hods . Proceedings of the \[ l~l?1'\],
Vol . 64, 532-556, April 11976.
Martin Kay , and Martin R Sscheisen .  1993 . Text-'lanslation Alignment . Computational Linguistics , 19(1):121-142 Wolfgang Wahlster . t993 . Verbmobil : ' l ? ransla-tion of Face-to-Face Dialogs  . Proceedings of the
MT'Summit IV , \]27-135, Kobe , Japan.

