Probabilistic Parsing and Psychological Plausibility 
Thorsten Brants and Matthew Crocker
Saarland University , COlnl ) U ; al ; ional Linguistics
D-6G041 Saarbriicken , Germany
brants , crocker@coi ?, un?-sb , de
Abstract
Given the recent evidence for prot ) a bilistic mechanisms in models of hmnanaml ) iguity resolution , this paper investigates the plausibility of exl ) loiting current wide-coverage ,  1 ) rob-al ) ilistic 1 ) arsing techniques to model hmnan linguistict ) ert'orman ( : e . Inl ) arl . i (: ulm ' , we investigate the , t ) cr for lnance of stan ( tar ( lstoclms-tic parsers when they arc revis (  ; ( ltoel ) crate incrementally , and with reduced nlenlory resources . Wet ) resent techniques for ranking and filtering ml Myses  , together with exl ) erimen-tal results . Our results confirm that stochastic parsers which a  ( lhere to these 1 ) sy ( ' hologi-cally lnotivated constraints achieve goo ( ll ) er-f ( )rman ( : e . Memory cast t ) ereduce ( t ( lown to 1% ( (: Oml ) are ( l to exhausit vesearch ) without reducing recall an ( l1 ) rox : ision . A ( lditionally , thes ( ; models exhil ) it substamtially faster l ) ertbrmance . 
FinMly , we ~ rgue that this gener M result is likely to hold for more sophisticated  , nndi ) sycholin-guistically plausil ) le , probal ) ilistic parsing models . 
1 Introduction
Language engineering and coml ) ut ~ tional psycholinguistics are often viewed as ( list net research progrmnmes : engineering so ht tion saim at practical methods which  ( ' an achieve good 1 ) erformance , typically paying little attention to linguistic or cognitive modelling  . Comlm-tationali ) sycholing , fistics , on the other hand , is often focussed on detailed mo ( lelling of humanlm haviour tbra relatively small number of well-studied constructions  . In this paper we suggest hat , broadly , the human sentence processing mechanism ( HSPM ) and current statis-ti ( : al parsing technology can be viewed as having similar ol  ) jectives : to optimally ( i . e . ral ) idly and accurately ) understand l ; he text and utl ; erances they encounter . 
Our aim is to show that largescale t ) robabilis-tict ) arsers , when subjected to basic cognitive constraints , can still achieve high levels of parsing accuracy  . If successful , this will contribute to at ) lausil ) h ; explanation of the fact th ~ t tI )( ; ()-\]) lc , in general , are also extremely accurate and rol ) llS(; . Sllcha1'o81111 ; Wollld also strellgth clt existing results showing that related l  ) robal ) ilistic lne ( ' hanisms can exl ) lain specific psycholinguistic phenomena . 
To investigate this issue , we construct a standard ' l ) as eline'stochastic parser , which mirrors t ; hepert brmance of a similar systems ( e . g . 
(, lohnson , 1998)) . We then consider an incre-re ( total version of th ( ' , parser , and ( ; v ~ , htat ( ; time tf'c ( : ts of several l ) rol ) al ) ilistic filtering strategies which m'eus (  , (lto1) runethel ) arser'search space , and ther ( ; l ) yr ('( lu ('(' , memory load . 
rio & , ,- ; sessth ( ; gener Mity of oltrresnll ; s for more Sol ) histi ( ; ate(t prot ) al ) ilistic models , we also conduct experiments using a model in which parent-node intbrmation is encoded on the  ( laughters . This increase in contextual information hast ) ( ; ( ; 11 shown 1 ; o improve t ) er for lnance ( . Johnson ,  1998) , and the model is also shown to be rolmst to the inerementality and memory constraints investigated here  . 
We present the results of parsing pertbr-mance ext ) eriments , showing the accuracy of these systems with respect to l  ) oth a parsed corpus and the 1 ) aseline parser . Our experiments suggest hat a strictly incremental model  , in which memory resources are substantially reduced through filtering  , can achievel ) reci-sion and recall which equals that of ' unrestricted ' systems  . Furthermore , implementation of these restrictions leads to substantially faster  1  ) ( ; rt brmance . In (: onchlsion , we argue that such 1 ) road-coverage probabilistic parsing plaining the human capacity to rapidly  , accurately , and robustly understand " garden variety " language  . This lends further supt ) ort top sycholinguistic a counts which posit probabilistic ambiguity resolution mechanisms to explain " garden path " phenomena  . 
It is important to reiterate that our intention here is only to investigate the performance of probabilistic parsers under psycholinguistically motivated constraints  . We do not argue for the psychological plausibility of SCFG parsers  ( or the parent-encoded variant ) per se . Our investigation of these models was motivated rather by our desire to obtain a generalizable result for these simple and well-understood models  , since obtaining similar results for more sophisticated models  ( e . g . ( Collins , 1996; Ratnaparkhi ,   199711 might have been attributed to special properties of these models  . Rather , the current result should be taken as support br the potential scale ability and performance of probabilistic I  ) sychological model such as those proposed by ( aurafsky , 1996) and ( Crocker and Brants , to appear ) . 
2 Psycholinguistic Motivation
Theories of human sentence processing have largely been shaped by the study of pathologies in tnnnan language processing behaviour  . Most psycholinguistic models seek to explain the df-ficulty people have in comprehending structures that are ambiguous or memory -intensive  ( see ( Crocker , 1999) for a recent overview ) . While often insight flfl , this approach diverts attention from the fact that people are in fact extremely accn rate and effective in understanding the vast majority of their " linguistic experience "  . 
This observation , combined with the mounting psycholinguistic evidence for statistically-based mechanisms  , leads us to investigate the merit of exploiting robust  , broadcoverage , probabilistie parsing systems as models of hm nan linguistic pert brmance  . 
The view that hm nan language processing can be viewed as an optimally adapted system  , within a probabilistic fl'a mework , is advanced by ( Chater et al ,  19981 , while ( Jurafsky ,   19961 has proposed a specific probabilistic parsing model of human sentence processing  . In work on human lexical category disambiguation , ( Crocker and Corley , to appear ) , have demonstrated that a standard ( iimrmnen-tal ) HMM-based part-of-speech tagger models the finding from a range of psycholinguistic experiments  . In related research , ( Crocker and Brants ,   19991 present evidence that an incremental stochastic parser based oll Cascaded Markov Models  ( Brants ,  1999 ) can account t brarange of experimentally observed local ambiguity preferences  . These include NP/S complement ambiguities , reduced relative clauses , noun-verb category ambiguities , and ' that '- ambiguities ( where ' that ' can be either a complementizer or a determiner  )   ( Crocker and
Brants , to appear).
Crucially , however , there are differences between the classes of mechanisms which are psychologically plausible  , and those which prevail in current language technology  . We suggest that two of the most important differences concern in crcmentality ~ and memory  7vso'urces   . There is overwhehning experimental evidence that people construct connected  ( i . e . semantically interpretable ) analyses for each initial substring of an utterance  , as it is encountered . That is , processing takes place incrementally , from left to right , on a word by word basis . 
Secondly , it is universally accecpted that people can at most consider a relatively small number of competing analyses  ( indeed , some would argue that number is one , i . e . processing is strictly serial ) . In contrast , many existing stochastic parsers are " unrestricted "  , in that they are opt infised t braccuracy , and ignore such t ) sychologically motivated constraints . Thus the appropriateness of nsing broadcoverage probabilistic parsers to model the high level of human performance is contingent upon being able to maintain these levels of accuracy when the constraints of " incrementality and resource limi -rations are imposed  . 
3 Incremental Stochastic
ContextFree Parsing
The fbllowing assumes that the reader is familiar with stochastic contextfree grammars  ( SCFG ) and stochastic chart-parsing techniques . A good introduction can be found , e . g . , in ( Manning and Schfitze , 19991 . We use standard abbreviations for terminial nodes  , 11051-terminal nodes , rules and probabilities . 

This t ) t q ) er in v c sl ; igate stochastic ( ; onl ; ( ; xl ; -fl ' ee parsing l ) as cdon ~ grmmmu " ( ; hatis(tcrivc(l from a trcel ) ank , starting with 1) art-ofsl ) eechta , gsast (; rlninals . The gl : ; ~nllnt ~ ris(lcriv ( ; dl ) y(:olle(:tingM1rul(' . s X -+ c ~ th ; t to c (: ur in the tr (' , (;-bankmM (; heirffe(lU(m(:i('~sf . The l ) l'()l ); tl ) ilil ; y of a rule is set to . f(xl'(x-(:l )
E . f(xfl\] , br ~ descril ) l ; ion of treebank grammars see ( Charniak ,  1 . 996) . The gr~mmm r does not coii-ta . inc-rules , oth ( : rw is ( : th ( : r ( : is no restriction oll the rules . In particular , w (: do not r (: quir ('
Chomsky-Norm M-Form.
In addition to the rult : stha ( ; corr(:st ) ond ( ; osl ; rucl ; ur (: sinth (: corpus , w(:a . dd ; ~ newst~u : l ; sylnl ) ol ROOT to l ; h ( ; grnmmar and rules ROOT-~X for all non-t ; ( ; rminals X to gel ; lwx with l ) rol ) al ) iliti ( ' s ( h: ) :iv ( : dl ' rolnth ( : root n ( ) ( t ( : sinth ( : tor t ) us I . 
For t ) m : singth(:segr ~ unmn)'s , w (: r (: lyuponnstan(tardl)oLi ; onl-Ut ) (: ha . rl , -t ) arsing t ( : ( : hniqu ( : with n modification for in ( : rcmental parsing , i . (: . , tbt " each word , all edges nr ( : proc ( :ss ( : d and l ) ossi-b\]y1 ) run ( : d1 ) (:ti ) r ( : \] ) ro ( :e ( : ( ling to the next word . 
Th (: outlil mofth (: Mgorithm is as follows.
A(:hart ; (: ntry1~(: onsists of asl ; ; u:I , aim (: n(l1) o-sitioni ; rodj , a ( tott (: drul (: X ~( ~: . '7, timinsi(t (: l ) rol ) nl ) ility fl(Xi , . j ) thud ; Xg(:n(:ra . tx : sl ; ll(:t(:rmi-hal string from t ) osi (: ionito . 7 , mM information M ) out th ( : most l ) robat ) \] ( : i lL~i ( t ( ' stru ( :i ; ur (: . 1t 7th ( : do to fth ( : dotte ( truh : is ntth ( ' rightmosti ) osi-tion , the corresl ) on dill g ( : ( lg ( : is an inactive edg ( : . 
If the ( totisatmty other 1) osition , i l ; is m t , , ctivc , edge . Imu : l ; ivo , e(tg cs repr (' , scntre('ogniz (' , dhypo-(:heti(:a , 1 constituents , whil(;a (: tiv(;(;(lg(' , sr ( ; 1)r( ; -s(:nt1) r (: lixes of hyl ) ol ; hetic M(:()llsi ; it ; ll(:lll ; s . 
Th (: itht(:rminal nod(:I , il ; lla , t ; (: nt(:rsth(:(:hart gencra , t cs an in active edge for l ;\] m span(i-1 , i ) . 
Ba , sed on this , n ( ; w active midinactive ( ; ( lges are generated according to the stan ( t~tr ( t algorithm . 
Sine (: we are ilf l ; (: r(:stcdinth(:mosti ) robM ) lepars(: , the chart can be minimized in th ( : tbl-lowing way whik:sti\]l1 ) crfi ) rming an ( ' xhaustiv ( : search . If " ther (: is mor(:l ; hm ~ one ( : ( lg ( ~ that covers a span ( i , j ) having ( ; h(' , sa , menon-t (: rm in M symbol on th ( ; lefIAmndside of th (: ( to ( , ( x : ( l rule , 1 The ROOT node is used in t ; ernally fl ) r parsing ; it is neither emitted nor count , ed for recall and l ) recision . 
only the one with the highest inside prol ) M ) ility is k (  ; 1) till tit ( ; (:\] mrt . The other scmmot con-trilmt ( ; to th ( ; most i ) rol ) M ) le1) nrse . .
For an in a ( ' tiv ( : edges i ) aiming i to j and rei ) -rcs ( mting the rule X--> y1 .   .   . yq  ~ the insidel ) robM ) ility/31 is set to \[ dil = 1" ( x-+H ( 2 ) /=\] wher ( : il and jlmm'k the start and end t ) ostition of Yl having i = i l n n d j = J r . The insid ( : prol ) M ) ility t bran active cdg ( : fiA with the dot after th ( : kth syml ) ol of th ( : right-hmM side is sol , to k
II < r'tItil,jl(3) l-d
W ( :  ( lo not use the t ) rol ) M ) i \] ity of th ( : rule a . t this point . This allows us to (' oral ) in (: a . ll(:(Ig(:s with ( ; h(:sam(:st)m ~ and th(:dotal ; th(:sam(:1) osition but with(liiI'er(:uI ; symbols on the l ( , ft-hm M side . 
Jntrodu ( : ing a distinguish ( : ( 1M't-handsid ( : only for in ~ mtiv ( :  ( lg ( 's significantly r ( :du (  ; (: sth (: nun > b (: r of a (:( ; iv (: (: dg (: s in the (: hm't . This goes onest , et ) furth ( :r than lint ) licitly right-1 ) in a rizing th ( : grmmnar ; not only suilix ( : s of right-hmMs i ( h : s are join ( : ( t , but also l ; hc('or r(:sponding l(:fi;-h and sid(:s . 
d Memory Restrictions \ ? ( : inv ( : stig~rt ( : th ( :  ( dimin ~ I ; ion ( pruning )   ( ) f edges from th ( :  ( ' hnrt in our in ( 'r ( :nl ( :n ; a \]) re's-ing sch(:m (: . Aft ( :r processing a word and b ( : fi ) )' ( : 1 ) roc ( : cding to then ( : xt word during incremental 1 ) re:sing , low rnnk ( , dedges ~ r (: removed . This is (: ( luivM(:lfl ; t ; ( ) imposing m ( : mory rcsia ' ictions on the t ) ro ( : ( 'ssing system . 
The , original algorithm k (' ei ) son ( ; edge in th ( :  ( : hart fi ) reach ( : oml ) ination of span ( start and cn ( l position ) ~md non-tcrmimd symbol ( for inactive edges ) or right-hand side l ) r ( : fixcs of ( lot ; -te(trules ( for active edges ) . With 1)tinting , we restric ( ; the mmfl ) cr of edges allowed per span . 
The limit ~ tion ( : anb ( : cxi ) ress cd in two ways : 1 . Va'riablebcam, . Sch : ct a threshold 0 > 1 . 
Edg (: c . is removed , ill its 1) rol ) ability is p~: , I ; lm1 ) csl ; l ) rol ) M ) ility fi ) r the span is Pl , and v , ; < pl_ . ( ~ l ) 2 . Fixed beam . Select a maximum number of edges per span m . An edge e is removed , if its prot ) ability is not in the first m highest probabilities t bredges with the same span  . 
We pertbrmed exl ) eriments using both types of beauls . Fixed beams yielded consistently better results than variable beams when t  ) lotting chart size vs . Fscore . There ibre , the following results are reported tbr fixed t ) eams . 
We , compare and rank edges covering the same span only  , and we rank active and in active edges separately . This is in contrast to ( Charniak et al , 1998) who rank all edges . They use nornmlization in order to account tbr different spans since in general  , edges for longer spans involve more nmltiplications of t  ) robabil-ities , yielding lower probabilities . Charniak et al . 's normalization value is calculated by a dil -ferent probability model than the inside probabilities of the edges  . So , in addition to the normalization for different span lengths  , they need a normalizatio11 constant hat accounts t br the different probability models  . 
This investigation is based on a much simt ) ler ranking tbrmula . We use what can be described as the unigram probability of a nonterminal node  , i . e . , the a priori prot ) ability of the colresl ) onding non-tern linal symbol ( s ) times the inside t ) robat ) ility . Thus , fi ~ raninactive edge ( i , j , X -->( ~ , /31(Xi , j ) , we use the l ) rob~fl ) ilityPm(Xi , j ) = P ( X ) . P ( tg .   .   . tj_IIX ) (5) = for ranking . This is the prol ) ability of the node and its yield being present in a parse  . The higher this value , ; lie better is this node . fl I is the inside probability for inactive edges as given in eqnation  2  , P ( X ) is the a priori probability t br nonterminal X ,   ( as estimated from the frequency in the training CO rlmS  ) and Pm is the probability of the edge t br the nonterminal X spanning positions i to j that is used t brranking  . 
For an active edge i , j , X--~y1... yk.
yk+lym , y ) k ? " ~ ,   , ~)) "'" ~ A(il , jl ( the ( to t is a I " ter the kth symbol of ll Se : the right hand side  ) we ( 7 ) = P ( YI .   .   . Yk ) . flA(EI ~, : h .   .   . Yi  ~ , jk ) (9) p(yl , , , y k ) can be read ( ) If the corpus . It is the a priori probability that the righthand side of a production has the prefix  y1   . . . y/c , which is estilnated by f(yl .   .   . y t ~ is prefix )00)
N where N is the total number of productions in the corpus  2  , i = ij , j = j / ~ and flA is the inside probability of the pretix  . 
5 Experiments 5.1 Data
We use sections 2  -  21 of the Wall Street Jour-l Yecl ) ank ( Marcusel ; al . , nal part of ' the Penn ~ " 1993 ) to generate a treebank grammar . Traces , flmctional tags and other tag extensions that do not mark syntactic ategory are removed before training  3  . No other modifications are made . For testing , we use the \] 578 sentences of length 40 or less of section 22  . The input to the parser is the sequence of i ) art-of speech tags . 
5.2 Evaluation
For evaluation , we use the parsew fimeasures and report label d Fscore  ( the harmol fiC mean of labeled recall and labeled precision  )  . R . eport-ing the Fscore make sore " results comt ) aral ) le to those of other previous experinmnts using the same datasets  . As an leasure of the anlount of work done by the parser  , we report the size of the chart . Themnnl ) er of active and imm-rive edges that enter the chart is given t br the exhaustive search  , not cored ; lug those hypothetical edges the ft are replaced or rejected because there is an alternative dge with higher t  ) roba-t ) ility 4 . For t ) runed search , we give : tie percentage of edges required . 
5.3 Fixed Beam
For our experiments , we define the beam by a maximunl number of edges per span  . Beams for active and in active edges are set separately  . 
The Imams run from 2 to 12 , and we test all 2 Here , we use proper prefixes , i . e . , all prefixes not including the last element . 
a As an example , PP-TMP=3 is replaced 173, PP.
4 The size of the chart is corot ) arable to the " number of edges popped " as given in  ( Chanfiak et al ,  1998) . 
114 i(D 87 ocJ ~74 z $
Results with Original and Parent Encoding
Active:8/ma ? . : l ' ~ v ? " , " ~ I ; t (: lix '(" ( i / jj ~ activ (' , : 3
C "'' ; " Fllld Ct1 V(12 active : 9 in active , : 2 active:3/1 . 0 1 . 2 in a (: tiv (',:6 ilmctive:8a(:l . ive:d . a('tivo , : 7
II1\[\]i IIi--\] . d1 . 6 1 . 8 2 . 0 2 . 2 2 . d2 . 6 2 . 8 3 . 0% (: hart ; size Figure 1:\]! xt ) erimental results tbrincreJnelfl ; al parsing and t ) rmfing . The figm : e shows the percentage of edges relative to  ( ' , xhaustiv(;s (; ar(:hmidl ; h(' , F-s(:()rea (: hieved with this chart size . Exhaustive search yiehled 71 . 21% fin " th (; originalen (: o(ting and 7!) . 28% for the I ) arent(m (: o(ting . l/ . c , sull ; s in the grey ar ( ; as are equiw flent with a ( : ( ) nli ( l ( ' n ( : ( '~  ( tegr ( ' , e of (~ = : 0 . 99 . 
12\]comlfi\]~ati(ms of the , s(~lmmus for ac:i ; iv can dill actiw ~ edges , l ~ ach setting results in alm . ri ; ic-ulm " average size of l ; he chart and an Fscore , which arc tel ) erredill ( ; he following se(:l ; ioll . 
5.4 Experimental Results
The results of our 121tes ( ; Hills with ( t if l ' erent settings for active and in ; u : tivc\])(~a . msm'e given in figure 1 . The ( tittgranl shows ch ~ trt sizes vs . 
labeled Fscores . It sorts char ; sizes across different sel ; l ; ings of the beams . If several beam set t ; ings result in equiw denfichart sizes , the diagramcent ; tins the one yielding th (' , highes , F -
SCOI'(L
The 111 ~ ill tinding is thai : we can r ( 'xlu ( : e the size of the chart to l ) el ; ween1% and 3% of the size required fi ) rexhaustives (  , ar (: h without affecting the results . Only very small 1) camsd(;grad('t)ert brmance 5 . Theeiti ; ct occurs for both models despite the simple ranking for mub ~  . 
This significantly reduces memory r ( , quirements ' ~ Givc , n the ' amount of test data (26 , 322 nonterminal nod(!s) , results within a rang (' of around 0 . 7% arccquiv-al(mt with a (: on fid cnc(; degr (' , ( , of (~=99% . 
(given as size of the chart ) and increases l ) m'sing qmed . 
i1 t Exhaustive search yields an I-Score of 71 . 21% when using the original Petal %' eel ) ank cn ( : od h~g . () nlyaround 1% the edges are re-(tuir (' . d to yield e . ( tuiwd cnt resul(;s with in crcm(, . n-tal processing and printing after each word is added to the chart  ;  . This result is , among other settings , obtained by a tixcd beam of 2 for inactive edges and 3 t in " active e ( lgesri1 , br the parmt tencoding , exhaustive search yields an l , - Scorc of 79 . 28% . Only 1 ) etween 2 mM 3% of the edges are required to y Md an equiw flcnt result with incremental t  ) l ' OCcS Sillg and pruning . As an cXmnl ) le , the point at size = 3 . 0% Fscore = 79 . 1% is generated by the beam setting of 12 for imml ; ive and 9 t bractive edges . The parent encoding yields around 8% higher Fscores but it also imposes a higher absolute and relative memory load on t  ; he process . 
The higher ( hw'ee of par~dlelism in l ; he inactive ( ; Using variable Imams , wc would nccd\] . 95% of the\[: hart entries 1 ; o a chieve an ( Klllivalen I ; F-scor(x node . In terms of pure node categories , the average number of parallel nodes at this point is  3  . 5 7  . 
Exhaustive search for the base encoding needs in average  140  , 000 edges per sentence , t brtile parent encoding 200 , 000 edges ; equivalent results for the base encoding can be achieved with around  1% of these edges , equivalent results t br the parent encoding need between  2 and 3%  . 
The lowermml ber of edges significantly increases parsing speed  . Using exhaustive search t br the base model , the parser processes 3 . 0 tokens per second ( measured on a Pentium III 500 ; no serious efforts of optimization have gone into the parser  )  . With a chart size of 1% , speed is 630 tokens/second . This is a factor of 210 without decreasing accuracy . Sl ) eed for the parent model is 0 . 5 tokens/second ( exhaustive ) and 111 tokens/seconds ( 3 . 0% chart size ) , yielding an improvement by factor 220 . 
6 Related Work
Probably mostly related to the work reported here are  ( Charniak et al , 1998) and ( Roark and Johnson ,  1999) . Both report on significantly improved parsing efl : iciency by selecting only subset of edges tbr processing  . There are three main differences to our at ) t ) roach . One is that they use a ranking fbr bestfirst search while we immediately prune hypotheses  . They need to store a large number edges because it is not known in advance how maw of the edges will be used until a parse is found  . Tile second difference is that we proceed strictly incrementally without lookahead  . ( Chanfiak et al , 1998) use a nonincremental procedure , ( Roark and Johnson , 1999) use a lookahead of one word . 
Thirdly , we use a much simpler ranking tbn nula.
Additionally , ( Chanfiak et al , 1998) and ( Roark and Johnson ,  1999 ) do not use the original Penn tree encoding t br the context-fl ' eestructures  . Betbre training and parsing , they change/remove some of the productions and introduce new part-of-speech tags tbr auxiliaries  . 
The exact effect of these modifications is unknown  , and it is unclear if these affect compa-7For the active chart , lm ralellism cannot be given for different nodes types since active edges are introduced fbr right hand side prefixes  , collapsing all possible lefthand sides . 
rability to our results.
Tile heavy restrictions in our method ( immediate pruning , no lookahead , very simple ranking formula ) have consequences on the accuracy . 
Using right context and sorting instead of pruning yields roughly  2% higher results ( compared to our base encoding S )  . But our work shows that even with these massive restrictions  , the chart size can be reduced to 1% without a decrease in accuracy when compared to exhaustive search  . 
7 Conclusions
A central challenge in computational psycholinguistics is to explaiu how it is that people are so accurate and robust in processing language  . 
Given the substantial psycholinguistic evidence t br statistical cognitive mechanisms  , our objective in this paper was to assess the plausibility of using wide-coverage probabilistic parsers to model lmman linguistic performance  . In particular , we set out to investigate the effects of imposing incremental processing and significant memory limitations on such parsers  . 
The central finding of our experiments i that incremental parsing with massive  ( 97% - 99% ) pruning of the search space does not impair the accuracy of stochastic on text-free parsers  . 
This basic finding was rotmstacross different settings of the beams and t br the original Penn Treebank encoding as well as the parent encoding  . We did however , observe significantly reduced memory and time requirements when using combined active/inactive dgefiltering  . To our knowledge , this is the first investigation on treebank grammars that systematically varies the beamt br pruning  . 
Our ainlin this paper is not to challenge state -of-the-art parsing accuracy results  . For our experiments we used a purely context-ti 'ee stochastic parser combined with a very simple pruning scheme based on simple " unigram " probabilities  , and no use of right context . We do , however suggest hat our result should apply to richer  , more sophistacted probabilistic SComparison of results is not straightforward since  ( Roark and Johnson ,  1999 ) report accuracies only tbr those sentences for which a parse tree was generated  ( between 93 and 98% of the sentences )  , while our parser ( except for very small Imams ) generates parses for virtually all sentences , hence we report ; accuracies for all sentences . 
116 models , e . g . when adding words t ~ tistics to the model ( Charni ~ d ? ,  1997) . 
We there ibre conclude the ftwide-covcr~ge , prol ) ~fl ) ilistic pnrsers do not suffer impaired a ( '-curacy when subject to strict cognii ; iv ( ~ meXnOl'y limitntions mM incremental processing . Fm'-thermore , parse times are sut ) stm~ti ~ fily reduced . 
This sltggt ' , sts that it ; m~ylie fruit ; tiff to tlur , sllC the use of these models within ?' , onlt ) utation all ) sycholinguistics , where it : is necessary to explain not Olfly the relatively r~tr  (  ; ' pathologies ' of the hmmm parser , but also its mor ( ; fl'e ( tuently ol ) scrved ~ u : ( : ur~my ~ ( 1 rol ) llSiilless . 

Thorst ; en\]h'mfl ; s .  1999 . Cascadt ; dMm'kov models . In P ' rocecdings Vf9th,Cm@~'t' . ' m : e . of the EuT vpea'n Chapter of the Association , fro " Com . p'atatiou , al Linguistics EA6'\])-99, B(;rg(;n,

Eugene Charni~k , Sharon ( \] ohlwater , and Mnrk , Johnson .  1998 . ltMge-b~sedlmst-tirst(:hart pro'sing . Inl ~' rocec . dings of l , hc . Si : cl , h . l , Vor/ , :-shop onl/cry LaT ~\] (: Corpora ( WVLC-9S ) , 
Montreal , K ~ ma(la .
Eugene Ch~rni ~ fl ?.199 6.'15: ee-bank grmmm~rs.
In P ' rocecding , ~ of t , h , cTh , irtec'nth , National Cm@rc' . nceonA'rt'ti/iciallntdlig(:,m : ~:, l ) a . g ( ,, s10311036, MenloPnrk:AAA\]Press/M1T l)i . ess . 
\]! htgen (' . Chm:nia . k .  1997 . Sl ; a . i ; isti(:al\]mrs-ing wit ; h~t context-fr(:(~gl: ; 411 11 llVtl '2 . 1 ~ 11 (1\voF ( statistics . In P~'occ . cdingsqf the\],b,a'rt,(:enthNationalCo ~@' r(' . nceo'nA'rt ~ ificial Intelligence , pagc . s10311036, Menlo Park : AAAI
Press/MIT Press.
Nicholas Chafer , Matthew Crock ( ; l' , ~md Martin Pickcring .  1998 . The rational analysis of inquiry : The case . for parsign . In Charter and O~ks for (1 , editors , Ratio ' halModelso / " Cog'ni-tion . Oxford University Press . 
Michael Collins .  1!196 . A newst ~ tistical l ) arscrb~tse . donl ) igr ~ unlexical depend (; neies , in Proceedings of ACL96 , Sa , llta , Cruz , CA , 

Matthew Crocker and Thorsten Br~mts . 1999.
Incremental probabilisti ( : l nodels of lmman linguistic perform ; race . In The 5th Cm@r-cnce on Arc : hit cctu ~ v . sa'nd Mcch , anism . , sfor La'nguagc Processing , Edi~flmrgh , U . K . 
Matthew Crocker and Thorst ; enBrmfl ; s . to~t)-l ) car . Wide cover ~ gel ) rol ) ~fl filistic sentence processing . Journal of Psych , oling'aistic Research , , November 2000 . 
M~t thew Cro (' kexmMSteil ' an Corley . to ~ l ) - peru : . Modulm " nrchitectures and statistic Mmcchnnisms : The case  . frolli lexical category disnmbiguntion . In Merlo and Stevenson , editors , The Lczical Basis of Sentence Process-in 9 . John Bcnjamins . 
1 VI~t the w Crocker .  1999 . Mech ~ misms for scn-;ellce , tn ' oe essing . In Garrod and Pieker-ing , editors , Language Proc~s sing . Psychology \]) ross , London , UK . 
Mm'k , Johnson .  1998 . PCFG models of linguistic t ; rec\]'el)rcse\]fl ; ~ tions . Com , p , utational Lin-g'aistic . ~, 24(4):613 632 . 
\]) ~ mi(;l . \]m : at ~ ky .  \]996 . At ) robabilistic no ( Moflexi ( : ~ tlnnd syntactica ( : ( : ess and ( lisambigua-tion . Cognitive Science , 20:137194 . 
Christot ) her Mmming mid Him'ieh S(:lfiil ; ze_19 99 . l , b , a . n datiou , sof Statistical Natural Lan-9'uag (: P ' roct ' . s , si'ng . MKI ' Press , Cmnl ) ridge,

Mit (: hell I Vlarmts , \] eatrice S~mtorini , and Mary Ann M~rcinkiewicz .  1993 . Building alm : gemmotated corl ) us of English : The P ( mnTreet ) ank . Computational Linguistics , !)(2):31330 . 
A ( twaitl/ . ~ ttnat)~trkhi .  11!)!)7 . A\]inem " ol ) served times tnl ; isI ; ic ; d t ) m ; ser based on m~tximmn entropy models . In \]) ' rocc . c:ding . s of the Co ' , : fcr-?:'m:co'n Empirical Methods in Nat'a'ralLa ' n-g'uafleP'lvccssing\]'?MNLP-gZProvidence  ,  11\] . 
\]h'innRonrk ~ md Mm:k Johnson .  1!199 . Efficient t ) rol ) al ) ilisl , ictot)-(lown ; rodleft-(:orner pars-illg . hi \]) ' l'o cccdi ~, . ( l , s of the . , ~7111, A ~ l , t,'ttalMcc . t-i'ng of the A . ssociation for Cou ~, p ' atation Lin-g'aistic . ~ACL-99, M~rybmd . 

