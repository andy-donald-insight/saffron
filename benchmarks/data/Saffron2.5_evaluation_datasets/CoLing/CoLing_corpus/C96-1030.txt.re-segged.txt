Example-Based Machine Translation in the Pangloss System 
Ralf D . Brown
( Jenter for Machine Translation
( ~ arncgie Mellon\[lniversity
5000 Fort ) (' sAw ; nuc
l'ittsburgll , PA 15213-', ~890
ralf@cs , cmu.edu
Abstract
The Pangloss Example-Based Machine
Translation engine ( I'an EI3MT ) lisa
translation system reql , iring essentially no knowledge of the structure of a language  , merely a large parallel corpus of example sentences at n\[a bilingual dictionary  . Input texts are segmented into sequences of words occurring in the corpus  , for which translations are determined by subsententia \[ alignment of the sentence pairs containing those sequences  . These partial translations are then combined with the results of other translation engines to form the final translation produced by the Pangloss system  . In an internal evaluation , Pan EBMT achieved 70 . 2% coverage of unrestricted Spanish newswire text , despite a simplistic sub-sententia\[alignment algorithm  , a subopritual dictionary , and a corpus Dora a different domain than the evalual  , iontexts . 
1 Introduction
Pangloss ( Nirenburgel ; al . , 1995 ) is a multi-engine machine translation system , in which several translation engines are . run in parallel to propose translations of various portions of the input  , Dora which the final translation is selected by a statistical anguage model  .   Panl'3BMT is one of the translation engines used by Pangloss  . 
EBMT is essentially translation-by-analogy : given a source-language passage S and a collection of aligned source/target text pairs  , lind the " best " match for S in the source -language half of the text collection  , and accep the target-language half of that match as the translation  . Pan EBMT , like other example-based translation systems , uses essentially no knowledge about its source or target languages  ; what little knowledge it does use is optional , and is supplied in ae on Iiguration file . Its 1This work as part of the l'angloss project was supported I  ) ytim U . S . I ) epartment of Defense three main knowledge sources arc : a sententially-aligned parallel bilingual corpus  ; a bilingual dictionary ; and a target-language root/synonym list ,  . 
The fourth ( minor and optional ) knowledge source is the hmguage-specific information provided in the conliguration tile  , which consists of n list of tokenizations equating words within classes such as  w0ekdays   , a list of words which n tay be elided during alignment  ( such as art Mes )  , and a list of words which may be inserted 2 Parallel Bilingual Corpus The corpus used by PanEBMT consists of a set of source/target sentence  , pairs , and is flflly indexed on t , he source-languages ntences . The corpus is not aligned at any granularity liner than the sentence pair  ; subsententia \] alignment is perfornled at runtime based on the sentence fragment se-let  ; ted and the other knowledge sources . 
The corpus index lists all occurrences of every word and punctuation mark in the source-language sentences contained in the corpus  . The index has been designed to permit incremental updates  , allowing new sentence pairs to be added to the corpus as they become a wulable  ( for example , to implement a translation memory with thesy s -tem's own output  )  . The text is tokenized prior to indexing , so that words in any of the equivalence classes detined in the EBMT contiguration tile  ( such as month names , countries , or measuring units ) , as well as the predetined equiw dence class < nunt lmr >  , are indexed under the equivalence class rather than their own names  . For each distinct token , the index contains a list of tile token's occurrences  , consisting of a sentence identifier and the word number within the sentence  . At translation time , f'an EI~MT back-substitute stile appropriate target-language word into any translation which involves any tokenized words  . 
' rile bilingual corpus used for the results reported here consists of  726  , 4 06 Spanish-English sentence pairs drawn primarily from the IIN Multilingual  ( ~' or pus available fl ' om tile l , inguistic Data ( Jonsortium ( Graff and Finch , 1992) ( Figurel) , with a small admixture of texts from the Pan -recomendacioness onlassiguientes : 
The sources of these comments and recommendations are:  E1 informe de la Junta de Auditoresala
A samble a General que in cluyel as observaciones del Director Ejecutivo del UNICE Fsobrelos comentariosy recomendaciones de la Junta de 
Auditores;
The report of the Board of Auditors to the General Assembly which incorporates the observations of the Executive Director of UNICEF on the comments and recommendations of the Board of 
Auditors;
Figure 1: Corpus Sentence Pairs ( A CADMICOSAC A DEMICS A CADE MICAL
TITLE SDEGREES )   ( ACAECIDOHAPPEN )   ( ACAECIDOSH APPEN )   ( A CANTONAD ASCANTON QUARTERTROOPS )   ( A CANTONAMIEN TO CANTON MENT )   ( A CARREACARRY CARTHAUL TRANSPORT
CAUSE OCCASION )   ( A CARREABACARRY CARTHAUL TRANSPORT
CAUSE OCCASION )   ( A CARREARON CARRY CARTH AUL TRANS PORT
CAUSE OCCASION )   ( A CARREARTRANS PORTH AUL CARTCARRY
LUGALONG BRING DOWN CAUSE OCCASION
ITSTRAIN RESULTGIVERISE)
Figure 2: Bilingual Dictionary Entries
American Health Organization and prior project evaluations  2  , indexed as described above . 
Together , the bilingual dictionary and target-language list  , of roots and synonyms ( extracted from WordNet when translating into English  ) provide the necessary information to lind associations between source-language and target -language words in the selected sentence pairs  . 
These associations are used in performing subsentential alignment  . A source word is considered to be associated with a target-language word whenever either the target word itself or any of the words in its root /synonym list appear in the list of possible translations for the source word given by the dictionary  . 
Not all words will be associated one-to-one ; however , the current implementation requires that at least one such unique association be found in order to provide an anchor for the alignment pro-tess  . 
3 Implementation
P an EBMT is implemented in C++ , using the Framepa C library ( Brown ,  1996 ) for accessing Lisp data structures stored in files or sent from the main Pangloss module via Unixpipes  . PanEBMT consists of approximately 13 , 300 lines of code , including the code for a glossary mode which will not be described here  . 
PanEBMT uses are processed version of the bilingual dictionary used by Pangloss's dictionary translation engine  ( Figure 2 )  . There processing consists of removing various high-frequency words and splitting all nmlti-word definitions into a list of single words  , needed to find one-to-one associations . 
2 10250 sentence pairs stern from the PAIO corpus and 552 pairs from evaluations . 
4 EBMT's Place in Pangloss
PanEBMT is merely one of the translation engines used by Pangloss  ; the others are transfer engines ( dictionaries and glossaries ) and a knowledge-based machine translation engine ( Figure 3 )  . Each of these produces a set of candidate translations for various segments of the input  , which are then combined into a chart ( Figure 3) . The chart is passed through a statistical language model to determine the best path through the chart  , which is then output as the translation of the original input sentence  . 
5 EBMT Operation
The EBMT engine produces translations in two phases :  1  . find chunks by searching the corpus index for occurrences of consecutive words from the input text  2  . perform subsentent i alignment on each sentence pair found in the first phase to determine the translation of the chunk In constrast with other work on example-based translation  , such as ( Maruyam and Watanabe ,  1992 ) or early Pangloss EBMT experiments ( Nirenburg et al ,  1993) , PanEBMT does not find an optimal partitioning of the input  . Instead , it attempts to produce translations of every word sequence in the input sentence which appears in its corpus  . The final selection of the " correct " cover for the input is left for the statistical an-guage model  , as is the case for all of the other translation engines in Pangloss  . An advantage of this approach is that ; it avoids discarding possible chunks merely because they are not part of the " optimal " cover for the input  , instead selecting the input coverage by how well the translations fit together to form a complete translation  . 

Transfer M'I)(\]MAT l'ost-cdit)
HA'l'al'getText
Source Text \] < 5/
Figure 3: l'angloss Machine q'r ; mslation System
Architecture 3'0 lind chunks , the engine sequentially looks up each word of tile input in the index  . The oc<:ur-rence list for each word is comp~tred ; tgains t the occurrence list for the prior word and against the list of chunks extending to the prior word  . For c , ~u ; hocctlrrence which is adjacent to all occn r-l ' elwe of the prior word  , a new chunk is created or an existing chunk is extended as appropriate  . 
Alterprocessing all input words in this tmmner , the engine has determined all possible substrings of the input containing at least two words which are  ; present in the corpus . Since the more Dequent word sequences <: a no < : curh undreds of times in the e or l  ) uS , the list of chunks is culled to eliminate all but the last tlve  ( by default ) occurrences of any distinct word sequence . By selecting the last occurrences of each word sequence  , one effectively gives the most recent additions to the corpus the highest weight  , precisely what is needed for a translation meanory  . 
Next , the sentence pairs containing tile chunks retold in the lirst phase are read from disk  , and alignment is performed on each in order to determine the translation of the chunk unless the match is agains the entire COl'pus entry  , in which case the entire target-languages ntence is taken as the translation  . Alignment currently uses a rather simplistic brnte-force approach very similar to that of  ( Nirenburgetel . , 1994 ) which identifies the minimum and maximum possible segments of the target-languages ntence which could possibly correspond to the chunk  , and then applies a scoring fimction to ew ' , rypossible substring of the maximum segment containing at least the luinimm n segment  . The suh string with the best score is then selected as the aligned match for the chunk  . 
The alignment scoring function is computed fl'om the weighted sum of a number of extremely simple test flmctions  . The weights call be changed for dit Dring lengths of the source chunk in order to adapt to varying impacts of the tests with varying numl  ) ers of words in the chunk , as well as vary-it , g impacts as some or all of the . raw test stores change . The test functions include ( inapproximate order el ' importance ) such measures as a ) the number o\[' source words without <: or rcspon -dences in the t  . ; trget , b ) the number of target words without c . or respondences in tile source , c ) matching words in source/target without correspondences  , d ) nmnber of words with COl'respon-dence itt the fifll target but not the candidate chunk  , e ) common sentence boundaries , f ) eli(t-able source words , g ) insertable target words , and It ) the difference in length between source and ta > get chunks  . 
There is one exception to the above procedure for retrieving and aligning chunks  . If any of the chunks covers the entire input string and the entire source-language half of a corpus sentence pair  , then all other chunks are discarded and the target-language half of the pair is prodnced as the translation  . This speeds up the system when opea'ating in tnmsl ~ tion memory mode  , as would be the case in a system used to translate revisions of previous texts  . Unlike a pure translation memory , however , PanI'\]IIMT does not require all exact ; match with a memorized translation . 
Figure 4 shows the set of translations generated fi'om one sentence  . The output is shown in the format used R ) rst and alone testing , which generates only the best translation for each distinct clnmk  ; when integrated with the rest of Pan-gloss , Panl , ;l/MT also includes information indicating which portion of tile input sentence and which pair fi ' om the corpus were used  , and can produce multiple translations for each chunk  . The . 
number next to the source-language chunk in the output indicates the w due of the scoring flnlction  , where higher values are worse . Very poor align-meats ( scores greater than five times the source chunk length  ) have already been omitted from the output . 
6 Recent Enhancements
The EBMT engine described here is a completely new implementation ill C ++ replacing an earlier Lisp version  . The previous version had performed very poorly ( to the point where its results were elegidoel lunes per las autoridades monetarias espanolas para comprare l 
Banco Espanol de Credito ( Banesto) , cuar to bancoes panol . 
" ElBan code " ( O )   ( " the Bank of " ) " ElBan code Santander " ( i )   ( "the Bank of Santander " ) " Bancode " ( 0 )   ( " Bank of " ) " Banco de Santander " ( I )   ( " Bank of Santander " ) " de Santander " ( 0 )   ( " of Santander " ) " habiaside " ( 0 . 5 )   ( " been " ) " elegidoel " ( 0 )   ( " chosent he " ) " ellunespor " ( 0 )   ( " Monday by the " ) " porlas " ( O )   ( " by the " ) " porlas autoridades " ( 14 . 2 )   ( " by the health authorities " ) " porl as autorida des monetarias " ( 0 )   ( " by the monetary authorities " ) " las autorida des monetarias " ( 0 )   ( " the monetary authorities " ) " comprarel " ( 0 )   ( " buying the " ) " Espanol de Credito " ( 13 . 2 )   ( " Spanish Institute of Credit for " ) " de Credito " ( 0 )   ( " of credit " ) " de Credito ( "  ( i )   ( " of credit ( " ) " Credito ( "  ( 0 )   ( " credit ( " )  " , cuar to " (0) (" , fourth " ) " bancoespanol " ( 0 )   ( " Spanish bank " ) " espanol . "(0) (" Spanish . ")
Figure 4: Sample ' Danslations
Input words 9169
Matched against corpus 90.4% 8294
Alignable 84.5% 7748
Good alignments 70.2Z 6439
Table 1: (\] overage and Sentence Alignability
Engine Proposed Selected
Name Arcs Words Arcs Words Cover
DICT 2748 22748 2345 1345 19167
EBMT 11005 34992 1527 4768 6439
GLOSS 1766 3192 49156 7177 45780
Overall : 4658071998 54159 1699169
Table 2: (\] onl , ributions of Panglossl ~ hlgines essentially ignored when combining the outputs of the various translation engines  )  , for two main reasons : inadequate corpus size and incomplete indexing  . 
The earlier incarnation had used a corpus of considerably less than  40 megabytes of text , compared to the 270 megabytes used for the results described herein . The seven-fold increase in corpus size produces a proportional increase in matches  . 
Not only was the corpus fairly small , the text which was used was not flflly indexed . To limit the size of the index file , along list of tile most frequent words were omitted from the index  , as were punctuation marks . Although allowances were made for the words on the stop Fist  , the missing punctuation marks always forced a break in clmnks  , fl ' equently limiting the size of chunks which could be found  . Further , allowance was made for the , m-indexed frequent words by per-mitring any sequence of frequent words between two indexed words  , producing many erroneous matches . 
The newer implementation fully indexes the corpus , antithus examines only exact matches with the input  , ensuring that only good matches are actually processed  . Further , PanEBMT can index certain word pairs to , in effect , precompute some two-word chunks . When applied to the five to ten most frequent words  , this pairing can reduce processing time during translation by dramatically reducing the amount of data which must be read from the index file  ( for example , there might be 10 , 000 occurrences of a word pair instead of 1 , 000 , 000 occurrences of one of the words and 100 , 000 of the other word ) , and thus the number of adjacency comparisons which must be made  . 
7 Performance 7.1 Accuracy
PanEBMT was first put to the test during an similar in design I  , ol , he ARI ' AM Tew dual , ions(White & , O'(kmnell ,  1 , )94) . During this ev Mual . ion , i ; weni ; y newswire arl ; icles ( seleel , ed from the l ( O articles used in L hel > riorA\]I ) Aev Mu ; tl , iol 0 averaging M ) ( ul , 450 wor ( lsea ( : h were lro ( : essed ~ mdsul ) se ( luently ex ~ Lmine ( t . For thisia , l ) er , an-ol ; tmreva \] u ~ t l ; i on w as I ) erformed using a sulset of the langloss system on ~ he  25?  , senl ; mces in the l ; wenl , y~l'ti:les . Talkie 2 shows the , oDd nltln-bet of arcs prol ) os el1 yeach , ranslation engiue use(I , the mm ~ lersele(:Led for out , lUt ,  153 , the st , > tisl , i (: Mbmgm~ge model , ~m ( I then tu~d)ert " source words represen ; edIy1 , hose ares . The inale . olumn shows l ; he , o ; Mnunder of source wor ( Is cover cd I yat ; leasl , lie15 rOl)OSed~r' . The vMue8 for in-lividual engines lo not sumt , ( the O~:cr . llv ; due 1ee a use multit ) leen gines cmlI ) r ( luce e ( luiva\[enl , arcs , which are (: ombine ( I in the : harl , , wil , h both engil ~ CS : redited for the arc . The engines lisl , e(Iint , hel , ables ~ re?DIC Tiouary : l'anl'31 ~ MT's asso:ia , l , iol ~ die-Li(It , a , l'y ~ tl , '- ; eI here prium rily 1 ; I rovi ( lecov-el'zt gef'\]'wordsIt() , ( ) l ; herwise evered ? EBMT : Pm\]EI ~ MT ? GLOS Saries : haa\]t-  ( :raf , e(twor I/l ) hrase bilinguM glossm'ies7 . 2S l ) e ( ; dIn lexing a 270 m:galyi ; c : rl ) US requircs all r()xi-tn ; tl ; ely 45nd maes on a SunSI ) arcs l , ; tti-nI , X when all tiles are located on local , lisks , an , lan ) lher ~ lllillll , eBI , ) l m c k , he . index ( n(l , required , Iul , iml roves speedal , runtime ) . It ~ cret~enl , alahlil , ion of new data .  1 , othe:orpl Sll'o:e2l :-; ; tl , ~ tl ' al , c(\["roughly six megalytes l)erndnute . 
A sample text ( f15 sentences l ; (t ; Mling 414 Wol'ds , ~ l , ll(lIllll:41& , iOll ll 10 , 1'k8c 0 , III ) ' ~ tl ' . : (': ss(':(Iinjus , under three minul , es . The 20 texts use linl ; heevalu ~ d , ion (: ~ mhe : On ~ llel , elyir ) eesse(Iinl , whours , inchMingsel ) ~ U '; d , ei ) asscs for ( ti : l , io- . ; try lool ~ ulS ; m(lsl , ~l ; istie M\]~ , leling I , yase Iar ; d , eirogr~tm((lescriletin(Ih'owum , tI " relerk-inp; , 1!95)); I ) m ~ EI~MTa(:c(unl , s for a , I ) oul , 8n fin-tiles ( fl ; hosel ; wo hollrs . 
The above t ; imings rel ) reselll ,  ;1 , vLricLy of slee(IOll , imizati ( ns which Imve been N llied since the Augusl ;  1 . ) 95 ew dm~t;i on , r ' , sulting in a hulling of t ; he in lexiug spee I and trit ) ling f1 , r ~ m slal , ionspeed . 
8 Strengths and Weaknesses
As : urren l , yi ~ q h . uenl , e(I , ) m ~ EI ~ MT has I , ()l , hsi , rengl , hs~tnd wea , knesses , ll , ss ; renglhs are l , ha , l , l , henfininmt knowledge req . ired all ( wslu M(re-I , arg cl , ing and flint , it , slesip ; nlroviles I'rgra . ee\['u \] degralal , in . Its we ~ knesses are thai ; ii , isumdlet , o conqlet , elyever in lsul , s , L h ; t , it , les not per\['or nl well when the correspondences I  ) e-tweensom'ce-ltutgu ~ gemMl , ~ rgct-l~mguage words ~ renotone-to- . one , ~nd that ~( like statistically-based tr~mshrl , ionsysl , ems ) i , is sensitive to dif-I'erelt ce , q1) eLweel/L he example corpll S 811 ( II ; he Sell-1 , encesl , obe I ; rans lat , ed , The astul , erca . (ler will have noticed that there have been virl , ually noment , ions of l , he source or t ; arget , langmt gesiP , this paper they ~ r ~ not rel cva . ; 1 , 15 discussions of the design ~ m do per-al , im of l , he engine , since t ; he only language--telsen len , kn ( , wledge consists of l , hee ( luivMence : lasses and the lists of insert ; able ; mlcli & d ) leworts , which are ln ' ovided via the : Ollligll ra , t , i Olllile . Thisl ; m guage-indelendent as le : l ; of EBMTmM < esI~mEI~MTr~Lpidly retarget M)le ; oother l ~ m guagel Mrs , and inf~t : l ; thcreare ; dready versions fI ; mF , I~MT proviling Serhocro ; Ll , ia . n-i:o-English and El\]glish-to-Serhocroati~m trml slal  ; i(ms(m)exl ) erimenl , M(t /- t l , a , i8 , ~ tsyel , ~ twuil M , lefor SePho : roa , Lia , llI)e::4118et . he:olnl)\]eLeli(:tin;~ryan(I's rpus ares l , it theiug acquire ( l ) . ( ~ iventhc1 , hree reluire ( I knowledge S(llP'es O\["e)rpllS , li : l , ionary , and word-root , list , , PanEIMT can begin proh . > iugtr~mslat , ions for a new langtmg c pair in only a , few hurs . I , ' i uetuniug will require one ; o two weeks 12 odel ; erl ninereas OllS , \]) Ie word (: \[ a , sses\['()ri ; okenizal ; ion(along ; with the required rc-indexing of the : or lms ) a . ndt ) adjusl ; the scoring flll(:l , i Oll weighl , s . 
Nun ~ ler ~ m(lqualil , y of I ; rml sl ; t l , ionslegra ( lesgradually as the size and ( lualil#\[theI ) ilin-gual diction & try aim synonym list ( leerease . An in-:mtltel , e(licl ; imary or rool , /synonym list m wely causes P an EBM'\['l ; omiss son . 2 potenl , i altr ; msla-t , in s . Similarly , as mMM':or pustrduces fewer lotential m~d , ches , Iut there i ~ no to in l , 12 r ~ my (51" l , hel , hreel ? nowle Ip #2 S Ollrces ~ tl , which the etlg ~ iltesuhlenly : eases 1 ; o\['t lll CLiOll .   ( ) lie can I M ( cadvantage of this gradual beh ~ wiorlytmihting he knowledg c sources in crcmen l  ; Mly and using I ! ; I~MT fOP l , ra , llSlaJ , i Oll Sevell I ) el ' or e the knwl edge source strove I ) ecneomplc (  , eI . In I ) ar , icul ~ tr , 1 ya(ht in p ; l , sl , -edil , edoul , l l l l , Of the MT sysl , elll Iack into 1 , 11':: Ol'l ) llSl ; \[IesysL':lllc ; I , llI : 1o ; s , ra , ile II ' rnlare la . tively moles l , inil , ial coP i ) ll 8 ( precisely the ite a , lehinl ~ l;r;-msla , (; i ) n n lenlory ) . 
I ) uring l ) repa . r ~ l ; i on of this l ) a . 1er , sever Mex-l , l' . ~tlle OIl Slines were discovered in the eorlms files  , wlich('all:-;(:(Illl(l'C , l ; ha , ll2!)/11108ell , : ll(:ep;Lil'S(over4%o\["Lhceorlls)l , ot ~ corrul ) l , ed . I ) 11 (21 , ) t , heexl ; r ~ llines , the corrut ) l , ed pairs consisl , ed of the English target senl ; eneet'r onlone pair and l , he Spanishs ot n'ecsenl , en ' , et ?( m the following I ) air . 
' l ' his error had n ( tIeen diseover eI earlier 1e:auseil , had nol ) vious effect ou I an El 3MT's perfor-lnmt ( : e~t clear exa . mlle of the sysl ; enlsgraceful h~gra . Ial , i ( snir qerl:y . 
I , ack ) f (: ~) mt ) lel , ein\[)/ll , : w , rage is a severe ) t-s , ;i . clel , OIl Sill ~ I'a nl , ' , l ~ IMT as a sl , and - ahme I , rans 173 lation system . The engine cannot generate a chunk for a word unless it both cooccurs with either the preceding or following word somewhere in the corpus  , and at least one occurrence can be suc-cess fiflly aligned  . Additionally , candidate chunks are omitted if the alignment was success fifl but the scoring function indicates a poor match  . Unless all of these conditions are met , a gap in output occurs for the particular input word  . In the context of the Pangloss system , such gaps are not a problem , since one of the other engines can usually supply a translation covering each gap  . 
As currently implemented , the EBMT engine is unable to properly deal with translations that do not involve one-for-one correspondences between source and target words  ( e . g . Spanish " rail mil-liones " corresponding to English " billions "  )  . Lack of a one-to-one correspondence between source-language and target : language expressions can often cause the alignment o be incorrect or fail altogether under the current alignment algorithm  . 
Since the corpus used in the experiments described here was based almost entirely on the UN proceedings rather than newswire text  , PanEBMT did not find many long chunks during the evaluation  . In fact , the average chunk was just over three words in length  , and less than three percent of the chunks were more than six words long  . 
This quite naturally affects the quality of the final translation  , since many short pieces must be assembled into a translation rather than one or two long segments  . 
Despite all these difficulties , PanEBMT was able to cover 70 . 2% of the input it was presented with good chunks , and generate some translation for more than 84ordinarily not outpnt at all )  . Integrating the handcrafted glossaries from Pan -gloss into the corpus  , thus adding 148 , 600 effectively prealigned phrases to the corpus , improved the matches against the corpus from 90 . 4% to 90 . 9% of the input , and the coverage with good chunks to 73 . 3% . 
9 Future Enhancements
Since Pan EBMT is a fairly new implementation , there is still much that could be done to enhance it  . Among the improvements being considered are : improving the qnality of the dictionary  ( in progress )  ; supporting one-to-many or many-to-one associations for alignment  ; optimizing the test-function weights ; other alignment algorithms ; using linguistic information such as morphological variants and source-language synonymy to increase the number of matches against the corpus  ; using approximate matchings when no exact matches exist in the corpus  ; and using of a classifier algorithm to remove redundancy from the corpus  ( suggested by C . Domashnev ) . 
References
Ralf Brown ( in preparation ) . Framepa C User's Manual Carnegie Mellon University  ( \] enter : for Machine Translation technical memoran-dnmhUp://ww  , es . cmu . edu/afs/cs,emu . edu/-user/ralf/pub/WW/papers . html Ralf Brown and Robert Frederking 1995 . Applying Statistical English Language Modeling to Symbolic Machine ~ l Yanslation  . In Proceedings of the Sixth International Conference on Theoretical and Methodoloqical Issues in Machine Translation  ( TMI-95 )  , pages 221-239 . Leuven,

David Graft and Rebecca Finch 1994 . Multilin-gum Text Resources at the Linguistic Data Consortium In Proceedings of the  1994 ARPA Human Language Technology Workshop Morgan

H . Maruyam and H . Watanabe 1992 . Tree Cover Search Algorithm for Example-Based' lYansla-tion  . In Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine ~ l Yanslation  ( TMI-92 )  , pages 173-184 . Montreal . 
M . Nagao 1984 . A Framework of a Mechanical ~ I Yanslation between Japanese and English by Analogy Principle  . In Artificial and Human Intelligence , A . Elithorn and R . Banerji ( eds ) . 
NATO Publications
Sergei Nirenburg , ( ed . ) .  1995 . " The Pangloss Mark IIl Machine Translation System  . " Joint Technical Report , Computing Research Laboratory ( New Mexico State University )  , Center for Machine Translation ( Carnegie Mellon Univer:sity )  , Information Sciences Institute ( University of Southern California )  . Issued as CMU technical report CMU-CMT-95-145 . 
Sergei Nirenburg , Stephen Beale , and Constantine Domashnev 1994 . A Full-Text Experiment in Example-Based Machine Translation  . In New Methods in Language Processing Manchester,

Sergei Nirenburg , Constantine Domashnev , and Dean J . Grannes 1993 . Two Approaches to Matching in EBMT . In Proceedings of the Fifth International Conference on Theoretical and Methodological Issues in Machine Translation  ( TM\[-93 )  . 
White , J . S . and T . O'Connell .  1994 . " Evaluation in the ARPA Machine Translation Program :  1993 Methodology . "\[ n Proceedings of the ARP Al ILT Workshop . Plainsboro , NJ . 

