Extended Models and Tools for High-performance Part-of-speech 

Masayuki Asahara and Yuji Matsumoto
Graduate School of Information Science , Nara Institute of Science and Technology
8916-5, Taka . yama-cho , Ikoma-shi , Nara ,  630-0101 , Japan masayu-a , matsu@is . aist-nara , ac . jp
Abstract
Statistical part-of-st ) eeeh ( POS ) taggers achieve high accuracy and robustness when based oil large  , scale maimally tagged e or l ) or a . I lowever , enhancements of the learning models are necessary to achieve better  1  ) erforma . nce . We are develol ) ing a learning tool for a Jalmnese morphological analyzer called Ch  , a Scn . Currently we use a finegrained POS tagset with about  500 tags . To al ) l ) ly a normal trigram model on the tagset , we need unrealistic size of e or l ) or a . Even , for a hi-gram model , we ean-no ~ ,  1 ) ret ) are allloderate size of an mmotated corpus , when we take all the tags as distinct . Ausual technique to Col ) e with such finegrained tags is to reduce the size of the tagset  1  ) y grouping the set of tags into equivalence classes  . We introduce the concept of position-wise 9rouping where the tagset is t ) artitioned into dill ' el ' lint equivalence classes at each t  ) osition in the . conditional 1) rohabilities in the Markov Model . Moreover , to e o i ) e with the data S l ) arsen ( ? ssprot ) lemcaused 1 )  3 , exception alt ) he non > en a , we introduce several other technique such as word -level statistics  , smoothing of word-level an ( lP ( )S-level statistics and a selective trigram model . 
To help users determine probabilistic1) arameters , we introduce an error-driven method for the pm 'mneter selection  . We then give results of exl ) eriments to see the effect of the tools applied to an existing Jat  ) an esem or phological naly zer . 
1 Introduction
Along with the increasing awfilability of mmotated e or l  ) ora , a number of statistic P ( )S tat ters have been developed which achieve high accuracy and robustness  . On the other hand , there is still continuing demand for the i in provement of learning l nod-els when sufficient quantity of annotated corpora are not available in the users domains or languages  . 
Flexible tools for easy tuning of leanfing models are in demand  . We present such tools in this paper . Our tools are originally intended for use with the Japanese morphological naly zer  , ChaSen ( Matsumoto et al ,  1999) , which at present is a statistical tagger based on the w ~ riable memory length Marker Model  ( lion et al ,  1 . 994) . We first give a brief overview of the features of the learning tools  . 
Thet ) art-of-speech tagset we use is a slightly modified version of tile IPAPOS tagset  ( RWCP , 2000) with about 500 distinct POS tags . The real tagset is even larger since some words are treated as distim't P  ( )S tags . The size of the tagset is unrealistic for buihting tri-grmn rules and even bigram rules which take all the tags as distinct  . The usual technique for coping with such finegrained tags is to reduce the size of the tag set by groul  ) ing the set of tags into equivalence classes ( Jelinek ,  1 . 998) . We introduce the concept of position-wise grouping where the tagset is partitioned into different equivalence  (  ; lasses at each position in the conditiol ml probabili-lies in the Marker Model  . This feature is especially useflfl for , lapanese language analysis in ceJal ) anese is a highly ( : on jugated language , where conjugation fOl ' lllShave a greatet feet on the succeeding  mor-1  ) homes , troth avelittle to do with the t ) receding nlor-phemes . Moreover , in colloquia language , a number of eolltrael ; ed expressio 11 sareeoinmon , where two or more morphemes are central : tedinto a single word  . 
The contracted word behaves as belonging to different t  ) arts-of-st ) eech by connecting to the previous word or to the next word  . Position-wise grouping enables users to grou I ) such words differently according to the positions in which they appear  . 
Data sparsenessial ways a serious problem when dealing with a large tagset  . Since it is unrealistic to adopt a simple POS trigram model to our tagset  , we base our model on a hi-gram model and augment it with selective trigrams  . By selective trigram , we mean that only special contexts are conditioned by trigram model and are mixed with the ordinary bi-grmn model  . We also incorporate some smoothing techniques for coping with the data sparseness prob-lell l  . 
Byeoln bining these methods , we constructed the learning tools for a high -lmr formance statistical morphological analyzer that are able to learn the probability i  ) arameters with only a moderate size tagged ( ' or l ) us . 
The rest of this paper is structured as follows.

Section 2 discusses the basic concet ) ts of tile statistical morphological nalysis and some problems of the statistical approach  . Section 3 presents the characteristics of the our learning tools  . Section 4 reports the result of some experiments and tile accuracy of the tagger in several settings  . Section 5 discusses related works . Finally , section 6 gives conclusions and discusses future works . 
Throughout this paper , we use morphological analysis instead of part-of -speecll tagging since Japanese is an agglutinative language  . This is the standard tern finology in Japanese literatures  . 
2 Preliminaries 2 . 1 Stat i s t i ca l morphological analysis The POS tagging problem or the Japanese morphological analysis problem must do tokenization and find the sequence of POS tags T = tl  ,   .  ? . , t :, ~ to t the word sequence W = w l , .   .   . , w,~in the int ) ut string S . Tile target is to find T that max in fize stile following probability : Using the Bayes ' rule of probability theory  , P ( W , T ) can be decomposed as a sequence of tile products of tag probabilities and word probabilities  . 
P ( TIW)P ( T,W ) = argmaxP(W ) = argu ~ . xP(T,W ) = . ,': uv/xP ( WlT ) F ( T ) We assumed that tile word probability is constrained only by its trig  , and that the tag probability is constrained only by its preceding tags  , either with the t ) i-grm nor the trigram model :
P ( WIT ) = HP(wilti)i = 1
P ( T ) = flP ( tilti_l)i = 1
P ( T ) = flP ( ti\[ti-2, i-1)) i = 1
The values are estimated from tile frequencies in tagged corpora using maximum likelihood estimation : p  ( w l t i ) -F ( w"L ) r ( t ) 
F ( ti_,, td
P ( tlt-l)-
F ( t;i-2, 1, i-1, ti ) =
F ( ti-2, ti1)
Using these parameters , timmost probable tag sequence is determined using the Viterbi algorithm  . 
2.2 Hierarchical Tag Set
We use the IPAPOS tagset ( RWCP , 2000) . This tagset consist of three eleinents : tile part-of speech  , the type of conjugation and tile form of conjugation  ( the latter two elements are necessary only for words that conjugate  )  . 
Tile POS tagset has a hierarchical structure : The top POE level consists of  15 categories ( e . g . , Noun , Verb, .   .   .  ) . The second and lower levels are th , esubdivision level . For example , Noun is fllrther subdivided into common nouns ( general )  , llroper nomls , numerals , and so on . Proper Noun issul ) divided into General , Person , Organization and Place . Person and Place are subdivided again . The bottom level of tile subdivision level is th . cword level , which is conceptually regarded as a part of the subdivision level  . 
In the Japanese language , verbs , adjectives and auxiliary verbs have conjugation . These are categorized into a fixed set of conjugation types  ( CTYPE )  , each of which has a fxed set of conjugal ; ion forms ( CFORM ) . It is known that in Japanese that the CFORM varies according to the words appearing in the succeeding position  . Thus , attile conditional position of the estimated tag probabilities  , the CFORM plays an important role , while in the case of other positions , they need not be distinguished . 
Figure 1 illustrates tile structure of the tagset . 
2 . 3 P rob lems in s tat is t ica l mode ls On the one hand  , most of the i ) rol ) lems in statistical natural language processing stem fi'om the sparse-hess of training data  . In our case , tilenu in ber of the most finegrained tags ( disregarding the word level ) is about 500 . Even when we use the bigram model , we suffer from the data sparseness problem . The situation is nmch worse in the case of the tri-grmn model  . This may be remedied by reducing tile tagset by grouping the tags into a smaller tagset  . 
On the other hand , there are various kinds of exceptions in language phenomena  . Some words have different contextual features fi ' omothers in the same tag  . Such exceptions require a word or some group of words to be taken itself as a distinct part-of -speech or its statistics to be taken in distinct contexts  . In our statistical earning tools , those exceptions are handled by position-wise grouping  , word-level statistics , smoothing of word-level and POS-level , and selective trigram model , which are described in turnill the next section . These features enable users to . , o 17571 " rile subdivision level ~ IIII lheword level GL~WI~yO\]NL ~  .   .   .   .   .   .   .   .   .   . 
( ; TYPE Godul > " K " G?dan " lS " Illl No Conjugation No Conjugation 
Type Type tbrmform . . . . No Conjugation No Conjugation Figure 1: The examples of the hierarchical tagset adjust tile balance between fine and coarse grained model settings  . 
3 Features of the tools
This section over views characteristic timtures of tim learning tools for coping with the above  , mentioned prolf lems . 
a . : t Position-wise grouping of POS tags Since we use a very finegrained tagset  , it is important to class it ' y them into some equiva  . lence classes l ; or educe the , size . of i ) rotm bilistic lmramelers . Moreover , as is discussed in the 1) reviou section , some words or P () Sbo , lmvesditli ; rently according to Ihel ) osition they at ) pear . In 3at ) aneso , t br instance , the CF ( )I/M play an iml ) or tmlt role only to dis-mnbiguate the words at their succeeding position  . 
In other words , the CFORM should be taken into account only when they appear attim position of  1  , i-1 in either bigram or tri-grain model ( ti-i in I ' ( t ~ lt ~__ ~ ) and P ( t , \]t~_ . , , t ~_l)) . This means that when the statistics of verbs are , taken , they should be grouped difl brently according to the positions  . Not (' , that , we named the positions ; The current position means the position of ti in the hi-gram statistics P  ( tiIti-1 ) or the tri-grmn statistics P ( till , i _ . , , ti-~) . 
The preceding position means the position of ti1.
The second preceding position means the position of ti-  . 2 . 
There are quite a few contracted t~rmsill colloquial expressions  . For example , auxiliary verb " chau " is a contracted tbrms consisting of two words " te  ( particle ) + simau ( auxiliary verb ) " and behaves quite differently from other words . One way to learn its statistical behavior is to collect various us ~ ges of the word and add the data to the training data after correctly mmotating them  . In contrast , the idea of pointwise grouping provides a nice alternative so-hltion to this problem  . By simply group this word into the same equivalence class of " te " for the cur-relfl  ; 1) osition I , i and grou I ) it into the same equiva-le , nt class of " simau " for the t ) rece ( ling position ti1 in P ( ti\]ti- .  ) , it learns the statistical behavior from these classes  . 
We now describe the pointwise grouping illanlore precise way  . l ? or simplicity , we assume , bigram model . Let 7-=A,/3,---bettw,original tagset . \? e introduce two partitions of the tagset , one is fin'the current position T~=A (' , /)~ ,  . .- , mid the other is for the preceding 1 ) osition Tv = AV , 13 v ,   . .  . . We define the equivalence mal>l ) ing of the current position : I (  '  ( T-~T " )  , and another mapping of the t ) rece ( ling position : U ' ( '\]~-4" yv )  . 
l q g u r e , 2 shows an exaln ple of the lmr titiol lS by those , mapl ) ings , where the equivalence mappings Ic = el --> Ac , L ? -9 A " , C ~ A': , \]) -+ B " , E -5
W , .   .   . fv = A--+AV , \]\]-4AJ' , C-4Bv , D - - + B v , E - ~

Supl ) ose we express the equivalence class to which the tag t  , belongs as It\]" for the current position and\[ t  , \] v for the preceding position , then := F ( w , \[ td\[W ) 1) ( till , i_l ) = 3 . 2 Word - leve l s tat is t ics Seine words behave ditt'erently flom other words even intile same POS  . Especially Japanese particles , auxiliary verbs a . nd some affixes are known to have different e ( m textual behavior . The tools can define gkuo--b(D
The Preceding Posit on TagSet
ALBC~Dlc?FIG\[H----~--B"/D"--~ .   .   .   .   .   .   .   .   .   .  ',  .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . i .   .   .   .   .   . i .   .   .   .   .   .   .   .   .   .   .   . 
.  .   .   .   .  ~  .   .   .   .   .   .   .   .   .   .   .  ~  .   .   .   .   .   .   .   .   .   .   .   .   .   . i .   .   .   .   .   . i .   .   .   .   .   .   .   . 

F-o = < n " E



The Preced in ( Position TagSet
Wb1
Biil . . ll , ll . qW br , B , ~
Figure 2: Position-wise grouping of tags some words as distinct POS and their statistics are taken individually  . 
The tagset T extends to a new tagset T~xt that defines some words as individual POSs  ( the word level )  . Modification to the probability formulas for such word level tags is straightforward  . 
Note that the statistics for POS level should be modified when some words in the same group are individuated  . Suppose that the tags A and B are defined in tile T and some words l  , l  ~ ,   .   .   . , l'l ~, ~ EA and l,tZb ~, .   .   . , I : F~,,~C17 arc individuated in 7"~ xt . \ Ve define tags A e , ~ , l , Bext ~ CT ~: t as follows:
Figure 3: the word extended tagset
We define two smoothing coetIicients : A ~ is the smoothing ratio for the current position and / ~ j  , is the smoothing ratio of the preceding position . Those values can be defined for each word . 
Suppose the word wi is individuated and its POS is ti  . If the current position is smoothed , then the tag probability is defined as follows ( note that wi itself is an individuated tag ) : /5 ( wilti_l )  =  ( (1 - A~ ) P ( tilti_\] ) + A~P ( wilti_l ) ) If the word at the preceding positions is smoothed  ( assume ti-\] is the POS of wi1 ) :  . ao ~=, 4\0v,~, .   .   .  ,~ , ,~ , ,
B cxt =/)\ wv ~, . . . , , ~
To estimate the probability fortile comlection A B  , tile frequency F ( A ea:t , Bext ) is used rather than the total frequency F ( A , B ) . Figure 3 illustrate the tagset extension of this situation  . 
These tagset extension is actually a special case of position-wise grouping  . The equivalence mappings are fl'om all word level tags to T~t  . The mapping I~maps all the words illA~xt into A ~ t and maps each of W ~  ,   .   .   .  , 14(~ . , into itself . In the same way , Ip maps all the words in B ~ . ~, t into B ~: ~ t and maps each of Wb , , .   .   . , Wb , , into itself . 
3 . 3 Smooth ing of word and POS level stat ist ics When a word is individuated while its occurrence frequency is not high  , x ~ e have to accumulate instances to obtain enough statistics  . Another solution is to smooth the word level statistics with POS level statistics  . In order to back off thest ) arseness of the words , we use the statistics of the POS to which the words belong  . 
P ( ti\]wi-1 )  =  ( 1-~X p ) P ( tilti-l ) + A  ~ , F ( tilwi-~ ) If the both words of the positions is extend : ~- /~ p  (   ( 1--~c ) \]~ ( ti\[~Oi_l )  ~-  ) tcr ( 'll ) i\]'ll ) i_1 ) ) + ( 1-Av ) ( ( 1-A ~ ) P ( tilti_\] ) + A~P ( wilti-t )   )  3 . 4 Select ive t r i - gram mode l Simple trigram models are not feasible for a large tagset  . As a matter of fact , only limited eases require as long contexts as trigrams  . We 1 ) rot ) ose to take into account Olfly limited trigram instances  , which we call sclcctive tri-flrams . Our model is a mixture of such trigram statistics with bigram ones  . 
The idea of mixture of different context length is not new  . Markov Models with varial ) le memory length are proposed by Ron ( Ron et al ,  1994) , in whict lamix tm'e model of ngrams with various value of n is presented as well as its learning algorithms  . 
In such a model , the set of contexts ( the set of states of the automata ) should be mutually disjoint for the automata to be deterministic and well-defined  . 
We give a little different interpretation to tri -grmn statistics  . We consider at ri-grmn as an exceptional
C The preceding position AIB

Bh IE
CA , ....; ; ...., . ABI(.......
Figure 4: Selective tri-gram context . Wheiiabi-granicon text and a tri-granicon text have sonie intersection  , the trigram context is regarded as an exception within the  , l ) i-grauicon text . In this sense , all tim cont , exts are , mutually disjoint as well in our niodel , and it is poss ii ) h , to convert our model into Ron's tormulal ; ion . Iowevei , we think that oul "% rnmlation is iilore straighforward if the longer COlltex  (  ; s ~ -/1"(! interln'eted as exc , el ) i ; ions to (; lie shorter (; onte , x ts . 
\ V e , assume that th ( ; grouping at the current 1) o-sit ; ion ( '7 -~ ) share the same grouping of the t ) i-grain case . But for the l ) re (: eding l ) osition and ( ; lies(!(: ond1) receding 1) osition , we can delined it l'erent groupings of tagsets fl ' om those of the bigram case  . We introduce the two new tagsets t br the preceding positions : The tagsol  ; of the preceding position : " P/-W " , S/ ,  . . . The tagset of the s(!(',()ii(lpre(:(!dingl >() . ~ ition : . T ),, '_ AI ', ', H ~, # .   .   . We define the equiv ~ tlence mal ) l ) ing for the 1 ) re-ceding position : Ip ' ( 7-4 " Yp ' )  , and tilenml ) ping for tile second 1 ) receding position : Is ' ; /(7--4" YJm') . As-stoning that an equivalence classes for 1 , detined by the mapping Ipp ' is expressed as \[ t\]pj /  , the , tri-granit ) robal ) ility is defined naturally as fl ) llows : P ( tilt ~-' ,   , ti-i)--s'(\[I , d " lb , <-  . _,\]'"",\[ l,~-,\]'")
F (\[ t.~_.4'"',\[l.~_,\],",It,;\]")
F (\[ ti_~\]m ", rl .  ,1, . > ' ~ L't--JJ \] Figure 4 shows an image of fl'equency counts for tri-gram model  . 
In case some hi-gram COll text overlaps with a tri -grmn context  , the bi-graln statistics are taken by excluding the trigram statistics  . 
For (: xmnl ) h :, if we inchide ( . lietri-grmn contextA-C-\]7 in our model , then the slat , ) sties of the hi-grail ) COilte X ( ; C-13 is taken as folh > ws ( F stands for true , frequency in training corpora while F's t and s for estimated frequency to lie used for  1  ) robat . > ility calculation ): s,"(c , . ) = s~'(c , J3)-F(A , C , ix ) Since selection of tii-gram contexts is not easy task  , the tools supports the selection based on ml error-driven method  . We omit the detail because of the sl ) a ( : elimitation . 
3 . 5 Estiinatioli for unseen word sine or i ) usSince not all the words in (  ; lie dictionary appear in the training corpus ,   ; lie occurrence probability i > fmi seen words should  1  ) e allocated ill $ Olil ( : way . 
There are a number of method for estimating un-se , en events . Ourera ' rentool adopts Lidstone's law of succession  , wlfie hadd;~fixed (: omit to each obser-vat ) Oil . 
~'('., 10 = F ( , ., , t ) + ~
E , , ~ F ( , ., t ) + ~:. Itl
At1) resent , l ; he de , fault frequency (: omit , ( t is set ( o0 . 5 . 
4 Experiments and Evaluation
For evaluating how the 1 ) rol ) osed extension lint ) roves a normal t ) i-glain model , we condllcted several experiments . We group verl ) s according ( othe conjugation forms at the preceding i /osition  , take word level statistics for all l/articles , auxiliary verbs and synll ) ols , each of which is smoothed with the illl liie , -dial : ely higher P () S level . Selective , tri-grani contexts are defined for dist : riniinating a few notoriously  ; lil/-hi ~ uous particle " no " and auxiliary ve , i ' l ) s " nai " and " aru Y This is a very simple extension but suffices tbrevaluating the ett'ect of the learning tools  . 
We use 5-tbld crossewfluation over ( ; he RWCP tagged corpus ( RX VCP ,  2000) . The corpus ( : late size is 37490 sentences ( 958678 words )  . The errors of the corpus are manually rood ) tied . The annotated corpus is divided into the traiifing dataset  ( 29992 sentences , 80%) and the test dataset (7498 se , ntence , s20%) . Experinients were repeated 5 times , and the reslllts\v (; r (; averaged . 
The , evaluation is done at the following 3 levels : ? le , vell : only word segmentation ( tokenizati ( m ) isew duated ? level 2: word segmentation mid (  ; lie to I ) level part-of-speech are ewfluated ? level3: all infornmtion is taken into at , count for evaluation Using the tools , we create the following six models :
D : hernial bi-granl model
D , , , : D + word level statistics for particles , etc . 

Table 1: Results for test data ( F-value % ) dataset
D 98.69 98.12 96.91
Dw 98.75 98.24 97.22
Dw.~t 98.80 98.26 97.20
Dws 98.76 98.27 97.23
Dwqt 98.78 98.35 97.27
Table 2: Results for learning data ( F-value % ) dataset
D 98.84 98.36 92.36
D , o 98.96 98.58 97.81
Dwq 98.92 98.46 97.6t
Dw ~ 98.96 98.58 97.80
D , vgt 98.92 98.55 97.70
Dwo : Dw + group il Lg
Dw , : D , o+smoothing of word level with POS level
D , ~, at : Dwo+selective tri-grmn
The smoothing rate between the part-of-sl ) eech and the words is fixed to 0 . 9 for each word . 
To evahm te the results , we use tile F-value defined by the tbllowing formulae : number of correct words  12ccall = number of words in corpus number of correct words Precision = number of words by system output  ( /32 + 1 ) -l  ~ , ecall . Precision
F~=/32? ( Precision+12 ccall)
For each model , we evaluate the F-value ( with ~ = 1 ) for timlearl fing data and test data at ; each level . 
The results are given in the Tables 1 and 2.
From the results the tbllowing observation is possible : Smoothing improve on grouping dataset in test data slightly  . But intile other enviromnents the accuracy isn't improved  . In this experiment , the smoothing rate for all words is fixed . We need to make the different rate for each word in the future work  . 
The grouping performs good result for the test dataset  . It is natural that the grouping is not good for learning dataset since all the word level statistics are learned in the case of learning dataset  . 
Finally , the selective trigram ( only 25 rules added ) achieves nonnegligible improvement at level2 and level3  . Compared with the normal bigram I node l , it improves about 0 . 35% on level 3 and about 0 . 2% on level 2 . 
5 Related work
Cutting introduced grouping of words into equiv-a . lence classes based on the set of possible tags to reduce the number of the parameters  ( Cutting et al . , 1992)  . Schmid used tile equivaleuce classes for smoothing  . Their classes define not a partition of POS tags , but mixtures of some POS tags ( Schmid ,  1995) . 
Brill proposed a transfbrmation-based method . In the selection of trigram contexts we will use a similar technique  ( Brill ,  1995)  . 
Haruno constructed variable length models based on the mistake-driven methods  , and mixed these tag models . They do not have grouping or smoothing facilities  ( Haruno and Matsumoto ,  1997) . 
Kit auchip resented a method to determine refine -meat of the tagset by a mistake-driven technique  . 
Their inethod determines the tagset according to the hierarchical definition of tags  . Word level discrimination and grouping beyond the hierarchical tag structure are out of scope of their method  ( Ki-tauchiel ; al . , 1999) . 
6 Conclusion and Future works
We proposed several extensions to the statistical model for Japanese morphological nalys is  . We also gave preliminary experiments and showed tile effects of the extensions  . 
Counting some words individually and smoothing them with POS level statistics alleviate the data sparseness problem  . Position-wise grouping enables an eflk ~ ctive refiimment of the probability parameter settings  . Using selective tri-grain provides an easy description of exceptional language phenomena  . 
In our future work , we will develol ) a method to refine the models automatically or semiautomatically  . 
For example , error-driven methods will be applicable to the selection of the words to be individuated and the usefl fl trigram contexts  . 
For the morphological naly zer Ch , a Sen , we are using the mixture modeh Position-wise grouping used for conjugation  . Smoothing of tile word level and the POS level used tbrp articles  . 
The analyzer and the learning tools are available publicly i  . 

E . Brill .  1995 . Transformation-Based Error-Driven Learning and Natural Language Processing : A Case Studyill Part-of Speecll Tagging  . Computational Linguistics , 21(4):543565 . 
D . Cutting , J . Kut)iec , J . Pedersen , and P . Sibun . 
1992 . A pracl ; ical part-of-speech tagger . In Pro-cccdings of the Third Conference on Applied Natural Language Processing  . 
1 http:l/el . aist-nara , ac . jpllab/nlt/chasen /\]) riven Mixtur ( ; of Iti ( ; rarchical ' . ? ~ g ; Contcxl ; Tre(~s . 
In 35th , Annual Meeting of the Association for (7 om , puational Linguistics and 8th Co'nfcv (' . nccofth , c  European Chapter of tit ( : Association for Computational Linguistics , 1) ages 230237 , July . 
F . Jelinek . 1998. Statistical Methods for @ cech
Recognition . MIT Press.
A . Kitauchi , T . Utsm'o , and Y . Matsmnoto .  1999 . 
Probabilistic Model Le ~ arning tbrJatml mSC ' Mor-1 ) hological Analysis 1 ) 3' l grror-driven Feat ;  , rcSelection ( in . lal)mmse ) . ' J'(t~t . sa(:l/io'l ~ , f\]'nfi)rmatio'nl'roccssi'ngSci('ty of , \] apa'n ,  40(5):2325 2337 ,  5 . 
Y . Matsmnoto , A . Kitau (' hi , T . Ymmtshita , Y . 1\]i-mno , H . M~tsuda , and 54 . Asahm ' a .  1999 . 
Japanese Morphologic M Analyzer Cha Sen Users Ma . mml version 2 . 0 . Technical l/el ) of tNAIST-IS-T1~99012 , Nma Institute of Science mM ~ ibx : lmol-og y~l ~ ( : lmicMlR , eport . 
l ) . I ~ . on , Y . Singer , and N . Tishby .  1994 . \] A ~ A/I ' ll-ingProlml ) ilistic Automal a with Vm'iM ) lcMemory
Length . In COLT-g4, tinges 35~16.
\]/, WCP . 2000. I\Y CTt~xtl ) at a bas(~.
http://www,rwcp . or . jp/wswg/rwcdb/text / . 
H . Schmid .  1995 . In proveln(;ntsInl ) art-of-S1) e( . , ( : h Tagging With an Applic ~ tion To ( -lermm LIn IM 6' LSIGDA'\]' workshop , tinges ~' 17-50 . 

