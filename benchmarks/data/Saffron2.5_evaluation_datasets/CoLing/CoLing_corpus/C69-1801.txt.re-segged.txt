

This paper discusses the feasibility of applying a
model of language use based on a modification and extension 
(to be discussed below ) of the generative semantic ( transformational ) theory of language competence recently developed by Paul Postal  , George Lakoff , John Robert Ross , ~ ames D . McCawley , and others , to problems of computational linguistics . 
The theory of generative semantics , to be discussed in section II , is an outgrowth of , and reaction to , Chomsky's 1965 theory of transformational linguistics . 
It is a radical theory which deals with a very great range of problems with very abstract methods  . Trose working in this paradigm hold that there is a linguist-iclevel reflecting conceptual or semantic structure which is directly convertible into surface syntax by a single set of garden -variety transformations  , with no O significant intermediary level , that is , no deep structure " . These of us working in generative semantics believe that methods substantially those long familiar in linguistics can achieve very absract  , very general results which treat semantics in a more serious and enlightening way than ever before  . I do not , I think , support this very strong claim very well in section II  , but I provide summaries of several studies and a lengthy bibliogrpahy of works which when consulted will hopefully give some feeling for what is being attempted  , I think not without results . 
But generative semantics is a model , or rather , a theory , of competence , like most serious theories of language now held to by American linguists  . ~venif , as might be claimed , our semantic structures are to be merely variants of the structures long familiar from formal logic  , so that if our assumptions are correct , we will ultimately be able to directly transform surface structures into underlying semantic structures  , the majority of actual sentences , as well as all hyper-sentential structures , the treatment of which has been swept under the rug of " performance "  , will remain unhandle-able . 
Accordingly , I propose initially cert@in extensions and modifications of the theory to make it in some sense  12 a model of performance . But if we are to apply it to the computer , a major component must still be added . The impe-tus to this application is ~ he possibility of creating an understanding machine  , dewcribed in section IV below . 
Since the actual human interpretation of language depends on past knowledge  ( consider which of these sentences is good and why : As for Albuquerque  , the ~ iffel Tower is pretty . 
As for Paris , the Eiffel ~ ower is pretty.
And these:
Shir Ley is a blonde and Susan is Nordic-looking too  . 
Shirley is a linguist and Susanis Nordic-lloking too  . ) the old split between semantics , syntax , and pragmatics must be revised , and our model closely linked with a memory and possibly a logic component as well  . 
Obviously this defines a very difficult task , but insofar as such goals as HT , artificial intelligence , and machine reading of handwritten material or writing of spoken material involve comprehension on the part of the machine jo ~ which there seems to be no doubt  , these important goals will continue to ~ ludeus until such time as we can devise such an understanding machine as I have ~ escribed below  . 
I believe that generative semantics lays the foundation for studies relevant to such a development  , and it is in this context that my proposals are made  . 
In section III will d~s cuss generative semantics.
In section IIII will discuss the body of my proposals here  . 
In section IVI will discuss what should be required of a generalized " understanding " machine  . 
...t l III\]\]......I\]\]
Part II . The theory of
Generative Semantics.
~aetheory of ~ enerative semantics is an outgrowth and reaction to the theory of transformational grammar as represented in Chomsky's  1965 book , As ~ cts of the Theor ~ of S~tax ( MIT Press ) . To a very ~- I - ~ extent , this theory has been the development of a small group of former students of Chomsky  , sor their close colleagues . John ( Ha J ) Ross has said that the theory is really Just an attempt to explicate Pa~lPostal's work of five years ago to date  . If Postal was the founder of this school , if you can call it that , its main workers have been HaJRoss and George Lakoff  , who between 1965 and 1968 swept aside most of transformational linguistics as it then was  . But perhaps best known of the group is J~mes McCawley  , who graduated from MIT in 1965 with a Ph . D . based on work in ubono-logy , not syntax or semantics . He promptly amazed Lakoff and 2oss by some very substantive work in the latter areas as well as phonology  . 
s~udent of McCawley's I will be emphasizing his contributions here  , and those of my collea ~ sat Chicago , Jerry L . Morgan and Georgia M . Green , but it should be kept in mind that people like Ross  , Lakoff , Postal , Arnold Zwicky , David Perlmutter , Emmon Bach , Robin Lakoff , and several others , have made the current theory possible , and that many others , such as Robert Wall , Lauri Kartunnen , Ronald Langacker , and others , have contributed as well . It should also be kept in mind that the Case Grammar of Fillmore and the work done by Gruber  , while differing from generative semantics , have contributed a great deal to it . 
~ae basic theory of generative semantics is built upon an attempt to relate the underlying semantic structure of language to the surface  , phonetic manifestation of that underlying structure  . That is , a phonetic reality is recognized , and a semantic reality is recognized . But unlike other versions of transformational grammar  , this theory assigns no special status to syntax ; syntax is subsumed in the semantics . 
McCawley has Jokingly referred to his the or ~ as being one of either " semantax " or " synantics "  . 
~ 11e name generative semantics is not a particularly good one  , since it implies that the ~ oal of the theory is , as with the work of Chomsky , to " separate the gr sumuatical sequences " of a language " from the ~ E ~__ ammatical Sequences  . "( Chomsky , S~_~ctic Structures ~ I ~ . ) In

As a
II-2 other words , to generate all and only grammatical sent-ences of a language  . ~ his is not at all the goal of generative semantics  . Rather , what we want to do is in some rigorous way specify the correlations of underlying semantic entities and surface phonetic entities : to specify for any underlying semantic structure what its possible phonetic realizations in some language are  , and for some phonetic structure what underlying semantic structures it can represent  . Naturally , so ~ e descript - iveability is predicated as well , that is , we want to be able to define ambiguity in some algorithmic fashion  , we want to be able to define levels or classes of ill-correlation between structures on different levels  , etc . Chomsky would say that a sentence ~ like " Golf plays John " is eminently deserving of a star  ; we would say ( I ) if it's supposed to mean ' John plays golf ' , it doesn't succeed in conveying the message ;   ( 2 ) if it's suppesed to mean ' John loves Marsha ' , then it's really bad ; and (3) if Golf is a man . sname and Gohn the name of a game or role , it's a good sentence --- indeed , one can very well imagine arc ane circumstances under which one might utter that sentence with the intent of saying that the game plays John  , that the tailwags the dog ~ as it were . Suppose , for example , that John's wife we retired of him spending all his freetime playing golf and shegrum bled to a heighbor about it  , and the neighbor rather unfeelingly replied , " Ohwell , John plays golf . " I can ~ ery well imagine John's wife complaining bitterly  , " Ohno , golf plays John . "In any case , it is for hus unimaginative approach to language that Chomsky has been Jokingly called a " bourgeo is formalist " o Even when we use stars  , we try to keep in mind that Just about any valid phonological string of a language conveys one or more meanings in some context  , and that it is artificial to take a string out of context and declare it good or bad  . So " generative semantics " is a bad ns ~ e . 
The following diagram of the components of the theory is based on McCawley's paper in the proceedings of the  4th Regional Feeting of the Chicago Linguistic Society  ( 1968 )  . A theory very similar is discussed in Ronald Langacker's book L Anguage and its Structure  ( Harbrace ,  1968) , pp .  114-34 . 



THOUGHT semantic ITRANSFORMA-surface--> represent---~TIONAL ~ repre-ation COMPONENT senta-  .   .   .   .   . tion $ phonetic .    PH0 NOLOGIC-represent a-~--~AL tion ~ COMPONENT_The above diagram comes from a report prepared by myself  , Jerry Morgan , and Georgia Green , called the Uame lot ~ o ' which attempted to describe the curren-~te ofrmational re  , search in the Sum~uercf 1968 , particularly in reference to the LSA Summer Linguistic Institute at the University of lllinois  , where HaJRoss , George Lakcff , and Jim McCawley had lectured to large groups on a huge number of very ' ~ airy "  ( i . e . , difficult and tickle is hlynovel ) topics . 
In that report ( which was prepared for Victor Yngve )  , we raised several questions concerning the above representation  . We asked : i . ~ hat will an adequate semantic representation have to include ? What form will it have ?  2  . ~ hatcan a transformation do ? What does one lock like ? stage  3  . At what A and in what manner are semantic repres @ ~tations converted into words of real languages ? ~ esewere by no means all of the questions asked  . 
Needless to say , the answering cf these questions has hardly begun and will undoubtedly guaranteelinguists a few gocd centuries of work at least  . It is only in the last decade that syntax has been the subject of serious work  , and we are still only discovering how ignorant we are  . Semantics is even newer , less than a decade old . If anyone doubts that this is true , consider a ) what the above 3 questions would have meant to a linguist in ( say )  1955 , and b ) why he would have been wrong in his ( lack of ) comprehension of them . One of the great contributions of Postal and Ross has been 
I I-4 their constant critical look at transformational grammar  . 
One of the things they saw was that our transformations were  ( and are ) extremely powerful devices , with practical-ly no constraints placed on their formulation  . 
~ at I will do here is summarize some of the attempts at partial answers to the three above questions  . 
In this way I can delimit and explicate generative semantics best  . 
i will start by abstracting parts of two papers by McCawley that deal with the nature of semantic representation  . In a paper in the Japanese Journal ~ otobano Uchu  ( World of Language ) in 1967 , McCawley argued tha ~ semantic representation would be similar to syntactic representation as familiar from ~- type grammar  , but that it would also be quite similar to symbolic logic as familiar from the tons of work that have followed Principia and such studies  . That semantic representation should resemble syntactic representation makes sense if only because we are arguing for a single set of rules that transforms  ( i . e . , reEates ) the underlying structure into ( to ) the surface structures . There will be more about that later . 
McCawley argues as follows : the following devices have all had a role in symbolic logic : I  . propositional connectives "' and ' , ' or ' , ' not ' . 
2. constants denoting individuals.
3 . predicates , denoting properties and relationships . 
4 . set symbols and the quantifiers ' all ' and ' there exists '  . 
5. descriptions of sets and individuals.
? x The following devices play a role in natural languages : I  . alligs . have words for ' and ', ' or ', and ' not ' . 
( he notes however that these words in naturaligs.
may connect more than sentences ) 2 . " indices " denoting individuals ; John loves John might be represented as xI loves  x2  , but John loves
II-5 himself is xI loves xI.
3 . predicates are expressed in natural Igs . ( by verbs , adjectives , nouns , etc . ) expressioms such as 4 . " Words such as all and & at least one are two members of a rather larg--e--clas B of expressions which are used to indicate not only the existence of an individual or a set but the absolute or relative number of members in that set  . " 5 . sets and individuals can be expressed as descriptions using modified noun phrases  . 
McCawley then gives further reasons for supposing symbolic logic representation to be proper for semantic representation  .   ( See the bibliography to this section where t ~ is and other papers that can be consulted for these arguments in detail are listed  .   ) In a paper prepared for the symposium on " Cognitive Studies and Artificial Intelligence Research " held by the Wenner-~ren Foundation at the University of Chicago in March of this year  , McCaWley discussed semantic representation at length  . Some of what he had to say there should be noted . He claimed , " semantic representation must indicate the immediate constituent structure of the elements invol~ediniti  . e . examples showing that different meanings can comsist of the same semantic elements combined in different ways \[ are easy to come b ~  ) " p . l ) He gave the example of John doesn't be a this wife because he loves her  . 
If the negation applies to John beats his wife , these ' ~ tence means ' the reason ~ at John does n ' t beat his wi ~ e is that he loves her '  , whereas if it applies to the Johnheatshiswife because he loves her  . , themg . is ' the reason that John beats his wi~e-~not- ~ Hathelovesher  . ' Notice that here a surface form represents at least two different underlying structures which nonetheless contain precisely the same semantic elements -- grouped differently  , however . 
Another point made is that " semantic representations must include  . ., some indication of presupposed coreference . "( p . 2) That is , the following sentence in neutral ( i . e . null ) context is ambiguous three-ways :

John told Harry that his wife was pretty.
Whose wife ? John's ? Harry's ? or a third's ? It could be any  . However , if we know who his refers to , there is no such ambiguity . This may--seem trivial , but it is a point often ignored . 
McCawley then gives an argument for referential indices being different from expressions used to de ~ ribe  . 
The sentence
Maxd~bied that he kissed the girl Bekissed . / is not contradictory if " the girlhekissed " is the speaker's description  . 
Another notion is that of presupposed set member o ship  . lh Max is more intelligent than most Americans . 
said with primary stress ~ n most , the sentence is good if and only if Maxis presupp--~a to be American  , that is , the sentence implies Max is American . With primary stress on Americans , however , Max is presupposed not to be American . Presupposition is in general a very hairy topic which was recently the subject of an entire conference  ( at the Ohio State University )  . We know very little about the nuamces of implication and are only beginning even to identify the problems  . But if a machine is ever to rea ~ Ga_tcheri ~ the Ryecatching all the nuances of the italicized words  , we had better find out how stress is used to alter the presupposit lon-alset of a sentence  . I need not be so unsubtle as to suggest the extreme value of such researches to psychology  . Perhaps they already know about all this , for all I know . In any case I cannot restrain myself from inclucing McCawley's beautiful example 
CIA Agents are more stupid than most

He had primary streos on the ~ but I prefer to think of it as going on the Americans  . 
Z would like ~ o interject at this point a minor apology  . I have been rather fan-clubish here and have waved my hand a lot  . Frankly I seen ovalue in rehearsing here all the arguments available elsewhere  . 
But I would like there a ~ r to bear in mind my skimpy resume in no way reflects ~ the quality of the original  . 
Let me also note , lest I seem u ~ uduly credulous towards tj he D thoughts of  . C ~ irman Quang 4 mild-maunered linguist ? . mc ~ awley IsIn reality Q . p . Dong , Chairman of Unamerican Studies at an unknown university J  , that most of us working within the paradigm of generative semantics would be the first to admit that our theories haven't a pra~er of being right  , that is , they ~~ approach even a partially realistic and naturalistic " J theory of language  . If we like it better than other paradigms it is because we believe that no other cureent theory is any better and that this one at least has a good chance of self-improvement  . ( Endofapologia .   ) If semantic representation looks much like logical representation  , it also differs from it . 
In the Kotobano Uchupaper McCawley noted the following differences : I  . " It is necessary to admit predicates which assert properties not only of individuals but also of sets and propositions  . " 2 . " In mathematics one enumerates certain objects which ~ one ~ will talk about  , defines other obJecSsinte ~ mso ? these objects , and co~Ifines\[onesel ~ to a discussion of objects which \[ one Shas either postulated or defined  . . . . However , one does not begin a conversation by giving a list of postulates and definitions  .   .   .   .   .  ? . . people often talk about things which either do not exist or which they have identified incorrectly ? indices exist in the minds of the speaker rather than in the real world  ; they are conceptual entities which the individual speaker creates in interpreting his experience  . " In the Wenner-Grensymposium , McCawley had more to say about the difference between logic and language  . 
1 . Immediate constituent structure ( trees ) rather than parentheses are basic . First , " semantic representations are to form the input to a system of ~ rans formations that relate meaning to superficial form  ; to the
I I-8 extent that these transformations have been formulated and Justified  , they appear to be state able only in terms of constituent structure and constituent type  , rather than in terms of configurations of parentheses and terminal symbols  . " Secondly , " it may be necessa-ry to operate in terms of semantic representations in which symbols have no left-to-right ordering  . . . . " 2 . There will have to be more ' logi ~ al operators ' , such as most , almost all , and m ~ . 
3 . " And and . . . or . . . cannot be regarded as Just binary operators but- ~ ust be allowed to take an arbitrary number of oper and s  . " 4 . The quantifiers must be restricted rather than unrestricted as in most logical systems  . Some quantifler simply existence : All dogs like to bite postmen  , involves the presupposition that dogs exist , whereas the unrestricted quantifiers logicians use have no such presupposition  . 
5 . " Adequate semantic representation of sentences involving ' shifters '  ( Jakobson , 1957) such as I , Y Ou ~_ ~ now ,   . . . , gestures and deictic ~6rds like this--and that , and tenses , will have to include re~rence-K~-the speech act . The most promising approach to this aspect of semantic representation  . . . isRose's ( 1969 ) elaboration of Austin's ( 1962 ) notion of ' performative verb ' . " ( Seenow too Searle's book , Speech A~ts , CUP , 1969 - -- RIB ) 6 . " The range of indices will ~ ave to be enormous . 
In particular , it will have to include not only indices that purport to refer to physical objects  , but also indices corresponding to my thical or literary objects  , so that one can represent the meaning of sentences such as The Trobri and Islanders believe in Santa  61aus   , but they call him Ubu Ubu . " 7 . McC . rejects " the traditional distinction between ' predicate ' and ' logical operator ' and trea~s ~ such ' logical operators ' as quantifiers  , conjunctions , and negation as predicates . . . ." To clarify the relationship of semantic to syntactic representations let me quote here from McCawley's 
Kotobano Uchupaper:
Since the rules for combining items into larger units in symbolic logic formulas must be stated in terms of categories such as ' preposition '  , I predicate l , and ' index ' ~ these categories can be regarded as labels on the nodes of these trees  . And since . . . these categories all appear to correspond to syntactic categories  , the same symbols ( S , V , NP , etc .   ) may be used as node labels in semantic representations as are used in syntactic representations  . Accordingly , semantic representations appear to be extremely close in formal nature to syntactic representations  , so close in fact that it becomes possible to catalogue the conceivable formal differences and determine whether those differences are real or apparent ? 
Among such differences helists :
I . " The items in as  #ntactic representation must be assigned a linear order  , whereas it is not obvious that linear ordering of items in a semantic representation makes shy sense  . " 2 . " Syntactic representations in wolve lexical items from the language as their terminal nodes  , whereas the terminal nodes in a semantic representation are semantic units rather than lexical units  . " " There are many syntactic categories which appear to play no role in semantic representation  , for ex . , verbphrase , preposition , and prepositional phrase . "( At the 5th Regional Meeting of the CLS , April of this year , A . L . Becket of the University of Michigan presented a paper in which heargued prepositions are underlying predicates  ; prepositional phrases are accordingly verb -phrases  .   ) McCawEey concluded nonetheless that these differences do not provide an argument that semantic ~ epresentations are different in formal nature from syntactic representations  . Again , I will omit his reasons for that conclusion . 
I might summarize all this by saying : i . Semantic representatio ~ is a modification of the representations long familiar from ~ ormal logic  . 


II-lO2 . Such representations do no tradically differ from the surface syntactic representations of Aspects -type grammar  . 
Let me close by posing more problems . McCawley asks the following questions at the end of his Eoto bano Uchupaper  . While they do not specifically reEate to semantic structure  , I include them to give some idea of what we believe to be the sort of questions that a serious theory of language should prowide Justifiable answers for : I  . How do themgs . of words change as a language evolves ?2 . How does a child learn rags . in learning to speak his native language ? 3 . ~ at mechanisms are involved in phenomena such as metaphor  . . . . ? ( Dorothy Lambert has written a Ph . D . thesis at Michigan on the subject of metaphor within the paradigm of Case Grammar  . 
This 500 page dissertation is probably one of the best studies of the subject to date from a linguistic point of view  . S--RIB ) 4 . To what extent are the units of semantic representatiom quniversel ?  5  . To what extent does the lexicon of a language have a structure ?  6  . Can all languages express the same ideas ?7 . To what extent doe's one's language affect his thinking ?  8  . To what extisone's ability to learn lexical items conditioned by his knowledge of the world ? I will now turn to the second question raised above on p  .   II-3o This question has a syet received little study . It is a very difficult topic , but a very important one . I will confine myself here to a few brief comments and a few references  . 

II-Ii
One of the important studies underway now is about syntactic variables  . This was the subject of Ross'1967 dissertation . Variables such as X and Y are familiar from transformational grammars  , but no one had attempted before to specify in general what the notion of syntactic variable entailed  . While Ross'study was important , and he came up with several important constraints on the form of transformations  , much work remains . Lakoffan ~ Postal are also working on related questions  . Let mells there some of the constraints Ross gave in his thesis : 
I ) The complex NP constraint.
No element contained in a sentence dominated by a moun phrase with x a lexical head noun may be moved out of that noun phrase by a transformation  . ( p , 127)2) Theo ~ oss-over condition . 
No NP mentioned in the structural index of a transformation may be reordered by that rule in such a way as to cross over a coreferential NP  . ( p . 132)3) T~e coordinate structure constraint . 
In a coordinate structure , no conjunct may be moved , nor may any element ~ ontained in a conjunct be moved out of that conjunct  . 
( p . 161) ~) The pied piping convention.
Any transformation which is stated in such a way as to effect the reordering of some specified node NP  , where this node is preceded and followed by variables in the structural index of the rule  , may apply to this NP or to any noncoordinate NP which dominates it  , as long as there are no occurences of any coordinate node  , nor of the node S , on the branch connecting the higher node and the specified node  . 
( That is,: .   .   . any NP above some specified one may be reorder-e  ~  , instead of the specified one , but there are environments where the lower NP ~ ay not be moved  , and only some higher one can , consonant with the conditions imposed ~ rn the convention  . ~7) ( p . 206) i311125) The sen~ential subject constraint . 
No element dominated b ~ an S ~ ay be moved out of that S if that node S is dominated hyan NP which itself is immediately dominated by S  . ( p . 243)6) The frozen structure constraint . 
If a clause has been , extraposed from a noun phrase whose head noun is lexical  , this noun phrase may not be moved , nor may any element of the clause be moved out of that clause  . 
( p . 295)7) Definition of identity.
Constituents are identical if they have the same constituent structure and are identical morpheme -for-morpheme  , or if they differ only as to pronouns , where the pronouns in each of the identical constituents are commanded by antecedents-in the nonidentical portions of the phrase-marker  . ( p . 348) A very important constraint occurs on p . 480 of the thesis , but Iomitithere because it contains many terms I would not care to define here  . Ireccomend Ross ' dissertation for anyone with doubts about any deep principles of language organization emerging from our studies in transformational grammar  . He will be cured . 
Recently George Lakoff has studied the notion of " derivational constraint "  . This study is quite recent and still very very hairy  , but hints in his 1969 CLS paper , and comments by Postal on it suggest that rule odering is merely a special case or manifestation of a deeper principle of grammar organization  . The next revolution effected by generative semantics may well be to drop rule ordering from our canons  . 
For various reasons ( partly that it interests memere ) I will have much more to say here about lexlcal insertion than I will about constraints on transformations  , although undoubtedly the . latter is ultimately o~much greater importance . 
Until 1965 or so , it was assumed that the terminal symbols of a P -marker are lexical items  ; the lexicon merely assigns properties to these items  . Bruber in his 1965 dissertation argued that certain transformations had to occur before lexical items entered trees : that is  , that there were pre-lex ? cal transformations . 
Before Gruber , the system of semantics was one in which T-rules generated from deep structures surfaces tr % ~ctures and P-rules generated semantic representations for those deep structures  . ? T ~ l is was the theory of intepretive semantics ( as in Katz and Postal , for ex . ) Gr ~ ber proposed a derivational semantics . Gruber intended to " show va-~consistently recurrent semantic relationships among parts of the sentence and among different sentences  , which can best be explained by the existence of some underlying pattern of which the syntactic structure is a particular manifestation  . "( p . l ) He concluded that " a level at which semantic interpretation w ~ ll be relevant will  . . . be deeper than the level of ' deep structure ' in syntax  . "( p . 2 ) Later Lakoff showed evidence that in fact the level of semantic interpretation was that of deep structure  , but argued that ( as Gruber said ) " syntax and semantics will have the same representation at the prelexical level "  ( p .  3 ) : a single set of rules would transform semantic structures containing no lexical items into surface syntactic representations containing them  . 
The s~udy of lexical insertion , the process by which the underlying semantic elements are grouped into units replaceable ~ y surface lexical items has led to a large literature containing a great many questions  , and some positive answers . An important paper was McCawley's 1968 paper , " Lexical insertion in a transformational grammar without deep structure  . " There he started by assuming various points concluded in other papers of his  . He very clearly presents some of the te hots of generative semantics  , so with some repetition from above I quote these points here : I  . Syntactic and semantic representations are of the same formal nature  . . . .
2 . There is a single system of rules . . . which relates semantic representation ~ o surface structure through intermediate stages  . 
3 . In the earlier stages of the conversion from semantic representation to surface structure  , terminal node a may have for labels ' referential indices ' such as were ~ ntroduced in Chomsky  1965   . . . . In semantic representation , only indices and ' predicates ' a ~ eterminal node labels  . . . .
McCawley then defi nedt dictionary entry ' as a transformation which replaced part of a tree by a surface lexical item  . He expressed doubt these rules could be ordered internally or external  , since it would hardly be possible , for example , that some question would arise as to the relative ordering of the transformation introducing the word horse and that extraposing NP's in two dialects  , that is , ~ he~rdering could not possibly matter . 
He then raised several possibilities as to the relative ordering of the lexical rules v~s-a-v is other rules  . Are the lexical rules last , first , or where ? McCawley argued for the lexical rules applying Just before the post-cyclic rules  , and adduced evidence for several rules , predicate-raising , equi-NP deletion , etc . , being pre-lexlcal . 
In his 1968 LSA paper , Jerry L . Morgan of the U ; ~Iversity of Chicago added to this . He pointed out'the rather strong assumptlon that lexical_items only ' replace ' constituents  . "( P . 3) He wrote , " ~ he process of syntactic derivation begins with semantic representation in terms of trees containing very highly abstract semantic terms  , operating upon this by means of rules permuting , deleting , and collapsing parts of the representation , finally deriving a structure whose constituents are replaced by lexical items  . "( p .  3 ) He then s~ated a very strong claim of the theory : Given the set of universal pre-~exical rules  , the set of universal semantloprimitives , and the set of universal constraints on the operation of rules  , such as those described by Ross 1967 , these define the universal set of possible lexical items in their semantic aspect  ; that is , they rule out as impossible amin f in it ~ class of a priori possible " meanings " a lexl cal item could have  . ( P . 4 ) A second very strong claim of the theory is: In so far as the selection from  , and details of implementation of , the universal set of rules is language-specific , the idiosyncracies of a given language in this respect will also be reflected by systematic gaps in the lexicon  . 
The same is true for the set of semantic primitives and these ~ of constraints on rules  . 
... ( P .4-5)
Morgan came up with some restrictions on lexical items : only I  ) " lexical items Jan replace a constituent which , ! is not labelled S . ( p . 6 )  2 ) " verbs cannot incorporate referential in di~es . "(p . 6 ) Onel ~ L her point to be made is that lexical items can only replace wellformed sub~rees  . 
Myown work has been concerned with specifying classes of possible lexical items and accounting for the syntactic properties of verbs in terms of their semantics  , thereby attempting to capture the intuition long familiar from traditional grammar that certain ~ emantic classes of verbs  , such as " verbs of giving and taking " or " verbs of motion " also form syntactic classes and hence their syntactic properties can be regarded as derived from their semantics  . 
Georgia Green of the University of Chicago has presented a paper  ( 1969 ) which is also interesting in terms of lexical insertion  . She tends to regard lexical insertion hs fairly ~ ivorced from morphology  , and views lexical insertion as the replacement of an entire subtree by a surface lexical item which may contain more than one morpheme as ~ lassically defined  . This position is somewhat different from my own , as I regard lexical insertion as primarily involving the replacement of items on ai-I basis  . However , this is an empirical question and only future research will decide which of us is more nearly correct  . 
So far I have discussed lexical ~ nsertion in terms of sweeping  , general p ~ inciples of the organization of the grammar  . I ~ order to more clearly specify what lexical insertion is all about  , Iought to present some of the kinds of problems which have generated  811 of this interest in the subject . 

At the Texas Conference on Universals in 1967 , the proceedings of whioh were p~lished in 1968 as Universal_s in Linguistic Theory , McCawley raised the ~ e ~0 sals in L , nguistic Theory , McCawley ralsed the ques ~ o ~ dictionary organiza ' tion a new  . He opted fo ~ a " We inreichian " lexicon in which lexical items were combinations of semantic  , syntactic ~ and phonological information . McCawley supported this with this evidence : the reason John is sadder than that book  . is bad is that the two sads in the underlying structure of the sentence ared ~ erent lexical items  . They therefore cannot participate in comparison : * John is assadas that book here adyester day  . 
* He exploits his employees more than the oppur -tunity to please  . 
* IsBrazilas independent as the continuum hypothesis ?  ( exx . of Chomsky's .   ) McCawley called for a theory of " implioatic nal relations "  , since in cases such as the ambiguity of warm the ambiguity is not a property of the item itself but-B'~--a class of items  , and therefore such an ambiguity must be specified in terms of general principles  . NcCawley was not clear about the nature of these implicational r ~ lations  , so that the nature of the relationship of the various sads was more or less left open  . I have discussed the no-~on of systematic ambiguity  , where the ambiguities of an entire class of verbs is specified in terms of the derivational process underlying them all  , not Just in terms o~a descriptive statement . Thus we are seeking to explain lexical gaps in terms of statements such as " The reason some language Llacks a verb ? glossing the verb W in the language M is that M  , but not L , has the transformation T . " Anyone familiar with the lexicons of French , ~ n ~ lish , and German , for example , knows that there are certain kinds of verb which are not typical of one or another of these languages which nonetheless readily occur in the others  . 
Such verbs are derived by processes occuring in one but not another language  , and our task is to discover and describe such processes  . Thus we may ultimately be able to tell how the classo ~ French verbs  , say , differs from the class of all possible verbs . 
I have attempted in these few pages to present a digest of some works in the paradigm of generative semantics  . I have not really attempted to provide even an elementary guide to the methods of generatives ~mantics or to its conclusions  , its findings , but I hope I have explicated somewhat its goals and given some insight into the direction in which it is moving  . 
Some very strong claims are forthcoming on the nature of grammars and languages and hence of language itself  . A tremendous amount of work needs to be done , but one can see clearly that one possible end point of this work will be a very comprehensive  , very strong theory of language competence that has a great deal to say about human beings  . 
One perhaps minor point , though , looms up large here : generative semantics relates semantic structures to stu~face sentences by a single Eet of r ~ les  . There are s-~veral versions of transformational grammar that do this  , but generative semantics is perhaps the most -Cevelop-ed of these  . But as the saying goes , what goes up must come down : we may paraphrase this as : what can be generated  , can be analyzed . T ~ e theory permits , idsally , an algorithmic translation of a surface string into one or more underlying semantic structures  . For computational linguistics , that may be its most appealing feature . 



A short , select bl blio ~ raphy of recent works in and on gener ~ ~ ive semantics  . 
Austin , J.L.
1962. How to do things with words.
London : 0UP . (1965 paper .) ed . J.0. Urms on.
Bach , Emmon.
1964 . " Have and be in English syntax . " Lg .  ~3 . 462-85 . 
196 ~." Problomlnalization III . " Mimeo.
196 ~." Nouns and noun phrases ." in Bach & Harms.
1969. " Anti-pronomlnalization . " Mimeo.
Forthcoming . " Binding."
Bach , E~on , and Peters , Stanley.
1968. " Pseudd-cleft sentences."Mimeo.
Bach , E~m~n , and Harms , Robert , edd.
1968 . Universals in Linguistic Theory . NYC : Halt , Rinehart,

Becket , A.L.
1969 . " Prepositions as Predicates . " in Binnick et al Benwick , Launce lot de , the GreenKnight , and Morganle Faye . 
(: R . Binnick , G . M . @teen , J . L . Morgan .  ) 1968 . Camelot 1968 . Internal memo . , MT Group , UC , mimeo . 
Bierwlsch , Manfred , and Heidolph ( edd ) to appear . Recent Advances in Linguistics . l ~ ut on . 
Binnick , 1967.






Robert I.
" Semantic and syntactic classes of verbs . " Mimeo.
" The lexicon in a derivational semantic theory ~ of Jrna ~ Sf~i ~? ~ lslnlg ~ i ~ tics  1 In Chicagoiiab from University Microfilms , AnnA ~ bor , Michigan , asser ~ ls-372 . 
" On the nature of the ' lexical item '" , in Darden et al . 
" On transform a ` ionally derived verbs in a ~ rammar of English "  ,   . Ditto , read at LSA o The characterization of abstract lexical entities "  , ~Ditto , read at ACL . 
" Transitive verbs and lexical insertion " , dittoed , read at Kansas and CLS . 
"Predicative structure . " Unpublished Ph.D . diss.


Bin_nick , R . , G . Green , J . Morgan , & A . Davison , edd , 1969 . Papers from the 5th Regional Meeting,
Chicago Linguistic Society . Chicag 6:
Department of Linguistics , University of
Chicago . ( Advt .: dirt cheap at $5.)
Camelot . see Benwick.
J ~ arden , B . J . , C . J . Bailey , A . Davison , odd . 
1968. Papers from the 4th Regional Meeting,
Chicago Linguistic Society . Chicago:
Department of Linguistics , University of Chicago . ( Advt . : still dirt cheap at $3 .  )
Oe RiJk ~ Rudolph.
19 ~ J . " A note on prelexical predicate raising."

Donnellen , Keith.
196 6. " Reference and definite descriptions."
Philosophical Review 75.281-304.
Green , Georgia M.
1968 . "0 n too and either , and not Just too an ~ eith-e-r , either-~-7-in Darden et a 1~----1969 . " Some - - - ~ - eoretical implications of the lexical expression of emphatic conjunction  . " Unpublisheo
M . A . thesis , dittoed.
1969 . " On the notion'related lexical entry . '" in
Binnick et al ~ orthcoming . Review of R . Lakoff , Abstract Syntax and Latin Complementation . To appear in L ~ . 
Gruber , Jeffrey S.
1965 . Studies in lexical relations . Unpublishe ~ diss . 
1967." Look and se ~', in L~K .43.9~7-47.
1967 . " The functions of t~-6 lexicon in formal descriptive grammars . " Systems Development
Corporation doctuuent TM-3770/0000/00.
Jakobovits and Steinberg , edd.
to appear . Semantics : an interdisciplinary reader . . . .
Jakobson , Roman 1957 . "Shifters , verbal categories and the Russian verb . "
Slavic Dept ., Harvard Univ.

Kartunnen . Lauri.
1968. ~ Coreference and discourse . " Read at LSA.
1968 . " ~ hat do referential indices refer to ?" ~ RAND
Corporation Rgport.
1969 . " Migs and pilots . " Mimeo 1969 . "Pronouns and variables . ~' In Binnick et al
Katz , J . and Postal , Paul.
1964 . An integrated theory of linguistic descriptions . 
HIT Press.
Kiparsky , Paul.
1968 . " Linguistic Universals and Linguistic Change . " in Bach and Harms . 
Kiparsky , P . & C.
1968 . " Fact . " To appear in Bierwisch and Heidolph . 
Lakoff , G ~ orge.
1965 . " On the nature of syntactic irregularity . " Indiana Univ . diss . = NSF-16 report , ed . 
A . 0 ettinger . Available from:
Clearing House for Federal Scientific and Technical Information  , Springfield , V a . , as document PB 169252 ($3) . See also 0ettinger below . ( Alias NSF #~7) . 
1966 . " Stative adjectives and verbs in English . "" A note on negation . " Both in NSF-17 .  ~ 1967 . " Pronominalization , negation , and the analysis of adverbs . " ms . 
k1966 . "Deep and surface gray , nat . " ms . Also several of these papers currently available from Linguistics Club  , Linguistics Department , 
Indiana University.
1968." Counterparts . " Read at LSA . Mimeo.
nd . " Pronoun ~ and reference ", ms.
196 9." Some semantic considerations in syntax."
In Binnick and al.
add : 1968 . " Repartee " . To appear in Foundations of Language . 
1968 . " Instrument a ~ adverbs and the Concept of Deep structure  . " in Foundations of Language . 
1969 . " Presuppositions and relative grammaticality . "
Read at LSA in 1968.
to appear . " On Generative Semantics " in Jakobovits and
Steinberg.

Lakoff , George , and Peters , Stanley.
1966 . " Phrasal conjunction and s~numetric predicates " , in NSF-17 . 
Lakoff , George , and Ross , John R.
1966 . "A criterion for verb phrase constituency .  "
In NSF-17.
Lakoff , Rob-nT.
1968 . Abstract syntax and Latin complementation . MIT Press . 
1968 . "Some reasons why the recan't be a some-any rifle  . "
Read at LSA , mimeo.
1969 . " Syntac%ic arguments for not-transportation . "
In Binnick et al/to appear . Review of Grammaire G~n ~ raleet Raisonnee , 1660 . To appear in L ~ . 
Langacker , Ronald.
1968 . Language and its Structure . Harb race to appear . " On pronominalization and the chain of command " , mlmeo 1966 , to appear in an anthology by Reibel and Schaneto appear  . 
> ~ Cawley , nd . ~1968.




add : 1967.


James Do " The annotated respective . " F ~ Imeo.
" On the role of semantics in a grammar."In
Bach & Harms.
Review of Cooper's Set Theory and Syntactic Oescription  . in Foundations of Language . 
" Concerning the base component of a transformational grammar  . " in Foundations of Lg . 
" A note on multiple negations . " mimeo.
"English as a VS0Lg . " mimeo , read at LSA 1968.
" Tense an ~ time reference in English . " Readat Ohio State Semantics funfest ; to appear in Working Papers in Linguistics 4 of 0SU Ling . dept . ; ~ eo , " Meaning and the description of ig . " Kotobano
Uchu , Tokyo , nos . 9, 10, Ii.
" Semantic representation . " Read at Symposium on Cognition etc . 
" On lexical insertion in a ~ rans formational grammar without d~epstructure  . "In Darden et al
Morgan , J.L.
1968 . " Some strange aspect so ? " it " . " In Da~den et al 1968 . " On the notion ' possible lexical item '" . Read at LSA . Ditto . 
196 ~. " Irving . " Dittoed.

II-221968 . " Three notes on Irving and other matters . " Ditto . 
1969 . " On arguing about semantics . "Ditto , read at SECOL . 
1969 . " On the memantic representation of lexical items  . "In Binnick et al 0ettinger , A . ed . 
1966 . NSF-17 . Available from Clearim ghouse as document PB 173   630   ( $3 )  . 
Perlmutter , David.
1968 . ~et wo verbs ' begin ' . " to appear in an anthology by Jacohs and Rosenbaum to appear  . 
1968." aeep and Surface Structure Constraints in
Syntax . " MIT diss.
Postal , Paul.
1966 . " Onsocalled'pronouns , in English . " in Georgetown University Monography Series on Languages  . . .  19, 177-206 . 
1968." Cross over constraints . " ms.
to appear-a . " On coreferential complement subject deletion " in Jakobovits and Steinberg  . 
to appear-b . " On the derivation of surface nouns . " in L~ngui_stic Inuq , u ~_ ~ ~/ a new Journa ! J 1969 . " On'remind '", read at Ohio State . 
1969. " Anaphoric Islands . " in Binnick et al
QuangFnuc Dong.
1968 . " English sentences without overt grammatical subject  . " Mimeo . 
1968."A note on conjoined NP . s ", mime o.
Ross , John R . ~ 196) . " A p ~ ogsed rule of tree pruning . " " Relativization in gxtraposed Clauses . "_ ~ Both in NSF-17 . 
~1967 . " Auxiliaries as main verbs . " ~ . z ~ c ~, ~\ 1966 . " On the cyclic nattu~e of English pronominalization  . 
\in Jakobson Festschrift , Mouton.
~1966 . " Adjectives as NP's " .  5 ,~, ~ 1967 . " Gapping and the order of constituents . " PEGS .  1968 . " On declarative sentences . " 0;~%~ 1967 . " Constraints on variables in syntax . " Dies . 
1969 ~ several forthcoming papers.


Vendler , Zeno.
19o7 . Linguistics in Philosophy . Cornell Univ . press . 
Wall , Robert.
1967 . " Selectional restrictions on subjects and objects of transitive verbs  . "

Zwicky . Arnold.
1968 . " Naturalnessargt ~ uents in Syntax . "In Darden et al
EXTENSION AND MODIFICATION O ~
TH~THEORY
Paul Postal , in a 196~paper , " Underlying and superficial linguistic structure "  , seemed to rule out any principled approach to the study of performance  . But it seems clear tome that performance has me , ely been a catch all term used by linguists with a lot of nasty facts on their hands they had no way of handling  . In section III mentioned the treatment of semi -grammatical sentences as they used to be called  . Now I think we should be able to treat socalled sentence fragments as being part of language proper  . I seen o reason , once we get over our hang-ups with sharp categorization of grammaticality and Judgments of grammaticality in null context  , why we cannot have a principled treatment of sentence fragments  . 
Another area ~ sually relegated to Never-never l and is that of the structure of discourse  . Obviously the sentence pairs Harry is a fool . He voted for Richard Nixon . 
Havoted for Richard Nixon . Harry is a vote.
are not equivalent . Imagine if we take every other sentence on a page  , say , the beginning of Matthew 2 . The result is hardly a wellformed discourse . 
Now the birth of Jesus came about in this way.
Buther husband , Joseph , was an upright man and did not wish to disgrace her  , and he decided to break off the engagement privately  . " Joseph , descendent of David , do not fear ~ otake Mary , your wife , to your home , for it is through the in/'luence of the holy Spirit that she is to become a mother  . " All this happened in fulfill-ment of what the Lord said through the prophet  . . . .
But he did not live with her as a husband until she had had a son  , and he named the child Jesus . 
" Where is the newly bornking of the Jews ? " To now  , it has generally been held that the structure of discourse is linear  , that is , sentences are strung together one after the other and wellformedness is based on kowwell these sentences string  . But the context is vital to the form of a sentence  . Similarly , whether two clauses are united or put into separate sentences depends on context : by context we cannot mean m~rely the two sentences on either side of the sentence in question  , nor can we mean then sentences to either side . ~ is is quite as mad as the foll ~ of the early 50's that syntax was a matter of which words had what probability of occurir ~ n words to either side of a given word  . ~at we need is a grammar , a generative grammar , a transformational grammar , of discourse , based on the same methods that have been developed in syntax over the last decade  . 
This worm was pioneered by George Lakoff's 196~ study of Russian folk tales , in which he revised Propp's phrase structure grammar of the " morphology " of Ruszian ~ olk -tales  . I subsequently re-modified Lakoff-s work and programmed it in COMIT for a  7090-7094 machine to generate plot outlines of Russian folktales  . The results were partly abominable and partly amusing  , but the point is that while hardly any discourse is asstere typed as Russian folk tales or US patents  , that certain structures nonetheless occur which are larger than the sentence  . The notions of subordination and coordination of sentences and even whole discourses are quite val id and quite a menable to invest ~ gation  . 
A third class of problems concern logic . The implications of a sentence may be quite as important as the statements made by it  . We linguists are only beginning to investigate presupposition  , implication , in sinuation , assertion , etc . , but ph i losophershave been aware of these problems for a long time a no a large literature exists  . 
We want a machine to get as much information out of a sentence as a human would  . 
A fourth class of problems concern memory . Any program must involve knowledge . H~mans do not use language in vacuo . Suppose I know that Sherlock Holmesis at all , thin man . Suppose further that a fat , short man comes up to me and tells me he is Sherlock Holmes  . If my memory and logic components are going full blast I immediately suggest to the gentleman that a  ) he is eitherlying , or b ) could use a good psychiatrist , or c ) he has a bad sense of humor . W ~ would not like the computer to read a sarcastic sentence  , such as " Surely they have a right to dount o others what they would not want others to dount o them " and file it away neatly  . We need to give the computer a & ertain amount of linguistic sophistocation as far asirony  , in sinuation , and such go . This might seem overly optimistic , since most human being slack this ability , but let me suggest that the goal of computational linguistics is to understand human capabilities  , not reproduce them , something which can be done farcheaper by producing new human being st ~ runatural means than producing software in our labs  . 
The only thing keeping us from programming ~ omputers to ~ for example  , have a sense of human , is our peculiar delusion that we can't do it . 
So these are the problems that have not been the subject of serious research  . Note that I do not mean by this that no one has ewr looked at them and found anything out  . ~en Newton was Platonian enough to realize that nothing new is ever discovered under the sun  . But no linguist operating in terms of a formalized or quasi-formalized system has studied these problems very much  . This is not to say that certain conclusions about the future construction of a theory of language use cannot be drawn from ou ~ present ignorance  . 
T ~ erest of this section will be devoted to how we with our Nean der thalic knowledge of language can outline a decent formal theory of ' lapa role '  , something that we would want to do , I think , even had the computer never been invented . (~ nd of sermon . ) One question which arises ~ ere is what the nature of underlying semantic structures is  . Do people think in trees ? McCawley in his article on the base rejected the notion of derivation  . Instead he instituted a system of " node -sd misslbility conditions "  . These are actually conditions on the wellformedness of trees  . Any object meeting these requirements is a wll -formed tree  , otherwise it is not ( although I have yet to settle in my own mind whether an ill-formed tree is still a tree  , Just as I have been confused about whether an Il l-formed sentence of English is still a sentence of gn gll shatal ~ l  . ) Each NAC has the form < a ; BC > which is read , " a node A is admissible if it immediate-ly and exclusively dominates a node labelled B and a node labelled C  . " NAC's generate trees directly , as opposed to rewriting rules which , in Choms~y's system , first go through a derivation , from which trees are then constructed . But the important point here is " Grammars are written by fool sllkeme  , but only God can make a tree ": meaning that l inguists need not concern themselves with the or ig in of trees to discover their properties  . 
Of course , if we are to be manipulating semantic struct -ures  , we are going to have to be concerned with where trees come from  . A more basic question is whether the kinds of trees generative semantics claims to be semantic are reasonable semantic structures  , that is , whether the investigator in artificial in tell igence  , for example , could live with them . I think there is a very good chance that this is the case  . The basic elements of these trees are as follows . We have referential indices referencing individuals  . I think that in any system we will need a device such as this  . Both these indices and larger entities called senetences or S  , s can be dominated by the category N . I think again that any system will heed to consider sentences recursive in this way  . Then we will need predicates of arbitrary " weight "  , tho ' in natural language the number of N's associated with any predicate V will undoubtedly be rather small  . One possible counterto this is obviated if we assure that we have ways of referring to sets  . Then we can define SasaV and associated N's . This is not really abad scheme . 
Where it does fall down is in its failure to reflect ~ y per-propositional relations  . The conceptual universe of a person is not a bunch of unrelated trees or sentences  ( propositions )  . We will want ways to connect the Napoleon of " Napoleonatecheese " with that of " Napo Eeon hated Elba "  . Thus the conceptual universe is a network , with a far more complex structure than our underlying semantic trees  . We therefore need some set of rules for isolating part of this network to serve as the underlying tree for some surface sentence or set of sentences  , since it may turn out from our study of the structure of discourse that the unit of generat ion is larger than the sentence  . 
More will be said on these matters in section IV.


PARTIV . The understanding machine.
One basic goal of research into computational linguistics might be to investigate how information is extracted from linguistic source data  .   ( ultimately this ties into such questions as that of automated abstracting  . ) That component of our projected understanding machine which will model the information abstracting process let us dubthe " info grabber ~  . 
The infograb ber of course is not isolated . It will have to be connected with a logic component and a memory with which it will interact  . 
Nor is this the whole picture . As shown below one needs also a way of encoding the semantic output of the_logic component for later output as linguistic data  . Therefore the whole system will look like : I ~ i nguist lc\[~INFO"'~LoGI cource  I1"~1 ~RAB-ata ( LSD ) ~ BER ~ output If SPEW-~ata ( LOD ) II~ER

Notice that I have dubbed that component ~ a hich synthesizes the LOD the " infox spewer "  . 
We can regard the above as a reasonable model not only of an understanding machine  , but of the speaker . The above model would certainly be of use in the study of the use of a natural language as a computer input-output language both for programming and for other applications  , such as interaction between student and teaching machine in an educational program  . I have made some study of such a system , which I called EASIOL ( English as an Input-output Language )  , taking into account the results of the two studies I know of which approximated what I was after  , namely Daniel Bobrow's STUDINT program , reported on in " A Question Answering System for High School Algebra Word Problems " Proc  . FJCC 25 , !9641 and in Scientific American S~ptember 198b , pp .  252-260 , which was BobroW's research for his doctorate . Bobrow modified LISPo_ in the direction of COMIT , walling the hybrid METEOR . Resystem heevolved has a fair amount of flexibdlity and generality  , and can doal with many kinds of problems expressed inst ~ llzed language  . I might criticize Bobrow for his naivete over natural language  , but since I am even more naive about information processing I will not do so  . 
A second system which I have heard later evolved into a more general system  , is the BASEBALL program reported on by Green , Wolf , Carol 0 homsky , and Laughery in the Feigenbaum-Feldman voltume , Computers and Thou~t . This system bases itself on a ra~-h-erstylized t ~e of data structure  . I have not followed the prc ~ ress of either of these projects  , but both Betray inherent faults that made it unlikely  t11at either could form the basis for a more general system operating on actual discourse  . Nonetheless , these systems are very convincing for those Who think that language is the sacrosanct birthright of human Beings and that computers will never Be able to hahdle such tasks as writing abstracts of articles  . 
The above model is also a reasonable model of human speakers  ( if we forget that people differ from machines in essential ways--vive ladlff@rence  . T ) The first part of the " infograb " is the read - ln  . Hope-fully this will some day Bedone by the machine itself  , via optical reader or speech analyzer . I think that research on readers and a ~ alyzers has in general been unhappy because of a failure to realize how complex recognition b~humans is  . Recognition is not simply a nootl calor auditory problem  . All levels of language must interact in the process  . " it is wellknown that real speech is more easily handled than ~ p proxlmants to speech  , x ~ this can only bed ~ e to the recognition process being cyclic and operating slm~ltaneously on all levels  . The slmpllst recognition routines would involve something lik ~:  $1 

SIGNAL - - - ~ ~ oise Filter ~ ( LSO ) - - - - = ' ~ Segm ~ ter --

IMORPHOLOGICALI~I~i ANALY ZERI~ 2~ ~" Indeed , we have to connect upt ~ elogic and the memory to this system  . Below is a real sample of my hand writing when writing rapidly  . 
No recognition routine , not even my own human one can at all times decipher this garbage  . Redundancy is pretty nearnil and such words as ' ! of "  , " as " , " a " , and " or " tend to be homologous . What a human reader can't do , we can hardly . expeet a machine to do . 
But humans cangmess from context what a word must be  , and then see if the squiggle on the page is close enough  . This involves both syntactic and semantic recognition  , and if weever want machine reading of handwriting  , we must give the machine this capability . 
But suppose the reader still can '~ handle the writing ? I suppose then we want~t to get the logic component to intiate a question such as  , " What is that ?"$2

That is , we want the computer to be able to go thruthe whole set of levels  . This will necessitate a much more complicated program than those around today  , incorporating a greater amount of linguistic expertise  , but undoubtedly it is necessary . 
Let us assume that the infograbber has grabbed the info  , it ~_ll have ( 1 ) to store this information in the memory , and ( 2 ) ~et the logic component examine the information . Suppose I know that Richard Daley is the mayor of Chicago  , and I read in a Chicago newspaper that the ~ ay or of Chicago is the greatest man in the world  . 
The LSDm~s t somehow be so stored that I can retrieve from my memory the fact that Richard Da__ ~ l ~ is thot~t to be the greatest man in the w-- -~~ tnewspaper  . 
This raises . the question of how to convert underlying semantic trees into subnets of the semantic network of which memory probably consists  , hmny of the features incorporated into ? Sidney Lamb's concept u-al networks will  , I think , be incorporatable into the moddl . 
In particular , all occurences of a particular entity ( concept ) will have to be linked in some way or identified . 
In a sense infograb bing starts by analyzing the LSD into semantic structures  , and ends by synthesizing these structures and those already in memory into a new memory network  . 
One point that should be made clear is that all information will have to be represented on the same level  . That is , both the program and the data will reside in the same memory net  , as in a computer . Reading an algorithm in a book , the machine will store this in its memory Just as it stores part of its own program  , and it will be able to either quote the algorithm later as linguistic material as part of info retrieval  , or use that algorithm as part of its own logical operations  . There is some question the as to ~ hether this quite ideal machine could actually function in this way  . But human beings are like this in some ways , and it is part of their language capa-bility that they should or could  . 
The process of infospewing is a reverse of the infograb  . The logic component will initiate the spew , using part of the memorynet and selecting one or more underlying trees to spew out  . It will then 3S
I V-5 go through the derivational process and ultimately genera@e an actual string of sentences  . Perhaps feedback will enter here , so that the machine can utilize part of its own spewings as immediate LSD  , although it is hard to see why the machine would need to do so  , altho humans are constantly correcting themselves mid-sentence  . 
An obvious question is what the role of generative semantics in all t ~ Is  . I think the experience of CL has been ~ in general that adhoc programs don't work  . We need a basic linguistic theory . I think generative semantics is the best bet . But as I noted , it is a theory of cometence . We will need to modify it . I think we need to l ) admit rules of non-recoverable deletion , 2) admit rules for hypersentential constructs , and 3 ) build strong interactions with lo~ic and memory components  . 
In particular , the relationship of underlying semantic structures to conceptual networks will have to he investigated in depth  . 
If the hypotheses of the GS linguists are correct , then we have a simple but powerful basis for programs directly transforming language source materials into semantic information usuable by programs  . For example , if the semantic structures turn out to be universal  , they can serv q as a pivot or intermediary for the currently out of fashion goal of MT  . 

