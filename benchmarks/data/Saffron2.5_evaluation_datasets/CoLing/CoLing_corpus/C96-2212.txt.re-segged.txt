Hierarchical Clustering of Words
Akira Ushioda *
ATR h:lt(;rth'e . 1; h:lgT (; l('(:omnnlni(;&l;ionsRes('ax(: . hI , M ) or t ~ tories
22 ltika , rid M , S(;ika-(:ho , Sor Mcu-gun , Kyoto ,   , \] a , l :) ~ m6\]9-02
ema,i \]: ushioda(a ) itl , air.co.jp
Abstract
This plq ) er ( lescril ) esa(hiti ~- ( trivennlet , hod for hiera , rchic Mchlstering of words ill whiciia , la , rgevo(:a J ) ul~ry of I , ;ii . 
glis\]'l words is (: histered botl ; oln--uf ) > with resl ) e(:t1 , o (: or por ; ~ rang higin size fi ' otn 5 to 50 nlillion wor(ts , using a greedy algorithm that I ; ries I , onliniluize i ~ veri ~ ge lOS8 Ofli Cllltllaliri for uu : l , l , ion of a , djax : ent classes . The resulting hierar (' . hi('al (: illS-tiers of woMs are then tumir Mly1 , rans-rorlned to a bitstring represent ld , ion of ( i . e . word bits for ) all the words ill the vocabulary , Introducing wor ( lbitshitoi . he ATII ) ecision-Tree DOS Tagger is shown to signific ~ mt , ly reduce l , heti~gging error rld ; e . PortM ) ility of word t ) il . sh : om Olle ( to nl Mntoi ~ Hotilel : iS ~ t l SO diss ( :ussed . 
1 Introduction () lie of bileful ida , rlrient ~ J issues concern hlg corpus -- l ) as ed NI , Pist ; he(tm La8I)a , rset less prot ) len'l . In view of the eft'e (' , tive liess of class-ha , sealll-gl ' a , lll \] < % ll--gll agen lodelsi ~ gMnstL he ( \]~ tas\]7 ) i ~ l ' Seliessi ) rol ) lenl ( Kneseri Lli ( lNey 1993 )  , it ; is expected t ; l-li~t classes of words are M so use fiil for NI , P tasks ill such a wi~y that statistics oil ( :\] ~ sses ; treused whenever stal ; is tics oil individua , l word sil , i ' euna , vaihdlle or unreli & i ) le . Allide , altype of clusi , ers for NI , P is the ( ) lie which gu ; tra , rltees in utii a\[substituI ; M ) i lit , y , i l l t e r n ' is ( ) f t ) oth s y n l ; a , ctica , udseltilultic SOUll(l-lleSs , & lnOllg words in the sa , rtle class . 
Furthermore , chlstering is nnl(:hmoreiiseful if the clusl ; ersi ~ i ' e of vn riM Jegrm nllarity , or hierar--chi (' al . We will consider i ~ tree represent ~ tl , ion of MI the words in t , hevocM ) uh ~ ryin which the root ; node l:ei ) resenl ; sthe whole vo ( :i~l ) uli ~ l'yi ~ lltl~le~fll Oderel ) rese\[lt ; Sa , word ill the vocl J ) ll li ~ ry . Also , ~ . uiy set of nodes in I ; iletree constil , utes ~ i ) m : t , i-tion ( or clusl x ) ring ) of the vo (: M ) ulary if t ; here exists one i % ll ( I only Ollello de iui , llese L , -% lollg the p~th from the root node ix ) ei ~ ( : leiff node , In the following sect k ) n < % we will describe i ~ n letliod Orcrea , th'lgbim ~ rytree represeuti ~ l;ion () f wor(s a , udpresent rest llts of ev ; tlua , tiilg a , nd conll ) aring the qualii ; y of i ; he hierarchi (: M clusters ot ) tMne(Ifronltexl , s()rW . q : y dilTerent sizes . 
* Calrrellta . ( hlress : Me(lbtlutegr~ttionI , alor ; ttory , Fujitsu L ; t boral , ories I , td . , Ka . w~tsMd , . \]~ tpa . n . gtil ~ til : ushioda(~gfl~d ) . f i ij its u . (:o . jp . 
2 Word Bits Construction
Our word bits coiJstruction < ~ l gor Mlmis ; ~ lno ( ti-flotation midmi extension of the mutual infornm  . 
l , i on chistering Mgorithm proposed ) ylrownetill ,  (1992) . We will first illustrate the dilTerelt ce between file original rormuh ~ eiul  ( t theoues we used , lind the ft introduce the word bits co ,  . st . ruc-tionMgorithni . We will use the same no ( . aA;ion ; ksill Ilrownet M . to tm . Lke the conll ); trisone ; ~ sier . 
2.1 Mutual Infi ) rmatlon Clust (; rhig
Algorithm
MutuM information chlstering niethoden lploys at ) ott uni-upmerging t ) roce , ( hire with the ~ wel'i % gell lUl . ll &\] illfOrl rlttioll(AMI ) or ; sit , cent . classes in the text msano\[)jective hmction . In the iltitial sta , ge , er ~ ( : h word in the vocM ) ul <' u ' ly of size Visi~s sig . , e(ltoil ; sown(listii , (: t class . We then in erge (, wo cla . sses if die merging of ; hem induces \[ . i ,   , immn AMI reduction arl long all pMrs of classes , ttll(\]werei)e~d ; then lergitlg8( , ef ) until , tie Numl ) er  of the ( : lasses is reduced to the pre ( leliiied nuni-t ) erC ,  . Time colitplexity or this basic algorithn lisO ( V5 ) when i in p h ; rueui , edsl , rM gl it forwardly , lystoring there su\]\[ , of all the trimnierges ~( ,  ( , lie previous in erging step , however , the tinie coniplexity call be reduced to O(V3) ; ~s shown t ) elow . 
Suppos (; dia . t , stm'thig with Vch~sses , we have Mre ; uiy made V-knlerges , lelwing k (: lasses ,  ( . :~ . ( J ), (:~(2), . . , c . '~( , :)_The AMIi~t , ~ hissti~ge is given by the rollowill ge ( llmtions . 
I ~: ~ q~(<',,,,)(J)q~(l,m)pk(l , ', . ) log p~:(l , . ,) (:~) pl~(l)pr ~ ( m ) where p/~(l , m ) is the probM ) ility that a wo MiN ( , ' ~(1) is followed l ) yi ~ word ill C~(m ) , midpl ~( l ) : ~_ f ~ l)~(l , , n ) , pr , :( m)-=~_~l , ~:(l , ,n) . 

In equ ;-~ tion\[ , qh's ; ~ resunlrHed overl , he entire kXk ( : lass bigrluntable ill which ( l , lu ) cell rel ) reseilts qx + , (f , m ) , hit his irlerging step we invesi ; igate a trial merge or c ' ~ ( i ) mM ( :~ ( j ) tbrMI ( : h~ss pairs ( i , j ) , lind con , pure the AMI reduction L~(i , j)I~:-l~:(i , j ) efre(:i~ed by this . ~ erg e , where l ~ : ( i , j ) is tile AMI aft ; or the lilertre ,  . 
Suppose that the I ); dr(Cx:(i ) , C ); ( j )) was chosen to merge , thai . is,l,~(i,j)~L~,(l,m ) for M11\] . 59 pairs ( l , m) . In the next merging st . el ) , we have L~;'J ) tlm ) for all the pairs ( l , m ) . to cMculate-1~ , ltere we use the superscript ( i , j ) to indicate that ( Ck(i ) , Ck ( j ) ) w as merged in the previous merging step . 
Now note that the difference between L ( i'j ) (l m ) and L ~ ( l , m ) only comes fronl k1 ~" the terms which are affected by mergiug the pair  ( C  ~ ( i )  , C~:(j)) . 
Since L ~ . ( l , rn ) = I~-1~(I , m ) and L"'J)(lm ) = k , -l  ~ , - we have -- , --( l(i'J)--lk ) , m ) ) + Some part of the summation region of I~'j~ ) ( l ,   , n ) and I~cancels out with a part of l ~ i ; ~) or a part of a(t ,  . ,) . Let "0, i(t . 0 , i and i ~ denote the values of l ~ iLJ ) ( l , rn ) , lt:(l , m) , l~i'_J1) and I ~ , respectively , after all the common terms among the ln which Call be canceled are canceled out  . Then , we have
L(i'J ) tlm)--Lk(l , m ) = k-lk , where \[( i'J ) llm ) k1~' = q~-l(l + rn , i ) + qk-l(i , l + m ) = q~(l + m , i ) + q~(i , l + m ) + q~(l + re , j ) + q~(j , l + m ) = q~_l(i , l ) + q~_l(i , m ) + qk-l(l , i ) + qk-l(m , i ) = qx : ( i , l ) + q~(i , m ) + q~(j , l ) + q~(j , rn ) + qk(l , i ) + qk(l , j ) + q~(m , i ) + qk(rn , j ) Because quation 3 is expressed as the summation of a fixed number of q's  , its value can be cMculated in constantime , whereas the cM culation of e ( tua-tion1 requires O ( V2 ) time . Therefore , the total time complexity is reduced by O(V ~) . 
The summation regions of I's in equation 3 are illustrated in Figure 1  . Brown et al seem to have ignored the second term of the right hand side of equation  3and used only the first term to calculate L~i , J~(l , m)_Lk(l , m ) 1 . However , since these coudterm has as much weight as the first terln  , we used equation 3 to mg ke the model complete . 
Even with the O(V a ) algorithm , the calculation is not practical for a large vocabulary of order  10   4 or higher . Brown et al proposed the following 1A (: tually , it is the first term of equation 3 times ( - l ) that appeared in their paper , but we believe that it is simply due to a misprint  . 
i j 1m
J ::::::::::::::::::::::::::::::::: ) , i
Jl+nijl+m::::::::::::::::::::::::::::::::::::::: :::::::::::::::::::::::::::::::::::::::::::::::::::: : : : : : : : : : :\[ i !!\[ ii kik  ( Lm )  ==========================  . . . .
:::::::::::  ( ~ ) l + mk-I 1 : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  , j ! l ~'( i , j ): lIn_ . I k_D , ~)
Figure 1: Summation Regions for l's
Merging History : Dendrogram
Merge(A , B -> A ) := >
Merge(C , D -> c )__ ~___
Merge(A , C->A)
Merge(X , Y -> Z ) reads ( A " mergeX and Y and name the new class as Z "
Figure 2: I ) endrogram Construction method , which we also adopted . We first make V singleton classes out of the V words in the vocabulary and arrange the  ( : lasses in descending order of frequency , then define the merging region as the first C+l positions in the sequence of classes  . At each merging step , merging of only the ( : lasses in the merging region is considered , thus reducing the number of trial merges from O ( V2 ) to O ( C'~ )  . 
After each actual merge , the most frequent singleton class outside of the merging region is shifted into the region  . With this algorithm , the time complexity is reduced to O(C~V ) . 
2 . 2 Word B i t s Const ruct ion Algor i thm The simplest way to construct a tree structured representation f words is to construct a dendrogram from the record of the merging order  . A simple example with a five-word vocabulary is shown in Figure  2  . If we apply this method to the above O ( C ' 2V ) algorithm , however , we obtain for each class an extremely unbalanced , M most left branching subtree . The reason is that after classes in the merging region are grown to a certain size  , it is much less expensive , in terms of AM1 , to merge a singleton class with lower frequency into a higher frequency class than merging two higher frequency classes with substantiM sizes  . 
A new approach we adopted is as follows.
1 . Ml-clustering : MakeC classes using the mutual information clustering algorithm with the merging " ORJ Figure  3: Sanlple Sub (  . ree for One ( : lass region constraint mentioned in ( 2 . 1) . 
2 . Outer-clustering : Replace all words in the text with their class token  2 and execute binary merging without l ; llemerging region constraint until all the classes are merged into a singe  ( : lass . Make a dendrogram out of this process . This dendrograrn , 1), . oo ?, constitutes the upper part of the final tree . 
a . l ,), ~ . , . -,:~,,s~ . , . i , , , j : Let C(I ), C(2), . . . , c(c ) )) e the set of the classes obtained at , step l . l , ' or each i(1 < i < C ) do the following . 
(3 ) Replace all words in the text except those in C ( i ) with their <: lass token , l ) efine a new vocabu--lary V ' = V1 UV > where V1 = all the words in (  \ ]  ( i )   , V2 = C'l ,  ( \ ]2  ,   .   .   . , C .   , i_l , C'i + l , C , c , and 65 is a token for (:( j ) for I < j < C . Assign each element in V ' to its own class and execute binm : y merging with a merging constraint such  (  , ha ( , only those classes which only contaiu elements of Vl can be merged  . 
( b ) Repeat merging until all the eletuents in VI are i  ) ut in a single ( : lass . 
Make a dendrogrmnl ) . ~, d ~ out of the merging pro-tess for each class . This ( teudrogram coust , it utes a subtree for each ( : lass with a leaf node rel ) resent-ing each word in the class . 
4 . Combine tile dendrograms by substituting each leaf node of l  ) root with core sponding l )  , ,Lb This algorithm produces a b , ~lanced binary tree represent ; ation of words in which ( , hose words which are close in meaning or syntactic feature come close in posit  , ion . Figure 3 shows an exam-pie of l ) . ,~b for orle class out of 500 ( : lasses constructed using this algorithm wit ) a vocabulary of the 70 , 000 most ; frequently occm:ring words in the Wall Street ; Journal Corpus . Finally , by tracing the path from the root node to a leaf node aud assigning a bit to each bra  , uch with zero or one representing a left or right branc\]b respectively  , we car , assign a bit string ( word bits ) to each word in the vocabulary . 
~\] n the actuM implement ~ t tion , we only h twe to work on the bigr~mlt*Lble instead of tim whole text  . 
Event-128: ( wo , ' cl(O ), " like " wore\]-1) . " flies " ( WOl'd(-2) , "ti , nt , " W () l'(l(l) , ll\[l /"( .   .   .   . '((~), ut\['I'()V Vutag(d ), " Ve . b-ard-Sg-lype3"(tag(2), " Nou ,,- Sg typeld ") .   .   .   .   .   .   .   . ( Basic Questions )
In class ?( word (0), Clasu 295), " yes "
Wo'dBits(~cVord(-1), 29), "1" .   .   .   .   .   .   .   . ( V~r Ol ' ClBi , sQuesl ; ions)
IsPrefix ?( Word(0), " anl ; i "), " no " .   .   .   .   .   .   .   . ( Linguist's Qtestions )
Tag , " Prep-type 5"
Figure 4: Examt ) le of a . n event 3 Experiments We used phdu texts from six years of tile WSJC  , or t ) us to create word bits . The sizes of tile texts are 5 million words ( MW )  , t0 MW , 20M W , and 50MW . '' he vocabulary is selected as the 70 , 000 most ; fl : eqneutly occurring words in the entire co > pus  . We set the number C of <: lasses to 500 . 
The obtained hierarchical clusters are ewdua . ted via the error rate of the ATIl ) ecision-Tree Part--Of-Speech Tagger which is based on SPAT'\['I  , ; t ( Magerman 199 , 1) . The tagger employs a set of 443 syntactic tags . In the training phase , a set of events are extracted from the training texts  . An event is a set of feature-value pairs or question-answer pairs  . A feature can be any attribute of the context in which the current word word  ( O ) appears ; it is conveniently expressed as a question . 
Figure 4 shows an example of a nevetlt , with a current word " like " . The last \[ ) air in the event is a special item which shows the answer  , i . e . , the colrect tag of the current word . The first three lines show questions about identity of words around tile current word and tags for previous words  . These questions are cM led basic que . slio ~, s and always used . The second type of questions , word bits questions , are on clusters and word bits such as what is the  29th bit of the previous word's word bits ? . The third type of quest kms are cM led lin-gui . sl's question and these are compiled by an expert grml mmrian  . 
Out of the set of events , a decision tree is constructed whose leaf nodes contain condition M probability distributi  ( ms of tags , conditioned by the feature values . Intile test phase the system looks up condition M probability distributions of tags R  ) reat : l , word in the test text and chooses the most probable tag sequences using beam search  . 
We used WSJ texts and the ATI cor\[ms ( lllack et al 1996 ) for the tagging experiment . Both colpora use the ATR syntactic tagset . Since the ATR corpus is still in the process of development  , the size of the texts we have at hand for this experiment is rather ndnimal considering tim large size of the tagset  . Table 1 shows the sizes of texts used for the experiment ;  . Figure 5 shows the t ; ag-ging error rat ; esplotted against various clustering 261 ? Word Bits 0 Lh~llQ~t & Word Bits 24I \ Text : ATR Corp: 20~ Reshuffled ( WSJ Text ) =\]\' ~ E1 Word Bits 16 . . . . . . "~  . . . . . . . . . .
10\[d5xS 1020 304 10 ~0
Clustering Text Size ( Million Words ) l , ' igure 5: Tagging Error Rate ( words ) Training q'est Ile ht-Out
WSJ Text 75 , 139 5 , 831 6 , 534 " ATR Text 76 , 132 23 , 163 6 , 6 8"0 Table 1: Texts for Tagging Experiments text sizes . Out of the three types of questions , basic questions and word bits questions are always used in this ext  ) eriment . ' lb see the effect of introducing word bits information into the tagger  , we performed a separate xperiment in which a randomly generated bitstring is assigned to each word  3 and basic questions and word bits questions are used  . The results are plotted at zero clustering text size  . For both WSJ texts and ATR corpus , the tagging error rate dropped by more than 30% when using word bits information extracted from the  5MW text , and increasing the clustering text size further decreases the error rate  . At 50 MW , the error rate drops by 43% . This shows their a : provement of the quality of the hierarchical clusters with increasing size of the clustering text  . In Figure 5 , introduction of linguistic questions 4 is also shown to significantly reduce the error rates for the WSJ corpus  . The dependency of the error rates on the clustering text size is quites in > liar to the e a  . seinwhich no linguistic questions are used , indicating the effectiveness of combin-3Since a distin < : tive bitstring is assigned to each word  , the tagger also uses a bitstring as an ID number for each word in the process  , In this control experiment bit-strings are assigned in a random way  , but no two words are assigned the same word lilts  . Random word bits are expected to given o class information to the tagger except for the identity of words  . 
4 The linguistic questions we used her ( . ' are still in the initial stage of development and are by no means comprelmnsive  . 
ing automatically created word bits and handcrafted linguistic questions  . Figure 5 also shows that reshuming the classe several times just after step I  ( ML clustering ) of the word bits construction process filrther improves the word bits  . One round of reshuffling corresponds to moving each word in the vocabulary from its original  ( : lass to another class whenever the movement increases the AMI  , starting from the most frequent word through the least frequent one  . The figure shows the error rates with zero , two , and five rounds of reshufi \] ing 5 . Overall high error rates are attributed to the very large tagset  ; and the small training set . Another notable point in the figure is that introducing word bits constructed from WSJ texts is as effective for tagging Aq'R text  . s as it is for tagging WSJ texts even though these texts are from very different domains  . To \[; hat extent , the obtained hierarchical clusters are considered to be portable across domains  . 
4 Conclusion
We presented an algorithm for hierarchical <: has : tering of words  , and conducted a clustering experiment using large texts of : varying sizes  . Highqtml-ity of the obtained clusters are confirmed by the POS tagging experiments  . By introducing word bits into the ATR l ) ecision-Tree POS Tagger , the tagging error rate is reduced by up to 43% . The hierarchical clusters obtained fi'orn WSJ texts are also shown to be usefld\['or tagging ATR texts which are fi'om quite different domMns than WSJ texts  . 

We thank John Lafferty for his helpful suggestions . 

Black , E . , Eubank , S . , Kashioka , H . , Magerman , D . , Garside , R . , and Leech , G .   ( 1996 ) " Beyond Skelc-ton Parsing : Producing a Comprehensive Large-Scale General-English Treebank With Full Grammatical Analysis "  . Proceedings of the 16th International C ' on-ference on Computational Linguistics  . 
Brown , P . , Della Pietra , V . , deSouza , P . , Lai , J . , Mercer , R .   ( 1992 ) " Class-Based ngram Models of Natural Language " . Computational Linguistics , Vol . 18, No 4, pp .  467 479 . 
Kneser , R . and Ney , H . (71993) " hn proved Clustering Techniques for C , lass-Bascd Statistical Language Modelling " . Proceedings <> f European C , on fl'n'cnce on Spee<:h
Communication and Technology.
? Magerman , D .   ( 1994 ) Nabural Language Parsing as Statistical Pattern Recognition  . Doctoral dissertation . 
Stanford University , Stanford , California.
~' The vocabulary used for tile reshuffling experiments is the one used for a preliminary e  . xperim cnt and its size is 638 50 . 

