SE(IMENTINGASENTENf,I?INTOMOItl)IIEM 1,;S
USINGSTNI'ISTICINFOIMATION BI , TFWEENWORI)S
Shiho Nobesawa
Junya Tsutsumi , Tomoaki Nitta , Kotaro One , Sun Da Jiang , M~Lsakazu Nakanishi
N ; tkanishi L ; d)oratory
Faculty of Science and Technology , Keio University

This paper is on dividing non-separated language sentences  ( whose words are not separated from each other with a space or other separaters  ) into morphemes using statistical information , not grammatical information which is often used in NLP  . In this paper we describe our method and experimental result on Japanese and Chineses e ~  , tences . As will be seen in the body of this paper , the result shows that this sys-tentise tlicient for most of tile sentences  . 
1 INTRODUCTION AND MOTIVATION
An English sentence has several words and those words are separated with a space  , it is e ~ usy to divide an English sentence into words  . I\[o we veraa a palms e sentence needs parsing if you want to pick up the words in the sentence  . This paper is on dividing non-separated language sentences into words  ( morphemes ) without using any grammatical information . Instead , this system uses the statistic information between morphenws to select best ways of segmenting sentences in non-separated languages  . 
Thinldng about segmenting a sentence into pieces , it is not very hard to divide a sentence using a certain dictionary for that  . The problem is how to decide which ' segmentation ' the t  ) est answer is . For ex-aml ) le , there must be several ways of segmenting a Japanese sentence written in lliragana  ( Jal ) a , lese alphabet ) . May be a lot more than ' several ' . So , to make the segmenting system useful , we have to cot > sider how to pick up the right segmented sentences from all the possible seems -like-scgrne  , ted sentences , This system is to use statistical inforn , ation between morphemes to see how'sentence -like'  ( how'likely ' to happen a . s a sentence ) these . gmented string is . To get the statistical association between words , mutual information ( MI ) comes to be one of the most interesting method . In this paper MI is used to calculate the relationship betwee  . n words found ill the given sentence . A corpus of sentence si used to ga in the MI . 
' Fo implement this method , weiml ) lemented a system MSS ( Morphological Segmentation using Statistical information  )  . What MSS does is to find the best way of segmenting a non-separated language  , sentence into morphemes without depending on granamatieal information  . We can apply this system to many languages . 
~2) /\[ ORPHOLOGICAL ANALYSIS 2 . 1 What ; a Morphological Analysis IsA morpheme is the smallest refit of a string of characters which has a certain linguistic l/leaning itself  . It includes both content words and flmction words  , in this l ) a per the definition of a morl ) heme is a string of characters which is looked u I  ) in tile dictionary . 
Morphoh ) gical analysis is to : l ) recognize the smallest units making uptile given sentellce if the sentence is of a lon-separated hm guage  , divide the sentence into morphenms ( automatic segmentation )  , and 2 ) check them or lflmmes whether they are the right units to make up the sentence  . 
2.2 Segmenting Methods
We have some ways to segment a non-separated sentence into meaning flll morphemes  . These three method sexl ) lained below are the most popular ones to segment , I apanese sentences . 
? The longest-sc'gment method : l ~ , ead the given sentence fi'om left to right and cut it with longest l  ) ossible segment . For exam-pie , if we get'is heohl'first we look for segments wilich uses the/irst few lette  , 's in it , ' i ' and ' is ' . 
it isol ) vious that'i';'is lo Ilgerthall'i' , SO tile system takes ' is ' as the segment . Then it tries the s ; tllle method to find the segnlents in ' he old ' and tinds ' he ' and ' old '  . 
The , least-bunsetsuegmenting m (' , thod : Getal the possible segmentations of the input sentence and choose the segmentation  ( s ) which has least buusets u in it . . ' l'his method is to seg:-ment Japanese sentence . s , which have content words antifunction words together in one bunsetsu most of the time  . This method helps not to cutase , ntenee into too small meaningless pieces . 
Let tm'-tyl)e , segmenting method:
In Japanese language we have three kinds of letters called Iliragana  , Katakana and Kanji . Thising fulsegments checking the type of letters . 
2.3 The Necessity of Morphological
Analysis
When we translate an English sentence into another language  , the easiest way is to change the words in the sentence into the corresponded words in the target language  . It is not a very hard job . All we have to do is to look up the words in the dictionary  , flow-ever when it comes to a non-separated language  , it is not as simple . An non-separated language does not show the segments included in a sentence  . For example , a Japanese sentence does not have any space between words  . A Japanese-speaking person can divide a Japanese sentence into words very easily  , however , without arty knowledge in Japanese it is impossible  . When we want a machine to translate an non -separated language into another language  , first we need to segmen the given sentence into words  . 
Japanese is not the only language which needs the morphological segmentation  . For example , Chinese and Korean are non-separated too . We can apply this MSS system to those languages too  , with very simple preparation . We do not have to change the system , just prepare the corpus for the purpose . 
2.4 Problems of Morphological
Analysis
The biggest problems through the segmentation of an non-separated language sentence are the ambiguity and unknown words  . 
For example , ni wanihani wat origairu.
~: ?-?2N~:w 6niwani wato rigairu
A cock is in the yard.
/EI , ct . ~-< NI ?'0~: v , 6 oniwani hani watorigairu
Two birds are in the yard.
1 ~ tc*gN ~ ~ . a:N7 on i wanihani wato rigairu
A clay-figure robber is in the yard.
Those sentences are all made of same strings but the included morphemes are different  . With dill > rent segments a sentence can have several meanings  . Japanese h ~ three types of letters : I\[i ragana , Katakana and Kanji . l Iiragana and Katakana are both phonetic symbols  , and each Kanji letters has its own meanings . We can put several Kanji letters to one lli ragan a word  . This makes morphological analysis of Japanese sentence very difficult  . A Japanese sentence can have more than one morphological segmentation and it is not easy to figure out which one makes sense  . Even two or nlore seglnentation can be ' cor rect'l brone sentence  . 
To get the right segmentation of a sentence one may need not only morphological nalys is but also semantic analysis or grammatical parsing  . In this paper no grammatical information is used arid MI between morphemes becomes the key to solve this problem  . 
riodeal with unknown words is a big problem in natural language processing  ( NLP ) too . To recognize unknown segments in tim sentences , we have to discuss the likelihood of timun known segment being a linguistic word  . In this pal ) erun known words are not acceptable as a ' morpheme '  . We define that ' morpheme ' is a string of characters which is registered in the dictionary  . 
3 CALCULATINGTI lESCORESOF
SENTENCES 3.1 Scores of Sentences
When the system searches the ways to divide a sentence into morphemes  , more than one segmentation come outmost of the time  . What we want is one ( or more ) ' correct'segmeutation and we do not need any other possibilities  . If there arc many ways of seg- , nenting , we need to select the best one of them . For that purpose the system introduced the ' scores of sentences '  . 
3.2 Mutual Information
A mutual information ( MI ) \ [1\]\[2\]\[3\] is tile information of the ~ ussociation f several things  . When it comes to NLI',MI is used I . o see the relationship between two ( or more ) certain words . 
The expression below shows the definition of the
MI for NI , P:l'(wl , w2)
Ml(wt ; w2 ) = 1 o9l ' ( Wl ) P ( w2 )   ( t ) loi : a word P ( wi ) : the probability wl appears in a corpus P ( wl , w , 2 ) : the probability w ~ and ' w2 comes out together in a corpus Tiffs expression means that when wl and w  . 2 has a strong association between them , P ( wt)P ( w~)<<P ( wt , w2) i . e . MI(wl , w2) > > 0 . When wl and w ~ do not have any special association  , P ( w , )P(w . a)P ( wl , w2) i . e . Ml(wl,'w2) ~ O . Andwl , enwx and w2 come out together very rarely , P ( wl)P ( w2) > > , '(~ , , ,   , , , ~) i . e . MX(w , ,, ~, ~) < < 0 . 
228 3 . 3 Ca lcu la t ing the Score o f a Sentence Using the words in the given dictionary  , it is easy to make up a ' sentence ' . l lowever , it is hard to consider whether the ' sentence ' is a correct one or not  . 
The meaning of ' correct sentence ' is a sentence which makes sense  . For example , ' I am Tom . ' can make sense , however , ' Greentheadzabakarct hear an four . ' is hardly took ms a meaningful sentence . ' Fhe score is to show how'sentence-like'the given string of morphemes i  . Segmenting ~ tnon-sel)arated language sentence , we often get a lot of meaningless strings of morphemes  . To pick up secms-likc-mea , fing fid strings from the segmentations , we use MI . 
Actually what we use in tim calculation is not l , here alMI described in section 3 . 2 . The MI expression in section 3 . 2 introduced the bigrams . A bigram is a possibility of having two certain words together in a corpus  , as you see in the expression ( l ) . Instead of the bigram we use a new method named d -bigram here in this  paper\[3\]  . 
3.3.1 D-bigram
The idea of bigrams and trigrai T~s are often used in the studies on NLP  . A bigram is the information of the association between two certain words and a trigram is the information among three  . We use a new idea named d-bigram in this paper\[3\]  . Ad-bigram is the possibility that two words wt and  w2 come out together at a distance of d words in a corpus  . For example , if we get'he is Tom'as input sentence , we have three d-bigram data : ( ' he '' is '1 )   ( ' is ' ' Tom'1 )   ( ' he '' Tom'2 )   ( ' he '' is '1 ) means the information of the association of the two words ' tie ' and ' is ' appear at the distance of  1 word in the corpus . 
3.4 Calculation
The expression to calculate the scores between two words  is\[3\]: t ' ( wl , w  ~ , d ) Mid(w1 , w , 2 , d ) = 1 o9 ~ ~ (2) lui: ; two rdd : distance of the two words Wl and w2 P ( wi ) : the possibility the wm'd wl appears in the coq  ) us P ( wl , w2 , d ) : the possibility wl and w2 eoll'le outdwords awayfl'om each other in the corpus As the value of Midgets bigger  , the more those words have the , association . And the score of a sentence is calculated with these Mid data  ( expression ( 2 ) ) . 
The definition of the sentence score is \[ l\]:i a ( W ) = 99 Mia ( wi , w'+d , d ) d -' ( a ) i :0 d:ld:distance of the two wordsm:dis tance limit  ?1  . : thell Ulltilel " Of Wol'd silltile Selttel lCe 
I~ll:itselttence wi:Thei-th morpheme in the sentence I~V This expression  ( 3 ) calculates the scores with the algoritlm t below :   1  ) Calculate Mld of every pair of words included in the given sentence  . 
2 ) Give a certain weight accordiug to the distance , d to all those Mid . 
3) Sumup those 3~7 ~ . The sum is the score of the sentence . 
Church and llanks said in their pN ) er\[1\] that the information between l . wo remote wo , ' dsh~sless meaning in a sentence when it comes to the semantic analysis  . According to the idea wel ) utd 2 in the expression so that nearer pair can be more effective in calculating the score of the sentence  . 
4 TnsSYST SMM SS 4.1 Overview
M , qS takes all iragan a sentence as its input . First , M , qSpicksUl ) the morphemes found ill the giwm sentence with checking the dictionary  . The system reads the sentence from left to rigltt  , cutting out every possibility . Each segment of the sentence is looked up in the dictionary and if it is found in the dictionary the system recognize the segnlent as a morpheme  . Those morphemes are replaced by its corresponded Kanji  ( orlliragana , Katakana or mixed ) morpheme(s) . As it is to hl in section 2 . 4 , all iragana morpheme can have several corresponded l  ( anji ( or other lettered ) morphemes . In that case all the segments corresponded to the found liragana morpheme  , are memorized as morl ) hemes found in the sentence ,  . All the found morphemes are nunfl ) ered by its position in the sentence . 
After picking Illlall then , or phenu . ' s in I . he sentence the system tries to put them together mtd brings them up back to sentence  ( tat ) h ~ I )  . 
\[ nl ) ut all iragan a sentence.
Cutoutt , he morphemes.

Make up sentences with the morphemes.

Calculate the score of sentences using the mutual information  . 

Compare . the scores of all the . made-up sentences and get the best-marked one as the most ' sentence-like ' sentence  . 
Then the system compares those sentences made up with found morl  ) he . mes and sees which one is the 012 344 5678
IT .  ~ "9 8 9 10 11 12  ( ('~ t /~" 03 )   (  '~" 12 )   ( ' ff ~" 23 )   ( " L'23 )   ( ' ~" ad )   ( '1:"4s )   (  '~"  , ls ) ('~''67) '') "78) ('77-"89) (' l ,  '" 910 )   ( " R " a "911 )   (  '~'~" 911 ) ( '~" 1112 ) )  ( ' ~" 0 a ) l failed 1 ( -~" sg )  !  ( '~ , ~"91 o ) ll~?cepted
It('P . " lll~t accepted(('~/ ,  ' '~" "  , ~"" t:""N--a ""~') most'sentence-like ' . For that purpose this system calculate the score of likelihood of each sentences  ( section 3 . 4) . 
4.2 The Corpus
A corpus is a set of sentences , These sentences are of target language . For example , when we apply this system to Japanese morphological nalys is we need a corpus of Japanese sentences which are already segmented  . 
The corpus prepared for the paper is the translation of English textbooks for Japanese junior high school students  . The reason why we selected junior high school textbooks is that the sentences in the textbooks are simple and do not include too many words  . 
This is a good environment for evaluating this system  . 
4.3 The Dictionary
The dictionary for MSS is made of two part . One is the heading words and the other is the morphemes corresponded to the headings  . There may be more than one morphemes attached to one heading word  . 
The second part which has morphemes i of type list  , so that it can have several morphemes . 
Japanese : (" I , , ~"(  " ~ : ~ . . . . ~:\[ o " ) ) heading word morphemes Chinese : ( " tiny " ( " ~ , ,  .   .   .   . ~ t " )   ) heading word morpherne ~ 5 RESULTS Implement MSS to all input sentences and get the score of each segmentation  . After getting the list of segmentations , look for the ' correct'segmented-sentence and see where in the list tile right one is  . 
The data shows the scores the ' correct ' segmentations got  ( table 2 )  . 
Table 2: Experiment in Japanese corpus dictionary input number of input sentence distance limit about  630 J ~ t p , 'tne sentences ( with three kinds of letters mixed ) about 1500 heading words ( includes morphemes not in tile corpus ) lion-segmented Ja . p ; ~neselltences using lllragana only about 100 e~tcha 99% loo % 7   100%   95% 
E 80%   2nd bestT ~ 3rd best 100%   100%   100  %  100  %  100%   :100%   98  %  98  %  90  %  95 % the very sentences in tile corpus replaced one rnorll heme in the sentence  ( the buried morpheme is in the corpus ) replaced one morpheme in the sentence ( tile buried morp beme is not in the corpus ) sentences not in the corpus ( the morphemes are all intim corpus ) sentences not in the corpus ( include morphemes not ; in the corpus ) 5 . 1 Ext ) eriment in Japanese According to the experimental results  ( table 2 )  , it is obvious that MSS is w . 'ry useful . The table 2 shows that most of the sentences , no matter whether the sentences are in the . corpus or not , are segmented correctly . We find the right segmentation getting the best score in the list of possible segmentations  , c ~ is tile data when the input sentences are in corpus  . 
That is , all the ' correct ' morphemes have association between each other  . That have a strong effect in calculating the sco , ' es of sentences . The condition is almost same for fl and 7 . Though the sentence has one word replaced , all other words in the sentence have relationship between them  . Tim sentences in 7 in-elude one word which is not in the corpus , but still tile ' correct ' sentence can get the best score among the possibilities  . We can say that the data c ~ , fl and 7 are very successfld . 
230 llowever , we shouhl remember that not all the sentences in the given corpus wouhtget the best score through the list  . MSS does trotcheek the corpus itself when it calculate the score  , it just use the Mid , the essential information of the corpus . That is , whether the input sentence is written in the corpus or not does not make any effect in calculating scores directly  . 
I lowever , since MSS uses Mid to calculate the . scores , the fact that every two morphemes in the sentence have connection between them raises the score higher  . 
When it comes to the sentences which are not in corpus themselves  , the ratio that the ' correct ' sentence get the best score gets down  ( see table 2 , data ~ , e ) . 
The sentences of 6 and g are not found in the corpus . Even some sentences which are of spoken language and not grammatically correct are included in the input sentences  . It can be said that those ~ and e sentences arc nearer to the realworhlof Japanese language  . For tisentences we used only morphemes which are in the corpus  . That means that all timmor-phenres used in the 5 sentences have their own MI , I . 
And e sentences have both morphemes it ( the corpus and the ones not in the corpus . The morphemes which arc not in the corpus do not have any Ml  ( l . Table 2 shows that MSS gets quite good result eve (  , though the input sentences arc not in the corpus . MSS do not take the necessary information directly from the co>pus and it uses the MI a instead  . This method makes the information generalize . d and this is the reason why 5 and e can get good results too . Midcomes to > e the key to use the effect of the MI between morphemes indirectly so that wc can put the information of thems soeiation between morphemes to practical use  . This is what we expected and MSS works successfldly at this point  . 
5.2 The Corpus
In this paper we used the translation of English text : books for Japanese junior high school students  . Primary textbooks are kiud of a closed worhl which have limited words in it an < l the included sentences are mostly in some lixed styles  , in good graummr . The corpus we used in this pal ) er has about 630 sentences which have three types of Japanese letters all mixed  . 
This corpus is too small to take ms a model of the  , ' ealworld , however , for this pal > e(it is big enough . Actually , the results of this paper shows that this system works efficiently even though the corpus is small  . 
The dictionary an < l the statistical information are got from the given corpus  . So , the experimental re=suit totally depends on the corpus  . That is , selecting which corpus to take to implement , we can use I . his system ill many purposes ( section 5 . 5) . 
5.3 Comparison with the Other

It is not easy to compare this system with other seg-  , nenting methods . We coral ) are with tile least-bunsetsu method here ill this paper  . 
The least-bunsels v method segment the given sentences into morphemes and fin  ( l the segmentations with least bunsels u . This method makes all the segmentation first an ( l selects the seems-like-best segmentations . This is the same way MSS does . The difference is that the least-bdn sets v method checkes then mn be r of tile bumsels u instead of calculating the scores of sen  ( elites . 
Let us think about implementing a sentence the morl  ) hcmes arel , ot in the dictionary . That means that the morphemes do not have any statistical informations between them  . In this situation MSS cannot use statistical informations to get the scores  . Of course MSS caliculate the scores of sentences accord : ing totile statistical informations between given morphemes  , llowe . ver , all the Ml , l say that they have no association I ) etween t\]le ( ~lorpherlles . When there is no possibility that the two morl > hemes appears together ill the corpus  , we give a minus score ~ stit (' . Ml , tw due , so , as the result , with more morphemes the score of the + sentence gets lower  . That is , tire segmentation which has less segments illit gets better scores  . Now compare it with the least-bunsetsu method . With using MSS the h . ' ast-morpheme segme . ntations are selected as the goo ( I answer , q ' hatistile same way the least-bunsetsu method selects the best one  . '\[' his means that MSS and the least-bttnscts . lemt hod have the same efficiency when it comes to the sentences which morl  ( hemes are not in the corpus . It is obvious that when the sentence has morphemes in the corpus the ellicie  . ncy of this systern get sumch higher ( table 2) . 
Now it is proved that MSS is , at least , a set li:cicn tas the least-b'unsets'~nmthod , no matter what sentence it takes . We show a data which describes
I . his ( tabh~3).
" Fable 3 is a good exan q ) le of the c ; use whel L the . 
input sentence has few morphemes which are in the corl  ) uS . This dal . a shows that in I . his situal . ionI . here is an outstanding relation between the number of mor-l  ) hemes and the scores of the segmented se . ntenees . 
This example ( table 3 ) has an ambiguity how to segment the sentence using the register e  ( l morphemes , and all the morphemes which causes the alnbiguity are not in the given  ( : or pus . Those umrl ) hemes not in the corpus do not have any statistical information betweel  , them and we have no way to select which is bett < . ' r . So , the scores of sentences are Ul ) to the length of the s < ~ gmented sentence , that is , the number how many morl ) hemes the sentence has . '\[' he segmented sentence which has least segments gets the best score  , since MSS gives a minus score for unknown mssocia -tion between morphemes  . That means that with more segments in the sentence the score gets lower  . This sit-

Table 3: MSS and Theleast-bvnselsu method input : a non -segmented 
Japanese tliragan a sentence not in the corpus all unknown morphemes in the sentence are registered in the  ( lictionary ( some morphemes in the corpus are included ) " sumomomonlonlo hieme monoilCh\]the number of the morphemes  6   7   8   9   10 the scores of the sentences -65   , 0 -79  . 6 -9,1 . 3 -108  . 9 -123 . 5 the number o f t i le segmented 5   20   21   8   1 sentence stile t correct l segmentation ~ k "
MSSO tile least-bunsetsu 0 method morphemes included : "? .   .   .   . ~2" in the corpus : " no .   .   .   . Ill(lllO"morphemes not included : " IAI .   .   .   .  ~4! . ~" in the corpus : " u chi .   .   .   . sunm " " sumomo "* ' hiej ~" ~ t " ' PnlOUlO ~ puation is resemble to the way how the least -bunseisu method selects the answer  . 
5.4 Experiment in Chinese
The theme of tiffs paper is to segment non -separa Le  ( \] language sentences into morphemes . In this paper we described on segmentation f Japanese non-segmented sentences only but we are working on Chinese sentences too  . This MSS is not for Japanese only . It can be used for other non-separated languages too  . "lb implement for other languages , we just need to prepare the corpus for that and make up the dictionary from it  . 
llere is the example of implementing MSS for Chinese language  ( table 4 )  . The input is a string of characters which shows the pronounciations of a Chinese sentence  . MSS changes it into Chinese characters en-teces , segmenting the given string . 
5.5 Changing the Corpus
To implement iffs MSS system , we only need a eelp us . The dictionary is made from the corpus . This
Tal ) le 4: Experiment in Chinese input : nashiyiz hangditu . 
correct answer output sentences scores - ~ ) J\[~~+-- , ~; t ~ .  15 . 04735) Jl ~ ~! - - ~ . t t ~\] .  -14 . 80836) JI~0~--'\]~~1~ .  -14 . 8 0836 gives MSS system a lot of usages and posibilities . 
Most of the NLP systems need grammatical i , ff of malleus , and it is very hard to make up a certain grammatical rule to use in a NLP  . The corpus MSS needs to implement is very easy to get  . As it is described in the previou section , a corpus is a set of real sentence . s . We can use IVISS in other languages or in other purposes just getting a certain corpus for that and making up a dictionary from the corpus  . That is , MSS is available in many lmr poses with very simple  , easy preparation . 
6 CONCLUSION
This paper shows that this automatic segmenting system MSS is quite efficient for segmentation of non-separated language sentences  . MSS do not use any grammatical information to divide input sentences  . 
Instead , MSS uses MI l ) etween morphenres included in the input sentence to select the best segmentation  ( s ) frorn all the possibilities . According to the results of the experiments , MSS can segmentah nost all the sentences ' correctly '  . This is such a remarkable result . 
When it comes to the sentences which are not in the corpus the ratio of selecting the right segmentation as the best answer get a little bit lower  , however , the result is considerably good enough . 
The result shows that using Midbetween morphemes is a very effective method of selecting ' correct ' sentences  , aml this means a lot in NLP . 
REFERENCES\[1\]Kenneth Church , William Gale , Patricklhmks , and Donaldllindle . Parsing , Word Associations and Typical Predlcate-Argument t  , elations . International Parsing Workshop , 1989 . 
\[2\]F rankSmadja . I towto compile a hilingual collocational lexicon automatically  . Statislically-based Natural Language Programming Techniques  , pages 57--63 ,  1992 . 
\[3\] dunya Tsutsurni , Tomoaki Nitta , Kotaro One , and Shlho Nobesawa . A MultiLingual Translation System Based on A Statistical Model  ( written in Jal ) anese )  . JSAI Technical report , SIG-PPAI-9302-2 , pages 712 ,  1993 . 
232\[4\]David M . Magerman and Mitchell P . Marcus . Parsing a Natural Language Using Mutual Information 
Statistics . AAAI , 1990.
\[5\]It . Brown , J . Cocke , S . Della Pietra , V . Della Pietra , F . Jelinek , R . Mercer , and P . Rooss in . A Statist , i-eal Approach to Language Translation . l ' roc , of
COLING-88, pages 71-76, 1989.

