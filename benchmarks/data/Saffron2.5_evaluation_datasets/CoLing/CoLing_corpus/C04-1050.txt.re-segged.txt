Improving Japanese Zero Pronoun Resolution
by Global Word Sense Disambiguation
Daisuke Kawahara and Sadao Kurohashi
Graduate School of Information Science and Technology  , University of Tokyo


This paper proposes unsupervised word sense disambiguation based on automatically constructed case frames and its incorporation into our zero pronoun resolution system  . The word sense disambiguation is applied to verbs and nouns  . We consider that case frames define verb senses and semantic features in a thesaurus define noun senses  , respectively , and perform sense disambiguation by selecting them based on case analysis  . In addition , according to the one sense per discourse heuristic  , the word sense disambiguation results are cached and applied globally to the subsequent words  . 
We integrated this global word sense disambiguation into our zero pronoun resolution system  , and conducted experiments of zero pronoun resolution on two different domain corpora  . Both of the experimental results indicated the effectiveness of our approach  . 
1 Introduction
For a long time , parsing has been a central issue for the area of natural language analyses  . 
In recent years , its accuracy has improved to over 90% , and it became the fundamental technology that is applied to a lot of NLP applications  , such as question answering , text summarization , and machine translation . Accordingly , anaphora resolution , which is positioned as the next step of parsing , has been studied actively ( Ng and Cardie , 2002; Yang et al , 2003; Iida et al , 2003; Isozaki and Hirao , 2003; Kawahara and Kurohashi ,  2004) . Its performance , however , is not satisfactory enough to benefit the NLP applications  . We investigated errors of our Japanese zero pronoun resolution system  ( Kawahara and Kurohashi ,  2004) , and found that word sense ambiguity causes a major part of errors  . 
Our zero pronoun resolution system utilizes the general-purpose thesaurus Nihongo GoiTaikei  ( Ikehara et al , 1997) ( hereafter , NTT thesaurus ) to do matching of example words . 
In this thesaurus , one or more semantic features are given to each word  , and similarity between words can be calculated by comparing closeness of their semantic features in the thesaurus tree  ( Appendix A )  . Multiple semantic features for a word , i . e . word sense ambiguity , cause incorrect matching , and furthermore deteriorate accuracy of the zero pronoun resolution system  . For instance , in the thesaurus , ? gobou ? ( burdock/priest/temple ) has four semantic features : < crop > , <vegetable > , < priest > and < temple >? . If < priest > is used for ? gobou ? in a cooking domain text  , though ? gobou ? means ? burdock ? ( agenus of coarse biennial herbs ) in its context , ? gobou ? is identified as an agent . 
That is , ? gobou ? is incorrectly analyzed as antecedents of the following nominative zero pronouns  . 
If such word sense ambiguity can be resolved , incorrect matching decreases , and the anaphora resolution system will improve . Word sense disambiguation is a basic issue of NLP  . Recently , the evaluation exercises for word sense disambiguation such as  SENSEVAL1   ( Kilgarriff and Palmer , 2000) and SENSEVAL2 ( Yarowsky , 2001) have been held , but word sense disambiguation has rarely been incorporated into deep analyses like anaphora resolution  . 
This paper proposes unsupervised word sense disambiguation based on automatically constructed case frames and its incorporation into the zero pronoun resolution system  . The word sense disambiguation is applied to verbs ? and nouns  . We consider that case frames define verb senses and semantic features in the thesaurus define noun senses  , respectively . A verb is disambiguated by selecting a corresponding case frame to its context  , and a noun is disambiguated by selecting an appropriate semantic ? In this paper  , <> means a semantic feature . 
? In this paper , we use ? verb ? instead of ? verb , adjective and noun + copula ? for simplicity . 

CONCRETEABSTRACT
AGENT PLACE CONCRETE
HUMANORGANIZATION
ABSTRACTEVENT ABSTRACT RELATION
TIME POSITION QUANTITY ....
Figure 1: The upper levels of the NTT thesaurus.
feature from the ones defined for the noun in the thesaurus  . In addition , according to the one sense per discourse heuristic  , the disambiguation results are cached and applied globally to the following words in the same text  . 
The remainder of this paper is organized as follows  . Section 2 briefly describes the NTT thesaurus and the automatic construction method of case frames  . Section 3 outlines our zero pronoun resolution system . Section 4 describes the method of word sense disambiguation and its integration into the zero pronoun resolution system  . Section 5 presents the experiments of the integrated system  . Section 6 summarizes the conclusions . 
2 Resources
We consider that verb and noun senses correspond to case frames and semantic features defined in the NTT thesaurus  , respectively . This section describes the NTT thesaurus and the case frames briefly  . 
2.1 NTT thesaurus
NTT Communication Science Laboratories constructed a semantic feature tree  , whose 3 , 000 nodes are semantic features , and a nominal dictionary containing about 300 , 000 nouns , each of which is given one or more appropriate semantic features  . Figure 1 shows the upper levels of the semantic feature tree  . 
The similarity between two words is defined by formula  ( 1 ) in Appendix A . 
2 . 2 Automatically constructed case frames We employ the automatically constructed case frames  ( Kawahara and Kurohashi ,  2002 ) as the basic resource for zero pronoun resolution and word sense disambiguation  . This section outlines the method of constructing the case frames  . 
The biggest problem in automatic case frame construction is verb sense ambiguity  . Verbs which have different meanings should have different case frames  , but it is hard to disambiguate verb senses precisely  . To deal with this problem , predicate-argument examples which are collected from a large corpus are distinguished by coupling a verb and its closest case component  . That is , examples are not distinguished by verbs ( e . g . 
? tsumu ? ( load/accumulate )), but by couples ( e . g . ? nimotsu-wo tsumu ? ( load baggage ) and ? keiken-wo tsumu ? ( accumulate experience ) ) . 
This process makes separate case frames which have almost the same meaning or usage  . 
For example , ? nimotsu-wo tsumu ? ( load baggage ) and ? busshi-wo tsumu ? ( load supply ) are similar , but have separate case frames . To cope with this problem , the case frames are clustered . 
Example words are collected for each case marker , such as ? ga ? , ? wo ? , ? ni ? and ? kara ? . They are case-marking postpositions in Japanese , and usually mean nominative , accusative , dative and ablative , respectively . We call such a case marker ? case slot ? and example words in a case slot ? case examples ?  . 
Case examples in a case slot are similar , but have some incorrect semantic features because of word sense ambiguity  . For instance , ? ni-motsu ? ( baggage ) , ? busshi ? ( supply ) and ? nise-mono ? ( imitation ) are gathered in a case slot , and all of them are below the semantic feature < goods >  . On the other hand , ? nisemono ? belongs to < lie > . < lie > is incorrect for this case slot , and possibly causes errors in case analysis . 
We delete a semantic feature that is not similar to the other semantic features of its case slot  . 
To sum up , the procedure for the automatic case frame construction is as follows  . 
1. A large raw corpus is parsed by the
Japanese parser , KNP ( Kurohashi and
Nagao , 1994b ) , and reliable predicate-argument examples are extracted from the parse results  . 
2 . The extracted examples are bundled according to the verb and its closest case component  , making initial case frames . 
3 . The initial case frames are clustered using a similarity measure function  . This similarity is calculated by formula ( 5 ) in Appendix

4 . For each case slot of clustered case frames , an inappropriate semantic feature that is not similar to the other semantic features is discarded  . 
We constructed two sets of case frames : for newspaper and cooking domain  . 
The newspaper case frames are constructed from about  21  , 000 , 000 sentences of newspaper articles in 20 years ( 9 years of Mainichi newspaper and 11 years of Nih on keizai newspaper )  . 
They consist of 23 , 000 verbs , and the average number of case frames for a verb is  14  . 5 . 
The cooking case frames are constructed from about  5  , 000 , 0 00 sentences of cooking domain that are collected from WWW  . They consist of 5 , 600 verbs , and the average number of case frames for a verb is  6  . 8 . 
In Figure 1 , some examples of the resulting case frames are shown  . In this table , ? CS ? means a case slot . < agent > in the table is a generalized case example  , which is given to the case slot where half of the case examples belong to < agent >  . < agent > is also given to ? ga ? case slot that has no case examples  , because ? ga ? case components are often omitted , but ? ga ? case slots usually mean nominative . 
3 The Outline of the Zero Pronoun
Resolution System
We have proposed a Japanese zero pronoun resolution system using the case frames  , antecedent preference orders , and a machine learning technique ( Kawahara and Kurohashi ,  2004) . 
Its procedure is as follows.
1 . Parse an input sentence using the Japanese parser  , KNP . 
2 . Process each verb in the sentence from left to right by the following steps  . 
Table 1: Case frame examples.
CS case examples ? ga < agent > , group , party , ??? youritsu (1) wo < agent > , candidate , applicant ( support ) ni < agent > , district , election , ??? ga < agent > your its u(2) wo < agent > , member , minister , ??? ( support ) ni < agent > , candidate , successor . . .   . . .   . . . 
or osu ( 1 ) ga < agent > ( grate ) woradish ga < agent > or osu ( 2 ) wo money ( withdraw ) karabank , post . . .   . . .   . . . 
it a daku ( 1 ) ga < agent > ( have ) wo soup ga < agent > it adaku ( 2 ) wo advice , instruction , address ( be given ) kara < agent > , president , circle ,  ? ? ?  . . .   . . .   . . . 
? case examples are expressed only in English for space limitation  . 
2 . 1 . Narrow case frames down to corresponding ones to the verb and its closest case component  . 
2 . 2 . Perform the following processes for each case frame of the target verb  . 
i . Match each input case component with an appropriate case slot of the case frame  . Regard case slots that have no correspondence as zero pronouns  . 
ii . Estimate an antecedent of each zero pronoun.
2 . 3 . Select a case frame which has the highest total score  , and output the analysis result for the case frame . 
The rest of this section describes the above steps  ( 2 . 1), (2 . 2 . i ) and (2 . 2 . ii ) in detail . 
3.1 Narrowing downcase frames
The closest case component plays an important role to determine the usage of a verb  . In particular , when the closest case is ? wo ? or ? ni ? , this trend is clearcut . In addition , an expression whose nominative belongs to < agent  >   ( e . g . 
? < agent > has accomplished ?) , does not have enough clue to decide its usage , namely a case frame . By considering these aspects , we impose the following conditions on narrowing downcase frames  . 
? The closest case component exists , and must immediately precede its verb . 
? The closest case component and the closest case meet one of the following conditions : ? The closest case is ? wo ? or ? ni ?  . 
? The closest case component does not belong to the semantic marker < agent >  . 
? A case frame with the closest case exists , and the similarity between the closest case component and examples in the closest case exceeds a threshold  . 
We choose the case frames whose similarity is the highest  . If the above conditions are not satisfied , case frames are not narrowed down , and the subsequent processes are performed for each case frame of the target verb  . The similarity used here is defined as the best similarity between the closest case component and examples in the case slot  . The similarity between two examples is defined as formula  ( 1 ) in Appendix

Let us consider ? your its u ? ( support ) in the second sentence of Figure 2 . ? your its u ? has the case frames shown in Table 1 . The input expression ? kouho-wo your its u ? ( support a candidate ) satisfies the above two conditions , and the case frame ? your its u ( 1 ) ? meets the last condition . 
Accordingly , this case frame is selected.
3 . 2 Matching input case components with case slots in the case frame We match case components of the target verb with case slots in the case frame  ( Kurohashi and Nagao , 1994a ) . When a case component has a case marker , it must be assigned to the case slot with the same case marker  . When a case component is a topic marked phrase or a clausal modifiee  , which does not have a case marker , it can be assigned to one of the case slots in the following table  . 
topic marked phrases : ga , wo , ga2 clausal modifie es:ga , wo , non-gapping The conditions above may produce multiple matching patterns  . In this case , one which has the best score is selected . The score of a matching pattern is defined as the sum of similarities of case assignments  . This similarity is calculated as the same way described in Section  3  . 1 . 
              fffi flffi  fi"! fi# fi$ffifl"!#%"! & fi'  ( ) fl   %  "!#%  % +* ,  -%  . / ,   01 ,  * % -1 ,    2 ff ,      4  fi # "!#% ( ) fl    5               %    4   5       78   7   9#:   79<  ;  79>= 79<?
Figure 2: Analysis example.
The result of case analysis tells if the zero pronouns exist  . That is , vacant case slots in the case frame , which have no correspondence with the input case components  , mean zero pronouns . In this paper , we concentrate on three case slots : ? ga ? , ? wo ? , and ? ni ? . 
In the case of ? your its u ? ( support ) in Figure 2 and the selected case frame ? your its u ( 1 ) ? , ? wo ? case slot has a corresponding case component  , but ? ga ? and ? ni ? case slots are vacant . 
Accordingly , two zero pronouns are identified in ? ga ? and ? ni ? case of ? your its u ?  . 
3.3 Antecedent estimation
The antecedents of the detected zero pronouns are estimated  . Possible antecedents are examined according to the antecedent preference order  ( Kawahara and Kurohashi ,  2004) . If a possible antecedent is classified as positive by a binary classifier and its similarity to examples in its case slot exceeds a threshold  , it is determined as the antecedent . 
For example , ? youritsu ? ( support ) in Figure 2 has zero pronouns in ? ga ? and ? ni ? cases . The ordered possible antecedents for ? ga ? are L7:?Minsyutou  ? , L14: ? Jim intou?(?ga) , L14: ? Ishiharachiji ? (? wo) ,  ? ? ? . The first candidate ? Minsyutou ( similarity : 0 . 73)? , which is labeled as positive by the classifier , and whose similarity to the case frame examples exceeds a threshold  ( 0 . 60), is determined as the antecedent . 
4 Global Word Sense

We integrate a global method of word sense disambiguation into the zero pronoun resolution system described in the previous section  . The word sense disambiguation is applied to verbs and nouns based on the case frames  . Furthermore , the word sense disambiguation results are cached and applied globally by the subsequent analyses based on the one sense per discourse heuristic  . In the rest of this section , we describe verb and noun sense disambiguation respectively using examples of cooking domain  . 
4.1 Verb sense disambiguation
The case frames are specific enough to the diversity of verb senses  , because their meanings are distinguished by the couple of the verb and its closest case component  ( Kawahara and Kurohashi ,  2002) . We regard the process of verb sense disambiguation as the case frame selection  ( Step ( 2 . 3) described in Section 3) . In addition , the verb sense disambiguation results are cached and applied globally in the same text  . In other words , the selected case frames are cached for each verb  , and only the case frames that are similar to the cache are used for the same verb following in the same text  . The similarity measure for two case frames is stated in Appendix B  , and the threshold is set to 0 . 60 empirically . 
Here is an example article that consists of three sentences  . 
or oshi-gane-degrater kabura-wo turnip or oshite -ikimasu  . 
grate(Let?s grateaturnip . ) kore-wa thisookii bigkaburaturnip desu . 
be ( This is a big turnip . ) konoyouni like this or oshi-masu . 
grate ( Gratelike this.)
For ? or oshite ?( grate ) in the first sentence , the case frame ? or osu(1) ? ( in Table 1) , which means ? grateradish ? , is selected , because the closest case component ? kabura ? ( turnip ) exists , and is very similar to the ? wo ? case example ? daikon ?  ( radish )  . This selected case frame is cached for the verb ? or osu ?  . For ? or oshi?(grate ) in the third sentence , case frames are not narrowed down for lack of the closest component  . The previous system performs the antecedent estimation process for all the case frames of ? or osu ?  , and incorrectly estimates the antecedent of ? wo ? zero pronoun as ? or oshi-gane ?  ( grater ) ? . On the other hand , our proposed method deals with only the similar case frames to the cached ? or osu  ( 1 ) ? . That is , the case frame ? or osu(2)? , which means ? with draw money from bank or post ? , is not similar to ? or osu(1)? , and is not used . Accordingly , the system certainly estimates the antecedent of ? wo ? zero pronoun as ? kabura ?  ( turnip )  . 
4.2 Noun sense disambiguation
We define the process of noun sense disambiguation as selecting an appropriate semantic feature from the ones given to a noun in the NTT thesaurus  . This process is performed based on the matching of the input case components and the case frame decided by the step  ( 2 . 3) described in Section 3 . For each input case component , its semantic features are matched against those of case examples of its corresponding case slot  , and the best matched one is selected . In addition , this disambiguation result is applied globally like the verb sense disambiguation  . The determined semantic feature is cached for each noun  , and is given to the same noun following in the same text  , instead of reconsidering all of its semantic features  . Here is an example article . 
mazuwa firstosu mashi-wo clear soup it adaki- masu  . 
have ( First , let?s have clear soup . ) honkakutekina real dashi - wo stock tori - mash it a  . 
prepare ( We prepared real stock . ) ? itadaki ? ( have ) in the first sentence has the closest case component ? osumashi ?  ( clear soup )  , and the case frame ? it a daku ( 1 ) ?  ( in Table 1 ) is selected , because its ? wo ? case example ? soup ? is very similar to ? osumashi ?  . 
In the NTT thesaurus , ? osumashi ? ( clear soup ) has three semantic features : < soup > , < look > and < eccentric > . < eccentric > is located below < agent > in the thesaurus  , and the previous system incorrectly estimates antecedents of ? ga?zero pronouns of the following verbs as ? osumashi ?  ( because almost all the ? In Japanese , ? gane ? of ? or oshi-gane ? ( grater ) exactly matches with ? kane ? ( money )  , the ? wo ? case example of ? or osu(2)? . 
Table 2: Accuracy ( newspaper).
precision recall F baseline 515/924 (0 . 557) 515/1087 (0 . 474) 0 . 512 our method 526/911 (0 . 577) 526/1087 (0 . 484) 0 . 527
Table 3: Accuracy ( cooking).
precision recall F baseline 696/1092 (0 . 637) 696/1482 (0 . 470) 0 . 541 our method 713/1081 (0 . 660) 713/1482 (0 . 481) 0 . 5 56 case frames have < agent > in their ? ga ? case slots  )  . In our approach , each of the semantic features are matched against the case example ? soup ?  , and only the best matched semantic feature < soup > is given to ? osumashi ?  . 
5 Experimental Results and

We conducted experiments of zero pronoun resolution on two different domain corpora  . One is newspaper articles of ? Relevance-tagged corpus ?  ( Kawahara et al ,  2002) , and the other is utterances of cooking TV programs  . These cooking utterances were handled by ( Shibata et al . , 2003) . They annotated various relations to closed captions of the cooking utterances based on the specification of the ? Relevance-tagged corpus ?   ( Kawahara et al ,  2002) . 
For newspaper domain , the antecedent preference and the classifier were trained with  1  , 841 sentences in the newspaper corpus , and the newspaper case frames were used . The experiment was performed on 6333 sentences . For cooking domain , we used 813 sentences (5TV programs ) , and conducted 5-fold crossvalidation using the cooking case frames . 
We evaluated ? ga ? , ? wo ? and ? ni ? cases that are the large majority of zero pronouns  . The experimental results are shown in Table 2 and Table 3  . The accuracies in these tables are calculated by evaluating both detection and antecedent estimation of zero pronouns together  . 
The baseline corresponds to our previous system without word sense disambiguation  . 
From Table 2 and Table 3 , we can see that the system accuracy is improved by the global word sense disambiguation  . The improvement is not big , but there are no analysis result that changed for the worse  . The improvement is hardly contributed by the verb sense disambiguation  , but mainly by the noun sense disambiguation . This is because appropriate case frames are used in many cases without the verb sense disambiguation  , and this process did not lead to the improvement . The number of case frames of the verbs to which the verb sense disambiguation is applied decreased to  16%  , and this indicates that the analysis efficiency improved significantly  . In addition , we evaluated randomly selected 100 nouns to which the noun sense disambiguation is applied  . 91 nouns were disambiguated correctly , and quite high disambiguation accuracy was achieved  . 
6 Conclusion
This paper has incorporated a framework of global word sense disambiguation into a Japanese zero pronoun resolution system  . The word sense disambiguation is applied to verbs and nouns  . A verb is disambiguated by selecting a corresponding case frame to its context  , and a noun is disambiguated by selecting an appropriate semantic feature  . Furthermore , the disambiguation results are cached and applied globally  . That is to say , it is utilized in the following analyses in the same text  . In the future , we will investigate the word sense disambiguation errors further  , and expect to improve the system accuracy . 

Ryu Iida , Kentaro Inui , Hiroya Takamura , and Yuji Matsumoto .  2003 . Incorporating contextual cues in trainable models for coreference resolution  . In Proceedings of the 10th EACL Workshop on The Computational Treatment of Anaphora  , pages 23?30 . 
Satoru Ikehara , Masahiro Miyazaki , Satoshi Shirai , Akio Yokoo , Hiromi Nakaiwa , Kentarou Ogura , and Yoshifumi Oyama Yoshihiko Hayashi , editors . 
1997. Japanese Lexicon . Iwanami Publishing.
Hideki Isozaki and Tsutomu Hirao .  2003 . Japanese zero pronoun resolution based on ranking rules and machine learning  . In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing  , pages 184?191 . 
Daisuke Kawahara and Sadao Kurohashi . 2002.
Fertilization of case frame dictionary for robust Japanese case analysis  . In Proceedings of the 19th International Conference on Computational Linguistics  , pages 425?431 . 
Daisuke Kawahara and Sadao Kurohashi . 2004.
Zero pronoun resolution based on automatically constructed case frames and structural preference of antecedents  . In Proceedings of the 1st International Joint Conference on Natural Language 
Processing , pages 334?341.
Daisuke Kawahara , Sadao Kurohashi , and Ko?iti Hasida .  2002 . Construction of a Japanese relevance-tagged corpus  . In Proceedings of the 3rd International Conference on Language Resources and Evaluation  , pages 2008?2013 . 
Adam Kilgarriff and Martha Palmer .  2000 . Introduction to the special issue on SENSEVAL . Computers and the Humanities , 34(1):1?13 . 
Sadao Kurohashi and Makoto Nagao . 1994a . A method of case structure analysis for Japanese sentences based on examples in case frame dictionary  . In IEICE Transactions on Information and Systems , volume E77-DNo . 2 . 
Sadao Kurohashi and Makoto Nagao . 1994b . A syntactic analysis method of long Japanese sentences based on the detection of conjunctive structures  . 
Computational Linguistics , 20(4):507?534.
Vincent Ng and Claire Cardie .  2002 . Improving machine learning approaches to coreference resolution  . In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics  , pages 104?111 . 
Tomohide Shibata , Daisuke Kawahara , Masashi Okamoto , Sadao Kurohashi , and Toyoaki Nishida . 
2003 . Structural analysis of instruction utterances . In Proceedings of Seventh International Conference on Knowledge-Based Intelligent Information and Engineering Systems  ( KES 2003 )  , pages 1054?1061 . 
Xiaofeng Yang , Guodong Zhou , Jian Su , and Chew Lim Tan .  2003 . Coreference resolution using competition learning approach  . In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics  , pages 176?183 . 
David Yarowsky , editor .  2001 .   SENSEVAL2: Second International Workshop on Evaluating Word Sense Disambiguating Systems  . The Association for Computational Linguistics . 

A Similarity between examples
The similarity between two examples e1 ,   e2 is calculated using the NTT thesaurus as follows : sim  ( e1 , e2) = max x?s1 , y?s2 sim(x , y ) (1) sim(x , y ) = 2Llx + ly where x , y are semantic features , and s1 , s2 are sets of semantic markers of e1 , e2 respectively . 
lx , ly are the depths of x , y in the thesaurus , and the depth of their lowest ( most specific ) common node is L . If x and y are in the same node of the thesaurus , the similarity is 1 . 0, the maximum score based on this criterion . 
BSimilarity between case frames
Two case frames , F1 and F2 , are first aligned according to the agreement of case markers  ( case slots )  . Suppose the result of the case slot alignment of F1 and F2 is as follows:
F1:C11,C12, ? ? ? C1 l???C1 mlll
F2: C21 , C22 ,  ? ? ?  C2l  ? ? ?  C2n where Cx x denotes a case slot which contains several case examples  . This result means that l case slots are aligned between  F1 and F2 and ( m ? l ) and ( n ? l ) case slots remained in F1 and F2 respectively . 
The similarity between two case slots , C1 i and C2 i , is the sum of the similarities of case examples as follows : sim  ( C1 i , C2i ) = ? e1?C1 i ? e2 ? C2i ? e1 e2 ? sim(e1 , e2 ) ? e1 ? C1 i ? e2 ? C2 i ? e1 e2 ( 2 ) where e1 and e2 represent the frequencies of e1 and e2 respectively . 
The similarities of case slots are summed up with the weight of frequencies of case examples as follows: 
WS of CS = ? l i=1 ? C1 iC2 i ? sim(C1i , C2i ) ? l i=1 ? C1 iC2 i ( 3 ) where C1i = ? e1?C1 ie 1 , C2i = ? e2?C2 ie 2On the other hand , the ratio of aligned case slots is calculated as follows: 
R of ACS = ?? l i=1    C1i?m   i=1    C1i ? ? l i=1    C2i?n   i=1    C2i    ( 4 ) Finally , the similarity between case frames is calculated as follows : sim  ( F1 , F2) = WS of CS?R of ACS(5)
