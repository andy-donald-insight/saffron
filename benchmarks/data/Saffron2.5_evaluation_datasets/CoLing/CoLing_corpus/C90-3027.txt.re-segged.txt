A Constraint-Based Approach to Linguistic Performance * 
HASIDA , KSiti
Tokyo University
7-3-1, Hongo , Bunkyoku , Tokyo 113, Japan
Electrotechnical Laboratory
1-1-4, Umezono , Tukuba , Ibaraki 103, Japan
Institute for New Generation
Computer Technology ( ICOT)
Mita-Kokusai Bldg . 21F , 1-4-28,
Mira , Minato-ku , Tokyo 108, Japan
-I-81-3-456-3194, hasida@i cot.jp
Abstract
This paper investigates linguistic performance , from the viewpo in that the information processing in cognitive system should be designed in terms of constraints rather than procedures in order to deal with partiality of information  . In this perspective , the same grammar , the same belief and the same processing architecture should underly both sentence comprehension adproduction  . A basic model of sentence processing , for both comprehension adproduction , is derived along this llne of reasoning . This model is demonstrated to account for diverse linguistic phenomenap parently unrelated to each other  , lending empirica ! support to the constraint paradigm  . 
1 Introduction
All the cognitive agents , with limited capacity for information processing , face partiality of information : Information relevant otheir activities is only partially accessible  , and also the distribution pattern of the accessible information is too diverse to predict  . In sentence comprehension , for example , the phonological or morphological information may or may not be partially missing due to some noise  , the semantic information may or may not be abundant because of familiality or ignorance on the topics  , and so forth . Thus the information distribution is very diverse and rather orthogonal to the underlying information structure consisting of the modules of morphological  , syntactic , pragmatic , and other constraints . 
This diversity of information distribution gives rise to a very complex  , non-modular flow of information in cognitive processes  , as information flows from places possessing information to places lacking information  . 
In order to deal with this complexity , a cognitive system must be designed to include two different logical layers : '  ( 1 ) Information represented in terms of constraints , * The work reported here started as the author's doctoral research at Tokyo University  , and has developed filrther at Elec-tro technical Laboratory and ICOT  . The author's current affiliation is ICOT . lt is thanks go to Pr of . YAMADAIt is a o , who was the supervisor f the doctoral program , and too many other people to enu , nerate hre . 
by abstracting away information flow.
(2 ) A general processing mechanism to convey information across constraints  , from places possessing information to places lacking it  . 
>' on-modular flow of information may be captured on the basis of modular design of cognitive architecture  , only by separating the representation f underlying information  ( as ( 1 ) ) and flow of information ( as ( 2 ) ) fl'om each other . 
Procedural approaches breakdown under partial-ity of information  , because procedures stipulate , and hence restrict , information flow . If one . be it human or nature , we reto implement such diverse information flow by procedural programming  , the entire system would quickly become too complex to keep track of  , failing to maintain the modularity of the system . This is what has always happened , for example , in the development of natural anguage processing systems  . 
The rest of the paper exemplifies the efficacy of the constraint paradig  , n with regard to natural language . 
We wil ! first discuss a general picture of language faculty immediately obtained fl ' om the constraint-based view  , and then derive a model of sentence processing neutral between comprehension adproduction  . This model will be shown to fit severalinguistic phenomena  . Due to the generality of the perspective , the phenomena discussed below encompass apparently unrelated aspects of natural anguage  . 
2 Language and Constraint
From tile constraint-based perspective immediately follows a hypothesis tI'l at the same constraints  ( i . e . , lexical , syntactic , semantic , pragmatic , and whatever ) , corresponding to (1) , and the same processing architecture , corresponding to (2) , should underly both sentence comprehension ad production  . Other authors have expressed less radical stances  . For instance , Kay \[11\] adopts two different grammars for parsing and generation  . Our hypothesis also stronger than Shieber's \[16\]  ; Although he proposes to share not only one grammar but also one processing architecture between the two tasks  , this ' common ' architecture is , unlike ours , parameterized so as to adapt itself to parsing and generation i accordance with different parameter settings  . 
1 we reject every approach postulating any procedure specific to sentence comprehension or production  . For instance , we disagree upon the ways in which the De -terminism Hypothesis  ( DH ) \[12\] has been instantiated so far . DH permits to assume only one partial structure of a sentence at a time  , and the approaches along this line\[2 ,  3 ,  12 , 14\] has postulated , beyond necessity , specific ways of disambiguation for specific types of ambiguity in sentence comprehension and production  . 
Instead we view sentence processing as parallel computation  . When a sentence is either comprehended or produced  , several partial structures of it , we assume , are simultaneously hypothesized . The degree of parallelism should be limited to fall within the small capacity of the shortterm memory  ( STM )  , so that we obtain the same sort of predictions as we do along the determinist account  . For instance , the difficulty in comprehending garden path sentences like  ( 3 ) may be attributed to the difficulty of keeping some structural hypotheses in STM  . 
(3 ) The chocolate cakes are coated with tastes sweet . 
As discussed below , our approach quantitatively estimates the difficulty in processing embedded constructions like  ( 4 ) also on the basis of the memory limitation . 
(4 ) The cheese the rat the cat the dog chased caught bit was rotten  . 
Since DH does not account for such difficulty , inciden-tally , it seems superfluous to postulate DH . We consider DHjnst ~ as approximation of severe memory limitation  , and avoid any stipulation of such a hypothesis . 
3 A Common Process Model
Among the partial structures hypothesized during comprehension or production of a sentence  , we pay attention to the maximal st ~ ' uctures ; the structure such that there is no larger structures  . Here we say one structure is larger than another when the former in-eludes the latter  . For example , \[ s\[NPTom\]\[vp sleeps\]\] is larger than \[ s\[NPTom\]VP\]  . Sentence processing , whether comprehension or production , is regarded as parallel construction of several maxim M structures  . 
Thus sentence processing as & whole is characterized by specifying what a maximal structure is  . 
We assume the grammatical structure of a sentence to be a binary tree  . Here we identify a word with its grammatical category  , so that a local structure , such as \[ NP Tom \] , is regarded as one node rather than a partial tree consisting of two distinct nodes  . 
It is just for expository simplification that we assume binary trees  . Our account can be generalized straightforwardly to allow nary trees  . Further , the essence of our discussion below is neutral between the constituency-based approaches and the dependency-based approaches  . Here we employ a representation scheme of the former type  , without committing ourselves to the constituency -based framework  . 
From the general speculation below , it follows that a maximal structure should be the lefthand half of  ( 5 )  . 
(5) s
This maximal structure consists of the path form S to A and the part to the left of this path  , except for Bi-1 and the nodes between Bi-1 and Ai ( those on tiles lant dotted lines ) for 1 < i < d + l ; Ai and the nodes be-tween Ai and Bi are included in the maximal structure  . 
Here B0 and Ad+lst and for S and A , respectively . A i is a leftmost descendant ( not necessarily the left daughter ) of Bi_lor they are identical for 1_<i<d+l . 
Bi is a rightmost descendant ( not necessarily the right d&v . ghter ) of Ai for 1Gi < d . Thus our model is similar to left-corner parser \[1\]  , though our discussion is not restricted to parsing  . 
This characterization of a maximal structure is obtained as follows  . First note that a maximal structure involves n words and n-i nonterminal nodes  , for some natural number n ; In the maximal structure in (5) , the connected substructure containing Ai(l < ; i _ < d ) contains as many nonterminal nodes as words , so that the maximal structure also contains as many nonterminal nodes as words  , except for word A . Note further that the entire sentence structure , being a binary tree , also involves oneless nonterminal nodes than words  . 
Accordingly , postulating n- 1 nontermin M nodes versus n words in a maximal structure amounts to postu-lating that the words and the nonterminal nodes are processed at approximately constant speed relative to each other  .   1 The number of words is a measure of lexical information  , and the number of nonterminal nodes is a measure of syntactic and semantic information  , among others . Hence if all the types of linguistic information ( lexical , syntactic , semantic , etc . ) are processed at approximately the same relative speed  , then a maximal process should include nearly as many words as nonterminal nodes  . 
This premise is justified , because if different types of information were processed at different speeds  , then t The rate of n words versus n- 1 nonterminals does not precisely represent the constant relative speed  , but the discrepancy here is least possible and thus acceptable enough as approximation  . 
1 50   2 there would arise imbalance of information distribution across the corresponding different domains of information  . Such imhalance should invoke information flow from the domains with higher density to the domains with lower density of information distribution  , when , as in the case of language , those domains of information are tightly related with each other  . That is , information flow eliminate such imbalance , resulting in approximately the same speed of processing across different but closely related domains of information  . 
Now that we have worked out how many nodes a maximal structure includes  , what is left is which nodes it includes . Let us refer to A in ( 5 ) as the current active word and the path from the root node S to the current active word as the current active path  . It is natural to consider that a maximal structure includes the nodes to the left of the current active path  , because all the words they dominate have already been proce  , ; sed . Thus we come up with the above formulation of a maximal structure  , if we notice that the nodes on the solid-line part  ( including Ai ) of the current active path in ( 5 ) are adjacent on odes to the left of the current active path  , whereas the other nodes on the current active path  ( those on the dotted lines , including Bi ) do not except for the mother of A , which will be processed at the next moment . 
4 Immediate Processing
According to this model , any word should be in , me-diately processed , particularly in parsing , in the sense that corresponding amount of syntactic and semantic structure is tailored with little delay  . The intrasentential status of a word is hence identified as soon as it is encountered  . This contrasts with the determinist accounts which  , ' assume lookahead to deal with local ambiguity . 
Empirical evidences support our position . In Marslen-Wilson's\[13\] experiment , for instance , the subjects were asked to listen to a tape -recorded utterance and to say aloud what they hear with the shortest possible delay  . Some subjects performed this task with a lag of only about one syllable  , and yet their error reflected both syntactic and semantic on text  . For example , one of such a subject said lie had heard that the Brigade  .   .   . upon listening to He had heard at the Brigade .   .   .   . Such a phenomenon cannot be accounted for in terms of the determinist accounts with fixed parsing procedures  . In our model , it is explained by just assuming that only the most active maximal structure tailored by the subject survives the experimental situation  . 
5 ~. ~ ansient Memory Load
By transient memory load ( TML ) we refer to tile amount of linguistic information temporarily stored in STM  . The measurements of TML during sentence processing proposed so far include the depth of center embedding  ( CE ) \[5\] and that of self embedding ( SE )  \[15\] . A syntactic on stituent a is center o embedded in another syntactic constituent  /3 when /3  = -  rc~5 for some non-null strings 7 and ? We further say that c , is self-embedded in /3 when they are of the same sort of category , say NP . 
However , neither CE nor SE can explain why ( 6 ) is much easier to understand than ( 7 )  . 
(6 )   2bm knows the story that a man who lived in Helsinki and his wife we repoor but they were happy  . 
(7 ) To m knows that the story on the fact that the rumor that Mary killed John was false is funny  . 
Note that these sentences are of about the same length  ; The former consists of 20 words and the latter 19 words . Almost all my informants ( including both native and nonnative speakers of English  ) reported that ( 6 ) is easier to understand than ( 7 )  . Those who felt contrariwise ascribed the difficulty of  ( 6 ) to the ambiguity concerning the overall structure of the cornple-me I~t clause after that  . 
The approach based on CE fails to account for this difference  , because the maximum CE depth of (6) a: . d that of (7) are both 3, as is shown below . 
(8 )   \[0Tom knows the story that \[ laman \[2 who \[3lived \] in Helsinki \] and his wife were poor \] but they were happy \]  ( 9 )   \[0 To m knows that \[~ the story on the fact that \[2 the rumor that Mary\[akilled\]John \] was false\]isfunny\]The maximumSE depth cannot distinguish these sentences :  ( 10 ) Tomknows\[NPotile story that \[ NP ~ aman who lived in\[NP ~ Helsinki \] and his wife\] we repoor but they were happy \]  ( 11 ) To m knows that \[ NP0 the story on the fact that \[ NP , the rumor that \[ NP2 Mary \] killed John \] was false \] is funny . 
Our model provides a TML measure which accounts for the contrast in question  . In order to pluga maximal structure with the rest of the sentence in a grammatical manner  , one must remember only the information contained in the categories  o11 the border between the maximal structure and the remaining context  ; i . e . , categories Ai , the mother of Bi (1~i_<d ) and A in (5) . Thus the value of d in ( 5 ) could serve as a TML measure . As is illustrated in (12) and (13) , in fact , the maximum of dis2 and 3 for (6) and (7) , respectively , explaining why (6) is easier . In (12) and (13) , enclosed in boxes are the nodes corresponding to A  , , Bi(1 < i < d ) and A when d is ttle maximum ; i . e . , 2 in the former and 3 in tile latter . 

NP\[


VNPo knows NP So the story Co that S~but
NP t VP
Pwerepoor
NPS this wife a man Cornp Saw Lo
S they were happy lived PNP2
II in Helsinki (13)
NP t


V ~ go
Aw + ooip NP on N $1 the fact Compt l P
NP$2 was false the rumor Comp Sa

Mary NP
Lkilled John
NP the story 152   4   6 Language Acquisition The Dutch language xhibits a type of cross-serial dependency  ( CSD ) in subordinate clauses : ( 14 )   .   .   . datWolf de kinderen Marie . . . that Wolf the children Mariezaghelpenz wemmen see-PAST help-INFs wim-INF '  .   .   . that Wolf saw the children help Maries wim ' Our . theory predicts that children learning Dutch come to recognize the CSD constructions " as having the following structure  , which coincides with the structure figured out by Bresnan et al  \[4\] ~ based on an analysis of adult language . 
(15) S
NP0 VP
XoZo / / " . . . . . . . t / . / . . . . . . . .
NP ~'" ... Vo . . . . . .

Xm_~Z ,,_a/'-,,
NP , ,-1 NP , , V , ,:1 V , ttere Vo is a finite verb and V ; is an infinite verb for 1 < i < n . Vi is a causative verb or a perception verb for 1 < i < n . NP l is the subject of V i for 0 < i < n , and NP l is an object of V , for n < i <_m ( m>m ) . Note that NP ~, .   .   . NP ,   , and V0 , "' V ~ constitute right-branching structures dominated by  X0 and Zo , respectively . 
Let us look at how a child regard a simple CSD construction  ( 16 ) to be ( 17 )  , which is an instance of (15) for m = n = 1 . 
(16) , . . dat Wolf/vlarie zag zwemmen . . . that Wolf Marie see-PASTs wim-INF
L .. that Wolf saw Maries wim ' (17) s
NP oVP
W ! lf NPIZ o I /""-
Marie V 0 Va
II zagzwemmen
According to our model , the relevant part of the most active maximal structure would look like the following  2  ( 15 ) is slightly different from the structure proposed by Bresnan et al  , because we regard a sentence structure as a binary tree whereas their proposal involves tertiary branching obtained by equating VP and  X0 in ( 15 )  . This difference is irrelevant to the essence of the following disc  . ssion . 
when zagh as just been acknowledged , provided that the child has already acquired the standard structure of a subordinate clause  , in which the finite verb appears at the end . 
(18) S
NPoVPo
I 1
Wolf VP1





Izag
VP o , VP 1 , Zo and Vo correspond to B , ~- t , A a , Bd and A in (5) , respectively ( so that VPo and Zo are not included in the maximal structure here  )  . When z wem-men is encountered , category \[ v , z wemmen \] must be inserted either between VP o and VPI or between Zo and Vo  . In the alleged subordinate clause construer - tion  , Zo ( which might be identical to Vo ) has a direct access to \[ NPj Marie \] , which is the object of zag , the alleged head of Zo . On the other hand , VP1 lacks such an access , because the relationship between Marie and zagis established not through but under VP ~  . It is hence more preferable that \[ v ~ z wemmen\ ] attaches beneath Zo  , if the child has already perceived extralingulsti -eally the situation being described  , in which Marie iss whnming . Now the most active maximal structure should look like this  ( Zo and Z1 are excluded from this maximal structure if they are distinct from Yo and  V1  , respectively ): (19) zo

VoZtzag Va
Izwemmen ( 17 ) is ttms obtained by setting VP o = VP hZo = Yo , at tdZl = Vl . 
Note that this reasoning essentially relies oil our formulation of a maximal process  . If a bottom-up model were assumed instead , for instance , there would be no immediate reason to exclude a structure  , say , as follows . 
(2o ) S
NP oVP

Wolf UV 1
NP1 Vozwemmen
Mariezag 51 53
The above discussion can be extended to cover more complex cases  ( where m > 1 in ( 15 ) ) in a rather straightforward manner , as is discussed by Hasida\[6\] . 
The structure under Xo is tailored as a natural extension of the way an ordinary subordinate clause is processed  , then Vo is inserted beneath VP , following the ordinary structure of a subordinate clause together with the semantic information about the situation described  , and Vi attaches near to Vi-~for 1 < i < n due to the semantic information again . The structure under Z0 must be right-branching so that V0 be the head of VP . 
Also by reference to the current model , Hasida \[7\] further gives an account of the unacceptability of some unbounded dependency on structions in English which is hard to explain in static terms of linguistics  . 
7 Concluding Remarks
We have begun with a general constralnt-based perspective about the cognitive mechanism  , and shown that a model of sentence processing derived thereof  , neutral between comprehension and production , accounts for several linguistic phenomena seemingly unrelated to each other  . It has thus been demonstrated that the speculation to derive the model has empirical supports  , lending justification for the constraint p ~ radigm  . In particular , our theory has been shown to be more adequate than the determinist approach  , which must postulate a procedural design of the human language faculty  . 
A computational formalization of our model will be possible in terms of constraint programming  , as discussed by Hasida et al\[8 ,  9 ,  17\] . Most of the time , a natural anguage processing system in terms of procedural programming has been designed to be a series of a syntactic analysis procedure  , a semantic analysis procedure , a pragmatic analysis procedure , and so on , in order to reflect the modularity of the underlying constraints  . to wever , such a design imposes a strong limitation on information flow  , restricting the system's ability to a very narrow range of context  . One naturally attempts to remedy this so as to , say , enable the syntactic analysis module to refer to semantic information  , but this attempt must destroy the modularity of the entire design  , ending up with a program too complicated to extend or even maintain  . Constraint paradigm seems to be the only way out of this difficulty  . 
References\[1\]Aho , A . V . and Ullman , U . D . (1972) The Theory of Parsing , Translation and Compiling , Prentice-

\[2\] Berwick , R . C . and Weinberg , A .   ( 1984 ) The Grammatical Basis of Linguistic Performance , 
MIT Press.
\[3\] Berwick , R . (1985) The Acquisition of Syntactic
Knowledge , MIT Press.
\[4\] Bresnan , J . Kaplan , R . M . , Peters , S . and Zaenen , A . (1982) ' Cross-serlal Dependencies in Dutch , ' Linguistic Inquiry , Vol . 13, pp .  613-635 . 
\[5\] Church , K . W .   ( 1980 ) On Memory Limitations in Natural Language Processing  , MIT/LCS/TR-245 , Laboratory for Computer Science , Massachusetts
Institute of Technology.
\[6\]t Iasida , K .   ( 1985 ) Bounded Parallelism : A Theory of Linguistic Performance  , doctoral dissertation , 
University of Tokyo.
\[7\] Haslda , K .   ( 1988 ) ' A Cognitive Account of Unbounded Dependency , ' in Proceedings of COL-
ING'88, pp . 231-236.
\[8\] Hasida , K . (1989) A Constraint-Based View of Language , presented at the F~rst Conference on Situation Theory and its Applications  . 
\[9\]t taslda , K . and Ishlzaki , S .   ( 1987 ) ' Dependency Propagation : A Unified Theory of Sentence Comprehension and Generation  , ' Proceedings of IJ-
CAI'87, pp . 664-670.
\[10\] Kaplan , R . M .   ( 1972 ) ' Augmented Transition Networks as Psychological Models of Sentence Comprehension  , ' Artificial Intelligence , Vol . 3, pp .  77-100 . 
\[11\]Kay , M .   ( 1985 ) ' Parsing in Functional Unification Grammar , ' in Dowty , D . , Karttunen , L . 
and Zwicky , A . M . ( eds . ) Natural Language Parsing : Psychological , Computational , and Theoretical Perspectives , Cambridge University Press . 
\[12\] Marcus , M . P .   ( 1980 ) A Theory of Syntactic Recognition for Natural Language  , MIT Press . 
\[13\] Marslen-Wilson , W . D .   ( 1975 ) ' Sentence Perception as an Interactive Parallel Process  , 'Science , 
Vol . 189, pp . 226-228.
\[14\] McDonald , D .   ( 1980 ) Natural Language Production as a Process of Decision Making under Constraint  , Doctoral Dissertation , Laboratory of Computer Science , Massachusetts Institute of

\[15\] Miller , G . A . and Chomsky , N . (1963)' Finitary Models of Language Users , ' in Luee , R . D . , Bush , R . K . , and Galanter , E . tlandbook of Mathematical Psychology , Vol . lI , pp . 419-491, John Wiley and

\[16\]Shieber , S . M .   ( 1988 ) ' A Uniform Architecture for Parsing and Generation  , ' in Proceedings of COL-
ING'SS , pp . 614-619.
\[17\] Tuda , H . , Hasida , K . , and Sirai , H .   ( 1989 ) ' JPSG Parser on Constraint Logic Programming , ' Proceedings of the European Chapter of ACL'89 . 

