Word Order Acquisition from Corpora
Kiyotaka Uchimoto I , Masaki Muratat , Qing Ma*,
Satoshi Sekine *, and Hitoshi Isaharat
t Communications Research Laboratory
Ministry of Posts and Telecommunications
588-2, Iwaoka , I waoka-cho , Nishi-ku
Kobe , Hyogo , 651-2492, Japan
\[ uchimoto , murata , qma , isahara \]@ crl . go . jp
* New York University
715 Broadway , 7th floor
New York , NY 10003, USA
sekYne ~ cs , nyu.edu

In this paper we describe a method of acquiring word order fl ' om corpora  . Word order is defined as the order of modifiers , or the order of phrasal milts called ' bunsetsu ' which depend on the stone modifiee  . The method uses a model which automatically discovers what the tendency of the word order in Japanese is by using various kinds of information in and around the target bunsetsus  . This model shows us to what extent each piece of information contributes to deciding the word order mid which word order tends to be selected when several kinds of information conflict  . The contribution rate of each piece of information in deciding word order is ei Iiciently learned by a model within a maximum entropy framework  . The performance of this traiimd model can be ewfluated by checking how many instances of word order st-letted by the model agree with those in the original text  . In this paper , we show t , hateven a raw cor-pits that has not been tagged can be used to train the model  , if it is first analyzed by a parser . This is possible because the word order of the text in the corpus is correct  . 
1 Introduction
Although it is said tha ~ word order is free in Japanese  , linguistic research shows that the reart certain word order tendencies -- adverbs of time  , for example , tend to t ) recede subjects , mM bunsetsus in a sentence that are modified by a long modifier tend to precede other bunsetsus in the sentence  . Knowledge of these word order tendencies would be useful in analyzing and generating sentences  . 
I i1 this paper we define word order as the order of nrodifiers  , or the order of bunsets ns wl fich depend on the same modifiee  . The rearc several elements which contribute to deciding the word order  , and they are summarized by Saeki ( Saeki ,  1 . 998) as basic conditions that govern word order . When interpreting these conditions according to our definition  , weer a : summarize them , ~t bllows . 
Component lale on ditlons ? A bunsetsu having a deep dependency tends to precede a bunsetsu having a shallow dependency  . 
When there is a long distance between a modifier and its modifiee  , the modifier is defined as a bunsetsu having a deep dependency  . For example , the usual word order of modifiers in Japanese is tlm following : a bunsetsu which contains an interjection  , a bunset su which contains an adverb of time , a bunsetsu which contains a subject , and a bunset su which contains an object . 
Here , the bunsetsu containing an adverb of time is defined as a bunsetsu having deeper dependency than the one containing a subject  . We call the concept representing the distance between a modifier and its modifiee the depth of dependency  . 
A bunsetsu having wide dependency tends to precede a bunsetsu having narrow dependency  . 
A bunsetsu having wide dependency is defined as a bunsetsu which does not rigidly restrict its modifiee  . For example , the bunsetsu " ~ btqlo_c ( to Tokyo ) " often depends on a bunsetsu whicl l contains a verb of motion such as " ih u  ( go ) " while the bunsetsu " watashi_ . qa ( I ) " can depend on a bunsetsu which contains any kind of verb  . 
Here , the bunsetsu " watashi_ga ( I ) " is defined as a bunsetsu having wider dependency than  1  ; 11 ot mn sets u ': Tok ~ . / o_c ( to Tokyo ) . " We call the concept of how rigidly a modifier restricts its modifiee the width of dependency  . 
Syntactic conditions ? A bunsetsu modified by a long in odifier to n  ( Is to precede a bunsetsu modified by a short lnodifier  . 
Along modifier is a long clause , or a clause that contains many bunsetsus . 
? A bunsel , su containing a reference pronoun tends to precede other bunsetsus in the sentence  . 
? A bunsetsu containing a repetition word tends to precede other bunsetsus in the sentence  . 
A repetition word is a word referring to a word in a preceding sentence  . For example , Taromid Hanako in the following text are repetition words  . "Taro and Hanakolove each other . Taro is a civil servant and Hanako is a doctor . " ? A bunsetsu containing the case marker " wa " tends to precede other bunsetsus in the sentence  . 
Amnnber of studies have tried to discover the relationship between these conditions and word order in for estimating JaI  ) aneseword order based on a dictionary . They focused on the width of dependency ( Tokunaga and Tanal ~ a ,  1991) . Under their model , however , word order is restricted to the order of case elements of verbs  , and it is pointed out that the model can deal with only the obligatory case and it cmmot deal with contextual information  ( Saeki ,  1998) . An Ngram model fbr detecting word order has also been proposed by Maruyama  ( Maruyama ,  1994) , but under this model word order is defined as the order of morpheines in a sentence  . The problem setting of Maruyama's study thus differed fl'om ours  , and the conditions listed above were not taken into account in that study  . As for estimating word order in English , a statistical model has been proposed by Shaw and Hatzivassiloglou  ( Shaw and Hatzivassiloglou ,  1999) . Under their model , however , word order is restricted to the order of premodifiers or modifiers depending on nouns  , and the model does not simultaneously take into account many elements that contribute to determining word order  . It would be difficult to apply the model to estimating word order in Japanese when considering the many conditions as listed above  . 
In this paper , we propose a method for acquiring from corpora the relationship between the conditions itemized above and word order in Japanese  . The method uses a model which automatically discovers what the tendency of the word order in Japanese is by using various kinds of information in and around the target bunsetsus  . This model shows us to what extent each piece of information contributes to deciding the word order and which word order tends to be selected when several kinds of information conflict  . 
The contribution rate of each piece of information in deciding word order is efficiently learned by a model within a maximum entrot  ) y ( M . E . ) framework . The performance of the trained model can be evaluated according to how many instances of word order selected by the model agree with those in the original text  . Because the word order of the text in the corpus is correct  , the model can be trained using a raw co > pus instead of a tagged corpus  , if it is first analyzed by a parser . In this paper , we show experimental results demonstrating that this is indeed possible even when the parser is only  90% accurate . 
This work is a part of the corpus based text generation  . A whole sentence can be generated in the natural order by using the trained model  , given dependencies between bunsetsus . It could be helpful for several application such as refinement support and text generation in machine translation  . 
2 Word Order Acquisition and
Estimation 2.1 Word Order Model
This section describes a model which estimates the likelihood of the appropriate word order  . We call this model a word order model , and we implemented it within an M . E . framework . 
Given tokenization of a test corpus , the problem of word order estimation in Japanese can be reduced to the problem of assigning one of two tags to each relationship between two modifiers  . A relationship could be tagged with "1" to indicate that the order of the two modifiers is appropriate  , or with "0" to indicate that it is not . Ordering all modifiers so as to assign the tag "1" to all relations hit ) s indicates that all modifiers art in the appropriate word order  . The two tags form the space of " futures " in the M  . E . 
formulation of our estimation problem of word order between two modifiers  . The M . E . model , as well as other similar models allows the computation of P  ( flh ) for any f in the space of possible futures , F , and for every h in the space of possible histories  , H . 
A " history " in maximum entropy is all of the conditioning data that enable us to make a decision in the space of futures  . In the estimation problem of word order , we could reformulate this in terms of finding the probability of f associated with the relationship at indext in the test cortms as : P  ( flht ) = P ( flhfformation derivable from the test corpus related to relationshipt  ) The computation of P ( flh ) in any M . E . models is dependent on a set of " features " which should behdpful in making a prediction about the flml re  . Like most current M . E . models in computational linguistics , our model is restricted to features which are binary functions of the history and future  . For instance , one of our features is 1 . : if has(h , x ) = true , x = " M dfl ' l-Head-g(h , f ) = POS ( Major ) : verb " ( 1 ) & f = l0: otherwise . 
Here " has(h , x ) " is a binary flmction which returns true if the history h has feature z  . We focus on the attributes of a bunsetsu itself and on the features occurring between bunsetsus  . 
Given a set of features and some training data , the maximum entropy estimation process produces a model ill which every feature  . qi has associated with it a parameter ai . This allows us to compute the conditional probability as follows  ( Berger et al , 1996): ag ~( h . f )
P (/ I h)-1L'(2)
Z(h)cti . (3)
Yi
The maximum entropy estimation technique guarantees that for every feature gi  , the expected value of gi according to the M . E . model will equal the empirical expectation of gi in the training corpus  . In other words :
P ( h,/).M h,f ) h,f = (4) h/
Here /5 is an empirical probability and l ~ I e is the ? ) I"I~H ( yesterday ) /? = x ~ ( tennis )  / ~ ( f~l~l$ ( Taro ) / bLo ( l ) It ~ yed . ) "/ I ~ , , ' ~< ~, ~ xllail . -)' nxt , x1 . 7:- . x,~j~m:::0 . 6 x0 . 8 x 0 . 3 0 . 144  ) \[" NI ! Iii:k ( Taro )   ) I ~1!11 ( yesterday ) /? : LX ~ ( tennis ) / bt : ? ( played . ) " IP : mi ~ ~ HxI ~* H ,)-: x ,,< xt?r,l ~ . ? : : x , : := 0 . 4 x 0 . 8 ? 0 . 7 0 . 224 \[" ) kl ' ! l~l : ~ ( Taro ) /- Y : : x ' ~ ( tennis ) /' a ~ H ( yesterday ) / b ? : o ( plowed . ) " I/)~:,,~llxtg . = . x'~ . , ~ HxP , k~a*~L-)::?~=:0 . 4 x 0 . 2 x 0 . 7 0 . 05 ( i1"9:-':;?~(tennis)/II~H(yesl . erd ; ty)/>kfl\[~l~:(l'&l'O)/\[,~: . ?( played . ) "\[13* lll , 2 < I , u : l . xl :': . xt , jallxl !) . : . x . ~, . j < r~m::0 . 6 x0 . 2 x 0 . 3 0 . 036\["5 := x ~ ( tennis )  /  ) kf ~ lll:t ( Taro ) /I ~ I~F t ( yesterday ) / blz 0 ( played . ) " ~? Hxl ~; . : . xr , ~, ~ ZI?xl?y: . x~l ~, ~= : 0 . 4 x 0 . 2 x 0 . 3 0 . 024 prol ) ability assigned by the M . E . model . 
We detine a word order model as a model which learns the at  ) l ) ropriate order of each pair of n lodifiers which depend on the same modifiee  . ' l'h is model is derived from Eq .   ( 2 ) as follows ? Assmne that there are two bunsetsus 231 and 23~ which depend on the buuse tsuB and that It is the information derivable from the test corpus  . \]? lie probability that " B\]B2" is the at ) propriate order is given by the following equation :\] i ik :  :1 ffi ( l , b ) ( ~' i , i where . qi(1<i<k ) is afl , at m ' e and "1" indicates that the order is at ) propriate . The term scq , i and (~0 , i are estimated fl ' oln ae or l ) us which is nlor pho-logically and syntactically analyzed  . When there are three or more b\]msets us that det ) endont it ( ' . Stlne?moditiee , the probability is estimated as follows : I or ~' t bunsetsus  231  ,  232 ,   .   .   .   ,   23n which depend on the bmt sets uB and for the information hderiva Me from the test corpus  , the prot ) ability t ; hat " 23\] 23 ~ .   .   .  23 , " is the at ) propriate order , or P ( llh ) , is represented as s the probability that every two bunsetsus " Bi ~ i-Fj  ( 1_<i<n-1 , 1 < j<'n-i ) " are the appropri ~ ate . order , or P (14~i , i_l . j = l\[l<i < . n--1 , l:<j<n--ilh ) , ilere " I4Li+j--l " represents that " . l ~ i23 i-Fj " is the appropriate order . Let us assume that every 14 Q , ~:+ j is independent each other . Then 1) (1 I h , ) is derived as follows : 2'( lib ) = \]1\] . i , .  - \ ] , 1 <  . i _ < ,   ,   , -ilh)n--n-ii--Ij--1=ll1Ij ) , i = lj = l where\[Si , i+j is the information derivable when fb-c using on the bunsetsu  13 m~dits modifiers 13i and

For example , in the sentence " I ~ U(kinou , yesterday)/:kfll  ~( Taro_wa , Taro)/-P ~ x~(tcnnis_wo , tennis)/bt:o(sita . , l ) layed . ) , " where a "/" represents a bunsetsu boundary , there are three bunsetsus that depend on the verb " b ~:  ( sita )  . " We train a word order in odel under the assmnl ) tion that the orders of three t ) airs of modifiers -" I~lU " and "~ f$1 . ~ , "" Net \]" and "?7-:7 ,  ~  , " and ": kl~l*l ~" and "5:m:7 ,  ~"  . . . . are al)ttropriate . We use various ldnds of intormation in and around the target bunsetsus as features  . For example , the information or the feature that a noun of timei  ) recedes at ) rot ) ernoun is derivable fl ' om the order " IPH ( yesterday ) /Y ; fll~I~(Taro)/b1=o(pl~\yed . ) , " and the feature that a case followed by a case marker " w ?' precedes a case followed by a caqe marker " wo " is derivable from the order ": ~ fll ~ It  . ( Taro_wa , Taro)/?~7 . ?k(tennis_wo , tennis)/b2C_o(sita . , t ) layed . ) . " 2 . 2 Word Order Es t imat ion This section describes the algorithm of estimating the word order by using a trained word order model  . 
The word order estimation is defined as deciding the order of n to ditiers or bunsetsus which depend on the same modifiee  . The input of this task consists of modifiers and informat  , ionnecessary to know whether or not features are found  . The output is the order of the inodifiers . We assume that lexical selection in each bunsetsu is already done and all del  ) endencies in a sentence are found . The information necessary to know whether or not features are found is morphological  , syntactic , semmltic , and (' on-textual information , and the locations of bunsetsu bonndaries . The features used in our ext ) eriments are described in Section 3 . 
Word order is estimated in the following steps.
Procedures 1 . All possible orders of modifiers are found . 
2 . For each , the probability that it is apt ) ropriate is estimated by a word order model , or Eq .  (6) . 
3 . The order with the highest probability of 1 ) eing approl ) riate is selected . 
l ) brexample , given the sentence " 1 ~ U(kinou , yesterday)/:kfilIl~(Taro_wa , Taro)/?::x~(tcn-nis_wo , temfis)/bt:o(sita . , played . ) , " timmodi-tiers of a verb " b ? :_ ( played ) " are three tmnsetsus , " l ~ U ( yesterday) , " : k ~ / i ~ ( Taro) , ""? = x~(tennis) . " Their apt ) ropriate order is estimated in the following steps . 
1 . The probabilities that the orders of the three pairs of modifiers " N-LI " and ": is : BIIl ~   , "" I~U " and "?~:7 , ~ , " and "~ fll Il ?" and "? c . x ~" are appropriate are estimated . Assume , for example , ~- H , ; k~l~ta , PrI~It , ? :-~ . ~, and P ; kfzlIla,~ca ~ are respectively 0 . 6, 0 . 8, and 0 . 7 . 
2 . As shown in Table 1 , probabilities are estimated for all six possible orders  . The order " I~U/:k fill IS/-7~-y . ~/ b \] Co , " which has the highest probability , is selected as the most apt ) ropriate order . 
2.3 Performance Evaluation
The pcr form an cc of a word order model can be evaluated in the following way  . First , extract from a test corpus bunsetsus having two or more modifiers  . 
Then , using those 1 ) unsetsus and their modifiers as ~ J tlnset sll lJ tlnset stlnHI YiberOf babelnllll ber modifier  0   1 P 1   5   2   3   3   4   4   5 P
Modifiers ( Bunsetsu number)
Strings in a bunsetsu > kflli ~ ( Taro_to , Taro and ) ~ Y'a(l Ianako_to , llanako)-Y-=x q~(tennis_no , tennis ) ~ lc . ( sial_hi , tournament ) lll ' C , ( dete , , participate , ) ~@ bt = , ( yusyo_sita . , won . ) Moditiers whose modiliee is the bunsetsu in the left column  . 
~ , ~ a ( 0 ) ? = x0 ~ ( 2 ) ~ a ( 0 ) ~- r-~a0 )  '~ :  ( a ) ~ e ( o )  ~?-~*  ( 1 ) If ' ~ ( 4 ) input , estimate the orders of the modifiers as described in Section  2  . 2 . The percentage of the modi-flees whose modifiers ' word order agrees with that in the original text then gives what we call the agreement rate  . It is a measure of how close the word order estimated by the model is to the actual word order in the training corpus  . 
We use the following two measurements to calculate the agreement rate  . 
Pair of modifiers The first measurement is the percentage of the pairs of modifiers whose word order agrees with that in the test corpus  . For exmnple , given the sentence in a test corpus " N kl ( kinou , yesterday)/~tIla(Taro_wa , Taro)/-7-=2`~2 ( tennis_wo , tennis)/t . ~: o(sita . , played . ) , " if the word order estimated by the model is " ~ H  ( yesterday )  / -7" -- 2 . ~( tennis)/~1~~:~( Taro)/b too ( played . ) , " then the orders of the pairs of modifiers in the original sentence are " NH /  ; k  ~ l  ~ , ""15H/-7-=-:7 ,  ~  , " and "~ lit:~/~--2` ~ , " and those in the estimated word order are " ~ H /   -~---2`~  , "" ~ H/1~1 ~ la ~ , " and " Y-=2`~/%:t~lllak . " The agreement rate is 67% ( 2/3 ) because two of the three orders are the same as those in the original sentence  . 
Complete agreement The second measurement is the percentage of the modifiees whose modifiers ' word order agrees with that in the test corpus  . 
3 Experiments and Discussion
In our experiment , we used the Kyoto University text corpus ( Version 2 )   ( Kurohashimid Nagao ,  1997) , a tagged corpus of the Mainichi newspaper . For training , we used 17 , 5 62 sentences from newspaper articles appearing in 1995  , from January 1st to Jmmary 8th and from Jmmary 10th to June 9th   . For testing , we used 2 , 3 94 sentences fl ' om articles appearing on January 9th and from June 10th to June 30th  . 
3 . 1 Def in i t ion of Word Order in a Corpus In the Kyoto University corpus  , each bunsetsu has only one modifiee . When a bunsetsuBm depends on a bunsetsuBd and there is a bunsetsu  /3p that depends on and is coordinate with \] ) d , Bp has not only the information that its modifiee is\]~d but also a label indicating a coordination or the information that it is coordinate with B d  . This information indirectly shows that the bunsetsu Bm can depend on both  \]3p and Bd . In this case , we consider Bm a modifier of both Bv and Bd . 
Under this condition , modifiers of a bunsetsu B are identified in the following steps  . 
1 . Bunsetsus that depend on a bunsetsu Bareclas -sifted as modifiers of B  . 
2 . When B has a label indicating a coordination , bunsetsus that are totile left of 13 and depend on the same modifiee as B are classified as modifiers of B  . 
3 . Bunsetsus that depend on a modifier of B and have a label indicating a coordination are clas-sifted as modifiers of B  . The third step is repeated . 
When the above procedure is completed , all bunsetsus that coordinate with each other are identified as modifiers which depend  oi1 the same nmdifiee . For example , from the data listed on the left side of To-ble2 , the modifiers listed in the righthand column are identified for each bunsetsu  . " Nt ~ I ; ~( Taro_to , Taro and ) , ""? ~ g-~IS ( Hanako_to , Hanako ) , "" ql "( , ( dete , , participate , )" are all identified as modifiers which depend on the same modifiee " ~ b  7=?   ( yusyo_sita . , won . ) . " 3 . 2 Exper imenta l Resu l ts The features used in our experiment are listed in Tables  3 and 4  . Each feature consists of a type and a value . The features consist basically of some attributes of the bunsetsu itself  , and syntactic and contextual information . We call the features listed in Tables 3 ' basic features . ' We selected them manually so that they reflect the basic conditions governing word order that were sunmmrized by Saeki  ( Saeki ,  1998) . The features in Table 4 are combinations of basic features ( ' combined features ' ) and were also selected manually . They are represented by then mne of the target bunsetsu plus the feature type of the basic features  . The total number of features was about 190 , 000 , and 51 , 5 90 of them were observed in the training cortms three or more times  . These were the ones we used in our experiment . 
The following terms are used in these tables : Mdf rl  , Mdf r2 , Mdfe : The word order model described in Section 2  . 1 estimates the probability that modifiers are in the appropriate order as the product of the probabilities of all pairs of modifiers  . When estimating the probability t breach pair of modifiers  , the model assmnes that the two modifiers are in the appropriate order  . 
Here we call the left modifier Mdfrl , the right modifier Mdf r2 , and their modifiee Md f e . 
Head : the rightmost word in a bunsetsu other than those whose major pro't-of-speech I category is  1Part-of-speech categories follow those of JUMAN ( Kurohashi and Nagao ,  1998) . 

Table 3: Basic features.
Basic features
Feature values ( Number of type ) Feature type tegors \] Target1 ) tm sets us 1Md frl , Mdf r2 , 
Mdfe2 Mdfrl , Mdfr2,
Mdfe3Mdfrl , Mdfr2,
Mdfe4 Mdfrl , Mdh'2,
Mdfemm\[\]ead-POS ( Major ) \[ tead-POS ( Minor ) \[ lead-hff ( Major ) \ [ I ead-Inf ( Minor ) \ [ I ead-SemFeat ( 110 ) \[1ead-SemFeat ( 111  ) \ [ I ead-SemFeat ( 433 ) 5M df rl , Mdff2 , rype(String )
Mdferype(Major ) type ( Minor ) 6 Mdfrl , Mdf r2 , lOSIlll(String )
Mdfel OSIIIl ( Minor ) lOSII 12 ( String ) lOSUI2 ( Minor ) 7M df rl , Mdf r2 , Period
Mdfe8Mdfrl,Mdf r2Num l)erOfMdfrs
MdfeNumberOfMdfrs9Mdfrl,Mdfr2,
Mdfe10 Mdfl'l , Mdfr2
Coordination ( Total : 90)
Mdfrl-Mdfr Type-ll ) to-Md fr2-Typc
Mdfl'2-MdfrTypeql ) to-Mdfl-I-q'ype
Mdfi'l-Mdfr Type-lD to-Md fr2-MdfrType   11 Mdfrl , Mdf r2 , Rel ) etition-l lead-l , ex
MdfeRepetition-Mdfr-lleadq , ex12 Mdfrl , Mdfr2 Reference Pronouni ~ eferenee Pronou ( String )  ' ,  . %066) ~ u(verb) , s ~ u ( adjective ) ,  ~ , ' ~( noun ) .   .   .   . (11) ~' ~: ~( commonoun ), m '~( quantifier) .   .   .   . (24) ~1 ~( vowel verb) .   .   .   .  (30) ' . ' ~( stem ), t ~* ~( f and a mental form ) .   .   .   . (60) rrue (1) true (1) true (1): ~ , : a , <- u < , t : u ,  ~ ,  ~= , t .   .   .   . (7:3) uJ:~(post positional particle), .   .   .  (43) ? , ~ JJ'~(e~use marker) , ~*$ ( imperative form ) .   .   . (102) ' ~', 5, ~<', a ~ , , . , l,~ .   .   .   .   . (63)) ill \], ~$ m(ease marker), .   .   .  (5) ~, ~, *, ~,*,,  . . . ((~3) ~, ~; ~1 ( ease marker) .   .   .   . (41\[nil\],\[exist\](2)
A(0), B(1), C(2), 1) (3 or more ) (4)
A(2), B(:3), C(4 or more ) (3)
P ( Coordim ~ te) , A(Apposition ) , I ) ( otherwise ) (3)\]' rue , False (2) rrue , False (2) true , False (2) 86 . 65% 73 . 87% \[--0 . 79%) (--1 . 54%) 87 . 07% 75 . 03% ', -- O . 37%) (--0 . 38%) 87 . 39% 75 . 20% ',-0 . 05%) (-0 . m %) 87 . 21% 75 . 20% \[--0 . 23%) (--0 . 21%) 84 . 78% 70 . 03% (-2 . 66%) (-5 . 38%) 87 . 32% 75 . 14% (-0 . 12% (--0 . 27%) 87 . 39% 7 ~ (-0 . 05%) (+0 . 13%) 87 . 14% 74 . 86% (-0 . 30%) (-0 . 55%) 87 . 40% 70 . 30? . / o(-0 . 04%) (-0 . 0~%) 8~ . 2~% 73 . 61% (--1 . 18% (--1 . 80%) 87 . 34% 75 . 09% (--0 . 10%) (--0 . '32%)\[nil\],\[exist\](2) 87 . 31% 75 . id %\[ nil\],\[exist\](2)(--0 . 13%) (--0 . 27%)\[nil\],\[exist\](2) 87 . 27% 75 . 12%: ~, : a , : ~ . ~,~= . ~, , ~ .   .   .   .   .  (42) (--0 . 17%) (--0 . 29%)'%@ ( special marks ) , ""1 12 N(1) ost-posi ~ ioual particles) , " or " ~ N ~? ( suffixes) . " HeadLex : the fllndalnental forth ( unintlected for ln ) of the head word . Only words with a frequency of tlve or more are used  . 
Head-Inf : the inflection type of a head.
SemFeat : We use the upper third layers of bunrui . qoih you ( NLl/I ( National Language Research Institute )  , 19641 as semantic features . Bunruigoi-hyouisa Japanese thesaurus that has a tree structure and consists of seven layers  . The tree has words in its leaves , and each word has a figure indicating its category number  . For example , the figure in parenthesis of a feature " Head - SemFeat  ( ll0 ) " in Table 3 shows the upper three digits of the category number of the head word or the ancestor node of the head word in the third layer in the tree  . 
Type : the rightmost word other than those whose major part-of-speech category is "~@  ( special marks )  . " If the major category of the word is neither " NJN  ( postpositional particles ) "nor " ~/ ~' ~ ( suffixes )  , " and the word is inflectable ,   2 then the type is represented by the inflection type  . 
JOSHI1 ,   JOSHI2  :  JOSHI1 is the rightmost positional particle in the bunsetsu  . And if there are two or more postpositional particles in the bunsetsu  ,   JOSHI2 is the second-rightmost post-positiol ml particle . 
NmnberOfMdfrs : number of modifiers.
2 The inflection types follow those of JUMAN.
Mdfrl-Mdfr Type , M df r2 -MdfrType : Types of tile modifiers of Mdfi'l and  Mdfr2  . 
X-ID to-Y : X is identical to Y .
Repetition-HeadLex : are t ) etition word all pear-ingilla precedings enteuce . 
Reference Pronourl : a reference pronoun appearing in the target bunsetsu or illits modifiers  . 
Categories 1 to 6 ill Table 3 reI ) resent attributes in a bunsetsu , categories 7 to 10 represent syntactic information , and categories 11 and 12 represent contextual information . 
The results of our experiment are listed in Table 5  . 
The first lines how stlle agreement rate when we estimated word order for  5  , 2 78 bunsetsus that have two or more modifiers and were extracted from  2  , 394 sentences al ) pearing on Jmmary 9th and from June 10th to June 301tl   . \ Veused bunsetsu boundary information and syntactic and contextual information which were derivable froln the test corpus and related to the input bunsetsus  . As syntactic ilff Orl nation we used dependency inforlnation  , coordinate structure , and information on whether the target bunsetsu is at thee M of a sentence  . As contextual information we used the preceding sentence  . The values in the row labeled Baseline1 in Table 5 are the agreement rates obtained when every order of all pairs of modifiers was selected randolnly  . And values in the B&seline2 row are the agreement rates obtained when we used the following equation instead of Eq  .   ( 5 ) : freq ( w12 ) PMu ' ( llh ) = freq ( w12 ) + frcq ( w21 ) "  ( 7 ) 
Accuracy without the feature
Pair of Complete modifiers : tgreement 87 . 23% 74 . 65% (-0 . 21%) (-0 . 76%)
Combined features -- Twint batures ( Md fr1-Type , Mdf r2-Type) , ( Mdfr1-Type , Mdfe-IIead-Lex ) , ( Mdfr1-Type , Mdfe-t tead-POS ) , ( Mdfr1-Type , Mdf r1-Coordlnrttion ) , ( Mdfr1-Type , Mdfr2-MdfrType-ID to-Mdf r1-2"ypc) , ( MdfrE-Type , Mdfe-Head-Lex ) , ( Mdfrg-Type , Mdfe-Head-POS ) , ( Mdfr2-Type , Mdf r2-Oo ordination ) , ( Mdfr2-Type , Mdf r1-Md fr Type-lD to-Md fr2-Type) , 
Mdfr1-Head-Lex , Mdfe-Period),
Mdf r1-nead-POS , Mdfe-Per lod),
Mdf r1-1 tead-POS , Mdfr1-Repetltlon-Head-I , ex ) , 
Mdfr2-Itead-Lex , Mdfe-Petlod),
Mdfr2-Ite,~d-POS , Mdfe-Per lod),
Mdf r2-II ead-POS , Mdf r2-I tepetlt lon-I lead-Lex ) ' t ~ iplett catures 87 . 22% i 74 . 86% Mdfrl-Wype , Mdf r2-Type , Mdfe-llead-Lex ) ,  (--0 . 220 . /0) (--0 . 55%) Mdfrl-Type , Mdf r2-Type , Mdfe-Head-POS ) , Mdfrl-Type , Mdfrl-Coordinatlon , Mdfe-Type ) , MdfrE-Type , Mdfr2-Coordlnatlon , Mdfe-Type ) , Mdfrl-JOSHI 1 , Mdfrl-JOSHI2 , Mdfe-IIead-Lex ) , Mdfrl-a OS Hll , Mdfrl-JOSHI2 , Mdfe-IIead-POS ) , Mdf r2-JOSHI1 , Mdf r2-JOSIII2 , Mdfe-Head-Lex ) , Mdf r2-aOSH\[1 , Mdf r2-JOS l~12 , Mdfo-llead-POS ) All of above combined features 85 . 79% 77 . 67% ~--1 . 65%) (--3 . 74%)
Table 5: Results of agreement rates.
Agreement rate
Pair of modifiers Coml ) lete agreement
Our method 87 . 44%(72,367/14,137) 75 . 41% (3,980/5,278) Baseline 148 . 96% (6,921/14,137) 35 . 10% (7,747/5,278) Baseline 249 . 20% (6,956/14,137) 53 . 84% (1 , 786/5 , 278 ) IIere we assume that B1 and \]32 are modifiers , their modifiee is B , the word types of B1 and \]32 are respectively Wl and we . The values frcq(wr2) and frcq(w . 27  ) then respectively represent the fl'equencies with which  w7 and w , 2 appeared in the order " WT , we , midw " and " w2 , WT , and w " in Malnichi newspaper articles fl'om 1991 to 1997  . a Equation ( 7 ) means that given the sentence "~ t~ltI:t ( Taro_wa )  / ~- -- ~ , ( tennis_wo)/b~-:o(sita . ) , " one of two possibilities , "1$( wa)/~(wo)/t , ~: o(sita . )" and " #c(wo)/tS(wa)/b ~ : o(sita . ), " which has the higher frequency , is selected . 
3.3 Features and Agreement Rate
This section describes how much each feature set contributes to improving the agreement rate  . 
The values listed in the rightmost columns in Tables  3 and 4 shows the performance of the word order estimation without each feature set  . The values in parentheses are the percentage of improvement or degradation to the formal experiment  . In the experiments , when a basic feature was deleted , the combined features that included the basic feature were also deleted  . The most useful feature is the type of 3When wl and w2 were the same word , we used the head words in Bt and 132 as Wl and w2 . When one of freq ( wt2 ) and freq ( w21 ) was zero and the other was five or more , we used the flequencies when they appeared in the order " Wlws " and "  w2 wt , " respectively ~ instead of frcq(wi2) al , dfreq(wsl ) . 
When both freq(wl . 2) and freq(w27) were zero , we instead used random figures between 0 and t . 
bunsetsu , which basically signifies the case marker or inflection type  . This result is close to our expectations . 
We selected features that , according to linguistic studies , as mudl as possible reflect the basic conditions governing word order  . The rightmost column in Tables 3 and 4 shows the extent o which each condition contributes to improving the agreement rate  . 
However , each category of features might be rougher than that which is linguistically interesting  . For example , all case marker such as " wa " and " wo " were classified into the same category  , and were deleted together in the experiment when single categories were removed  . An experiment that considers each of these marker separately would help us verify the importance of these markers separately  . If we find new features in future linguistic research on word order  , the experiments lacking each feature separately would help us verify their importance in the same manner  . 
3 . 4 Tra in ing Corpus and Agreement Rate The agreement rates for the training corpus and the test corpus are shown in Figure  1 as a function of the amount of training data ( nt unber of sentences )  . 
The agreement rates in the " pair of modifiers " and  '?!19~   .   .   .   .   . % : I : :' i , ~ .   .   .   .   .   .   .   .   . ~': z~-,~',::::i=?I9090!'
S5~5 i ~-
E75~757 O7O 65.......(,50....
0 2000 400 ) 600080 O0 Io ( mO 1200 14O00 16000 18000 2000 40006 , 300 ~000 IO DO 012 ( ; 0 0   14000   16O00   la001  \]  11o Ntll Oboro ~ Snnlon cos \] o 111o Traifl in9 Datallm Numberol Sentences in llmTf 4dlli11U  \[ ) ala Figure 1: Relationship between tile amount of training data and the agreement rate  . 
" Complete agreement " measurements were respec-tiw ~ ly  82  . 54% and 68 . 40% . These values were obtained with very small training sets  ( 250 sentences )  . 
These rates m'e considerably higher than those of the baselines  , indicating that word order in Japanese can be acquired fl ' om newspaper articles even with a small training set  . 
With 17 , 562 training sentences , the agreemenl , rate in the " Complete agreement " measurement was  75  . 41% . We randomly selected and analyzed 100 modifiees from 1  , 2 98 modifiees whose modifiers ' word order did not agree with those in the original text  . 
We found that 48 of them were in a natural order and 52 of them were in an unnatural order . The former result shows that the word order was relatively fl'ee and several orders were acceptable  . The latter result shows that the word order acquisition was not sufficient  . To complete the acquisition we need more training corpora and features which take into account different information than that m Tables  3 mid 4  . We found many idiomatic expres-ffl\[~il ~:5~ ( houchi-kokka_ga , country under the rule of law ) /\[ ll ~ ( ki it c , to listen ) / ~ #t ~ ( alcireru , to disgust ) , ~ rj~b ? = a ~ ( souan-s ~ , ta-no_ga , or lgl- , ration)/~*o ~-* oco(somosomo-no , at all )/~ t ~ U(hg~'ima ~'4 the beginning ) , " and "" ~ l ~( g#-~4 taste)/~'~B ( seik on , one's heart and soul)/g ~6 ( hom cru , to trot some thil , g into so inething ) . " We think that the apt ) ropriate word order for these idiomatic expressions could be acquired if we had more training data  . We also found several coordinate structures in the Ulnlatural word order results  , suggesting that we should survey linguistic studies on coordinate structures and try to find efllcient features for acquiring word order from coordinate structures  . 
We ( lid not use the results of semantic and contextual analyses as input because corpora with semantic and contextuM tags were not available  . If such corpora were available , we could more et\[iciently use features dealing with seinantic features  , reference pronouns , and repetition words . We plan to make corpora with semantic and contextual tags and use these tags as input  . 
3.5 Acquisition from a Raw Corpus
In this section , we show that a raw cortms instead of a tagged corpus can be used to train the l node l  , if it is first analyzed by a parser . We used the lnorl ) holog-ical analyzer JUMAN and atmrser KNP ( Kurohashi , 11198) which is based on a det ) endency grain lnar , it , order to extractiu forumtion from a raw corpus for detecting whether or not each feature is found  . 
' l?tm accuracy of JUMAN for detecting in or phologi -cal boundaries and part-of-speech tags is about  98%  , and the parsecs dependency accuracy is about 90% . 
These results were obtained from analyzing Mainichi newspaper articles  . 
We used 217,562 sentences for training . When these sel~t , ences were all extracted from a raw corlms , the agreement rate was 87 . 64% for " pair of modifiers " and was 75 . 77% for " Colnplete agreement . " When the 217 , 5 62 training sentences were sentences fl ' oln the tagged cortms  ( 17 , 562 sentences ) used in our for-real exl ) eriment a Inlfroln a raw cortms , the agree-"eS:~mentrate for " pair of l no ( hfi . r , was 87 . 66% and for " Complete agreement " was 75 . 88% . These rates were about 0 . 5% higher than those obtained when we used only sentences from a tagged corlms  . Thus , we can acquire word order by adding inforlnation frolnarm v corpus even if we do not have a large tagged corpus  . The results also indicate that the parser accuracy is not so significant for word order acquisition and that an accuracy of about  90% is sufficient . 
4 Conclusion
This paper described a method of acquiring word order froln corpora  . We defined word order as the order of l nodifiers which depend on tile same l nodifiee  . The lnethod uses a model which estimates the likelihood of the apt  ) ropriate word order . The l node l automatically discovers what the tendency of the word order in Japanese is by nsing various ldnds of information in and arouud the target bunsetsus plus syntactic and contextual inforlnation  . The contribution rate of each piece of inforination in deciding word order is efficiently learned by a model implemented within an  )  , , I . E . framework . Comparing results of experiments controlling for each piece of information  , we found that the type of inforinatiou having the great ~ est influence was the case marker or inflection type in a bunsetsu  . Analyzing the relationship between the amount of training data and the agreement rate  , we fimnd that word order could be acquired even with a small set of training data  . We also folm d that a raw cortms as well as a tagged cortms can be used to train the model  , if it is first , analyzed by a parser . The agreement rate was 75 . 41% for the Kyoto University corpus . We analyzed the lnodifiees whose modifiers ' word order did not agree with that in the original text  , and folm d that 48% of the lnwere in a natural order . This shows that , in umny cases , word order in Japanese is relatively free and several orders are acceptable  . 
The text we used we relmws paper articles , which tend to have a standard word order , but we think that word orders tend to differ between ditferent styles of writing  . We would therefore like to carry out experiments with other types of texts  , such as novels , having styles different froln that of newspapers . 
it has been ( lift \] culto evaluate tile reslflts of text generation objectively becmlse there have been no good stmldards for ewllmtion  . By using the stan- ( lard we describe in this paper , however , we canevN-uate results objectively , at least for word order estimation in text , generation . 
We expect hatour l node l can be used for several applications as well as linguistic veritication  , such as text ; refinement silt ) port and text generation in nla-chine translation . 

Adam L . Better , Stephen A . I ) ella Pietra , and Vincent J . Della Pietra . 199 ( LA Maximum t'\]ntropy Approach to N ~ ttural , ~tn-gmtge Processing . Computational Linguistics , 22(11:39-71 . 
Sadao Kurohashi and Makol . o Nagao .  1997 . Kyoto University Text Corl ) uS Project . In Proceedings of The Third Annual Mectin9 of The Association for Natural Language Processing  , pages 115-118 . ( in Japanese) . 
Sadao Kurohashi and Makoto Nagao , 1998 . Japanese Morphological Analysis System JUMAN Version  3  . 6 . l ) epartment of
Informatics , Kyoto University.
Sadao Kurohashi , 11198 . Japanese Dependency/Case Structure Analyzer KNP Version  2  . 0b6 . Department of Inforln~tties , Ky-ore University . 
IIiroshl Maruyama .  1994 . Experhnents on V ~ rord-Order Recovery Using N-Cram Models  . In YTte \] ~9th Annual ( ; onvention IPS
Japan . ( in Japanese).
NLRl ( National Language Research Institute) .  1964 . Word List by Semantic Pri + ciples . Syuei Syuppan . ( in Japmlese) . 
Tetsuo Saekl .  1998 . Yousets unih ong on o9oj un ( Survey : Word Order in Japanese )  . Kuroshio Syupl ) an . ( in Japanese) . 
James Shaw and Vasileios Ilatzivassiloglou .  1999 . Ordering Among l're modifiers . In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics  (  , 4CL ) , pages 135-143 . 
Takenobu Tokunaga and I Iozumi Tanaka .  1991 . On Estimating Japanese Word Order B~used on Valency Information  . 
Keiryo Kokugogaku ( Mathematical Linguistics) ,  18(21:53-(;5 . 
( in Japanese).

