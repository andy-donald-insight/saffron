LINGUISTIC PROCESSINGUSING ADEPENDENCYSTRUCTURE GRAMMAR 
FORSPEECHRECOGNITION AND UNDERST ANDING
Sho-lchl MAT SUN AGA
NTT Human Interface Laboratories
Musashino , To ; ? yo , 180, Japan

Masaki KOHDA
University of Yamagata
Yonezawa , Yamagata , 922, Japan

This paper proposes an efficient linguistic processing strategy for speech recognition and understanding using a dependency structure grammar  . 
The strategy includes parsing and phrase prediction algorithms  . After speech processing and phrase recognition based on phoneme recognition  , the parser extracts the sentence with the best likelihood taking account of the phonetic l ikelihood of phrase candidates and the linguistic likelihood of the semantic inter-phrase dependency relationships  . A fast parsing algorithm using breadth-first search is also proposed  . The predictor preselects the p ~ . ase candidates using transition rules combined with a dependency structure to reduce the amount of phonetic processing  . The proposed linguistic processor has been tested through speech recognition experiments  . 
The experimental results show that it greatly increases the accuracy of speech recognitions  , and the breadth-first parsing algorithm and predictor increase processing speed  . 
1. Introduction
In conventional continuous speech recognition and understanding  systems\[1~4\]  , linguistic rules for sentences composed of phrase sequences are usually expressed by a phrase structure grammar such as a transition network or contextfree grammar  . In such methods , however , phoneme recognition errors and rejections result in incorrect transition states because of the strong syntactical constraints  . 
These erroneous transitions cause the following candidates to be incorrectly chosen or the processing system to halt  . Therefore , these errors and rejections can be fatal to speech understanding  . 
Furthermore , a complete set of these grammatical rules for speech understanding is very difficult to provide  . 
To address these problems , this paper proposes a new linguistic processor based on a dependency structure grammar  , which integrates a bottom ~ up sentence parser and a topdown phrase predictor  . ~lis grammar is more semantic and less syntactic than phrase structure grammar  , and , therefore , syntactic positional constraint in a sentence rarely occurs with this parser  . This effectively prevents extreme degradation in speech recognition from errors and rejections in phoneme recognition and greatly increases the accuracy of speech processing  . This grammar only has two syntactic rules , so this parser is free of many cumber some grammatical rules that are indispensable to other grammars  . This grammar particularly suits phrase-order-free languages such as Japanese  . 
4 a ~ rule I rule 2
For the parser of this grammar , a depth-first parsing algorithm with backtracking which guarantees the optimal solution was  devised\[5\]  . However , parsing long sentences composed of many phrases with this algorithm can be time-consuming because of combinatorial explosion  , since the amount of computation is exponential order with respect to the number of phrases  . Therefore , a fast parsing algorithm using breadth-first search and beam search was developed  . This algorithm is based on flm damental algorithms\[6  , 7\] which only take account of the dependency relationships of the modifier and modificant phrases  , and it handles higher linguistic or semantic processing such as case structure  . The processing ability of this breadth-first algorithm is equivalent to that of the depth -first algorithm  . 
To effectively recognize speech , the amount of phonetic processing must be reduced through topdown prediction of hypotheses  . However , topdown centre \] using the principal dependency structure is impossible  . To solve this problem , this novel phrase predictor was devised . This predictor preselects hypotheses for phoneme recognition using prediction rules  , and then it reduces the amount of phonetic processing  . Prediction rules are created by integrating connection rules and phrase dependency structures  . 
The effeetiveness of this linguistic processing was ascertained through speech recognition experiments  . 
2 . Linguistic processor 2 .   1 Dependency ntJ cueture grammar This grammar is based on semantic dependency relationships between phrases  . The syntactic rules satisfy the following two constraints  . First , any phrase , except the last phrase of a sentence , can modify only one later phrase . Each modification , called a dependency relationship or dependenc 2 structure , can be represented by one arc . Second , modification arcs between phrases must not cross . 
These rules are illustrated in Fig . I . In two unacceptable sentences , one sentence is unacceptable because one phrase modifies the former phrase  , and Acceptable sentence ~ ~ a ~ le sentence
ABCD
ABCD ?
ABCD ?
ABCD
Fig .  1 . Examples using a dependency structure grammar
A , B , C and D are sentence phrases.
the ether sentence is unacceptable because arcs cross in its dependency structures  . 
2.2 Parser
After phonetic phrase recognition , recognition results are represented in a phonetic score matrix form as shown in Fig  .  2 . When analyzing dependency relationships , the parser extracts the most likely sentence in this matrix by taking into account the phonetic \ ]_ikelihood of phrase candidates and thel inguistic likelihood of semantic inter-phrase dependency relationships  . The parser also obtains the dependenqy structure that corresponds to the semantic structure of the extracted sentence  . 
2.2.1 Objective function
This parsing is equivalent to solving the following objective function using the constraints of dependency structure grammar  . For simplicity , the following linguistic formulation is described for speech uttered phrase by phrase  . The process for sentence speech is described in section  4  . 

T--max\[2;'c(xi , p ) + maxZ dep(wIj_1 , Xj , piY1 , j , p)\](I ) pj : : l " Y j = l ' where I~j . < N , 1K p ~ M , N is the number of input phrases , M is the maximum number of phonetic recognition candidates for each phrase  , Xj , p is a candidate of the j~th input phrase with the pth best phonetic likelihood  , and c(x j , p ) is its phonetic likelihood ( positive value ) . Also , Xi , j , p is a phrase sequence with one phrase candidate for each ith to jth input phrase and whose last phrase is Xj  , p . Yi , j , p is one of the dependency structures of X i , j , p , wi , j_I is the set of phrases that modify Xj , p in the sequence X i , j , p . Here , dep(w , xlY ) is the linguistic likelihood ( negatiw ! value ) of dependency relationships between w and x taking Y into account  . Namely , the first item of the teem on the right in Eq .   ( I ) is the summation of phonetic likelihoods of the hypothesized sentence composed of its phrase sequence  , and the second item is the summation of linguistic likelihood  . Maximizing Eq .   ( 1 ) gives the sentence and the dependency structure of it as speech recognition and t~dersta~ling results  . 
Because dependency structure grammar is compatible with case  grammar\[8\]  , the linguistic semantic likelihood ( dep ) of the dependency structure is easily provided using case structure  . The following are example : ~ of items for evaluating dependency relationships : the disagreement between the semantic primitives of the modifier and that requested by the modificant  , the lack of the ob\] . igatory case phrase requested by the modificant , and the existence of differenb phrases with the same case and modifying the same phrase  . The likelihood values for these items are given heuristically  . 
To so \] reequation ( I ) , a fast parsing algorithm using breadth-first search and beam search was developed  . This algorithm deals with higher linguistic or semantic processing such as the case structure  . . Although this algorithm offers suboptimal solutions  , it is practical because it requires less processing than depth-first search  . 
2.2.2 l~ce~t~z-first pe~csing algorithm
The breadth-first algorithm is formulated as order of candidates  1   2   . . . . . . M x \ [ , lXl , 2Xl , ~4I12 x2 , 1 x2 , 2/x 3 , Ixs , 2 x 3 , MJB
Fig .  2 . A matrix of phrase candidates input utterance : number 
N1 follows.
First , dep(w , xlY ) can obviously be divided into two terms . 
dep(wl , j-1, Xj , plYl , j , p ) =
Edep1(x , Xj , p ) + dep2(Y1 , j , p , Xj , p ) (2) xCwI ,   j1 where depl is the likelihood associated with dependency relationships of only the modifier and modificant phrases  , and dep2 is the likelihood associated with Y1 , j , p . An example of dependency relationships is shown in Fig  .  3 . 
Eqs .   ( I ) and ( 2 ) give the objective function's w tlue S ( 1 , Xj , p ) of a phrase sequence including the top phrase to Xj  , p in the sentence as:
S(1, Xj , p ) =
JJJ
ZC(Xh , p ) + ~ ~ dep1(x , xh , p ) + Edep2(Y1 , h , p , xh , p ) (3) h::1h=1 xCwI , h1h = 1On the other hand , the value of a phrase sequence not including the top phrase  ( i ? I ) is defined as:
D(i , Xj , p ) = jjj1_ZC(X h , p ) + ZZ depl(X , X h , p ) + Z dep2(Yi , h , p , xh , p)h::ih=ixCwi , h_Ih=i (~) Timma in difference between Eqs . (3) and (4) is that phrase sequence wh , F
I i


E(A , E , F ) depl(F , G ) depl(E , Gdepl(A , G ' ,  ' . -tdep2(YA , Gdep(wA , p . GiYA,G = dep2(YA . G'G ) + dspl(F,G ) + depl(E . G ) + depl(A,G ) Fig .  3 . Illustration of dependency relationships Using notation S and D  , the recurrence relation among the objective functions are derived  . This is shown in Fig .  4 . The recurrence relation are transforms into the following equations using beam search  . 
S(1 , Xj , p , r ) = rth-max\[S(1 , Xk , q , r l ) + D(k + 1 , Xj , p , r2) k , q , rl , r2+dep1(Xk , q , Xj , p ) + dep2(Y1 , j , p , Xj , p ) , if i = I(5') D(i , xj , p , r ) = rth-max\[S(i , Xk , q , r l ) + D(k + 1 , Xj , p , r2) k , q , rl , r2+dep1(xk , q , Xj , p ) 4 dep2(Yi , k , q , Xk , q ) , if i ~ I (6') where i ~ k ~ j-1 , 1 ~ q~M , and 1 ~ r , rl , r2<_L . Here , r , rl and r2 indicate the rank of beam , L is the maximum number of beams , S(1 , Xj , p , r ) and D(i , xj , p , r ) are the rth value of the element whose phrase sequence is Xi  , j , p and the dependency structure is Yi , j , p . 
ilere , rth-max\[\] is a function for deriving the rth best value  . When Eq . (5') or (6') is calculated , Yi , j , p is stored for use in the later stage of evaluating  dep2  . 
Initial values are given as follows.
S(1 , Xl , p , 1) = C(Xl , p ) + dep2 ~1 , 1 , p , Xl , p ) , if i = 1 ( top phrase ) (7) D(i , Xi , p , 1) = e(Xi , p ) , if i ~ I ( not top phrase )   ( 8 ) After calculating the recurrence relation , the value of the objective functions is obtained , 
T = max\[S(1,XN , p , 1)\], (9)
P where 1 ~ p ~ M . The best sentenc ~ and its dependency structure are given through YI  , N , p where p maximizes Eq .  (9) . The parsing table is shown in Fig . 5 and the parsing algorithm is shown in Table I . In Fig .  5 , the first row corresponds to S , and others correspond to D . The phrase sequence for first to Nth phrase corresponds to the rightmost top cell  . Each cell is composed of ML subcells . Arrows show the sequence of calculating the recurrence relation  . The processing amount order for this algorithm is O  ( N3M2L2 )  . 
Comparing the theoretical amount of processing for these two parsing algorithms  , the breadth-first parsing algorithm clearly requires much less processing than the depth-first parsing algorithm  . 
The amount of processing for each parsing algorithm is shown in Fig  .  6 . 
2.3 Predictor
To preselect the phrase hypotheses for the speech recognition  , the predictor is devised\[9\] , using prediction rules created by integrating connection rules and dependency structures of phrases  . These rules are described with rewriting rules : ( Xi , j)->(Xi , k)(Xk+1 , j ) where Xi , j is the phrase sequence for the ith to jth phrase  . ( Xi , j ) is the sequence with a Closed-dependency -structure where the tail phrase xj has the dependency relationships with phrases out of Xi  , j , and other phrases in Xi , j have dependency relationships with phrases within Xi  , j . ( Xi , j ) is divided into two phrase sequences with the closed-dependency-structure by modifying xk by xj  , and following Xi , k by X k+1 , j . A single phrase x i is also 4O4 ( J ) if i = l
Phraseno.
k . . . j depl(X k , q,X j , p)~dep2(Y .., x.
S(I , Xk , q ) D(k + l,Xj , p
S(1. X.p.
) = S(l , xkq ) + \] J(k + l , x.)
S(I , Xj , p+depl(Xk , q ' , Xj , p ) + dep2t'P ~ l , j , p , Xj , p ) (5) ? if i?1 Phrase no . 
i . . . k '" j\[ .   .   .   .   . xUU___~kdepl(,xj)Xk . q , pdep2(Yl , k , q'Xk , q)
D(t,Xk,q)D(k + l,Xi,p)
I.--">1F "
D(I , x . p )) = D(t , xkq ) + ~( k + l , x , j_)D(I , Xj , p+depl(Xk , q ' , Xj , p ) + dep2('~/i , k , q , xk , q ) (6) Fig .  4 . Illustration of deriving recurrence relat iun among objective functions 
N12' joNorder of candidates bea~(1, xN ? . ) Fig .  5 . Configuration of a parsing table regarded as a phrase sequence with a closed-dependency -structure  . These rules are described for the sequence , the ith phrase to jth phrase modified by the ith phrase  , as ( Xi , j)->(xi)(Xi+1 , j ) The hypotheses are predicted as follows . 
< I>xi is detected as a reliable phrase recognition result  . If there are no reliable phrase candidates in the ith phrase recognition results  , the following procedure is not carried out . 
< 2> The rules whose left term is scanned are such as ( Xi + l , j)->(Xi+1 , k)(Xk+1 , j ) After the leftmost derivation is repeated to detect hypotheses for  i+1~thphrase speech recognition , xi+I is detected in the following form . 

Generally , there is more than one ( Xi+1 , j ) , so xi+I is a set of phrases . 
< 3> The phrase recognition is carried out for the i+l -th ~\] case utterance whose hypotheses are elements of the set xi+I  . 
< 4> If the reliable phrase recognition result was detected in operation  <3>  , the rules which derived elements of xi+I are scanned again and hypotheses for the next utterance are derived using same procedure as  <2>  . 
<5>Thes~operations , namely hypotheses derivation and its phonetic verification  , are carried out until xj is detected . 
< 6> The detected phrase sequence Xi~j and its dependency structure Yi  , j is passed to the parser . 
During bhese operations , if the phrase recognition results are unreliable in operation  <3>  , the detectio a process of X i , j is halted and phrase recognit S on for all hypotheses is carried out  . 
Althe~ , gh Japanese is a phrase-order-free language , there are some relatively fixed please-order parts in a sentence  . These rules are applied to these parts . 
The number of hypotheses and the amount of acoustic processing can thus be reduced  , maintaining the above characteristics of the dependency structure grammar  . 
By linking the predictor to the parser , parsing can be accomplished using the dependency structures detected in operation  <6> of the prediction procedure . This linkage method greatly increases parsing speed  . 
3* Speech recogn i t ion exporimen %~ 3-   1St~echrecognibion system The speech recognition and understanding system is shown in Fig  .  7 . The system is composed of acoustic and linguistic processing stages  . The acoustic processing stage consists of a feature extraction part and a phoneme recognit ion  part\[10  , 11\] . The linguistic processing stage consists of a phrase recognition  patti11\]  , a parsing part ( a dependency relationship analysis part )  , and a phrase prediction part . The linguistic processing stage uses a word dictionary  , word connection rules for intra-phrase syntax , dependency relationships rules and phrase prediction rules  . The word dictionary is composed of pronunciation expressions  , parts of speech and case structures . Dependency relationship rules produce negative eva % ~ ation values that are set to the dependency relationships contrary to case structure discipline  . 
3.2 Speschreeognition process
For separately uttered phrases , acoustic feature parameters are extracted and bottom-up phoneme recognition is carried out  . The phrase hypotheses for topdown phoneme recognition are preselected by the 
Table 1 . Parsing algorithm 1 Loop for the end phrase of the partial sequence
DO2 to 5 for j = 1 , 2 , --- , N2 Loop for the candidate
DO3 to 5 for p = 1 , 2 , --- , M\]3 Setting the initial value SETS(1 , Xl , p , 1) or D(j , Xj , p , 1) ( Eqs .  (7),(8))
If J = 1, go back to 2.
4 Loop for the beginning phrase of the part ial sequence 
DO5 for i = 1-1 j-2 ,  - -  , 1  5 Calculation of reccurence relation < Loop for tlleen d phrase of the former sequence >  51 DO 52 to 54 for k = i , i?1 , -~- , 1-152 DO 53 to 54 for q = 1 , 2 , --~ , Mk < Loop for the beam width > 53 DO 54 for r l = 1  , 2 , --- , L54 DO for r2 = 1 , 2 , --- , L * Evaluation of S(1 , x1n , r ) or D(j , x  ~ n , r ) taking account of YtZ , por(Eq ,  . 
* Store of Ytj , p 6 Acquisition of the parsing results * Detection of value pmaximizing Eq  .   ( 9 ) * Acquisition of the phrase sequence and its dependency structure using  Y1  , N , p
I @?
F--mr , olo
NUMBER OF PHRASES 0 enumerative method
A depth-first method
V breadth-first method ( L = oo ) ill breadth-first method ( L=8 )  #breadth-flrst method ( L = l ) Fig .  6 . Comparison of processing amount 10   20   30 predictor . The preselection is also carried out using bottom -upphoneme recognition  results\[12\]  . 
Next , topdown phoneme verification is carried out and phrase recognition results are generated  . Phrase recognition results are represented in the form of score matrix with phonetic recognition scores averaged for each hypothesized phrase  . When the end of a sentence is detected , the parser extracts the phrases with t~ebest sentence likelihood by scanning this matrix  , and determines the dependency structure of the extracted phrases  . 
3-3 Performance
The effectiveness of the proposed linguistic processor was tested in speech recognition experiments  . The experiments were carried out using 20 sentences containing 320 phrases uttered by 2 male speakers . These results are shown in Table 2 . 
4 O5 recognition and understanding results Iwg\[dd~c ~l  . _o + ary __~--~\] dependency relationships analysls ~- - ~ i dependency  , I case J ~/\ I relatlonshlps L structure ~ hypotheses prediction  ~----7  \[  \[~2~   .   .   .   .   .   .   .   .   . Ji  ~ .   .   .   .   .   .   .   .   .   .   .   .   1 illinguistic I\[pronounciatlon\[+M/II processing  , expression ~ hypotheses derivation L_~predic ( lon .   .   .   .   .  ?  . . . . . . . . . . . . . . i+J - t , \[-- L~uJS ~ .   .   .   .   .   .   .   .   . J ~ tntra-phrase\[pre-selectoL ~ Y ~ . . . . . . . . J ~ phrase @ ( word ) recognition .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . bottom-sp honeme ~ honeme \ [ acoustic recognition recognition __\] processlng ~\] feature parameter extraction \] input speech Fig  .  7 . Speech recognition and understanding system , ~ 10~ =8 10 0 13 . .


OO
Olb1 ~ i'"<2b '' 25
NUMBER OF PHRASES
INEACHSENTENCE lO4
F-102tl , I~10o00000000~~Y~i01~i'2'0
NUMBER OF PHRASES
INEACHSENTENCE
Odepth-first method ~7 breadth-first method ( L=8 )   ( b ) speaker KS
O depth-first method breadth-first method ( L=8 ) Fig .  8 . Comparison of processing time for dependency relationships analysis 
Table 2 . Speech recognition results phonetic recognition S number of iphonetic hypotheses i processing i tlme 
I without predictor 542 i1 . 0': ~ i ~ hp~i ~ i :__ . . . . . . . . i ~~ . . . . . . . i 6: ~ h . . . . . . . .
without parser phrase recognition rate \[~\] 0 : within top 8 candidates 57   ( 87 )  60  ( 89 ) with parser depth-flrst parsing breadth-first parsing phrase i parsing recognition itime rate \[~\]* Recognition of predicted phrases  (   33~ to the total input phrases The proposed parser using the depth-first parsing algorithm increased phrase recognition rate by approximately  2OZ   ( from 57Z without the parser to 77~ with the parser )  . This result shows the effectiveness of a parser using a dependency structure grammar  . 
The processing time with the breadth-first algorithm was reduced to approximately IZ of that with the depth-flrst algorithm for sentence parsing  , while keeping the same level of speech recognition  77 i l . 
phrase iparslng recognition itlmerate \[~3 i 77   ~1  . 

78 iO . 89 rate as with the depth-first algorithm . This result shows the great effectiveness of the breadth-first parsing algorithm  . This result is shown in Fig . 8 for each speaker when M is 3 and L is 8 . 
Next , using 26 rules , the prediction was carried out for 33~ of the total input phrases . It reduced acoustic processing time to 60Z at these parts in a sentence , and it increased speech recognition speed . 
Finally , linking the predictor to the parser reduced depth -first parser  , and to approximately 90% of the time for the breadth-first parser . This shows the usefulness of the linkage . 
4- Breadth-firs % parsing a lgor i thm for sentence speeehreco~t ion The breadth-f irst parsing algorithm for the sentence speech or connected phrase speech is  devised\[13\] by the same procedure as in section 2  .  2 . 
Based on basic expansion algorithms \[14 , 15\] from phrase-wise to sentence speech , the speech recognition and understanding accuracy using the proposed algorithm is greatly increased compared to the accuracies using the basic algorithms  . In the sentence speech , phrase recognition results after phonetic processing are represented in a score lattice form with phonetic recognition scores averaged  . The parser extracts the best sentence composed of a phrase sequence by scanning this lattice  . The processing order is O(NSM2L2) , which is practical amount of computation , where N is the number of detected phrase boundaries in the uttered sentence  , M is the maximum number of phonetic recognitio ~ candidates for each phrase segment f~emone bounds ry to the next boundary  , and L is the maximum number of beams . 
The effectiveness of this parser was tested through sentence speech recognition with one speaker uttering  i0 sentences containing a total of 67 phrases . This parser increased phrase recognition performance in the sentences by approximately  49Z   ( from 27~ without the parser to 76~ with the parser )  . 
5-Conclusion
This paper proposed an efficient linguistic processing strategy for speech recognition and understanding using a dependency structure grammar  . 
This grammar suits processing of phrase-order-free languages such as Japanese and processing the result of frontend speech recognition  , which is usually erroneous ~ This linguistic processing strategy includes bottom-up parsing and a topdown phrase hypotheses predictor  . In particular , the bottom-up parser , taking account of the phonetic and linguistic likelihood  , greatly increases the accuracy and speed of speech recognition  . The predictor reduces the amount of phonetic processing by pre-selecting the phrase hypotheses  . The effectiveness of this parser and predictor was shown in speech recognition experiments  . 
Future development is include the statistical likelihood of dependency relationships  , integration with the statistical phonetic method like Hidden Markov Models  , and higher linguistic processing using the semantics and context knowledge  . 

The authors would like to express their appreciation to Dr  . Kiyoshi Sugiyama and Dr . Sadaoki Furui , for their invaluable guidance . The authors would also like to thai ( to Dr . Kiyohiro Shikano and Shigeki Sagayama for their useful suggestions  . 
References\[I \] Leser V . R . , et al1975 ) WO rganization of the Hear say II speech understanding system  . ', IEEE Trans . 
ASSP ., 23, I , pp . 11-24.
\[2\]WoodsW . A . (1976)' Speech understanding system-
Final Report ', Tech . Rep ., 3438.
\[3\] Levinson S . E . (1985 ) ' Structural Methods in automatic speech recognition '  , Proceeding of the IEEE ,  11 , pp . 1625-I6 50 . 
\[4\] Ney H . (1987) , ' Dynamic programming speech recognition using a contextfree grammar  . ' Proc . 1987 ICASSP , pp .  69-72 . 
\[5\] Matsunaga S . & Kohda M .   ( 1986 ) ' Post-processing using dependency structure of inter-phrases for speech recognition  . ' Proc . Acoust . Soc . J p n . Spring
Meeting , pp . 45-46.
\[6\] Hidaka T . & Yoshida S . (1983 ) ' Syntax analysis for Japanese using case grammar . ' Natural Language Processing Technique Symposium , pp41-46 . 
\[7\] OzekiK . (1986 ) ' A multistage decision algorithm for optimum hunsetsu sequence selection  . ' Paper Tee . 
Group , IECE Japan , SP86-32, pp . 41-48.
\[8\] Filmore C . (1968)' The case for case . ' in Bach and
Harms ( eds . ), 1-88\[9\] MatsunagaS . & Sagayama S . (1987 ) ' Candidates prediction using dependency relationships for minimal phrase speech recognition  . ' Paper Tec . Group , IEICE
Japan , SP87-29, pp . 5962.
\[10\]Aikawa K . , Sugiyama M . & Shikano K . (1985 ) ' Speken word recognition based on topdown phoneme segmenta-tion  . 'Proe . 1985 ICASSP , pp 33-36 . 
\[11\] Matsunaga S . & Shikano K . (1985 ) ' Speech recognition based on topdown and bottom-up phoneme recognition  . ' Trans . IECE Japan , J68-D , 9, 1641-I648 . 
\[12\] MatsunagaS . & Kohda M . (1987 ) ' Reduction of word and minimal phrase candidates for speech recognition based on phoneme recognition  . ' Trans . IEICE Japan , J70-
D , 3, pp . 592-600.
\[13\] Matsunaga S . (1988 ) ' Dependency relationships analysis for connected phrase recognition  . ' Nat . Conv . 
IEICE Japan , 84-I-3, pp . 345-346.
\[14\]OzekiK . (1986 ) ' A multistage decision algorithm for optimum bunsetsu sequence selection from bunsetsu lattice  . ' Paper Tec . Group , IECE Japan , COMP 86-47, pp .  47-57 . 
\[15\] Kohda M . (1986 ) ' An algorithm for optimum selection of phrase sequence from phrase lattice  . ' Paper Tech . Group , IECE Japan , SP86-72, pp . 9-16 . 

