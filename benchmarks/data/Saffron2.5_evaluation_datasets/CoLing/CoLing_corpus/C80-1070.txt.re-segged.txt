ATRIAL OF JAP ANESETEXTIN PUTSYSTEM
USING SPEECHRECOGNITION
K . Shirai Y . Fukazawa T . Matzui II . Matzuura
Department of Electrical Engineering
Waseda University
3-4-10kubo Shinjuku-ku
Tokyo , 160 Japan

Since written Japanese texts are expressed by many kinds of characters  , input technique is most difficult when Japanese information is processed by computers  . Therefore a task-independent Japanese text input system which has a speech analyzer as a main input device and a keyboard as an auxiliary device was designed and it has been implemented  . The outline and experience of this system is described in this paper  . 
The system consists of the phoneme discrimination part and the word discrimination part  . 
Acoustic analysis and phonemic discrimination are effectively performed using an estimation method of the articulatory motion from speech waves in the phoneme discrimination part  . And its outputs are lattices of Japanese pseudo -phonemes corresponding to speech inputs  . 
In the word discrimination part , phonemic strings are corrected using various kinds of a priori knowledge anutrans formed into suitable kinds of characters  . On behalf of it , this part mainly consists of the refrieval of the word dictionary indexed by vowels  , and the similarity calculation using dynamic programming  . 
i . Introduction
Speech recognition systems can be classified according to whether they recognize isolated or connected words  . But in either case , word recognition has been thought a basic problem  . In the speech understanding system , a method of utilizing various kinds of a priori knowledge  , which is obtained from a phonemic , a syntactic and a semantic level on clearly defined tasks  , has been developed . And it has been clarified how information at each level contributes to recognition  . I-~While many understanding systems set the goal at the understanding of contents of speech  , our nain object is the task-independent Japanese texts input system  . Because a system which uti-lizes properties of a task too much is not so practical and the phonemic discrimination must be the core of the speech recognition system  . 
This system consists of the phoneme discrimina ~ tion part and the word discrimination part  . 
In the phoneme discrimination part , connected spoken words is segmented into discrete phonemic strings  . The main difficulties at this step are the large variation of each utterance and the coarticulation  , and the difference between talkers . So it is important to select acoustic parameters which aren't relatively affected by these variations  . 
This system employs the feature extract on method based on the speech production model  . The articulatory motion for the utterance is estimated from speech waves  , and the phonemic discrimination is performed using the trace of them  . The followings are the merits to describe the acoustic feature at an articulatory level :  ( i ) Processing of coarticulation may be considered most clearly in relation to the physiological factor of speech production  . 
(2 ) Adaptation to a speaker is easier than other methods  . 
Nevertheless it is almost impossible to const-ruct perfect phonemic strings corresponding to spoken words only by the phonemic discrimination  . 
So in the word discrinimation part , the phonemic strings are corrected using various kinds of a priori information  . At the same time , it is neccessary to transform the phonemic strings into the suitable kinds of characters in order top rint the texts  . Because written Japanese texts are expressed by many kinds of characters  , which are Kanji ( ideographic character ) , Hira-gana ( cursive form of Kana , Japanese syllabary ) , Katakana ( square form of Kana ) and special symbols . 
The confusion matrix between phonemes , the phonemic connective rules , the word dictionary and so on have been already used as a priori information in word discrimination systems  , and their validity has been also ascertained l '~?6 Therefore , the word discrimination part mainly consists of the retrieval of the word dictionary which is indexed by vowels  , and the similarity calculation using dynamic programming and the phonemic confusion matrix  . Because the phoneme discrimination part can recognize vowels more correctly than consonants and the phonemic confusion matrix reflects the characteristics of the phoneme discrimination part exceedingly  . 
In order to translate from Kanato Kanji , some , ~systems with the word dictionary for Kana-Kanji translation and with syntax and semantics analyzer  , are suggested . But the task domain should be restricted so as to get the high translation rate  . 
Main difficulties of Kana-kanji translation areas follows:  ( i ) Segmentation from input characters ' trings to words  . 
(2) Identification of synonyms
In this system , identifiers inserted from a simple keyboard serve as spaces between words  . And the difficulty of the latter is solved by redundant inputs since speech input is much easier than other input techniques  . 
m --464--micropho~ne phoneme recognition keyboard syntactic ~_ approachll\]~wo~a~efe~en~e ~___~matchingI KANJI printer 

Fig 11 Block diagra B of the system
The paper is structured as follows . The next section gives a feature extraction method using the articulatory model  . And then the segmentation of the continuous speech  , and the discrimination technique of vowels and consonants are described  . 
Section 3 gives an outline of the word discrimination method which uses the word dictionary ? constructed systematically and employs some priori knowledge  . Section 4 concludes with a brief description of the results obtained with the system thus far  . 
.2 . Phoneme discrimination . ? 2 . 1 Segmentation In the recognition of continuous speech  , it is effective to discriminate voiced , unvoiced and silence . Next four parameters are used for voiced-unvoiced -silence decision  . 141) Power of signal . E : lO . log\[1N1s2 ( n ) \]N n~- ( 21 ) where N is the number of samples in one frame . 
0  , SO 100   250 hi"""""i "' h " d "' h "' h "' h "' h " ' h " ' hmh"''d'"'hmh"i "" h"d""""i"h "'"'  . . . . . . 1' ' \[ frame \] a w a "*'?" f - - q I s i lence

Fig . 21 Example of analysis/mika wasima/2 ) The ratio of the power in low frequency and the total power of the signal  . 
Power in low frequency = NEr = TotalPowern ~ ( n ) / n - ~2 ( n )  3 ) Zero-crossing count in one frame .   ( 22 )  4 ) Normalized autocorrelation coefficient at unit delay  , Plas
N//'NN 1i2-3)
Pl = n\[lS ( n ) s ( n-1 )  / /  ( n~lS2 ( n )   )   ( Z s2 ( n ) )  n=0 These parameters are computed for each frame of samples and the example of analysis shown in F ig  . 21  . The decision is performed using the next quadratic discrimination function  . 
d i ( x ) = ( x-~i ) tci-l ( x-~i ) + Inlcil ( 24 ) After this decision is made for each frame , smoothing algoritl ~ is used . By this smoothing algorithm , noises and disturbances in the VUS-decision can be modified  . The result of this decision is shown in Fig . 2-1 . 
2.2 Articulatory model
Several authors have studied the estimation of the vocal tract shape from the speech wave  . If the artieulatory motion is successfully estimated  , the result may be a good feature of the speech and effective for the recognition  . 79 In this paper , a new technique is used to estimate the vocal t ract shape  , which depends on the precise analysis of the real data of the articu-latory mechanism  . 101 3 In this section , the articulatory model which constitutes the base of this method is introduced  . This model relies on the statistical analysis of the real data and makes it possible to represent the position and the shape of each articulatory org anvery precisely using the small number of parameters  . And further , the physiological and phonological constraints are included automatically in the model  . 
The total configuration of the model is shown in Fig  . 2-2 . The jaw is assumed to rotate with the fixed radius FJ about the fixed point F  , and the location J is given by the j a wangle X j . The lipshape is specified by the lipopening height L h  , the width Lw and the protruction Lp relating to the jaw position on the mids a gittal plane  . 
The tongue contour is described in terms of a semi -polar coordinate system fixed to the lower 
Velump ALATE_5 ter..
F ( O , O ) pJT ? ngu ' 0 oso . iption of orticuato.y
G--G'(ag , bg)
Glottis --465--jaw and represented as a 13 dimensional vector r = ( rl . . . . . r la ) . The variability of the usual articulatory process  , there may be some limitations on account of physiological and phonological constraints  , which can be expressed by the strong correlation of the position of each segment along the tongue contour  . Therefore , it becomes possible to represent the tongue shape effectively using only a few parameters  . These parameters can be extracted from the statistical analysis of X-ray data  . 
A principal component analysis is applied and the tongue contour vector for vowels r v may be expressed in a linear form as  , rv = li = lXTiVi + rL(25) where vi's(i = l , 2  . . . . . p ) are eigenvectors , and rv is a mean vector for vowels which corresponds roughly to the neutral tongue contour  . 
The eigenvectors are calculated from the next equation Cv  .  = ~ . v .  (26)
V 1   1   1 where Cv is the covariance matrix which is defined by N 
I_-_?)T (2-7)
Cv = ~ ~= l(rvkrv )( rvl ~ , , and Xi i s the cor respond ing e igenva lue to sa t - i s fy the character i s t i c equat ion 
Icv-~II=o ~2-~
The same stastical technique as described above may be used for the expression of lip shape  . ( k --
L h1 Js+Lb
I'w=\[k2'Jl~3+~';
Lp\]~where Ossignifies the distanc , per and the lower in cisors . The active movement of the lips is reflected in the second term  , and
XL is the lipparameter.
The total model with the nasal cavity is shown in Fig  . 2 -3 and the characteristics of the articulatory parameters are summarized in Table  21  . 
Details about the model are found in the references  . 
2.3 Estimation of articulatory motion
In this section , the estimation of ' the articu-
F .\]
Let an m-dimensional vector y be the acoustic feature to represent the vocal tract transfer function of the model  . In this study the ceps-trum coefficients are adopted as the acoustic parameters  . The acoustic parameters are express-ed as a nonlinear function h  ( x ) of the articulatory parameters which means the transformation from x to y through the speech production model  . 
On the other hand , lets Y be acoustic parameters measured from the speech wave after glottal and radiation characteristics are removed  . Then , the estimate ~ of the articulatory parameters is obtained as to minimize the next cost function  . 
2  ( 210 ) J ( = k3 = 11 sYk-b ( Xk ) lf ~+ II ? kI1 ~* II%-~k-lllr where R , Q and F are the matrices of weight , k is frame number and ~ k1 is the estimate at the previous frame . In the cost function , the 1st term is the weighted square errors between the acoustic parameters of the model and the measur -ed values  . The 2nd and 3rd terms are used to re-strict the deviation from the neutral condition  ( x = 0 ) and from the estimate of the previous frame , respectively . These terms are also effi~cient to reduce the compensation of the articulatory parameters  . 
rilslatory parameters from the speech wave is con : sidered in the framework of the speech produc- Glottistion model  . This problem becomes nonlinear opti-XG mization of parameters under a certain criterion  ,  - ( - --9+ and it must be solved by an iterative procedure . 
Therefore , the uniquness of the solution and the stability of the convergence are significant problems  . In this method , these problems are solved by introducing constraints to confine the articulatory parameters within a restricted region and by the selection of the appropriate in-+itial estimate using the continuity property  . 
And also the model adjustment to the speaker brings good effects for the estimation  . 
and latory
Table 21 Articulatery parameters
Articulatory parameters
Tongue Tonguc J : twIAp
XTI?T2XjXL
Glottis Vehlm
XGXN back high open round open open front low cleses pread close ? lose-  -466-If the method is applied to a speaker , first of all , the mean length of the vocal tract should be adjusted for the speaker  . The length factor SFK which gives a uniform change in the vocal tract length  , and the articulatory parameters can be estimated simultaneously  , Once the mean vocal tract length is fixed , only the articulatory parameters are estimated . In Fig .   24 the estimated result for the utterance/aiue o/ is shown  .  1517 2 . 4 Normalization of articulatory parameters The size and shape of each articulatory organ are different between talkers  , and further the manner of articulation varies each other  . These differences make difficult the talker independent speech recognition problem  . Therefore , a method to be able to adapt for the speaker differences is inevitable for speech recognition  . 
In the estimated artieulatory parameters in the preceding section  , the average length of the vocal tract is taken into consideration and automatically normalized  . While the vocal tract length is the most significant in this problem  , other factors still remain . 
vocal tract real estimated shape spectrum spectrum glott is The differences in Japanese  5 vowels between adult females are shown in Fig . 2-6 . In order to cancel these differences , a linear transformation which changes the distribution of female articulatory parameters into that of male onesi:s introduced  . 
Number of male a poiker ll-I0
XT2 lul
IIIIII Illel
Illi ; / o /

Fix . 2-$ DLs trlbution formal espeakers ( A group ) in XTI-X~2 spacilx ~ ,   . ~ . roffe, . l . peak . ,, . -lOlul ~, ~
A , , , - .   , ~'') /'/" l , ~lal " , 1I0--"'" , /' ~ I ~' , ~ , :'\ I , -- IIIIIIIi ~ ~ Itl Ii I
IIIIIi IIIII
I~L---L ~ III , IIu_Ill-PLB . 2-6 Distribut ? on for female speakers ( b group ) in XTI-X~2 space 0-9_o3 OkHz ~ frame\]-
Fig .   24 Estimated result for the utterance/aiueo/lips
Nw . ber offema lespeakers-10
XT 2 , % / i /
Ioi/u /,


PiJ , 2 ~7 Distribution for female speakers ( b group ) in XTI-X~2 space after linear transformation - -467- 
X = AX+B(2-ii ) where x = ( X T1 X T 2 , Xj , XL ) ~: modified vectors coming ' near the male di stribution  . 
X=(XTI , XT2 , Xj , XL ) T : estimation value for the female Thematr ices A and B are caiculated by the least squares method  . In Fig .  25  , 2 6 and 27 distribution for 10 males and females are shown respectively . The distribution of Fig . 2 -6 is transformed by the above linear transformation into Fig  . 2-7 . It should be noticed that the matrices A and B were determined by data of  i0 female who are not contained in the group shown in Fig  . 2-6 . The results for the vowel recognition experiment are shown in Table  22  . 
In this experiment , the number of subjects is 50 , that is 30 males and 20 females . Each subject uttered every vowel five times . They are grouped every ten subjects , then there are three male groups A , B , C and two female groups a , b . 
Experiment I ; Recognition of male voices after learning by male voices Experiment II  ; Recognition of female voices after learning by female voices Experiment HI  ; Recognition of female voices after leaning by male voices Experiment ~  ; Recognition of female voices using normalization of linear transformation  , where the refer-"ence is made bZmale voices These results show the effectiveness of the articulatory parameters for the talker independent speech recognition and the normalization procedure is useful for the male -female transformation  . 
2 . 5 Discrimination of continuous vowels by usin ~ articulatory parameters The locus of estimated articulatory parameters about/aiueo/is shown in Fig  .  28 . 
Initially , continuous vowels are discriminated frame by frame using a quadratic discrimination function of articulatory parameters  . As this re-Table 22 Confusion matrix for the result of vowel recognition Experiment I  98  . 8% Experiment ~ I 94 . 9% / i / / u / / e / / o / / a / / i / / u / / e / / o / / a  ,   298  -  2   296   4 -/ i /- 300 --/ u /- 2  -  298 / e / 9  -  1  -  290 / o /
Experiment 1I 98.0%
I ~/ a/li//u / / el/o / / a / / i / / u / / el/el  99  -  1   96   4   99   1   i00   2   2  -  96 lalIlllul ~ el Ioi 295  -  1   4  -  286  -  14   -1   298   1   -2   1   9   288   -37   6   257 
Experiment ~97 . 3% lal l i l lul ~ el Iol / a / 289 - Iili/- 298  -  2 / u / - 299   1 / el- 15  -  285 / o / I ii 288 sults involve transient parts , it is necessary to estimate them . Then S ( k ) is computed as the parameter of discriminating them  . 
S ( k ) = I~T l ( k + l ) - X T l ( k11+\[X To ( k + 1 ) -XTo+\]Xj ( k + I ) -Xj ( k ) J + JXL~+I ) -XL ( k ~ t k ) l ( 212 ) It can be regarded the valley of S ( k ) ~s the stable part and the high part of S ( k ) than the threshold tas transient part . But , it is difficult to distiguish between stable part and transient part enough  . So stable part is decided with changing threshold t  . 
The speakers in this experiment are 2 malea-dults . Each speaker spoke 23 kinds of continuous 5 vowels 2 times . This continuous vowels involve all 60 kinds of connected vowel VIV2Va  ( VI , V2 , V3: a , i , u , e , o ) such as aie , oei . 
Table 23 shows the errors occurred by coartic-ulation . 
front
Fig . 28
The locus of articulatory parameters
Table 2-\] low 3 Vowel discrimination
XT2 high/aiueo / .   .   .   .   .   .   .   .   . i errors speaker S . M . 
NO . INo . 2 No .   3 !/ aiueol 2 / aeiou / 3 / aeuol / 4 / iaoiu / 5o i u ~ ouu \ [6ao~o   5 / iuoea / 5uo--u   6 lieaou / 7 / uaieo / 17   8 / uaoel/lei ~ ii 7a  ?~?  8 uaO~uOO~ei--ii\[ei ~ i 9 / ueaio/\ [9eai~eei 
I0/u oaeo /
II/eiuae / 13 / eoaui / 14 / eouei / 15 / oiaue / 16 / oieua / 17 / ouiae / 18 /auoua / 19 / uieia / 20 / eauau / 21 / eoioe / ; ioe ~ iue 22/o ai ai/~lai ~ iei23/oeueu / ) a ~ o\[Oei ~ ii20io~ue O2~i ~ i
L~e~ii22 iae ~ ie e
L22 ~
OU~OO auo ~ ao
L ~ au ~" Ou_~
L 5ueu ~ ulu

No .429 eai ~ eel3 Oeau ~ eeu
Joe~lue2 . 6 Consonant recognition using articulatory parameters By the data of the gliding section between consonant and vowel  , consonants are discriminat-ed . In this discrimination experiment , a method of DP matching applied for the loci of the articulatory parameters which are estimated for the gliding section  . Simple acoustic parameters are used together . 
Speech data in this experiment is uttered by 2 male adults , each speaker spoke continuous V z C V 2 . ( V z is fixed at / a / , V2 is vowel/a/ , / i / , / u / , /e/ , / o / , C is consonant / g/ , /z/ , /d/ , /b/ , / h / , / s / , /p/ , /t/ , /k / where / adi / , /a du/were omitted . ) One data is for reference pattern , another is for the test . 
Since the articulatory organs show relatively speedy motion in the interval from the consonant to the vowel  , the frame length is selected as 12
ESTi~TIONOF ~' liATQIING FINkL

Fig . 2-8  . Block diagram of the recognition experiment Table  24 Results of the recognition experiment ~ m / ~/ ~ / ~ / ga /  20 ooo 2 / za / 2   z6 o/cYa/oo 2o o . /h Vooo 2 , ) n '/' , ca / / a a // p a , ' / ha / 20   0   0   0   0 / aa / 1   17   2 oo/tus / 6   0   13   1 o/pa/\]o 1  ~ . 1/ha/010019/~// zi//hi/
Iqil 1811 , /~/02 O0/bL/0119/' , ct // , , hi / / c :~ . // pi / , 4 4j /k : L/ 13   0   2   0   5 / ~ ht / 0   20   0   0   0 /d  ~ L/ 0   0   20   0   0 / ~ / o : e 20 o , q~/500015/cS . //= u // bu/loW'191o/~v'218 o . /Ix*/002 O/~//de // , , , ,/ / ge / 20 0 0 /~/ 0 18 2 / de / o 1 19
Foe/ool/ .   . / itsu/ipu / , ,~/ ~ o , 4 ~/ o ~1elo I ~ I19/pc//he/~'~19 ol 17 , o/I ~ ol/bol/go / 15   0   3   2 / zo / 0   17   3   0 / do / 0   1   16   3 ~ o / 0 I 4   15 voiced : 91   . 4 % / k~V /~ V / b~u / / pu / / m / 19   0   0   1   0   0   16   1   1   0   1   9   10   0   0   1   0   0   18   1   0   0 o 1   19   /~1 leml ~ eel/be / ~ , , el 19   0   0   0   1   0   20   0   0   0   0   2   14   4 ' O 0   0   1   16   3   0   0   0   2   18 / ko / 1"/   0   3 oo/~:~/o 2o   0 oo/to / 0   1   17   2   0 lpo / 1 o 4   14 I
I/ho/000416 unvoiced:85 . 4 % ms with Hamming Window and the pitch syncronous method is used for the analysis  . For the estimation initial value is obtained by the piecewise linear estimation method at stable vowel section  . 
And articulatory parameters are estimated backward to consonant section  , using the parameters at the previous frame as the initial value  . The process of the discrimination experiment is roughly shown in Fig  . 2-9 . 
Initially , we estimate the articulatory parameters of every data by the method mentioned a-bove and perform the end-free DP matching for the reference pattern of every kind of data for each speaker  . The total power and the power in high frequency are also used for the discrimination  . For the recognition of the voiced con-sonants , the results of the DP matching of articulatory parameters are reliable  . For that of the unvoiced consonants , the acoustic parameters are mainly used . These results are shown in
Table 24.
The attempt to extract the feature of consonants from the gliding part of the articulatory motion in the following vowel was successful to some extent  . But there are some problems in the accuracy in the estimation of the articulatory parameters  . And the individual difference has influence on the parameters  . 
3 . Word discrimination 3 . i How to input Japanese texts Ideally speaking , texts which are arbitrarily pronounced should be recognized perfectly and suitable Kana-Kanji translation should be performed  . But the phonemic discrimination , the processing of synonyms and so on are actually very difficult  . So the system uses a simple keyboard as an auxiliary input device  , it has only 7 keys and it is enough manageable by one hand . 
In case of input , first of all , a key indicating the kind of the character must be pushed  . For example , ' H ' indicates Hira-gana . For Kanji , the reading of the compound word , On-yomi ( the phone-tical reading of Kanji ) and Kun-yomi ( the Japanese rendering of Kanji ) constructing the word are pronounced . Since the acoustic input is much easier than the other input techniques  , some redundant data can be inputted with less burden  . 
In the current version , a key indicating the kind of the reading pronounce d is pushed for the sake of the easy processing  . 
For Hira-gana , a speaker may pronounce a word which contains its reading as the ending  . Otherwise a user just pronounces Japanese texts as a way of speaking with the control information from the keyboard  . 
control part tmain control character\[sub control character input data < phonemic partlph ? nemic data ~ phonemic probability 
Fig 3-i the component of input data to the word discrimination part  --469--3  . 2 Input data The component of input data to the word discrimination part is indicated in Fig  . 3-1 . 
The phonemic part is an acoustic data which is recognized in the phoneme discrimination part  . 
If it cannot decide only one phonemic candidate , the phoneme and its probability for each candidate are passed as a lattice  . That is to say , input data is a lattice of Japanese pseudo -phonemes corresponding to the acoustic input  . 
Fig . 3-2 is an example of input data.
The control part is inputted from the keyboard in order to correct the phonemic data and perform effective Kana-Kanji translation  . 
letter reading meaning tensaiwa kyuju-kyu paasent on oase dear u Genius is ninety-nine percent perspiration  . 
data CJ(tensai ) TTENSAI
Y(ten)TTEN , ( area ) TA ~
Y ( sai)TSAI , ( toshi ) TTOSHI
HHY(wa ) TWA
SY(ku ) TKU
Y(ku ) TKU
KY ( paasento ) TPAASENTO
HY(no)TNOfiY(ase)TASE,(Ran)TKAN
HY ( dearu ) TDEARU * ( ten ) - - - t8 p6 k3 e7 aln 7m 7
Fig 342 Example of an input data parameters as follows are got 
S ; sequence of vowels
IN ; number of vowels
LR ; reliable phonemes get the restricted dic tionary  ( gL ) ith N ~ riet BL with SJ
F . : ou . r "' calculate the similarity for the BL items I satisfying the R restriction \[ \ [ I then  , some of the most similar items , CA are l\]\[selected as candidates ___ ~ l
L ; boundary of similarity ( given )
Fig 3~3 Algorithm to decide some candidates 3 . 3 Word discrimination algorithm For Kanji , redundant data , which consists of a reading of a compound word , On-yomi , and Kun-yomi , are given . Some candidates of Kanji corresponding to each reading are sought using the algorithm of Fig  . 3-3 . This algorithm depends on the fact that vowels can be discriminated more precisely than consonants  , and the tendency which one phoneme is apt to be misdiscriminated from others are statistically known previously  . 
Kanji dictionary used contains about i000 readings of Kanji and corresponding characters . 
This dictionary is divided by the number of vowels and further indexed by the sequence of them  . A part of it is shown in Fig . 3-4 . 
_reading of KAN3I \] dictionarynt ~ nber of vowels .   . 
vowo , ????.
items . . . . ?''
Fig 34 Example of a dictionary
Similarity between a discriminated phonemic string and a word contained in Kanji dictionary is computed by dynamic programming using the confusion matrix  . This matrix is made using the results of the recognition experiments and contains the misdiscrimination probability to other phonemes  , the probability of omission and that of addition of other phonemes  . Among these , the probability of addition depends upon the tendency that the addition is influenced by the following phoneme a stile nature of the phoneme discrimination part  . 
In dynamic programming , it is assumed that the number of the continuous omission or addition of phonemes is less than two  . The similarity S(D , W ) between the phonemic string D whose length is I , ansW whose length is J is calculated as follows : s  ( D , w ) = g(1 , 1)/ig(i , j ) = maxLl(i , j ) , La(i , j ) , Lo(i , j )
Ll(i , j ) = sim(i , j)+g(i + l,j + l)
La(i , j ) = sim(i , j+l)+g(i+l , j+2)+adp(i , j ) Lo(i , j ) = sim(i + l , j ) + g(i+2 , j + I ) + omp(j)g(l , j ) = sup(J-j)g(i , J ) = sup(I-i ) where sim(i , j ) is the similarity between phoneme i and j , and given from the confusion matrix , adp(i , j ) is the probability indicating that phoneme i is added before phoneme j  , omp(i ) is the probability omitting i , and sup ( i ) is the penalty value of unmatched length . 
This algorithm selects some candidates of Kanji which have a high matching score  . Since some redundant data is given for each kanji  , one most suitable character is selected and printed  . 

For Hira-gana , the system doesn't have redundant readings originally in contrast with Kanji As it is especially difficult to discriminate too short phonemic strings  , phonemic corrections are neccessary . So the system utilizes the nature of Japanese that almost all short Hira-gana strings are found as the inflectional part or Joshi  ( auxiliary word in Japanese )  . For the former , a user may pronounce a word which contains the inflection and the system uses them as a redundant data using the inflection dictionary  . 
For the latter , the dictionary containing Joshi and its connection information is used as a priori information  . 
In case of Katakana , as the usage is restricted to the words of foreign origin  , the phonemic correction is performed using the loanword dictionary  . At this time , the algorithm of
Fig . 3-2 is available.
Besides , Japanese text input system must process special characters  , for example numbers , alphabets , punctuation marks and so on . But they are a few , so the above algorithm can processed them using special character table  . 
4. Conclusion
At present , the system uses YHP-21MX and the special purpose hardware for the phonemic data extraction  , and the phoneme and the word discrimination part are built on HITAC  8800/8700 at the Computing Centre of the University of Tokyo  . 
Though the performance , the facility and the bottleneck as the total system has not been clarified as yet  , partially some results and problems have been found  . 
The phoneme discrimination part aims at recognizing connected speech uttered by an unspecified speaker  . On behalf of it , the feature extraction method at the articulatory level is tried  . As a result , stable articulatory motion is estimated with the satisfactory precision for vowels  , and the validity of it is confirmed . And the adaptation for speakers is effectively performed  . For consonants , the feature extraction method using the transient part of the articulatory motion is tried  , but it has a little draw point at the precision and the stability of articulatory parameters  . It is also influenced by a speaker . 
A combination to the other sound parameters will perhaps make the system improve  . 
For the word discrimination , the syntax has not been used except for Hira-gana at this version  . Hereafter it is planned to incorporate the syntactic information as much as possible for the improvement of the performance  . But the tradeoff between the facility and the processing speed is an important problem  . 

The authors wish to thank J . Kubota , T . Kobayashi and M . Ohashi for their contributions to designing and developing this system  . 
References ( i ) Lea , W . A . Ed . ' Trends in Speech Recognition '
Prentice-Hall (1980) (2) Ready , D . R . Ed . ' Speech Recognition ' Academic
Press (1975) (3) Newell , A . et al'Speech Understanding System : Final Report of a Study Group ' 
North-Holland (1973) (4) Denes , P . ' The Design and Operation of the Mechanical Speech Recognition at University College London ' Jour  . Brit . I . R . E . Vol . 19
No . 4 pp . 219-234 (1959) (5) Woods , W . A . ' Motivation and Overview of SPEECHLIS : An experimental Prototype for Speech Understanding Research ' IEEE Trans  . 
A . S . S . P . VoI . ASSP-23pp . 2(1975) (6) Sakai , T . ,Nakagawa,S . ' A Classification Method of Spoken Words in Continuous Speech for Many Speakers ' Jour  . Inf . Proc . Soci . of Japan
Vol . 17 No . 7 pp . 650-658 (1976) (7) Shirai , K . ' Feature extraction and sentence recognition algorithm in speech input system '  4th Int . J . Conf . on Artificial Intelligence , 506-513, 1975 . 
(8)Wakita,H . ' Direct estimation of the vocal tract shape by inverse filtering of acoustic speech waveform  , IEEE Trans . , ASSP-23, 574-580, 1975 . 

' Estimation of vocal tract are a functions by adaptive inverse filtering methods ' Bull  . 
Electrotech . Lab ., 37, 467-481, 1973.
(10)Stevens,K . N . , House , A . S . , ' Development of a quantitative description of vowel articula-tion '  , JASA ,  27 ,  484-493 ,  1955 . 
(ll)Coker,C . H . ,Fujimura,O . ,'Model for specification of the vocal-tract area function '  , JASA ,  40 ,  1271 ,  1966 . 
(12)Lindblom,B . ,Sunberg,J . ' Acoustical consequences of lip , tongue , jaw and larynx movement ' JASA ,  50 ,  1166-1179 , 1971 . 
(13)Shirai,K . ,H~nda,M . , , An articulatory model and the estimation of articulatory parameters by nonlinear regression method ' Trans  . 
IECE , Vol . J59-A , No . 8, 668-674, 1976.
(14)Atal,B . S . ,Rabiner,L . R , ,'A pattern recognition approach to voiced -unvoiced-silence classification with applications to speech recognition ' IE~E Transo  , ASSP-24 ,  201-212 ,  1976 . 
(15)Shirai,K . ,Honda,M . ,'Estimation of articulatory parameters from speech waves ' Trans  . 
IECE , Vol . 61-A , No . 5, 409-416, 1978.
(16)Shirai,K . ,Honda,M . ,'Feature extraction for speech recognition based on articulatory model ' Proc  . of 4th Int . J . Conf . on Pattern
Recognition 1978.
(17)Shirai,K . ,Matzui,T . ,'Estimation of articulatory states from nasal sounds ' Trans  . IECE,
Vol . J63-A , No . 2, 1980.

