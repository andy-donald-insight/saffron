Finite state phrase parsing by rule sequences
Marc Vilain and David Day
The MITRE Corporation
202 Burlington Rd.
Bedford , MA 01720 USA
mbv@mitre.org , day@mitre.org

We present a novel approach to parsing phrase grammars based on Eric Brill's notion of rule sequences  . The basic framework we describe has somewhat less power than a finite-state machine  , and yet achieves high accuracy on standard phrase parsing tasks  . The rule language is simple , which makes it easy to write rules . Further , this simpli-city enables the automatic acquisition of phrase-parsing rules through an error-reduction strategy  . 
This paper explores an approach to syntactic analysis that is unconventional in several respects  . To begin with , we are concerned not so much with the traditional goal of analyzing the comprehensive structure of complete sentences  , as much as with assigning partial structure to parts of sentences  . The fragment of interest here is demonstrably a subset of the regular sets  , and while these languages are traditionally analyzed with finite-state automata  , our approach relies instead on the rule sequence architecture defined by Eric Brill  . 
Why restrict ourselves to the finite-state case ? Some linguistic phenomenare easier to model with regular sets than contextfree grammars  . Proper names are a case in point , since their syntactic distribution partially overlaps that of noun phra`ses in general  ; as this overlap is only partial , name analysis within a full contextfree grammar is cumber some  , and some approaches have taken to include finite -state name parsers as a frontend to a principal contextfree parsing stage  ( Jacobs et al I99i )  . Proper names are of further interest , since their identification is independently motivated as valuable to both information retrieval and extraction  ( Sundheim ~996 )  . Further , several promising recent approaches to information extraction rely on little more than finite-state machines to perform the entire extraction analysis  ( Appelt et al I993 , Grishman I995) . 
Why approach this problem with rule sequences ? In this paper we maka the case that rule sequences succeed at this task through their simplicity and speed  . Most important , they support mixed-mode acquisition : the rules are both easy for an engineer to write and easy to learn automatically  . 
Rule sequences
As part of our work in information extraction , we have been extensively exploring the use of rule sequences  . 
Our information extraction prototype , Alembic , is in fact based on a pipeline of rule sequence processors that run the gamut from part-of-speech tagging  , to phrase identification , to sentence parsing , to inference ( Aberdeeen et al I995) . In each case , the underlying method is identical . Processing takes place by sequentially relabeling the corpus under consideration  . 
Each sequential step is driven by a rule that attempts to patch residual errors left in place in the preceding steps  . 
The patching process as a whole is itself preceded by an initial labeling phase that provides an approximate labeling as a starting point for rule application  . 
This patching architecture , illustrated in Fig .  1 , was codified by Eric Brill , who first exploited it for part-of-speech tagging  ( Brill I993 )  . In the part-of-speech application , initial labeling is provided by lexicon lookup : lexemes are initially tagged with the most common part of speech assigned to them in a training corpus  . This initial labeling is refined by two sets of transformations  . 
Morphological transformations relabel the initial ( default ) tagging of those words that failed to be found in the lexicon  . The morphological rules arc followed by contextual transformations : these rules inspect lexica\[context to relabel lexemes that are ambiguous with respect op art-of-speech  . In effect , the morphological transformations patch errors that were due to gaps in the lexicon  , and the contextual rules patch errors that were due to the initial assignment of a lexeme's most common tag  . 
Phrase identification : some examples
Sequencing , patching , and simplicity , the hallmarks of Brill's part-of-speech tagger , are also characteristic of our phrase parser . In our approach , phrases are initially built around word sequences that meet certain lexical or part-of-speech criteria  . The sequenced phrase-finding rules then grow the boundaries of phrases or set their label  , according to a repertory of simple lexical and contextual tests  . For example , the following rule assigns a label of o a ( ; to an unlabeled phrase just in case the phrase is ended by the word " Inc  . "( def-phraser labe JNONE ; phrase is currently ; unlabelled right-wd-1 lexeme " in c . "; rightmost word in the ; phrase is " inc . " labebaction ORG ); change the phrase's label , ; but not its boundaries Now , consider the following partially labelled string : < none > Donald F  . DeScenza </ none > , analyst with < none > Nomura Securities Inc . </ none > @ n processed ) ? lexlconlabelling : lookupj ~ transformatlons : ~ ? morphological rules J Figure  1: Brill's rule sequence architecture as applied to part mf-speech tagging  . 

The SGML markup delimits phrases whose boundaries were identified by the initial phrase-finding pass  . 
Of these phrases , the second successfully triggers the example rule  , yielding the following relabeled string . 
< none > Donald F . PeScenza </ none > , analyst with < org > Nomura Securities Inc . </ org > The rule , which seems both as obvious as walking and as fool proof comes from the name-findinig processor we developed for our participation i the  6 mMessage Understanding Conference ( MtJC-6 )  . As it turns out , though , the rule is in fact not error-proof , and causes both errors of omission ( i . e . recall errors ) and commission ( i . e . precision errors ) . Consider the case of " Volkswagen of America Inc . " Because the initial phrase labeling is only approximate  , the string is broken into two sub-phr~es separated by " of "  . 
< none > golkswagen </ none > of < none > America

The example rule designates the partial phrase " America Inc  . " as an out; , a precision error because of its partiality ,   , and fails to produce a not to label spanning the entire string  ( a recall error )  . 
< none > golkswagen < lnone > of < org > America Inc . </ org > This problem is patched by a subsequent name-finding rule  , namely the following . 
(def-phrasee label ORG left-wd-1 test country ? left-ctxt-I lexeme " og ' le%-ctxt-2 phrase NONE bounds-action MERGE labbel-ac~ion ORG  )   ; this is an organization ; is the leftmost lexeme ; in the phrase on a list ; of country words ? ; to the left of the ; phrase is the word " og ' ; to the left of that is an ; unlabelled phrase ; merge the entire left ; context into the OIZG ,   ; phrase and all The first two clauses of the rule are antecedent shat look for phrase such as " America in c  . " The next two clauses are further antecedents hat look to the left of the phrase for contextual patterns of form " < non ~>  ,  . , </ none > of " . 
The final two clauses incorporate the left context whole sale into the triggering phrase  , yielding : < org > golkswagen of America Inc . </ org > This rule effectively patchestile errors caused by its predecessor in the rule sequence  , and simultaneously eliminates both a recall and a precision error  . 
The phrase finder
With these examples as background , we may now turn our attention to the technical details of the phrase finding process  . As noted above , this process occurs in two main steps , an initial labeling pass followed by the application of a rule sequence  . 
Initial phrase labeling
The initial labeling proces seeds the phrase -finder with candidate phrases  . These candidate phrases need not be any more than approximations  , in partict dar , it is not necessary for these candidates to have wholly accurate boundaries  , as their left and right edges can be adjusted later by means of patching rules  . It is also not neccss at T for these candidates to be unfragmented  , as fragments can be reassembled later , just as with " Volkswagen of America Inc . " Further , applications that require multiple types of phrase labels  , need not choose such a label during the initial phrase-finding pass  . 
What is important is that the initial phrase identification Fred the cores of phrases reliably  , even if complete phrases arc not identified . That is , it must partially align some kind of candidate phrase ~ for every phrase  ( ~ that is actually present in the input . Extending a concept from information retrieval , this amounts to maximizing what we might call initial recall  , i . e . , lit = I (1) I I / I ( i ) I , where ( IJ is the set of actual phrases in a test set , K is the set of candidate phrases generated by the initial phrasing passs  , and cI ) I is tile set of those ( D < q ~ that arc partially aligned with some 1 ( cK . 
The general strategy we have adopted for finding initial phrase seeds is to look for either runs of l c x c mes in a fixed word list or runs of lexemcs that have been tagged a certain way by our part-of-speech tagger  . 
1 ) if fercn t instantiations of this general strategy for initial phrase labeling naturally arise for different phrase-finding tasks  . For example , on the classic " proper names " task in mixed -case text  , we havc achieved good results starting from runs of lexemes tagged with Nm  , or m ' ~ ps , the Penn Treebank proper noun tags . This strategy achieves the desired high initial recall RI  , as these tags are well-correlated with bona fide proper nanles ~ md are reliably produced in mixed -case text by our part-of-speech tagger  . This strategy does not yield quite as good initial precision  ( i . e . , it yields false positives ) for a number of r casons , such as the fragmentation problcms noted above , e . g . , golkswagen/NNP of/IN America/NNP Inc . /NNP Once again , though , these initial precision errors arc readily addressed by patching rules  . 

Clauee type Syntax Definition
Contextual tests
Phrase-internal tests
Label test
Actions left-ctx ~- l , ef ~ - ctxt-2 right-ctxt ~ l , right-ctxt-2 le %- wd-1 , left-wd-2 right-wd-1 ,   right-wd-2 wd-anywd-span label label-action bounds ~ action Test one place  ( resp . two places ) to the left of the phrase Test one place ( resp . two places ) to the right of the phrase Test first ( resp . second ) word of phrase Test last ( resp . next-to-last ) word of phrase Test each word of phrase in succession  . Succeeds if any word in the phrase passes the test  . 
Test entire string spanned by phrase
Test phrase's label
Sets the label of the phrase
Modify the phrase's ! eftor right boundaries Table h Repertory of unary rule clauses  . 
Phrase-finding rules
A phrase-finding rule in our framework is made up of several clauses  . The corc of the rule consists of clauses that test th c lexical context around a candid at c phrase  1< or that test lcxcmcs spanned by 1  (  . The repertory of these test loci is given in " Fable  1  . At any given locus , a test may either search for a particular lcxcmc , match a lexeme against a closed word list , match a part of speech , or match a phrase of a given type . Most rules also test the label of thc candidate phrase  1  (  . 
The unary contextual tests in the table may also bccombin cd to form binary orternary tests  . For example , combining I , EVT-C'IXW-I and i ~ mrr-cwxa'-z clauses yields a rule that tests for the left bigram contcxt  . This was done in theoredefragmentation rule described earlier  . 
A rule also contains at least one action clause , either a clause that sets the label of the phrase  , or one that modifies the boundaries of the phrase  . Finally , some rule actions actually introduce new phrases that embed the candidate m adits test context  ; this allows one to build nonrecursive parse trees . 
Phrase rule interpreter
The phrase rule interpreter implements the rule language in a straightforward way  . Given a document to be analyzed , it proceeds through a rule sequence one rule rat a time  , and attempts to apply r to every phrase in every sentence in the document  . The interpreter first attempts to match the test label of r to the label of the candidate phrase  . If this test succeeds , then the interpreter attempts to satisfy the rule 's contextual tests in the context of the candidate  . If these test succeed , then the rule's bounds and label actions are executed  . 
Beyond this , the only real complexity arises with phrase -finding tasks that require one to maintain a temporary lexicon  . The clearest such example is proper name identification  . Indeed , short name forms ( e . g . , " Detroit Diesel " ) can sometimes only be identified correctly once their component terms have been found as part of the complete naxne  ( e . g . , " Detroit Diesel Corp . ") . The converse is also true , as short forms of person names ( e . g . , " Mr . Olatunji " ) can help identify fitllnanm forms ( e . g . , " Babat unde Olatunji ") . 
The interprcter maintains a temporary lexicon on a document-by-document basis  . Every time the interpreter changes the label of a phrase $  , pairs of form < Z , " c > are added to the lexicon , where ~ is alc xcmc in ~ , and " c is the label with which (~ is tagged . This lexicon is then exploited to form the associations between short and long proper name forms  ( through an extension to the rule repertory defined above  )  . 
Correspondence to the regular sets
It is straightforward to prove that this approach recognizes a subset of the regular sets  , so we will only sketch the outline of such a proof here  . The proof proceeds inductiv cly by constructing a finite state machincbt that accepts exactly those strings which receive a certain label in the phrase-finding process under a given rule sequence Z  . We consider each rule p in Z in order , and correspondingly elaborate the machine so as to reproduce the rule's effect  . 
To begin with , consider that the initial phrase labeling proceeds by building phrases around lexemes  0~   1   . . . . . fzn in a designated word list or by finding runs of certain parts of speech ~ t  1   . . . . . 7 Zm . The machine that reproduces this initial labeling is thus pl/rq  . . . . . pn/n1 pl/nm . . . . . pn/nmPl/nl . . . . . pn/rqAs usual , node labeled " S " is th c start state , and any node drawn with two circles is , an accepting state . The Pi/~iarc labels stand for all lcxemes in the lexicon that may be labeled with the part of speech gJ ' The induction step in the construction procccds from ~ l  . bl , the machine built to reproduce Zupl~hrough rule l \] bl in the sequence  , and adds additional states and arcs so as to reproduce Zup throughruh '  . pi . 
For example , say Pi tests for the presence of a lexeme to the left of a phrase and e~tends the phrase's l x a undaries to include  ) v . We extend the machine bt to with a new one S ' , and adding a ~ , transition from S ' to the former start state S . Thus becomes
Pv,U>l@>O--->0
For a rule I ~ that tests whether a phrase contains a certain lcxcme ~' i  , wc construct an " acc cptor " machinc that accepts any string with  ) ~ i in its midst . 

Noting that the regular sets are closed trader intersection  , wc them proceed to build the machine that " intersects " the acccptor with bli  . 
Other rule patterns arc handled with constructions of a similar flavor--space considerations preclude their description hcre  . Note , how cw : r , that extending the fl : a mework with a temporary lexicon makcs it trans-finite-state  , lqnally , as with all semi-parsers , the machines we construct in this way must actually be interpreted as transducers  , not just acceptors . 
Learning rule sequences automatically
Our experience with writing rule sequences by lt , -md in this approach as been very positive . "\[' he rule patterns thcmselves are simple , and the fact that they arc sequenced localizes their effccts mid reduccs the scope of their interactions  . These hand-engineering advantages are also conferred upon learning programs that att cmp to acquire these rules at ttomatica \[ ly  . 
The approach we have taken towards discovering phrase rule sequences automatically is a maximum error-reduction scheme for selecting the next rule in a sequence  . This approach originated with Brill's work on part-of-speech tagging and bracketing  ( Brilli 993 )  . 
Brill's rule learning algorithm "\[' he search for a rule sequence in a given training corpus begins hyfirst applying the initial labeling function  , just as would be the case in running a complete sequence  . Following this , the learning procedurc needs to consider every rule that can possibly apply at this juncture  , which itself is a function of the rule schema laaaguage  . For each such applicable rule * ; the learner considers the possible improvement in phrase labeling conferred by r in the current state  . The rule that most reduces the residual error in the training data is selected as the next rule in the sequence  . 
This generate-and-test cycle is contimmd until a stopping criterion is reached  , which is usually taken as the point where performance improvement falls below a threshold  , or ceases altogether . Other a \[ ternativ cs include setting a strict limit on the number of rules learned  , or cross-testing the performance improvement of a rule on a corpus distinct from the training set  . 
The rule search space
The language of phrase rules supports a large number of possible rules that the phrase rule learner might need to consider at any one time  . Take one of our small cr training sets , in which the rearc ~9I sentences consisting of 6 , 8 IZ word tokens , with z , o77 unique word types . 
(ionsidcring only lexical rules ( those that look for particular words )  , this means that there are as many as I8 , 693 possibh ' , unary lexical rules (%077 x9 rule schemata ) , mad IZ , 941 , 787 bin at T lexical rules (? . , o77 z x 3 simple bigram rule schemata ) in the search space . 
However , by inverting the process , and tabulating only those lexical contexts that actually appear in the training texts  , this search spacc is reduced to z , : . I 9 unalTlcxical rules and 854 binary lexical rules . 
There are two substantively different kinds of rules to acquire : rules that only change the label of a phrase  , and those that change the boundary of a phrase . The latter prcsent a problem \[: or accurately estimating the improvement of a rule  , since sometimes the boundary realignment necessary to fix a phrase problem exceeds the amount by which a single rule can move a boundary -- namely  , two lexemcs . For thcsephrascs to be fixed there will have to be more than one rule to nudge the appropriate phrase botm daries over  . We handle this through a heuristic scoring ftm ction that estimates the wtluc of moving a boundary in such cases  . 
Error estimation methods
A rule that fixes a problem in some cases might well introduc errors in some other cases  . This kind of overgeneralization can occurearly in the learning process  , as new rules need only improve over an approximate initial labcting  . The extent o which a candidate rule is rewarded for its specificity and penalized for its overgeneralization can have a strong effect on the final performance of the rule sequences discovered  . 
We explored the use of three different types of scoring metrics for use in selecting the " best " of the competing rules to add to the sequence  . Initially we made use of a simple arithmetic difference metric  , y-s , wimrcy ( for yield ) is the number of additional correct phrase labelings that would be introduced if a rule were to be added to the rule sequence  , and s ( for sacrifice ) is the number of new mistaken labelings that would bc introduced by the addition of the rule  . '\[' his is Brill's original metric , but note that it does not differentiate between rules whose overall improvement is identical  , but whose rate of overgeneralization is not . For example , a rule whose yield is IOO and sacrifice is 7 ? is treated as equally valuable as one whose yield is only  3 ? but which introduce suo overgeneralization at all  ( sacrific e = o )  . This can lead to the selection of low precision rules  , and while small numbers of precision errors may be patched  , whole sale precision problems make subsequent improvement more difficult  . 

Scoring metric Training Test
Recall Precision P&R Recall Precision P&R Arithmetic  ( y-s )  88 . 88 I . z8+887 . 2 79 . 0 82 . 9 Loglikelihood 81 . 9 85 . 7 78 . 48 t . o73 . 4 77 . 0 Fmeasure , ~: o . 8 86 . 38 z .  9 84 .  5 85 . 08 I .  5 83 . z Table 2: Comparative contributions of three scoring measures after  100 learning epochs . 
(Training on i495 sentences from the MUc-6 named entities task )  . 
The next measure we investigated was one advocated by Dunning  ( I993 ) which uses a loglikelihood measure for estimating the significance of rare events in small populations  . This measure did not improve predsion or recall in the learned sequences  . 
The third scoring measure we investigated was the Fmeasure  ( Van Rijsbergen 1979 )  , which was introduced in information retrieval to compute a weighted combination of recall and precision  . The Fmeasure is also used extensively in evaluating information extraction systems at MUG  ( Chinchor I995 )  . It is defined as:
F = (32+1)PR(32 + P ) R
This measure is conservative in the sense that its value is closer to precision  , p , or recall , R , depending on which is lower . By manipulating the ~ paraaneter one is able to control for the relative importance of recall or precision  . Preliminary exploration shows that a ~ of 0 . 8 seems to boost precision with no significant loss in the longterm recall or Fmeasure of the rule sequences  . 
Table z summariz ~ es the contributions of these three error measures towards learning rule sequences for the  MUC6 named entities task ( for task details , see below ) . 

We have applied this rule sequence approach to a variety of realistic tasks  . These largely arose as part of our information extraction efforts  , and have been either directly or indirecdy evaluated in the context of two evaluation conferences :  MUC6 and Mffl ' ( for Multilingual Entity Tagging )  . In this paper , we will primarily report on evaluation conducted in the context of the  MuC-6 named entities task ( Sundheim I995 )  .   1 The named entities task attempts to measure the ability to identify the basic building blocks of most newswire analysis applications  , e . g . , named entities uch as persons , organizations , and geographical locations . 
Also measured is the identification of some numeric expressions  ( money and percentiles )  , dates , and times . 
This task has become a classic application for finite-state pre-parsers  , and indeed our work was in part motivated by the success that has been achieved by such systems in past information extraction evaluations  . 
We have applied a variety of techniques towards this task  . The easy cases of dates midtimes are identified by a separate preprocessor  , leaving numeric expressions 1We have also measured performance on several syntactic constructs  , ( e . g . , the socalled noun group ) , and on semantic subgrammars , ( e . o <, person-title-organization appositions ) . 
(also easy ) and " proper names " ( the interesting hard part ) to be treated by the rule sequence processor . 
Handcrafted Rules
We first approached this task as an engineering problem  , and wrote a rule sequence by hand to identify these named entities  . The rule sequence comprises I45 named-entity rules , Iz rules for expressions of money and percentiles  , and 6I rules for geographical complements ( as in " Hyundai of Canada " )  . In addition , the rules refer to a few morphological predicates and some short word lists--one such list  , for example lists words designating business subsidiaries  , e . g . , " unit " . The initial phrase labeling for the proper name cases is implemented by accumulating runs of NNP - and NN eS-tagged lexemes  . A similar strategy is used for number expressions  , using numeric tags . 
The performance of our handcrafted rule sequence is summarized in Table  3  , below , which gives component scores on the Mt3c-6 blind test set . The most interesting measures are those for the difficult proper name cases  . Our performance here is high , especially for person names . Our lowest score is on organizational names , but note that the system lacks any extensive organization amelist  . Aside from ten hardwired names , all names are found from first principles . On the easy numeric expressions , perform an cc is a hnost perfect -- precision appears poor for percentiles  , but this is due to an artifact of the testing procedure  .  2
Machine-crafted Rules
To evaluate the performance of our learning algorithm  , we attempted to reproduce substantially the same environment as is used for the handcrafted rules  . The learner had access to the same predefined word lists  , including the less-than-perfect TU'S't mR gazetteer  . 
Further , we only acquired rules for the hardest cases , namely the person , organization , and location phrases . 
We cutoff rule acquisition after theio othrule.
The results for this acquired ruleset are surprisingly encouraging  . As Table 3 shows , these rules achieved higher recall on the very hardest phrase type  ( organization ) than their handcrafted counterparts , albeit at a cost in precision . Overall , however , the machine-crafted rules still lag behind . When we incorporated them into our information extraction  2Our performance vis-a-vis other MUC6 participants placed us in the top third of participating systems  . Except for the absolute highest performer , all these top-tercile systems were statistically not distinguishable from each other  . 

Phrase type N
Organization 419
Person 34gl , ocation m 9
Money 74
Percent ~6
All phrases zt5o
Handcrafted rules
Recall Precision 85   87   94   94   94   87   99   97   tO0   6   7   9  ~  9 z
Overall t , ' = 91.2
Machine-learned rules
Recall Preckion 8779 7879
D 688883
Overall F = 85.2
Table 3: Performance on the MUC6 named entities blindt cst . 
system , the machinc-learned rules achieved an overall named cntitics Fscore of  85  . 2, compared to the 91 . 2 achieved by the handcrafted rttlcs , it should be noted , however , that the system loaded with these machine-crafted rules still outpcrfimned about a third of systems participating in the  MUc-6 evaluation . 
Multilingual evaluation ( MH')
After the Muc-6evahtation , then amcd entity task was extended in various ways to make it more applicable crosslinguistically  . Predictably , this was followed by a new round of evaluations : Mv : r  . The target languages in tlt is case were Spanish , Chinese , and Japanese . We applied our approach mall three . 
The Mt'l'cvahtation rcquir cdactual system performance resuhs to be kept strictly  , -monymotts , which precludes our reporting here any scores as specific as we have cited for English  . What wc may legitimately report , however , is that w c have effectively reproduced or bettered our hand-engineered English results in the Spanish mid Japanese t~ks  , despite having no native speakers of either language  ( and only the most rudimentary readings ldlls in Kanji  )  . In both cases , we were d~le to exploit part-of-speech tagging and some existing word lists fbr person names and locations  . 
For Chinese , although we had available a word segment cr , we had neither part-o6speech tagger , nor word lists , nor even the elementary reading skills we had for Japanese  . As a result , we had to rely a hnostentirely on the learning procedure to acquire any rule sequences  . 1) cs pitcth cse impediments , wccmnc dose to reproducing our results with thc English machinc-lcarned named entid cs rule sequcn cc  . 

What is most encouraging about this approach is how well it performs on so many dimensions  . We have only reported here on nature-finding tasks  , but early invcsti-gations in other areas arc encouraging as well  . With rule sequences that parse noun groups , for instance , we hope to reproduce the utility of other rulc -scqucnce approaches to text chunking  ( Ramshaw & Marcus I995 )  . We are also excited by the promise of the learning proccdure  , not just because it learns good rules , but d so because the rules it learns can be freely intermixed with hand-cngineered rules  . This mixed-mode acquisition is unique among natural language learning procc durcs  , mid we put it to good use in building our multilingual name-tagging sequences  . 
l ) espitcrcsuhs that comparc favorably to those of more mature systems  , this work is still in its infancy . 
We still have much to explore , especially with the learning procedure , lnd ccd , while the l cam cr induces /' tile sequences that pc rfi ~ rm well in t i maggr cg at c  , individual rules clearly show their mechanical genesis  . 
For instm ~ cc , wh cn the learner must breaktics between identically-scoring rule candidates  , it often does so in lhl guistically clums y ways . At times , the learner may acquire a good contextual pattern  , but may bcunable to extend it to closcly-related cases that would occur naturally malinguist  . 
We belic vethcs c problems arc solvable in then car ~ term  , and w c have partial solutions in place already . As our tccl miques mature , this validates not only ottr particular approach Io phrase-finding  , but the whole field of language processing through rule sequences  . 

Aberdeen , J . , Burger , J . , Day , D . , llir sehman , \] . , Robinson , P . , & Vilain , M . t995 . "Description of the Alembic " system used for MIJC-6"  . Ill Prcdgs . of ' MUC-6, (\] olumbia MD . 
Appch , I) . E . , t to bbs , J . R . , Bear , J . , Israel , D . , & Tyson , M . I993 . " I ; As TUS : A finite-state processor for information extraction fi'om rcd-world text  . " in Prcdgs . 
q'IJC At-93, Chantb & y , France.
Brill , E .  093 . A corpus-based approach m language learning . 1) octoral 1) issertation , Univ . of Pennsylvania . 
Chinchor , N .  094 . " Muc-5 evaluation metrics " . In
Prcdgs.t ~' MUC-5, Baltimore , Ml3.
Dunning , T .  093  . "Accurate methods for the statistics of surprise and coincidence "  . Comput . Ling 19 . 
Grishmml , R . 095 -" The NVu systemf in " MtJC -6 , or where's the syntax ?" Ill Prcdgs . of MOO-6, Cohunbia Ml3 . 
Jacobs , P . S . , Krupka , G . , & Rau , L . 199 i . " I . exico-semantic pattern-matching as a companion m parsing "  . 
in Prcdgs . of the Fourth Da Ue A Speech and Nat . Lang . 
Workshop , San Marco , CA : Morgan Kaufinan.
Ramshaw , I . . c/r Marcus , M .  095 . " Text chunking using transformation based l arning "  . \[ n Preys . of 3rd Wkshp on Very Large Corpora , (; ambridge , MA . 
Sundhcim , B .  095 . " Named entity task definition " . 
In Prcdgs.e ~ MUC-6, Columbia MD.
Van Rijsbergen , (' . J . I979 . Information Retrieval . 
London : Butt crs worth.

