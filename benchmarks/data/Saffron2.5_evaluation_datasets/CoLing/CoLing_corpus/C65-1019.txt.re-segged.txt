
1965 International Conference on Computational
Linguistics
PUSHDOWNSTORES AND SUBSCRIPTS
Jacob Mey
Lingvistisk Institutt
Universiteteti0slo
PoBo 1012, Blindern,
Oslo3, Norway.
t-~,,%%\

'-_Si "
Mey 2
PUSHDOWNSTORES AND SUBSCRIPTS
Abstract
Va`rious devices for the improvement of phrase structure grammars  ( PSG ) have been suggested recently . In particular , the PSG model with a pushdown store ( PSG/PDS ) as described by VoYngve , and the PSG with subscripts ( PSG/S ) as described by GoHarman are considered . 
It is contended that such devices , even if they may do away with some of the dif ficul_ties of PSG  , do not contain sufficient gene_rative power to produce the structurally corn_pl icated sentences that are generated by other g rammars  ( e . g . , of transformational type) . 
The handling of multiple disco , L tinuous con_stituents ( DC ) in PSG/PDS , as well as the use of : leletion rules in PSC  . /S is examined and criticized . It is shown that the improvements on PSG will not allow the grammar to generate aii the sentences of the language that a trans_format ional grammar  ( TG ) does ; more ovdr , the improvements on PSG are obtained only at the cost of introducing too much power at the PS level  , so that the improved gr ; ~mmars in some cases will exceed the require m @ nts of the de_scription  , i . e . generate non_grammatical sent_ences . 
Mey 3
O . Introduction
No Chomsky has argued that a PSG is not suff_ic ient to generate all the grammatical sent_ ences of a language  ( Chomsky 1957:3~ff . ) . 
Recently , this conceotion of PSG has been critici zed as being too primitive  ( Yngve 1960:445 a , Harman 1963:604 fro) , and several ways of improving such a grammar have been suggested : a PDS has been connected wi tha PSG  ( Yngve 1960 ,  1961 ,  1962) ; the use of subscript notation has been recommended to give PSG a fair chance in compet ition with 
TG ( Harman 1963).
i . PSG/PDS l.loPSG and DC
The problem of the so_called discontinuous sonstituents  ( for a detailed treatment , see Wells 1947:96ff . ) has always been a crux in IC analysis . One of the drawbacks of PSG as described by Chomsky  , is that it is not able to handle these consti tuents in a way that satisfies both the formal criteria of the grammar and the intu itive feeling that call and up in  , e . g . , I ca l led h im u ~ , belong to_gether and should be treated accord ingly in the analysis  . Chomsky , in his discussion of PSG limitations , admits the possibility of " extending the notions of phrase structure to account for di scontinuities "  ( Chomsky
Mey 41957:41), but , headds," . . . fairly serious difficulties arise in any systematic attempt to pursue this course  . " An attempt in this direction is described by V  . Yngvein several articles ( see especially Yngve 1960 )  ; although the presence of DC is the most annoy ing of the complications under the PSG model  ( Yngve 1960:448a )   , the solution ~ ffered to this particular problem implies a wider claim  , namely , that " any shortcomings/of PSG , JM/can be overcome " ( Ib . :445 a ) . Accordingly , I will discuss be_low not only the problem of DC  , but also the more general one of structure in a PSG/PDS  . 
1 o2 oDC and PDS
The crucial step in the derivation of DC by the automaton  ( for a full description , see Yngve 1960:448_9 ) is the question asked : Does the right half of the grammar rule in question  ( GRi ) contain the symbol " . . . "?( where " . . o " stands for " d i scont inu i ty in rewr i t ing the symbol on the lefthand side of the rule  "  ) If the answer is Yes , we have to roll out the temporary memory ( TM ) ta~e one space ( in a flow chart , one woul ~ sym_bolize this by the index not at ion  1 -- I --> i , where 1 stands for " leftmost ": " rolling in " tape would then be indicated by  1 +? --> i , see Fig .  ~) . During this operation , the original content of TM1 ( the leftmost loca_
Mey 5 tion of TM ) has to be kept in place , that is , the blank has to occur after the original TM  1   ( on the right side , if the tape is thought of as moving from the left  , see Fig ? ~) . If , how_ever , the answer is No , we have to make sure that we have space for all the symbols on the right hand side of the rule and roll out tape accordingly  . Let ~ be the number of symbols on the right hand side of GRi : then we can symbolize the roll ing out by the index formula i - -  ( n - - ~ the first symbol always goes to the computing register  . 
Let further ~ be the subscript for right hand side symbols of GRi  . The rest of the operation is then performed as routine counting on GRi  .   , 3 i being set at 2 ( the first symbol has already been taken care of  )  . There should , of course , be a proviso for the symbol " . . . " itself , so that it will not be copied onto the TM taoe . 
The method as described here will workneatly even in those cases where DC are " nested ~ that is  , if the expansion of some DC turns out to be another DC  ( and so on , at least theoretical_ly ) . As an example , one may try out the doubly discontinuous as far as the corner  , where all the necessary rules ares Declfied by Yngve himself  ( 1960:449 a )   . 
An implicit assumption throughout the descr ip_tion of the mechanism is that DC can be repres_ented by the simple formula A-->B+  9   , o + C . 
It follows that there are two cases that cannot
Mey 6 be handled directly by the machine : the f irst one can be symbolized by A-->B+  . . . + C + . . . + D ( " mul~plediscontinuous constituents " )   ; this reduces easily to double discontin ui ty by a suitable manipulation of the in out rules  . 
The other case could be labeled " discont inu_ous multiple constituents ": formula A - -> B + C +  . . o + D ( or some var ia t ion on th is theme ) , which would imply that the blank has to occur two spaces from leftmost in s ~ a d of one  . Foll_owing the instructions given by Yngve we would not obtain the right string of symbols in this case  ( as examples , one may try : He's not that bin a fool , or : As nice a little parl or as ever youd id see  , or the Soanish sentence : Hablamas de lo que sabe ' He talks more than what he knows '  ( Bolinger 1957:63 )   , where common sense would prefer the analyses that bi ~  . . . fool , as nice(? . . ~ ar lo r , mas de . . . que see diagrams in Fig .  2) , thus pre3erv ing analogy with construc_tions like such a fool etc  . The program could be accommodmted to perform this by combining a counting operation with the check on "" o  . o , where after the continuous part of GRi's r ight hand side could be thrown in with the non_DC rules  . Derivation being different , there would be no interference from const ructions like that big fool  , that are treated in the normal way by the mach in e  . 
A device like the one described here will , within its obvious limitations , be able to randomly generate sentences that are for the most part quite grammatical  ( Yngve 196 2:70 )   . 
Mey 7
The question is : will it generate all , and only , the grammatical sentences of a fang_uage ? I will try to answer this question in the next paragraph  . 
1.3. Limitations of PSG/PD ~
Although the model as proposed by Yngve in its original form only uses the PDS technique to solve a minor problem in syntactic analysis by the machine  , the scope and use of PDS are by no means limi ted to this particular pro_blem of DC  ( For a detailed discussion , see Oettinger 1961:126_7) . The elegancy and sim_plicity of PDS algor ithms make them well_suited for procedures of automatic syntactic analysis of languages  . 
There are , however , some inherent limitations . 
Common to all PDS techniques is the fact that in formation stored in this way only is access_ ible in accordance with the formula " last in  , first out " . Being essentially a linear array of info rmation  ( Oettinger 1961:i04 )   , the user ( the machine ) will not be able to draw on other informat i on than is given by the leftmost sym_bolin a left_to_right production  ( the tem Do_rary memory tape in Yngve's machine , see Fig . l ) . 
Since , on the one hand , the machine output is past control ( what is Drinted , is no longer available to the machine for inspection  ) and , on the other hand , the internal state of the machine is entire ly determined by the current input symbol  , one has to keep careful account
Mey 8 not only of the current derivatlonal steps , but also of the " left_overs " from earlie r steps  . This is exactly what a PDS can do , and the ~ roblems in connection with this technique are  , as shown above in the case of the so_called di scontinuous multiple constituents  , are mainly technical ( provid_ing indexes etc . ) The linear character of the memory , however , together with the finite state or operties of the model itself give rise to a ~ other prob lem that seems unsolvable under the following ass_umptions for our machine : a finite number of states  , a linear temporary memory , and a transition from one state into anether by one_symbol in out  . The problem is the following : given any internal state of th ~ achine that is determined by more than one symbol simultane_ousiy  , will the supplementary device of a PDS be able to suDply the necessary instructions to the machine that are not contained in the cur rent symbol ? The answer is in the negative  , precisely be_cause the memory is linear , and there is no " look_up " for items in the memory ? What is stor_ed in the memory can only be brought up to the surface by something outs ide the memory itself  , that is , I have to create an " expectancy " that is spec if ic for each item in the PDS  . Only under these conditions the state of the machine can be defined as determined by the current symbol plus theo ontents of the temporary mem_ory  ( Yngve 1960: ~49 )  . This is essentially the
Mey 9 procedure described by Harris for keeping t rack of nested constructions  ( " incurrence and discharge of requirements " , Harris 1962:53) . The reason why the machine is able to handle DC is that this " nesting " occurs in one level  , so that the symbols involved can be uniquely determined as belonging to the same dimension of analysis  . 
Where " surface structure " is explained only by underlying " deed structure "  ( Hockett 1959:246 ff . )  , the machine will not be able to carry out the analysis correctly  . The structure that underlies a symbol X 1 may be bound up with a special PS derivat ion  , so that rules concern_ing structures like , say , X 1 + X 2 + X ~ will be ambiguous in their a ~ plicat ion  . One could place restrictions ( in Harris ' sense ) on ( one of ) the symbols , thus creating a multiple path through the derivation  , possibly combined with a cycling device : this is what the subscriot technique does  , see 2 . 4 for a detailed discuss_ion . Some of the difficulties are removed in this way  , but others persist , like those cases where pairs of symbol formulae are involved  ( the so_called " ~ eneralized transform at ions " of early TG  , Chomsky 1957:113) ; this point is also discussed below . While placing too many restr_ictions on the symbols has serious disadvant_ages  ( some of which will be discussed in sect_ ion  2 of this paper )  , it certainly exceeds the capacity of the model as described by Yngve : his rules are all of the context_freeform  . 
Mey I0
Thus , structure in a sufficiently powerful PSG is not only a matter of specifying the right rules  , but also of choosing the right rules and combining them at the right places  . 
There is still another factor that we have left out of consideration so far : the order_ing of the rules  . Y ngvestates that any order will do : an a lphabetical order may be conven_lent  ( 1960:445 ) oNOW this has two consequen_ces : first , all of the rules have to be run through every t imea symbol is expanded  ( per_haps only a minor drawback in a computer_o riented analysis  )   , second , the advantages of ordered rules ( economy , elegancy , accuracy ) are lost ( " forcing all kinds of low_level detail into the rules "  , Bach 1964:53) . Besides , ordering of the rules is indispensable in cases where complicated high_levels t ructural descriptions are involved : thus an immediate derivation of each non_terminal symbol all the way down to word level would not be permitted in any kind of PSG  , not even the most context_sensitive ones . Being es~entially context_free , Yngve's grammar will 6~enerate what is usually called " kernel sentences " ( Chomsky 1963:152 ) : unambiguous derivation of more complex struct_ urea  ( derived sentences ) will only be feasible under a careful speci fication of the order in which the rules have to apply  ( as an example , cf . the discussion of w__hh_transformations as depending on the interrogative transform at ion in Chomsky  1963:140  )  . 
Meyii
There is another way out of the difficult ies that have been sketched in this section : phrase_structurizing at different levels  , these being kept together by the representation relation  ( see Sgall 1964b )  . This solution is based on a somewhat diffe rent interprd ~ tion of PSG functions  ( not only syntactic , but also semant_ic rules az'e included ) ; a PDS is coupled with the PSG of the lowest level  . A detailed dis_cusslon of this system will have to wait for more details  , but it seems that grammars based on dependency re lations have received too little attent ionsofar  ( for a compalison of IC and dependency the or i es  , see Hays 1964:519_22) o1 . 4 . Grammar and psycholqgy Referring to exper iments performed by G  . Ao Mill ~ r , Yng veestablishes an analogy between the " depth " of memory ~ n the human brain and the depth of sentence construction in the model  ( 1960:452 )   . The human brain is not capable of stvrlng more than  , say , seven plus minus two items at a time ( for references , see Yngveibid . )  . In other words , the human brain has a limited capacity , just like the temporary mem_ory of Yngve's machine  . One of the conditions to be put on a flawless handling of " deep " constructions is that the storage capacity is not exceeded by the number of symbols to be developed later on  . In this connection Yngve makes the in ~ ere ~ ting observation that senten_ces and constructions in general actually do 
Mey12 have a sort of limited depth , i . e . the number of regressive nodes is bound by more or less the same u operlimit as that for human memory's simultaneous storage caoa city  . 
Now , I think that the analogy between the two kinds of " storage " should not be overstressed  . 
It rests primarily on the tacit assumption that the model should  , or could , be considered as a more or less true_to_li fere presentation of human linguistic activ ity  . As I have remark_ed before , this supposition is altogether groundless , and will at best hamper an exola_nation of such activity in truly linguistic terms  . A remark made by Yn~ve in this connect_ion may c larify the issue  . Yngve says (1960:452b ; see also 1961:135_6 for an even more ex_plicit commitment ) : " The depth limitation does not apply to a lgebra  , for example , because it is not a spoken langua_ge . The user has paper available for tempmrarys to rage  . "But so has the user of any other language , e . g , human everyday sooken language . The fact that we do not use paper actually when speaking has nothing to do with greater or lesser depth of sentences  ( or , if it does , the depth differences occur only to one side , namely that of decreas_ing depth ) . One could pursue this analogy a dab_surdumby assuming two kinds of depth  , one un_limited , for written languages , and one limited , for spoken languages . The results would be dis_astrous for any description of any language : sentences of the type : " That that that they are 
Mey 13 both is osceles is true is obvious isn't clear " ( Yngve 1960:458b ) are as ungrammatical in written as they are in spoken English  . Of course Yngve is perfectly right in attr i but_ing the difference between the above non_grammatical  ( deep regressiv ~ that_clause and its grammat ical  ( progressive ) counterpart : " It isn't clear that it is obv ious that it is true that they are both is osce les " to ex_cess depth  . So , there is a depth limitation and this limi tation is gramatically relevant  . 
But this linguistically fruitful concept should not be confounded with hypotheses from des_criptive psychology  . 
That the claim for descriptive similar i tybe_tween psychology and linguistics is latent in Yngve's model can be seen from another instan_ce  . \] I is second assumption for the model ( 1960: 445 ) is that " the model should share with the human speaker o  . . the prooer ty that words are ~ roduced one at a t ime in the proper time se_ quence  , that is , in left_to , right order . . . " ( the first assumption , vim . that any short com_ings of the PS model can be overcome  , hasDart_1y been dealt with above , and will be treated at length in the second ha lf of this paper  )  . 
This restriction , I think , on a model ( or a grammar , insofar as the grammar is based on the model ) is unnecessary and self_contradict_ory . It is unnecessary , since the model should only copy relevant tra its in the speech pro_duction of the indiv idual  ; and even though it may be true that words are produced in a linear 
Hey14 sequence ( as already Saussure has remarked )   , it has not yet been shown how this linearity is to be interpreted in human speech production : I think it is only weakly relevant  , that is tossy , linearity alone will never suffice to g ive a complete picture of the speech event  . 
For a full_fledged description of speech I suppose the assumption that we speak in senten_ ces rather than in words will have many advant_ ages  . 
Moreover , the claim that the model should du_plicate the property of left_to_right product_ i on in the human speaker cannot be brought to  harmon2ze with the model . In fact , the model can only examine one symbol at a time : the machine mayerase or delete or read only that section of the memory tape that is closest to the roll  , i . e . the leftmost symbol only ( Yngve 196 o:446) . 
Now , the limitation of human memory is on re_I ) roducing more than a certain number of items at the same time  . The analogy clearly does not hold between human memory and machine storage : the explan at ion is that the machine produces symbols  , whereas the speech of humans is struct_ured . In other words , a left_to_right product_ion may in many cases be explained by a linear structure in the or oducer  ; the pushdown store is a linear memory device . But there are other left_to_right product ions that are structured in such a way that a PDS or other left_to_right arrangements will not suffice  . It is of course true that a structural descr iption is not alto_gether absent from a PSG /PDS : Yngve's machine produces as its output a string of symbols 
Mey15 containing both syntactical markers ( " flattened_out trees " ) and terminal symbols . This will suffice to " infer the derivat ional history of each string from that s tring in a single way "  ( S ~ all 1963:41 )   , but only insofar as the struct_ure can be described in one_level terms  , cf . 
discussion above ( see also Sgall 1963; 1964a ) . 
The question will be treated at length in part two of this paper  . 
2 pso/s 2 . 1 . The subscript notation The subscript method referred to here is not in the first place thought of as a machine v program  ( even though its close affinity ~ ith the computer language COMIT is asserted  , see Harman 1963:608 fn .  )  . Accordingly , it has a more general scope : namely , to offer a full_fledged alternative , in PS form , to other grammars ( e . g . of transforms tlonal obedience ) , thereby proving that " transformation alg ramm_ar has no advantage over the phrases t ruct_uregrammar "  ( Harman 1963:598 )   . 
? ubscripts are added to the PSG rules in two ways : first  , to introduce restrictions on such rules , second , to s : ~ ecify where those restr_ictions apply  . An example of the first kind is the rule S-- >   S1 /NUMBER_SG ( Harman 1963:609 )   , and , in general , any rule of the type A-->B/J + o , o . The second case obtains e . g . in the following rule : NP/NOT_WI--> DETER MINER+NOUN  , and , of course , in all rules where subscripts
Mey 16 are " lost " during expansion . I think there will be a third type as well , eventiough this is not expressly mentioned in the artic Le  , namely , sub_scripts that do both : introduce new subscripts at places indicated by old ones  ; but this is on_lyamin or point . More important is the obser_vation that subscr ipts can take care of all sorts of const itue Jlts  , both continuous and dis_continuous . For the latter , the generation ru_les are adapted Prom rules suggested by Victor Yngve  ( IIarman 1963:606 ; the reference quoted is Yngve 1960) . Like in Yngve's model , the rules of PSG/S are unordered : all necessary informa_tion about when and where to a ' iply a rule is contained in the subscripts  ( which , by the way and perhaps a fortiori , are said to occur in an unordered sequence ) . But , as will be seen from the following paFs graphs  , this " when " and " where " is not only a not at ional problem : in fact  , it is one of the big underlying diffe rences be_tween PSG and TG  .   ( 0n the d i f f i cu l ty of o rder ing ru les in a PSG  , see Chomsky 1957:35) . A further important difference from other PSG interpreta_tions is the admission of de letion rules  , that is rules of the form A-->@ ( Harman 1963:60 ~ )  ; also this point will be discussed at length below  . 
2 . 2  . Subscripts And Transformations In general , One cannot deny the possibility of incorporating  ( by means of subscripts or other devices ) some of the information that is con_tained in a transformational grammar into a 
Mey 17 ~ ne_level grammar of PS type.
But the grammar thus constructed will never generate all and only the grammatical sent_ enc~s of the language  . Either it will generate too little ( the normal case for PSG without subscripts or similar devices  ) or , if it gen_erates more , it will also generate some non_grammatical sentences  ( Harman 1963:611:" .   .   . 
not all sentences constructed in accordance wi th this grammar ' are well_formed  . ") A very simole example will show this . Supoose we ~ ant to transform optionally a sentence in _ to its question counterpart  . To do this in the PSG/S according to Harman , we have to choose an appropriate expansion of the symbol  $2   ( the same paths hold for number_and mode_restr icted S :  S1  , resp . S2 , Harman : 600) , na_mely either the second or the fourth rule in  3  . , the set of expansion rules for $2 . We choose the second rule ( normal question , the fourth rule concerns wh_questions ) : S2-->VP/TYPE_QUES , NOT_WH+NP/CASE_NOM , NOT_WH . 
Now , note two things : in order to conform to the ru les for this grammar  , we have already added some of the subscripts from Rules  1 and 2 to the symbol $2   ( e . g . , NUMBER_SG and MODE_ACT) . 
These subscripts , together with the new ones , are to appear on every symbol that is contained in every rule from now on  ( unless a delete sub_script is introduced , cf . below ) . This is nec_essary , since we cannot let any information that is conveyed by the subscripts be lost  , even if
Mey 18 it beir relevant to the symbol in question ( such as , say , a MODE restriction on a NP ) . One can easily imagine that rewrite rules of this type so on become very unwieldy  ( even if we do not allow , urselves to be frightened by the prospects of " millions of rules "  , Harman : 605) . Thus , in rule 7 of this comparatively simple grammar weal ready have  6 subscripts to each symbol . This number is substantially increased in the more elaborate version of the grammar  ( see Appendix to Harman's article )   . This is certainly not what one would calls implicity of description  . 
\[ Jut objections of this kind can be met by the following consideratiun : even if the multiDli_cation of entia  , i . c . symbols and subscripts , seems without rationale for humans , one can conceive of it as 8 necessity for computer data handling , and the computer certainly does not mind go ing through all the subscripts  , adding some , deleting others , etc . , every time a sym_bol is mentioned or expanded . So , if one has a working program in which these res trictions can he written out as subrout in es  , and if the com_i ) uter space needed does not exceed that avail_ able  , the objection just made does not hold ( cf . Harman : 61 Of n . : " Many of these grammars are in the form of computer programs for generating actual sentences  . ") The other question is far more important . It can be split up into ~? oparts : i . Can all the data of the grammar be put into the subscript_restriction schema ?  2  . Will the subscript_restriction schema not 
Mey 19 put more data into my grammar than wnnted ? The f irst question concerns the adequate re_ presentation of the structure  , the other ex_presses the fear that I may adds tructure to my grammar  , thus or oduc in ~ sentences that are not grammat ical  ( see Chomsky 1962:514 ff . ) Adopt in ~ a distinction made by Chomsky , I make the following assertion : APSG/S wil l serve as a more or less adequate observation a l and descriptive representation of the facts covered by a normal PSG  ; as far as TG is concernd d , the structure of the transformational mode l  ( how trees mad into trees ) will not be represented adequately on the descriptive  ( and perhaps not even on the observational ) level by a PSG/S . 
In no case the PSG/S will attain the level of explanatory adequacy  . 
The first Dart of my assertion can easily be proved from the observation that a normal PSG and a PSG/S are strongly equivalent grammars  , the only difference being the notation . ( On the notion of equivalence , of . also Hays 1965:519) . 
In fact , it makes no difference whether one ex_p and s a symbol on the basis of a rule to be a f_f i xed to the constituent by means of a sub_scr ipt  , or on the basis of a rule contained so_mew here else in the grammar  . The essential is that ~ eration proceeds from left to right  , and one symbol is F ) roduced at a time . ( See discuss_i on above , 1 . 2) . 
To Drove the other half of the assertion male above  , I will try to give an answer to the two_
MeT 20 fold question about representation of struc t_ure  . Let's go back to the elementary example of the optional T  , and try to imagine how this q is handled in a PSG/S  . The main difference be_tween PSG and TG is that the rules in PSG oper_ate on symbols  , in TG on strings of symbols . 
When I put a subscript on a symbol that is part of a string  , and I want to mark off a struct_ure that is based on several symbols occurring in a certa in order  , I will have to mark a Iithesymbols of mys tring in the same way  , and this way of : ~ larking must be unique , i . e . de_finea unique path through the rules . This path may , in due course , require additions , deletions , permutations and the like . Now , in TG these op_erations are carried out af ter the PS deriv & tion has been completed  . In PSG/S , ho ~ Tever , the cleavage between affirmative and inter rogative sentences occurs already in the thi rd rule  , where $2 is expanded into NP+VP , VP+NP , respectively ( omitting the subscripts ) . The two derivations follow separate paths through the rules : in terms of tree diagrams  , what is left in the one is right in the other of the two trees  , In this way , many PSG rules are un_necessarily duplica ted  ( see above )  ; moreover , the relationship between interrogative and de_clara ~ ive sentences  , as defined in TG , is reduced to a remote common source of derivat ion  , namely $2 . It is not true that " Sentences are ~ rans_fo rmationally related ' to the extent that the 
Mey 21 same choice of restrictions is made in the ir derivations and if the same lexical c '  , oices are made where i ~ ossible " ( Harman : 608 sincle quotes are his )  , unless one takes "' transfor_mationally related '" in a sense rather differ_ent from Chomsky's  , namely : sentences that have a ( partial ) oath through the rules in common . 
This is , in fact , the only ' transformational relation ' that it is possible to define in a PSG/S  , but unfortunately , it is not transfor_mational . Even in the case that two paths coin_cide , and coincide altogether , we do not have ' transformational rela tedness '  , but " grammatical similarity " ( Harman : 6OS )   . Lexieal choices have nothing to do with this relation : both in PSG and in rG the choice on the lexical level is made after the ao Dlic at ion of expansion  , reso_ectively transformational rules . ( This is not altogether cor ~ . ect : lexical choices may be made earlier and thus affect the derivation  , but this is beside the point ; complex symbols ( see Klima 1964 ) are not taken into consideration here , but they could be built into a PSG as well as into any other generative grammar  . I think , e . g . , that some com , ~lex symbol could be devised to prevent sentences like Theman walks the men  , that could easily be generated in accordance with the rules described on TIp  . 
609_10 of Harms n's article.)
In my opinion , a PSG/S will never be able to show trans formational relationships as formally de fined and described by Chomskyando Zhers  ; hence such a grammar , even though it may attain
Mey22 a certain descriptive adequacy , will never give an explanation of the fact that precisely this  , and not some other sentence , is t ~ ansformed into another structure . 
2 o 3. Deletion in a PSG
Another difficulty in PSG/S concerns the problem of deletion rules  . In normal PSG , no deletes are permitted ( Chomsky 1961:9 ) oHarman gives as reason for this restrict ion that trees must be uniquely recoverable in a I~SG  ( p . 603)  . This is , however , only part of the motivation . Deletes are not symbols : they cannot be expanded  ( un_less one chooses to ex ~ ) and them into deletes , which is obviously useless in a descript ion  ) ? Whenever a deletion rule occurs , the structure of the derivate is altered in such a way that rules may a ~ ply which orig in ally should not  . 
One could say that deletes are extremely con_ text_sensitive : in ! at  ; nan's PSG/S , which in reality is a highly restric ted PSG  , the number of rules having the form A-->Z is very limited indeed  , even though the author advocates their use (9 . 605)  . In passing , I would like to remark that nearly all of the deletion rules have to do with the ex  , o ansion of NP/I~H ( this subscript occurs only once in the smal ler ~ rammar  , p . 609 , and should therefore be rejected by the mach in e  , since there are no constituents on which the rule could a ~ ply  .   ) The real reason why a delete cannot be admit ted in a PSG  ( especially a highly context_sensitive
Hey23 one ) is that the rules following the deletion rules should be modified or alte ~' ed completely  , otherwise it would not be possible to keep the distinction between not_rewritten and re_written symbols clear : the rules following de_letion might thus operate on symbols or if ~ inally belonging to the context  . ( Note , by the x?ay , that in the case of wh_words the context l ~ ro_b lem is somewhat simplified by the fact that these words normally stand at the beginning of a sentence  , so that the left context can be thought of as zero  . ) In our example , the transformational rule for interrog at ivesent_ences to be generated from declar at iveones operates on a string of symbols that may be symbolized X  1 _X 2 _X 3   ( Chomsky 1957: i12 )   , carrying it into the shade X2_XI-X3 . 
Now , suppose that in the course of the deriv_R tion to non_termins l symbols  ( the kernel string ) we have a deletlon rule operating , say , on " ( Suppose more over that the non_terminal i ? symbo l following X  3 qualifies for the condi_tions origina llyput on X  5  . The transformatio_nal rule will then operate on a string X  2 _X ~_ _ X4  , and carry it into X3_X2_X ~ , thus generating a non_grammatical sentence . I do not pretend that the actual PSG/S as Drooosed and described by Harman in his arti cle ~  , , ill generate these sentences : as a l ready sai ~ l  , the grammar makes a very cautious use of delet ions  , so that sentences like the ones mentioned wil l not occur  . This does not , ho ~ , yever , invalidate the criticism . 
Mey 24
Subscripts may not only be added in APSG/S , but also deleted . In this manner a restriction that has been put on a certain rule can be re_moved  ( this deletion of subscripts is of course qu ite another matter than the deletion of sym_ bols discussed above  )  . Subscripts may besu ) er_fluous , such as in Rule 8 . 1(p . 609) , where the subscript AUX_MODAL is removed from the constituent INFINITIVE by the subscrip t--AUX_MODAL  , even though the lexicon would offer no ambiguous rewrites in the case of a non_ removal of the superfluous subscript  . One could perhaps wonder why this precaution is taken  , since in many other instances superfluous sub_scripts persist all the way through the deriva_tion  ( see discussion above )  . In other cases , the removal of subscripts can be motivated by the desire to or event ungrammatical " loops  "  , i . e . endless recursive expansions that have no jus tification in the grammar  . Thus in Rule 8ol the symbol VP3/AUX_MODAL is expanded into INFINITIVE / ? .   . + VP3/AUX_HAVE , -- AUY_MODAL , thus preventing another expansion by the same rule of VP  3  . If , on the other hand , we wish the symbol in question to be expanded recursively  ( and according to the latest develo , )ment in TG there should be no d i f f i cu l ty in admi t t ing recurs lv i ty for all symbols  , Snot excluded : see Klima 1964) , we can restart the cycle by wiping ours late  , i . e . deleting all the sub_scripts by means of the instruction ERASE  . 0 TIIERS , to be incorporated as a subscript on the right
Mey 25 hand side of the rule oNaturally , we would ex_pect a subscript of this kind to occur in those cases where a whole sentence is to be embedded into another hymeans of what in early TG was called " generalized trans formations "  ( Chomsky 1957:113 ) oTh ~ ominalizing transformation is an ins tance i ~ ind " under ~ g in the extended PSG/S  ( p . 613) , we find , among others , the entry : NP8--> Sl/CLAUSE . TYPE:NOMINALIZATION , SUBJ . INoGENITIVE , B , C , D , E , Z , Y , ERASE oO THERS This means that all the subscripts originally found on  NP8 are to be deleted ; the new sub_scripts deal exclusively wi th the derivation of the embedded clause  ( as can easily be veri_fled from the rules of the PSG/S as given in the Appendix of the article  )   .   1~'hereas TG keeps track of the chan~es to be made by means of a structural description of the pair of kernel sentences involved  , together with a formula for sh ~/ ctural change , in PSG/S we have only a con_stituent NP to be expanded by means of DS rules ? How this NP fits into the stmucture of the ori_ginal kernel sentence  ( being essentially its path through the PS der ivation  ) can be fo\]low_ed in . nSG by tracing back the nodes of the tree representation  . In PSG/S , this path is marked by the subscriots added to the NP in question  . 
Now , all this information is struck from the record by the removal of the subscripts in a c  , : ordance with the instruction ERASEOTHERS ? ~stru Cturaldescri !  ) tion of the sentence as a whole is not availab le : the expansion of  NP8 destroyed our bridge back to the original So It is as if we ha~e en expanding a constituent while forgetting what it was we were expanding  . 
Mey 262 o Conclusion
Of the two models discussed here , the first one ( PSG/PDS ) has not actually been proposed as a full _scale grammatical mo:Iel  , but I have tried to show that the implicat ions of the claim that any shorgcomings of PSG can beow : rcome lead to difficulties of about the same nature as those encountered in the second molel  ( PSG/S )  . 
Descriptive adequacy is not attained in those cases where structural descriptions are rele_vant for the operation of the rules : neither PSG/PDS nor PSG/S permits one struc tural descr_iption to be carried over into another  . As one will have noticed , the argument in both cases runs alon ~ the same lines  . Moreover , of the several devices proposed by Har : ~ anto boost the  . ) ower of PSG , the deletion rule was explicitly rejec ted on the ground that it would add too much power to the ~ rammar o On the other hand  , the use of subscripts , no matter how carefully chosen , will not help enlarge the descriptive Dower oi " the gramm  , ~ r ( Harman 1963:605 ) enough to account for all the grammatical sentences of the language  . Thus , one_level grammars like the ones discussed above will not attain explanatory adequacy in any case  , and in some cases not even descriptive adequacy  . "Dieser Versuch/namely , the defense of phrase structure , JM/verfehlt denents cheiden den Punktabet in zwei facher in sicht : Ersten suberschrei ten die Regeln Harmans die Kapazitateiner PSGoUndz weitens losen such sienicht das P roblemeiner geigneten Zuordnungyon Stammbaumen  . "( I ~ ier wisch 1964:49fn . ii )
Mey 27

Chomsky 1957:
Chomsky 1961:
Chomsky 1962:\]larman 1963:
Harris 1962:
Hays 1964:
Hockett 1959:
Bach 1964: E . Bach , An Introduction to Trans_formational Grammarst Ne  , ~' York etc .  ,
Grnmmatik ( Preorint IId Internatio_nal Symposium " Zeichen und System der Sprache  , Mag deburg , Germany , 
Seotember 1964)
Bolinger 1953: D wight Lo Bolin ~ er , Addenda to the
Comparison of Inequality in Spanish
Lg .29 (1953), 62_6 ?
N . Ao Chomsky , Syntactic Structures
The Hague , 1957
I d . , On the Notion Rule of Grammar , in : Structure of Language and its Mathematical Aspects  , ~ SAMXII (1961) ,  6_24 , Providence , RoI . , 1961
Id ., A Transformational Approach to
Syntax , in : ThiFd Texas Confersnce on Problems of Linguistic A/~aiysis  ( 1958 )  , Austin , Tex . , 196~, 124_58
Gilbert II . Harman , Generative Grammar without Transformation Rules : A De_fense for Phrase Structure Grammar  , 
Lg .39 (1963), 597_616
Zellig S . Harris , Strin 6 Analysis of
Sentence Structure , Theague , 1962
David Go Hays , Dependency Theory:
A Formalism and some observations,
Lg 40 (1964), 511_25
Charles F . Hockett , A Course in
Modern Linguistics , Ne ~, York , 1959
Mev 28
Klima 1964: Eo SoKlima , Current developments in Generative ~ rammar ( in press )  . 
AMScoDy of this 0a 0er , which was originally read before the 1764
Colloquium on Algebraic Linguistics,
Prague , Czechoslovakia , waskindly put at my disposition by the author . 
Oettinger 1964: A . G . Oettinger , Automatic Syntactic
Analysis and the Pushdown Store , in : PSAMXII (1961) , 104_29 Sgall 1963: U?Sgall , The Intermediate Language in Machine Translation and the Theory of Grammar  , in : American Documentation Institute , 26th Annual Meeting , Chica_go , Ill . , 1963, 41_2 . 
Sgall 1964 a:P . Sgall c . s . , Cestymodernijazy kovedy,
Praha 1964
Sgall 1964b : PoSgall , Zum Verhaltnisyon Grammat kk und Semantik ( Pre Drint IId Intez'n . 
Symp ., Mag deburg , Germ ,, 1964)
Yngve 1960: VoH . Yngve , A Model and an Hypothesis for Language Structure , in : Proceed_ings of t ~ , American Philoso Dhical
Society 104 (1960), 444_66
Yngve 1961: Id ., The De~th Hypothesis , in : PSAM
XII (1961), 130_8
Yngve 1962: Ido ,, Random Generation of English
Sentences , in : 1961 International
Conference on Machine franslation of
Languages and ADplied Language Analysis,
London 196 2,65_8 1
Wells 1947: Rulon SoWells , Immediate Constituents , 
Lg2~(1947), 81_117
Mey 29
I\]l+i I+2"~OLL . IN"i--~I"ROLL.(N/T "
FIG . I . THET~I~MPORA~y MEMORY th@~big as nlce-\]a fool j ! ~ j little parl or m~s de Io que sabe FIG  .  2 . DISCONTINUOUS ~ IULTIPLECONSTITUENPS
