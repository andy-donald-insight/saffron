A Rule-Based Approach to Prepositional Phrase Attachment 

Eric Brill Philip Resnik *
Spoken Language Systems Group Sun Microsystemsl , aboratories , In (: . 
Laboratory for Colnputer Science , M . I . T . Chelmsford , MA 01824~1195 U . S . A
(\] ambridge , Ma . 02139 U . S . Aphilip . resnil;(~east . sun . com


I : n this paper , we describe a new corpus-based approach to prepositional phrase attachment disambiguation  , and present results coln paring peffo>mange of this algorithm with other corpus-based approaches to this problem  . 

Prel ) ositioual phrase attachment disambiguation is a difficult problem  . Take , for example , the sen-rouge : ( l ) Buyaear\[p , o with a steering wheel \] . 
We would guess that the correct interpretation is that one should buy cars that come with steer-ing wheels  , and not that one should use a steering wheelas barter for purchasing a car  . \] n this case , we are helped by our world knowledge about automobiles and automobile parts  , and about typical methods of barter , which we can draw upon to correctly disambignate he sentence  . Beyond possibly needing such rich semantic or conceptual int ' or nla-tion  , Altmann and Steedman ( AS88) show that there a , recertain cases where a discourse model is needed to correctly disambiguate prepositional phrase atta  . chment . 
However , while there are certainly cases of an > biguity that seem to need some deep knowledge  , either linguistic or conceptual , one might ask whag sort of performance could 1oe achieved by a system thai uses somewhat superficial knowledge au-*Parts of this work done a  . t the Computer and h Plb rmation Science Department  , University of Pennsylvania were supported by by DARPA and AFOSR jointly under grant No  . AFOSR-90-0066, and by ARO grant No . DAAL03-89-C0031 PR\[ ( first author ) and by an IBM gradmtte fellowship ( second author )  . This work was also supported at MIT by ARPA under Contract  N000t4-89-J-la32= monitored through the Office of Naval resear <: h ( lirst a . uthor) . 
tomatically ~ xtracted from a large corpus . Recent work has shown thai ; this approach olds promise ( H \] ~ , 91 , HR93) . 
hit his paper we describe a new rule-based approach to prepositional phrase attachment  , disambiguation . A set of silnple rules is learned automatically to try to prediet proper attachment based on any of a number of possible contextual giles  . 
Baselinellindle and Rooth ( IIR 91 ,  1\[1793 ) describe corpus-based approach to disambiguating between prepositional phrase attachlnent to the main verb and to the object nonn phrase  ( such as in the example sentence above )  . They first point out that simple attachments rategies snch as right association  ( Kim 73 ) and miuimala . tb tchment ( Fra 78) do not work well i , l practice ' ( see ( WFB90)) . They then suggest using lexical preference , estimated from a large corpus of text , as a method of resolving attachment ambiguity , a technique the ' call " lexical association . " From a large corpus of pursed text , they first find all nonn phrase heads , and then record the verb ( if ' any ) that precedes the head , and the preposition ( if any ) that follows it , as well as some other syntactic inforlnation about the sentence  . An algorithm is then specified 1 , otry to extract attachment information h'om this table of cooccurrences  . I !' or instance , a table entry is cousidered a definite instance of the prepositional phrase attaching to the noun if : '\[' he noun phrase occm:s in a context where no verb could license the prepositional phrase  , specifically if the noun phrase is in a subjeet or other preverbal position  . 
They specify seven different procedures for deciding whether a table entry is a u instance of no attachment  , sure noun attach , sm : ever battach , or all ambiguous attach . Using these procedures , they are able to extract frequency information , or ncmn a . ppe~u:s with a pal:tieuh ~ rl ~ reposition . 
These frequen ( ; ie serve a . straining d~t ; a for the statistical model they use to predict correcti ~ ttachmen LTo dismnbigu  ; ~tesntence(l ) , they would compute the likelihood of the preposition with giwm the verb buy  , rode olltrast that with the likelihood of that preposition given I : heli Ottll whed  . 
() he , problem wit ; h this , ~pproa ~ chistll ~ ttit is limited in what rel ~ tionships are examined to make mi~d  ; tachment decision . Simply extending t\[indle and l , ooth's model to allow R ) rrelalion-ships such as tlml ~ I ) e . tweell the verb and the ' object o\['the preposition would i : e sultill too large a  . parameters pa . ce , given ~ my realistic quantity of traiuing data . Another prol ) lem of the method , shared by ma . ny statistical approaches , is that the . model ~ ( : quired ( In ring training is rel ) reser~ted in a huge , t ~ d ) le of probabilities , pl : e cludiug any stra . ig htf ' orward analysis of its workings . 
' l~-ansformation-Based Error-Driven
Learning
Tra , nS\]bl'm~d ; ion-lmsederrol:-dHven learlting is ~ sin@e learning a  . lgorithm tlmt hast ) eeu applied to a . number of naturalla . ngm , geprol ) ie . ms , includ-Jllgl ) a . t't O\['speech tagging and syut a . cl , icl)m:sing(1h:i92 , \]h:i93 a , Bri !) gb , Bri9d ) . Figure : 1 illustrates the learning l ) l : OCC'SS , l:irsL , tlll ; 21 nlola , ted text ; is l ) assed through the initial-st ; atemmota-to t . ' l ' lw ~ initial-stat , eare a ) tater can range in complexity from quite trivial ( e . g . assigning rmt domstrll (: ttll:C ) to quit , esophistica . ted(e . g . assigning the output of a . In owledge-based ;/ l/llot ; ~ l , to l ' that was created by hand ) . Ouce text has beeu passed through the iuitia . l-state almOl , at . or , it . is then (; or e-pared to the h'ugh , , as indicated ill a luamlally an-nota , teAe or l ) llS , and transformations are le~u'ned that can be applied to the oul  , put of the iuitial state remora , tott ; o make it , better resemble the : ruffs . 
So far , ouly ~ greedy searchal ) proach as been used : at ea e hiter a . tiono\['learning , t . he transfo>nl ~ tion is found whose application results in the greatestiml  ) roven mnt ; ha . t transfk ) rmation is then added to the ordered trmls for lmLtioulist and the corpus is upd~d  . ed by a . pplying the . learned transformation . ( See , ( I , Mg , \[ ) for a detailed discussiou of this algorithm in the context of machiue  , le , aru--iug issues . ) Ottce 3 , 11 ordered list ; of transform ~ tions i learned , new text , can be mmotated hyfirstaI > plying the initial state ~ mnotator to it and then applying each o\ [' the traaM'ormations  , iu order . 
UNANNOTATID\]"I'I~X'I'1NH'\[AI,l


TEXTTI ~ . Ij'\['ll , ~ e , NEl(~-~RUI , IS Figure \[: Transfonm ~ tion-I~ased Error . -Drivenl , earlfi Ug . 
rlh : ansformation-Based Prepositional
Phrase Attachment
We will now show how transformation based e . rrol > driw mIGmfing can be used to resolve prep ( ~si-tiered phrase at , tachment ambiguity . The l ) reposi-tioiml phrase a . tt ~ Munentea . riter learns tra . nsfor--Ill ~ ttiollS\[?on laC , )l:l>tls O \[ 4-tuples of the \[' orm ( vI11I\]19 )  , where v is ~1w ; r l ) , nl is the head of its objecl , llolni \] phrase , i ) is the \]) l'e positiol l , and 11:2 is the head of the noun phrase , governed by the prel ) c , sition ( for e , -: an q~le , sce/v:1 ~' b o:q/ , lo , / p the h711/~2) .  1 , ' or all sentences that conlbrm to this pattern in the Penn Treeb~mkW dlSt  , l : eet 3 our lml corpns ( MSM93) , such a 4-tupl c was formed , att deach : l-tuple was paired with the at ~ae hnteut decision used in the Treebauk parse  ) '\[' here were 12 , 7664 q ; ul ) les in all , which were randomly split into 12 , 206 trninings ** mples and 500 test samples . 
\[ nthise ? periment ( as in (\[ II ~ , 9\] , I\]l93)) , tim attachment choice Forl ) repositional i ) hrases was I ) e-I , ween theoh . ie cl~mmn and l , hematrix verb . \[ n the initial sl , ~temmotator , all prepositional phrases I \]) at . terns were extra . clx xlus J . ngt grep , a . tree-based grep program written by Rich Pito . '\]'\] te4-tuples were cxtract ; edautom~tk:ally , a . udmist a . kes were not . m~vntta . lly pruned out . 
1199 are attached to the object , noun .   2 This is tile attachment predicted by right association  ( Kim 73 )  . 
The allowable transforlnations are described by the following templates : ? Change the attachment location from X to Y if : - nl is W -  n2 isW-visW--pisW-nlis W1 and n2 is W2 -nlisWl and v isW2 Here " from X to Y " can be either " from nl to v " or " from v to nl  , " W(W1 , W2 , etc . ) can be any word , and the ellipsis indicates that the complete set of transformations permits matching on any combination of values for v  , nl , p , and n2 , with the exception of patterns that specify vahms for all four  . For example , one allowable transformation would be Change the attachment location from nl to v if p is " until "  . 
Learning proceeds as follows . First , the training set is processed according to the start state annotator  , in this case attaching all prepositional phrases low  ( attached to nl )   . Then , in essence , each possible transtbrmation is scored by applying it to the corpus and cornputing the reduction  ( or increase ) in error rate . in reality , the search is datadriven , and so the vast majority of allowable transformations are not examined  . The best-scoring transformation then becomes the first transformation i the learned list  . It is applied to the training corpus , and learning continues on the modified corpus . This process is iterated until no rule can he found that reduces the error rate  . 
In the experiment , atol , alof 471 transformations were learned -- Figure 3 shows the first twenty . 3 Initial accuracy on the test set is 64 . 0% when prepositional phrases are always attached to the object noun  . After applying the transformations , accuracy increases to 80 . 8% . Figure 2 shows a plot of test set accuracy as a function of then uln ber of training instances  . It is interesting to note that the accuracy curve has not yet  , reached a 2If it is the case that attaching to the verb would be a better start state in some corpora  , this decision could be parameterized . 
ZIn transformation #8 , word token amount appears because it was used as the head noun for noun phrases representing percentage amounts  , e . g .  "5% . " The rule captures the very regular appearance in the Penn Treebank Wall Street Journal corpus of parses like Sales for the yea  , "\[v'Prose\[Np5Yo\]\[pPinfiscal1988\]\] . 
Accuracy 81 . 00 rl 80 . 00!!79,00t 77 . 00!--R .   .   .   .   . /--F .   .   .   .   . % oo ! 1/I 74:001 .   .   .   . __t . . . . _ _ 73 . 00 j - 72 . 00 lli ___/__ .  ,?!>2 -70 . 00 69 . 00 68 . 00 67 . 00 64 . 00 0 . 00 5 . 00
Iq ! it
T ! aining Size x 103 10.00
Figure 2: Accuracy as a function of l ; raining corpus size ( no word class information ) . 
plateau , suggesting that more training data wonld lead to further improvements  . 
Adding Word Class Information
In the above experiment , all trans\[brmations are . 
triggered hywords or groups of words , and it is surprising that good performance is achieved even in spite of the inevitable sparse data problems  . 
There are a number of ways to address the sparse data problem  . One of the obvious ways , mapping words to part of speech , seerns unlikely to help . h > stead , semanl , ic class information is an attrac Live alternative . 
We incorporated the idea of using semantic in o t brmation in the l bllowing way  . Using the Word~Net noun hierarchy ( Milg0) , each noun in the ffaning and test corpus was associated with a set containing the noun itself ph  . ts the name of every semantic lass that noun appears in  ( if any )  .   4 The transformation template is modified so that in addition to asking if an mm matche some word W  ,   4Class names corresponded to unique " synonynl set " identifiers within the WordNet noun database  . 
A noun " appears in " a class if it falls within the hyponym  ( ISA ) tree below that class . In the experiments reported here we used WordNet version : l  . 2 . 

II\]4
Change Att : ~ ehment
Location l " r~m ~ To(;o Mition
N1 VP is at
N\]\/P is as
N1VI ) isiulo
N:I\/P is , l'om
N:IVP is with
N\]VN2 is year
N1 VP is by
I ? isi ~ and
NIVN Iixamounl
N\[\/\]' is lhrough
NIV\]) is d'urb~g
NIV Vixp , ul
N1 VN2 is mou.lk
N\[V1' is ull dcr
NJV 1 ) is after
V is have and
N1 VI'isb ~
N :\[ VP is wilk . oul
VNIP is of
V is buy and
N1\/P is for
N:IVP is beJbl "(
V is have and
NI VP iso ~ x/

Vv  ~
Vv /

V ,/
Figure 3: The\[irst20 transfornt at ; ions learned tbr preposil ; ional phrase ~ ttach me , n ; . 
it : ( ~ ana/soaskif " it is a ~ member of some class C  . sThisal ) proaeh I ; o data . sparseness i similar to tllat of ( l , es 93b , li , l\[93) , where ~ method ix proposed for using WordNet in conjunction with a corpus to ohtain classbased statisl  , i e , q . () lit'method here is ltlll C\]l simpler , however , in I ; hatwea . reonly using Boolean values to indieal ; ew hel ; her ~ word can be a member of ' a class , rather than esl , imating ~ fills e , of joint probabilities involving (: lasses . 
Since the tr ; ulsformation-based al ) l/roach with classes cCm gener ~ dize illa way that the approach without classes is ml ~ l  ) let o , we wold dexpect f'cwerl ; ransf'ormal ; ions to be necessary , l !; x perime a H , all y , this is indeed the case . In a second experiment ;, l ; raining a . ml testing were era : tied out on the same samples as i  , the previous experiment , bul ; I ; his time using the ext , ende , d translbrmation t(;ml ) la . tes for word classes . A total of 266 transformations were learned . Applying l . hese transt'ormai . ions to the test set l'eslllted in a . n accuracy of '81 . 8% . 
\ [ n figure 4 we show tile lirst 20 tra . nsform ~ l , ionslem ' ned using il Olll classes . Class descriptions arc surrounded by square braclets  . ( ; ' Phe first ; grans-Ibrmations t ~ l . cs thai . if " N2 is a . nomtI , hal ; describes time ( i . e . ixa . member of WordNet class that include stim nouns " y  (  ; ar , "" month , "" week , " and others ) , the ll the prel to sitiom d phrase should be al ; tache(\[t , ( ) the w ; rb , since , tim ( ; is \] nl Ml more likely Io modify a yet ' It ( e . g . le , vclh(:re ( clingiuan hour ) tha Jla , lloun . 
This exlw , riment also demonstrates how rely\[~?~l ; ul : e-based lexicon or word class if l cat , ion scheme cautrivia Jlybe incorlJorated into the learner  , by exLencling l ; ransfot ' nlal , iolls to allow then t to make l'ef el'eAlc (? ; oitWOl : ( \ [ gilt\[lilyO\['its features . 
\], valuation against Other

In ( lIl ~91, HR93), tra . in itt g is done on a superset el ' sentence types ttsed ill training the transforl J~atiolFbased learner  . The transformation based learner is I , rained on sentences containing v , n \[ and p , whereas the algo-rithm describe . d byll indle and I ~, ooth ca . nzd so use sentences (; on tailfing only v and p , ( n'onlynlandi1 . \[11 their lmper , they tra . in o now ~ r200 , 000 sen-Lettces with prel ) ositions f ' rotn the Associated Press ( APtnewswire , trod I ; heyquote a . naccuracy of 78-80% on AP test & ~ ta . .
~' For reasons of ~: un-time c\[lk:icn(:y , transfonm Ll , ionstmddngre\['crence 1:o tile classes of both nla , ndn2 were
IlOI ; p(~l?lXiitl,tR(I.
GI ; or expository purposes , the u . iqm ' . WordNet id (' . ntilic rslu we been replaced by words L h ~ LL describe the cont  , cnt of the class . 
1207(~lml~.ge\]
Attachment,/
Location/#li'romt'Fo\[Condition 1   N1 V N2 is\[time\] 2   N1 VP is al 3   N1 VP is as 4   N1 VP is into 5 N 1 VP is from 6   N1 V 1   ) is wilh 7N1 VP is of
P is in and
NI is 8N1 V\[measure , quanlily , amou ~ l\]
P is by all . el 9N1 VN2 is \[ abslraclion \]
I0NIVP is lhro'ugh1) is in and
NI is 11 NIV\[group , group . in . g \] 12 VN 1 V is be 13 NIV Vispul 14 NIVP is under
Pisi ~ and
N\] is 15N1 V\[writtenco . mmlu~ication \] 16N1 Vl ) is wilhoul 17   N1 VP is during 18 N 1 V 19 NIV 20   N1  . Vl ) is on and
Ntis\[U ~ . ing\]
P is after
V is buy and
P is for
Figure 4: The first 20 transformations learned for prepositional phrase attachment  , using noun classes . 

Method Accuracy Transforms t-Scores 70 . 4- 75 . 8'\]? anstbrma ~ ions 80 . 8 471
Trans\['ormations(noN2) 79.24 18
Transformations ( classes ) 8:1.826 (5
Figure 5: Comparing lesults in PP Attachment.
In order to compare the two approaches , we reimplemen:ed the ~ flgorithm fi'om(IIR . 91 ) and tested it using the same training and test set used for the above experiments  . Doing sore-sull ; ed in an attachment accuracy of 70 . 4% . Next , the training set was expanded to include not only the cases o\['ambiguous attachment \] Fonndin the parsed Wall Street Journal corpus  , as before , but also all the unambiguous prepositional phrase attachment stbnnd in the corpus  , as well ( contiml-ing to exclnde the tesl , set , of course ) . Accuracy improved to 75 . 8% rusing the larger training set , still significantly lower than accuracy obtained us--lagtamtl : ansformal  ; ion-based approach . Thet . ech-nique described in ( Res93b , 11 t 93) , which combined Hindle and Rooth's lexical association technique with a WordNet-based conceptual association measure  , resulted in an accuracy of 76 . 0% , also lower than the results obtained using transformations  . 
Sincellindle and Rooth's approach does not make reference to  n2  , we reran the transformation-learner disalk ) wing all transformations that make reference ~ o n2  . Doing so resulted in an accuracy of 79 . 2% . See figure 5 h ) rasun > mary of results . 
It is possihle Lo compare ; the results described here with a somewhat similar approach devel-  . 
oped independently by Ratnaparkhi and I/ , oukos(lR94) , since they also used training and test datt ~ drawn from the Penn Treebank's Wall Street Journal corpus  . Instead of ' using mammlly coustructed lexical classes  , theynseword classes arrived at via mutmd information clustering in a training corpus  ( BDd+92 )  , resulting in a representation i which each word is represented by a sequence of bits  . 
As in the experiments here , their statistical model also makes use of a 4-tuple context ( v , c < l , p , n2) , and can use the ident it . ies of the words , class in l ' or-marion ( t br them , w dues of any of the class bits ) , r The difference between these results ~ nd tile result they quoted is likely due to a much bLrger training set used in their origim d experiments  . 
1202 or both Mnds of ild ' ormation as eotll ; extual fea-t lll ? eSriley lescril ) ea search process use ( \ [ toletePn6\]m what , sul ) set of the available ill\['or , ~ Ht-lion will Im used in the model . (\] iv;\]\]a e hice of features , they train ; tprolabi/islie model For I)r ( Sitcl coutext ) , and in . esl . ing choose Site : -: voPSite=nla ~ ccordi\]lgI ; owhich hashe higher eomlitional probal)i\]ity . 
t~ , at nal)~Pkhi and Roukosrelortana ecuraeyoi'81 . 6% using bot , hword and classiui ' or ma , tion on Wall SI ; re . et3 our na\]text , , using a t:raining COl:-pus twice as la , rg cas that used in ou P experiments . 
They also report that a ( leeision tree mode/e on-st\] ; u (: t ~ d using the same features m , dI , i ; aining data ac\[lieve II ) er formanee of 77 . 71 ~, ( nt\[:esameI . est set , All Ullll ) el'o \[' other resea Pehers have exl ) lore de or lms-I ) ased approaches I ; ol ) repositional phrase attaeh met , tdisaml ) iguation tM~tn\]~dc use of word classes , l " or example , We ise hedq clal . ( WAIH91) and Basilielal . ( BIV 91) bol , \] l deseril ) e the use of lnanually coustrueted , donm hvSl ) ~eitic word classes together with corius-tmsed si , t ~ tisties in of d2r to resolve i ) rel ) ositional 1 ) hrase a . t , taehlllellt&Ill- . 
Iiguity . I e (; a . ll Se these papers deseril ) e result sol ) -tained on different corpora , however , it is ( lifIicull ; to II ~ , : ' tl , : . ; a . 1) (; r\['(rllla,iic!COl\[lD~/l:iSOll,
Conclusions
The . tPansl'ormation-hased approach to resolving preposition alphl : ased is an lbiguation has a mlmt  ) er of advai H ; ages over ( l , \] ler ; i . l ) l ) roate hes . \[11 a(\]i recteoml) ; u : is on with lexical association , higher ble ( ; ll-vaey is achieved using words a lolm ( wenthough attachment in f\rnlation is capture di * la relatively small numl  ) er of simple , rea(lable rules , as opl ) osed to a . largelllll \] ) eFOf lexical co-oe et lrrelte el ) l'o\] ) a --

\] u addition , we have shown how the l ; raus\['orln ~ Lion-based larner can casity bee ? . -tended to incorporate word classi/f formatiou . 
This resulted in a slight ; increase in 1) er formane e , but , more notal )\] y it resulted in a reduct ; i on hyroughly half in the l ; ot a\[m nn l)er of transfor-mation rules needed . And in ( : outrast to ap-pro~ches using class--based prol ) a bilistic models ( BPV 91 , Res 93 e , WAI ~ F91) or classes derived vi ; ~statistical clusl . ering methods (1~ . R94) , t : hise ch-llique pro(hleesa , I : HIO set that (: all ; ltreseol teepl : ~ lalgeueralizal ; ionscoucise Iya . mlill\]mman-reaIal\]e for In . 
F/\]rthel:lllOl:e , iuso \[' aras (: oH q ) a , risollse & llI ) oina(h-all lOllgse para , Leexl'el : llllel/l ; sllsilt ~ Wail Street Jour \] ml training aml test data  ( ( l lRgl )   , reiml)l('meute(lasreI ) oPted above ; ( les93 e , 1t 1193); ( IH 1 . 94)), the rule-based approach de . .
scribed here achieves better perl ' or lttaucc , using mlalgol : ithm tlmtise on celtually quite Mml  ) leam/iul ) l'~l . ( ; tiea\]te FlttSextretuely easy to ilnplen lel~t , s
A more genera \] point ix tha . t the transl ' or m ~ d , ion-based ; ~ l ) lroate his easily a(lapl , edt ; osituations in which some learning 1"re in a ( : or pus is desiral ) le , 1 ui , hand-construet cIll : i or knowledge is also available  . Existing knowl-elge , such as structural strategies or even a priori h ; xie all references ,  ( ; all 1) eincorl ) orated into I ; he start state annotator , so the ft the learning ~ d go . 
I : ithm begins with n , or e refiued input . And knowu exceptious : au1 ) e h and h ' ( l transparently simply hyadding add \]:\[ on al rules to timset thai  ; is learned , Il Sillgtiles allle representatio \] l . 
A disadwm tage of the al ) l ) roach is that it requires supervised training that is  , are presentative set of " true " c~sest'FOlll which Colearn  . I low-ever , this l ) eeomes less of a probh ' . mas at motate deorlor abee olne increasingly available  , and suggests the comhination o1:' supexvised and uusupervised methods as a . uilfl ; eresthGave\]me\['or\['urtherrese ; ire\]\[ . 
References\[AS88\](~ . All , mann and M . Steedmau . Interaction with context during hm nansen-ten(u ' , I ) ro <' essing . Co . qnitio ~, ; 0:191238, 1988 . 
\[ l ; DdF92\]IeterI ? . \] ~ Powa , Vhleell ~ J . l ) ella\]ietr ; ~, l)eter V . leSouza , 3enni\['er(L
I , ai , and Robert I , . Mereer . (21ass.
based n-gr + url models of natural \ [ aw-gua . ge . Compulationall , ingui . slic . % 18 ( d ) : 467480, December 1(,)92 . 
\[BPV91\]H . Basili , M . Pazienza , and P . Velardi . 
Combining NLI~md statistica . lteel Pniques for lexical a equisition . \]: n Pro-ccedings of the AAA1F all5' ymposhtm . 
on Probabilistie Approaches to Natural
Language , Cambridge , MassaehusetLs,
Octobe w1!)9I.
\[Ih:i92\]F , . trill . A simple rule-bused part of speecht ~ Gge\]: . Int ' voeecding ~ of lh P
Third UoT@'rence on Applied Natural Lan . guagc Processing , A ( . .' g , Trent;o , ltaly , 1992 . 
\[Bri93 a\]I'; . trill . Automatic grammar m-duel ; ion and parsing fi:ee text : Atrarts\[' orlnation-1ased al~l ) roaeh . I\]1 Proceedings o , f t i t ;   31sl Mceling of lhe Association of Compulational 1  , inguis-tics , Columbus , Oh . , 1993 . 
8 ( ) urcode is being made pullicly ~ waihd A e . (? on-tact . Its ' . aJ\]thors\[brinl)-l:ltla J , ioll Oll how to obtain it . 
1 203  \[  Bri93b  \] \[  Bri94\]  \[  l!'ra78\]  \[  Hi9 ~ I\]\[n , ~ , ga \]\[1 ( i m73\]  \[  Mil90\]  \[  MSM93\]  \[  Res93a  \] \[  Res93b \]\[ l , es93c\]\[ltnga\]
E . Brill . A Corpus-Based Approach lo
Language l , cmming . Phi ) thesis , Department of ' Computer and lnfbrrna-lion Science  , University of Pennsylvania ,  1993 . 
E . Brill . Some advances in rule-based part of speech tagging  . In Proceedings of lhe 7' welflh National Co , @ r-once on Artificial \] nl clligence ( AAAI-94) , Seattle , Wa . , 1994 . 
I , . Frazier .  0 ) ~comprehending sentences : synlaelic parsi~Lq sleategies  . 
PhD thesis , University of Connecticut , 1978.
1) . Hindle and M . f , ooth . Structural ambiguity and lexicai relations . In Procced in ~ ls of the ~ f , J ~ l ~ Annual Mee # in9 of lheAssocia * ionfor " Computation all , i ~ Lquisties , Berkeley , Ca . , 1991 . 
I ) . Iiindle and M . looth . Structura . 1 ambiguity and lexical relations . Computational Li ~ Lq ' uislies , 19(l ): 103120, 1993 . 
J . Kimball . Seven principles of surface structure parsing in natm ' allanguage  . 
Coq ~ i limh 2, 1973.
G . Miller . Wordnet : an online lexical cla . l ~ abase , hfler national , our ~ al of
Lezieography , 3(4), 1990.
M . Marcus,
B . Santorini , and M . Marcinkiewicz.
Building a large annotat , ed corpus of l !; nglish : the Petm ' Freebank . Compu-talional Linguistics , 19(2), 1993 . 
P . lesnik . Selection a . ~dlnforma-lion : A Chtss-lhtsed Approach Io Lezi -ealtelatio ~ ships  . PhD thesis , Unive > sity of Pennsylvania , December 1993 . 
(Institute for H , esearch in Cognitive
Science report IRCS-9'3-42).
P . l , esnik . Semantic lasses and syntactic ambiguity . In Proceedings of/he
A~PA Workshop on I~urna ~ Language
Technology . Morgan Kamfinan , \[993.
P . H , esnik . Semantic lasses and syntactic ambiguity . Aft PA Workshop on
Ihman Language q?echnology , Mz~rch 1993 , Princeton . 
P . Resnik and M . Hearst . Syntactic ambiguity and conceptual relations . In K . Church , editor , Pr ' oceedings of lhe
ACL Workshop on Very Large Corpora , pages 5864, . June 1993 . 
\[R,M94\]\[mv~q\[WAB+9:l\]\[WFB90\]
L . iamshaw and M . Marcus . Explo > ing the statistical derivation of transformational rule sequences for part-of -speech tagging  . In a . Klavans and P . lesnik , editors , The Balanci ~ gAc*:
Proceedings of lheA C . L Workshop on
Combining Symbolic and 5' tatislical
Approaches to Language , New Mexico
State University , July 1994.
A . Ratnaparkhi and S . Roukos . A maximum entropy model \[ brprepositional phrase attachment  . In Proceedings of lhe ARPA Workshop on \]\[ uma ~ Language Technology  , Plains-
I , or o , NJ , March 1994.
IC Weischedel , D . Ayuso , R . . Bobrow,
S . Boisen , R.lngria , and J . Palmucei.
Partial parsing : a report of work in progress . In Proceedings of the l , b ' urth
DA . RPA5' pecch and Nalural Language
Workshop , February . 199l , 1991.
G . Whi~temore , K . Ferrara , and
H . Brunner . Empirical study of predictive powers of simple attach-lnent schemes for postmodifier pepo -sitional phrases  . In . Procecdi~gs of the 281h Annual Meeting of the Association for Comlmtalional Linguistics  ,  1990 . 

