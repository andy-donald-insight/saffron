Structural Feature Selection For English- Korean Statistical 
Machine Translation
Seonho Kim , Juntae Yoon , Mansuk Song
pobi , jtyoon , mssong@december . yonsei , ac . kr
\ ]) ept . of Computer Science,
Yonsci University , Seoul , Korea

When aligning texts in very different languages such as Korean and English  , structural features beyond word or phrase give useful intbrmation  . In this paper , we present a method for selecting struetm ' al features of two languages  , from which we construct a model that assigns the conditional probabilities to corresponding tag sequences in bilingual English-Korean corpora  . For tag sequence mapl ) ing1)et ween two langauges , we first , define a structural feature fllnction which represent statistical prol  ) erties of elnpirical distribution of a set of training samples  . 
The system , based on maximmnent rol ) y cone et ) t , se-le(:ts only ti ; atures that pro ( lue e high increases in loglikelihood of training saln l  ) les . These structurally mat ) ped features are more informative knowledge for statistical machine translation t  ) etween English and Korean . Also , the inforum . tion can help to reduce the 1 ) a rameter sl ) ace of statisti ( ' al alignment 1 ) yeliminating synta ( : tically u if likely alignmenls . 
1 Introduction
Aligned texts have been used for derivation of 1 ) i lin-gual dictio imries and termino h ) gy databases which are useflfl for nlachine translation and cross languages infornmtion retriew f l  . Thus , a lot of alignment techniques have been suggested at  ; the sentence ( Gale et al ,  1993) , phrase ( Shin et al ,  1996) , nomtt ) hrase ( Kupiec ,  1993) , word ( Brown et al , 1993; Berger et al , 1996; Melamed ,  1997) , collocation ( Smadja et al , 1996) and terminology level . 
Seine work has used lexical association measures for word alignments  . However , the association measures could be misled since a word in a source language frequently cooccurs with more t it an one word in a target language  . In other work , iterative reestimation techniques have beetsemt ) loyed . They were usually incorporated with the EM algorithm middynmnic progranmfing  . In that case , the prob-al ) ilities of aligmnents usually served as 1 ) a rameters in a model of statistical machine translation  . 
In statistical machine translation , IBM 1~5 models ( Brown et al ,  1993 ) based on the source-chmmel model have been widely used and revised for many languaged on mins and applications  . It has also shortconfing that it needs much iteration time for parameter estimation and high decoding complexity  , however . 
Much work has been done to overcome the problem . Wu ( 1996 ) adopted chammls that eliminate syntactically unlikely alignments and Wang et al  ( 1998 ) presented a model based on structures of two la . nguages . Tilhnann et al ( 1997 ) suggested the dynanfie programming lmsed search to select the best alignment and preprocessed bilingual texts to remove word order differences  . Sate et al ( 1998 ) and Och et al ( 1998 ) proposed a model for learning translation rules with morphological information midword category in order to improve statistical translation  . 
Fur them lore , llla , lly researches assullle ( t Olle-to-one correspondence due to the coml ) lexity and com-Imtati ( m time of statistical aliglunents . Although this assumption Ire'ned outt ; o1 ) e useful for alignment of closel all guage such as English and French  , it is not , applicabh ~ to very different languages , in particular , Korean and English where there is rarely ( : lose corresl ) ondence in order at the word level . For such languages , even phrase level alignment , not tomei ~ tion word aligmnent , does not gives good translation due to structural difl brence  . Itence , structural features beyond word or t ) hrase should t ) e considered to get t ) etter translation 1 ) etween English and Koreml . In addition , the construction of structural bilingual texts would be more informative for extracting linguistic knowledge  . 
In this paper , we suggest a method for structural mat ) t ) ing of bilingual language on the basis of the maximum entorl  ) y and feature induction fl ' alne work . 
Our model based on POS tag sequence mapl ) ing has two advantages : First ;  , it can reduce a lot of 1 ) arm ne-ters in statistical machilm translation by eliminating syntactically unlikely aligmnents  . Second , it : can be used as at ) reprocessor for lexical alignments of bilingual corpora although it  ;   ( : an be also exl ) loited 1 ) y itself t bralignment . In this case , it would serve as the first stet ) of alignment for reducing the 1 ) arameters I ) ace . 
439 2 Motivation
In order to devise parameters for statistical modeling of translation  , we started our research from the IBM model which has bee::widely used by : nany researches  . The IBM model is represented with the formula shown in  ( 1 ) l17 tv ( f , al ) = IIIIt(fJl %) d(jlaj , m , l ) i=1 j=l (1) Here , n is the fertility probability that an English word generates nh'end : words  , tistim aligmnent probability that the English word c generates the French word f  , and d is the distortion probability that an English word in a certain t  ) osition will generate alh ' enchword in a certain 1  ) osition . This formula is Olm of many ways in which p(f , ale ) cantie writtm . 
as the product of a series of conditional prot ) at ) ilities . 
In above model , the distortion probability is re--lated with positional preference  ( word order )  . Since Korean is a free order language , the probability is not t ~ a sible in English-Korean translation  . 
Furthermore , the difference between two languages leads to the discordance between words that the one-to-one correst  ) ondence bt ween words generally does not keel )  . Then : odel (1) , however , as--sumed that an English word cat : be connected with multiple French words  , but that each French word is connected to exactly one English word in ch : ding the empty word  . hl conclusion , many-to-:nany : nap--pings are not allowed in this model  . 
According to our ext)eri:nent , in any-to-nmny mappings exceed 40% in English and Korean lexical aligninents . Only 25 . 1% of then : can be explained by word for word correspondences  . It means that we need a statistical model which can lmndlephrasal mat  ) pings . 
In the case of the phrasal mappings , a lot of parameter should be searched eve::if we restrict the length of word strings  . Moreover , in order to prop--erly estimate t ) arameters we need much larger vo I--ume of bilingual aligned text than it in word-for-word modeling  . Even though such a large corpora exist sometimes , they do not come up with the lex--ical alignments . 
For this problem , we here consider syntactic features which are importmlt in determining structures  . 
A structural feature means here a mapt ) ing between tag sequences in bilingual parallel sentences  . 
If we are concerned with tag sequence alignments , it is possible to estimate statistical t ) arm neters in a relatively small size of corpora . As a result , we can remarkably reduce the problem space for possible lexical alignments  , a sort of t probability in (1) , which improve the complexity of a statistical machine translation model  . 
If there are similarities between corresponding tag sequences in two language  , tile structural features would be easily computed or recognized  . However , a tag sequence in English can be often translated into a completely different tag sequence in Korean as follows  . 
can/MD-+~- ,   cul/ENTR1   su/NNDE1 ' iss/AJMAda/ENTEItnmans that similarities of tag features between two languages are not  ; kept all the time and it is neces-saw to get the most likely tag sequence mappings that reflect structural correspondences between two languages  . 
In this paper , the tag sequence mappings are ob-taind by automatic feature selection based on the maximum entropy model  . 
3 Problem Setting
Int iff sctlat ) ter , we describe how the features are related to the training data  . Let t c be an English tag sequence and t k be a Korean tag sequence  . Let Ts be the set of all possible tag sequence niapI  ) ings in a aligned sentence , S . We define a feature function ( or a feature ) as follows : 1 pair ( t~ , tk)C"\]-sf(t ~ , tk ) = 0 oth crwi . s'cIt indicates cooccurrence information l ) etween tags appeared in Ts . f(t ? , t k ) expresses the information for predicting that te maps in tota  . . A feature means a sort of inforination for predicting something  . In our model , cooccurrence information on the same aligned sentence is used for a feature  , while context is used as a feature in I nost of systems using maximum entropy  . It can be less informative than context . Hence , we considered an initial supervision and feature selection  . 
Our model starts with initial seed ( active ) features for map I ) ing extracted by SU l ) ervision . In the next step , that ure pool is constructed from training samples fro:n filtering and oifly features with a large gain to the model are added into active feature set  . The final outputs of our model are the set of active t'ea-tures  , their gain values , and conditional probabilities of features which maximize the model  . Tim results can be embedded in parameters of statistical machine translation and hell  ) to construct structural bilingual text . 
Most alignment algorithm consists of two steps : ( 1 ) estimate translation probabilities . 
(2 ) use these probabilities to search for most t ) roba-ble alignment path . 
Our study is focused on (1) , especially the part of tag string alignments . 
Next , we will explain the concept of the model.
We are concerned with a not ) timal statistical i node l which can generate the traiifing samples  . Nmnely , our task is to construct a stochastic model that pro-To Thel  ) roblem of interest is to use Salnt/les of ?-- J ~\ , What . . . .
tagged sentences to observe the / ) charier of the ran-u ~ ,   , ~( loint ) roees s . ' rile model pestinm testile conditional tt ' 2 , Y probability that tile process will outlmtt , ~ , given t ~ . . ~  , ~/~  , o ~!! It is chosen out of a set of all allowed probability o ~  , ~~ e ~ . .?0,, me ( tistributions .   .   .   . 
The fbllowing steps are emt ) loyed for ( ) tit " model , v ~/ Input : a set L of POS-labeled bilingual aligned sentences  . 
I . Make a set ~: of corresl ) ondence pairs of tag sequences , ( t  ~ , tk ) from a small portion of L by supervision . 
2. Set 2F into a set of active features , A.
3 . Maximization of 1) arameters , A of at : tire features 1 ) yIIS ( hnproved Iterative Sealing ) algorithm . 
4 . Create a feature pool set ?9 of all possible align-nmnts a ( t (  , , tk ) from tag seqllellces of samples . 
5 . Filter 7) using frequency and s intilarity with M . 
6 . Coml ) ute the at it ) roximate gains of tkmtm : es in " p . 
7 . Select new features ( A /' ) with a large gain vahle , and add A . 
Outt ) ut : p(tklt ~ , ) wh crc(t(,,t ~ . ) CM and their Ai . 
WeI ) egan with training samples comi ) osed of English-Korean aligned sentence t ) airs , ( e , k ) . Since they included long sentences , w (' , 1) roke them into shorter ones . The length of training senl ; en (: es was limited to ml(h ' , r14 on the basis of English . It is reason a , bh ;\]) (' , (:& llSe we are interested in not lexical alignments lint tag sequence aliglmients  . The samples were tagged using brill's tagger and qVI or any ' that weiml  ) lenmnted as a Korean tagger . Figure \] shows the POS tags we considered . For simplicity , we adjusted some part of Brill's tagset . 
In the , sut ) ervision step ,   700 aligned sentences were used to constructhe tag sequence smal  ) I ) ings wl fich are referred to as an active feature set A  . As Figure 2 shows , there are several ways in constructing the corresl  ) ondem ; es . We chose the third mapping although ( 1 ) can be more useflll to explain Korean with I ) redieate-argunmnt structure . Since a subject of a English sentence is always used for a subject t brl n in Korean  , we exlcuded a subject case fi'on lar-gulnents of al/redicate  . For examl ) le , ' they'is only used for a subject form , whereas ' me ' is used for a object form and a dative form  . 
II1tile next step , training events , ( t , :, It . ) are constructed to make a feature 1 ) eelfroln training sam-pies . The event consists of a tag string t , , of a English ( 2 )   ( 3 1 ? ~ lm-~"--+~'~What ever Figure 2: Tag sequence corresl ) ondences at the phrase level 1 ) OS-tagged sentence and a tag string tL~of the corresponding Korean POS-tagged sentence and it Call be represented with indicator functions fi  ( t  ~ , tk ) . 
For a given sequence , the features were drawn fl'om all adjacent i ) ossible I ) airs and sonic interrupted pairs . Only features ( tci , tfii ) out of the feature pool that meet the following conditions are extracted  . 
?#( l , ei , t~:i)_>3 ,  #is count ? there exist H : . ~ , , where ( t ( , i , tt . ~ . ) in A and the similarity ( sanle tag ; colin ; ) of lkian(1tkx_>0 . 6 Table \] shows possible tL ' atures , for a given aligned sentence , ' take her out-g'md COrculbagg curo dcrfle of lara '  . 
Since the set of the structural ti ; at m ' es for alignment modeling is vast , we constructed a maximum entrol ) y model for p ( tkltc ) by the iterative model growing method . 
4 Maximum Entropy
To explain our method , wel ) riefly des ( : ri be the con- ( : ept of maximum entrol ) y . Recently , many al ) -lnoaches l ) ased on the maximum entroi ) yl node l have t ) een applied to natural anguage processing ( Bergere Lal . , \]994; Berger et al , 1996; Pietra et al , 1997) . 
Suppose a model p which assigns a probability to a random variable  . If we don't have rely knowledge , a reasonal ) lesolution for p is the most unif brnl distribution  . As some knowledge to estilnate the model pare added  , tile solutions t ) ace of p are more constrained and the model would lie close to the ol\]timal probability model  . 
For the t ) url/ose of getting tile optimal 1 ) robability model , we need to maxi \] nize the unifl ) rnlity under some constraints we have . ltere , the constraints are related with features . A feature , fi is usually rel/re-sented with a binary indicator funct  , ion . The inlpor-tahoe of a feature , fi can be identified by requiring that the model accords with it  . 
As a (: on straint , the expected vahle of fi with respect totile model P  ( f i ) is supposed to be the same a stile exl ) ected value of fi with respect o empiri ( : aldistril ) ution in training saml ) les , P ( f i ) . 

TAG cb




















DESCRIPTION comma conjunction , coordinating determiner foreign word adjective , ordinal adjective , superlative modal auxiliary noun , proper , singular predeterminer pronoun , personal adverb adverb , superlative symbol interjection verb , past tense verb , past participle
WH-pronoun , possessive not be verb , past tense be verb , present participle have verb , past participled over b , past tense




JJR

















DESCRIPTION sentence terminator TAG numeral , cardinal existential there NNIN 1 preposition , subordinating NNIN2 adjective , comparative NNDE1 NNDE2 list item marker PN noun , common NU noun , proper , plural VBMA genitive marker AJMA pronoun , possessive CO adverb , comparative AX particle ADCO to or infinitive marker APSE verb  , present tense CJ verb , present participle ANCO
WH-determiner ANDE
WH-adverb ANNU be verb . present tense EX be verb , past participle LQ have verb , present tense RQ do verb , present tense SY do verb , past participle
POS proper noun common noun common-dependent noun unit-dependent noun pronoun number verb adjective copula auxiliary verb constituent adverb sentential adverb conjunctive adverb configurative adnominal demonstrative adnominal numeral adnominal exclamination left quotation mark right quotation mark symbols 





















Figure 1: English Tags ( left ) and Korean Tags ( right ) 
POS nominative postposition accusative postposition possessive postposition vocative postposition adverbial postposition conjunctive postposition auxiliary postposition final ending coordinate ending subordinate ending auxiliary ending adnominal ending nominal ending adverbial ending ending+post position pre-ending suffix prefix comma termination ~ P ~ II ' ~ ll qllll l~l'~l~tIlliiLUiIf  ( iIIIL'IIII ~ ; '~  ; ~lil'tlllIIIllILI~M\[VBI'+IN\] [ take+out \]  \[1+3\] \[ wp\]\[tak ( q\[t\]\[VBP+PI~P\]\[take+her\] \[1+2\]  \[  W3P+PRP+IN \]\[ take+her+out\] \[1+2+3\] \[ PRP\][ho , -\][2\]\[IN\]\[out\]\[3\]\[I'PCA2+P1 ) AD-FVBMA\]\[rcul+euro+deryeoga\] \[2+4+5\] \[ PN\]b . . . . :/~ o \] i l l \[ PI ) AI ) + VBIvlA-FENTE\]\[reu/+cure-I-dcrycoga+ra\] \[4+5+6\]  \[  NNIN2\] [ bagg\] \[3\]  \[  NNIN2+PPAD \]\[ bagg+euro\] \[3+41 \[ ENTE\]\[ra\] [6\]  \[  P1  ) AD+VBMA\]\[curo+deryeoga\] \[4+5\] \[ PPAD+VBMA+ENTE\][euro+deryeoga+ra\] \[4+5+6\]  \[  PPCA2+NNIN2+PPAD+VBMA \]\[ reul+bagg + curo+deryeoga\] \[2+3+4+5\]  \[  PPCA2+NNIN2+PPAI  ) + VBMA+ENTE\]\[reul+bagg+euro+dcryeoga+ra\] \[2+3+4+5+6\]  \[  P1  ) C A2+NNIN2+PPAD+VBMA \]\[ renl+deryeoga\] \[2+3+4+5\]  \[  PPCA2+NNIN2+Pt  ) AD+VBMA+ENTE\]\[reul+deryeoga+ra\] \[2+3+4+5+6\] 
Table 1: possible tag sequences
In sun 1 , the maxiln unlentropy fralnework finds the model which has highest entropy  ( most uniform ) ~ given constraints . It is related to the constrained optimization . To select a model from a constrained set , Cof allowed l ) rol ) ability distributions , the model p , CC with maximum entropy H(p ) is chosen . 
In general , for the constrained optimization problem , Lagrange in ultipliers of the number of features can be used  . However , it was proved that the model with maximum entropy is equivalent o the model that maximizes the log likelihood of the training samples like  ( 2 ) if we can assume it as an exponential model . 
hi(2), the left side is Lagrangian of the condi-.
tional entropy and the right side is in axiln lHn log -  . 
likelihood . We use the right side equation of ( 2 ) to select I . for the best model p , . 
~, g , , ~ . .~,(- ~ . ,, . ~(~) v(yl ~) log v(vlx ) + ~ , ,(v(f , )-~(/ , ))) (2): a , '9 , , , ax  ~ ,   . ,, . ~( x , v ) lo . ~n , ( ylx ) Since t , cannot betbund analytically , we use the tbllowing improved iterative scaling algorithm to colnpute I  , of n active features in . 4 in total sam--ples . 
1 . Start with li = 0 for all i 61, 2, .   .   . , n2 . Do for ca . ohi ~\], 2, .   .   .   , n : ( a ) Let AAi be the solution to the loglikelihood ( b ) Update the value of Ai into li+A , h ,  ~ .   . . . . .  ~( . , , v ) A(:~, v ) where AAi = log ~, :, ~ i . ~)v~(?11 . ~)/~( . ~, v ) px(yl:r ) = ~- A--e(~x'f'("':')) zx(:~ . )', z(x ) = E :, c(E , 3 . Stop if not all the Ai have converged , otherwise go to step 2 Tile exponential model is represented as ( 3 )  . Here , li is the weight of feature f i . In ore " model , since only one feature is applied to each pair of x and y  , it can be represented as ( 4 ) and fi is the feature related with x and y . 
~( ylx ) = ~ iC'f'(x'Y ) (3) cAifi(x , Y )  =  ( 4 ) Only a small subset of features will 1 ) e emph ) yed in a model by sele ( : ting usef lflfeal ; m'es from ( ; tieflm ture 1) ool 7) . Let 1) . ,4 lie ( ; tie optimal mo ( lel constrained by a set of active features M and AUJ'i  1  ) e , / lfi . L e ( ; PAf ~ be theot ) timal model in the space of l ) rol ) a bil-ity distribution C ( Afi )   . The optimal model can be tel ) resented as (5) . Here , the optimal model means a maxilm nn entropy n lodd . 
1 v ~: ,= z , , . (:,;) p'~(:11,)::'("'") zo,(: . ) = ~ v . ~(: : l : , ,) c " S '(* '"> (5)

The imi ) rove ment of l ; he model regarding the addition of a single feature fi can be estiumted by measuring the difference of maximmn loglikelihood between L  ( pAf ~ ) and L ( pA )  . We denote the gain of t ~ a turefitiy A ( ~ lfi ) an ( lit canber ( ! t / resented in ( G )  . 
A(A . I ' d- . , . .,:,;,, cAI ~( . )('A :,( , , , ) = J ~(> t : ,)-- L(v,O = _~(:~)~, . ~(:,/ I, . ) : :'('''') xy?'~P ( . fi)IS ) Note that a model PA has a , set of t ) a rameters A which means weights of teatures . Them ( idel P . Afl contains the l ) ara . lnetc . rsan(I the new \[) a . l ' a , l l l  C i ; (' ~ r(11 with l'eSl)ect () the t'eal ; uref i . W'hen adding a new feature to A , the optimal values ( if all parame ( ers of probability ( list ril ) u (  , i on change . To make th ( ; (: om-i ) utation of feature selection tractal ) le , weal ) l ) roxi-mate that the addition of a feature fi affec (  ; s only the single 1) a ranxeter a , as shown in (5) . 
qShe following a . lg or it ln n is used for ( ; omputing the gain of the model with rest ) ect of i . We referred to the studies of ( Berger et al , 1996; Pietrae . tal . , 1997) . We skiptile detailed contents and 1) root ~ . 
1 . Let 1 if P ( fi ) <_PA(J ;) r = -1 oth , erwise 2 . Seta0=03 . Repeat the following until GAff (% , ) has converged : Coill l ) llte 0@1 , + if r Olllognll Sillgalog ( 1!-~6t:' (  ' "~ ) ~ ctn+l = ( xn+7" ,  .  (;~:~(~,,):
Compute GaV ~( a , ~+ l ) using
GAA(a ) = - Ea , /3(a , ') logZ , ,(: , :) + ctf)(fi ) , c'A : ,  ( , ~) = ~( k)-Ex~(~0M(: ,   . ), G ": ct ~ A : , , ,=-- E . ~ P (") V2i : , (( fi--M ( ; , ;))" la ; ) set description # of disjoint total features cv tults 
A active feat m ' es 148 341 13
Pfeature Calldidat , es317263773
Nnewf'ea Lures 975503
Table 2: Summery of Features Selected where ( :~ ~  ( l~n+1
Af ~= Auf ~,
M(z)-p~f~(fila-),
PP4S , ( fil':)---E . ~' ~ s , (:, ? l : r ) k (: . , ~ J ) d . Set ~ AL(Afi ) <--GAS , ( ct . ) This algorittun is iteratively comtmted using Net-Wel l ' S method  . \? ecmt recognize the iml ) ortance of afl ; ature with the gain value . As mentioned above , it means how much the feature accords with the model  . 
We viewed the feature a stile information that Q . and t , occur together . 
6 Experimental results
The total saml ) les consists of 3 , 000 aligned Sellteiice pairs of English-Korean , which were extracted from news on the website of ' Korea Times ' and a  . magazine f l ) r English learning . 
In the initial step , we manually constucted ( ; tie correspondences of tag sequences with 700 POS-tagged sentence I ) airs . hithe SUl ) ervision step , we extracte ( tl . ,d83 correct tag sequence corresl ) on-it ( miles its shown in Table 2 , and it work as active features . As a feature I ) OOI ,  3 , 172 ( lisjoint % a ( ; ures of tag sequence ma . I ) pings were retrieved . 1% is very important omake atomic thatures . 
We maxinfized A of active features with resl ) ect to total smnples using improved the iterative scaling algoritlun  . Figure 3 shows Ai of each feature . f(Q31,:P+ . m , ttOCA . Therea . renlany corresl ) on-dence 1 ) atterns with resl ) ec to the Englsh tag string , ' BEP+JJ' . 
Note that p ( t t ~ lQ ) is comtmted by the exponential model of ( 4 ) mid the conditional probability is the saine with empirical probal  ) ility in ( 7 )  . Since the w flue of p ( ylx ) shows the maxinmm likelihood , it is proved that each A was converged correctly . 
 #of (.% y ) occurs in sam , pie
P ( ylx)-n , um , ber of times of a : ( 7 ) hi feature selection step , we chose usefll features with the gain threshold of  0  . 008 . Figure 4 shows some feaures with a large gain . Anion \ then 1 , tag sequences mapping including ' RB ' are erroneous  . It means that position of adverb in Korean is very complicated to handle  . Also , proper noun in English aligned coInmon nouns in Korean 















AJMA




NNIN2+PPCA1+VBMA+ENTE


NN IN2 + PPCA1 + VBMA8 . 8520 8 . 6787 8 . 2628 7 . 2379 7 . 1372 6 . 9909 8 . 8402 6 . 8308 6 . 4256 6 . 4250
Figure 3: A\[PRP \] you ~/'\ [ PN\]cJ ~ ( dangs in ) "-\[ PPAU\]~- ( eun ) \[RB\]usually\[ADCO\]EttX41_~ ( dachero ) HVP\]have , /\[ NNIN2\]~uf~(ilbanseok )
TO\]to/~\[PPAD\]0 tl(e)\[VBP\]take ; /\[ VBMA\]N ( anj ) , \[ ENCO3\]O ~ OIE~(ayaman)\[JJ\]regular .   .   .   .   .   . ':-:-\[AX\]"$F ( ha ) \[ NN\]seating/'\[ENTE\]LE ( nda ) 
Figure 5: Best Lexical alignment because of tagging errors . Note that in the case of ' PN+PPCA2+PPAD+VBMA' , it is not an adjacent string but an interrupted string  . It means t tlat a verb in English generally map to a verb taking as argument the accusative and adverbial postposition in Korean  . 
One way of testing usefulness of our method is to construct structured aligned bilingual sentences  . 
Table 3 shows lexical alignments using tag sequence al ignments drawn from our algorithm for a given sentence  , ' you usually have to take regular seating - dang sine undach cwilbans cok can jayaman handa ' and Figure  5 shows the best lexical alignment of the sentence . 
We conducted the exi ) eriment on 100 sentences composed of words in length 14 or less and siln-lilY chose the most likely paths  . Astim result , the accuray was about 71 . 1~ . It shows that we can partly use the tag sequence alignments for lexical alignments  . We will extend tlle structural mapping model with consideration to the lexical information  . 
The parm neters , the conditional probabilities about stuctural mappings will be embedded in a statistical model  . Table 4 shows conditional probabilities of seine features according to ' DT+NN'  . In general , determiner is translated into NULL or adnominal word in Korean  . 
7 Conclusion
When aligning Englist > Korean sentences , the differences of word order and word unit require structural information  . Fortiffs reason , we tried structural tag c(x , y ) 162 . 0 C 45 . 00 39 . 00 25 . 00 9 . 00 8 . 00 7 . 00 6 . 00 6 . 00 4 . 00 4 . 00
P ( Ylx ) 0 . 4247 0 . 1180 0 . 0996 0 . 0655 0 . 0236 0 . 0210 0 . 0183 0 . 0157 0 . 0157 0 . 0105 0 . 0105

English are + prepared are + carefulam + healthy is + new am+suream+richis+selfishis+patrioticis + reasonable is + reprehensible is + helpful 
Korean ~ kllKl+0~+?~+~ LI El-0-94   8i ~ J 8t + ~ LIC\[~ X\[+01   0191~+01+~LIEt   011~  ; Xt+9\[+~+EF'NPJ~+01 + g~ . ~N+01 + . Elofactive features in At ~






I)T-FNN etct ~






ADCO eI ; cp(t~l*~)0 . 524131 0 . 15161 0 . 091036 0 . 063515 0 . 058322 0 . 05768 0 . 049622
Table 4: Conditional Probability string mapping using maximum entropy modeling and feature selection concept  . We devised an lodel that generates a English tag string given a Korean tag string  . From initial active structural features , useful features are extended by feature selection . 
Tile retrieved features and parameters can be embedded in statistical macl fine translation and reduce the complexity of searching  . We showed that they can helpful to construct structured aligned bilingual sentences  . 

Adam L . Berger , Peter F . Brown , Stephen A.
Della Pietra , Vincent J . Della Pietra , John R.
Gillett , John D . Lafferty , Robert L . Mercer , Harry Printz , and Lubos Ures .  1994 . The Call die system for machine translation , hi Proceedings of the ARPA Conference on Human Language Technology  , Plains borough , New Jersey . 
Adam L . Berger , Stephen A . Della Pietra , and Vincent J . Della Pietra .  1996 . A maximum entropy approacll to natural anguage processing  . Computational Linguistics , 22(1):39-73 . 
Peter F . Brown , John Cocke , Stephen A . Della Pietra , Vincent J . Della Pietra , Fredrick Jelinek , John D . Lafferty , Robert L . Mercer , and Paul S . 
Rooss in .  1990 . A statistical approach to nlachine translation . Computational Linguistics , 16(2):79-Della Pietra , Robert L . Mercer .  1993 . The math-enlatics of statistical machine translation : pa- 

























PPCA 1+AJMA
PPCA 1+AJMA+ENTE










PPAD+VBMA+PEr,~;P ( ylx)/~L(Afi ) 9 . 8687 0 . 1722 0 . 0194 9 . 6780 0 . 3265 0 . 0192 9 . 2799 0 . 2449 0 . 0190 9 . 6343 0 . 2450 0 . 0190 9 . 9542 0,3269 0 . 0189 9 . 6720 0 . 2941 0 . 0188 9 . 2724 0 . 1961 0 . 0188 8 . 7481 0 . 1225 0 . 0182 9 . 1397 0 . 1337 0 . 0180 9 . 5634 0 . 2307 0 . 0180 9 . 2604 0 . 1730 0 . 0180 9 . 2445 0 . 1548 0 . 0177 9 . 2564 0 . 1548 0 . 0177 8 . 5435 0 . 0934 0 . 0177 9 . 1597 0 . 1470 0,0176 8 . 9928 0 . 1278 0 . 0174 9 . 1511 0 . 1704 0 . 0173 9 . 1636 0 . 1706 0,0173
English send+Nm+to--is+more+thanthe+two smarter + than serving + to is are 

Harvard to + himto+her ?? should+be
English+books request+to+send was+thrown + to were +sent+to 

Korean 21+   N+-0tF +- ~ L\]I- h20+~ ? + N~H + ~ PA + N+L ~ OII3tI+-~+U  ~  -0I+OA   -01+~+H 
IBM+~-3+OIl ) II~LI+)F ? ?
Ol+Oi OI+N-N+~Ll l+Kf~+5/N bF ~011?t1+c3   ~1XI+~+FA Figured : Some fbatm'es with a large gain Taga lignment  ( km ( litiolml Lexical aliglul w , ntl ) lP:PN+I ) PAU 0 . 150109 you:dangsin+cunlt\]3:AI)CO 0 . 14219 3usmtlly : dachero II , B:NNIN2 + PI'AI ) 0 . 038105 usually:ilbanseol?-l-eIIVP+TO:I~N ( JO3-bAX+I"NTE 0 , 982839 have + to:ayaman+handa VBP:P1 ) AI ) + VBMA 0 . 05022 , l take : e + anj VBP:VBMA+F , NCO3 + AX+I , ; NTE 0 . 011110 take : anjay + aman-Fha+nda VI3P:P1 ) AI ) + VI~MA+ENCO3--AX-I-ENT\] , ~ 0 , 0 01851 take : e-Fanjay aimm+h and a V I3P -FJJ : NNIN2+PI   ) AI ) -\]- VBMA 0 . 057657 take-t-regular : ilbansenk+e+ , all j . I . J+NN:NNIN 20 . 581791 regular+seating : ill ) anseokan ( , 3: l , exi ( : alaligmnents using tag alignments rameter estimation  . Computational Linguistics , 19(2):263-311 . 
Stanley F . Chert .  1993 . Aligning sentences in bilingual corpora using lexical information  . Inl'rocccd-ings of ACL , 71, 916 . 
A . P . Dempster , N . M . Laird and 1) .  13 . lubin . 
1976 . Maximum likelihood fi'om in complet , e data via the EM algorithm . The Royal , S'tatistics Society , 39(B ) 205-237 . 
Willia in A . Gale , Kenneth W . Church .  1993 . A program fbraligning senten ( : esin bilingual ( - orl ) ora . 
Coml ) utational Linguistics , \]9:75-102.
Frederick Jelinek . \]997. Statistical Methods for
Speech Recognition MIT Press.
Marin Kay , Martin Roscheisen .  1993 . Text-translation alignment . Computational Linguistics , 19:121-142 . 
Julian Kupiec .  1993 . An algorithm tbr finding noun phrase corresl ) onden ccs in bilingual corl ) ola . In
Proceedings of ACL 31, 1722.
Yuji Matsmno ~ o , Hiroyuki Ishimoto , Takehito Ut-sure .  1993 . Structural in atching of para . llel texts . 
In Proceedings of ACL3I , 2330.
I . Dan Melame ( l .  1997 . A word-to-word model of translation equivalence . In PT vcccdings of ACL35/EACL8, 1 . 6-23 . 
Frmlz Josef Ochmid Ilans Wel ) cr .  1 . 998 . hnt ) rov-ing Statistical Natural Language Translation with Categories and Rules  . In Procccdings of ACL36/COLING , 985-989 . 
Stephen A . Della Pietra , Vincent J . Della Pietra , John D . La . tl'erty .  1997 . llnducing features of ran-dora fields . IEEE ~ I Yans actions on Pattern Analysis and Machine Intelligence  ,  19(4):380-393 . 
Frank Smadja , Kathleen R . McKeown , and Vasileios Hatziw ~ ssiloglou .  1996 . Translating collocations fi ) r bilingual lexicons : A statistical approa ( :h . 
Computational Linguistics , 22(1):138.
Kengo Sate 1998 . Maximum Entrol ) y Model Learning of the Translation Rules . In Procc cdinfls of
ACL35/COLING , 1171-1175.
Jung H . Shin , Y ( mngS . Han , and Key-Sun
Choi .  1996 . Bilingual knowledge acquisition from Korean English paral Mcort  ) us using aligmnent method . In Proceedings of COLING96 . 
C . Tilhnann , S . Vogel , H . Ney , and A . Zubiaga . 
1997 . AI)Pt ) ased sea . rch using monotone a . lign-ments in statistical translation . In Procccdings of
ACL35/EACL 8, 289-296.
Ye-Yi Wa . ng and Alex Waibel .  1997 . Decoding algorithm in statistical machine translation  . In Pro-cccd infls of ACL35/EACL 8, 366-372 . 
Ye-Yi Wang and Alex Waibel .  1998 . Modeling with structures in machine translation . In Procccdings of ACL36/COLING Dekai Wu 1996 . At ) oly nonlial-time algorithm for statistical machine translation  . In Proceeding of
ACL 34.

