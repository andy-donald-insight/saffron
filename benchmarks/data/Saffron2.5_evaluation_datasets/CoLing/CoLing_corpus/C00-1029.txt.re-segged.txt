A Class-based Probabilistic approach to Structural 
Disambiguation
Stephen Clark and David Weir
School of Cognitive and Computing Sciences
University of Sussex
Brighton , BN19 IIQ , UK
stephec : l_,david ~ rOcogs , usx.ac.uk

Knowledge of which words are able to fill p~rtic -ular argum  . ent slots of a predicate can be used tbr structural disambiguation  . This paper describes a proposal : for acquiring such knowledge  , and in line with much of the recent work in this area  , a probabilistic approach is taken . We develop a novel way of using a semantic hierarchy to estimate the probabilities  , and demonstrate the general approach using a prepositional phrase at ta  . chment experiment . 
1 Introduction
Knowledge of which words are able to fill particular a  . rgument slots of a . l?redl ca . teca , n be used t br structural disa . mbiguation . In the following example ( Charnial ~ ,  1993) , the fact that dog , rather than prize , is often the su . 1) ject of r'lm , cant ) e used to decide on the attachment site of the relative clause : Fred awarded aprize for the dog that ran the fastest We describe a proposal for acquiring such knowledge  , and as in other recent work in this area ( Resnik , 1993; l , i and A be , t998) , a probabilistic approach is taken . Using probabilities accords with the intuition that there are no absolute constraints on the arguments of predicates  , bu . t rather that constraints are satisfied to a certain degree  ( Resnik ,  1993) . Unfortunately , defining probabilities in terms of words leads to a model with a vast number of parameters  , resulting in a sparse data problem . To overcome this , we propose to define a probability model in terms of senses from a semantic hierarchy  , exploiting the fact that senses of nouns can be grouped together into semantically similar classes  . 
We use the semantic hierarchy of noun senses in WordNet  ( Fellbamn ,  1 . 998) , which consists of qexicalised concepts ' related by the qs-a-kind-of'relation  . If c'isakind of c , then c is a hy-pcrnym of c ' , and c ' a hyponym of c . Counts are passed u . p the hierarchy fl ' om the senses of nouns appearing in the data  . Thus if catchicl ~ cn a . p-pears in the data , th . ecount for this item passesu \]) to ( meat , ( good , and all the other hypernyms of that sense of chicken  . 1In . order to estimate the probability that a sense of chich cn~tp pea  . rs as the object of the verb cat , we represent ( chicken using a . suitable hypern 3qn , such as (: eood ) , and base our probability estimate on that instead  . The level at which ( chicken ) is represented is crucia hit should be high enough for adequate counts to have accumulated  , but not too high so that the hypernym is no longer representative of  ( chicken . An exanlple of a hypernym whidl would be too high is  ( erttity )   , as not all entities are semantically similar with respect othe object position  ot7 cat . 
The problem of choosing an appropria . telvel in the h . ierarchy at which to represent a particular noun sense  ( given a predicate and argument position ) has been investigated by Resnik ( 1993 )  , Li and Abe (1998) and ll , iba , s(1995) . 
The learning mechanism presented \] lore is a novel approach based on tinding semantically similar sets of concepts in a hierarchy  . We demonstrate the effectiveness of our approach using a PP-attachment experiment  . 
2 The Input Data and Semantic

The data used to estimate the probal ) ilities is a multiset of ' cooccurrence triples ': a noun I We use italics when referring to words  , ~Lndangled bra . ckets for concepts . This notation does not alwa . yspick out a concept uniquely , but the context should make cle ~ n : the concept being referred to  . 
194\]enltna . , verb len , in a , and argunien/,i ) osition . 2l , et the li ; i livc . r so of verbs ~ argll lilent positions aud , lOtl BS tha . t call appear in the input data . be denoted " l ) = " Vl ~ . . . ~' vkv ~"\] ~ . , :  , ,  .  , ,  .   .   .  , , . ,, ~ a , dN =', ~,, .   .   . , ", J,H,~'esi , e,-tiw:ly . Sucll datac ~ ui I ) e obtMned fro , , , a . tree-ban , s , or from a . shallow pa:rser . Note tha . twe dolie , distinguish I ) etwee , ia . lternative seiises of ve , '\]) s~a , ll dass U If let ha . teach \] llsta . llCeO\[a . no , illi , Itile data , refel?s to exactly olle conc , ept . 
Thesei-i-la . iiichiel : a . rchyused is the tie , Illhy-\[)efltyl Ilta . xo:nonly of\/Vo,'dNet(vel'SiOll\] . .6) . : 3 l , e ~ (7, '= el , .   .   .   , Chc betile sot ; of concepts in WordNet ( lq: , -~ 66 , 000) . A concel , t is rel ) re-sented in \ ? or dNet 1 , y a synset : ase lo 1' syilolly - , nous words which cat , I ) < 7: used to ( lenot clha . 1c . once i ) t . I ! kil7exa . nll ) iO ~ the CO liC ; el ) l,~co(' . a . iile ~ a . siiithe(\[rtlg~iS represented l ) yl . he following synset : cocain e , co(-ai't ~ , coD(; ,   . , no'w , C . l , et syn(c)C ; Vl , e the syll scl ; I ' or the (: olicel ) tc , a . dletc , , ( , , . )-cI""msy ,, (+:) I , ethe . set of concepts that (; a , ii be denoted by the llO , lli17 . .
The \] liera . rc\]U has the stl'llCtlll'eO\[a directed acyciicgi'a  . l , \] i : althouglithen unll ) er of nodes in the gra . phwil ; hlllO , ' e1, ha . l/olle\])al'elll . is only a , rOtllid citei)er(tent of the total . The edges ill the graph \[' or ni what we call the dii'ecl  . - is a rela--Lion(dire (' . t-is;, . CC'x') . l , et is a = d ii'ect-is a . X be , lietI'a , ll Sitiv c ~ re\[lexive( ; \] OStlreOf(lirect-isa , sot\]iat(c / , c ) ~\] sa :=> cisa\]iy \]) ernynlo ; \[' (/ ; a . ndlet ? ~= c'\[(c',c)m is a . I ) e the set consisting of the concept c and all of its hyponyn is  . Thus , the set (* ood ) conta . in sall the concel ) tsw tiich : q , rekinds of food , in clllditig ( food ) . 
Note tha . twords in the data canal ) pear in SyllSCts a , liy\vh01'0 ill the hiel ; archy . Even COll-cel)ts Sllcha . s(entity ) ~ which a , l ) p e . ar1 , eart \] iel : O Otel " the hierarchy , have synsets containing words which may a . pl ) ear hithed a . ta . The synset for ( entity ) is c'n titg , something , and the words cn tit ; qa . nd something (';/, llapl ) ear in the a . rgulnent positions of verbs in the data . 
3 Probability Estimation
The problem being a . ddressed in this section is to estimate p(civ , r ) , for cCC , v < P , and 2Only verbs a . reconsidered here , but this work applies to other predicates which take a  . rgun mnts that can bcorga . nised into a semantic hierarchy . 
3 When w crefer ( . o concepts ill\'VoMNct , wcn lea . n concepts ill WordNet's nomita , xonomy . 
rC'R . . The I~roba . bilityp(eiv,r ) is the 1) rob-ability tha . tsome lie \ illinsyn ( c ) ~ when dellOfitlg cone et ) tc , appears in position ' r of " verl ) ' v ( given ra . ndv ) . Using the relative clause ex-anl piefl ' Otiltile hi  , reduction > the p , ' obal ) ilities p((doglru , z , subj ) a , ndp((prize ) lrv . ',z,sub . i ) ca . n betoni_pared to decide on the attachluent site f li  , i Iredawa ' rdedap ? ' izcJotiit c dogt/tatran ~ l  , ; f ,  . ~:~; . , . : . We expe~t1'(( dog ) l ""', ~ m, . i ) to 1, egrea . ter than p((pr?ze ) l'l'zt'~z , subj) . Althougt , the \[' OCll SiSO 117 , (c1~ , , , ) , the tochniqlies described here can be used to estimat  , e other lm ) babilities , such a . sp(c , fly ) . ( infa . ct , the latter prol ) alfii-il , y is used hit he Pl > - a ? ta . clinientCXl ) erhnents de . qcril)ed in Section 5 . ) Using n , axinlun/likelihood to cstinia . tc\])(C\['V,'/')i Siiotvia . hie1) eca . use of the liugcnuill-t)er of I ) al'al , lor . or si \] ivo \] red . ~/ lally COl IIt , iII a . tiollsO\[C,'l ) a . lld 7' will/lotOCCIII " in the data . '1 ' oreduce then tii , il ) ei ' of i ) a . ranlet crs which need to \]) eestima . ted ~ we utilise tilefa . ct tha . tCOllCeptsCall be groupedini ; ocla . sses , a . nd\]'e presentCIlS-ing a class (/ , for some hypernyn lc'ofc . I low-ever , p(c'lv , r)ca . n not be used as a . nestiniate of v((l ,   ,   ,   , ') , as V ((: I"' , "') is give . l , y the foliowi , , g : E . ( c'l , , , . , . ) = v((:'l .   ,   , ) < : " C ~ The probal ) ility ~)((" l '~") i , ,c , ' eases as c'moves up the hiera . rchy . For example , s ) (( eooa ) l , ; a Z , oi , . i ) is , , or a good estiu , a , te of 1 , ((chicken)leat , obj ) . What can be done though , is to condition on sets of concepts , and use the probability p(v\[c' , r ) . If it ca:nbe shown tha . tp(v\[c',r ), for some hypernym c'ofc , is a . 
reasoliable estilnate o1'v(vlc , v ) , then we have a . 
wa . y of estiniati , igp(clv , r) . Toget\])(vie ; , r ) from l , ( dv ,   , ') i ~ ayes , ' , ie is , , sed : p(4* , , , ') p(vl ~ , "' v ( cl '' )  = 7  ) ~ The prol ) abilities p ( clr ) and p ( v\[r ) cm~be estimated using maximum likelihoodesti , n~tes , a . s the conditioning event is likely to occur often enough for sp  ; tl'se data not to be a problem . 
( Alternatively () tie could ha . ok-off top(c ) and ply ) respectively , or use a . linear combhia . tion of p(d ', ) a . , d\],(c ),, . , dP0,1v)a,dV ('0,,' espoc-tively . ) The formula . e for these estimates will I , e give , , shortly . This only leaves plY\[c , r) . Theing p(eat\](food ) , ohj ) , or something similar . The following proposition shows that if p(vlc " , r ) is
I the same for each c"inc' , where c'is some hypernym of c , then p(v\]c' , r ) will be equal top ( v\[c , r ) :-- I\] , (~1c" , 7 . ): / ~ for all c "~ c'~\],(vlc',7 . )=
The proof is as follows: , \] , (~17") \] , ( v l 7 , 7") = \ ]  ,  (71~ ,   , ) ~-~' ( L17") ~ p(c " lv , 7 , ) z , ( c'l , ') - clinG1_z , ( ~ l ~') V"\] , ( ~ d ' ,  7 . ~\]'( c'17) z , ( c~lT , ) ~'' p(~lT)_2 a , ~ p(c"l , 0 p(c'lT , ) _
Cling I = / ~
So in order to estimate p(v\[c , r ) , we need a way of searching for a set c ' , where c'isa hypernym of c , which consists of concepts c " which have similar p  ( v\]c " , r ) . Of conrse we cannot expect to find a set consisting of concepts which have identical p  ( vlc " , r ) , which the proposition strictly requires , but if the p(vlc " , 7") are simila . r , then we can expect p(vld , r ) to be a . reasonable estimate of p(vlc , 7") . We refer to the set c ' as the % imilarity-class ' of c  , and the suitable hypernym , cl , a stop(c , v , r ) . The next section explains how we determine similarity classes  . The maxim . urn likelihood estimates for the relevant probabilities m : e given in Ta  . ble 1 . 4  4 Finding Similarity-classes First we explain how we determine if a set of concepts has similar p  ( vlc " , r ) for each concept c " in the set . Then we explain how we determine top(c,v,r ) . 
4 Since we area . ssuming the data . is not sense dis-a . mbiguated , f , : eq(c , v , r ) cannot be obtained by simply counting senses . The standard approach , which is adopted here , is to estimate f l'eq(c , v , r ) by distributing the count to reach noun n in syn ( c ) evenly among all senses of the noun . Yarowsky ( 1992 ) and \] esnik ( 1993 ) explain how the noise introduced by this technique tends to dissipate as counts are passed up the hierarchy  . 
Table 1: Maximum Likelihood Esti:lnates freq(c , v , r ) is the number of ( n , v , r ) triples in the data in which n is being used to denote c  . 
fl'eq(c,r)Ev'EV freq(c,v ', r)
P ( CI ?') : " frcq(r)--Ev'EVE dccfrcq(c' , v ' , r ) freq(v , r)E c'E c freq(ct , v , r)/)(VlT")-freq(r):Zv , EVZ c , ccfreq(c' , v' , r )\] ( view ,  7" ) -- freq ( c-i " v'r ) Z ~" c ~ 77 freq ( c't'v'r ) rreq ( d , ,-) = Ev , evE~ , ,~Tf , -eq(~" , '~ , ' , ,-) Tile method used for comparing the p(vlc " , r ) for c " in some set c ' , is based on the technique ill Clark and Weir ( 1999 ) used for tinding homogeneou sets of concepts in the WordNet noun hierarchy  . Rather than directly compare estimates of p(vlc " , r ) , which are likely to be unreliable , we consider the children of c ' , and use estimates based on counts which have accumulated I  ,  /  , /at the children . If c ' has children Q , %, .   .   . , c , , , , ,
I we compare e '(~ l <, ") for each i . Th ~ s is an
I a . p proximation , but if the p(vlc,r ) arc similar,
I then we assume that the p(vlc " , r ) for c"inc ' are similar too . 
To deterl nine whether the children of some ? .  ,  . / is the hyperny , ~ c'have simila , '\]'('~'14) where c~ith child , we apply a X2 test to a contingency tM ) leoffrequency counts . Table 2 shows some e ? a . mplefrequencies for c'equM to ( nutriment ) , in the ol ) ject position of cat . The figures in brackets are the expected values , based on the marginal to tMs in the table . The null hypothesis of the test is that p ( vl@r ) is the same for each i . librTM)1e2 the null hypothesis is tlmt , It brevery child , ci , of ( nutr?ment , the probability p(cat lc ~ , obj ) is the same . 
The loglikelihood X 2 statistic corresponding to TM ) le2 is 4 . 8 . The loglikelihood X 2 statistic is used rather than the Pearson's X 2 statistic because it is thought to be more appropriate when the counts in the contingency table are low  ( \] ) unning ,  1993) . This tends to occur when the test is being applied to a set of concepts near the foot of the hierarchy  , s We compared 5Fisher , sexa . ct test could be used for tables with low counts , but we do not do so because tables dolninated by low counts are likely to have a  . high percentage of noise , due to the way counts for a noun are split ~ unong cimilk  ) < meal )   ( course )   ( d?s ~ )   ( del?cacy ) fr\[N ( ~ , cat , oh . i ) o . o(o . (~)
J . a(l . r ) s . a(s . r ) o . a ( ~ . s)\]5 . dr , . ~, q(~,oh . i)-l't'(xI ( ~, cal , ohj ) 9 . 0( s . .,~) rs . o(so . .o ) 24 . r (24 . a ) s2 . a(s  ~ . .0) 2r . 4(~ s . 9) 221 . 4r , 4q ( ~, oh . i ) =
E , , ~ v r , . ~, q(W , . , , oh . i ) 9 . 0 86 . 5 26 . 0 87 . 6 27 . 7 thel ) erformance of loglikelihood X 2 and Pear-son's X ~2 using the l > P-~tttaehment experhnent described in Section  5  . It was found that the loglikelihood ~2 test ; did perform slightly bet-t('r . \] " or a signitic ~ n celw ; Iot ' 0 . 05 ( which is the level used in the exl)eriments ) , with 4 degrees of freedom , the critical w due is 1 , 1 . 86 ( llowell , ; 1!197) . Thus in this ca . se , tllenullhyl ~ othesis would not be rejected . 
In order to determine top(c , v , r ) , we conl parel , (vl~7 , v ) re , : the children of the hypernyms of c . hlitially top(c , ' v , r ) ix assigned to I ) e the con-eet ) t c itself . Then , l > y worldngUll the hierarrclly , top ((: , ' V , r ) is reassigned to I ) (' successive hyl ) er-nyms of cuntil the siblings of to l ) ( C , ~7+7') have siglfifi(:a . ntly different prol ) abilities . In cases where a . concept has more than one I ) a . J ' ent , the parent is chosen which results in tile lowest  :\~2 w flue as this indicates the p ( v\[U , r ) are more simila . r . The set top(c,v,r ) is the sinfi\]a . rity-cla . ss of ct ' or verb v and position r . 
Th ( ; next section provides evidence that tile technique for choosing lOl  ) ( C , v , r ) , which we call the ' simihu'ity-class'technique , does select an appropriate level of generalisation  . 
5 Exper iments us ing PP - a t tachment ambigu i ty The l>P-atta  . chme:nt problem we address considers 4-tuples of the form v , : , t , ,pr , n2 , and thel ) robleln is to decide wl lethertile prel ) o-sitional phrase pr n2 attaches to the verl > vorthe 71oun nl . For exat nl ) le , in the following cas ( ; timl ) rol ) lent is to decide whether alternative senses . Y Verely on the log-likelihood X , 2 test returning a , non-significant result in these cases . 
J ) ' omminister attaches to a waii or approva ha . waitapt ) 7' owd from minister We chose the l~P-attachn~entl ) roblen l be ca . use Pl >- attaehment is a perw, . sive form of ambiguity , and there exists ta . ndard training and text da . ta ~ which ma . kes for easy comparisons with other a . pproache ~ s . This p7'oblenl has been tackled by an unlber of resea . rehers , lh'ill and Resnik (1994) , Ratnal ) arkhi et al(\]994) , Collins (1995) , Za-w : el and l ) aelemans ( \] 997 ) all report results between 81% and 85% , with Stetina . and Nagao (\]997) tel)erring a result of 88% , which matches lhehunm , t+l > erf ' or nlan ( ; e on this task rel ) orted by
Ratnal > arkhi (% al . (199.'1).
Althougllth ( ' l ) l ) -attachnwntl ) roblem has chara ( ' teristics that n , a . keitsuita . blef or (' valua . - t;ion , it ; I ) resents ai nuch bigger sparse data . t)\]:ol)-le,mtlla . n would 1 ) eexl ) ected in other l ) roblems such as relative ( : laus cat Sadlment . The reason for this is that we need 1; (7 cot , sider how ~ lC ( )l ~ - Cel ) t is associated with combi ~ zations of predicates and prel  ) ositions . T\]leal ) proach described 11( ; 7"( ; uses prolml ) ilities of the Ibrnlp(c , prlv ) , u , d  ~ , , ( c . z , , l , , . ,), who ,; o , ~~ ~, l (, + ~) . This . lea , is that for many predicate/prel ) osition combinations which occur infl'equently in the d~ta  . , there are few examples of n2 which ca . n be used lot populating Wo 7' dNet in these cases . Despite this , we were still able to carry out a new l . lu-ation by considering subsets of the test ( ta . ta for which the relewmt predicate ~ preposition com -I  ) inations did occur frequently in tit (  ; training data ,  . 
We deckle on tile a . tta('hnmnt site by compar -= argnaxl , (c , p , ' lv)c , z1 = argmax p(c , prlTq ) The sense of n2 is chosen which maximises the relevant probability in each potential attachment case  . If p(c , , , p , jv ) is greater than 1) (% , : m'l~l ) , the attachment is made to v , otherwise to nl . If n2 is not in WordNet we compare p ( prlv ) and p ( prl~t  ~ )  . Probabilities of the form p(c , prlv ) and p(c , prl~tl ) are used rather than p(clv , pr ) and p(cl~l , p , j , because the association between the preposition and v and ~ q contains useful information  . In fact , for a lot of cases this intbrmation alone can be used to decide on the correct attachment site_ The original corpus-based method of \] Jindle and ll  . ooth (1993) used exactly this information . Thus the method described here can be thought of as Hindle and Rooth's method with additional classbased information about  n2  . 
In order to estimate p(c , ,  , prlv ) ( and p(C , ~l , ln'l , , , , ) ) we apply the same procedure as described in Section  3  , first rewriting the probability using Bayes ' rule : p  , , . ) p(c , , , p , . ) p(c , , , j , , lv)--p(vlcv,v(v)p , . )!'( P'q c , , ) : p(dc , , , l , ( v ) The probabilities p(c . ~ ) and p ( v ) can be estimated using maximum likelihood estimates  , a . ndp(vlcv , p , ') and j , ( p , ' lc ,) can be esti-m . ated using maximum likelihood estimates of p ( vltop ( c  ~ , v , p , ') , pr ) and p(prltop (% , pr )) respectively .   6 We used the training and test data described in l/  . atn . a parkhi et al (1994:) , which , was taken Doln the Penn %: eebank and has now become the standard dataset for this task  . The dataset consists of tuples of the form ( v , ~ zl , p ~' , n2) , together with the attachment site for each tuple . There is also a development set to prevent implicit training on the test set during development  . \ ~ e extracted ( v , pr , ' ~2) and ( hi , pr ,   , z2 ) ~ln Section 4 we only gave the procedure for determining top ( c  ~ , v , pr ) , but top ( c  ~ , pr ) can be determined in an analogous fashion . 
triples from the training set , and in order to increase the number of training triples  , we also extracted triples Kern unambiguous cases of at-tachlnent in the Penn %' eebank  . We preprocessed the training and test data by \] emmatising the words  , replacing numerical amounts with the words ~ definite_quantity '  , replacing monetary amounts with the words ' sum_ol L money ' etc  . We then ignored those triples in the resulting training set  ( but not test set ) for which 7 z2 was not in WordNet , which left a total of 66 , 881 triples of training data . . The test set contains 3,097 examples . 
Table 3 gives seine examples of the extent to which the similarity-class technique is generalising  , using the training data just described , and a significance level of 0 . 05 . 
The chosen hypernym is shown in Ul ) per case . Note that the WordNet hierarchy consists of nine separate subhierarchies  , headed by such concepts as ( entity > , ( abstraction ) , ( psychological ~ eature ) , bnt we assume the existence of a single root which dominates each of the subhierarchies  , which is referred to as ( root > . In cases where WordNet is very sparsely populated  , it is preferable to go to ( root ) , rather than stay at the root of one of the subhierarchies where the data may be noisy or too sparse to be o\[' any use  . The table shows that with the amount of data ava . ilable from the Treebank , the similarity-class technique is selecting a . level at or close to ( root > in many cases . 
We compared the similarity-class technique with fixing the level of generalisation  . Two tixed levels were used : the root of the entire hiera J'-chy  ( ( root > )  , and the set consisting of the roots of each of the  9 sul > hierarchies . The procedure which always selects ( root ignores any information about ~ z2 , and is equivalent o comparing p(prlv ) and p(prl , h ) , which is the ltindle and Rooth approach . The results on the 3 , 097 test cases are shown in Table 4 . We used a . significance level a of 0 . 05 t br the X2 test . rAs the table shows , the disambiguation accuracy is below the state of the art  . However , the results are comparable with those of l , i and r Similar results were obtained using alternative lvels of signifiea  . nce . Rather than simply selecting a value for a , such as 0 . 05 , a ' can be tree , ted as a parameter of the model , whose optimum value ca Jl be obtained by running the disambiguation method on some heldout supervised data  . 
198' l ' al ) le 3: I low the simila . rity-cla . ss technique choose stop(c , v , pr ) a . lld top(c , nq , pr ) (? Zl ,  \])?' , C ) IIy pernyms of c(bid , for , ( company )) (~ i ~ io , , , ,i , , , ,<c~sh > ) ( v , l " , c ) (' l , ol , i , J!q , OJ ; < ttansaction >) ( clo . 5" , 8 , ( t\[ , , < def i n i t e_quant ity >) ( ,   , ~  , wiU , , , < oeeioi a ~> ) < company > < establishment > ( organisation ) < social_group > ( GROUP ) < root > ( risk ) <venture > ( task ) (+york > ( activity ) < act > ( ROOT > ( cash > ( curtency ) (monetary mystem )   ( asset ) (POSSESS I ON ) (root ) < transaction ) < group_action ) < act > ( ROOT ) < DRFII~ITE_QUAN = TY > < mea~ure > < abst taction > < root > < o ~ i ~ ial > < adjudicator > < perso ~><li~e~orm><CAUSA~  , ~G~ , NT > < e~tity > < root > ' l'able < 1: ( , o  ~ ) lete test set : ~ ( )97 test cases ( ~ eneralisation technique % co:red . 
SiJnila.rity-cla.ss 80.3
Select root of sub-hiera . rchy 77.9
Alwa , ys select ( root > 79.0
Table 5: ( root > 1) eing selected for 1) oth a . ttach-nlent1 ) oints \] 713 test cases ( hmeralisa + tiontechnique

Select root of subhierarchy
Ahva . ysselect(root > % tort'cot 90 . 3 811 . 4 79 . 6 Abe (119!)8) who a . dol)t a similar a . l > proa (: hus-i:ng\VorclNet , but with a , differ < rotraining and test set . I , ia . ndAbeiml > rOVed on the l\[\]n-die and Rooth techn i  ( luel ) y1 . 5%, whh ; hisi , line with our results . As a . nevahla . tion of the simibu'ity-class tec\]lnique , the result is inconclusive . Therca . son for this is tha . t when the technique wa , s being used to estima . te\])(vlc , ,,\]) r)a . HdP (? ~ . : I\[c . ,zl , I )?') , in many cases tile root o1"1 lie hiera . rchywa . s being chosen as the apl > r Ol ) riat ; e level of genera . lisa . tion , due to a . sparsely popu-la . ted WordNet in tha . t insta . nce . Recall that this is la . rgely due to tit <', fa . ct that we a . rca . ttem ltt-ing to popula . te WordNet fbr comlt in a . tions of predic ~ tes~md prepositions . In such case stiles in lil ~ u ' ity-elass technique is not helping because there is very little or no inform a  . tiona . 1) otlt ~, 2 . saln an effort to obtahl more do . to , wea , pplicd the extraction heuristic of lla . tna . parkhi (1998) to \? all Street Journa . l text , which increased then untl ) er of training triples by ~ L factor of 111 . '\[' his only a . chievcd comparable results , however , presumably boca . use the high volume of noise in the dat ~ outweighs the benefit of the increase in da  . tasize .  \] . at na parkhi reports only 69% a . ccuracy to tTable 6: ( root > being select ( , ( I for at most one o1'ill ( , a . tl . a (' hnmnt points 1032 i . cst('asc . ~( l ( , , eral is a Ciolltechniqu (, ~%(' ol : re('t
Sitnilaril.y-cla.ss 88. I
Select root of sul ) - hierar (: hy 85.5
Alwa . ys select ( root ) 8, 5.6
In order to eva . lua . te the similarity-class technique further , we took those test cases for which tile root wa , snot being selected whenest ima . ting bet : t,J,(,,I ,, ~ . J , ') . +,, d\])('/, . 1I ~ , , . pv ) . n : \], is . pplied to 113 c~ses . The results ~ u ; e given in Table 5 . 
Wea . l so took those test cases for which the root was I ) eing selected when estimating + ~ t most one of p ( v\[c+ , ,pr ) a . ndp(,q\[c , ~, pr) . This a . pplie , dto\]032 test ca . so s . The results a . reshown in %> ble 6 . 
the extraction heuristic when applied to the \]% nnTree-ba  . nk ( excluding cases where the ln : e position is of ) . 
1996 Conclusions
We have shown that when instances of WordNet are well populated with examples of  n2  , the method described here for solving P1 ) -attachment ambiguities is highly accurate . 
When WordNet is sparsely populated , the method automatically resorts to comparing just the preposition and each of the potential attachment sites  , as the similarity-class technique will select root as the appropriate lvel of general-\]s at \ ] on for  n2 in such cases . We have also shown the similarity-class technique to be superior to using a fixed level of general \ ] sat\]on in WordNet  . 
Further work will look at how to integrate probabilitie such as p  ( clv , r ) into a model of dependency structure , similar to that of Collins ( 1996 ) and Collins ( 1997 )  , which can be used \[' or parse selection . However , knowledge of se-\]ectional preferences cannot by itself solve the problem of structural disambiguation  , and this further work will also look at using additional knowledge  , such a . ssubcategorisation information . 
References
Eric Brilla . nd Philip Resnik .  1994 . A rule-based approach to prel ) ositional phrase a . tta . chment disanfl ) iguation , in 1) ~ vcccdi'ngs of the . \[ iJ-Icc ~ th International Co ~@ rcncconC ' ompu -rational Linguistics  . 
Eugene(~harnia.k . 1993., 5' tali.slical Language
Lcarni'ng . The MIT Press.
Stephen Clark and l ) avid Weir .  1999 . An iterative approach to estimating frequencies over a semantic hierarchy  . In P ' lvcccdinqs of the Joint , 5 7GDA   2' ConJ ~ rcnccon Empirical Methods in Natural Language Proccs si'ng and Very Large  Co17~ora   , \ ]) ages 258265 . 
Michael Collins .  11995 . Prepositional phrase attachment through a backed -off model  . In Proceedings of the Thirdl ? or hshop on Very Large Cou  ) ora , pages 27-38 , Cambridge , 

Michael Collins .  1996 . A new statistical parser based on bigram lexical dependencies  . In P~vcecdings of the 3/tth Annual Meeting of the
ACL , pages 184-1911.
Michael Collins .  1997 . Three generative , lexicalised models for statistical parsing . In Pro-cccdings of the 351h A ~ mual Mccti'ng of the Association for Computational Linguistics  , pages 1623 . 
Ted\])unning .  1993 . Accurateluethods Ibr the statistics of surprise and coincidence  . Computational Linguistics , 19(1):61-74 . 
Christiane Fellbaum , editor .  1998 . WordNct Anl'2 lcctronic Lcxical Database . The MIT

Donaldltindle and Mats Rooth .  1993 . Structural ambiguity and lexical relations . Computational Linguistics , \]9(1): 103-120 . 
l ) avid Howell . 11997. Statistical Methods for
Psychology : ~ thcd . Duxbury Press.
Hang Li and Naoki Abe .  11998 . Genera . liz-ing case frames using a thesaurus and the MI ) L principle . Computational Linguislics , 24(2):17-244 . 
Adwaitla.t : na.parldli , Jeff Reynar , and Saliln
Roukos .  1994 . A maximum entropy model fbr prepositional phrase attachment  . In P , v-ccc dings of l , hcARI ) A Human Language " l~ch-nology Workshop , pages 250-255 . 
Adwait \]/ . at na parkhi .  1998 . Unsupervised statistical models tbr prepositional phrase attachment  . In P~vcccdings of thc , 5'cvcnlccnth hzicrnalionol ConJE'rencc on Computational 
Linguistics , Montreal , Canada , Aug.
Pllilip Rcsnik .  71993 .   , 5'clcction a~zd hdbrma-lion : A Class-Based Approach to l  , czical Relationships . Ph . l ) . thesis , University of Pennsylvania . 
Francesclibas .  1995 . On learning more appropriate selectional restrictions  . In Procccdings of the , 5'cvcnth ConJ~rcncc of the IJuropcan Chapter of the Association for Computational 
I , i ~ tguistics , l ) Ublin , I relal , d.
;liri Stetina and Makoto Na . gao .  1997 . Corpusbased PP attachment ambiguity resolution with a semantic dictionary  . In Proceedings of thcFiflh I?orteshop on Very Large Corpora  , pages 6680 , Beijing and ltong Kong . 
David Yarowsky .  11992 . Word-sense disambiguation using statistical models of Roger's categories trained on large corpora  . \] nP , v-cccd in . qs of COLING-92, pages 454-460 . 
Jakub Zaw : el and Walterl ) aelemans . 1997.
Melnory-based learning : Using similarity for smoothing  . In Proceedings of ACL/EACL-97, Madrid , Spain . 

