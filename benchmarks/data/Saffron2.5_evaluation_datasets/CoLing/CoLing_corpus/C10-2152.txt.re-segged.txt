Coling 2010: Poster Volume , pages 1327?1335,
Beijing , August 2010
MIEA : a Mutual Iterative Enhancement Approach for CrossDomain
Sentiment Classification

Qiong Wu1,2, Songbo Tan1, Xueqi Cheng1 and Miyi Duan1
1Institute of Computing Technology , Chinese Academy of Sciences
2 Graduate University of Chinese Academy of Sciences
{wuqiong,tansongbo}@software.ict.ac.cn

Abstract
Recent years have witnessed a large body of research works on cross-domain sentiment classification problem , where most of the research endeavors were based on a supervised learning strategy which builds models from only the labeled documents or only the labeled sentiment words . Unfortunately , such kind of supervised learning method usually fails to uncover the full knowledge between documents and sentiment words . Taking account of this limitation , in this paper , we propose an iterative reinforcement learning approach for cross-domain sentiment classification by simultaneously utilizing documents and words from both source domain and target domain.
Our new method can make full use of the reinforcement between documents and words by fusing four kinds of relationships between documents and words . Experimental results indicate that our new method can improve the performance of cross-domain sentiment classification dramatically.
1 Introduction
Sentiment classification is the task of determining the opinion ( e.g ., negative or positive ) of a given document . In recent years , it has drawn much attention with the increasing reviewing pages and blogs etc ., and it is very important for many applications , such as opinion mining and summarization ( e.g ., ( Ku et al , 2006; McDonald et al , 2007)).
In most cases , a variety of supervised classification methods can perform well in sentiment classification . This kind of methods requires a condition to guarantee the accuracy of classification : training data should have the same distribution with test data so that test data could share the information got from training data . So the labeled data in the same domain with test data is considered as the most valuable resources for the sentiment classification . However , such resources in different domains are very imbalanced.
In some traditional domains or domains of concern , many labeled sentiment data are freely available on the web , but in other domains , labeled sentiment data are scarce and it involves much human labor to manually label reliable sentiment data . The challenge is how to utilize labeled sentiment data in one domain ( that is , source domain ) for sentiment classification in another domain ( that is , target domain ). This raises an interesting task , cross-domain sentiment classification ( or sentiment transfer ). In this work , we focus on one typical kind of sentiment transfer problem , which utilizes only training data from source domain to improve sentiment classification performance for target domain , without any labeled data for the target domain ( e.g ., ( Andreevskaia and Bergler , 2008)).
In recent years , some studies have been conducted to deal with sentiment transfer problems.
However , most of the attempts rely on only the labeled documents ( Aue and Gamon , 2005; Tan et al , 2007; Tan et al , 2009; Wu et al , 2009) or the labeled sentiment words ( Gamon and Aue , 2005) to improve the performance of sentiment transfer , so this kind of methods fails to uncover the full knowledge between the documents and the sentiment words.
In fact , the opinion of a document can be determined by the interrelated documents as well as by the interrelated words , and this rule is also tenable when determining the opinion of a sentiment word . This rule is based on the following intuitive observations : (1) A document strongly linked with other positive ( negative ) documents could be considered as positive ( negative ); in the same way , a word strongly linked with other positive ( negative ) words could be considered as positive ( negative).
1327 (2) A document containing many positive ( negative ) words could be considered as positive ( negative ); similarly , a word appearing in many positive ( negative ) documents could be considered as positive ( negative).
Inspired by these observations , we aim to take into account all the four kinds of relationships among documents and words ( i.e . the relationships between documents , the relationships between words , the relationships between words and documents , and the relationships between documents and words ) in both source domain and target domain under a unified framework for sentiment transfer.
In this work , we propose an iterative reinforcement approach to implement the above idea.
The proposed approach makes full use of all the relationships among documents and words from both source domain and target domain to transfer information between domains . In our approach , the opinion of a document ( word ) is reinforced by the opinion of all its interrelated documents and words ; and the updated opinion of the document ( word ) will conversely reinforce the opinions of its interrelated documents and words.
That is to say , it is an iterative reinforcement process until it converges to a final result.
The contribution of our work is twofold . First , we extend the traditional sentiment-transfer methods by utilizing the full knowledge between interrelated documents and words . Second , we present a reinforcement approach to get the opinions of documents by making use of graph-ranking algorithm.
The proposed approach is evaluated on three domain-specific sentiment data sets . The experimental results show that our approach can dramatically improve the accuracy when transferred to another target domain . And we also conduct extensive experiments to investigate the parameters sensitivity . The results show that our algorithm is not sensitive to these parameters.
2 Proposed Methods 2.1 Problem Definition In this paper , we have two document sets : the test documents DU = { d1,?,dnd } where di is the term vector of the ith text document and each di?DU(i = 1,?,nd ) is unlabeled ; the training documents DL = { dnd+1,?,dnd+md } where dj represents the term vector of the jth text document and each dj?DL(j = nd+1,?,nd+md ) should have a label from a category set C = { negative , positive }. We assume the training dataset DL is from the interrelated but different domain with the test dataset DU . Also , we have two word sets : WU = { w1,?,wnw } is the word set of DU and each wi?WU ( i = 1,?,nw ) is unlabeled ; WL = { wnw+1,?,wnw+mw } is the word set of DL and each wj?WL(j = nw+1,?,nw+mw ) has a label from C.
Our objective is to maximize the accuracy of assigning a label in C to di?DU ( i = 1,?,nd ) utilizing the training data DL and WL in another domain.
The proposed algorithm is based on the following presumptions : (1) WL?WU??.
(2) The labels of documents appear both in the training data and the test data should be the same.
2.2 Overview
The proposed approach is inspired by graph-ranking algorithm whose idea is to give a node high score if it is strongly linked with other high-score nodes . Graph-ranking algorithm has been successfully used in many fields ( e.g . PageRank ( Brin et al 1999), LexRank ( Erkan and Radev , 2004)). We can get the following thoughts based on the ideas of PageRank and HITS ( Kleinberg , 1998): (1) If a document is strongly linked with other positive ( negative ) documents , it tends to be positive ( negative ); and if a word is strongly linked with other positive ( negative ) words , it tends to be positive ( negative).
(2) If a document contains many positive ( negative ) words , it tends to be positive ( negative ); and if a word appears in many positive ( negative ) documents , it tends to be positive ( negative).
Given the data points of documents and words , there are four kinds of relationships in our problem : z DD-Relationship : It denotes the relationships between documents , usually computed by their content similarity.
z WW-Relationship : It denotes the relationships between words , usually computed by knowledge-based approach or corpus-based approach.
z DW-Relationship : It denotes the relationships between documents and words , usu-a word in a document.
z WD-Relationship : It denotes the relationships between words and documents , usually computed by the relative importance of a document to a word.
Meanwhile , our problem refers to both source domain and target domain , so our approach considers eight relationships altogether : DDO-Relationship ( the relationships between DU and DL ), DDN-Relationship ( the relationships between DU ), WWO-Relationship ( the relationships between WU and WL ), WWN-Relationship ( the relationships between WU and WU ), DWO-Relationship ( the relationships between DU and WL ), DWN-Relationship ( the relationships between DU and WU ), WDO-Relationship ( the relationships between WU and DL ), WDN-Relationship ( the relationships between WU and DU ). The first four relationships are used to compute the sentiment scores of the documents , and the others are used to compute the sentiment scores of the words.
The iterative reinforcement approach could make full use of all the relationships in a unified framework . The framework of the proposed approach is illustrated in Figure 1.

Figure 1. Framework of the proposed approach The framework consists of a graph-building phase and an iterative reinforcement phase . In the graph-building phase , the input includes both the labeled data from source domain and the unlabeled data from target domain . The proposed approach builds four graphs based on these data to reflect the above relationships respectively.
For source-domain data , we initialize every document and word a score (?1? denotes positive , and ?-1? denotes negative ) to represent its degree of sentiment orientation , and we call it sentiment score ; for target-domain data , we set the initial sentiment scores to 0.
In the iterative reinforcement phase , our approach iteratively computes the sentiment scores of the documents and words based on the graphs.
When the algorithm converges , all the documents get their sentiment scores . If its sentiment score is between 0 and 1, the document should be classified as ? positive ?. The closer its sentiment score is near 1, the higher the ? positive ? degree is . Otherwise , if its sentiment score is between 0 and -1, the document should be classified as ? negative ?. The closer its sentiment score is near -1, the higher the ? negative ? degree is.
The algorithms of sentiment graph building and iterative reinforcement are described in details in the next sections , respectively.
2.3 Sentiment-Graph Building
Symbol Definition
In this section , we build four graphs to reflect eight relationships , and the meanings of symbols are shown in Table 1.
Relationship
Similarity matrix
Normalized form Neighbor matrix
DDO UL=[ULij]ndxmd LU ? Kndij
LL UnUn ?= ][
DDN UU=[UU ij]ndxnd UU ? Kndij
UU UnUn ?= ][
WWO VL=[VLij]nwxmw LV ? Knwij
LL VnVn ?= ][
WWN VU=[VU ij]nwxnw UV ? Knwij
UU VnVn ?= ][
DWO ML=[MLij]ndxmw LM ? Kndij
LL MnMn ?= ][
DWN MU=[MUij]ndxnw UM ? Kndij
UU MnMn ?= ][
WDO NL=[NLij]nwxmd LN ? Knwij
LL NnNn ?= ][
WDN NU=[NUij]nwxnd UN ? Knwij
UU NnNn ?= ][
Table 1: Symbol definition
In this table , the first column denotes the name of the relationship ; the second column denotes relationship ; in consideration of convergence , we normalize the similarity matrix , and the normalized form is listed in the third column ; in order to compute sentiment scores , we find the neighbors of a document or a word and the neighbor matrix is listed in the fourth column.
Document-to-Document Graph
We build an undirected graph whose nodes denote documents in both DL and DU and edges denote the content similarities between documents . If the content similarity between two documents is 0, there is no edge between the two nodes . Otherwise , there is an edge between the two nodes whose weight is the content similarity.
The edges in this graph are divided into two parts : edges between DU and DL ; edges between DU itself , so we build the graph in two steps.
(1) Create DU and DL Edges
The content similarity between two documents is computed with the cosine measure . We use an adjacency matrix UL to denote the similarity matrix between DU and DL . UL=[ULij]ndxmd is defined as follows : mdjndi dd dd
U ndji ndjiL ij ,...,1,,...,1, ==? ?= + + (1) The weight associated with word w is computed with tfwidfw where tfw is the frequency of word w in the document and idfw is the inverse document frequency of word w , i.e . 1+log(N/nw ), where N is the total number of documents and nw is the number of documents containing word w in a data set.
In consideration of convergence , we normalize UL to LU ? by making the sum of each row equal to 1: ?? ?? ? ?= ?? == otherwise
UifUU
U md j
L ij md j
L ij
L ijL ij ,0 0,? 11 (2)
In order to find the neighbors ( in another word , the nearest documents ) of a document , we sort every row of LU ? to LU ~ in descending order . That is : ijLU ~ ? ikLU ~ ( i = 1,?,nd ; j,k = 1,?,md ; k?j).
Then for di?DU ( i = 1,?,nd ), ijLU ~ ( j = 1,?,K ) corresponds to K neighbors in DL . We use a matrix
Kndij
LL UnUn ?= ][ to denote the neighbors of DU in source domain , with ijLUn corresponding to the jth nearest neighbor of di.
(2) Create DU and DU Edges
Similarly , the edge weight between DU itself is computed by the cosine measure . We get the similarity matrix UU=[UUij]ndxnd , the normalized similarity matrix UU ? , and the neighbors of DU in target domain:
Kndij
UU UnUn ?= ][ .
Word-to-Word Graph
Similar to the Document-to-Document Graph , we build an undirected graph to reflect the relationship between words in WL and WU , in which each node corresponds to a word and the edge weight between any different words corresponds to their semantic similarity . The edges in this graph are divided into two parts : edges between WU and WL ; edges between WU itself , so we also build the graph in two steps.
(1) Create WU and WL Edges
We compute the semantic similarity using cor-pus-based approach which computes the similarity between words utilizing information from large corpora . There are many measures to identify word semantic similarity , such as mutual information ( Turney , 2001), latent semantic analysis ( Landauer et al , 1998) etc . In this study , we compute word semantic similarity based on the sliding window measure , that is , two words are semantically similar if they cooccur at least once within a window of maximum Kwin words , where Kwin is the window size . We use an adjacency matrix VL to denote the similarity matrix between WU and WL . VL=[VLij]nwxmw is defined as follows : ?? ?? ? ?? ? = ++ + otherwise wwif wpwp wwpN
V nwjinwji nwji ij
L ,0 , )()( ),( log (3) where N is the total number of words in DU ; p(wi , wj ) is the probability of the cooccurrence of wi and wj within a window , i.e . num(wi , wj)/N , where num(wi , wj ) is the number of the times wi and wj cooccur within the window ; p(wi ) and p(wj ) are the probabilities of the occurrences of wi and wj respectively , i.e . num(wi)/N and num(wj)/N , where num(wi ) and num(wj ) are the malize VL to LV ? to make the sum of each row equal to 1. Then we sort every row of LV ? to LV ~ in descending order , and we use a matrix
Knwij
LL VnVn ?= ][ to denote the neighbors of WU in source domain.
(2) Create WU and WU Edges
Then we also compute the edge weight between any different nodes which denote words in WU by the sliding window measure . We get the similarity matrix VU=[VUij]nwxnw , the normalized similarity matrix UV ? , and the neighbors of WU in target domain:
Knwij
UU VnVn ?= ][ .
Document-to-Word Graph
We can build a weighted directed bipartite graph from documents in DU and words in WL and WU in the following way : each node in the graph corresponds to a document in DU or a word in WL and WU ; if word wj appears in document di , we create an edge from di to wj . The edges in this graph are divided into two parts : edges from DU to WL ; edges from DU to WU , so we also build the graph in two steps.
(1) Create DU to WL Edges
The edge weight from a document in DU to a word in WL is proportional to the importance of word wj in document di . We use an adjacency matrix ML to denote the similarity matrix from DU to WL . ML=[MLij]ndxmw is defined as follows : ? ? ? ?= ++ i nwjnwj dw ww wwL ij idftf idftf
M (4) where w represents a unique word in di and tfw , idfw are respectively the term frequency in the document and the inverse document frequency.
We normalize ML to LM ? to make the sum of each row equal to 1. Then we sort every row of LM ? to
LM ~ in descending order , and we use a matrix
Kndij
LL MnMn ?= ][ to denote the neighbors of D
U in
WL.
(2) Create DU to WU Edges
Similarly , we can also compute the edge weight from a document in DU to a word in WU in the same way . We get the similarity matrix MU=[MUij]ndxnw , the normalized similarity matrix
UM ? , and the neighbors of DU in WU:
Kndij
UU MnMn ?= ][ .
Word-to-Document Graph
In this section , we build a weighted directed bipartite graph from words in WU and documents in DL and DU in which each node in the graph corresponds to a word in WU and a document in DL or DU ; if word wj appears in document di , we create an edge from wj to di . The edges in this graph are also divided into two parts : edges from
WU to DL ; edges from WU to DU.
(1) Create WU to DL Edges
Similar to 3.3.4, the edge weight from a word in WU to a document in DL is proportional to the importance of word wi in document dj . We use an adjacency matrix NL=[NLij]nwxmd to denote the similarity matrix from WU to DL . We normalize NL to LN ? to make the sum of each row equal to 1.
Then we sort every row of LN ? to LN ~ in descending order , and we use a matrix
Knwij
LL NnNn ?= ][ to denote the neighbors of WU in DL.
(2) Create WU to DU Edges
We can also compute the edge weight from a word in WU to a document in DU in the same way.
We get the similarity matrix NU=[NUij]nwxnd , the normalized similarity matrix UN ? , and the neighbors of WU in DU:
Knwij
UU NnNn ?= ][ .
2.4 Proposed Method
Based on the two thoughts introduced in Section 2.2, we fuse the eight relationships abstracted from the four graphs together to iteratively reinforce sentiment scores , and we can obtain the iterative equation as follows : ?? ?? ?? ?? ?? ?? ?+?+ ?+?= i
U i
L i
U i
L
Mnr rir
U
Mnl lil
L
Unh hih
U
Ung gig
L i wsMwsM dsUdsUds )?()?( )?()?( ?? ?? (5) ?? ?? ?? ?? ?? ?? ?+?+ ?+?= j
U j
L j
U j
L
Nnr rjr
U
Nnl ljl
L
Vnh hjh
U
Vng gjg
L j dsNdsN wsVwsVws )?()?( )?()?( ?? ?? (6) where ? i means the ith row of a matrix ; Ds = { ds1,?,dsnd , dsnd+1,?, dsnd+md } represents the sentiment scores of DU and DL ; Ws = { ws1,?,wsnw , wsnw+1,?, wsnw+mw } represents the sentiment scores of WU and WL ; ? and ? show scores from source domain and target domain when calculating DD-Relationship and WW-Relationship , and ? + ? =1; ? and?show the relative contributions to the final sentiment scores from source domain and target domain when calculating DW-Relationship and WD-
Relationship , and ? +?=1.
For simplicity , we merge the relationships from source domain and target domain . That is , for formula (5), we merge the first two items into one , the last two items into one ; for formula (6), we merge its first two items into one , its last two items into one . Thus , (5) and (6) are transformed into (7) and (8) as follows : ?? ?? ?? ??+??= ii Mnl lil
Ung gigi wsMdsUds )?()?( ?? (7) ?? ?? ?? ??+??= jj Vnl ljl
Nng gjgj wsVdsNws )?()?( ?? (8) where ? and ? show the relative contributions to the final sentiment scores from document sets and word sets , and ?+?=1.
In consideration of the convergence , Ds and Ws are normalized separately after each iteration as follows to make the sum of positive scores equal to 1, and the sum of negative scores equal to -1: ??? ??? ? > <? = ? ? ? ? 0, 0,)( i
Dj ji i
Dj ji dsifdsds dsifdsds ds
U pos
U neg i (9) ??? ??? ? > <? = ? ? ? ? 0, 0,)( j
Wi ij j
Wi ij j wsifwsws wsifwsws ws
U pos
U neg (10) where U negD and
U posD denote the negative and positive document set of DU respectively;
U negW and
U posW denote the negative and positive word set of WU respectively.
Here is the complete algorithm : 1. Initialize the sentiment score vector dsi of di?DL ( i = nd+1,?, nd+md ) with 1 when di is labeled ? positive ?, and with -1 when di is labeled ? negative ?, and initialize the sentiment score vector wsi of wi?WL ( i = nw+1,?, nw+mw ) with 1 when wi is labeled ? positive ?, and with -1 when wi is labeled ? negative ?. And we normalize dsi ( i = nd+1,?, nd+md ) ( wsi ( i = nw+1,?, nw+mw )) to make the sum of positive scores of DL ( WL ) equal to 1, and the sum of negative scores of DL ( WL ) equal to -1. Also , the initial sentiment scores of DU and WU are set to 0.
2. Alternate the following two steps until convergence : 2.1.Compute and normalize dsi ( i = 1,?, nd ) using formula (7) and (9): 2.2.Compute and normalize wsj ( j=1,?,nw ) using formula (8) and (10): where )( kids and )( k jws denote the ids and wsj at the kth iteration.
3. According to dsi?Ds ( i = 1,?,nd ), assign each di?DU ( i = 1,?,nd ) a label . If dsi falls in the range [-1,0], assign di the label ? negative ?; if dsi falls in the range [0,1], assign di the label ? positive?.
3 Experiments
In this section , we evaluate our approach on three different domains and compare it with some state-of-the-art algorithms , and also evaluate the approach?s sensitivity to its parameters.
Note that we conduct experiments on Chinese data , but the main idea in the proposed approach is language-independent in essence.
3.1 Data Preparation
We use three Chinese domain-specific data sets from online reviews , which are : Book Reviews1 ( B , www.dangdang.com /), Hotel Reviews 2 ( H , www.ctrip.com /) and Notebook Reviews 3 ( N , www.360buy.com /). Each dataset has 4000 labeled reviews (2000 positives and 2000 negatives).
We use ICTCLAS ( http://ictclas.org /), a Chinese text POS tool , to segment these Chinese reviews . Then , utilizing the part-of-speech tagging function provided by ICTCLAS , we take all adjectives , adverbs and adjective-noun phrases as candidate sentiment words . After removing the repeated words and ambiguous words , we get a list of words in each domain.
For the list of words in each domain , we manually label every word as ? negative ?, ? posi-1www.searchforum.org.cn/tansongbo/corpus/Dangdang_Book_4000.rar 2www.searchforum.org.cn/tansongbo/corpus/Ctrip_htl_4000.rar 3www.searchforum.org.cn/tansongbo/corpus/Jingdong_NB_4000.rar and ? positive ? words as a sentiment word set.
Note that we use the sentiment word set only for source domain , while using the candidate sentiment words for target domain.
Lastly , the documents are represented by vector space model . In this model , each document is converted into bag-of-words presentation in the remaining term space . We compute term weight with the frequency of the term in the document.
We choose one of the three data sets as source-domain data DL , and its corresponding sentiment word set as WL ; we choose another data set as target-domain data DU , and its corresponding candidate sentiment words as WU.
3.2 Baseline Methods
In this paper we compare our approach with the following baseline methods : Proto : This method applies a traditional supervised classifier , prototype classifier ( Tan et al , 2005), for the sentiment transfer . And it only uses source domain documents as training data.
LibSVM : This method applies a state-of-the-art supervised learning algorithm , Support Vector Machine , for the sentiment transfer . In detail , we use LibSVM ( Chang and Lin , 2001) with a linear kernel and set al options as default . This method only uses source domain documents as training data.
TSVM : This method applies transductive
SVM ( Joachims , 1999) for the sentiment transfer which is a widely used method for improving the classification accuracy . In our experiment , we use Joachims?s SVMlight package ( http://svmlight.joachims.org /) for TSVM . We use a linear kernel and set al parameters as default . This method uses both source domain data and target domain data.
3.3 Overall Performance
In this section , we compare proposed approach with the three baseline methods . There are three parameters in our algorithm , K , Kwin , ? (? can be calculated by 1-?). We set K to 50, and Kwin to 10 respectively . With different ?, our approach can be considered as utilizing different relative contributions from document sets and word sets . In order to identify the importance of both document sets and word sets for sentiment transfer , we separately set ? to 0, 1, 0.5 to show the accuracy of utilizing only word sets ( referred to as WORD ), only document sets ( referred to as DOC ), and both the document and word sets ( referred to as ALL ). It is thought that the algorithm achieves the convergence when the changing between the sentiment score dsi computed at two successive iterations for any di?DU ( i = 1,?,nd ) falls below a given threshold , and we set the threshold 0.00001 in this work . The parameters will be studied in parameters sensitivity section.
Table 2 shows the accuracy of Prototype , LibSVM , TSVM and our algorithm when training data and test data belong to different domains.
As we can observe from Table 2, our algorithm produces much better performance than supervised baseline methods . Compared with the traditional classifiers , our approach outperforms them by a wide margin on all the six transfer tasks . The great improvement compared with the baselines indicates that our approach performs very effectively and robustly.
Traditional Classifier Our Approach Proto LibSVM
TSVM
DOC WORD ALL
B->H 0.735 0.747 0.749 0.772 0.734 0.763 B->N 0.651 0.652 0.769 0.714 0.785 0.795 H->B 0.645 0.675 0.614 0.671 0.668 0.703 H->N 0.729 0.669 0.726 0.749 0.727 0.734 N->B 0.612 0.608 0.622 0.638 0.667 0.726 N->H 0.724 0.711 0.772 0.764 0.740 0.792
Average 0.683 0.677 0.709 0.718 0.720 0.752 Table 2: Accuracy comparison of different methods Table 2 shows the average accuracy of TSVM is higher than both traditional classifiers , since it utilizes the information of both source domain and target domain . However , the proposed approach outperforms TSVM : the average accuracy of the proposed approach is about 4.3% higher than TSVM . This is caused by two reasons . First , TSVM is not dedicated for sentiment-transfer learning . Second , TSVM requires the ratio between positive and negative examples in the test data to be close to the ratio in the training data , so its performance will be affected if this requirement is not met.
Results of ? DOC ? and ? WORD ? are shown in column 4 and 5 of Table 2. As we can observe , they produce better performance than all the baselines . This is caused by two reasons . First , ? DOC ? and ? WORD ? separately utilize the sen-Second , both ? DOC ? and ? WORD ? involve an iterative reinforcement process to improve their performance . The great improvement indicates that the iterative reinforcement approach is effective for sentiment transfer.
Besides , Table 2 also shows both document sets and word sets are important for sentiment transfer . The approach ? ALL ? outperforms the approaches ? DOC ? and ? WORD ? on almost all the six transfer tasks except ? B->H ? and ? H->N?.
The average increase of accuracy over all the six tasks is 3.4% and 3.2% respectively . The reason is : at every iteration , the classification accuracy of documents and words is improved by each other , and then the accuracy of sentiment transfer is improved by the documents and words that are classified more accurately . As for ? B->H ? and ? H->N ?, the performance of utilizing only document sets is so good that the word sets couldn?t improve the performance any more . The improvement of the approach ? ALL ? convinces us that not a single one of the four relationships can be omitted.
3.4 Parameters Sensitivity
The proposed algorithm has an important parameter , ? (? can be calculated by 1-?). In this section , we conduct experiments to show that our algorithm is not sensitive to this parameter.
To investigate the sensitivity of proposed method involved with the parameter ?, we set K to 50, and Kwin to 10. And we change ? from 0 to 1, an increase of 0.1 each . We also evaluate ? on the six tasks mentioned in section 3.1, and the results are shown in figure 2.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1?
Ac cu ra cy
B->H B->N H->B H->N N->B N->H
Figure 2: Accuracy for Different ?
We can observe from Figure 2 that the accuracy first increases and then decreases when ? is increased from 0 to 1. The accuracy changes gradually when ? is near 0 or 1, and it changes less when ? is between 0.2 and 0.8. It is easy to explain this phenomenon . When ? is set to 0, this indicates our algorithm only uses word sets to aid classification , without the information of document sets . And if ? is set to 1, our algorithm only uses document sets to calculate sentiment score , without the help of word sets . Both cases above don?t use all information of four relationships , so their accuracies are worse than to equal the contributions of both document and word sets . This experiment shows that the proposed algorithm is not sensitive to the parameter ? as long as ? is not 0 or 1. We set ? to 0.5 in our overall-performance experiment.
3.5 Convergence
Our algorithm is an iterative process that will converge to a local optimum . We evaluate its convergence on the six tasks mentioned above.
Figure 3 shows the change of accuracy with respect to the number of iterations . We can observe from figure 3 that the curve rises sharply during the first 6 iterations , and it is very stable after 10 iterations are performed . This experiment indicates that our algorithm could converge very quickly to get a local optimum.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 4 7 10 13 16 19 22 25 28 31 34 37 40Iteration
Ac cu ra cy
B->H B->N H->B H->N N->B N->H
Figure 3: Performance for Iteration 4 Conclusions In this paper , we propose a novel cross-domain sentiment classification approach , which is an iterative reinforcement approach for sentiment transfer by utilizing all the relationships among documents and words from both source domain and target domain to transfer information between domains . First , we build three graphs to reflect the above relationships respectively . Then , to denote its extent to ? negative ? or ? positive?.
We then iteratively calculate the score by making use of the graphs . Finally , the final score for sentiment classification is achieved when the algorithm converges , so we can label the target-domain data based on these scores.
We conduct experiments on three domain-specific sentiment data sets . The experimental results show that the proposed approach could dramatically improve the accuracy when transferred to a target domain . To investigate the parameter sensitivity , we conduct experiments on the same data sets . It is observed that our approach is not very sensitive to its four parameters , and could converge very quickly to get a local optimum.
In this study , we employ only cosine measure , sliding window measure and vector measure to compute similarity . These are too general , and perhaps not so suitable for sentiment classification . In the future , we will try other methods to calculate the similarity . Furthermore , we experiment our approach on only three domains , and we will apply our approach to many more domains.
5 Acknowledgments
This work was mainly supported by two funds , i.e ., 60933005 & 60803085, and two another projects , i.e ., 2007CB311100 & 2007AA01Z441.
References
Alina Andreevskaia and Sabine Bergler . 2008. When Specialists and Generalists Work Together : Overcoming Domain Dependence in Sentiment Tagging.
In Proceedings of ACL : 290-298.
Anthony Aue and Michael Gamon . 2005. Customizing sentiment classifiers to new domains : a case study . In Proceedings of RANLP.
Sergey Brin , Lawrence Page , Rajeev Motwami , and Terry Winograd . 1999. The PageRank citation ranking : bringing order to the web . Technical Report 1999-0120, Stanford , CA.
Chinchung Chang and Chinjen Lin . 2001. LIBSVM : a library for support vector machines.
http://www.csie.ntu.edu.tw/~cjlin/libsvm.
Gunes Erkan and Dragomir Radev . 2004. LexRank : Graphbased Centrality as Salience in Text Summarization . Journal of Artificial Intelligence Research , 22 (2004): 457-479.
Michael Gamon and Anthony Aue . 2005. Automatic identification of sentiment vocabulary : exploiting low association with known sentiment terms . In Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP : 57-64.
Songbo Tan , Xueqi Cheng , Moustafa Ghanem , Bin Wang , Hongbo Xu . 2005. A novel refinement approach for text categorization . In Proceedings of
CIKM 2005: 469-476
Thorsten Joachims . 1999. Transductive inference for text classification using support vector machines.
In Proceedings of ICML.
Jon Kleinberg . 1998. Authoritative sources in a hyperlinked environment . Journal of the ACM , 46(5): 604-632.
Lunwei Ku , Yuting Liang , and Hsinhsi Chen . 2006.
Opinion extraction , summarization and tracking in news and blog corpora . In Proceedings of AAAI.
Thomas Landauer , Peter Foltz , and Darrell Laham.
1998. Introduction to latent semantic analysis . Discourse Processes 25: 259-284.
Ryan McDonald , Kerry Hannan , Tyler Neylon , Mike Wells , and Jeff Reynar . 2007. Structured models for fine-to-coarse sentiment analysis . In Proceedings of ACL.
Songbo Tan , Yuefen Wang , Gaowei Wu , and Xueqi Cheng . 2007. A novel scheme for domain-transfer problem in the context of sentiment analysis . In
Proceedings of CIKM.
Songbo Tan , Xueqi Cheng , Yuefen Wang , and Hongbo Xu . 2009. Adapting Na?ve Bayes to Domain Adaptation for Sentiment Analysis . In Proceedings of ECIR.
Qiong Wu , Songbo Tan and Xueqi Cheng . 2009.
Graph Ranking for Sentiment Transfer . In Proceedings of ACL-IJCNLP.
Peter Turney . 2001. Mining the web for synonyms : PMI-IR versus LSA on TOEFL . In Proceedings of
ECML : 491502.

