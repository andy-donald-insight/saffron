Experiments with Open-Domain Textual Question Answering 
Sanda M . Harabagiu and Marius A . Pa~ca and Steven J . Maiorano
Department of Computer Science and Engineering
Southern Methodist University
Dallas , TX 75275-0122
sanda , marius , steve@renoir , seas.smu.edu

This paper describes the integration of several knowledge-based natural language processing techniques into a Question Answering system  , capable of mining textual answers from large collections of texts  . Surprizing quality is achieved when several lightweight knowledge-based NLP techniques com-I  ) lement mostly shallow , surface-based approaclms . 
1 Background
The last decade has witnessed great advances and interest in the area of Information Extraction  ( IE ) fi'om realworld texts . Systems that participated in the TIPSTER MUC competitions have been quite sue-cessflll at extracting information from newswire rues-sages and filling templates with infor Ination pertaining to events or situations of interest  . Typically , the templates model queries regarding who did what to wh  , om , when and where , and eventually why . 
Recently , a new trend in information processing from texts has emerged  . Textual Question Answering ( Q/A ) aims at ; ident it ~ ying the answer of a question in large collections of online documents  . Instead of extracting all events of interest and their related entities  , a Q/A system higt flights only a short piece of text  , accounting for the answer . Moreover , questions are expressed in natural anguage , are not constrained to a specific domain and are not limited to the six question types sought by IE systems  ( i . e . 
wh,ol(lid what . 2 to whoma , when 4 and w h , eres , and eventually why a ) . 
In open-domain Q/A systems , the finite-state technology and domain knowledge that made IE systems successful are replaced by a combination of  ( 1 ) kuowledge-based question processing ,   ( 2 ) new forms of text indexing and ( 3 ) lightweight abduction of queries . More generally , these systems coml ) inecre-atively components of tile NLP basic research ill-frastructure developed in the  80s   ( e . g . the computational theory of Q/A reported in ( Lehnert 1978 ) and tim theory of abductive interpretation of texts reported in  ( Hobb set a 1 . 1993 ) ) with other shallow teclmiques that make possil ) le the open-domain processing on realworld texts . 
Timidea of building open-domain Q/A systems that perform on realworl document collections was initiated by the eighth Text REtrieval Conference  ( TREC8 )  , by organizing the first competition of answering fact-based question such as " Who came up with the name  , El Nino ?' . Resisting the tempta-tion of merely porting and integrating existing IE and IR technologies into Q/A systems  , the developers of the TREC Q/A systems have not only shalm d new processing methods  , but also inspired new research in the challenging iutegration of surface-text-based methods with knowledge-based text inference  . 
In particular , two clear knowledge processing needs are presented :  ( 1 ) capturing the semantics of open-domain questions and  ( 2 ) justifying the correctness of answers . 
In this paper , we present our experiments with integrating knowledge-based NLP with shallow processing techniques for these two aspects of Q/A  . Our research was motivated by the need to enhance the precision of an implemented Q/A system and by the requirement to preI  ) are it for scaling to more complex questions than those t  ) resented in the TREC competition . In the remaining of the paper , we describe a Q/A architecture that allows the integration of knowledge-based NLP processing with shallow processing and we detailtlmir interactions  . Section 2 presents the flmctionality of several knowledge processing modules and describestile NLP techniques for question and answer processing  . Section 3 explains the semantic and logical interactions of processing questions and answers whereas Sectiou  4 higlllights the inference aspects that inlplement the justification option of a Q/A system  . Section 5 presents the results and the evaluations whereas
Section 6 concludest impaper.
2 The NLP Techniques
Surprising quality for open-donlain textual Q/A can be achieved when several lightweight knowledge-based NLP techniques e omt  ) lenmnt mostly shallow , surface-based approaches . The processing imposed by Q/A systems must be distinguished  , oi 1 the one band , from IR techniques , that locate sets of doc-
Queslion lIoeumcnlsAnswer(s )
Question ' Qul , ~ . _k ~\ [ Semantic Logi ~ ~ l , " L Transfimnati?n . -) j~J('r ransI , -\[-(Question ~/\ /\/\ . . . . . "'/ c /, -, . , . ., . \[!/ Ot , estion\~=g ~
Taxonomics ~/ Expanded/V ~ . ~ExpandedWordClasses\[Question\] ) - f-~-:~2q . _1, ' ~ Expansion
I//P ~~?\\ k.~J.j ~ JqSxp and cd
Knowledge-lhrwd Ouestio Hl ' roces , dng ) tWShallowDocumentl ' roces . d~tgKmm , ledge-Based Answerl ' rocessing Figure 1: An architecture t brk now h ; dge-l ) ased Question/Answeringuments ( ' ontaining the required information , based on keyword stech , niques . Q/A systems are presented with natural anguage questions  , far richer in semantics than a set of keywords eventually st  , ru (' flured around some , oi ) erators . I hn th er nm re , the outtlut of Q/A systems is either the actual answer identified in a text or small text  ; fragments containing the answer . This eliminates the user's trouble of tind-ing the required in ibrnlation in some time  . slarge sets of retrieved o(-uments . Op ( m-donm in Q/A systems must also l ) edistinguished , on the other hand , h'om IE syst ( ; ms that model the inforlnation eed through ( latal ) as (  ; t ( ; mt ) lates , thus less naturally than a textual answer . Moreovei ' , open-domain IE is still ditli-(:ult to achieve , beea use its linguistic t ) at Cernre ( : og-nition relies on domain-dependent lexico -semantie knowledge  . 
To t ) eable to satisf ~ ytileol ) en-donm in constraints , textual Q/A systems replace the linguistic pattern matching capabilities of IE systems with methods that rely  ( m the recognitioil of tile question type and of thee  . ' rpectcd answer type . Generally , this information is available by accessing a classification based on the question stem  ( i . e . what , how much , who ) and the head of the first n Oml phrase of the ques -ti  ( m . Question 1 ) rocessing also includes the identification of the question keywords  . Empirical methods , based on a set of ordered heuristics ot ) erating on the phrasal parse of the question , extract keywords that are passed to the search engine  . The overall precision of tile Q/A system depends also on th  (  , recognition of the question focus , since the answer extraction , suet : ceding the IR phase , is centered around the question focus . Unl ) rtmmtely , eml ) irical ninth-ods fl ) rt ' oe us recognition are hard to develop without the availability of richer semantic knowledge  . 
S1)eeial requir ( nnents are set Oil the docume id ; pro-(:essing COml ) Olmnt of a Q/A system . To speed-ul ) the answer extraction , the search engine returns only those 1 ) aragrai ) hs from a document that contain all queried keywords  . The paragraphs are ordered to 1) roInote the , eases when the keywords not only art ; as close as 1) ossibh ~' , lint also t ) reserve the syntactic de-1 ) enden ( : iesre ( : ognized in the question . Answers are (' . xtra ( : ted whenever the question topic and them>s wer tyI  ) e are recognized iil a 1 ) aragraph . Thereafl ; er the answers :/1( ; scored 1) ased on several bag-of-words hem'isties . Throughout all this 1) roees sing , the NLP te ( : hniques are limited to ( 21 ) named entity recognition ;   ( b ) semantic lassification of the question tyt ) e , l/ased oil information 1 ) rovided by an offline question taxon only 21 . i1 ( tsenmntic lass int brmation available from WordNet  ( Felll ) mml 1998 )  ; mid ( c ) phrasal parsing produced by enhancing Brill's part-of-sl  ) eech tagger with some rules tbr phrase tbrmation . 
I lowever simt/le , this technology surl ) asses 75% precision on trivia questions , asposed in the TREC8 (: ompetition ( of . ( Moldovan et al 1999)) . An impressive improven mnt of 14% is achieved when more knowledge-intensive NLP techniques are ai  ) plieda . t both question and answer processing level . Figure 1 illustrates the architecture of a system that has enhanced Q/A performance  . 
As represented in Figure 1 , all three modules of the Q/A system preserve the shallow process in geomi/onents that determine good t  ) er formane e . Int ; t1(' , Quest , i on Processing module , the QuestionClass re (: ognizer , working against a taxonomy of questions , place at this stage . However , a far richer representation of the quest ; ion classes is employed . To be able to classify against the new question taxonomy each question is first flflly parsed and transfommd into a semantic representation that captures all relationships between I  ) hrase heads . 
The recognition of the question class is based on the comparison of the questions mnantic representation with the semantic representation of the nodes from tlm question taxonomy  . Taxonomy nodes encode also the answer type , the question focus and the semantic lass of question keywords  . Multiple sets of keywords are generated based on their semantic class  , all pertaining to the stone original question . This that ure enables the search engine to retrieve multiple sets of documents  , pertaining to mul-tit)le sets of answers , that are extracted , combined and ranked based on several heuristics , reported in ( Moklovaneta1 . 1999) . This process of obtaining multiple sets of answers increases l  ; he likelihood of finding the correct answer . 
However , the big boost in the precision of the knowledge -based Q/A system is provided by the option of enabling the justification of the extracted answer  . All extracted answers are parsed and transformed in semantic representations  . Thereafter , both semantic transformations for questions and answers are translated into logic forms and presented to a simplified theorel n prover  . The proof back-chains Dora the question to the answer  , its trace generating a justification . The prover may access a set of abduction rules that relax the justification process  . Whenever an answer cmmot l ) e1) roven , it is discarded . This option solves multiple situations when the correct answer is not ranked as the first return  , due to stronger surface-text-based inicators in some other answers  , which unfortunately are not correct . 
This architecture allows for simple integration of semantic and axiomatic knowledge sources in a Q/A system and determines efficient interaction of text-surface-based and knowledge-based NLP techniques  . 
3 Interactions
Three main interactions between text-surface-based and knowledge-based NLP techniques are designed in our Q/A architecture :  1  . When multiple sets of question keywords are passed to the search engine  , increasing the chance of finding the text paragraph containing the answer  . 
2 . When the question focus and the answer type , re-suiting from the knowledge-based processing of the question  , are used in the extraction of the answer , based on several empirical scores . 
3 . When the justification option of the Q/A system is available  . Instead of returning answer scored by some empirical measures  , a proof of the correctness of the answer is produced  , by accessing the logical transformations of the question and the answer  , as well as axioms encoding world knowledge . 
All these interactions depend on two Nctors : ( 1 ) the l ; ransformations of the question or answer into semantic or logical representations  ; and (2) the availability of knowledge resources , e . g . the question taxonomy and the world knowledge axioms  . The availability of new , high-performace parsers that operate on real world texts determines the transformation into semantic and logic formulae quite simple  . In addition , the acquisition of question taxonomies i alleviated by machine learning techlfiques inspired from bootstrapping methods that learn linguistic patterns and semantic dictionaries for IE  ( of . ( Riloff and Jones , 1999)) . World knowledge axioms can also be easily derived by processing the gloss  ( lefinitions of
WordNel ; ( Fellbaunl 1998).
a . 1 Semantic and Logic Transformations
Semantic Transtbrmations
Instead of t ) roducing only a phrasal parse for the question and answer  , we lnake use of one of the new statistical parsers of large realworld text coverage  ( Collins ,  1996) . The parse trees produced by such a parser can be easily translated into a seinantic representation that  ( 1 ) comprises all the phrase beads and ( 2 ) captures their in t , er-relationships by anonymous links . Figure 2 illustrates both the I ) arse tree and the associated semantic representation fa  TIEC-8 question . 
Question : Why did I ) avid Kores hask the FBI for a word processor ?

SBAP . Q - - - .   .   .   .   . SQ///------v , , / / / / 4 - - -~ -- Pl ' i / / / \ / WRB VBI ) NNP NN\[' VBl ) TNNI'IN DT NN NN
IIIIIIIII lI
Why did David Kores hask the Fill for a word processor 
Semantic representation : ~ ask word ~..
REASONI ) avid %/ Y---~"~\ processor
Koresh FBI
Figure 2: Question semantic transformation The actual transformation i to semantic representation of a question or an answer is obtained as a byproduct of the parse tree traversal  . Initially , all leaves of the parse tree are classified as verbs  , adjectives a . n d adverl ) s are categorized as non-skit modes . All the other h ~ aves are skip nodes . 
Bottom-u 1)trav (' . rsaloftim 1 ) arse tree ( : ntail stlmt ) roi ) agation of leaf labels wh ( ' amverl ; hc1) arc nt no d ( ; has more than one non-skip nod ( ; child . A rule based on the syntactic ategory ( it'th ( . ' father selects one of the childr ( m to 1 ) ropa gat cits label a , t the next level in the tree . The winning node will then be considered linked to all the other for nmrsiblings thai  ; al'e non-skil modes . The prot ) agation (: ontimms ml tilthel ) arse 1 ; l'(~ . croot receives a label , and thus a scmanti (" gral ) his (: rc ; tl ; ( ; ( lasa1) y-1) rodu(:t . Part of th (' . labeli ) roI ) agation , we also (: onsider that whenever all (' hildr ( ; n of a nonterminal are skilm o(l ( ;  , % the parent ; becomes a . skip node as well . 
Figure 3 rel ) r(~'s ( ; nts the lal ) el I ) rOl ) agation for th ( ;  1 ) arse tree of the question l'el ) res ( mt ( M in Figure 2 . 
The labels of Korcsh , ask , FB1 and processor are l ) rot ) agated to tlw , next level . This entails t ; hatKo-r'c . s ' h is linked to David , ask to I , ' BI and procc ' , s's or mM procc , ssortoword . As a . @ be (: (/ m (! s the lab (: lofth (; tree to ( it , it is a . lso linked to I~ds'A , 9ON , the qu(~sti ( ) ntype(l('~I;(n'lnin(' , dl ) ytlm questions t('m : wh , y . The lal ) el1 ) rot ) agation rules are id ( mtical t < ) the rules f l ) r mapping from trees to d ( ~t ) (m ( hm ( : ys\[ . rlt(:l;lll . ' csllSC , ( lmyMi('ha (, q Collins ( of . ( Collins , 199(5)) . These rules i(lent i\[ , the head child , and pl ' Ol ) a gat c its label up in the tree . 
/_>(/vV(>k)wllm)w'/~Nl'(K , , , , , ~ IO ''' i , /' NI'\[I:IH , I//~~~//\/'\WRB VBI ) NNP NNP ,  ' , VIII ) TNNP , INI)TNNNN )
III I /', III ~" IIII,'
Wily did l ) avid Korcs haskfile\[:BI for a word lUl ) ccssiw Figur ( '  . 3:\]) ~ II'S(~t,l'(~ . (, ~ l;ra , v (', rsa\[
I'l'(processor!-/NI~I ) rOCCss or I/,.
Logical Transformations
The logical formulae in whi ( : h questions or answers arc translated are inspired\[  ) y the notal ; i(mprol/os(' . din(ltobbs,1 . 986-1 . ) and implemented in TACITUS ( ttobbs , 1986-2) . 
Based on the david soniantl " eatmen l ; of action sen- ; (?llC( ; S , in which events a retr(~at , ed as individuals , every question and every answer are transform ( ' xlina tirst-order t ) redicat ct brmula for which ( 1 ) verbs are mapped in 1 ) red\[caresv cvb ( e , x , y , z ,  .   .   . ) with the . 
(: onvention thai ; variable erel ) res ( ; nts the evc ' nl ; ual-i( ; y of that a cl ; ionoreven ( ;  ( ; otake place , wh('a'eas ( ; lmoth cuarguments ( e . g . z , y , z, . . . ) repr(~ , s(mtl ; lmt ) rcd-icate argmnents of the verb ;   ( 2 ) nouns arcmal ) l ) ed into their lexicalized predicates ; raM ,   ( 3 ) mo ( lific ws have the same argument as the predicate they mod-i\[y  . For (; X alnpl (' ~ , l ; he qu(~si ; ion illustrated in Figure 2 has l ; he following logical for nl transforln at ion  ( LFT ) :\[ REASON ( x ) ~David ( y ) ~ Koresh ( y ) ~ ask ( e , x , y , z , p ) ~ FBI ( z ) q processot ( p ) ~ word ( p ) \] The process of trml slat ; ingasema . nticret ) resent a . -tion into a logic form has the following steps : 1  . For each node in the semantic rcprcs cntation , create a prcdicat c with a distinct argument . 
2 . a . If it noun and ( titad , jective predicate arcl in lcc dtht:y should have , the sam (" argument . 
2 . b . T l t c . qam cfin " verbs and adverbs , pairs of l wuns or an adjective and an adv crb . 
3 . l , b'r each verb predicate , add a T~lumcnts corrc . sponding to each predicate to which it is directly linked in the semantic representation  . 
Predicate argunmnts ( : an be identified because timsoma , hi ; itl ' ol ) res ( . ~nl ; al ; ion using & nollymous relations represenl ; suniformly adjuncts mM thematic roles . 
I lowevel: , sl ; e1)2 of the l ; l"mtsl~d ; ion procedure l'eCOg-nixes the adjuncts , making predicate argmnen l ; s the remaining ( : ( ) nn ( ~ ( : tions of tlmv ( n ' ) in t ; l ( , ~semal ~ I ; icr q ) resentation . 
3.2 Question Taxonomy
The question taxonomy rel ) rcs cnl ; seach question nod (' , as a quintuple : (1) a set nan ( ; icr cpr(~s(!ni ; ation of a qucs Lion ; (2) th ( ; question type ; (3) them ~ swertyp ( , ; (4) the question focus an , l (5) the question keywo Ms . By using over 1500 questions provided by \] lcme ( lia , as well as otlm r2000 quest ; ions retrieved from FAQF inder , we have b(!enabh ~ . to learn classiti-cation rules mM buihla ( : o in plex question taxonomy . 
\  . . . . . ,\[\]I1SIIIIICeI,,/-;,,,\__//,, . 2_\[Ar two , kjl LI . ocation 1 Figure 4: A snapshot of the top Question Taxonomy Initially  , we stm'tcd with a seed hit ; rarchy of 25 question classes , manually built , in which all the semantic classes of the nodes fl ' om the semantic representations were decided oil -line  , by a hm n an expert . 
3 00 questions were processed to create this seed hierarchy  . Figure 4 illustrates some of the nodes of tions were considered  , we started classifying them semiautomatically , using the following two steps : ( 1 ) first a hm nanwonld decide the semantic lass of each node in the semantic representation f the new question  ;   ( 2 ) then a classification procedure would decide whether the question belongs to one of the existing classes or a new class should be considered  . 
To be able to classify a new question in one of the existing question classes  , two conditions must be satis-fled : ( a ) all nodes fi'om the taxonomy question must correspond to new question nodes with the same semantic lasses  ; and ( b ) unifyable nodes must be linked in the same way in both representations  . The hierarchy grew to 68 question nodes . 
Later ,   2700 more questions were classified fully automatically  . To decide the semantic lasses of the nodes , we used the WordNet semantic hierarchies , by simply assigning to each semantic representation node the same class as that of any other question term from its WordNet hiera  . rchy . 
The semantic representation , having the same format for questions and answers , is a case fi'ame with anonymous relations , that allows the unification of the answer to the question regardless of the case relation  . Figure 5 illustratest burnodes fi'om the question taxonomy  , two for the " currency " question tyI ) eattd two for the " person name " question type . The Figure also represents the mappings of four TREC8 questions in these hier ~ r chy nodes . The mappings are represented by dashed arcs . In this Figm'c , the nodes front the semantic representations that con-rain a question mark are placeholders for the expected answer type  . 
An additional set of classification rules is as soei-ated with this taxonomy  , hfitially , all rules are based on the recognition of the question stem and of the answer type  , obtained with class int brmation from WordNet . However we could learn new rules when in or phologie alnd semantic variations of the semantic nodes arc allowed  . Moreover , along with the new rules , we enrich the taxonomy , because often the new questions unify only partially with the current tax-enemy  . All semantic and morphologic variations of the semantic representation nodes are grouped into word classes  . Several of the word classes we used are listed in Table  1  . 
\[IWordUlassWordsl\]
Value words " monetary value " , " money " , " price " Expenditure words " spend " , " buy " , " rent " , " invest " Creation words " author " , " designer " , a invent ~ . . . 
Table 1: Examples of word classes.
The bootstrapping algorithm that learns new classification rules and new classes of questions is based on an intbrmation extraction measure : scorc  ( rulei ) -- Ai * lo . q2(Ni ), where Ai stands for the
Class Name : currency \ [~ l O7: What debts did Q intex ,   .   .   .   .   .   .   .   . rouploav :
Class Name . " Ilalne per soll
Q 32:Who received Ihe Will ? Rogers Award in 1989? conlp ctiliol l "
HFI2 semalltic reprd~-~representation
Class Nanle : author
Q 92: Who released 1he lnlernet ' ~ worm in the late 1980s ? el'talc "" .  2 ~  .   .   .   .   .   . I 1980 s1~c~B fi- ) ~semant t c representation_Figure 5: Mapping Questions in the Taxonomy number of different lexicon entries for the answer type of the question  , whereas Ni =  Ai/Fi , where Fi is the number of different focus categories classified  . 
The steps of the bootstrapping algorithm are : 1 . Retrieve concepts morphologically // semantically reated to the semantic representations  2  . Apply the classification rules to all questions that contain any newly retrieved concepts  . 
3. New_Classificatiton_Rules =
MUT UAL BOOT STR APPING LOOP 4 . Sc or e . all new classification rules 5 . best_CR = thchighest scoring classification rule 6 . Add b cst_CR to the classification rules 7 . Add the questions classified by best_CR to the taxonomy  8  . Goto step 4 three times . 
9 . Discard all new rules but the best scoring three . 
10. Goto 3. until the Q/A performance improves.
296 4 The Justification Option
AQ/A system that provides with the option of jus -til~ving the answer has the advantage that erroneous answers can be ruled out syst  (  , ' matieally . In our quest of enh~mcing the precision of a Q/ A system by incofporating additional knowledge  , we fount 1 this option very help flH . However ~ the generation of justifications for ol ) en-domain textual Q/A systems poses some challenges  . First ; , we needed to develol ) a very efficient prover , operating on logical form transfer-mat ; ions . Our 1 ) rool'q are backchaining Do\]n the qlles-tions through a mixture of axioms  . We use thl ' ee for lll S of axioms : ( 1 ) axioms derived fl ' om the facts stated in I ; hel ; eXtl lalalls wel ; (2) ~/ XiOlllSro4) resent-ing world knowledge , ; and (3) axioms ( let ; ermined by coreference reso hlt ; ion in the at is wer text ;  . For ex-alni ) le , some of the axiom seml ) loyed 1 ; oprove the answer to the TREC ~8 question ( 26: Why did David Kor'csh , ask th , c1 , 7\]1 for a ' wo ~' dp ~ vccssor ? " are:
SET 1
Mr (71) := null . Koresh (71) := null . word (72) := null . 
processor (72) := null , sent (77767871):= null . 

SET 2
David(1): = Mr(1) . REASON(5):=enable(536) . 
SET 3
FBI(i ) := null.

The first set represents fa (' , l ; sextraete ( through LFT\]~re(licat(' , s of the textual answer : "0'?; (' , ~' t\]t (:' mc(~k(m , dMy Kovcsh ,   . s'c'ntavcq ', , c . stfo'ra'u ; ord proce , s-so'rtoc ' , ,ablc h , i'H ~ , to ' record his vt : ' ~; clatio ' ~ , s " . The sec-Ol~d set ret ) r(' , senl ; sworl (1 knowledge axioms that ; we acquired senfi-auto \] na . tically . For instance we know that David is a male name , thus tlmt person cmt be addressed with Mr . . Similarly , events are ( real > led for some reason . The third set of axioms represent the fact that the t  , ' l ~ l is in the context of the text answer . To be noted that the axioms derived from the answer have construct argument  , s , relnesent c , d by convention with numl ) ers larger titan 70 . All the other arguments are variables . 
Q52\?ho invented the road trallic cone , ? Answer ST ~ iliT ~ g proudly for the caTl ~ cras , Gover ~ tor ( shallow Pete Wilson , US TraTts portatio TtSecretary methods ) l , ' edcricol ) eT ~ aa Ttd Mayor l~ichavdl ~ iovda~t removed ~ thal f-doze Ttphtstic or ~ t~tgcco Ttcsfirm the road ~ oaya Ttd the first ca~'s passed IA nswer David Mor~la ~  , the comp ~ tT ty'smaT tagiT ~9 ( kb-based di ~' ectort ~ diT ~? ~ eltlo ~" of the plastic methods  ) co~te cycle , collects them . 
Table 2: Examples of trot ) roved answer correctness . 
Timjustification of this answerist ) rovided by the r ~ ( following proof I ; r  ~ lee . \] h ' , 1) rover ; tt telllI ) tSt ; o1 ) rove the LFT of the question ( QLF ) corre ( : t13 , proving from left to right each term of QLF . 
- >Answer:0ver the weekend Mr Kores hsent a request for a word processor to enable him to record his revelations  . 
->~LF:David ( l ) ~Koresh ( 1 ) "word ( 2 ) ^processor ( 2 ) 'FBI ( 4 ) ~ ask ( 342 i 5 ) '_REASDN ( 5 ) '_PER ( 1 ) *0RG ( 4 ) -> ALF : Mr ( 71 ) "Koresh ( 71 ) 'word ( 72 ) 'processor ( 72 ) 'revelations ( 74 ) " record ( Z 37475 ) ^enable ( 75 73 76 ) ~request ( 76 ) 'sent ( 77 76 78 71 ) ^ weekend ( 78 ) "PEK ( 71 ) ' DATE ( ?8 ) --> Proving : David ( 1 ) ^Koresh ( 1 ) ^word ( 2 ) 'processor ( 2 ) ^FBI ( 4 ) " ask ( 342 i 5 ) 'REASON ( 5 ) '_PER ( 1 ) '_0RG ( 4 ) There are i target axioms . Selected axiom : David(1): = Mr(1) . 
Unifying : itoi . Ok-->Proving:Mr ( 1 ) 'Koresh ( J ) ~word ( 2 ) ^processor ( 2 ) 'FBl ( 4 ) ^ ask ( 342 i 5 ) ^_REASDN ( 5 ) '_PER ( 1 ) ^_0RG ( 4 ) There are i target axioms . Selected axiom : Mr (71) := null . 
Unifying : i to 71 . Dk - -> Proving : Koresh ( 71 ) *word ( 2 ) -processor ( 2 ) ~FBl ( 4 ) " ask ( 342 Yl 5 ) * REASDN ( 5 ) ^_PER ( 71 ) '_0RG ( 4 ) There are i target axioms . Selected axiom : Koresh (71) := null . 
Unifying : Y1 to 71 . Dk-->Proving:word ( 2 ) 'processor ( 2 ) ^FBl ( 4 ) ~ask ( 3 4 2 71 5 ) *_REASON ( 5 ) ^_PER ( 71 ) '0RG ( 4 ) There are i target axioms . Selected axiom : word (72) := null,
Unifying : 2 to 72 . Ok-->Proving : processor ( Y2 ) 'FBl ( 4 ) ^ask ( 3 4 72 71 5 ) ^REASON ( 5 ) ^_PER ( 71 ) ^_DRG ( 4 ) There are i target axioms . Selected axiom : processor (72) := null . 
Unifying : ?2 to 72 . Ok-->Proving:FBl ( 4 ) 'ask ( 3 4 72 71 5 ) ^REASON ( 5 ) ^_PER ( 71 ) '_0RG ( 4 ) There are i target axioms , Selected axiom : FBI(1) := null . 
Unifying : 4 to i . Ok-->Proving : ask ( 3 4 72 71 5 ) ~REASON ( 5 ) ^_PER ( ZI ) ^_BRG ( 4 ) There are 2 target axioms . Selected axiom : ask ( l23 45 ) := sent ( l674 ) 'request ( 5 )  . 
Unifying : ito 2 . 3 to i . 5 to 5 . 71 to 4 . 72 to 3 . Ok-->Proving:sent ( l 67 ? I ) ^request ( 6 ) ^_REASON ( 5 ) ^_PER ( YI ) ^_0RG ( 2 ) There are I target axioms , Selected axiom:sent(Z 7767871):= null . 
Unifying : Ito 77 . 6 to Y 6 . Y to 78 . 71 to 71 . Ok --> Proving : request ( 76 ) ^REASON ( 5 ) '_PER ( 71 ) ^_0RG ( 2 ) There are i target axioms . Selected axiom : request (76) := null . 
Unifying : 76 to 76 . Ok-->Proving:_REASON ( 5 ) ?PER ( 71 ) ?8~G ( 2 ) There are 1 target axioms . Selected axiom :_REASON(5):=enable(536) . 
Unifying : 5 to 5 . flk --> Proving : enable ( 5 3 6 ) ^_PER ( YI ) -_flRG ( 2 ) There are i target axioms . Selected axiom : enable (757376):= null . 
Unifying : 3 to ?3 . 5 to 75 . 6 to 76 . Dk-->Proving:_PER ( 71 ) ^_ORG ( 2 ) There are 3 target axioms . Selected axiom :_PER (71) := null . 
Unifying : 71 to gl . 0k - -> Proving:_ORG ( 2 ) There are i target axioms . Selected axiom :_0 RG(1) := FBI(1) . 
Unifying : 2 to i . Ok --> Proving : null Jl \] J We found : Success . 

There are cases when our simple prover fails to prove a  . correct answer . We have notice ( 1 that this hal ) pens1 ) ecause in the answer semantic representation , s t ) me concepts that are connected in the question semantic representation are no longer directly linked  . This is due to the f~mt that there are either parser errors or there are new syntactic dependencies between the two concepts  . To acconmmdm ; eth is situation , we allow diflhrent constants that are arguments of the sanle predicate to be unifiable  . The special cases in which this relaxation of the unification i  ) roeedul'e is allowed constitute our abduction rl tles  . 
2975 Evaluation
Both qualitative and quautitative valuation of the integration of surface text based and knowledge -based methods for Q/A is imposed  . Quantitatively , Tal ) le 3 summarizes the scores obtained when only shallow methods were emI  ) loyed , in contrast with the results when knowledge-based methods were integrated  . We have sepm'ately measured the effect of tile integration of the knowledge-based methods at question processing and answer processing level  . 
We have also evaluated the precision of thesys -tern when both integrations were implemented  . The results were the first five answes's returned within  250 bytes of text , when approximatively half million TREC documents are mined  . Wch ave used the 200 questions from TREC8 , midtile correct answers provided by NIST . The performance was measured both with the NIST scoring method employed in the  TREC8 and by simply assigning a score of 1 t br the question having a correct answer , regardless of its position . 
Percentage of NIST score correct answers in top 5 returns
Tezt-suTJ ' acc-bascd 77.7% 64.5%
Knowledgebased 83.2% 71.5%
Question Processing ( only ) ~l ~: rt-su@zce-bascd 77 . 7% 73% only ' with Answer

Knowledgebased 89.5% 84.75%
Question Pw ces . sin 9 with Answer

Table 3: Accuracy t ) erformance
When using the NIST scoring method to evaluate an individual answer  , we used only six values : (1 ,   . 5,  . 33,  . 25,  . 2 ,  0) , representing the score the answer's question obtains  . If the first answer is correct , it obtains a score of 1 , if the second one is correct , it is scored with . 5 , if the third one is correct , tile score becomes . a a , if the four this correct , the score is . 25 and if the fifth one is correct , the score is . 2 . Otherwise , it is scored with 0 . No credit is given if multiple answers are correct  . Table 3 shows that both knowledge-based methods enhanced the precision  , regardless of the scoring method . 
To further evaluate the contribution of timjusti -ficat  , i on option , we evaluated separately the precision of the provert brthose questions for which tile surface -text-based methods of our system  , when operating alone , emmetfind correct answers . We had 45   TREC8 questions for which the evaluation of the prover was performed  . Table 4 summarizes the accuracy of the prover . 
hl correct ~ il swors ( no knowledge )
Correct m~s wers ( KB-based )
Incorrect answers ( KB-based )
Provell con'ectl ' roven Precision incorrect 210   98  . 5% 5 96 . 2% 38 90 . 04%
Table 4: Prover performmme
Qualitatively , we find that the integration of knowledge-based methods is very beneficial  . Table 2 illustrate stile correct answer obtained with these methods  , in contrast to tile in correcl , answer provided when only the shallow techniques m'eal  ) plied . 
6 Conc lus ions \ Ve believe that the lmrfol'nmnce of a Q/A system del  ) ends on the knowledge sources it employs . In this paper we trove presented the effect of tile integration of knowledge derived from question tax-enemies and produced by answer justifications on the Q/A precision  . Our knowledge-based methods are lightweight , since , we do not generate precise semantic rel ) resentations of questions or answo . rs , but mereatt proximations determismd by syntactic de-1  ) en ( lencies . Fm'thermore , our prover operates on very simple logical representations  , its which syntactic and semantic ambiguities are completely ignored  . 
Nevertheless , we have shown that these approximations are functional  , since we implemented a prover that justifies answers with high precision  . Similarly , our knowledge-t ) ased question l ) rocessillg is an lel'e combination of word class information and syntactic dependencies  . 
References
Michael Collins . A New Statistical Parser Based on Bigram Lexical l  ) et ) endencies . In Proceedings of the 2dst Annual Mectin9 of the Association for " Computational Lin . quistics,
ACL-.96~pages 18419t , 1996.
Christlane Fellbaum ( Ed) . WordNet-An Electronic Lexicall ) at a base . MIT Press , 1998 . 
Jerry R . II obbs . Discourse and Inference . Unpublished malmscrlpt , 1986 . 
Jerry R . . Itobbs . Overview of the TACITUS Project; . In Computational Linguistics , 12:(3), 1986 . 
Jerryltobbs , Mark Sticke I , Doug Appelt , and Paul Mm'-tin . Interpretation as abduction . Artificial Intelligence , 63, pages 69142, 1993 . 
Y Vendy Lehnert . The processing of question answering . 
Lawrence Erlbaum Publishers , 1978.
l ) an Moldovan , Sandallarabagiu , Marius Pasta , Rada Mihalcea , Richard Goodrum , Roxana Gh:iual K1 Vasilell . us . 
Lasso : a tool for surfing the answer net . In Proceedings of
TREC 8,1999.
Ellen Riloff and Rosie Jones . Learning l ) ictionaries for hffor-mation Extraction by Multi -Level Bootstrat  ) plng . In Pro-cccdings of the 16th National Conference on Artificial Intelligence , AAAI99 ,  1999 . 

