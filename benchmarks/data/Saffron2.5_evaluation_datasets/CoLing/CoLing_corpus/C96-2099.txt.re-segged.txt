Segmenting Sentences into Linky Strings
Using D-bigram Statistics
Shiho Nobesawa
Junya Tsutsumi , Sun Da Jiang , Tomohisa Sano , Kengo Sato
Masakazu Nakanishi
Nakanishi Laboratory , Keio University
3-\]41 Hiyoshi , Kohoku-ku
Yokohama 223 Japan
shiho@nak.math.keio.ac.jp

It is obvious that segmentation takes an important role in natural language processing  ( NLP )  , especially for the languages whose sentences are not easily separated into morphemes  . In this study we propose a method of segmenting a sentence  . The system described in this paper does not use any grammatical information or knowledge in processing  . Instead , it uses statistical information drawn from non -tagged corpus of the target language  . Most of the segmenting systems are to pick out conventional morphemes which is defined for human use  . However , we still do not know whether those conventional morphemes are good units for computational processing  . 
In this paper we explain our system's algorithm and its experimental results on Japanese  , though this system is not designed for a particular language  . 
1 Characteristics of Japanese
Text 1.1 Letters in Japanese
Japanese text is composed of four kinds of characters kanji  , hiragana , katakana , and others such as alphabetic haracters and numeral characters  . 
It iragana is used fbrJapanese words , inflections and flmction words , while k~takana is used for words from foreign languages and for other special purposes  . 
Table 1 shows examples of rates of those four characters in texts  ( Teller and Batchelder ,  1994) . 
The bus__ . _= , corpus consists of a set of newspaper articles on business ventures from Yomiuri  . The ed__: , corpus contains a series of editorial columns from Asahi Shinbun  . 
Table 1: Character Rates in Japanese Text bus __ ~ , ed . _=size(K chars ) 422 75% hiragana 30 . 2 58 . 0% kanji 47 . 5 34 . 6% katakazla 19 . 3 4 . 8% num/alph2 . 9 2 . 6 1 . 2 Morphemes in Japanese Segmenting a Japanese text is a difficult task  . A phrase " ~ b-C ~ ~ b f ~ ( was studying ) " call be a single lexical unit or can be separated into as m ~  , ny assix elements ( Teller and Batchelder ,  1994 ) : ' study''do'particle progressive polite past Acquiring " morphemes " from Japanese text is not a simple task because of this flexibility  . 
2 Linky Strings
This paper is on dividing non-separated language sentences into meaningful strings of letters without using any grammar or linguistic knowledge  . 
Instead , this system uses the statistical information between letters to select hebest ways to segment sentences in non-separated languages  . 
It is not very hard to divide a sentence using a certain dictionary for that  . The problem is that a ' certain dictionary ' is not easily obtainable  . There never is a perfect dictionary which holds all the words that exist in the lmlguage  . Moreover , building a dictionary is very hard work , since there are no perfect automatic dictionary -making systems  . 
586 llowever , machine-readable dictionaries are needed anyway . ~ br this reason , we propose a new method for picking out meaning fl fl strings  . Our purpose is not to segment a sentence into conventional morphemes  . We introduce a concept for a type of language unit for machine use  . We named the unit a ' linky string ' . A linky string is a series of letters extracted from a corpus using statistical intbrmation only  . It is a series of letters which share a strong statistical relationship  . 
3 LINKING SCORE 3.1 Linking Score
To pick out linky strings , we need to find highly connectable letters in a sentence  . We introduce the linking score . , which shows the linkability between two neighbor letters in a sentence  . This score is estimated using d-bigram statistics . 
3.2 D-bigram
The idea of bigrams and trigrams is often used in studies on NI  , P . Wgram is the information of the association between certain events  . In this study we use the d-bigram data ( Tsutsumi et al ,  1993) , which is a kind of bigrmn data with the concept of distance between events  ( Figurel )  . l)-higram is equal to bigram when d = l , thus d~bigrmn data includes the conventional bigrmn relation  . 
d . bigrsm7 q\[E3D
L__IL___I\[__lL__J ?='
I ; II , jJ
LJr\]...~....
Figure l : i ) - bigram 3 . 3 Ca lcu la t ion Mutua l In f i J rmat ion w i th Distance Expression  ( 1 ) iv for calculating mutual intbrma-tion between two events  ( Nobesawa et al , 1994): l'(ai , bj , d ) bj , d ) = log v ( . d /'( b ,   )   ( 1 ) ai : a letter P ( ai ) : the possibility the letter ai appears l ? ( ai , bj , d ) : the possibility ai and bj appear together with the distanced in a sentence The parameter d shows the distance between two events  . In Figure 2 , the distance between " a"m~d " pen " is 1 , and the dist m , ce between " is " and " pen " is 2 as well . Since the event order has a meaning , in this case the distance between " pen " and " a " is defined as  -1   . 
ThL ~ is a pen.
dM2 d=3
Figure 2: D-bigram Example
As the vahm of MI gets bigger , the stronger is the association between the two events  . 
Linking Score
Expression ( 2 ) is t br calculating the linking score between two letters in a sentence ~  . 
Z ( 2 ) d :-: lj = i- ( d-1 ) dmax : max distance used wl : the ith letter in the sentence wg  ( d ) : a certain weight for iV // concerning distance between letters The information between two remote words has less nmaning in a sentence when it comes to the semantic analysis  ( Church and Hanks ,  1989) . 
According to the idea we put g ( d ) in the expression so that nearer pair can be more effective in calculating the score of the sentence  . 
h'i ,, II
I--'1
BC@@FGH
Figure 3: Calculation of Linking Score
A pair of far away letters do not have strong relation between each other  , neither syntactically nor semantically . For this reason we used ma , , and in this paper we set tile dmax value 2 to 5 and 1  . 
When the dma , is 1 , the MI used in calculation is only bigram data . 
1 We made a Japanese word "> ( i ~" tlar the word " linky " , We used it's pronunciation " UK\[ju:kei\]"in the expre ~ ion  . 
~ We had experiments for tinding a good value for dmaa :  , lt
Figure 5: The ScoreGraph 4 THE SYSTEML~S 4 . 1 Overview This system is called LSS , a " linky string segmentor " . This system takes a corpus made of non-separated sentences as its input and segments it into linky strings using d-bigram statistics  . 
Figure 4 shows the flow of LSS's processing.
Input sentences to segment.
Calculate the linking score of each pair of neighboring letters  . 
Check the score graph to see where to segment.
pick out each linky string found in the given corpus  . 
Figure 4: System Processing Flow
In this paper we used a fixed score for the starting score  , so that/S~candecide whether the first letter should be a one-letter linky string  . 
4.2 The Score Graph
What a ScoreGraph Is
To segment a sentence into statistically -meaningful strings  , we use the linking scores to locate boundaries between linking strings  . A score graph has the letters in a sentence on the x-axis and linking scores on the y-axis  ( Figure 5 )  . We get one score graph for each sentence . Figure 5 shows two sentences ( one above and one below )  , each of 14 letters ( including an exclamation/question mark as the sentence terminator  )  . 
When the linking score between a pair of neighboring letters is high  , we assume they are part of the same word . When it is low , we assume that the letters , though neighbors , are statistically independent of one another . In a score graph , a series of scores in the shape of mountain ( ex . : AB and CF part in Figure 5 ) becomes a linky string , and avalley ( ex . : between the letter B and C in
Figure 5) is aspot to segment.
Score-Graph Segmenting Algorithm
The system LSS finds the valley-points in a sentence and segments the sentence there into strings  . 
Following is the algorithm to find the segmenting points in a sentence  . 
1. Do not segment in a mountain.
2. Segment at the valley point.
3 . Cut before and after a one-lettered linky string . 
One-Lettered Linky String
A one-lettered linky string needs to ( a ) place at the valley point , and ( b ) look flat 3 in the score graph . In Figure 5, one-lettered linky strings are
G , L , N4'0'Y , Z and ?.
Mountain Threshold
A linky string takes amountain shape because of high linking scores  . Note that a linky string is not equal to a morpheme in human-hand made grammars  . When a certain pair of morphemes occurs in a corpus very often  , the system recognizes the pair's high linking score and puts them together into one linky string  . For example ,  "~" "~ "2/~:: ) k : ~$ ~ ( President Bush ) " is often treated as a linky string , since ": ) ""9 " J ~ ( Bush ) " and " gk: ) ~ YI ( president ) " appear next to each other very frequently . 
The mountains of letters are not always simple hat -shaped  ; most of time they have other smaller mountains in them  . This means that there can be shorter strings in one linky string  . In one linky string "7" y "5' J-:%i ~ ( President Bush ) " , there must be two smaller mountains , just like H-I and JK in the mountain HK in Figure  5  . To control the size of linky strings we introduce amountain threshold  , which is shown in the sentence below in Figure 5 . When the score of a valley point is higher than the mountain threshold  , the system judges the point is not a segmenting spot  . In this paper the mountain threshold value is 5 . 0 . 
3We use a constant value as a threshold.
a N is a special one-lettered linky string which places at the beginning of a sentence  . 

Figure 6:
Bigram r - - - 1
BC ~ DEF blr h'n ~ ly

I i
I'+'i II--\]
ASC ~ DEd-lol Cz , amsumup
The Difference between D-bigram and ' Fable 3: Output d-bigram bigram dmaz = 5  #of input sentences 302   302  #of linky strings 6  , 145 7 , 098  #of linky strings per sentence 20 . 35 23 . 5 0  #of over segmented spots 454   689 over segmented spots 7  . 39% 9 . 71% 4 . 3 Corpus LcN accepts all the non-separated sentences with little preparation  . All we need is a set of certain amount of the target-language corpus for training  . 
In this paper we show the experimental results on Japanese  . The corpus prepared for this paper is of Asahi Shinbun Newspaper  . 
5 RESULTS 5.1 Experimental Results
Experiment Condition
LK ?/ takes a set of non-separated sentences as its input and segments them into linky strings  . For the test corpus we chose sentences at random from the training corpus  . 
into 2025 linky strings on average 5 . And in one sentence there are only one or two spots on average which break a morpheme into meaningless strings  . 
With no linguistic knowledge , this can be said to be quite a good result . 
It is hard to check whether an extracted linky string is a right one  , however , it is not that difficult to find over segmented strings  , for a linky string needs to hold the meaning . We check those over segmented linky strings according to a dictionary  , 
Iwanami Kokugo Jiten.
Table 4 shows the numbers of over segmented spots . The figure is the ~ mmber of over segmented spots , not the number of morphemes over segmented 6 . 
In Table 4 A and B are neighboring letters in a sentence which are forced to separate  . The row " kanji hiragana " standsfdr over segmented spots between a km~ji letter and a hiragmm letter  . 
' Fable 2: Training Corpus Condition language : Japanese form : non-separated kanji-kana mixed sentences 
As a hiSh in bun Newspaper corpus : of sentences for training corpus:  7  , 5 02  #of sentences for test corpus: 302 To see the efficacy of d-bigram , we compare the experimental results of two data : d-bigram data and bigram data  . 
Experimental Results
As shown in Table 3 , with d-bigrmn information only 7 . 39% of the segment spots are over segmented . 
Table 3 shows that a sentence gets separated Table 4: Over-Segmented Morphemes by Character Types and Segmentation Methods d-bigram 
ABd .   .   .   .  =  5 anji kanji kanji hiragana hiragana kanji hiragana hiragana katakana katakana total  454 bigram 59   65   29   43   18   22   333   507   15   52 each part of speech is shown in Table 5  . ' K ' stands for kanji , ' h ' is for hiragan and ' k ' is for katakana . 
There was no missegmentation between katakana and other character types  . There also was not any 5'Fhe range of numbers of linky strings found in a sentence is  560 with d-bigram and 6-66 with bigram . 
6Thus a morpheme gets counted twice when it is divided into three strings  . 

Table 5: Over-Segmented Morphemes in Output with D-bigram

B noun proper noun pronoun verb aux . verb adjective adj . verb adverb rent ai-shi conjunction funcion word suffix compound word total 
KK hhk V - - - - ~
KhKhk 49   19   6   49   6   129   5   8   13   16   16   1   3   12   84   100   60   60   4   12   16   4   13   17   1   53   1   55   11   11   7   7   15   15   1   4   5   1   15   16   59   29   18   333   151454\ ] missegmentation concerning alphabets , numeral characters and other symbols . 
5.2 A Linky String
Characteristics of Linky Strings Linky strings in Japanese are not equal to conventional morphemes in Japanese  . As discussed in section 1 . 2 , it is not easy to decide an absolutely correct segmenting spot in a Japanese sentence  . 
That is one of the reasons that we decided to extract linky strings  , instead of conventional morphemes . I to we ver , if those linky strings do not keep the meanings , it is useless . 
The result shows the linking score works well enough not to segment senteces too much  ( Table 3 )  . That is , we succeeded in extracting memfing fld strings using only statistical information  . Figure 7 shows some examples of extracted linky strings . 
.~1~ bank ( s ) meaating ful ~ Z ~ move/shift to meaning fifl ? ) ~T  ~ ) J action of meaningful ~ T ~ ~: did meaningful ( ? ) ~= Y ( ? ) over segmented ( ~ V ) q~'~\]':~over segmented Figure 7: Examples of Linky Strings ( 1 ) Sometimes LS8 extracts strings that look too long ( Figure 8 )  . This is not a bad result , though . 
When a linky string contains several morphemes in it  , it is something like picking out idioms . A linky string with several morphemes may be a compound word  , or an idiom , or a fixed locution . 
17 t ~ a )$ help
London Summit nuclear non-proliferation treat the end of  17th century
Japan Railway Kyoro Station
Figure 8: Examples of Linky Strings (2)
The Concept of the Linky Strings
Grammar-based NLP systems generally specify a target language  . On the other hand statistically-based approachs do not need rules or knowledge  . 
This makes a statistically-based approach suitable to nmltilingual processing  . 
IS g is not only for Japanese . With a corpus of non-separated sentences of any language  , LSS can perform the same kind of segmentation . 
To deal with natural languages most systems use conventional morphemes or words as their processing units  . That is , most systems need to recognize morphemes or words in sentences  , and they need to make up a fairly-good morphological-ys is before the main processing  . We have been working for processing natural languages in linguistic ways  , though we do not know whether it is a rightway in computational linguistics  . A linky string is extracted only with statistical information  , using no grammars nor linguistic knowledge . 
The system does not need to behave like a native speaker of the target language  ; all it has to do is check statistical information  , which is what computers are good at . We expect that linky strings can be a key to solve problems of NLP  . 
Compound Words
The results show that the system has 7 . 39% incorrect segmentation . This result is based on a Japanese dictionary , and when a morpheme listed in the dictionary gets separated  , we count it as over segmented . However , a dictionary often holds compound words . That is , some number of tile segmented spots which we have counted as " over segmented " ones are not really over segmented  . 
From this point of view , the percentage of oversegmentation is actually even lower  . 
Inflections
Verbs , adjectives , adverbs and auxiliary verbs are inflected in Japanese  . In the experimental result , 89 . 7%  ( with d-bigram data ) of over segmented spots between kanji and hiragana occurs in inflective morphemes  . We decided correct segmenting spots dictionary . According to statistical information , segmenting method for inflective morphemes i different fl'om grammatic alone  . So most of the over segmented spots can be treated as correct segmenting spots according to statistical information  . 
5.3 D-bigram Statistics
According to Table 3 , its cents that using the bigram method the output is apt to be more segmented than with the d -bigram method  . 
This happens t . ) ecause bigrmn cannot pick out long strings . Bigrmn does not hoht information between remote ( actually more than one letter away ) letters . That makes long strings of letters easily segmented  . When LcN checks a three-letter morpheme ABC , with bigram data it can see the string only as AB and BC  . If those strings AB and BC do not . appear often , the linking scores get low and Lq . S decides to segment between AB and BC . I Iowever , with d-bigram data ISS can get the information between A and C as well  , that helps to recognize tlmtA , B and C often come out together . This happens frequently between two katakana letters  ( Table 4 )  , because of the usage of katakana letters in Japanese  . 
This does not mean that with d-bigram method sentences are less likely to be segmented  . As shown in Figure 9 , the distribution is not so different between two methods  . The x-axis shows then mn-bers of linky strings in sentences and the y-axis shows the number of sentences with x linky strings  . 
s ~ nton cos ' ~, . . . . b ig ram 1 6 i 14 7 .   .   .   .   .   . 
i_st : I hlgs
Figure 9: The Number of Strings in Outlmt Sentences According to Figure  9  , the distributions of sentences are not so different between the method with d-bigram and the one with bigram  . 
6 CONCLUSION
This paper shows that this automatic segmenting system/NS is quite efficient for segmentation of non-separated language sentences  . /AN does not use any grammatical information to divide input sentences into linky strings  , that is , a new refit for NLP . According to the results of the experiments , Lcx ? ~ can segmentah nost all the sentences ' correctly '  , with string skeeping their meanings . 
This remarkable result of a statistic-based system l ~ shows that d-bigram statistical information can be a key to meaningful-string extracting  . 
This result also shows that the concept of linky strings is an interesting concept for NLP  . We expect that this linky string can heaun it for machine translation systems or keyword /phrasex-traction systems  , and other NLP systems . 
References\[1\]Tsutsumi , J . , Nitta , T . , Ono , K . and Nobesawa , S . A MultiLingual Translation System Based on A Statistical Model  ( written in Japanese )  . JSAI Technical report , SIG-PPAI-9302-2 , pages 712 ,  1993 . 
\[2\] Nobesawa , S . , Tsutsumi , J . , Sun D . J . , Sano , T . , Sato K . and Nakanishi , M . Automatic Extraction of Linky Strings in Natural Languages  ( written in Japanese )  . 6 2nd Annual Meeting of the ANLP ( NLP 96) , pages 181-184 ,  1996 . 
\[3\] Nobesawa , S . , Tsutsumi , J . , Nitta , T . , Ono , K . , Sun , 1) . J . and Nakanishi , M . Segmeting a Japanese Sentence into Morphemes Using Statistical Information between Words  . Coling-94, pages 227-233, 1994 . 
\[4\] Teller V . and Batchelder E . O . A Probabilisitic Algorithm for Segmenting Non -Kanji & q  ) anese
Strings . AAAI , 1994.
\[5\] Brown , P . , Cocke , J . , Pietra , S . D . , Pietra , V . D . , Jelinek , F . , Mercer , R . and l~oossin , A . A Statistical Approach to Language Translation . Coling-88, pages 71--76, 1988 . 
\[6\] Church , K . and Hanks , l' . Word Association Norms , Mutual Information , and Lexicography . In Proceedings of the 27th Annual Conference of the association of Computational Linguistics  ,  1989 . 
\[7\] Nishio , M . , Iwabuchi , E . ~ld Mizutani , S . 
Japanese-Japanese Dictionary The 3rd Edition.
Iwanami Shoten , 1985.

