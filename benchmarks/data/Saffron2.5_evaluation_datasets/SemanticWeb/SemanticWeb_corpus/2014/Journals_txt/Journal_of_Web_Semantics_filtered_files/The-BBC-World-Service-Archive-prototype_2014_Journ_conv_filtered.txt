Web Semantics: Science, Services and Agents on the World Wide Web 2728 (2014) 29

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

journal homepage: www.elsevier.com/locate/websem

The BBC World Service Archive prototype
Yves Raimond, Tristan Ferne, Michael Smethurst, Gareth Adams

BBC R&D, London, United Kingdom

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 14 March 2014
Received in revised form
20 May 2014
Accepted 7 July 2014
Available online 16 July 2014

Keywords:
Crowdsourcing
Semantic Web
Automated tagging
Speaker identification
Interlinking
Archives

Most broadcasters have accumulated large audio and video archives stretching back over many decades.
For example the BBC World Service radio archive includes around 70,000 English-language programmes
from over 45 years. This amounts to about three years of continuous audio and around 15 TB of data. The
metadata around this archive is sparse and sometimes wrong, but the full audio content is available in
digital form. We have built a system to process the existing audio and text and automatically annotate
programmes within the archive with Linked Data web identifiers. The resulting interlinks are used to
bootstrap search and navigation within this archive and expose it to users. Automated data will never
be entirely accurate so we built crowdsourcing mechanisms for users to correct and add data. The
resulting crowdsourced data is then used to improve search and navigation within the archive, as well
as evaluate and improve our algorithms. As a result of this feedback cycle, the interlinks between our
archive and the Semantic Web are continuously improving. This unique combination of Semantic Web
technologies, automation and crowdsourcing has dramatically reduced the amount of time and effort
required to publish this rich archive online. The BBC World Service archive prototype is available online
at http://worldservice.prototyping.bbc.co.uk, last accessed March 2014.

 2014 Elsevier B.V. All rights reserved.

1. Introduction

The BBC (British Broadcasting Corporation) has broadcast
radio programmes since 1922 and has accumulated a very large
archive of programmes over the years. A significant part of this
archive has been manually catalogued by professional archivists
to facilitate reuse, in other words to enable programme makers
to easily find snippets of content to include in their own, newly
commissioned, programmes. Creating this metadata manually is
a time and resource expensive process; a detailed analysis of a
30 min programme can take a professional archivist up to 9 h.
The coverage of such metadata is not uniform across the BBC
archive. For example, it excludes the BBC World Service, which
has been broadcasting since 1932. Little reuse is made of such
parts of the BBC archives as there is little or no metadata to help
locate content within them. Most of the programmes within the
BBC World Service archive, for example, have not been listened to
since they were originally broadcast.

 Corresponding author.

E-mail addresses: yves@raimond.me.uk (Y. Raimond), tristan.ferne@bbc.co.uk
(T. Ferne), michael.smethurst@bbc.co.uk (M. Smethurst), gareth.adams@bbc.co.uk
(G. Adams).

http://dx.doi.org/10.1016/j.websem.2014.07.005
1570-8268/ 2014 Elsevier B.V. All rights reserved.

Since 2009 some BBC programmes have been tagged with DBpedia [1] web identifiers for places, people, subjects or organisations [2]. A benefit of using Linked Data web identifiers as tags is
that they are unambiguous, and that we can retrieve more information about those tags when needed. For example, programmes
tagged with places can be plotted on a map, or aggregation pages
can be enriched with information about the corresponding topic.
By having these anchor points in the Linked Data web, we can accommodate a wide range of unforeseen use-cases. This process of
manual tagging is naturally very time-consuming, and with the
emphasis on delivering new content, would take considerable time
to apply to the entire archive.

Typically the publication of BBC archive content consists of
selecting a thin slice of the archive (a particular programme, for
example, or a set of clips around a particular topic), manually
annotating the programmes within it, and publishing it. However
this approach does not scale well. In this article we describe the
BBC World Service Archive prototype, an experiment developed
and run by BBC R&D to investigate an alternative approach for
publishing a large archive on the web.

Our system starts by automatically deriving links from archive
programmes to Linked Data IRIs, identifying topics (we call this
process tagging in the rest of this paper) and speakers. We use
the resulting interlinks to publish the archive and bootstrap search

text, which we then map to Linked Data identifiers using the DBpedia Lite1 service. Each of these topics is also associated with a
confidence score. We store the resulting weighted associations between programmes and topics in a shared RDF store. For the whole
archive, this process generated around 1 million RDF triples, interlinking this archive with DBpedia.

2.1.2. From audio

We also use the audio content itself to identify topics for these
programmes. This is motivated by the fact that a lot of these
programmes will have very little or no associated textual metadata
(in the World Service archive 19,000 programmes have no titles
and 17,000 have no synopsis). And even where textual metadata is
present we found it will rarely cover all the topics discussed within
the programme.

The full description of this algorithm to extract topics from
audio as well as its evaluation is available in [4]. The core algorithm
and our evaluation dataset are available on our Github account.2
We start by identifying the speech parts within the audio content.
We then automatically transcribe the speech parts using the open
source CMU Sphinx software. The resulting transcripts are very
noisy. Most off-the-shelf concept tagging tools perform badly
on noisy automated transcripts as they rely on the input text
being well written and include useful clues such as punctuation
or capitalisation. We therefore designed an alternative concept
tagging algorithm which does not assume any particular structure
in the input text and can cope with significant noise on the input.
We start by compiling a list of IRIs used to tag content across the
BBC. These IRIs identify people, places, subjects and organisations
within DBpedia.3 We look for possible occurrences of these IRIs
within our automated transcripts. For example if London was
found in the transcripts it could correspond to at least two
possible DBpedia IRIs: d:London and d:London,_Ontario. Our
algorithm uses the structure of DBpedia itself to disambiguate and
rank these candidate terms, and in particular a similarity measure
capturing how close two IRIs are from each other in the DBpedia
graph [4]. For example if the automated transcripts mention
London, and England a lot, our algorithm will pick d:London as
the correct disambiguation for the former, as it is very close to one
possible disambiguation of the latter, i.e. d:England.

Given this technology we estimated that it would take 4 years
to process the entire World Service archive on commodity
hardware. We therefore developed a cloud-based infrastructure to
process entire radio archives in a reasonable time [5]. With this
infrastructure in place, we processed the 3 years of continuous
audio in the archive within two weeks for a pre-determined
cost and generated a collection of ranked Linked Data tags for
each programme. For the whole archive, the automated audio
interlinking generated around 5 million RDF triples, interlinking
this archive with DBpedia and the rest of the Linked Data cloud.

2.2. Speaker identification

We also built a set of tools for identifying the speakers appearing in this archive. The diarize-jruby library,4 itself built on top
of the LIUM SpeakerDiarization toolkit [6], splits up programmes
and trains a model for each voice found. However identifying
speakers across programmes is hard to scale to large archives, as
the models have a large number of dimensions and we have a large

1 See http://dbpedialite.org/.
2 See https://github.com/bbcrd.
3 See http://dbpedia.org.
4 See https://github.com/bbcrd/diarize-jruby.

Fig. 1. Overview of the BBC World Service Archive prototype architecture.

and navigation within it. We then let users validate, correct and
augment these automatically derived links. As a result of this
feedback, the interlinks between our archive and the Semantic
Web are continuously improving. We also use this feedback to
continuously evaluate and improve our automatic interlinking
algorithms. An architectural overview of our system is available in
Fig. 1.

The paper is organised as follows. In Section 2 we describe our
automated tools for interlinking archive content with the Semantic
Web. In Section 3 we describe how such automatically derived
links are being used to publish this archive content online, and the
mechanisms we put in place to enable people to feed back on the
quality of those links. In Section 4 we describe how this prototype
has been used as of March 2014. We also describe different ways
in which we use this user feedback to constantly evaluate and
refine our algorithms. In Section 5 we describe similar efforts
and compare them with our approach. Finally we conclude in
Section 6.

2. Automated interlinking

Between 2005 and 2008 the BBC World Service digitised the
contents of its recorded radio programme library. The digitisation
project was a great success but the metadata for it was of limited
quality and quantity. It would take a significant amount of time
and resource to manually annotate this archive. We therefore
considered bootstrapping this annotation process using a suite of
automated interlinking tools working from pre-existing metadata
and from audio.

2.1. Automated tagging

2.1.1. From pre-existing metadata

In some cases, textual metadata is available alongside the
archive content. In the case of the BBC World Service archive, this
data could be a synopsis or a title for the programme. In other cases,
it could be a script, production notes, etc. We use this data when it
is available to try and associate the programme with a number of
topics identified by Linked Data IRIs.

We process the textual metadata using an instance of Wikipedia
Miner [3]. Wikipedia Miner learns from the structure of links
between Wikipedia pages and uses the resulting model to provide a service detecting potential Wikipedia links in the unstructured text. We trained a Wikipedia Miner instance with a
Wikipedia dump from August 2012. Wikipedia Miner returns a
set of Wikipedia identifiers for the various topics detected in the

Y. Raimond et al. / Web Semantics: Science, Services and Agents on the World Wide Web 2728 (2014) 29

Fig. 2. The homepage of the World Service Archive prototype.

For example we aggregate Ookaboo7 images linked to DBpedia
topics. We use an image associated with the highest ranked topic
to automatically generate depictions of programmes.

Automated data will never be entirely accurate so mechanisms
are in place for registered users to correct data when it is found
to be wrong and to add missing data. When logged in, users can
upvote or downvote each individual topic for a programme and
add new topics through an auto-completed list, using DBpedia as
a target vocabulary. A screenshot of the interface for a Discovery
programme on smallpox8 is available in Fig. 3.

Gathering this user feedback makes it possible to automatically
refine the automated algorithms. This in turns leads to better
automated metadata for the rest of the archive creating a useful
feedback cycle that leads to a better and better archive experience.
For example the aggregate of positive and negative votes on
each tag is used to improve the machine-generated ranking on programme pages. If a tag gets three positive and two negative votes,
its aggregate voting score will be one. We then use a weighted
sum of this voting score and the machine-generated scores to rerank tags. The resulting scores are constantly updated in our search
engine and will have an impact on which programmes will be retrieved when a user searches for a particular topic.

number of them (one per speaker per programme, which amounts
to several million for the whole archive). We therefore built an index based on Locality Sensitive Hashing (LSH) techniques [7], as
implemented in our ruby-lsh5 library, to perform fast nearestneighbour search. When queried with a seed speaker model, this
index will return a reasonably small list of candidate speakers that
may correspond to the same person, which can be used to greatly
reduce the number of comparisons needed to cluster these models.
We demonstrated the feasibility of such a method for large-scale
speaker identification by applying it to the entire archive [8].

Once the models have been clustered, we can create a unique
identifier for each speaker and link it to all programme segments
detected as featuring the same voice. However at this stage we do
not know who these speakers are.

3. Publishing the archive

We now have an automated set of links for each programme,
identifying topics and speakers, which we can use to bootstrap
search and navigation within an online archive. The topic data
can be used for browsing between programmes, generating topicbased aggregations and searching for programmes on specific
topics. The speaker data can be used to aggregate programmes featuring a particular person and to segment programmes by speak-
ers. We built an application using these links to publish this archive
on the web6 (see Fig. 2).

This web site is built using the data held within a shared
RDF store. This store includes the automated interlinks mentioned
above as well as all the data we could gather around this archive,
and aggregated Linked Data around our topic and speaker data.

As mentioned previously we can identify unique speakers
within the archive, but we do not know the person associated with
that voice. As depicted in Fig. 4, users can also associate speakers
detected within individual programmes with a specific person.
We use the LSH-based index mentioned above to propagate these
identities to other speakers in the archive detected as having a
very similar voice. The aggregation of all these programmes can
be accessed by clicking on the speaker name. When an identity

5 See https://github.com/bbcrd/ruby-lsh.
6 See http://worldservice.prototyping.bbc.co.uk.

7 See http://ookaboo.com/.
8 See http://worldservice.prototyping.bbc.co.uk/programmes/X0909348.

Fig. 3. A set of topics along with their origin and associated user validation data
around a Discovery programme on smallpox. Topics can be derived from textual
metadata (synopsis), audio or can be added by users. When logged in, users can
upvote or downvote individual tags by clicking on the thumbs button.

Fig. 4. Adding an identity to an anonymous speaker.

Fig. 5. Confirming an identity automatically propagated to an anonymous speaker.

is automatically propagated (and therefore could be wrong), we
also ask users to confirm the identity by clicking a tick button, as
depicted in Fig. 5. Ultimately, this leads to the example depicted in
Fig. 6, where all speakers are identified, and clicking on a speaker
leads to a page featuring all its other contributions.

Finally, we provide the ability for users to edit or correct the
synopsis and episode titles of programmes. By clicking the Correct
this description button users are given two text fields to edit. This
feature uses a wiki-model for editing and allows admin users to
revert to any previous version to fix any mistakes or vandalism.

As well as refining search and discovery within the archive and
helping us improve our algorithm, this crowdsourced data is also
helping us to continuously evaluate our automated interlinking
results, speaker and topic-based, as described in Section 4.2.

We are also providing a visualisation based on this everevolving set of interlinks and topics extracted from live BBC News
subtitles and described in [9]. This visualisation shows archive
programmes related to current news events. It enables journalists
or editors to quickly locate relevant archive content which can
then be used to provide more context around particular events. For
example, a recent news event about replacing poppy cultivation by

Fig. 6. A programme with its five speakers identified.

Fig. 7. Visualising archive programmes related to current news events. This capture
of the visualisation was taken during the May 2013 Prime Ministerial election in
Pakistan (involving Imran Khan, a politician and former cricketer) was discussed on
the news. The red programmes in this visualisation include a 1990 Benazir Bhutto
documentary and a 2003 Imran Khan interview.

cotton in Afghanistan led to the topics Opium poppy, Afghanistan
and Cotton being extracted from BBC News subtitles. This
visualisation then picked up a 2008 radio programme about a new
opium ban in Afghanistan and the impact it had on local farmers.
An example visualisation is given in Fig. 7.

4. User activity and evaluation

Launched in July 2012 to a limited panel of World Service
listeners, the prototype was opened to all in January 2013. As of
March 20149 it has 3685 registered users, and the evolution of
registered users over time is depicted in Fig. 8. Only registered
users can browse, listen to or tag programmes in the prototype
because of a restriction on content rights and so we can track
activity and usage. The users include a mixture of World Service
listeners, radio fans, industry professionals and BBC staff.

4.1. Activity across the archive

Of the 70,234 programme records in the archive there are
35,820 listenable programmes and 34,414 that have no audio

9 All figures for the rest of the paper, unless indicated otherwise, will be as of
March 2014.

Y. Raimond et al. / Web Semantics: Science, Services and Agents on the World Wide Web 2728 (2014) 29

Fig. 8. Cumulative registrations over time. Spikes in this graph are generally where
we have had publicity for the prototype. We suspended new registrations from
FebMay 2013.

Table 1
Totals of user tagging activity. Total is the total number of activities associated with
a given action. Programmes is the unique number of programmes affected. Users
is the number of unique users having performed a given action.

Action
Added
Voted up
Voted down

Total

Tags

Programmes

Table 2
Totals of other activities.

Action
Synopsis edits
Speakers identified
Listens

Total

Programmes

Users

Users

available to users. We are either unable to stream the audio for
these programmes to listeners for rights issues or a database record
was created in the original archive but there is no associated
archive audio file.

4.1.1. Editing activity

As reported in Table 1, users are nearly twice as likely to vote
up (or agree with) or vote down (or disagree with) an existing tag
than to add a completely new tag for a programme. The two voting actions are much less effort for a user (just clicking a button)
compared to adding a new tag (typing and selecting a tag from an
auto-complete list) so this is expected, and demonstrates the usefulness of bootstrapping the tagging process with machine tags.
Auto-generated tags with a high confidence are presented at the
top of the list on the programme page, leading to these being interacted with more frequently. As we see more up votes than down
votes, this indicates that the machine-generated ranking tends to
be accurate.

In total there have been 67,082 tagging actions in the prototype,
9248 listenable programmes (26%10) have had at least one tagging
action and 666 users (18%) have carried out a tagging action.

As reported in Table 2, aside from tags the other actions in the
prototype that are tracked are listening, editing descriptions and
labelling speaker names. 14,867 programmes (41%11) have been
listened to at least once and 1458 users (40%) have listened to
something at least once.

4.1.2. Activity by the user

Looking at the activity done by each user we can see that
one prolific user has done 32% of the total user edits, 10 people

Fig. 9. Cumulative % of edits by a number of users.

Table 3
User tagging activity by type.

Type of tag added
People
Places
Other
Type of tag voted up
People
Places
Other
Type of tag voted down
People
Places
Other

Number

Percentage of total
19.5%
7%
73.5%

28%
10%
62%

10%
14%
76%

have done 70% of the edits and 10% of the users have done 98%
of the work. This distribution is depicted in Fig. 9. Other crowdsourcing studies have shown similar long tail results, e.g. in
the steve.museum experiment 0.7% of users contributed 20% of
tags [10], the Flickr Commons experiment showed 40% of tags were
added by 10 users [11] and in the Australian Historic Newspapers
experiment the top 10 users were correcting significantly more
than all other users [12]. As mentioned above, 18% of the prototypes users have done at least one tagging action but that is not a
particularly good measure of current activity. Instead we use something Wikipedia defines as Active users  registered users who
have performed an action in the last 30 days.12 Using this metric
at the time of writing there are 44 active users (1.2%).

We have noticed particular groups of people using the proto-
type, particularly a large community of radio drama enthusiasts
who catalogue all radio drama and fans of the panel game, Just a
Minute. Having been made aware of this archive and experiment
a couple of users have even sent in 4 programmes missing from the
archive that they had recorded off-air at the time of broadcast.

4.1.3. Types of tags

We can classify the tags added or voted upon by users by
broad categories (people, places or other concepts), easily done
by using DBpedia. Note that these are not necessarily correct 
we define places as anything with a latitude and longitude in
DBpedia (which might be events for example) and things identified
as people in DBpedia may sometimes be roles or events associated
with a person. This means we also end up defining some tags as
both people and places so there is a small discrepancy between the
results in Table 3 and those reported in Table 1.

4.1.4. Most frequent tags with interactions

The programmes that are tagged are likely to be influenced by
the interests of users who have interacted with the prototype and

10 Out of 35,820 listenable programmes, although you can tag unlistenable
programmes.
11 Out of 35,820 listenable programmes.

12 See http://en.wikipedia.org/wiki/Wikipedia:Statistics.

3,0002,5002,0001,5001,0005000# of registrationsOctober2013AprilJulyOctober2014TimeY. Raimond et al. / Web Semantics: Science, Services and Agents on the World Wide Web 2728 (2014) 29

by the programmes that have been featured on the prototypes
home page.

The 20 most frequently added tags are: Panel game (88 adds),
Comedy (79), 30 min13 (78), Play of the Week (70), Just a Minute
(69), Ian Messiter (59), Drama (41), William Franklyn (37), Patricia
Hughes (34), London (33), Interview (31), African Theatre (31),
Australia (30), Quotation (29), Family (27), 5 min (26), India (24),
Fletcher (24), Clement Freud (24), 60 min (23).

This list is dominated by tags related to a panel game, Just a
Minute (e.g. Ian Messiter) and to radio drama (e.g. Play of the
Week and African Theatre)  both areas where significant fan
groups interacted with the prototype.

The 20 tags most frequently voted up are: Music (265 up
votes), Panel game (262), Comedy (193), BBC World Service (171),
William Shakespeare (147), Nicholas Parsons (142), Nigel Rees
(123), Quotation (123), United States (116), Just a Minute (108),
London (106), Film (100), Clement Freud (96), Whistle (96), Drama
(94), India (92), Technology (89), BBC (89), Paul Merton (87), South
Africa (87).

Again, many tags relate to two particular panel game pro-
grammes, Just a Minute and Quote, Unquote  Nicholas Par-
sons, Nigel Rees, Clement Freud and even whistle are associated
with these (a whistle is used to signal the end of a games round!).
The 20 tags most frequently voted down are: Doctors (TV series)
(223 down votes), Black (English band) (210), Play (theatre) (206),
Moon (film) (177), Brother (film) (175), James (band) (165), Game
(food) (161), Smile (The Beach Boys album) (147), Queen (band)
(145), Michael (archangel) (135), Greek (TV series) (129), Ride
(band) (111), Joe (singer) (109), Royal Dick School of Veterinary
Studies (109), Heroes (David Bowie song) (106), Madness (band)
(104), Hole (band) (99), Subjectivity (98), Bottom (TV series) (98),
Prince (musician) (97).

It can be seen that most of these are ambiguous words and
are often incorrect machine-generated tags pertaining to cultural
artefacts (bands, albums, TV series, etc.) named after very common
words. We can use this feedback to derive a prior of a tag being
wrong, which is used to refine the ranking outputted by our
automated tagging algorithm.

It also appears that disputed tags (i.e. with a similar number
of positive and negative votes) are not disputed because of their
accuracy (the corresponding topics will be mentioned during
the course of the programme), but because of their relevance
to describing the full programme. For example, is Satellite an
appropriate tag for describing a documentary about the 1969 moon
landing?

4.2. Evaluation

4.2.1. Tagging

We also asked 13 professional archivists to contribute to the
prototype and annotate the 95 programmes with the most user ac-
tivity. When going to a particular programme page, they were presented with a blank slate (no machine-generated tags) and asked
to start tagging from scratch. We used the resulting annotations as
ground truth to evaluate the automatically extracted tags as well as
the tags resulting from the crowdsourcing experiment. In the latter case we consider as correct every tag where the sum of upvotes
minus the sum of downvotes is 1 (added tags are initialised with
one validation).

We see in Table 4 that the results are very promising. As
more users contribute to the prototype, the precision and recall of

Table 4
Precision and recall of tagging over time. In August 2012 only the automatically
extracted tags as considered. In September 2013 the crowdsourcing experiment had
been running for a year. In March 2014 for 18 months.

Time
August 2012
September 2013
March 2014

Precision
16.4%
29.1%
30.3%

Recall
24.8%
35.5%
36.7%

F-Measure
19.7%
32%
33.2%

Fig. 10. Normalised tag additions, upvotes and downvotes over time.

the resulting tagging has improved. Therefore, the quality of the
interlinks between our archive and the rest of the Semantic Web
continuously improve, which means that search and discovery
within our archive is getting better and better.

However more work is needed to evaluate the quality of the
resulting interlinks. This is complex as tags can be very subjective.
We noticed for example that most of the errors spotted in our
evaluation were due to a different understanding of what a
good tag is. For example one programme about the Higgs Boson
particle was tagged by archivists with all related branches of
physics (physics, particle physics, experimental physics, quantum
mechanics...), where machine-generated and crowdsourced tags
only mentioned one (particle physics) but also added a wide
range of other concepts (e.g. CERN). Archivists are primarily using
taxonomies to annotate BBC content at the moment rather than a
large Linked Data repository like DBpedia, which might explain the
differing tagging behaviour.

Another way of evaluating the success of the tagging component of our prototype is to study the number of user tag agree-
ments, additions and disagreements over time, normalised by the
total user activity. We show these results in Fig. 10.

Over time we would hope to see that the user tag additions
starts to trend down, as the tags for programmes become more
numerous and describe it completely. However the graph looks
approximately level, possibly because tagging is very subjective
and the tagging space is so large so there is always space for new
tags to be added. The graph for users voting up tags is trending
upwards as expected. As the list of tags for a programme becomes
generally more correct through crowdsourcing we would expect to
see more agreements with these tags. The graph for users voting
down tags is trending down as expected. As the list of tags for a
programme becomes generally more correct then we would expect
to see less disagreement with these tags. This is all with a caveat
that we cannot account for users behaviours changing over time
or as different groups of users interact with it.

13 We think this is people tagging programmes with their duration, though
the Wikipedia article for 30 min http://en.wikipedia.org/wiki/30_Minutes_
(disambiguation) is for various cultural works. See also 5 min and 60 min.

4.2.2. Speaker identification

We also constantly track new speaker identities being added
or confirmed by users, and use them to quantify how well our

Y. Raimond et al. / Web Semantics: Science, Services and Agents on the World Wide Web 2728 (2014) 29

speaker interlinking feature works  how accurate the speaker
aggregations we automatically construct are and how well our
identity propagation algorithm is working. This means we can
evaluate our algorithm on a real archive, with real user data. We
are evaluating the precision and recall of this feature as more and
more user edits come in and as of March 2014, we have an 85.1%
precision and a 43.1% recall.

5. Related work

5.1. Crowdsourcing approaches

The presentation of crowdsourcing in this experiment is similar to that found on Wikipedia. The prototype is useable as just
a reference resource, but with relatively prominent crowdsourcing features. That is made possible by automatically annotating
our content as an initial step. It is less similar to more task-based
crowdsourcing projects, e.g. GalaxyZoo [13], which are generally
designed to enable people to do the work as efficiently (and sometimes enjoyably) as possible. It would be a valuable experiment to
try a comparable task-focused approach to gathering the data we
want. We also use two crowdsourcing interface paradigms in the
prototypes interface. There is a wiki-like last edit wins approach
for the synopsis editing and an aggregated voting approach (as
popularised by Reddit14) for the tags. It is difficult to directly compare the approaches here as they are used for different purposes 
the tag interface is promoted more and is much more widely used.
But it is worth noting that we have not knowingly suffered any vandalism or malicious tagging in the prototype from either approach,
despite it being an open experiment on the internet.

5.2. Crowdsourcing for linear audiovisual media

There are relatively few other examples of crowdsourcing
metadata for linear audiovisual media. There is the Waisda video
labelling game, a game with a purpose [14] from The Netherlands Institute of Sound & Vision which was applied to a number
of long-form and short-form videos [15]. Similar approaches, including PopVideo, a project from Luis von Ahn at Carnegie Mellon
University [14] and the Yahoo! Video Tag Game from Yahoo! Research are reviewed in [16]. It is worth noting that these video tagging games often lead to tags that describe who and what is seen
in the video, rather than identifying higher-level concepts such as
Barack Obama talking about the recent events in Syria, which our
prototype targets.

There are even fewer examples of crowdsourcing for audio
alone: Mooso, a tagging game for live music radio [17] and Annotatable Audio for speech radio [18] both from the BBC. A number of
web applications do allow timed comments (e.g. Soundcloud and
YouTube) but these are not primarily used for collecting descriptive metadata.

We are unaware of any other examples of crowdsourcing at
scale using Linked Data tags, we believe that using Linked Data in
user- and machine-tagging has major benefits: for linking internal
systems together, for ensuring tags are unambiguous and for
linking out to other places on the web, therefore being able to
retrieve more information about these tags when needed.

5.3. Crowdsourcing and algorithms

Another unique feature of our prototype is the combination
of crowdsourcing with machine listening, and the feedback loop

14 See http://www.reddit.com/.

between algorithms and humans. One of the most notable efforts
in that space is Pop Up Archive15 which automatically transcribes
uploaded audio and let users correct the transcripts through the
Amara platform.16 We do not apply this approach to transcriptions,
but to automated tagging and speaker identification, and each edit
is used to evaluate and refine the corresponding algorithms.

Another experiment combining machine learning algorithms
with crowdsourced data, although in the scope of task-based
experiment, was run on the GalaxyZoo dataset [19], in order to
guide people to the tasks that are most suited to them.

6. Conclusion and future work

In this article we described the BBC World Service Archive
prototype, an experiment run by BBC R&D to investigate an
alternative approach for publishing a large archive on the web. Our
system starts by automatically deriving links from archive content
to Linked Data IRIs, describing topics and speakers associated
with these programmes. We use the resulting data to publish the
archive online and bootstrap search and discovery within it. We
then let users validate, correct and augment these automatically
derived links. We use this feedback to continuously evaluate and
improve our automatic interlinking algorithms. We have shown
that a combination of automated interlinking algorithms and
crowdsourcing can be used to publish a large archive quickly and
efficiently.

We have a few usability issues to solve, such as better visualisations for programmes featuring a large number of speakers, the
ability to jump to the specific point in the audio which triggered
a particular DBpedia tag, and a less ambiguous auto-suggest box
when adding new topics. The current architecture could be simplified by storing user activity and automatically extracted data
in a single place, ideally the RDF store. Handling fast-moving user
data in such a store requires a different performance profile, so our
current solution would need to be re-evaluated. There is a lack of
off-the-shelve tools for handling user registration, authentication
and activity in RDF stores, so this solution would require significant development time. We are investigating automatically identifying speakers through projects like the Speakerthon event,17
which will eventually lead to an openly licenced database of voices
for famous people. We want to investigate grouping topics into
actual events, e.g. Measles, Outbreak and Swansea could be
grouped into a single event as defined in the Storyline ontology.18
This would enable more precise event-based discovery within the
archive. We are also working on automated programme segmentation  some programmes are fairly long and tackle multiple topics
which has a negative impact on our automated interlinking algorithm and on granular access to individual programmes. Finally,
we have started work on a platform for sharing our automated interlinking tools and cloud-based processing framework with other
content owners outside of the BBC, called COMMA.19 We are considering developing a white-label version of this prototype, which
could be applied to any archive and would use COMMA as a processing back end.

There also remain a few questions to answer. As mentioned before tagging can be very subjective, and it is very difficult to quantify what constitutes a good tag. We are currently investigating a
number of techniques to try and answer that question, for example
by analysing search and access logs for topic pages.

15 See http://popuparchive.org/.
16 See http://www.amara.org/.
17 See http://bbc.in/1ccQgrW.
18 See http://www.bbc.co.uk/ontologies/storyline/2013-05-01.html.
