Web Semantics: Science, Services and Agents on the World Wide Web 29 (2014) 1930

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

journal homepage: www.elsevier.com/locate/websem

Domain-specific summarization of Life-Science e-experiments from
provenance traces
Alban Gaignard a,, Johan Montagnat a, Bernard Gibaud b, Germain Forestier c,
Tristan Glatard d,e
a Universite de Nice Sophia Antipolis / CNRS UMR7271 I3S, MODALIS team, Sophia Antipolis, France
b Universite de Rennes 1 / INSERM U1099 LTSI, Rennes, France
c Universite de Haute-Alsace, MIPS (EA 2332), Mulhouse, France
d Universite de Lyon 1 / CNRS UMR5220 / INSERM U1044 / INSA-Lyon CREATIS, Lyon, France
e McConnell Brain Imaging Centre, Montreal Neurological Institute, McGill University, Canada

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 26 July 2013
Received in revised form
6 July 2014
Accepted 6 July 2014
Available online 19 July 2014

Keywords:
E-Science
Workflows
Provenance
Linked data

Translational research in Life-Science nowadays leverages e-Science platforms to analyze and produce
huge amounts of data. With the unprecedented growth of Life-Science data repositories, identifying
relevant data for analysis becomes increasingly difficult. The instrumentation of e-Science platforms
with provenance tracking techniques provides useful information from a data analysis process design or
debugging perspective. However raw provenance traces are too massive and too generic to facilitate the
scientific interpretation of data. In this paper, we propose an integrated approach in which Life-Science
knowledge is (i) captured through domain ontologies and linked to Life-Science data analysis tools,
and (ii) propagated through rules to produced data, in order to constitute human-tractable experiment
summaries. Our approach has been implemented in the Virtual Imaging Platform (VIP) and experimental
results show the feasibility of producing few domain-specific statements which opens new data sharing
and repurposing opportunities in line with Linked Data initiatives.

 2014 Elsevier B.V. All rights reserved.

1. Life-Science data acquisition and production

Digital Life-Science data, ranging from molecular scale (e.g. proteins structural information) to human-body scale (e.g. radiological
images) and including records as diverse as biological samples, epidemiological data, and clinical information, is acquired using many
kinds of sensors. Its proper interpretation usually requires dense
information on the acquisition context, the subject studied, and
possibly the socio-economical environment of patients concerned.
Consequently, many medical data storage and communication formats tightly associate metadata with the raw data acquired, to
produce as much as possible self-contained and informative data
sets. With the generalization of digital data acquisition sensors,

 Corresponding author. Tel.: +33 492965176.

E-mail addresses: alban.gaignard@cnrs.fr (A. Gaignard),

johan.montagnat@cnrs.fr (J. Montagnat), bernard.gibaud@univ-rennes1.fr
(B. Gibaud), germain.forestier@uha.fr (G. Forestier), tristan.glatard@mcgill.ca
(T. Glatard).

http://dx.doi.org/10.1016/j.websem.2014.07.001
1570-8268/ 2014 Elsevier B.V. All rights reserved.

the standardization of data acquisition formats,1 and the online
availability of Life-Science data,2 the community has clearly turned
towards the use of standard semantic data description and manipulation technologies developed in the context of the Semantic
Web.3

To speed-up time-to-discovery in medical research, the socalled Translational Medicine movement reuses and relates information generated through uncoordinated multi-disciplinary data
acquisition procedures and stored into very large, geographically

1 Among which the Digital Image and COmmunication in Medicine (DICOM
medical.nema.org) or the Health Level Seven (HL7www.hl7.org) standards, just to
name a few.
2 Not only bioinformatics data is commonly available in public or researchoriented databases nowadays, but also international-scale biology and epidemiological data is published openly to boost research against health societal challenging
diseases such as cancer and mental disorders.
3 Especially through the use of taxonomies and ontologies among which the
Foundational Model of Anatomy (FMAhttp://sig.biostr.washington.edu/projects/
fm) or the Systematized Nomenclature of MedicineClinical Terms (SNOMED-CT
http://www.ihtsdo.org/snomed-ct), just to name a few.

A. Gaignard et al. / Web Semantics: Science, Services and Agents on the World Wide Web 29 (2014) 1930

distributed data sources (e.g. genomic and radiological data).
Annotations-aware data formats and communication standards facilitate raw data archiving at the level of each acquisition site. They
pave the way toward data search, reuse and repurposing in the
context of Linked Data [1] that underlies translational medicine, beyond the boundaries of a single discipline or community [2]. How-
ever, many different standards have emerged especially when
linking data from different sub-disciplines. Data deluge in Life Sciences is not only a matter of volume but also a matter of diversity [3,4] as both structural heterogeneity (incompatible formats)
and semantic heterogeneity (multiple terminologies and concep-
tualizations) are common.

To face the data deluge and facilitate resources sharing, scientists increasingly use e-Science platforms [5] dedicated to Life
Sciences in order to capture raw data and transform it into welldocumented data sets of interest for future exploration. Collaborative e-Science platforms are typically used to perform in-silico
experiments, share the resources involved, and produce new valuable data (e.g. to evaluate a data analysis procedure onto several
open databases, or to quantitatively compare several data analysis procedures through a common reference database). But to
enable the reuse of (and possibly to repurpose) data in future stud-
ies, it is critical for e-Science platforms to keep track of the links
between source data, produced data, and annotations associated
either to the source data or the transformation process itself. This
data provenance information facilitates data reinterpretation, data
quality assessment, data processing validation, debugging, experiment reproducibility, scientific outcomes ownership control, etc.
Platforms are nowadays commonly instrumented with provenance
data capture.

When large data sets are manipulated, the provenance capture
process generates very large annotation stores. Although provenance provides useful fine-grained and technical information on
data analysis procedures, it does not ensure a better understanding
of data produced from a scientist perspective due to (i) the size and
the fine granularity of provenance information, (ii) the reference
to technical details of the analysis pipelines, and (iii) the lack of
links with relevant domain concepts. Valuable information may be
available, yet deeply buried in the data stores. The first objective of
this work is to instrument data processing tools with domain-specific
information describing both the kind of data processed and the data
transformation process implemented (see Section 4). Based on this
captured knowledge, the second objective of this work is to analyze the dense provenance traces generated, combined with the
tools and source data annotations, to produce experiment summaries
which are both human-tractable and informative for scientists (see
Section 5).

This paper proposes a methodology, leveraging Semantic Web
technologies and standards, to instrument e-Science medical data
processing platforms in order to capture and produce knowledge
related to processed medical data. It discusses the resulting metadata deluge challenge and introduces new ways of reducing the
amount of metadata generated to tractable, scientifically informative summaries through the use of domain-oriented ontologies and
production rules. Concrete results are demonstrated through an
implementation of this methodology in the Virtual Imaging Platform4 (VIP) [6].

The remainder of this paper is organized as follows: Section 2
describes the VIP platform and exemplifies the limitations of raw
provenance usage through a concrete use case. Section 3 illustrates
the overall approach. Section 4 gives more details on how
domain knowledge can be captured and associated to e-Science
workflows and Section 5 describes how this knowledge can be

4 http://vip.creatis.insa-lyon.fr.

used to generate experiment summaries. Section 6 provides some
qualitative and quantitative experimental results. Limitations of
our approach, as well as related works are discussed in Section 7
and perspectives are drawn in Section 8.

2. Platform and scenario

2.1. The VIP simulation platform

The Virtual Imaging Platform is an e-Science platform for
medical image simulation. Medical image simulations combine
descriptions of a medical image acquisition device (physical characteristics and parameterization), an object to image (anatomical
and possibly pathological or physiological object), and a simulation scene (geometry and spatial coordinates of both the device
and the object to image). The platform is multi-modal since it integrates several simulators and predefined simulation workflows
for each modality (Computed Tomography, Magnetic Resonance,
Positron Emission Tomography, and Ultrasound), and multi-organ
since several anatomical or physiological models can be used. Simulating medical images has a variety of applications in research and
industry, including fast prototyping of new devices and the evaluation of image analysis algorithms [79].

Performing medical image simulation is challenging for several reasons. First, simulators are complex softwares with a steep
learning curve (fine parameterization, requiring a deep understanding of their physical principles) and hardly interoperable. Sec-
ond, the organ models are complex, possibly involving complex
anatomical/pathological characteristics, movement or longitudinal
follow-up. Finally, realistic simulations are compute-intensive, and
thus require dedicated computing infrastructures. VIP relies on the
European Grid Infrastructure (EGI)5 to support its computing and
storage needs. Between October 2012 and January 2014, 6723 simulations were run, which corresponds to more than 700 CPU years,
for more than 380 users originating from 40 countries.

VIP massively produces simulated data. Handling provenance in
VIP is crucial to face the coherent sharing of (i) input organ models,
(ii) simulator themselves, (iii) simulated data and their associated
knowledge. VIP faces the issues of producing not only raw data, but
also populating its simulated data repository with meaningful data.
It thus needs to bridge the gap between provenance in technical
simulation workflows and domain knowledge formalized with the
OntoVIP domain ontology [10,11] (see Section 4.1).

2.2. Usage scenario

VIP simulators are complex and they are described as multistep workflows to facilitate their parallelization. The enactment
of medical imaging simulation workflows produces large amounts
of data. Some is only intermediate data, whereas the resulting
simulated data is useful for end-users. The usage scenario proposed
here tracks provenance in Sorteo [12], one of the VIP simulation
workflows, in order to address:
 a technical concern, allowing for workflow designers and experiment operators to more easily determine the cause of failure
or abnormalities; and
 a reliability concern, making scientists more confident in the
data produced through their experiments since the reproducibility of simulation experiments is made easier and data
lineage can be controlled.

5 EGI, www.egi.eu, is a distributed multi-sciences computing platform federating
hundreds of thousands of CPU cores distributed in hundreds of computing centres
all over Europe and beyond.

entities. Depending on workflow parameters such as the size of
the virtual medical image and the number of jobs used to compute
the singles of the Monte-Carlo simulation, a simulation workflow
execution may generate a huge amount of intermediate data files
(one PET emission file per Monte-Carlo job computing singles).
Finally, the simulation workflow produces a single reconstructed
file (the sinogram) based on all intermediate PET emissions.
Depending on their goals, users have different interests for the
data sets produced. Inspecting intermediate data such as PET
emission may have an interest when debugging the simulation
process, but these files may probably be ignored in other scenarios.
Provenance information capture. We consider in this scenario a
provenance-instrumented workflow engine able to trace all finegrained simulation activities. Provenance information is actually
represented in an RDF graph and relies on the OPM ontology [13].
OPM represents causal dependencies between things through
directed graphs. A Causal dependency is defined as a directed relationship between an effect (the source of the edge) and a cause
(the destination of the edge). A node of the provenance graph
might either be an Artifact (immutable, stateless element), a Process (action performed on Artifacts and producing new ones), or
an Agent (entity controlling or affecting the execution of a Pro-
cess). Graph edges represent (i) dependencies between artifacts
(wasDerivedFrom) to track data lineage, (ii) dependencies between
two processes (wasTriggeredBy) to track the sequence of pro-
cesses, and (iii) dependencies between artifacts and processes
(used/wasGeneratedBy) to track artifacts production and consumption through processes. In addition, OPM tracks the links between
processes and their enactor agents through wasControlledBy de-
pendencies.

As an example, Listing 1 illustrates the main provenance statements describing the execution of the last processing step of the
workflow. It traces the execution of the Lmf2RawSino process. An
instance of the Process class is created with the http://vip.cosinus.
anr.fr/run-LMF2RAWSINO-1 URI, constructed from a prefix, the
name of the workflow processor and a uniform unique identifier (UUID). This process execution is attached to an OPM Account,
which represents the overall workflow execution. Note that all
OPM Artifacts and Processes registered through a single workflow
execution are also attached to an OPM Account.
<http://vip.cosinus.anr.fr/runLMF2RAWSINO1>

a opmv:Process ;
opmo:account <http://vip.cosinus.anr.fr/workflow1> .
Listing 1: OPM statements describing the Lmf2RawSino process execution.

Listing 2 illustrates the causal data production dependency
registered between the previous Lmf2RawSino process execution
and the output sinogram. This dependency is represented by
an instance of the WasGeneratedBy OPM class and is identified
similarly to processes. This instance is linked to both the process
execution through the cause OPM property, and the Artifact
describing the output sinogram through the effect OPM property. In
addition, the process input or output ports are described through
the role OPM property linking together the data dependency and
an instance of the OPM Role class which corresponds to the label
of the process input or output port. Finally, the data production is
timestamped through the OPM time property towards an instance
of the OPM OTime class.
<http://vip.cosinus.anr.fr/wgb1>

a opmo:WasGeneratedBy ;
opmo:account <http://vip.cosinus.anr.fr/workflow1> ;
opmo:cause <http://vip.cosinus.anr.fr/runLMF2RAWSINO1> ;
opmo:effect <http://vip.cosinus.anr.fr/artifact1> ;
opmo:role <http://vip.cosinus.anr.fr/role1> ;
opmo:time <http://vip.cosinus.anr.fr/time1> .

Listing 2: OPM statements describing the WasGeneratedBy dependency between
the output sinogram and the Lmf2RawSino process.

Fig. 1. Graphical representation of the Sorteo PET medical image simulation
workflow. (For interpretation of the references to color in this figure legend, the
reader is referred to the web version of this article.)

This scenario shows that raw provenance traces can hardly be
exploited by end-users since their technicality, their size and the
lack of semantics hamper the interpretation of produced data from
the scientist perspective.

Sorteo is a Monte Carlo-based medical image simulator dedicated to the production of synthetic Positron Emission Tomography (PET) data. PET is a functional imaging modality, used in
the field of nuclear medicine, that shows in-vivo quantitative
metabolic activities. A simplified version of the Sorteo simulation workflow is presented in Fig. 1. Blue boxes represent either
compute-intensive activities whose executions are relocated on
the EGI grid or lightweight activities executed locally. Green ellipses represent input or output data. Intermediate data sets produced by each processing step are not represented explicitly in this
graph but the corresponding data flow is shown as black arrows
linking computational processes.

The main workflow inputs are the protocol, storing all simulation parameters, and the phantom representing the object model
to be virtually imaged. The Sorteo simulation workflow produces a
single output, a sinogram, representing the simulated PET data. The
core of the simulation consists in two steps:
(i) the parallel computation of singles through the sorteoSingles

(ii) the parallel computation of the emissions through the sorte-

activity; and

oEmission activity.

In each execution of the Sorteo workflow, these two activities are
instantiated concurrently several times, depending on the size of
the simulation. The remaining activities can be considered either
as pre- or post-processing steps, needed to assemble simulation
parameters, or to convert data throughout the simulation work-
flow.
Produced data. In a single workflow execution and for a fixed set
of parameters, this Sorteo simulator generates more than 150 data

A. Gaignard et al. / Web Semantics: Science, Services and Agents on the World Wide Web 29 (2014) 1930

Finally listing 3 describes the OPM Artifact corresponding to
the output sinogram of the Sorteo PET simulation workflow. An
Artifact instance is created. It has already been attached to the
WasGeneratedBy causal dependency through the effect property
of the previous listing. An Artifact is an abstract entity and OPM
allows for associating their concrete values. The Artifact is thus
linked to an instance of the AValue OPM class through the avalue
property. Finally, a content is associated to the value through the
OPM content property. This content finally gives the logical file
name (LFN) of the sinogram, a URI locating the data on the EGI
grid infrastructure. Data might be later on downloaded through a
dedicated data transfer interface.
<http://vip.cosinus.anr.fr/artifact1>

a opmv:Artifact ;
opmo:account <http://vip.cosinus.anr.fr/workflow1> ;
opmo:avalue <http://vip.cosinus.anr.fr/value1> .

<http://vip.cosinus.anr.fr/value1>

a opmo:AValue ;
opmo:account <http://vip.cosinus.anr.fr/workflow1> ;
opmo:content "lfn://lfcbiomed.in2p3.fr/grid/biomed/creatis/vip/data
/users/rafael_silva/sorteo2/2401{2012}_{1}0:13:30
/dataLMF.ccs.sino"^^<http://www.w3.org/2001/XMLSchema#anyURI> .

Listing 3: OPM statements describing the sinogram produced as an output of the
Lmf2RawSino process.

The use of the OPM ontology leads to verbose provenance
annotations. Indeed, more than 14 RDF statements (timestamping
has not been represented) are necessary to represent a single
data item production in the Sorteo workflow. This is mainly due
to the reification of all dependencies, leading to complex paths
between provenance entities (we consider here only a single data
production).

Finally, OPM statements illustrated above represent technical information such as the location of produced files in a distributed computing infrastructure, the name of the processing
tools involved in simulation experiments, or time-stamping. They
represent precise and fine-grained information, beneficial when
inspecting logs of medical imaging simulations; however, they do
not convey any domain-specific information such as simulation
modality or high-level parameters, useful for medical imaging ex-
perts.
Needs for concise and domain-specific provenance. Although precise provenance statements are definitely necessary for technical
workflow refinement or debugging, the size, the fine granularity
of provenance and its lack of links with domain-specific concepts
makes it unmanageable from a scientist perspective, possibly running workflows on large input datasets. As an example, a single run
of the Sorteo workflow leads to a large OPM technical provenance
graph composed of 4523 nodes and 15 154 edges.

To address this issue, we propose to distinguish between
two levels of provenance information. First, fine-grained domainagnostic provenance (represented through standards provenance
models such as OPM), useful for technical workflow refinement
or debugging. Second, coarse-grained domain-specific provenance,
representing concise domain-specific statements resulting from
production rules relying on the VIP medical image simulation on-
tology. These produced statements will finally constitute semantic experiment summaries in which a minimal set of statements
link together simulation experiment results to experiment parameters through the OntoVIP [10,11] domain ontology.

3. Global approach

Our approach is based (i) on knowledge capture, i.e. data and
services semantically annotated with a domain ontology (see Section 4), and (ii) on knowledge production, by applying production

Fig. 2. Knowledge capture and production to produce semantic experiments
summaries from medical imaging workflow executions.

rules to annotate the processed data with new concise domainspecific statements finally assembled into semantic experiment
summaries (see Section 5).

Fig. 2 illustrates the proposed approach. The left part of the
figure focuses on raw data: organ model and medical image
simulator selection, simulator parameterization and execution,
and simulated data production.

The right part of the figure focuses on semantic data and illustrates our approach to produce semantic experiment summaries.
 First we rely on (i) semantically annotated input data (organ
models), and (ii) semantically annotated services actually
composed into simulation workflows .
 Then, domain-agnostic provenance  is tracked on the fly at
workflow runtime and represented through a standard model
(OPM in the current implementation).
 When the workflow successfully produced a simulated data, the
set of available domain-specific production rules  are applied.
Each applicable rule involves semantic service annotations, and
produces new domain-specific statements.
 These new statements finally constitute the semantic experiment summaries and populate a dedicated catalog .
The joint querying of the catalogs for organ models, simulators,
and experiment summaries, published in the platform following
Linked Data principles, opens interesting perspectives in terms of
simulated data and organ models sharing and reuse.

4. Knowledge capture in e-Science workflows

To propagate knowledge from domain ontologies to data produced through e-Science workflow executions, concepts defined
through a domain ontology must be associated to data processing services syntactical elements. If we consider the last processing
step of the Sorteo medical image simulation workflow (Fig. 1) for
example, it consumes two input parameters that share the same
syntactic type. The first parameter is typed with a URI representing the imaging protocol file path; the second parameter is also

typed with a URI which represents the path of the directory containing all generated emissions to be effectively reconstructed by
Lmf2RawSino. These syntactic types do not precisely characterize input data. To have a clear understanding of the transformation
realized on input and the output data, both the service itself and
its parameter should refer to concepts of a domain ontology.

We rely on the OntoVIP [10,11] ontology (Section 4.1) to model
the medical image simulation domain and the OWL-S [14] generic
service ontology (Section 4.2) to semantically annotate services
composed into workflows. We also highlight the issue of ambiguous semantic service annotations and propose in this knowledge
capture process, to pay a particular attention in distinguishing Role
and Natural concepts when annotating service parameters (Sec-
tion 4.3).

4.1. Overview of the OntoVIP medical image simulation ontology

OntoVIP was developed to facilitate the sharing and automated
processing of information managed within the VIP Virtual Imaging
Platform. OntoVIP provides a coherent conceptual framework,
grounded on the DOLCE (Descriptive Ontology for Language and
Cognitive Engineering) foundational ontology [15]. The ontology
was built through intensive interviews with researchers involved
in image simulation. It took almost a year to reach a consensual
modeling and several
incremental versions were produced.
OntoVIP is publicly available through the BioPortal.6

It includes medical information object models (called for short
Object models in the following) whose sharing and reuse are essential since such models are hard to build from scratch and can
often be easily derived from existing ones. This part of the ontology
involves a taxonomy of object models, highlighting their content:
e.g. geometrical phantom object model or biological object model,
whether they contain some external agent (Object model with external agent) or some foreign body (Object model with foreign body),
their compatibility with simulators (i.e. whether they specify the
physical properties of objects required with a particular class of
simulator, e.g. CT7 simulation compatible model), whether they are
static (Static object model) or dynamic, i.e. modeling a moving object (Moving object model) or an object undergoing some evolution
in time (Longitudinal follow up object model). This taxonomy is complemented by entities describing the content of the object models geometry files (e.g. 3D voxel matrices or meshes) to relate them
to classes of real-world objects (e.g. Anatomical object, Pathological objects, Foreign body objects). The latter classes were extracted
from existing ontologies such as FMA [16] (Foundational Model of
Anatomy), RadLex [17] (Radiology Lexicon), MPATH [18] (Mouse
pathology).

OntoVIP also includes a detailed taxonomy of simulated data, i.e.
data resulting of the execution of some medical image simulation
software. This taxonomy involves three major semantic axes.
The first is related to imaging modality (e.g. CT simulated data,
MR8 simulated data); the second makes a distinction between
non-reconstructed data and reconstructed data (i.e. images); the
former are further categorized into classes denoting the spatial
or spatiotemporal organization of the data (e.g., list-mode data,
sinogram, set of signals, set of projection images); and finally the
third distinguished between static simulated data and dynamic
simulated data.

Simulated data are the result of the execution of some medical
image simulator, i.e. software whose function is to perform medical image simulation. Medical image simulators and medical image

simulations are further categorized depending on imaging modalities (i.e. CT, MR, US,9 PET10). Medical image simulators are composed of simulator components addressing the different stages of a
simulation: pre-processing (implemented by pre-processing simulator component), core simulation (implemented by core simulation simulator component) and post-processing (implemented
by post-processing simulator component). OntoVIP models the relationships between simulated data and object models, and between simulated data and parameter sets or parameters; such
relationships (derivedFromModel, derivedFromParameterSet, de-
rivedFromParameter, respectively) are of key importance with regard to the domain-specific modeling of data lineage.

OntoVIP was developed based on OntoNeuroLOG, an ontology
developed during the NeuroLOG project11 for supporting the sharing of heterogeneous and distributed medical images and image
processing tools in neuroimaging [19,20]. The OntoVIP ontology is
used in the VIP software to support the annotation and querying of
models, as well as the annotation and querying of simulated data
and of the data processing actions that actually produced this data.

4.2. Semantic service annotation

To complete the Knowledge Capture task on medical image simulation workflows, Semantic Services associate concepts of the OntoVIP ontology to the service descriptors composing simulation
workflows. The field of Semantic Web Services aims at exploiting
semantic web technologies to enhance service oriented architectures and thus e-Science workflow environments. Through a rich,
formal and standard semantic description, benefits are expected
both at workflow design-time, when discovering, composing and
mediating services, and at workflow run-time, when linking back
processed data to semantic service annotations. Our approach focuses on the latter to produce human-tractable and informative
enough semantic experiment summaries.

Several initiatives have been targeting the standardization of
semantic service description such as, for extended frameworks,
OWL-S [14], WSMO [21], FLOWS [22], or for lighter approaches,
SAWSDL [23] or WSMO-Lite [24]. Although SAWSDL has been
proposed by the W3C as a recommendation in 2007, no consensus clearly emerged, and OWL-S and SAWSDL provide good
compromises for semantically annotating e-Science workflow
components.

We reused the OWL-S Profile ontology concepts to describe
semantically the key processing steps of medical image simulation workflows, in terms of functionality, input and output pa-
rameters. These descriptions have been bridged to the OntoVIP
domain ontology through refers-to properties. As an example,
the Lmf2RawSino refers to the image-reconstruction-simulator-
component OntoVIP class to describe its functionality and refers to
the PET-Sinogram class to describe the produced data through its
output parameter.

4.3. Role concepts in semantic service descriptions

We showed in [25] that only considering the intrinsic Nature
of parameters would possibly lead to ambiguous semantic service
annotations.

When exploiting a workflow execution in terms of provenance
information, it may not be possible to identify a unique path in
the data production chain, due to some parameters of a particular

6 OntoVIP: http://bioportal.bioontology.org/ontologies/3253.
7 X-ray Computed Tomography.
8 Magnetic Resonance.

9 Ultrasound.
10 Positron emission tomography.
11 NeuroLOG project: http://neurolog.unice.fr.

A. Gaignard et al. / Web Semantics: Science, Services and Agents on the World Wide Web 29 (2014) 1930

processing step, sharing the same Nature. For example, two input
parameters may share the same Nature (e.g. Magnetic Resonance
modality) but be considered differently from a data processing
perspective. The first input parameter may be considered as a
reference data, and the second one may be considered as the data to
be analyzed or transformed. More generally, data can play different
roles in the context of a data processing tool.

Without this contextual knowledge specifying how data are
related, through one or more parameters, to a specific data
processing step, it is difficult to deduce domain-specific information from the workflow executions and their associated prove-
nance. Few approaches, such as FLOWS fluents [22] or BioCatalogue
functions [26], may be used to make the distinction between the
nature of service parameters and their role from the service per-
spective. We also pay a particular attention in making the distinction between Role and Nature concepts at domain ontology design
time. Roles can then be used, to disambiguate semantic service descriptions finally enabling reasoning and the production of new
domain-specific statements from workflow executions.

5. Producing semantic experiment summaries from e-Science
workflow runs

Based on disambiguated semantic services and provenance
traces, reusable production rules instrumenting domain ontologies
enable the production of new domain statements. Due to computeintensive tasks, a single workflow execution may lead to a huge
amount of fine-grained provenance information, as explained in
Section 2.

Section 5.1 first introduces the OPM-O domain-agnostic provenance ontology. Section 5.2 then proposes to use domain-specific
production rules (through SPARQL Construct queries) to propagate domain knowledge, from raw provenance traces, to the
processed data through new domain-specific statements, finally
assembled into semantic experiment summaries.

5.1. Domain-agnostic provenance ontologies

The Open Provenance Model [13] initiative (OPM) is a community effort aiming at homogenizing the expression of provenance information on the wealth of data produced by e-Science
applications. OPM broadly addresses workflow environment interoperability through a standardized representation and easier exchanges of provenance information. It also eases the development
of tools to process such provenance information, and finally facilitates the reproducibility of e-Science experiments.

Provenance ontologies are crucial initiatives helping in precisely tracking provenance information, which open interesting interoperability and reproducibility perspectives, in the context of
e-Science applications. However, these standardization initiatives
do not consider any specific domain. When presenting such provenance information to e-Scientist, we face two main issues:
 Granularity: e-Scientists are often not aware of all the constituents of a particular workflow and they generally consider
workflows as gray-boxes in which only part of the produced
data is of interest. Systematic provenance tracking and representation through domain-agnostic provenance ontologies
leads to large fine-grained bunch of information, hampering the
interpretation of workflow results.
 Abstraction: e-Scientists are nowadays used to attach precise
meaning (through domain ontology) to their data or processing
tools, to enhance their representation and sharing. However,
standard provenance ontologies are domain-agnostic. Domainagnostic provenance ontologies are not sufficient to properly
interpret and share processed data. This requires, in addition,
leveraging a domain-oriented ontology.

To tackle these issues, we propose to design domain-specific
production rules. They address (i) the automated semantic annotation of generated raw data, and (ii) the semantic summarization of
e-Science experiments through few domain-specific statements.

5.2. Reusable and service independent rules to produce new concise
domain-specific statements

New domain-specific statements are produced from e-Science
workflow executions using (i) domain ontologies, (ii) domainagnostic provenance information, and (iii) domain-specific production rules. SPARQL is the standard language dedicated to
Semantic Web graphs querying. Although Select is its most popular query form, for graph data selection, Construct queries allow for producing new RDF statements when a graph pattern is
matched. It can thus be considered as a production rule composed
of an antecedent (an If  condition), its Where clause, and a consequent (a Then consequence), the Construct clause.

As a detailed example, we propose the production rule illustrated in Listing 4. Its Where clause identifies a sub-graph into
the full OPM-O provenance statements while its Construct clause
produces new domain-specific statements describing, in a concise form, the whole simulation experiment. More precisely, this
Construct query augments the initial RDF graph with new triples
leveraging the OntoVIP ontology. They state (i) the nature (type)
and the location (is-stored-in-file) of the input parameters and output data, (ii) the nature and relations of the produced medical images with respect to the input organ model and the simulation
workflow (derives-from-model, is-a-result-of-at, etc.), and (iii) the
nature and global parameters of the simulation workflow (uses-
as-model-in-simulation, etc.). Being concise and domain-specific,
these statements semantically annotate the produced raw data and
helps e-scientists interpret data produced along their experimental campaigns and link the simulation parameters and components
to the produced data.

PREFIX rdf: <http://www.w3.org/1999/02/22rdfsyntaxns#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdfschema#>
PREFIX opmo: <http://openprovenance.org/model/opmo#>
PREFIX opmv: <http://purl.org/net/opmv/ns#>
PREFIX ws: <http://www.irisa.fr/visages/team/farooq/ontologies/webserviceowllite.owl#>
PREFIX iec: <http://www.irisa.fr/visages/team/farooq/ontologies/iecowllite.owl#>
PREFIX vipmodel: <http://vip.cosinus.anr.fr/vipmodel.owl#>
PREFIX vipsimulation: <http://vip.cosinus.anr.fr/vipsimulation.owl#>
PREFIX vipsimulateddata: <http://vip.cosinus.anr.fr/vipsimulateddata.owl#>

CONSTRUCT {
?inPhantom rdf:type vipmodel:medicalimagesimulationobjectmodel
?inPhantom vipmodel:isstoredinfile ?cInPhantom
?inProtocole rdf:type vipsimulation:simulationparameterset
?inProtocole vipmodel:isstoredinfile ?cInProtocole
?out vipmodel:derivesfrommodel ?inPhantom
?out vipsimulation:derivesfromparameterset ?inProtocole
?out rdf:type vipsimulateddata:PETsinogram
?out vipmodel:isstoredinfile ?cOut
?out vipsimulation:isaresultofat ?wf
?wf rdf:type vipsimulation:PETsimulation
?wf vipsimulation:usesasmodelinsimulation ?inPhantom
?wf vipsimulation:usesasparameterinsimulation ?inProtocole
} WHERE {
?agent (iec:refersto/rdf:type)

<http://vip.cosinus.anr.vip.fr/vipsimulation.owl#imagereconstructionsimulatorcomponent> .

?wcb opmo:cause ?agent .
?wcb opmo:effect ?x .
?x rdf:type opmv:Process .
?wgb opmo:cause ?x .
?wgb opmo:effect ?out .
?agent2 (iec:refersto/rdf:type)

<http://vip.cosinus.anr.vip.fr/vipsimulation.owl#parametersgenerationsimulatorcomponent> .

?wcb2 opmo:cause ?agent2 .
?wcb2 opmo:effect ?y .
?y rdf:type opmv:Process .

?used1 opmo:cause ?inPhantom .
?used1 opmo:effect ?y .

?used2 opmo:cause ?inProtocole .
?used2 opmo:effect ?y .

?used1 opmo:role/rdfs:label ?techRolePhantom .
?used2 opmo:role/rdfs:label ?techRoleProtocole .
?agent2 ws:hasinput ?inPortPhantom .
?inPortPhantom (iec:refersto/rdf:type)

<http://vip.cosinus.anr.fr/vipmodel.owl#geometricalphantomobjectmodel> .

?inPortPhantom rdfs:comment ?techRolePhantom .
?agent2 ws:hasinput ?inPortProtocole .
?inPortProtocole (iec:refersto/rdf:type)

<http://vip.cosinus.anr.fr/vipmodel.owl#qualityproceduredataset> .

?inPortProtocole rdfs:comment ?techRoleProtocole .

?x opmo:account ?wf .

?inPhantom opmo:avalue ?vInPhantom .
?vInPhantom opmo:content ?cInPhantom .

?inProtocole opmo:avalue ?vInProtocole .
?vInProtocole opmo:content ?cInProtocole .

?out opmo:avalue ?vOut .
?vOut opmo:content ?cOut .

Listing 4: Production rule based on a SPARQL CONSTRUCT query to associate the
input phantom to the produced output sinogram resulting from an execution of the
Sorteo simulation workflow.

 Lines 1327 represent the new statements resulting from the
application of this rule. They only involve domain-specific entities (vip-model, vip-simulation and vip-simulated-data prefixes
of the OntoVIP ontology) whereas the Where clause of the
query only involves OPM-O entities. These new statements semantically describe the nature of input parameters (medical-
image-simulation-object-model, simulation-parameter-set) and
simulated output medical images (PET-sinogram). In addition,
these new annotations represent the nature of the simulation
(i.e. Positron Emission Tomography) through the use of the PETsimulation class, and the relation between the medical object
imaged and the produced simulated image (derives-from-model
property). They are particularly useful because they involve
medical imaging concepts and relations forming part of the OntoVIP ontology.
 Lines 2933 identify a process execution, its corresponding service description through an ?agent instance, and the achieved
class of action through the iec:refers-to property. In this rule, the
class of action is an image reconstruction.
 Lines 3435 identify the output Artifact (?out) through an instance of the WasGeneratedBy causal dependency (?wgb). This
dependency is linked to the Process (?x) through an opmo:cause
property and to the output Artifact through an opmo:effect
property. The value and content are associated to the Artifact
through the opmo:avalue and the opmo:content properties (lines
7071).
 Lines 3741 identify a process execution realizing a parameters
generation action, similarly to lines 2933.
 Lines 4360 identify the Artifacts used as input of a process execution realizing a parameters generation action. Additionally,
the ?role characterizing how the Artifact has been used by the
process is identified. It identifies the parameters in the semantic
service description associated to the process (line 49 and 50).
 Lines 5260 finally join the service description of (?agent2) to
the process execution (?y) through the label associated to the
input port (?role), this input port referring to a geometrical
phantom (lines 5355).
Due to the fine granularity of the OPM-O provenance ontology,
the graph pattern to be matched is large. Developing this kind
of production rules is time-consuming and error-prone; it is thus
important to foster the reusability of such rules.

By relying on domain specific taxonomies to describe services
we enhance the rule reusability. As an example, if we consider a

new version of the Sorteo workflow where the last process has
been updated to Lmf2RawSino_v2, the same production rule can
be reused. Indeed, we consider that its implementation is completely different (technical parameters may have changed) but
its functionality is still the same. Since the semantic description
of Lmf2RawSino_v2 is subsumed by the semantic description of
Lmf2RawSino, and the production rule involves semantic description of Lmf2RawSino, the same production rule can successfully be
applied to also annotate the results of Lmf2RawSino_v2.

Through the use of (i) fine-grained technical provenance and
(ii) domain-specific production rules, we presented a method exploiting domain ontologies, not only at workflow design-time, but
also at workflow run-time. Our method propagates domain knowledge on processed data to finally constitute semantic experiment
summaries. In the following section, we propose experimental results showing the interest of generating few domain-specific state-
ments, to enhance workflow results interpretation, especially in
the context of Linked Data.

6. Results

6.1. Experimental setup

The VIP simulation platform hosts a semantic catalog of organ
models which associates the set of raw source files with the set
of semantic annotations describing each model. It leverages the
OntoVIP medical image simulation ontology and enables advanced
querying on available models. VIP consumes organ models through
the MOTEUR data-intensive workflow manager [27] coupled to
the European Grid distributed computing Infrastructure (EGI12)
to produce simulated data. It keeps records of all running and
completed simulations, enabling simulated data search, postanalysis and reuse. More details on the VIP platform are available
in [6].

Through the work presented in this paper, simulated data entities are annotated with OntoVIP-based semantic information linking them to (i) an input organ model, (ii) an input parameter sets,
and (iii) a brief description of the overall simulation workflow. To
achieve this result, the VIP platform was instrumented with:
 a semantic catalog of composable simulator components;
 a fine-grained OPM provenance traces generation plugin for the
MOTEUR workflow manager to capture on-the-fly provenance
information;
 domain-specific production rules (SPARQL Construct) aiming
at automating the semantic annotation and the summarization
of medical image simulation workflows for 4 modalities (Com-
puted Tomography (CT), Magnetic Resonance (MR), Ultrasound
(US) and Positron Emission Tomography (PET)); and
 a graphical viewer for the resulting experiment summaries
providing a tabular representation of the simulated data catalog
and their relations with input organ models and medical image
simulators.

Finally, both the catalog of simulation services (describing the
function of data processing steps and the nature and the role of
their parameters) and the catalog of organ models (describing for
example anatomical knowledge) are used with domain-agnostic
provenance traces to populate the simulated data catalog with new
experiment summary statements as illustrated in Fig. 2.

Technically, the semantic repository and reasoner are built
upon open source libraries such as Apache JENA for RDF data
persistency, Corese/KGRAM13 for Semantic Web querying and

12 EGI: http://egi.eu.
13 Corese/KGRAM: http://wimmics.inria.fr/corese.

A. Gaignard et al. / Web Semantics: Science, Services and Agents on the World Wide Web 29 (2014) 1930

Fig. 3. New produced statements (dashed arrows) constituting the semantic experiment summary. (For interpretation of the references to color in this figure legend, the
reader is referred to the web version of this article.)

reasoning, Apache Commons and Log4J for helper classes, and JSPF
for a simple java plugin framework.

Two experiments are proposed below to assess the scalability
of our approach when producing semantic experiment summaries,
and to show the usability of these summaries, especially in the
context of Linked Open Data.

6.2. Semantic experiment summaries for scalable e-Science data
annotation

New statements resulting from production rules provide highlevel and concise semantic experiment summaries. We consider
experiment summaries as high-level descriptions since they only
involve domain-specific classes and properties defined in the
OntoVIP ontology, compared to the generic and technical entities
provided by the OPM provenance ontology. We also consider them
concise since for Sorteo, only 12 statements might be produced,
compared to thousands of OPM statements produced through our
provenance-instrumented workflow engine.

Fig. 3 illustrates the experiment summary resulting from the
execution of the Sorteo medical imaging workflow. Green ellipses
represent input or output data, the blue ellipse represents the
Sorteo workflow shown as a black box, and red rectangles represent VIP ontology classes. The production rule presented in Listing 4 automates the semantic annotation of the output sinogram
and the corresponding input phantom and input protocol. Dashed
green arrows represent the new inferred statements. For instance,
the output sinogram is related to its corresponding input phantom
through the vip-model:derives-from-model property (Listing 4, line
19). The nature of the sinogram is also determined through the is-a
property towards the VIP class PET-Sinogram (Listing 4, line 21).
Scalability. Since technical fine-grained OPM provenance information is useful at workflow design-time and workflow debug-time,
it is temporarily stored in a short-term semantic repository. To produce new domain-specific statements, only the provenance information related to a single execution, and the service descriptions
are needed. We finally store in a long-term repository the few experiment summary statements.

When running the Sorteo medical imaging workflow, only 12
RDF triples are produced as experiment summary when more than
1400 OPM-O RDF triples are recorded through the provenanceinstrumented workflow engine. Between December 2012 and June
2013, 136 medical image simulations have been summarized.
These summaries consists in 3114 domain-specific RDF triples.
They represent only 3.5% of the size of the corresponding full
OPM-O provenance graphs (87 587 triples). As an illustration, Fig. 4
gives a visual idea on the content of the VIP long-term repository
storing the experiment summaries. It shows that the VIP platform
has been mainly used, during this period, for CT, MR and US
simulation (lot of instances for the CT-simulation, MR-simulation,
and US-simulation classes). This kind of graphical representation
also shows that a single organ model (organes.pegs4dat) has
been significantly used as the input model for CT simulations. To
extract more precise information with respect to the VIP platform
usage, it is still possible to perform quantitative SPARQL queries
on the RDF experiment summaries. As an example, SPARQL count
queries involving OntoVIP domain-specific entities show that the
VIP long-term repository is composed of 3114 summary triples
and represents 39 US simulations, 31 CT simulations, 64 MR
simulations, and 3 PET simulations. Another quantitative query
shows that the organes.pegs4dat organ model has been used
in all the 31 CT simulations, which represents 22% of the overall
simulations. It shows that this organ model has been intensively
used in the VIP platform to perform CT simulations.
Performance. In terms of performance, both the capture of raw
provenance and the production of experiment summaries are negligible compared to the processing time of raw data on a dedicated
computing infrastructure. Over 233 simulation workflow runs, we
measured a mean summary production time of 2 s with a standard
deviation of 1.1 s. The mean ratio of summary production time over
workflow execution time is 0.76%.

We pay special attention to scalable data production through
(i) the materialization of few produced statements into a longterm simulated data catalog, and (ii) short-term fine-grained
OPM provenance datasets stored independently and available for
workflow design and debug concerns.

Fig. 4. Visual content of the long-term simulated data catalog showing three main groups of medical image simulations.

6.3. Semantic experiment summaries exploitation

6.3.1. Linked Data querying

Based on provenance information, our approach automates the
annotation of e-Science workflow results with domain-specific
concepts, finally assembled, following Link Data [1] principles, into
semantic experiment summaries. These summaries can be combined with external data sources such as FMA [16], the Foundational Model of Anatomy. We exemplify in Listing 5 a SPARQL query
leveraging three kinds of interlinked data, VIP simulated data, VIP
organ models, and FMA anatomical concepts. This query aims at retrieving simulated data from VIP organ models which contain brain
white matter, as defined in the FMA ontology.
PREFIX mo: <http://vip.cosinus.anr.fr/vipmodel.owl#>
PREFIX partic: <http://www.irisa.fr/visages/team/farooq/ontologies/particularowllite.
PREFIX iec: <http://www.irisa.fr/visages/team/farooq/ontologies/iecowllite.owl#>
PREFIX fma: <http://sig.uw.edu/fma#>

owl#>

SELECT ?dataFile ?dataClass ?anat WHERE {
?organModel rdf:type mo:medicalimagesimulationobjectmodel .
?organModel partic:hasforproperpartduring ?x .
?x rdf:type mo:anatomicalobjectlayer .
?x partic:hasforproperpartduring ?y .
?y rdf:type mo:anatomicalobjectlayerpart .
?y iec:refersto ?anat .

?anat rdf:type fma:White_matter_of_neuraxis .
?dataArtifact mo:derivesfrommodel ?model .
?dataArtifact mo:isstoredinfile ?dataFile .
?dataArtifact rdf:type ?dataClass .

FILTER (CONTAINS(?organModel,?model))

Listing 5: SPARQL query exploiting both the VIP organ model catalog and the newly
populated simulated data catalog and FMA concepts.

The first six triple patterns appearing in the WHERE clause
aim at searching for VIP organ models (medical-image-simulation-
object-model) and their anatomical constituents (anatomical-
object-layer and anatomical-object-layer-part).
Reference anatomical concepts are retrieved through the refersto property and the ?anat variable. Then, the next triple pattern
specifies the FMA anatomical concept to be matched: brain white
matter (White_mater_of_neuraxis). Finally, the last three triple
patterns aim at retrieving, from the simulated data catalog, data
files (is-stored-in-file) and their associated medical image modality
simulated from organ models (derives-from-model) including
white matter.

6.3.2. Simulated data catalog

One of the main objectives of the VIP platform is to ease the
setup of medical image simulation experiments. The VIP webbased graphical user interface hides the complexity of the underlying simulation workflows and the distributed data management.
In this context, the proposed semantic experiment summaries
have been directly exploited, through SPARQL queries, to populate
a catalog of simulated data. This simulated data catalog finally
helps e-Scientists in searching or retrieving simulated medical
images, based on their modality, on the organ models used in
the simulation, or on the simulation parameters. In this catalog,
simulated data are linked to the simulations used to produce
them, so that users can retrieve the exact parameters and logs on
request.

7. Discussion

7.1. Related works

With regard to generic provenance ontology standardization,
OPM recently made a step further. It evolved through a W3C standardization process towards the PROV- specifications.14 PROV-
O [28] is an OWL specification of the W3C provenance data model
(PROV-DM). Evolving from basic OPM-O provenance representation to PROV-O is almost direct. There is a mapping between the
root classes: Artifact  Entity, Process Activity, Used Usage,
or WasGeneratedBy Generation. A noticeable enhancement is the
definition of simple properties for usage and generation causal de-
pendencies. Whereas these dependencies must be reified with
OPM-O, leading for instance to two triples which link a process
instance to an artifact through an intermediate instance of the
opmo:WasGeneratedBy class,15 only a single triple is needed with
PROV-O (the instantiation of the dependency is not required any-
more).

In addition, PROV-O extends OPM-O with some classes and
properties especially useful in the context of e-Science workflows.
For instance, PROV-O introduces the notion of Plan to describe
the context of execution of an Activity, which can be seen as a
set of instructions, as a recipe, or a workflow. Another interesting

14 http://www.w3.org/TR/prov-primer.
15 See the example of Listing 2.

CT simulationsMR simulationsUS simulations28

A. Gaignard et al. / Web Semantics: Science, Services and Agents on the World Wide Web 29 (2014) 1930

extension is the alternateOf property aiming at representing
several aspects of the same thing. For instance, in medical imaging,
it would be well adapted to link several datasets resulting from
data conversion tools.

Madougou and coworkers propose in [29] a provenance-based
approach aimed at analyzing the e-BioInfra e-Science platform
usage and identifying the causes of application failures. From a
post mortem analysis of the MOTEUR workflow enactor logs, the
proposed system populates a relational SQL backend with OPM
provenance statements, queried through HQL (Hibernate Query
Language). The natural graph representation of provenance is
buried into a relational representation, and the system cannot
benefit from graph-based querying languages such as SPARQL.
The system addresses the technical characterization of workflow
executions through a statistical analysis of fine-grained domainagnostic provenance.

We rather focus on result interpretation from the e-scientist
perspective by leveraging domain-specific ontologies and preserving the underlying graph structure of provenance, thus allowing for
graph-based querying and reasoning through Semantic Web tech-
nologies.

The Wings/Pegasus environment [30] addresses through semantic reasoning on application-level constraints, the generation of valid and execution-independent workflows, to be enacted
over distributed computing infrastructures. The system proposed
is able to produce both application-level and execution prove-
nance. Wings/Pegasus uses a proper OWL ontology to model
application-level provenance data and uses a provenance tracking catalog, based on a relational database, to record execution
provenance. Two languages are thus required to query provenance
data, SPARQL for design-time application-level (and thus domain-
specific) provenance, and SQL for run-time domain-agnostic execution provenance.

Rather than using two representations for execution-level and
application-level provenance, we rely on RDF, for a graph-based
representation of these two levels. Wings/Pegasus also attaches
domain knowledge to workflow templates which is an interesting
perspective to reduce the design complexity of our production
rules.

In Janus, [31] introduce semantic provenance as technical
provenance graphs coupled with domain knowledge. The main objective is to enhance the usefulness of provenance graphs in responding to typical user queries. Semantic provenance was first
introduced by [32]. Missier and coworkers propose with Janus
a domain-aware provenance model by extending the Provenir
upper-level ontology [33] grounded to BFO [34] (Basic Formal
Ontology) concepts, and a prototype implementation within the
Taverna workflow workbench. The modeling of domain entities relies on four ontologies registered in NCBO, the National Center for
Biomedical Ontologies, namely the BioPAX (dedicated to the modeling of biological pathways), the National Cancer Institute (NCI)
Thesaurus, the Foundational Model of Anatomy (FMA) and the Sequence ontology. Once web services composed into Taverna workflows are semantically annotated, simple inference rules for each
service execution are responsible for the propagation step-by-step
of semantic annotations to the produced domain-agnostic prove-
nance, thus providing new domain-specific provenance. To answer
provenance queries, a specific transitive closure implementation
was proposed based on low-cost SPARQL Ask queries.

Janus is definitely the closest approach to our proposal for generating e-Science experiment summaries. The main differences
are the use of OPM-O and the medical imaging ontologies OntoVIP grounded to DOLCE in our work, compared to Provenir and
biomedical ontologies in Janus. To address scalability issues, we
propose to make a clear distinction between short-term finegrained domain-agnostic provenance and produced long-term

domain-specific provenance through semantic experiment sum-
maries. Janus extends domain-agnostic provenance with domain
specific statements, which requires to manage in a single dataset
the large amount of fine-grained provenance.

Also addressing the exploitation of e-Science workflows from
an end-user perspective, Alper and coworkers analyze in [35] why
raw provenance traces are difficult to exploit and share in the
context of data publication. They motivate the distillation of raw
provenance into more usable and focused provenance, hiding the
noise of less significant processing steps or intermediate data. They
propose an interesting solution based on knowledge capture which
consists in annotating at design-time, workflow templates or Mo-
tifs. They address a similar objective which consists in generating
origin-annotations on input parameters and propagating them,
at run-time, onto produced data through table representations. In
addition, they propose to create workflow summaries based on
Motifs annotations; however, the bindings between produced
and annotated data with origin-annotations and workflow summaries are not obvious.

Our approach addresses similar objectives and is in line with
the analysis of Alper and coworkers. We try to provide an integrated way of producing semantic experiment summaries involving coarse-grained domain-specific annotations which interlink
produced/analyzed data to (i) input parameters, and (ii) designtime annotations of processing services. We rely on Semantic
Web standards to ease the publication of experiment summaries
through Linked Data principles.

7.2. Added value and limitations

Semantic web services. Services involved in e-Science workflows are
generally described through detailed WSDL descriptors, possibly
allowing for syntactic validation. However, RESTful services have
recently been largely adopted due to lighter deployment and better flexibility. As an example, the KEGG16 WSDL services were decommissioned in december 2012 and migrated to REST interfaces.
No consensus emerged to semantically annotate RESTful services,
but SA-REST [36], which relies on RDFa to describe a service with
RDF triples embedded into a companion HTML document, or [37],
bridging WADL descriptors to OWL-S appear as potential solutions,
both in line with our approach.
Production rule design. Although the design of production rules can
be complex, the simulation workflows deployed in production in
the VIP platform keep stable. Rules are therefore reused all along
the platform life-time. More precisely, it took 2 persons/month to
design the 18 production rules, grouped into 4 modalities: 1 rule
for Ultrasound, 1 rule for PET, 1 rule for CT, and 15 rules for MR
(with very slight variations due to similar workflow structures).
As an example, during 6 months of VIP operation, we recorded
137 medical imaging simulations in which 39 of them were
Ultrasound. The single US rule has been reused 39 times for this
modality. Similarly, during the same period, the rule summarizing
CT simulations has been reused 31 times.

When developing production rules, the order of triple patterns
may have a significant impact on performance. Their design is
thus crucial and they should be reused as much as possible when
workflows evolve. This is made possible by the loose coupling between production rules and services descriptions. The proposed
rules adapt to several service implementations as long as they are
semantically annotated with the same domain ontology concepts
(or sub-concepts). However, they remain highly dependent on the
structure of scientific workflows. Workflow evolutions would require adapting the production rules. Abstract (or conceptual/tem-
plate) workflow initiatives such as the conceptual workflows

16 Kyoto Encyclopedia of Genes and Genomes.

introduced in [38,39] could help in the design of production
rules. Indeed, fine-grained workflow structures could be hidden by
higher level conceptual workflow elements and production rules
could be attached to these abstract workflow components instead
of being attached to fine-grained provenance statements, thus enhancing their reuse.

More practically, the MOTEUR workflow designer could be extended to generate, based on the workflow structure and selected
elements, the summarization rules. In terms of production rules
correctness, this extension could validate the rules proposed by
the workflow designer through a set of SPARQL queries that would
check some domain constraints. As an example, a validation query
would check that each produced data has a domain-specific type
and is linked to input data through specific properties (derives-
from-model and derives-from-parameter-set).

Graph summarization techniques have also been proposed to
reduce graph complexity and to extract informative content [40].
The genericity of these approaches is appealing and would reduce
the design cost of domain-specific rules. However, it remains
to be seen to what extent the graph structure criteria used in
summarization are relevant in the context of e-Science workflows.
Usability and quality. Our approach aims at enhancing the usability of data produced through e-Science workflows, and more pre-
cisely, medical imaging workflows involved in the VIP platform.
Both usability and quality are considered. Workflow designers can
exploit raw fine-grained OPM-O provenance information while designing and debugging workflows. But due to provenance traces
size and genericity, it is not aimed at being directly exploited by
scientists. Through the proposed semantic experiment summaries,
we aim at enhancing the confidence of scientists in the quality of
their experiments by providing concise domain-specific annotations describing the produced data and coarse-grained relations
between the data produced and the experiment parameters.

A user-oriented evaluation would be necessary to validate our
approach and study the possible usage of experiment summaries.
It would also bring valuable inputs on how e-scientists search for
their simulated data, and if the proposed approach fosters sharing
of simulation data/models. Currently, these summaries are used to
populate the simulated data catalog exposed to end-users through
the VIP web portal. Platform logs show that for the last 6 months
(December 2013April 2014), 137 experiment summaries were
produced and the simulated data catalog has been viewed 68 times.
Sharing of experiment summaries. To tackle the interpretation of
possibly massive data production in the context of e-Science work-
flows, we automate the generation of semantic experiment sum-
maries. We produce these summaries from OPM-O provenance
datasets. These experiment summaries represent new concise
domain-specific statements in the sense that we associate the produced data to concepts and relation of the OntoVIP domain on-
tology. These summaries make sense for e-scientists if they are
aware of the OntoVIP ontology and the medical image simulation
domain. To enhance the sharing of experiments summaries outside
this community, we could also rely on extensions of the PROV-O
provenance ontology to represent these summaries.

However, although it is possible to extend the PROV-O ontology
with domain-specific taxonomies, these extensions may raise ontology design issues, typically if the domain ontology (e.g. OntoVIP)
is grounded to foundational ontologies such as DOLCE or BFO [34]
(Basic Formal Ontology). Grounding PROV-O to a foundational ontology would allow smart articulations with domain ontologies
also grounded to foundational ontologies such as BIOTOP [41]
(Top-Domain ontology for the life sciences) or OBI [42] (Ontol-
ogy of Biomedical Investigation). Garijo and coworkers proposed
P-PLAN [43] an extension of PROV-O to represent workflows and
also stressed the interest of grounding it to DOLCE. However, the
counterpart would certainly be a consequent design effort needed
to bridge together PROV-O with foundational ontologies.

Simulated data reuse perspective. The SPARQL query illustrated in
Listing 5 shows the relevance of producing domain-specific provenance information in e-Science platforms by joining interlinked
data catalogs. Not only does it allow accurate search for simulated
data but it also enhances the sharing, reuse and repurposing of existing simulated data. Reusing already computed simulated data
could save a lot of computing and storage resources, and opens interesting perspectives towards smarter simulation platforms (less
CPU, memory, and time). Re-exploiting medical image simulation
experiments from the perspective of anatomical models also opens
interesting educational perspectives (e.g. learning medical imaging through simulation, and quick understanding of the parameters impact on simulated data).

8. Conclusion & future works

E-Science experimental platforms use data-intensive workflows to massively process data. Tracking workflow provenance
is crucial to improve reproducibility of e-experiments and confidence in both data and processing chains. Due to its size, its finegranularity and the lack of relations with domain ontologies, the
exploitation of raw provenance traces is however humanly in-
tractable.

Our approach enables domain-specific knowledge capture and
generation in the context of medical image simulation workflows.
It promotes a clear delineation between Role and Natural concepts
in domain ontologies to disambiguate the semantic annotation
of service parameters, thus providing more accurate semantic
service descriptions. It proposes a way of augmenting domain
ontologies with inference rules that produce human-tractable and
informative experiment summaries out of fine-grain provenance
trace sets.

Results show that it is possible to instrument the main medical imaging workflows of the VIP platform with domain-specific
provenance summarization rules to produce few domain-specific
statements. Besides, representing and querying experiment summaries through Semantic Web technologies opens exciting sharing
and repurposing perspectives, especially in the context of Linked
Open Data.

We consider two main continuations for this work. First, to link
domain-specific experiment summaries with the fine-grained raw
traces used for their generation, so that detailed technical execution traces can be retrieved when necessary. Second, to improve
the genericity of our approach. The methodology proposed in this
paper could easily be applied to other disciplines massively producing data (e.g. Bioinformatics) but it may require a modeling
effort to instrument domain ontologies with proper production
rules. We plan to study how generic graph summarization techniques or abstract graph representations could help in producing
experiment summaries at a lower design cost.

Acknowledgments

This work is funded by the French National Research Agency
(ANR) under Grant ANR-09-COSI-03 and the CNRS interdisciplinary mission MASTODONS under program CNRS-12-MI-
MASTODONS-CrEDIBLE. We thank the European Grid Initiative and
France-Grilles.

We would also like to thank Olivier Corby and Catherine Faron
Zucker for their support and advises regarding the Corese/KGRAM
Semantic Web engine.
