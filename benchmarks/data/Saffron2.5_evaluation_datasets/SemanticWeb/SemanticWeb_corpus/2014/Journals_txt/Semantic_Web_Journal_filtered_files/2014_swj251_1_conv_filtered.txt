Semantic Web 0 (0) 1
IOS Press

(Partial) User Preference Similarity as
Classification-Based Model Similarity

Editor(s): Claudia dAmato, Universita degli Studi di Bari, Italy
Solicited review(s): Vojtech Svatek, University of Economics, Prague, Czech Republic; Alexandra Moraru, Jozef Stefan Institute, Slovenia;
anonymous reviewer

Amancio Bouza a, Abraham Bernstein a
a Department of Informatics, University of Zurich, Binzmuhlestrasse 14, 8050 Zurich, Switzerland
E-mail: {bouza,bernstein}@ifi.uzh.ch

Abstract. Recommender systems play an important role in helping people finding items they like. One type of recommender
system is collaborative filtering that considers feedback of like-minded people. The fundamental assumption of collaborative
filtering is that people who previously shared similar preferences behave similarly later on. This paper introduces several novel,
classification-based similarity metrics that are used to compare user preferences. Furthermore, the concept of partial preference
similarity based on a machine learning model is presented. For evaluation the cold-start behavior of the presented classificationbased similarity metrics is evaluated in a large-scale experiment. It is shown that classification-based similarity metrics with
machine learning significantly outperforms other similarity approaches in different cold-start situations under different degrees
of data-sparseness.

Keywords: User Similarity, Partial User Similarity, Collaborative Filtering, Cold-Start Problem

1. Introduction

The vast amount of consumables and items (e.g.,
movies, books, restaurants) offered by Web stores or
Web guides provides an amazing number of options
to people but also challenges people with choosing
proper options. Traditional information filtering techniques (e.g., keyword-based filtering approaches) are
not suitable to reduce the vast amount of items to a reasonable size. Additionally, they do not consider peoples personal preferences to filter proper items. Thus,
people need to invest a lot of effort to filter proper
items.

For that reason, recommender systems gain popularity and are employed in Web stores (e.g., Amazon [18])
to help people in finding proper items. The difference
between traditional
information filtering (IF) techniques and recommender systems is that recommender
systems propose proper items to the user whereas IF
lets the user search for these.

One type of recommender system is so-called collaborative filtering, which considers feedback of likeminded people to provide recommendations. The fundamental assumption of collaborative filtering is that
people who previously shared similar preferences behave similarly later on. Consequently, each user benefits from the previous actions of like-minded users.
Prior research proposed different similarity metrics to
compute the similarities between two users preferences and to discover like-minded people based on
items that both users have rated (i.e., common rated
items). Based on these similarity metrics, collaborative
filtering provides accurate recommendations as long as
people provide many ratings.

However, collaborative filtering does not provide
accurate recommendations to people who have few
prior ratings in general. This problem  known as the
cold-start problem  commonly affects collaborative
filtering-based recommender systems. The reasons are

1570-0844/0-1900/$27.50 c 0  IOS Press and the authors. All rights reserved

Bouza and Bernstein / Partial User Preference Similarity as Classification-Based Model Similarity

twofold. First, few ratings generally do not adequately
express the whole spectrum of a users preferences.

erence models and additionally, conceptualizes partial preferences together with a partial similarity met-
ric. In Section 6, the performance of the presented
approaches and their behavior in different cold-start
situations is evaluated and compared to other, traditional collaborative filtering approaches. Specifically,
the recommendation performance is analyzed for data
sets of different rating sparseness degrees. Finally,
conclusions are drawn from the evaluation in the final Section 7 together with the limitations and future
work.

2. Related work

Collaborative filtering is a method to provide personalized recommendations to a user by considering
the preferences from many other users [27,16,17]. The
underlying assumption of collaborative filtering is that
people who shared the same preferences tend to share
the same preferences in the future. To provide personalized recommendations, preferences need to be formalized and conceptualized. Built on this, similarities
among user preferences are computed to determine
like-minded people. Afterwards, collaborative filtering
is applied to provide personalized recommendations
to a user by considering prior actions of like-minded
users.

2.1. User preference modeling

To provide personalized recommendations and recognize like-minded people, a users preferences needs
to be modeled first. Commonly, user preferences are
represented as an item rating vector with each element representing the users rating for the corresponding item [8,26]. On the one side, no additional content
information is required. On the other side, sparse item
rating vectors model a users preferences poorly and
thus, results in poor recommendations.

For this reason, [13] proposes to represent a users
preferences as a topic preference vector instead, where
items are assigned to one or more topics. Each value of
the topic preference vector describes the users preferences for the corresponding topic. In [21], the synergy
between ontologies and recommender systems are ex-
ploited. Similarly to [13], the users preferences are
modeled as topics of interest with topics being specified by an ontology. Each topic is associated with a
score for every user individually which represents the
users individual preference for a certain topic. Based

Second, even when two users provide many ratings,
common rated items may not be a representative sample of both userss preferences, especially when people
share ratings only for few items. Furthermore, people
who share similar preferences for specific types of consumable may not necessarily share similar preferences
in general. In other words, they share partially similar
preferences. Hence, similarity metrics that compute an
overall user preference similarity do not account for
partially similar preferences.

In contrast to collaborative filtering, applying machine learning to model or rather hypothesize the
users preferences to provide recommendations has
been found effective in cold-start situations similar to
collaborative filtering [29]. Machine learning is applied to generalizes from a users ratings to the users
preferences. But such models are limited to the general preferences of a user and do not consider the individual quality of a consumable or item. For instance,
a user may prefer action movies, but not every action
movie is a good one.

This paper provides an empirical study, which compares the retrieval of like-minded users which is used
in collaborative filtering. More precisely, the retrieval
of like-minded people are compared, which is based
either on ratings for common rated items or on hypothesized user preferences. To this goal, this paper introduces a formal framework for the comparison of user
preferences based on classification similarity of the
respective hypothesized user preference models. The
user preference models are built with machine learning algorithms. The formal modal is instantiated in two
concrete methods to compare hypothesized user pref-
erences. In addition, a preprocessing step for methods is introduced that allows the comparison of partial preferences. More precisely, partial preferences are
extracted from the representation of a machine learning model. The new methods  with and without
preprocessing  are then compared in conjunction
with different machine learning algorithms with the
state-of-the-art of user-based collaborative filtering ap-
proaches.

The reminder of this paper is as follows: In Section 2, previous research on user preference modeling,
and collaborative filtering is presented. Then, the formal framework is introduced in Section 3 leading to
an explanation on how user preferences are hypothesized with machine learning in Section 4. Section 5
presents two similarity metrics to compare user pref-

on this approach, semantic topic similarity is considered additionally in [22]. Likewise, a topic taxonomy
is assumed in [33,35,1]. Scores for topics are passed
to the corresponding super-topic. A more complex approach is proposed in [7]. Semantic concepts are used
to build an ontology-based decision tree to represent
a users preferences. The ontology-based decision tree
is then used to classify an item as relevant or not, an
ontology-based decision tree learner is presented to
hypothesize a users preferences. The problem of these
approaches is that depending on the specification of the
topics the resulting recommendation are too generic.

A correlation between user preference similarity and
trust is reported in [34]. Based on this relationship,
[12] shows how trust in Web-based social networks
can be used to filter like-minded people and provide
accurate recommendations for movies. This approach
does, however, require a trust network to make accurate recommendations.

2.2. User preference similarity

Generally, any vector similarity metric can be applied to compute the similarity between item rating
vectors. Past research focused mainly on cosine sim-
ilarity, Pearson correlation, and Spearmans rank correlation [14,8]. In addition to user preference similar-
ity, [1] propose to describe items with semantical concepts and incorporate semantic item similarity to the
computation of user preference similarity. Following
the same idea, a multilayer ontology-based hybrid recommendation model is proposed in [10]. Similar items
are clustered whereby each cluster represents a topic
of interest. Users are related to these clusters to form
communities of interests [11]. Then, all items liked by
a community of interests is recommended to its mem-
bers. However, the computational complexity of nearest neighbor-based collaborative filtering is O(n2) because the preference similarity of each user pair has to
be computed. However, the computational complexity
of computing the preference similarity of users in nearest neighbor-based collaborative filtering is O(n2) because the preference similarity of each user pair has to
be computed. As empirically shown in [31], users have
rather partially similar preferences than similar overall
preferences. User-item subgroups are proposed by [31]
to perform collaborative filtering. A user can belong to
multiple user-item subgroups. To recommend an item
to a user, the users within the corresponding subgroups
are used to predict the relevance of the particular item.
These subgroups contain a subset of users having sim-

ilar preferences for a subset of items. These subgroups
do not represent partial preferences because the items
properties can be completely different. Users are modeled with partial preference relations in [24]. The general idea is to cluster similar users based on their partial preferences. In fact, the preferences of users are
bootstrapped by classifying the user to a cluster and inheriting partial preferences of other users of the same
cluster.

To address the computational complexity clusterbased approach are proposed to reduce the number of
comparisons. [32] cluster people according to their geographical location and compare preference similarities within a geographical cluster. Similarly, clustering
user into groups of similar users is suggest in [25] with
ClustKNN. Then, similarity is computed only between
the centroids of these clusters. It is suggest to use k-
Means clustering algorithms to control the tradeoff of
computational complexity and recommendation accuracy which depends on the number of clusters. Empirical evidence is given in [25] that clustering techniques
support the scalability of collaborative filtering whilst
not sacrificing much recommendation accuracy. Given
its reliance on item vector ratings, however, ClustKNN
does not address the partial preference similarity prob-
lem.

In [3], social aspects of the users (e.g., age) and
content-based information are used to compare users
preferences. Specifically, single item properties are
used to create item groups and preference similarity
is then computed within such an item group. In other
words, partial preference similarity is defined in [3]
with respect to a single item property. The limitation
of this approach is that it limits the partiality to one
single, pre-defined item-property.

Generally, users differ in their rating behavior such
that some users rate items on average higher then
other users. Hence, it is shown in [4] that normalizing
the users ratings improves the accuracy of computed
user preference similarities and consequently results in
more accurate recommendations.

2.3. General framework for collaborative filtering

A general framework for a nearest neighbor-based
collaborative filtering is proposed in [14,26]. The general framework is shown in Eq. 1. The goal is to pre-

dict a ratingra,j for the active user a and item j based

on his/her average rating ra and of k nearest neighbors (i.e., other k users with most similar preferences)
weighted rating deviation rb,j  rb of that item.

Bouza and Bernstein / Partial User Preference Similarity as Classification-Based Model Similarity

b=1

ra,j = ra +

wa,b  (rb,j  rb)

(1)

wa,b

b=1

Whilst many similarity metrics for the computation
of wa,b have been proposed [14] show that the Pearson
correlation outperforms the Spearmans rank correlation and cosine similarity.

2.4. Cold-start problem

All recommender systems face the cold-start problem when recommendations are required for users
or items for which not enough information (i.e., rat-
ings) is known. Given that users only use a recommender system that provides reasonable recommen-
dations, providing poor recommendations will lead
to user attrition. Hence, a recommender system may
never achieve to attain a critical mass of ratings to provide reasonable recommendations. Thus, the challenge
is to provide reasonable recommendation even with little information about the users preferences.

Three different types of the cold-start problem can

be distinguished [21]:

 new-system cold-start refers to the initial stage of
a recommender system where no or only few initial ratings are provided. Every recommender system performs poorly in this situation because it
lacks crucial information to build a good user pro-
files. Even content-based recommender systems
need a few observations of the users interest to
provide reasonable recommendation.

 new-user cold-start refers to the situation where
too few ratings for a user exist even in the light
of sufficient information about others. Note that a
similar problem may arise when ratings for some
items are extremely sparse as the resulting set of
common rated items may be small.

 new-item cold-start refers to the problem that

arises with items that have no ratings yet.

Generally, content-based approaches are incorporated to counter the cold-start problem. Content filtering approaches are combined with collaborative filtering approaches to build hybrid recommender systems.
A classification and survey of hybrid recommender
system is found in [9]. One of the first hybrid recom-

mender system is the Fab system [2], which recommends documents. Its users are asked to create a user
profile by selecting topics of interest. Users are similar if they share many topics of interest. Documents
are recommended when they match the users profile
and have been liked by users with similar user profiles.
Whilst this approach addresses the cold-start problem
its results are too generic and lack precision.

Another approach is to preprocess the data before
replacing missing ratings with predicted ones. [20],
e.g., predict unknown user ratings based on known rat-
ings. For that purpose, a users preferences are learned
(or hypothesized) with a machine learning algorithm
that predicts the ratings for not yet rated items. The
mediation of user models across different domains is
proposed in [5] for enhanced personalization of user
models. It is empirically shown that like-minded people with respect to a domain tend to be like-minded in
another domain. For instance, people sharing similar
preferences for music tend to share similar preferences
for movies. However, this is shown for overall preference similarity between users and does not necessarily
apply to partial preference similarity between users.

3. Formal framework and notation

The basic elements for a collaborative filtering
based recommender system is the set of users I, the
set of items G, and the set of ratings R in which users
explicitly or implicitly state about items. The rating
set R is represented as the m  n rating matrix as it
is shown in Figure 1 with m = |I| number of users
and n = |G| number of items. A particular user is referred to as i  I, a particular item as g  G, and a
particular rating of user i for item g as rig. Note that
rig corresponds to the element at the ith row and gth
column of the rating matrix R. User is rating vector
and item gs rating vector are referred tp as Ri and
Rg, respectively.
The value space of the ratings rig is denoted as K
which consists of rating concepts C or  in case of no
rating. In other words, K = C  . A particular rating
concept is referred to as c  C = {1, . . . , k  1} with
k = |K| number of rating concepts. Generally, a value
space is classified to one of the following four groups:
 Nominal rating: The task of item recommendation can be treated as a classification problem
that associates an item with one ore more rating
classes. Popular classes of rating concepts C are
{relevant, irrelevant} or {likes, does not likes}.

follows:Ra = f (a, U, G, Ra, R, C)
with Ra as the predicted rating vector for the active

(2)

user a.

4. User preference modeling

Generally, the true user preferences can be repre-
i which consented as the true item rating vector Rtrue
tains for every item g  G the user is true rating
rig  C:

i = (cid:104)rtrue
Rtrue

i1

in (cid:105)
, . . . , rtrue

,rtrue

(3)

Fig. 1. Representation of the userss ratings for all items as rating
matrix R

 Ordinal rating: The rating concepts are interrelated and can be ordered. The typical example for
ordinal rating concepts is the star-rating on a 1-
5 integer scale: {, . . . , }. With ordinal ratings only the assertion can be done that a
4-star rated item is better then a 2-star rated item,
but not that it is two times better.

 Interval rating: Items can be rated with a numeric
value from R. Ratios on this scale are not mean-
ingful. In general, such ratings can be normalized
to a [1, 1] scale.
 Ratio rating: Items can be rated with a numeric
value from R. In general, such ratings can be normalized to a [0, 1] scale.

The subset of items that user i has rated with r = 
is called the users i rated item set Gi  G. The
subset of users which have rated item g is called the
items g rated user set Ig  I. The user for which we
compute the recommendations is denoted as the active
user a  I.

Based on the introduced notation, a recommender
system is defined as a total function f which returns

for the active user a the item rating vector Ra. The
item rating vector Ra provides the user as rating rag
for item g  Ga or a predicted ratingrag for item

g  G \ Ga. For the computation, the function f considers the active user a as well as all other users in U,
all items in G, user as item rating vector Ra as well
as the rating matrix R and the set of rating concepts
C. Hence, a recommender system is conceptualized as

In case that all true item ratings of a user are known,
providing recommendation comes down to the trivial
recommendations of the top rated items from the vector Rtrue
) . Generally, a recommender system has only
partial knowledge about the user is true preferences
. The reasons for a user not providing all ratings
Rtrue
can be:

 Costs: Temporal or monetary costs limit

the
amount of items that a user is able to consume
and ultimately to rate

 Usability: The rating effort is too high because the
item has to be found first (search costs) and then
being rated with too much effort (usability)

 Privacy: The user may have an interest in not to

publish some ratings.

Since the true item rating vector Rtrue

is only partially known, the representation of the users preferences is adjusted to a more general way that is adaptable to machine learning. It is assumed that every user
i  I is able to assign the proper rating concept
c  C to every item g  G based on his preferences.
Hence, the user is preferences are defined as the mental rating function or rather utility function ui(g):

ui(g) : g  c = rtrue

ig

,g  Gi

(4)

Note that ui(g) is a total function. Since user i always
 C,
associates the item g with the true rating rtrue
the probability that the utility function ui(g) provides
the true rating concept is always 1:

Pui(g) = c|rtrue

ig = c = 1 ,g  Gi

(5)

ig

r11usersitemsri1r1grig1irm1rmgr1nrinrmnm1gnRiIRgG6

Bouza and Bernstein / Partial User Preference Similarity as Classification-Based Model Similarity

With the representation of the users preferences as
utility function ui(g) (see Eq. 4), a computer program
is able to learn an approximation of ui(g) based on the
user is ratings ri and the item set Gi. A computer program is said to learn from past experience E if its performance in some tasks T is improved with more experience E [23]. the learning problem of learning the
users preferences needs to be well-defined. Learning
the users preferences is a well-defined learning prob-
lem. According to [23], the following three features
need to be defined: the class of tasks, the measure of
performance to be improved and the source of experi-
ence:

 Task T : For the user i, predict for the item g  G
the proper rating concept c  C such that
c = rtrue

ig

ig

 Performance measure P : A metric which represents the accuracy of the prediction (e.g, percentage of correct predictions of rating concepts
c = rtrue

of items g  Gi for the user i).

 Experience E: The user is item rating vector Ri

and the set of items Gi

The learner faces the problem to hypothesize the rating concepts c  C for the item g  Gi. Hence, the
learner has to find the hypothesis hi(g) from the hypothesis space H of all possible hypotheses which best
predicts the rating concept c the user i associates with
item g. For this purpose, the performance measure P
is used to determine the best hypothesis hi. This hypothesis hi(g) is the user is preference model or rather
user i hypothesized utility function. A perfect hypothesized utility function hi(g) for g  Gi is defined as:

hi(g) = ui(g)

,g  Gi

(6)

In general, hi(g) is an approximation of ui(g) such
that the probability that hi(g) = ui(g) is below 1. This
is expressed as:

Phi(g) = ui(g)  1

(7)

Consequently, hi(g) approximates ui(g) by the er-

ror term i(g):

In case of an adequate hypothesis space H, it can be
argued that the error term i(g)  0 because the performance of hi(g) improves with more experience E.
Thus, it is feasible to conclude that hi(g) approximates
ui(g):

hi(g)  ui(g)

(9)

Comparison of user preference models

Traditionally in collaborative filtering, the user is
preferences are represented as the user is item rating
vector Ri. The item rating vector Ri approximates the
user is preferences ui(g) proportionally to the number
of ratings |Gi| because each rating is independent from
each other. In other words, the approximation rate with
every additional rating rig  C remains constantly 1
m.
In contrast, the approximation rate of user is preferences with machine learning is characterized by a sigmoid function. In other words, the approximation rate
increases in the beginning, but towards the end, it converges to 0. The reason for this behavior is that with
every additional rating rig  C, additional information
about the item gs properties are provided such that
item properties get interrelated and more information
is gained. Towards the end, additional ratings mostly
reinforce the user preference model and do not contribute to the user preference models accuracy. Referring to Eq. 8, the prediction accuracy of the user preference model hi(g) is limited due to the limitations of
the defined hypothesis space H representing the user
is preferences. The hypothesis space needs to be to
limited to reduce the search space, thus reducing computational complexity.

However, in combination with collaborative filter-
ing, both types of user preference models (i.e., item
rating vector and machine learning models) get interrelated to each other such that missing ratings can be
predicted to complement a user is preference model
respectively user is item rating vector Ri such that

the predicted item rating vector Ri approximates the

user is preferences super-proportional.]

ui(g) = hi(g) + i(g)

(8)

5. Hypothesized user preference similarity

The performance of hi(g) depends on the one side
on the hypothesis space H, which is based on human
designers choice, and on the other side on the amount
of Experience E and the number of the user is rating.

This section introduces this papers novel metrics
for comparing user preferences. The general idea is to
approximate the users preferences with a learned hypothesis about his/her preferences instead of his/her

item rating vector containing all ratings the user pro-
vides.

The preference similarity between two users a and
b is denoted as sim(a, b). If user a and b rate the same
item identically, then a and b share identical prefer-
ences. If user a and b rate the same item differently the
similarity metric determines the preference similarity
between both users.

5.1. Classification-based user preference similarity

Referring to Eq. 4, the preference similarity between
two users a and b is equivalent to the similarity of both
utility functions:

sim(a, b)  simua(g), ub(g)

,g  G (10)

As neither ua(g) nor ub(g) are known, both are approximated with ha(g) and hb(g) based on Eq. 9. Con-
sequently, the similarity of both utility functions ua(g)
and ub(g) is approximated by the similarity of the hypothesized user preferences ha(g) and hb(g):

simua(g), ub(g)  simha(g), hb(g)

,g  G
(11)

Based on Eq. 11, a probabilistic classification similarity in Section 5.1.1 and a Pearson correlation-based
classification similarity in Section 5.1.2 are presented

to define the similarity simha(g), hb(g).

5.1.1. Probabilistic classification similarity

If the target concept c  C is a nominal class then
the task of rating item g by user i is equivalent to classifying item g by user i. The probabilistic classification similarity metric is defined as the probability that
two users a and b both classify item i identically respectively rate item i identically:

sim(a, b) = Pua(g) = ub(g)

(12)

The probabilistic classification similarity is defined
on the interval [0, 1] with 0, no similar preferences,
and 1, identical preferences. With Eq. 9, the user rating
function is approximated with the hypothesized user
preferences:

Pua(g) = ub(g)  Pha(g) = hb(g)

(13)

set. In the following, three candidates for the appropriate item set are discussed. The first candidate is the
common rated item set Ga  Gb of both users. The
common rated item set may not necessarily be a representative sample of both users preferences. The reason is that it corresponds to the intersection of both
users preferences which may represent only partially
each users preferences. The second candidate is the
set of all items. Typically, it is very large and thus inappropriate due to the computational effort involved. The
third candidate is the set of unified rated items GaGb
which either of both users has rated. This set represents
both users entire preferences, thus countering the issue
of partial preference representation of the set of common rated items. Furthermore the set of unified rated
items needs less computational effort than the set of all
items. For these reasons, the probabilistic classification
similarity of hypothesized user preferences is defined
for the unified rated items Ga  Gb:

sim(a, b) = Pha(g) = hb(g)

,g  Ga  Gb
(14)

5.1.2. Correlation-based classification similarity

If the target concept c  C is either an interval rating
or a ratio rating as defined in Section 3 then we can
exploit the notion of correlation that is defined on both
scales. Hence, the preference similarity between two
users a and b is defined as the correlation between both
utility functions ua(g) and ub(g):

sim(a, b) = corr(ua(g), ub(g)

,g  G (15)

As proposed in [14], the Pearson correlation is used
to measure the user preference similarity. The Pearson
correlation is computed as the covariance of both utility functions ua(g) and ub(g) divided by the product
of their standard deviations:

covua(g), ub(g)

ua(g)ub(g)

ua(g),ub(g) =

The probabilistic classification similarity of hypothesized user preferences has to be defined on an item

sim(a, b) =

Hence, the similarity of two users a and b is defined
as the Pearson correlation between both user prefer-
ences:

covua(g), ub(g)

ab

(16)

(17)

Bouza and Bernstein / Partial User Preference Similarity as Classification-Based Model Similarity

In contrast to the probabilistic classification similarity in Section 5.1.1, the correlation-based classification
similarity is defined on the interval [1, +1], with -1,
contrary preferences, 0, no similar preferences, and 1,
identical preferences.

With Eqs. 11 and 17, the user preference similarity
is approximated by the similarity between the hypothesized user preferences such that the correlation-based
similarity is defined as:

covha(g), hb(g)

ha(g)hb(g)

sim(a, b) =

,g  Ga  Gb

(18)

5.2. Partial user preference similarity

Generally, users preferences are divers and manyfold such that they prefer items with different properties over others. As a consequence, people share
similar preferences for a limited number of items,
or in other words they share partially similar preferences as it is shown in [3]. Current preference similarity metrics do not account for partially similar preferences among people. Hence, collaborative filtering
approaches wrongly consider users as like-minded in
some contexts. Thus, partial user preference similarity
metrics are needed to retrieve the correct (or true) set
of like-minded users within a specific context.

To account for partial preference similarity, a users
preferences are interpreted as a set of individual pref-
erences. Referring to Eq. 4, the users utility function
u(g) is therefore interpreted as a function with case
distinctions, whereby each case refers to a single partial preference uv(g):


u(g) =

u1(g)

uv(g)

if g satisfies case 1
...
if g satisfies case v

(19)

Note that all cases describe disjoint item sets. Each
case is considered as a users partial preference that
constitute the users entire preferences. Considering
two users a and b, some items are evaluated to be a single case by ua(g) by user a whereas the same items are
evaluated to be distinct cases by by user b in ub(g). For
that reason, partial user preference similarity is defined

as a directional preference similarity and is defined as:

sim(a, b|g) = simua

s (g), ub(g)

,g satisfying caseDa

(20)

s is satisfied for the items g where ua

s (g) is ap-
s specifies the condition under which

where Da
plicable. I.e., Da
s (g) gets chosen over other ua (g).
ua
The partial preference similarity is defined as the
probability that the user b rates item g identically to
user a and is expressed as:

simua

s (g), ub(g) = Pua

s (g) = ub(g)

,g satisfying caseDa

(21)

Putting Eq. 9, a users preferences or rather utility
function u(g) is approximated with the hypothesized
user preferences or rather hypothesize utility function
h(g). Consequently, h(g) is also interpreted as a function with case distinctions whereby each case refers to
a single preference hv(g). A single preference is realized as a preference rule and consists of a premise Dv
and its conclusion ck with the premise being a conjunction of constraints of the set of features [23]. Ul-
timately, a case corresponds to the proper premise respectively conjunction of constraints. Thus, h(g) is defined as:

h(g) =

h1(g)

hv(g)

if g satisfies premise D1
...
if g satisfies premise Dv

(22)


An example of a hypothesized partial user preference is presented in Figure 2. The YOULIKE ontology provides the semantic concepts and properties to
describe peoples partial preferences. In the following
the preprocessing step is described to specify a users
partial preferences.

Referring to Eq. (22), the products properties determine which case is satisfied, thus determining the
proper hypothesized partial preference. In this paper,
a case is interpreted as a conjunction of constraints on
the items properties and the rating as the conclusion.
The conjunction of constraints is represented as a set
of constraints whereby each case has to be fulfilled.

In the following, the preprocessing step is introduced to extract hypothesized partial preferences from

ences corresponds to the number of leafs of the decision tree.

With Eq. 20, partial preference similarity between
s (g) is approx-

user a and b for the single preference ua
imated as:

simua

s (g), ub(g)  sim(ha

s (g), hb(g))

(23)

Fig. 2. Example of a users partial preference using the YOULIKE
ontology by [6].

Fig. 3. Partial preferences encoded as branches from the root to the
leaf of the decision tree.

a decision tree learner. A decision tree learner represents a hypothesis as a decision tree in which each
node corresponds to a test of some property of a prod-
uct. Each edge corresponds to a possible evaluation of
such a test. A test in combination with an evaluation
specifies a condition. Thus, each branch which starts
at the root node and ends at a leaf corresponds to a
conjunction of constraints with the leaf as the conclu-
sion. Therefore, a hypothesized partial preference is
encoded as a branch in a decision tree which starts at
the root and ends in some leaf.

Figure 3 presents an exemplified decision tree representation of a userss preferences. The highlighted path
in Figure 3 corresponds to a hypothesized partial preference and consists of the evaluated tests {P, A, T}
and its conclusion h.

To extract all hypothesized partial preferences, all
all possible branches have to be parsed and extracted.
In total, the number of hypothesized partial prefer-

Hence, the partial preference similarity of hypothesized user preferences for g  GaGb and g satisfying
s is defined as:
Da

sima, b|g  Pha

s (g) = hb(g)

(24)

Every machine learning algorithm whose model is
representable as case distinctions (e.g., decision tree
learner) can be applied to hypothesize a users partial
preferences.

6. Evaluation

The performance of the classification-based user
preference similarity metrics (Section 5.1) and the
partial user preference similarity metric (Section 5.2)
is empirically evaluated with a movie data set and
compared to state-of-the-art collaborative filtering ap-
proaches. The cold-start behavior of the presented similarity metric is evaluated with different data sets of
different rating sparseness degrees.

6.1. Dataset

The MovieLens data set is used that has been collected by the GroupLens Research Project at the University of Minnesota. The data set provides 100 000
ratings of 943 users about 1 682 movies. The ratings
are discrete values on a 1-to-5 integer scale with a rating mean of 3.53, a standard deviation of 1.13 and a
rating median of 4. Each user provides at least 20 rat-
ings. The genres of the movies are considered exclu-
sively, which are provided by the MovieLens data set.
The data set provides 18 different genres. Every movie
is related to at least 1 genre and at most 6 genres. The
median number of genres per movie is 2.

Based on the original data set, 10 different data sets
have been created with an increasing rating sparseness degree of 10%. Specifically, for each of the 10
datasets additional 10% have been randomly removed
of the ratings a user provides. Figure 4 shows the rating

foaf:AgentProfilePreferenceex:ActionMovie#considers#features#requires5Constraintex:merdf:typeex:myprefsrdf:typeex:actionreachSciFirdf:type#ratedrdf:typemo:Actionmo:hasGenremo:Movierdf:typeex:SciFiMovie#requiresrdf:typerdf:typemo:SciFimo:hasGenreTTThPA...Th...............RootTestUtilityPartial preference10

Bouza and Bernstein / Partial User Preference Similarity as Classification-Based Model Similarity

Fig. 4. MovieLens data set with increasing rating sparseness degree. (a) corresponds to the original MovieLens data set. In (b) and (c), 50%
respectively 90% of the ratings are removed.

sparseness of the original data set and the two sparse
data sets for which 50% and 90% of all ratings have
been removed. Note that the 10th data set with a rating
sparseness degree of 100% does not contain any ratings and thus is not considered in the evaluation. As is
shown in Figure 4, the overall number of ratings decreases with each data set. However, the relative number of ratings per user and per item remains the same
such that the rating distribution per user and item remains the same.

6.2. Experimental setting

6.2.1. Performance metrics
As suggested in [15],

the mean absolute error
(MAE) and root mean square error (RMSE) are used
as performance metrics to measure the rating prediction accuracy. The MAE is the mean of the absolute
difference between the users rating ri and the corre-

sponding predicted ratingri:
|ri ri|

M AE =

i=1

Similarly, the RMSE is the root of the mean squared
error. The RMSE complements the MAE because it
is sensitive to large prediction errors rather then small
prediction errors. The RMSE is computed as follows:

(cid:118)(cid:117)(cid:117)(cid:116) 1

i=1

(ri ri)2

RM SE =

The prediction errors are normally distributed. There-
fore, the absolute prediction errors are folded nor-

mally distributed. For that reason, the non-parametric
Wilcoxon signed-rank test for dependent samples is
used to test the pairwise the MAE and RMSE of the
recommendation approaches for significance. The significance level is set to  = 0.01. The Bonferroni correction is applied to control the family-wise error.

The filtering effectiveness of relevant items is evaluated to complement to the evaluation of the prediction accuracy. The filtering effectiveness is measured
by means of P recision and Recall. The value of the
ratings are 1-to-5 integer scale. Since a binary sale
is required, an item is classified as relevant for user
i, if he rated the item above the overall rating mean
that is 3.53. Additionally, the F1-score is computed,
which accounts for the well-known trade-off between
P recision and Recall. The accuracy of how well
items are filtered is measured by the area under the
ROC curve (AU C), which is equally to the probability
that a randomly selected relevant item is higher ranked
then a randomly selected not relevant item.

The P recision corresponds to the relation of rec-

ommended relevant items and recommended items:

P recision =

|rel. items  rec. items|

|rec. items|

The Recall corresponds to the relation of recom-

mended relevant items and all relevant items:

Recall =

|rel. items  rec. items|

|rel. items|

The F1-score is the harmonic mean of the P recision

and Recall:

F1-score =

2  P recision  Recall
P recision + Recall

020040060080010001200140016000100200300400500600700800900100000 ratingsitemsusers02004006008001000120014001600010020030040050060070080090050240 ratings02004006008001000120014001600010020030040050060070080090010037 ratingsitemsusersitemsusersBouza and Bernstein / Partial User Preference Similarity as Classification-Based Model Similarity

The P recision and Recall values are based on a sequence of Bernoulli experiments and are thus binomial
distributed. As stated by the central limit theorem, a
binomial distribution approximates a normal distribution for large amount of Bernoulli experiments. Hence,
paired-samples t-test is used to test for significance.
The significance level is set to  = 0.01 and Bonferroni correction is applied.

The experimental setup consisted of a 5-fold cross
validation for each of the 10 data sets. For this pur-
pose, the ratings for each user are divided into 5 folds
of equal size. For each user, 4 folds are hold out which
are used as the training set. The remaining fold is used
as the test set. That results in a total of 50 experiments
per evaluated approach.
6.2.2. Recommendation approaches for comparison

For the approaches presented in this paper the machine learning algorithms J48, SMO, and Naive Bayes
are chosen from the Weka library [30] to hypothesize user preferences and to evaluate the probabilistic
classification similarity and the correlation-based user
preference similarity. As suggested in Section 5.2, J48
is used to hypothesize user preferences because each
path from the root to the leaf in a decision tree is equivalent to a self-containing individual preference. These
similarity metrics are applied to the general framework for k-nearest neighbor-based collaborative filtering [14] that is explained in Eq. 1. As evaluated in [14],
k = 10 is used as the neighborhood size.

Since the approaches presented in this paper rely on
machine learning algorithms, we evaluate the performance of approximating users preferences, with the
respective machine learning algorithms. Note that this
corresponds a content filtering approach. The first is
the decision tree learner J48, the second is the support
vector machine SMO with a linear kernel, the third is
Naive Bayes, and the last is Bayesian Network. These
four approaches are evaluated to to investigate how the
performance of the user preference models correlated
As state-of-the-art collaborative filtering, single
value decomposition (SVD) [28] and nearest neighborbased collaborative filtering with Pearson correlation
[14] (kNN Rating Pearson Corr.) are evaluated. Fi-
nally, the average item rating is used as the general
baseline.

6.3. Results

6.3.1. Recommendation accuracy

Regarding to M AE and RM SE, collaborative filtering approaches (SVD, kNN Pearson corr., kNN J48

class., kNN J48 Pearson corr., kNN Naive Bayes class.,
kNN Naive Bayes Pearson corr., kNN SMO Pearson
corr., and kNN Partial J48 class. sim.) perform significantly better than the content filtering approaches (J48,
SMO, Naive Bayes, and Bayes Net) in general as it
is presented in Figure 5 and Table 1. For readability
reason, Figure 5 presents a selection of the evaluated
approaches that are presented in Table 1.

The kNN-based collaborative filtering approaches
rely on the performance of the user preference similarity metric. The novel similarity metrics based on hypothesized user preferences (kNN J48 class. sim., kNN
J48 Pearson corr., kNN Naive Bayes class. sim., kNN
Naive Bayes Pearson corr., kNN SMO Pearson corr.,
kNN Partial J48 class. sim.) perform significantly better then the Pearson correlation of users item rating
vectors (kNN Pearson corr.) for data sets with high
degree of sparseness. However, the performance of
the hypothesized user preference similarity metrics are
limited by the learning effect cap of the underlying
machine learning algorithm such that these similarity
metrics do not improve much with more data. Note that
the correlation of the content filtering approaches and
the kNN-based collaborative filtering approach based
on hypothesized user preferences is a causal relation-
ship.

In contrast, kNN Pearson correlation benefits more
from much data such that it significantly outperforms
all hypothesized user preference similarity metrics although the difference is marginally small especially
compared with kNN Naive Bayes Pearson corr.

Based on the evaluation results, it is concluded that
Naive Bayes and SMO perform best as the underlying machine learning algorithm for hypothesized user
preferences similarity metrics. Both perform significantly better then hypothesized user preferences with
J48 in general. Probabilistic classification performs
significantly better then correlation-based classification similarity with J48. However, this does not hold
for Naive Bayes where the correlation-based classification similarity performs significantly better with
sparse data but significantly worse with much data.
However, it is shown that the probabilistic classification similarity reaches faster the learning effect cap
whereas the correlation-based similarity benefits more
from more data.

Though partial preference similarity metric with J48
performs significantly better then correlation-based
classification similarity with J48, it performs significantly worse then the probabilistic classification similarity metric with J48. To conclude, partial preference

Bouza and Bernstein / Partial User Preference Similarity as Classification-Based Model Similarity

Metric

Recommendation accuracy for different degree of rating sparseness.

Table 1

Algorithm
J48

Naive Bayes
Bayes Net
Average

kNN Pearson corr.
kNN J48 class. sim.
kNN J48 Pearson corr.
kNN Naive Bayes class. sim.
kNN Naive Bayes Pearson corr.
kNN SMO Pearson corr.
kNN Partial J48 class. sim.

J48

Naive Bayes
Bayes Net
Average

kNN Pearson corr.
kNN J48 class. sim.
kNN J48 Pearson corr.
kNN Naive Bayes class. sim.
kNN Naive Bayes Pearson corr.
kNN SMO Pearson corr.
kNN Partial J48 class. sim.

0%

10%

20%

30%

40%

50%

60%

70%

80%

90%

similarity with J48 does not provide a significant improvement but it performs similar to the hypothesized
user preference similarity with J48.

The only exception is SVD which performs significantly worse in terms of RMSE at a sparseness degree of 90%. In other words, SVD is vulnerable to
poor recommendations when the sparseness degree is
extremely high. This suggests to avoid SVD in newsystem cold-start situations.

Within the content filtering approaches, the Naive
Bayes algorithm performs significantly best with high
degree of sparseness. However, the learning effect of
SMO is higher then the one of Naive Bayes such that
SMO hypothesizes a users preferences significantly
better then Naive Bayes with lower degree of sparse-
ness. Generally, the average rating significantly outperforms SVD even though the recommendation accuracy
is similar.

In conclusion, retrieving like-minded individuals
based on the comparison of hypothesized user prefer-

ences is the better method, especially in a cold-start
situation. In the case when users provide many ratings,
this approach is comparable to the traditional approach
based on ratings for common rated items.
6.3.2. Comparison of collaborative filtering

approaches

In the following, both collaborative filtering ap-
proaches, namely kNN Naive Bayes Pearson correlation and kNN kNN Pearson corr., are compared. The
first approach is chosen because it is the best performing approach that is based on the formal framework of
Section 5.

The M AE and RM SE measure the overall performance and thus, do not measure how well a recommender system performs in each of the three coldstart problems presented in Section 2.4. For that rea-
son, the predicted recommendations are grouped according to the number of ratings a user provides (rat-
ing effort) and the number of times an item has been
rated (item popularity). For this goal, the users and

Fig. 6. Recommendation accuracy behavior of kNN Naive Bayes
Pearson corr. with respect to item popularity and a users rating effort
for the original MovieLens dataset (0% of sparseness degree).

ored sinks represent good recommendation accuracy
respectively low MAE. The area with unpopular items
and little rating effort of the user defines the the newsystem cold-start area, the area with unpopular item
defines the new-item cold-start area and the area with
little rating effort of a user defines the new-user coldstart area.

As it is shown in Figure 6, collaborative filtering approaches perform poorly in the new-system cold-start
area. Additionally, nearest neighbor-based collaborative filtering is shown to perform poorly in the newitem cold-start area independent of the number of ratings a user provides. The reason is that the number
of potential neighbors with respect to a given item
is very small, such that the k-nearest neighbors consists of less like-minded users. In contrast, the number of potential neighbors for popular items is much
higher such that the k-nearest neighbors consists most
likely of like-minded users. Interestingly, the number
of ratings a user provides does not have that much impact on the overall recommendation accuracy because
machine learning algorithms quickly approximate user
preferences. To summarize, the item popularity has
a bigger impact on the recommendation accuracy of
nearest neighbor-based collaborative filtering then the
number of ratings a user provides.

The recommendation accuracy of kNN-based collaborative filtering with Pearson correlation of item
rating vectors (kNN Pearson corr.) is similarly distributed as in Figure 6. For that reason, the M AE dif-

Fig. 5. Behavior of prediction accuracy of recommendations with
increasing degree of rating sparseness from 0% (original data set) to
90%. The rating prediction accuracy is measured in terms of MAE
and RMSE.

items have been binned to 20-quantiles. The behavior
of kNN Naive Bayes Pearson corr. is exemplified in
Figure 6.

Referring to Figure 6, the highest and red colored
peaks represent poor recommendation accuracy respectively high MAE. In contrast, lowest and blue col-

Sparseness degree90%80%70%60%50%40%30%20%10%0%MAE1.0501.0251.0000.9750.9500.9250.9000.8750.8500.8250.8000.7750.750Page 1kNN J48 partial class.kNN Naive Bayes corr.kNN J48 class.kNN Pearson CorrSVDNaive BayesSMOJ48Sparseness degree90%80%70%60%50%40%30%20%10%0%RMSE1.5001.4501.4001.3501.3001.2501.2001.1501.1001.0501.0000.9500.900kNN J48 partial class.kNN Naive Bayes corr.kNN J48 class.kNN Pearson CorrSVDNaive BayesSMOJ48VariablesPage 1kNN J48 partial class.kNN Naive Bayes corr.kNN J48 class.kNN Pearson CorrSVDNaive BayesSMOJ4805101520051015200.70.80.911.1 rating effortitem popularity MAE0.650.70.750.80.850.90.951MAE14

Bouza and Bernstein / Partial User Preference Similarity as Classification-Based Model Similarity

Lens data set), the kNN Pearson correlation performs
significantly better.

6.3.3. Filtering performance

The evaluation of the filtering performance is presented in Table 2 and confirms the evaluation results
presented in Section 6.3.1 because the evaluated approaches behave similar.

Generally, content filtering approaches (J48, SMO,
Naive Bayes, and Bayes Net) have significantly lower
P recision then collaborative filtering approaches
(kNN J48 class. sim., kNN J48 Pearson corr., kNN
Naive Bayes class. sim., kNN Naive Bayes Pearson corr., kNN SMO Pearson corr., kNN Partial J48
class. sim.). In contrast, content filtering have partially
higher Recall then collaborative filtering approaches
with the exception of the kNN SMO Pearson corre-
lation. As the F1-score suggest, the correlation-based
classification similarity with SMO performs best on
every sparseness degree. Exceptionally, correlationbased classification similarity with Naive Bayes provides the highest F1-score for the data set with a
sparseness degree of 80%.

To complement the F1-score that combines P recision

and Recall, the AU C values are presented in Table 3.
The hypothesized user preference similarity metrics
with Naive Bayes have a higher AU C value then oth-
ers. The probabilistic classification similarity performs
better in extreme cold-start settings with a high degree
of sparseness whereas the correlation-based classification similarity approach performs better with much
data.

7. Limitations, future work, and conclusions

This paper introduces the notion of using hypothesized user preferencesby the means of a machine learning algorithmto model user preferences
in the context of collaborative filtering. Second, it
proposes to exploit the capabilities of some machine
learning algorithms (such as decision trees) to partition the feature-space to model partial user pref-
erences. Third, it introduces three novel similarity
metricsnamely probabilistic classification similarity,
correlation-based classification similarity, and partial
similarityoperating on the hypothesized user prefer-
ences. These are then combined into hypothesized user
preference based recommendation systems.

The paper then compares these systems to content
based filtering approaches and state-of-the-art collab-

Fig. 7. Difference between kNN Naive Bayes Pearson correlation
(modeling hypothesized user preferences) and kNN Pearson correlation (using the item vector as user preferences) regarding to MAE.
Numbers below 0 indicate a superiority of the hypothesized method.

ference of the kNN Naive Bayes Pearson correlation
and kNN Pearson Correlation is presented in Figure 7
to compare both. As the distribution of M AE differences shows, kNN Pearson correlation performs better than similarity metrics based on hypothesized user
preferences when the rating effort of a user is high and
the item popularity is low. In contrast, similarity metrics based on hypothesized user preferences perform
comparably better for users providing few ratings and
popular items. Therefore, the relative performance depends on item popularity and rating effort. The reason why kNN Pearson correlation benefits more from
the rating effort is that the common rated item set is
generally bigger and hence, the Pearson correlation of
item rating vectors is more accurate. The reason why
hypothesized user preference similarity benefits more
from item popularity is that they do not stress preference similarity as much as Pearson correlation of item
rating vectors.

To conclude, hypothesized user preferences model
the users preferences better then item rating vectors
for sparse data and consequently perform significantly
better. Especially Naive Bayes approximates the user
preferences best and performs significantly best as
kNN Naive Bayes Pearson correlation by means of
M AE and RM SE for mostly all sparseness degrees.
Only for the sparseness degree of 0% (original Movie-

0510152002468101214161820 rating effort item popularity0.10.0500.050.10.15MAE diff.Metric
Precision

Recall

F1-Measure

Bouza and Bernstein / Partial User Preference Similarity as Classification-Based Model Similarity

Table 2

Filtering performance of relevant items.

Algorithm
J48

Naive Bayes
Bayes Net
Average

kNN Pearson corr..
kNN J48 class. sim.
kNN J48 Pearson corr.
kNN Naive Bayes class. sim.
kNN Naive Bayes Pearson corr.
kNN SMO Pearson corr.
kNN Partial J48 class. sim.

J48

Naive Bayes
Bayes Net
Average

kNN Pearson corr.
kNN J48 class. sim.
kNN J48 Pearson corr.
kNN Naive Bayes class. sim.
kNN Naive Bayes Pearson corr.
kNN SMO Pearson corr.
kNN Partial J48 class. sim.

J48

Naive Bayes
Bayes Net
Average

kNN Pearson corr.
kNN J48 class. sim.
kNN J48 Pearson corr.
kNN Naive Bayes class. sim.
kNN Naive Bayes Pearson corr.
kNN SMO Pearson corr.
kNN Partial J48 class. sim.

0%

10%

20%

30%

40%

50%

60%

70%

80%

90%

orative filtering approaches in a large experiment. It
shows that the presented similarity metrics using hypothesized user preferences significantly outperform
the compared collaborative filtering and content filtering approaches in cold-start situations with regards to
prediction accuracy of recommendations and filtering

performance. Furthermore, correlation-based classification similarity with Na ve Bayes as the user preference hypothesis learning algorithm performs similar to
the best traditional collaborative filtering approach in
non-sparse settings suggesting that it overall performs
well and is the most robust against data sparseness.

Bouza and Bernstein / Partial User Preference Similarity as Classification-Based Model Similarity

Table 3

AUC for different sparseness degrees.

Algorithm
J48

Naive Bayes
Bayes Net
Average

kNN Pearson corr.
kNN J48 class. sim.
kNN J48 Pearson corr.
kNN Naive Bayes class. sim.
kNN Naive Bayes Pearson corr.
kNN SMO Pearson corr.
kNN Partial J48 class. sim.

0%

10%

20%

30%

40%

50%

60%

70%

80%

90%

Partial user preference similarity is shown to perform
similarly as overall preference similarity with the same
underlying machine learning algorithm.

It is shown that the recommendation performance of
nearest neighbor-based collaborative filtering that retrieves like-minded users based on the the comparison
of user preference models depends primarily on item
popularity. The reason is that the set of users who rated
an item defines the set of potentially like-minded peo-
ple. The number of ratings a user provides, in contrast,
is secondary because machine learning algorithms tend
to take further ratings to verify and strengthen the current preference model.

Furthermore, nearest neighbor-based collaborative
filtering approaches are only able to recommend items
which have been at least rated ones. Indeed, the useful
recommendations are limited to items that have been
rated by like-minded users. Our approach, in contrast,
does not depend on commonly rated items but on a
similar hypothesis space, allowing the recommendation of items that have not been rated by like-minded
users resulting in a bigger coverage.

A limitation of the presented similarity metrics
is their dependency of the defined hypothesis space
which limits the accuracy of the hypothesized user
preferences. A trade-off exists between a large hypothesis space and hypothesized user preferences being a
partial function and, thus, not able to classify all items.
This limitation could be addressed by incorporating
domain ontologies, which define semantical relationships among item properties to hypothesize user pref-
erences. Here, general concepts of item properties are
used to learn the preferences of users with few ratings and more specific item property concepts are used

for users which provide many ratings using algorithms
that can exploit such ontological relationships such as
[7]. An additional source of ontological knowledge
about items is the Linked Data Cloud that could be
exploited analogously or even opportunistically to describe specific items in more detail or gather missing
information.

Second, users preferences are assumed to be stationary as well as users rating being truthfully. This
enables a valid computation of user preference similar-
ity.

Third, the presented similarity metrics depend strongly

on the set of rated items, which bias the comparison
when a user provides much more ratings then another
user. One approach to address this problem is to weight
every rating of a user by the users total number of ratings boosting the importance of ratings for users with
fewer ratings.

Fourth, since a recommender system may not know
all user ratings, a methodology needs to be developed
to compare user preferences solely based on the syntactic representation and semantic meaning of the hypothesized user preferences.

Fifth, further research should investigate a methodology to select the proper recommendation approach
for a given cold-start situation regarding to sparseness
degree and the three types of cold-start problems.

Last but not least, as it is shown, partial user preference similarity performs similar to overall preference
similarity. However, as discussed in [19], recommendation accuracy is only one quality aspect of a recommender system. Recommending diverse items may
lead to higher user satisfaction, a better user experi-
ence, and satisfy a users expectations. For that reason,

partial preference similarity needs further research regarding other quality aspects then simply recommendation accuracy.

In summary, the method presented herethe notion of using hypothesized user preferences by the
means of a machine learning algorithm to model user
preferences in the context of collaborative filtering
provides a highly promising avenue for improving the
performance of recommendation systems and paves
the way for the inclusion of background knowledge in
the form of ontologies via the use of ontology-aware
machine learning algorithms. In addition, it provides
the foundations for recommender systems in the Semantic Web to incorporate collaborative filtering by interpreting preferences from machine learning models.
As such, it provides an important building block for
further improving automated recommendations both
on the traditional, human and the semantic web.
