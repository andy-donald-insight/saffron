Semantic Web 0 (0) 1
IOS Press

Making Sense of Social Media Streams
through Semantics: a Survey

Editor(s): Andreas Hotho, University of Wurzburg, Germany
Solicited review(s): Harald Sack, Universitat Potsdam, Germany; Ashutosh Jadhav, Kno.e.sis Center, Wright State University, USA; one
anonymous reviewer

Kalina Bontcheva a, Dominic Rout a
a Department of Computer Science, University of Sheffield, Regent Court, 211 Portobello, Sheffield,
United Kingdom
E-mail: Initial.Surname@dcs.shef.ac.uk

Abstract.

Using semantic technologies for mining and intelligent information access to social media is a challenging, emerging research
area. Traditional search methods are no longer able to address the more complex information seeking behaviour in media streams,
which has evolved towards sense making, learning, investigation, and social search. Unlike carefully authored news text and
longer web context, social media streams pose a number of new challenges, due to their large-scale, short, noisy, context-
dependent, and dynamic nature.

This paper defines five key research questions in this new application area, examined through a survey of state-of-the-art
approaches for mining semantics from social media streams; user, network, and behaviour modelling; and intelligent, semanticbased information access. The survey includes key methods not just from the Semantic Web research field, but also from the
related areas of natural language processing and user modelling. In conclusion, key outstanding challenges are discussed and
new directions for research are proposed.

Keywords: semantic annotation, semantic-based user modelling, semantic search, information visualisation, social media streams

1. Introduction

The widespread adoption of social media is based
on tapping into the social nature of human interactions,
by making it possible for people to voice their opin-
ion, become part of a virtual community and collaborate remotely. If we take micro-blogging as an exam-
ple, Twitter has 100 million active users, posting over
230 million tweets a day1.

Engaging actively with such high-value, high-volume,

brief life-span media streams has now become a daily
challenge for both organisations and ordinary people.
Automating this process through intelligent, semantic-

*Corresponding author. E-mail: K.Bontcheva@dcs.shef.ac.uk.
1http://www.guardian.co.uk/technology/pda/2011/sep/08/twitter-

active-users (Visited May 7, 2012)

based information access methods is therefore increasingly needed. This is an emerging research area, combining methods from many fields, in addition to semantic technologies, namely natural language process-
ing, social science, machine learning, personalisation,
and information retrieval.

Traditional search methods are no longer able to address the more complex information seeking behaviour
in social media, which has evolved towards sense mak-
ing, learning and investigation, and social search [107].
Semantic technologies have the potential to help people cope better with social media-induced information overload. Automatic semantic-based methods that
adapt to individuals information seeking goals and
summarise briefly the relevant social media, could ultimately support information interpretation and decision
making over large-scale, dynamic media streams.

1570-0844/0-1900/$27.50 c 0  IOS Press and the authors. All rights reserved

Bontcheva and Rout / Semantics of Social Media Streams

Unlike carefully authored news and other textual
web content, social media streams pose a number of
new challenges for semantic technologies, due to their
large-scale, noisy, irregular, and social nature. In this
paper we discuss the following key research ques-
tions, examined through a survey of state-of-the-art ap-
proaches:

1. What ontologies and Web of Data resources can
be used to represent and reason about the semantics of social media streams?

2. How can semantic annotation methods capture

the rich semantics implicit in social media?

3. How can we extract reliable information from

these noisy, dynamic content streams?

4. How can we model users digital identity and so-

cial media activities?

5. What semantic-based information access methods can help address the complex information
seeking behaviour in social media?

To the best of our knowledge, this is the first comprehensive meta-review of semantic technology for
mining and intelligent information access, where the
focus is on current limitations and outstanding chal-
lenges, specifically arising in the context of social media streams.

The paper is structured as follows: section 2 provides background on social media, their different char-
acteristics, and the corresponding technological chal-
lenges. Section 3 focuses on ontologies which model
different kinds of social media, user profiles and net-
works, information sharing, and other typical social
media activities (research question 1). Section 4 discusses methods for semantic annotation of social media streams, in particular the ways in which they capture the rich implicit semantics (research question 2)
and deal with the noisy, streaming nature of this type
of content (research question 3). Section 5 investigates
in depth research question 4, i.e. how are users, net-
works, and activities modelled semantically and how
can this knowledge be used to personalise information
access. Next, section 6 analyses state-of-the-art in intelligent information access for social media streams,
in the context of research question 5. In conclusion,
section 7 defines outstanding challenges and provides
directions for future work.

2. Social Media Streams: Characteristics,

Challenges and Opportunities

Social media sites allow users to connect with each
other for the purpose of sharing content (e.g. web links,
photos, videos), experiences, professional information,
and online socialising with friends. Users create posts
or status updates and social media sites circulate these
to the users social network. The key difference from
traditional web pages is that users are not just passive
information consumers, but many are also prolific content creators.

Social media can be categorised on a spectrum,
based on the type of connection between users, how
the information is shared, and how users interact with
the media streams:

 Interest-graph media [115], such as Twitter, encourage users to form connections with others
based on shared interests, regardless of whether
they know the other person in real life. Connections do not always need to be reciprocated.
Shared information comes in the form of a stream
of messages in reverse chronological order.

 Social networking sites (SNS) encourage users
to connect with people they have real-life relationships with. Facebook, for example, provides
a way for people to share information, as well as
comment on each others posts. Typically, short
contributions are shared, outlining current events
in users lives or linking to something on the internet that users think their friends might enjoy.
These status updates are combined into a timeordered stream for each user to read.

 Professional Networking Services (PNS), such as
LinkedIn, aim to provide an introductions service in the context of work, where connecting to
a person implies that you vouch for that person
to a certain extent, and would recommend them
as a work contact for others. Typically, professional information is shared and PNS tend to attract older professionals [130].

 Content sharing and discussion services, such
as blogs, video sharing (e.g. YouTube, Vimeo),
slide sharing (e.g. SlideShare), and user discus-
sion/review forums (e.g. CNET). Blogs usually
contain longer contributions. Readers might comment on these contributions, and some blog sites
create a time stream of blog articles for followers
to read. Many blog sites also advertise automatically new blog posts through their users Facebook and Twitter accounts.

These different kinds of social media, coupled with
their complex characteristics, make semantic interpretation extremely challenging. State-of-the-art automatic semantic annotation, browsing, and search algorithms have been developed primarily on news articles
and other carefully written, long web content [20]. In
contrast, most social media streams (e.g. tweets, Facebook messages) are strongly inter-connected, tempo-
ral, noisy, short, and full of slang, leading to severely
degraded results2.

These challenging social media characteristics are
also opportunities for the development of new semantic technology approaches, which are better suited to
media streams:
Short messages (microtexts): Twitter and most Facebook messages are very short (140 characters
for tweets). Many semantic-based methods reviewed below supplement these with extra information and context coming from embedded
URLs and hashtags3. For instance, Abel et al [2]
augment tweets by linking them to contemporaneous news articles, whereas Mendes et al exploit
online hashtag glossaries to augment tweets [87].
Noisy content: social media content often has unusual spelling (e.g. 2moro), irregular capitalisation (e.g. all capital or all lowercase letters),
emoticons (e.g. :-P), and idiosyncratic abbreviations (e.g. ROFL, ZOMG). Spelling and capitalisation normalisation methods have been developed [57], coupled with studies of location-based
linguistic variations in shortening styles in microtexts [53]. Emoticons are used as strong sentiment
indicators in opinion mining algorithms (see Section 4.4).

Temporal: in addition to linguistic analysis, social
media content lends itself to analysis along temporal lines, which is a relatively under-researched
problem. Addressing the temporal dimension of
social media is a pre-requisite for much-needed
models of conflicting and consensual informa-
tion, as well as for modelling change in user
interests. Moreover, temporal modelling can be
combined with opinion mining, to examine the
volatility of attitudes towards topics over time.

2For instance, named entity recognition methods typically have

85-90% accuracy on news but only 30-50% on tweets [75,116].

3A recently study of 1.1 million tweets has found that 26% of English tweets contain a URL, 16.6%  a hashtag, and 54.8% contain
a user name mention [26].

Social context is crucial for the correct interpretation
of social media content. Semantic-based methods
need to make use of social context (e.g. who is the
user connected to, how frequently they interact),
in order to derive automatically semantic models
of social networks, measure user authority, cluster
similar users into groups, as well as model trust
and strength of connection.

User-generated: since users produce, as well as consume social media content, there is a rich source
of explicit and implicit information about the
user, e.g. demographics (gender, location, age,
etc.), interests, opinions. The challenge here is
that in some cases, user-generated content is relatively small, so corpus-based statistical methods
cannot be applied successfully.

Multilingual: Social media content is strongly multi-
lingual. For instance, less than 50% of tweets are
in English, with Japanese, Spanish, Portuguese,
and German also featuring prominently [26]. Un-
fortunately, semantic technology methods have
so far mostly focused on English, while lowoverhead adaptation to new languages still remains an open issue. Automatic language identification [26,10] is an important first step, allowing applications to first separate social media in
language clusters, which can then be processed
using different algorithms.

The rest of this paper discusses which of these challenges have been addressed by semantic technologies
to date and how.

3. Ontologies for Representing Social Media

Semantics

Ontologies are the corner stone of semantic technology applications. In this section we focus specifically
on ontologies created to model different kinds of social
media, user profiles, sharing, tagging, liking, and other
common user behaviour in social media. Table 1 provides an overview of these ontologies, alongside different dimensions, which are discussed in more detail
next:

Describing People and Social Networks: Friend-of-
a-Friend4 (FOAF) is a vocabulary for describing people, including names, contact information,

4http://xmlns.com/foaf/0.1/

Bontcheva and Rout / Semantics of Social Media Streams

Table 1

Ontologies and what they model

Ontology

People

SIOC(T)

Bottari

Yes
Yes

Yes
Yes
Yes
Yes

Online
posts

Social
networks
knows

Yes

Yes
Yes

Yes
Yes

Yes

Micro
blogs

Partial

Yes
Yes

User

Tags

Geo-

User

Interests
Partial
Yes

Yes
Yes
Yes

location

Behaviour

Yes
Yes
Yes

Yes

Yes

Yes
Yes

and a generic knows relation. FOAF also supports limited modelling of interests by modelling
them as pages on the topics of interest. As acknowledged in the FOAF documentation itself,
such an ontological model of interests is somewhat limited.

Modelling Social Media Sites: The Semantically Interlinked Online Communities5 (SIOC) ontology
models social community sites (e.g. blogs, wikis,
online forums). Key concepts are forums, sites,
posts, user accounts, user groups, and tags. SIOC
supports modelling of user interests through the
sioc:topic property, which has a URI as a
value (posts and user groups also have topics).

Modelling microblogs: SIOC has recent extensions
(SIOCT), modelling microblogs through the new
concept of MicroblogPost, a sioc:follows
property (representing follower/followee relationships on Twitter), and a sioc:addressed_to
property for posts that mention a specific user
name. Bottari [28] is an ontology, which has been
developed specifically to model relationships in
Twitter, especially linking tweets, locations, and
user sentiment (positive, negative, neutral), as extensions to the SOIC (Socially-Interlinked Online
Communities) ontology. A new TwitterUser class
is introduced, coupled with separate follower
and following properties, similar to those in
SIOCT. The Tweet class is a type of sioc:Post and,
unlike SIOCT, Bottari also distinguishes retweets
and replies. Locations (points-of-interest) are represented using the W3C Geo vocabulary6, which
enables location-based reasoning.

Interlinking Social Media, Social Networks, and

Online Sharing Practices: DLPO (The LivePost

Ontology) provides a comprehensive model of social media posts, going beyond Twitter [124]. It
is strongly grounded in fundamental ontologies,
such as FOAF, SOIC, and the Simple Knowledge
Organisation System (SKOS)7. DLPO models
personal and social knowledge discovered from
social media, as well as linking posts across personal social networks. The ontology captures six
main types of knowledge: online posts, different kinds of posts (e.g. retweets), microposts, online presence, physical presence, and online sharing practices (e.g. liking, favouriting). However,
while topics, entities, events, and time are well
covered, user behaviour roles and individual traits
are not addressed as comprehensively as in the
SWUM ontology [108] discussed below.

Modelling Tag Semantics: The MOAT (Meaning-
Of-A-Tag) ontology [101] allows users to define
the semantic meaning of a tag through Linking
Open Data and ultimately, to create manually semantic annotations of social media. The ontology defines two kinds of tags: global (across all
content) and local (particular tag on a given re-
source). MOAT can be combined with SIOCT
to tag microblog posts [100]. The DLPO on-
tology, introduce above, also models topics and
tags associated with online posts (including mi-
croblogs).

User modelling ontologies are key to the represen-
tation, aggregation, and sharing of information
about users and their social media interactions.
The General User Modelling Ontology (GUMO)
[59], for instance, aims to cover a wide range of
user-related information, such as demographics,
contact information, personality, etc. However, it

5http://sioc-project.org/
6http://www.w3.org/2003/01/geo/

7http://www.w3.org/2004/02/skos/. Developed to model thesauri,

term lists and controlled vocabularies.

falls short of representing user interests, which
makes it unsuitable for social media.
Based on an analysis of 17 social web applica-
tions, Plumbaum et al [108] have derived a number of user model dimensions required for a social
web user modelling ontology. Their taxonomy of
dimensions includes demographics, interests and
preferences, needs and goals, mental and physical state, knowledge and background, user be-
haviour, context, and individual traits (e.g. cognitive style, personality). Based on these, they have
created the SWUM (Social Web User Model) on-
tology. A key shortcoming of SWUM, however,
is its lack of grounding in other ontologies. For
instance, user location attributes, such as Country
and City, are coded as strings, which severely limits their usefulness for reasoning (e.g. it is hard to
find all users based in South West England, based
on their cities). A more general approach would
have been to define these through URIs, grounded
in commonly used Linked Data resources, such as
DBpedia and Freebase.
Lastly, the User Behaviour Ontology [6] models user interactions in online communities. It has
been used to model user behaviour in online forums [6] and also Twitter discussions [118]. It has
classes that model the impact of posts (replies,
comments, etc), user behaviour, user roles (e.g.
popular initiator, supporter, ignored), temporal
context (time frame), and other interaction infor-
mation. Addressing the temporal dimension of
social media is of particular important, especially
when modelling changes over time (e.g. in user
interests or opinions).

To summarise, there are a number of specialised on-
tologies, aimed at representing and reasoning with automatically derived semantic information from social
media. However, given that they address different phe-
nomena, many applications adopt or extend more than
one, in order to meet their requirements.

4. Semantic Annotation of Social Media

The process of tying semantic models and natural
language together is referred to as semantic annota-
tion. It may be characterised as the dynamic creation of
interrelationships between ontologies and unstructured
and semi-structured documents in a bidirectional manner [69]. From a technological perspective, semantic

annotation is about annotating in texts all mentions
of concepts from the ontology (i.e., classes, instances,
properties, and relations), through metadata referring
to their URIs in the ontology. Approaches which enhance the ontology with new instances derived from
texts are typically referred to as ontology population.
For an in-depth introduction to ontology-based semantic annotation from textual documents see [20].

The automatic semantic annotation of user-generated
content enables semantic-based search, browsing, fil-
tering, recommendation, and visual analytics (see Section 6), as well the building of semantic models of the
user, their social network, and online behaviour (see
Section 5). It is relevant in many application contexts,
e.g., knowledge management, competitor intelligence,
customer relation management, eBusiness, eScience,
eHealth, and eGovernment.

Semantic annotation can be performed manually,
automatically, or semi-automatically, i.e., first an automatic system creates some annotations and these are
then post-edited and corrected by human annotators.

In the context of social media, the Semantic Microblogging (SMOB) framework has been proposed
[100], in order to allow users to add manually machinereadable semantics to messages. SMOB supports also
interlinking with the LOD cloud, through hashtags.
Hepp [60] proposes a different manual semantic annotation syntax for tweet messages, which is then
mapped to RDF statements. The syntax supports relationships between tags (including sameAs), properties from ontologies such as FOAF, and multiple RDF
statements in the same tweet.

However, while such manual semantic annotation
efforts are valuable, automatic semantic annotation
methods are required, in order to make sense of the
millions of messages posted daily on Facebook, Twit-
ter, LinkedIn, etc. Consequently, in this section we focus primarily on automatic approaches.

Information Extraction (IE), a form of natural language analysis, is becoming a central technology for
bridging the gap between unstructured text and formal
knowledge expressed in ontologies. Ontology-Based
IE (OBIE) is IE which is adapted specifically for the
semantic annotation task [73]. One of the important
differences between traditional IE and OBIE is in the
use of a formal ontology as one of the systems inputs
and as the target output. Some researchers (e.g., [83])
call ontology-based any system which specifies its outputs with respect to an ontology, however, in our view,
if a system only has a mapping between the IE outputs

Bontcheva and Rout / Semantics of Social Media Streams

and the ontology, this is not sufficient and therefore,
such systems should be referred as ontology-oriented.

Another distinguishing characteristic of the ontologybased IE process is that it not only finds the (most spe-
cific) class of the extracted entity, but also identifies it,
by linking it to its semantic description in the target
knowledge base, typically via a URI. This allows entities to be traced across documents and their descriptions to be enriched during the IE process. In practical
terms, this requires automatic recognition of named
entities, terms, and relations and also co-reference resolution both within and across documents. These more
complex algorithms are typically preceded by some
shallow linguistic pre-processing (tokenisation, Part-
Of-Speech (POS) tagging, etc.)

Linking Open Data resources, especially DBpedia,
YAGO and Freebase, have become key sources of ontological knowledge for semantic annotation, as well
as being used as target entity knowledge bases for dis-
ambiguation. These offer: (i) cross-referenced domainindependent hierarchies with thousands of classes and
relations and millions of instances; (ii) an inter-linked
and complementary set of resources with synonymous
lexicalisations; (iii) grounding of their concepts and instances in Wikipedia entries and other external data.
The rich class hierarchies are used for fine-grained
classification of named entities, while the knowledge
about millions of instances and their links to Wikipedia
entries are used as features in the OBIE algorithms.

The rest of this section focuses specifically on meth-

ods for semantic annotation of social media streams.

4.1. Keyphrase Extraction

Automatically selected keyphrases are useful in representing the topic of a document or collection of doc-
uments, and less effective in delivering arguments or
full statements contained therein. Keyphrase extraction can therefore be considered as a form of shallow
knowledge extraction, giving a topical overview. Keywords can also be used in the context of semantic annotation and retrieval, as a means of dimensionality reduction and allowing systems to deal with smaller sets
of important terms rather than whole documents.

Some keyword extraction approaches exploit term
co-occurrence; forming a graph of terms with edges
derived from the distance between occurrences of a
pair of terms and assigning weights to vertices [91].
This class of keyword extraction was found to perform
favourably on Twitter data compared to methods which
relied on text models [142].

These graph-based approaches to extracting keywords from Twitter perhaps perform well because the
domain contains a great deal of redundancy [146]. For
example, in the context of trending topics on Twitter (frequently denoted by hashtags), [127] extracted
keyphrases by exploiting textual redundancy and selecting common sequences of words. While redundancy in Twitter and other social media is somewhat
beneficial when producing keyword summaries, another less helpful trait is the sheer variety of topics dis-
cussed. In cases where documents discuss more than
one topic, it can be more difficult to extract a coherent
and faithful set of keywords from it.

Personal Twitter timelines, when treated as single
documents, present this problem. Users are generally
capable of posting on multiple topics. While [142] use
TextRank on the whole of a users stream, they do
not attempt to model or address topic variation, unlike [143], who incorporated topic modelling into their
approach. Theirs is not the only application of Topic
Modelling to Twitter data, as it is similar to [114].
However in the latter work topics are discovered but
never summarised.

In the context of social tagging and bookmarking
services such as Flickr, Delicious, and Bibsonomy, researchers have studied the automatic tagging of new
documents with folksonomy tags. One of the early approaches is the AutoTag system [94], which assigns
tags to blog posts. First, it finds similar pre-indexed
blog posts using standard information retrieval meth-
ods, using the new blog post as the query. Then it
composes a ranked list of tags, derived from the top
most relevant posts, boosted with information about
tags used previously by the given blogger.

More recent approaches use keyphrase extraction
from blog content, in order to suggest new tags. For
instance, [111] generate candidate keyphrases from
n-grams, based on their POS tags, then filter these
using a supervised, logistic regression classifier. The
keyphrase-based method can be combined with information from the folksonomy [131], in order to generate tag signatures (i.e. associate each tag in the folksonomy with weighted, semantically related terms).
These are then compared and ranked against the new
blog post, in order to suggest the most relevant set of
tags.

Ontology-Based Semantic Annotation: Selected Research Tools

Table 2

DBpedia Spotlight [85]

LINDEN [128]

Ritter [116]
Ireson [64]

Laniado&Mika [72]

Meij [84]
Gruhl [54]
Rowe [119]

Choudhury [34]

Ontology/

LOD resource used
DBpedia, Freebase

Freebase
GeoPlanet
Freebase
Wikipedia
MusicBrainz

DBpedia
Wikipedia

Annotations
produced

Over 30 classes
YAGO classes

10 classes
Locations
Freebase
Wikipedia

Songs and albums
Conference-related

Cricket players, games

4.2. Ontology-Based Entity Recognition in Social

Media

Ontology-based entity recognition is often broken
down into two main phases: entity annotation (or candidate selection) and entity linking (also called reference disambiguation or entity resolution). Ontologybased entity annotation is concerned with identifying
all mentions in the text of classes and instances from
the ontology (e.g. DBpedia). The entity linking step
then uses contextual information from the text, as well
as knowledge from the ontology to choose the correct
URI. However, it must be noted that not all methods
carry out both steps, i.e. some only identify mentions
of entities in the text and their class [73].

Table 2 provides an overview of the various ap-

proaches discussed in more detail next.
4.2.1. Wikipedia-based Approaches

Most recent work on entity recognition and linking has used Wikipedia as a large, freely available
human-annotated training corpus. The target knowledge bases are typically DBpedia [85] or YAGO [128],
due to being derived from Wikipedia and thus offering
a straightforward mapping between an entity URI and
its corresponding Wikipedia page. These more recent,
ontology-based approaches have their roots in methods
that enrich documents with links to Wikipedia articles
(e.g. [93]).

Ontology-based entity disambiguation methods typically collect a dictionary of labels for each entity URI,
using the Wikipedia entity pages, redirects (used for
synonyms and abbreviations), disambiguation pages
(for multiple entities with the same name), and anchor
text used when linking to a Wikipedia page. This dictionary is used for identifying all candidate entity URIs
for a given text mention. Next is the disambiguation

Disamb.
performed

Target
domain

Corpora

Used

Yes
Yes
No
Yes
Yes
Yes
Yes
Yes
No

Open domain Wikipedia
Open domain Wikipedia
Open domain

Photos

Tweets
Flickr
Tweets

Open domain
Open domain Wikipedia
Music domain MySpace
Conferences
Tweets
Sport events

Wikipedia

Evaluated

On
News

TAC-KBP 2009

Tweets
Flickr
Tweets
Tweets

MySpace posts

200 tweets

Cricket tweets

stage, where all candidate URIs are ranked and a confidence score is assigned. If there is no matching entity in the target knowledge base, a NIL value is re-
turned. Text mentions can be disambiguated either independently of each other, or jointly across the entire
document (e.g. [93]).

Typically methods use Wikipedia corpus statistics
coupled with techniques (e.g. TF/IDF) which match
the context of the ambiguous mention in the text
against the Wikipedia pages for each candidate entity
(e.g. [85]). Michelson et al [90] demonstrate how such
an approach can be used to derive from a users tweets,
her/his topic profile, which is based on Wikipedia cat-
egories. The accuracy of these algorithms has so far
been evaluated primarily on Wikipedia articles and
news datasets, which are in nature very different from
the shorter messages in social media streams.

One widely used Wikipedia-based semantic annotation system is DBpedia Spotlight [85]. It is a freely
available and customisable web-based system, which
annotates text documents with DBpedia URIs. It targets the DBpedia ontology, which has more than 30 top
level classes and 272 classes overall. It is possible to
restrict which classes (and their sub-classes) are used
for named entity recognition, either by listing them explicitly or through a SPARQL query. The algorithm
first selects entity candidates through lookup against
a Wikipedia-derived dictionary of URI lexicalisations,
followed by a URI ranking stage using a vector space
model. Each DBpedia resource is associated with a
document, constructed from all paragraphs mentioning that concept in Wikipedia. The method has been
shown to out-perform OpenCalais and Zemanta (see
Section 4.2.3) on a small gold-standard of newspaper
articles [85].

Bontcheva and Rout / Semantics of Social Media Streams

Fig. 1. DBpedia Spotlight results on tweets

Figure 1 shows several tweets annotated with DBpedia Spotlight. The results clearly demonstrate the need
for tweet spelling normalisation, as well as the difficulties Spotlight has with recognising URLs. As exemplified here, by default the algorithm is designed to
maximise recall (i.e. annotate as many entities as pos-
sible, using the millions of instances from DBpedia).
Given the short, noisy nature of tweets, this may lead
to low accuracy results. Further formal evaluation on
a shared, large dataset of short social media messages
is required, in order to establish the best values for
the various DBpedia Spotlight parameters (e.g. confi-
dence, support).

The LINDEN [128] framework makes use of the
richer semantic information in YAGO (semantic sim-
ilarity), in addition to Wikipedia-based information
(using link structure for semantic associativity). The
method is heavily dependent on the Wikipedia-Miner8
toolkit [93], which is used to analyse the context of
the ambiguous entity mention and detect the Wikipedia
concepts that appear there. Evaluation on the TACKBP2009 dataset showed LINDEN outperforming the
highest ranked Wikipedia-only systems, which participated in the original TAC evaluation. Unfortunately,
LINDEN has not been compared directly to DBpedia
Spotlight on a shared evaluation dataset.

4.2.2. Social Media Oriented Approaches

Named entity recognition methods, which are typically trained on longer, more regular texts (e.g. news
articles), have been shown to perform poorly on
shorter and noisier social media content [116]. How-
ever, while each post in isolation provides insufficient
linguistic context, additional information can be derived from the user profiles, social networks, and interlinked posts (e.g. replies to a tweet message). This

8http://wikipedia-miner.cms.waikato.ac.nz/

section discusses what we call social media oriented
semantic annotation approaches, which integrate both
linguistic and social media-specific features.

Ritter et al [116] address the problem of named
entity classification (but not disambiguation) by using Freebase as the source of large number of known
entities. The straightforward entity lookup and type
assignment baseline, without considering context,
achieves only 38% f-score (35% of entities are ambiguous and have more than one type, whereas 30% of
entities in the tweets do not appear in Freebase). NE
classification performance improves to 66% through
the use of labelled topic models, which take into account the context of occurrence and the distribution
over Freebase types for each entity string (e.g. Amazon
can be either a company or a location).

Ireson et al [64] study the problem of location disambiguation (toponym resolution) of name tags in
Flickr. The approach is based on the Yahoo! GeoPlanet
semantic database, which provides a URI for each location instance, as well as a taxonomy of related locations (e.g. neighbouring locations). The tag disambiguation approach makes use of all other tags assigned to the photo, the user context (all tags assigned
by this user to all their photos), and the extended user
context, which takes into account the tags of the user
contacts. The use of this wider, social network-based
context was shown to improve significantly the overall
disambiguation accuracy.

Another source of additional, implicit semantics are
hashtags in Twitter messages, which have evolved as
means for users to follow conversations on a given
topic. Laniado and Mika [72] investigate hashtag semantics in 369 million messages, using four metrics:
frequency of use, specificity (use of the hashtag vs use
of the word itself), consistency of usage, and stability over time. These measures are then used to determine which hashtags can be used as identifiers and

Fig. 2. Zemantas online tagging interface

linked to Freebase URIs (most of them are named
entities). Hashtags have also been used as an additional source of semantic information about tweets, by
adding textual hashtag definitions from crowdsourced
online glossaries [87]. Mendes et al [87] also carry
out semantic annotation through a simple entity lookup
against DBpedia entities and categories without further disambiguation. User-related attributes and social
connections are coded in FOAF, whereas semantic annotations are coded through the MOAT ontology (see
Section 3).

Wikipedia-based entity linking approaches (see Section 4.2.1) benefit significantly from the larger linguistic context of news articles and web pages. Evaluation of DBpedia Spotlight [85] and the Milne and Witten method [93] on a tweet dataset has shown significantly poorer performance [84]. Meij et al [84] propose a Twitter-specific approach for linking such short,
noisy messages to Wikipedia articles. The first step
uses n-grams to generate a list of candidate Wikipedia
concepts, then supervised learning is used to classify
each concept as relevant or not (given the tweet and
the user who wrote it). The method uses features derived from the n-grams (e.g. number of Wikipedia articles containing this n-gram), Wikipedia article features
(e.g. number of articles linking to the given page), and
tweet-specific features (e.g. using hashtag definitions
and linked web pages).

Gruhl et al. [54] focus in particular on the disambiguation element of semantic annotation and examine
the problem of dealing with highly ambiguous cases,

as is the case with song and music album titles. Their
approach first restricts the part of the MusicBrainz ontology used for producing the candidates (in this case
by filtering out all information about music artists not
mentioned in the given text). Secondly, they apply
shallow language processing, such as POS tagging and
NP chunking, and then use this information as input
to a support vector machine classifier, which disambiguates on the basis of this information. The approach
was tested on a corpus of MySpace posts for three
artists. While the ontology is very large (thus generating a lot of ambiguity), the texts are quite focused,
which allows the system to achieve good performance.
As discussed by the authors themselves, the processing of less focused texts, e.g. Twitter messages or news
articles, is likely to prove much more challenging.

4.2.3. Commercial Entity Recognition Services

There are a number of commercial online entity
recognition services which annotate documents with
entities and assign Linked Data URIs to them. The
NERD online tool [117] allows their easy comparison
on user-uploaded datasets. It also unifies their results
and maps them to the Linking Open Data cloud. Here
we focus only on the services used by research methods surveyed here (e.g. [121,2,119]).

Zemanta (http://www.zemanta.com) is an online semantic annotation tool, originally developed for blog
and email content to help users insert tags and links
through recommendations. Figure 2 shows an example text and the recommended tags, potential in-text

Bontcheva and Rout / Semantics of Social Media Streams

Fig. 3. Calais results on tweets

link targets (e.g., the W3C Wikipedia article and the
W3C home page), and other relevant articles. It is then
for the user to decide which of the tags should apply
and which in-text link targets they wish to add. In this
example, in-text links have been added for the terms
highlighted in orange, all pointing to the Wikipedia articles on the respective topics.

Open Calais is another commercial web service for
semantic annotation, which has been used by some
researchers on social media. For instance, Abel et al
[2] harness OpenCalais to recognise named entities in
news-related tweets9. The target entities are mostly lo-
cations, companies, people, addresses, contact num-
bers, products, movies, etc. The events and facts extracted are those involving the above entities, e.g.,
acquisition, alliance, company competitor. Figure 3
shows an example text annotated with some entities.

The entity annotations include URIs, which allow
access via HTTP to obtain further information on that
entity via Linked Data. Currently OpenCalais links to
eight Linked Data sets, including its own knowledge
base, DBpedia, Wikipedia, IMDB and Shopping.com.
These broadly correspond to the entity types covered
by the ontology.

The main limitation of Calais comes from its proprietary nature, i.e., users send documents to be annotated by the web service and receive results back, but
they do not have the means to give Calais a different

9Unfortunately they do not evaluate the named entity recognition

accuracy of OpenCalais on their dataset.

ontology to annotate with or to customise the way in
which the entity extraction works.

4.3. Event Detection

Much as trending topics can be used to monitor
global opinions and reactions, social media streams
can be used as a discussion backchannel to real world
events [41], and even to discover and report upon such
events, almost as soon as they occur. While it may at
first appear that trending topics alone are sufficient for
this task, there are a few reasons why they are unsatis-
factory:

 Generality: trending topics may discuss events,
but may also refer to celebrities, products or online memes.

 Scale: only the topics with which a huge margin
of Twitter users engage can appear as trending
topics.

 Censorship: it is believed by many that the trending topics displayed by the official Twitter service
are censored for political and language content.

 Algorithm: the method used to select trending
topics is not published anywhere and is generally
not understood.

Automatic event detection therefore presents an interesting task for social media streams. While it is possible to have access to an enormous quantity of tweets,
enough to reveal global trends and events, the problem of developing and evaluating scalable event detection algorithms which can handle such magnitudes of
streaming text remains.

The majority of approaches to event detection do not
utilise ontologies or other sources of semantic infor-
mation. One class of methods use clustering on tweets
[105,14,15] or blog posts [123]. Another class of event
detection methods take inspiration from signal pro-
cessing, analysing tweets as sensor data. [122] used
such an approach to detect earthquakes in Japan on the
basis of Tweets with geolocation information attached
to them. Similarly, individual words have been treated
as wavelet signals and analysed as such in order to discover temporally significant clusters of terms [141].

Once an event is detected in social media streams,
the-
the next problem is how to generate useful
matic/topical descriptors for this event. Point-wise mutual information has been coupled with user geolocation and temporal information, in order to derive n-
gram event descriptors from tweets [96]. By making
the algorithm sensitive to the originating location, it is
possible to see what people from a given location are
saying about an event (e.g. those in the US), as well
as how this differs from tweets elsewhere (e.g. those
from India).

Collections of events in a larger sequence could
be referred to as sagas; they may be perfectly legitimate events in their own right, or their individual constituents might similarly be coherent on their own.
Citing the example of an academic conference, [119]
point out that tweets may refer to the conference as a
whole, or to specific sub-events such as presentations
at a specific time and place. Using semantic information about the conference event and its sub-events from
the Web of Data, tweets are aligned to these sub-events
automatically, using machine learning. The method includes a concept enrichment phase, which uses Zemanta to annotate each tweet with DBpedia concepts.
Tweets are described semantically using the SIOC and
Online Presence semantic ontologies (see Section 3).
Another semantic, entity-based approach to subevent detection has been proposed by [34], who use
manually created background knowledge about the
event (e.g. team and player names for cricket games),
coupled with domain-specific knowledge from Wikipedia (e.g. cricket-related sub-events like getting out).
In addition to annotating the tweets with this semantic information, the method utilises tweet volume (sim-
ilarly to [78]) and re-tweet frequency as sub-event
indicators. The limitation of this approach, however,
comes from the need for manual intervention, which is
not always feasible outside of limited application do-
mains.

4.4. Sentiment Detection and Opinion Mining

The existence and popularity of websites dedicated
to reviews and feedback on products and services is
something of a homage to the human urge to post what
they feel and think online. When the most common
type of message on Twitter is about me now [95], it
is to be expected that users talk often about their own
moods and opinions. Bollen et al [19] argue that users
express both their own mood in tweets about themselves and more generally in messages about other
subjects. Another study [66] estimates that 19% of microblog messages mention a brand and from those that
do, around 20% contain brand sentiment.

The potential value of these thoughts and opinions
is enormous. For instance, mass analysis could provide
a clear picture of overall mood, exploring reactions to
ongoing public events [19] or feedback to a particular
individual, government, product or service [71]. The
resulting information could be used to improve ser-
vices, shape public policy or make a profit on the stock
market.

The user activities on social networking sites are
often triggered by specific events and related entities
(e.g. sports events, celebrations, crises, news articles,
persons, locations) and topics (e.g. global warming, financial crisis, swine flu). In order to include this in-
formation, semantically- and social network-aware approaches are needed.

There are many challenges inherent in applying typical opinion mining and sentiment analysis techniques
to social media [81]. Microposts are, arguably, the
most challenging text type for opinion mining, since
they do not contain much contextual information and
assume much implicit knowledge. Ambiguity is a particular problem since we cannot easily make use of
coreference information: unlike in blog posts and com-
ments, tweets do not typically follow a conversation
thread, and appear much more in isolation from other
tweets. They also exhibit much more language vari-
ation, tend to be less grammatical than longer posts,
contain unorthodox capitalisation, and make frequent
use of emoticons, abbreviations and hashtags, which
can form an important part of the meaning. Typically,
they also contain extensive use of irony and sarcasm,
which are particularly difficult for a machine to detect.
On the other hand, their terseness can also be beneficial in focusing the topics more explicitly: it is very
rare for a single tweet to be related to more than one
topic, which can thus aid disambiguation by emphasising situational relatedness.

Bontcheva and Rout / Semantics of Social Media Streams

[99] present a wide-ranging and detailed review of
traditional automatic sentiment detection techniques,
including many sub-components, which we shall not
repeat here. In general, sentiment detection techniques
can be roughly divided into lexicon-based methods
(e.g. [125,135]) and machine-learning methods, e.g.
[18]. Lexicon-based methods rely on a sentiment lexi-
con, a collection of known and pre-compiled sentiment
terms. Machine learning approaches make use of shallow syntactic and/or linguistic features [98,52], and
hybrid approaches are very common, with sentiment
lexicons playing a key role in the majority of methods,
e.g. [38].

The majority of sentiment and opinion mining methods tested on social media utilise no or very little se-
mantics. For instance, [98,52] classify tweets as having positive, negative, or neutral sentiment, based on
n-grams and part-of-speech information, whereas [38]
use a sentiment lexicon to initially annotate positive
and negative sentiment in tweets related to political
events.

The use of such shallow linguistic information leads
to a data sparsity problem. Saif et al [121] demonstrate
that by using semantic concepts, instead of words such
as iPhone, polarity classification accuracy is improved.
The approach uses AlchemyAPI for semantic annotation of 30 entity classes, the most frequent ones being Person, Company, City, Country, and Organisation.
The method is evaluated on the Stanford Twitter Sentiment Dataset10 and shown to outperform semantics-
free, state-of-the-art methods, including [52].

Semantic annotation has also been used for the more
challenging opinion mining task. In particular, [79]
identify people, political parties, and opinionated statements in tweets using a rule-based entity recogniser,
coupled with an affect lexicon derived from WordNet.
Subsequent semantic analysis uses patterns to generate
triples representing opinion holders and voter inten-
tions. Negation is dealt with by capturing simple patterns such as isnt helpful or not exciting and using
them to negate the extracted sentiment judgements.

4.5. Cross-Media Linking

The short nature of Twitter and Facebook messages,
coupled with their frequent grounding in real world
events, means that often short posts cannot be understood without reference to external context. While

10http://twittersentiment.appspot.com/

some posts already contain URLs, the majority do not.
Therefore automatic methods for cross-media linking
and enrichment are required.

Abel et al [2] link tweets to current news stories
in order to improve the accuracy of semantic annotation of tweets. Several linkage strategies are explored:
utilising URLs contained in the tweet, TF-IDF similarity between tweet and news article, hashtags, and
entity-based similarity (semantic entities and topics
are recognised by OpenCalais), with the entity-based
one being the best one for tweets without URLs. The
approach bears similarities with the keyphrase-based
linking strategy for aligning news video segments with
online news pages [42]. [62] go one step further by aggregating social media content on climate change from
Twitter, YouTube, and Facebook with online news, although details of the cross-media linking algorithm are
not supplied in the paper.

An in-depth study comparing Twitter and New York
Times news [148] has identified three types of topics:
event-oriented, entity-oriented, and long-standing top-
ics. Topics are also classified into categories, based on
their subject area. Nine of the categories are those used
by NYT (e.g. arts, world, business) plus two Twitterspecific ones (Family&Life and Twitter). Family&Life
is the predominant category on Twitter (called me
now by [95]), both in terms of number of tweets and
number of users. Automatic topic-based comparison
showed that tweets abound with entity-oriented topics,
which are much less covered by traditional news me-
dia.

Going beyond news and tweets, future research on
cross-media linking is required. For instance, some
users push their tweets into their Facebook profiles,
where they attract comments, separate from any tweet
replies and retweets. Similarly, comments within a
blog page could be aggregated with tweets discussing
it, in order to get a more complete overall view.

4.6. Discussion

Even though some inroads have been made already,
current methods for semantic annotation of social media streams have many limitations. Firstly, most methods address the more shallow problems of keyword
and topic extraction, while ontology-based entity and
event recognition do not reach the significantly higher
precision and recall results obtained on longer text
documents. One way to improve the currently poor automatic performance is through crowdsourcing. The
ZenCrowd system [37], for instance, combines algo-

rithms for large-scale entity linking with human input
through micro-tasks on Amazon Mechanical Turk. In
this way, textual mentions that can be linked automatically and with high confidence to instances in the LOD
cloud, are not shown to the human annotators. The latter are only consulted on hard to solve cases, which not
only significantly improves the quality of the results,
but also limits the amount of manual intervention re-
quired. We return to crowdsourcing in more detail in
Section 7.

Another way to improve semantic annotation of social media is to make better use of the vast knowledge
available on the Web of Data. Currently this is limited
mostly to Wikipedia and resources derived from it (e.g.
DBpedia and YAGO). One of the challenges here is
ambiguity. For instance, song and album titles in MusicBrainz are highly ambiguous and include common
words (e.g. Yesterday), as well as stop words (The,
If) [54]. Consequently, an automatic domain categorisation step might be required, in order to ensure that
domain-specific LOD resources, such as MusicBrainz,
are used to annotate only social media content from
the corresponding domain. The other major challenges
are robustness and scalability. Firstly, the semantic annotation algorithms need to be robust in the face of
noisy knowledge in the LOD resources, as well as being robust with respect to dealing with the noisy, syntactically irregular language of social media. Secondly,
given the size of the Web of Data, designing ontologybased algorithms which can load and query efficiently
these large knowledge bases, while maintaining high
computational throughput is far from trivial.

The last obstacle to making better use of Web of
Data resources, lies in the fairly limited lexical information available. With the exception of resources
grounded in Wikipedia, lexical information in the rest
is mostly limited to RDF labels. This in turn limits
their usefulness as a knowledge source for ontologybased information extraction and semantic annotation.
One recent strand of work has focused on utilising the
Wiktionary [89] collaboratively built, multilingual lexical resources. It is particularly relevant to analysing
user-generated content, since it contains many neulogisms and is updated continuously by its contributor community. For English and German, in particular,
there is also related ongoing work on creating UBY
[55]  a unified, large-scale, lexico-semantic resource,
grounded in Wikipedia and Wordnet, and thus, indi-
rectly, to other LOD resources as well. Another relevant strand is work on linguistically grounded ontologies [22], which has proposed a more expressive model

for associating linguistic information to ontology ele-
ments. While these are steps in the right direction, nevertheless further work is still required, especially with
respect to building multilingual semantic annotation
systems.

In addition, it is axiomatic that semantic annotation
methods are only as good as their training and evaluation data. Algorithm training on social media gold
standard datasets is currently very limited. For exam-
ple, there are currently fewer than 10,000 tweets annotated with named entity types and events. Bigger,
shared evaluation corpora from different social media genres are therefore badly needed. Creating these
through traditional manual text annotation methodologies is unaffordable, if a significant mass is to be
reached. Research on crowdsourcing evaluation gold
standards has been limited, primarily with focus on using Amazon Mechanical Turk to acquire small datasets
(e.g. tweets with named entity types) [47]. We will revisit this challenge again in Section 7.

In the area of sentiment analysis, researchers have
investigated the problems of sentiment polarity detec-
tion, subjectivity classification, prediction through social media and user mood profiling, however, most
methods use no or very little semantics. Moreover,
evaluation of opinion mining is particularly difficult
for a number of methodological reasons (in addition
to the lack of shared evaluation resources discussed
above). First, opinions are often subjective, and it is
not always clear what was intended by the author. For
example, a person cannot necessarily tell if a comment
such as I love Baroness Warsi, in the absence of further context, expresses a genuine positive sentiment or
is being used sarcastically. Inter-annotator agreement
performed on manually annotated data therefore tends
to be low, which affects the reliability of any gold standard data produced.

Lastly, social media streams impose a number of
further outstanding challenges on opinion and sentiment mining methods:

 Relevance: In social media, discussions and comment threads can rapidly diverge into unrelated
topics, as opposed to product reviews which
rarely stray from the topic at hand.

 Target identification: There is often a mismatch
between the topic of the social media post, which
is not necessarily the object of the sentiment
held therein. For example, the day after Whitney Houstons death, TwitterSentiment and similar sites all showed an overwhelming majority

Bontcheva and Rout / Semantics of Social Media Streams

of tweets about Whitney Houston to be negative;
however, almost all these tweets were negative
only in that people were sad about her death, and
not because they disliked her.

 Volatility over time: More specifically, opinions
can change radically over time, from positive to
negative and vice versa. To address this prob-
lem, the different types of possible opinions can
be associated as ontological properties with the
classes describing entities, facts and events, discovered through semantic annotation techniques,
similar to those in [80] which aimed at managing
the evolution of entities over time. The extracted
opinions and sentiments can be time-stamped and
stored in a knowledge base, which is enriched
continuously, as new content and opinions come
in. A particularly challenging question is how to
detect emerging new opinions, rather than adding
the new information to an existing opinion for
the given entity. Contradictions and changes also
need to be captured and used to track trends over
time, in particular through opinion aggregation.

 Opinion aggregation: Another challenge is the
type of aggregation that can be applied to opin-
ions. In entity-based semantic annotation, this
can be applied to the extracted information in a
straightforward way: data can be merged if there
are no inconsistencies, e.g. on the properties of
an entity. Opinions behave differently here, how-
ever: multiple opinions can be attached to an entity and need to be modelled separately, for which
we advocate populating a knowledge base. An
important question is whether one should just
store the mean of opinions detected within a specific interval of time (as current opinion visualisation methods do), or if more detailed approaches
are preferable, such as modelling the sources and
strength of conflicting opinions and how they
change over time. A second important question
in this context involves finding clusterings of the
opinions expressed in social media, according to
influential groups, demographics and geographical and social cliques. Consequently, the social,
graph-based nature of the interactions requires
new methods for opinion aggregation.

However, even though state-of-the-art methods have
a large scope for improvement, semantic annotation results are already being used by methods that automatically derive models of users and social networks, from
the information implicit in social media streams. This
is where we turn to next.

5. Semantic-Based User Modelling

A User Model (UM) is a knowledge resource containing explicit semantic information about various aspects of the user, which the system has a priori (e.g. by
importing a Facebook profile) or has inferred from user
behaviour, user-generated content, social networks or
other sources. Some important characteristics of user
models are:

 UM is a distinct knowledge resource within the

overall system;

 semantic information is represented explicitly.
Implicit information disclosed in social media is
used to derive this explicit knowledge.

 abstraction, i.e., representation of types of users,

roles and groups, as well as of individual users.

 multi-purpose  the semantically encoded user
model can be used in different ways, e.g. personalised content recommendation, filtering.

 reasoning  the representation should allow for
reasoning about the knowledge, as well as reasoning with it.

 interconnected  a user model is more than a collection of attributes. Usually, there are also complex relations between them, as well as relations
to other types of knowledge (e.g. posts made by
the user).

Ontology-based user models have been used extensively on content other than social media streams, especially in the context of Personal Information Management (PIM). PIM work originated in research on
the social semantic desktop [36], where information
from the users computer (e.g. email, documents) is
used to derive models of the user. For a detailed
overview of user modelling for the semantic web see
[8].

In this paper, we focus on the extension of this
work towards social media streams, as well as mention
sensor-based information where relevant (e.g. GPS coordinates in tweets). As discussed in Section 2, the social and user-generated nature of these streams make
it possible to derive rich semantic user models. More
specifically, we examine the application of semantic
annotation for user model construction. Consequently,
we consider outside the scope of this paper research
which is focused purely on social network analysis
(e.g. [92]) and/or uses purely quantitative user and post
characteristics (e.g. number of threads/posts, number
of replies/re-tweets [118]) and/or post metadata only
(e.g. the re-tweet and in-reply-to JSON fields).

5.1. Constructing Social Semantic User Models from

Semantic Annotations

Among the various kinds of social media, folksonomies have probably received most attention from
researchers studying how semantic models of user
interactions and interests can be derived from usergenerated content. Many approaches focused on exploring the social and interaction graphs, using techniques from social network analysis (e.g. [92]). In this
section, however, we are concerned with methods that
discover and exploit the semantics of textual tags instead (including hashtags). This section also includes
semantic-based user modelling research on online fo-
rums, blogs, and Twitter.

Based on the kinds of semantic information used,

methods can be classified as follows:

 Bag of words ([31]);
 Semantically disambiguated entities: mentioned
by user (e.g. [2,67]) or from a linked longer Web
document (e.g. [2]);

 Topics: Wikipedia categories (e.g. [2,133]), latent
topics (e.g. [147]), or tag hierarchies (e.g. [149]).
In order to model tag semantics more explicitly,
researchers [27] have proposed grounding tags
into WordNet and then using WordNet-based semantic similarity measures, to derive the semantic relatedness of folksonomy tags.

This is typically supplemented with more quantitative
social network information (e.g. how many connec-
tions/followers a user has [6]) and interaction information (e.g. post frequency [118], average number of
posts per thread [6]).

The rest of the section discusses in more detail the
kinds of user information that have been extracted
from semantically annotated social media, and concludes with a discussion of open issues.

5.1.1. Discovering User Demographics

Every Twitter user has a profile which reveals some
details of their identity. The profile is semi-structured,
including a textual bio field, a full name, the users lo-
cation, a profile picture, a time zone and a homepage
URL (most of these are optional and often empty).
The users attributes can be related to the content of
their posts, for example their physical location can determine to a degree the language they use [33] or the
events on which they comment [144].

There have been efforts to discover user demographics information, when it is not available in the fields

in their profile. [23] classify users as male or female
based on the text of their tweets, their description fields
and their names. They report better-than-human accu-
racy, compared to a set of annotators on Mechanical
Turk. [103] present a general framework for user classification which can learn to automatically discover
political alignment, ethnicity and fans of a particular
business.

Twitter users may share the location from which
they are tweeting by posting from a mobile device and
allowing it to attach a reading from its GPS receiver to
the message, or by setting their own location in a field
of their profile. However, [33] found that only around
36% of users actually filled in their location field in
their profile with a valid location as specific as their
nearest city. Furthermore, when we analysed a dataset
of over 30,000 tweets discussing the 2011 London Ri-
ots, less than 1% of messages contained any GPS in-
formation.

There have been attempts to automatically locate
Twitter users. Content-based methods (you are where
you write about) typically gather the textual content
produced by the user and infer their location based
on features, such as mentions of local place names
[48] and use of local dialect. In the work of [44,33],
region-specific terms and language that might be relevant to the geolocation of users were discovered auto-
matically. A classification approach is devised in [77]
that also incorporates specific mentions of places near
to the user. One disadvantage to this method is the fact
that someone might be writing about a popular global
event which is of no relevance to their actual location.
Another is that users might take deliberate steps to hide
their true location by alternating the style of their posts
or not referencing local landmarks.

In contrast, network-based geolocation methods
(you are where your friends are) aim to use the users
social network to infer their location. To the best of our
knowledge, the only existing method of this kind (i.e.,
relying on the users social network alone) is the work
of [9], who first create a model for the distribution of
distance between pairs of friends, before using this to
find the most likely location for a given user. The influence of distance on social network ties is demonstrated by the earlier work of [74]. The main disadvantage of their approach is that it assumes that all users
globally have the same distribution of friends in terms
of distance. Also, they do not account explicitly for the
density of people in an area.

We have developed a method for collecting a large
dataset of users with known, ground truth locations,

Bontcheva and Rout / Semantics of Social Media Streams

which is based on semantically disambiguating the
user defined location field in their Twitter profile by
assigning the corresponding DBpedia URI. A thorough analysis of how users use this field is presented in
[58]. Other work has relied instead on small amounts
of geotagged data (e.g FourSquare checkins) as an extra feature for user location or as the only way of locating users. The work of [77] uses these checkins as part
of their classification. Although this often leads to high
accuracy results, the approach is limited by the very
small amount of geo-tagged data available, mainly due
to practical constraints (e.g. battery usage) and privacy
concerns.

5.1.2. Deriving User Interests

Abel et al [2] propose simple entity-based and topicbased user profiles, built from the users tweets. The
entity-based profile for a given user is modelled as a
set of weighted entities, where the weight each entity e
is computed based either on the number of user tweets
that mention e, or based on frequency of entity occurrences in the tweets, combined with the related news
articles (which are identified in an earlier, linking step).
Topic-based profiles are defined in a similar fashion,
but represent higher level Wikipedia categories (e.g.
sports, politics). Both entities and topics are identified
using OpenCalais (see Section 4.2.3). Abel et al have
also demonstrated that hashtags are not a useful indicator of user interests  a finding which is also supported by [90]. A major limitation of the method is that
it depends heavily on the news linking, which the authors have shown applies successfully to only 15% of
tweets.

In a subsequent paper [3], Abel et al refine their approach to modelling user interests in a topic, to take
also into account re-tweets, as well as changes over
time (when users become interested in a topic, for how
long, and which concepts are relevant to which topic).
Evaluation is based on global topics (e.g. the Egyptian
revolution). Their findings demonstrate that a timedependent topic weighting function produces user interest models, which are better for tweet recommendation purposes. They also identify different groups of
users, based on the duration of their interest in a given
topic: long-term adopters who join early for longer vs.
short-term adopters who join global discussions later
and are influenced by public trends.

Kapanipathi et al [67] similarly use semantic annotations to derive user interests (entities or concepts from DBpedia), weighted by strength (calculated
on the basis of frequency of occurrence). They also

demonstrate how interests can be merged based on information from different social media (LinkedIn, Facebook and Twitter). Facebook likes and explicitly stated
interests in LinkedIn and Facebook are combined with
the implicit interest information from the tweets. The
Open Provenance Model11 is used to keep track of interest provenance.

A similar entity- and topic-based approach to modelling user interests is proposed by Michelson and
Macskassy [90] (called Twopics). All capitalised, nonstop words in a tweet are considered as entity candidates and looked up against Wikipedia (page titles
and article content). A disambiguation step then identifies the Wikipedia entity which matches best the candidate entity from the tweet, given the tweet content
as context. For each disambiguated entity, the subtree of Wikipedia categories is obtained. In a sub-
sequent, topic-assignment step, all category sub-trees
are analysed to discover the most frequently occurring categories, which are then assigned as user interests in the topic-based profile. The authors also argue
that such more generic topics, generated by leveraging the Wikipedia category taxonomy, are more appropriate for clustering and searching for users, than the
term-based topic models derived using bag-of-words
or LDA methods.

Researchers have also investigated the problem of
deriving user interests from tags and other metadata in
folksonomies. For instance, [17] build a shallow model
of user interests from the user-created folksonomy tags
and the words appearing in other user-authored metadata (e.g. title, description). This user profile is then
used to recommend items from the folksonomy (e.g.
Del.icio.us URLs, Bibsonomy articles).

[147] propose a topic-based probabilistic method
for identifying user interests from folksonomy tags
(Del.icio.us). The first step is to induce hierarchies of
latent topics from a set of tags in an unsupervised man-
ner. This approach, based on Hierarchical Dirichlet
Process, models topics as probability distributions over
the tag space, rather than clustering the tags themselves
[149]. Next, user interest hierarchies are induced via
log-likelihood and hierarchy comparison methods. Zavitsanos et al however stop short of assigning explicit
semantics to the topics through URIs.

In order to ground folksonomy tags semantically,
Cattuto et al [27] mapped pairs of tags in Del.icio.us
to pairs of synsets in Wordnet. Then WordNet-based

11http://openprovenance.org

measures for semantic distance are used to derive semantic relations between the mapped tags. The researchers demonstrated that a semantically-sound and
computationally affordable metric for semantic similarity between tags is the tag context similarity, which
measures the tags co-occurrence with the 10,000 most
popular tags in the folksonomy. In this way, tags which
belong to the same semantic concept can be identi-
fied. The same approach could, in theory, be applied to
hashtags in tweets, although its effectiveness remains
to be proven.

Others [133] have used Wikipedia as a multi-domain
model, that can be used to model semantically user
interests. They also propose a method for consolidation of user profiles across social networking sites.
Tags from different sites are filtered based on WordNet
synonymy and correlated to Wikipedia pages. Subse-
quently, Wikipedia categories are used, in order to select representative higher-level topics of interest for the
user. The approach is very similar to Twopics [90].

Researchers have also demonstrated a link between
the kinds of tags and content created and user behaviour categories (see Section 5.1.3), which has direct implications on how well we can derive user interests and/or recommend content. In the context of capturing tag semantics in folksonomies, previous work
[70,132] has shown that different users of the same social tagging system can have different tagging motivation (categorisation vs description), which in turn influences the kinds of tags entered in the system. In par-
ticular, for the purposes of discovering emergent tag
semantics, it is more beneficial to use as input the more
prolific and descriptive tags produced by describer
users. Going beyond tags, Naaman et al [95] have
shown that the two different categories of Twitter users
(meformers and informers) produce significantly different kinds of tweet content. For instance, informers
post primarily information sharing messages, whereas
meformers write mainly about themselves and voice
opinions and complaints.

5.1.3. Capturing User Behaviour

As demonstrated above, user behaviour is key to understanding interactions in social media. In this section we focus primarily on approaches which utilise
automatically-derived semantics, in order to classify
user behaviour.

In the case of online forums, the following user behaviour roles have been identified [29]: elitist, grunt,
joining conversationalist, popular initiator, popular
participant, supporter, taciturn, and ignored. For so-

cial tagging systems, researchers [132] have classified
users according to their tagging motivation, into categorisers and describers. In Twitter, the most common
role distinction is drawn on the basis of tweet content
and users are classified into meformers (80% of users)
and informers (20% of users) [95].

In order to assign behaviour roles in online forums
automatically, Angeletou et al [6] create skeleton rules
in SPARQL, that map semantic features of user interaction to a level of behaviour (high, medium, and low).
These levels are constructed dynamically from user
exchanges and can be altered over time, as the communities evolve. User roles, contexts, and interactions
are modelled semantically through the User Behaviour
Ontology (see Section 3) and are used ultimately to
predict the health of a given online forum.

The problem of characterising Twitter user be-
haviour, based on the content of their posts has yet
to be fully explored. [143] generated keyphrases for
users with the aid of topic modelling and a PageRank
method. Similarly, [142] use a combination of POS
filtering and TextRank to discover tags for users. It
should also be noted that while [95] went some way towards categorising user behaviour and tweet intention,
their method is not automatic and it remains unclear
whether or not similar categories could be assigned by
a classifier.

5.2. Discussion

As demonstrated by our survey, a key research challenge for semantic user modelling lies in addressing the diverse, dynamic, temporal nature of user be-
haviour. An essential part of that is the ability to represent and reason with conflicting personal views, as
well as to model change in user behaviour, interests,
and knowledge over time. For instance, in the context of blogs, Cheng et al [32] have proposed an interest forgetting function for short-term and long-term
interest modelling. Angeletou et al [6] recently developed time-contextualised models of user behaviour
and demonstrated how these could be used to predict
changes in user participation in online forums.

With respect to tweets, automatically derived user
interests could also be separated into global ones
(based on the users tweets on trending topics) versus user-specific (topics which are of more personal
interest, e.g. work, hobby, friends). Further work is
required on distinguishing globally interesting topics
(e.g. trending news) from interests specific to the given
user (e.g. work-related, hobby, gossip from a friend,

Bontcheva and Rout / Semantics of Social Media Streams

etc.). In other words, we need to go beyond modelling
what is interesting to a user, to capture also why it is
of interest. In more detail, a given tweet could be interesting to a user for social reasons (e.g. my brother
posted them), cultural reasons, topical relevance (e.g.
match my hobby), or be part of a larger sequence of
tweets, forming a conversation. Current methods (see
Section 5.1.2) have largely focused on topically relevant tweets, leaving ample scope for future research.

There is also need for further research into modelling how user interests change over time, going beyond the work of Abel et al [3], which focused on
global topics derived from tweets linked to political news. One challenge is to establish how well the
method generalises to tweets from domains other than
news, as well as tweets without URLs12 The latter
are likely to prove particularly challenging, since the
method most likely benefits significantly from the ad-
ditional, longer content of the URL. Moreover, as already discussed, tweets can be interesting for a number
of reasons, other than global importance. We hypothesize that the interestingness of a given tweet is likely
to decay or change differently over time, depending on
the reason(s) that make it interesting to a given user.

What is interesting to a user also ties in with user behaviour roles (see Section 5.1.3). In turn, this requires
more sophisticated methods for automatic assignment
of user roles, based on the semantics of posts, in addition to the current methods based primarily on quantitative interaction patterns.

Since many users now participate in more than one
social network, the issue of merging user modelling
information across different sources arises, coupled
with the challenge of modelling and making use of
provenance information. As discussed in Section 5.1.2
above, there has been some preliminary work [67]
on merging implicit interests derived from the users
tweets with explicit interests, given on LinkedIn and
Facebook. The method currently gives equal weights
to the three social sites used to derive the interests.
However, more sophisticated models could be derived,
for instance, giving higher weights to professional interests derived from LinkedIn for Twitter users who
tweet predominantly in a professional capacity. Con-
versely, personal interests from Facebook might be
more important for social users of Twitter. Another
outstanding question is how to carry out detailed quan-

titative and user-based evaluations of such merged user
models, as this has not been discussed by [67]. In addition to these open issues, Kapanipathi et al have themselves suggested that future work needs to address also
the use of inferencing, based on the richer semantics
present in the linked data resources, which are used to
ground the automatically derived entities and topics of
interest.

Lastly, another challenging question is how to go
beyond interest-based models and interaction-based
social networks. For instance, Gentile et al [49]
have demonstrated how peoples expertise could be
captured from their email exchanges and used to
build dynamic user profiles. These are then compared
with each other, in order to derive automatically an
expertise-based user network, rather than one based
on social interactions. Such an approach could be extended and adapted to blogs (e.g. for discovery and
recommendation of blogs), as well as to information
sharing posts in Twitter and LinkedIn streams.

6. Semantic-based Information Access over Media

Streams

Semantic annotations enable users to find documents that mention one or more concepts from the ontology and, optionally, their relations [69]. Depending on the methods used, search queries can often mix
free-text keywords with restrictions over semantic annotations (e.g. GATE Mimir [35]). Search tools often
provide also browsing functionality, as well as search
refinement capabilities [69]. Due to the fact that social media streams are high volume and change over
time, semantic search and browsing is a very challenging task.

In general, semantic-based search and retrieval over
social media streams differ from traditional information retrieval, due to the additionally available ontological knowledge. On the other hand, they also differ
from semantic web search engines, such as Swoogle
[40], due to their focus on semantic annotations and
using those to retrieve documents, rather than forming
queries against ontologies to obtain sets of machinereadable triples.

This section discusses methods specifically devel-

oped for social media streams.

12Estimates suggest

that only 25% of tweets contain links:
http://techcrunch.com/2010/09/14/twitter-seeing-90-million-tweets-
per-day/.

Searching social media streams differs significantly
from web searches [137] in a number of important

6.1. Semantic Search over Social Media Streams

ways. Firstly, users search message streams, such as
Twitter, for temporally relevant information and are
mostly interested in people. Secondly, searches are
used to monitor Twitter content over time and can be
saved as part of user profiles. Thirdly, Twitter search
queries are significantly shorter and results include
more social chatter, whereas web searches look for
facts. Coupled with the short message length, noisy na-
ture, and additional information hidden in URLs and
hashtags, these differences make traditional keywordbased search methods sub-optimal on media streams.
Here we focus on recent work on semantic search, addressing these challenges.

The TREC 2011 Microblog track13 has given impetus to research by providing a set of query topics, a
time point, and a corpus of 16 million tweets, a subset of which was hand-annotated for relevance as a
gold standard. In addition to the widely used keywordbased and tweet syntax features (e.g. whether it contains a hashtag), Tao et al [136] experimented with
entity-based semantic features produced by DBpedia
Spotlight, which provided significantly better results.
The Twarql system [86] generates RDF triples from
tweets, based on metadata from the tweets themselves,
as well as entity mentions, hashtags, and URLs [87].
These are encoded using standard Open Data vocabularies (FOAF, SIOC) (see Section 3) and can be
searched through SPARQL queries. It is also possible
to subscribe to a stream of tweets matching a complex semantic query, e.g. what competitors are mentioned with my product (Apple iPad in their use case).
At the time of writing, Twarql has not been evaluated
formally, so its effectiveness and accuracy are yet to be
established.

Abel et al propose an adaptive faceted search framework for social media streams [1]. It uses semantic entity annotations by OpenCalais, coupled with a
user model (see Section 5.1.2), in order to create and
rank facets semantically. Keyword search and hashtagbased facets are used as the two baselines. The best
results are achieved when facets are personalised, i.e.
ranked according to which entities are interesting for
the given user (as coded in their entity-based user
model). Facet ranking also needs to be made sensitive
to the temporal context (essentially the difference between query time and post timestamp).

13http://sites.google.com/site/trecmicroblogtrack/

6.2. Filtering and Recommendations for Social

Media Streams

The unprecedented rise in the volume and perceived
importance of social media content has resulted in individuals starting to experience information overload.
In the context of Internet use, research on information
overload has shown already that high levels of information can lead to ineffectiveness, as a person cannot
process all communication and informational inputs
[13]. Consequently, researchers are studying information filtering and content recommendation, in order to
help alleviate information overload arising from social
media streams. Since Facebook streams are predominantly private, the bulk of work has so far focused on
Twitter.

As discussed in [31], social media streams are particularly challenging for recommender methods, and
different from other types of documents/web content.
Firstly, relevance is tightly correlated with recency, i.e.
content stops being interesting after just a few days.
Secondly, users are active consumers and generators
of social content, as well as being highly connected
with each other. Thirdly, recommenders need to strike
a balance between filtering out noise and supporting
serendipity/knowledge discovery. Lastly, interests and
preferences vary significantly from user to user, depending on the volume of their personal stream; what
and how they use social media for (see Section 5.1.3
on user roles); and user context (e.g. mobile vs tablet,
work vs home).

Chen et al [31] and Abel et al [3] focused on recommending URLs to Twitter users, since it is a common
information sharing task. The approach of Chen et al is
based on a bag-of-words model of user interests, based
on the user tweets, what is trending globally, and the
users social network. URL topics are modelled similarly as a word vector and tweet recommendations are
computed using cosine similarity.

Abel et al [3] improve on this approach by deriving semantic-based user interest models (see Section 5.1.2), which are richer and more generic. They
also capture more information through hashtag seman-
tics, replies, and, crucially, by modelling temporal dynamics of user interests.

Recently, Chen et al [30] extended their work towards recommending interesting conversations, i.e.
threads of multiple messages. The rationale comes
from the widespread use of Facebook and Twitter for
social conversations [95], coupled with the difficulties that users experience with following these conver-

Bontcheva and Rout / Semantics of Social Media Streams

sations over time, in Twitter in particular. Conversations are rated based on thread length, topic (using bag-
of-words as above) and tie-strength (higher priority
for content from tightly connected users). Tie strength
is modelled for bi-directionally connected users only,
using the existence of direct communication, its fre-
quency, and the tie strengths of their mutual friends.
Results showed that different recommendation strategies are appropriate for different types of Twitter users,
i.e. those who use it for social purposes prefer conversations from closely tied friends, whereas for information seekers, the social connections are much less im-
portant.

In the context of Facebook, researchers from Microsoft [97] have trained SVM classifiers to predict,
for a given user, the importance of Facebook posts
within their news feed, as well as the overall importance of their friends. They also demonstrate a correlation between the two, i.e. the overall importance of a
friend influences significantly the importance of posts.
In terms of semantic information, the method utilises
the Linguistic Inquiry and Word Count (LIWC) dictionary and its 80 topic categories [104]. One of the
key findings was the empirical validation of the need
for filtering and recommendation of user posts, going beyond reverse chronological order. A second very
important, but less strongly substantiated, finding is
the need for personalisation (i.e. the same post could
be very important for one user, while marked as nonrelevant by another).

The issue has recently been recognised by Face-
book, who have started to filter the posts shown in
the users news feed, according to the systems proprietary EdgeRank model of importance [68]. EdgeRank
takes into account the tie strength (affinity) between
the posting user and the viewing user, the type of post
(comment, like, etc), and a time decay factor. How-
ever, the full details of the algorithm are currently un-
known, as is its evaluation. Anecdotally, in 2010 50%
of all users were still clicking on the reverse chronological timeline of their feeds. This feature has since
been removed and the EdgeRank algorithm refined fur-
ther. However, it is still not yet possible for the users
themselves to train the system, by marking explicitly
which posts they consider important.

6.3. Stream Browsing and Visualisation

The main challenge in browsing and visualisation of
high-volume stream media is in providing a suitably
aggregated, high-level overview. Timestamp-based list

interfaces that show the entire, continuously updating
stream (e.g. the Twitter timeline-based web interface)
are often impractical, especially for analysing high-
volume, bursty events. For instance, during the royal
wedding in 2011, tweets during the event exceeded
1 million. Similarly, monitoring long running events,
such as presidential election campaigns, across different media and geographical locations is equally com-
plex.

One of the simplest and most widely used visualisations is word clouds. These generally use single word
terms, which can be somewhat difficult to interpret
without extra context. Word clouds have been used
to assist users in browsing social media streams, including blog content [11] and tweets [126,96]. For in-
stance, Phelan et al [106] use word clouds to present
the results of a Twitter based recommendation sys-
tem. The Eddi system [16] uses topic clouds, showing
higher-level themes in the users tweet stream. These
are combined with topic lists, which show who tweeted
on which topic, as well as a set of interesting tweets
for the highest ranked topics. The Twitris system (see
Figure 4) derives even more detailed, contextualised
phrases, by using 3-grams, instead of uni-grams [96].
More recently, the concept has been extended towards
image clouds [41].

The main drawback of cloud-based visualisations
is their static nature. Therefore, they are often combined with timelines showing keyword/topic frequencies over time [4,16,62,141], as well as methods for
discovery of unusual popularity bursts [11]. [38] use a
timeline which is synchronised with a transcript of a
political broadcast, allowing navigation to key points
in a video of the event, and displaying tweets from that
time period. Overall sentiment is shown on a timeline
at each point in the video, using simple colour seg-
ments. Similarly, TwitInfo (see Figure 6 [78]) uses a
timeline to display tweet activity during a real-world
event (e.g. a football game), coupled with some example tweets, colour-coded for sentiment. Some of these
visualisations are dynamic, i.e. update as new content
comes in (e.g. topic streams [41], falling keyword bars
[62] and dynamic information landscapes [62]).

In addition, some visualisations try to capture the
semantic relatedness between topics in the media
streams. For instance, BlogScope [11] calculates keyword correlations, by approximating mutual information for a pair of keywords using a random sample
of documents. Another example is the information
landscape visualisation, which conveys topic similarity
through spatial proximity [62] (see Figure 5). Topic-

Fig. 4. The Twitris Social Media Event Monitoring Portal (http://twitris.knoesis.org)

we have chosen to see the related topics discussed
in social media originating from California. Clicking
on the topic female democratic senators displays
the relevant tweets, news, and Wikipedia articles. For
comparison, Figure 7 shows the most discussed topics related to the election, extracted from social media originating from Great Britain. While there is significant topic overlap between the two locations, the
differences become also clearly visible.

Opinions and sentiment also feature frequently in
visual analytics interfaces. For instance, Media Watch
(Figure 5 [62]) combines word clouds with aggregated sentiment polarity, where each word is coloured
in a shade of red (predominantly negative sentiment),
green (predominantly positive), or black (neutral/no
sentiment). Search results snippets and faceted browsing terms are also sentiment coloured. Others have
combined sentiment-based colour coding with event
timelines [4], lists of tweets (Figure 6 [78]), and mood
maps [4]. Aggregated sentiment is typically presented
using pie charts [141] and, in the case of TwitInfo,
the overall statistics are normalised for recall (Figure 6
[78]).

Researchers have also investigated specifically the
problem of browsing and visualising social media
conversations about real-world events, e.g. broadcast

Fig. 7. Different Topics Extracted by Twitris for Great Britain

document relationships can be shown also through
force-directed, graph-based visualisations [43]. Lastly,
Archambault et al [7] propose multi-level tag clouds,
in order to capture hierarchical relations.

Another

important dimension of user-generated
content is its place of origin. For instance, some tweets
are geo-tagged with latitude/longitude information,
while many user profiles on Facebook, Twitter, and
blogs specify a user location. Consequently, mapbased visualisations of topics have also been explored
[88,78,62,96] (see also Figures 5 and 6). For instance,
Twitris [96] allows users to select a particular state
from the Google map and shows the topics discussed
in social media from this state only. Figure 4 shows the
Twitris US 2012 Presidential elections monitor, where

Bontcheva and Rout / Semantics of Social Media Streams

Fig. 5. Media Watch on Climate Change Portal (http://www.ecoresearch.net/climate)

Fig. 6. TwitInfo tracks a football game (http://twitinfo.csail.mit.edu/)

events [126], football games (Figure 6 [78]), conferences [41], and news events [88,4]. A key element here
is the ability to identify sub-events and combine these
with timelines, maps, and topic-based visualisations.

Lastly, given the user-generated and social nature
of the media streams, some visualisations have been
designed to exploit this information. For instance, the
PeopleSpiral visualisation [41] plots Twitter users who
have contributed to a topic (e.g. posted using a given
hashtag) on a spiral, starting with the most active and
original users first. User originality is measured as
the ratio between the number of tweets authored by the
user versus re-tweets made. OpinionSpace [46] instead
clusters and visualizes users in a two-dimensional
space, based on the opinions they have expressed on
a given set of topics. Each point in the visualisation shows a user and their comment, so the closer
two points, the more similar the users and opinions
are. However, the purely point-based visualisation was
found hard to interpret by some users, since they could
not see the textual content until they clicked on a point.
ThemeCrowds [7] instead derives hierarchical clusters
of Twitter users through agglomerative clustering and
provides a summary of the tweets generated by this
user cluster, through multilevel tag clouds (inspired by
treemap visualisation). Tweet volumes over time are
shown in a timeline-like view, which also allows the
selection of a time period.

6.4. Discussion

Most current search, recommendation, and visualisation methods tend to use shallow textual and
frequency-based information. For instance, a comparison between TF-IDF weighted topic models and LDA
topic modelling has shown the former to be superior [30,114]. However, these can be improved further
through integration of semantic information, as suggested by [30]. In the case of personalised recommen-
dations, these could be improved by incorporating user
behaviour roles, making better use of the latent semantics and implicit user information, as well as better integration of the temporal dimension in the recommender algorithms.

Browsing and visualisation interfaces can also be
improved by taking into account the extra semantic
knowledge about the entities mentioned in the media
streams. For instance, when entities and topics are annotated with URIs to LOD resources, such as DBpedia,
the underlying ontology can underpin hierarchicallybased visualisations, including semantic relations. In

addition, the exploration of media streams through
topic-, entity-, and time-based visualisations can be enriched with ontology-based faceted search and semantic query interfaces. One such example is the KIM semantic platform, which is, however, aimed at largely
static document collections [110].

Algorithm scalability and efficiency are particularly
important, due to the large-scale, dynamic nature of social media streams. For instance, the interactive Topic
Stream visualisation takes 45 seconds to compute on 1
million tweets and 325,000 contributing users, which
is too long for most usage scenarios [41]. Similarly,
calculating keyword correlations through point-wise
mutual information is computationally too expensive
on high volume blog posts [11]. A frequently used solution is to introduce a sliding window over the data
(e.g. between one week and one year) and thus limit
the content used for IDF and other such calculations.
In conclusion, designing effective semantic search,
browsing and visualisations interfaces for media streams
has proven particularly challenging. Based on our survey of the state-of-the-art, we have derived the following requirements:

 designing meaningful and intuitive visualisa-
tions, conveying intuitively the complex, multidimensional semantics of user-generated content
(e.g. topics, entities, events, user demographics
(including geolocation), sentiment, social net-
works);

 visualising changes over time;
 supporting different levels of granularity, both at
the level of semantic content, user clusters, and
temporal windows;

 allowing interactive, real-time exploration;
 integration with search, to allow users to select a

subset of relevant content;

 exposing the discussion/threaded nature of the so-

cial conversations;

 addressing scalability and efficiency.
Amongst the systems surveyed, only Twitris [96]
and Media Watch [62] have started to address most
of these requirements, but not without limitations.
Firstly, their current visualisations are mostly topicand entity-centric and could benefit from integration
of event-based visualisations, such as TwitInfo [78]
and Tweetgeist [126]. Secondly, user demographics as
means for stream media aggregation and exploration is
mostly limited to map-based visualisations. Additional
search and browsing capabilities, based around users
age, gender, political views, interests, and other such

Bontcheva and Rout / Semantics of Social Media Streams

characteristics are also needed. Thirdly, methods for
information aggregation and exploration, based on social networks (e.g hubs and authorities) could be combined with the currently prevailing topic- and contentcentric approaches. Lastly, we would like to advocate
a more substantial end-user involvement in the design
and testing of new intelligent information access sys-
tems. In this way, the resulting user interfaces will address the emerging complex information seeking re-
quirements, in terms of better support for sense mak-
ing, learning and investigation, and social search [107].

7. Outstanding Challenges and Conclusions

This paper set out to explore a number of research
questions arising from applications of semantic technologies to social media.

Firstly, we examined existing ontologies in the
context of modelling the semantics of social media
streams. Our conclusion is that most applications tend
to adopt or extend more than one ontology, since they
model different aspects. With respect to Web of Data
resources, current methods have made most use of
Wikipedia-derived resources (namely DBpedia and
YAGO) and, to a lesser degree  Geonames, Freebase,
and domain-specific ones like MusicBrainz. Better exploiting this wealth of semantic knowledge for semantic annotation of social media remains a challenge,
which we discussed in more detail in section 4.6.

Next, the questions of capturing the implicit semantics and dealing with the noisy, dynamic nature of social media streams, were addressed as part of our analysis of semantic annotation state-of-the-art. We identified the need for more robust and accurate large-scale
entity and event recognition methods, as well as finergrained opinion mining algorithms to address target
identification, volatility over time, detecting and modelling conflicting opinions, and opinion aggregation
(see section 4.6 for details).

Thirdly, current methods for modelling users digital identity and social media activities were discussed.
Limitations with respect to modelling user interests
and integration of temporal dynamics were identified,
coupled with emerging need for cross-media user mod-
els. A more in-depth discussion appears in section 5.2.
Lastly, semantic-based methods for search, brows-
ing, recommendation, and information visualisation of
social media content were reviewed, from the perspective of supporting complex information seeking be-
haviour. As a result, seven key requirements were iden-

tified and limitations of current approaches were discussed in this context.

In conclusion, we discuss three major areas where

further research is necessary.

7.1. Cross-Media Aggregation and Multilinguality

The majority of methods surveyed here have been
developed and evaluated only on one kind of social
media (e.g. Twitter or blog posts). Cross-media link-
ing, going beyond connecting tweets to news articles,
is a crucial open issue, due to the fact that increasingly
users are adopting more than one social media plat-
form, often for different purposes (e.g. personal vs professional use). In addition, as peoples lives are becoming increasingly digital, this work will also provide a
partial answer to the challenge of inter-linking our personal collections (e.g. emails, photos) with our social
media online identities.

The challenge is to build computational models of
cross-media content merging, analysis, and visualisation and embed these into algorithms capable of
dealing with the large-scale, contradictory and multipurpose nature of multi-platform social media streams.
For example, further work is needed on algorithms
for cross-media content clustering, cross-media identity tracking, modelling contradictions between different sources, and inferring change in interests and attitudes over time.

Another related major challenge is multilinguality.
Most of the methods surveyed here were developed
and tested on English content only. As discussed in
Section 4.6, some initial steps are being made through
multilingual lexicons, such as Wiktionary [89] and
UBY [55], and linguistically grounded ontologies [22].
Other work has focused on widening the range of
available linguistic resources to less studied languages,
through crowd-sourcing. Amazon Mechanical Turk, in
particular, has emerged as particularly useful, since
crowd-sourcing projects are easily setup there, coupled with the fact that it allows access to foreign
markets with native speakers of many rare languages"
[145]. This feature is particularly useful for researchers
working on less-resourced languages, such as Arabic [45], Urdu [145] and others [5,24,65]. Irvine and
Klementiev [65], for example, have shown that it is
possible to create lexicons between English and 37
out of the 42 low resource languages that they experimented with. Similarly, Weichselbraun et al [140]
crowd-source domain-specific sentiment lexicons in
multiple languages, through games with a purpose. A

related aspect is designing crowdsourcing projects, so
that they can be re-used easily across languages, e.g.
[65,76] for Mechanical Turk and [109,113] for games-
with-a-purpose. There is also the related issue of annotated corpora and evaluation, to which we return in
Section 7.3 below.

Lastly, as users are increasingly consuming social
media streams on different hardware platforms (desk-
tops,
tablets, smart phones), cross-platform and/or
platform-independent information access methods need
to be developed. This is particularly challenging in the
case of information visualisation on small screen de-
vices.

7.2. Scalability and Robustness

In information extraction research, large-scale algorithms (also referred to as data-intensive or webscale natural language processing) are demonstrating
increasingly superior results compared to approaches
trained on smaller datasets [56]. This is mostly thanks
to addressing the data sparseness issue through collection of significantly larger numbers of naturally occurring linguistic examples [56]. The need for and the
success of data-driven NLP methods to a large extent
mirrors recent trends in other research fields, leading
to what is being referred to as the fourth paradigm of
science [12].

At the same time, semantic annotation and information access algorithms need to be scalable and robust,
also in order to cope with the large content volumes
encountered in social media streams. Many use cases
require online, near real-time processing, which introduces additional requirements in terms of algorithm
complexity. Cloud computing [39] is increasingly being regarded as a key enabler of scalable, on-demand
processing, giving researchers everywhere affordable
access to computing infrastructures, which allow the
deployment of significant compute power on an ondemand basis, and with no upfront costs.

However, developing scalable and parallelisable algorithms for platforms such as Hadoop is far from triv-
ial. Straightforward deployment and sharing of semantic annotation pipelines and algorithm parallelisation
are only a few of the requirements which need to be
met. Research in this area is still in its infancy, especially around general purpose platforms for scalable
semantic processing.

GateCloud.net [134] can be viewed as the first step
in this direction. It is a novel cloud-based platform for
large-scale text mining research, which also supports

ontology-based semantic annotation pipelines. It aims
to provide researchers with a platform-as-a-service,
which enables them to carry out large-scale NLP experiments by harnessing the vast, on-demand compute
power of the Amazon cloud. It also minimises the need
to implement specialised parallelisable text processing
algorithms. Important infrastructural issues are dealt
with by the platform, completely transparently for the
researcher: load balancing, efficient data upload and
storage, deployment on the virtual machines, security,
and fault tolerance.

One example application of GateCloud was in a
project with the UK National Archive [80], which used
it to annotate semantically 42 TB of web pages and
other textual content. The semantic annotation process
was underpinned by a large-scale knowledge base, acquired from the LOD cloud, data.gov.uk, and a large
geographical database. The results were indexed in
GATE Mimir [35], coupled with a user interface for
browsing, search and navigation from the document
space into the semantic knowledge base via full-text
search, semantic annotations and SPARQL queries.

7.3. Evaluation, Shared Datasets and Crowdsourcing

The third major open issue is evaluation. As discussed in all three application areas of semantic technologies for social media streams, lack of shared goldstandard datasets is hampering repeatability and comparative evaluation of algorithms. At the same time,
comprehensive user- and task-based evaluation experiments are also required, in order to identify problems
with existing search and visualisation methods. Particularly in the area of intelligent information access,
most of the papers surveyed either did not report evaluation experiments, or those that did, tended to carry
out small-scale, formative studies. Longitudinal evaluation with larger user groups is particularly lacking.

Similarly, algorithm training and adaptation on social media gold standard datasets is currently very lim-
ited. For example, no gold standard datasets of Twitter and blog summaries exist and there are fewer than
10,000 tweets annotated with named entities. Creating sufficiently large, vitally needed datasets through
traditional expert-based text annotation methodologies
is very expensive, both in terms of time and funding
required. The latter can vary between USD 0.36 and
1.0 [109], which is unaffordable for corpora consisting of millions of words. Some cost reductions could
be achieved through web-based collaborative annotation tools, such as GATE Teamware [21], which sup-

Bontcheva and Rout / Semantics of Social Media Streams

port distributed teams and are tailored to non-expert
annotators.

Another alternative are commercial crowdsourcing
marketplaces have been reported to be 33% less expensive than in-house employees on tasks such as tagging and classification [61]. Consequently, in the field
of language processing, researchers have started creating annotated corpora with Amazon Mechanical Turk
and game-based approaches as less expensive alterna-
tives. A comprehensive survey of crowdsourcing is beyond the scope of this paper, however, for details see
[120,25,112].

With respect to corpus annotation in particular, Poesio et al [109] estimate that, compared to the cost of
expert-based annotation (estimated as $1.000.000), the
cost of 1 million annotated tokens could be indeed reduced to less than 50% by using MTurk (i.e., $380.000
- $430.000) and to around 20% (i.e., $217,927) when
using a game based approach such as their own
PhraseDetectives game. With respect to crowdsourcing social media annotations, there have been experiments on, e.g., categorizing tweets [102] and annotating named entities in tweets [47]. In the Semantic
Web field, researchers have explored mostly crowdsourcing through games with a purpose, primarily for
knowledge acquisition [129,138] and LOD improvement [139].

At the same time, researchers have turned to crowdsourcing as a means for scaling up human-based evaluation experiments. The main challenge here is in
how to define the evaluation task, so that it can be
crowdsourced from non-specialists, with high quality
results [82].This is far from trivial, and researchers
have argued that crowdsourcing evaluation tasks need
to be designed differently from expert-based evaluations [50]. In particular, Gillick and Liu [50] found that
non-expert evaluation of summarization systems produces noisier results, thus requiring more redundancy
to achieve statistical significance and that Mechanical
Turk workers cannot produce score rankings that agree
with expert ranking.

One successful design for crowdsourcing-based
evaluation has used a four phase workflow of separate
tasks, which has been tried on reading comprehension
of machine translation [24]. Another, simpler task design has been used by [63] for evaluation of tweet sum-
maries. Inouye et al asked Mechanical Turk workers
to indicate on a five point scale, how much of the information from the human produced summary is contained in the automatically produced summary. A third
evaluation example, which has achieved successful re-

sults on Mechanical Turk, is pair-wise ranking [51].
The task in this case is to identify the most informative sentence from a product review. In this case, the
crowdworkers are asked to indicate whether a sentence
chosen by the baseline system is more informative than
a sentence chosen by the authors method. Sentence
order is randomised and it is also possible to indicate
that none of these sentences are a good summary.

To conclude, crowdsourcing has recently emerged
as a promising method for creating shared evaluation datasets, as well as for carrying out user-based
evaluation experiments. Adapting these efforts to the
specifics of semantic annotation and information visu-
alisation, as well as using these to create large-scale
resources and repeatable, longitudinal evaluations, are
key areas for future work.

Acknowledgements

This work was supported by funding from the Engineering and Physical Sciences Research Council
(grant EP/I004327/1). The authors wish to thank Marta
Sabou and Arno Scharl for the discussions on crowdsourcing and its role in semantic technologies research,
as well as Diana Maynard for the discussions on opinion mining of Twitter and Facebook messages and for
proof-reading the paper. We would also like to thank
the reviewers and the editor for their comments and
suggestions, which helped us improve this paper sig-
nificantly.
