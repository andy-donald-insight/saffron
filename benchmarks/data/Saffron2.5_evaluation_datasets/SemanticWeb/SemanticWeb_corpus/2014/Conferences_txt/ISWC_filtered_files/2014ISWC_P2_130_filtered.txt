Holistic and Compact Selectivity Estimation

for Hybrid Queries over RDF Graphs

Andreas Wagner1, Veli Bicer2, Thanh Tran3, and Rudi Studer1

1 Karlsruhe Institute of Technology

2 IBM Research Centre Dublin

{a.wagner,rudi.studer}@kit.edu, velibice@ie.ibm.com,

3 San Jose State University

ducthanh.tran@sjsu.edu

Abstract. Many RDF descriptions today are text-rich: besides structured data they also feature much unstructured text. Text-rich RDF data
is frequently queried via predicates matching structured data, combined
with string predicates for textual constraints (hybrid queries). Evaluating hybrid queries efficiently requires means for selectivity estimation.
Previous works on selectivity estimation, however, suffer from inherent
drawbacks, which are reflected in efficiency and effectiveness issues. We
propose a novel estimation approach, TopGuess, which exploits topic
models as data synopsis. This way, we capture correlations between structured and unstructured data in a holistic and compact manner. We study
TopGuess in a theoretical analysis and show it to guarantee a linear space
complexity w.r.t. text data size. Further, we show selectivity estimation
time complexity to be independent from the synopsis size. In experiments on real-world data, TopGuess allowed for great improvements in
estimation accuracy, without sacrificing efficiency.

Introduction

RDF data contains descriptions of entities, with each description being a set of
triples. Many RDF descriptions feature textual data: On the one hand, structured
RDF data often comprises text via predicates such as comment or description.
On the other hand, unstructured Web documents are frequently annotated with
structured data (e.g., via RDFa or Microformats).

Hybrid Queries. Such text-rich RDF descriptions are often queried with
queries, which comprise predicates that match structured data as well as words
in text data (hybrid queries). Consider the following example (cf. Fig. 1-a/b):

SELECT * WHERE {

?m ex:title ?title .
?m ex:starring ?p .
?m a Movie .
FILTER (contains(?title,"Holiday") && contains(?name,"Audrey") &&

?p ex:name ?name .
?p ex:bornIn ?l .
?p a Person .

?l ex:name ?name2 .

contains(?name2,"Belgium")) }

 This work was supported by the European Union through project XLike (FP7-ICT-

2011-288342).

P. Mika et al. (Eds.) ISWC 2014, Part II, LNCS 8797, pp. 97113, 2014.
 Springer International Publishing Switzerland 2014

A. Wagner et al.

(a)

Movie

Audrey
Tautou

Person

name

type

type

type

p1

starring

type
bornIn

p2
comment

spouse

m1

title

Roman 
Holiday

Location

type

motto

l1

name

p3

name

Strength 
through 
Unity

(b)

Holiday

Person

Belgium

Belgium

Mel 
Ferrer

title

m

type

starring

name

type

p

bornIn

name

l

name
Audrey
Kathleen  
Hepburn

Audrey Hepburn was a British 
actress and humanitarian. 
Born in Ixelles, Belgium as 

Audrey Kathleen Ruston

Movie

Audrey

(c) W = {"Roman", "Holiday", "Aud-
rey", "Tautou", "Kathleen", ...}

Fig. 1. (a) RDF graph about Audrey Hepburn and her movie Roman Holiday.
(b) Hybrid query graph asking for movies with title Holiday and starring a person
with name Audrey, who was born in Belgium. (c) Vocabulary W comprising all
words from attributes values in the data graph.

Hybrid queries are highly relevant for RDF stores with SPARQL fulltext ex-
tension, e.g., LARQ1 or Virtuoso2. In fact, every store that supports FILTER
clauses on texts faces hybrid queries.

Selectivity Estimation. For finding an optimal query plan, RDF stores rely
on selectivity estimates to approximate the result size of a query (fragment) [19].
Various selectivity estimation techniques have been proposed for relational
data [1,7,9,17,18,21] as well as for RDF data [10,16,19,20,23]. These techniques
summarize the data via a data synopsis (e.g., histograms, join synopses, tuplegraph synopses, or probabilistic relational models (PRM)) to capture data corre-
lations/statistics. Based on these synopses, different estimation approaches have
been proposed to efficiently approximate the querys selectivity.

However, when applying state-of-the-art selectivity estimation techniques for

hybrid queries effectiveness and efficiency issues (I.1 and I.2) arise:

(I.1) Effectiveness Issues. Queries over RDF data typically comprise a large
number of joins. Thus, a data synopsis must capture statistics for multiple
(joined) query patterns in order to allow effective estimates. Recent work on selectivity estimation for RDF data captured multiple patterns either via computing statistics for frequent (star/chain) join patterns [10,16], via heuristics [19,20],
or by conditional probabilities [23]. These strategies work well for structured
query patterns (i.e., class, relation, and attributes), because the number of structured elements (i.e., class, relation, and attributes) is usually small and independent of the instance data size. However, in the presence of textual data and
hybrid queries, capturing multiple patterns is much harder, since the number
of words is oftentimes very large. For instance, the DBLP dataset (used in our
evaluation) has 25 million words vs. 56 structure elements.

Example. In Fig. 1-a, there can be many entities of type Person (i.e., bindings
for ?p a Person), while only few entities have a name Audrey. So, in order

1 http://jena.sourceforge.net/ARQ/lucene-arq.html
2 http://virtuoso.openlinksw.com
?

?

?
to estimate the # bindings for ?p in Fig. 1-b, a synopsis has to capture statistics
for any word associated (via name) with Person entities.

Moreover, the number of words is strongly dependent on the data and text size,
respectively. So, with growing data sets, statistics become increasingly complex
and space consuming. To address this, all previous works summarized the words
via a small synopsis, which is constructed either by hashing [19], heuristics [20],
categorization [16], discretization [10], or via string synopses [23]. Unfortunately,
such coarse-grained synopses result in an information loss. That is, oftentimes
heuristics must be employed to estimate selectivities of query keywords (e.g.,
Audrey), which leads to severe miss-estimations.

(I.2) Efficiency Issues. All previous works [10,16,19,20,23], aim at constructing
a query-independent data synopsis at offline time. In fact, previous approaches
directly use this offline constructed data synopsis to estimate the query selec-
tivity. Thus, a large synopsis would influence the estimation efficiency. In order
to guarantee an efficient selectivity estimation at runtime, existing approaches
either construct only small synopses [10,16,19,23] or purely rely on heuristics for
selectivity estimation [20].

Contributions. (1) In this paper, we propose a novel approach (TopGuess),
which utilizes relational topic models as data synopsis. Such synopses are wellsuited to summarize text data, because they provide statistics for the complete
vocabulary of words by means of topics. So, no information loss can occur
due to coarse-grained synopses. Furthermore, correlations between structured
query patterns (e.g., ?m ex:starring ?p and ?m rdf:type Movie, see Fig. 1-
b) can also be captured. Thus, we have an effective and holistic synopsis for
hybrid queries (effectiveness issues, I.1). The TopGuess approach also constructs
a query-independent data synopsis at offline time. However, in contrast to previous approaches, we do not directly use this large synopsis at runtime. Instead,
we only employ a small and compact synopsis (Bayesian network), which is constructed specifically for the current query (efficiency issues, I.2).

(2) We provide a theoretical analysis: TopGuess achieves a linear space complexity w.r.t. text data size (cf. Thm. 1). Further, TopGuess has an estimation
time complexity that is independent of the synopsis size (given the number of
topics), cf. Thm. 4.

(3) We conducted experiments on real-world data: TopGuess could improve

the effectiveness by up to 88%  without sacrificing runtime performance.

Outline. First, in Sect. 2 we outline preliminaries. We introduce the TopGuess
approach in Sect. 3. In Sect. 4, we present evaluation results, before we outline
related work in Sect. 5. We conclude with Sect. 6.

2 Preliminaries

Data and Query Model. We use RDF as data model:

Definition 1 (RDF Graph). Let a (r) be a set of attribute (relation) labels.
A RDF graph is a directed labeled graph G = (V, E, a, r), with nodes V =

A. Wagner et al.

VE  VA  VC where VE are entity nodes, VA are attribute value nodes, and VC
are class nodes. Edges (so-called triples) E = ER  EA  type are a disjoint
union of relation edges ER and attribute edges EA. Relation edges connect entity
nodes: s, r, o  ER, with s, o  VE and r  r). Attribute edges connect an
entity with an attribute value: s, a, o  EA, with s  VE, o  VA and a  a.
Triple s, type, o  E models that entity s  VE belongs to class o  VC .

We conceive an attribute value in VA as a bag-of-words. Further, let a vocabulary W comprise all such words. That is, W is derived from words in attribute
values: for each triple s, p, o  EA we add all words in o to W . See also Fig. 1.
Conjunctive queries resemble the basic graph pattern (BGP) feature of

SPARQL. In this work, we use hybrid queries:
Definition 2 (Hybrid Query). A query Q is a directed graph GQ = (VQ, EQ),
with VQ = VQV  VQC  VQK , VQV as variables, VQC as constants, and VQK as
keywords. Edges EQ are called predicates: (1) Class predicates s, type, o, with
s  VQV , o  VQC . (2) Relation predicates s, r, o, with s  VQV , o  VQC  VQV ,
and r  r. (3) String predicates s, a, o, with s  VQV , o  VQK , and a  a.
Fig. 1-b shows an example query. Query semantics follow those for BGPs. That
is, results are subgraphs of the data graph, which match all query predicates.
The only difference is due to keyword nodes: a value node o  VA matches a
keyword w  VQK , if the bag-of-words from o contains word w.

We rely on two data synopses: topic models and Bayesian networks (BNs):

Topic Models. Topic models assume that texts are mixtures of hidden topics,
where a topic is a probability distribution over words. These topics are abstract
clusters of words  formed according to word co-occurrences. More formally, a
text collection can be represented by k topics T = {t1, . . . , tk}, where W is
the vocabulary (see above definition) and each topic t  T is a multinomial
distribution of words in W : P(w | t) = tw and
Example. Three topics are depicted in Fig. 2-c: T = {t1, t2, t3}. Every topic t

w  W tw = 1.
?

?

?
assigns a probability (represented by vector t) to each word in the vocabulary.
Probabilities in t indicate the importance of words within topic t. For instance,
Belgium is most important for topic t3 (tw = 0.014), cf. Fig. 2-c.

Bayesian Networks. A Bayesian network (BN) is a directed graphical model,
which compactly represents a joint probability distribution via its structure and
parameters [12]. The structure is a directed acyclic graph, where nodes stand
for random variables and edges represent dependencies. Given a node Xi and its
parents Pa(Xi) = {Xj, . . . , Xk}, Xi dependents on Pa(Xi), but is conditionally
independent of all non-ancestor random variables (given Pa(Xi)).

BN parameters are given by conditional probability distributions (CPDs).
That is, each random variable Xi is associated with a CPD capturing the con-

ditional probability P(Xi | Pa(Xi)). The joint distribution P(X1, . . . , Xn) can
be estimated via the chain rule [12]: P(X1, . . . , Xn)  

i P(Xi | Pa(Xi)).

Example. A BN is shown in Fig. 3. Nodes such as Xm and Xholiday stand
for random variables. Edges stand for dependencies between those nodes. For
instance, the edge Xm  Xholiday denotes a dependency between the parent,
?

?

?
Xm, and the child, Xholiday. In fact, given its parent, Xholiday is conditionally
independent of all non-ancestor variables, e.g., Xp. Every node has a CDP. For
example, Xholiday has a CDP for P(Xholiday | Xm), cf. Fig. 3-b.
Problem. Given a hybrid query Q, we aim at a result size estimation function
F (Q) as [9]: F (Q)  R(Q)  P(Q).
Let R be a function R : Q  N that provides an upper bound cardinality for a
result set for query Q. Further, let P be a probabilistic component, which assigns
a probability to query Q that models whether Qs result set is non-empty.
R(Q) can be easily computed as product over class cardinalities of Q [9].
That is, for each variable v  VQV we bound the number of its bindings, R(v), as
number of entities belonging vs class c: |{s|s, type, c  E}|. If v has no class, we
use the number of all entities (i.e., |VE|) as a bound. Finally, R(Q) =
v R(v).
In the remainder of the paper, we provide an effective (I.1) and efficient (I.2)
instantiation of the probabilistic component P.
?

?

?
starring
2

t1
t2
t3

(b)

t2

t1
t3
0 7 2
0 1 0
1 3 2

movie
person

(a)

t2

t1
t3
3 0 1
0 5 2

1

film
play
...
...
holiday
roman
...
hepburn
...
belgium

t1

0.024
0.023
...
...
0.011
0.010
...
0.004
...
0.001

(c)

born
woman
...
audrey
hepburn
...
belgium
...
holiday
...

t2

0.027
0.026
...
0.013
0.012
...
0.009
...
0.002
...

city
location
...
belgium
...
...
holiday
...
hepburn
...

t3

3

0.025
0.024
...
0.014
...
...
0.004
...
0.002
...

Fig. 2. Synopsis parameters (stored on
disk): (a) movie and person parameter for three topics. (b)  matrix for
starring relation and three topics 
rows (columns) represent source (tar-
get) topics of the relation. (c) Words
from the vocabulary W and their corresponding probabilities for each topic,
t. Note, data is taken from Fig. 1-a/c.

3 TopGuess

Targeting the effectiveness (I.1) and efficiency (I.2) issues of existing works w.r.t.
hybrid queries (cf. Sect. 1), we now introduce our novel TopGuess approach.

More precisely, we present an uniform data synopsis based on relational topic
models in Sect. 3.1 (I.1), and show in Thm. 1 that this synopsis has a linear
space complexity w.r.t. vocabulary W (I.2). Further, we introduce a probabilistic
component P in Sect. 3.2 and 3.3, and show in Thm. 4 selectivity computation
complexity to be independent of the synopsis size (I.2).

Note, the topic model (data synopsis) is learned at offline time and may be
stored on disk. At runtime, we construct a small BN for each given query 
reflecting our data synopsis as well as query characteristics via topic mixtures.

3.1 Relational Topic Models as Data Synopsis

Synopsis Parameters. For an effective synopsis over text-rich RDF data,
TopGuess exploits relational
topic models [2,4,14,25]. These topic models

A. Wagner et al.

summarize the data by means of one uniform synopsis  considering structured
and text data. More precisely, our synopsis comprises of two parts:

(1) First, the TopGuess synopsis captures text data in a low-dimensional
representation via a set of k topics T = {t1, . . . , tk}.
Example. Fig. 2-c groups words from the vocabulary W = {Roman, Holi-
day, . . .}, cf. Fig. 1-c, via three topics, T = {t1, t2, t3}. This way, text data in
Fig. 1-a, e.g., associated via attribute comment, is compactly represented.

The number of topics is dictated by the data characteristics. In particular,
previous works allow to learn the optimal number of topics [8]. By means of this
compact summary, TopGuess achieves a linear space complexity linear w.r.t. a
vocabulary, see Thm. 1.

(2) Second, the TopGuess synopsis captures correlations between those topics
and structured data. For our query model, we rely on two correlation parameters
for selectivity estimation:  and . Note, for other kinds of queries, further types
of correlation parameters may be considered.
 Class-Topic Parameter . We capture correlations between a class c  VC
and topics in T via a vector c, where each vector element, c(t), represents
the weight between class c and topic t. A higher weight indicates that a class
is more correlated with a topic.
Example. Fig. 2-a shows the two class-topic parameters for the classes Movie
and Person: movie and person. Both indicate correlations w.r.t. topics T =
{t1, t2, t3}. For instance, movie states that class Movie is highly correlated
with topic t1, has some correlation with t3, and has no correlation with t2.
 Relation-Topic Parameter . We measure correlations between a relation r
and the topics in T via a matrix r. Since relation r is observed between
two entities, say s, r, o, the topic of its subject s and its object o is consid-
ered. Given k topics, matrix r has k  k dimensions and entries such that:
if entity s is associated with topic ti and entity o has topic tj, the weight of
observing a relation r between s and o is given by the entry i, j, denoted
as r(ti, tj). Note, TopGuess features a matrix  for each relation.
Example. Fig. 2-b depicts the relation-topic parameter for the starring re-
lation: starring. According to starring, starring is most often observed
(weight 7) if its subject (object) contains words from topic t1 (t2).

Parameter Learning. For training above parameters, we do not restrict
TopGuess to a single topic model. Instead, different approaches can be used.
For instance, classical topic models such as LDA [3] may be employed to learn
the first part, i.e., word/topic probabilities, cf. Fig. 2-c. Then, correlations between those topics and classes/relations must be obtained. For this, topic models
have been extended to consider structured data, so-called relational topic models [2,4,14,25]. Most notably, a recent approach, the Topical Relational Model
(TRM) [2], trains topics as well as class/relation correlations simultaneously
from RDF data. We used a TRM as data synopsis in our experiments.
?

?

?
(a)

Xm

Xp

(b)

Xl

Xholiday

Xmovie

Xstarring Xaudrey Xperson XbornIn Xbelgium

CPD: P(Xholiday|Xm)
Xholiday = T      Xm
0.647          t1
0.118          t2
0.235          t3

Fig. 3.
(a) Query-specific BN for the query in Fig.1-b. It contains three topical
variables (color: blue, e.g., Xm), two class predicate variables (color: light gray, e.g.,
Xmovie), two relation predicate variables (color: dark gray, e.g., Xstarring), and three
string predicate variables (color: white, e.g., Xholiday). Observed variables (color:
white/light gray/dark gray) are independent from each other and only dependent on
hidden topical random variables (color: blue)  as dictated by Def. 4. (b) CDP for
random variable Xholiday, cf. parameters in Fig. 2-c.

Discussion. The TopGuess synopsis comes with key advantages: First, in contrast to existing work [23], we do not need separate synopses for structured and
text data. This way, we may learn correlations in a uniform manner.

Moreover, TopGuess parameters are not required to be loaded in mem-
ory. This is a crucial advantage over state-of-the-art selectivity estimation systems [9,21,23], as memory is commonly a limited resource. So, TopGuess can
utilize the complete vocabulary W for learning word/topic probabilities .

Last, as empirically validated by our experiments, correlations between topics
and structured data suffice for an accurate selectivity estimation. Since even a
small number of topics can capture these correlations, our synopsis does not
grow exponentially in its vocabulary size.

In fact, we can show that a topic-based data synopsis has linear space com-

plexity w.r.t. its vocabulary:

Theorem 1 (Synopsis Space Complexity). Given k topics, a vocabulary
W , classes VC , and relations r, TopGuess has a storage space complexity in
O(|W|  k + |VC|  k + |r|  k2).
Proof. See our report for a proof [22]
?

?

?
3.2 Probabilistic Component: Query-Specific BN

In this section, we exploit the synopsis parameters for an efficient probabilistic
component (I.2, Sect. 1). For this, we first construct a small, query-specific BN
and afterwards compute its joint probability in Sect. 3.3. Both steps are done at
runtime. In contrast to [9,21,23], all synopsis parameters may be kept on disk,
while only the query-specific BN is loaded into memory.
To construct a BN specifically for a query Q, we capture every query predicate in Q via a random variable: For a class predicate, s, type, c, and relation
predicate, s, r, o, we create a binary random variable Xc and Xr. Similarly, for
a string predicate, s, a, w, we introduce a binary random variable Xw. Most
importantly, we assume that each variable v in Q belongs to one or more topics
in T . So, we model variable v via a topical random variable, Xv. More formally,
Xv has a multinomial distribution over the topics:

A. Wagner et al.

Definition 3 (Topical Random Variable). For a set of topics T , a query
Q, and its variable v  VQV , the random variable Xv is a multinomial topical
random variable for v, with topics T as sample space.

Based on topical random variables, we perceive query variables as topic mix-
tures. Thus, Xv captures query variable vs relatedness to every topic. In the
following, we denote the set of all string, class, relation, and topical random
variables for query Q as Xw, Xc, Xr, and Xv.

We create a simple BN structure by means of a fixed structure assumption:

Definition 4 (Topical Dependence Assumption). Given a class/string
predicate v,,, the corresponding variable X depends only the topical random variable Xv. Given a relation predicate v,, y, the corresponding variable
X depends only on two topical random variables: Xv and Xy.

The topical dependence assumption lies at the core of the TopGuess approach.
It considers that query predicate probabilities depend on (and are governed by)
the topics of their associated topical random variables. Further, the assumption
allows us to model the query probability, P(Q), via a tree-shaped BN.

Example. Fig. 3-a depicts a BN for the query in Fig. 1-b. Adhering to Def. 4,
each topical variable (Xm, Xp, and Xl) forms a small tree of dependent random
variables. For instance, random variable Xholiday is only dependent on its topical
variable, Xm. In fact, given Xm, Xholiday is conditionally independent of all
other variables, e.g., Xaudrey. This way, topic mixtures of Xm, Xp, and Xl
govern the overall query probability, P(Q).
Last, note that the topical dependence assumption leads to a valid BN:

Theorem 2. A query-specific BN constructed according to Def. 4 is acyclic.

Proof. See [22] for a proof
?

?

?
3.3 Probabilistic Component: Query Probability Computation

Having formed the BN structure for a given query Q, we may compute the query
probability, P(Q), via the chain rule (CR) [12]:

P(Q) = P

 
?

?

?

v,a,w  Q
 

v,r,y  Q

Xw = T
?

?

?
Xc = T
?

?

?
P(Xw = T | Xv)  
P(Xr = T | Xv, Xy)

v,type,c  Q
?

?

?
Xr = T

(1a)

P(Xc = T | Xv)

(1b)

In order to solve Eq. 1, we require a CPD for each random variable, cf. Fig. 3-
b. We rely on TopGuess parameters as well as distributions of topical random
variables to approximate these CPDs. As topical variables Xv are hidden, we
learn their distributions from observed random variables (Xw, Xc, Xr).
?

?

?
In the following, we first discuss CPD estimation for observed random vari-
ables, given topical random variables (topic distributions). Subsequently, we
present learning of topic distributions for hidden topical variables.

Query Predicate Probabilities. Probabilities for query predicates are influenced by their associated topical random variables and their TopGuess parame-
ters. In other words, we may compute the CPDs for Xw, Xc, and Xr using topic
distributions of topical variables and probabilities estimated by the corresponding , , or  parameter:
(1) Class Predicates. Adhering to the topical dependence assumption, the
probability of observing a class, P(Xc = T), is only dependent on its topical
variable Xv. We use the class-topic parameter  to obtain the weight c(t),
which indicates the correlation between topic t and class c:

P(Xc = T | Xv, ) =

P(Xv = t)
?

?

?
c(t)
t  T c(t)
?

?

?
t  T

Example. Fig. 3-a shows two random variables, Xmovie and Xperson, which
dependent on their topical variables Xm and Xp. For computing P(Xmovie = T)
and P(Xperson = T), the parameters movie and person are used, cf. Fig. 2-a.
Assuming P(Xm = t1) = 0.6, P(Xm = t2) = 0.1, and P(Xm = t3) = 0.3, we
get: P(Xmovie = T) = 0.6  0.75 + 0.1  0 + 0.3  0.25 = 0.525.
(2) Relation Predicates. A relation predicate v, r, y connects two query vari-
ables, which have the two topical variables Xv and Xy. Random variable Xr
solely depends on the topics of Xv and Xy. The correlation between relation r
and these topics is given by the relation-topic parameter r:

P(Xr = T | Xv, Xy, r) =
?

?

?
t,t  T

P(Xv = t) r(t, t
?

?

?
) P(Xy = t
?

?

?
)

t,t  T r(t, t)

Example. In Fig. 3-a, we have the variables Xstarring and XbornIn  both
dependent on two topical variables. For instance, Xstarring depends on Xm and
Xp. P(Xstarring = T) is estimated via matrix starring, cf. Fig. 2-b.
(3) String Predicates. For each string predicate v, a, w, there is a random
variable Xw. The word-topic parameter tw represents the probability of observing word w given topic t. Thus, P(Xw = T) is calculated as probability of
observing w, given the topics of vs topical variable, Xv:

P(Xw = T | Xv, 1:K) =
?

?

?
t  T

P(Xv = t)

tw

t  T tw

Example. Fig. 3-a depicts three random variables for string predicates. Given
P(Xm) as in the above example, the probability for holiday is (cf. Fig. 3-b):

P(Xholiday = T) = 0.6  0.011
0.017

+ 0.1  0.002
0.017

+ 0.3  0.004
0.017

= 0.47

Learning Topic Distributions. Finally, we wish to estimate topic distributions for the hidden topical variables based on Xw, Xc, and Xr. We aim at
finding a topic distribution for every topical variable, so that the query probability in Eq. 1 is maximized. Thus, this optimal topic distribution directly gives

A. Wagner et al.

us P(Q). Let vt denote a set of topic parameters for topical random variable
Xv. Further, let  = {vt | v  VQV , t  T } be the set of all parameters vt.
Then, we search for parameter  that maximizes the log-likelihood of Eq. 1:

arg max



L( : Xw, Xc, Xr)

(2)

where L( : Xw, Xc, Xr) is the log-likelihood defined as:
L( : Xw, Xc, Xr) = P(Xw, Xc, Xr | , , , )
?

?

?
=

+

v
?

?

?
Xw  Xv
?

?

?
w

v,y

Xr  Xv,y

r

logP(Xw | Xv, )+

logP(Xr | Xv, Xy, )

v
?

?

?
Xc  Xv

c

logP(Xc | Xv, )

w and Xv
r

c is the set of all string/class random variables having Xv as
is the set of all relation random variables with parents Xv and Xy.
We use gradient ascent optimization to learn the parameter . First, we

where Xv
parent. Xv,y
parametrize each P(Xv = t) with vt such that
evt

P(Xv = t) =

t  T evt

to obtain a valid probability distribution over the topics. Obtaining the gradient
requires dealing with the log of the sum over the topics of each topical variable.
Therefore, we make use of theorem [12]:
Theorem 3. Given a BN and D = {o[1], . . . , o[M ]} as a partially observed
dataset. Let X be a variable in that BN with Pa(X) as its parents. Then:

L( : D)
P(x | pa)

=

1P(x | pa)

M

m=1

P(x, pa | o[m], ),

This provides the necessary form of the gradient. Now, the gradient of the loglikelihood w.r.t. parameter vt is:

L( : Xw, Xc, Xr)

vt

=

L( : Xw, Xc, Xr)
?

?

?
P(Xv = t)
?

?

?
 P(Xv = t)
?

?

?
vt
?

?

?
(3)

(i)

(ii)

The (i)-part of the gradient, Eq. 3, may be obtained via Theorem 3:

L( : Xw, Xc, Xr)

P(Xv = t)
?

?

?
=

1P(Xv = t)
?

?

?
P(Xv = t | Xc, ) +

+

Xc  Xv

c



?

?

?
Xw  Xv
?

?

?
y

Xr  Xv,y

r

P(Xv = t | Xw, )

w


P(Xv = t | Xr, Xy, )

?

?

?
Using Bayes rule we get:

L( : Xw, Xc, Xr)

P(Xv = t)

=

+

+
?

?

?
Xw  Xv

w
?

?

?
P(Xv = t)P(Xw | , t)
t P(Xv = t)P(Xw | , t)
P(Xv = t)P(Xc | , t)
t P(Xv = t)P(Xc | , t)
?

?

?
P(Xv = t)
t P(Xv = t)

t P(Xr | Xy, , t

t P(Xr | Xy, , t)

)
?

?

?
Xc  Xv
?

?

?
c
?

?

?
y

Xr  Xv,y

r

Finally, the (ii)-part of the gradient in Eq. 3 is given by:

P(Xv = t)

tv

=
?

?

?
etv

(

tt etv
t et v )2

Time Complexity. Query probability estimation has a complexity bound:
Theorem 4 (Time Complexity for P(Q) Estimation). Given k topics and
a query Q, the time for computing P(Q) in Eq. 1 is in O(  |Q|  k), with  as
number of iterations needed for optimization and |Q| as # predicates in Q.
Proof. A proof is given in [22]
?

?

?
Note,  is determined by the specific algorithm used for optimization. So, overall
complexity for computing P(Q) is independent of the synopsis size given the
topics.

4 Evaluation

We conducted experiments to analyze the effectiveness (I.1) and efficiency (I.2)
of TopGuess. Overall, our results are very promising: we achieved up to 88% more
accurate selectivity estimates, while runtime was comparable to the baselines.
Further, in contrast to the baselines, we noted TopGuesss runtime behavior to
be much less influenced by the synopsis size  thereby confirming Thm. 4.

4.1 Evaluation Setting

Systems. We employ two categories of baselines: (1) String predicates are combined with structured predicates via an independence assumption: IND baseline.
That is, the selectivity of string predicates and structured predicates is estimated
using two separate synopses: a string synopsis (explained below) and a synopsis
for structured RDF data based on histograms [10]. Obtained probabilities are
combined in a greedy fashion while assuming independence. (2) We reuse our
previous work on BNs for text-rich data graphs [23]: BN baseline. Here, all query
predicates are captured uniformly via a single BN. To handle string predicates,
we employ n-gram string synopses [24]

A n-gram synopsis summarizes the vocabulary by omitting certain n-grams.
Thus, a synopsis represents a subset of all possible n-grams occurring in the

A. Wagner et al.

Table 1. Data synopsis memory/disk space in MB

BN/IND

Mem. {2, 4, 20, 40}  0.1
281.7
Disk

TopGuess

TopGuess

BN/IND

{2, 4, 20, 40}  0.1
229.9

data. A simplistic strategy is to choose random n-gram samples from the data.
Another approach is to construct a top-k n-gram synopsis. For this, n-grams are
extracted from the data together with their counts. Then, the k most frequent
n-grams are included in the synopsis. Further, a stratified bloom filter (SBF)
synopsis has been proposed [24], which uses bloom filters as a heuristic map that
projects n-grams to their counts. Note, we refer to omitted n-grams as missing.
The probability for missing n-grams cannot be estimated with a probabilistic
framework, as such strings are not included in a sample space. So, a string
predicate featuring a missing n-gram is assumed to be independent from the
remainder of the query. Its probability is computed via a heuristic. We employ
the leftbackoff strategy, which finds the longest known n-gram that is the preor postfix of the missing n-gram. Then, the probability of the missing n-gram is
approximated based on statistics for its pre-/postfix [24].

Combining string synopses with the two categories of baselines yields six sys-
tems: INDsample, INDtop-k, and INDSBF rely on the independence assumption,
while BNsample, BNtop-k, and BNSBF represent BN approaches.

Data. We employ two real-world RDF datasets: DBLP [15] and IMDB [6]. From
both datasets we have large vocabularies: 25, 540, 172 (DBLP) and 7, 841, 347
(IMDB) words. Note, while DBLP and IMDB feature text-rich attributes, they
differ in their overall amount of text. On average an attribute in DBLP contains
2.6 words, with a variance of 2.1 words. In contrast, IMDB attributes contain
5.1 words, with a variance of 95.6 words. Moreover, we observed during learning
of the BN baseline that there are more data correlations in IMDB than in DBLP.
We expect correlations between query predicates to have a strong influence on
the system effectiveness.

Queries. We used IMDB [6] and DBLP [15] keyword search benchmarks: We
generated 54 DBLP queries from [15]. Further, we constructed 46 queries for
IMDB based on queries in [6]. We omitted 4 queries from [6], as they could not
be translated to conjunctive queries. Overall, our load features 100 queries with:
0-4 relation, 1-7 string, 1-4 class predicates, and 2-11 predicates in total. Further
query statistics and a complete query listing can be found in [22].

Synopsis Size. We employ baselines with varying synopsis size. For this, we
varied # words captured by the string synopsis. The top-k and sample synopsis contained # words  {0.5K, 1K, 5K, 10K}. The SBF string synopsis captured {2.5K, 5K, 25K, 50K} words for each attribute. Note, SBF systems featured most keywords occurring in our query load. Different string synopsis sizes
translated to a memory consumption of baselines  {2, 4, 20, 40} MB. IND and
?

?

?
BN baselines load their synopsis into main memory. In contrast, TopGuess keeps
a large topic model at disk and constructs a small, query-specific BN in memory
at runtime ( 100 KBytes). Table 1 depicts further details.
Implementation and Offline Learning. For IND and BN baselines, we started
by constructing their string synopses. Each synopsis was learned in  1h.

Then, we constructed BN systems based on [23]. That is, we capture words and
structured data elements using random variables and learn correlations between
them, thereby forming a BN structure. For efficient selectivity estimation the
network is reduced to a lightweight model capturing solely the most important
correlations. Then, we calculate model parameters (CPDs) based on frequency
counts. For IND systems, we do not need the model structure and merely keep
the marginalized BN parameters. Structure and parameter learning combined
took up to 3h. To compute query selectivities the BN systems need inferencing
strategies. For this, we used a Junction tree algorithm [23].

TopGuess exploits an off-the-shelf TRM from [2]. The number of topics is
an important factor  determining which correlations are discovered. We experimented with a varying number of topics  [10, 100]. We found 50 topics are
sufficient to capture all strong correlations in our datasets. The TopGuess learning took up to 5h and its parameters were stored on hard disk, cf. Table 1. At
query time, we employed a greedy gradient ascent algorithm for learning the
topic distributions. To avoid local maxima, we used up to 10 random restarts.

We implemented all systems in Java 6. Experiments were run on a Linux server
with: 2 Intel Xeon CPUs at 2.33GHz, 16 GB memory assigned to the JVM, and
a RAID10 with IBM SAS 10K rpm disks. Before each query execution we cleared
OS caches. Presented values are averages over five runs.

4.2 Selectivity Estimation Effectiveness

We employ the multiplicative error metric (me) [7] for measuring effectiveness:

me(Q) =

max{Fe(Q),Fa(Q)}
min{Fe(Q),Fa(Q)}

with Fe(Q) and Fa(Q) as exact and approximated selectivity. Intuitively, me(Q)
is the factor to which Fa(Q) under-/overestimates Fe(Q).
Overall Results. Figs. 4-a/e (b/f) show the multiplicative error vs. synopsis
size (# predicates) for DBLP and IMDB. Baseline system effectiveness strongly
varies with their synopsis size. In particular, for small synopses  20 MB IND and
BN performed poorly. We explain this with missing words in their string synopses,
which led to heuristics being used for probability computation. In simple terms,
IND and BN systems traded synopsis space for estimation accuracy.

TopGuess, on the other hand, did not suffer from this issue. All its parameters
(cf. Sect. 3.1) could be stored at disk and solely the query-specific BN was loaded
at runtime. Thus, TopGuess could exploit very fine-grained probabilities and
omitted any kind of heuristic. We observed that TopGuess reduced the error
of the best BN system, BNSBF, by 88% (33%) for IMDB (DBLP). Further, we
outperformed the best IND system, INDSBF, by 99% (35%) on IMDB (DBLP).
?

?

?
1E+03 

1E+02 

 
.
r
r

.
i
l

p
i
t
l
u

1E+3 

1E+2 

 
.
r
r

.
i
l

p
i
t
l
u

1E+01 

Synopsis Size (MByte) 

0.5 

1E+1 

Num. of Predicates 
[2,3] 

[4,5] 

[6,7]  

1E+3 

(c) 

1E+3 

(d) 

 
)
s

m

(
 
e
m

i

1E+2 

Synopsis Size  (MByte) 

)
s

m

(
 
e
m

i

1E+2 

 
 
.
r
r

.
i
l

p
i
t
l
u

1E+4 

1E+3 

1E+2 

1E+1 

Synopsis Size (MByte) 

0.5 

1E+3 

(g) 

1E+2 

 
)
s

m

(
 
e
m

i

1E+3 

1E+2 

 
.
r
r

.
i
l

p
i
t
l
u

1E+1 

1E+3 

Num. of Predicates 

[2,4] 

[5,6] 

[7,11] 

(h) 

1E+2 

 
)
s

m

(
 
e
m

i

A. Wagner et al.

(a) 

1E+4 

(b) 

1E+5 

(e) 

1E+4 

(f) 

Num. of Predicates 
[2,3] 

[4,5] 

[6,7] 

1E+1 

Synopsis Size (MByte) 

1E+1 

0.5 

Num. of Predicates 

[2,4] 

[5,6] 

[7,11] 

Fig. 4. Effectiveness: (a)+(b) for DBLP and (e)+(f) for IMDB. Efficiency: (c)+(d) for
DBLP and (g)+(h) for IMDB. Y-axes are given in logarithmic scale.

Synopsis Size. Figs. 4-a/e show estimation errors w.r.t. in-memory synopsis
size. An important observation is that the synopsis size is a key factor for effec-
tiveness. Top-k and sample-based string synopsis systems were strongly affected
by their (string) synopsis size. Given a small synopsis  4 MB, we observed that
top-k/sample-based systems performed poorly. Here, many relevant query keywords were missed, leading to inaccurate heuristics being applied. With increasing synopsis size  [4, 20] MB, the performance of top-k approaches converged to
the most accurate baseline (SBF-based systems). For instance given 4 MB space,
the BNtop-k approach preformed 95% worse than BNSBF on IMDB, but only 33%
worse given 20 MB. Further, we noted SBF-based approaches to perform fairly
stable. We explain this with SBF systems using bloom filters as an effective
summary. Such systems were able to capture most query keywords. Thus, few
heuristic-based estimations were necessary. However, SBF-based systems also
have a limited memory space and must eventually omit words.
In contrast, we observed TopGuess to use  0.1 MB memory for IMDB as
well DBLP. We explain this extremely compact BN with: (1) TopGuess has a
network size, which is bound by the query size. (2) The BN only contains random
variables that either are binary or have a sample space, which is bounded by the
number of topics. For example, over the DBLP query load TopGuess needed on
average 40 KB. Yet, TopGuess resolves the issue of missing words completely:
the TopGuess parameters (stored on disk) capture all words in the vocabulary.
At runtime, TopGuess retrieves the necessary statistics for a particular query
and constructs its query-specific BN. This way, TopGuess achieved up to by
88% (33%) better results on IMDB (DBLP) than the best baselines.

Overall, we can conclude that estimation effectiveness is driven by accurate
string predicate probabilities. Thus, there is a strong need for a data synopsis
allowing for extensive word/text data statistics.

Correlations. We found system performances to vary for IMDB and DBLP.
For the IMDB dataset, BNSBF could reduce errors of the INDSBF approach by up
?

?

?
to 93%. On the other hand, for DBLP improvements were much smaller. These
differences are due to the varying degree of correlations in our two datasets.
While learning the BNs for BN, we observed less correlations in DBLP than in
IMDB. For instance, for DBLP queries with string predicates name and label,
we noted no significant correlations. Thus, the probabilities obtained from BN
systems were almost identical to the ones from IND.

In contrast, even for the less correlated dataset DBLP, TopGuess outperforms
the best baselines, INDSBF and BNSBF, by 35% and 33%. We explain this our a
fine-grained, query-specific BN. More precisely, we observed that BN approaches
exploited data correlations, which were observed in the data graph. However,
TopGuess captured even minor correlations via its topic mixtures at query time
 learned for each query individually.

Query Size. We depict the multiplicative error vs. # query predicates in Figs. 4-
b/f. As expected, estimation errors increase for all systems in # predicates.
For our baselines, we explain this behavior with: (1) Given an increasing #
predicates, the chances of missing a query keyword increase. (2) When missing
a single query keyword, the error is propagated throughout the computation.
However, while the TopGuess approach also led to more misestimates for
larger queries, the degree of this increase was smaller. For instance, considering
IMDB queries with 7-11 predicates, we could observe that TopGuess performs
much more stable than BN or IND baselines, cf. Fig. 4-f.

4.3 Selectivity Estimation Efficiency
We now analyze the estimation efficiency vs. synopses size (# query predicates),
cf. Figs. 4-c/g (d/h). For TopGuess, the reported times comprise parameter
loading, BN construction, and topic learning. For BN and IND, the times represent
only selectivity computation, i.e., no model learning or parameter loading.

Overall Results. Considering BN and IND systems, we saw that their string
synopsis was a key performance factor. Intuitively, the more words were missed,
the simpler and the more efficient these systems became. However, such gains
came at the expense of effectiveness: the fastest baseline system, INDsample, also
computed the least accurate selectivity estimates.

Comparing the two systems with the best effectiveness, TopGuess and BNSBF,
TopGuess led to a better performance by up to 45%. Unfortunately, in comparison to top-k systems, TopGuess resulted in a performance decrease of 40%. We
explain these drawbacks with the time-consuming disk I/O, which was needed for
loading the statistics. However, while BN and IND clearly outperformed TopGuess
w.r.t. small synopses  4 MB, TopGuess results are comparable for synopses
 20 MB. We expect such effects to be more drastic for large BN/IND synopses
 100 MB. So, TopGuess guarantees a much more stable behavior.
Synopsis Size. Figs. 4-c/g show time vs. synopsis size. For the baselines, we
saw a correlation between synopsis size and runtime behavior: While BN and IND
reach a high efficiency for synopses  4 MB, their performance decreases rapidly
for synopses  20 MB. We explain this effect with the larger CPDs, which led
to longer probability estimation times. We observed SBF-based approaches to

A. Wagner et al.

be less driven by their synopsis size. This is because their computational costs
are mainly determined by bloom filters. In contrast, TopGuess did not suffer
from this issue at all. That is, for a given query, TopGuess only loads/processes
statistics necessary for that query. All others statistics are kept on disk.

Query Size. All systems had increasing estimation times in query size, cf.
Figs. 4-d/h. This is because each additional query predicate translated to more
probability computations. However, as TopGuess exploits a compact queryspecific BN, we expected its performance to be less influenced by query size. To
confirm this, we compared the standard deviation of the estimation time w.r.t. a
varying # query predicates. For instance, the standard deviations was 82.48 ms
(213.48 ms) for TopGuess (BN). The low deviation for TopGuess indicates that
its probability estimation times varied less than those from BN systems.

5 Related Work

For selectivity estimation on structured data, existing works exploit various data
synopses, e.g., join samples [1], graph synopses [18], or graphical models [9,21,23].
In particular, those synopses have been applied for selectivity estimation of structured SPARQL/BGP queries on RDF data, e.g., [7,10,16,19].

Unfortunately, such synopses can not effectively capture statistics for combinations of words and structured data elements. In order to guarantee a manageable synopsis size, various summarization techniques are commonly applied on
words, e.g., hashing [19], heuristics [20], categorization [16], discretization [10],
or string synopses [23] (I.1, Sect. 1). Moreover, since the offline constructed
data synopsis is directly used for the selectivity estimation at runtime, existing
works must either keep the data synopsis small [10,16,19,23] or solely employ
heuristics [20] (I.2, Sect. 1). In contrast, our TopGuess approach overcomes both
issues (I.1 and I.2) by relying on relational topic models as data synopsis and a
query-specific BN for selectivity estimation.

With regard to selectivity estimation on text data, language models and other
machine learning techniques have been employed [5,11,13,24]. More specifically,
some works aim at substring or fuzzy string matching [5,13], while other approaches target extraction operators, e.g., dictionary-based operators [24].
However, such works do not consider correlations among multiple string predicates or correlations between string predicates and structured query predicates.

6 Conclusion

We proposed a holistic approach (TopGuess) for selectivity estimation of hybrid
queries. We showed space and time complexity bounds for TopGuess. Further, we
conducted empirical studies on real-world data and achieved strong effectiveness
improvements, while not requiring additional runtime.

As a future work, we plan to extend TopGuess in order to become a more
generic selectivity estimation approach for RDF data and BGP queries, respec-
tively. For this, we replace the topic distributions in our data synopsis with
different application-specific probability distributions, e.g., a continuous distribution for estimating the selectivity of range queries over numerical data.
?

?

