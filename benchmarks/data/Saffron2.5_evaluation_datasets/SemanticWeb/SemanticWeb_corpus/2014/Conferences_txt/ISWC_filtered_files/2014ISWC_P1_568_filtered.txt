OBDA: Query Rewriting or Materialization?

In Practice, Both!

Juan F. Sequeda1, Marcelo Arenas2, and Daniel P. Miranker1

1 Department of Computer Science, The University of Texas at Austin

2 Department of Computer Science, PUC Chile

Abstract. Given a source relational database, a target OWL ontology and a mapping from the source database to the target ontology, Ontology-Based Data Access (OBDA) concerns answering queries over the target ontology using these
three components. This paper presents the development of UltrawrapOBDA, an
OBDA system comprising bidirectional evaluation; that is, a hybridization of
query rewriting and materialization. We observe that by compiling the ontological
entailments as mappings, implementing the mappings as SQL views and materializing a subset of the views, the underlying SQL optimizer is able to reduce the
execution time of a SPARQL query by rewriting the query in terms of the views
specified by the mappings. To the best of our knowledge, this is the first OBDA
system supporting ontologies with transitivity by using SQL recursion. Our contributions include: (1) an efficient algorithm to compile ontological entailments
as mappings; (2) a proof that every SPARQL query can be rewritten into a SQL
query in the context of mappings; (3) a cost model to determine which views to
materialize to attain the fastest execution time; and (4) an empirical evaluation
comparing with a state-of-the-art OBDA system, which validates the cost model
and demonstrates favorable execution times.

1 Introduction

Given a source relational database, a target OWL ontology and a mapping from the
relational database to the ontology, Ontology-Based Data Access (OBDA) concerns
answering queries over the target ontology using these three components. Commonly,
researchers have taken two approaches to developing OBDA systems: materialization
or rewriting. In the materialization approach, the input relational database D, target
ontology O and mapping M (from D to O) are used to derive new facts that are stored
in a database Do, which is the materialization of the data in D given M and O. Then
the answer to a query Q over the target ontology is computed by directly posing Q over
Do [3]. In the rewriting approach, three steps are executed. First, a new query Qo is
generated from the query Q and the ontology O: the rewriting of Q w.r.t to O. The
majority of the OBDA literature focuses on this step [19]. Second, the mapping M is
used to compile Qo to a SQL query Qsql over D [21,22]. Finally, Qsql is evaluated on
the database D, which gives us the answer to the initial query Q.

We develop an OBDA system, UltrawrapOBDA, which combines materialization and
query rewriting. Our objective is to effect optimizations by pushing processing into the
Relational Databases Management Systems (RDBMS) and closer to the stored data,

P. Mika et al. (Eds.) ISWC 2014, Part I, LNCS 8796, pp. 535551, 2014.
c Springer International Publishing Switzerland 2014

J.F. Sequeda, M. Arenas, and D.P. Miranker

hence making maximal use of existing SQL infrastructure. We distinguish two phases:
a compile and runtime phase. In the compile phase, the inputs are a relational database
D, an ontology O and a mapping M from D to O. The first step is to embed in M the
ontological entailments of O, which gives rise to a new mapping M, the saturation of
M w.r.t. O. The mapping M is implemented using SQL views. In order to improve
query performance, an important issue is to decide which views should be materialized.
This is the last step of the compilation phase. In the runtime phase, the input is a query
Q over the target ontology O, which is written in the RDF query language SPARQL,
and the problem is to answer this query by rewriting it into some SQL queries over the
views. A key observation at this point is that some existing SQL optimizers are able to
perform rewritings in order to execute queries against materialized views.

To the best of our knowledge, we present the first OBDA system which supports ontologies with transitivity by using SQL recursion. More specifically, our contributions
are the following. (1) We present an efficient algorithm to generate saturated mappings.
(2) We provide a proof that every SPARQL query over a target ontology can be rewritten
into a SQL query in our context, where mappings play a fundamental role. It is important to mention that such a result is a minimal requirement for a query-rewriting OBDA
system relying on relational database technology. (3) We present a cost model that help
us to determine which views to materialize to attain the fastest execution time. And
(4) we present an empirical evaluation using (i) Oracle, (ii) two benchmarks including
an extension of the Berlin SPARQL Benchmark, and (iii) six different scenarios. This
evaluation includes a comparison against a state-of-the-art OBDA system, and its results
validate the cost model and demonstrate favorable execution times for UltrawrapOBDA.

Related Work. This research builds upon the work of Rodriguez-Muro et. al. implemented in Ontop [24,25] and our previous work on Ultrawrap [27]. Rodriguez-Muro
et. al. uses the tree-witness rewriting algorithm and introduced the idea of compiling
ontological entailments as mappings, which they named T -Mappings. There are three
key differences between Rodriguez-Muro et. al. and our work in this paper: (1) we
have extended the work of Rodriguez-Muro et. al. to support more than hierarchy of
classes and properties, including transitivity; (2) we introduce an efficient algorithm
that generates saturated mappings while Rodriguez-Muro et. al. has not presented an
algorithm before; and (3) we represent the mappings as SQL views and study when the
views should be materialized. Ultrawrap is a system that encodes a fix mapping, the
direct mapping [4,26], of the database as RDF. These mappings are implemented using unmaterialized SQL views. The approach presented in this paper extends Ultrawrap
in three important aspects: (1) supports a customized mapping language; (2) supports
reasoning through saturated mappings; and (3) considers materializing views for query
optimization. Another related work is the combined approach [16], which materializes
entailments as data, without considering mappings, and uses a limited form of query
rewriting. The main objective of this approach is to deal with the case of infinite mate-
rialization, which cannot occur for the type of ontologies considered in this paper.
?

?

?
2 Preliminaries

Relational Databases. Assume, a countably infinite domain D. A schema R is a finite
set of relation names, where for each R  R, att(R) denotes the nonempty finite set
of attributes names associated to R and arity(R) is the arity of R (that is, arity(R) is
the number of elements of the set att (R)). An instance I of R assigns to each relation
symbol R  R a finite set RI = {t1, . . . , t} of tuples, where each tuple tj (1  j  )
is a function that assigns to each attribute in att(R) a value from D. We use notation
t.A to refer to the value of a tuple t in an attribute A. Moreover, we say that R(t) is a
fact in I if t  RI, and we use notation R(t)  I in this case (that is, we also view
instances as sets of facts).

Example 1. We use a relational database for an organization as a running example. The
schema of this database consists of the table EMP with att(EMP) = {SID, NAME, JOB},
this schema: I = {EMP(1, Alice, CEO),
and the following is an instance of
EMP(2, Bob, JavaProgrammer), EMP(3, John, SysAdmin)}.

In what follows, we assume some familiarity with the syntax and semantics of firstorder logic. In particular, we assume that a formula over a relational schema R is constructed by using the relation names in R, the equality predicate = and the elements
(also referred as constants) in D. Moreover, a tuple of variables is denoted by  x and
a tuple of elements from D is denoted by  c, notation ( x) is used to indicate that the
free variables of  are exactly the variables in  x, and ( c) is the formula obtained from
( x) by replacing every variable in  x by the corresponding element in  c. Finally, given
an instance I over a relational schema R and a set  of first-order formulae over R,
notation I |=  is used to indicate that a first-order formula  over R holds in I, while
notation  |=  is used to indicate that  is implied by .

RDF and Ontologies. Assume there are disjoint countably infinite sets U (URIs) and
L (literals). A tuple (s, p, o)  U  U  (U  L) is called an RDF triple,1 where s is
the subject, p is the predicate and o is the object. A finite set of RDF triples is called an
RDF graph.
In order to define the notion of ontology, define O as the following set of reserved keywords:{subClass, subProp, dom, range, type, equivClass, equivProp,
inverse, symProp, transProp}, and assume that O  U. Moreover, following [28]
say that an RDF triple (a, b, c) is ontological if : (1) a  (U  O), and (2) either
b  (O  {type}) and c  (U  O), or b = type and c is either symProp or
transProp. Additionally, say that an RDF triple (a, b, c) is assertional if (a, b, c) is not
ontological. Then an ontology O is simply defined as a finite set of ontological triples.
The semantics of an ontology O is usually defined by representing it as a set of description logic axioms, and then relying on the semantics of this logic [5] (which, in turn,
is inherited from the semantics of first-order logic). For our purpose, it is more convenient to directly define a set O of first-order formulae encoding the ontology O. More
precisely, assume that triple is a ternary predicate that is used to store RDF graphs in

1 For simplicity, we do not consider blank nodes as a skolemization process can be used to

replace them by URIs.
?

?

?
the obvious way: every triple (a, b, c)  G is stored as triple(a, b, c). Then for every
triple t  O, define a first-order formula t over triple as follows:
(a,subClass,b) = x (triple(x, type, a)  triple(x, type, b))
(a,subProp,b) = xy (triple(x, a, y)  triple(x, b, y))

(a,dom,b) = xy (triple(x, a, y)  triple(x, type, b))
(a,range,b) = xy (triple(x, a, y)  triple(y, type, b))
(a,equivClass,b) = x (triple(x, type, a)  triple(x, type, b))
(a,equivProp,b) = xy (triple(x, a, y)  triple(x, b, y))
(a,inverse,b) = xy (triple(x, a, y)  triple(y, b, x))
(a,type,symProp) = xy (triple(x, a, y)  triple(y, a, x))
(a,type,transProp) = xyz (triple(x, a, y)  triple(y, a, z)  triple(x, a, z)),

and define O as {t | t  O}.

Note that subClass, subProp, dom, range, type, equivClass, equivProp are in
RDFS [7], inverse, symProp are in OWL 2 QL but not in OWL 2 EL and transProp
is in OWL 2 EL but not in OWL 2 QL [17]. Actually, the ontologies we consider are in
the non-standard profiles known as RDFS-Plus [1], OWL-LD2 and RDFS 3.03.

In an OBDA system, we need to map relational databases into RDF graphs, which
forces us to transform every constant from D into either a URI or a literal. This process
is usually carried over by using some built-in transformation functions [4,26]. For the
sake of simplicity, we assume that D = (U  O)  L, which allows us to use the
constants in a relational database directly as URIs or literals in an RDF graph, and
which also allows us to view a set of facts of the form triple(a, b, c) as an instance
over the relational schema {triple}. Notice that the keywords in O are not allowed in
D, as these as reserved for the specification of ontologies.

3 Mapping Relational Databases to RDF for OBDA

We describe how mappings are handled in our approach. We start by defining the mappings from relational databases to RDF, which are called RDB2RDF mappings, and
then introduce the notion of saturation of an RDB2RDF mapping w.r.t. an ontology,
which plays a fundamental role in our approach. We provide an efficient algorithm
to compute it for ontologies not containing transitive predicates, and then show how
our results can be extended to deal with transitive predicates. Finally, we show how
RDB2RDF mappings are implemented in our system.

3.1 Relational Databases to RDF Mappings

We introduce the notion of mapping from a relation database to RDF, which is denoted
as an RDB2RDF mapping. Two proposals to standardize this notion can be found in
[4,11]. However, we follow here an alternative approach that has been widely used in

2 http://semanticweb.org/OWLLD/
3 http://www.w3.org/2009/12/rdf-ws/papers/ws31
?

?

?
the data exchange [3] and data integration areas [15], and which is based on the use of
first-order logic and its semantics to define mappings. More precisely, given a relational
schema R such that triple  R, a class RDB2RDF-rule  over R is a first-order
formula of the form:

spo x (s,  x)  p = type  o = c  triple(s, p, o),

(1)
where (s,  x) is a domain-independent first-order formula over R and c  D. More-
over, a predicate RDB2RDF-rule  over R is a first-order formula of the form:

spo x (s, o,  x)  p = c  triple(s, p, o),

(2)
where (s, o,  x) is a domain-independent first-order formula over R and c  D. Finally,
an RDB2RDF-rule over R is either a class or a predicate RDB2RDF-rule over R. In
what follows, we omit the universal quantifiers spo x from RDB2RDF rules, and
we implicitly assume that these variables are universally quantify.

Example 2. Consider the relational database from Example 1. Then the following
RDB2RDF rule maps all the instances of the EMP table as instances of the Employee
class: EMP(s, x1, x2)  p = type  o = Employee  triple(s, p, o).

Let R be a relational schema. An RDB2RDF mapping M over R is a finite set of
RDB2RDF rules over R. Given an RDB2RDF mapping M and an instance I over R,
the result of applying M over I, denoted by MI , is an instance over the schema
{triple} that is defined as the result of the following process. For every RDB2RDF
rule of the form (1) and value c1  D, if there exists a tuple of values  d from D such that
I |= (c1,  d),4 then triple(c1, type, c) is included as a fact of MI , and likewise
for every RDB2RDF rule of the form (2). Notice that this definition coincides with
the notion of canonical universal solution in the context of data exchange [3]. Besides,
notice that MI represents an RDF graph and, thus, mapping M can be considered as
a mapping from relational databases into RDF graphs.
Example 3. Consider the relational database from our running example, and let M be
an RDB2RDF mapping consisting of the rule in Example 2 and the following rule:

EMP(s, x1, CEO)  p = type  o = Executive  triple(s, p, o).
If I is the instance from Example 1, then MI consists of the following facts:

(3)

triple(1, type, Employee), triple(2, type, Employee),

triple(3, type, Employee), triple(1, type, Executive).

3.2 Saturation of RDB2RDF Mappings

As mentioned in Section 1, being able to modify an RDB2RDF mapping to embed a
given ontology is a fundamental step in our approach. This process is formalized by
means of the notion of saturated mapping.

4 Given that (s,  x) is domain-independent, there exists a finite number of tuples (c1,  d) such
that I |= (c1,  d).

J.F. Sequeda, M. Arenas, and D.P. Miranker

Table 1. Inference rules to compute saturated mappings

(A, subClass, B) :

(A, subProp, B) :

(A, dom, B) :

(A, range, B) :

(A, equivClass, B)
or (B, equivClass, A)

(A, equivProp, B)
or (B, equivProp, A)

(A, inverse, B)
or (B, inverse, A)

:

:

:

(A, type, symProp) :

(s,  x)  p = type  o = A  triple(s, p, o)
(s,  x)  p = type  o = B  triple(s, p, o)
(s, o,  x)  p = A  triple(s, p, o)
(s, o,  x)  p = B  triple(s, p, o)

(s, o,  x)  p = A  triple(s, p, o)

(s, y,  x)  p = type  o = B  triple(s, p, o)

(s, o,  x)  p = A  triple(s, p, o)

(y, s,  x)  p = type  o = B  triple(s, p, o)
(s,  x)  p = type  o = A  triple(s, p, o)
(s,  x)  p = type  o = B  triple(s, p, o)
(s, o,  x)  p = A  triple(s, p, o)
(s, o,  x)  p = B  triple(s, p, o)
(s, o,  x)  p = A  triple(s, p, o)
(o, s,  x)  p = B  triple(s, p, o)
(s, o,  x)  p = A  triple(s, p, o)
(o, s,  x)  p = A  triple(s, p, o)

Definition 1 (Saturated mapping). Let M and M be RDB2RDF mappings over a
relational schema R and O an ontology. Then M is a saturation of M w.r.t. O if for
every instance I over R and assertional RDF-triple (a, b, c):

MI  O |= triple(a, b, c) iff triple(a, b, c)  MI .

In this section, we study the problem of computing a saturated mapping from a given
mapping and ontology. In particular, we focus on the case of ontologies not mentioning any triple of the form (a, type, transProp), which we denote by non-transitive
ontologies. In Section 3.3, we extend these results to the case of arbitrary ontologies.

In our system, the saturation step is performed by exhaustively applying the inference
rules in Table 1, which allow us to infer new RDB2RDF rules from the existing ones
and the input ontology. More precisely, given an inference rule t: 1
from Table 1, where
t is a triple and 1, 2 are RDB2RDF rules, and given an RDB2RDF mapping M and
2
an ontology O, we need to do the following to apply t: 1
over M and O. First, we have
to replace the letters A and B in t with actual URIs, say a  U and b  U, respectively.5
Second, we need to check whether the triple obtained from t by replacing A by a and B
by b belongs to O, and whether the RDB2RDF rule obtained from 1 by replacing A by
a belongs to M. If both conditions hold, then the inference rule can be applied, and the
consisting of the rules in M and the rule obtained
result is an RDB2RDF mapping M
from 2 by replacing A by a and B by b.

2

5 If t = (A, type, symProp), then we only need to replace A by a.
?

?

?
Example 4. Consider the RDB2RDF rule (3) from Example 3, and assume that we are
given an ontology O containing the triple (Executive, subClass, Employee). Then
by applying the first inference rule in Table 1, we infer the following RDB2RDF rule:
EMP(s, x1, CEO)  p = type  o = Employee  triple(s, p, o).

Given an RDB2RDF mapping M and an ontology O, we denote by SAT(M,O) the
RDB2RDF mapping obtained from M and O by successively applying the inference
rules in Table 1 until the mapping does not change. The following theorem shows that
SAT(M,O) is a saturation of M w.r.t. O, which justifies its use in our system.
Theorem 1. For every RDB2RDF mapping M and ontology O in RDFS, it holds that
SAT(M,O) is a saturation of M w.r.t. O.

Theorem 1 is a corollary of the fact that the first six rules in Table 1 encode the rules
to infer assertional triples from an inference system for RDFS given in [18]. A natural
question at this point is whether SAT(M,O) can be computed efficiently. In our setting,
the approach based on exhaustively applying the inference rules in Table 1 can be easily
transformed into a polynomial time algorithm for this problem. However, if this transformation is done in a na ve way, then the resulting algorithm is not really efficient. For
this reason, we present here an efficient algorithm to compute SAT(M,O) that is linear
in the size of the input RDB2RDF mapping M and ontology O, which are denoted by
)M) and )O), respectively.
Theorem 2. There exists an algorithm that, given an RDB2RDF mapping M and a
non-transitive ontology O, computes SAT(M,O) in time O()M)  )O)).

We now give the main ingredients of the algorithm mentioned in Theorem 2. Fix
a mapping M and an ontology O. In the first place, the algorithm transforms O
into an instance IO over the relational schema {triple}, which satisfies that for
every (a, b, c)  O: (1) if b  {subClass, subProp, dom, range, type}, then
triple(a, b, c)  IO; (2) if b = equivClass, then triple(a, subClass, c)  IO
and triple(c, subClass, a)  IO; (3) if b = equivProp, then triple(a, subProp,
c)  IO and triple(c, subProp, a)  IO; and (4) if b = inverse, then triple(a,
inverse, c)  IO and triple(c, inverse, a)  IO. Obviously, IO can be computed
in time O()O)).
In the second place, the algorithm transforms as follows M into an instance IM over
a relational schema consisting of binary predicates Fclass Fpred, Ch, Rs and Ro. First, for
every class RDB2RDF-rule in M of the form (1), a fresh natural number m is assigned
as an identifier of formula (s,  x), and then fact Fclass(m, c) is included in IM (thus,
Fclass is used to store the class RDB2RDF-rules in M). Second, for every predicate
RDB2RDF-rule in M of the form (2), a fresh natural number n is assigned as an identifier of formula (s, o,  x), and then fact Fpred(n, c) is included in IM (thus, Fpred is
used to store the predicate RDB2RDF-rules in M). Moreover, in this case fresh natural numbers k1, k2 and k3 are assigned as identifiers of formulae (o, s,  x), (s, y,  x)
and (y, s,  x) (where y is a fresh variable), respectively, and then the facts Ch(n, k1),
Ch(k1, n), Rs(n, k2) and Ro(n, k3) are included in IM (thus, these predicates are used
to store some syntactic modifications of formulae that are needed in the inference rules

J.F. Sequeda, M. Arenas, and D.P. Miranker

in Table 1). Finally, in this case fresh natural numbers 1 are 2 are assigned as identifiers of formulae (o, z,  x) and (z, o,  x) (where z is a fresh variable), respectively,
and then the facts Rs(k1, 1) and Ro(k1, 2) are included in IM. It is easy to see that IM
can be computed in time O()M)).
With all the previous terminology, the problem of computing SAT(M,O) can be
reduced to the problem of computing the minimal model of a Datalog program M,O,
which consists of the facts in (IO  IM) together with the following set  of rules
representing the inference rules in Table 1:

triple(X, subClass, Y ), Fclass(U, X)  Fclass(U, Y )
triple(X, subProp, Y ), Fpred(U, X)  Fpred(U, Y )
triple(X, dom, Y ), Fpred(U, X), Rs(U, V )  Fclass(V, Y )
triple(X, range, Y ), Fpred(U, X), Ro(U, V )  Fclass(V, Y )
triple(X, inverse, Y ), Fpred(U, X), Ch(U, V )  Fpred(V, Y )
triple(X, type, symProp), Fpred(U, X), Ch(U, V )  Fpred(V, X),

where X, Y , U and V are variables. Notice that  is a fixed set of rules (it depends
neither on M nor on O), and also that  does not include rules for the keywords
equivClass and equivProp, as these are represented in IO by using the keywords
subClass and subProp, respectively.

M,O is defined as (IO  IM)  
?

?

?
In order to compute the minimal model of M,O, we instantiate the variables in

M,O having the same minimal
the above rules to generate a ground Datalog program 

model as M,O. The key observation here is that 
M,O can be computed in time
O()M)  )O)), which proves Theorem 2 as the minimal model of a ground Datalog
program can be computed in linear time [10] and the time needed to compute (IOIM)
is O()M) + )O)). More precisely, 

, where 
is generated from  as follows. For every fact triple(a, b, c)  IO, we look for the
only rule in  where this fact can be applied, and we replace this rule by a new one
where X is replaced by a and Y is replaced by c (or just X is replaced by a if b = type
and c = symProp). For example, if we consider a triple triple(a, subClass, c), then
we generate the rule triple(a, subClass, b), Fclass(U, a)  Fclass(U, b). Let 1 be
the result of this process. Given that the set of rules  is fixed, we have that 1 can
be computed in time O()O)). Now for every rule  in 1, we do the following to
transform  into a ground rule. We first replace the variable U in  by a value n in
IM. If  also contains a variable V , then we notice that there exists at most one value
m in IM for which the antecedent of the rule could hold, as there exists at most one
value m such that Rs(n, m) holds, and likewise for predicates Ro and Ch. Thus, in this
case we replace variable V in  for such a value m to generate a ground Datalog rule,

and we conclude that the resulting set 
of ground Datalog rules is computed in time
O()M)  )O)) (given that the size of 1 is O()O))). This concludes the sketch of the
proof of Theorem 2.

3.3 Dealing with Transitive Predicates

We show here how the approach presented in the previous section can be extended with
recursive predicates. This functionality is of particular interest as the current work on
?

?

?
OBDA does not consider transitivity, mainly because the query language considered
in that work is SQL without recursion [8]. From now on, given a first-order formula
(x, y), we use TC(x, y) to denote the transitive closure of (x, y). This formula can
be written in many different formalisms. For example, if (x, y) is a conjunction of
relational atoms, then TC(x, y) can be written as follows in Datalog:

(x, y)  TC(x, y),

(x, z), TC(z, y)  TC(x, y).

In our system, TC(x, y) is written as an SQL query with recursion. Then to deal with
an ontology O containing transitive predicates, the set of inference rules in Table 1 is
extended with the following inference rule:

(A, type, transProp) :

{i(s, o,  xi)  p = A  triple(s, p, o)}k
TC[

 xii](s, o)  p = A  triple(s, p, o)

k

i=1

i=1

.

!

This rule tell us that given a transitive predicate A, we can take any number k of
RDB2RDF rules i(s, o,  xi)  p = A  triple(s, p, o) for this predicate, and we can
generate a new RDB2RDF rule for A by putting together the conditions i(s, o,  xi) in
 xii(s, o,  xi), and then using the transitive closure TC(s, o)
a formula (s, o) =
of  in an RDB2RDF rule TC(s, o)  p = A  triple(s, p, o). In order for this
approach to work, notice that we need to extend the syntax of RDB2RDF rules (1) and
(2), so that formulae  and  in them can be arbitrary formulae in a more expressive
formalism such as (recursive) Datalog.

i

3.4 Implementing RDB2RDF Mappings as Views
We conclude this section by showing how RDB2RDF mappings are implemented in
our system. Inspired by our previous work on Ultrawrap [27], every RDB2RDF rule
is implemented as a triple-query, that is, as a SQL query which outputs triples. For
example, the RDB2RDF rules:

EMP(s, x1, CEO)  p = type  o = Employee  triple(s, p, o)
EMP(s, x1, SysAdmin)  p = type  o = Employee  triple(s, p, o)

give rise to the following triple-queries:

SELECT SID as S, type as P, Employee as O FROM EMP WHERE JOB = CEO

SELECT SID as S, type as P, Employee as O FROM EMP WHERE JOB = SysAdmin

In practice, the triple-queries may include additional projections in order to support
indexes, URI templates, datatypes and languages. However, for readability, we will
consider here this simple version of these queries (we refer the reader to [27] for specific details). Then to implement an RDB2RDF mapping, all the class (resp. predicate)
RDB2RDF-rules for the same class (resp. predicate) are grouped together to generate a
triple-view, that is, a SQL view comprised of the union of the triple-queries for this class
(resp. predicate). For instance, in our previous example the following is the triple-view
for the class Employee:

CREATE VIEW EmployeeView AS

SELECT SID as S, type as P, Employee as O FROM EMP WHERE JOB = CEO UNION ALL

SELECT SID as S, type as P, Employee as O FROM EMP WHERE JOB = SysAdmin

J.F. Sequeda, M. Arenas, and D.P. Miranker

4 Executing SPARQL Queries

In this section, we describe how SPARQL queries are executed and optimized over the
RDBMS through a cost model that determines which views should be materialized.

4.1 SPARQL Rewriting
The runtime phase executes SPARQL queries on the RDBMS. We reuse Ultrawraps approach of translating SPARQL queries to SQL queries in terms of the views defined for
every class and property, which are denoted as triple-views in our system (see Section
3.4). Thus, we make maximal use of existing query optimization tools in commercial
RDBMS, such as Oracle, to do the SPARQL query execution and rewriting [27].
Continuing with the example in Section 3.4, consider now a SPARQL query which
asks for all the Employees: SELECT ?x WHERE {?x type Employee}. It is clear that this
query needs to be rewritten to ask for the CEO and SysAdmin. The EmployeeView
triple-view in Section 3.4 implements the mappings to the Employee class which
consists of two triple-queries, one each for CEO and SysAdmin. Therefore, it is
sufficient to generate a SQL query in terms of the EmployeeView. Given that a
triple-view models a table with three columns, a SPARQL query is syntactically
translated to a SQL query in terms of the triple-view. The resulting SQL query is
SELECT t1.s AS x FROM EmployeeView t1.

A natural question at this point is whether every SPARQL query has an equivalent SQL query in our context, where RDB2RDF mappings play a fundamental role.
In what follows we give a positive answer to this question, but before we introduce
some terminology. Due to the lack of space, we do not formally present the syntax and
semantics of SPARQL. Instead, we refer the reader to [23,20] for this definition, and
we just point out here that PG denotes the answer to a SPARQL query P over an
RDF graph G, which consists of a set of solution mappings, that is, a set of functions
that assign a value to each selected variable in P . For example, if P is the SPARQL
query SELECT ?x WHERE {?x type ?y}, and G is an RDF graph consisting of the triples
(1, type, Employee) and (2, type, Employee), then PG = {1, 2}, where 1 and
2 are functions with domain {?x} such that 1(?x) = 1 and 2(?x) = 2. Moreover,
given a SQL query Q (that may use recursion) over a relational schema R and an instance I of R, we use notation QI to represent the answer of Q over I, which consists
of a set of tuples in this case. Finally, to compare the answer of a SQL query with the
answer of a SPARQL query, we make use of a function tr to transform a tuple into a
solution mapping (this function is defined in the obvious way, see [26]). Then given an
RDB2RDF mapping M over a relational schema R and a SPARQL query P , an SQL
query Q over R is said to be a SQL-rewriting of P under M if for every instance I of
R, it holds that PMI = tr(QI ). Moreover, P is said to be SQL-rewritable under
M if there exists a rewriting of P under M.
Theorem 3. Given an RDB2RDF mapping M, every SPARQL query is SQL-rewritable
under M.

The proof that the previous condition holds is by induction on the structure of
a SPARQL query P and, thus, it gives us a (na ve) bottom-up algorithm for translating P into an equivalent SQL query Q (given the mapping M). More precisely,
?

?

?
in the base case we are given a triple pattern t = {s p o}, where each one of
its component is either a URI or a literal or a variable. This triple pattern is first
translated into a SPARQL query Pt, where each position in t storing a URI or a
literal is replaced by a fresh variable, a filter condition is added to ensure that these
fresh variables are assigned the corresponding URIs or literals, and a SELECT
clause is added to ensure that the output variables of t and Pt are the same. For
example, if t = {?x type Employee}, then Pt is the following SPARQL query:
SELECT ?x WHERE {?x ?y ?z} FILTER (?y = type && ?z = Employee). Then
a
SQL-rewriting of Pt under M is computed just by replacing a triple pattern of the form
{?s ?p ?o} by a union of all the triple-queries representing the RDB2RDF rules in M,
and also replacing the SPARQL filter condition in Pt by a filter condition in SQL.

In the inductive step, we assume that the theorem holds for two SPARQL queries
P1 and P2. The proof then continues by presenting rewritings for the SPARQL queries
constructed by combining P1 and P2 through the operators SELECT, AND (or . oper-
ator), OPTIONAL, FILTER and UNION, which is done by using existing approaches
to translate SPARQL to SQL [2,9].

4.2 Cost Model for View Materialization

A common approach for query optimization is to use materialized views [13]. Given
that we are implementing RDB2RDF mappings as views, it is a natural to pursue this
option. There are three implementation alternatives: (1) Materialize all the views: This
approach gives the best query response time. However, it consumes the most space. (2)
Materialize nothing: In this approach, every query needs to go to the raw data. However,
no extra space is needed. (3) Materialize a subset of the views: Try to find a trade-off
between the best query response time and the amount of space required.

In this section, we present a cost model for these three alternatives. First we must
introduce some terminology. We consider ontologies consisting of hierarchy of classes
which form a tree with a unique root, where a root class of an ontology is a class that
has no superclasses. Then a leaf class of an ontology is a class that has no subclasses,
and the depth of a class is the number of subclass relationships from the class to the
root class (notice that there is a unique path from a class to the root class). Moreover,
the depth of an ontology is the maximum depth of all classes present in the ontology.

First, we consider the cost of answering a query Q is equal to the number of rows
present in the relation used to construct Q. For example, if a relation R has 100 rows,
then the cost of the query SELECT  FROM R is 100. Second, assume we have a single relation R and that mappings are from a query on the relation R with a selection
on an attribute A, to a class in the ontology. In Example 3, the relation R is EMP,
the attribute A is JOB and the mapping is to the class Executive. Finally, we consider a query workload of queries asking for the instances of a class in the ontology,
i.e. SELECT ?x WHERE {?x type C}, which can be translated into the triple-view implementing the mapping to the class C.
Our cost model is the following: If all the views implementing mappings are mate-
rialized, the query cost is n  NR  S(A, R) where n is the number of leaf classes
underneath the class that is being queried for, NR is the number of tuples of the relation
R in the mapping, and S(A, R) is the selectivity of the attribute A of the relation R in

J.F. Sequeda, M. Arenas, and D.P. Miranker

the mapping. The space cost is NR + (NR  d) where d is the depth of the ontology.
The reason for this cost is because the number of rows in a materialized view depends
on the selectivity of the attribute and the number of leaf classes. Additionally, the sum
of all the rows of each triple-view representing the mapping to classes in a particular
depth d of an ontology, is equivalent at most to the number of rows of the relation. If no
views are materialized, then the query cost is n  NR, assuming there are no indices.
The space cost is simply NR. The reason for this cost is because to answer a query, the
entire relation needs to be accessed n times because there are no indices6.

The question now is: How can we achieve the query cost of materializing all the
views while keeping space to a minimum? Our hypothesis is the following: If a RDBMS
rewrites queries in terms of materialized views, then by only materializing the views
representing mappings to the leaf classes, the query cost would be n  NR  S(A, R),
the same as if we materialized all the views, and the space cost would only be 2  NR.
The rationale is the following: A triple-view representing a mapping to a class, can be
rewritten into the union of triple-views representing the mapping to the child classes.
Subsequently, a triple-view representing the mapping to any class in the ontology can
be rewritten into a union of triple-views representing the mappings to leaf classes of
an ontology. Finally, given a set of triple-views representing mappings from a relation
to each leaf class of an ontology, the sum of all the rows in the set of triple-views is
equivalent to the number of rows in the relation.

Given the extensive research of answering queries using views [14] and the fact that
Oracle implements query rewriting on materialized views7, we strongly suspect that
our hypothesis will hold. The following evaluation section provides empirical results
supporting our hypothesis.

5 Experimental Evaluation

Benchmarks: The evaluation requires benchmarks consisting of a relational database
schema and data, ontologies, mappings from the database to ontologies and a query
workload. Thus, we created a synthetic benchmark, the Texas Benchmark, inspired by
the Wisconsin Benchmark [12] and extended the Berlin SPARQL Benchmark (BSBM)
Explore Use Case [6]. More precisely, the Texas Benchmark is composed of a single
relation with 1 million rows. The relation has a first attribute which serves as a primary
key, a set of additional filler attributes in order to take up space and then a set of six
different integer-valued attributes which are non-unique. The main purpose of these
attributes is to provide a systematic way to model a wide range of selectivity factors.
Each attribute is named after the range of values the attribute assumes: TWO, FIVE,
TEN, TWENTY, FIFTY and HUNDRED. For example, the attribute FIVE assumes a range
of values from 1 to 5. Thus, the selection FIVE = 1 will have a 20% selectivity. In
addition to the data, we created five different ontologies, consisting of a depth between
2-5. The branching factor is uniform and the number of leaves is 100 for each ontology.

6 In the evaluation, we also consider the case when indices are present.
7 http://docs.oracle.com/cd/B28359 01/server.111/

b28313/qrbasic.htm
?

?

?
The query workload consists of asking for an instance of a class at each
depth of the ontology. On the other hand,
the extension of BSBM replicates the
query workload of an e-commerce web-
site. Products have a type that is part of
a ProductType ontology. Every product
is mapped to one leaf class of the ProductType ontology. In our experiments,
we created a dataset consisting of 1 million products with the benchmark driver,
hence the product table has 1 million
rows. The resulting ProductType ontology has a depth of 4 and consists of
3949 classes from which 3072 are leaflevel classes. The selectivity of the attribute in the mappings to ProductTypes
is approximately 0.1%. In order to replicate the results of the Texas Benchmark,
the query workload also consists of asking for an instance of a class at each
depth of the ontology. In order to evaluate queries with transitivity, we use the
child-parent relationship in the ProductType table which models the subclass re-
lationship. The query workload for the
transitivity part consists of asking for
Products of a particular ProductType including the label and a numeric property
of the Products, therefore including joins. More details about the benchmarks can be
found at http://obda-benchmark.org

Fig. 1. Results of Texas Benchmark (sec)

Measurements and Scenarios: The objective of our experiments is to observe the behavior of a commercial relational database, namely Oracle, and its capabilities of supporting subclass and transitivity reasoning under our proposed approach. Therefore, the
evaluation compares execution time and queries plans of SPARQL queries. With the
Texas Benchmark, we compare w.r.t. two dimensions: depth of an ontology and selectivity of the attribute that is being mapped. In BSBM, because we are using a fixed 1
million product dataset, the depth of the hierarchy and selectivity is also fixed. We ran
each query ten times and averaged the execution time, hence the experiments ran on a
warm cache. In the evaluation we considered six scenarios: (all-mat) all the views are
materialized; (union-leaves) only views representing mappings to the leaf classes are
materialized, implemented with UNION; (or-leaves) same as in the previous scenario
but with the views implemented with OR instead of UNION, (union-index) none of the
views, implemented with UNION, are materialized, instead an index on the respective
attributes have been added, (or-index) same as in the previous scenario but with the

J.F. Sequeda, M. Arenas, and D.P. Miranker

views implemented with OR; and (ontop) we compare against Ontop, a state of the art
OBDA system [25]. We only compare against Ontop because to the best of our knowl-
edge, this is the only OBDA system that supports RDB2RDF mappings and SPARQL.
The experiments were conducted on Oracle 11g R2 EE installed on a Sun Fire X4150
with a four core Intel Xeon X7460 2.66 GHz processor and 16 GB of RAM, running
Microsoft Windows Server 2008 R2 Standard on top of VMWare ESX 4.0.

Results: An initial assessment suggests the following four expected observations: (1)
The fastest execution time is all-mat; (2) our hypothesis should hold, meaning that
the execution time of union-leaves should be comparable, if not equal, to the execution
time of all-mat; (3) given that the Ontop system generates SQL queries with OR instead
of UNION [25], the execution time of ontop and or-index should be comparable if not
equal; (4) with transitivity, the fastest execution time is when the views are materialized.
Figure 1 shows the results of the Texas Benchmark in a form of a heat map, which
evaluates subclass reasoning. The darker colors corresponds to the fastest query execution time. The x-axis consists of the six scenarios. In the y-axis, D6 100 means Depth 6
on Selectivity of 100. The values are the average execution time of the query workload.
Notice that the expected observations (1), (2) and (3) hold. The fastest execution time
corresponds to all-mat. The execution time of union-leaves is comparable, if not equal,
to the execution time of all-mat, because Oracle was able to rewrite queries in terms
of the materialized views. The number of rows examined is equivalent to the number
of rows in the views where everything was materialized. This result provides evidence
supporting our hypothesis and validates our cost model. Finally the execution time of
ontop and or-index are comparable.

Fig. 2. Results of Subclass reasoning on BSBM

Figure 2 shows the results of the BSBM Benchmark for subclass reasoning. Our
expected observations also hold in this case. Note that we do not report results for
Ontop because the setup of the SPARQL endpoint timed-out after 2 hours.8 Given that
the selectivity is much lower compared to the selectivities in the Texas Benchmark, we
observe that for queries asking for instances of classes that are in depth 1 (child of the
root Class), the or-index outperforms union-leaves. We speculate that there is a slight

8 We have reported the issue to the Ontop developers.
?

?

?
overhead when rewriting queries over a large amount of views. However, for the rest of
the queries, the overhead diminishes. We observe that the execution time of or-leaves
is the worst because the database is not able to rewrite the query into the materialized
views when the views are implemented with OR. Finally, throughout both benchmarks,
we observe that or-index is competitive w.r.t union-leaves.

Figure 3 shows the results of the transitivity experiments on the BSBM Benchmark.
Notice that the expected observations (4) holds. Given that Ontop does not support
transitivity, we cannot compare with them. Therefore we only compare between materialized and non-materialized views. The Simple query requests all the ancestors of the
given ProductType. The Join query requests all ancestors of the given ProductType and
its corresponding Products. Therefore there is a join between ProductType and Product.
The More Join query is similar to Join query, however it requests the name and a numeric property of the products, hence there are more joins. It is clear that materializing
the view outperforms the non-materialized view for the following reasons: when the
view is materialized, the size of the view is known beforehand and the optimizer is able
to do a range scan with the index. However, when the view is not materialized, the size
is not known therefore the optimizer does a full scan of the table. Detailed results can
be found at http://ribs.csres.utexas.edu/ultrawrap.

Fig. 3. Results of Transitivity reasoning on BSBM

6 Concluding Remarks

We presented UltrawrapOBDA, which to the best of our knowledge, is the first OBDA
system supporting ontologies with transitivity by using SQL recursion. UltrawrapOBDA
is able to push processing into the RDBMS by implementing mappings using materialized views and taking advantage of existing query rewriting techniques.

Per related work, existing OBDA approaches only exploit the relational algebra capabilities of RDBMS. Our experimental results provide evidence that existing advanced
capabilities implemented in RDBMS, such as recursion and query rewriting using materialized views, can be utilized for OBDA. We are not saying that we should rely
exclusively on RDBMS technology to do the heavy lifting. RDBMS such as MySQL
lack these advanced optimizations. However, we strongly suggest that the OBDA community should exploit the advanced optimizations in existing RDBMS.

J.F. Sequeda, M. Arenas, and D.P. Miranker

Several open questions remain unanswered: What is the cost of maintaining views
when the underlying data is updated? What is the state of the art of other RDBMSs
optimizers in order to support OBDA? How does this approach respond to complex
query workload? What is the trade-off between reasoning over relational databases with
mappings and using native RDF databases supporting reasoning? We believe that these
questions can be answered by developing systematic and real-world benchmark consisting of relational database schemas, data, ontologies and mappings and evaluating
beyond just query rewriting. The Texas Benchmark and OBDA-Benchmark.org is a
first step in this process and we invite the community to contribute. As future work,
we plan to evaluate UltrawrapOBDAon other RDBMSs and compare against additional
OBDA systems and native RDF databases that support reasoning.

Acknowledgments. Sequeda and Miranker were supported by NSF Grant IIS 1018554.
Arenas was supported by the Millennium Nucleus Center for Semantic Web Research
under Grant NC120004 and Fondecyt grant 1131049. We thank the anonymous referees
for many helpful comments.
