Dynamic Provenance for SPARQL Updates

Harry Halpin and James Cheney

1 World Wide Web Consortium/MIT, 32 Vassar Street Cambridge, MA 02139 USA

hhalpin@w3.org,

2 University of Edinburgh, School of Informatics,

10 Crichton St.,

Edinburgh UK EH8 9AB

Abstract. While the Semantic Web currently can exhibit provenance information by using the W3C PROV standards, there is a missing link in connecting PROV to storing and querying for dynamic changes to RDF graphs using
SPARQL. Solving this problem would be required for such clear use-cases as the
creation of version control systems for RDF. While some provenance models and
annotation techniques for storing and querying provenance data originally developed with databases or workflows in mind transfer readily to RDF and SPARQL,
these techniques do not readily adapt to describing changes in dynamic RDF
datasets over time. In this paper we explore how to adapt the dynamic copypaste provenance model of Buneman et al. [2] to RDF datasets that change over
time in response to SPARQL updates, how to represent the resulting provenance
records themselves as RDF in a manner compatible with W3C PROV, and how
the provenance information can be defined by reinterpreting SPARQL updates.
The primary contribution of this paper is a semantic framework that enables the
semantics of SPARQL Update to be used as the basis for a cut-and-paste provenance model in a principled manner.

Keywords: SPARQL Update, provenance, versioning, RDF, semantics.

1 Introduction

It is becoming increasingly common to publish scientific and governmental data on
the Web as RDF (the Resource Description Framework, a W3C standard for structured
data on the Web) and to attach provenance data to this information using the W3C
PROV standard. In doing so, it is crucial to track not only the provenance metadata,
but the changes to the graph itself, including both its derivation process and a history
of changes to the data over time. Being able to track changes to RDF graphs could, in
combination with the W3C PROV standards and SPARQL, provide the foundation for
addressing the important use-case of creating a Git-like version control system for RDF.
The term provenance is used in several different ways, often aligning with research
in two different communities, in particular the database community and the scientific
workflow community. We introduce some terminology to distinguish different uses of
the term. There is a difference between static provenance that describes data at a given
point in time versus dynamic provenance that describes how artifacts have evolved over
time. Second, there is a difference between provenance for atomic artifacts that expose

P. Mika et al. (Eds.) ISWC 2014, Part I, LNCS 8796, pp. 425440, 2014.
c Springer International Publishing Switzerland 2014

H. Halpin and J. Cheney

no internal structure as part of the provenance record, versus provenance for collections
or other structured artifacts. The workflow community has largely focused on static
provenance for atomic artifacts, whereas much of the work on provenance in databases
has focused on dynamic provenance for collections (e.g. tuples in relations). An example of static provenance would be attaching the name of an origin and time-stamp to a
set of astronomical RDF data. Thus static provenance can often be considered metadata
related to provenance. Currently the W3C PROV data model and vocabulary [15], provides a standard way to attach this provenance, and other work such as PROV-AQ [13]
provides options for extracting this metadata by virtue of HTTP.

An example of dynamic provenance would be given by a single astronomical dataset that is updated over time, and then if some erroneous data was added at a particular
time, the data-set could be queried for its state at a prior time before the erroneous data
was added so the error could be corrected. Thus dynamic provenance can in some cases
be reduced to issues of history and version control. Note that static and dynamic provenance are not orthogonal, as we can capture static provenance metadata at every step of
recording dynamic provenance. The W3C Provenance Working Group has focused primarily on static provenance, but we believe a mutually beneficial relationship between
the W3C PROV static provenance and SPARQL Update with an improved provenanceaware semantic model will allow dynamic provenance capabilities to be added to RDF.
While fine-grained dynamic provenance imposes overhead that may make it unsuited
to some applications, there are use-cases where knowing exactly when a graph was
modified is necessary for reasons of accountability, including data-sets such as private
financial and public scientific data.

1.1 Related Literature

The workflow community has largely focused on declaratively describing causality or
derivation steps of processes to aid repeatability for scientific experiments, and these requirements have been a key motivation for the Open Provenance Model (OPM) [17,18],
a vocabulary and data model for describing processes including (but certainly not limited to) runs of scientific workflows. OPM is important in its own right, and has become
the foundation for the W3C PROV data model [19]. The formal semantics of PROV
have been formalized, but do not address the relationship between the provenance and
the semantics of the processes being described [7]. However, previous work on the semantics of OPM, PROV, and other provenance vocabularies focuses on temporal and
validity constraints [14] and does not address the meaning of the processes being represented  one could in principle use them either to represent static provenance for
processes that construct new data from existing artifacts or to represent dynamic provenance that represents how data change over time at each step, such as needed by version
control systems. Most applications of OPM seem to have focused on static provenance,
although PROV explicitly grapples with issues concerning representing the provenance
of objects that may be changing over time, but does not provide a semantics for storing
and querying changes over time. Our approach to dynamic provenance is complementary to work on OPM and PROV, as we show how any provenance metadata vocabulary
such as the W3C PROV ontology [15] can be connected directly to query and update
?

?

?
languages by presenting a semantics for a generalized provenance model and showing
how this builds upon the formal semantics of the query and update languages.

Within database research, complex provenance is also becoming increasingly seen as
necessary, although the work has taken a different turn than that of the workflow com-
munity. Foundational work on provenance for database queries distinguishes between
where-provenance, which is the locations in the source databases from which the data
was extracted, and why-provenance, which is the source data that had some influence
on the existence of the data [4]. Further, increasing importance is being placed on
how-provenance, the operations used to produce the derived data and other annotations,
such as who precisely produced the derived data and for what reasons. There is less
work considering provenance for updates; previous work on provenance in databases
has focused on simple atomic update operations: insertion, deletion, and copy [2]. A
provenance-aware database should support queries that permit users to find the ultimate
or proximate sources of some data and apply data provenance techniques to understand why a given part of the data was inserted or deleted by a given update. There is
considerable work on correct formalisms to enable provenance in databases [9]. This
work on dynamic provenance is related to earlier work on version control in unstructured data, a classic and well-studied problem for information systems ranging from
source code management systems to temporal databases and data archiving [23]. Version control systems such as CVS, Subversion, and Git have a solid track record of
tracking versions and (coarse-grained) provenance for text (e.g. source code) over time
and are well-understood in the form of temporal annotations and a log of changes.

On the Semantic Web, work on provenance has been quite diverse. There is widely
implemented support for named graphs, where each graph G is identified with a name
URI [6], and as the new RDF Working Group has standardized the graph name in the
semantics, the new standards have left the practical use of such a graph name under-
defined; thus, the graph name could be used to store or denote provenance-related information such as time-stamps. Tackling directly the version control aspect of provenance
are proposals such as Temporal RDF [10] for attaching temporal annotations and a
generalized Annotated RDF for any partially-ordered sets already exist [25,16]. How-
ever, by confining provenance annotations to partially-ordered sets, these approaches do
not allow for a (queryable) graph structure for more complex types of provenance that
include work such as the PROV model. Static provenance techniques have been investigated for RDFS inferences [5,8,20] over RDF datasets. Some of this work considers
updates, particularly Flouris et al. [8], who consider the problem of how to maintain
provenance information for RDFS inferences when tuples are inserted or deleted using
coherence semantics. Their solution uses colouring (where the color is the URI of the
source of each triple) and tracking implicit triples [8].

Understanding provenance for a language requires understanding the ordinary semantics of the language. Arenas et al. formalized the semantics of SPARQL [21,1], and
the SPARQL Update recommendation proposes a formal model for updates [22]. Horne
et al. [12] propose an operational semantics for SPARQL Updates, which however differs in some respects from the SPARQL 1.1 standard and does not deal with named
graphs.

H. Halpin and J. Cheney

1.2 Overview

In this paper, we build on Arenas et al.s semantics of SPARQL [21,1] and extend it
to formalize SPARQL Update semantics (following a denotational approach similar to
the SPARQL Update Formal Model [22]). Then, as our main contribution, we detail a
provenance model for SPARQL queries and updates that provides a (queryable) record
of how the raw data in a dataset has changed over time. This change history includes
a way to insert static provenance metadata using a full-scale provenance ontology such
as PROV-O [15].

Our hypothesis is that a simple vocabulary, composed of insert, delete, and copy operations as introduced by Buneman et al. [2], along with explicit identifiers for update
steps, versioning relationships, and metadata about updates provides a flexible format
for dynamic provenance on the Semantic Web. A primary advantage of our methodology is it keeps the changes to raw data separate from the changes in provenance meta-
data, so legacy applications will continue to work and the cost of storing and providing
access to provenance can be isolated from that of the raw data. We will introduce the
semantics of SPARQL queries, then our semantics for SPARQL updates, and finally
describe our semantics for dynamic provenance-tracking for RDF graphs and SPARQL
updates. To summarize, our contributions are an extension to the semantics of SPARQL
Update that includes provenance semantics to handle dynamic semantics [21,1] and a
vocabulary for representing changes to RDF graphs made by SPARQL updates, and a
translation from ordinary SPARQL updates to provenance-aware updates that record
provenance as they execute.

2 Background: Semantics of SPARQL Queries

We first review a simplified (due to space limits) version of Arenas et. al.s formal semantics of SPARQL [21,1]. Note that this is not original work, but simply a necessary
precursor to the semantics of SPARQL Update and our extension that adds provenance
to SPARQL Update. The main simplification is that we disallow nesting other operations such as ., UNION, etc. inside GRAPH A {. . .} patterns. This limitation is inessential.
Let Lit be a set of literals (e.g. strings), let Id be a set of resource identifiers, and let
Var be a set of variables usually written ?X. We write Atom = Lit  Id for the set of
atomic values, that is literals or ids. The syntax of a core algebra for SPARQL discussed
in [1] is as follows:

A ::=   Lit |   Id | ?X  Var
t ::= A1 A2 A3
C ::= {t1, . . . , tn} | GRAPH A {t1, . . . , tn} | C C

 | R
R ::= BOUND(?x) | A = B | R  R
P ::= C | P . P
 | P FILTER R
Q ::= SELECT ?X WHERE P | CONSTRUCT C WHERE P

 | R  R
 | P OPT P

 | P UNION P
?

?

?
Here, C denotes basic graph (or dataset) patterns that may contain variables; R denotes conditions; P denotes patterns, and Q denotes queries. We do not distinguish
between subject, predicate and object components of triples, so this is a mild generalization of [1], since SPARQL does not permit literals to appear in the subject or predicate position or as the name of a graph in the GRAPH A {P} pattern, although the formal
semantics of RDF allows this and the syntax may be updated in forthcoming work on
RDF. We also do not consider blank nodes, which pose complications especially when
updates are concerned, and we instead consider them to be skolemized (or just replaced
by generic identifiers), as this is how most implementations handle blank nodes. There
has been previous work giving a detailed treatment of the problematic nature of blank
nodes and why skolemization is necessary in real-world work.
The semantics of queries Q or patterns P is defined using functions from graph
stores D to sets of valuations . A graph store D = (G,{gi  G1 . . . , gn  Gn})
consists of a default graph G0 together with a mapping from names gi to graphs Gi.
Each such graph is essentially just a set of ground triples. We often refer to graph stores
as datasets, although this is a slight abuse of terminology.
We overload set operations for datasets, e.g. D  D

or D \ D
pointwise. If a graph g is defined in D and undefined in D

denotes the dataset
obtained by unioning or respectively subtracting the default graphs and named graphs of
, then (DD
D and D
)(g) =
D(g) and similarly if g is undefined in D and defined in D
(g);
if g is undefined in both datasets then it is undefined in their union. For set difference,
if g is defined in D and undefined in D
)(g) = D(g); if g is undefined in
D then it is undefined in (D \ D
A valuation is a partial function  : Var  Lit  Id. We lift valuations to functions
 : Atom  Var  Atom as follows:

). Likewise, we define D  D

then (D  D

)(g) = D

then (D \ D

= D  D

.

as D

(?X) = (X)

(a) = a

a  Atom

that is, if A is a variable ?X then (A) = (X) and otherwise if A is an atom then
(A) = A. We thus consider all atoms to be implicitly part of the domain of . Fur-
thermore, we define  applied to triple, graph or dataset patterns as follows:

(A1 A2 A3) = (A1) (A2) (A3)
({t1, . . . , tn}) = ({(t1), . . . , (tn)},)

(GRAPH A {t1, . . . , tn}) = (,{(A)  {(t1), . . . , (tn)}})
?

?

?
) = (C)  (C
?

?

?
)

(C C

where, as elsewhere, we define D  D
The conditions R are interpreted as three-valued formulas over the lattice L =
{true, false, error}, where false  error  true, and  and  are minimum
and maximum operations respectively, and true = false, false = true, and
error = error. The semantics of a condition is defined as follows:

as pointwise union of datasets.

H. Halpin and J. Cheney

BOUND(?X) =
?

?

?
false if ?X  dom()

true if ?X  dom()
 error if {A, B}  dom()
true if (A) = (B) where A, B  dom()
false if (A) = (B) where A, B  dom()
R = R

A = B =

R  R
R  R

 = R  R
 = R  R




We write  |= R to indicate that R = true.
We say that two valuations , 
variables x  dom()  dom(

valuation   
following operations on sets of valuations .

), we have (x) = 
that behaves like  on dom() and like 
?

?

?
are compatible (or write  compat 
?

?

?
) if for all
(x). Then there is a unique

). We define the

on dom(
?

?

?
1 1 2 = {1  2 | 1  1, 2  2, 1 compat 2}
1  2 = { |   1 or   2}
1 \ 2 = {  1 | 
1  2 = (1 1 2)  (1 \ 2)

  2. compat 

}

Note that union is the same as ordinary set union, but difference is not, since 1 \ 2
only includes valuations that are incompatible with all those in in 2.
Now we can define the meaning of a pattern P in a dataset D as a set of valuations
PD, as follows:

CD = { | dom() = vars(C) and (C)  D}

P1 . P2D = P1D 1 P2D
P1 UNION P2D = P1D  P2D
P1 OPT P2D = P1D  P2D
P FILTER RD = {  PD |  |= R}

Note that, in contrast to Arenas et al.s semantics for SPARQL with named graphs [1],
we do not handle GRAPH A {. . .} patterns that contain other pattern operations such as
. or UNION, and we do not keep track of the current graph G. Instead, since graph
patterns can only occur in basic patterns, we can build the proper behavior of pattern
matching into the definition of (C), and we select all matches  such that (C)  D
in the case for CD.
?

?

?
Finally, we consider the semantics of selection and construction queries. A selection
query has the form SELECT ?X WHERE P where ?X is a list of distinct variables. It
simply returns the valuations obtained by P and discards the bindings of variables not
in X. A construction query builds a new graph or dataset from these results. Note that
in SPARQL such queries only construct anonymous graphs; here we generalize in order
to use construction queries to build datasets that can be inserted or deleted.
?

?

?
SELECT ?X WHERE PD = {|
CONSTRUCT C WHERE PD =
{(C) |   PD}

|   PD}

Here, note that |

X stands for  restricted to the variables in the list X.

We omit discussion of the FROM components of queries (which are used to initialize
the graph store by pulling data in from external sources) or of the other query forms
ASK, and DESCRIBE, as they are described elsewhere [1] in a manner coherent with
our approach.

3 The Semantics of SPARQL Update

We will describe the semantics of the core language for atomic updates, based upon [22]:

U ::= INSERT {C} WHERE P | DELETE {C} WHERE P

| DELETE {C} INSERT {C
| CREATE g | DROP g | COPY g TO g

} WHERE P | LOAD g INTO g

 | CLEAR g

 | MOVE g TO g

 | ADD g TO g
?

?

?
We omit the INSERT DATA and DELETE DATA forms since they are definable in terms
of INSERT and DELETE.

SPARQL Update [22] specifies that transactions consisting of multiple updates should
be applied atomically, but leaves some semantic questions unresolved, such as whether
aborted transactions have to roll-back partial changes. It also does not specify whether
updates in a transaction are applied sequentially (as in most imperative languages), or
using a snapshot semantics (as in most database update languages). Both alternatives
pose complications, so in this paper we focus on transactions consisting of single atomic
updates.
We model a collection of named graphs as a dataset D, as for SPARQL queries.
We consider only a single graph in isolation here, and not the case of multiple named
graphs that may be being updated concurrently. The semantics of an update operation
u on dataset D is defined as UD.

H. Halpin and J. Cheney

The semantics of a SPARQL Update U in dataset D is defined as follows:
DELETE {C} WHERE PD = D \ CONSTRUCT C WHERE PD
INSERT {C} WHERE PD = D  CONSTRUCT C WHERE PD
} WHERE PD = (D \ CONSTRUCT C WHERE PD)
WHERE PD

DELETE {C} INSERT {C

 CONSTRUCT C
?

?

?
CLEAR gD = D[g := ]
CREATE gD = D  {g  }
DROP gD = D[g := ]

LOAD g1 INTO g2D = D[g2 := D(g1)  D(g2)]
D[g2 := D(g1)] if g1 = g2
D[g2 := D(g1), g1 := ] if g1 = g2

COPY g1 TO g2D =
MOVE g1 TO g2D =
ADD g1 TO g2D = D[g2 := D(g1)  D(g2)]

otherwise

otherwise

Here, D[g := G] denotes D updated by setting the graph named g to G, and D[g := ]
denotes D updated by making g undefined, and finally D [g := G] denotes D updated
by adding a graph named g with value G, where g must not already be in the domain of
is used for set union and G\ G
D. Set-theoretic notation is used for graphs, e.g. G G
?

?

?
for set difference, and  stands for the empty graph. Note that the COPY g TO g
and
. Also, observe that we do not model
MOVE g TO g

external URI dereferences, and the LOAD g INTO g
operation (which allows g to be an
external URI) behaves exactly as ADD g TO g
operation (which expects g to be a local
graph name).

operations have no effect if g = g
?

?

?
4 Provenance Semantics

A single SPARQL update can read from and write to several named graphs (and possibly also the default graph). For simplicity, we restrict attention to the problem of tracking the provenance of updates to a single (possibly named) RDF graph. All operations
may still use the default graph or other named graphs in the dataset as sources. The
general case can be handled using the same ideas as for a single anonymous graph, only
with more bureaucracy to account for versioning of all of the named graphs managed
in a given dataset.

A graph that records all the updates of triples from a given graph g is considered a
provenance graph for g. For each operation, a provenance record is stored that track of
the state of the graph at any given moment and their associated metadata. The general
concept is that in a fully automated process one should be able to re-construct the state
of the given graph at any time from its provenance graph by following the SPARQL
queries and metadata given in the provenance records for each update operation tracked.
?

?

?
r

b

t

a

s

c
c

u

d
d

r

b

a

t

dd

r

b

tt

a

u

dd

g_v0

version

g
g_vg
g_v1

version

input

u
u1

o u t p u t 

input

u2

_v2

g
g
g_v2
o u t p u t 

delete

insert
?

?

?
a
t
a
d

meta

m2

G_u11

m1

G_u2

5pm

James

a

s

u

c
c

d

4pm

DELETE WHERE {
 g {?X s ?Y . 
    ?Y u ?Z }
}

Harry

a

u

dd

INSERT {
 g {?X u ?Y }
} WHERE {
 g {?X t ?Y}
}

prov

Fig. 1. Example provenance graph

We model the provenance of a single RDF graph G that is updated over time as a
set of history records, including the special provenance graph named prov which consists of indexed graphs for each operation such as G_v0,. . . ,G_vn and G_u1. . . ,G_um.
These provenance records are immutable; that is, once they have been created and ini-
tialized, the implementation should fixed so that their content cannot be changed. The
index of all provenance records then is also strictly linear and consistent (i.e. non-
circular), although branching could be allowed. They can be stored as a special immutable type in the triple-store. Intuitively, G_vi is the named graph showing Gs
state in version i and G_ui is another named graph showing the triples inserted into or
deleted from G by update i. An example illustration is given in Figure 1.

The provenance graph of named graph G includes several kinds of nodes and edges:

 G_vi upd:version G_vi+1 edges that show the sequence of versions. Whenever a upd:version link is added between G_vi and G_vi+1, a backlink called
upd:prevVersion between G_vi+1 and G_vi;

 nodes u1,. . . ,un representing the updates that have been applied to G, along with
a upd:type edge linking to one of upd:insert, upd:delete, upd:load,
upd:clear, upd:create, or upd:drop.

 For all updates except create, an upd:input edge linking ui to G_vi.
 For all updates except drop, an upd:output edge linking ui to G_vi+1.
 For insert and delete updates, an edge ui upd:data G_ui where G_ui is a

named graph containing the triples that were inserted or deleted by ui.

H. Halpin and J. Cheney

 Edges ui upd:source n linking each update to each named graph n that was
consulted by ui. For an insert or delete, this includes all graphs that were consulted
while evaluating P (note that this may only be known at run time); for a load update,
this is the name of the graph whose contents were loaded; create, drop and clear
updates have no sources.

 Additional edges from ui providing metadata for the update (such as author, commit time, log message, or the source text of the update); possibly using a standard
vocabulary such as Dublin Core, or using OPM or PROV vocabulary terms.

Note that this representation does not directly link triples in a given version to places
from which they were copied as it only contains the triples directly concerning the
update in the history record. However, each history record in combination with the rest
of the history records in the provenance graph does provide enough information to recover previous versions on request. As we store the source text of the update statements
performed by each update in each history record of the provenance graph, we can trace
backwards through the update sequence to to identify places where triples were inserted or copied into or deleted from the graph. For queries, we consider a simple form
of provenance which calculates a set of named graphs consulted by the query. The set
of sources of a pattern or query is computed as follows:
?

?

?
{names(C) |   CD}

SCD =

SP1 . P2D = SP1D  SP2D
SP1 UNION P2D = SP1D  SP2D
SP1 OPT P2D = SP1D  SP2D
SP FILTER RD = SPD
SSELECT ?X WHERE PD = SPD
SCONSTRUCT C WHERE PD = SPD

where the auxiliary function names(C) collects all of the graph names occurring in a
ground basic graph pattern C:

names({t1, . . . , tn}) = {DEFAULT}

names(GRAPH A {t1, . . . , tn}) = {(A)}

names(C C
?

?

?
) = names(C)  names(C
?

?

?
)

Here, we use the special identifier DEFAULT as the name of the default graph; this can
be replaced by its URI.
The SPD function produces a set S of graph identifiers such that replaying
QD|S = QD, where D|S is D with all graphs not in S set to  (including the default
graph if DEFAULT / S). Intuitively, S identifies graphs that witness Q, analogous to
why-provenance in databases [4]. This is not necessarily the smallest such set; it may be
an overapproximation, particularly in the presence of P1 OPT P2 queries [24]. Alterna-
tive, more precise notions of source (for example involving triple-level annotations [8])
could also be used.

We define the provenance of an atomic update by translation to a sequence of updates that, in addition to performing the requested updates to a given named graph, also
?

?

?
constructs some auxiliary named graphs and triples (provenance record) in a special
named graph for provenance information called prov (the provenance graph). We apply this translation to each update posed by the user, and execute the resulting updates
directly without further translation. We detail how provenance information should be
attached to each SPARQL Update operation.

We consider simple forms of insert and delete operations that target a single, statically known, named graph g; full SPARQL Updates including simultaneous insert and
delete operations can also be handled. In what follows, we write (metadata) as a
placeholder where extra provenance metadata (e.g. time, author, etc. as in Dublin Core
or further information given by the PROV vocabulary [15]) may be added. DROP commands simply end the provenance collection, but previous versions of the graph should
still be available.

 A graph creation of a new graph CREATE g is translated to

CREATE g;
CREATE g v0;
INSERT DATA {GRAPH prov {
g version g v0,g current g v0,
u1 type create,u1 output g v0,
u1 meta mi, (metadata)
}}

 A drop operation (deleting a graph) DROP g is handled as follows, symmetrically to

creation:

DROP g;
DELETE WHERE {GRAPH prov {g current g vi}};
INSERT DATA {GRAPH prov {
ui type drop,ui input g vi,
ui meta mi, (metadata)
}}

where g vi is the current version of g. Note that since this operation deletes g, after
this step the URI g no longer names a graph in the store; it is possible to create a
new graph named g, which will result in a new sequence of versions being created
for it. The old chain of versions will still be linked to g via the version edges, but
there will be a gap in the chain.

 A clear graph operation CLEAR g is handled as follows:

CLEAR g;
DELETE WHERE {GRAPH prov {g current g vi}};
INSERT DATA {GRAPH prov {
g version g vi+1,g current g vi+1,
ui type clear,ui input g vi,
ui output g vi+1,ui meta mi,
(metadata)
}}

H. Halpin and J. Cheney

 A load graph operation LOAD h INTO g is handled as follows:

LOAD h INTO g;
DELETE WHERE {GRAPH prov {g current g vi}};
INSERT DATA {GRAPH prov {
g version g vi+1,g current g vi+1,
ui type load,ui input g vi,
ui output g vi+1,ui source hj,
ui meta mi, (metadata)
}}

where hj is the current version of h. Note that a load will not create any new graphs
because both the source and target should already exist. If no target exists, a new
graph is created as outlined above with using the create operation.
 An insertion INSERT {GRAPH g {C}} WHERE P is translated to a sequence of updates that creates a new version and links it to URIs representing the update, as
well as links to the source graphs identified by the query provenance semantics and
a named graph containing the inserted triples:

CREATE g ui;
INSERT {GRAPH g ui {C}} WHERE P ;
INSERT {GRAPH g {C}} WHERE P ;
CREATE g vi+1;
LOAD g INTO g vi+1;
DELETE DATA {GRAPH prov {g current g vi}};
INSERT DATA {GRAPH prov {
g version g vi+1,g current g vi+1,
ui input g vi,ui output g vi+1,
ui type insert,ui data g ui
ui source s1, . . . ,ui source sm,
ui meta mi, (metadata)}}

where s1, . . . , sm are the source graph names of P .

 A deletion DELETE {GRAPH g {C}} WHERE P is handled similarly to an insert,

except for the update type annotation.

CREATE g ui;
INSERT {GRAPH g ui {C}} WHERE P ;
DELETE {GRAPH g {C}} WHERE P ;
CREATE g vi+1;
LOAD g INTO g vi+1;
DELETE DATA {GRAPH prov {g current g vi}};
INSERT DATA {GRAPH prov {
g version g vi+1,g current g vi+1,
ui input g vi,ui output g vi+1,
ui type delete,ui data g ui
ui source s1, . . . ,ui source sm,
ui meta mi, (metadata)}}
?

?

?
Note that we still insert the deleted tuples into the g ui.

 The DELETE {C} INSERT {C

} WHERE P update can be handled as a delete followed by an insert, with the only difference being that both update steps are linked
to the same metadata.

 The COPY h TO g, MOVE h TO g, and ADD h TO g commands can be handled similarly
to LOAD h INTO g; the only subtlety is that if g = h then these operations have no
visible effect, but the provenance record should still show that these operations
were performed.
Our approach makes a design decision to treat DELETE {C} INSERT {C

} WHERE P
as a delete followed by an insert. In SPARQL Update, the effect of a combined delete
insert is not the same as doing the delete and insert separately, because both phases of
a deleteinsert are evaluated against the same data store before any changes are made.
However, it is not clear that this distinction needs to be reflected in the provenance
record; in particular, it is not needed to ensure correct reconstruction. Moreover, the
connection between the delete and insert can be made explicit by linking both to the
same metadata, as suggested above. Alternatively, the deletion and insertion can be
treated as a single compound update, but this would collapse the distinction between
the sources of the inserted and deleted data, which seems undesirable for use-cases
such as version control.

Also note that our method does not formally take into account tracking the provenance of inferences. This is because of the complex interactions between SPARQL
Update and the large number of possible (RDFS and the many varieties of OWL and
OWL2) inference mechanisms and also because, unlike other research in the area [8],
we do not consider it a requirement or even a desirable feature that inferences be preserved between updates. It is possible that an update will invalidate some inferences
or that a new inference regime will be necessary. A simple solution would be that if
the inferences produced by a reasoning procedure are necessary to be tracked with a
particular graph, the triples resulting from this reasoning procedure should be materialized into the graph via an insert operation, with the history records metadata specifying
instead of a SPARQL Update statement the particular inference regime used. We also
do not include a detailed treatment of blank nodes that takes their semantics as existential variables, as empirical research has in general shown that blank nodes are generally
used as generic stable identifiers rather than existential variables, and thus can be treated
as simply minting unique identifiers [11].

5 Update Provenance Vocabulary

For the provenance graph itself, we propose the following lightweight vocabulary called
the Update Provenance Vocabulary (UPD) given in Table 2. Every time there is a
change to a provenance-enabled graph by SPARQL Update, there is the addition of a
provenance record to the provenance graph using the UPD vocabulary, including information such as an explicit time-stamp and the text of the SPARQL update itself. Every
step in the transaction will have the option of recording metadata using W3C PROV vocabulary (or even some other provenance vocabulary like OPM) explicitly given by the
meta link in our vocabulary and semantics, with UPD restricted to providing a record

H. Halpin and J. Cheney

of the cut-and-paste operations needed for applications of dynamic provenance like
version control. We align the UPD vocabulary as a specialization of the W3C PROV
vocabulary. A graph (upd:graph) is a subtype of prov:Entity and an update of
a graph (upd:update) is a subtype of prov:Activity. For inverse properties, we
use the inverse names recommended by PROV-O [15].

Name

upd:input
upd:output
upd:data

upd:version

Description
Link to provenance record from graph before an update prov:wasUsedBy
Link from provenance record to a graph after update
Changed data in insert/delete operation
Sequential link forward in time between a version of a
graph and an update

prov:generated
prov:wasUsedBy
prov:hadRevision

PROV Subtype

upd:prevVersion sequential link backwards in time between a version of

prov:wasRevisionOf

upd:type

a graph and an update
Type of update operation (insert, delete, load, clear, cre-
ate, or drop)
Link to most current state of graph

upd:current
upd:source Any other graph that was consulted by the update
upd:meta
upd:user
upd:text
upd:time

Link to any metadata about the graph
User identifier (string or URI)
Text of the SPARQL Update Query
Time of update to the graph

prov:type

prov:hadRevision
prov:wasUsedBy
rdfs:seeAlso
prov:wasAttributedTo
prov:value
prov:atTime

Fig. 2. Lightweight Update Provenance Vocabulary (UPD)

6 Implementation Considerations

So far we have formalized a logical model of the provenance of a graph as it evolves
over time (which allow us to derive its intermediate versions), but we have not detailed
how to store or query the intermediate versions of a graph efficiently. For any given
graph one should likely store the most up-to-date graph so that queries on the graph
in its present state can be run without reconstructing the entire graph. One could to
simply store the graph G_vi resulting from each update operation in addition to the
provenance record, but this would lead to an explosive growth in storage requirements.
This would also be the case even for the provenance graph if the storage of an auxiliary graph G_ui in a provenance record involved many triples, although we allow
this in the UPD vocabulary as it may be useful for some applications. For those operating over large graphs, the contents of the named graphs G_ui that store inserted
or deleted triples can be represented more efficiently by just storing the original graph
and the SPARQL Update statements themselves in each provenance record given by the
upd:text property, and not storing the auxiliary named graphs given by upd:data.
Strategically, one can trade computational expense for storage in provenance, due to
the immutability of the provenance information. A hybrid methodology to ameliorate
the cost of reconstruction of the version of a graph would be to store the graph at various
temporal intervals (i.e. snapshots). For small graphs where storage cost is low and
processing cost is high, it would make more sense to store all provenance information
?

?

?
for the entire graph. In situations where the cost of processing is high and storage cost
is low, storing the SPARQL Updates and re-running them makes sense to reconstruct
the graph. In this case, it also makes sense to store snapshots of the graph at various
intervals to reduce processing cost. Simulation results for these scenarios are available.1

7 Conclusion

Provenance is a challenging problem for RDF. By extending SPARQL Update, we have
provided a method to use W3C PROV (and other metadata vocabularies) to keep track
of the changes to triple-stores. We formalized this approach by drawing on similar work
in database archiving and copy-paste provenance, which allow us to use SPARQL Update provenance records to reconstruct graphs at arbitrary instances in time. This work
is a first step in addressing the important issue of RDF version control. We hope this will
contribute to discussion of how to standardize descriptions of changes to RDF datasets,
and even provide a way to translate changes to underlying (e.g. relational or XML)
databases to RDF representations, as the same underlying cut-and-paste model has
already been well-explored in these kinds of databases [2]. Explorations to adapt this
work to the Google Research-funded DatabaseWiki project, and implementation performance with real-world data-sets is a next step [3]. A number of areas for theoretical
future work remain, including the subtle issue of combining it with RDFS inferences [8]
or special-purpose SPARQL provenance queries [25,16].

Acknowledgements. This work was supported in part by EU FP7 project DIACHRON
(grant number 601043). The authors, their organizations and project funding partners
are authorized to reproduce and distribute reprints and on-line copies for their purposes
notwithstanding any copyright annotation hereon.
