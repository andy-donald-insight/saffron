Toward Matching the Relation Instantiation from DBpedia 
Ontology to Wikipedia text: Fusing FrameNet to Korean 

Younggyun Hahm 

KAIST, Korea 

hahmyg@kaist.ac.kr 

Jongsung Woo 

KAIST, Korea 

woo88@kaist.ac.kr 

Seongbae Park 

Kyungpook National Univ. Korea 

sbpark@sejong.knu.ac.kr 

Youngsik Kim 

KAIST, Korea 

twilight@kaist.ac.kr 

Jiwoo Seo 
KAIST, Korea 

jiwoo35@kaist.ac.kr 

Dosam Hwang 

KAIST, Korea 

dshwang@yu.ac.kr 

Yousung Won 

KAIST, Korea 

styner0305@kaist.ac.kr 

Jiseong Kim 
KAIST, Korea 

jiseong@kaist.ac.kr 

Key-Sun Choi 

KAIST, Korea 

kschoi@kaist.ac.kr 

this  problem, 

resources 

In  overcoming 

ABSTRACT 
Nowadays,  there  are  many  ongoing  researches  to  construct 
knowledge bases from unstructured data. This process requires an 
ontology  that  includes  enough  properties  to  cover  the  various 
attributes  of  knowledge  elements.  As  a  huge  encyclopedia, 
Wikipedia  is  a  typical  unstructured  corpora  of  knowledge. 
DBpedia,  a  structured  knowledge  base  constructed 
from 
Wikipedia,  is  based  on  DBpedia  ontology  which  was  created  to 
represent  knowledge  in  Wikipedia  well.  However,  DBpedia 
ontology is a Wikipedia-Infobox-driven ontology. This means that 
although  it  is  suitable  to  represent  essential  knowledge  of 
Wikipedia,  it  does  not  cover  all  of  the  knowledge  in  Wikipedia 
text. 
representing 
semantics or relations of words such as WordNet1 and FrameNet2 
are  considered  useful.  In  this  paper  we  determined  whether 
DBpedia  ontology  is  enough  to  cover  a  sufficient  amount  of 
natural  language  written  knowledge  in  Wikipedia.  We  mainly 
focused  on  the  Korean  Wikipedia,  and  calculated  the  Korean 
Wikipedia  coverage  rate  with  two  methods,  by  the  DBpedia 
ontology  and  by  FrameNet  frames.  To  do  this,  we  extracted 
sentences  with  extractable  knowledge  from  Wikipedia  text,  and 
also  extracted  natural  language  predicates  by  Part-Of-Speech 
tagging.  We  generated  Korean  lexicons  for  DBpedia  ontology 
properties and frame indexes, and used these lexicons to measure 
the Korean Wikipedia coverage ratio of the DBpedia ontology and 
frames.  By our measurements, FrameNet frames cover 73.85% of 
the  Korean Wikipedia sentences, which  is  a sufficient portion  of 
Wikipedia  text.  We  finally  show  the  limitations  of  DBpedia  and 
FrameNet  briefly,  and  propose  the  outlook  of  constructing 
knowledge bases based on the experiment results. 

Permission to make digital or hard copies of all or part of this work for personal or 
classroom use is granted without fee provided that copies are not made or distributed for 
profit or commercial advantage and that copies bear this notice and the full citation on the 
first page. Copyrights for components of this work owned by others than ACM must be 
honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on 
servers or to redistribute to lists, requires prior specific permission and/or a fee. Request 
permissions from Permissions@acm.org.  
SEM '14, September 04 - 05 2014, Leipzig, AA, Germany 
Copyright 2014 ACM 978-1-4503-2927-9/14/09...$15.00. 
http://dx.doi.org/10.1145/2660517.2660534  

1 http://wordnet.princeton.edu/ 
2 https://framenet.icsi.berkeley.edu/ 

Keywords 
Wikipedia,  Knowledge  Representation,  Ontology,  DBpedia, 
FrameNet. 

1.  INTRODUCTION 
Knowledge is traditionally written in natural language, and can be 
founded  in  texts  within  the  document-oriented  Web  which  is 
considered  as  human-readable  data  more  than  a  structured 
knowledge base  (KB) [1] [2]. In the perspective of the Semantic 
Web, publishing knowledge more generically and flexibly on the 
Web  of  machine-readable  data  is  a  major  task.  To  achieve  this 
goal, Semantic Web concepts are used for constructing KB from 
knowledge  of  the  Web  [3],  and  RDF  is  used  as  a  typical 
knowledge  representation  [4].  DBpedia  [5]  is  a  state-of-the-art 
approach  to  construct  KB  from  Web  data.  However,  knowledge 
extraction  from  unstructured  data  remains  a  challenging  issue  in 
natural language understanding and the Semantic Web. 

There  are  previous  research  to  extract  knowledge  from  natural 
language  text.  As  a  staple  knowledge  description  approach  to 
provide a larger meaning of the text, coherence relation detection 
is  considered  an  important  factor  [7]  [8].    In  particular,  the 
Rhetorical Structure Theory approach uses grammatical cohesion 
and lexical cohesion to  extract  knowledge from natural language 
[9]. For these works, some pre-defined lexicons containing words 
such as because or although are used. Language resources are also 
used  to  figure  out  semantics  of  text.  Knowledge  in  text  is 
described  with  semantic  lexicons  and  relations,  so  there  are 
resources to analyze text such as WordNet and frames. For these 
reasons, there are some approaches to integrate each resource [10] 
[11] [12] [13].  

One  of  the  goals  of  the  above  approaches  is  to  find  and  extract 
knowledge from natural language text [14], via analyzing lexical 
or  relational  semantics.  Above  all,  representation  of  extracted 
knowledge  in  KB  is  based  on  ontologies  because  of  the 
interoperability  paradigm  of  the  Semantic  Web.  OLIA  [15]  and 
NLP2RDF  [16]  is  a  part  of  this  effort  to  represent  linguistic 
information  for  ontological  KB.  These  efforts  aimed  to  extract 
knowledge  from  linguistics  information  in  text.  Likewise,  [17] 
aimed  to  link  FrameNet  to  SUMO  ontology.  These  linguistic 


[19]. 

Therefore,  we  consider 
that  vocabularies  for  representing 
knowledge  such  as  ontology,  WordNet,  and  frames  are  the 
essential  elements  to  construct  KB  from  natural  language.  For 
example,  DBpedia  is  a  typical  KB  constructed  from  Wikipedia, 
and uses DBpedia ontology to represent that. However, extracted 
knowledge in DBpedia does not cover all knowledge in Wikipedia 
text  because  DBpedia  considers  semi-structured  data  on  a 
preferential  basis.  Thus,  the  task  of  constructing  KB  from 
Wikipedia  text  would  require  some  hypothetical  ontologies  that 
include enough properties to represent natural language semantics.  

The  motivation  of  this  paper  is  to  examine  whether  DBpedia 
ontology by itself covers the knowledge in Wikipedia text enough, 
and if not, investigate whether using linguistic information could 
make  up  for  such  a  shortcoming.  We  analyzed  the  Korean 
Wikipedia and its coverage rate using DBpedia and frames.  

In section 2, we define the problem and the methodology we used 
in  this paper.  We list the  target resources used  for  knowledge in 
section 3, and the resources used for representation of knowledge 
in section 4. In section 5 we introduce the  evaluation results and 
limitations  of  covering  the  Korean  Wikipedia  using  DBpedia 
ontology and frames, and we discuss limitations and future work 
in section 6. 

2.  PROBLEM DEFINITION 
To  represent  natural  language  written  knowledge  in  RDF,  it 
requires  an  ontology  that  has  enough  properties  to  describe 
attributes of  every knowledge element. Likewise, there are many 
attempts  to  make  RDF  KB  from  data  in  both  academic  and 
industrial  fields.  For  example,  DBpedia  is  a  typical  RDF  KB 
extracted  from  Wikipedia  knowledge.  DBpedia  ontology  is  used 
to  represent  the  relationships  between  entities  or  attributes  of 
entities using  dataproperty and objectproperty. In this 
case,  DBpedia  ontology  works  as  the  ontology  to  represent 
knowledge in Wikipedia. But Wikipedia is an encyclopedia that is 
written  in  natural  language  while  describing  strong-facts.  In 
natural  language,  we  can  imagine  that  there  are  several  types  of 
knowledge.  For  example,  birth  data  and  nationality  are 
considered as strong-facts, but we can also consider about weak-
facts, such as love and missing. 

The  Infobox is an  essential knowledge component of  Wikipedia, 
and  we  can  consider  it  as  the  source  of  strong-facts,  and  thus 
DBpedia ontology would be suitable for representing strong-facts. 
However,  although  Wikipedia  is  an  encyclopedia  so  that  its 
natural language knowledge describes the strong-facts, it does not 
guarantee that the DBpedia ontology is enough to describe all of 
the  knowledge  in  Wikipedia  natural  language  text.  Therefore, 
although  we  want  to  represent  knowledge  in  RDF  about 
Wikipedia via DBpedia ontology, we cannot assume that RDF KB 
extracted from Wikipedia using DBpedia ontology is enough.  

Therefore, we need to analyze how Korean Wikipedia is covered 
via DBpedia ontology before constructing RDF KB from Korean 
Wikipedia  text.  If  DBpedia  ontology  is  not  enough  to  cover 
Korean Wikipedia text, we calculate the coverage using frames to 
figure out how to make up this shortcoming of DBpedia ontology.  

We now define our methodology and some terms below: 

Terms 

  KS: Sentences with  extractable knowledge. In this case, the 

sentences include at least one DBpedia entity. 

  NL  predicate:  Korean 

lexicons  consisting  of  natural 

language predicates in sentences 

  DBO: DBpedia ontology, especially properties. 

 

LP:  Korean  lexicons  that  are  relevant  to  DBpedia  ontology 
properties. 

Methodology 

Firstly,  we  define  KS  as  the  set  of  natural  language  knowledge. 
KS represents the sentences with extractable knowledge from the 
Korean Wikipedia.  

Secondly,  we  define  NL  predicates  describing  the  attributes  or 
relations of knowledge elements, which in this case are entities.  

Thirdly,  we  calculate  the  Korean  Wikipedia  coverage  rate  via 
DBO. To calculate this, we assume that the NL predicate coverage 
of  an  ontology  is  similar  to  its  Korean  Wikipedia  coverage.  So, 
we  calculate  the  NL  predicate  coverage  of  DBO.  We  also 
calculate the Korean Wikipedia coverage rate via frames. 

Finally,  we  calculate  the  coverage  between  frames  and  DBO  to 
find  out  that  how  much  coverage  frames  could  make  up  to 
describe  knowledge  in  natural  language  if  DBO  is  not  enough, 
especially for Korean Wikipedia text. 

3.  NATURAL LAGUAGE KNOWLEDGE 
RESOURCE 
Korean  Wikipedia  is  a  natural  language  written  encyclopedia. 
Although it includes semi-structured data such as CATEGORY or 
INFOBOX,  we  only  considered  plain 
though 
Wikipedia  is  well-written  encyclopedia  to  explain  knowledge, 
some  sentences  could  just  describe  weak-facts  such  as  opinions. 
Also some sentences  such as he doubted that  or she  waited 4 
hours. do not contain anything that can be considered knowledge.  

text.  Even 

To  simplify  the  problem,  we  define  KS  as  the  set  of  sentences 
including  extractable  knowledge.  Also,  we  extract  a  list  of  NL 
predicates that could be matched with ontology properties.  

3.1  Sentences with extractable knowledge 
3.1.1  Filtering sentences with extractable knowledge 
(KS) from the Korean Wikipedia 
Before we calculate the Korean Wikipedia coverage rate, we need 
to  know  which  sentences  within  the  Korean  Wikipedia  actually 
contain knowledge that is extractable. Even though all Wikipedia 
articles  contain  knowledge  about  their  topic,  the  same  does  not 
hold  for  each  individual  sentence  within  each  article.  Sentences 
such as It is said that he did not worry at all about this. or She 
was  very  suspicious  and  territorial.  are  examples  of  sentences 
that do not contain extractable knowledge. 

A  typical  sentence  with  extractable  knowledge  must  contain  a 
subject  entity,  an  object  entity,  and  a  predicate.  Here,  we  first 
determine  which 
extractable 
knowledge  by  selecting  the  ones  that  have  at  least  one  subject 
entity  and  object  entity.  This  in  turn  leads  to  the  problem  of 
named entity recognition in Korean Wikipedia sentences. 

sentences  potentially  have 


filtering 
Now that we have information  of which  entities are contained in 
each sentence, we have to distinguish between subject entities and 
object  entities.  This  normally  would  be  a  difficult  task,  but  we 
utilize a special property of Wikipedia articles: Most sentences of 
a  Wikipedia  article  describe  some  fact  about  the  topic  of  the 
article.  If  a  topic-derived  entity  exists  within  a  sentence,  then  it 
must  be  a  subject  entity,  and  all  non-topic-derived  entities  are 
probably object entities. Based on this observation, we  classified 
all  retagged  entities  into  subject  and  object  entities,  and  in  turn 
divided  sentences  into  4  groups  depending  on  the  existence  of 
subject and object entities. 

Table 2 Number of KS 

At least 1 

subject entity 

No subject 

entities 

At least 1 

object entity 

(17.56%) 
1,261,259 
(44.04%) 

No object 

entities 

(12.75%) 

(25.62%) 

As shown in table 2, the ratio of sentences with both subject and 
object  entities  is  much  smaller  than  the  ratio  of  sentences  with 
only object entities. This is because the subject in many sentences 
is either omitted or substituted with a pronoun. Sentences like He 
was  a  community  organizer  in  Chicago  before  earning  his  law 
degree.  are  common  within  Wikipedia  articles.  Based  on  this 
observation,  we  decided  that  the  set  of  sentences  with  at  least  1 
object entity have potentially extractable knowledge. 

3.2  Natural language predicates 
To calculate the KS coverage via DBO and Frame automatically, 
we used the concept of natural language predicates. We assumed 
that  NL  predicates  would  be  used  to  explain  semantic  relations 
between  entities 
languages.  We  compared  NL 
predicates with the DBO lexicon and frame lexicon. 

in  natural 

For  this  work,  we  analyzed  Part-Of-Speech  tags  of  Korean 
Wikipedia text via NLPHub3, which is a web platform for Korean 
NLP tools. We restricted NL predicates to ones that matched the 
pre-defined POS tag patterns: 

Pattern 1) nc+xsv+

 (e.g. 

 

 

Pattern 2) pv + 

 (e.g. 



++
+



Table 1 Amount of entities in Korean Wikipedia by retagging 

In this case, the POS tag nc means Common noun, and xsv 
is  Verb  derivational  suffix.  pv  is  Verb  and  the  Korean 

Retagged entity set 

Amount 

morpheme 

 is used as an Ending word in Korean. 

EXISTING 
EXACT MATCH 
REDIRECT EXACT MATCH 

3,541,989 
1,763,084 

Our  dataset is the 2,862,181 sentences in  the Korean Wikipedia. 
We extract KS in 3 cases, 1) Subject only, 2) Object only, and 3) 
Both Subject and Object KS. The numbers of NL predicates in the 
Korean Wikipedia is shown in table 3. 



3 http://nlphub.kaist.ac.kr  

3.1.2  NER in Korean Wikipedia sentences via link 
retagging 
In  order  to  detect  named  entities  within  Korean  Wikipedia 
sentences, we use the simple heuristic of considering all and only 
Wikipedia links within each sentence to be named entities. In this 
case, we consider only linked words are entities, however, a few 
of  words  are  linked  in  Wikipedia.  To  consider  all  entity,  we  do 
retagging tasks for Korean Wikipedia sentences. We first perform 
link retagging for each article based on existing links on the same 
article  in  order  to  increase  the  amount  of  detected  entities  (and 
thus the amount of selected sentences). 

All  Wikipedia  links  consist  of  two  forms:  The  surface  form, 
which is the form of the link as it appears in text, and the lexical 
form, which is the name  of  the  entity the link actually points  to. 
Because  the  Korean  language  has  no  general  syntax  (such  as 
capitalization  in  English)  that  distinguishes  named  entities  from 
non-entities within text and many named entities are 1 to 2 letters 
long,  simple  retagging  via  exact  matching  of  surface  forms  of 
existing  links  yields  too  many  false  positives  to  be  considered 
useful.  In  this  paper,  we  performed  retagging  of  links  into  three 
sets  (EXISTING,  EXACT  MATCH,  REDIRECT  EXACT 
MATCH)  so  that  the  ratio  of  false  positives  is  greatly  reduced 
without sacrificing too much recall 

EXISTING:  This  set  consists  of  existing  links  within  the 
article, and the title of the article itself. 

EXACT  MATCH:  This  set  consists  of  links  that  were 
retagged based on exact string matching against lexical forms 
of the links in the EXISTING set. We argue that this set does 
not  contain  many  false  positives  because  we  used  lexical 
forms which typically have longer string length, and we kept 
the range  of retagging  source links  to links  within the  same 
article. 

REDIRECT EXACT MATCH: This set consists of links that 
were retagged based on exact string matching against lexical 
forms of all links that redirect to links in the EXISITNG set. 
The  set  of  redirected  links  L  was  found  by  querying  the 

, dbpedia-owl:wikiPageRedirects, L} 
Korean DBpedia for {L
relations  with  a  link  L  in  the  EXISTING  set.  For  the  same 
reasons for EXACT MATCH, we argue that this set does not 
contain many false positives as well. 

This 
retagging  method  was  performed  against  2,862,181 
sentences  in  the  Korean  Wikipedia  (April  30,  2014),  obtaining 
5,639,418 named entities. Table 1 shows how many entities were 
found in each set. 



This  method  obviously  sacrifices  some  recall  by  limiting  the 
retagging  scope  within  each  article.  Approaches  that  might 
increase  recall  without  introducing  an  inacceptable  amount  of 
false positives are left as future work. 


Type of KS 

Subject only 
Object only 
Both 
Object 

Subject 

and 

# of 

sentences 

1,261,259 

# of NL 
predicate 

3,128,629 
1,022,492 

per S 


sum 

2,128,832 

4,488,316  2.11 

first step of ontology lexicalization [20] task for Korean. We also 
calculate  frame  coverage  for  representing  semantic  relations  in 
natural language. 

4.1  DBpedia Ontology and its lexical pattern 
We extract lexical patterns (LP) from Korean Wikipedia sentences 
by detecting the node that have shortest length in the dependency 
structure  between  two  entities.  We  use  39  well-defined  DBO 
properties and their Korean LP which 3 annotators agreed upon. 

Table  3  shows  that  an  average  of  2.11  NL  predicates  occur  in  a 
sentence, and sentences that have both Subjects and Objects have 
2.03 NL predicates in average.  

 

Table 4 shows the top 20 NL predicates in the Korean Wikipedia. 
The unique number of NL predicates is 20,588. 

Table 4 Top 20 NL predicates in the Korean Wikipedia 

NL predicate 

Occurrences 

DBO  list:  birthDate,  capital,  child,  city, 
class,  colour,  colourName,  commander, 
country,  developer,  division,  drug, 
dynasty,  education,  equipment,  family, 
fareZone, 
genre, 
influencedBy,  kingdom,  languageFamily, 
locatedInArea, 
oreder, 
otherName,  parent,  party,  phylum,  place, 
predecessor,  publisher,  relative,  spouse, 
successor, 
vicePresident 

formationYear, 

mainInterest, 

target, 

type, 

vein, 

Limitations 

In  contrast  to  English,  defining  Korean  LP  is  not  trivial  because 
DBO  is  written  in  English.  Korean  Lexicalization  for  DBO  is  a 
significant  task  in  representing  knowledge  via  DBO  for  the 
Korean natural language. Even though there are Korean DBpedia 

properties  such  as  
(death  date), 
we do not consider these in this study because almost all Korean 
DBpedia  properties  are  nouns  so  that  it  is  difficult  to  map  these 
with NL predicates.   

(born  place),  

4.2  Frames for Korean 
To  calculate  the  KS  coverage  via  frames,  we  automatically 
produced  mappings  between  the  Sejong-verb-dictionary  and 
frame  indexes.  The  Sejong-verb-dictionary  consists  of  21,390 
Korean verbs and their respective English translated words. First, 
we find all English-translated words in the frame-annotated text4, 
and  then  we  map  Korean  verbs  with  their  respective  frame 
indexes.  This  approach  allows  the  mapping  of  17,251  Sejongverbs  with  889  frame  indexes.  Finally,  we  calculate  the  KS 
coverage  by  Sejong-verbs, assuming that if  Sejong-verbs cover a 
set  of  NL  predicates,  frames  cover  sentences  that  include  these 
NL predicates. 





Limitations 

We  do  not  evaluate  the  confidence  of  our  automatic  mapping 
between  Sejong-verbs  and  frame  indexes.  Also,  our  mapping 
algorithm should be improved, like [17].  

5.  COVERAGE EVALUATION 

5.1  Methodology 
We  need  to  check  whether  DBO  or  frames  sufficiently  cover 
natural language written knowledge in Korean Wikipedia text. To 
simplify the problem, we consider only KS sentences that include 
entities.  To  calculate  the  coverage  rate  for  KS,  we  defined  NL 
predicates  by  Part-Of-Speech  patterns  in  section  3,  and  made  a 
Korean  dictionary  for  DBO  and  frames  in  section  4.  In  this 
section, we calculate the coverage rate via the following formula: 

4 https://framenet.icsi.berkeley.edu/fndrupal/index.php?q=fulltextIndex 

start

located in

do, statement

face, treat

follows

make

use

through

have

by

for

take

being

 (

 (
 (

 (

 (

 (
 (
 (
 (
 (
 (
 (
 (
 (
 (
 (
 (
 (
 (
 (

speak

show

notice

cause


use, write

be used

see, regard


4.  RESOURCES TO REPRESENT 
KNOWLEDGE 
Representing knowledge in KB requires ontology as a vocabulary. 
DBpedia,  a  typical  KB  for  Wikipedia,  uses  DBO  to  represent 
attributes  of  entities  properties.  However,  DBO  is  Wikipedia-
Infobox-driven  ontology  so  it  is  not  guaranteed  to  cover 
knowledge  in  Wikipedia  text,  but  only  in  the  Wikipedia 
Infoboxes. In this section, we calculate whether DBO sufficiently 
covers  Wikipedia  text  or  not  using  some  well-defined  Korean 
lexical  patterns  for  DBO  properties.  This  is  also  considered  as  a 

Coverage:	

  



where    is  the  number  of  KS  of  the  3  types  which  we 
defined  in  section  3.2,  and     is  the  number  of  KS 
which are covered by Korean lexicons for DBO and frames.  

The number of used DBO properties are too small (39) because of 
the effort to make reliable Korean lexicons for DBO. This can be 
improved  via  enriching  Korean  lexicons  for  DBO.  By  this 
ontology lexicalization, the coverage ratio would be increased but 
we believe that there is an upper limit which cannot be overcome. 
We will discuss about this in section 5.4.  

5.3  KS Coverage by Frame 
We calculate the KS coverage by frame of the 3 types of KS.  

Before  calculating  the  KS  coverage,  we  calculate  the  Korean 
Wikipedia  NL  predicate  coverage  via  DBO  and  frames.  The 
results are shown in table 5 

Table 8 KS coverage by Frame 

Table 5 NL predicate coverage rate by DBO and Frame 

# of NL predicate 
in ko.Wikipedia 

5,835,324 

# of covered NL 
predicate by DBO 
381,008 (6.5%) 

# of covered NL 

predicate by Frame 
5,548,857 (95.09%) 

The results show that  our Korean  lexicons for  DBO covers  only 
6.5% of NL predicates in the Korean Wikipedia, but frames cover 
95.09%.  This  means  that  Sejong-verbs  mapped  with  frame 
indexes sufficiently cover NL predicates. 

5.2  KS Coverage by DBpedia Ontology 
We  calculate  the  KS coverage in 3 types of KS. 1) Subject  only 
(the sentences including an entity as subject), 2) Object only (the 
sentences  including  entities  as  object),  and  3)  both  Subject  and 
Object. Table 3 shows the numbers of KS of these 3 types.  

Table  6  shows  the  KS  coverage  by  DBO,  and  the  NL  predicate 
coverage by DBO is shown in table 7. 

Table 6 KS coverage by DBO 

Type of KS 

# of KS 
sentences 

Subject only  364,899 
Object only 
Both S & O 
sum 

1,261,259 

2,128,832 

# of covered KS 
sentences by DBO 


Coverage 

5.24% 
14.25% 
15.74% 

13.06% 

Table 7 NL predicate in KS coverage by DBO 

Type of KS 

# of NL 
predicate 

# of covered NL 
predicate by DBO 

Coverage 

Subject only  337,195 
Object only 
Both S & O 
sum 

3,128,629 
1,022,492 
4,488,316 


6.23% 
6.42% 
8.66% 

6.92% 

The results show that Korean lexicons for DBO cover just 13.06% 
of  KS, and 6.92% of NL  predicates. This implies  two  aspects of 
calculation for DBO. First, the number of DBOs used is too small 
to cover many KS. Second, the KS coverage ratio is more than the 
NL predicate coverage ratio. This shows that KS includes several 
NL predicates, but DBO covers just some specific NL predicates 
which were pre-defined in section 4.1. 

Limitations 

Type of KS 

# of KS 
sentences 

Subject only  364,899 
Object only 
Both S & O 
sum 

1,261,259 

2,128,832 

# of covered KS 

Coverage 

sentences by Frame 

1,081,929 

35.86% 
85.78% 
71.49% 

1,572,205 

73.85% 

Table 9 NL predicate in KS coverage by Frame 

Type of KS 

# of NL 
predicate 

Subject only  337,195 
Object only 
Both S & O 
sum 

3,128,629 
1,022,492 
4,488,316 

# of covered NL 

Coverage 

predicate by Frame 

2,977,792 

94.66% 
95.17% 
95.42% 

4,272,730 

95.19% 

Compared to the results in table 6 and table 7, table 8 and table 9 
show a big improvement. Especially, frames cover 95.19% of the 
NL predicates, not depending on the type of KS. And also we can 
find significant differences between DBO and frames. It is that the 
DBO coverage ratio is better when calculating for NL predicates, 
but  the  frame  coverage  ratio  is  better  when  calculating  for  KS 
coverage. This implies that frames cover almost all NL predicates, 
and there are some knowledge which were never represented with 
frames.  

Limitations 

We  do  not  analyze  KS  that  were  not  covered  by  frames.  It  is 
another  challenge  to  construct  KB  from  Korean  Wikipedia  text. 
Also, it  is  not defined  what types  of  knowledge  are  valuable for 
KB. For  example, the  motion of  entities  can be  considered to be 
knowledge for real-time QA system but not suitable for stable KB 
such as DBpedia.  

5.4  FrameNet Coverage by DBpedia Ontology 
DBpedia  ontology  properties  by  themselves  are  not  enough  to 
represent all knowledge in the plain texts from Wikipedia because 
DBpedia  extracts  only  structured  information  such  as  title, 
infobox from Wikipedia and we can assume that DBO covers only 
structured information. 

In order to supplement the information that cannot be covered by 
DBpedia,  we  try  to  utilize  frame  indexes  from  FrameNet.  By 
applying  frames  to  Wikipedia  text,  predicates  that  are  not 
expressed in DBpedia can be extracted 
5.4.1  Redundant predicates between Frame and 
DBpedia 
The total number of frame indexes are currently 1,179. The total 
number  of  DBpedia  ontology  propertyies  are  2,215.  Here, 


datatype  properties  (except  ontology  properties  which  are 
dependent on certain classes). 

By comparing lexical units on each frame with DBpedia ontology 
properties,  we  calculated  the  ratio  of  duplicated  predicates 
between  frame  and  DBpedia.  If  at  least  one  of  the  lexical  units 
and  DBpedia  ontology  property  match,  we  define  that  these 
predicates are mapped with each other. 

As  a  result,  the  number  of  redundant  predicates  between  frames 
and  DBpedia  is  about  277.  There  are  two  cases  of  duplicated 
predicates. First, the frame index and DBpedia ontology property 
match  exactly. (e.g. Topic) Second, the  lexical  unit of  the  frame 
and DBpedia ontology property expresses the same predicate even 
though  the  frame  index  is  not  exactly  matched  to  the  DBpedia 
ontology property. (e.g. Achievement) 

Table 10 The result of manual mapping between DBO and 

Frame 
DBpedia 

Frame 

Total number of 

Predicates 


Redundant 
Predicates 

277 (in Frame) 

5.4.2  Benefits of using frames 
By  utilizing  frames,  we  expect  to  express  about  900  predicates 
(1179-277=902)  which  cannot  be  covered  in  DBpedia  ontology 
properties.  

First, we  can  express  verbs such as escape, fall  asleep  which 
describe motion or states. When using the DBpedia ontology itself, 
there  is  no  way  to  describe  verbs  that  are  not  represented  in 
infobox templates.  

Second, we can include adjective words such as wet, desiring 
in  plain  texts  by  mapping  those  to  frame  indexes.  Because 
DBpedia  ontology  properties  include  objective  information  or 
facts,  it  is  hard  to  express  adjective  words  using  only  DBpedia 
ontology properties. 

Finally,  we  can  represent  additional  nouns  such  as  attention, 
destiny that were not covered by DBpedia ontology properties. 
5.4.3  Limitations 
There  are  some  cases  we  have  to  consider.  First,  certain  lexical 
units  can  be  mapped  to  several  different  frames.  Second,  some 
frame  indexes  cover  the  same  meaning.  Thus,  the  number  of 
predicates that can be covered by using frames is actually smaller 
than 900 as estimated above. 

Also,  as  we  need  to  consider  the  domain  and  range  of  DBpedia 
ontology  properties  when  mapping  frame  and  DBpedia  ontology 
properties,  the  total  number  of  duplicated  predicates  between 
frames and DBpedia can differ by some amount. 

to 

represent  natural 

in  Korean  Wikipedia  text.  There 

6.  CONCLUSION AND OUTLOOK 
In  this  paper,  we  assume  a  hypothetical  ontology  which  has 
language  written 
enough  properties 
knowledge 
is  a  typical 
knowledge  base  DBpedia  for  Wikipedia,  and  it  uses  DBpedia 
ontology  as  a  knowledge  vocabulary.  However,  the  DBpedia 
ontology is driven from Wikipedia Infobox, so it is not guaranteed 
to  sufficiently  cover  all knowledge  in Wikipedia text. Therefore, 
we analyzed the  coverage  of  the  Korean Wikipedia via DBpedia 
ontology. 

Also,  there  are  resources  to  show  semantics  and  relations  of 
entities  such  as  WordNet  and  FrameNet,  and  frames  are 
considered in this study. We found out that frames could make up 
for  the  incompleteness  of  the  DBpedia  ontology.  Overall,  this 
study has some limitations but the results shows the following: 

Ontology lexicalization 
The  DBpedia  ontology  is  incomplete  for  representing  all  natural 
language  written  knowledge  in  Wikipedia  text,  but  good  for 
Infobox-written  knowledge.  For  the  first  step  of  constructing  a 
knowledge  base  from  Wikipedia  text,  using  DBpedia  ontology 
would  be  a  good  approach.  In  Korean,  DBpedia  ontology 
lexicalization [20] [21] is certainly necessary. This task remains as 
future work. 

Other ontologies 
DBpedia  uses  other  ontologies  such  as  YAGO,  and  FOAF  to 
make up for the incompleteness of its own ontology. In this study 
we  just  considered  frames.  However,  there  are  some  approaches 
to link frames into SUMO. If FrameNet could cover the DBpedia 
ontology sufficiently, SUMO would be another suitable ontology 
to represent natural language written knowledge. 

Other resources 
In this paper, we only considered the Korean Wikipedia, and there 
have  been  some  obstacles.  For  example,  a  Korean  WordNet  is 
required  for  ontology  lexicalization.    But  for  English,  more 
sophisticated research might be progressable using LEMON [22].  

Remaining work 
We have tried hard to figure out the difference between FrameNet 
and  DBpedia  ontologies  but  it  is  still  far  from  clear.  Therefore, 
more  deep  analysis  such  as  similarity  measurement  is  required. 
Also,  defining  Korean  natural  language  predicates  only  by  Part-
Of-Speech  patterns  may  not  be  sufficient.  For  example,  the 

Korean  morpheme  
different things by combining other morphemes.  

  (do,  statement)  represents  many 

7.  ACKNOWLEDGMENTS 
This work was supported by the IT R&D program of MSIP/KEIT. 
[10044494,  WiseKB:  Big  data  based  self-evolving  knowledge 
base and reasoning platform] 


