A Scalable Approach for Efficiently Generating

Structured Dataset Topic Profiles

Besnik Fetahu1, Stefan Dietze1, Bernardo Pereira Nunes2,

Marco Antonio Casanova2, Davide Taibi3, and Wolfgang Nejdl1

1 L3S Research Center, Leibniz Universit at Hannover, Germany

{fetahu,dietze,nejdl}@L3S.de

2 Department of Informatics - PUC-Rio - Rio de Janeiro, RJ - Brazil

{bnunes,casanova}@inf.puc-rio.br

3 Institute for Educational Technologies, CNR, Palermo Italy

davide.taibi@itd.cnr.it

Abstract. The increasing adoption of Linked Data principles has led
to an abundance of datasets on the Web. However, take-up and reuse
is hindered by the lack of descriptive information about the nature of
the data, such as their topic coverage, dynamics or evolution. To address
this issue, we propose an approach for creating linked dataset profiles.
A profile consists of structured dataset metadata describing topics and
their relevance. Profiles are generated through the configuration of techniques for resource sampling from datasets, topic extraction from reference datasets and their ranking based on graphical models. To enable
a good trade-off between scalability and accuracy of generated profiles,
appropriate parameters are determined experimentally. Our evaluation
considers topic profiles for all accessible datasets from the Linked Open
Data cloud. The results show that our approach generates accurate profiles even with comparably small sample sizes (10%) and outperforms
established topic modelling approaches.

Keywords: Profiling, Metadata, Vocabulary of Links, Linked Data.

Introduction

The emergence of the Web of Data, in particular Linked Open Data (LOD) [3],
has led to an abundance of data available on the Web. Data is shared as part of
datasets and contains inter-dataset links [17], with most of these links concentrated on established reference graphs, such as DBpedia [1].

Linked datasets vary significantly with respect to represented resource types,
currentness, coverage of topics and domains, size, used languages, coherence, accessibility [7] or general quality aspects [11]. The wide variety and heterogeneity
of these dataset aspects pose significant challenges for data consumers when attempting to find useful datasets without prior knowledge of available datasets.

V. Presutti et al. (Eds.): ESWC 2014, LNCS 8465, pp. 519534, 2014.
c Springer International Publishing Switzerland 2014

B. Fetahu et al.

Hence, a large proportion of datasets from the LOD cloud1 has been overlooked
in favor of well-known datasets like DBpedia or YAGO [19].

To facilitate search and reuse of existing datasets, descriptive and reliable
metadata is required. However, as witnessed in the popular dataset registry
DataHub2, dataset descriptions are often missing entirely, or are outdated, for
instance describing unresponsive endpoints [7]. This issue is partially due to the
lack of automated mechanisms for generating reliable and up-to-date dataset
metadata, which hinders the retrieval, reuse or interlinking of datasets. The
dynamics and frequent evolution of datasets further exacerbates this problem,
calling for scalable and frequent update mechanisms of respective metadata.

In this work, we address the above described challenge of automatically describing linked datasets with the goal of facilitating dataset search and reuse.
This paper proposes an approach for creating structured dataset profiles, where
a profile describes the topic coverage of a particular dataset through a weighted
graph of selected DBpedia categories. Our approach consists of a processing
pipeline that combines tailored techniques for dataset sampling, topic extraction from reference datasets and topic relevance ranking. Topics are extracted
through named entity recognition (NER) techniques which use reference datasets
and then scored according to their relevance for a dataset based on graphical
models like PageRank [6], K-Step Markov [20], and HITS [15]. Although this
is a computationally expensive process, we experimentally identify the parameters which enable a suitable trade-off between representativeness of generated
profiles and scalability. Finally, generated dataset profiles are exposed as part
of a public structured dataset catalog based on the Vocabulary of Interlinked
Datasets (VoID3) and the newly introduced vocabulary of links (VoL)4. During
our experimental evaluation, dataset profiles were generated for all LOD cloud
datasets which were responsive at the time of writing and our approach showed
superior performance to established topic modelling techniques.

Our main contributions consist of (i) a scalable method for efficiently generating structured dataset profiles, combining and configuring suitable methods for
NER, topic extraction and ranking as part of an experimentally optimised con-
figuration, and (ii) the generation of structured dataset profiles for a majority of
LOD cloud datasets according to established dataset description vocabularies.
The remainder of the paper is structured as follows. Section 3 describes the
automated processing pipeline to create and expose datasets profiles. Section 4
shows the experimental setup, with the datasets and baselines used, along with
the generation of the ground truth and Section 5 presents the results and their
discussion. Section 6 reviews related literature. Finally, Section 7 presents the
conclusion and future work.

http://datahub.io/group/lodcloud
http://www.datahub.io
http://vocab.deri.ie/void
http://data.linkededucation.org/vol/
?

?

?
2 Problem Definition

This section introduces and formalises the used notions of dataset profiling. Recall that an RDF statement is a triple of the form s, p, o, where s is the subject
(an RDF URI reference or a blank node), p is the property, and o is the object
(a URI, a literal or a blank node) of the triple, respectively.
A resource instance r is a set of triples and is identified by a URI s. The
resource type is determined by the triple c = s, rdf:type, o. A literal l describes
a resource instance r iff there exists a triple of the form s, p, l. Given a set
of datasets D = {D1, . . . , Dn}, we denote the set of resource instances Ri =
{r1, . . . , rk} and resource types Ci = {c1, , ck} for Di  D (i = 1, . . . , n) by
R = {R1, . . . , Rn} and C = {C1, . . . , Cn}, respectively.
A reference dataset or knowledge base R represents a special case of a dataset
D by providing a topic vocabulary. We distinguish two resource types in R,
C = {entity, topic}. An instance e of type entity has a literal l describing its
label (e.g. e, rdfs:label, l) and at least one triple that refers to an instance
of type topic describing its topic. On the other hand, an instance t of type
topic is described with a literal l, i.e. the topic label (e.g. t, rdfs:label, l).
In our work, DBpedia is used as reference dataset where DBpedia entities and
categories represent entity and topic instances.

(for i = 1, . . . , n) is extracted through a named entity recognition function ap-

The set of entities Ek = {e1, . . . , em} of a specific resource rk  Ri of Di
plied to literal values from rk. The set of corresponding topics Tk = {t1, . . . , tq}
for rk is computed by accumulating all objects indicated by triples of the form
ej, dcterms:subject, t (for j = 1, . . . , m). Consequently, T = {t1, . . . , tp} corresponds to the set of topic classifications for all resource instances r  R.
A profile graph is a labelled, weighted and directed bipartite graph P =
(, , ), where  = D  T, and  = {D, t|D  D  t  T} is a set of
edges between datasets and topic classifications, extracted from R. Finally,  is
a function that assigns an edge weight for each edge in . Correspondingly for
a dataset Dk a dataset profile graph PDk represents a sub-graph of P, hence,
 = Dk  T, and  = {Dk, t|t  T}.

3 Profiling of Linked Datasets

In this section, we provide an overview of the processing steps for generating
structured dataset profiles. The main steps shown in Figure 1 are the following:
(i) dataset metadata extraction from DataHub; (ii) resource type and instance
extraction; (iii) entity and topic extraction; (iv) topic filtering and ranking; and
(v) dataset profile representation. Step (i) uses the CKAN API to extract dataset
metadata for datasets part of the LOD-Cloud group in DataHub. Steps (ii) - (v)
are explained in detail below.

B. Fetahu et al.

Fig. 1. Processing pipeline for generating structured profiles of Linked Data graphs

3.1 Resource Type and Instance Extraction

From the extracted dataset metadata (i.e. SPARQL endpoint) from DataHub
in step (i), step (ii) extracts resource types and instances via SPARQL queries5
that conform to the definition of resource types and instances in Section 2.
Considering the large amount of resources per dataset, we investigate samplebased strategies as follows:

Random Sampling: randomly selects resource instances from Ri of Di for
further analysis in the profiling pipeline.

Weighted Sampling: weighs each resource as the ratio of the number of
datatype properties used to define a resource over the maximum number of
datatype properties over all resources for a specific dataset. The weight for rk
is computed by wk = |f (rk)|/max{|f (rj)|} (rj  Ri|j = 1, , n), where f (rk)
represents the datatype properties of resource rk. An instance is included in
a sample if, for a randomly generated number p from a uniform distribution,
the weight wk fulfils the condition wk > (1  p). Such a strategy ensures that
resources that carry more information (having more literal values) have higher
chances of being included earlier at low cut-offs of analysed samples.

Resource Centrality Sampling: weighs each resource as the ratio of the number of resource types used to describe a particular resource (V
by the total number of resource types in a dataset. The weight is defined by
ck = |C

k. Similarly to weighted sampling, for a randomly generated number p, rk is included in the sample if ck > (1  p). The
main motivation behind computing the centrality of a resource is that important
concepts in a dataset tend to be more structured and linked to other concepts.

k  Vk) divided
?

?

?
k|/|C| with C
?

?

?
k = C  V
?

?

?
http://data-observatory.org/lod-profiles/profiling.htm
?

?

?
3.2 Entity and Topic Extraction

Here we describe the process of entity and topic extraction from sampled resource
instances in step (ii). Recall that we use DBpedia as our reference dataset due to
its broad topics coverage. To extract entities, first we combine all textual literal
values of a resource (in order to provide contextual information) and consequently
extract named entities from the resulting textual content using the NER tool of
choice, DBpedia Spotlight [16]. The topics T of sampled resources R represent
DBpedia category instances assigned to extracted entities through the datatype
property dcterms:subject. The topics in T are expanded with related topic instances (associated through datatype property skos:broader) up to two levels
(l=2) (determined experimentally as the best expansion level, see Figure 3b).

3.3 Constructing a Profile Graph
An important step in generating the profile graph P is the ranking of associated
topics in T for datasets in D. Recall that P represents a bipartite graph, hence,
a topic t  T can have one or more edges connecting to datasets in D. For
instance, given two edges Di, t and Dj, t, where Di = Dj the computed
weights Di, t = wi and Dj, t = wj can be different depending on how
well they represent, Di and Dj, for i, j = 1, . . . , z, respectively.

Furthermore, the function  relies on probabilistic graphical models. Such
models are suitable as they measure the importance of each vertex with respect
to other vertices in the corresponding profiles. Given a profile graph P and
for datasets Di, respectively its analysed resource instances are assumed to be
prior knowledge. The computation of vertex weights with Di as prior knowledge
results in the computation of importance of the vertices which are part of the
sub-graph connected to Di. Consequently, this translates into computing the
importance of topics tk  T (k = 1, . . . , n) with regards to Di. Additionally,
to ensure certainty of importance for Di, the prior probability is distributed
uniformly to all analysed resources in Ri, while for resources Rj from Dj the
prior probabilities are set to zero.
Finally, the assigned weight to vertex tk, with Di as prior knowledge, infers
exactly Di, tk. Hence, the relationships (edges) between topic tk and individual datasets (given as prior knowledge) have different weights, depending on
the set of resources that link tk with Di. One of the advantages of computing
the edge weights  is that any new dataset, which is not part of the profiles P,
can be added incrementally to the existing ones by simply computing the edge
weights with its associated topics.

To illustrate why this works, consider the following example with a pro-

file graph P consisting of datasets D = {D1, D2} with sets of resources
R1 = {r11, r12, r13, r14} and R2 = {r21, r22, r23, r24}, and the set of topics
T = {t1, t2, t3}. The individual topics are associated with the following resources:
t1 = {r11, r22}, t2 = {r11, r23, r24}, t3 = {r11, r12, r13, r14, r24}. Assume we want
to compute the edge weights between dataset D1 and topics in T. First, we consider D1 as prior knowledge. Hence, we uniformly distribute the prior probability

B. Fetahu et al.

zero. Finally, depending on the connectivity in the corresponding dataset profile,

(1/|R1|) to its resources. For resources in R2, the prior probabilities are set to
the topics would be ranked as follows: t3, t1, t2. The computed weights would
represent the edge weights by the tuples: D1, t3  D1, t1  D1, t2.
Similarly, the edge weights are computed for dataset D2.

3.4 Topic Ranking Approaches
Due to the large number of topics associated with the profile graph P, ranking
topics with respect to their relevance to datasets in D is crucial. A ranked set
of topics enhances the usefulness of the generated profiles and facilitates the
dataset recommendation and querying with higher accuracy.

Since topic extraction from the extracted entities is prone to noise from nonaccurately disambiguated entities, we compute a Normalised Topic Relevance
(NTR) score. NTR is a variant of the well-known tf-idf measure and is used to
filter out noisy topics. In combination with other topic ranking approaches, it
is used to determine the ideal topic expansion level. The topic rankings (edge
weights) are computed through the PageRank, K-Step Markov and HITS [6,15]
graphical models, applied to the profile graph. The adoption of the graphical
models is discussed in what follows.

Normalised Topic Relevance (NTR): The NTR score is an important step
for pre-filtering noisy topics as a result of non-accurate entity extraction. It is
computed by taking into account (i) the number of entities (t, D) assigned for
a topic t within a dataset D and that of entities (t,) across all datasets D and
(ii) the number of entities (, D) assigned to a dataset D and for datasets in D
(,). Topics are filtered out if they have a score below a given threshold:

N T R(t, D) =

(, D)
(t, D)

+

(,)
(t,)

, t  T, D  D

(1)

PageRank with Priors: is a variant of the PageRank [6] algorithm (Equa-
tion 2) that, given a data graph, in this case a dataset profile PDk for dataset
Dk  D, computes the importance of dataset-topic edge weights, for each t  T
such that there is an edge Dk, t. The computation of edge weights Dk, t is
biased towards the resource instances r  Rk of Dk. Hence, the importance of
a topic t is highly influenced by its connectivity with resource instances in Rk.
Prior knowledge is the analysed resource instance r  Rk with prior probabilities assigned as the ratio 1/|Rk|, while for the remaining vertices a probability
of zero is assigned.

(i+1)

(t)

= (1  )



din(t)





p(t|u)(i)

(u)

 + pt

(2)

u=1

where, t is a topic such that Dk, t = , part of the dataset profile PDk .  is
the probability of jumping back to vertices that are a priori known, r  Rk.
(t) quantifies the relative importance of t w.r.t vertices in PDk and is biased
towards the prior knowledge r  Rk. The summation in the equation quantifies
?

?

?
the importance of t relative to vertices that have incoming connections (resource
instances classified with t), din(t).

HITS with Priors: although similar to PageRank with Priors, it represents a
slightly different approach. The flow of visiting one vertex depends on a randomly
generated binary value, where in cases it is zero it visits a vertex from an inlink for an even step, while for an odd step it follows an out-link. Otherwise,
it visits one of the given vertices in Rk. As we cannot distinguish between hubs
and authoritative vertices from the set of topics t  T (due to their equivalent
importance), the process is simplified by having no hub or authoritative vertices.

a(i+1)

(t) = (1  )















din(t)

u=1
?

?

?
tT

h(t)(u)
din(t)

u=1

h(i)(t)

a(t)(u)

h(i+1)

(t) = (1  )

dout(t)

u=1
?

?

?
tT

dout(t)

u=1

a(i)(u)





 + pt






 + pt


(3)

(4)

K-Step Markov: the previous approaches represent Markov Chains in which
the number of steps taken from the random walk is stochastic. K-Step Markov
limits the number of steps to K. That is, the random walk starts for the given
vertices of interest t  T and stops after K steps. For a large enough K, the result
of the ranking converges to the limit of PageRank. The main advantage of such
an approach is scalability for large data graphs. On the other hand for step sizes
not large enough the ranking lacks accuracy.

3.5 Dataset Profile Representation
The resulting profiles P are represented in RDF using the VoID vocabulary and
are publicly available according to Linked Data principles6. However, VoID alone
does not provide the representativeness required to capture the computed topic
ranking scores. Hence, the complementary Vocabulary of Links (VoL) is introduced to complement the dataset description with a set of links to the associated
dataset topics, the used ranking method and the respective score. Thus, we enable
queries to select relevant datasets for a given topic. For further details, we refer
the reader to the website at http://data-observatory.org/lod-profiles/

4 Experimental Setup

This section describes the experimental setup used for the evaluation of our
approach and data. We introduce the used data, the ground truth and evaluation
metrics used to measure the profiling accuracy, and the baseline approaches for
comparison. Furthermore, the use of DBpedia in our experimental setup, for

http://data-observatory.org/lod-profiles/sparql

B. Fetahu et al.

extracting structured information for the dataset profiles, does not present a
limitation on using other more specialised reference dataset or a combination of
reference datasets.

4.1 Data and Ground Truth

In our experiments we covered all LOD Cloud datasets whose endpoints were
available. This resulted in 129 datasets with approximately 260 million resource
instances and billions of triples.

For the evaluation we considered a subset of datasets for which we have constructed a ground truth7 in the form of dataset profiles. For this task, we have
exploited crowd-sourced, topic profiles already available from existing datasets.
Several datasets provide a sufficient amount of manually assigned topic indicators
for their resources (not the datasets themselves). These are represented by keywords (in the case of bibliographic resource metadata) or tags (for user-generated
content metadata). We exploit these topic indicators, usually assigned by domain
experts, to generate dataset profiles. To link such term-based topic indicators to
DBpedia categories, we manually extracted entities (and eventually categories) for
each topic indicator, unless the topic indicators were already available in the form
of DBpedia entities. Queries to DBpedia were used to retrieve candidate entities
where matching ones were selected manually. The resulting topics were ranked according to their accumulated frequency from all resources within a dataset. This
is assumed to provide a more representative dataset profile.

Table 1 shows for each dataset the number of resources and the datatype
properties from which topic indicators were extracted. However, due to nonaccurately extracted entities, we manually checked for correctness of the named
entity recognition process.

Table 1. Entity and Category from annotated resource instances with topic indicators
for the specific datatypes properties (in the form of keywords, tags, subjects) for the
ground truth datasets

Dataset-ID
yovisto

Properties
skos:subject,
discipline, kategorie, tagline}8

dbpedia:{subject, class,

#Resources

oxpoints
socialsemweb-thesaurus skos:subject,

dcterms:subject,dc:subject

tag:associatedTag,

dcterms:subject

semantic-web-dog-food dcterms:subject, dc:subject
lak-dataset
dcterms:subject, dc:subject



The datasets are accessible under: http://datahub.io/dataset/DATASET_ID
?

?

?
http://data-observatory.org/lod-profiles/ground-truth
http://dbpedia.org/property/
http://www.holygoat.co.uk/owl/redwood/0.1/tags/
?

?

?
4.2 Evaluation Metrics

The profiling accuracy of the generated dataset profiles is measured using
the NDCG metric (normalised discounted cumulative gain). It takes into account
the ranking of topics generated using the methods in Section 3.4 compared to
the ideal ranking indicated by the ground truth. The computation of NDCG is
shown in Equation 5.

N DCG@l =

DCG@l
iDCG@l

where DCG@l = rel1 +

l

i=2

reli
log2i

(5)

where DCG@l represents the discounted cumulative gain at rank l, whereas
iDCG@l is the ideal DCG@l computed from the ground truth.

Note that, the set of topics from the computed dataset profiles and the ones
from the ground truth are overlapping, but not identical. Hence, for the cases
where topics from the dataset profiles do not exist in the ranked set of topics in
our ground truth, we set the ranking value to zero.

4.3 Baselines

As baselines, we chose well established approaches for topic detection. The baselines of choice generate a profile graph based on (i) simple tf-idf term weighting
and (ii) LDA topic modelling10 tool. In order to generate profiles consisting of
DBpedia categories according to our definition from the sets of terms generated
by the baselines, we followed the same approach as in Section 3.2. For the base-
lines, we consider the full set of resource instances for analysis. The output of
each method is a set of ranked terms:

tf-idf : as the standard term frequency weighting in Information Retrieval. For
tf-idf we assessed several initialisations of top ranked included terms (excluding
stop words) {50, 100, 150, 200}, sorted based on their score. Relating to standard
usage of tf-idf, each resource instance represents a document.

LDA: produces terms describing topics using machine learning approaches.
As in the case of tf-idf, we use several initialisations with varying number of
topics and terms defining a topic. The number of topics are {10, 20, 30, 40, 50},
with various numbers of terms per topic {50, 100, 150, 200}. The datasets are
represented as single documents, since the decomposition into resource instances
as documents does not influence the topic modelling tool.

The generated and ranked terms from the corresponding baseline approaches
are used as seeds to generate dataset profiles. For each individual term a DBpedia
entity is extracted, when there is a match from the automatic NER process.
From the extracted entities, we construct the dataset profiles by taking the
corresponding DBpedia categories assigned to the property dcterms:subject
and additionally expand with equivalent broader categories. Finally, the edge
weights in the profile graph P consist of topic scores assigned for the individual
datasets and correspond to the term weight (computed from one of the baselines).

http://mallet.cs.umass.edu

B. Fetahu et al.

5 Results and Evaluation

In our experimental evaluation, we focus on two aspects: i) profiling accuracy
which assesses the topic rankings induced by the graphical-models and baselines
against those in the ground truth, and ii) scalability of the profiling approach
finding the right trade-off between profiling accuracy and computation time.

5.1 Profiling Accuracy

In this section, we compare the profile accuracy from our profiling pipeline in
different configurations with those of the baseline approaches.

The profiling accuracy results shown in Figure 2a are generated based on
our profiling pipeline (using PRankP, HITSP, KStepM for topic ranking) and
tf-idf, LDA. The results from PRankP and HITSP are generated with only 10
iterations and parameter  = 0.5, which indicates the probability of jumping
back to a known vertex (in our case, an analysed resource instance of a specific
dataset). For KStepM the number of steps was set to K = 5. In the case of
baseline approaches we ran the experiments with several initialisations; however
here we report the best performing. For tf-idf, the dataset profiles were generated using the top-200 terms. For the second baseline, LDA, we used the topic
modelling tool, Mallet, with 20 topics and top-100 ranked terms. The results
shown in Figure 2a correspond to an analysis conducted on the full set of resource instances. Hence, the various sampling strategies in the profiling pipeline
are equal. Furthermore, the NDCG scores are averaged for all datasets. In the case
of PRankP, HITSP, KStepM, the values reflect the ranking gained in combination with NTR as a pre-filtering step. Similarly, the results in Figure 2b show
the profiling accuracy for the individual datasets and the best performing ranking approach, KStepM, where PRankP has comparably similar ranking with a
negligible difference.

Highlighting the benefits of applying the ranking approaches in combination
with NTR, Figure 3a shows the difference in profiling accuracy for KStepM
approach at NDCG@100 (averaged over all datasets) for different sample sizes.
The topic scores computed by NTR are used to filter noisy topics, when their
values are below the average from all topics in a dataset profile PD.

To determine the correct topic expansion level, we measure the correlation
between the expansion level and profiling accuracy Figure 3b. The results show
the impact of the expansion level on the profiling accuracy for the case of the
topic ranking approach, KStepM. The intuition is that, at a certain expansion
level, the dataset profiles are associated with noisy topics (when a topic is assigned to too many entities). Figure 3b shows that the highest overall ranking
accuracy was achieved at the expansion level of two.

5.2 Scalability vs. Accuracy Trade-Off: Impact of Sample Size

We analyse the impact of the various sampling strategies (random, weighted and
centrality) at different sample sizes on ranking accuracy, to identify a suitable
balance between ranking time and profiling accuracy.
?

?

?
(a)

(b)

Fig. 2. (a) Profiling accuracy for the different ranking approaches (in combination with
NTR) using the full sample of analysed resource instances with NDCG score averaged
D  D; (b) Best performing topic ranking approach KStepM (in combination with
NTR) for the full set of analysed resource instances
?

?

?
                   
?

?

?
(a)
?

?

?
(b)

Fig. 3. (a) Comparison of profiling accuracy for KStepM +NTR and KStepM at
NDCG@100; (b) Category level expansion impact on profiling accuracy for KStepM +NTR

To find the ideal trade-off between scalability and accuracy, we analyse the
behaviour of the ranking metric NDCG as follows: (i) average performance (NDCG)
over all datasets and computed ranks (l = 1, . . . , 1000), (ii) profiling accuracy
and topic ranking time, using KStepM ranking approach.

For (i), Figure 4 shows the results of the NDCG score for KStepM at different
sample sizes (x-axis). The plot for the individual datasets shows the standard
deviation from the average value of NDCG, indicating the stability of the profiling accuracy. While, for (ii), Figure 5 shows the correlation between profiling
accuracy and ranking time. It assesses attributes such as the amount of time
taken to rank topics (KStepM, HITSP, PRankP ) and the different sample sizes.
In detail, the leftmost y-axis shows the log-scale of the amount of time (in sec-
onds) it takes to rank the topics at the different sample sizes. The rightmost
y-axis shows the profiling accuracy achieved at a specific sample size.

B. Fetahu et al.
?

?

?
Fig. 4. Profiling accuracy averaged for all ranks l = {1, . . . , 1000}. The graph shows
the standard deviation of NDCG from the expected ranking at the different sample
sizes (x-axis).
?

?

?
Fig. 5. The trade-off between profiling accuracy (NDCG averaged over all datasets and
ranks) and the topic ranking time based on the different graphical-models

5.3 Discussion and Analysis

The results shown in the previous two sections support the proposed choice of
steps in the profiling pipeline and identified suitable parameters. The combination of topic ranking approaches (PRankP, HITSP and KStepM ) with NTR
significantly improves the profiling accuracy. In Figure 3a and for KStepM, a
drastic increase in accuracy can be noted for all sample sizes. This is rather
expected as the NTR scores serve as a pre-filtering mechanism for noisy topics.
From the overall ranking comparisons in Figure 2a, KStepM achieves the best
results (Figure 2b), with PRankP having comparably similar results.
?

?

?
By contrast, the baseline ranking approaches show that the overall performance is relatively low. The LDA-based baseline approach achieves comparable
accuracy only at rank l = 500. The results for the second baseline based on
tf-idf are uniformly very low at all ranks, with most values being well below
0.1. The results from the baselines are attained from the best performing initialisations (see Section 4.3). In the case of LDA we used 20 topics with top-100
terms per topic, and for tf-idf an increase of more than top-200 analysed terms
did not benefit significantly the profiling accuracy. The difference between the
best performing baseline based on LDA and that based on KStepM +NTR is
NDCG@100=+0.21 in favour of the latter.

The results in Figure 4 show that, at low sample sizes, the accuracy is already
fairly stable. In other words, the average profiling accuracy for the different
ranking approaches and sampling strategies increases slightly with the increase
of sample size, while its standard deviation decreases. We could identify sample
sizes of 5% and 10% as nearly optimal, which are also nearly optimal with regards
to the balance between accuracy and scalability in Figure 5. The dataset profiling time is reduced significantly while aiming for a suitable trade-off between
scalability and profile accuracy. The process of generating profiles contains three
computationally intensive steps: (i) indexing resources for further analysis; (ii)
performing the NER process; and (iii) topic ranking. With respect to (i), indexing 10% of resource instances takes on average, 7 minutes per dataset, in contrast
to up to 3 hours on average when considering all resource instances. For (ii), since
we use the online service of DBpedia Spotlight, the process is non-deterministic,
as it is dependent on the network load and the number of simultaneous requests.
Such process could be optimised by hosting the service locally. Finally, for (iii),
the topic ranking process is optimised down to 2 minutes, for 10% resources,
from 45 minutes, when considering the full set of resources (Figure 5).

Finally, the fluctuations in profiling accuracy in Figure 4 show high deviations
for dataset oxpoints. This can be explained by the fact that its resources
contain geo-information about the University of Oxford and as such it presents
a difficult case due to the low coverage from DBpedia content.

6 Related Work

Although no approach considers specifically the problem of generating metadata
about Linked Data sets profiles, our approach is closely related to a LOD Cloud
dynamics of changes analysis [13]. The work in the reported paper is related to
several fields ranging from VoID data generation [5,4], semantic indexing [18],
graph importance measures [20,12], and topic relevance assessment [8,9] address
similar problems. Thus, in this section, we briefly review the literature and compare our approach with related literature.

Generating VoID data about Linked Data sets is considered in [5], where individual triples are analysed and, based on commonly shared predicates, the corresponding datasets are clustered. In a later work, B ohm et al. [4] cluster resources
based on the dataset specific ontologies used by considering the relationship

B. Fetahu et al.

between the different resource classes. In spite of using specific ontologies to
create clusters, we use established reference datasets.

Recently, Hulpus et al. [12] proposed the Canopy framework that, for a given
set of extracted topics from analysed textual resources, the matching DBpedia sub-graph is retrieved and the corresponding relationships are quantified
using graph importance measures. In our case, we automatically extract entities from textual resources and further expand to the related DBpedia category
sub-graphs. A different approach is presented by White et al. [20], where they
measure the relative importance of a node in a data graph by incorporating
knowledge about prior probability of a specific node. We follow the same strategy to measure the importance of topics in the generated dataset profiles.

T`palo, a framework introduced by Gangemi et al. [10], analyses heuristics
for typing DBpedia entities using information extracted from Wikipedia pages
mentioning a specific entity. In our work, we focus on topic assessment using
DBpedia graph and the context of analysed resources of a dataset from which
an entity is extracted.

Another framework is Sindice [18], that indexes RDF documents and uses
DBpedia entities as a source to actively index resources. Additionally, Kiryakov
et al. [14] index the Web of Documents and capture extracted named entities
in a manually crafted ontology. Comparing to our work, we go beyond mere
annotations and generate an interlinked data graph of datasets based on topics
which are quantified for their importance based on the support given from the
individual resource instances.

K afer et al. [13] have crawled and analysed the LOD cloud focusing mostly on
the dynamics of changes in datasets (predicates, number of instances, etc). The
crawling process relies on pre-selection of prominent resources (ranked based
on PageRank). We aim at generating dataset profiles and analysing the temporal aspects of topics on how they evolve during time. LODStats [2] analyses
the LOD-cloud structure and provides statistical characteristics of datasets and
metrics related to vocabulary usage. In spite of the insights gained through such
an analysis, we focus at a content-wise analysis.

7 Conclusions

In this paper, we proposed an approach to automatically generate structured
dataset profiles with the overall goal of facilitating the assessment, search and
discovery of LD datasets. Aiming for a method which is scalable and efficient and
yet, at the same time, provides a high level of accuracy and representativeness of
the generated data, our approach uses sampling techniques together with ranking
methods to provide the profile graph. Based on experimental evaluation, the most
suitable trade-off is found between small sample sizes to cater for efficiency and
representativeness of the resulting profiles.

As part of our experiments, we generated dataset profiles for all datasets
in the LOD Cloud. The evaluation shows that, even with comparably small
sample sizes (10%), representative profiles and rankings can be generated (i.e.
?

?

?
NDCG=0.31 for socialsemweb-thesaurus), when applying KStepM combined
with the NTR. The results demonstrate superior performance when compared
to LDA with NDCG=0.10 (with the full set of resource instance).

It has been noted that meaningfulness and comparability of topic profiles
can be increased when considering topics associated with certain resource types
only. As part of our current work we are developing resource type-specific dataset
profiles and the tracking of topic profile evolution. These take advantage of our
profile graph to provide more specific dataset search and browsing capabilities.

Acknowledgements. This work was partly funded by the LinkedUp (GA
No:317620) and DURAARK (GA No:600908) projects under the FP7 programme of the European Commission.
