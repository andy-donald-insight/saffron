Semantics Inside!

But Lets Not Tell the Data Miners:
Intelligent Support for Data Mining

J org-Uwe Kietz1, Floarea Serban1, Simon Fischer2, and Abraham Bernstein1

1 DDIS, University of Zurich, Switzerland

{serban,juk,bernstein}@ifi.uzh.ch, fischer@rapid-i.com

2 Rapid-I, Dortmund, Germany

Abstract. Knowledge Discovery in Databases (KDD) has evolved significantly over the past years and reached a mature stage offering plenty
of operators to solve complex data analysis tasks. User support for building data analysis workflows, however, has not progressed sufficiently: the
large number of operators currently available in KDD systems and interactions between these operators complicates successful data analysis.
To help Data Miners we enhanced one of the most used open source
data mining toolsRapidMinerwith semantic technologies. Specifi-
cally, we first annotated all elements involved in the Data Mining (DM)
processthe data, the operators, models, data mining tasks, and KDD
workflowssemantically using our eProPlan modelling tool that allows
to describe operators and build a task/method decomposition grammar
to specify the desired workflows embedded in an ontology. Second, we
enhanced RapidMiner to employ these semantic annotations to actively
support data analysts. Third, we built an Intelligent Discovery Assistant,
eIda, that leverages the semantic annotation as well as HTN planning
to automatically support KDD process generation.

We found that the use of Semantic Web approaches and technologies
in the KDD domain helped us to lower the barrier to data analysis. We
also found that using a generic ontology editor overwhelmed KDD-centric
users. We, therefore, provided them with problem-centric extensions to
Prot eg e. Last and most surprising, we found that our semantic modeling
of the KDD domain served as a rapid prototyping approach for several
hard-coded improvements of RapidMiner, namely correctness checking
of workflows and quick-fixes, reinforcing the finding that even a little
semantic modeling can go a long way in improving the understanding of
a domain even for domain experts.

Keywords: #eswc2014Kietz.

Introduction

Over the last years Knowledge Discovery for Large Databases (KDD) has grown
immensely and attracted more focus from both research and industry since large
 The research leading to these results has received funding from the European Union
Seventh Framework Programme FP7/2007-2011 under grant agreement No 231519.

V. Presutti et al. (Eds.): ESWC 2014, LNCS 8465, pp. 706720, 2014.
c Springer International Publishing Switzerland 2014
?

?

?
amounts of data have been generated that need to be analyzed. New algorithms
and methods have been proposed and even integrated in current Data Mining
(DM) tools. But KDD is a complex process transforming the raw data into
actionable knowledge by applying a series of successive algorithms. Current DM
tools allow users to manually build KDD workflows and select each step from
a large pool of possible solutions. Yet, this is very tedious and often leads to
workflows that crash after a few hours runtime. In addition, it has been shown
that users and even DM experts tend to stick to a set of preferred methods
and do not explore the entire design-space [13]. This often produces sub-optimal
analyses. Despite the progress such tools have made during the last years, their
user support is, hence, still far from perfect.

Some of the issues highlighted above have been explored by the EU funded e-
Lico project.1 An important task of this project was to provide intelligent support
for DM to simplify the data analysis process for users. Inspired by [4] the plan
was to intelligently support users by leveraging the semantic annotation of DMoperations to allow a correct composition of workflows. Hence, we annotated the
main components of KDD workflows (data, algorithms, and goals) and operators
available in RapidMiner [14]one of the most used open source Data Mining
toolsusing ontologies. Treating the problem as a service composition problem,
where algorithms are services provided by DM tools, we employed ontologies,
Hierarchical Task Network (HTN) planning [7] to automatically explore the large
design-space of correct workflows and recommender-system technology to help
users navigate this space [20].

As KDD is a dynamic domain, we developed eProPlana tool that enables
DM service providers to efficiently model their services and define semantics
facilitating the semantic description process and providing tools for testing and
maintaining the model over time. eProPlan was used by several partners (in-
cluding the RapidMiner developers) to model different operators (e.g., basic DM
operators, text mining operators, etc.). As Section 5 outlines, insights from this
modeling effort where included in the productive RapidMiner version to check
for correctness of workflows and suggest possible fixes.We also designed an Intelligent Discovery Assistant (IDA), eIda, that enhances RapidMiner and Taverna
[16] with the ability to automatically generate semantically correct workflows
(i.e., exploring the design space of valid workflows) starting only from a dataset
and a DM problem. Finally, we developed a recommender-component that can
rank generated workflows in terms of their expected accuracy, which is able
to advise a user on which of the workflows generated by eIda are expected to
perform well [20] simplifying the choice significantly.

This papers main contributions are the distillation of our experiences implementing the e-Lico tool-suite, the evaluations we undertook of the different
parts, and the lessons we learned from applying semantic technologies in this
increasingly important application area that is so heavily influenced by induction instead of logical reasoning. In particular, it is witness how insights from

http://www.e-lico.eu/

J.-U. Kietz et al.

modeling the KDD domain transcended into the actual code of one of the most
used KDD-software.2

The discussion is structured as follows: Next, we succinctly explore related
work, followed by a discussion of the three implemented systems: eProPlan for
describing the semantics of services in Section 3, eIda to automatically generate
the KDD workflows in Section 4, and the auto-experimenting recommender in
Section 4.3. The side effects of semantics are discussed in Section 5. Finally we
explore the lessons learned from this project in Section 6 and we draw the main
conclusions in Section 7.

2 Related Work

Researchers have been extensively looking at service composition especially in
the area of web services [5,15]. The most common automation techniques for
web services are either based on workflow composition [3] or AI planning [18].
Some even used a more advanced form of AI planning called Hierarchical Task
Network (HTN) to find the matching web services [24]. Similarly, we use HTN
planning to generate the design-space of KDD workflows. In contrast to others,
we did not look for one possible solution but were exploring the space of all
correct solutions. Also we did not rely on OWL-S but simple ontologies, where
the conditions and effects were stored as annotations.

Other researchers looked into automating KDD [21]. Most of those tools
are, however, limited to being research prototypes and did not provide explicit
support for modeling their background knowledge. The Intelligent Discovery
Automatic Assistant (IDEA) [4] was among the first prototypes to propose a
combination of ontology-based planning, result ranking, and operator information sharing for supporting KDD. The RDM system [26] uses an OWL-DL [17]
ontology for knowledge discovery. This ontology is queried using the Pellet reasoner [23] and SPARQL-DL queries [22] for retrieving the operator inputs and
outputs, which are then fed into the planner. Two AI planners are used that
query the ontology during the planning process.Similarly, KDDVM [6] interacts
with the ontology by using the Pellet reasoner and SPARQL-DL queries. Instead of applying a standard planning algorithm it utilizes a custom algorithm
that starts at the goal state and iteratively adds operators forming a directed
graph until the first operator is compatible with the given dataset. Operators are
added using an algorithm matching procedure, which checks the compatibility
of inputs and outputs. The operator interfaces are not matched perfectly do not
need to be perfectly but according to a similarity computed via their distance
in the ontology graph. Finally, the produced workflows are ranked based on the
similarity scores as well as other operator properties stored in the ontology (e.g.
soft pre-conditions or computational complexity estimates). Finally, MLWizard,3

2 According to a recent poll http://www.kdnuggets.com/2013/06/

kdnuggets-annual-software-poll-rapidminer-r-vie-for-first-place.html
http://madm.dfki.de/rapidminer/mlwizard
?

?

?
e.g., an IDA available at the RapidMiner marketplace only supports a few classification tools and no regression and clustering. It also focuses the induction
step and does not consider evaluation (e.g. building a cross-validation or test-set
evaluation process) and preprocessing (e.g. handling missing values, attribute
type conversions, and normalization).

eProPlan: A Semantic DM Service IDE

To reason and plan about KDD operators one needs to specify the main element of the KDD domain and the capabilities of each individual operator (or
service) in terms of their IOPE (Inputs, Outputs, Pre-conditions, and Effects).
To facilitate these tasks we built eProPlan4 [10] to be used by the DM service
providers who are the DM experts. They have the knowledge about the services,
how their services work, what data they can model, what they produce, and how
they are implemented. eProPlan is a Prot eg e plugin that not only allows to
describe semantically the services but also to improve, test, debug, and extend
the service model as well as the HTN used for planning (see Figure 1a). In the
following we first discuss eProPlans ontology and models before discussing its
features supporting editors and its usage.

DM Service Providers

Model
Test

Maintain

Modeling & Checking 

Operators

HTN Modeling

Plan Testing

e

r
o

l

a
n

DM Ontology

Planner

Protege Ontology Editor

a
s
i
c
?

?

?
p
e
r
a
t
o
r
s

DMWF-HTN Ontology

RapidI-Generated Ontology

RapidI Ontology

DMWF Ontology

Base Ontology

(a) eProPlan components

(b) Ontologies

Fig. 1. eProPlan system and its ontologies

3.1 DM Ontology for Workflow Planning

In our case services are actual algorithms that can be organized by their capabilities and data applicability. Hence, one can use an ontology to structure and
model them. We built a set of OWL-DL ontologies that are describing the DM
domain. To ensure decoupling and re-use we built different ontologies organized
in a stack (see Figure 1b), where each ontology imports the one directly below
it (and the others below it indirectly).

4 which is available from the e-Lico web-site.

J.-U. Kietz et al.

The Base ontology (see Figs 1b and 2) contains the building blocks for modeling
the services (operators). Operators use different inputs and produce one or several outputsall IO-Object s. They also have different types: abstract (used only
to organize operators in hierarchies), basic (can be executed), and dominating
or loops (allowing a sequence of operators that can be repeated). Parameters are
used to tune algorithms or to set mandatory values by defining data- or object
properties (e.g., parameter, simpleParameter ).

The composition of services starts by defining a Goal that takes as input
the data that needs to be analyzed. Next, the main goal uses a Task that can
be solved by one or more Methods. Each method has then a set of steps that
represent the services in the composition. Such a step can either be a Task or
an Operator allowing the definition of very complex workflows and even loops
over the same tasks.

The main advantages of embedding the HTN-grammar into an ontology are
that (1) operators are organized in an hierarchy and (2) abstract operators can
be used in the HTN-grammar. The abstract HTN operator ClassificationLearner,
e.g., has several executable sub-concepts in RapidMiner and Weka. This enabled
a compact and, often, execution-system independent HTN-grammar.

The DM Workflow ontology. (DMWF in Figs 1b and 2) defines sub-classes for
all the basic classes in the Base ontology. The focus of this ontology is on the
composition of specific services from DM. Operators are specified for each step
of the workflow from pre-processing to evaluation (e.g., FormatData, Modeling,
ModelEvaluation, etc.). Data can be of different types as for example DataT-
able, Model or Report. It has nominal or ordinal columns and attributes. The
characteristics of data are specified using sub-classes of the MetaData class (e.g.,
Attribute, AttributeType, DataColumn, DataFormat, etc.). The MainGoal materializes now to concrete tasks from DM like Descriptive- or PredictiveModeling.
Here we leveraged the completion of IO-Object s via ABox realization and
SWRL-rule reasoning, which enabled compact and understandable descriptions
similar to axioms in the Planning Domain Description Language (PDDL) [25].
We, e.g., defined a concept for missing value (MV) free data tables
M V F reeDataT able  DataT able  inputColumn.M V F reeColumn 
targetColumn.M V F reeColumn, where M V F reeColumn in itself is a defined
concept M V F reeColumn  DataColumn  amountOf M V s = 0

The RapidI-Generated ontology. (see Figure 1b) specifies the RapidMiner provided algorithms (services). The ontology contains the RapidMiner operators as
well as their pre-conditions and effects. These are rules that define when certain
operators can be used in terms of the characteristics of the data and what they
produce (either new data or changes on the input data). OWL-S [1] already
provides the semantics and models for web service composition. Its support for
editing in Prot eg e was limited when we built eProPlan and was difficult to
explain to our user-base (KDD domain experts and RapidMiner developers)
 experiences that were reflected by others [19]. Also, it modeled services as
instances. To simplify matters we chose to describe services as OWL classes.
?

?

?
Attribute

attributePossible

Type

AttributeType

Nominal

Ordinal

Predictive
Modeling

columnHasType

hasAttribute

DMWF Ontology

FormatData

DataColumn

Descriptive
Modeling

DataTable

hasTable

DataSet

Modelling

DataTable
Processing

Induction

Task

TableFormat

hasFormat

hasFormat

Data

DataFormat

Model

Report

Model

Evaluation

Model

Application

MetaData

Domain
Model

startObject

IOObject

parameter*

MainGoal

Goal

worksOn+

contributesTo

useTask

Task

TaskMethod

solvedBy+

decomposedTo

produces*
uses+

decomposed

To

Method

Operator

operatorHas

Type

Operator

Type

Base Ontology

Fig. 2. Main classes from the Base and Workflow ontologies

To model the pre-conditions and effects we relied on the Prot eg es SWRL plu-
gin. Since the complexity of the rules needed was higher than it was possible
to express in SWRL, we have added some new built-ins [12] and saved them as
annotations in the ontology.

The operators are organized in an strict inheritance concept-hierarchy. We
used the OWL-inheritance to propagate parameter restrictions from abstract
to concrete, implemented operators as well as conditions/effect representations.
This provided an effective way to build and maintain descriptions of over hundred
different operators (see section 2.3 in[11] for an example).

The DMWF-HTN ontology. (see Figure 1b) is used to define the tasks and
methods for planning. HTN planning is like a top-down grammar that specifies the skeleton or template of workflows. Each step of the workflow can be
described in term of specific tasks or even sub-tasks (e.g., DataMining, Mod-
elingWithCrossValidation, Preprocessing, CleanMissingValues, CleanManyVal-
uedNominals, NormalizeScalar, etc.). Further, the planner employs the template
and fills in the services that match. The planning relies on external reasoners
available in Prot eg e for TBox reasoning and an internal ABox reasoner. Details
of the operator models and of the used HTN-planning can be found in [11]

Domain Independence. The modularity of our approach would allow using our
tools in another application domain by starting with the Base ontology and
then adapting the upper elements. To ensure that this actually works we also

J.-U. Kietz et al.

modeled the Missionaries and Cannibals problem from classical planning. Hence,
eProPlan can be used as a generic ontology editor for HTN planning for different domains.

3.2 eProPlan: Support for Editing, Testing, and Optimization

To allow domain experts to model operators/services for planning we developed
eProPlan, which has several Prot eg e views. The Operators tab allows users
to define conditions and effects and includes a semantic checker that highlights
errors in the definition and suggests fixes. This avoids erroneous definitions of
services and maintains ontology correctness.

The operators applicability can be tested using the Applicable operators tab,
where each operator can be applied on different types of data. Furthermore,
it supports a step by step composition to ensure that correct workflows can
be generated. Since standard OWL-reasoners do not support states, we built
the planner in Flora2/XSB.5 Consequently, the ontological domain needs to be
compiled for planning at which stage the conditions and effects are checked again
for correctness and the errors are stored as annotations to the corresponding
operators. This ensures that the planning domain is defined correctly.

In the HTN editor tab one can create new tasks and methods and specify each
step of the workflow. The editor shows the hierarchy of tasks and methods and
also each independent step with their inputs and outputs. The user needs to define the matching IO-Objects from one step to another to ensure the correctness
of the workflow.

3.3 Usage of eProPlan

eProPlan was used and tested by different parties as it is the main tool developed for the e-Lico project. The partners from Rapid-I were responsible for
modeling the RapidMiner operators. Without prior experience in ontology modeling they successfully delivered a complex ontology that contained more than
100 modeled operators. As the number of services is relatively large (a few hun-
dreds) they automatically generated the code by mapping their algorithm specifications to the conditions and effects in the ontology. Some additional work
was then needed to check the correctness of the produced service model using
eProPlan.

To gather feedback about the usability of eProPlan we designed a questionnaire based on the System Usability Scale (SUS) [2]. SUS has been used in many
tests and is often described as a quick and dirty usability test. SUS scores can
range from 0 (very little satisfaction) to 100 (very high satisfaction). Usual average satisfaction scores are reported to be between 65 and 70. For eProPlan
we obtained SUS questionnaires from 5 users varying from 42.5 to 72.5 with a
mean score of 63.5. [8] reports a SUS score for Prot eg e (with 15 subjects) that

http://flora.sourceforge.net/
?

?

?
varies between 20 to 78 with a mean of 47.6 We are well aware that comparing
SUS scores between settings is highly problematic. We can, therefore, only infer
that eProPlans usability seems comparable to Prot eg es.

eIda  A Tool for Users

eIda is a programming interface that provides all the functionality (in particular
to the reasoner & planner) needed to build an Intelligent Discovery Assistant
(IDA). It provides methods for retrieving the DM workflows starting from the
datasets meta-data and the selection of a main goal. So far it was used to build
IDAs for both RapidMiner and Taverna.7 The RapidMiner IDA Extension can be
downloaded (or even auto-installed) from the Rapid-I Marketplace8. So far it was
downloaded over 8000 times. The Taverna extension9 can execute the workflows
generated by the IDA10 using any RapidAnalytics11 server that provides all
RapidMiner operators as web-services. Extensions for other KDD tools (e.g.,
KNIME, Enterprise Miner, etc.) would require two steps: first modeling their
corresponding operators in the DMWF, second an implementation of the GUI
and the plan-converter using the IDA-API.

We tested the IDA on 164 datasets from the UCI repository of Machine Learning datasets.12 It produced executable plans for all 117 classification and 47 regression problems. These datasets have between 3 and 1558 attributes, being all
nominal (from binary to many different values like ZIP), all scalar (normalized
or not), or mixed. They also have varying degrees of missing values. We are not
aware of any other Machine Learning or DM approach that is able to adapt itself
to so many different and divergent datasets. The IDA also works for less well
prepared datasets like the KDD Cup 1998 challenge data (370 attributes, with
up to 50% missing values and nominal data), where it generates plans of around
40 operators. Generating and ranking 20 of these workflows took 400 sec. on a
3.2 GHz Quad-Core Intel Xeon.

4.1 Ease of Use

Without an IDA data mining is typically done by specialized highly-trained
professionals such as DM consultants. They have to know a lot about DM methods and how they are implemented in different tools. They need to inspect the
data and combine the operators into an adequate workflow. However, the large
number of available algorithms is overwhelming even for specialists.

6 That is rather a rather low score for Prot eg e might be the result of its comparative

evaluation setting with with CLOnE, a Controlled Language Ontology Editor.
http://www.taverna.org.uk/
http://rapidupdate.de/UpdateServer/faces/
product details.xhtml?productId=rmx ida
http://e-lico.eu/taverna-ida.html
http://e-lico.eu/taverna-rm.html
http://rapid-i.com/content/view/182/196/
http://archive.ics.uci.edu/ml/

J.-U. Kietz et al.

2 2

5 5
?

?

?
Fig. 3.

IDA Interface in RapidMiner

The IDA reduces the technical burden and offers DM with 7 clicks (see
Figure 3). (1) Show the IDA-Perspective of the tool; (2) drag the data to be
analyzed from the repository to the view or import (and annotate) your data;
(3) select your main goal in DM; (4) ask the IDA to generate workflows for data
and goal; (5) evaluate all plans by executing them in RapidMiner; (6) select the
plan you like most to see a summary of the plan (the screenshot in Figure 3 is
made after this step); and finally, (7) inspect the plan and its results. Note that
these steps do not require any detailed technical knowledge. Still a user should
be aware of what (s)he is doing when (s)he uses DM, i.e. (s)he should know the
statistical assumptions underlying DM (e.g., a user should know what it means
to have a sample that is representative, relevant, and large enough to solve a
problem with DM/statistics).

4.2 Speedup of Workflow Design

Besides making DM easier for inexperienced users, our main goal in building
the IDA was to speed-up the design of DM workflows. To establish a possible
speed-up we compared the efficiency of CS students after attending a DM class
to a person using the IDA. The study comprises in total 24 students (9 in 2011
and 15 in 2012). They had to solve the following DM problems:
?

?

?
 Take the UCI Communities and Crime data-set13 and

a) generate a fine clustering that allows to find very similar communities
b) generate a description of the clusters (learn a model to predict the cluster

label built in task a)).

c) generate a function to predict ViolentCrimesPerPop and evaluate it

with 10-fold cross-validation.

 Take the UCI Internet Advertisement data-set14 and generate an evaluated

classification for the attribute Ad/Non-Ad.

All data was provided already imported into RapidMiner (via a local RapidAnalytics server they could access). All students needed the full 3 hours (see Figure
10 in [11] for details on typical errors the students made and accuracys reached
by IDA and students).

As a comparison we asked a non-specialist (a member from our Group, not
involved in the e-LICO project) to accomplish the same task using the IDA. It
took the non-specialist 30 minutes to (IDA planning, minimal manual adapta-
tion, and execution of the workflows) with a comparable output.

The study confirmed that standard DM problems (such as clustering and
prediction tasks on complex UCI data) can be sped-up considerably (30 minutes
vs. 3 hours) by using an IDA whilst maintaining a comparable quality.

The experiments also showed clearly that even students at the end of a termlong DM class were overwhelmed when confronted by two typical DM task. The
IDA operated by a DM novice, with minimal guidance about which proposed
workflow to use,15 was able to provide comparable results when. Note that the
students are an optimal user-group for the IDA, as they have limited DM experience but they understand the principles of DM.

4.3 Auto-Experimentation and Performance of the Generated

Workflows

One of the most complex aspects of Data Analysis is to decide which KDD
workflows are likely to be successful. Hence, we employed eIda for another research task: the ranking of the generated KDD workflows. This is an important aspect since the number of valid workflows is quite large (easily over 1000
for classification problems) due to the large number of operators available in
RapidMiner and modeled in our ontology. To solve this problem we combined
auto-experimentation (the planing and execution of experiments) with collaborative filtering [20]. The detailed description of this approach is clearly beyond
the scope of this paper. We summarize that we evaluated the ranking on 100
datasets from the UCI repository. For new datasets our approach was able to
make recommendations that were at most 5% worse than the best workflow by
executing only 3%-5% of the overall workflows.

http://archive.ics.uci.edu/ml/datasets/Communities+and+Crime
http://archive.ics.uci.edu/ml/datasets/Internet+Advertisements

15 A problem we addressed with auto-experimentation (discussed in the next section).

J.-U. Kietz et al.

This excellent result shows that an IDA based on Semantic Technology and
collaborative filtering approaches is able to automatically decide how to analyze
a data-set with a result that is close to the best possible analysis. We believe
that this result clearly shows the power of Semantic Technology applied to DM
problems.

5 Side Effects of Semantic Technology Usage

Besides the development of the systems described in this manuscript, several
improvements were made as side effects in the core of the RapidMiner user
interface. Those simple things in fact contributed a lot to the usability of RapidMiner as a data analysis workbench. They were one of the most relevant features
culminating in the release of RapidMiner 5. We will briefly sketch them in this
section.

RapidMiner improvements When building the DM ontology as described in Section 3.1, a lot of work went into the specification of pre-conditions and effects
of operators. This information is not only useful for the DM ontology and the
IDA, but also when designing processes in the traditional way. The RapidMinerteam therefore, decided to make as much information as possible also available
in Java. To that end, as of RapidMiner 5.0, each operator can (and all operators
in the core do) provide so-called meta-data propagation rules. They must be
very quick, since they are frequently evaluated at process design time, whenever
changes to the process are made. They serve three purposes.

First, they compute, given the (possibly incompletely specified) meta data
delivered at the input ports, what the meta data of the output will be. Note
that sometimes, this information cannot be known at design time, but only at
run-time. A good example is the Pivot operator which transforms data values
into new attributes. Since the set of all values is not necessarily known without
loading the full data set, the complete set of attributes after the execution of
the Pivot operator may not be known before execution time. Another example is loading data from dynamic sources like Web services. In such cases, the
information can be marked as partially unknown.

Second, once the meta data are evaluated, these rules check pre-conditions
required by the individual operators. As an example, a learning scheme like an
SVM requires all input attributes to be numerical. If these pre-conditions are
not satisfied, a warning is displayed in a special view (akin to errors in IDEs)
and the operator is highlighted in red to signify that it cannot be executed. This
saves the user the effort of test-wise executing the process for debugging.

Finally, when an operator is found to be non-applicable quick-fixes are offered
to the user. Using the example above, when the user tries to apply an SVM on
non-numerical data, a quick fix could be to insert an operator for dummy-coding
nominal values (as shown in Figure 4).

In summary, the semantic annotation of the DM operators done for the IDA
have served as a rapid-prototype that inspired the further development of Rapid-
Miner. It significantly improved the usability of RapidMiner, which is supported
?

?

?
Fig. 4. A quickfix for making an SVM applicable on non-numeric data

by a significant increase of users and community activity after the release of
RapidMiner 5 (including over 350000 downloads and an increase in marketshare from 21%16 to 37.8%17 since its launch).

Sharing scientific workflows. Designing KDD workflows is a complex task requiring both a good understanding of the problem and of the algorithms available
in KDD tools. This makes sharing of such workflows a valuable feature akin to
sharing program code: it allows researchers to convey their knowledge and experience to others. Imagine that people can browse through a repository of good
performing workflows for a certain area and then they can test and evaluate
them on their datasets. This is especially useful for novice data miners who do
not know all the subtleties of the domain and do not have the required experience to avoid mistakes. But it may be beneficial even for specialists, as they may
come across interesting workflows that they did not use or think about before.
Another side-effect of our modeling of the DM domain is that sharing these
workflows has become easier. Within e-Lico we developed a myExperiment [9]
plugin for RapidMiner. It allows scientists to have groups per research topics and
share various workflows and their experience on different data. Whenever people
get good workflows using the RapidMiner IDA Extension or when they manually
design them, they can now share them using the myExperiment plugin. Both
Taverna and RapidMiner offer this feature. Free re-usability, search, and storage
of workflows is, therefore, ensured. myExperiment users can tag, write reviews,
comments, and even annotate and rate workflows.

The e-Lico project participants have already shared and uploaded several
dozen DM workflows on this platform, and several hundreds have been added
since then by the community.

6 Lessons Learned

During the 3 year EU project we were facing the challenge of adapting and
intermingling various Semantic Web techniques to build an IDA for the data
analysis process. This required us to support two types of users and, therefore,
provide different types of support: the DM service providers who are responsible
for modeling the DM domain and the end-users who only want to magically
get valuable knowledge out of their dataset/task at hand. An important lesson

http://www.kdnuggets.com/polls/2009/data-mining-tools-used.htm
http://www.kdnuggets.com/polls/2010/data-mining-analytics-tools.html

J.-U. Kietz et al.

discovered in the first year is that service providers are not ontology modeling
specialists and, therefore, need debugging support for Semantic Modeling, despite
being AI specialists. In addition, end-users are not interested in semantic technology and the underlying details. They want to solve problems, in our case
KDD analyses, with as little effort as possible. The less we showed them about
the inner workings (and semantic technology) the happier they were.

Ontologies have become more popular and are used for modeling and structuring different domains. Ontology editors have adapted and provide different
plugins to facilitate the transition to the semantic notations. From our experience with the users from the project we observed that domain specialists are
extremely reluctant to climb the learning curve to use these editors. Therefore,
to simplify and expedite their learning process, we found it helpful to build domain specific ontology editors (such as eProPlan) that hide the basic ontology
constructs by replacing them with domain constructs. These facilitate bridging
the application domain and semantic annotation. Reinforcing this finding we
asked all e-Lico project participants what their most liked eProPlan feature
was. The winner was the special editor for conditions and effects as well as the
applicable operators tabboth elements highly applicable to the DM operator
modeling problem they had to solve. They also appreciate having any kind of
validation and especially a way of finding out the problems or errors in their
modeling. Consequently, it seems that one of the biggest hurdle for wide-spread
Semantic annotation is the provision of low-effort domain-specific editors with
built-in validation facilities.

One of the biggest findings of our project is that the payoff of unexpected sideeffects of using semantic technology may easily surpass the effort invested. In the
spirit of a little semantics goes a long way18 we could enhance RapidMiners
interface with simple semantic functions that immensely simplified its usage.
Whilst we lack proof that this is the cause of the surge of RapidMiners popularity we gathered evidence from user testing at Rapid-I that a number of test
users were able to fix problems with the quick-fix functionality that specialist
data miners did not think were possible. Even if the semantic techniques where
not used in this improvements, they served as a rapid-prototyping approach that
influenced the further development of RapidMiner.

Last but not least, the ability to explore the design-space of a realistic-sized DM
domain and successfully propose well-performing workflows replied on semantic
technology and allowed us to solve practical problems that have been pursued for
decades [21].

7 Conclusions

In this paper we presented our experiences with using semantic technologies in
the KDD/DM domain. In this domain the number of available operators and resulting design space of possible DM workflows goes beyond the capability of even

http://www.cs.rpi.edu/~hendler/LittleSemanticsWeb.html
?

?

?
specialists. Our solution couples HTN planning as well as auto-experimentation
with recommendations with semantic web technologies to address this problem.
Specifically, we provided eProPlan, an editor and planning environment for
services, and eIda, an Intelligent Discovery Assistant engine that was included in
both RapidMiner and Taverna. Leveraging the functionalities of these tools we
were able to semantically annotate the KDD domain and use those annotations
in practical usage: we found that these tools relying on semantic technologies
were able to support novice users to get good results in data mining tasks.
To put it to real-world tests we shipped these capabilities with RapidMiner.
Furthermore, we found that when complementing these tools with advanced
auto-experimentation based recommendation technology we could automatically
propose good performing workflows.

We also found that the side-effects of Semantic annotation led to a series
of high-impact improvements in the usability of RapidMiner indicating that a
little semantics goes a long way  in our case helping to boost the popularity
of RapidMiner. Most interestingly, we managed to put semantic technology at
work on the computers of tens of thousands and influenced the user experience
of possibly hundreds of thousands users making their work simpler whilst leaving
them completely oblivious about the usage of such technology  a goal that we
believe every semantic technology project should strive for.
