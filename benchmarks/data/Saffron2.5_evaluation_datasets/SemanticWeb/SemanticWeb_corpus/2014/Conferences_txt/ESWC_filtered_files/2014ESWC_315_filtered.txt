A Probabilistic Approach for Integrating

Heterogeneous Knowledge Sources

Arnab Dutta, Christian Meilicke, and Simone Paolo Ponzetto

Research Data and Web Science, University of Mannheim, Germany

{arnab,christian,simone}@informatik.uni-mannheim.de

Abstract. Open Information Extraction (OIE) systems like Nell and
ReVerb have achieved impressive results by harvesting massive amounts
of machine-readable knowledge with minimal supervision. However, the
knowledge bases they produce still lack a clean, explicit semantic data
model. This, on the other hand, could be provided by full-fledged semantic networks like DBpedia or Yago, which, in turn, could benefit
from the additional coverage provided by Web-scale IE. In this paper,
we bring these two strains of research together, and present a method
to align terms from Nell with instances in DBpedia. Our approach is
unsupervised in nature and relies on two key components. First, we automatically acquire probabilistic type information for Nell terms given
a set of matching hypotheses. Second, we view the mapping task as the
statistical inference problem of finding the most likely coherent mapping
 i.e., the maximum a posteriori (MAP) mapping  based on the outcome of the first component used as soft constraint. These two steps are
highly intertwined: accordingly, we propose an approach that iteratively
refines type acquisition based on the output of the mapping generator,
and vice versa. Experimental results on gold-standard data indicate that
our approach outperforms a strong baseline, and is able to produce everimproving mappings consistently across iterations.

Keywords: #eswc2014Dutta.

Introduction

The last few years have witnessed much work in information extraction (IE).
Although Wikipedia-based IE projects such as DBpedia [3] and Yago [24]
have been in development for several years, systems like Nell [6] and ReVerb
[12] have gained importance more lately. State-of-the art IE systems work on
very large, i.e., Web-scale text corpora and are based on the general paradigm
of Open Information Extraction (OIE) [2], which identifies IE systems that are
not constrained by the boundaries of encyclopedic knowledge or a corresponding
fixed schemata, unlike, for instance, those used by Yago or DBpedia.

The data maintained by OIE systems is important for analyzing, reasoning
about, and discovering novel facts on the web and has the potential to result in
a new generation of web search engines [10]. However, while OIE systems have

V. Presutti et al. (Eds.): ESWC 2014, LNCS 8465, pp. 286301, 2014.
c Springer International Publishing Switzerland 2014
?

?

?
very large coverage, they lack a full-fledged, clean ontological structure which,
on the other hand, is essential in order to be able to exploit their output for Semantic Web applications. Often, the facts extracted by these systems are hard
to decipher, and terms occurring in these very same facts can be highly am-
biguous. For instance, let us consider a typical Nell extraction in the form of
a property(subject, object) triple such as agentcollaborateswithagent(knight,
indiana). While we might have an intuitive understanding of the property, it is
difficult to determine the correct references of the terms in the triple. Actually,
our example refers to Bob Knight, the head coach of the basketball team In-
diana Hoosiers. In contrast, an ontological resource, like DBpedia, uses a URI
to uniquely identify each entity that appears within a triple. In our case, any
DBpedia triple that talks about Bob Knight uses an unique URI to refer to that
specific person. Thus, the meaning of a triple is precisely and uniquely specified.
In general, OIE systems trade-off large coverage for a weak, e.g., schema-less
or schema-poor, semantic representation. In this work, we address this problem
by bringing together information from a state-of-the-art OIE system and DB-
pedia. We achieve this by mapping the subject and object terms from Nell to
DBpedia entities. Specifically, we propose a method to automatically determine
the correct references of terms from OIE systems using probabilistic reasoning.
We embed our probabilistic model that exploits the type hierarchy from DBpedia within a bootstrapping approach. As a result of this, we are able to provide
via linking a clear semantic representation for both subject and object terms that
occur within triples generated by a OIE system. Our hunch here is to provide a
framework that makes it possible for different OIE projects to take advantage of
the schema information provided by structured ontological resources like Yago
and DBpedia. This way the output of OIE is fully semantified within structured
resources with an ontological model: by converse, the reference ontologies can
benefit from the broader coverage of OIE projects.

2 Problem Statement

We present a methodology to map the output of OIE systems to an ontological
resource like any of DBpedia, Yago or Freebase. Key to our method is the
synergistic integration of (i) information about the entity types the OIE terms
can refer to and (ii) a method to find a global, optimal solution to the mapping problem across multiple extractions on the basis of statistical reasoning
techniques. These two phases are highly intertwined, thus, we alternate between
them by means of an iterative approach.

Given an OIE triple, there can be multiple plausible mappings to a set of
highly related entities in the target ontology. For instance, the term tom sawyer
occurring within the Nell triple bookwriter(tom sawyer, twain) can be mapped
to a set of DBpedia entities: the fictional character Tom Sawyer (Tom Sawyer ),
the actual book written by Mark Twain (The Adventures of Tom Sawyer ), or
the many screen adaptions of the book (e.g., Tom Sawyer (1973 film)). While
all these entities provide plausible meanings for the occurrence of tom sawyer,

A. Dutta, C. Meilicke, and S.P. Ponzetto

knowing (or estimating) their types would allow us to further filter out meanings
which are incompatible with the types of entities that occur as arguments of the
property bookwriter. For instance, knowing that bookwriter relates books and
authors would allow us to conclude that the correct mapping for tom sawyer in
DBpedia is probably not a film or a fictional character, but rather an instance of
a book. However, domain or range restriction in terms of DBpedia concepts are
not defined for the extraction results of OIE systems. When including entity type
information in the mapping problem, we are faced with two challenges, namely:
(i) to estimate weights for the domain and range type of a Nell property term
using the terminology of DBpedia; (ii) to effectively exploit this information in
the actual mapping task.

With respect to the range of bookwriter, a good weight distribution would,
for example, entail that the type Writer is more probable than Politician
and Politician is more probable than Location. Note that its not sufficient to
determine Writer as range of bookwriter, because many entities writing books
are not explicitly typed as Writer but are of different types (e.g. Athletes
can also write books). Given a weight distribution for domain and range types
of entities, we want to exploit this information in a way as to automatically
identify the correct mappings. Using a statistical approach, we can start with
some prior probabilities for each of the mapping candidates, and combine these
priors with weighted type information such that we produce the best set of
matches as output.

We show in the following that, for the resource mapping task at hand, acquiring type information and producing high-quality mappings are two highly
intertwined problems. Our method starts with a set of mapping hypotheses between OIE terms and DBpedia instances. We combine these potential mappings
with automatically learned entity type information (Section 3.1), and define a
joint inference task within Markov Logic Network (Section 3.2). Furthermore,
we propose a bootstrapping algorithm (Section 3.3) that generates better mapping hypotheses and refines the weight distribution for the learned types over a
repeated number of iterations. In Section 4 we report about the experimental
results. In Section 5 we provide an overview of related work and, finally, we
conclude in Section 6 with scopes of possible extension.

3 Methodology

Our approach consists of three main phases. We first derive a distribution of
weights over the possible domains and ranges of a given set of matching hypotheses for those triples that share the same property (Section 3.1). Next, we
formalize the task of choosing a set of mappings from a set of candidates using
Markov Logic Networks (Section 3.2). Finally, we use the previously explained
components within a bootstrapping architecture in order to iteratively improve
the final outcome (Section 3.3).
Our algorithm requires a set of matching hypotheses, also referred to as map-
pings, as input. In the rest of the paper, we use the notation n:x  d:y to refer
?

?

?
to such mappings, where n:x refers to a term that occurs within a Nell triple,
and d:y to a DBpedia instance. x and y can refer to the s(ubject), p(redicate)
and o(bject) of an arbitrary triple. We require mappings to be annotated with
weights, which quantify the likelihood of an OIE term referring to a DBpedia
instance. Intuitively, the higher the weight, the more likely it is that the mapping is true. An example of three different candidate mappings for the Nell
term n:hemingway are the following ones.

+1.34 : n:hemingway  d:Ernest Hemingway
2.22 : n:hemingway  d:Hemingway, South Carolina
2.58 : n:hemingway  d:Hemingway (comics)

The framework we propose in this paper is independent of the specific method
of generating the initial mappings. However, the method should generate meaningful weights that can be interpreted in a probabilistic context. An example for
such a method is presented in our previous work [9] where, we extract the top-k
most frequent senses of OIE terms from the Wikipedia corpus.

3.1 Probabilistic Type Generation

Let us in the following consider Nell triples of the format n:p(n:s, n:o), namely
consisting of a n:s(ubject) and n:o(bject) that share the same property n:p. For
each triple we create the matching hypotheses both for n:s and n:o by applying
an arbitrary matching method that generates mappings annotated with weights.
We refer to this set of mappings as H. We next select a subset of H that consists
of only the best (top-1 ) candidate for each Nell term. The resulting set of
matching hypotheses contains two mappings for each triple, namely n:s  d:s
and n:o  d:o. In the rest of the paper, we denote this set of functional mapping
hypotheses as M, which is essentially a subset of H.

In the next step, the types of d:s and d:o are used as markers for the domain
and range restrictions of n:p. We distinguish between the direct and indirect type
of an instance. Class C is a direct type of instance a, denoted by C(a), if there
exists no sub class D of C, denoted by D  C, such that D(a) exists. We count
the direct type of each mapped DBpedia instance in M. Finally, we obtain a
distribution over the direct type counts for the possible concepts, both for the
domain and range of n:p. Figure 1 depicts a snippet of the concept hierarchy for
the range of the property bookwriter, where the nodes represent the concepts
and the numbers (in non-bold) denote their direct type counts. The sum of the
counts at a particular level do not add up to their parent nodes count, since we
are only counting the direct types of each instance.

Key to our method is the observation that an appropriate weight distribution helps us establish whether a certain candidate mapping is correct or not,
according to the type of d:s or d:o. Considering the most frequent class as a
hard domain/range restriction could potentially perform well, but this would
fail to consider other instances writing books, e.g. philosophers, researchers or
even athletes. On the other extreme, it seems rational to also count the indirect

A. Dutta, C. Meilicke, and S.P. Ponzetto

Place
(0, 0.002)

Agent
(0, 0.159)

Organisation
(0, 0.159)

Person
(147, 0.318)

Athlete
(0, 0.320)

Philosopher
(45, 0.331)

Artist
(66, 0.525)

City
(8, 0.009)

ComicsCreator
(5, 0.526)

MusicalArtist
(6, 0.527)

Writer
(1311, 0.896)

Fig. 1. Counting and weighing the range types of the bookwriter property. Each
concept is accompanied by the counts of the direct types and the normalized Sd score
for  = 0.5 (shown in bold).

types or to propagate the count for a direct type recursively up to the parent
nodes. However, this would result in a type restriction that takes only top-level
concepts into account and completely disregards the finer differences expressed
in the lower levels of the hierarchy. For example, a writer is more likely to write
a book compared to an athlete. Accordingly, we opt for a hierarchical scaling
of weights along the levels, such that the most likely class in the hierarchy is
determined by the instance distribution of both its children and parent.

Hence, we propose a simple formulation to compute an appropriate score for

each concept n. First, we introduce the up-score, Su which is defined as

Su(n) = So(n) + 
?

?

?
cchild(n)

Su(c)

where child(n) denotes the children of n, So(n) refers to the direct type count
and  is a constant, which works as a propagation factor with   [0, 1]. The
computation of this score starts from the leaf nodes, which are initialized with
their direct count So(n). Su is defined recursively and, accordingly, the Su score
for n is computed based on the Su score for the children of n. Furthermore, we
also define a down-score Sd as

Sd(n) =
?

?

?
Sd(parent(n)) + (1  )Su(n) ; n = top-concept
Su(n)

; n = top-concept

where parent(n) denotes the parent node of n. We refer to the concept hierarchy
annotated with the Sd scores as the so-called -tree in the rest of the paper.
?

?

?
[40, 40, 67.5]

[0, 47.5, 47.5]

[30, 55, 75]

[25, 25, 87.5]

[20, 20, 85]

[5, 5, 77.5]

Fig. 2. Propagating direct counts in the alpha tree. Shown scores are [So, Su, Sd]

We present in Figure 2 an example illustrating a simple hierarchy consisting
of six concepts.The relevant scores for  = 0.5 are shown adjacent to the nodes
as [So, Su, Sd]. This example illustrates that the sibling classes D, E and F ,
eventually, have the highest Sd scores, while the order among them, as defined
by So, is still preserved in the order defined by Sd. As a final step, the down-scores
are normalized by dividing them by the sum of the direct counts So for each node.
With respect to Figure 2, the sum of So is 40 + 30 + 25 + 20 + 5 = 120 and so the
normalized Sd for node D, say, is estimated as a probability of 87.5/120 = 0.73.
Obviously, the choice of the constant  is critical in achieving the desired
result. Setting  = 0, neutralizes the effect of child nodes on parent nodes.
In this case we have Sd(n) = So(n), which means that the type hierarchy is
completely ignored. On the other extreme, setting  = 1 propagates the scores
to the full degree, but always creates the same scores for all concepts in the
same branch. With respect to the example shown in Figure 1, we would learn
that all concepts in the Agent branch have the same weight, while there are
no differences between the concepts Organisation and Writer. In Section 4 we
discuss the choice of the optimal  and report about experimental results related
to different  values.

3.2 Modeling with Markov Logic Network

Markov Logic Networks (MLN) [23] are a framework for combining probability theory and first-order logic. Probabilities allow to quantify the uncertainty
associated with complex processes and tasks, while first-order logic helps to capture the logical aspects of the problems. Formally, an MLN is a set of weighted
first-order logic formulae. Under a set of constants, it instantiates into a ground
Markov network where every node is a binary random variable and called a
ground atom. In our task, we use three atoms to build the formulae of the MLN.
We use map for mapping Nell terms to DBpedia instances, isOfType for specifying the types of the DBpedia instances, and propAsst for representing the
Nell triples. Our set of constants is the set of Nell terms and the set of DBpedia instances that are the potential mapping candidates.

A. Dutta, C. Meilicke, and S.P. Ponzetto

We can have several ( 2#groundings) network states for different boolean
assignments of the ground atoms. Every such state is also called a world. In
those worlds different formulae hold true and if some do not, then the world is
penalized according to the weights attached with the violating formulae. As a
result, that world becomes less likely to hold (note that unweighted formulae
can instead never be violated). According to [23], the probability of a world x
is defined as P (X = x) = 1
i wini(x)) where wi is the weight attached
to the first-order logic formula Fi, ni(x) is the number of true groundings of
Fi in x and Z is the normalizing factor (also called partition function) given
k k(x{k}), where, k(xk) is the feature function (usually binary)
as
defined over the kth clique in the ground network.

xX 

Z exp(
?

?

?
In our task, we employ both hard and soft formulae. The hard formula (one
which cannot be violated) for our model is a restriction on the maximum number
of mappings a particular Nell term can have. This is formally stated as

|map(n:t, d:t)| <= 1

which denotes, for all possible instantiations of the map atom, that every Nell
term n:t can have at most one mapping to a DBpedia instance d:t, i.e. we
force the mapping to be functional. For each mapping candidate in H, we add a
weighted formula of the form

w : map(n:t, d:t)

where w is computed by applying the logit function1 to the original probability
awarded by the method that was used for generating H. This adapts the weights
to the underlying log-linear model of MLN.

To additionally take type information into account, we extend our model
with two soft formulae for each possible combination of Nell property P and
DBpedia type C. The first formula reflects a weighted domain restriction and
the second formula reflects a weighted range restriction.

wd(P, C) : isOfType(C, d:s)  propAsst(P, n:s, n:o)  map(n:s, d:s)
wr(P, C) : isOfType(C, d:o)  propAsst(P, n:s, n:o)  map(n:o, d:o)

Note that P and C are replaced by constant values, while n:s, n:o, d:s, and d:o
are quantified variables. wd(P, C) and wr(P, C) are the weights for type C with
respect to property P computed with the -tree as discussed in Section 3.1. If
the type weight wd(P, C) is high, it makes the mapping n:s  d:s more likely

in case that n:s appears in subject position of P and d:s is of type C.

Based on our model, we compute the MAP state, i.e., the most probable world
which coincides in our scenario with the most probable mapping. As a result we
select a subset M from the set of all mapping hypotheses H. In particular, we
want to select a better subset than just choosing the top-1 candidate for each

1 The logit function is the inverse of the sigmoidal logistic function. For a probability

P , the logit function is defined as log P

1P .
?

?

?
H  set of mapping hypotheses
i  0
while Mi = Mi1 do

Algorithm 1. Bootstrapping algorithm
1: procedure bootstrap
2:
3: M0  top-1(H)
4:
5:
6:
7:
8:
end while
9:
return Mi
10:
11: end procedure

i  i + 1
Ti  alphaT ree(Mi1)
Mi  computeM AP State(H,Ti)

 filtered output

Nell term. Note also that our model can easily be extended by adding more
complex rules. The MAP inference is conducted with the RockIt system [20].
RockIt computes the most probable world by formulating the task as an Integer
Linear Program. The solution of the resulting optimization is the MAP state of
the Markov Logic Network.

3.3 Bootstrapping
So far, we used a subset of H, namely the top-1 candidates, as input for computing the -tree. Obviously, the quality of the chosen set of mappings directly
impacts the quality of the resulting -tree. At the same time, a better -tree,
represented as soft constraints in the MLN, can be expected to result in a better
MAP state. Thus, we explore how to use the mapping that corresponds to the
MAP state as input for constructing the -tree and to use the resulting -tree
as input to recompute the MAP state.

We present our bootstrapping approach in Algorithm 1. The algorithm starts

with the set of matching hypotheses H. In the first iteration, we initialize M0 
H by selecting the top-1 candidates in H. M0 is used as input to create the -tree
T1, which is then used together with H to generate the set of mappings for the
next iteration, namely M1. Next, the algorithm checks if there is any difference
between M1 and M0. If this is the case, the algorithm continues and repeats
the same procedure, this time based on M1. With every iteration, each mapping

going additionally into the hypothesis set, creates a refined -tree, makes the
added mapping more probable and so it stays in. Now, it cannot happen that it
is added and then removed in subsequent iterations since it defies the reason for
which it was added the first place. But, removal and then subsequent addition can
happen. But once added, it stays. This continues as long as there are differences

between Mi and Mi1 and terminates eventually. Finally, the last mapping set
Mi that was generated in the i-th iteration is returned. Note that the functions

alphaTree and computeMAPState refer to the modules described in Section 3.1
and Section 3.2 respectively.

A. Dutta, C. Meilicke, and S.P. Ponzetto

4 Experiments

4.1 Metrics and Datasets

We apply the frequency based method as proposed in [9] to generate the set of
input mappings H for our experiments. In the paper, we reported a micro-average
precision of 82.78% and a recall of 81.31% for the subset defined by selecting the
best candidate from H. By choosing this subset as M0 in Algorithm 1, we are

able to start with a set of input mappings that is already highly precise, thus
ensuring high-quality information as seed for the bootstrapping method. We will

use M0 as baseline and report about the improvements gained by each iteration

of our algorithm.
The performance of our proposed approach is measured in terms of precision
and recall. Let M refer to the set of mappings generated by our algorithm,
and G refer to the set of mappings in the gold standard. Precision is defined as
|MG|/|M| and recall as |MG|/|G|. The F1-measure is the equally weighted
harmonic mean of both values. In particular, we compute these values for Mi
after every ith iteration (including the baseline M0).
We compare the results of our method with the gold standard G, presented
in [9].2 This dataset has been created by randomly choosing twelve Nell prop-
erties. For each of these properties 100 triples have been sampled from the Nell
dataset resulting in a rich set of 1200 triples for which subject and object mappings to DBpedia have been specified by human annotators.

4.2 Learning 

Our method relies on the choice of a proper value for the parameter . In a
first set of experiments, we analyze how to learn an appropriate  score. With
respect to these experiments we report about results of our algorithm related to
the final outcome of its last iteration. Experiments that focus on the impact of
the different iterations are presented in Section 4.3.

Parameter Search: In the following we report about repeatedly performing a
2-fold cross validation on different samples of the whole dataset. For that purpose we restrict the possible values of  to a be multiples of 1/8 in the interval
[0, 1], which allows us to repeat the overall process over a large number of sampling steps ( 100000). At each sampling step, we first randomly pick half of the
properties. This choice defines two datasets consisting of 6 properties (the chosen properties and the residual properties). We call one the training set Dtrain
and the other the testing set, Dtest. For every Dtrain we find the  giving the
maximum averaged F1 score over Dtrain. Then we apply our algorithm with that
 on Dtest and compute the resulting F1. For 35% of the samples we learned
=0.5, for 30% we learned =0.375, and for 18% we learned =0.625. Approximately 85% of all samples yield an  in the interval [0.375, 0.625], signifying that
learning produces a stable outcome. Applying the learned  on Dtest results in

2 The dataset is publicly available at https://madata.bib.uni-mannheim.de/65/
?

?

?
Table 1. Effect of  on the overall performance compared to the baseline



0.0

0.125
0.25
0.375
0.5
0.625
0.75
0.875

1.0

prec (prec)

95.1 (+12.30)
94.8 (+12.04)
94.7 (+11.96)
94.4 (+11.58)
93.1 (+10.34)

92.3 (+9.48)
91.4 (+8.63)
90.3 (+7.53)
87.6 (+4.80)

Baseline

82.78

rec (rec )

76.1 (-5.20)
77.6 (-3.70)
78.5 (-2.80)
79.0 (-2.26)
79.9 (-1.39)
80.2 (-1.08)
80.4 (-0.96)
80.6 (-0.67)
81.0 (-0.35)

81.31

F1 (F1 )

84.1 (+2.26)
84.9 (+3.08)
85.4 (+3.66)
85.6 (+3.84)
85.7 (+3.94)
85.6 (+3.76)
85.3 (+3.46)
85.0 (+3.15)
84.0 (+2.17)

81.8

an increased average F1 score of 85.74% (+3.94%) compared to the baseline
with an average F1 score of 81.8%. Thus, it is possible to learn  based on a
small training set (600 triples) that results in a significant improvement of the
mapping quality.

Parameter Effect: Finally we compute recall, precision and F1 values on the
entire dataset for all values of  in the [0, 1] range with step sizes of 0.125. This
helps us to better understand the impact of  on precision and recall. In Table
1 we report the absolute scores along with the differences () in scores over

the most frequent baseline of [9] (M0 in Algorithm 1), and the output of the

final iteration of the bootstrapped approach. Our results corroborate the findings
from our cross-validation runs in that we achieve the best performance on the full
dataset for  = 0.5, which yields an improvement of +3.94% in terms of F1 with
respect to the baseline. Low values of  increase the precision by up to +12.3%
( = 0.0), thus resulting in an overall precision of 95.1%, with a loss of 5.2%
of recall as trade-off. While low values of  increase precision by aggressively
eliminating many incorrect mappings, increasingly higher values lead to a drop
in precision, indicating an ever increasing number of incorrect mappings being
produced. This illustrates nicely that we can use  to adapt our approach to the
needs of a certain application scenario, where precision or recall might be more
important.

4.3 Algorithm Performance

In Table 2 we report the performance figures of our approach for each of the

properties in the evaluation dataset. As baseline and inital starting point M0
we use the most frequent mapping presented in [9]. The following columns (Mi,
i = 0) report the F1 scores of our proposed approach. For all experiments we

set =0.5. This choice is supported by the results of the previously-described
cross-validation approach as well as by the theoretical considerations presented
above. The results highlight that, thanks to our iterative approach, we are able
to beat the baseline, and improve our results in average across iterations.

In our experiments, we found no improvements beyond the third iterations

M3, thus indicating that our method quickly converges after few iterations.

A. Dutta, C. Meilicke, and S.P. Ponzetto

Table 2. F1 scores of the baseline (Mo) and our bootstrapping approach, =0.5

Nell Property

actorstarredinmovie

agentcollaborateswithagent

animalistypeofanimal
athleteledsportsteam

bankbankincountry
citylocatedinstate

bookwriter

companyalsoknownas

personleadsorganization

teamplaysagainstteam
weaponmadeincountry

lakeinstate

Cumulative Gain (%)

M0
81.3
83.7
85.9
87.0
79.6
80.7
82.6
64.1
76.7
81.3
87.0
91.4

-

M1
87.7
76.4
86.0
92.6
86.4
83.2
86.7
64.6
78.1
89.6
87.0
94.4
2.62

M2
94.5
82.0
86.7
93.2
83.4
83.2
86.7
67.1
77.2
90.9

-

94.7
3.89

M3
-
-
-
-

82.8
83.0
89.0

-

77.6

-
-
-

3.94

Performance figures indicate the gradual saturation in the scores after each it-
eration. As expected, with each iteration the output gets more refined until a
plateau in the results is reached, and no further improvement is gained with our
method. In some cases our approach does not modify the original baseline (e.g.
weaponmadeincountry). This is mostly attributed to missing type information
in DBpedia. Note that results get slightly worse for some properties in some
of the iterations. In the following section we analyze the behavior of our algorithm in details by looking at some concrete examples. These examples help to
understand why some cases deviate from the general positive trend.

4.4 Analysis

First, we focus on the object of the Nell triple actorstarredinmovie(al pacino,
scarface). The object term scarface has three possible mapping candidates, which
are shown in the first column of Table 3 together with the case in which no
candidate is chosen (identified by None in the table). In the table, we identify
the candidate chosen in each iteration with a grey cell. None is chosen if the
sum of all weights is less than 0, which means that all mapping candidates
have a probability of less than 50%. For the column baseline, the only relevant
weight corresponds to the most frequently-linked mapping candidate. No typerelated weights are available, and accordingly the wrong entity Scarface (rapper)
is chosen. In the first iteration, the weights for the types are added to those of
the mapping hypothesis. These type weights are obtained by applying the -
tree computation to the baseline. With respect to our example, this results in
rejecting all of the candidates, because each has a probability of less than 50%.
Specifically, we observe that the weight attached to the range type Film is not yet
high enough to increase the overall score for the two movies up to a score greater
than 0. The second iteration uses the type weights computed on the refined -
tree, which is created on the basis of the outcome from the previous iteration.
The weights attached to Film have now increased significantly, and consequently
one of the two movies is chosen. In this case, the algorithm chooses the right
?

?

?
Table 3. Weight refinements across iterations for the object of the triple
actorstarredinmovie(al pacino,
the triple
bookwriter(death of a salesman, arthur miller ). Grey cells refer to the mappings generated at each iteration.

scarface) and for

the subject of

Candidate

Type

Scarface (rapper)
Scarface (1983)
Scarface (1932)

None

Salesman

Salesman (1951)

MusicalArtist

Film
Film
[-]

Play
Film

Baseline
0.06
0.58
2.22
0.0
1.59
2.50

1st Iteration

0.06  3.04=2.98
0.58 + 0.36=0.22
2.22 + 0.36=1.86

2nd Iteration

0.06  13.81=13.75
0.58 + 3.37=2.79
2.22 + 3.37=1.15

0.0 + 0.0=0.0

0.0 + 0.0=0.0

1.59 + 0.08=1.67
1.59 + 0.83=2.42
2.50  0.05=2.55 2.50 + 0.57=1.93

candidate. However, this example also reveals the limits of our approach, namely
the fact that our method solely relies on type-level information. For this reason,
the final choice between the movie from 1983 and the movie from 1932 is only
based the fact that the movie from 1983 has a higher mapping weight, namely it
is more often referred to by the surface form Scarface. That is, all things being
equal (i.e., given the same type-level information), our approach will still choose
the most popular entity, which might be a wrong choice.

The second example is the mapping of the subject term in bookwriter(death
of a salesman, arthur miller ). In this example, the candidate chosen by the
baseline is also that chosen in each subsequent iterations. Contrary to the first
example, the type of the chosen candidate is not the highest scoring type according to the -tree, namely Book. While for the second iteration the weight
for Book is +1.62, the weight for Play is +0.83, based on the fact that Play is
a sibling of Book. Thus, its weight is not only supported by all matched plays,
but also indirectly by all matched books. This example illustrates the benefits
of the -tree and the differences of our approach compared to an approach that
simply uses the majority type as a hard restriction.

5 Related Work

Information Extraction: Key contributions in Information Extraction from
the past years have concentrated on minimizing the amount of human supervision required in the knowledge harvesting process. To this end, much work
has explored unsupervised bootstrapping for a variety of tasks, including the acquisition of binary relations [4], facts [11] and instances [21]. Open Information
Extraction further focused on approaches that do not need any manually-labeled
data [12] however, the output of these systems still needs to be disambiguated
to entities and relations from a knowledge base. Recent work has extensively
explored the usage of distant supervision for IE, namely by harvesting sentences
containing concepts whose relation is known, and using them as training data
for supervised extractors [27]. High-quality data can then be ensured by means
of heuristics based on syntactic constraints [27] or encyclopedic content [14].

A. Dutta, C. Meilicke, and S.P. Ponzetto

Matching Candidates: For over quite some time, researchers have made considerable efforts in solving the tasks of Entity Linking (EL) [15] and Word
Sense Disambiguation (WSD) [17]. Seminal work in EL includes contributions by
Bunescu and Pa sca [5] and Cucerzan [7], who focused on the usage of Wikipedia
categories and global contexts, respectively. The Silk framework [26] discovers
missing links between entities across linked data sources by employing similarity
metrics between pairs of instances. Dredze et al. [8] achieved remarkable results
using supervised approaches, in which they were able to link entities with missing knowledge base entries. Similarly for WSD, supervised systems have been
shown to achieve the highest performance, although questions remain on whether
these approaches perform well when applied to domain-specific data [1]. Besides,
recent work indicates that knowledge-based methods can perform equally well
when fed with high-quality and wide-coverage knowledge [22,18]. In contrast to
all these approaches, our method employs the most frequent sense of a term from
Wikipedia. This consists of a simple, yet high-performing baseline that provides
us with high-quality seeds for our bootstrapping approach.

Weighing Domain and Range: The task of learning domain and range weights
closely matches with the ontology learning task. In this regard, association rule
mining techniques have been employed to learn the axioms of an ontology by
V olker er. al. [25]. Our way to generate weighted domain and range restrictions
can be considered as a special case of ontology learning where we also assign a
probabilistic rank to each of the concepts. However, our algorithm computes the
weighted domain and range restrictions not as an end in itself, but as a means
for improving the mapping quality between terms and instances.

Reasoning and Optimization Techniques: Niepert et.al. [19] focused in
previous work on probabilistic alignment of ontologies: in our work, we focus
instead on refining instance mappings by exploiting an ontological backbone.
Recently, Gal arraga et. al. [13] tackled the task of integrating knowledge bases
(KB) by aligning instances across different KBs. Their approach involves the
combination of multiple KBs into one, and learning logical rules using rule mining
techniques. We have a minor overlap with their work in aligning instances across
KBs. Our work of finding inconsistencies in a KB is more closely related to that of
Jian et al. [16], who also use Markov Logic Networks to refine a knowledge base.
Jian et al. also rely on data from Nell. However, in their work they manually
assign weights to first order logic rules, whereas we use only a single parameter
, which can be learned in an automated way. Wang et. al
[28] have employed
(ProbKB) MLN for the task of automated KB creation through deduction and
using parallel Gibbs sampling, which sets it different from our task of refinement.

6 Conclusions and Future Work

We presented a probabilistic approach to link terms from an OIE system to instances of a semantic resource in order to unambiguously determine the meaning
of terms occurring within triples generated by an OIE system. In particular, we
?

?

?
introduced a way to learn and assign weights to the possible domain and range
types, defined in the terminology of the semantic resource, of a property from
the OIE system. We formalized the task as a Markov Logic Network and solve
the resulting optimization problem as a MAP inference task. As a result, we
are able to achieve highly refined mappings in terms of both precision and re-
call. Moreover, we have shown how to extend this approach in an iterative way
to improve the results across iterations. Our method does not use any specific
parameter settings apart from the propagation factor , which is automatically
learned from our data. Based on our iterative approach, we are able to increase
the F1-measure from 81.8% to 85.74%.

In the future, we plan to extend our method to exploit more than just the type
hierarchy, where alternatives are typed with the same or a similarly weighted
concept. In particular, we plan to jointly reason combinations of candidates for
object and subject terms of a given triple by exploiting their attached properties.
For instance, two instances in a bookwriter relation cannot have a negative
difference between the books publishing year and the writers birth year. To
this end, we will explore kernel density estimation techniques to learn the typical
relation between the relevant data values. Note that this component can easily
be integrated in the iterative framework, working as a booster for the type
acquisition component and vice versa.

We will apply the overall framework on other OIE datasets, ReVerb [12] in
particular. While Nell has a fixed number of cleaned properties, this is not
the case for ReVerb. A property like bookwriter might be expressed by terms
like written by, has author, or by an inverse term like is author of. The
challenging task herein is to cluster corresponding property terms and apply the
proposed mechanism. Finally, we plan to run more comprehensive evaluations
with the aim to re-build a significant part of, e.g., DBpedia using the triple sets
generated by different OIE systems. Ultimately, our vision is to extend knowledge
bases like DBpedia with a rich set of highly precise novel RDF triples that fully
exploit the potential of OIE systems. Our current results are promising in that
they indicate that this could lead to a highly beneficial solution to synergistically
exploit IE and OIE systems together.

Acknowledgments. We thank Mathias Niepert for his contributions and valuable feedback on this work. The authors gratefully acknowledge the support of a
Google Faculty Research Award (see http://dws.informatik.uni-mannheim.
de/en/projects/current-projects/ for details).
