 

Identifying Diachronic Topic-Based Research 
Communities by Clustering Shared Research 

Trajectories  

Francesco Osborne, Giuseppe Scavo, and Enrico Motta 

Knowledge Media Institute, The Open University, MK7 6AA, Milton Keynes, UK     

{f.osborne,g.scavo,e.motta}@open.ac.uk 

Abstract. Communities of academic authors are usually identified by means of 
standard community detection algorithms, which exploit static relations, such 
as  co-authorship  or  citation  networks.  In  contrast  with  these  approaches,  here 
we focus on diachronic topic-based communities i.e., communities of people 
who  appear  to  work  on  semantically  related  topics  at  the  same  time.  These 
communities are interesting because their analysis allows us to make sense of 
the  dynamics  of  the  research  world  e.g.,  migration  of  researchers  from  one 
topic  to  another,  new  communities  being  spawn  by  older  ones,  communities 
splitting,  merging,  ceasing  to  exist,  etc.  To  this  purpose,  we  are  interested  in 
developing  clustering  methods  that  are  able  to  handle  correctly  the  dynamic 
aspects  of  topic-based  community  formation,  prioritizing  the  relationship 
between researchers who appear to follow the same research trajectories. We 
thus  present  a  novel  approach  called  Temporal  Semantic  Topic-Based 
Clustering  (TST),  which  exploits  a  novel  metric  for  clustering  researchers 
according  to  their  research  trajectories,  defined  as  distributions  of  semantic 
topics over time. The approach has been evaluated through an empirical study 
involving 25 experts from the Semantic Web and Human-Computer Interaction 
areas. The evaluation shows that TST exhibits a performance comparable to the 
one achieved by human experts. 

Keywords:  #eswc2014Osborne,  Community  Detection,  Scholarly  Data, 
Scholarly  Ontologies,  Semantic  Technologies,  Clustering,  Similarity  Metrics, 
Fuzzy C-Means. 

Introduction 

Communities of academic authors are usually identified by using standard community 
detection  algorithms,  which  typically  exploit  co-authorship  or  citation  graphs  [1]. 
However, an interesting type of community, which has received much less attention 
in  the  literature  [2],  is  formed  by  the  set  of  researchers  who,  at  a  given  time,  are 
working  on  the  same  topic.  Obviously,  this  type  of  topic-based  community  has  a 
degree  of  overlap  with  co-authorship  and  citation  communities;  nonetheless  it 
provides  a  distinct  way  of  identifying  groups  of  related  researchers.  Co-authorship 
communities can certainly be seen as examples of topic-based communities, however 
one does not need to co-author with another researcher in order to be part of the same 

V. Presutti et al. (Eds.): ESWC 2014, LNCS 8465, pp. 114129, 2014. 
 Springer International Publishing Switzerland 2014 
?

?

?
topic-based  community.  Hence,  co-authorship  networks  only  provide  an  incomplete 
view  of  a  topic-based  community.  In  addition  co-authorship  relations  can  span 
different  topics,  hence  providing  a  noisy  mechanism  to  identify  a  topic-based 
community. An analogous argument applies to the use of citation networks to identify 
topic-based  communities:  on  the  one  hand  citations  may  cut  across  different  topics 
and  on  the  other  hand  there  is  no  guarantee  that  people  working  on  the  same  topic 
actually cite each other. Hence, citation networks also define poor approximations of 
topic-based communities. 

Topic-based communities are interesting because their analysis allows us to  make 
sense of the dynamics of the research world e.g., migration of researchers from one 
topic  to  another,  new  communities  being  spawn  by  older  ones,  communities  (and 
therefore associated topics) splitting, merging, ceasing to exist, etc. More precisely, the 
formal identification and characterization over time of topic-based communities allows 
us to give an extensional computational treatment of a topic (or set of topics), say T, in 
terms of all the researchers and publications related to T at a given time. Thus, we can 
then measure precisely the size of the topic, its scientific impact (in terms of a variety 
of  academic  impact  measures),  its  evolution,  relations  between  topics  in  terms  of 
overlap of researchers, migrations across topics, etc. In the rest of the paper we will use 
the term temporal topic-based community to refer to this type of communities.  

In  this  paper  we  propose  a  novel  approach  to  identifying  temporal  topic-based 
communities, called Temporal Semantic Topic-Based Clustering (TST).  TST exploits a 
novel  metric,  called  ATTS  (Adjusted  Temporal  Topic  Similarity),  which  measures  the 
similarity  between  research  trajectories.  These  are  in  turn  defined  as  distributions  of 
semantically-characterized topics over time i.e., topics structured in terms of semantic 
relationships, such as skos:broaderGeneric or relatedEquivalent [3]. Thus, TST is able 
to detect diachronic groups of authors with similar behavior over a period of time.  

An  important  aspect  of  TST  is  that,  in  contrast  with  methods  which  rely  on  coauthorship  or  citation  networks,  it  does  not  require  a  complete  graph  of  relations 
between community members. Hence, it can also be used in non-academic contexts, 
where such relations are typically not available. In addition, we characterize temporal 
topic-based  communities  as  fuzzy  clusters  and  as  a  result  each  author  is  then 
associated with a set of membership values, which express the degree of work done 
for  different  communities.  Hence,  this  model  naturally  handles  both  the  common 
situation  in  which  an  author  contributes  to  more  than  one  community  and  also  the 
situation in which a community is defined in terms of multiple dynamic topics over 
time e.g., the community of all researchers who worked in Knowledge Acquisition 
during the 90s and then worked primarily on the Semantic Web during the 00s. 

Our  approach  increases  the  granularity  of  the  representation  of  the  research 
environment and makes it possible to discover interesting dynamics. For example, we 
can  highlight  the  behaviour  of  groups  of  researchers  reacting  to  a  mutation  in  the 
scientific  environment,  such  as  the  introduction  of  a  new  technology  (e.g.,  Mobile 
Devices), a new vision (e.g., Semantic Web), or a grant on a particular theme (e.g., 
Smart  Cities).  We  can  also  get  interesting  insights  into  the  DNA  of  specific 
communities.  For  example,  a  topic-centred  analysis  of  Semantic  Web  (SW) 
researchers  over  time  reveals  that  the  authors  with  a  World-Wide-Web  (WWW) 
background, who joined the SW research area in the first years of this century, were 
by and large the ones who progressed the Linked Data topic at the end of the decade. 

F. Osborne, G. Scavo, and E. Motta 

A similar analysis in the Human-Computer Interaction (HCI) area shows that authors 
in  the  HCI  community  who  had  a  background  in  User  Modeling  and  Ubiquitous 
Computing  were  the  ones  at  the  forefront  of  research  on  Mobile  Devices,  once  the 
smartphone became a reality.  

TST is integrated within Rexplore [4], a system that combines statistical analysis, 
semantic  technologies  and  visual  analytics  to  provide  support  for  exploring  and 
making sense of scholarly data. To evaluate our approach we performed an empirical 
study involving 25 experts from the SW and HCI areas, who were asked to aggregate 
a set of  selected topics to generate the  main topic-based communities in their  field. 
The results indicate a  high degree of agreement among the experts, confirming that 
topic-based  communities  are  indeed  objective  entities  that  can  be  recognized  by 
experts. In addition, TST performed at expert level  i.e.,  its results are  statistically 
consistent with those of the experts.  

State of the Art 

Current  approaches  to  community  detection  are  usually  classified  according  to  the 
strategy they  use [1], as either  optimization-based or heuristic  methods. The former 
use either local search [4] or spectral methods [6], whereas the latter exploit domainspecific assumptions to direct the clustering [7]. Unfortunately these methods tend to 
rely on  topological structures, such as the ones defined by  citation or co-authorship 
networks, and as a result they are not applicable to our scenario, where, as explained 
in  the  previous  section,  we  do  not  have  topological  structures  that  completely  and 
correctly define our space. As discussed by Ding et al. [2], it is therefore important to 
develop  novel  approaches  to  community  detection,  which  are  able  to  focus  on  the 
relationship between communities and topics and can correctly model their dynamics 
over time. A first step in this direction is provided by the work of Upham et al. [8], 
who define an algorithm for identifying topic-based communities which, in addition 
to  the  citation  graph,  also  exploits  language-level  similarities  between  papers  to 
identify communities. Hence, they are able to group together authors who work on the 
same  topic  but  are  not  necessarily  related  through  explicit  co-authorship  or  citation 
relationships.  However,  while  this  approach  provides  an  improvement  over  purely 
topological analyses, it seems to us that the focus on publications (rather than authors) 
and  the  reliance  on  language  similarities  provide  too  weak  a  method  to  detect 
temporal topic-based communities. In particular, it is not possible in this approach to 
express  explicitly  which  authors  belong  to  a  particular  community  (or  set  of 
communities) at a particular time.  

Racherla  and  Hu  [9]  identify  topic  communities  by  exploiting  a  topic  similarity 
matrix  and  assigning  a  predefined  research  topic  to  each  document  and  author. 
However, this approach is much too limited, as they assume a rigid 1-1 relationship 
between researchers and topics. In contrast with this work, TST is more flexible and 
can  correctly  handle  both  the  situation  where  a  researcher  belongs  to  multiple 
communities  and  also  that  where  a  community  is  characterised  by  a  distribution  of 
topics over time. 

Semantic  technologies  have  been  shown  to  improve  the  quality  of  clusters  of 
different kinds of entities, such as images [10] and tags [11]. Some approaches rely on 
the detection of latent topics for capturing semantic relationships between keywords,  

 
?

?

?
using methods such as Probabilistic Latent Semantic Indexing (pLSI) [12] or Latent 
Dirichlet  Allocation  [13].  For  example,  the  Author-Conference-Topic  model  (ACT) 
[14]  treats  authors  and  venues  as  probability  distributions  over  topics  extracted  by 
means of an unsupervised learning technique. Mei et al. [15] propose a framework to 
model topics by regularizing a statistical topic model through a harmonic regularizer, 
which  is  based  on  a  graph  structure.  Differently  from  these  methods,  we  exploit  an 
automatically  generated  knowledge  base  [3] 
topics 
semantically and we use this as the basis for associating a diachronic semantic topic 
distribution  with  each  author.  The  knowledge  base  is  extracted  from  publication 
metadata  by  means  of  Klink  [3],  an  algorithm  that  combines  machine-learning 
methods  and  background  knowledge  to  identify  research  topics  and  to  generate 
semantic  relations  between  them.  Adopting  a  similar  perspective,  Ereteo  et  al.  [16] 
proposed  SemTagP,  an  algorithm  which  uses  existing  ontologies 
to  detect 
communities  from  the  directed  typed  graph  formed  by  RDF  descriptions  of  social 
networks  and  folksonomies.  However,  their  approach  is  based  on  label  propagation 
and, in contrast with TST, does not take in account the temporal dimension, which is 
important for gaining an understanding of community evolution over time and is also 
being investigated in the emergent field of temporal networks [17]. 

to  characterize  research 

TST relies on the Fuzzy C-Means [18] algorithm, which is a popular unsupervised 
clustering  algorithm  that  has  been  applied  successfully  to  a  number  of  real  life 
problems.  Clustering  techniques  (e.g.,  modularity-based  clustering  [19]  or  the  k-
means  algorithm  [20])  have  also  been  used  by  other  authors  to  detect  research 
communities. However, these approaches exploit the similarity between topic vectors 
associated to publications and, as a result, exhibit limitations when compared to our 
method.  In  particular,  their  topic  vectors  lack  a  semantic  characterization  and,  in 
addition, by focusing on publications rather than authors, they fail to take into account 
the  diachronic  dimension.  As  we  will  show  in  Section  4,  in  contrast  with  the 
aforementioned approaches, the use of semantic topics and the adoption a diachronic 
approach yields a dramatic increase in the quality of the detected communities.  

Detecting Temporal Topic-Based Communities  

We  will  now  discuss  the  TST  approach  to  identifying  clusters  of  researchers  who 
share  common  research  trajectories    i.e.,  researchers  who  appear  to  work  on  the 
same  topics  at  the  same  time.  We  refer  to  these  clusters  as  temporal  topic-based 
communities (TTCs). 

The TST approach for automatically computing TTCs in a given research area, say 

R, follows three steps: 

1.  Semantic  topic  enrichment,  during  which  the  topic  distributions  associated 
with  each  author  are  semantically  enhanced  by  taking  into  account  the 
semantic relationships between research topics. 

2.  Topic vector weighing, during which each component of a topic vector, say T, 

is given a bonus proportional to the degree of similarity between T and R.  

3.  Temporal topic-based clustering, during  which the authors are clustered by 
means of a Fuzzy C-Means algorithm, using the aforementioned ATTS metric.  

These steps are discussed in the following sub-sections.  

F. Osborne, G. Scavo, and E. Motta 

Semantic Topic Enrichment 

3.1 
The authors to be clustered are characterized as a collection of topic vectors, one for 
each year over the examined timeframe,  where each value represents the number of 
publications in a topic during a certain year.  

A  naive  approach  here  would  be  to  use  as  topics  the  keywords  associated  to  the 
publications. However this method may yield poor results, since, as discussed in [3], 
the keywords associated to academic publications lack structure and are often noisy. 
Analogously,  the  keywords  extracted  by  natural  language  techniques  may  also  be 
noisy and may include terms that do not define research areas.  

To address this issue  we use the Klink algorithm [3],  which is able i) to identify 
keywords that refer to a research area and distinguish them from those which do not 
and ii) to detect three types of semantic relationships. Specifically, Klink can detect: 
skos:broaderGeneric  (topic  T1  is  a  sub-topic  of  topic  T2),  relatedEquivalent  (two 
topics are alternative names for the same research area) and contributesTo (research 
in topic  T1 is an important contribution to research in topic  T2,  however  T1 is  not a 
sub-topic  of  T2).  Hence,  the  output  of  an  application  of  Klink  to  a  corpus  of 
publications tagged  with  keywords is a knowledge base comprising  semantic topics 
structured  according  to  three  relations  and  linked  to  the  relevant  publications  (and 
therefore with the relevant authors and organizations). 

Taking advantage of this knowledge base, we label with topic T1 any publication 
tagged  with topic  T2, if  T2 is  a sub-topic of  T1  or it is  relatedEquivalent to  T2. This 
simple step can yield a dramatic increase in the quality and quantity of data about a 
certain  topic.  For  example,  as  a  result  of  applying  Klink  to  a  corpus  of  about  15 
million publications in Computer Science, we were able to identify 18 sub-topics of 
Semantic  Web  (e.g.,  Linked  Data,  Semantic  Wiki  and  OWL)  and  11 
contributesTo relationships (e.g., Description Logic), thus increasing the number of 
publications  in  the  Semantic  Web  from  11998  to  20751. In  the  same  way  we  were 
able  to  detect  22  sub-topics  of  HCI  (e.g.,  Affective  Computing  and  User 
Interface) and 7 contributesTo relationships (e.g., Task Analysis), thus increasing 
the number of publications tagged as HCI from 9850 to 93583. 

We then build the topic distribution for each author in year t as a vector in which 
each topic is associated with the number of publications in the same year. Finally, for 
each couple of topics, <T1,T2>, sharing a contributesTo(T1,T2) relationship, we assign 
to T2 a fraction of the publications in T1 according to the formula: 

(cid:1829)(cid:1846)(cid:4666)(cid:1846)(cid:4667)(cid:3404)(cid:3533)(cid:1842)(cid:3435) (cid:1846)(cid:3627)(cid:1855)(cid:1872)(cid:4666)(cid:1861),(cid:1846)(cid:4667)(cid:3439)(cid:3101)
 

(cid:3041)
(cid:3036)(cid:2880)(cid:2869)

where (cid:1855)(cid:1872)(cid:4666)(cid:1861),(cid:1846)(cid:4667) indicates the set of topics associated with the i-th publication that is in a 
contributesTo relationship with T.  (cid:1842)(cid:3435)(cid:1846)(cid:3627)(cid:1855)(cid:1872)(cid:4666)(cid:1861),(cid:1846)(cid:4667)(cid:3439) is the probability for a paper with such 
with  T.  The  exponent (cid:2030)  serves  to  modulate  the  contributesTo  relationship  and  was 

a  set  of  topics  to  be  also  explicitly  associated  with  topic  T  (or  with  a  topic  having  a 
broaderGeneric or relatedEquivalent relationship with T) at the time of publication of 
the i-th paper. The summation is carried out over the number n of publications that are 
not already associated with T but have at least one topic in a contributesTo relationship 

empirically  set  to  0.5.  The  outputs  are  semantic  topic  vectors  that  include  only 
semantically  characterized  research  areas,  whose  associated  values  are  weighed 
according to the semantic relationships between research areas. 

 
?

?

?
3.2  Topic Vector Weighing 

In most cases it is useful to detect the communities within a certain main topic (e.g., 
Semantic Web), to allow a user to make sense of elements of the research dynamics 
within the topic. For this reason we take as input only the authors with a significant 
amount of publications in the main topic. For example in the evaluation we will take 
in  consideration  only  the  authors  who  have  published  at  least  10  papers  in  the 
Semantic  Web  area  in  the  2005-2010  interval.  Moreover,  to  highlight  the 
communities strongly related to the main topic, we weigh each topic according to its 
relationship  with  the  main  topic.  Given  a  semantic  topic  T,  the  weight  W(T)  is 
calculated as follows: 

(cid:1849)(cid:4666)(cid:1846)(cid:4667)(cid:3404)1(cid:3397)(cid:1863)(cid:1829)(cid:4666)(cid:1846)(cid:4667)
(cid:1845)(cid:4666)(cid:1846)(cid:4667) 

where  C(T)  is  the  number  of  co-occurrences  of  topic  T  with  the  main  topic  in  
the selected time interval; S(T) is the number of total occurrences of the topic T in the 
selected  time  interval,  and  k  is  an  arbitrary  constant  (empirically  set  to  2  in  the 
evaluation) that can be tuned to amplify the effect of the weight on the system. Here it 
is important to emphasise that, as a result of the semantic topic enrichment carried out 
in the previous step, the co-occurrences used in this formula are actually applied on 
semantic topics rather than defining standard keyword co-occurrences. 

This step can be skipped if the main topic is not defined.  

3.3  Temporal Topic-Based Clustering 

In the final step of TST, a Fuzzy C-means (FCM) algorithm is applied to the weighted 
topic  vectors  to  compute  a  set  of  fuzzy  clusters  of  authors  associated  with  their 
distribution  of  topics  over  the  years.    Here,  we  have  adopted  a  fuzzy  clustering 
technique  since  most  researchers  tend  to  work  in  more  than  one  community,  and  a 
clustering algorithm that forced them to be members of only one would be unfeasible. 
Moreover,  associating  authors  with  a  degree  of  memberships  to  each  community 
allows for a more granular characterization of their research interests.  

FCM is one of the main unsupervised clustering algorithms and has been applied 
successfully  to  a  number  of  scenarios.  It  classifies  entities  by  minimizing  the 
following objective function: 

(cid:1836)(cid:3040)(cid:3404)(cid:3533)(cid:3533)(cid:1873)(cid:3036)(cid:3037)(cid:3040)(cid:3630)(cid:1876)(cid:3036)(cid:3398)(cid:1855)(cid:3037)(cid:3630)(cid:2870)  ,   1(cid:3409)(cid:1865)(cid:3407)
(cid:3015)
(cid:3036)(cid:2880)(cid:2869)

(cid:3004)
(cid:3037)(cid:2880)(cid:2869)

 

where N is the number of entities (in this case authors), C the number of the chosen 
centroids, uij is the degree of membership of xi in the cluster j, m is a number  1, xi is 
the  ith  entity,  cj  is  the  centroid  of  the  cluster,  and  ||*||  is  a  norm  expressing  the 
similarity between any entity and the centroid. 

We will not elaborate here on the details of the algorithm since it is well known 

see [18] for an in-depth description. 

In our case, we need a norm that takes into account the topic vectors over the years. 
To this end we have introduced a novel similarity measure called adjusted temporal 
topic similarity (ATTS).  

F. Osborne, G. Scavo, and E. Motta 

interval t1-t2 as:  

We  first  define  the  topic  similarity  (TS)  between  two  authors  A  and  B  in  a  time 

where (cid:1853)(cid:3548)(cid:3036) and (cid:1854)(cid:3552)(cid:3036) are the topic vectors of the two authors in the i-th year and cos(s,t) is 

TS(A,B,t1, t2) = cos(

(cid:3047)(cid:3118)(cid:3036)(cid:2880)(cid:3047)(cid:3117)

(cid:3047)(cid:3118)(cid:3036)(cid:2880)(cid:3047)(cid:3117)

(cid:1853)(cid:3548)(cid:3036)

,

) 

(cid:1854)(cid:3552)(cid:3036)

the cosine similarity. 

This metric however does not take into account possible common shifts of interests 
of  the  authors.  In  fact,  if  author  A  worked  on  topic  T1  and  then  shifted  to  topic  T2, 
he/she will be considered similar to author B who was originally in T2 and then moved 
to T1. To avoid this problem, we need a metric that pays attention to the period of time 
in which an author addresses a specific topic, rewarding common trajectories. Hence, 
in order to strengthen the importance of the time factor we compute TS recursively on 
increasingly  shorter  time  intervals  and  then  average  the  results.  More  formally,  we 
define  the  temporal  topic  similarity  TTS  between  author  A  and  author  B  in  the 
interval t1-t2 as: 

 (cid:1846)(cid:1846)(cid:1845)(cid:4666)(cid:1827),(cid:1828),(cid:1872)(cid:2869),(cid:1872)(cid:2870)(cid:4667)(cid:3404)  (cid:3428)(cid:3436)
TS(cid:4672)(cid:3002),(cid:3003),(cid:3047)(cid:3117)(cid:2878)(cid:4690)(cid:3285)(cid:4666)(cid:3295)(cid:3118)(cid:3127)(cid:3295)(cid:3117)(cid:4667)
(cid:3118)(cid:3284)(cid:3127)(cid:3117)
(cid:3288)(cid:3284)(cid:3128)(cid:3116)
(cid:3285)(cid:3128)(cid:3116)
(cid:3118)(cid:3284)
(cid:4666)(cid:3040)(cid:2878)(cid:2869)(cid:4667)
(cid:1865)(cid:3404)(cid:1729)(cid:1864)(cid:1867)(cid:1859)(cid:2870)(cid:4666)(cid:1872)(cid:2870)(cid:3398)(cid:1872)(cid:2869)(cid:4667)(cid:1730) 

(cid:4691),(cid:3047)(cid:3117)(cid:2878)(cid:4692)(cid:4666)(cid:3285)(cid:3126)(cid:3117)(cid:4667)(cid:4666)(cid:3295)(cid:3118)(cid:3127)(cid:3295)(cid:3117)(cid:4667)
(cid:3118)(cid:3284)

(cid:4693)(cid:4673)

(cid:3440)/(cid:2870)(cid:3284)(cid:3432)

  , 

 
The  temporal  topic  similarity  covers  well  the  case  in  which  both  authors  are 
present in the  same time interval. However an author  may start publishing after the 
beginning of the interval or suspend his/her career before the end of it. These cases 
may be accounted for by introducing a penalty for authors who do not share the entire 
timeframe.  We  quantified  the  penalty  P  as  the  average  TS  of  n  authors  randomly 
extracted from the input (n=500 in the prototype).  

Finally, we define the adjusted temporal topic similarity, ATTS, as: 

(cid:1827)(cid:1846)(cid:1846)(cid:1845)(cid:4666)(cid:1827),(cid:1828),(cid:1872)(cid:2869),(cid:1872)(cid:2870) (cid:4667)(cid:3404)(cid:1846)(cid:1846)(cid:1845)(cid:4666)(cid:1827),(cid:1828),(cid:1872)(cid:2869),(cid:1872)(cid:2870) (cid:4667)(cid:1837)(cid:3046)(cid:3397)(cid:1842)(cid:1837)(cid:3041)(cid:3046)   , 

(cid:1837)(cid:3046)(cid:3404) (cid:3010)(cid:3294)(cid:3330)(cid:3010)(cid:3294)(cid:3330)(cid:2878)(cid:3010)(cid:3289)(cid:3294)(cid:3330)  , (cid:1837)(cid:3041)(cid:3046)(cid:3404) (cid:3010)(cid:3289)(cid:3294)(cid:3330)(cid:3010)(cid:3294)(cid:3330)(cid:2878)(cid:3010)(cid:3289)(cid:3294)(cid:3330)  

number  of  years,  and (cid:2011)>1  a  parameter  for  weighing  their  relationship  ((cid:2011)=  2  in  the 
present evaluation). If (cid:2011) is high, an author active in a good portion of the interval is 

where Is is the number of years in which both authors were active, Ins is the remaining 

barely  penalized,  thus  allowing  for  latecomers  to  be  assigned  to  the  cluster  if  their 
topic  trajectory  is  similar  enough  to  the  community  centroid.  Since  ATTS  is  a 
similarity measure that varies in the interval [0,1], while a FCM needs a distance in 
the interval [0, ], we use as norm the inverse of the ATTS minus 1. 

The output of FCM depends on the initial guess on the number of clusters and the 
candidate  centroids.  In  this  scenario  there  is  no  absolutely  correct  initial  number  
of  centroids,  since  even  different  user  experts  will  suggest  a  different  number  of 
communities.  However,  we  suggest  two  techniques  to  select  the  initial  number  of 
centroids. The  first,  and  most  conservative one, is  to compute  the  set of  clusters  for 
different numbers of centroids and for different random initializations and then select 
the one with the highest compactness (see Section 5). In this paper we used the PCAES 

[21] (Partition Coefficient and Exponential Separation) as measure for compactness. 

This is a cautious approach that will produce very compact communities, but may also 

 
?

?

?
miss some of the minor ones. The second approach is the subtractive clustering method 
[22]. This technique estimates the initial centroids by assigning a potential to each 
individual in the dataset, so that an individual with many neighbours will have a high 
potential.  While  this  approach  may  build  less  compact  communities,  it  nevertheless 
appears to produce results that are very similar to the ones generated by the domain 
experts (see Section 4). 

FCM returns a list of cluster centroids and a partition matrix where each element is 

associated with its degree of membership to each cluster.  

The centroids of the clusters detected by the FCM algorithm are characterized by 
the topic vectors of the communities for each year in the interval, which can be used 
to study the community evolution. In fact, by studying the change in the distribution 
of topics in  subsequent  years it is possible  to detect trends (e.g., a  topic is  growing 
considerably and thus may continue to grow in the future) and shifts (e.g., a marginal 
topic  is  becoming  dominant,  such  as  Augmented  Reality  becoming  a  more 
important  component  of  the  Virtual  Reality  community  after  the  introduction  of 
mobile devices). This possibility opens up very interesting scenarios and it is one of 
the main assets of TST. 

By  summing  the  vectors  over  the  years  and  selecting  the  topics  with  the  highest 
values it is possible to label communities according to their  most significant topics. 
For  example,  a  key  community,  which  emerges  when  analysing  the  Semantic  Web 
area,  has  the  highest  values  associated  to  the  topics  Artificial  Intelligence, 
Knowledge Base and Ontology, and therefore we can refer to it as the AI, KB, 
Ontology community. 

Evaluation 

We conducted an empirical study with 25 human experts, 13 from the Semantic Web 
and  12  from  the  Human  Computer  Interaction  field.  These  were  chosen  among 
experienced researchers in the two fields. Specifically,  we  wanted to verify i) if the 
experts  could  agree  on  the  main  topic-based  communities  in  a  field  -  i.e.,  if  the 
concept of topic-basic community is clear and well defined enough for human users, 
and  ii)  if  the  proposed  method  could  perform  similarly  to  the  experts  and  thus  be 
considered reliable in detecting this kind of communities.  

For setting up the study we first built a dataset covering the SW and HCI areas, by 
exploiting  the  Microsoft  Academic  Search  (MAS)  API1,  a  service  that  makes  it 
possible to access metadata about authors,  publications and keywords. We retrieved 
authors and papers labelled with HCI or SW or with their first 50 co-occurring topics 
and  we  then  ran  Klink  on  this  dataset  to  obtain  a  populated  ontology  of  these  two 
research  areas  for  the  semantic  topic  enrichment  phase  (see  Section  3.1).  We  then 
selected as basic topics the 35 semantic topics2 that were most often used as tags for 
SW or HCI papers in the years 2005-2010 as a result, some topics that have grown in 
importance since 2010 may be missing from this sample. We then removed from this 
                                                           
1 http://academic.research.microsoft.com/ 
2 See http://rexplore.kmi.open.ac.uk/data/tce.rtf for a list of the topics 

used in the experiment. 

F. Osborne, G. Scavo, and E. Motta 

set  highly  generic  topics  (e.g.,  Artificial  Intelligence)  to  simplify  the  task  for  the 
experts. In fact, these topics tended to be associated with pretty much every single one 
of the 35 topics used in the experiment and therefore held no discriminatory power. 
Here, it is important to emphasise that keeping such highly generic topics would have 
not affected the algorithm, which would have simply assigned them to more than one 
community  with  different  degrees,  while  of  course  it  would  have  complicated 
significantly the task for the experts.  

We used WebSort3, a card sorting online service, to assist the experts in building 
the clusters. We allowed for each topic to be associated with only one community at a 
time,  since  it  would  have  been  cumbersome  to  ask  experts  to  create  overlapping 
communities  or  communities  characterized  by  potentially  different  topics  for  each 
year, as our algorithm is able to do. We thus modified the output of the algorithm to 
follow  the  same  limitations  by  merging  the  topic  vectors  of  the  different  years  and 
assigning  each  topic  only  with  the  community  with  which  it  had  the  highest  score. 
Hence,  we  gave  the  experts  a  collection  of  basic  topics  related  to  their  field  and 
asked them to aggregate the topics together to shape what they considered to be the 
main communities in their field. For example an expert in HCI could decide to group 
together  topics  such  as  Ubiquitous  Computing,  Mobile  Device  and  Context 
Aware and label them as Mobile Interaction community. 

The  SW  experts  suggested  an  average  of  7.9    2.3  communities,  whereas  HCI 
experts suggested an average of 6.7  1.9. We then examined the degree of agreement 
among experts and with our algorithm. To compute the agreement between two sets 
of  clusters  we  used  the  pairwise  F-Measure,  the  harmonic  means  of  the  pairwise 
precision and recall. 

We tested four algorithms on the same dataset: 1) FCM using cosine similarity on 
regular  keywords  (labelled  F),  2)  FCM  using  cosine  similarity  on  semantic  topics 
(FC),  3)  FCM  using  ATTS  on  semantic  topics  (FT)  and  4)  FCM  using  ATTS  on 
weighted semantic topics (TST).  We selected as input the set of authors with at least 
10 publications about SW/HCI in the 2005-2010 interval. The total amounted to 431 
authors  for  SW,  and  458  authors  for  HCI.  The  initial  centroids  were  estimated  by 
means of the subtractive clustering method [22]. 

Figure 1 and Figure 2 show the average degree of agreement of each expert with all 
the  others.  For  SW,  the  ANOVA  version  of  the  variance  test  over  all  experts 
evidenced  statistically  significant  differences  (visible  also  in  the  graph),  yielding 
p=0.02. Only seven experts exhibited agreement among themselves (p=0.18) and they 
also agreed with the final version of the algorithm, TST (p=0.12). Actually there is a 
fair degree of agreement between the SW experts and our algorithm: the average F-
Measure  is  0.480.04  for  the  former  and  0.440.07  for  the  latter.  For  HCI,  the 
ANOVA  test  on  experts  yielded  p=0.45.  Including  as  a  special  expert  the  final 
version of our algorithm (TST) yielded p=0.14. Since in both cases p >> 0.05, we can 
conclude that there are no statistically significant differences among the experts and 
between experts and the final version of the algorithm.  

The  results  of  the  three  most  basic  versions  of  our  algorithm  are  significantly 
different,  both  from  the  TST  version  and  also  from  the  experts  (in  all  comparisons  

                                                           
3 http://uxpunk.com/websort 

 
?

?

?
ntifying Diachronic Topic-Based Research Communities 

p  <0.0001  with  Friedman
without  semantics  (F)  pe
increasingly better results b
characterization of topics a
able to perform consistently

n  test  for  correlated  samples).  In  particular,  the  vers
rformed  disastrously.  The  FC  and  FT  version  yiel
both in SW and HCI, showing how the use of a seman
and the ATTS metric crucially ensures that our  method
y with the experts. 

sion 
lded 
ntic 
d is 

Fig. 1. Average F-measure bet
topic. The red line represents t

tween each expert/algorithm and all the other experts for the 
the average F-measure of the experts.  

Fig. 2. Average F-measure

e between each expert/algorithm and all other experts for HCI 

 

 

A  careful  look  at  the  c
agreed on the general pictu
disagreed  on  how  to  split
different  perspectives.  For 

crafted  communities  evidences  that  most  experts  actu
ure (the macro-communities) of their field, but sometim
t  some  macro-groups,  creating  sub-groups  according
example  in  SW  the  topics  Ontology  Engineering 

ally 
mes 
g  to 
and 

F. Osborne, G. Scavo, and E. Motta 

Ontology Mapping are aggregated by some experts within the Formal Ontology 
community, while, according to other experts, they should instead be in two different 
communities.  

Table 1 shows the macro-communities on which most experts agree. We composed 
it  by  analysing  the  labels  of  the  experts  and  the  usual  topic  components.  Thus,  for 
example, an area such as Ubiquitous Computing/Mobile Device may either include 
or  not  include  Context  Aware  according  to  different  experts,  but  it  is  usually 
associated  with  the  same  topics  and  yields  similar  labels  to  Mobile  interaction  or 
Mobile  HCI.  SW  enjoys  4  size  macro-communities  on  which  more  than  70%  of 
experts  agree,  while  HCI  has  6  of  them.  Some  macro-communities,  such  as 
Description Logic in SW and Virtual Reality in HCI, are so well defined that they get 
almost  full  agreement.  We  can  say  that  the  skeleton  or  general  frame  of  the 
communities appears to be well defined, whereas the details, such as the position of 
individual fine-grained topics, may vary according to individual experts.  

Table 1. The macro-communities (with more than 40% agreement) in SW and HCI according 
to the experts  

HCI Communities

Virtual Reality
Information retrieval/WWW

Interaction Design/Usability Testing 

% 
?

?

?
77%  Ubiquitous Computing/Mobile Device 
77% 
69%  Pattern Rec./Gesture Rec./Speech Rec.  
61%  System Design/Software Engineering 
61%  AI /Machine Learning/Neural Network 
46%  Human Robot Inter. /Affective Comp. 

SW Communities 
Knowledge Base/Des. Logic 
Linked Data/Sem. Annotation 
Semantic Web Service 
Ontology Mapping/O. Matching 
Intelligent Agents
Ontology Engineering 
WWW/Information Retrieval 
Social Semantic Web 
 
To  study  the  similarities  and  differences  between  the  results  obtained  by  our 
approach and those generated by the experts, we ran TST over an increasing number 
of  clusters  (from  4  to  10)  to  highlight  the  macro-areas  and  how  they  split  as  the 
number of clusters  grows. Figure 3 and Figure 4 show the result. In  most cases the 
algorithm  behaved  as  a  human  expert,  for  example  splitting  the  macro-community 
Ontology in its main sub components as the number of required clusters increased. 
Our approach found 5 macro communities in both SW and HCI, which can be further 
split in 10 sub-communities for SW and 9 for HCI. 

% 
?

?

?
While here we label each community with the name of the most frequent topics for 
the sake of simplicity, actually the TTCs are described by a rich distribution of topics 
over  time,  which  can  reveal  interesting  insights  on  the  dynamics  of  the  research 
communities.  For  example,  the  Linked  Data  community  includes  a  variety  of 
equally  represented  topics  up  to  2007,  such  as  Query  Language,  Semantic 
Annotation and Information Retrieval, while from 2008 we see the strong onset of 
the  actual  Linked  Data  topic.  This  reflects  an  interesting  dynamics,  where  the 
different  research  areas  that  were  addressing  alternative  challenges  associated  with 
research on Semantic Web eventually converged on Linked Data once a number of 
underlying  technologies  became  sufficiently  mature.  In  the  same  way,  by  analysing 
the  topic  distribution  of  the  Virtual  Reality  community,  we  can  see  the  onset  of 

 
?

?

?
ntifying Diachronic Topic-Based Research Communities 

topics such as Mobile Dev
analyse the impact of the in
2007) and anticipate the va
following years.  

vice and Augmented Reality after 2007, which help
ntroduction of smartphones (the first iPhone was realized
ast amount of work that will be done on these topics in 

p to 
d in 
the 

All  macro-communities 
Social  Semantic Web for
Web is usually composed 
The experts found it natura
today  is  becoming  more 
according to the dataset thi
considered as a main comm
an  unfortunate  consequenc
recent data (the MAS API d

  in  Table  1  are  detected  by  our  algorithm,  except 
r SW and the  AI-Reasoning for HCI. Social  Seman
by topics such as Social Networks and Semantic Wik
al to aggregate these research areas into one category t
and  more  important.  The  algorithm  did  not,  beca
is area did not have enough authors and publications to
munity during the time frame in question. In sum, this w
ce  of  not  being  able  to  run  the  experiment  on  the  m
did not provide us with much data after 2010). 

for 
ntic 
ki. 
that 
ause 
o be 
was 
most 

 

 

Fig. 3. The SW main commun
To increase the readability of t

nities and how they are split in sub-communities by our algorit
the image, only the most important topics are shown.  

thm. 

The AI-Reasoning ma
an  abstract  category  where
Machine Learning, Neural 
fact it makes sense to have
be applied in different field
these  topics  to  the  comm
Learning  was  associated 
Information  Retrieval/Wor
Device with IR /WWW and
In  conclusion,  human 
appropriate, while TST can
clustering process). TST de
in  a  research  area.  We
complementary:  we  need  t

acro-community is a particularly interesting case, since i
e  different  human  experts  placed  AI  techniques,  such
Networks, User Model and Data Mining. To a human
e this kind of abstract categorization of techniques that 
ds. The algorithm instead is designed to assign each one
munities  who  mostly  use  them.  For  example,  Mach
in  most  years  with  the  Pattern  Recognition  and 
rld-Wide-Web  communities;  Data  Mining  and  Mob
d User Model mostly with Recommender Systems. 

it is 
h  as 
n in 
can 
e of 
hine 
the 
bile 

experts  are  able  to  create  abstract  categories,  w
nnot do this (unless an abstract category emerges from 
etects categories on the basis of the trends and practical 
e  believe 
two  perspectives  are  actua
the  abstract  classification  provided  by  experts  in  order

when 
the 
use 
ally 
r  to 

these 

that 

F. Osborne, G. Scavo

o, and E. Motta 

identify  groups  of  gener
communities,  but  we  also 
communities and in which c

ically  applicable  techniques/tools  relevant  to  differ
need  the  data-driven  perspective,  to  understand  by  wh
context these are used. 

rent 
hich 

 

 

Fig.  4.  The  main  communiti
algorithm. To increase the read

es  in  HCI  and  how  they  are  split  in  sub-communities  by 
dability of the image, only the most important topics are show

our 

wn. 

Evaluation of Cl

luster Compactness 

In  this  section  we  briefly
community cluster. We do 
PCAES [21]. PCAES varie
large PCAES  value  means 
others.  We  ran  the  differen
SW/HCI, with 4<n<10. Fig
runs:  the  best  performan
correspondence of n=4, wh
the use of TST with n=5. F
slightly inferior (but still wi
SW  and  6.7    1.9  for  HC
favor  a  more  articulate  cl
communities. 

y  present  an  evaluation  of  the  compactness  within  e
so by using a standard validity index for fuzzy clusteri
es between n and n, where n is the number of clusters
that each cluster is compact and well separated from 
nt  versions  of  the  algorithm  to  find  n  communities  un
gure 5 shows the average PCAES for SW and HCI over
nce  for  all  three  techniques  is  reached  for  HCI 
hereas for SW the best overall performance correspond
FC and FT obtain the best result with n=4. These values 
ithin two standard deviations) to the values of 7.9  2.3
I  indicated  by  the  experts,  possibly  because  they  tend
lassification,  even  at  the  cost  of  some  less  well-defi

each 
ing, 
s. A 
the 
nder 
r 20 
in 
s to 
are 
 for 
d  to 
ined 

We  have  thus  chosen  n=
statistical evaluation of the
TST  relative  to  the  other
correlated pairs. In the SW 
for  TST  vs.  FC.  For  n=4
threshold of 0.05 in both co

=4  and  n=5  as  the  number  of  clusters  on  which  to  ru
e performance of the three techniques, and in particular
r  two,  based  on  the  Wilcoxon  non-parametric  test 
case, for n=5 we obtain p=0.005 for both TST vs. FT 
4,  the  difference  gets  less  marked,  with  p  reaching 
omparisons. FT and FC have essentially similar behavio

un  a 
r of 
for 
and 
the 
ours 

 
?

?

?
for  both  values  of  n  (p=0.35).  In  the  HCI  case,  using  n=4  (best  value  for  all  three 
techniques), the comparison of TST relative to FT and to FC evidences in both cases 
statistically  significant differences, respectively  with p= 0.01 and 0.005. Using n=5, 
TST still dominates over FC (p=0.02) but no longer over FT (p=0.23).  

This confirms that TST is able to produce significantly more compact clusters, in 
particular when using the optimal value for n, mainly due to the use of topic vector 
weighing  (see  Section  3.3).  We  obtained  similar  results  by  selecting  the  maximum 
PCAES over 20 runs. In the SW case, given 4 clusters we obtained PCAES=2.09 for 
TST,  1.42  for  FT,  and  1.17  for  FC  (with  5  clusters  the  values  were  2.89,  1.19  and 
1.16). In HCI, given 4 clusters, we obtained PCAES=0.79 for TST, -0.32 for FT, and -
0.28 for FC. 

Interestingly, the cluster sets in SW seems to be more compact than the HCI ones. 
The  results  seem  to  contradict  the  human  experts,  who  actually  showed  a  higher 
degree  of  agreement  when  composing  HCI  communities.  However,  what  is 
considered the best clustering according to these  metrics is not always perceived as 
such by human experts. The reasons why HCI clusters have a lower PCEAS may in 
fact simply lie in the fact that HCI authors tend to address more heterogeneous themes 
and work across different communities. On the contrary a number of people working 
in  the  Semantic  Web  tend  to  publish  most  of  their  work  within  a  particular 
community. We thus may need novel evaluation metrics to be able to take in account 
the peculiarities associated with different topic-based research communities.   

 

Fig. 5. Average PCAES for Semantic Web and HCI  

Conclusions 

In  this  paper  we  have  presented  TST,  a  novel  approach  to  automatically  detect 
diachronic  topic-based  communities  i.e.,  communities  of  researchers  who  work  on 
semantically related topics at the same time.  

The user study presented in this paper shows that our approach yields results that 
are  statistically  consistent  with  those  obtained  from  domain  experts.  The  study  also 
shows that the adoption of i) a semantic characterization  of the research topics (see 
Section 3.1), ii) the topic vector weighing (see Section 3.2) and iii) the ATTS metric 

F. Osborne, G. Scavo, and E. Motta 

(see  Section  3.3)  dramatically  increases  the  quality  of  the  detected  communities. 
Moreover,  according  to  the  PCAES  index,  the  use  of  topic  vector  weighing  also 
increases significantly the degree of compactness of the detected communities. 

Our  approach  opens  up  many  interesting  directions  of  work.  Currently  we  are 
working on a novel method to automatically detect different kinds of patterns in the 
research  flow,  such  as  the  merging/splitting  of  different  communities  or  the 
occurrence of topic shifts within a community. In addition, we also plan to build on 
this approach to develop effective methods to measure the impact of specific events 
on  the  research  environment,  such  as  the  introduction  of  a  new  technology  or  the 
award  of  a  new  grant.  Such  functionality  is  of  particular  importance  to  research 
managers and funding bodies, who need better tools to measure the impact of policy 
decisions.  Finally,  we plan to  work on a predictive technique, aimed at forecasting 
the behaviour that a community is likely to exhibit in the short and medium term.  
