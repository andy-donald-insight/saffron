Web Semantics: Science, Services and Agents on the World Wide Web 7 (2009) 344356

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

j o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / w e b s e m

An efficient and scalable algorithm for segmented alignment of ontologies
of arbitrary size
Md. Hanif Seddiqui, Masaki Aono

Toyohashi University of Technology, 1-1 Hibarigaoka, Tempakucho, Toyohashi, Japan

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 26 October 2007
Received in revised form 1 September 2009
Accepted 2 September 2009
Available online 8 September 2009

Keywords:
Ontology alignment
Locality of reference
Segmented alignment

It has been a formidable task to achieve efficiency and scalability for the alignment between two mas-
sive, conceptually similar ontologies. Here we assume, an ontology is typically given in RDF (Resource
Description Framework) or OWL (Web Ontology Language) and can be represented by a directed graph.
A straightforward approach to the alignment of two ontologies entails an O(N2) computation by comparing every combination of pairs of nodes from given two ontologies, where N denotes the average
number of nodes in each ontology. Our proposed algorithm called Anchor-Flood algorithm, boasting of
O(N log(N)) computation on the average, starts off with an anchor, a pair of look-alike concepts from
each ontology, gradually exploring concepts by collecting neighboring concepts, thereby taking advantage of locality of reference in the graph data structure. It outputs a set of alignments between concepts and
properties within semantically connected subsets of two entire graphs, which we call segments. When
similarity comparison between a pair of nodes in the directed graph has to be made to determine whether
two given ontologies are aligned or not, we repeat the similarity comparison between a pair of nodes,
within the neighborhood pairs of two ontologies surrounding the anchor iteratively until the algorithm
meets that either all the collected concepts are explored, or no new aligned pair is found. In this way, we can
significantly reduce the computational time for the alignment. Moreover, since we only focus on segment-
to-segment comparison, regardless of the entire size of ontologies, our algorithm not only achieves high
performance, but also resolves the scalability problem in aligning ontologies. Our proposed algorithm
reduces the number of seemingly-aligned but actually misaligned pairs. Through several examples with
large ontologies, we will demonstrate the features of our Anchor-Food algorithm.

 2009 Elsevier B.V. All rights reserved.

1. Introduction

An ontology is an explicit specification of a conceptualization is
a prominent defintion by T.R. Gruber in 1995 [13]. The definition
was then extended by Studer et al., in 1998 as an ontology is an
explicit, formal specification of a shared conceptualization of a domain
of interest [39]. An ontology generally consists of entities such
as concepts, properties and relations. It is the backbone to fulfill
the semantic web vision [2,23] and is a knowledge base to enable
machines to communicate each other effectively. The knowledge
captured in ontologies can be used to annotate data, to distinguish
between homonyms and polysemies, to drive intelligent user interfaces and even to retrieve new information. A number of ontologies
are increasing day by day with new semantic web contents, because
an ontology is being developed to formalize the conceptualization
behind the idea of semantic web. Therefore, in order to achieve

 Corresponding author.
E-mail addresses: hanif@kde.ics.tut.ac.jp (Md.H. Seddiqui), aono@ics.tut.ac.jp

(M. Aono).

1570-8268/$  see front matter  2009 Elsevier B.V. All rights reserved.
doi:10.1016/j.websem.2009.09.001

semantic interoperability and integrity, ontology alignment would
be playing an important role [1].

Ontologies may be of different size, small to large, having hundreds to millions of RDF triples, where an RDF triple is an RDF
statement between a subject, a predicate, and an object. A smallscale ontology is usually defined within a single segment, where
the term segment is referred by Seidenberg et al. [34] as a fragment that stands alone as an ontology in its own right. A large-scale
ontology is generally very complex in nature and may contain multiple segments [34]. Stuckenschimdt et al. [38] also reveals that
a large-scale ontology contains a set of modules about a certain
subtopic that can be used independently of the other modules. A
large-scale ontology expressed in Web Ontology Language (OWL)
[25] is appearing to capture distributed knowledge in a centralized
knowledge base. For example, the biomedical domain has enormous large-scale ontologies, such as FMA [32] and OpenGALEN
[31].

Although there are many diverse solutions to the ontology alignment problem [7], only a few are capable to handle large ontologies
efficiently [17,19]. Many previously developed ontology alignment
systems are also facing severe performance problem when they

attempt to deal with large ontologies. Resolving scalability problem is also an important current issue in the ontology alignment
research [35]. Moreover, users (humans or machines) may want to
find a specific part of a large ontology which will fit only to their
interest.

The main contribution of our approach is of attaining performance enhancement by solving the scalability problem in aligning
large ontologies. The key idea is to start from an anchor point of a
taxonomy of an ontology and to proceed towards the neighboring
nodes considering the locality of reference. Eventually our proposed algorithm aligns a part of gigantic ontologies and outputs a
segmented alignment, (i.e. an alignment across two related segments
or fragments across ontologies) which are relatively small, however
sufficient to satisfy users interest in a particular domain. Therefore,
performance, scalability, and segmented alignment (i.e. an alignment across two related segments or fragments of ontologies) are
the key motivations of our proposed algorithm in this paper.

In our previous research work, we also focused on the ontology alignment [16]. Our system was capable of aligning small-scale
ontologies effectively with the feature of eliminating misalign-
ments. However, its major drawback was against the aligning of
large-scale ontologies, as its complexity was O(N2). To overcome
this limitation of scalability problem, we developed a new algo-
rithm, we called as Anchor-Flood algorithm.

Our algorithm assumes a seed called an anchor, where the
notion anchor is derived from the Anchor-PROMPT algorithm [30],
although it is clearly different from the notion used in that algo-
rithm. The Anchor-PROMPT algorithm augments existing methods
by determining additional possible aligned pairs across ontologies,
while our Anchor-Flood algorithm starts aligning from an anchor
and produces segmented alignments.

Our algorithm also shares the term flood with similarityflooding algorithm [26] without major similarities in their action
blocks. Although our work is inspired by the idea of similarity flooding algorithm in that the elements of two distinct hierarchy models
are likely to be similar when their adjacent elements are similar.
However, our Anchor-Flood algorithm does not propagate similarity value to the adjacent nodes iteratively unlike similarity-flooding
algorithm.

In the main process block of our algorithm, we collect aligned
pairs from the neighboring concepts computing similarities among
the collected concepts across ontologies starting from an anchor.
It begins exploring to the neighboring concepts to collect more
aligned pairs. As our algorithm starts off an anchor and explores
to the neighboring concepts, it has a salient feature of scalability.
Our algorithm achieves enhancement in terms of scalability and
performance in aligning large ontologies. It can reduce the number
of seemingly-aligned but actually misaligned pairs [16]. It also outputs segmented alignment, which is a unique characteristic in the
field of ontology alignment research.

The rest of the paper is organized as follows. Section 2 introduces
general terminologies to be used in the later sections. Section 3 contains the description of our proposed Anchor-Flood algorithm.
Section 4 includes experiments and evaluation, while Section 5
describes the complexity factor of the algorithm. Some related work
and the differences with our algorithm are described in Section
6. Section 7 includes concluded remarks along with some future
directions of our work.

2. General terminologies

This section introduces some of the basic definitions in the field
of ontology alignment research and familiarizes the reader with the
notions and terminologies used throughout the paper. It includes
the definitions of ontology, taxonomy, alignment across ontologies,

the similarity measures and the idea of a segment in an ontol-
ogy.

2.1. Ontology, concept, relation, and taxonomy

According to Ehrig [7], an ontology contains a core ontology,
logical mappings, a knowledge base, and a lexicon. Furthermore, a
core ontology is defined as a tuple of five sets: concepts, concept
hierarchy or taxonomy, properties, property hierarchy, and concept to property function. In a taxonomy, concepts are organized in
subsumption relations. For example, if C represents a taxonomy
and if c1 < c2 in a taxonomy for c1, c2  C, then c1 is a subconcept of
c2, and c2 is a superconcept of c1.

A taxonomy or a concept hierarchy and concept relations are
widely used throughout this paper and taxonomies are the primary
element of our algorithm. We also use some of the components of
the knowledge base of an ontology, such as instances.

2.2. Ontology alignment

Alignment A is defined as a set of correspondences with quadruples < e, f, r, l > where e and f are the two aligned entities across
ontologies, r represents the relation holding between them, and l
represents the level of confidence [0, 1] if there exists in the alignment statement. The notion r is a simple (one-to-one equivalent)
relation or a complex (subsumption, or one-to-many) relation [7].
The correspondence between e and f is called aligned pair throughout the paper. Alignment is obtained by measuring similarity values
between pairs of entities.

2.3. Similarity measures

Similarity is usually measured by considering textual contents,

structure, and semantics available in an ontology.

Concepts, properties or instances often contain labels and comments as their textual contents. Sometimes URI itself is informative.
The textual contents associated with an entity, such as a concept, a
property or an instance, are referred to as a description. A description usually contains informative terms, therefore, description
based similarity measure is often called as terminological similarity
measure. The terminological similarity measures are widely used
in ontology alignment methods. We use terminological similiarity
measures using WordNet [9] and the Jaro-Winkler string distance
[43] in our algorithm.

We employ WordNet based equality referred to by SimWN.
WordNet based equality states that two terms are equal if there
is at least one sense in common, which is synonym of the second
[11] and is defined as,
SimWN(t1, t2) =

cond(t1, t2)holds

(1)

1.0,

0.0, otherwise

where,

cond(t1, t2) = x{x senses(t1)  x senses(t2)}

The function senses(t1) returns the WordNet based synonym

senses of a particular term, t1.

String metric based similarity is defined in [43] and used in [19]
and [36]. Let di be the description of concept ci and dj be the description of concept cj. The string metric based similarity between ci and
cj is then defined as follows:
SimSM(di, dj) = comm(di, dj)  diff(di, dj) + winkler(di, dj)
where comm(di, dj) stands for the commonality between di and dj,
diff(di, dj) for the difference and winkler(di, dj) for the improvement of the result using the method introduced by Winkler [43].

(2)

Md.H. Seddiqui, M. Aono / Web Semantics: Science, Services and Agents on the World Wide Web 7 (2009) 344356

On the other hand, structural similarity measures in our research
rely on the intuition that the elements of two distinct models are
similar when their adjacent elements are similar [26]. Structural
similarity values are computed by the methods called structural
internal matching and structural external matching [36]. In structural
internal matching, the similarity value between two concepts is
computed by the ratio of the number of terminologically similar
properties to the number of total properties of the pair of concepts
across ontologies, and is defined as follows:

Siminternal(ci, cj) = 2  |Aligned Properties|
|Properties ci, cj|

(3)

(4)

In structural external matching, the similarity value is computed
between two concepts by the ratio of the number of terminologically similar direct superconcepts, siblings, and subconcepts to the
number of total direct superconcepts, siblings and subconcepts .

Simexternal(ci, cj) = 2  |Aligned Pairs|
|Concepts aroundci, cj|

2.4. Segment

The notion segment is playing an important role in this paper.
Generally, a segment within an ontology is a conceptually connected
subset of the entire ontology graph that is not a mere fragment,
but stands alone as an ontology in its own right [34,38]. A segment can be independent of the other subtopics or other segments
of an ontology. However, a segmented alignment in this paper is
defined as a pair of connected components across two ontologies
as far as aligned pairs are found among them. In this paper, the
size of a segment is defined dynamically at the time of aligning
as our Anchor-Flood algorithm starts off an anchor, and explores
to the neighboring concepts until either all the collected concepts
are explored, or no new aligned pair is found and finds aligned
pairs among the neighboring concepts. An example of a pair of
segments is illustrated by the enclosed polygons in Figs. 1 and 2.
Hence, starting from a single anchor, our algorithm may find two
aligned segments from two ontologies, and we propose the alignment found within the segments as segmented alignment. As long
as the aligned pairs are connected within neighbors across ontolo-
gies, the connected components of each ontology form a segment.

3. Anchor-Flood algorithm

Our proposed algorithm takes an anchor as one of the inputs
along with two ontologies, O1 and O2. It outputs a set of aligned
pairs within two segments across ontologies. Before proceeding to
the detail of our algorithm, we define four frequently used terms
in this paper.

Definition 1 (Anchor). An anchor is defined as a pair of similar concepts across ontologies. If e O1, f  O2 and e  f , meaning that e and
f are aligned, then an anchor, X is defined as a pair (e(O1), f (O2)),
where the first element e(O1) comes from ontology O1 and the second element f (O2) comes from the ontology O2. The notion anchor
was first introduced in an ontology alignment technique by Noy et
al [30].

Definition 2 (Neighbors). As concepts are organized in a hierarchical structure called a taxonomy, we consider neighbors in this
paper as a set of concepts, and defined as:
neighbors(c)=U{children(c)  grand children(c)  parents(c)
 siblings(c)  nephews(c)  grand parents(c)  uncles(c)}
where children(c) and parents(c) represent the children and the
parents of a particular concept c, respectively within a texonomy,
whereas grand children(c) is children(children(c)), grand parents(c)

(5)

children(parents(c))  {c},
is parents(parents(c)),
nephews(c) is children(siblings(c)) and the last of all, uncles(c)
is children(grand parents(c))  parents(c). The term neighboring
concept is used as an element of neighbors.

siblings(c)

is

Definition 3 (Descendant set). Descendant set, D, is a set of one or
more children of concepts explored in a taxonomy and is defined
as follows:
D = {d| d is a descendant of an explored concept}

Definition 4 (Ancestor set). An ancestor set or parent set, P, is a set
of one or more ancestor concepts of the set of concepts available in
descendant set, D:
P = {p| p is ancestors of concepts D}

We assume that every concept within a taxonomy of an ontology
has semantic relationship among the neighboring concepts. The
concepts of two distinct hierarchies are similar if their neighbors
are similar [26]. Conversely, if the concepts of two distinct hierarchies are similar, there is a possibility that their neighbors are
similar.
Assume that there are two concepts e and f such that e O1 and
f  O2, Ne be the neighbors(e), Nf be the neighbors(f ) and e  f . Then
there is a possibility that some elements of Ne are similar to the
elements of Nf . If there is no sufficient similarity measured between
the elements of Ne and those of Nf , then it is considered that there
is no relation holds between e and f. Hence, the alignment e  f is
a misalignment [16].

Before showing the details of our Anchor-Flood algorithm later
in Fig. 4, we will illustrate the algorithm in a simplified form and
with a comprehesive example.

The simplified form of the algorithm is depicted in Fig. 3. On the
other hand, the example consists of two simple ontologies that are
to be aligned. The two ontologies O1 and O2 describing the domain
of bibliography are given in Figs. 1 and 2, respectively. Ontologies of these figures are taken from the benchmark of Ontology
Alignment Evaluation Initiative.1Fig. 1 is taken from the benchmark
ontology number 301 while the benchmark ontology number 101 is
shown in Fig. 2. Our algorithm aligns the ontologies starting with an
anchor. For the given example, the algorithm starts with an anchor
as (Inproceedings (O1), InProceedings(O2)).

3.1. Simplified form of the algorithm with an example

Our Anchor-Flood algorithm preprocesses ontologies. The algorithm starts off an anchor, taken as input parameter. For each
iteration, it selects a pair of concepts to explore, collects neighboring concepts from the selected concepts across ontologies, and
then starts the aligning process to produce aligned pairs from the
collected neighbors. The process repreats until either all the collected concepts are explored, or no new aligned pair is found. The
simplified form of the algorithm is illustrated in Fig. 3.

For example, if we start our Anchor-Flood algorithm off an
anchor, (Inproceedings (O1), InProceedings (O2)) across ontologies O1 as depicted in Fig. 1 and O2 as displayed in Fig. 2, it explores
to the neighboring concepts and collects them. It collects Entry
in O1 and Part in O2, using anchor-concepts Inproceedings
and InProceedings respectively. Alignment process starts aligning the collected concepts and their properties. Then, the algorithm
explores from Entry and Part to further neighboring concepts
and collects them. Eventuatlly, our algorithm collects all the neighboring concepts across ontologies from the anchor and retrieve

1 Available at: http://oaei.ontologymatching.org/2006/benchmarks/.

Fig. 1. Bibliographic Ontology, O1 (OAEI-2006 benchmark dataset #301). The segment found in our algorithm is enclosed by polygon.

aligned pairs by aligning process. The algorithm repeats the steps
until either all the collected concepts are explored, or no new aligned
pair is found. Hence it produces aligned pairs within two segments
across ontologies. The segments are illustrated by enclosed polygons in Figs. 1 and 2.

On the other hand, if the algorithm starts with an anchor,
e.g. (Conference(O1), Conference (O2)), it cannot find any other
aligned pair even after exploring to their neighboring concepts.
Thus, the provided anchor is considered as misaligned, although
they are terminologically similar.

The next subsections elaborate the descriptions of the compo-

nents of the algorithm in more detail.

3.2. Preprocessing of Anchor-Flood algorithm

As we describe at the lines 1 and 2 of the pseudo code in Fig. 4,
the input ontologies are preprocessed. The Anchor-Flood algorithm
takes two ontology files (either in RDF or OWL) as input. Then
the preprocessing module parses each ontology file using the ARP

parser2 which produces RDF triples available in the ontology file.
This module produces a taxonomy of concepts, a list of proper-
ties, and their relations. It also produces comments, and labels
of uris as their textual contents, or description of entities. Before
aligning the ontologies, the preprocessing module normalizes the
textual contents of entities by the process of tokenization, stopword removing, word-stemming and text normalization such as
person names, dates in a uniform format.

3.3. Initialization of Anchor-Flood algorithm

As we describe at the lines 3 through 6 of the pseudo code in
Fig. 4, the basic components of the algorithm are initialized. An
anchor, which is one of the inputs to our algorithm, consists of a
pair of two concepts e and f denoted by (e(O1), f (O2)). This means
that concept e is supposed to be similar to concept f. We will use

2 Another RDF Parser, http://www.hpl.hp.com/personal/jjc/arp/.

Md.H. Seddiqui, M. Aono / Web Semantics: Science, Services and Agents on the World Wide Web 7 (2009) 344356

five sets of entities to keep track of important pieces of information
during the algorithm. They are two ancestor sets P1 and P2, two
descendant sets D1 and D2 and one alignment A (see Figs. 4 and 5).
If we consider an anchor as (Inproceedings (O1), InProceedings
(O2)), P1 = D1 = {Inproceedings}, P2 = D2 = {InProceedings}, and
A = (Inproceedings, InProceedings,=, 1.0).

In the following, we define terminologies used in our Anchor-

Flood algorithm.

The alignment A is a set of correspondences with quadruple
defined in Section 2. Note that the algorithm starts with initializing
P1, D1 with e, P2, D2 with f, and A with (e, f,, 1.0).

3.4. Exploring the neighboring concepts

Having initialized the parent sets (P1, P2), descendant sets
(D1, D2), and alignment (A), the algorithm starts exploring towards
the neighboring concepts of the anchor, as we describe at the lines
8 through 36 of the pseudo code in Fig. 4. The main processing block
of the algorithm contains three mutual exploring steps, i.e. exploring aligned concepts (EA), exploring unaligned concepts (EU), and
exploring the parents (EP). The EA step has the highest priority. Concepts of aligned pair are explored until there is no more unexplored
aligned pair in set A. The EU step has the next priority. If there is no

Fig. 2. Bibliographic Ontology, O2 (OAEI-2006 benchmark dataset #101). The segment found in our algorithm is enclosed by polygon.

Fig. 3. Simplified macro steps of the Anchor-Flood algorithm to align within two
segments across ontologies to produce segmented alignment.

more aligned pair to be explored at a particular iteration, the EU step
is executed as per condition stated in the algorithm, otherwise the
EP step is executed. When the algorithm terminates, it outputs a set
of aligned pairs within two segments across ontologies. The three
exploring steps, involved in the algorithm, are elaborated below.

3.4.1. Exploring aligned concepts (EA step)

As we describe at the lines 8 through 12 of the pseudo code
in Fig. 4, the EA step explores neigboring concepts of each of the
aligned pairs available in the alignment set, A. Since the given
anchor is a pair of concepts as defined above, the first aligned pair
naturally comes from an anchor. Both of the concepts of an anchor
are explored to their neighboring concepts. Then the explored
neighboring concepts are inserted into corresponding descendant
set, either D1 or D2 as depicted in Fig. 5 with a directed line labeled
by ea1 or ea2. After every step, AlignDescendantSets module in
Fig. 6 is started to find new aligned pairs by computing similarities
between the concepts available in D1 and D2. The concepts belong
to every aligned pair of concepts are explored one after another
(see Fig. 4).
if ai( A) =< Entry, Reference, =, 1.0 in
Figs. 1 and 2 then, after exploring, the descendant sets D1 and D2 will
be increased by the new concepts {Article, Proceedings, Con-
ference, Manual, Booklet, Book, Incollection, Inproceedings,
TechReport, Inbook, MasterThesis, PhdThesis, Unpublished,
Misc } and {Academic, Book, Informal, Report, MotionPic-
ture, Part, Misc }, respectively.

For example,

3.4.2. Exploring unaligned concepts (EU step)

The EU step is described at the lines 13 through 24 of the pseudo
code in Fig. 4. It explores the unexplored concepts to their neighboring concepts possibly from the descendant sets, D1 or D2 or from
both. Exploring the unaligned concepts only in either descendant
set D1 or D2, may cause unbalanced growth of any of the sets. We
detect that this happens when the distance of the concepts in one
ancestor set from leaves is not close to that of the concepts of other
ancestor set. To overcome the problem, the size of both D1 and D2
are observed in the algorithm.
By taking the example of Figs. 1 and 2, if Article  D1 and is
not yet explored, then D1 will be increased by . If Academic  D2
and is not yet explored, then D2 will be increased by the concepts
{MasterThesis, PhdThesis }.

Fig. 4. Pseudo code of the Anchor-Flood algorithm. Each local declared sets (P1, P2,
D1, D2 and A) are indexed sets and associated with flags that contain index number
to indicate up to which they are already explored. The next function always returns
the next element of an indexed set and the operator  is used for a union operator,
where new distinct data are always appended at the set.

3.4.3. Exploring the parents (EP step)

We describe the process of the EP step at the lines 28 through 36
of the pseudo code in Fig. 4. Since we assume two ancestor sets P1
and P2 are available to hold ancestors of ontology O1 and O2, respec-
tively. Which set will be explored to its own ancestor depends on
the difference between the number of unaligned concepts of D1
and that of D2. It is decided observing the size of both D1 and D2.
If the size of one set is more than the double of the other set, then
the other parent set is explored to its ancestor and the ancestor is

Md.H. Seddiqui, M. Aono / Web Semantics: Science, Services and Agents on the World Wide Web 7 (2009) 344356

Fig. 5. The figure shows the process of Anchor-Flood algorithm where anchor is taken as an input to produce a segmented alignment. The process AlignDescendantSets
works locally to produce correspondences among the concepts of Decsendant Sets D1, D2.

added to its corresponding descendant set. Otherwise, the concepts
of the both ancestor sets will be explored. The explored concepts
are inserted into the descendant sets D1 or D2 as depicted in Fig. 5
with a directed line labeled by ep1 or ep2.
For example, if P1 = {Inproceedings} in Fig. 1, then the concepts
of P1 will be replaced by the parents of it, i.e {Conference, Entry
}. Similarly, if P2 = {InProceedings } in Fig. 2, then the concepts of
P2 will be replaced by the parents of it, i.e. {Part }.

3.5. Aligning descendant sets

The alignment process, AlignDescendantSets, is described at
the line 38 of the pseudo code in Fig. 4. It processes alignment with
the sets of concepts in D1 and D2 as shown in Fig. 5 and produces
terminological and structural alignments, as the concepts and the
properties of ontologies contain terminologies and are organized
in well defined structures. The total process, described in Fig. 6,
includes the terminological alignment, and the structural align-
ment. The terminological alignment and the structural alignment
are described in the following sections briefly.

3.5.1. Terminological alignment

and
simf (tip, tjq) =

simL(tip, tjq) =

simL(tip, tjq),
0,

if simL  
otherwise

SimWN(tip, tjq),
SimSM(tip, tjq),

if tip, tjq  WN
otherwise

where SimWN and SimSM are defined in Eqs. (1) and (2), respectively,
and we choose the threshold  as 0.65 in our implementation.

The experimental results shown in [36] indicate that the value
0.65 is a good threshold. However, our methods of similarity measure split the descriptions into terms and then compute similarities
among them. Therefore, the overall similarity values decreases.
Our experiments show that the value 0.50 is a good threshold in
terminological similarity measure.

The terminological alignment process uses terminological sim-

ilarity values and it is depicted in the pseudo code of Fig. 6.

The process of producing terminological alignment is depicted
in Fig. 6 at line 1 through 7. Our system computes WordNet and
string metric based similarity values as in Eqs. (1) and (2), respec-
tively. String metric based similarity value is obtained as it is stated
in [19,36]. Suppose, concept ci  D1 contains description di and concept cj  D2 contains description dj. Moreover, di is a set of terms,
Ti = {ti1 . . . tin} and dj is a set of terms Tj = {tj1 . . . tjm}, where i and
j are index values, and n, m are the number of terms available in

di and dj, respectively. Then, the similarity value is calculated as
follows:
Simt(ci, cj) = min

max1qm(simf (tip, tjq)),

1pn

max1pn(simf (tip, tjq))

1qm

(6)

where max function returns the maximum value from a list of val-
ues, min function returns the minimum one between two values,

Fig. 6. Pseudo code of the AlignDescendantSets, where the process of terminological alignment, and structural alignment are displayed.

Table 1
The results obtained by the participants on the benchmark test cases during OAEI-2008, where H-mean represents harmonic means [3].

Sys

test

1xx
2xx
3xx

H-mean

Sys

test

1xx
2xx
3xx

H-mean

refalign

Prec.

GeRoMe

Rec.

Prec.

Rec.

edna

Prec.

Aflood

Rec.

Prec.

Rec.

Prec.

Rec.

Prec.

Rec.

Prec.

Rec.

DSSim

Prec.

Lily

Prec.

MapPSO

RiMOM

SAMBOdtf

SPIDER

TaxoMap

Rec.

Prec.

Rec.

Prec.

Rec.

Prec.

Rec.

Prec.

Rec.

Prec.

Rec.

Prec.

Rec.

Rec.

scalability, memory consumption and segmented alignment. As
we participated in two tracks of the OAEI-2008 campaign [33], we
obtained the precision and recall for the benchmark dataset and
that of the anatomy track as a feedback. We also obtained the performance comparisons among different systems at the campaign.
Moreover, we experimented to observe the effects on memory con-
sumption, to evaluate the scalability, and after all, to observe the
nature of the segmented alignment of our algorithm. In the following subsections we describe each of the factors.

4.1. Precision and recall in OAEI-2008

We participated with our Anchor-Flood algorithm based system in the OAEI-2008 campaign. The campaign restricts of feeding
manual anchors to a system. However, our Anchor-Flood algorithm
starts off anchors. Therefore, we adapted our system to generate
anchors automatically and fed the anchors to retrieve alignments.

4.1.1. Adaptation

As our Anchor-Flood algorithm requires an anchor to produce
a segmented alignment, we have developed a program module to
retrieve anchors automatically considering ontology entities such
as concepts, properties, instances and restrictions. This fragment of
code is simple and runs faster.

In this module, we use exact string match of the description of concepts, properties and instances across ontologies. All
exactly matched concepts are considered as anchors. Moreover, the
domains and the ranges of matched properties are added to the list
of anchors. We also consider the concept-type of a pair of matched
instances as an anchor. Furthermore, structural similarities also
provide us some additional anchors.

3.5.2. Structural alignment

The process of producing structural alignment is depicted in
Fig. 6 at line 8 through 14. It shows that an structural alignment is
retrieved after collecting and computing the terminological alignment for all of its neighboring concepts across ontologies. In our
research we compute structural similarity values within the direct
neighbors from a reference node, e.g. children, parents, and sib-
lings. Terminological alignment is considered as a prerequisite of
computing structural similarity values. During the terminological
alignment process, properties of the referenced concepts are also
aligned by the their similarity values. Then, the structural similarity value is computed by Eqs. (3) and (4), and then it is averaged as
defined in Eq. (7) below:

Simavg = Simexternal + Siminternal

(7)

We also consider threshold for structural similarities as 0.50.
The structural alignment process uses structural similarity val-

ues and it is illustrated in the pseudo code of Fig. 6.

3.6. Aggregating alignments

An aggregation (see Fig. 5) process is described at the line 45
of the pseudo code in Fig. 4. The process is named as Aggre-
gateAlignedSets. We consider the structural alignment as the
basic aligned pairs, then we test each of the terminological aligned
pair whether there is any aligned neighboring concepts.

If a terminological aligned pair is not aligned by structural meth-
ods, then the terminological aligned pair is considered as probable
misalignment [16]. However, the AggregateAlignedSets further
looks for aligned pairs among neighboring concepts. If any neighboring concept is found as aligned pair within neighboring concepts
of other, the concept pair is considered as aligned, otherwise it is
discarded as a misaligned pair.

The process of the aggregation of the alignments is also illus-

trated in the pseudo code of Fig. 7.

4. Experiments and evaluation

We applied our Anchor-Flood algorithm to a variety of datasets,
including the benchmark dataset,3 and larger ontologies, such
as anatomical ontologies (FMA.owl, OpenGALEN.owl, human.owl,
mouse.owl and so on), web directory ontologies in OWL format.

In this section, we discuss the results obtained from the OAEI2008 and acquired by our extensive experiments in terms of

3 http://oaei.ontologymatching.org/2008/benchmarks/.

Fig. 7. Pseudo code of the AggregateAlignedSets, where the terminological alignment and the structural alignment are aggregated to produce final alignment.

Md.H. Seddiqui, M. Aono / Web Semantics: Science, Services and Agents on the World Wide Web 7 (2009) 344356

pairs. We obtained comparatively lower recall over 3xx of benchmark dataset due to the removal of subsumption module from the
local aligning process [33] and other necessary changes to work
with large ontologies. Moreover, our system did not pass the test
303. Since this test seems to be the most difficult one among the 3xx
test, results were not significant. However, we achieved the best
time efficiency in the anatomy track of the OAEI-2008 campaign.

4.2. Time efficiency

The anatomy track of OAEI-2008 campaign contained two moderately large ontologies on the anatomy of human and mouse. Our
system, based on Anchor-Flood algorithm, achieved the best runtime among nine other systems in terms of runtime. It required
1 min and 5 s to produce aligned results across human and mouse
ontologies executed by the organizers. The nearest faster system,
AROMA [4], took 3 min and 50 s on the same pair of ontologies and
was also executed by the organizers. AROMA produced lower precision than that of our system. Moreover, our system was more than
15 times faster than DSSim [28] and more than 22 times faster than
the RiMOM [45]. The other systems even took much more time than
our system. Table 2 depicted the fact.

for

We also performed other different experiments over large
ontologies
testing performance, which are figured in
Tables 3 and 5. The average number of comparisons and elapsed
time, displayed in these tables, depict the efficiency of our
algorithm. However, aligning process for OpenGalen vs. mouse
ontology and FMA vs. OpenGalen ontology took much time and
more average number of comparisons. It is due to the large number
of children available in several different nodes.

We also conducted experiments of FMA.owl ontology against
itself, which contains around 72 thousand concepts to observe the
nature of the operation over highly similar ontologies. Table 3
summarizes the average number of alignments as well as the
average number of comparisons, by taking 300 randomly chosen
anchors from FMA dataset. The table also shows the standard deviation which clealry demonstrates the stability and scalability of
the Anchor-Flood algorithm. The total number of correspondences
with FMA dataset is 71,978 and the total number of comparisons
was 3,748,970.

From these tables, it is obvious that the average number of
comparisons is significantly smaller than the number of comparisons with brute-force algorithm, which amounts to O(N2) (N2 =
71, 9782  5 billion) comparisons. The average number of comparisons per aligned pair of our algorithm is approximately 53. It means
that each concept is not necessarily compared against every concept across ontologies. Therefore, our algorithm clearly showed the
performance enhancement over the other systems.

4.3. Memory efficiency

We experimented with five large sized quite popular ontologies
of the domain of anatomy. Table 4 contains the name of ontologies,
memory consumption to store their own persistent model in mem-
ory, the number of concepts, properties, triples and the average
number of children in intermediate concepts of their taxonomies.
We create our own persistent model of an ontology, which stores
textual contents, a taxonomy of concepts, relations and restrictions
of concepts, properties and instances. We experimented for retrieving alignments across several pairs of ontologies. Table 5 displays
information of the experiments. The first two columns represent
the name of the ontology pair. The third column contains the total
memory required for processing including to store their persistent memory model. The total number of aligned pair is displayed
in column 4, whereas the fifth column shows the average number of comparisons per aligned pair. The last column is dedicated

Fig. 8. The average precision and recall graph for the systems participated in OAEI
2008 campaign, including our Anchor-Flood algorithm based system, aflood [3].

Table 2
Alignment of anatomy ontologies in OAEI-2008 [3], where BK stands for background
knowledge.

System

SAMBO [21]
SAMBOdtf [21]
RiMOM [45]
Aflood [33]
Label Eq.
Lily [41]
ASMOV [20]
AROMA [4]
DSSim [28]
TaxoMap [15]

Runtime

12 h
17 h
24 min
1 min 5s

3 h 20 min
3 h 50 min
3 min 50s
17 min
25 min

Yes
Yes
No
No
No
No
Yes
No
No
No

Precision

Recall

F-Measure

4.1.2. Precision and recall results

The correct alignment discussed herein is provided along with
the benchmark, which is a set of small-scale ontologies of OAEI
website and the OM-2008 proceeding [3] contains the results from
the participant systems of ontology alignment. We are one of the
participants, participated with our Anchor-Flood algorithm and the
adaptation is described in our paper of OM-2008 [3] and in the
Section 4.1.1 above.

Table 1, Fig. 8 and Table 2 depict the results in terms of precision
and recall among the Anchor-Flood algorithm based system and
other systems participated in the OAEI-2008. The results show that
we are among the best four in terms of precision and recall in both
the benchmark datasets and the anatomy track. Our Anchor-Flood
algorithm achieved the best precision over the other systems in the
test set 3xx, which were real ontologies, as Anchor-Flood algorithm
successfully reduced seemingly-aligned but actually misaligned

Table 3
Results of FMA.owl against itself.

Aligned pair

Total comparisons

Comparisons per aligned-pair

Memory consumption (MB)

Elapsed time (min)

3,748,970

Table 4
Various statistics of some ontologies we used.

Ontology

FMA.owl
Full-Galen
OpenGalen
Human
Mouse

Memory consumption (MB)

|Concepts| number

|Properties| number

|Triple| number

Avg. child

Table 5
Memory consumption, number of aligned pairs, elapsed time and the average number of comparisons per aligned pair.

Ontology O1

Ontology O2

Memory consumption (MB)

|Aligned-pair| number

Time (s)

Avg. comp

Full-Galen
Full-Galen
Full-Galen
OpenGalen
OpenGalen

Full-galen
OpenGalen
human
mouse
OpenGalen
human
mouse
human
mouse

for displaying elapsed time. The internal structure of a taxonomy,
where there are a large number of children in several intermediate concepts, has negative effect of performance as shown in FMA
vs. OpenGalen and OpenGalen vs. mouse ontology alignment cases.
Inspite of all the negative effects of the large sized ontologies, our
algorithm is use almost stable memory consumption despite of the
change of size of an ontology (see Tables 3 and 5).

4.4. Scalability

The data contained in Tables 3 and 5 shows the fact of the scal-
ability. Our algorithm keeps the memory consumption low, keeps
the number of average comparisons stable, and keeps the elapsed
time linear. Our algorithm, starting from an anchor, retrieves
aligned pairs across related segments only. Moreover, our algorithm explores gradually to neighboring concepts and collects
concepts based on locality of reference within the segment, and computes similarity values among the collected concepts only. Thus, the
algorithm reduces an average number of comparisons as it restricts
within segments (see Tables 3 and 5). Although the ontology alignment of FMA vs. OpenGalen, and OpenGalen vs. mouse ontologies
made us take large elapsed time and large number of average comparisons due to the internal structure of the taxonomies of the
ontologies. As they contained large number of children in several
intermediate concepts, our system collected large number of neighbors from the taxonomy, producing large number of comparisons
and, hence, resulted in large elapsed time. Out of these exceptions,
our algorithm managed to keep elapsed time short, even when the
ontologies are very large.

4.5. Segmented alignment

To demonstrate segmented alignment, another feature of our
Anchor-Flood algorithm, we carried out the experiment using pair
of ontologies of web directory structures called source.owl and
target.owl, which are also available in OAEI as another alignment
task. Table 6 shows the results of segmented alignment between
two ontologies of web directory structures. One anchor (pair of

concepts) produced one segment pair across ontologies. The first
column of the table represents the name of the ontologies while
the second column shows the abbreviated URI of concepts in the
ontology. Two consecutive concepts in the second column form
an anchor. The number of concepts in segments is displayed in
the third column. The last column of Table 6 shows the number
of aligned pair of equivalent relation within segments.

The anchor, (Health(Source), Health(Target)), produced two segments with 274 and 313 concepts, respectively and the process
AlignDescendantSets, portrayed in Fig. 6, produced 90 aligned
pairs of equivalent () relation. Similarly, other anchors of Table 6
produced segments of different interests. For example, anchor
(Music(Source), Music(Target)) produced pair of segments which
contains only the concepts about Music.

5. Complexity analysis

As the Anchor-Flood algorithm starts off with aligning an
anchor, and explores to its neighboring concepts, the performance

Table 6
Different segmented alignments starting from anchors across directory web
ontologies.

Ontology
name

Anchors
(concepts)

|Concepts| in
segment

|Aligned-pairs| Average

comparisons

Source
Target

Source
Target

Source
Target

Source
Target

Source
Target

Source
Target

Health
Health

Wrestling
Wrestling

Food Wine
Food Wine

Buddhism
Buddhism

Science
Science

Music
Music

Md.H. Seddiqui, M. Aono / Web Semantics: Science, Services and Agents on the World Wide Web 7 (2009) 344356

is not heavily affected by the size of ontologies. However, the performance depends on the size of neighbors defined in Eq. (5), and on
the size of segments in a segmented alignment, which is illustrated
in Section 2.4.
Consider two ontologies O1 with N1 entities and O2 with N2 enti-
ties, and for simplicity, if we assume N1 = N2  N, and the average
number of children of each node be k for both of them, then with
a brute-force method, the number of comparisons will be O(N2).
That is, every entity included in one ontology must be compared
against every entity included in the other ontology.
Let us assume that the size of each of the segments in a segmented alignment is M in both ontologies where usually M  N. In
any case, our algorithm compares entities related to the neighboring concepts of neighbors defined in Eq. (5). Therefore, the number
of concepts to be compared from each ontology on the average at
a particular iteration, collected from children, grand-children, par-
ents, siblings, nephews, grand-parents and uncles concepts from a
reference concept pair.

As k is the average number of children of a node, we can retrieve

the following average value for Eq. (5).
|children(c)| = k
|grand children(c)| = k2
|parents(c)| = 1
|siblings(c)| = k  1
|nephews(c)| = k2  k
|grand parents(c)| = 1
|uncles(c)| = k  1

Therefore, the total number of concepts in one neighbors as Eq.

(5) would be:
O(k + k2 + 1 + (k  1) + (k2  k) + 1 + (k  1)) = O(2(k + k2)) = O(k2)
which shows that the average number of neighboring concepts
in the neighbors of a reference concept to be compared is O(k2).
Therefore, the average number of comparisons among neighboring
concepts of two concepts will be O(k4). There are M/k intermediate concepts within a segment which has k children on an average.
Thus, the average number of comparisons for a segmented alignment will be:

Now, if we consider that all entities are associated with some of
the segments which have similar segments across ontologies, then
there are N/M segments. Therefore, the total number of comparisons will be,

As k is usually small in number in real ontologies, our algorithm

works faster.

In the worst case, when the total depth of an ontology is less
than or equal to 3, the complexity becomes N2. However, only a
small increase in the depth-number leads exponential imrovement
in the performance as the size of k is reduced drastically. If the total
depth increases, the value of k decreases logarithmically. Hence,
if the depth is larger compared to log2 N, a taxonomy becomes a
binary tree. Then, in the best case, the computational complexity
will become
O(23N)  O(8N)  O(N)

The best case of the computational complexity O(N) comes from
the advantage of the locality of reference, meaning that we can
skip unrelated part of ontologies. A significant number of unnecessary comparisons are skipped automatically. Inspite of some
additional operations, it needs to collect neighboring concepts in
Anchor-Flood algorithm, it works well. However, the taxonomy of
an ontology is neither a binary nor a balanced tree. Ideally, the

k4

= O(Mk3)

Mk3

= O(Nk3)

average number of comparisons is measured as O(Nk3), where k is
usually significantly smaller, however, is not negligible. The number of children k of a node depends on the the depth d of a taxonomy
and k is defined as
k  log N
log d

Therefore, the average complexity of our Anchor-Flood algorithm is O(N log(N)). Moreover, experimental results over large
ontologies also imply the average complexity as O(N log(N)).

6. Related work

There are some related works which target to deal with ontology
alignment such as COMA++ [6], NLM Anatomical Ontology Alignment [44], PBM based Falcon-AO [17,18], iMAP [5] and Chimaera
[24]. Moreover, there are some other approaches [14,27,42], which
address large-scale ontologies or large-scale class-hierarchies.
These related work, which are relevant to our Anchor-Flood algo-
rithm, are described in the subsequent sections.

6.1. COMA++

COMA++ [6] is a generic schema matching tool, providing a
library of individual matchers and a flexible infrastucture to combine the matchers and refine their results. A scalable approach
is introduced to identify context dependent correspondences
between schemas with shared elements and a fragment based
matching approach which decomposes a large match task into a
smaller tasks. By fragment they denote a rooted sub-graph down
to the leaf level in the schema graph. In general, fragments have
little or no overlap to avoid repeated similarity computation and
overlapping match results.

There are some major differences between COMA++ and our
Anchor-Flood algorithm. COMA++ identifies fragments before the
matchers applied. Then the similar fragments are identified by comparing the contexts of their schema roots to the fragment roots.
There is always a chance of spitting really related components
across fragments in one schema, however, of being well formed
in another schema. Matchers of COMA++ would face problem to
align across several fragments then. If the process of fragmentation does not consider enough semantics, the above problem would
arise severely. Again, the fragmentation and identification of the
similar fragments will cost additional clock cycles.

On the other hand, our Anchor-Flood algorithm does not split
the ontology schema before applying the aligning process. Rather
it starts off an anchor, collects neighboring concepts, finds aligned
pairs, and eventually produces related fragments or segments
across ontologies.

6.2. NLM Anatomical Ontology Alignment System

NLM Anatomical Ontology Alignment System [44] shares the
notion of anchor from Anchor-PROMPT and the use of shares
paths between anchors across ontologies to validate the proximity
among related terms. Therefore, Anchor-PROMPT is undoubtedly
the system to which their approach is the most closely related. This
system uses a simpler validation scheme based on paths restricted
to combinations of taxonomic and partitive relations, suitable for
the anatomical domain. Unlike Anchor-PROMPT, this approach
does not rely on path length and is therefore less sensitive to differences in granularity between ontologies. NLM does not use anchors
to exploit neighbors for alignment, whereas Anchor-Flood algorithm mainly propagates from anchor to get further alignments.

6.3. PBM based Falcon-AO

Analogy to COMA++, Falcon-AO [17] is extended with the integration of Partition-based Block Matching, PBM [19,18] to their
original ontology alignment system. PBM is introduced to cope
large ontologies. In PBM, large-scale ontologies are hierarchically
partitioned into blocks based on both the structural affinities and
terminological similarities, and then blocks from different ontologies are matched via predefined anchors.

In contrast, our algorithm does not partition the ontology into
block first. Rather, our algorithm extracts pair of segments eventu-
ally.

6.4.

iMap

iMap system [5] addresses block matching and matches
between relational schemas, but the ideas can be generalized to
other data representations. The iMAP architecture consists of three
main modules: match generator, similarity estimator, and match
selector. The match generator takes two schemas S and T as input.
For each attribute t of T, it generates a set of match candidates,
which can include both one-to-one and complex matches. The similarity estimator then computes for each match candidate a score,
which indicates the candidates similarity to attribute, t. Thus, the
output of this module is a matrix that stores the similarity scores of
target attribute, match candidate pairs. Finally, the match selector examines the similarity matrix and output the best matches for
the attributes of T. It is also based on block matching in contrast to
our system.

6.5. Some other systems

There are some other systems of aligning large ontologies or
partitioning ontologies into blocks. Some of them are introduced
here briefly. The Chimaera ontology-merging environment [24],
and an interactive merging tool based on the Ontolingua ontology editor [8] consider a limited ontology structure in suggesting
merging steps. They consider an environment for large ontolo-
gies. However, the only relations that Chimaera considered are
the subclass-superclass relation and slot attachment. The issue of
partitioning large-scale ontologies (including large class hierar-
chies) is also addressed in [12,38,40]. In [12], an efficient solution
for partitioning ontologies is provided by using -Connections. It
guaranteed that all concepts, which have subsumption relations,
can be partitioned into one block, which becomes a limitation for
ontology matching. In [38], large class hierarchies are automatically partitioned into small blocks. The background techniques
are dependency graph and island algorithm. Although the main
contribution of [40] is for ontology visualization, it also presents
a method for ontology partitioning by Force Directed Placement
algorithm. Although these tools focus to solve the problem of
large ontologies, they are quite different than our Anchor-Flood
algorithm as they partition ontologies into blocks prior to other
operations.

Moreover, without major relationship in processing unit of
alignment, there are some interesting approach to deal with multistrategy ontology alignment techniques such as oMap [37], RiMOM
[22,45] and so on. Readers can go through the references for further
information.

7. Conclusions and future work

In this paper, we described the Anchor-Flood algorithm that
can align ontologies of arbitrary size effectively, and that makes it
possible to achieve high performance and scalability over previous
alignment algorithms. To achieve these goals, the algorithm took

advantage of the notion of segmentation and allowed segmented
output of aligned ontologies. Specifically, owing to the segmenta-
tion, our algorithm concentrates on aligning only small sets of the
entire ontology data iteratively, by considering locality of refer-
ence. This brings us a by-product of collecting more alignments
in general, since similar concepts are usually more densely populated in segments. Although we need some further refinement
in segmentation, we have an advantage over traditional ontology
alignment systems, in that the algorithm finds aligned pairs within
the segments across ontologies and it has more usability in different discipline of specific modeling patterns. When the anchor
represents correct aligned pair of concepts across ontologies, our
Anchor-Flood algorithm finds segmented alignment within conceptually closely connected segments across ontologies efficiently.
Even if the input anchor is not correctly defined, our algorithm is
also capable of handling the situation of reporting misalignment
error. The complexity analysis and a different set of experiments
demonstrate that our proposed algorithm outperforms in some
aspect to other alignment systems. The size of ontologies does
not affect the efficiency of Anchor-Flood algorithm. The best running time computational complexity of our algorithm is O(N), and
the worst case is O(N2), when the taxonomy is flat. The average
number of children per intermediate nodes in FMA, Full-Galen,
OpenGalen, Mouse and Human ontologies varies from 4 to 6, which
is sufficiently small compared to their average number of concepts,
N. Therefore, the average running time computational complexity of our algorithm is O(N log(N)). In OAEI-2008 campaign, our
Anchor-Flood algorithm based system obtained the best runtime
in anatomy track.

Our future target includes strengthening the local process of
alignment by extending the alignment algorithm to handle one-to-
many (1 : n) (complex) mapping, and improving the subsumption
alignments. We have a plan to integrate WordNet sense filtering
and to integrate a process of computing semantic similarity and
discovering missing background knowledge.

Acknowledgment

This work was partially supported by the Global COE Program
Frontiers of Intelligent Sensing, from the ministry of Education,
Culture, Sports, Science and Technology.

Annex.

Our Anchor-Flood algorithm can be downloaded from the URL

http://www.kde.ics.tut.ac.jp/hanif/res/anchor flood.zip.
