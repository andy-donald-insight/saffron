Web Semantics: Science, Services and Agents on the World Wide Web 7 (2009) 235251

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

j o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / w e b s e m

Ontology matching with semantic verification
Yves R. Jean-Mary a, E. Patrick Shironoshita a, Mansur R. Kabuka a,b,

a INFOTECH Soft, Inc., 1201 Brickell AVE STE 220, Miami, FL 33131, USA
b University of Miami, Coral Gables, FL 33124, USA

a r t i c l e

i n f o

a b s t r a c t

Automated Semantic Matching of Ontologies with Verification (ASMOV) is a novel algorithm that uses lexical and structural characteristics of two ontologies to iteratively calculate a similarity measure between
them, derives an alignment, and then verifies it to ensure that it does not contain semantic inconsisten-
cies. In this paper, we describe the ASMOV algorithm, and then present experimental results that measure
its accuracy using the OAEI 2008 tests, and that evaluate its use with two different thesauri: WordNet,
and the Unified Medical Language System (UMLS). These results show the increased accuracy obtained
by combining lexical, structural and extensional matchers with semantic verification, and demonstrate
the advantage of using a domain-specific thesaurus for the alignment of specialized ontologies.

 2009 Elsevier B.V. All rights reserved.

Article history:
Received 7 March 2008
Received in revised form 13 February 2009
Accepted 22 April 2009
Available online 3 May 2009

Keywords:
Ontology
Ontology alignment
Ontology matching
Ontology mapping

1. Introduction

An ontology is a means of representing semantic knowledge
[20], and includes at least a controlled vocabulary of terms, and
some specification of their meaning [43]. Ontology matching
consists in deriving an alignment consisting of correspondences
between two ontologies [8]. Such an alignment can then be used
for various tasks, including semantic web browsing, or merging of
ontologies from multiple domains.

Our main motivation lies in the use of ontology matching for the
integration of information, especially in the field of bioinformatics.
Nowadays there is a large, ever-growing, and increasingly complex body of biological, medical, and genetic data publicly available
through the World Wide Web. This wealth of information is quite
varied in nature and objective, and provides immense opportunities to genetics researchers, while posing significant challenges in
terms of housing, accessing, and analyzing these datasets [10]. The
ability to seamlessly access and share large amounts of heterogeneous data is crucial towards the advancement of genetics research,
and requires resolving the semantic complexity of the source data
and the knowledge necessary to link this data in meaningful ways
[27]. Semantic representation of the information stored in multiple data sources is essential for defining correspondence among

 Corresponding author at: 1201 Brickell AVE STE 220, Miami, FL 33131, USA.
Tel.: +1 305 371 5111x13; fax: +1 305 371 5112.

E-mail addresses: reggie@infotechsoft.com (Y.R. Jean-Mary),

patrick@infotechsoft.com (E.P. Shironoshita), kabuka@infotechsoft.com
(M.R. Kabuka).

1570-8268/$  see front matter  2009 Elsevier B.V. All rights reserved.
doi:10.1016/j.websem.2009.04.001

entities belonging to different sources, resolving conflicts among
sources, and ultimately automating the integration process [36].
Ontologies hold the promise of providing a unified semantic view
of the data, and can be used to model heterogeneous sources within
a common framework [25]. The ability to create correspondences
between these different models of data sources is then critical
towards the integration of the information contained in them.

In the biomedical and bioinformatics knowledge domain, efforts
at deriving ontology alignments have been aided by the active
development and use of vocabularies and ontologies. The Unified Medical Language System (UMLS) is a massive undertaking
by the National Library of Medicine to create a single repository
of medical and biological terminology [4]. Release 2007AB of the
UMLS contains over 1.4 million biomedical concepts and 5.3 million concept names from more than 120 controlled vocabularies
and classifications, including the NCI Thesaurus developed by the
National Cancer Institute as a comprehensive reference terminology for cancer-related applications [18].

In this paper, we describe the Automated Semantic Matching
of Ontologies with Verification (ASMOV) algorithm for ontology
matching. Most current approaches handle only tree-like struc-
tures, and use mainly elemental or structural features of ontologies
[14]. ASMOV is designed to combine a comprehensive set of
element-level and structure-level measures of similarity with a
technique that uses formal semantics to verify whether computed
correspondences comply with desired characteristics. We begin
with a discussion of the current state of the art in ontology match-
ing. Following, we present a brief definition of the problem and
a general description of the algorithm. Next, we provide details
of the similarity measure calculations, of the semantic verifica-

Y.R. Jean-Mary et al. / Web Semantics: Science, Services and Agents on the World Wide Web 7 (2009) 235251

tion executed after an alignment is obtained, and of the conditions
for algorithm termination. Then, we provide the results of two
sets of experiments; the first set shows the accuracy of the algorithm against the OAEI 2008 benchmark tests, and the second set
analyzes the results of running the algorithm against two sets
of anatomy ontologies, using both the general-purpose WordNet
thesaurus (wordnet.princeton.edu) and the UMLS Metathesaurus
(www.nlm.nih.gov/research/umls/). Finally, the limitations of the
system and the direction of future work are discussed, and our
conclusions are stated.

2. Background and related work

Ontology matching is an active field of current research, with
a vigorous community proposing numerous solutions. Euzenat
and Shvaiko [14] present a comprehensive review of current
approaches, classifying them along three main dimensions: gran-
ularity,
input interpretation, and kind of input. The granularity
dimension distinguishes between element-level and structure-level
techniques. The input interpretation dimension is divided into syn-
tactic, which uses solely the structure of the ontologies; external,
which exploits auxiliary resources outside of the ontologies; and
semantic, which uses some form of formal semantics to justify
results. The kind of input dimension categorizes techniques as ter-
minological, which works on textual strings; structural, which deals
with the structure of the ontologies; extensional, which analyzes the
data instances; and semantic, which makes use of the underlying
semantic interpretation of ontologies.

Most work on ontology matching has focused on syntactic or
structural approaches. Early work on ontology alignment and mapping focused mainly on the string distances between entity labels
and the overall taxonomic structure of the ontologies. However,
it became increasingly clear that any two ontologies constructed
for the same domain by different experts could be vastly dissimilar in terms of taxonomy and lexical features. Recognizing this,
systems such as FCA-Merge [39] and T-Tree [12] analyze subclass
and superclass relationships for each entity as well as the lexical correspondences, and additionally require that the ontologies
have instances to improve comparison. PROMPT consists of an
interactive ontology merging tool [35] and a graph based mapping dubbed Anchor-PROMPT [34]. It uses linguistic anchors as
a starting point and analyzes these anchors in terms of the structure of the ontologies. GLUE [11] discovers mappings through
multiple learners that analyze the taxonomy and the information
within concept instances of ontologies. COMA [29] uses parallel
composition of multiple element- and structure-level matchers.
Corpus-based matching [28] uses domain-specific knowledge in
the form of an external corpus of mappings which evolves over
time. RiMOM [41] discovers similarities within entity descriptions,
analyzes instances, entity names, entity descriptions, taxonomy
structure, and constraints prior to using Bayesian decision theory in
order to generate an alignment between ontologies, and additionally accepts user input to improve the mappings. Falcon-AO [19]
uses a linguistic matcher combined with a technique that represents the structure of the ontologies to be matched as a bipartite
graph. IF-Map [22] matches two ontologies by first examining their
instances to see if they can be assigned to concepts in a reference ontology, and then using formal concept analysis to derive
an alignment. Similarity flooding [32] uses a technique of propagation of similarities along the property relationships between
classes. OLA [15] uses weighted averages between matchers along
multiple ontology features, and introduces a mechanism for computation of entity-set similarities based on numerical analysis; the
approach used in ASMOV for the calculation of similarities at a lex-
ical, structural and extensional level is similar to OLA, but affording

more flexibility to the design of similarity measure calculations for
different features.

In the particular realm of ontology matching in the biological
domain, the AOAS system developed by the U.S. National Library
of Medicine [5,44], designed specifically to investigate the alignment of anatomical ontologies, uses the concept of anchors and
implements a structural validation that seeks to find correspondences in relationships between anchors. Sambo [24] uses a similar
approach to lexical and structural matching, and complements it
with a learning matcher based on a corpus of knowledge compiled from published literature. Notably, both AOAS and SAMBO
take advantage of the part-of relation between entities, widely used
in biomedical ontologies but not defined in general languages such
as OWL; such a relation would be modeled as a property in a general
ontology.

Semantic techniques for ontology matching have received recent
attention in the literature. Semantic reasoning is by definition
deductive, while the process of ontology matching is in essence
an inductive task [14]. Semantic techniques therefore need a preprocessing phase to provide an initial seeding alignment, which is
then amplified using semantic methods. This initial seeding can
be given by finding correspondences with an intermediate formal ontology used as an external source of common knowledge
[1]. Deductive techniques for semantic ontology matching include
those used in S-Match [17], which uses a number of element-level
matchers to express ontologies as logical formulas and then uses
a propositional satisfiability solver to check for validity of these
formulas; and CtxMatch [6], which merge the ontologies to be
aligned and then uses description logic techniques to test each
pair of classes and properties for subsumption, deriving inferred
alignments.

Semantic techniques have also been used to verify, rather than
derive, correspondences. The approach by Meilicke et al. [30] uses
model-theoretic semantics to identify inconsistencies and automatically remove correspondences from a proposed alignment.
This model, however, only identifies those correspondences that
are provably inconsistent according to a description logics formula-
tion. The same authors have extended this work to define mapping
stability as a criterion for alignment extraction [31]; the approach
in ASMOV introduces additional rules that seek to find positive verification that consequences implied by an alignment are explicitly
stated in the ontologies.

3. Ontology matching algorithm

3.1. Ontology matching

In this section, we present a succinct definition of the concepts
of correspondences between entities and ontology matching; the
reader is referred to Ref. [14] for a more formal definition. An ontology O contains a set of entities related by a number of relations.
Ontology entities can be divided in subsets as follows: classes, C,
defines the concepts within the ontology; individuals, I, denotes
the object instances of these classes; literals, L, represents concrete
data values; datatypes, T, defines the types that these values can
have; and properties, P, comprises the definitions of possible associations between individuals, called object properties, or between
one individual and a literal, called datatype properties. Four specific
relations form part of an ontology: specialization or subsumption,
; exclusion or disjointness,; instantiation or membership,; and
assignment, =.

The Web Ontology Language (OWL), a World Wide Web
Recommendation, is fast becoming the standard formalism for representing ontologies. In particular, the OWL-DL sublanguage of
OWL supports those users who want the maximum expressiveness

include, for example, the correspondences a:Book, b:Volume and
a:publishedBy, b:publisher.

without losing computational completeness and decidability [38],
by restricting type separation so that the sets C, P, L, I, and T in the
ontology are disjoint. The ASMOV alignment algorithm presented in
this paper assumes that the ontologies to be aligned are expressed
in OWL-DL.

The objective of ontology matching is to derive an alignment
between two ontologies, where an alignment consists of a set of
correspondences between their elements. Given two ontologies, O
and O, a correspondence between entities e in O and e in O, which
we denote as e, e, signifies that e and e are deemed to be equiva-
lent. Consider the two example ontologies in Fig. 1; based upon the
meaning of the labels of the elements, it would be expected that
an ontology matching algorithm will find an alignment that would

3.2. ASMOV algorithm

The ASMOV process, illustrated in the block diagram in Fig. 2, is
an iterative process divided into two main components: similarity
calculation, and semantic verification. ASMOV receives as input two
ontologies to be matched, such as the two ontologies shown in the
example in Fig. 1, and an optional input alignment, containing a set
of pre-determined correspondences.

First, the similarity calculation process computes a similarity
value between all possible pairs of entities, one from each of the

Fig. 1. Example ontologies. Entities are identified by their id; where the label is different, it is shown in parenthesis. Comments for entities are not shown. Subsumption is
indicated by a directional arrow; equivalence by a bidirectional arrow, and disjointness by a dotted arrow. Cardinalities are shown next to each property. Part (a) shows the
classes and properties of the ontology; part (b) shows individuals belonging to each ontology. The ontologies themselves and graphical notation are based on an example in
[14], modified to illustrate multiple inheritance, compound property domains, disjointness, multiple cardinalities, and individual matching.

Y.R. Jean-Mary et al. / Web Semantics: Science, Services and Agents on the World Wide Web 7 (2009) 235251

Fig. 1.

(Continued ).

two ontologies, and uses the optional input alignment to supersede
any calculated measures; the details of this calculation, including the description of the different attributes examined for each
pair of entities, are provided in Section 4. This process results in
a similarity matrix containing the calculated similarity values for
every pair of entities; partial views of this matrix for the ontologies in Fig. 1 are shown in Table 1. From this similarity matrix, a
pre-alignment is extracted, by selecting the maximum similarity
value for each entity. For example, a:Product, b:Volume has the
highest value for a:Product, while a:Book, b:Volume has the
highest value for b:Volume; both are included in the pre-alignment.
This pre-alignment is passed through a process of semantic veri-
fication, detailed in Section 5, which eliminates correspondences

that cannot be verified by the assertions in the ontologies, resetting the similarity measures for these unverified correspondences
to zero. For example, the potential correspondence a:Science,
b:Recording is eliminated due to the existence of a:Book,
b:Volume, because a:Science is a subclass of a:Book, while
b:Recording is not asserted to be a subclass of b:Volume.

This process results in a semantically verified similarity matrix
and alignment, which are then used to evaluate a finalization condition as detailed in Section 6. If this condition is true, then the
process terminates, and the resulting alignment is final. Fig. 3
shows the alignment obtained from running the ASMOV process
over the ontologies in Fig. 1, which includes expected correct correspondences in terms of our interpretation of the ontologies,

Fig. 2. ASMOV block diagram.

Table 1
Partial similarity matrices for (a) classes, (b) properties, and (c) individuals after iteration 1 for example in Fig. 2.

(a)

Product
Book
Pocket
Science
Popular
Textbook

(b)
id
price
createdBy
authoredBy
title
publishedBy

(c)

myourcenar
acamus
brussel

Item

Volume

subject

Essay

Literature

Narrative

publisher

creator

Novel

Biography

Autobiography

writtenBy

title

Bertrand Russel

Albert Camus

Marguerite Yourcenar

such as a:Book, b:Volume, but also includes other correspondences such as a:price, b:year that do not agree with a human
interpretation of their meaning. The accuracy of ASMOV has been
evaluated against a set of well-established tests, as presented in
Section 7.

4. Similarity calculations

The ASMOV similarity calculation is based on the determination
of a family of similarity measures which assess the likelihood of
equivalence along three different ontology kinds of input as classified in [14]. The nature of this calculation process is similar to the
approach used in OLA [15], both in its use of a normalized weighted
average of multiple similarities along different ontology facets, and
especially in its use of similarities between entity sets. OLA, how-
ever, uses a graph structure to represent the ontology, and performs
its set of calculations based on entity sets identified from this graph;
ASMOV uses a more diversified approach, working from the OWLDL ontology directly and proposing ad-hoc calculations designed
specifically for each ontology facet. In addition, the ASMOV process
is made more tolerant of the absence of any of these facets in the
ontologies to be matched, by automatically readjusting the weights
used in the weighted average calculation. ASMOV also is designed
to accept an input alignment as a partial matching between the
ontologies.
At each iteration k, for every pair of entities e  O, e  O, ASMOV
obtains a calculated similarity measure k(e,e), as a weighted average of four similarities:
 a lexical (or terminological) similarity, sL(e,e), using either an
external thesaurus or string comparison;
 two structural similarities:
 a relational or hierarchical similarity sH
specialization relationships in the ontology; and
 an internal or restriction similarity sR
established restrictions between classes and properties.
 an extensional similarity, sE
in the ontology.

k (e, e), which uses the
k(e, e), which uses the
k(e, e), which uses the data instances

The lexical similarity does not vary between iterations and is
therefore calculated only once, during pre-processing. Consider

F ={L,E,H,R} to be the set of similarity facets used in the calculation;
k(e,e) is computed as

(wf sf

k(e, e
))

wf

f  F

, if e and e

are the same type of entity;

f  F


k(e, e

)=

0.0, otherwise

where wf are weights assigned to each of the features in the calcu-
lation. Using fixed weights presents a problem, as noted in [2]: if a
given facet f is missing (e.g., if an entity in an ontology does not contain individuals), the corresponding similarity value sf
k is marked as
undefined, and its weight wf is changed to zero.
In addition, ASMOV accepts an optional input alignment Ao as a
set of correspondences, Ao =e,e, where each correspondence in
Ao has a confidence value n0(e,e). This input alignment is used
to supersede any similarity measures, defining a total similarity

measure sk(e,e) as follows:
sk(e, e

e, e

n0(e, e), if
k(e, e), otherwise

 A0

) =

(1)

(2)

The initial calculated similarity value between entities, 0(e,e),
is given by the lexical similarity between the entities multiplied by
the lexical similarity weight. The total similarity measures for every
possible pair of entities e in O and e in O define a similarity matrix
Sk ={sk(e,e)} for each iteration k.

4.1. Lexical similarity

The lexical feature space consists of all the human-readable
information provided in an ontology. Three such lexical features are
considered in OWL ontologies: the id, the label, and the comment.

4.1.1. Lexical similarity for labels and Ids
Let the two labels being compared be l and l, belonging respectively to entities (classes or properties) e and e. ASMOV is capable
of working with or without an external thesaurus; if an external
thesaurus is not used, only string equality is used as a measure. Let
	 denote a thesaurus, and syn(l)the set of synonyms and ant(l) the

Y.R. Jean-Mary et al. / Web Semantics: Science, Services and Agents on the World Wide Web 7 (2009) 235251

set of antonyms of label l; the lexical similarity measure between
the labels of e and e, sL(e,e), is then given as follows:

(3)

the label l, by dividing a string at punctuation and separation marks,
blank spaces, and uppercase changes; when at least one of the labels
to be compared is not found in the thesaurus, and if they are not
exactly equal, the lexical similarity is computed as the number of
overlapping tokens.
ASMOV optionally finds a lexical similarity measure between
identifiers of entities e and e, sid(e,e), in the same way as with
labels, except that the Lin function is not used; in case that the
identifiers are not found to be synonyms or antonyms, the number
of overlapping tokens is computed. In principle, identifiers in OWL
are meant to be unique, and do not necessarily have a semantic
meaning [38], and thus the similarity measurement is made to be
more restrictive.
The lexical similarity measure sL(e,e) is designed to privilege
labels (and ids) that can be found within the thesaurus used by the


sL(e, e

) =

1.0, if l = l
0.99, if l  syn(l)
0.0, if l  ant(l)
Lin(l, l), if l     l     l / syn(l)
tok(l)  tok(l)
max(|tok(l)|,|tok(l)|)

, otherwise

The similarity measure for synonyms is set slightly lower than
the measure for actual string equality matches, in order to privilege
exact matching between terms. Lin(l,l) denotes the informationtheoretic similarity proposed by Lin in [26]; it provides a good
measure of closeness of meaning between concepts within a the-
saurus. The tokenization function tok(l) extracts a set of tokens from

Fig. 3. Example alignments. Part (a) shows the alignment obtained by ASMOV between classes and properties of the ontology; part (b) shows alignment between individuals.

Fig. 3.

(Continued ).

system; thus, it avoids using other commonly used metrics such
as string edit or n-grams. While this design choice results in less
tolerance for spelling mistakes, on the other hand it avoids influencing the matching process with similarities between identifiers
that happen to share the same letters or n-grams. Nevertheless,
as part of our future work, we are exploring the inclusion of non-
language-based techniques within a weighted average with the
thesaurus-based measure.

Examples of lexical similarity measures for both labels and ids
for some classes in the ontologies in Fig. 1, are provided in Table 2,
where the results have been calculated using WordNet as the the-
saurus.

4.1.2. Lexical similarity for comments

Comments are processed differently, since they usually consist
of a phrase or sentence in natural language. In this case, we compute
the similarity between the comments of entities e and e, sc(e,e), as
a variation of Levenshtein distance but applied to tokens. First, an
ordered set of tokens is obtained from the comment of each of the
entities; then, we calculate the number of token operations (inser-
tions, deletions, and substitutions of tokens) necessary to transform
one of the comments into the other. Let x, x be the comments of e, e
respectively, and let op(x, x) denote the number of token operations

needed, and tok(x) denote the number of tokens in a comment,
sc(e, e

op(x, x)

) = 1 

max(|tok(x)|,|tok(x)|)

(4)

Consider for example the comments for classes a:Book
and b:Volume. The comment for a:Book is A written work
or composition that has been published, printed on
pages bound together.
cal object consisting of a number of pages bound
together. Each of these phrases is tokenized, where a:Book
results in 14 tokens and b:Volume in 11 tokens. The total number
of token operations necessary to transform a token into another is
10, 7 substitutions and three insertions (or deletions). The lexical
similarity for these two comments then is 1 (10/14) = 0.286.

For b:Volume,

A physi-

is

it

4.1.3. Lexical similarity measure calculation

The lexical similarity measure is calculated as the weighted
average of the label, id, and comment similarities. The weights
used in this calculation have been determined experimentally, as
label weight wlabel = 0.5, id weight wid = 0.3, and comment weight
wcomment = 0.2. From the results given above, then, the lexical similarity measure between a:Book and b:Volume can be calculated
as 0.849.

Table 2
Examples of lexical similarity calculations.

a:

Book

Book
Male

b:

Volume
