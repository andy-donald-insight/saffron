Web Semantics: Science, Services and Agents on the World Wide Web 7 (2009) 2839

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

j o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / w e b s e m

Specifying and enforcing high-level semantic obligation policies
Zhen Liu, Anand Ranganathan, Anton Riabov

IBM T.J. Watson Research Center, Hawthorne, NY 10532, USA

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 28 August 2007
Received in revised form 11 December 2007
Accepted 17 February 2008
Available online 9 April 2008

Keywords:
Ontology
Policy
Automatic workflow composition
High-level events
Semantic policy language

1. Introduction

Obligation policies specify management actions that must be performed when a particular kind of event
occurs and certain conditions are satisfied. Large scale distributed systems often produce event streams
containing large volumes of low-level events. In many cases, these streams also contain multimedia data
(consisting of text, audio or video). Hence, a key challenge is to allow policy writers to specify obligation
policies based on high-level events, that may be derived after performing appropriate processing on raw,
low-level events. In this paper, we propose a semantic obligation policy specification language called Eagle,
which is based on patterns of high-level events, represented as RDF graph patterns. Our policy enforcement
architecture uses a compiler that builds a workflow for producing a stream of events, which match the
high-level event pattern specified in a policy. This workflow consists of a number of event sources and
event processing components, which are described semantically. We present the policy language and
enforcement architecture in this paper.

 2008 Elsevier B.V. All rights reserved.

Obligation policies define actions that must be performed by
some entity when a certain kind of event occurs, provided certain conditions are satisfied. These policies are often specified in
the form of ECA (or eventconditionaction) rules. A key challenge
in the enforcement of obligation policies is detecting the relevant
events of interest.

Large scale distributed and pervasive computing systems often
produce streams of different kinds of events. Such events include
logging and audit messages from different applications and computing nodes, events from different kinds of sensors (such as
temperature, humidity and RFID sensors) and events about user
activity and system reconfiguration. These systems also often produce streams of unstructured data (such as text and multimedia)
from various sources like surveillance video cameras and emergency radio broadcasts. Many of these events are very low-level
and high-volume, and often require further processing to extract
useful information from them. Event processing techniques include
filtration, aggregation, classification, correlation, multimedia data
analysis, etc., and they help in producing lower volumes of highlevel events. Policies based on such high-level events are easier to
specify and maintain. Hence, there is a real motivation to allow policy specification in terms of high-level events rather than low-level,
raw events.

 Corresponding author. Tel.: +1 914 784 6257.
E-mail addresses: zhenl@us.ibm.com (Z. Liu), arangana@us.ibm.com

(A. Ranganathan), riabov@us.ibm.com (A. Riabov).

1570-8268/$  see front matter  2008 Elsevier B.V. All rights reserved.
doi:10.1016/j.websem.2008.02.002

Existing policy specification languages and enforcement architectures do not allow for the derivation of high-level events
from low-level events (particularly events containing unstructured
data), and the specification of policies based on these high level
events (see Section 9 for a discussion of related work). These systems assume that the events are directly consumable by the
policies, i.e. the events that these policies depend on are somehow
available in the system. However, in many systems, there is often
a gap between the kinds of policies that business users would
like to specify and the kinds of policies that can actually be written
in the system. Hence, there is a need for event processing in these
systems to derive high-level events from raw events.

In this paper, we propose a high-level, semantic obligation policy specification language called Eagle, that allows policy writers
to specify policies based on high-level, semantic event patterns. An
event pattern is represented as an RDF graph pattern, that describes
a conjunctive query in Description Logics [3]. The policies also specify conditions to be checked and actions to be performed when
there is an event in the system that matches the high-level event
pattern. These events, which flow in streams, are represented using
an RDF graph, which describes a set of OWL facts (ABox assertions)
[21]. The matching of events to high-level event patterns is based
on Description Logic (DL) reasoning. The policies use terms defined
in domain ontologies, that are expressed in OWL. This semanticsbased approach to specifying policies makes the policy language
highly expressive, allows the specification of high-level policies and
enables all policies in the system to use a common vocabulary, that
is defined in domain ontologies.

In order to enforce policies in Eagle, the system needs to produce appropriate high-level events from one or more low-level

events. We model the processing of events using a processing graph
(or workflow) of components, that are deployed in a distributed
system, and that can extract meaningful information from streaming events. A processing graph is a directed acyclic graph (DAG)
consisting of event sources and processing elements (PE) that are
inter-connected by event streams. Event sources bring in raw events
to be analyzed. PEs are reusable software components that can perform various kinds of operations on events to produce new, derived
events. Such a component-based programming model has various
advantages including reusability and scalability.

A key challenge lies in the construction of the processing graphs
that can produce high-level events specified in a policy. There may
be large numbers of disparate event sources and processing elements to choose from, and this set can change dynamically as new
sources are added or new PEs are developed. Hence, we cannot
expect the policy writer to craft these processing graphs manually.
In fact, it would be preferable to allow policy writers to be unaware
of the kinds of low level events in the system and the kinds of event
processing that can be done by the system.

Our policy enforcement framework tackles this challenge by
automatically composing a processing graph for a given policy. It
takes a stream-centric approach by considering all events to be part
of streams. The notion of a stream, which is defined as an aggregation of events, is useful for routing certain kinds of events between
different components in a processing graph. In our framework,
event streams are associated with semantic annotations, expressed
as OWL facts and represented using RDF graphs.

Our enforcement framework consists of two parts: a policy compiler and a set of Stream Monitors. The policy compiler builds
processing graphs for producing a stream of high-level events,
which match the event patterns specified in the policies. It makes
use of descriptions of different event sources and PEs in terms of
the semantics of the event streams they consume and produce.
The compiler uses classical AI planning methodologies, reasoning based on Description Logic Programs (DLP) [15](which is
a tractable fragment of Description Logics) and multi-objective
optimization techniques to produce the processing graphs. The
Stream Monitors actually enforce the policy. These components
monitor the events on the high-level streams, and if certain conditions are satisfied, they perform actions by invoking various
services.

Our work has several similarities with the areas of stream processing and complex event processing. These areas also focus on
processing streaming data for the purposes of extracting useful
information or detecting patterns (see Section 9 for a description
of some of these systems). However, our work is different from
existing systems in these areas since we allow the use of arbitrary
operators for processing events, and the composition of workflows
that can analyze both structured and unstructured data.

The key contributions of our semantics based approach for spec-

ifying and enforcing obligation policies include:
 Mechanism for high-level policy specification. Policy-writers can
specify policies using a familiar vocabulary. They do not need
to be concerned about the format of low-level events and of the
functionalities of different operators or processing elements. This
makes it easier to write and maintain policies.
 Novel policy enforcement mechanism through workflow assembly.
The high-level policies in Eagle are enforced through an automatically assembled workflow of arbitrary operators that analyze the
raw events entering the system to produce the high-level events
described in the Eagle language. This is a novel aspect compared
to other policy systems, and allows the use of an extensible set of
operators for analyzing events. In addition, the use of a planning
approach to build a workflow for event detection allows adap-

tation of the enforcement mechanism to the current state of the
system, especially on the currently available sources of events.
 Use of ontologies to establish a common vocabulary between myriad users in a policy enforcement system. We use ontologies in a
novel manner to allow different users (policy writers, component
developers, event source administrators, etc.) to share a common
vocabulary. This is necessary for the scalable management and
evolution of policy enforcement systems.

We have deployed this policy enforcement framework on top
of the System S stream processing system [16]. In this paper, we
present the semantic policy model and the framework. We do not
consider the problem of detecting policy action conflicts in this
work. The paper is organized as follows. In Section 2, we present
an overview of the system and a sample application. In Section 3,
we describe the ontologies that form the backbone of our policies.
In Section 4, we present the Eagle language. In Sections 5 and 6,
we describe the semantic model of streams and the policy enforcement framework, respectively. In Sections 7 and 8, we evaluate our
compiler and discuss various aspects of our approach. Sections 9
and 10 review related work and conclude.

2. Overview

We illustrate the operation of our system through an example
scenario. Consider the customer service department of a fictional
multi-national company named EGS (Enterprise Global Services).
Events may be generated based on incoming and outgoing service
calls, call-transfers, dropped calls, system load and performance,
etc. In addition, company policy allows some types of information
to be extracted from the corporations telecommunications traffic
(your call may be monitored for quality assurance). Hence VoIPbased calls may be analyzed to determine customer satisfaction,
employee courtesy, call quality, etc. Managers, and other personnel
in the quality control department, may create policies that specify
actions to be performed if certain kinds of events occur (e.g., if there
are many dropped calls, if the average call waiting time is high, if
an employee is discourteous, etc.). Many of these events are high-
level, and can only be produced after performing some processing
on the raw events produced by the system.

Let us consider an example policy which states if an employee is
found to be discourteous during any call, then a notification should
be sent to his manager and the employee should not be allowed to
take further calls. In order to enforce this policy, the system needs to
analyze the VoIP network to identify the employee in a call, determine the courtesy level of the employees (for example, by analyzing
the tone of the voice and the words used) and trigger actions if the
courtesy level is below a threshold. Fig. 1 shows a portion of the
processing graph that was constructed by our policy compiler. The
graph includes a single event source, EVAS (EGS VoIP Audio Seg-
ment) connected to the EGS VoIP network. It also includes various
PEs that extract VoIP channel information from VoIP RTP packets
(SPA), filter periods of silence (VBF), perform speaker identification
(PSD), dig up employees profile (DUP), analyze tone and courtesy
level (SCF), etc. The result of this processing graph is a stream that
contains events associating an employee with the name of his manager and a real-time courtesy level. The Stream Monitor (SM) for
this policy monitors these events and triggers actions by invoking
various remote services if the courtesy level is below a threshold.
For example, it invokes an email service to send an email to the
employees manager; and it invokes a call router service to bar the
employee from taking further calls.

To construct such a processing graph, our policy compiler uses
formal descriptions of PEs, sources and policies, expressed as OWL-

Z. Liu et al. / Web Semantics: Science, Services and Agents on the World Wide Web 7 (2009) 2839

Fig. 1. Sample application (EGS).

based semantic graphs. It uses enhanced AI planning technologies
to compose these graphs [27]. It also makes use of DLP reasoning to
compare the requirements of PEs and policies to events produced
by other PEs and sources. The use of reasoning allows components
and streams to be matched to one another based on semantics and
not just syntax. If more than one alternative processing graph can
satisfy an inquiry, the planner selects optimal graphs according to
various quality metrics. The generated processing graphs are finally
deployed in the System S Stream Processing Core [16], which is a
scalable distributed runtime for data stream processing. In addition,
Stream Monitors that enforce the policy are instantiated.

There are a number of advantages in using a semantics-based,
stream-centric approach to policy specification and enforcement.
These include:
 High expressiveness. The use of OWL/RDF allows us to represent complex inter-relationships between different entities in an
event pattern in a policy.
 Composition power. Since policies, as well as event sources and
PEs are described using expressive semantic models, this provides
more information to the policy compiler for generating plans (or
processing graphs) that produce high-level events specified in the
policies. In addition, the use of OWL allows the compiler to perform description logic reasoning and also use domain knowledge
in ontologies to generate the plans. The notion of streams helps
in representing and deploying such processing graphs.
 Formal definition of all terms in ontologies. All policies and descriptions of event sources and PEs are based on one or more
ontologies, which are written in OWL. These ontologies define all
terms, in a certain domain of interest, using a formal logic. Thus,
events and policies have well-defined and clear semantics (or
meaning). This helps reduce the possibility of ambiguity of what
a policy means and helps increase interoperability across different parts of a system. While there are many existing information
models used for defining policies such as IETF/DMTF [23,13],
ontologies allow establishing more formal semantics with terms.
In our system, we assume that all components in the system use
a common set of domain ontologies. The mapping or integration
of heterogeneous ontologies is a separate problem, which is out
of our scope in this work.

OWL ontologies describe concepts (or classes), properties and
individuals (or instances) relevant to a domain of interest. For
the EGS example, we draw on several ontologies that describe
domain independent concepts such as Physical Thing, Person and
Location, as well as domain-specific concepts like ServiceVoIPCall,
and Employee. Concepts may be related via subClassOf relation-
ships. A property has a domain and a range. For instance, the domain
of atLocation is PhysicalThing and the range is Location. OWL Object
Properties (like atLocation) have a range which is a concept, and
OWL Datatype properties (like hasName) have a range which is an
xsd datatype. Individuals (like EGS and Bob) belong to one or more
concepts and are related to one another, or to literal values (like
the string Bob Roberts), through various properties. Fig. 2 shows
a portion of the ontologies.

Now, we review some definitions from RDF and OWL, which we
shall use when we define policies and the semantic model of event
streams, event sources and PEs.

3.1. RDF term

Let U be the set of all URIs. Let RDFL be the set of all RDF Literals
(which are data values, that may be typed). The set of RDF Terms,
RDFT, is U  RDFL.

3.2. RDF triple

An RDF triple contains three components: a subject, a predicate
and an object. The subject and predicate are URIs, while the object
is an RDF term.

3.3. RDF graph

An RDF graph is a set of RDF triples.

3.4. OWL axiom

An OWL axiom (or TBox axiom) is a statement in the ontology
that gives information about classes and properties. This information includes subclass and subproperty relationships, whether
a certain property is transitive, symmetric or functional, or the
inverse of another property, restrictions on the values of a property,
etc. OWL axioms may be represented as RDF triples.

3. Ontologies

3.5. OWL fact

Ontologies form the backbone for Eagle policies by providing
a formal description of the kinds of salient entities and how they
are related. Policies use terms defined in ontologies to describe the
high-level event patterns.

An OWL fact (or ABox assertion) is a statement about individuals,
in the form of classes that an individual belongs to or relationships
between individuals. OWL facts may also be represented as RDF
triples. An example is (EGS a Company). The property a indicates

Fig. 2. Example ontology in EGS domain.

that the subject, EGS, is an individual of type Company, an OWL
concept.

4. The Eagle policy specification language

An Eagle policy is defined on a pattern of events. An event
pattern describes an equivalence class of events which can trigger the execution of the policy. Policy actions are performed
based on specific events in the system which match the event
pattern.

A policy in Eagle consists of an event pattern and a set of
conditionaction pairs. The event pattern describes the data that
must be contained in the event as a set of variables. It also describes
additional semantics on this data as a graph-pattern. Conditions are
described as a conjunction of expressions, where an expression is
either a constraint on a value that appears in a matched event, or
a boolean-valued result of invoking a method on a service. Actions
are specified as invocations to services with some parameters. The
general syntax of Eagle is shown below.

An example of an Eagle policy is shown below. This policy is
defined on an event pattern that contains the name of an employee,
the email address of his manager and the current courtesy level
of the employee in a VOIP call (see Fig. 3). The graph pattern is
described in N3 [7] format (the : in front of some of the terms
represents the default namespace). The policy states that if the
courtesy level is less than 0.4, then an instant message alert should
be sent to the manager; and if the courtesy level is less than 0.2,
then the employee should not be allowed to answer any further
customer service calls.

Fig. 3. Event pattern example.

Z. Liu et al. / Web Semantics: Science, Services and Agents on the World Wide Web 7 (2009) 2839

as an RDF graph. The metadata provides rich information about the
meaning of the events on the stream, together with its format.

The semantic metadata description of a stream is in terms of the
semantics of a typical (or exemplar) event in the stream. It describes
the data present in the typical event and any constraints that are
satisfied by the data in terms of a graph of OWL facts.

5.1. Exemplar individuals and literals

In order to describe the semantics of a stream of events, we introduce the notion of exemplar individuals and literals. An exemplar
individual is a member of the set EI where EI is infinite and EI  U. In
an OWL ontology, it is represented as belonging to a distinguished
concept called Exemplar. An exemplar literal is a member of the set
EL where EL is infinite and EL  RDFL. In an OWL ontology, they are
represented using the convention that the Unicode string of the
literal begins with  .

Exemplar individuals and literals represent existentially quantified variables. In a particular event, they may be substituted by
a value that belongs to the set of non-exemplar individuals (i.e.
U  EI) or non-exemplar literals (i.e. RDFL  EL). In this paper, we
represent all exemplar individuals and literals with a preceding  .

5.2. Stream

The semantic description of a stream identifies the data present
in a typical event (on the stream) and any constraints on the data,
expressed using a graph of OWL facts. The semantic description of
a stream is a 3-tuple of the form (SN,SD,SG) where
 SN is the name of the stream. The stream name is represented as
a URI, i.e. SN U
 SD is the set of data items contained in the typical event in the
stream. The data in the typical event is represented as exemplar
individuals and exemplar literals. That is, SD 2EIEL
 SG is an RDF graph that describes the semantics of the data on the
typical event in the stream. The graph describes the constraints
associated with all the exemplar individuals and exemplar literals
that are contained in the stream.

MgrEmail 1,

which contains the exemplars

An example of a stream is the EmpCurt VoIP CallStream (Fig. 4),
Department 1, MgrName 1,
partly
described by the example stream graph. Each event in the stream
contains elements that satisfy all the constraints described on the
exemplars.

CourtesyLevel 1and

EmployeeName 1,

Note that the actual event is not in the form of an RDF graph.
It contains the different elements in some format (like XML, Java
objects, audio or video format, etc.). For example, the stream
description states that the format of Department 1 is a Java class
(com.egs.Dept). In our system, each event in a stream is represented
as a serialized Java object. The class definition for an event in the
example stream is shown below:

Also, note that a stream description only contains OWL facts,
i.e. assertions about different individuals (exemplar and non-
exemplar) and how they are related. It does not define new concepts
or properties, or extend the definitions of existing concepts and
properties. A stream description only uses concepts and properties
defined in the ontologies, such as the one in Fig. 1.

We now describe the elements of a policy formally.
A variable is a member of the set V where V is infinite and disjoint

from RDFT.

A triple pattern is an RDF triple, containing a subject, predicate
and object, where either the subject or the object is a variable. An
example is (?Employee employedBy EGS).

A graph pattern is a set of Triple Patterns. An example graph
pattern appears in the Where clause of the sample policy described
earlier. Note that a graph pattern can be used to represent a conjunctive query in Description Logics. We have borrowed the notion of a
graph pattern from SPARQL [26], a standard RDF query language.

An event pattern is a 2-tuple of the form EP(VS,GP) such that

 VS is a set of variables representing the data that must be contained on a matching event. VS 2V
 GP is a graph pattern that describes the semantics of the data in
the event.

A value constraint is a boolean-valued expression of variables

and RDF Terms, e.g., (?CurtLevel 0.4)

A boolean method is a remote method invocation on a service
that returns either true or false. The method may have parame-
ters, which could be either variables or constants. An example is
IMServ.isOnline(?MgrEmail)

A condition is a conjunction of value constraints and boolean

methods.

An action is a remote method invocation on a service. The
method may have parameters, which could be either variables or
constants. An example is IMServ.sendAlert(?MgrEmail,?EmpName)

A policy is a 3-tuple, (PN,EP,C, A), where

 PN is a string that represents the name of the policy
 EP is an event pattern
 C, A represents a set of conditionaction pairs. All variables used
in the definitions of conditions and actions must appear in the
variable set of EP.

5. Descriptions of event streams

Our policy enforcement

framework use a stream-centric
approach. This allows it to connect different event sources and PEs
in a processing graph using streams. Descriptions of event sources
and PEs are in terms of streams they consume and produce.

A stream carries zero or more events that satisfy a common set
of constraints. These constraints are expressed in the stream meta-
data, which is described semantically, using OWL facts represented

Fig. 4. Example semantic description of stream.

Exemplar individuals and literals in a stream-graph act as
existentially quantified variables. In a specific event, they are
replaced by non-exemplar individuals or literals. For example,
a specific event may contain a specific courtesy level, say 0.6,
for a specific employee JDoe123. Fig. 5 describes the semantics of an example event. The event itself may be represented
as a serialized instance of the EmployeeCourtesy class defined
earlier.

5.3. Matching a stream to an event-pattern

Policies are defined in terms of event patterns. Our policy compiler works by generating a processing graph (or workflow) that
produces a stream of events which match this event pattern. We

now describe what is required for a stream of events to match an
event pattern.

In order to define a match, we first define a pattern solution,
which expresses a substitution of the variables in an event pattern.
We then define the conditions for a match in terms of the existence
of an appropriate pattern solution.

5.3.1. Pattern solution

A pattern solution is a substitution function (
 : V  RDFT) from
the set of variables in a graph pattern to the set of RDF terms. Variables may also be mapped to exemplar individuals and exemplar
literals. For example, some of the mappings defined in a possible definition of 
 for the graph pattern, in the example policy is:

(?EmpName)= EmployeeName 1, 
(?CurtLevel)= CourtesyLevel 1,
etc.

Fig. 5. Example of an event in the stream.

Z. Liu et al. / Web Semantics: Science, Services and Agents on the World Wide Web 7 (2009) 2839

The result of replacing a variable, v is represented by 
(v). The
result of replacing all the variables in a graph pattern, GP, is written
as 
(GP).

5.3.2. Condition for match

Consider an event-pattern EP(VS, GP), and a stream, S(SN,SD,SG).
We define the event pattern, EP to be matched by the stream, S,
based on an ontology, O, if and only if there exists a pattern solution,

, defined on all the variables in GP, such that following conditions
hold:

 
(VS)  SD, i.e. the stream contains at least those elements that
the pattern says it must contain.
 SG  O  
(GP) where O is the common ontology, and  is an
entailment relation defined between RDF graphs. In other words,
we see if it is possible to infer the substituted graph pattern from
the stream-graph according to some well defined logical reasoning framework. The entailment may be based on RDF [25] or
on different fragments of description logics such as OWL-Lite,
OWL-DLP, OWL-DL [3], etc.

The first condition describes the syntactic requirements of
matching, i.e. the stream contains all the data elements specified in the pattern. The second condition describes the semantic
requirements of matching, i.e. the semantic properties of the data
elements (specified as a set of OWL ABox assertions) match the
semantic requirements specified in the policy (specified as a conjunctive query in Description Logic along with, possibly, a set of
value constraints on literal values).

We represent this match as S
EP to state that stream S matches
event pattern, EP with a pattern solution, 
. One way of looking at the above definition is that the stream should have at
least as much semantic information as described in the policy.
Fig. 6 shows how the EmpCurt VoIP CallStream might match the
EmployeeCourtesyEventPattern. The bold arrows show the variable
substitutions. In order to make the match, some DLP reasoning
based on subclass and inverse property relationships must be done.
The dotted arrows denote the inferred relationships. For example,
the tookCall relationship on Employee 1 is inferred, since tookCall
is declared to be an inverse property of ofEmp. Once the inferences
are done, it is clear to see that the graph on the right is a subgraph
of the graph on the left; hence a match is obtained.

Fig. 7. Policy enforcement architecture. Policy writers specify Eagle policies, which
get compiled by the policy compiler to generate workflows. These workflows produce the high level events of interest as specified in the policy, and are deployed
on the Stream Processing Core (SPC) through the Job Manager. The policy compiler
makes use of OWL ontologies and component (PE and event source) descriptions.
The deployed workflows include Stream Monitors that watch for events of interest,
evaluate conditions in the policy and take the specified actions if the conditions are
satisfied.

6. Policy enforcement

Policy enforcement consists of two major steps: policy compilation and stream monitoring. Policy compilation is the process of
producing a processing graph that can generate a stream of events
that match the pattern defined in the policy. Stream monitoring is
the process of monitoring the events on the matched stream and
performing actions if certain conditions are true.

6.1. Policy enforcement architecture

Fig. 7 shows the overall policy enforcement architecture. Policy writers specify policies in Eagle that include the high-level
semantic event pattern specification along with conditions to be
evaluated on matching high-level events and actions to be performed if the conditions are satisfied. The policy compiler compiles
the high-level semantic event patterns to generate a workflow of
event sources and stream processing elements. The compiler makes
use of formal semantic descriptions of components (PEs and event
sources) to generate the workflow. Both the queries and the component descriptions refer to concepts, properties and instances

Fig. 6. An example of matching a stream to an event pattern. The left side shows the RDF graph representing the stream, specifying a set of OWL facts (ABox assertions)
describing the data elements in the stream and the semantic properties of these data elements. The right side shows the RDF graph pattern representing the event pattern
in the policy specification. This RDF graph pattern specifies a conjunctive query in Description Logic (DL) that describes the required properties on the event. The bold lines
in blue show substitutions of variables in event pattern by terms in stream description. Dotted lines in red show inferences performed on stream description.

defined in a collection of OWL ontologies that capture domain
knowledge. The OWL ontologies and component descriptions are
stored in a Knowledge Repository. In our system, the Knowledge
Repository is implemented by Boca [28], an open-source RDF store
that provides support for multiple users to access and author
ontologies and component descriptions in a collaborative manner.
The constructed workflows are deployed by a Job Manager and executed on the Stream Processing Core (SPC) [16], running in a cluster
of machines. The Job Manager makes resource allocation decisions
and chooses which SPC nodes to deploy different PEs on.

A special kind of PE, called Source PE (denoted by S in Fig. 7),
accesses data from an event source, using the protocol supported by
the source. Event sources in our system can include different kinds
of sensors, multimedia sources (audio, video or image sources), system and application components, etc. The Source PE also packages
the raw data (possibly containing multimedia) into internal Stream
Data Objects (SDOs). Intermediate PEs perform different kinds of
processing on the data. The workflows deployed in the SPC produce
the streams of desired high level events as desired by Eagle poli-
cies, which are monitored by Stream Monitors (denoted by SM in
Fig. 7). The Stream Monitors evaluate the policy conditions on the
events on the stream and take specified actions if the conditions
are satisfied. The actions in our system are typically in the form of
invocations to different services.

6.2. Model of event sources and PEs

A event source is described as producing a single event
stream. An example event source is shown in Fig. 8. This
source produces a stream that contains an exemplar called
: VoIP AudioSegment 1. The semantic graph describes the constraints on : VoIP AudioSegment 1.

PEs are described in terms of the kinds of streams they require
as input and the kinds of streams they produce as output. They
are modeled in terms of graph transformations. The inputs and
outputs are described using graph patterns. The basic PE model
is that it takes m input graph patterns, processes (or transforms)
them in some fashion and produces n output graph patterns. An
example PE is shown in Fig. 9. This PE requires an input stream

which contains ?VoIP CallAudioSegment 1that is associated with
various constraints. The format of ?VoIP CallAudioSegment 1is an
RTP packet. The PE extracts the start time and call channel information from the incoming RTP packet and computes the end time from
the duration. It then puts this information along with the original
?VoIP CallAudioSegment 1 on the output stream.

The semantic description of a PE gives a general, application-
independent, description of the kinds of streams it requires and the
kinds of streams it produces. In a given application (or processing
graph), a specific set of input streams may be connected to the PE.
As a result, the PE produces a specific set of output streams.

6.3. Policy compilation

Fig. 10 illustrates the architecture of the policy compiler. The
compiler uses semantic descriptions of event sources and PEs to
compose a processing graph using planning techniques. The planning is based on SPPL (Stream Processing Planning Language) [27],
a specialized language for describing stream-based planning tasks.
There are two phases in this compilation process: pre-reasoning,
that takes place off-line, before any policies are compiled,
and semantic planning of individual Eagle policies. In the offline pre-reasoning phase, SPPL domain information (comprising
descriptions of sources and PEs) is generated, after translating and
performing DLP-reasoning on semantic descriptions of PEs and
sources. The domain is then persisted and re-used for multiple poli-
cies. During the semantic planning phase, the policy is parsed, and
OWL ontologies are used to validate the policy. The event pattern
in the policy is translated into a definition of the SPPL problem; i.e.
it becomes a planning goal. The planner produces a plan (or work-
flow) consisting of actions that correspond to PEs and sources. The
plan is constructed by recursively connecting components to one
another based on their descriptions until the goal stream is pro-
duced. This plan is then deployed in the System S stream processing
system.

If the number of sources and PEs is large, there may exist multiple alternative processing graphs for the same policy. Our planner
uses a number of metrics to compare processing graphs, and returns
only the processing graphs that are Pareto optimal, i.e., cannot be

Fig. 8. Event source semantic description.

Z. Liu et al. / Web Semantics: Science, Services and Agents on the World Wide Web 7 (2009) 2839

Fig. 9. PE semantic description.

improved on any quality dimension without sacrificing in another.
The metrics in use include resource utilization, security and privacy
risks, and application-specific quality measures.

The resource utilization metric is evaluated based on a fixed
computation cost assigned to each PE. The resource metric is additive across the PEs, and can also include the communication cost,
especially in case of distant event sources. The security and privacy
risks are calculated based on a fuzzy multi-level security model that
captures the risk associated with the disclosure of potentially confidential data to a person at a certain security level. Further details
of the model can be found in [9]. The quality measures are computed using symbolic computation, under an assumption that PEs
are capable of producing streams at fixed quality levels. Examples
of such application-specific measures can be output video quality,
image resolution, confidence in a forecast, etc.

6.3.1. Pre-reasoning and SPPL generation

The use of reasoning helps make the matching process more
powerful. Reasoning helps us to match streams to stream patterns
based not on syntax, but on semantics. In order to improve efficiency of planning, the policy compiler performs pre-reasoning on
descriptions of sources and PEs to generate expanded descriptions
that include the results of reasoning. The expanded descriptions
are transformed into SPPL and later used by the semantic planner
when compiling policies. We do reasoning based on DLP (Descrip-
tion Logic Programs) both on the stream-graphs of the streams
produced by event sources and on the output graph patterns of the
streams produced by PEs. DLP lies in the intersection of Description Logic and Horn Logic Programs (like Datalog). Reasoning on
DLP is sound, complete and decidable, and takes polynomial time

Fig. 10. Compiler architecture.

[15]. Inference on the ABox in DLP can be performed using a set of
logic rules. This allows us to take a certain assertion and enumerate all possible assertions that can be inferred from this assertion
and an ontology using the rules. Since OWL semantics does not
cover reasoning on variables, we convert variables into individuals
that belong to a special concept called Variable. Using this process,
a graph pattern can be converted into an OWL/RDF graph for the
purposes of reasoning. The actual reasoner used by our system is
Minerva [31], a scalable DLP reasoner that uses a backend relational
database to store ontologies along with inferred facts.

We now introduce the concept of an expanded stream descrip-
tion. An expanded stream description contains a stream-graph that
has been expanded with the results of reasoning. The expanded

stream-graph, SG
, is defined as the set of triples obtained as a
result of doing reasoning on the original stream-graph, SG, based
on an ontology, O. Reasoning is done by applying the DLP logic
rules [15] recursively, in a bottom-up fashion, on the triples in SG
based on definitions in the ontology, O, and generating additional
triples about variables and exemplars, until a fix point is reached.
As a result of reasoning, more triples, which describe further information about the exemplar individuals and literals defined in the
stream graph, may be added to the expanded stream graph. For
example, there are rules defined for DLP that allow making inferences based on subclass and subproperty relationships, symmetric,
transitive and inverse property definitions, domain and range def-
initions, value restrictions, etc. These rules allow inferring many
facts about a stream that are not contained in the stream description itself. For example, consider the stream produced by the event
source in Fig. 8. The expanded stream graph includes additional
inferences like VoIP Call 1is of type VoIPCall(based on subclass
relationships).

6.3.2. Semantic planning for a given policy

An Eagle policy received by the planner is translated into an SPPL
problem. The SPPL model yields a recursive formulation of the planning problem, where policy stream patterns are expressed similarly
to PE input requirements, and PE outputs are described similarly to event sources. The planner performs branch-and-bound
forward search by connecting all compatible PEs to streams produced by already added PEs, or available from sources, and then
selecting Pareto optimal solutions that match to specified goals
[27].

When the planner attempts to connect a stream to a PE as input,

it tries to match the expanded stream-graph of the stream, SG
, with
the graph pattern, GP, which describes a PEs input requirement.
This matching process is similar to the matching of a stream to an
event pattern (describe in Section 5.1). Since reasoning has already
been done, the matching reduces to a subgraph-matching problem
after some variable substitutions. The planner attempts to find a

solution, 
, such that 
(GP) is a sub-graph of SG
. If
it is able to find such a solution, then the graph-pattern is matched
by the stream-graph.

, i.e. 
(GP)  SG

6.4. Stream monitoring

Once a plan, for a certain policy, is generated and deployed in the
System S stream processing system, an additional Stream Monitor
component is also instantiated for monitoring the events on the
high-level stream and performing the policy actions if the associated conditions are satisfied. This Stream Monitor parses any event
that comes on the stream and extracts its contents. Since in our sys-
tem, an event is a serialized Java object, it deserializes the object
using a class definition of the event obtained from the planning
process. Next, it checks the conditions in the policy. For some condi-
tions, it may need to call other services in the system. In our system,
these invocations are done using Java RMI and discovery of service
references is done using a Naming Service. If a condition evaluates
to true, then it performs the associated action by invoking the specified service with the appropriate parameters. Note that some of the
parameters used in invoking the service may have been extracted
from the event.

7. Compiler performance

Scalability of our approach depends on the ability of the compiler to plan with large numbers of event sources and PEs. Since
there are no standard datasets for stream composition, we evaluate
compiler performance by measuring planning time on increasingly
large randomly generated sets of PEs and sources. Experiments
were carried out on a 3 GHz Intel Pentium 4 PC with 500 MB mem-
ory.

For our experiments, we generated random DAG plans that
contained PEs and sources with randomly constructed semantic
descriptions, and then evaluated the time it took the compiler to
construct these plans. The DAGs were generated by distributing the
nodes randomly inside a unit square, and creating an arc from each
node to any other node that has strictly higher coordinates in both
dimensions with probability 0.4. The link may reuse an existing
output stream (if one exists) from the PE with probability 0.5, otherwise a new output stream is created. Each link is associated with
a randomly generated RDF graph from a financial services ontology in OWL that had about 200 concepts, 80 properties and 6000
individuals. The time taken to plan the DAGs are shown in Table 1.
The table has columns for the number of streams and PEs in the
generated graph, as well as time measurements for the online and
offline phases of semantic planning.

The experiments show that there is a noticeable increase in planning time as the size of the problem increases. Our pre-reasoning

Table 1
Pre-reasoning & planning time (s)

#PEs & sources

Streams

Prereason

Plan

approach, nevertheless, makes semantic planning practical by
improving planner scalability. Although pre-reasoning is time
consuming, the results of pre-reasoning can be shared between
multiple policy compilations. Therefore, the actual response time of
the planning system in practice is close to planning phase time. We
can see that even for plan graphs involving 100 PEs, the compiler is
able to produce the plan in less than 30 s, which is an acceptable
performance.

8. Discussion and experiences

The Eagle language allows high-level policies to be written.
These policies are not written for a specific event or even a specific
stream; they are written in terms of a pattern of event streams.
This pattern is then matched by the policy compiler to a specific
stream depending on which event sources and processing elements are currently available in the system. This approach allows
policies to be independent of the actual raw events that may be
available to the system at any point of time. Hence, even if the
system configuration changes, and new kinds of events may be
produced by the system, the policies can remain more or less
stable.

It is possible, however, that as the system configuration changes,
policies may need to be recompiled to take advantage of new kinds
of events and processing elements, or to adapt to changes in event
structure or semantics. The recompilation may result in the generation of a new processing graph for producing the high-level event
streams specified in the policy. Even in this case, the policy itself
does not change; just the way it is enforced changes.

A key aspect of our framework is that all policies are defined
based on terms described in domain ontologies. This simplifies
the task of policy writers since the vocabulary is formally defined,
using description logic, in OWL ontologies. At the same time, it
brings up interesting issues with regard to policy maintenance
as the ontologies evolve. In any large-scale dynamic system, the
ontologies will change in order to define new terms (concepts,
properties and individuals), or to modify the definition of existing terms. These changes need to be propagated to policies that
use these terms. This is a difficult problem, that must be solved in
an automatic or semi-automatic manner to allow scalable policy
maintenance.

In our system, all events are part of streams. A distinguishing
feature of a stream is that events in a stream are totally ordered.
Typically, the order is based on time. The total order on events also
implies that policy enforcement is totally ordered for a single policy with a single event pattern. The compilation of an Eagle policy
results in a workflow that produces a stream of high-level events,
and the policy is applied in a serial manner on the events flowing on
this stream. Hence, the problem of policy concurrency is addressed
by the implicit serialization of events as enforced by our streaming model. We currently assume that the action execution step is
atomic and transactional. Our system does not currently have a general means for dealing with action failures, although this is part of
ongoing research.

We have experimented with our framework in a number of different domains including the call-center domain (described in this
paper), a disaster recovery domain and a real-time traffic monitoring domain. For each domain, we developed domain ontologies and
PEs, and used real or simulated event sources. We also specified
high-level event stream patterns of interest and defined policies
based on these stream patterns. From this experience, we are confident that our policy language and framework can be easily used
in different domains. One of our main experiences from the use
of this framework in different domains is the need for tools that
can support collaborative ontology and policy editing and main-

Z. Liu et al. / Web Semantics: Science, Services and Agents on the World Wide Web 7 (2009) 2839

tenance. Such tools should provide source control and versioning
capabilities for ontologies and policies.

9. Related work

Existing policy specification languages and enforcement architectures do not allow for the derivation of high-level events from
low-level events, and the specification of policies based on these
high level events. The Ponder [11] language uses a subset of Object
Constraint Language (OCL) to specify conditions under which
actions must be taken. The PRIME project proposes the use of parametric obligation policies, which are parameterized by user privacy
preferences [10]. The Prolog-based Rei [17] language allows policies to be described in terms of entities in domain ontologies.
These languages describe obligation policies in the form: WHEN
Events happen THEN perform Actions on Target. However, they do
not address, in a general manner, the crucial problem of generating
the right kind of events that can be consumed by the policy from
the actual raw events produced in the system.

Many policy specification languages are based on Datalog. For
example, the RT Role-based Trust-management framework [18]
uses Constraint Datalog to deduce if a request should be satisfied by
a certain set of policies. SecPAL [5] is a decentralized authorization
language that is also based on Datalog. In terms of expressiveness,
these can be used to express graph patterns, which form the basis
for event specifications in our Eagle policy language. However, there
is a key difference between the enforcement semantics of these
policy languages and Eagle. Policy languages like RT and SecPAL
use Datalog rules, which are reasoned upon during enforcement
to arrive at decisions. While policy specification in Eagle can be
expressed as Datalog queries, the reasoning during enforcement is
not based on Datalog. Enforcement of Eagle policies is through an
automatically assembled workflow of arbitrary operators that analyze the raw events entering the system to produce the high-level
events described in the Eagle language. The other policy languages
do not come with a supporting infrastructure that supports the
invocation of arbitrary operators to analyze events; hence, they are
limited in the kinds of event processing they can accomplish (for
instance, they cannot be used directly for analyzing unstructured
data).

The area of policy refinement has some related work as well.
Bandara et al. [4] describe a goal based approach to policy refinement based on a formal representation of a system using Event
Calculus. The Power prototype [22] allows refining policies in a
semi-automatic manner. Our work focuses on a different problem,
which is a way of specifying policies based on high level event
streams, and automatically composing processing graphs for producing these streams.

One area of related work is in stream query languages and
stream processing architectures. Various stream query languages
and stream processing architectures have been proposed in the
recent past. Examples include Aurora [1], TelegraphCQ [8] and CQL
[2]. However, most existing systems focus on streams containing
structured data and only consider a limited set of operators like
relational and windowing operators. Hence, many of the techniques
used in these systems cannot be applied for constructing plans
that involve processing unstructured data in different media like
audio, video, text, etc., and that use an arbitrary, extensible set
of operators. While some systems allow invoking external stored
procedures, such procedures have to be invoked explicitly and cannot be automatically incorporated into plans. For example, Aurora
[1] uses a query algebra (SQuAl) that contains built-in support for
seven primitive operations including filter, aggregate, union, etc.
TelegraphCQ [8] proposes a declarative language for continuous

queries that uses standard relational operations along with expressive windowing constructs. CQL [2] or Continuous Query Language,
is an expressive SQL-based declarative language for registering continuous queries against streams and updatable relations. The SMILE
system [14] extends a traditional publish-subscribe system to allow
subscriptions on relational views (involving aggregation, joins, and
other transforms) that are derived from published event streams.
The policy compilation process in our system has a lot of similarities to various planning approaches to semantic web service
composition. Some of these approaches ([24,6,30,29], etc.) work on
semantic web service descriptions like OWL-S [20](or the earlier
DAML-S), including the process model of a service. They model services in terms of their preconditions and effects on the state of the
world, and inputs and outputs described as concepts from an ontol-
ogy. Our problem of composing event stream processing workflows
requires highly expressive descriptions of the inputs and outputs of
components since these workflows are primarily concerned with
data processing and transformation, and less on changing the state
of the world. OWL-S models have limited expressiveness in describing inputs and outputs. These models describe inputs and outputs
using classes in an ontology, while in our approach, inputs and outputs are modeled as instance-based graph patterns. OWL-S does
not allow expressions with variables in describing inputs and out-
puts, or in describing constraints on data elements in a stream. Our
model allows logical expressions with variables in the form of graph
patterns to describe inputs and outputs, and can also relate the
semantics of outputs to the semantics of inputs and allow semantic
propagation. Hence, our model is more suitable for composition of
stream processing workflows. A more detailed discussion of the differences between our component model and existing web service
models like OWL-S and WSMO [12] can be found in [19].

10. Conclusions

We have proposed a semantics-based approach to specifying
obligations policies. These policies specify high-level event patterns
and actions to be performed for matching events. The main features
of our approach are the ontology-based high-level policy specification and the use of semantic graph based planning for constructing
optimized processing graphs that can produce high-level events.
We have integrated the policy framework into the System S stream
processing system.

We are working on extending the policy framework to detect
conflicts in invoked actions through richer semantic action mod-
els, and to determine dependencies between policies. We are also
extending the planning model to allow actions to be expressed as
goals. Extensions to the policy language include specifying composite policies, as well as specifying authorization and delegation
policies.

Another on-going work is on policy authoring tools. While
Eagle is expressive and extensible through ontology extensions,
authoring policies requires knowledge of the ontologies. We are
developing tools to facilitate this authoring process by using technologies of ontology browsing/editing and of natural language
processing.
