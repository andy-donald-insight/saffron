SCOVO: Using Statistics on the Web of Data

Michael Hausenblas1, Wolfgang Halb2, Yves Raimond3, Lee Feigenbaum4,

and Danny Ayers5

1 DERI, National University of Ireland, Galway

IDA Business Park, Lower Dangan, Galway, Ireland

michael.hausenblas@deri.org

2 Institute of Information Systems & Information Management,
JOANNEUM RESEARCH, Steyrergasse 17, 8010 Graz, Austria

wolfgang.halb@joanneum.at

3 BBC Audio & Music interactive

London, United Kingdom
yves.raimond@bbc.co.uk
4 Cambridge Semantics,

PO Box 425003, Cambridge, MA 02142, United States

lee@cambridgesemantics.com

5 Talis Ltd,

Knights Court, Birmingham Business Park, B37 7YB, United Kingdom

danny.ayers@talis.com

Abstract. Statistical data is present everywherefrom governmental
bodies to economics, from life-science to industry. With the rise of the
Web of Data, the need for sharing, accessing, and using this data has
entered a new stage. In order to enable proprietary, closed-world formats,
to enter the Web of Data, we propose a framework for modelling and
publishing statistical data. To illustrate the usefulness of our approach
we demonstrate its application in real-world statistical datasets.

1 Motivation

Statistical data is present everywherefrom governmental bodies to economics,
from life-science to industry. With the rise of the Web of Data, the need for sharing,
accessing, and using this data has entered a new stage. Available technologies are
either not compatible with the Semantic Web or are too complex to be useful for
a range of use cases. We have identified the need for a more general and flexible
solution to the problem of modelling and publishing statistics on the Web.

Our motivation stems from ongoing work in three distinct efforts, namely riese
(RDFizing and Interlinking the EuroStat Data Set Effort) [11,12], U.S. Census
Bureaus annual Statistical Abstract of the United States, and making the UN
data accessible on the Web of Data. In riese, we provide statistical data about
European citizens. One of our main use cases of riese was in the context of an
advertising analysis application [23] allowing for example a market researcher to
better and faster understand a certain market or product. Further, in the U.S.
Census Bureaus annual Statistical Abstract of the United States case, one of
the authors and representatives of the U.S. Environmental Protection Agency

L. Aroyo et al. (Eds.): ESWC 2009, LNCS 5554, pp. 708722, 2009.
c Springer-Verlag Berlin Heidelberg 2009
?

?

?
were exploiting the semantics implicit in spreadsheets published yearly since
1878 by the U.S. Census Bureau. This data corpus is a comprehensive collection
of social, political, and economic statistics compiled from information from over
250 agencies. The goal here was to enable a fast and efficient publishing of the
statistics on the Web. Currently, this means making MS Excel documents and
PDF documents available for download from the Web.

The paper is structured as follows: Firstly, we review related efforts in section 2. Then, in section 3 we discuss issues with representing statistical data and
derive requirements for a generic modelling. The core of the work is presented
in section 4where we propose a modelling framework for statistical data
and section 5 in which two reference implementations are discussed. Finally, we
conclude our work and outline future steps in section 6.

2 Related Work

Representing statistical data has a long tradition, hence a plethora of proposals and solutions exists [3]mainly driven by governmental and international
institutions dealing with high volumes of data. In the 1990s the U.S. Bureau
of the Census has developed a statistical metadata content standard, which
allows to describe all aspects of survey design, processing, analysis, and data
sets [15]. Later, the United Nations Economic Commission for Europe (UN-
ECE) has developed guidelines [27] covering search, navigation, interpretation,
and post-processing of statistical data in their realm. A standard proposed by
the International Organization for Standardization (ISO) is the Statistical data
and metadata exchange (SDMX) [14]. More recently, the OECD has released
a report on the management of statistical metadata at the OECD [18]. Another
related effort is the Data Documentation Initiative (DDI)1, which aims at establishing an XML-based standard for the content, presentation, transport, and
preservation of documentation for datasets in the social and behavioural sciences.
As we have already pointed out in [11], there are known attempts concerning the
modelling and use of statistical data on the Web of Data [4,10,24,25]. However,
unlike earlier attempts such as [16,19], we aim at a light-weight solution enabling
a quick uptake and wide deployment.

The Web of Data is understood as the part of the Web where the linked
data principles are applied. The basic idea of linked data was outlined by Sir Tim
Berners-Lee [5]. The Linking Open Data (LOD) community project2 is an open,
collaborative effort applying the linked data principles. It aims at bootstrapping
the Web of Data by publishing datasets in RDF on the Web and creating large
numbers of links between these datasets [6].

We finally highlight the modelling issue with n-ary relations on the Web
of Data. The data model of the Web of Data is RDF, hence modelling n-ary
relations is a non-trivial task. In 2006 the W3C Semantic Web Best Practices
and Deployment Working Group has published a note dealing with this issue [17];
we will use this as a base for our framework.

http://www.ddialliance.org/
http://linkeddata.org/

M. Hausenblas et al.

3 Requirements and Issues

3.1 Issues with Modelling Statistical Data

Independent of the original format of the data (such as a table in an Excel
sheet, etc.), the issues discussed in the following need to be addressed properly
to ensure a lossless representation of the statistical data.

Handling of Multiple Dimensions. It is very often the case that a data item
has several dimensions. Roughly, two types of dimensions can be identified, (i)
generic dimensions, such as location and time, and (ii) domain specific dimen-
sions. For example we may be interested in the reliability of flights (domain
specific) from Cambridge, MA, US to London, UK (both are locations) between
2003 and 2007 (time period). It is crucial that a vocabulary aiming at representing statistics is capable of denoting such dimensions and allows to attach as
many as needed to a single data item.

Reusability and Uptake. Statistics are no ends in themselves; rather they are
about somethingbe it money, flights, death rates or the consumption of
YouTube videos. It is therefore essential being able to reuse existing information;
both on the schema as on the instance level. Related to reusability is the issue
of community uptake: Most statistical metadata formats are rather complex,
yielding a small deployment.

Structural vs. Domain Semantics. Two kinds of semantics come into mind when
modelling statistics:

 structural semantics, stemming from how statistics are presented, such as

grouping into time-periods, primary dimensions, etc.;

 domain semantics, stemming from the domain the statistic is about (money,

airports, etc.).

Performance and Scalability Issues. As discussed elsewhere [13] performance
and scalability issues may arise from the way data is encoded and served.

3.2 Requirements

Based on the issues listed above we state the following requirements for our
framework:

1. The framework must be directly usable on the Web of Data. This implies for
example that a vocabulary used in the framework must be expressed in RDF.
This requirement addresses the issue of structural and domain semantics, as
well as ensuring reusability;

2. The framework must be extensible both on the schema level and the instance

3. The framework should be light-weight, addressing uptake, and performance

level, enabling reusability;

and scalability issues.
?

?

?
It has to be noted that the first requirement does not stem from the issues
discussed earlierit was rather introduced to benefit from the large deployment
base of domain vocabularies3 as well as available tools and systems4.

4 Statistical Modelling Framework

Driven by the requirements we propose a modelling and publishing framework
for statistics on the Web of Data consisting of:

 a core vocabulary for representing statistical data
 a workflow to create the statistical data

The framework is depicted at a glance in Fig. 1. The lower half is the generic part,
defined in this work, the upper part is the application-specific part, depending
on the technologies used to implement the framework.

Fig. 1. The Statistical Modelling Framework

4.1 Statistical Core Vocabulary (SCOVO)

One of the main contributions of our work at hand is the Statistical Core Vocabulary (SCOVO)5. SCOVO defines three basic concepts:

 a dataset, representing the container of some data, such as a table holding

some data in its cells;

 a data item, representing a single piece of data (e.g. a cell in a table);
 a dimension, representing some kind of unit of a single piece of data (for

example a time period, location, etc.)

http://schemacache.test.talis.com/
http://esw.w3.org/topic/SemanticWebTools
http://purl.org/NET/scovo

M. Hausenblas et al.

A statistical dataset in SCOVO is represented by the class Dataset; it is a SKOS
concept[22]inorder to allowhooking into a categorisationscheme.Astatisticaldata
item Item belongs to a dataset (cf. inverse properties dataset and datasetOf). An
Item is subsuming the Event concept, as defined in the Event ontology6. The Event
ontology essentially adopts the view from Allen and Fergusson [2]:

[..] events are primarily linguistic or cognitive in nature. That is, the
world does not really contain events. Rather, events are the way by
which agents classify certain useful and relevant patterns of change.

An event is then defined in this ontology as the way by which cognitive agents
classify arbitrary time/space regions. Our Item concept is subsuming this Event
concepta statistical item is a particular classification of a time/space region.
Dimensions of a statistical item are factors of the corresponding events, attached through the dimension property, pointing to an instance of the SCOVO
Dimension class.

This model is easily extensible by defining new factors and agents pertaining
to the actual statistical data. For example, we can relate to a statistical data
item the institutional body responsible of it as well as the methodology used.
A Dimension can have a minimum (and respectively a maximum) range value,
captured through the min and max properties.

Fig. 2. The Statistical Core Vocabulary (SCOVO)

The Statistical Core Vocabulary (depicted in Fig. 2) is currently defined in RDF-
Schema. It is possible to express SCOVO in OWL-DL, if advanced reasoning is of
necessity. Although we have depicted the range of both :min and :max in Fig. 2
being of literal value, we emphasize that in the RDF-Schema the ranges have not
been specified in order to allow an extension for domain-specific purposes. Hence,
this can be seen as a kind of recommendation for the default case.

http://purl.org/NET/c4dm/event.owl
?

?

?
Example. To demonstrate the usage of SCOVO, let us assume we want to
model airline on-time arrivals and departures. The input in our example is the
Table 1047. On-Time Flight Arrivals and Departures at Major U.S. Airports:
20067 (cf. Fig. 3) from the US Census data set. Every airport, for each time
period has an on-time arrival percentage and an on-time departure percentage.

Fig. 3. On-Time Flight Arrivals and Departures at Major U.S. Airports: 2006

In listing 1 an excerpt8 of the modelling of the airline on-time arrivals and
departures is shown9. We note that the example has http://example.org/
on-time-flight# as its base along with the prefix ex: (line 1). Lines 4 to 16
define domain-specific entities, such as a ex:TimePeriod, an ex:Airport, etc.
From line 18 to 22 the one and only dataset (ex:ontime-flights) is defined,
corresponding to the entire Excel table used as an input. Further, from line 24
on an exemplary data item is defined, stating that the on-time arrival of the
Atlanta, Hartsfield airport in the first quarter of 2006 was round 74%. This
corresponds to the highlighted cell in Fig. 3.

The SPARQL query from listing 2 can be used to explore high-performing
airports. With high-performing we define in this context airports with ontime arrivals or departures higher than 85%. Lines 2 to 6 provide the generic
pattern for an Item. Note how in line 7 and 8 the dimensions are constrained.
Line 9 of listing 2 basically expresses give me all kinds of on-timeness, and
finally line 10 implements the high-performance filter criteria.

The query resultdepicted in Fig. 4shows the list of high-performing airports along with the time period, starting with the best airport in terms of
on-timeness. We note that the complete example, including the exemplary
queries in an executable form, is available at http://purl.org/NET/scovo

http://www.census.gov/compendia/statab/tables/08s1047.xls

8 The full example in RDF/XML is available at http://sw.joanneum.at/scovo/

otf-example-full.rdf

9 Note, that in all of our examples the well-known prefixes such as rdf:, rdfs:, etc. have

been omitted due to readability reasons.

M. Hausenblas et al.

2 @prefix ex : < h t t p : / / e x a m p l e. o r g / on - time - f l i g h t# > .
3 @prefix scv : < h t t p : / / p u r l . o r g / N E T / s c o v o # > .

5 ex : T i m e P e r i o d rdfs : s u b C l a s s O f scv : D i m e n s i o n; dc : title " time period " .

7 ex : Q12006 rdf : type ex : T i m e P e r i o d; dc : title " 2006 Q1 " ;

scv : min " 2006 -01 -01" ^ ^ xsd : date ;
scv : max " 2006 -03 -31" ^ ^ xsd : date .

11 ex : OnTime rdfs : s u b C l a s s O f scv : D i m e n s i o n; dc : title " on - time ... " .

13 ex : ota rdf : type ex : OnTime ; dc : title " on - time arrivals " .

15 ex : Airport rdfs : s u b C l a s s O f scv : D i m e n s i o n; dc : title " airport " .

17 ex : A t l a n t a H a r t s f i e l d rdf : type ex : Airport ; dc : title " Atlanta , ... " .

19 ex : ontime - flights rdf : type scv : Dataset ;

dc : title " On - time Flight Arrivals ... " ;
scv : d a t a s e t O f ex : atl - arr -2006 q1 .

23 ex : A t l a n t a H a r tsf ie ld - ota -2006 - q1 rdf : type scv : Item ;

rdf : value 74 ;
scv : dataset ex : ontime - flights ;
scv : d i m e n s i o n ex : Q12006 ;
scv : d i m e n s i o n ex : ota ;
scv : d i m e n s i o n ex : A t l a n t a H a r t s f i e l d .

Listing 1. Modelling flight on-time arrival statistics

1 SELECT ? a i r p o r t _ n a m e ? p e r c e n t _ o n t i m e ? period ? o n t i m e _ t y p e WHERE {

? item rdf : type scv : Item ;

scv : d i m e n s i o n ? airport ;
scv : d i m e n s i o n ? t i m e _ p e r i o d ;
scv : d i m e n s i o n ? ontime ;
rdf : value ? p e r c e n t _ o n t i m e .

? airport rdf : type ex : Airport ; dc : title ? a i r p o r t _ n a m e .
? t i m e _ p e r i o d rdf : type ex : T i m e P e r i o d; dc : title ? period .
? ontime rdf : type ex : OnTime ; dc : title ? o n t i m e _ t y p e .
FILTER ( ? p e r c e n t _ o n t i m e > 85)

11 } ORDER BY DESC ( ? p e r c e n t _ o n t i m e)

Listing 2. SPARQL query for high-performing airports

4.2 WorkflowGood Practice Rules

In this section we discuss the overall workflow as shown in Fig. 1. Based on our
findings from publishing real-world statistical datasets, the following should be
seen as strong advises, helping to avoid failings and to enable a quick adoption.

RDFication. In the very first step, the data needs to be converted into an RDFbased form. This is equally true for the schema level as for the instance level.
The schema level (e.g. XSD, etc.) is a typical starting point which is followed by
the conversion of the actual data in a second step. While creating and populating
the ontology with instances several issues arise.

URI Design. It has to be ensured that every entity has a URI assigned, which is
usually referred to as URI minting; cf. [7] for a more detailed discussion on URI
?

?

?
Fig. 4. Results for high-performing airports

design. For example http://dbpedia.org/resource/Airport has been minted
by DBpedia to represent the concept of an airport.

More specifically we recommend using HTTP URIs in order to be compliant
with the linked data principles: When dereferencing the aforementioned URI for
Airport it yields

curl -I http://dbpedia.org/resource/Airport
HTTP/1.1 303 See Other
Server: Virtuoso/05.00.3028 (Solaris) x86_64-sun-solaris2.10-64
Content-Type: text/html; charset=UTF-8
Date: Thu, 08 May 2008 10:32:29 GMT
Location: http://dbpedia.org/page/Airport

basically telling us that the concept URI redirects to an information resource at
http://dbpedia.org/page/Airport, see also [21].

Domain Ontologies. As already mentioned, statistics are always about a certain domain.
In order to use domain vocabularies together with SCOVO, several hooks can be used:

 Subclassing the SCOVO-Dimension class. In most cases it is sufficient to use this

technique to incorporate domain-specific concepts, for example

ex:Airport rdfs:subClassOf scv:Dimension
ex:AtlantaHartsfield rdf:type ex:Airport

from the example in listing 1;

 Use the built-in support for event:Event and skos:Concept. The latter is of particular help if an existing taxonomy or thesaurus is used as a base. The earlier
can be used to capture more information pertaining to the creation of a particular
statistical item;

 Defining sub-properties of using SCOVO-min and max. Whenever the need arises
to more explicitly declare what kind of range is intended, this technique can be
used (e.g. an xsd:date).

M. Hausenblas et al.

Interlinking. Classes and instances of the domain vocabulary should be interlinked
to existing LOD entities. The rationale behind is that any dataset can be enriched
through this at low costs. For example, to connect the airports to the LOD datasets,
one could use the query from listing 3 to find according targets in DBpedia (note that
this query can be executed at http://dbpedia.org/snorql/).

1 SELECT ? a i r p o r t s _ s t a t e ? airport
2 WHERE {

? a i r p o r t s _ s t a t e skos : broader
< h t t p : / / d b p e d i a. o r g / r e s o u r c e/ C a t e g o r y: A i r p o r t s _ i n _ t h e _ U n i t e d _ S t a t e s > .
? airport skos : subject ? a i r p o r t s _ s t a t e ;

8 }

< h t t p : / / d b p e d i a. o r g / p r o p e r t y/ n a m e > ? name .

FILTER regex ( ? name , " Atlanta " , " i " )

Listing 3. Interlinking airports to DBpedia

The result of the query from listing 3 may subsequently be used to enrich our

example, that is adding for example the triple

ex:AtlantaHartsfield owl:sameAs
<http://dbpedia.org/resource/Hartsfield-Jackson_Atlanta_International_
Airport>

in order to express that the two URIs are actually identifying the same thing. With this
interlinking we have significantly broadened the possibilities for querying our dataset;
for example we could issue a location-based query with the geo-data from DBpedia
or could further follow down the path to other LOD datasets containing even more
information related to the Hartsfield airport.

Publication. When publishing the dataset, one needs to make choices on the formats
to be used for the data. While certain circumstances may require the usage of specialized and/or proprietary formats such as PDF or the SPSS file format, there are four
basic technologies that we (unsurprisingly) see central to our setup: URIs, HTTP, RDF
and (X)HTML; every publishing system on the Web of Data should use these as primary technologies. We have discussed URI minting above. Regarding HTTPbeside
its basic transport functionwe encourage people to use light-weight REST interfaces.
One particular issue, however, is how to deploy the metadata. Several options exist,
we list some widely used in the following:

 use an RDF standalone format such as RDF/XML, N3, etc. along with 303 redi-

rects or links such as described in [21];

 use XHTML+RDFa10 for both humans and machines (see also [11]);
 SPARQL-endpoints and RDF dumps [20,13].

Note that in practice very often the approaches listed above are used in combination.
For example offering an RDF dump (in N-Triples) for semantic search engines such as
Sindice [26] along a SPARQL-endpoint for cross-site query is a typical pattern.

To allow semantic search engines to efficiently and effectively process the dataset
it is advisable to use proper announcement mechanisms such as the semantic crawler
sitemap extension protocol [8].

http://www.w3.org/TR/rdfa-syntax/
?

?

?
4.3 Comparison with Other Approaches
The following table presents a comparison between three different approaches for modelling statistics in RDF. The comparison is based on 11 and highlights some differences
between the modelling from the D2R Server for Eurostat12, the 2000 U.S. Census
Data [25], and SCOVO itself.

The most distinguishing feature of SCOVO is the ability to express complex statistics over time while still keeping the structural complexity very low. Both other approaches are not capable of representing historical data and only provide statistics for
one point-in-time. From the table below we conclude further that SCOVO seems to
be the best combination of flexibility and usability, allowing to recreate the data-table
structures with a reasonable degree of fidelity in another environment (that is, on the
Web). Additionally, in our understanding SCOVO is more aligned with the linked data
principles, compared to D2R Eurostat and 2000 U.S. Census.

D2R Eurostat
Simple, limited
Expressivity
Modelling of time Point-in-time*
Historic data
Easy
table
)generation
Access
statistical data
Location of classifying features

Using
predicates
Concatenated in the
name of the predicate

individual

to actual

No*
No

(re-

Structural
plexity

com-

Flat model

predi-

Use of blank nodes No
Different
cates
Knowledge
needed for query

Many

Predicate names of
desired dimensions

What is modelled? Real-world,

ignoring
statistical
artefacts
such as time, table,
sub-tables, etc.
Very limited

Use of deref.-able

2000 U.S. Census Scovo
Complex
Point-in-time
No
Yes

Complex
Over time
Yes
Yes

individual

Using
predicates
Concatenated in the
name of the predicate
and a chain of predi-
cates
Complex,
related
via individual pred-
icates;
sometimes
inconsistent
Yes
Many

Predicate names of
desired
dimensions
and nesting chain
Statistical domain in
question

attached to

Value
items
Attached to items

Relatively flat model;
information at-
all
tached
item;
datasets as container
No
Few

to

Name or URI of desired dimensions

Statistics in general

Limited

Each statistical item
has an explicit URI

* The use of different named graphs for different points in time is planned in D2R.

http://www.thefigtrees.net/lee/blog/2008/03/modeling statistics in rdf
a s.html
http://www4.wiwiss.fu-berlin.de/eurostat/

M. Hausenblas et al.

5 Usages in the Wild

5.1 Eurostat Datariese

In riese13, the RDFizing and Interlinking the EuroStat Data Set Effort [11,12] we
have RDFized, interlinked, and published the Eurostat dataset on the Web. First of
all the dimensions and dataset hierarchies defined in Eurostat get translated to RDF
and interlinked. The actual data is translated to RDF on-the-fly from the raw Eurostat
tables. Both human users and machines can access the data from the same location
thanks to embedding RDF on the human-readable pages with XHTML+RDFa. Additional access methods (as described below) are available as well.

RDFication. The translation to RDF is performed using SWI-Prolog. The SWIProlog Semantic Web Library provides an infrastructure for reading, querying and
storing Semantic Web documents; additionally the Prolog-2-RDF (p2r) modules14 and
individually defined mappings are used for translating the input data to RDF. The
input data consists of a table of contents in HTML defining the overall structure of the
datasets, dictionary files for resolving the approximately 80,000 different data codes
used, and the actual data itself in tab-separated values format. The Eurostat data
consists of more than 4,000 datasets containing roughly 350 million data values/items.
In riese HTTP URIs are used, which are compliant with the linked data principles.
For instance the currency dimension Euro has been minted with the concept URI of
http://riese.joanneum.at/dimension/currency/eur which can be dereferenced for
accessing the information resource that describes the concept. Accordingly datasets and
individual items have an URI assigned for unambiguous identification of all resources.
Several domain ontologies are being re-used in riese. Geographical dimensions such as
countries, etc. for instance make use of the Geonames ontology as they are subclasses of
geonames:Feature which allows more expressive descriptions. It is hence easily possible
to express further classifications such as the class of the geographical dimension (e.g.
country, administrative division, etc.). Furthermore, the riese schema re-uses the Event
ontology in the same way as described above.

Interlinking. The automated interlinking of country descriptions between riese and
Geonames for instance is done using the ISO-3166 alpha2 country codes which are
available in both datasets assuring that exactly the same resource is addressed. In the
practical implementation this means that a search using the country code is performed
in both datasets. According to the nomenclature used by Eurostat it is also possible
to identify only country descriptions in the source dataset. The result from the target
dataset is restricted to return only countries as well. Finally all matches are being
interlinked using owl:sameAs. In this case it is possible to create exact matching highquality interlinks. The generation of interlinks to other LOD datasets such as DBpedia,
CIA Factbook, and Wikicompany follows a similar approach.

Publication. All data on riese is accessible for both humans and machines (Semantic
Web agents) equally. An Apache 2 Server with a set of PHP scripts is used to render
the pages in XHTML+RDFa. A dump of the entire data is also available in RDF/XML
which should be used by Semantic Web agents like indexers that want to crawl the

http://riese.joanneum.at
http://moustaki.org/p2r/
?

?

?
entire content. Providing a simple file download enables the indexers to easily acquire all
data and reduces the server load. Furthermore a SPARQL endpoint using SWI-Prolog
and p2r is provided. It operates on the raw input data and maps the data on-the-
fly. Experiments have shown that for average queries this approach offers acceptable
response times at reasonable server cost.

5.2 In Other Vocabularies
SCOVO is used in voiD, the Vocabulary of Interlinked Datasets [1] to express information about the number of triples, resources and so forth. Using SCOVO in voiD
allows a simple and extendable description of statistical information, however, a shortcoming has been identified: as scovo:Items are grouped into scovo:Datasets, there
is an implicit assumption that all items in such a dataset share the same dimensions.
This yields to complex SPARQL expressions, as it will often require a verbose check
to make sure that an item has only certain dimensions and no others. An examplary
usage of SCOVO in voiD is given below in listing 4. Further, we have gathered that
SCOVO is used in the RDFStats framework15, see Fig. 5 (kudos to Andreas Langegger
for the screen shot), that generates statistics for datasets behind SPARQL-endpoints
and RDF documents.

1 : DBpedia a void : Dataset ;

void : statItem [

6 }

scovo : d i m e n s i o n void : n u m b e r O f T r i p l e s ;
rdf : value 2 1 2 5 7 6 2 3 9 ;

] ;

Listing 4. Usage of SCOVO in voiD

Fig. 5. SCOVO driving the statistics in RDFStats

http://semwiq.faw.uni-linz.ac.at/node/9

M. Hausenblas et al.

6 Conclusion and Future Work

We have proposed a vocabulary, SCOVO, and discussed good practice guidelines for
publishing statistical data on the Web in this paper. The framework aims at supporting
people to publish their statistics on the Web of Data in an effective and efficient manner.
The framework includes good practice rules stemming from our experience in publishing
linked datasets. Finally, we have demonstrated the implementation of our framework
and discussed practical issues with it.

However, there are limitations we are aware of. We advocate simplicity, hence there
exist edge cases where it is hard to find an appropriate semantic modelling. Take for
example our scovo:Dataset concept. In the current representation it is not explicitly
defined how the overall range of the dataset is expressed. This may yield performance
problems when one determines to figure out the overall range of a dataset. It is for sure
possible to concatenate single dimensions used on the scovo:Item-levelfor example
concluding from the range of the four quarters ex:Q12006 to ex:Q42006 that the dataset
actually is referring to the year 2006. Additionally, from the application of SCOVO in
voiD we have learned that there is a demand for aggregates. Hence, we plan to add
support for data aggregation in a future version of the SCOVO schema.

The United Nations provide a range of statistics about various domains16. We plan
to publish this available data using the Talis Platform [9]. Due to the high volume of
the data, scalability issues are at the centre of the entire design. For example, one part
of the UN data setthe Commodity Trade Statistics Database (COMTRADE)alone
provides commodity trade data for all available countries and areas since 1962, containing almost 1.1 billion records. Further, our ongoing work focuses on broadening the
deployment base available17, making converters (from and to SCOVO) available, and
extending the framework itself. One very important issue is what we call statistical-
presentation fidelity. When the data is present in a table with a certain layout, it
turns out to be advantageous to not only repurpose and link the data, but also reuse
the data table in the authors intended form.

Acknowledgements

The research reported in this paper was partially supported by the Understanding
Advertising (UAd) project18, funded by the Austrian FIT-IT Programme and the
ICT-2007.1.2 ROMULUS project19, partially funded under the 7th Framework Programme of the European Commission.
