RadSem: Semantic Annotation and Retrieval

for Medical Images

Manuel M oller1, Sven Regel2, and Michael Sintek1

1 German Research Center for Artificial Intelligence (DFKI) GmbH

Kaiserslautern, Germany

manuel.moeller@dfki.de, michael.sintek@dfki.de

2 Chemnitz University of Technology, Chemnitz, Germany

sven.regel@cs.tu-chemnitz.de

Abstract. We present a tool for semantic medical image annotation
and retrieval. It leverages the MEDICO ontology which covers formal
background information from various biomedical ontologies such as the
Foundational Model of Anatomy (FMA), terminologies like ICD-10 and
RadLex and covers various aspects of clinical procedures. This ontology
is used during several steps of annotation and retrieval: (1) We developed
an ontology-driven metadata extractor for the medical image format DI-
COM. Its output contains, e. g., person name, age, image acquisition
parameters, body region, etc. (2) The output from (1) is used to simplify the manual annotation by providing intuitive visualizations and to
provide a preselected subset of annotation concepts. Furthermore, the
extracted metadata is linked together with anatomical annotations and
clinical findings to generate a unified view of a patients medical history.
(3) On the search side we perform query expansion based on the structure
of the medical ontologies. (4) Our ontology for clinical data management
allows us to link and combine patients, medical images and annotations
together in a comprehensive result list. (5) The medical annotations are
further extended by links to external sources like Wikipedia to provide
additional information.1

1 Introduction

Advances in medical imaging have enormously increased the volume of digital
images produced in clinical practice. At the same time, modern hospital information systems have also become more complex. Radiological findings are kept
separately from images which, in turn, are kept separately from patient accounting and billing information.

Currently, these systems are more or less isolated from each other and do not
allow queries to span across these systems. Thus it has become challenging for
clinicians to query for and retrieve relevant historical data due to the volume

1 This research has been supported in part by the THESEUS Program in the MEDICO
Project, which is funded by the German Federal Ministry of Economics and Technology under the grant number 01MQ07016. The responsibility for this publication
lies with the authors.

L. Aroyo et al. (Eds.): ESWC 2009, LNCS 5554, pp. 2135, 2009.
c Springer-Verlag Berlin Heidelberg 2009

M. M oller, S. Regel, and M. Sintek

of information as well as the complexity of information systems. In particular,
historical patient images would be useful for analyzing images of a current examination since they help in understanding any progression of pathologies or
development of recent abnormalities. Today, such images can only be retrieved
by attributes stored in the DICOM2 headers of the images such as patient name,
age or gender. However, these attributes do not contain any information about
the anatomy or disease associated with the image. Hence, radiologists are often
overwhelmed with irrelevant images not connected with the current examination
orthe other extremeare unable to retrieve any similar case.

To overcome these limitations our system uses the Semantic Web standards
OWL [9] and RDF [7] as a common umbrella to represent domain knowledge and
annotations in the same formalism. We have developed the MEDICO Ontology
Hierarchy (see Fig. 1) which models various aspects of clinical data management and medical background knowledge. Our rationale was to reuse background
knowledge represented in formal ontologies such as the Foundational Model of
Anatomy ontology (FMA) [19] and terminologies like RadLex [8] and the International Classification of Diseases version 10 (ICD-10).3 Wherever possible we
used existing translations to OWL. For the ICD-10 we implemented our own
technique which is described in Sect. 3.2. These ontologies were integrated into
our hierarchy to provide background knowledge about the medical domain during annotation and retrieval.

On the search side we leverage the structural information in the ontologies
to allow multilingual search and to perform query expansion to retrieve images
which are annotated with semantically similar concepts.

Semantic Web technologies allows us to reuse highly qualified medical domain
knowledge for annotation and query expansion without being medical experts
ourselves. Storing the medical annotations as instances of well defined OWL
classesrather than in a proprietary relational databasefosters an open interchange of this data with other applications and makes them easily available for
other research goals, such as clinical data mining. The fusion of medical annotations with automatically extracted patient and image metadata in the same
representation formalism provides the basis for new intuitive visualizations.

The research on semantic annotation and retrieval described in this paper is
part of a broader effort to understand the semantic of medical images in the
THESEUS MEDICO [12] project.

2 Related Work

The need for representing high-level annotations of medical images on an abstract level has been emphasized in various publications in recent years, e. g.,
in [13, 23, 14].

Biomedical ontologies and terminologies received high attention in the last
decade and provide promising technologies. [3] evaluated popular large scale

2 Digital Imaging and Communication standard, http://medical.nema.org/

http://www.who.int/classifications/apps/icd/icd10online
?

?

?
ontologies such as SNOMED, FMA, and Gene Ontology and stated that on-
tologies play an important role in biomedical research through a variety of ap-
plications. Only recently, [10] presented an approach for integrating FMA and
RadLex as FMA-RadLex to compile a robust application ontology. Besides efforts combining ontologies and radiological reports (e. g., [15]), other approaches
using ontologies in medical image processing have been proposed [18, 22].

The work in [20] is similar to our research in annotating medical images with
ontology-based semantics and the use of context for faster annotation. However,
their notion of context is primarily oriented on anatomy than diseases.

3 Ontological Modeling

d
e
g
n
a
h
c
 
e
b
o
o

 

l

t
 
y
e
k

gives

Ontology

i

c
c
i
f
i
c
e
p
s
 
n
a
m
m
o
d
e
r
o
m

 

i
t
-images, texts

t

Upper Ontology:

Medical Ontologies

Information Element

time space organization person event
time, space, organization, person, event

Representational Ontology: RDFS,OWL,...

Fig.
an
overview of the general structure of the
MEDICO Ontology
Hierarchy. The
fol-
lowing
paragraphs
describe the different
parts of the hierarchy
in detail as well as
their
interrelations
and the design ra-
tionales. We
follow
Grubers definition [5]
for the term ontology
that an ontology is a
formal specification of a (shared) conceptualization. An OWL model of this
ontology hierarchy can be browsed online.4

Fig. 1. MEDICO Ontology Hierarchy

Clinical Ontology

tgy

Thesauri & Taxonomies

 

l

y
g
o
o
o
t
n

n
o
i
t
a
t
o
n
n

-doctor, nurse, patient

-DICOM Ontology

Visual 
Charac.

-medical case

ly
y
e
e
g
v
o
e
l
o
-
t
n
o

w
o

l

mapping&
mapping&
mapping&
mapping&
merging
merging
merging
merging

-MPEG7

ICD-10

t
o
 
e
x
t
e
r
n
a

l

a
p
p
n
g
s

i

t
o
 
e
x
t
e
r
n
a

l

a
p
p
n
g
s

i

e
b
s
i
t
e
?

?

?
-
?

?

?
l

e
v
e
l
-
-
d
m

i

l

y
g
o
o
o
t
n
o

x
t
e
n
.
?

?

?
annotation
annotation
annotation
annotation

extraction
extraction
extraction
extraction

i
l
 

e
r
o
m
m

l

s
o
u
r
c
e
s

s
o
u
r
c
e
s

a
d

e
x

Ontologies are usually structured in various layers or levels, based on the assumption that those at higher levels are more stable, shared among more people,
and thus change less often than those at lower levels. Following [21], we distinguish representational ontologies, upper-level ontologies, mid-level ontologies,
and low-level or domain ontologies.

3.1 Medical Ontologies

In the medical domain, large amounts of knowledge are already formulated in
ontologies like the FMA. Therefore we decided to reuse those sources and opted
for a mapping to/merging of those ontologies and thesauri each covering different
dimensions of image annotation.

We differentiate between three different aspects or dimensions of medical
annotation. For anatomy we use the FMA. The concepts for the visual manifestation of an anatomical entity on an image is derived from the modifier and

http://www.dfki.uni-kl.de/~moeller/ontologies/medico-browser

M. M oller, S. Regel, and M. Sintek

imaging observation characteristic sub-trees of RadLex. We consider the disease
aspect as the interpretation of the combination of the previous two. Here we use
the ICD-10.

3.2 ICD-10

At the time of writing, ICD-10 was not available in any format following the
Semantic Web standards. Due to our decision to use OWL and RDF in MEDICO,
we generated a light-weight ICD-10 model in OWL which mainly reflects the
terminological structure.

From the semistructured online version of the terminology, we have created a
simple model using the OWL API [2]. It reflects the hierarchy as defined in the
ICD-10 manual [4]. We modeled this hierarchy on the class level while instances
are considered to reflect concrete diseases of concrete patients. The whole OWL
model contains 11,290 classes and has a maximum depth of 5.

3.3 Annotation Ontology

The Annotation Ontology is used to connect a given medical documentbe it
image or textwith the concepts in the ontologies described in the previous
sections. The diagram in Fig. 2 illustrates the ontology we created to generate
annotations. (Ovals depict properties, rectangles classes.)

hasAnatomicalAnnotation

hasModifier

hasAnnotation

ImageAnnotation

Anatomical

Entity

Image

Observation
Characteristic

InformationElement

hasComponent

hasPatient

Patient

hasDiseaseAnnotation

Disease

annotatedBy

Annotator

Fig. 2. Annotation Ontology

The image is connected to the patient via the hasPatient property. Addi-
tionally, we add annotations to the study and series data the image belongs to.
For the sake of clarity we omitted these links in the illustration.

The image itself is decomposed into InformationElement instances with
each of these instances being connected via the property hasAnnotation to
an ImageAnnotation instance. InformationElement subsumes arbitrary documents (text, images, 3D volumes, ...) as well as parts or regions of them.

For each ImageAnnotation we store information on the annotation prove-
nance. Annotator can either be a Physician instance for manual annotations
by a medical expert or an instance of Classifier for automatically generated
image annotations which we plan to integrate in the future.
?

?

?
Finally, ImageAnnotations can be further qualified by adding instances of
VisualModifier. Here we reuse the existing RadLex vocabulary enriched by
terms from our own studies on radiology reports to identify further terms that
are common in the clinical practice but not yet part of RadLex. These qualifiers
comprise attributes to describe alterations from normal and healthy features like
bulky or compressed.

3.4 Mapping from DICOM Body Regions to FMA Body Regions

Standardized formats such as DICOM have been developed for digitally representing the acquired images from the various modalities. Apart from the image
pixels, a DICOM image also contains a header, which is used to store certain
patient information such as name, gender, demographics, etc.

In order to create a deep interconnection between data from the DICOM
metadata and the medical domain knowledge, we manually defined mappings
for the 20 allowed values of the DICOM tag Body Part Examined to FMA
concepts. We are aware of former studies like [6] which showed that the value of
this field is in practice neither mandatory nor always correct. But in our opinion
it would be wrong to completely ignore this information since it is correct in
most of the cases. Instead we propose to put a bias on their value, signifying
that their value is not absolutely reliable. In fact this is also the case with all
results from automatic object recognition algorithms we plan to incorporate.

If there is no body region set in the DICOM header, we offer all concepts
from the FMA for the anatomical annotation. If the wrong body region was
saved to the DICOM header, this becomes apparent by comparing the body
region visualization and the anatomical structures on the image. In both cases
the user can set the correct body region manually in the application.

In the manual annotation tool we use the body region information from the
DICOM header to select only those concepts from the FMA which belong to
the body region. This reduces the number of potential anatomical annotation
concepts from approx. 80,000 down to a few hundred to a few thousand.

Another application in the area of visual object recognition of the body region
annotations is described in [11]: A Metadata Extraction Module retrieves the
BodyRegion from the DICOM header. Based on the mapping, the FMA is used
to retrieve a list of anatomical entities that could possibly be detected in this
body region. This narrows down the search space of potential objects on the
image significantly.

3.5 Body Substance Annotations for FMA

Each body substance has its own characteristic radiation attenuation spectrum
which is typically measured in Hounsfield Units.5 On CT and X-ray images this
manifests in specific pixel contrast ranges for the tissue types. We have modeled
the class TissueType which currently subsumes classes for 13 different body

5 The Hounsfield Scale is a quantitative scale describing radiodensity.

M. M oller, S. Regel, and M. Sintek

substances like Bone, Blood, etc. each annotated with the respective Hounsfield
range.

In a second step they were mapped manually to FMA concepts. We considered implementing generic heuristics which traverse
particular relationships in the FMA to retrieve all related concepts belonging to a specific tissue type. But so far we did not discover
any heuristic generic enough to treat organs
and their sub-components the same way as
blood vessels which belong to organs.

in Fig. 3 of

The snapshot

the FMA
shows a part of the part of sub-tree of
the concept Liver. It clearly shows, that
veins and arteries are also considered to be
part of the liver and not the artery system.
This also holds for the more specific properties systemic part of, regional part of
and constitutional part of. We decided against the annotation of all constitutional parts of an organ with the same tissue type as their super class. The
reason was that constitutional parts also comprise blood supply which has a
different radiodensity.

Fig. 3. part of sub-tree of the
Liver in the FMA

3.6 Mappings to External Biomedical Data Sources

Here, our goal was to augment the annotations of search results with mappings
to external data sources such as Wikipedia to provide additional information
explaining the annotation concepts. The first step was to extract a list of all
class labels and synonyms from the ontologies. Next, we iterated over this list and
tried to identify the most relevant document in a corpus of potentially relevant
web pages by performing a statistical keyword/phrase search. The corpus was
indexed offline using the DynaQ API [1].

We searched for English labels and synonyms of the FMA in Wikipedia. In
our first attempt we did a plain full text keyword search for labels and synonyms
and created one mapping for the best match if there was one. This resulted in
mappings for 86% (84 685 absolute, column keyword in Tab. 1) of all FMA labels
and synonyms. Prima facie this number is surprisingly high since most of the
FMA concepts are very specific and we did not expect Wikipedia to cover them
to such a high extent.

However, the quality of these mappings turned out to be very limited. We
imposed a systematic evaluation by picking a random set of 100 mappings and
checking each of them manually. We distinguished between (1) perfect match
where the Wikipedia article is covering exactly the FMA concept; (2) a use-
ful match meaning that the Wikipedia article covers (a) an anatomical part
of the search label, (b) the FMA label is a part of the concept covered in
the Wikipedia article, or (c) the Wikipedia article deals with a direct sub- or
?

?

?
Table 1. Precision for different mapping types

search method keyword phrase phrase AND Anatomy phrase in title
perfect match
useful match
non-match

64.7
15.7
19.6

26.7
18.7
54.6
?

?

?
superclass of the FMA concept; and (3) the rest is regarded as non-match. It
turned out that only 33% of the mappings were perfect matches. Subsequently,
we switched to phrase-based search which lowered the number of mappings to
6.7% (6 439 absolute, column phrase in Table 1). To overcome the problem of
obviously irrelevant mappings we confined the results to articles containing the
FMA label as a phrase and the keyword Anatomy. This resulted in almost
50% wrong mappings (column phrase AND Anatomy in Tab. 1). By further
restricting the search to pages which also contain the keyword Anatomy we
tried to limit the results to articles of the target domain. However, this dropped
the number of matches because Anatomy does not necessarily appear in all
articles dealing with anatomical topics. Still this did not lower the number of
irrelevant articles significantly. Finally we confined the search to the article titles
which reduced the ratio of non-matches to less than 20% (column phrase in title
in Table 1).

The additional restrictions cut down the overall number of mappings from
6 439 to 4 104. We decided to trade precision for recall, provide less mappings
and in turn avoid dissatisfaction due to irrelevant mappings.

3.7 DICOM Ontology
Since one of our main goals is to incorporate existing data based on established
standards, the design of the clinical ontologies is to quite some extent aligned
with the DICOM standard. This standard is a world wide established format
used by hospitals to store the results of visual examinations (e. g., X-Ray, CT,
PET, etc.) as well as patient and image metadata.

There are other efforts towards creating a DICOM ontology6 but to our knowledge no OWL model has been released so far. Therefore we decided to model a
MEDICO DICOM ontology. Our rationale was to keep closely with the DICOM
standard in its version 3 from 2008. The reason was that many processes of
clinical data management are use this structure and our intention is to support
these processes instead of replacing them.

As discussed above (Sect. 3.4), the imaging equipment manufacturers do not
always follow standards when it comes to storing imaging metadata. We also see
this in our corpus. To cope with this fact, we decided on a hybrid methodology
for our modeling. We combined a data-driven bottom-up with a top-down approach driven by the DICOM standard. We started by looking at the metadata
available in the images of our corpus (see Sect. 5.1) and checked the standard
for regulations regarding format, value ranges and meaning.
6 For example within the caBIG project, http://cabig.nci.nih.gov/

M. M oller, S. Regel, and M. Sintek

The DICOM standard organizes data elements in a simple hierarchy. In a
clinical data management system each patients images arranged in one or more
Studies. A Study contains one or more Series which in turn contains one or
more Image(s). Our modeling reflects this hierarchy and assigns properties only
once depending on their domain. Each DICOM file has a comprehensive DICOM
header which contains information about the patient, study and series it belongs
to, date and time and several image acquisition parameters. In the following we
introduce our approach by example.

Each metadata element in the DICOM standard has a fixed so called DICOM
tag identifying it in the DICOM file header. To control the mapping process
from DICOM metadata fields to properties in our ontology we annotated those
properties with the respective tag using the annotation property dicomTag.

For example, the DICOM tag 0010,1010 always identifies the patients age
for the current image/series, which we store in the datatype property age. Since
DICOM specifies date and time formats differently from the XSD-based datatype
properties in our ontology we have added the annotation property converter in
order to control the conversion of each datatype property separately.

4 Application

Fig. 4 shows the annotation frontend of our application. Images are loaded and
displayed using the API of the image processing program ImageJ7. Using this library images are segmented into regions of interest (ROI). Currently we support
rectangular, ellipsoid and arbitrary polygons to confine these regions. These three
types are modeled as different classes in our ontology. As discussed in Sect. 3.1,
we distinguish between anatomy, image observation characteristic and disease
which can be annotated separately for each ROI. To ease the task of finding
appropriate annotations we use auto-completing combo-boxes. While typing in
a search term, concept names with matching prefixes are shown in a drop down
box and can be selected.

4.1 DICOM Metadata Extractor

Based on our modeling of the DICOM ontology we have extended the metadata
extraction framework Aperture8 with a metadata extractor for DICOM images.
Controlled by the special annotation properties described in Sect. 3.7, it crawls
file systems, web servers and WebDAV repositories and converts all available
DICOM metadata to instances of the classes modeled in our ontology.

In fact, each DICOM image contains all information about patient, series and
study which leads to a huge data redundancy considering volume datasets (i. e.,
a stack of images generated by a CT scan) which often contain more than 100
images. In such cases all images of a series are linked to the same instance of a
patient in contrast to the redundant storage in the DICOM headers.

http://rsbweb.nih.gov/ij/
http://aperture.sourceforge.net/
?

?

?
Fig. 4. Annotation Frontend

In our annotation application this metadata can be accessed during annotation and retrieval and is rendered using the presentation vocabulary Fresnel [16].
Additionally, we have set up a separate faceted browsing application9 using
MITs Longwell.10

4.2 Annotation Interface

Contrast Windowing: The medical background knowledge tells a radiologist
that, e. g., he has to adjust the contrast range to 80-1000 if he wants to highlight
bones in a CT scan. Likewise we offer the TissueType annotations introduced
in Sect. 3.5 in the annotation tool to dynamically adjust the contrast range of
the current image (see Fig. 4). This feature is included in many medical imaging

http://www.dfki.uni-kl.de/medico-longwell-2.5.5
http://simile.mit.edu/wiki/Longwell

M. M oller, S. Regel, and M. Sintek

Fig. 5. Search Frontend

applications. But here we use one single abstract modeling of tissue types both
for manual annotation as well as to control the preprocessing in automatic image
annotation workflows, which is beyond the scope of this publication and further
described in [11].
Body Region Visualization: The mapping between DICOM body region and
FMA body region annotations as introduced in Sect. 3.4 is also used to provide
a visualization of the body region from which the current image is taken. Upon
loading an image, the body region metadata tag of the DICOM image is used to
visualize the body region of the current image in the silhouette of a prototypical
human (see Fig. 4).
Patient History Visualization: Medical diagnosis is largely dependent on
the medical history of a patient. On this account we offer a visual timeline
of previous radiological examinations. Each available image series is displayed
as a box on a time axis in chronological order (see Fig. 4). This timeline is
generated on the fly from the automatically extracted information from the
?

?

?
DICOM headers and former manual annotations from the radiologists. Even
without any manual annotations, the user immediately gets an overview about
former visual examinations of the patient. Clicking on a series opens a window
with all available information about this series and links to corresponding images.

4.3 Search Interface

In the Lucene11-like free text search, arbitrary queries can be created by specifying Lucene labels such as anatomy: and corresponding values like Index.
This allows the creation of hybrid queries searching for manual medical annotations as well as specific metadata values as extracted from the DICOM
metadata extractor (Sect. 4.1). For example, you can specify that the name of
the patient has to match a certain substring, e. g., by adding name:Manuel
to the query. The values of search labels such as anatomy and disease are
mapped in the background to concepts in the medical ontologies.

The results contain a thumbnail, the full path of the image and all available annotations. By clicking on the RDF icon a separate frame is opened which
visualizes all annotations. A click on the Expand button retrieves all child concepts of all annotated concepts and thus allows the exploration of the semantic
neighborhood.
Query Expansion: For searching we leverage the structural information in the
medical ontologies. In the part-of hierarchy of the FMA there is a path from the
concept Index finger (the concept used originally to annotate the image) to Index
which was the search concept in the example in Fig. 5. Internally we use this
information to compute a query expansion and translate it into a SPARQL [17]
query. Using the FMA, a search for concept X searches for the concept itself and
all concepts which have a (transitive) link via the properties regional part of and
constitutional part of to X. To rank the results, we compute the path length from
the search concept to the concept used for annotation of the search results. Using
ICD-10, we traverse the subClassOf property downwards.

Traversing the FMA recursively with SPARQL queries at runtime turned out
to be way too slow. The FMA uses the equivalent of multiple inheritance in
the part of hierarchy. E. g., Heart is both part of Set of thoracic viscera and of
Content of middle mediastinum. Thus we decided to materialize a spanning tree
for the properties regional part of and constitutional part of starting from the
concept Human body. By materializing the shortest distances to the root node
of this closure, we are able to perform a query expansion with arbitrary search
depth limit in two retrieval operations: The first query is used to retrieve the
list of classes in the part of spanning tree down to a certain depth limit and the
second to retrieve the annotated images.

Of course we would prefer to find a solution that did not require to materialize
closure and distance information, since this would save us a lot of memory. But
at the time of writing, our system setup with SwiftOWLIM12 version 3 on a

http://lucene.apache.org/
http://www.ontotext.com/owlim/

M. M oller, S. Regel, and M. Sintek

quadcore machine with 32 GB RAM did not allow reasonable retrieval times
without materialization.
Multilingual Annotation and Search: Another advantage of using existing
ontologies such as the FMA is the availability of multilingual synonyms. Cur-
rently, we allow annotation and search using any synonym available in the FMA.
English, Latin, French, German and Filipino are covered and the preferred13 annotation language can be changed at runtime.

5 Evaluation

5.1 Our Corpus

Acquiring medical images for testing purposes is a big problem. Legal regulations enforce a very strict anonymization both for clinical patient records as well
as any personal information in the DICOM headers. Because we wanted to test
our extractor with real data and avoid systematic errors by testing only with
values generated by anonymization algorithms, we added non-anonymized personal images to the corpus. All images and metadata shown on screenshots in
this publication belong to examinations performed on one of the authors during
the summer of 2008.

5.2 Qualitative Evaluation

When it comes to the evaluation of systems using technologies from the Semantic Web, evaluation turns out to be problematic. Measuring only quantitative
features of such systems is certainly possible but in many cases does not reflect
a measurement of the claimed scientific contributions, since they are first and
foremost of a qualitative nature.

To our knowledge, there is no clinical imaging database system that fuses
image metadata (patient name, age, gender, etc.) and medical annotations
(anatomy, imaging characteristic, disease) in the same formalism and allows
searching for both in a unified manner. Thus we decided against presenting
tables of retrieval times of our system.14 Firstly, because they do not really reflect the important features of this system and secondly because they are not
comparable to existing systems in clinical practice. Instead we would like to discuss three query scenarios which contrast the features of RadSem with existing
systems.
Scenario 1: Give me all images of patient X. This can be handled directly
by existing clinical image data base systems (PACS15) since this query can be

13 The multilingual synonyms are not available for all FMA classes.
14 Our efforts, e. g., for materialization of distances as described above, result in re-

trieval times always below one second even for large query expansions.

15 Picture Archiving and Communications Systems.
?

?

?
reduced to a simple keyword search across metadata elements. RadSem supports
this type of query in the same manner.
Scenario 2: Give me all images from patients with a broken index finger.
Existing systems require two tasks to be performed for the retrieval of this type
of query. In the first task, the radiology information system (RIS) which stores all
radiological findings has to be searched for textual descriptions of broken index
fingers. The facts that radiologists tend to use abbreviations in their reports
and that the same medical incident could also be described using different words
(e. g., fracture of index finger) or a different language limit the recall significantly.
The second task for the radiologist is then to choose to retrieve a selection of
images referenced by the radiology reports. Not all of these images necessarily
have to deal with a broken index finger but might also deal with fractures of
other bones which the patient might have had during the same time.

In RadSem both tasks can be performed in a single step. In the free text

search, the query would simply be anatomy:index disease:fracture.
Scenario 3: Give me all pictures dealing with enlarged lymph nodes. Patients
potentially suffering from lymph node cancer (lymphoma) are usually scanned
using Computed Tomography, treated if cancerous lymph nodes are detected and
called in for a number of follow-up examinations after the treatment over a period
of several months. During these follow-up examinations the radiologist searches
for lymph nodes which have changed in size compared to previous examinations.
This requires access to previous images. For the same patient, this query can be
handled by existing PACS. Their interfaces usually allow opening of any image
series available for the current patient.

One important advantage of RadSem is that the radiologist here can also
directly search for images from other patients with the clinical finding of having enlarged lymph nodes. Additionally, the query is not limited to a specific
lymph node. The query anatomy:lymph node visual:enlarged retrieves all images which are annotated with the anatomical concept Lymph node or any
part of it and the visual characteristic enlarged. One could also think of refining
this query by adding date ranges, patient gender, etc.
Summary: These examples clearly show that there are severe limitations for
searching existing medical image database systems. Despite the fact that there
are standardized vocabularies such as ICD-10 and well developed ontologies like
the FMA, these technical developments have not yet found adaptation in clinical
practice. This section pointed out these limitations and described our approach
to overcome them by using a common representation for medical background
knowledge and annotations, patient metadata and image acquisition parameters
using the Semantic Web standards RDF and OWL.

6 Conclusion and Future Work

We have presented a semantic annotation and retrieval tool RadSem for medical
images. It leverages on background information about the medical domain from

M. M oller, S. Regel, and M. Sintek

formal ontologies and also covers clinical data management at several steps. Ontology based metadata extraction and medical annotations are represented using
RDF and OWL and lay the basis for clearly arranged information visualizations
of patient history, image annotations and search results. We presented different
approaches to minimize the additional effort for adding medical annotations.

We have described our approach for formalizing the structure of the ICD-10 in
OWL and presented several augmentations and mappings for FMA with further
medical knowledge (e. g., about tissue types) and external data sources.

Articles about medicine in Wikipedia are in the majority of the cases understandable for a general audience but at the same time the scientific level of
the articles is rather low compared to peer reviewed scientific publications as
for example in PubMed.16 Therefore we will apply the technique described in
Sect. 3.6 to generate links from ICD-10 concepts to relevant PubMed articles.

On the imaging side, we plan to integrate low-level content-based image retrieval techniques to additionally support query by image in conjunction with
the presented search across medical annotations and patient metadata. We also
aim to integrate fully-automatic semantic image parsing techniques to provide
facilities for automatic image segmentation and the automatic suggestion of semantic image annotations.
