ONTOCOM Revisited: Towards Accurate Cost
Predictions for Ontology Development Projects

Elena Simperl, Igor O. Popov, and Tobias B urger

Semantic Technology Institute (STI) Innsbruck, University of Innsbruck,
{elena.simperl,igor.popov,tobias.buerger}@sti2.at

6020 Innsbruck, Austria

Abstract. Reliable methods to assess the costs and benefits of ontologies are
an important instrument to demonstrate the tangible business value of semantic technologies within enterprises, as an argument to encourage their wide-scale
adoption. The economic aspects of ontologies have been investigated in previous
work of ours. With ONTOCOM we proposed a cost estimation model for ontologies and ontology development projects. This paper revisits this model and
presents its latest achievements. We report on a comprehensive calibration of
ONTOCOM based on a considerably larger data set of 148 ontology development projects. The calibration used a combination of statistical methods, ranging
from preliminary data analysis to regression and Bayes analysis, and resulted a
significant improvement of the prediction quality of up to 50%. In addition, the
availability of a representative data set allowed us to identify meaningful directions for customizing the generic cost model along particular types of ontologies,
and ontology-like structures as those specific to the emerging Web 3.0. Last but
not least, we developed a software tool that allows ontology development project
managers to easily use and adapt and to systematically calibrate the model, thus
facilitating its adoption in real-world projects.

1 Introduction

Ontology-based technologies use machine-understandable formalizations of expert
knowledge to provide intelligent and economically feasible solutions to many prevailing
problems in todays information and knowledge management systems. In order to encourage the wide uptake of ontology-based technologies at corporate level, instruments
to assess their economic benefits and to predict the total costs of their development and
deployment are a must.

CIOs are starting to acknowledge the technical value of these technologies for en-
terprises. In the last years, early adopters have been increasingly using them in various
application settings ranging from content management to social networks and enterprise
application integration. Core technological building blocks and development platforms
are meanwhile available from established vendors. Despite this promising position, it
is still difficult to argue in favor of ontology-based technologies in front of CFOs because of the lack of convincing measurable business benefits, and of proven-and-tested
methods to determine and quantify these.

The economic aspects of ontologies have been investigated in previous work of
ours. With ONTOCOM we presented the first cost estimation model for ontologies and

L. Aroyo et al. (Eds.): ESWC 2009, LNCS 5554, pp. 248262, 2009.
c Springer-Verlag Berlin Heidelberg 2009
?

?

?
ontology development projects [19]. Essentially, estimating the efforts required to build
an ontology can be undertaken in similar ways as in every other engineering discipline
dealing with the question of costs and benefits. It requires the identification and study
of those aspects of the engineered artifact, in our case, the ontology, and the engineering process thereof, that are likely to have an impact on the total development costs.
Moreover, it uses specific methods and techniques to make effort predictions based on
these so-called cost drivers. Given these baseline commonalities, it is understandable
that ONTOCOM largely benefits from the experiences in cost estimation made in the
related area of software engineering. Through expert interviews, we identified the most
relevant cost drivers for a wide class of ontology development projects. In a second step
we acquired relevant data from 36 ontology-related projects and calibrated a parametric
cost prediction equation. This resulted in a first release of the ONTOCOM model, which
was used as a baseline for further studies. This model was presented in [19].

In this paper we revisit the model and presents its latest achievements. We report
on a second, more comprehensive calibration of ONTOCOM based on a considerably
larger data set of 148 ontology development projects. The calibration used a combination of statistical methods, ranging from preliminary data analysis to regression and
Bayes analysis, and resulted in a significant improvement of the prediction quality of
up to 50%. In addition, the availability of a critical mass of data allowed us to identify
meaningful directions for customizing the generic model along particular types of on-
tologies, and ontology-like structures as those specific to the emerging Web 3.0. Last
but not least, we developed a software tool that allows ontology development project
managers to easily use and adapt, and to systematically calibrate the model, thus facilitating its adoption in real-world projects.

The remainder of this paper is structured as follows: in Section 2 we briefly introduce
the ONTOCOM model, its assumptions, main cost drivers, and prediction algorithm.
Section 3 gives full particulars on the newest calibration of the model and analyzes the
implication of its results. The software tool for using, adapting and calibrating ONTOCOM is presented in Section 4, whilst related work is discussed in Section 5. The paper
is concluded with a summary of the most important findings and planned future work
(cf. Section 6).

2 ONTOCOM in a Nutshell

ONTOCOM is a generic cost model for ontology development. The model is generic
in that it assumes a sequential ontology life cycle, according to which, an ontology development project starts with the analysis of the domain and of the requirements to be
fulfilled by the ontology; once an initial set of requirements is specified, the ontology
is conceptualized, implemented and evaluated. The outcomes of the evaluation can lead
to further iterations of the process either at the domain analysis, the conceptualization
or the implementation phase. This simplified ontology development process is depicted
in Figure 1. Alternative engineering strategies such as rapid prototyping or agile meth-
ods, which are based on different life cycles, are not yet considered.1 This limitation

1 We refer to [7] for a discussion on the relation between this process model and the IEEE

standards [8].

E. Simperl, I.O. Popov, and T. B urger

o
c
u
m
e
n
a

t

t
i

o
n

v
a
u
a

l

t
i

o
n

l

n
o
w
e
d
g
e
a
c
q
u
s
i
t
i

i

o
n

Requirements analysis
motivating scenarios, use cases, existing solutions,
cost estimation, competency questions, application requirements

Conceptualization
conceptualization of the model, integration and extension of
existing solutions

Implementation
implementation of the formal model in a representation language

Fig. 1. Typical Ontology Development Process

was addressed in previous work of ours; the interested reader could refer to [18], which
describes how the generic model should be customized to suit collaborative ontology
engineering scenarios. The cost estimation model is realized as follows. First, a work
breakdown structure for ontology development processes is defined. This is required in
order to reduce the complexity of project budgetary planning and controlling operations
down to more manageable units [1]. The global costs can then be derived using various
prediction methods. Examples thereof include the expert judgement method, the analogy method or the parametric/algorithmic method [19]. ONTOCOM applies primarily
the latter. The effort estimates are calculated using a parameterized mathematical for-
mula. The parameters of this formula are given start values in predefined intervals, and
are subsequently calibrated on the basis of previous project data. This empirical information complemented by expert opinions is used to evaluate and revise the predictions
of the initial a-priori model, thus creating a validated a-posteriori model.
?

?

?
P M = A  Size 

CDi

(1)

According to the parametric method, the total development effort is associated with
cost drivers specific for the ontology development process and its main steps and ac-
tivities. Experiences in related engineering areas [1,9,20] let us assume that the most
significant cost driver is the size of the ontology. This is reflected in Equation 1 in the
parameter Size that corresponds to the size of the ontology, that is, the number of kilo
primitives which are expected to result from the conceptualization phase (including
fragments built by reuse or other knowledge acquisition methods). The possibility of a
nonlinear behavior of the model with respect to the size of the ontology is covered by the
parameter . The constant A represents a baseline multiplicative calibration constant in
person months and stands for the costs which occur if everything is normal for developing an ontology with the size of 1 (1000 ontological primitives). The remaining cost
drivers CDi have a rating level (from V eryLow to V eryHigh) that expresses their
impact on the development effort. For the purpose of a quantitative analysis each rating
level of each cost driver is associated to a weight referred to as effort multiplier EMi.
The productivity range P Ri of a cost driver stands for the ratio between the highest and
the lowest effort multiplier of a cost driver P Ri = max(EMi)
min(EMi) ) and is an indicator for
the relative importance of the respective cost driver for the overall cost estimation [1].
?

?

?
The ONTOCOM cost drivers can be divided into three categories [19]:

Product-related cost drivers that account for the impact of the characteristics of the
ontology on the overall costs. Cost drivers that were identified for the task of ontology development are, for instance, the domain analysis complexity, the conceptualization complexity, or the documentation needs.

Personnel-related cost drivers that emphasize the role of team experience, ability and
continuity with respect to the effort required by an ontology development project.
In this category, we mention the ontologist/domain expert capability, the level of
expertise with respect to languages or tools, and the personnel turnover.

Project-related cost drivers that are related to the characteristics of the overall ontology development project and their impact on the total costs. Relevant cost drivers
include the level of support and automation provided by ontology engineering soft-
ware, and the additional efforts related to operating the project in a decentralized
environment.

The ONTOCOM cost drivers were defined after extensively surveying ontology engineering literature from the last decade and conducting expert interviews, and from
empirical findings of numerous case studies in the field [16]. For each cost driver we
specified in detail the decision criteria which have to be taken into account by the user
of the model in order for her to determine the concrete rating level of the driver in a particular project setting. For example, for the cost driver CCPLX, accounting for the costs
produced by a particularly complex conceptualization, we predefined the meaning of the
rating levels as shown in Table 1. The decision criteria associated with a cost driver are
typically more complex than for the CCPLX example, and might be even divided into
further subcategories, whose impact is aggregated to a final rating value by means of normalized weights. The first release of the ONTOCOM model was evaluated on a data set
of 36 ontologies. Using regression and Bayes analysis as described in [19] we calibrated

Table 1. The Conceptualization Complexity Cost Driver CCPLX

Rating Level Description
Very Low
Low

Nominal

High

Very High

The conceptual model is a concept list.
The conceptual model is a taxonomy. A high number of patterns supporting the
creation of the taxonomy are available. No special modeling constraints are imposed through application requirements.
The model contains a taxonomical structure and domain properties. Again, modeling patterns are available, while the application setting imposes some simple
constraints which produce additional modeling overload.
The model contains in addition to the previous case axioms. the engineering team
can not resort to a feasible number of modeling patterns to ease the conceptualization task. In the same time, the number of application-driven constrains increases.
The conceptual model is an axiomatized ontology containing both schema and
instance data. There are few to no modeling patterns to support the conceptualization task, while the number of application-driven constrains is significantly
higher.

E. Simperl, I.O. Popov, and T. B urger

the model and obtained an initial prediction quality of 75% of the data points within a
range of 75% of the estimates. For the corresponding 30% range the model covered
32% of the real-world data. These results, thus promising, required further optimization,
both with respect to the size of the data set, and the statistical methods used. The next
section explains how the second calibration of ONTOCOM overcame these limitations.

3 Towards Accurate Cost Predictions

The second calibration of ONTOCOM started with a comprehensive survey, in which
participants in ontology development projects were interviewed with respect to aspects
related to the different cost drivers of the model. The survey was supported by an online
questionnaire. The questionnaire consisted of 42 questions divided into four parts. The
first part referred to general aspects of the ontology development project at hand, including the size of the resulting ontology, its purpose or the effort needed to build it. The
second part consisted of questions related to particular ontology development phases,
such as domain analysis, conceptualization, implementation, documentation and evalu-
ation. The third part of the questionnaire was related to characteristics of the development team which was engaged in the project. Finally, the fourth part contained questions
about the software used to support and automatize process. A detailed presentation of
the structure of the ONTOCOM questionnaire can be found in [17]. This questionnaire
was used, to a large extent, in the initial calibration of the model in 2006 as well. Minor
modifications were made based on feedback received in the first round.

The survey contains data from 148 ontology development projects. Experiences in
effort estimation in related engineering disciplines, notably software engineering, let
us assume that this data set is large enough to form the basis for a reliable prediction model of the size of ONTOCOM [5]. Approximately 50% of the data was collected
during face-to-face or telephone interviews, the rest via a self-administered online ques-
tionnaire.2 Approximately 95% of the ontologies collected were built in Europe, whilst
nearly 35% originated from industry parties. The size of the ontologies in the data set
varied from 60 to 11 million entities. Most ontologies were implemented in OWL DL
(30%), followed by WSML DL and WSML Flight (around 10% each) and RDF(S)
(9%). The duration of the projects varied from 0.02 to 156 person months.

We then calibrated the model on this enlarged data set using the same statistical
methods as in the previous ONTOCOM release. This led to a comparatively minor improvement in the prediction accuracy. The previous calibration of the model, on 36 data
points, resulted in an accuracy of 31.82% within a 30% error margin. The new calibra-
tion, based on a three-times larger data set, led to an accuracy improvement of around
2% in the same margin. The reason for this minimal change could be traced back to
the issue of unbalanced data sets [2]. To solve this issue, and to enable the model to
reach an accuracy level which allows it to be used in real-world enterprise settings, we
needed to further analyze the nature of the data collected, as described for instance in
[10,11,14,13].

Consequently, we conducted a preliminary data analysis on the enlarged data set following the framework of Lin & Mintram [13]. We were particularly interested in the first

2 The questionnaire is available at http://ontocom.sti-innsbruck.at
?

?

?
six steps of the framework, which dealt with the identification and removal of outliers
from data sets and with the determination of the cost drivers with the greatest impact on
the estimated effort. The preliminary data analysis resulted in a considerable improvement of our predictions. First, outliers in the numerical variables were removed and
the remaining data was transformed to a normal distribution. The same was applied to
ordinal variables and the entire process was repeated until all outliers were eliminated.
Once this was achieved, we performed a correlation and regression analysis, followed
by an analysis of variance (ANOVA) on the ordinal variables. Finally the data set was
calibrated (using multivariate regression and Bayes analysis) in order to combine the
empirical data with the expert data.

The ONTOCOM model contains 2 numerical variables and 11 ordinal ones. The
numerical variables are Person Months (PM), which is a response variable, and Size
which is an exploratory variable.3 For the calibration of the model we considered only
those cost drivers for which expert data was available. These are listed in Table 2. The
rating values collected for each cost driver for each data point (ranging from Very Low
to Very High) were transformed to nominal numbers (ranging from 1 to 5).

Table 2. Variables Used in the Preliminary Data Analysis

Person months
Size of the ontology
Domain Complexity
Conceptualization Complexity
Implementation Complexity
Required Reusability
Documentation Needs
Ontology Evaluation

Type
Abbreviation Cost Driver
Numerical

Numerical

Ordinal

Ordinal

Ordinal

Ordinal

Ordinal
?

?

?
Ordinal
OCAP/DECAP Ontologist/Domain Expert Capability Ordinal
OEXP/DEEXP Ontologist/Domain Expert Experience Ordinal
Ordinal

Ordinal
LEXP/TEXP

Ordinal

Personnel Continuity
Language/Tool Experience
Multisite Development

In the following we briefly describe each step of the framework, and discuss the

results of the subsequent calibration.

3.1 Step 1: Outlier Detection in Numerical Variables

Table 3 summarizes the analysis of the numerical variables PM and Size. For each
variable the name of the variable, the number of observations, the range of values and
the mean and standard deviation are shown. The PM variable ranges from 0.02 to 156

3 Response variables are those that are observed to change in response to the explanatory vari-
ables. In our case the effort variable (PM) is the response variable; all other are explanatory
variables.

E. Simperl, I.O. Popov, and T. B urger

Table 3. ONTOCOM Numerical Variables

Var. name Obs. Minimum Maximum Mean Std. Dev.

Size

10.22 25.60
76.14 904.09

148 0.02
148 0.006
?

?

?
person-months. The average size of PM is 10.22 person-months with a standard deviation of 25.60. The Size variable ranges from 0.06 to 11000 kilo entities, with an average
size of 76.14 kilo entities with a standard deviation of 904.09.

The outlier detection was performed using box plots, which is a graphical way of
depicting groups of numerical data. Our initial run identified 5 data points as outliers
which were removed from the analysis.

3.2 Step 2: Data Transformation

In this step we transformed the data so that it approximates a normal distribution.
As both numerical variables were not normally distributed, we created new variables,
named LnPM and LnSize, by applying the natural logarithm, and tested their distribution using histograms (cf. Figures 2 and 3, respectively). The new variables LnPM and
LnSize were used in the subsequent steps of the preliminary data analysis.

Fig. 2. Histogram of the Variable LnPM

Fig. 3. Histogram of Variable LnSize

3.3 Step 3: Outlier Detection in Ordinal Variables

Ordinal variables correspond to the three categories of cost drivers in the ONTOCOM
model introduced in Section 2. The outlier analysis was performed again using box plot
diagrams and led to the removal of 44 data points (in two iterations). The box plot in
Figure 4 depicts the remaining data set without outliers in the ordinal variables. We also
remeasured the normal distribution of this data set through Quantile-Quantile (Q-Q)
normality plots for the numerical variables LnPM and LnSize (Figures 5 and 6, respec-
tively). Steps 4 to 6 of the preliminary data analysis and the subsequent calibration were
performed on the remaining data set of 99 ontology development projects.
?

?

?
Fig. 4. Boxplot for Outlier Detection of Ordinal Variables

Fig. 5. Normal Q-Q plot for LnPM

Fig. 6. Normal Q-Q plot for LnSize

3.4 Steps 4 and 5: Correlation Analysis and Regression Analysis

Correlation analysis involves calculating a correlation coefficient which measures the
relationship between the response and the explanatory variable. The coefficient has a
value between +1 and 1. If the value of the coefficient if greater than 0 then the variables are positively correlated, in other words, a rise of the explanatory variable will
result in a rise of the response variable, whilst a decrease in the explanatory variable
will result in a decrease of the response variable. The value is an indicator on how
strong this relationship is, with 1 staying for a pair of perfectly positively correlated
variables. Conversely, if the value is below 0 then an increase in the explanatory variable will result in a decrease of the response variable and viceversa, with 1 standing
for variables that are perfectly negatively correlated. 0 means that the variables are not
correlated. Two commonly used measures for correlation analysis are the Spearmanss
rank coefficient (for ordinal variables) and Pearsons rank coefficient (for interval or ration type data). Both coefficients are relevant to our data set. The threshold values are set
to Pr > |0.65| and Pr < |0.1|, for high and low correlation, respectively. The correlation

E. Simperl, I.O. Popov, and T. B urger

analysis indicated no correlation in the numerical or ordinal variables. A subsequent regression analysis confirmed that the correlation between LnSize and LnPM is in the
allowed intervals.

3.5 Step 6: Stepwise ANOVA Analysis

In the next step we performed a stepwise ANOVA (Analysis of Variance) analysis to
compute the best 6-variable ONTOCOM model. ANOVA is a statistical method commonly used to analyze data sets with ordinal type variables [2,10,14,13]. The data set
variables are referred to as factors and the scale values of the variables as factor lev-
els. ANOVA tests whether there are significant differences between factor level means.
It is assumed that the variance of the level means will be large if there are differences
between the groups, and small if there are none. The variance of the means between
groups is divided by the variance within the groups to calculate the F-statistic mea-
sure(Equation 2). The result will be close to 1 if the group means are not significantly
different from one another.

F = V ariance

between
V ariance within

(2)

The best 6-variable model is defined iteratively. In each iteration one identifies the
next explanatory variable that has the most impact on the response variable. This is
done by examining the adjusted coefficient of determination (i.e., the R2 value). The
variable with the highest adjusted R2 is the variable that best explains the variation in
the response variable and is thus kept in the model [13]. In each iteration the adjusted
R2 values are recalculated in order to account for the variables previously selected.

Applying this procedure to ONTOCOM and our data set revealed that the six variables that best explain the behavior of the effort are (in this order): Domain Complexity (DCPLX),Ontology Evaluation (OE), Language\Tool Experience (LEXP\TEXP),
Ontologist\Domain Expert Capability (OCAP\DECAP), Documentation needs
(DOCU), and Personnel Continuity (PCON). Our analysis based on the adjusted R2
values also pointed out that any additional variables added to the model would not have
a significant impact on the response variable.

3.6 Calibration Results and Discussion

The preliminary data analysis resulted in a set of 99 data points and identified the six
variables which best explain the variation in the response variable. We performed a
calibration on this best 6-variable model to combine the expert opinion with the data
collected. Our results showed a substantial improvement of the accuracy of predicting
results within a 30% margin from 33.78% to 45.82% and within a 75% margin
from 66.89% to 79.80. We then performed our calibration with all eleven variables to
examine the impact of the other variables. Our result was a slight increase in prediction
accuracy of 46.46% within the 30% error margin, whilst for the 75% margin the
differences were even less significant.

The analysis of the behavior of the cost drivers is consistent with our previous findings in [19], based on a 36 data point set, particularly for two of the three most dominant
?

?

?
cost drivers, Domain Complexity (DCPLX) and Ontology Evaluation (OE). Addition-
ally, we examined the behavior of productivity, i.e., the ratio between the Size and PM
variables, and prediction accuracy for the larger margin error of 75%. The box plot
of the productivity factor showed a stable set with only 7 outliers from the full data set.
The accuracy for predicting results within 75% increased from 66.67% to 79.80%
after the preliminary data analysis. This supports the fact that ontology development
projects rarely exhibit any erratic behavior in their costs, in other words, there are only
rare cases when the actual costs might exceed the anticipated costs by several times.
Exponential behavior of the effort w.r.t. Size confirmed our previous findings that ontology development projects exhibit economies of scale. This suggests that building larger
ontologies is economically feasible.

The dominance of the cost drivers DCPLX and OE indicates that any facilitation
in these tasks may result in major efficiency gains. While tools such as wikis may be
helpful especially in collaborative settings, they are still rarely used for the purpose
of domain analysis. More generally, the results of the interviews indicated a low tool
support for this task. This situation could be improved by applying methods such as
automated document analysis, or ontology learning approaches, to support the analysis
of the domain and most notably the assessment of the information sources available.
A similar conclusion can be drawn for ontology evaluation. Despite of the availability
of automated approaches such as unit tests, consistency checks or taxonomy cleaning
techniques, ontology evaluation still lacks tools which are easy to use and comprehensible for most users. The dominance of personnel-related cost drivers like PCON,
LEXP/TEXP, or OCAP/DECAP suggests that more training programs in the area of
ontology engineering, better collaboration support, and an improved, more fine granular documentation of decisions taken during ontology engineering may also result
in efficiency gains. In this context, interviews also revealed that ontology engineering
methodologies are rarely used. A more structured process-level support within development environments may again lead to an improvement.

It should be noted that the data in our set was collected from ontology development
projects from various domains, and that the ontologies were built for different purposes
and used in different environments. Experiences with cost estimation models suggest
that a customized model, optimized for particular project or enterprise settings, would
yield even better prediction results [5]. This potential mismatch between the generality
of the model and its suitability for specific environments can be (partially) compensated by a large, representative data set used to calibrate the model. The data set in our
current calibration enabled us to develop a second release of the ONTOCOM with a significantly improved prediction quality. This can be traced back to the preliminary data
analysis performed, but also to the size of the data set, which is sufficiently high for the
number of variables in the model. The results we obtained on the calibration on 148 data
points suggest, nevertheless, that the predictions of ONTOCOM as a generic model
should improve with the availability of additional data. Data analysis on an extended
set might show slight differences with respect to the dominant behavior of certain vari-
ables, however, our experience shows us that the relevancy of the cost factors is very
much consistent. The quality of the ONTOCOM predictions could be further optimized
by establishing variants of the model tailored to reflect particularities of certain classes

E. Simperl, I.O. Popov, and T. B urger

of ontological structures. Following this line of reasoning, we have developed ONTOCOM -Lite, a cost model for lightweight ontologies, in particular folksonomies and
taxonomies. Taxonomies are widely used in various commercial environments and the
process of developing taxonomies is mature (such well defined development processes
are a prerequisite for adapting our model, notably to modify the work breakdown structure and to identify new cost drivers, cf. Section 2). Folksonomies, on the other side, are
semi-structured, collaboratively emerging knowledge structures which are increasingly
used for information and knowledge management purposes. Despite their popularity
among users, the total costs of tagging are an interesting figure to argue in favor of this
new technology within an enterprise.

4 ONTOCOM Tooling

We developed a software tool in order to allow project managers to automatically perform the calibration process on an arbitrary data set, and to use the model to make

Fig. 7. The ONTOCOM Main User Interface
?

?

?
Fig. 8. The ONTOCOM cost predictions UI

estimations for planned ontology development projects.4 The tool provides an easy-to-
use user interface for running calibrations and predictions. It provides support for the
calibration of the ONTOCOM model using either existing, self-owned data, or a combination of both. Furthermore, users are able to customize the model, selecting the cost
drivers to be taken into account for a calibration. The ONTOCOM tool is based on Microsoft Excel. It consists of 3 spreadsheets reflecting the user interface, and additional
spreadsheets that are automatically created in the course of the calibration of the model.
The first spreadsheet, depicted in Figure 7, can be used to perform a calibration or to
calculate predictions. Furthermore, it contains information about the calibration results,
including information on the number of data points used in the calibration, the use of
cost drivers, and on the accuracy of the predictions for different thresholds.

The second spreadsheet shows the expert data available for each cost driver and, in
the subsequent columns, the updated productivity range and effort multipliers based
on the calibration of the model. The user is allowed to change the expert values to
better reflect her own opinion. The calibrated values of the cost drivers are used for
predictions. On the third spreadsheet the user has the option to add or remove data
points to the calibration data set.

When the user starts a new calibration the tool creates additional spreadsheets, reflecting the individual calibration steps performed. In the fourth spreadsheet (Empirical
Data (Mapping)) categorical values of the empirical data are mapped to their corresponding values from the expert data. If the latter is not available for a cost driver,
the cost driver will not be used in subsequent calibration steps. In the user interface

4 The tool is available online at http://ontocom.sti-innsbruck.at/tools.htm

E. Simperl, I.O. Popov, and T. B urger

spreadsheet (cf. Figure 7) the user is then notified which cost drivers were not used
in the calibration due to this unavailability. In the next spreadsheet the data is mapped
to logarithmic values, a procedure that is required when using multivariate regression
analyis. The two next spreadsheets present the results of the correlation analysis. The
Correlation analysis spreadsheet contains information about the correlations between
explanatory and predictor variables and between the explanatory variables themselves.
The Post-correlation data contains the selected (logarithmic) data from the previous
spreadsheet based on the result of the correlation analysis. The next spreadsheet is dedicated to the Statistical Analysis. It shows the results of the multivariate regression
and Bayes analysis. For each cost driver, the result is an exponent which is used on the
original values to get a new calibrated value for each of its 5 rating levels. In the last
spreadsheet the data points are tested against the predictions which are derived by the
model using the new calibrated values. The prediction results are evaluated for different
thresholds, which are presented as an accuracy percentage in the interface spreadsheets.
After calibration a user can make predictions for future projects by using an interface
where he selects the values of the parameters for his project, including an estimate of
the size of the ontology (cf. Figure 8). Using the calibrated values the user is presented
with an estimate of the costs in person months.

5 Related Work

Cost estimation has a long-standing tradition in more mature engineering disciplines
such as software engineering or industrial production [1,9,21]. Although the importance of cost issues is well-acknowledged in the semantic technologies community, as
to the best of our knowledge, no cost estimation model for ontology development besides ONTOCOM has been proposed so far. Analogue models for the development of
knowledge-based systems (e.g., [6]) implicitly assume the availability of the underlying conceptual structures. [15] provides a qualitative analysis of the costs and benefits
of ontology usage in application systems, but does not offer any model to estimate
the efforts. [4] presents empirical results for quantifying ontology reuse. [3] proposes
a model for assessing the benefits of using ontologies in particular settings, but the
model proposed in not quantitative, and the analysis of cost and benefit information
is not straightforward. [23] proposes a cost and benefit analysis model for personal
knowledge management which, most notably, takes externalization of knowledge in
the form of formal statements into account. They apply their model to semantic wikis,
but do not provide any empirical evaluation. [12] adjusts a cost estimation model for
Web applications with respect to the usage of ontologies. The resulting cost drivers are,
however, not adapted to the requirements of ontology development and no evaluation
is provided. Nevertheless, the question of cost estimation for ontology-based software
application is relevant, and a possible direction of research for our ontology engineering
economics work. Wolff et al. address the efforts related to the engineering of Semantic Web Services in [24]. They argue that Semantic Web Services projects require less
time compared to their traditional Web services equivalents, but do not treat ontologies,
or any other semantic technology, as a separate part of their observations. Other researchers recently started to investigate economic issues pertaining to the development
of ontology-based applications: In [22] gOnntt, a planning tool for ontology projects
?

?

?
using GANTT charts is introduced in which activities related to the life cycle of networked ontologies are mapped to existing life cycle models. These activities should
be augmented with effort-related information in one of the next releases of the tool.
ONTOCOM could be used to calculate the required effort information.

6 Conclusions

The adoption of ontology-based technologies in commercial settings depends on the
availability of appropriate methodologies and software assisting the ontology engineering process, and on methods for an effective cost benefit management. In earlier work,
we proposed a parametric cost estimation model for ontologies which we recalibrated to
improve the accuracy of effort predictions. By applying a careful preliminary data analysis we could improve the accuracy of our model in a 30% margin by 35% compared
to a first calibration reported in 2006. The calibration confirmed earlier observations
regarding the impact of particular cost drivers and furthermore showed that prediction
accuracy improves with the availability of more data points. The cost drivers that had
the most impact on the results are Domain Complexity (DCPLX), Ontology Evaluation (OE), Language\Tool Experience (LEXP\TEXP), Ontologist\Domain Expert Capability (OCAP\DECAP), Documentation needs (DOCU), and Personnel Continuity
(PCON). We also introduced an Excel-based software tool which offers on-the-fly cost
estimation for ontology projects and automatic calibration support. Future tool development plans include integrating cost estimation into ontology development and management environments such as gOnntt in which the user can assess costs in the same
platform in which the overall ontology development process is managed.

In the context of the EU IP project ACTIVE we will design ONTOCOM Lite, an adaptation of the generic model for lightweight knowledge structures such as folksonomies
and taxonomies. Given the increased number of useful ontologies available on the Web,
ontology reuse slowly becomes feasible. A cost model to estimate the efforts related
to this engineering strategy and to compare it to alternatives such as development from
scratch is a natural extension of the current ONTOCOM model.5 Last, but not least,
we will use ONTOCOM to provide information about the total development costs of
software applications using ontologies and related technologies. It remains to be investigated whether this will result into an extension of established software engineering
cost models such as COCOMO [1] or whether a new model will be more suitable.

Acknowledgements. The research leading to this paper was partially supported by the
European Commission under the contract FP7-215040 ACTIVE.
