Bridging the Gap between Linked Data

and the Semantic Desktop

Tudor Groza, Laura Dr agan, Siegfried Handschuh, and Stefan Decker

DERI, National University of Ireland, Galway

{tudor.groza,laura.dragan,siegfried.handschuh,stefan.decker}@deri.org

IDA Business Park, Lower Dangan, Galway, Ireland

http://www.deri.ie/

Abstract. The exponential growth of the World Wide Web in the last
decade brought an explosion in the information space, which has important consequences also in the area of scientific research. Finding relevant
work in a particular field and exploring the links between publications
is currently a cumbersome task. Similarly, on the desktop, managing
the publications acquired over time can represent a real challenge. Extracting semantic metadata, exploring the linked data cloud and using
the semantic desktop for managing personal information represent, in
part, solutions for different aspects of the above mentioned issues. In
this paper, we propose an innovative approach for bridging these three
directions with the overall goal of alleviating the information overload
problem burdening early stage researchers. Our application combines
harmoniously document engineering-oriented automatic metadata extraction with information expansion and visualization based on linked
data, while the resulting documents can be seamlessly integrated into
the semantic desktop.

1 Introduction

The World Wide Web represents an essential factor in the dissemination of scientific work in many fields. At the same time, its exponential growth is reflected in
the substantial increase of the amount of scientific research being published. As
an example, in the biomedical domain, the well-known MedLine 1 now hosts over
18 million articles, having a growth rate of 0.5 million articles / year, which represents around 1300 articles / day [1]. In addition, we can also mention the lack
of uniformity and integration of access to information. Each event has its own
online publishing means, and there is no central hub for such information, even
within communities in the same domain. Consequently, this makes the process
of finding and linking relevant work in a particular field a cumbersome task.

On the desktop, we can find a somewhat similar problem, though on a smaller
scale. A typical researcher acquires (and stores) an significant number of publications over time. Generally, the files representing these publications have a nonintuitive name (often the same cryptic name assigned by the system publishing

1 http://medline.cos.com/

A. Bernstein et al. (Eds.): ISWC 2009, LNCS 5823, pp. 827842, 2009.
c Springer-Verlag Berlin Heidelberg 2009

T. Groza et al.

them), and may, in the best case scenario, be structured in intuitive folder hier-
archies. Thus, finding a particular publication or links between the existing ones
represents quite a challenge, even with the help of tools like Google Desktop2.

Semantic Web technologies have been proved to help at alleviating, at least par-
tially, the above mentioned issues. And at the foundation of the Semantic Web we
find semantic metadata. Used in particular contexts, semantic metadata enables
a more fertile search experience, complementing full text search with search based
on different facets (e.g., one can search for a publication by a specific author and
with some specific keywords in its title). In addition, subject to its richness, it can
also leverage links between publications, e.g. citation networks.

Looking at the status of semantic metadata for scientific publications in the
two directions, i.e. the Web and the Desktop, we observe the following. With the
emergence of the Linked Open Data (LOD)3 initiative, an increasing number of
data sets were published as linked metadata. Regarding scientific publications,
efforts like the Semantic Web Dog Food Server started by M oller et al. [2] represent pioneering examples. The repository they initiated acts as a linked data
hub, for metadata extracted from different sources, such as the International or
European Semantic Web conferences, and now hosts metadata describing over
1000 publications and over 2800 people. The manual creation of metadata is their
main drawback, as well as of the other similar approaches. Within the second
direction, i.e. on the desktop, different Semantic Desktop efforts improve the sit-
uation, by extracting shallow metadata, either file-related (e.g. creator, date of
creation), or even publication-related, such as title or authors. In conclusion, we
currently have two directions targeting similar goals and having the same foun-
dation: (i) the <LOD  semantic metadata> bridge, linking publications on the
web, and (ii) the <Semantic Desktop  semantic metadata> bridge, linking publications and personal information, on the desktop.

In this paper, we propose a solution for bridging the two directions, with the
goal of enabling a more meaningful searching and linking experience on the desk-
top, having the linked data cloud as the primary source. Our method consists of a
three step process and starts from a publication with no metadata, each step carried out incrementally to enrich the semantic metadata describing the publication.
The three steps are: (i) extraction  we extract automatically metadata from the
publication based on a document-engineering oriented approach; (ii) expansion 
we use the extracted raw metadata to search the linked data cloud, the result being
a set of clean and linked metadata; and (iii) integration  the metadata is further
enriched by embedding it within the semantic desktop environment, where it is automatically linked with the already existing personal metadata. The main results
of our process are: a simple and straightforward way of finding related publications
based on the metadata extracted and linked automatically, and the opportunity
of weaving the linked publication data on the desktop, by means of usual desktop
applications (e.g. file and Web browser).

2 http://desktop.google.com/
3 http://linkeddata.org/
?

?

?
The remainder of the paper is structured as follows: Sect. 2 introduces the scenario used for exemplifying our approach, while in Sect. 3 we detail the
technical elements of each of the three steps in the process. Sect. 4 describes the
preliminary evaluation we have performed, and before concluding in Sect. 6, we
have a look at related efforts in Sect. 5.

2 Scenario

To illustrate the problem mentioned in the previous section in a particular context,
in the following, we will consider a typical scenario for an early stage researcher (or
any kind of researcher) that steps into a new field. The amount of existing publi-
cations, and their current growth rate, makes the task of getting familiarized with
the relevant literature in a specific domain highly challenging. From an abstract
perspective, the familiarization process consists of several stages, as follows: (i) the
researcher starts from a publication provided by the supervisor; (ii) she reads the
publication, thus grasping its claims and associated argumentation; (iii) reaching
a decision point, she either decides to look for another publication, or she follows
backwards the chain of references, possibly including also publications by the same
authors. This last step usually involves accessing a search engine (or a publication
repository) and typing the names of the authors, or the title of the publication, to
be able to retrieve similar results.

Each of the above activities has an associated corresponding time component:
(i) the time assigned to reading the entire publication (or most of it)  needed to
decide whether the publication is of interest or not, (ii) the time associated with
finding appropriate links and references between publications,4 and (iii) the time
associated with searching additional publications, based on different metadata el-
ements, and (manually) filtering the search results. This time increases substantially when an individual is interested in all the publications of a particular author.
Finally, analyzing the pairs <activity, time component> from the metadata
perspective, and what it can do to improve the overall process, we can conclude
that:

 searching and linking related publications as mentioned above, entails the
(manual) extraction and use of shallow metadata. Thus, performing automatic
extraction of shallow metadata and using it within the linked data cloud, will
significantly reduce the time component associated with this activity.

 reading the publication to a large extent corresponds to mining for the discourse knowledge items, i.e. for the rhetorical and argumentation elements
of the publication, representing its deep metadata. Consequently, extracting
automatically such discourse knowledge items from a publication, will provide the user with the opportunity of having a quick glance over the publica-
tions main claims and arguments and thus decrease the time spent on deciding whether the publication is relevant or not.

4 Following all the references of a publication is obviously not a feasible option. Thus,
the decision is usually done based on the citation contexts mentioning the references
in the originating publication.

T. Groza et al.

Transposing these elements into engineering goals led us to a three step process,
detailed in the following section: extraction  automatic extraction of shallow and
deep metadata; expansion  using the extracted metadata within the linked data
cloud for cleaning and enriching purposes; integration  embedding the resulted
linked metadata within the personal desktop to ensure a smooth search and browse
experience, by using the ordinary desktop applications.

As a side remark to the proposed scenario, an interesting parallel can be made
with the music domain. Similarly to publications, music items (e.g. music files,
tracks, etc) are also acquired and stored by people on their personal desktops, in
numbers usually increasing with time. And as well as publications, these can embed (depending on the format) shallow metadata describing them, such as, band,
song title, album or genre. Thus, conceptually, the extraction  expansion  integration process we propose can be applied also in this domain. In practice, there
already exist tools that deal with parts of this process. For example, on the extraction side, there are tools that help users to create or extract ID3 tags embedded into MP3 files or on the expansion side, there exist tools, such as Picard,5
that clean the metadata based on specialized music metadata repositories (e.g.
MusicBrainz). As we shall see in the next section, the result of our work is quite
similar to these, but applied on scientific publications.

3 Implementation

One of our main goals was reducing as much as possible the overhead imposed
by collateral activities that need to be performed while researching a new field,
in parallel with the actual reading of publications. And at the same time, we targeted an increase of the users reward, by ensuring a long-term effect of some of the
achieved results. An overall figure of the three step process we propose, is depicted
in Fig. 1. The first step, extraction, has as input a publication with no metadata
and it outputs two types of metadata:

(i) shallow metadata, i.e. title, authors, abstract, and (ii) deep metadata, i.e.
discourse knowledge items like claims, positions or arguments. It represents the
only step that appears to have no direct reward (or value) for the user (except for

Fig. 1. Incremental metadata enrichment process

5 http://musicbrainz.org/doc/PicardTagger
?

?

?
the discourse knowledge items). Nevertheless, it is compulsory in order to start
the process, each subsequent step building on its results, and thus enabling an
incremental approach to the enrichment of the semantic metadata describing the
publication. Since the extraction process is based on a hybrid document engineering  computational linguistic approach, the resulting metadata may contain er-
rors. These errors can be corrected in the expansion step, in addition to enriching
the basic set of metadata with linked data, coming from different sources. As we
shall see, we opted for a clear distinction of the semantics of the owl:sameAs and
rdfs:seeAlso relations. Finally, the integration step embeds the linked metadata
into the semantic desktop environment, thus connecting it deeper within the personal information space, and fostering long-term effects of the overall process.

In terms of implementation, the first two steps are developed as part of a standalone application6. The extraction currently targets publications encoded as PDF
documents and preferably using the ACM and LNCS styles, while the expansion
is achieved via the Semantic Web Dog Food Server and the Faceted DBLP7 linked
data repositories. The integration of the metadata is done using the services provided by the KDE NEPOMUK Server,8 while the searching and browsing experience is enabled via the usual KDE Desktop applications, such as Dolphin (the
equivalent of Windows Explorer) and Konqueror (a KDE Web browser). The application we have developed is highly customizable, each step being represented
by a module. Therefore, adding more functionality is equivalent to implementing
additional modules, for example, an extraction module for MS Word documents,
or an expansion module for DBpedia.

To have a better understanding of the result of each step, we will use as a running example throughout the following sections, the metadata extracted from a
publication entitled Recipes for Semantic Web Dog Food  The ESWC and ISWC
Metadata Projects.9 More precisely, we will assume that a user would start her
quest from this publication, and show the incremental effect of using our application on the created metadata.

3.1 Extraction

The extraction of shallow metadata was developed as a set of algorithms that follow a low-level document engineering approach, by combining mining and analysis
of the publications text based on its formatting style and font information. The
algorithms currently work only on PDF documents, with a preference for the ones
formatted with the LNCS and ACM styles. Each algorithm in the set deals with
one aspect of the shallow metadata. Thus, there are individual algorithms for extracting the title, authors, references and the linear structure.

A complete description of the algorithms can be found in [3]. Nevertheless, to
provide the basic idea of how they work, we will describe shortly the authors extraction algorithm. There are four main processing steps: (i) We first merge the

6 Demo at http://sclippy.semanticauthoring.org/movie/sclippy.htm
7 http://dblp.l3s.de/
8 http://nepomuk.kde.org/
9 http://iswc2007.semanticweb.org/papers/795.eps

T. Groza et al.

Fig. 2. Authors extraction algorithm example

consecutive text chunks on the first page that have the same font information and
are on the same line (i.e. the Y coordinate is the same); (ii) then, we select the
text chunks between the title and the abstract and consider them author candi-
dates; (iii) the next step is the linearization of the author candidates based on the
variations of the Y axis; (iv) finally, we split the author candidates based on the
variations of the X axis.

Fig. 2 depicts an example of a publication that has the authors structured on
several columns. The figure shows the way in which the authors columns containing the names and affiliations are linearized, based on the variation of the Y
coordinate. The arrows in the figure show the exact linearization order. The variations on the X axis can be represented in a similar manner.

The extraction of deep metadata, i.e. discourse knowledge items (claims,
positions, arguments), was performed based on a completely different approach.
Having as foundational background the Rhetorical Structure of Text Theory
(RST) [4], we have developed a linguistic parser that mines the presence of rhetorical relations within the publications content. In order to automatically identify
text spans and the rhetorical relations that hold among them, we relied on the
discourse function of cue phrases, i.e. words such as however, although and but.
An exploratory study of such cue phrases provided us with an empirical grounding for the development of an extraction algorithm. The next phase consisted of
an experiment for determining the initial probabilities for text spans to represent
knowledge items, based on the participation in a rhetorical relation of a certain
type and its block placement in the publication (i.e. abstract, introduction, conclusion or related work). The parser was implemented as a GATE10 plugin. Detailing the actual extraction mechanism is out of the scope of this paper, as our
focus is on the incremental process that realizes the bridging the Linked Web of
Data and the Semantic Desktop. Nevertheless, it is worth mentioning that we do
extract also deep metadata, as it brings added value to the user, and as we shall
see later in this section, enables meaningful queries in the bigger context of the
full set of extracted metadata.

As mentioned, the first two steps of our process are implemented as a standalone application. The left side of Fig. 3 depicts the main interface of this

10 http://gate.ac.uk/
