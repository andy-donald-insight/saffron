Bridging the Gap between Linked Data and the Semantic Desktop

Fig. 3. Screenshot of the applications interface: [A]  The main window; [B]  Coauthors graph visualization

application, while with <1> we indicated the place where the result of the extraction is displayed. At the same time, the listing below summarizes elements
of the metadata extracted after this first step, i.e. title, authors, the text of the
abstract, and a list of claims (i.e. the most important contributions statements
of the publication). For shaping the metadata, we used a mixture of ontologies
and vocabularies, such as SALT (Semantically Annotated LATEX) framework [5],
DublinCore and FOAF. One particular element that should be noted here, is that
this metadata may contain errors. As it can be seen in the listing below the name
of the first author is incorrect: Moller instead of M oller. The user has the chance
to correct such mistakes manually, or advance to the next step, where the correction can be done automatically  if the publication under scrutiny is found in the
linked data repository. In any case, already at this point, the user can decide to
jump to the integration step, simply just export this metadata as an individual
file, or embed it directly into the originating PDF. From the scenarios point-of-
view, the researcher already gains value, as she can quickly grasp the main claims
of the publication, by inspecting the extracted deep metadata.

<pub> a sdo:Publication .

<knud> foaf:name Knud Mo"ller .

<pub> dc:title Recipes for Semantic Web ... . <abs> a sro:Abstract .

<pub> dc:creator knud .

<pub> dc:creator tom .

<abs> konnex:hasText Semantic Web ... .

<pub> dcterms:abstract abs .

<pub> dc:creator siegfried .

<claim> a sro:Claim .

...

<claim> konnex:hasText This paper ... .

<knud> a foaf:Person .

<pub> konnex:hasClaim claim .

3.2 Expansion

The expansion step takes the metadata extracted previously and, under the users
guidance, corrects existing errors and enriches it, by using Linked Data reposito-
ries. We have currently implemented expansion modules for the Semantic Web

T. Groza et al.

Dog Food Sever and Faceted DBLP. The actual expansion is done based on the
extracted title and authors. On demand, these are individually used for querying
the SPARQL endpoints of the Linked Data repositories. As a prerequisite step,
both the title and the authors (one-by-one) are cleaned of any non-letter char-
acters, and transformed into regular expressions. The title is also chunked into
multiple variations based on the detected nouns, while each author name is chunked based on the individual parts of the full name, discarding the parts that are
just one letter long. Consequently, each element will have an associated array of
sub-strings used for querying.

In the case of the title, the query result will be a list of resources that may contain duplicates, and among which there might also be the publication given as
input. In order to detect this particular publication, we perform a shallow entity
identification. First, to mask possibly existing discrepancies in the title, we use
string similarity measures. An empirical analysis led us to using a combination
of the Monge-Elkan and Soundex algorithms, with fixed thresholds. The first one
analyzes fine-grained sub-string details, while the second looks at coarse-grained
phonetic aspects. The titles that pass the imposed thresholds (0.75 and 0.9) advance to the next step. Secondly, we consider the initially extracted authors and
compare them with the ones associated with the publications that pass over the
above mentioned thresholds. The comparison is done using the same similarity
measures, but with different thresholds (0.85 and 0.95). The publications satisfying both conditions have their models retrieved and presented to the user as
candidates. A similar approach is also followed on the authors side.

The outcome of the expansion features three elements: (i) a list of candidates,
to be used for cleaning and linking the initially extracted metadata (with their
linked model and authors models), (ii) a list of similar publications, mainly the
ones that did not satisfy the two conditions of the shallow entity resolution (again
with their linked model and authors models), and (iii) for each author of the given
publication found, the full linked model and the complete list of publications existing in the respective repository. From the scenario perspective, this outcome
provides the researcher with the chance of analyzing both publications that might
have similar approaches and inspect all the publications of a particular author.

At this stage, there are three options that can be followed. The first option is
to use the best retrieved candidate to correct and link the initial metadata. Both
the publication and the authors will inherit the discovered owl:sameAs links, that
will later provide the opportunity to browse different instances of the same entity
in different environments. The second option is to link other publications that she
considers relevant to the one under scrutiny. While at the interface level this is
done based on the users selection (see pointer 2 in Fig. 3), at the model level
we use the rdfs:seeAlso relation. We thus make a clear distinction in semantics
between owl:sameAs and rdfs:seeAlso. The former represents a strong link between different representations of the same entity, while the latter acts as a weak
informative link, that will later help the user in re-discovering similarities between
several publications. The third and last option is specific for authors, and allows
the user to navigate through the co-authors networks of a particular author (part
?

?

?
B of Fig. 3). An interesting remark here, is that the visualization we have developed can act as a uniform graph visualization tool for any co-author networks
emerging from a linked dataset.

Returning to our running example, the output of this step is an added set of
metadata, presented briefly in the listing below. Thus, in addition to the already
existing metadata, we can now find the above mentioned owl:sameAs and
rdfs:seeAlso relations, and the incorrectly extracted name Moller, now corrected to Moeller, based on the foaf:name found in the linked data.

<knud> foaf:name Knud Moeller .

<knud> owl:sameAs http://data.semanticweb.org/person/knud-moeller .

<knud> owl:sameAs http://dblp.l3s.de/d2r/resource/authors/Knud_M\%C3\%B6ller .

...

<pub> owl:sameAs http://data.semanticweb.org/conference/iswc-aswc/2007/.../papers/795 .

<pub> owl:sameAs http://dblp.l3s.de/d2r/resource/publications/conf/semweb/MollerHHD07 .

<pub> rdfs:seeAlso <pub2> .

...

<pub2> a sdo:Publication .

<pub2> dc:title DBPedia: A Nucleus for a Web of Open Data ...  .

<pub2> dc:creator <richard> .

<pub2> dc:creator <georgi> .

<pub2> owl:sameAs http://dblp.l3s.de/d2r/resource/publications/conf/semweb/AuerBKLCI07 .

<pub2> owl:sameAs http://data.semanticweb.org/conference/iswc-aswc/2007/.../papers/715 .

3.3 Integration

The last step of our process is the integration, which embeds the extracted and
linked metadata into the personal information space, managed by the Semantic
Desktop, and thus realizing the actual bridge between the Linked Data and the
Semantic Desktop. To achieve this, we have used the NEPOMUKKDE implementation of the Semantic Desktop. This provides a central local repository for
storing structured data and it is well integrated with the common desktop appli-
cations, such as the file and Web browsers. In terms of foundational models, it uses
the NEPOMUK Ontologies11 suite. In our case, the actual integration was done
at the metadata level, where we had to align the instances previously extracted
with the ones already existing in the local repository.

Currently, we deal with two types of alignments: person alignment and publication alignment, that are described within the Semantic Desktop context by means
of the NCO (NEPOMUK Contact Ontology), NFO (NEPOMUK File Ontology)
and PIMO (Personal Information Model Ontology) ontologies.

The person alignment resumes to checking whether an extracted author is already present in the personal information model, and in a positive case, merging
the two models in an appropriate manner. This is done based on a two step mech-
anism, similar to finding authors in a Linked Data repository. We first query the
local repository for the name of the author and the associated substrings resulted

11 http://www.semanticdesktop.org/ontologies/

T. Groza et al.

from chunking the name into several parts. Using the same similarity measures, we
filter out only the realistic candidates. These candidates are then analyzed based
on the existing owl:sameAs links and their linked publications. If a candidate is
found to have one identical owl:sameAs link and one identical publication with the
initial author, we consider it a match and perform the merging of the two mod-
els. In a negative case, the authors model is stored as it is and we advance to the
publication alignment. The result of this alignment is exemplified in the listing
below. In addition to the already existing metadata, the author now has attached
an email address and the birth date, both found within the users personal information space.

The publication alignment is straightforward, as from a local and physical per-
spective, the publication is represented by a file. Thus, considering that the user
started the process from such a file (which is always the case), we query the repository for the information element corresponding to that file, having the fileUrl (or
path) as the main indicator. The conceptual model found to be associated with
the file is then merged with the extracted publication model. The listing below
shows this alignment as part of our running example, the last statement creating
the actual grounding of the publication onto a physical file.

The integration enables, in particular, two important elements: (i) firstly, more
meaningful ways of finding and linking publications on the desktop, and (ii) sec-
ondly, an opportunity of weaving the linked data present on the desktop, using
ordinary desktop applications. Fig. 4 depicts the first aspect, using Dolphin, the

<knud> nco:birthDate 1980-11-01 .

<knud> nco:emailAddress knud.moeller@deri.org .

...

<pubFile> a nfo:FileDataObject .

<pubFile> nfo:fileSize 1353543 .

<pubFile> nfo:fileUrl file:///home/user/research/papers/p215.eps .

<pub> pimo:groundingOccurence <pubFile> .

Fig. 4. Deep metadata integration in KDE applications: [A] SemNotes; [B] Dolphin
?

?

?
KDE file browser (part B), and SemNotes,12 a KDE semantic note-taking application (part A). As shown in the figure, to retrieve all publications having knud
among the authors, claim-ing that they deal with real deployments of Semantic Web... and being related to (seeAlso) publications that speak about open
data, resolves to using the following query in Dolphin:

nepomuksearch:/ type:publication creator:knud hasClaim:real

deployments of Semantic Web* seeAlso:open data*

The result of the query will be a virtual folder that is automatically updated in
time (enabling the long-term effect), thus showing also the publications that are
added at a later stage and that satisfy the query, independently of the name of the
physical file or its location. Similarly, while taking notes during the presentation of
this particular paper, SemNotes will automatically link both the publication and
the author mentioned in the note, therefore providing added information about
the publication and its authors.

Fig. 5. Browsing deep integrated resources with Konqueror

The second aspect is presented in Fig. 5, which shows how resources such as
publications or authors can be visualized by means of an ordinary Web browser
(here, Konqueror). More important, this enables the visualization of the rich information space surrounding a publication, both from a local and linked data per-
spective. Without even opening the actual publications, the user can: (i) quickly
grasp the main ideas of the publications, via the presented claims, (ii) see related
publications, via the rdfs:seeAlso links, or (iii) inspect the publications au-
thors, either via their personal contact information, or via their different instances
on the Web (owl:sameAs links). We believe that this approach combines harmoniously the Linked Data perspective with the Semantic Desktop perspective, thus
enabling the weaving of Linked Data on the desktop.

12 http://smile.deri.ie/projects/semn

T. Groza et al.

4 Preliminary Evaluation

We evaluated the extraction of semantic metadata and performed a short-term usability study of the overall approach. The shallow metadata extraction achieved
high accuracies for the title (95%) and abstract (96%) extraction, and a lower accuracy for authors extraction (90%). The evaluation method and complete results
can be found in [3]. In this section, we focus on the usability study, as we believe
that the developed application has to be easy to learn and use, and to provide the
most appropriate information.

The study was conducted together with 16 evaluators, a mixture of PhD students and Post Doctorands from our institute, that were asked to perform a series of tasks covering all the applications functionalities. Example of such tasks
included: extraction and manual correction of metadata from publications, expansion of information based on the same publications or exploration of the coauthors graph. At the end, the evaluators filled in a questionnaire, comprising of
18 questions, with Likert scale-based or free form answers, concentrating on two
main aspects: (i) suitability and ease of use, and (ii) design, layout and conformity to expectancies. The complete results of the questionnaire can be found at
http://smile.deri.ie/sclippy-usabilitystudy

Overall, the application scored very well in both categories we have targeted.
The vast majority of the evaluators (on average more than 90%) found the tool well
suited for the extraction and exploration of shallow and deep metadata. The same
result was achieved also for the exploration of the information space surrounding
the chosen publication, based on the extracted and linked metadata. In addition,
the information presented by the application, both for publications and authors,
was found helpful (100% for publications and 72.8% for authors), while 93.8% of
the evaluators found an added value in our tool when compared to the original
expansion environment.

In the other category, all evaluators considered the application easy to learn and
use (100%) while having the design and layout both appealing (87.5%) and suited
for the task (93.6%). Issues were discovered in two cases: (i) the self-descriptiveness
of the applications interface (only 68.8% found it self-descriptive), mainly due to
the lack of visual indicators and tooltips, and (ii) the suggested list of similar publications (again only 68.8% found it relevant). Although the application always
found the exact publication selected for expansion in the repository, the proposed
list of similar publications created some confusion.

Apart from these findings, directly taken from the questionnaires, we observed
that even without any training and documentation, the evaluators experienced
a very smooth learning curve. Additionally, most of them enjoyed our exercise,
while some were interested in using the application on a daily basis. On the other
hand, the study pointed out a number of issues and led us to a series of directions
for improvement. First of all, the need to make use of a more complex mechanism for suggesting similar publications. As we expected, the shallow similaritybased heuristics we used for building the list of suggested publications left plenty of
space for improvement. Unfortunately, its improvement is directly dependent on
the quantity and quality of information provided by the linked data repository. As
?

?

?
an example, while we could use the abstract provided by the Semantic Web Dog
Food Server to extract discourse knowledge items, and then perform similarity
measures at this level, this would not be possible when using the Faceted DBLP,
where such information does not exist. For this case, a possible solution, would be
to drill deeper into the linked web of data. Secondly, augmenting the expanded information with additional elements (e.g. abstract, references, citation contexts),
thus providing a deeper insight into the publications and a richer experience for
the users.

5 Related Work

To our knowledge, until now, there was no attempt to combine in such a direct
manner automatic metadata extraction from scientific publications, linked open
data and the semantic desktop. Nevertheless, there are efforts that deal with parts
of our overall approach, and, in this section, we will focus on them. Hence, we will
cover: (i) automatic extraction of shallow metadata, including the context of the
semantic desktop, and (ii) information visualization for scientific publications.

Before detailing the two above-mentioned directions, we would like to discuss
the position of the alignments described in the expansion and integration steps
to the current state of the art. To a certain extent, these person and publication
alignments are similar to performing coreference resolution. While in the person
case the resolution is solved directly via string similarity measures, in the publication case we add the authors list as an extra condition. This makes our approach
more simple and straightforward than the more accurate algorithms existing in the
literature. Examples of such techniques include: naive Bayes probability models
and Support Vector Machines [6], K-means clustering [7] or complex coreference
based on conditionally trained uni-directed graph models using attributes [8].

Extensive research has been performed in the area of the Semantic Desktop,
with a high emphasis on integration aspects within personal information man-
agement. Systems like IRIS [9] or Haystack [10] deal with bridging the different
isolated data silos existing on the desktop, by means of semantic metadata. They
extract shallow metadata from the desktop files and integrate it into a central
desktop repository. Compared to our approach, the metadata extraction is fileoriented and shallow, whereas we extract specific publication metadata and integrate it within the already existing semantic desktop data. The closest effort to
ours was the one of Brunkhorst et al. [11]. In their Beagle++ search engine, developed in the larger context of the NEPOMUK Semantic Desktop [12], the authors
also perform metadata extraction from scientific publications, but limited to title
and authors.

Regarding the general context of automatic extraction of metadata from publi-
cations, there have been several methods used, like regular expressions, rule-based
parsers or machine learning. Regular expressions and rule-based systems have the
advantage that they do not require any training and are straightforward to im-
plement. Successful work has been reported in this direction, with emphasis on
PostScript documents in [13], or considering HTML documents and use of natural language processing methods in [14]. Our approach is directly comparable

T. Groza et al.

with these, even though the target document format is different. In terms of accu-
racy, we surpass them with around 5% on title and authors extraction, and with
around 15% on linear structure extraction, while providing additional metadata
(i.e. abstract or references).

Although more expensive, due to the need of training data, machine learning
methods are more efficient. Hidden Markov models (HMMs) are the most widely
used among these techniques. However, HMMs are based on the assumption that
features of the model they represent are not independent from each other. Thus,
HMMs have difficulty exploiting regularities of a semi-structured real system.
Maximum entropy based Markov models [15] and conditional random fields [16]
have been introduced to deal with the problem of independent features. In the
same category, but following a different approach, is the work performed by Han
et al. [17], who uses Support Vector Machines (SVMs) for metadata extraction.
With respect to information visualization of scientific publications, a number of
methods and tools have been reported in the literature. The 2004 InfoVis
challenge had motivated the introduction of a number of visualization tools highlighting different aspects of a selected set of publications in the Information Visualization domain. Faisal et. al. [18] reported on using the InfoVis 2004 contest
dataset to visualize citation networks via multiple coordinated views. Unlike our
work, these tools were based on the contents of a single file, which contained manually extracted and cleaned metadata. As noted by the challenge chairs, it was a
difficult task to produce the metadata file [19] and hence the considerable efforts
required made it challenging for wide-spread use. In [20], a small scale research
management tool was built to help visualizing various relationships between lab
members and their respective publications. A co-authorship network visualization
was built from data entered by users in which nodes represented researchers together with their publications, and links showed their collaborations. A similar
effort to visual domain knowledge was reported by [21], with their data source
being bibliographic files obtained from distinguished researchers in the network
science area. While this work was also concerned with cleansing data from noisy
sources, the metadata in use was not extracted from publications themselves and
no further information available from external sources such as Faceted DBLP was
utilized. Another tool targeting the exploration of the co-authorship network is
CiteSpace [22]. CiteSpace tries to identify trends or salient patterns in scientific
publications. The source of information for CiteSpace is also from bibliographic
records crawled from different publishers on the web, rather than extracted meta-
data.

6 Conclusion and Future Developments

In this paper we presented an approach for dealing, at least to some extent, with
the information overload issue both on the Web and on the Desktop, and having
as target early stage researchers. Our solution, inspired from the typical process of
getting familiarized with a particular domain, combines elements from the Linked
Web of Data and the Semantic Desktop, using semantic metadata as a common
?

?

?
denominator. The result consists of three steps (extraction  expansion  integra-
tion) that incrementally enrich the semantic metadata describing a publication,
from no metadata to a comprehensive model, linked and embedded within the
personal information space.

Each step has associated a series of open challenges that we intend to address
as part of our future work. As currently the extraction works only on publications
published as PDF documents, and formatted preferably with the LNCS and ACM
styles, we plan to improve extraction algorithms to accommodate any formatting
style, as well as develop new extraction modules for other document formats, such
as Open Document formats. At the moment, the expansion uses only two Linked
Data repositories, i.e. the Semantic Web Dog Food Server and the Faceted DBLP.
Future developments will include also other repositories, in addition to means for
creating ad-hoc mash-ups between them, thus allowing the user to see data coming
from different sources in an integrated and uniform view. Last, but not least, we
plan an even tighter integration within the Semantic Desktop, therefore enabling
more meaningful queries and a richer browsing experience, and ultimately a complete automatization of the process, thus reducing the overhead to the minimum
possible.

Acknowledgments

The work presented in this paper has been funded by Science Foundation Ireland
under Grant No. SFI/08/CE/I1380 (Lion-2).
