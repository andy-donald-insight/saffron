Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 2951

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

j o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / w e b s e m

LexInfo: A declarative model for the lexicon-ontology interface
P. Cimiano a,, P. Buitelaar b, J. McCrae a, M. Sintek c

a Semantic Computing Group, Cognitive Interaction Technology Excellence Center (CITEC), Bielefeld University, Universitatsstrasse 21-23, 33615 Bielefeld, NRW, Germany
b Unit for Natural Language Processing, Digital Enterprise Research Institute, National University of Ireland, Galway, Ireland
c Knowledge Management Dept. & Competence Center Semantic Web, DFKI, Germany

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 17 March 2010
Received in revised form
14 November 2010
Accepted 18 November 2010
Available online 26 November 2010

Keywords:
Lexicon-ontology interface
Lexicon ontologies
Ontologies
Natural language processing

In this paper we motivate why it is crucial to associate linguistic information with ontologies and why
more expressive models, beyond the label systems implemented in RDF, OWL and SKOS, are needed to
capture the relation between natural language constructs and ontological structures. We argue that in
the light of tasks such as ontology-based information extraction (i.e., ontology population) from text,
ontology learning from text, knowledge-based question answering and ontology verbalization, currently
available models do not suffice as they only allow us to associate literals as labels to ontology elements.
Using literals as labels, however, does not allow us to capture additional linguistic structure or information
which is definitely needed as we argue. In this paper we thus present a model for linguistic grounding of
ontologies called LexInfo. LexInfo allows us to associate linguistic information to elements in an ontology
with respect to any level of linguistic description and expressivity. LexInfo has been implemented as an
OWL ontology and is freely available together with an API. Our main contribution is the model itself,
but even more importantly a clear motivation why more elaborate models for associating linguistic
information with ontologies are needed. We also further discuss the implementation of the LexInfo API,
different tools that support the creation of LexInfo lexicons as well as some preliminary applications.

 2010 Elsevier B.V. All rights reserved.

1. Introduction

Several standards for representing ontologies have been developed in the last decade, in particular RDF Schema [29,10] and OWL
[5,33]. While ontologies are logical theories and independent of any
natural language,1 a grounding in natural language is nevertheless
needed for several reasons:

 When engineering an ontology, human developers will be able
to better understand and manipulate ontologies. Associating linguistic information to ontologies (in the simplest form by labels)
allows people to ground concepts and relations defined in the
ontology with respect to their own linguistic and cognitive sys-
tem.

 Corresponding author. Tel.: +49 0 521 106 12249.
E-mail addresses: cimiano@cit-ec.uni-bielefeld.de (P. Cimiano),

paul.buitelaar@deri.org (P. Buitelaar), jmccrae@cit-ec.uni-bielefeld.de (J. McCrae),
sintek@dfki.de (M. Sintek).

1 Some authors have argued that ontologies should be constructed following our
understanding of language [50,3]. Irrespective of the way in which ontologies are
constructed, they are certainly artifacts engineered for a computer in the first place
as outlined also in [38]. Therefore, a grounding in language is not needed by the
computer itselfit can not make reasonable use of such a grounding anyway!but
by humans interacting with the ontological structures, the results of a query, etc.

1570-8268/$  see front matter  2010 Elsevier B.V. All rights reserved.
doi:10.1016/j.websem.2010.11.001

 In ontology population, automatic procedures for ontology-based
information extraction from text will be able to better link textual data to ontology elements if the latter are associated with
information about how they are typically realized linguistically.
 When querying an ontology in natural language (see [31,7]), we
need information about how the words/constructs used in the
query map to classes, instances and properties modeled in the
ontology.
 When verbalizing an ontology, i.e., generating natural language
text for easier human consumption (as in [16,9]), richer models
capturing how concepts and relations can be realized linguistically will be needed.

All the above mentioned applications would benefit from a
principled approach supporting the enrichment of ontology elements with information about how they are realized linguistically.
However, the development of models that allow us to associate
linguistic information (part-of-speech, inflection, decomposition,
subcategorization frames, etc.) to ontology elements (concepts,
relations, individuals, etc.) is not as advanced as the corresponding
ontology representation languages. While RDF(S) and OWL allow
us to associate labels to ontology elements, we argue that this is not
enough for actual use of ontologies in connection with human users
and textual data as described above. SKOS [36] merely introduces
further typology of labels (preferred, alternative, hidden, etc.) and

P. Cimiano et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 2951

does not go beyond RDF(S) as it only supports the representation of
atomic terms without a possibility for representing their linguistic
(sub-)structure. However, SKOS was not developed with the aim of
associating lexical and linguistic information to arbitrary (domain)
ontologies, but with the goal of producing a data-model (building
on RDF(S) and OWL) to represent classification schemas, such as
thesauri. Thus, by definition SKOS does not fulfill the need for a
richer model allowing us to associate linguistic structure to arbitrary domain ontologies (and it was not designed for this purpose).
In this paper we introduce a principled model allowing to associate linguistic information to ontologies with respect to any level
of linguistic description and expressivity, i.e., the LexInfo model.
The main characteristic of LexInfo is that it allows us to represent
the connection between an ontology and the way we speak about
the different ontology elements in a declarative way,2 such that the
information is reusable across systems. The current situation in this
respect is that each system supporting one of the above mentioned
applications, be it an ontology population, ontology-based question
answering or ontology verbalization system, needs to establish a
connection between language and the ontology in an ad hoc fashion for every ontology the system supports. This situation is clearly
undesirable as it does not allow a distribution of efforts. A clear
modularization of tasks (separating the creation of lexicons from
their usage in a particular system) as we envision here would allow
to distribute efforts in the sense that some parties could create a
lexicon for a specific ontology and make the lexicon publicly available in a machine readable format (e.g. the LexInfo model proposed
here). Other parties could customize their information extraction
system to the ontology in question by searching and downloading
an appropriate lexicon from the Web. To realize this vision, we need
a sound, principled and declarative model by which we can represent and share ontology lexica. The development of such a model
was the goal of the work described in this paper and LexInfo is our
answer.

LexInfo conceptually builds on three main components: the
LingInfo [13,14] and LexOnto [15] models as well as the Lexical Markup Framework (LMF) [20]. LingInfo and LexOnto were
developed independently from each other in previous work, but
with similar goals and motivations. The LingInfo model provides
a mechanism for modeling label-internal linguistic structure, i.e.,
lexical inflection and morphological or syntactic decomposition of
ontology labels, interpreted as terms. The LexOnto model on the
other hand enables the representation of label-external linguistic
structure, i.e., predicate-argument structures that are projected by
lexical heads of ontology labels and their mapping to corresponding ontological structures. While the two models have the same
aim of providing more expressive lexicon models for ontologies,
they have focused on rather complementary aspects. The LexInfo
model presented in this article combines aspects of both (LingInfo
and LexOnto) to yield a rich lexicon model for associating linguistic
information with the elements defined in an ontology. The LexInfo
model builds on the Lexical Markup Framework (LMF)3 (see [20,23])
as a core and extends it appropriately to accommodate the essential
aspects of the LingInfo and LexOnto models. From a more general
perspective, we hope that the LexInfo model can provide a basis
for future discussion on standardization of lexicon models for OWL
ontologies.

The paper is structured as follows. In Section 2 we provide an
extensive motivation for the work discussed here as well as a com-

2 What we mean by declarative in this context is that the specification of linguistic descriptions associated to ontology elements is independent of the way a
concrete application exploits this linguistic knowledge.

3 The Lexical Markup Framework has been recently accepted as an ISO standard

under ISO-24613:2008.

parison with related work. In Section 3, we discuss in detail the
three building blocks of LexInfo: the LingInfo and LexOnto models
(LexOnto and LingInfo) as well as the Lexical Markup Framework
(LMF). In Section 4 we present the LexInfo model, which combines
aspects of both models, using LMF as a glue to bring these models
together. Furthermore, in Section 5 we present the first implementation of an API for the LexInfo model and discuss both the
tool support that is currently available to create ontology lexicons
according to the LexInfo model as well as preliminary results from
applying the LexInfo model. Finally, in Section 6 we draw some conclusions of the work presented and discuss ideas for future work.

2. Motivations and related work

In the following, we argue firstly that labels such as specified by
the RDF(S) and OWL standards are not sufficient for the purpose
of associating linguistic information with ontologies. Secondly, we
also argue that existing related work on the association of linguistic
information with ontologies still fails to address the need for richer
models to capture the lexicon-ontology interface. In addition, we
also formulate a number of requirements which should be fulfilled
by models for linguistic grounding of ontologies and discuss in how
far the different proposals fulfill these.

2.1. Separation between lexical and ontological layer

RDF(S) and OWL allow us to represent what could be termed a
verbal anchor for concepts, properties, individuals, etc. by way of
the rdfs:label property, which is defined for Resource as domain
and Literal as range. We could use this to specify that the class
River is typically expressed in natural language with the word
river:
<rdfs:Class about=#River>

<rdfs:label>river</rdfs:label>

</rdfs:Class>

To allow for multilingual representation, the rdfs:label property enables the representation of labels with an indication of
language, e.g., for English (as above) and for German by way of
language tags:
<rdfs:Class about=#River>

<rdfs:label xml:lang=en>river</rdfs:label>
<rdfs:label xml:lang=de>Fluss</rdfs:label>

</rdfs:Class>

If we additionally want to represent linguistic variants of river,
e.g., the plural rivers, the RDF data model gives us only one choice,
i.e., adding a further independent label, i.e.
<rdfs:Class about=#River>

<rdfs:label xml:lang=en>river</rdfs:label>
<rdfs:label xml:lang=en>rivers</rdfs:label>
<rdfs:label xml:lang=de>Fluss</rdfs:label>
<rdfs:label xml:lang=de>Flusse</rdfs:label>

</rdfs:Class>

Although RDF(S) thus supports the representation of linguistic
variants (across languages), the way this is done is very unsatisfactory for the following reasons:
 RDF(S) does not allow us to capture relations between different
labels, e.g., morphological relations such as the fact that rivers is
the plural of river. Certainly, we could ask ourselves if we want to
represent such morphological relations explicitly in an ontology,
but this raises the question why we should represent linguistic
and morphological variants at all, as these are not ontologically
relevant but rather should be attached to a base entry (e.g., the
lemma) in a lexicon rather than included in the ontology itself.
This is exactly the approach we have pursued in LexInfo, where
a separate ontology structured according to the LexInfo model is
used to model the linguistic information.

 When attempting to extend the ontology with such simple information as to which syntactic category (part of speech) a label
belongs to, we will fail as the labeling system only allows literals
to be attached without further information. This is a serious limitation and could also be overcome by separating the lexical and
ontological levels, having lexical entries in a lexicon pointing to
the ontology.
 The RDF label system rules out the possibility of having different
lexica for a given ontology. The RDF label system in particular
ties the labels very closely to the ontology elements. A modular
approach in which the ontology and the lexicon are separated
would clearly allow us to have different lexica for a given ontol-
ogy.

Models such as SKOS that tie the conceptual and the lexical layer
together more tightly do not solve the above issues in any way. In
fact, SKOS only introduces additional typology for the labels, distinguishing between preferred, alternative and hidden labels. Certainly,
we could specify that rivers is a hidden label for the class River,
but this does not solve any of the above mentioned problems.

The principled solution in our view is to separate the ontological
and lexical information into two different domains of discourse. On
the one hand, we would have the ontology domain of discourse, talking about classes, properties, individuals, etc. On the other hand, we
would have the lexical domain of discourse talking about lexical elements as first-class citizens. This would allow us to add linguistic
information to the lexical elements in the lexical layer with respect
to any level of linguistic description and expressivity required by
applications. Hereby, the lexical layer is clearly separated from the
ontology domain of discourse except for referencing its elements.
A crucial question we address here is how the lexicon layer can
be structured as a principled model. LexInfo is our answer to this
question.

2.2. More flexible coupling between ontological and the linguistic
system

The label property for RDF(S) and OWL in essence specifies an
n : m relation in the sense that one class, property or individual, etc.
can be associated to many labels and on the other hand one label can
be ambiguous and refer to many ontology elements (class, property
or individual). In essence, the relation between ontology elements
OE of an Ontology O and a set of labels L is specified by the following
two functions (specific to a given language S):
 fS : L 2OE (i.e., a label can denote different ontological entities).

: OE  2L (i.e., a given ontological entity can be represented

by various labels).

The sets in the ranges of the above described functions, i.e., fs

, have what could be termed a disjunctive interand its inverse f

pretation in the sense that a label l can denote some member of

fs(l) (but not a composition of these), while the same holds for f

i.e., an ontological element e can be realized lexically as any mem-

(e). Thus, the labeling system in RDFS relies on the fact
ber of f

that there is always a counterpart at the ontological level for each
label specified in the lexicon, without allowing for a more complex correspondence between a class or property on one side and
a syntagmatic4 composition of several labels on the other.

The reason why a more complex correspondence is needed may
be explained with the following example. Let us consider a compos-

ite term like the German Autobahnkreuz (highway interchange). We
have the following possibilities to associate this term with ontological elements:

be

class

composite

 There might be a class HighwayInterchange to which Autobahnkreuz can directly refer to.
 There might
Interchange 
locatedAt.Highway to which Autobahnkreuz can point.
 There might be simply the general class Interchange, in which
case we would like to specify that only the second part of the
composite term highway interchange, i.e., interchange refers to
the class Interchange.
 There might be both classes Highway and Interchange contained in the ontology, in which case we would like to specify
that the second part of the composite term (kreuz) refers to the
class Interchange, while the first part (Autobahn) refers to the
class Highway.

Thus, it seems that we require a flexible system allowing us to
associate terms to concepts in a way which is sensitive to the way
concepts or properties have been modeled and allowing us to assign
them to the whole term or to individual parts of a term. Further, we
see it as a requirement that this model does not assume that the
linguistic and ontological levels are fully synchronized.5 In this
way, we do not need to add a class just because we want to include
the term in the lexicon nor the other way round as in RDFS. It is
in this sense that synchronization is not required. For this we need
appropriate means to represent the decomposition of terms and
to associate ontological entities to terms and their sub-structure.
Obviously, this is out of the scope of the RDFS label system, as it
does not support the modeling of any of the semantic implications
of the morphosyntactic structure of complex labels (i.e., composite
terms). Clearly, an approach based only on the use of rdfs:label
does not allow us to model the semantic implications of such labelinternal morphological (in the case of German) or syntactic (in the
case of English) structure.

2.3. Capturing syntactic behaviour

When we speak, we certainly do not do so in telegraphic style
using only content words (or labels). Words have a clear syntactic
behaviour which to a great extent is determined by their syntactic
category (verb, noun, adjective, etc.) The structure of a sentence
crucially depends on the syntactic behaviour of the different words
that the sentence is composed of.

When analyzing language, interpreting it with respect to an
ontology (e.g., in information extraction or question answering) or
generating language output on the basis of the ontology, it is crucial to have access to information about the linguistic behaviour of
words. To illustrate this, let us look at the following properties:
<rdf:Property about=#capital>

<rdfs:domain rdf:resource=#Country/>
<rdfs:range rdf:resource=#City/>
<rdfs:label xml:lang=en>capital</rdfs:label>

</rdf:Property>
<rdf:Property about=#flowThrough>

<rdfs:domain rdf:resource=#River/>
<rdfs:range rdf:resource=#City/>
<rdfs:label xml:lang=en>flow through</rdfs:label>

</rdf:Property>

4 Syntagmatic relations are between words in a sentence in sequence, whereas
paradigmatic relations are between words according to meaning, i.e., between
synonyms.

5 Hirst [25] even argues they cannot be synchronized as there are ontological distinctions that are never lexicalized and linguistic distinctions that are ontologically
irrelevant.

P. Cimiano et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 2951

<rdf:Property about=#locatedAt>

<rdfs:domain rdf:resource=#City/>
<rdfs:range rdf:resource=#Highway/>
<rdfs:label xml:lang=en>located at/rdfs:label>

</rdf:Property>

Although each property in these examples has been associated
with meaningful labels (capital, flow through, located at) this is
not sufficient for various reasons:
 Lack of linguistic information about the part-of-speech of the
lexical item expressed by the label. Consider, e.g., the capital
property and assume we want to generate a natural language
description for the triple (Germany, capital, Berlin). To
prevent a system from generating a sentence like Germany capitals Berlin, it needs to know that capital is a noun and cannot be
used as a verb. Capturing part-of-speech information (defining if
it expresses a noun, verb, etc.) for labels is thus essential.
 Lack of deeper linguistic knowledge on subcategorization
frames6 that constrain the linguistic constructions in which
such labels may appear. Consider the case that we want to
generate a natural language description of the triple (Rhein,
flowThrough, Karlsruhe). Here we need to know that flow
is an intransitive verb7 which requires a prepositional phrase
introduced by the preposition through in order to generate an
appropriate sentence like The Rhein flows through Karlsruhe
(provided we have also access to morphological information
about the verb flow, in particular that the 3rd person singular
is flows, see the discussion on inflection above).
 Lack of ways for capturing the variation in relation expression,
as there are many ways in which a certain relation or property can be expressed in language. Consider, for example, the
locatedAt relation, which can be expressed by The A8 passes
by Karlsruhe, The A8 connects Karlsruhe, The A8 goes through
Karlsruhe, etc. Although we would not necessarily want to add
pass, connect and go as labels to the locatedAt property, we
may want to express that all of the corresponding verbal forms
are valid ways of expressing the locatedAt property.
 Lack of ways for expressing how and in which order linguistic
arguments of a certain verb map to corresponding semantic arguments of a predicate as modeled in the ontology. For example,
given a transitive verb such as connects, we may want to specify that its linguistic subject maps to the range of the locatedAt
property and its direct object to the domain, as in [The A8: subject]
connects [Karlsruhe: direct object], which would map to the triple
(Karlsruhe, locatedAt, A8).

We see also from our examples above that properties can be
realized by various constructions, e.g., verbal constructions (flow
through), by way of relational nouns (capital of), but also by way
of participle constructions (located at). The linguistic information
necessary for analysis and generation purposes obviously differs
for the different syntactic categories. Capturing these differences
and modeling the relevant information by introducing appropriate
classes is an important issue for any model allowing us to associate
linguistic information to ontologies.

2.4. Requirements

Given the above explanations, it is clear that more expressive models than those currently available are needed to associate

6 A subcategorization frame of a word specifies the number, type and structure
of the syntactic arguments (subject, direct object, prepositional object, etc.) that it
requires.

7 Transitive verbs (e.g., love) require both a subject and a (direct) object, while

intransitive verbs do require only a subject but no direct object (e.g., sleeps).

linguistic information with ontology elements, particularly with
properties. In general, we derive from the discussion above at least
the following specific requirements on a richer model for enriching
ontologies with linguistic information:

(1) We require that the model fosters a clear separation and
independence between the ontological and linguistic levels.
Separation is important to allow different lexica for one ontology to co-exist, while independence is important to ensure that
the different levels (ontological and linguistic) do not constrain
nor restrict each other.

(2) We require a model that allows us to express (structural) information about linguistic realization with respect to any level of
linguistic description and expressivity required by applications.
For this we clearly need a separate domain of discourse where
lexical entries are first-class citizens and arbitrary complex
information can be attached to them. This includes part-of-
speech, morphological information, etc.

(3) The model should be able to specify the morphological or
syntactic decomposition of complex terms, allowing the
semantics of the single components to be specified with respect
to ontological entities (classes, properties, etc.)

(4) As lexical elements never appear in language in isolation but
interact with other words in a variety of ways (through syntagmatic relations), we need to capture also their syntactic
behaviour as well as how this syntactic behaviour translates
into ontological representations and structures defined in the
ontology.

(5) It should allow the meaning of linguistic constructions to be

specified with respect to an arbitrary (domain) ontology.

Further, a lexicon model for ontologies should fulfill also the

following general requirements (outlined already in [41]):
 support for multilinguality: allowing us to represent complex
lexical entries for multiple languages,
 accessibility: supporting the querying, updating and navigation
of the model,
 interoperability: building on standards that allow models to be
shared.

Our standpoint here is that accessibility and interoperability
will be fulfilled by any model building on some standard representation language (RDFS, OWL, UML, XML, etc.) and having appropriate tool support. The requirement for multilingual representation
can be accommodated by most of the models we discuss below,
though it has not always been a focus. In the simplest case, multilinguality can be taken into account by attaching language information
(e.g., by way of a language tag) to every lexical entry. The Linguistic Information Repository (LIR), a more elaborate model for
representing multilingual information has been presented in [37].

2.5. Related approaches

In what follows we briefly discuss some related approaches and
state whether they fulfill the requirements we have defined. Table 1
summarizes this discussion:
 SKOS: The Simple Knowledge Organization System (SKOS) essentially defines a formal data-model for informal concept schemas
such as thesauri and taxonomies by use of the RDFS and OWL
vocabularies [35,36]. The main focus of SKOS is on exploiting the
RDFS and OWL data-models to model the relations that are typically used in such resources but lack a formal interpretation. For
this purpose, SKOS defines relations such as skos:broader and
skos:narrower on the basis of the syntax and formal semantics

Table 1
Requirements 15 fulfilled by the different models.

(1)
Separation
and indep.

(2) Struct.
ling. infor-
mation

(3)
Syntactic
behaviour

(4)
Morph.
decomp.

(5)
Arbitrary
ontologies

No
No
No
Yes

RDF/OWL

Penman GUM Yes
OntoWordNet No
Yes
LingInfo
Yes
LexOnto
LexInfo
Yes

No
No
Yes
Yes
No
Yes
Yes
Yes
No
Yes

No
No
Yes
No
No
Yes
No
No
Yes
Yes

No
No
Yes
No
No

No
Yes
No
Yes

Yes
n.a.
No
Yes

Yes
No
Yes
Yes
Yes

of the RDFS and OWL vocabularies. Although the representation
of (multilingual) terms is a shared objective, the aims of SKOS
differ compared to ours as our aim is to design a model which
allows us to associate linguistic information to arbitrary ontolo-
gies, while SKOS mainly uses RDF and OWL (as a data model)
to represent classification schemas such as thesauri, technical
vocabularies, etc. In addition to RDFS and OWL, SKOS allows labels
to be modeled in different flavors, i.e., as a preferred label (pre-
fLabel), as an alternative label (altLabel) or as a hidden label
used to capture information for text mining purposes and not
visible to the person inspecting the model for example. SKOS also
incorporates multilingual support by allowing language tags to be
assigned to the labels.

With respect to our criticism that plain labels without any
further linguistic structure do not suffice, SKOS does not add
anything beyond RDFS and OWL. Clearly, SKOS fails on our
requirement 1 as it does not aim for a clear separation between
the knowledge representation and the linguistic levels. Further,
it does neither meet requirements 25.
 LMF: The Lexical Markup Framework (LMF) aims to provide a
meta-model as a standard framework for modeling and representing computational lexicons such as WordNet [18], the SIMPLE
lexicon [30] and others, which is similar to the aims of SKOS to
provide a standardized framework for modeling and representing thesauri. LMF clearly fulfills requirements 14, but fails on
requirement 5 as it does not attempt to establish any connection with domain ontologies, but instead stops where the lexical
semantics of words stops.
 Linguistic Watermark: The Linguistic Watermark (LW) (see [39])
is an ontological and software framework for describing, referring
and managing heterogeneous linguistic resources and for using their
content to enrich and document ontological objects. In essence,
the LW framework includes a meta-model in order to describe
linguistic resources from simple synonym dictionaries, to complex resources such as WordNet. In this sense it is quite similar
to the LMF framework, striving for a uniform model for representing linguistic resources with the goal of interoperability. An
aspect which clearly distinguishes the LW from the LMF is that
the former clearly aims at connecting/integrating the ontological information with the linguistic one. This connection comes in
two flavors: (i) integration proper where parts of the linguistic
resources are directly imported into the ontology, and (ii) linking of concepts to so called SemanticIndexes (e.g., a WordNet
synset) with the aim of documenting the concepts meaning.
With respect to our requirements, it seems that the Linguistic
Watermark clearly fails on requirements 2, 3 and 4. With respect
to requirement 1, it is not clear in how far the linguistic and
the ontological level are really separated as the LW Suite allow
WordNet sub-trees to be imported into the ontology (thus clearly
mixing both levels). Further, it can be expected that the LW allows

arbitrary ontologies to be taken into account but as this is not
completely clear we refrain from filling these fields in Table 1.
 LIR: The Linguistic Information Repository (LIR) [41] is a model
for associating lexical information to OWL ontologies inspired
in the LMF model (see above). The main goal of LIR is to
provide a model supporting the enrichment of the ontology
with a lexico-cultural layer allowing to capture the languagespecific terminology used to refer to certain concepts and to
capture variations across languages. The LIR model has focused
on multilingual aspects as well as on capturing specific variants of terms (such as abbreviations, short forms, acronyms
and transliterations) which are all modeled as subclasses
of the property hasVariant. To account for multilinguality,
the classes LexicalEntry, Lexicalization, Sense, Defi-
nition, Source and UsageContext are all associated to a
certain Language to model variants of expression across lan-
guages. It also allows to document the meaning of certain
concepts in different cultural settings. LIR certainly fulfills
requirements 1 and 5, but certainly not requirements 2, 3 and
4.
 The Penman Generalized Upper Model (GUM): aims at simplifying the mapping between language and knowledge by
introducing a level of linguistically motivated knowledge organization (see [4]). The categories modeled in the Penman Upper
model are linguistically motivated in the sense that they constrain the linguistic realization of knowledge. It relies on a
classification-based paradigm in which the classes, relations, etc.,
which are relevant to a given domain are assumed to be classified
with respect to the linguistically motivated semantic categories
of the Penman upper model. The knowledge organization level
of the Penman upper model is thereby assumed to provide a
domain-independent, reusable knowledge organization that is
valid across domains. According to the rationale of the Penman
project, domain experts should not be required to model linguistic expressions, but only link their own models to a general level of
knowledge organization that is linguistically motivated but keeps
the linguistic details hidden. The Penman upper model contains
about 200 categories [2] and its main goal is to ease the generation of text from knowledge models. While the Penman model
seems to fulfill all requirements 15 in principle, it remains however unclear how variants of expression (relevant for analysis and
generation) are specified in the Penman model. This is a crucial
aspect of the LexInfo model.
 OntoWordNet: OntoWordNet [21] is a project that attempts to
ontologize WordNet in the sense of transforming it (as far as
possible) into a logical theory building on the repertoire of analysis and modeling techniques developed within formal ontology.
In particular, the goal of Gangemi et al. is to map (part of) WordNets synsets to the DOLCE ontology [32] and formally capture
the implicit relations between a synset and other synsets mentioned in its gloss. In particular, this consists of several tasks: i)
identifying WordNet synsets as classes, individuals or relations;
ii) aligning the top of the WordNet hierarchy to DOLCE; iii) consistency checking and iv) adding extra domain relations. This
approach does not offer separation between the ontological and
lexical layer and works only for a single ontology, i.e. WordNet
in this particular case. In the same line, Ovchinnikova et al. have
tried to analyze FrameNet from an ontological point of view and
transform it into OntoFrameNet [40].

3. Building blocks

In the following, we discuss the three main building blocks
of LexInfo: LingInfo, LexOnto and the Lexical Markup Framework
(LMF).

P. Cimiano et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 2951

 rdfs:Class 

 metaclasses 

 ClassWithLingInfo  

 lingInfo 

rdf:type

rdf:type

 Highway 

 lingInfo 

partOf

partOf

 LingInfo  

 lang 
 term 

 HighwayLane 
 lingInfo 

 HighwayInterchange 
 lingInfo 

rdf:type

lingInfo

lingInfo

semantics

 fahrbahn 

 lang = "de" 
 term = "Fahrbahn" 

 autobahnkreuz 

 lang = "de" 
 term = "Autobahnkreuz" 

morphoSyntacticDecomposition

 autobahn 

 kreuz 

 lang = "de" 
 term = "Autobahn" 
 pos = "N" 

 lang = "de" 
 term = "Kreuz" 
 pos = "N" 

 classes 

 instances 

Fig. 1. LingInfo model with example domain ontology classes and LingInfo instances (simplified).

3.1. LingInfo: multilingual terms and morpho-syntactic
information

3.1.1. Basic idea

LingInfo [14,13] was developed as an ontology-based lexicon model that fosters an integrated but modular approach to
the representation of (multilingual) terminology relevant for a
given ontology. LingInfo defines a lexicon model where terms can
be represented as objects that include lexical, morpho-syntactic
decomposition and point to semantics as defined by a (domain)
ontology. Consider for instance the previously discussed example
Autobahnkreuz (highway interchange). This term can be linguistically decomposed into the following morphological stems
Autobahn (highway) and Kreuz (interchange), each of which can
be linked to lexical information and semantics as expressed by a
domain ontology class. Even more complex examples of this can
be found for instance in medical terminology, e.g. muscular branch
of lateral branch of dorsal branch of right third posterior intercostal
artery.

This complex term corresponds to a complex nominal phrase and
can be linguistically decomposed into the following sub-phrases,
where each (sub-) phrase may in turn express an ontology class:
sub-phrase 1 muscular branch
sub-phrase 2 lateral branch
sub-phrase 3 dorsal branch
sub-phrase 4 right third posterior intercostal artery

The LingInfo model has been developed to represent this kind of
morpho-syntactic information on (multilingual) terms for ontology
classes and properties. Among the requirements listed in Section 2
above, the LingInfo model therefore clearly addresses requirements
1 and 2, as well as 4 and 5 as the meaning of decomposed terms
will be represented with respect to a domain ontology, clearly separating linguistic knowledge from ontological knowledge.

3.1.2. Design

LingInfo supports the representation of linguistic information that is needed to handle the cases discussed above, which
includes: language-ID (ISO-based unique language identifier), part-
of-speech (of the head of the term), morphological and syntactic
decomposition, and statistical/grammatical context models (linguistic
context represented by N-grams, grammar rules, etc.). The LingInfo
model supports the association of such information with ontology elements by way of a meta-class (ClassWithLingInfo) and
a meta-property (PropertyWithLingInfo) which are instantiated
by the class or property in question. This allows to link these classes
and properties to instances of the class LingInfo, which represents
the linguistic features of the class or property. Fig. 1 provides an
overview of the model with example domain ontology classes and
associated LingInfo instances. The domain ontology consists of the
class Highway with parts HighwayLane and HighwayInterchange,

Fig. 2. Main elements of LexOnto.

each of which are instances of the meta-class ClassWithLingInfo with the property lingInfo pointing to the corresponding
LingInfo objects.

3.2. LexOnto: representing syntactic behaviour

3.2.1. Basic idea

As already discussed in the previous sections, words do not
appear in isolation in natural language but enter into a variety of
syntagmatic relations with other words which constrain the sentences that can be constructed. When developing richer models
allowing linguistic information to be associated with ontologies, it
is thus crucial to capture the syntactic behaviour of words and the
relation between this behaviour and the ontology. This is the goal
for which the LexOnto model was designed [15]. LexOnto focuses
in particular on the representation of the syntactic behaviour of
nouns, verbs and adjectives and also on capturing their meaning
with respect to a domain-specific ontology. More generally, as any
lexicon, LexOnto clearly focuses on the representation of open-class
words following the rationale described by Graeme Hirst (see [26]):

The words that are of interest [in a lexicon] are usually openclass or content words, such as nouns, verbs and adjectives rather
than closed-class or grammatical function words, such as arti-
cles, pronouns, and prepositions, whose behaviour is more tightly
bound to the grammar of the language.

to predicate-argument structures for verbs, nouns as well as lexical
entries for adjectives (see Fig. 3). In order to simplify the representation of the mapping between lexical structures (LexicalElement
s), there is one single relation anchor between a LexicalElement
and a Class (understood as a frame representing the semantics of
the LexicalElement). This allows the mapping from lexical elements to ontological structures to be represented in a uniform
fashion for all types by pointing to a single class (the so called
anchor class). For instance, the pattern X is capital of Y lexicalizes a structure anchored at the class Country (because this class is
the domain of the hasCapital property). While the Y maps to the
domain of the property hasCapital in the sense that it would fill
the subject position of a corresponding triple, X maps to the range
of the hasCapital property in the sense that the X position will
represent the range of the hasCapital property. The verbs write
and flow (through) lexicalize a structure anchored at the classes
Document and River, respectively (i.e., these are the domains of
the properties they refer to, i.e., hasAuthor and flowsThrough).

PredicativeLexicalElements are in all cases linked to a
WordForm, e.g., a verbal predicate-argument structure is linked to
its head verb, a nominal predicate-argument structure to its head
noun, etc. As the treatment of verbs, nouns and adjectives in LexOnto has been basically transferred to the LexInfo model, we discuss
the details in Section 4.

3.3. Lexical Markup Framework

3.2.2. Design

At an abstract level, the design of the LexOnto Model is conceptually very simple. The main class of the LexOnto model is
the class LexicalElement, which has the subclasses PredicativeLexicalElement (PLE) and WordForm (see Fig. 2). WordForms
correspond to nouns, verbs and adjectives as plain words ignoring
the predicate-argument structures they project. PLEs correspond

The Lexical Markup Framework (LMF) is a meta-model that
provides a standardized framework for the creation and use of computational lexicons, supporting interoperability and reusability
across applications and tasks [23]. As the lexicon for an ontology is a special type of a computational lexicon, we build on
the LMF framework to describe lexica for ontologies. The LMF
meta-model
is organized in a number of packages (depicted

Fig. 3. Predicative lexical elements in LexOnto (showing only data-type but no object properties).

P. Cimiano et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 2951

Fig. 4. Package structure of LMF taken from [23].

in Fig. 4). The core package contains the basic elements of the
model and their dependencies (depicted in UML-style notation in
Fig. 5).

The central entity in the LMF meta-model

is Lexical
Resource, which has an associated Global Information object
capturing administrative details and information related to encod-
ing. A Lexical Resource consists of several language-specific
Lexicons. A Lexicon then comprises of Lexical Entries (i.e.,
words, multi-word entities such as terms and idioms, etc.) which
are realized in different Forms and can have different meanings or
Senses.

Global Information

Lexical Resource

1..*

Lexicon

Form

1..*

1..*

Lexical Entry

0..*

Form Representation

0..*

0..*
Sense

Representation

0..*
Text Representation

0..*

0..*

Definition

0..*

Statement

Fig. 5. Core package model taken from [23].

Other packages which are of relevance to our work here are:

(1) Morphology extension: provides a mechanism for describing
the morphological structure of lexical entries (extensionally, i.e.
for specific lexical entries without supporting the definition of
general patterns or rules).

(2) NLP syntax extension: supports the description of the syntactic
behaviour and properties of a lexical entry, in particular the subcategorization frame structure for predicative elements such as
verbs, etc.

(3) NLP semantics extension: provides a way to associate semantic representation structures to syntactic structures, which
clearly has a strong relation with the syntax package, supporting the definition of semantic predicates and their semantic
arguments as well as their association with syntactic arguments
of a subcategorization frame.

The other LMF packages are not that important for our current
purposes as they cover: (i) the intensional definition of patterns for
morphological operations (NLP Morphological Pattern extension), (ii)
the representation of data stored in machine readable dictionaries (MRD extension), (iii) the representation of sense and syntactic
behaviour equivalents across languages (NLP multilingual notations
extension), and (iv) the description of the internal structure of a
multi-word entity (MWE) (NLP MWE Pattern extension). However,
it might be the case that these packages become relevant at a later
stage of the development of LexInfo.8

At first sight, the LMF model serves our goal of associating linguistic information to ontology elements as it clearly distinguishes
between the syntactic and semantic levels of description (NLP Syntax vs. NLP Semantic packages). In our case, the NLP Semantic
package contains classes, properties and other ontological structures as described in a (domain) ontology with which we associate
the computational lexicon(s). As shown in Figs. 6 and 7 however,
both levels are clearly interlinked as described below.

In the

syntactic

syntactic
behaviour

extension, we
can model
(lmf:SyntacticBehaviour)

(i)
of

the
lex-

8 In fact, at the time of writing we are working on integrating the NLP Morphological Pattern extension into the LexInfo API in order to represent generative inflection
patterns and avoid representing inflectional variants for all lexical entries explicitly.
However, this part of the work is not completed. The LexInfo API is currently still
under development and a description of all features is out of the scope of this article.

Lexicon

Lexical Entry

0..*

0..*

Syntactic Behaviour

0..*

0..*

Sense

0..1

0..*

0..*

0..*

0..*

Lexeme Property

Subcategorization Frame

0..*

0..*

0..1

0..* 0..*

0..*

0..*
Subcategorization Frame Set

0..*

0..*

0..*

{ordered}

0..*

SynArgMap

0..1

0..*

Syntactic Argument

SynSemArgMap

0..*

Described in Semantic package

Fig. 6. Structure of the NLP Syntax extension, taken from [23].

entries

in

the

ical
frames
(lmf:SubcategorizationFrame) and their corresponding syntactic arguments (lmf:SyntacticArgument), such as subject, object,
etc. The lmf:SynSemArgMap, which is located in the semantic

subcategorization

form of

extension package, is the key entity allowing us to associate syntactic arguments to semantic ones. From a semantic point of view,
entities of type lmf:LexicalEntry, lmf:SyntacticBehaviour,
and lmf:PredicativeRepresentation are all associated to a

Lexical Entry

Sense Relation

Described in syntactic annex

0..*

Syntactic Behaviour

0..1

0..*

0..*

0..*

Subcategorization Frame

Predicative Representation

Syntactic Argument

0..*

0..*

0..*

0..*

SynSemArgMap

0..*

0..*

0..*

Semantic Argument

0..*

Semantic Predicate

0..*

0..*

0..*
Argument Relation

0..*
0..*

0..*

0..*

Predicate Relation

SynSemCorrespondence

0..*

0..*

Lexicon

0..*

0..*

0..*

Sense

Sense Example

Monolingual External Ref

0..*

0..*

0..*

1..*

0..1

Synset

0..*

0..*

0..*

0..*

0..*

0..*

0..*

0..*

Synset Relation

0..*

Definition

0..*

Statement

Fig. 7. Structure of the NLP Semantics extension, taken from [23].

P. Cimiano et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 2951

 LexicalEntry  

 Conjunction  

 Noun 

 Verb 

 ProperNoun  

 Phrase 

 Adjective 

 Preposition 

 Adverb 

 Determinant  

 NounPhrase 

 VerbPhrase 

 ProperNounPhrase 

Fig. 8. Subclasses of LexicalEntry.

 River:Class 

hasSense

 River_LE:Noun 

Fig. 9. The lexical entry for river points to the class River through the hasSense
property. The lexical and ontological levels are clearly separated but linked to each
other.

lmf:Sense which captures their (lexical) semantics. For our pur-
poses, the crucial entities are lmf:PredicativeRepresentation,
lmf:SemanticPredicate and lmf:SemanticArgument, which
will be refined in terms of specific subclasses which allow us to
connect to predicates defined in the ontology (e.g., classes and
their properties). The essential class for mapping syntactic to
semantic arguments is the lmf:SynSemCorrespondence class,
which consists of a number of SynSemArgMaps mapping specific
syntactic arguments of a subcategorization frame to semantic
arguments of a lmf:SemanticPredicate. In the following section
we will describe how the LMF model has been extended to support
the association of ontological structures to lexical entries and,
conversely, of linguistic information to ontological classes and
properties.

4. LexInfo: enriching ontologies with linguistic information

The starting point for our unifying model for associating linguistic information to ontologies are the LingInfo, LexOnto and LMF
models discussed in Sections 3.1, 3.2 and 3.3, respectively. The glue

for the three frameworks will essentially be provided by the LMF
model. We proceeded as follows to arrive at our unifying model:
 We downloaded the OWL version of the LMF model available at
http://www.lexicalmarkupframework.org/.
 As the ontology has been originally created starting from an UML
model and only uses the properties isAssociated, isPartOf
and isAdorned, we have introduced appropriate subproperties
for most of the associations between entities described in the
LMF Specification [20]. Further, we have commented most of the
ontology classes on the basis of the descriptions of the LMF Specification [23]. The resulting ontology is available for download at
http://lexinfo.net/lmf.
 Then, we created a new ontology LexInfo importing the corrected LMF ontology, introducing our monotonic extensions
on top of it. The LexInfo ontology can be downloaded here:
http://lexinfo.net/lexinfo.

In what follows we discuss how LexInfo meets our requirements

15.

4.1. Separation between linguistic and ontological levels

The separation between the ontological and linguistic levels is
achieved in LexInfo by introducing two separate domains of discourse with different namespaces (and different ontologies). On the
one hand, we have the domain ontology with its own namespace
defining the relevant classes, properties and individuals in the given
domain. And on the other hand, the lexical information is factored
into a separate domain of discourse, which is structured accord-

 River:WordForm 

 hasWrittenFrom="river" 

hasSyntacticProperty

 Singular:SyntacticProperty 

 synPropName="number" 
 synPropValue="singular" 

hasWordForm

hasWordForm

 Rivers:WordForm  

 hasWrittenForm="rivers" 

hasSyntacticProperty

 Plural:SyntacticProperty 

 synPropName="number" 
 synPropValue="plural" 

 River_LE:Noun 

hasLemma

 River_Lemma:Lemma 
 hasWrittenForm="river" 

Fig. 10. Modeling river and its morphological variations using the LMF machinery. Here both the plural and singular forms are represented with appropriate syntactic
properties.

 Autobahnkreuz_LE:Term  

hasLemma

 Autobahnkreuz_Lemma:Lemma  
 writtenForm="Autobahnkreuz" 

listOfComponents

 Autobahnkreuz_LC:ListOfComponents

hasComponent

hasComponent

 Autobahnkreuz_C1:Component
 order="1" 

 Autobahnkreuz_C2:Component
 order="2" 

lexicalEntry

lexicalEntry

 Autobahn_LE:Noun  

 Kreuz_LE:Noun  

hasLemma

hasSense

hasLemma

hasSense

 Autobahn_Lemma:Lemma  
 writtenForm="Autobahn" 

 Highway:Class  

 Kreuz_Lemma:Lemma  
 writtenForm="Kreuz" 

 Junction:Class  

Fig. 11. Example of decomposition (Autobahnkreuz) with linking to ontology concepts (LexInfo extension). Here we see the lexical entry is decomposed into two components,
both of which are lexical entries with their own lemmas.

ing to the LexInfo model. The main entities in the lexical domain
of discourse are instances of the class LexicalEntry. Fig. 8 shows
the subclass hierarchy of lexical elements. Fig. 9 shows how a LexicalElement representing the Noun river is linked to the class
River defined in the ontology by pointing via the hasSense9 property to an individual of an OWL meta-ontology [49] standing proxy
for the class.

4.2. Linguistic expressivity

The separation between the linguistic and the ontological levels also enables linguistic information to be attached to lexical
elements for any level of required linguistic expressivity. In par-
ticular, LexInfo allows for a high degree of linguistic expressivity
by attaching part-of-speech and morphological information to lexical entries. This is in contrast not possible in RDF(S), OWL or
SKOS, which restrict the range of the label property to Literal.10

9 A reviewer has pointed us to the fact that we are using the term Sense here in an
unorthodox fashion (from a lexical semantics point of view). However, let us note
that in standard sense-enumerating lexicons (the ones criticized by Pustejovsky in
his theory of the Generative Lexicon [42]), the sense of a word is defined by pointing
to different elements of the (finite) sense inventory, as for instance in WordNet [18].
We are using sense in this line but assuming that the sense inventory is provided by
a given domain ontology.
10 SKOS-XL partially has lifted this restriction: http://www.w3.org/TR/skos-
reference/skos-xl.html.

Part-of-speech information is attached to lexical entries by specifying the lexical entry as an instance of classes such as verb, noun,
adjective, etc. This is done according to the subclass hierarchy of
LexicalEntry (see Fig. 8).

Morphological information and relations between the different
morphological variants are captured by directly building on the
structure provided by LMF. Fig. 10 shows how information about
morphological variants of a word (here river) are modeled through
WordForms. In the figure we see how the fact that the plural of river
is rivers is specified through an instance of WordForm which has
the syntactic property Plural and the value rivers for the property
hasWrittenForm.

4.3. Morphological or syntactic decomposition of composite
terms and multi-word expressions

The morphological decomposition of terms is modeled in LexInfo by building on the morphological extension package of LMF,
which essentially allows us to associate a ListOfComponents with
a LexicalEntry. A list of components represents an ordered list of
components (with a minimum of 2) (see [23]). We have modeled
this in OWL by introducing an additional datatype property order
specifying the absolute order of a Component within a ListOfCom-
ponents. Components then point to LexicalEntries which can
again be composite, thus allowing for recursion. In order to capture
how the parts of a compound are associated to the ontology, we
build on the general mechanism of LMF allowing LexicalEntries to

P. Cimiano et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 2951

 StructuralAlignmentReport:NounPhrase 

hasListOfComponents

 StructuralAlignmentReport_LC:ListOfComponents 

head

 Report:Noun 

hasComponent

hasComponent

head

 StructuralAlignementReport_C1:Component 
 order="1" 

 StructuralAlignnmentReport_C2:Component 

 order="2" 

lexicalEntry

lexicalEntry

lexicalEntry

 Structural:Adjective 

 AlignmentReport:NounPhrase 

hasListOfComponents

 AlignmentReport:ListOfComponents 

hasComponent

hasComponent

 AlignmentReport_C1:Component 
 order="1" 

 AlignmentReport_C2:Component 
 order="2" 

lexicalEntry

 Alignment:Noun 

Fig. 12. Example of phrase decomposition of structural alignment report. Here we see that a tree is created by first decomposing the lexical entry into structural + NP,
and then the noun phrase is decomposed into alignment + NP and the final noun phrase into report.

 SubcategorizationFrame  

 AdjectiveMod 

 NounPP 

 Noun2PP 

 Transitive 

 IntransitivePP 

 TransitivePP 

Fig. 13. Subclasses of SubcategorizationFrame.

 SyntacticArgument  

 SemanticArgument 

 Subject  

 Object  

 PObject  

 Mod  

 Domain  

 Range 

Fig. 14. Subclasses of syntacticargument and semanticargument.

 PredicativeRepresentation 

 ClassPredicativeRepresentation 

 PropertyPredicativeRepresentation 

 LiteralPropertyPredicativeRepresentation 

 ScalarPropertyPredicativeRepresentation 

Fig. 15. Subclasses of PredicativeRepresentation.

be associated with a Sense, of which owl2:Entity is a subclass.11
In this way, we are able to state that Autobahnkreuz is composed of
two lexical entries where the first refers to the class Highway and
the second to the class Interchange (see Fig. 11). In this sense the
LexInfo model thus captures the relevant aspects of the LingInfo
model, allowing the morphological decomposition of terms to be
modeled and thus fulfilling requirement 4.

Multi-word expressions are modeled in a similar manner by way
of the subclass Phrase of LexicalEntry indicating that the lexical
entry is actually a complex expression. Phrase has a number of sub-
classes, such as NounPhrase or VerbPhrase, representing a phrase
with the head being a noun or verb, respectively. Each phrase is
connected to a LexicalEntry representing the head via the property
head. Each component of the phrase is then modeled in the same
manner as for composite terms (see Fig. 12).

4.4. Syntactic behaviour

We have argued already that the representation of syntactic
behaviour is crucial for any model supporting the attachment of
linguistic information to ontology elements. For this purpose, we
have extended the LMF model by reusing its classes but refining
them in the LexInfo model, in particular introducing the following
subclasses of LMF classes:

(1) Subclasses of lmf:LexicalEntry, i.e., lexinfo:Verb, lex-
info:Noun, etc., which are distinguished by way of attributes
in the LMF model (see Fig. 8).

(2) Subclasses of lmf:SubcategorizationFrame,
info:Transitive, lexinfo:IntransitivePP,
Fig. 13).

i.e., lex-
(see
etc.

(3) Subclasses

of

lmf:SyntacticArgument,

i.e.,

lex-

(4) Subclasses

the

classes,

e.g.,

lmf:SemanticPredicate

lexinfo:ClassPredicate

info:Subject, lexinfo:Object, lexinfo:PObject,
etc. (see Fig. 14).
of

lmf:PredicativeRepresentation
the
and
classes
lexinfo:ClassPredicativeRepresentation
as well
and
lex-
and
info:PropertyPredicativeRepresentation
to
lexinfo:PropertyPredicate allowing us
respectively (see
a class or property (as predicate),
Figs. 15 and 16).
there are subclasses
(ScalarPropertyPredicativeRepresentation and Liter-
alPropertyPredicativeRepresentation) for representing

In addition,

to refer

as

the behaviour of the predicate if the range is valued (i.e.,
integer, string, etc.).

(5) Subclasses of the lmf:SemanticArgument class, i.e., lex-
info:Domain, lexinfo:Range, etc., as well as appropriate
subclasses allowing the semantic arguments of a class to be
specified (where properties are understood as slots of the frame
represented by the class) (see Fig. 14).

It is important to note that LMF also distinguishes between different types of subcategorization frames. However, the distinction
is encoded by way of attributes, i.e., regularSVO for a transitive
verb, for example. The advantage of modeling the different subcategorization frames as subclasses (as we have done) is that this
allows us to formulate additional axioms, requiring for example
that a Transitive subcategorization frame has exactly two syntactic arguments: a subject and an object. Such general axioms
allowing the lexicons to be checked for consistency are clearly not
possible in the original LMF model.

There are a number of classes in the LexInfo model that stem
from the LMF model and allow us to represent syntactic structures together with their mapping to ontological structures. The
main classes defining the syntactic behaviour of lexical elements
is SyntacticBehaviour (linked to LexicalEntry through the
property hasSyntacticBehaviour). Each instance of SyntacticBehaviour is associated to an instance of Subcategorization
Frame (through the property subcatFrame which specifies the type
of syntactic structure (transitive verb, intransitive verb, etc.) as
well as the syntactic arguments it requires to be realized linguis-
tically. This is accomplished through the hasSyntacticArgument
property linking to various instances of the class SyntacticArgument representing the Subject, Object, etc. of the syntactic
construction. At the semantic side, an analogous specification of
the predicate in question and its semantic arguments is assumed.
PredicativeRepresentation stands proxy for a class or property

 SemanticPredicate 

11 We
http://owlodm.ontoware.org/OWL2.

build

the

on

OWL2 meta-ontology

 ClassPredicate 

 PropertyPredicate 

for

this

purpose:

Fig. 16. Subclasses of SemanticPredicate.

P. Cimiano et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 2951

 Lemma 

 hasWrittenForm: String 

 SyntacticProperty 

 synPropName: String 
 synPropValue: String 

 SynSemCorrespodence 

hasSynSemCorrespondence

 PredicativeRepresentation 

hasLemma

hasSyntacticProperty

hasSynSemArgMap

 LexicalEntry 

hasWordForm

 WordForm  

 hasWrittenForm: String 

hasSyntacticBehaviour

 SynSemArgMap 

predicate

 SyntacticBehaviour 

subcatFrame

synArg

semArg

 SubcategorizationFrame  

hasSyntacticArgument

 SyntacticArgument  

 SemanticArgument  

hasSemanticArgument

 SemanticPredicate 

Fig. 17. General schema for SynSemCorrespondence: in this diagram the SynSemArgMap is used to link the syntactic argument of the subcategorization with the semantic
argument of the corresponding ontology predicate. The structure on the left shows the lexical part of the lexicon with the entry, lemma, word forms and syntactic properties.
On the right side the semantic mapping includes the predicative representation and the predicate. The SynSemCorrespondence has the function of gluing together all the
mappings from syntactic to semantic arguments for the given subcategorization frame.

defined in the ontology. This proxy relation is captured through the
property predicate, which links a PredicativeRepresentation
to a SemanticPredicate which can be either a PropertyPredicate or a ClassPredicate pointing to the appropriate instance of
the OWL2 meta-ontology representing the corresponding class or
property. The crucial link between the syntactic and semantic levels
is accomplished through the class SynSemCorrespondence, which
is related to one or more SynSemArgMaps establishing the correspondence between a syntactic argument in the subcategorization
frame (through the property synarg) and a semantic argument
of the PredicativeRepresentation (through semArg). Fig. 17
depicts all of the above schematically. We describe how this scheme
is instantiated for the different types of part of speech (verbs, nouns,
adjectives) in the subsections below.

4.5. Verbs

Verbs inherently express relations between their arguments and
project different predicate-argument structures which we can not
capture if we only model verbs as plain strings (in the form of
labels). In the LexInfo model we thus explicitly represent verb argument structures (so called subcategorization frames) and the way
they map to corresponding (domain-specific) ontology structures
(especially properties). In particular, in the model we consider the
following classes of verbs (so far):

 Transitive verbs (Transitive): these are those requiring a subject and a direct object, e.g., love, write, etc.
 Intransitive verbs subcategorizing a prepositional complement (IntransitivePP): these are those requiring a subject and
a prepositional complement headed by a specific preposition, e.g.,
flow through, pass by, etc.
 Transitive verbs subcategorizing a prepositional complement
(TransitivePP): those requiring a subject, an object and a
prepositional complement, e.g., write an article about sth.

The hierarchy for verbal (and nominal) subcategorization
frames is shown in Fig. 13. We illustrate the interplay between the
different classes on the basis of the example of the subcategorization frame for the verb flow which requires a subject (i.e., that what
flows) as well as a prepositional object introduced by the preposition through. Obviously, it is possible that a verb projects different
subcategorization frames and syntactic behaviours. Fig. 18 shows
how the link between the syntactic behaviour of the verb flows
(through) and the ontology is modeled. First of all we have a LexicalEntry (a Verb more specifically) representing the verb itself.
The verb has a base form or lemma flow. Further, the lexical entry
for flow is related to an instance of SyntacticBehaviour representing one of its possible syntactic realizations. The instance
of SyntacticBehaviour is then linked to an instance of type
SubcategorizationFrame (an IntransitivePP in this particular case). The modeling makes explicit that this subcategorization
frame requires two syntactic arguments: a Subject and a POb-
ject. At the semantic/ontological level we introduce an instance
of the class PredicativeRepresentation which will stand proxy
for the object property flowsThrough with a River as Domain and
a Location as Range. The instance of PropertyPredicativeRepresentation is linked to an instance of PropertyPredicate
(through the property predicate), which in turn points to an
object property defined in the domain ontology via the property property. The PropertyPredicativeRepresentation has
two arguments: the Domain and the Range of the property. The
link between syntax and semantics is achieved via an instance of
the SynSemCorrespondence class which is related to two SynSemArgMaps via the hasSynSemArgMap property. One instance of
SynSemArgMap links the subject of the subcategorization frame to
the domain of the property flowsThrough, while another instance
of SynSemArgMap links the object syntactic argument to the range
of the predicative representation of flowsThrough.

In principle, there is no limit to the classes considered and the
model can be extended to cover new types at any time. However,
so far we have limited the model to the most frequently occurring ones. The model will be certainly extended in the future in

 flow_subject:Subject  

synArg

 map1:SynSemArgMap  

semArg

 flowThrough_domain:Domain  

 flow_pobject:PObject  

synArg

 map2:SynSemArgMap  

semArg

 flowThrough_range:Range  

hasSyntacticArgument

preposition

hasSynSemArgMap

hasSyntacticArgument

 through:Preposition  

 hasWrittenForm="through" 

hasSynSemArgMap

hasSemanticArgument

hasSemanticArgument

 flow_SF:IntransitivePP  

subcatFrame

 flow_SB:SyntacticBehaviour  

 flowThrough_PPR:PropertyPredicativeRepresentation

 synsem1:SynSemCorrespondence  

predicate

 flowThrough_P:PropertyPredicate  

hasSyntacticBehaviour

proper ty

 flow:Verb  

hasLemma

 flow_lemma:Lemma  
 hasWrittenForm="flow" 

 flowThrough:ObjectProperty  

Fig. 18. Modeling of the syntactic behaviour of the LexicalEntry (Verb) flow together with the mapping to the ontological predicate flowsThrough via the SynSemCorrespondence class.

line with needs of future applications. Further, we intend to hook
up the model to existing linguistic categories and resources as our
goal is not to reproduce work on standardization of linguistic categories conducted in the computational linguistics community, in
particular under the auspices of ISO.12

external subject over which the property is predicated. For exam-
ple, in the case of the syntactic construction X is the capital of Y,
the head of the prepositional object, i.e., Y, represents the domain
of the property capital, while the X is realized here as the subject
of a copula construct13 and represents the range of the property
capital.

4.6. Nouns

Nouns have been noted in general to correspond to ontology
classes, but noun phrases can have an internal structure and subcategorize one (or more) prepositional phrases: e.g., the mother of
John, the capital of Ireland, the flight from Dublin to Bielefeld, the
distance between Bielefeld and Saarbrucken, etc. Thus, nouns subcategorizing for a prepositional complement are modeled in LexInfo by
associating them to a corresponding subcategorization frame. Such
relational nouns thus represent a relation or a property rather than
a single class. We have shown in Fig. 9 how simple nouns representing a class are represented in LexInfo. In this section we discuss how
relational nouns which actually represent a property are modeled.
Relational nouns are modeled in LexInfo by way of nominal subcategorization frames that require a prepositional object as well as an

4.7. Adjectives

Adjectives subcategorize syntactically the noun they modify
and semantically the entity which the property expressed by the
adjective is predicated over. Each adjective is assumed to have
the subcategorization frame AdjectiveMod indicating that it modifies another lexical entry and the modified syntactic argument
is denoted by the argument type Mod. We model the semantics
of adjectives relative to properties, normally by defining a set of
permissible object values for the adjective, i.e., we may say that
big can be applied to all cities with more than 100,000 inhabi-
tants. We then split the semantic behaviour into different forms by
having subclasses of PredicativeRepresentation and Seman-

12 http://www.isocat.org/.

13 In linguistics, a copula is a word used to link the subject of a sentence with a
predicate, e.g., John is nice.

P. Cimiano et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 2951

 Green_PR:LiteralPropertyPredicativeRepresentation 

predicate

 Green_SP:PropertyPredicate 

predicativeRepresentation

property

 Green_SB:SyntacticBehaviour 

 hasColor:Sense  

hasSyntacticBehaviour

In the range of

 Green:Adjective 

hasSense

 green:Sense 

Fig. 19. Modeling the adjective green and its relation to the ontology. Here we see that the literal predicate matches to a value green which is an instance of the range of
the property hasGreen, represented here as pseudo-property in the range of.

ticPredicate. In particular we define the following specific types
of adjectives:

 Literal adjective: This is used when the adjective models the
value of an object property. The standard example are colors, for
example. If there was a property hasColor with a range defined
as the class Color, with green as an instance of this class, then
we would model green as corresponding to this individual when
used in the given relation. We model this as a LiteralPropertyPredicativeRepresentation and a semantic predicate
PropertyPredicate, and we associate the LexicalEntry for
green with the individual green using the hasSense property
and the PropertyPredicate with the property hasColor using
the property property. This modeling is shown in Fig. 19. We
have to admit that alternative formalisations of the semantics of
color adjectives are possible, but this is not important here. In
fact, LexInfo is agnostic with respect to which semantic theory
and modeling to adopt, as long as the corresponding predicates
are defined in the respective ontology. The important point that
we want to stress is that LexInfo has been designed in such a
way that different formalisations of the semantic of adjectives can
be represented. The specific choice of semantic representation is
certainly left to the designers of the ontology and/or lexicon.
 Class adjective: A class adjective is associated to a class in the
ontology. This is modeled by way of the predicate representation
ClassPredicativeRepresentation and the semantic predicate ClassPredicate. An example here is the adjective female
which expresses the membership in the class Female. As such we
can say that the semantics of female dog is that it is a Dog that
is also in the class Female, which corresponds to an intersective
interpretation of the adjective.
 Scalar adjective: A scalar adjective models a data-type property
whose range is totally ordered. Ideally, if we have a data-type
property like size, then we would like to define the semantics of
large as a subset of the range of the property size, i.e., a city is
large if it has size in the set {x : x 100,000}. This corresponds
to the interpretation of vague or graded adjectives with respect
to what Bennett has called relevant observables [6].

As with literal adjectives we have PropertyPredicate that
refers to the data-type property and we have a ScalarProp-
ertyPredicativeRepresentation. For open domain subsets,
e.g., x 100,000, we add two properties to the ScalarProper-
tyPredicativeRepresentation, namely dataValue indicating the minimum or maximum value of this range, and polarity

indicating if this value is the minimum or maximum. This means
that we also have a natural modeling of comparative and superlatives as if the polarity is positive this mean that larger
indicates an increase in the data-type property value. We also
allow an optional contextSense to be defined, which states that
this particular adjective only holds if the subject of the data-type
property belongs to a given class. In the example above it is clear
that the data-type property size may apply to several domains,
but that the range corresponding to large dogs is not the
same as the range corresponding to large buildings (see
also [6] on this issue).

4.8. Specification of meaning w.r.t. ontology

The requirement of allowing the meaning of linguistic constructs (terms, compounds, subcategorization frames)
to be
specified with respect to a given ontology trivially follows from the
way we have conceived the LexInfo model (see particularly Section
4.3 for how the semantics of terms and compounds are specified
with respect to a domain ontology and Section 4.4 for how the
ontological meaning of subcategorization frames is represented).

4.9. Multilinguality

The LexInfo model allows us to associate linguistic information
in various languages to ontology elements. This support for multilinguality is a byproduct of the design choice to build on LMF as
a basis for our model. The assumption in LMF is that lexicons are
language-specific artifacts, such that there is a different lexicon for
every language supported. While this is a reasonable assumption,
other alternatives are conceivable. For example, we could assign a
language tag to every single lexicon entry, having just one lexicon
per ontology. By building on the LMF machinery we have however opted for having one lexicon per language, which does not
constitute a restriction in any way.

Following the LexInfo model, the linguistic information for
different languages (Lexical Entries, Subcategorization Frames,
Syntactic behaviour, etc.) is linked to certain ontology elements,
which are regarded as language-independent. In this way, while
the different variants for expressing a certain class or property
are not directly related, they are linked via the ontology (see
Fig. 20). The figure shows how realizations in different languages
(flows through in English and fliesst durch in German) are linked
via SynSemCorrespondence objects to the same PropertyPred-

 lex_german:Lexicon  
 language="de" 

hasLexicalEntry

 fliessen_Verb:Verb  

hasLemma

 fliessen_Lemma:Lemma
 writtenForm="fliessen" 

hasSyntacticBehaviour

 fliessen_SB:SyntacticBehaviour

subcatFrame

hasSense

 fliessen_SSM:SynSemCorrespondence

 fliessen_SF:IntransitivePP

synSemCorrespondence

 flowThrough_PPR   

Prope rt yPre dicat ive Re pre se nt at ion 

predicate

 flowThrough_PP:PropertyPredicate

property

 flowThrough:ObjectProperty

synSemCorrespondence

 flow_SSM:SynSemCorrespondence

 flow_SF:IntransitivePP  

subcatFrame

hasSense

 flow_SB:SyntacticBehaviour

hasSyntacticBehaviour

 lex_english:Lexicon  

 language="de" 

hasLexicalEntry

 flow_verb:Verb  

hasLemma

 flow_Lemma  
 writtenForm="flow" 

Fig. 20. Modeling multilinguality: on the top we have a lexical entry for the German verb fliessen in a German lexicon and on the bottom a lexical entry for the English
verb flow in an English lexicon. The connection between the two entries is realized by sharing the same ontology sense and the same property predicate representation and
semantic predicate.

icativeRepresentation object representing the flowsThrough
object property. The fact that there is no direct link is reasonable
as the exact relation between different linguistic realizations might
be unclear (they will not always be translation equivalents), while
they might certainly express the same meaning with respect to the
vocabulary of the ontology.

4.10. Formalisation as an ontology

While the LingInfo model was originally implemented in the
RDF(S) language [14], LexOnto was already created using OWL-DL.
The version of LexInfo described in this paper14 has been implemented in OWL-DL. In this section we discuss the different possible
choices for implementing LexInfo, together with their advantages
and disadvantages:
 RDF(S): In RDF(S), Classes are themselves resources, so that we
can attach linguistic information to classes by treating them as
individuals. This is exactly the way that linguistic information
has been attached to classes and properties in the LingInfo model

14 Available here: http://lexinfo.net/lexinfo.

by introducing the meta-classes ClassWithLingFeat and PropertyWithLingFeat which have other classes as instances, thus
allowing the linguistic information to be attached. This modeling
allows combined reasoning over the lexicon and the ontology, but
is limited in expressivity as it is restricted to the expressiveness
of RDFS.
 OWL-DL (with ontology meta-model): While it is forbidden to
use the same URI to refer to an individual and a class in OWL-DL,
this can be circumvented if we keep the lexical and ontological
layers separate, thus not allowing reasoning over both ontologies
together. This is exactly the way that linguistic information has
been modeled in the LexOnto model and in the LexInfo model,
building on an appropriate OWL meta-model to talk about properties and concepts in the lexicon ontology.
 OWL 2: in OWL2, a technique known as punning allows us to
use the same URI to refer to an individual and a class. This is
possible by treating the URIs logically as different symbols, clearly
distinguishing between the individual and the class (see [22]).
OWL2 thus allows combined reasoning over lexica and domain
ontologies, overcoming the above mentioned limitations of OWLDL while clearly going beyond the expressive power of RDF(S).
 OWL Full: As OWL Full is the only dialect of OWL which subsumes RDF(S), it allows the same URI to be used to represent
an individual and a class and is the most expressive ontology

P. Cimiano et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 2951

language available. While it therefore provides the necessary
expressiveness to represent lexica and their association to ontology elements, the disadvantage is that it is undecidable and thus
not relevant for practical implementations.

5. Tools and applications

In this section we describe the tool support that is so far available
to create, maintain and serialize LexInfo lexicons. We discuss on the
one hand the design of the LexInfo API and present a plug-in for the
NeOn toolkit15 that supports the semi-automatic development of
LexInfo lexicons. Further, we also describe our observations from
initial experiments in which several people had the task of creating LexInfo lexica. Finally, we briefly discuss the application of
LexInfo in ontology-based question answering systems as well as
in search applications which operate across languages, discussing
in particular the CLOVA system and architecture.

5.1. The LexInfo API

5.1.1. Interfaces and aggregates

The LexInfo Application Programming Interface (API) has been
designed to be closely aligned with the LexInfo ontology, such
that there are Java classesinterfaces and implementations (with
appropriate methods)for all the classes defined in the ontology.
One important design choice of the LexInfo API is to have three
different representation levels: (i) interfaces, (ii) implementations
and (iii) so called aggregates.

For example, in order to represent subcategorization frames
for transitive verbs, there is one interface (Transitive.java) corresponding to the class Transitive in the LexInfo ontology, an
implementation thereof (TransitiveImpl.java) as well as a so called
aggregate (TransitiveAggregate.java). Aggregates are introduced to
facilitate the usage, querying and manipulation of lexicon entries
and provide all relevant information for each lexical entry in
one object, accessible through appropriate methods. In this sense
aggregates encapsulate from the user all individuals necessary to
represent a particular lexical entry and make the manipulation of
lexical entries easier by hiding all the modeling details. The current
version of the API includes the following aggregates:
 NounAggregate: representing a single noun mapping to a class
or individual in the ontology.
 NounPhraseAggregate: as above, but representing a complex
noun phrase.
 ProperNounAggregate, ProperNounPhraseAggregate: representing named entities which are interpreted as individuals or
classes with respect to the ontology.
 VerbAggregate, VerbPhraseAggregate: representing a verb or
verb phrase.
 ClassAdjectiveAggregate,
ScalarAdjectiveAggregate,
LiteralAdjectiveAggregate: representing an adjective of the
corresponding type.
 NounPPAggregate, NounPhrase NounPPAggregate: representing a noun which subcategorizes a prepositional object and is thus
interpreted as a property with respect to the ontology.
 TransitiveAggregate, VerbPhrase TransitiveAggregate,
IntransitivePPAggregate,
VerbPhrase IntransitivePP
Aggregate: a verb subcategorizing a corresponding number and
type of arguments.

Each of these aggregates are implemented as a class in the
LexInfo API with a number of methods that make it easier to

15 http://neon-toolkit.org.

access the individuals that constitute this element. For example,
NounPPAggregate has methods such as getNoun(), getPreposi-
tion, getNounPP() (the subcategorization frame), getSubject()
(the syntactic argument), getRange() (the semantic argument),
etc. There is also a special aggregate called LexiconAggregate
which includes all aggregates which are associated with a single
Lexicon for a specific language.

5.1.2. Default generation of lexicons using LILAC

In order to enable the extraction of a LexInfo lexicon from a
given ontology, we provide a method which can be used to automatically generate a lexicon from an existing ontology. This works
by using either the rdfs:label annotation, the fragment of the
URI in the domain ontology or, if that is not available, the local
name of the URI. We then apply a simple tokenization procedure (e.g., for http://www.mygrid.org.uk/ontology#has identifier
we extract has identifier and for http://dbpedia.org/resource/
Shibuya%2C Tokyo we extract Shibuya, Tokyo). Once we have the
label in text form, we apply a POS tagger to obtain part-of-speech
information for each token.

We have developed a rule language called LILAC (LexInfo Label
Analysis and Construction) to process the labels. The LILAC rules
essentially define a phrase structure grammar which is used to
parse labels (after they have been tokenized). Phrase structures for
labels are then used to construct corresponding aggregates. The
following LILAC rule for example states that if a label consists of a
noun followed by a preposition (e.g., capitalOf), it will give raise
to a NounPP aggregate in the lexicon:
NounPP: <noun> <preposition>

When the above rule matches (e.g., on the label capitalOf), it creates a NounPPAggregate as well as the appropriate lexical entries
and uses the setNoun(...) and setPreposition(...) methods
of the aggregate. However, this is not sufficient as it does not capture the mapping between the syntactic and semantic arguments.
The LILAC rules also support the explicit specification of this map-
ping. The following rule matches on labels consisting of a verb. The
rule creates a TransitiveAggregate and assumes that the subject
of the verb is interpreted as the domain and the object as the range:
Semantic form specification (SFS): <verb> { subject -> domain,

object -> range }
The above rule would for example match the label produce.
The mapping can also be reversed, for example in the case that

we have a label such as isProducedBy:
Transitive: is <verb> by { subject -> range, object ->

domain }
The above rule would also create a TransitiveAggregate with

reversed mappings.

These annotations allow LILAC to map the syntactic and semantic arguments correctly. The second rule above can be refined by
adding an annotation to match the word form:
Transitive: is <verb> [verbform=participle] by {

subject -> range, object -> domain }
The above cases have assumed that the values of the aggregates
(the head) are always filled by words contained in the label. How-
ever, LILAC is not restricted to this case, allowing us to include
further words when generating the aggregate. The following LILAC
rule which matches a label such as hasAddress illustrates this:
NounPP: has <noun> { subject -> range, pObject -> domain,

preposition -> of }
The above constructs an aggregate of type NounPP for the head
noun < noun> subcategorizing a prepositional object introduced
by the preposition of. This will allow us to interpret constructions
such as The address of X is Y in terms of the property in question.
We support the construction of phrase expressions by allowing
rules to match recursively. For example, for the term structural
alignment report we would generate a noun phrase as in Fig. 12
with the following rules:

Table 2
Results of lexicon generation using LILAC.

Stanford

TreeTagger

MyGrid

SWRC Wine

MyGrid

SWRC Wine

94.6%

98.8%

95.7%

89.0%

93.9%

88.9%

Noun
NounPhrase
ProperNoun
ProperNounPhrase
Verb
ClassAdjective
ScalarAdjective
Adverb

Transitive
VP Transitive
IntransitivePP
VP IntransitivePP
NounPP
NP NounPP

Not recognized
Total (coverage)

NounPhrase: <noun> <NounPhrase>
NounPhrase: <noun> <noun>

Overall, LILAC rules build on the observation of several authors
(see [19,24,34]) that labels in ontologies typically show some reg-
ularities.

We created the LILAC rules using a small test ontology we developed from common ontology labels and then further refined it by
developing extra rules based on the SWRC ontology [45]. We then
tested the rules on two further ontologies: the MyGrid ontology
[44] and the WINE ontology.16 We then developed a set of approximately 100 rules to create aggregates corresponding to the different
types supported by the LexInfo API. These rules can be applied to
generate a default LexInfo model for any ontology. Table 2 shows
results for these three ontologies. Two different taggers were used
before applying the LILAC rules to the labels: the Stanford POS
Tagger [46] and TreeTagger [43]. Table 2 indicates the number of
aggregates per type created for each ontology and part-of-speech
tagger. In the last row (Total) the table indicates the percentage
of labels for which at least one aggregate was created. These first
results are certainly encouraging and show that we can indeed create lexica automatically for any ontology by processing the labels
according to the LILAC rules. While there could be some overfitting here to the three ontologies considered, we are quite confident
after inspection that the rules obtained bear a reasonable degree of
generality.17

5.2. Manually creating and editing LexInfo lexicons

While the creation of LexInfo lexica can be automated as
described in the previous section, we can not expect that automatically generated lexica are sufficient for real applications. First of
all, the analysis and aggregates created will not always be correct.
Second, so far, we do not get any alternatives for expressing classes,
properties, etc. LILAC rules stay actually very close to the label and
are thus not able to capture linguistic variants.

In order to allow people to create new lexica or interact with
already existing lexica (either created automatically or by other
people), we have implemented a plug-in for the Neon toolkit18
which allows people to select an ontology element (individual,
class or property), see the existing lexical entries for it or create
a new lexical entry by selecting an appropriate type and introduc-

Fig. 21. Screen-shot of the NeOn toolkit plug-in.

ing basic information. Fig. 21 shows a screen-shot of the plug-in.
We see that the user has selected the concept mountain from the
Geobase ontology.19 The part of the plug-in labeled with 1 shows
the empty list of lexical entries that have been already modeled
and associated to the concept mountain. The action buttons in
part 2 of the main interface allow the user to (i) delete existing entries, (ii) save the current entry, (iii) create a new entry, (iv)
save the whole lexicon and (v) open an existing lexicon. The third
part of the main interface shows the current lexicon entry the user
is creating/editing. It shows that the user is currently creating a
scalar adjective high associated to the height property of the
class mountain. He has further specified that a mountain with a
height greater than 750 m counts as high and that the adjective
is positively correlated with the scale represented by height. The
plug-in is available at http://code.google.com/p/lexiconmapper/.

We report on the results of a small experiment in which different test subjects created LexInfo lexicons for the Geobase ontology.
In our experiment, we let five test persons model a LexInfo lexicon for the Geobase ontology using the NeOn Toolkit plug-in. All
of these test subjects were computer scientists. Only two of them
had knowledge about ontologies and only one person had knowledge about natural language processing. The session with each of
these subjects lasted for 40 min. In the first 10 min they received
instructions on the LexInfo model and the different lexical entry
types supported. They received some written guidelines for this.
They also received some basic instructions about how to use the
NeOn Toolkit plug-in. In the remaining 30 min, the test subjects
had to fulfill two tasks: creating their own lexicon from scratch as
well as modifying an automatically created default lexicon. None of
the test subjects had any problems understanding the NeOn toolkit
plug-in nor the LexInfo model and they created lexica containing
between 21 and 32 lexical entries (manual mode) and 30 and 44
lexical entries (semi-automatic mode). Fig. 22 shows the average
number of lexical entries created for each type of lexical entry.
We describe how such lexica can be used in question answering
systems below.

16 http://www.w3.org/TR/owl-guide/wine.rdf.
17 Software for generating lexica is available at http://www.lexinfo.net/.
18 http://neon-toolkit.org/wiki/Main Page.

19 Available at http://www.cimiano.org/philipp/home/Data/Orakel/geobase.owl.

P. Cimiano et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 2951

Overview of the modelled subcategorization frames

Transitive

Intransitive+PP

Noun+PP
Subcategorization frames

Class Adjective

Literal Adjective

Scalar Adjective

manually created lexicon

semi-automatically generated lexicon

automatically created lexicon

Fig. 22. Average number of modeled lexical entries.

that LexInfo provides. If we assume a reasonable effort of 2 min
for building one such entry, the amount of work needed for lexicalizing the ontology amounts to a bit more than 2 hours. Then
we used the generator to automatically generate 2785 grammar
entries (LTAG trees together with a semantic representation) from
those LexInfo entries. In addition, we manually specified 149 grammar entries for domain-independent elements such as determiners,
wh-words, auxiliary verbs, and so on. Applied to a gold standard set
of 859 questions and corresponding database queries, our system
achieves a recall of 67% with 82% precision [47]. The main source
of discrepancies between our systems output and the gold standard are due to the context-dependence of lexical elements and to a
still required fine-tuning of the mechanism that translates semantic
representations into queries.

5.3.2. LexInfo for cross-language knowledge retrieval

Data models and knowledge representation formalisms in the
Semantic Web allow us to represent data independently of any natural language. In order to facilitate the interaction of human users
with semantic data, supporting language-based interfaces seems
crucial. As human users speak different languages, it is important
to support the quick development of applications that allow to
access semantic data in multiple languages. However, currently
there is no principled approach supporting the access of semantic
data across multiple languages. To fill this gap, we have designed
an architecture called CLOVA (Cross-Lingual Ontology Visualization
Architecture) designed for querying a semantic repository in multiple languages. A developer of a CLOVA application can define the
search interface independently of any natural language by referring
to ontological relations and classes within a semantic form specification (SFS) which is a declarative and conceptual representation
of the search interface with respect to the ontology, specified in
a proprietary XML format. The search interface can then be auto-

Fig. 23. LTAG tree for the intransitive PP aggregate address of.

5.3. Using LexInfo lexicons

In the following we describe some applications were the LexInfo
model has been deployed successfully. We briefly describe how
LexInfo can be used in ontology-based question answering systems as well as in applications supporting cross-language access
to semantic data.

5.3.1. LexInfo for question answering

LexInfo can be used to partially generate lexicalized grammars
for question answering systems. We have implemented a generator that automatically generates LTAG20 elementary trees from the
lexical entries in the LexInfo lexicon (cf. [48]). These elementary
trees constitute a domain-specific lexicon, which can be used for
parsing and semantic interpretation of questions with respect to an
underlying ontology. For example, an elementary tree generated
for the label addressOf is given in Fig. 23. In this tree, PObject
and Subject refer to the semantic arguments corresponding to
the subcategorization frame in the lexicon. In Fig. 24, we see that
by using the syn-sem argument maps we are capable of linking this
instance to a datatype property, and state that the Subject corresponds to the domain of the property and PObject to the range of
the property.

We applied this approach to an OWL version of Mooneys
Geobase dataset.21 The LexInfo model constructed for this ontology
contains 762 lexical entries. 692 of them correspond to common
nouns representing individuals, which are constructed automati-
cally. The remaining 70 entries were built by hand, using the API

20 Lexicalized Tree Adjoining Grammars [27].
21 http://userweb.cs.utexas.edu/users/ml/geo.html.

Fig. 24. Using LexInfo to select substitution into elementary trees.

Fig. 25. CLOVA general architecture.

matically localized by the use of a lexicon ontology model such as
LexInfo [12], which enables the system to automatically generate
the form in the appropriate language. The queries to the semantic
repository are generated on the basis of the information provided
in the SFS and the results of the query can be localized using the
same method as used for the localization of the search interface.
The CLOVA framework is generic in the sense that it can be quickly
customized to new scenarios, new ontologies and search forms.
Additional languages can be added without changing the actual
application, even at runtime if we desire.

CLOVA is a modular, reusable and extensible architecture implemented in Java which is fully configurable and easy to adapt to
different data sources, user interfaces and localization tools. Fig. 25
depicts the general architecture of CLOVA and its main modules.
The form displayer is a module which translates the semantic form
specification into a displayable format, e.g. HTML. Queries are executed by the query manager and the results are displayed to the user
using the output displayer module. All of the modules use the lexicalizer module to convert the conceptual descriptions (i.e., URIs) to
and from natural language. Each of these modules are implemented
independently and can be exchanged or modified without affecting
the other parts of the system. In what follows we briefly describe
the main components of the system:

Semantic form specification: One of the most important
aspects of CLOVA is the form specification, which consists of a
list of fields which can be queried. Each field is associated to the
URI of a property in the data source, an internal name, a usage
property to determine if it can be included in the output and a
so called property range. The property range defines semantically what values the property should take. Each property range
is defined not by the data type of the range (e.g., integer) but by
the set of queriable values it has. For example we may use an integer range to describe a property such as year founded but for
a property for which it is not practical to be queried through a
single value, e.g., population, we specify the range as a ranged
integer and for a property queried as a fixed set of brackets, e.g.,
age we specify it as a segmented integer. It is important to include
these semantic distinctions in order to choose the right UI elements
for the user to search the data source and to formulate the query
correctly. CLOVA includes a number of property ranges and supports mechanisms for defining new property ranges and including
them in the SFS. This document is in principle similar to the concept of a lens in the Fresnel display vocabulary [8], although
our formalisms support additional information that is crucial
when automatically generating a search interface given a conceptual specification of the relevant properties in the semantic form
specification.

Query manager: Once the form is presented to the user, he or
she can fill the fields and select which properties he or she wishes
to visualize in the results. When the query form is sent to the Query
Manager, it is translated into a specific query for a particular knowledge base. We have provided modules to support the use of SQL
queries using JDBC and SPARQL queries using Sesame [11]. We created an abstract query interface which can be used to specify the
information required in a manner that is easy to convert to the
appropriate query language allowing us to change the knowledge
base, ontology and backend without major problems. The query
also needs to be processed using the lexicalizer, due to the presence
of language dependent terms introduced by the user which need
to be converted to language-independent URIs.

Output displayer: Once the query is evaluated, the results are
processed by the output displayer to determine a proper visualiza-
tion. The displayer has a display element for each different kind of
output to represent, i.e., tables, bar charts, etc., which can be tested
in order to see if it can display the data. The output displayer uses
the lexicalizer to display the appropriate natural language labels in
the same manner as the form displayer.

Lexicalizer: In order to lexicalize ontology URIs in different lan-
guages, CLOVA builds on the LexInfo Model and uses its API. In
particular, CLOVA builds on the LILAC rule language in order to
generate valid textual forms on the basis of the linguistic information contained in LexInfo. In fact, while we have not mentioned
this explicitly so far, LILAC can be also applied in reverse mode to
generate lexicalizations for ontology elements (classes, properties,
etc.) The lexicalization proceeds as follows: for each concept that
needs to be lexicalized, the API is consulted to find the entry that has
a sense with the same URI as the concept. Then, appropriate LILAC
rules to lexicalize the corresponding aggregate are selected. As this
process requires only the URI of the concept, by changing the LexInfo model and providing a reusable set of LILAC rules, the language
of the interface can be changed to any suitable form. This allows to
move flexibly between languages as one has to add only a LexInfo
lexicon for a given language without any further modification of
the system.

Besides being used in the lexicalization of property URIs in different languages, LexInfo can also be used to resolve textual form
queries into appropriate URIs that can be used to query the backend.
For example, if the user queried for food then the LexInfo model
could be queried for all lexical entries that have either a lemma or
word form matching this literal. The corresponding URIs referred
to by this word can be used to query the knowledge base. This
means that a user can query in their own language and expect the
same results, for example the same concept for food processing
will be returned by an English user querying food and a Ger-

P. Cimiano et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 2951

man user querying for Lebensmittel (part of the compound noun
Lebensmittelverarbeitung).

We have realized the CLOVA architecture as a web application
using HTML and JSP by implementing modules to translate the form
specification into HTML. This involves creating a form display element for each property range containing the appropriate HTML
code, and the form displayer, which organizes these into a single
HTML form. Each of these elements depends on the lexicalizer which
converts the ontology URIs to natural language. This form can then
be incorporated into a JSP page and the layout and formatting of
the pages can be easily changed by providing CSS stylesheets.

To demonstrate CLOVA we created a web portal for accessing
information about companies.22 We used a dataset that was a subset of the infobox data extracted from Wikipedia using the Strict
Infobox Ontology for DBPedia 3.4 [1], reduced to the information
relevant for companies. We have chosen this type of data because
it is structured and measure units are normalized, which allows
for more sophisticated querying. Also for most of the URIs there
is a translation of the term provided in DBPedia, extracted from
links between different language versions of Wikipedia, so we can
quickly extract multilingual lexica. We used the LexInfo API to
develop lexica for English, Spanish and German and created a form
using several of the properties commonly used in the ontology.

The bottom line is thus that LexInfo is providing a principled
and declarative model that is used in the lexicalizer component of
the CLOVA architecture to lexicalize ontology elements (proper-
ties in particular) in such a way that all the necessary information
for accomplishing this is contained in the language-specific LexInfo
models only. Thus, we can easily extend the languages covered by
an application by adding an additional LexInfo ontology for the corresponding language without modification of the application code
itself. This would even allow us to add new languages at runtime
if necessary. LexInfo is thus contributing to defining a clear interface between an application supporting cross-language access to
information and the linguistic information needed for this pur-
pose. This leads to very modular architectures where lexica can
be developed independently of an application and even shared
without any implications for the application code using these
models.

6. Conclusions and future work

The interface between language and knowledge as captured by
ontologies is much richer and more complex than can be expressed
by current ontology models, in particular the labeling system of
RDFS, OWL, and SKOS. Enhanced models that couple linguistic
information with ontological structure are certainly required for
tasks such as ontology learning and population from text, natural language generation from ontologies, etc. While there is no
doubt that ad-hoc models for representing linguistic information
might be suitable for individual systems and solutions to these
problems, we have proposed a sound and principled model that
is independent of any specific application (this is what we mean by
declarative) and can be exchanged across systems. For this purpose we have clearly spelled out the requirements for such models
and argued that many related proposals fall short of fulfilling
these.

LexInfo fulfills the requirements stated and clearly details how
the mapping from language to ontologies might work. In order to
design LexInfo, we have built on two previously developed complementary models (LingInfo and LexOnto). In our current proposal we
used the LMF meta-model to glue together the crucial ingredients

22 http://www.sc.cit-ec.uni-bielefeld.de/clova/demo.

of these models. LMF represents a solid and principled framework
for representing computational lexica, of which we regard ontological lexicons as a special case. The LingInfo, LexOnto, LexInfo and
LMF ontologies are available from the project website,23 as well as
a corresponding Java API with implementations for OWL API.

We have further shown how LexInfo can be applied in the
context of two applications with different needs. Concerning question answering applications, we have argued that LexInfo has
the required expressivity to generate lexicalized grammars from
it. LexInfo has also clear applications in cross-language semantic
search applications. There LexInfo can be used to lexicalize both
interfaces and query results to the language of choice of a user. The
lexical information is captured in the LexInfo model only and can
thus be exchanged in a modular fashion, allowing to include LexInfo
lexica created by others and thus creating clear interfaces for the
ontology-language interface. Finally, small experiments have been
conducted to verify that users can indeed understand the model and
are able to create LexInfo lexicons if appropriate GUIs are available.
In future work we intend to further develop the LexInfo API,
extending it with more aggregate types as required by other appli-
cations. We are currently working on an extension of LexInfo to
incorporate (generative) patterns that support the generation of
morphological and inflectional variants such as the plural instead
of storing them explicitly. Our mid-term goal is also to improve
our default lexicon generation mechanisms by including information from linguistic resources (WordNet, CELEX,24 COMLEX25), etc.
as well as corpora such as Wikipedia that can give us information
about frequency of usage of the aggregates in a lexicon. In this line
we intend to develop a corpus-based approach which uses statistical measures defined on the corpus to assess the relevance and
appropriateness of a certain entry for the domain in question. It
is also our goal to extend our automatic lexicon creation methods
to multiple languages. This will require the development of techniques for translating labels much as done in LabelTranslator [17].
It would also be interesting to investigate the relation between LexInfo and Linked Open Data.26 LexInfo provides a formalism that can
also be used to publish lexical information about data resources
which could become itself part of the Web of data. People could
thus create and publish their lexica much as they publish ontologies or data on the Web. This is a fascinating issue to explore which
might allow us to add a multilingual lexicon layer to the Web
of data, thus bringing it closer to the idea of a truly Multilingual
Semantic Web.27 Primarily however, we also hope to have provided a solid basis for any future discussion on standardization
of lexicon models for OWL ontologies. At the time of preparing
the final version of this manuscript, there is a new lexicon (meta-
) model developed by some of the authors in the context of the
Monnet project28 which brings together the LexInfo and the LIR
models. In designing this new modellemon (Lexicon Model for
Ontologies)29we have attempted to provide a very minimal core
vocabulary that can be extended with further modules as required
by a specific application. In addition, lemon is linguistically agnostic
in the sense that it does not prescribe the usage of any specific linguistic categories. Instead, the model offers flexibility by enabling
the definition of instantiations of lemon through the selection of
specific data categories from a registry such as ISOcat [28].

23 http://www.lexinfo.net.
24 http://catalog.elra.info/product info.php?products id=439.
25 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC98L21.
26 http://linkeddata.org/.
27 http://msw.deri.ie.
28 http://www.monnet-project.eu/.
29 http://lexinfo.net/.

Acknowledgements

This work is supported in part by the Science Foundation Ireland
under Grant No. SFI/08/CE/I1380 (Lion-2) as well as by the European
Union under Grant No. 248458 for the Monnet project30 and by the
German Research Foundation (DFG) under the Multipla project31
(grant 38457858). We thank all those that have contributed in the
last years to the development of LexInfo and its API, including
(in alphabetical order): Hammad Afzal, Matthias Mantel, Thomas
Wangler, and Tobias Wunner. Our thanks go also to Christina Unger
for providing the examples related to grammar generation and to
Peter Haase for his contributions to the design of LexInfo and its
precursor LexOnto.
