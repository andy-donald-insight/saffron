Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 113127

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

j o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / w e b s e m

Supporting domain experts to construct conceptual ontologies: A holistic approach
Ronald Denaux a,

, Catherine Dolbear c, Glen Hart b, Vania Dimitrova a, Anthony G. Cohn a

a School of Computing, University of Leeds, Woodhouse Lane, Leeds, LS2 9JT, UK
b Ordnance Survey Research, Romsey Rd., Southampton, SO16 4GU, UK
c Sharp Laboratories of Europe Limited, Edmund Halley Rd., Oxford Science Park, OX4 4GB, UK

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 22 January 2010
Received in revised form 24 September 2010
Accepted 1 February 2011
Available online 30 March 2011

Keywords:
Ontology engineering
Controlled natural language
Intelligent user interfaces
Semantic web usability

A recent trend in ontology engineering research aims at encouraging the active participation of domain
experts in the ontology creation process. Ontology construction methodologies together with appropriate
tools and technologies, such as controlled natural languages, semantic wikis, intelligent user interfaces
and social computing, are being proposed to enable the direct input from domain experts and to minimize the dependency on knowledge engineers at every step of ontology development. The time is ripe
for consolidating methodological and technological advancements to create intuitive ontology engineering
tools which can make Semantic Web technologies usable by a wide variety of people without formal
knowledge engineering skills. A novel, holistic approach to facilitate the involvements of domain experts
in the ontology authoring process is presented here. It integrates (i) an ontology construction methodology,
(ii) the use of a controlled natural language, and (iii) appropriate tool support. The integrated approach is
illustrated with the design, implementation and evaluation of ROO  a unique ontology authoring tool
which combines intelligent techniques to assist domain experts in constructing ontologies. The benefits
and limitations of the proposed approach are analyzed based on user studies with ROO. A broader discussion is provided pointing at issues to be taken into account when assisting the involvement of domain
experts in ontology construction.

O 2011 Elsevier B.V. All rights reserved.

1. Introduction

Public organizations and businesses hold rich data sets whose
large scale integration and sharing can be enabled with Semantic
Web (SW) technologies. This requires ontology-based architec-
tures, and includes an important stage dedicated to the development of ontologies, ranging from small domain ontologies to
large ontologies linked to legacy datasets [1,12,14]. The time and
effort required to create ontological structures is one of the major
reasons for the reluctance of large organizations and businesses to
utilize SW technologies [1,34]. This is aggravated by the fact that
most ontology construction tools are designed to be used by specialists with appropriate knowledge engineering skills but who
may lack the necessary domain expertise to create the relevant
ontologies [20]. Finding knowledge engineers competent in the
specific domain is a luxury; the most common case is to ask domain experts to provide relevant knowledge sources, or apply
knowledge elicitation techniques to discover information directly
from the expert, while knowledge engineers encode the ontology

 Corresponding author. Tel.: +44 7892882419; fax: +44 113343 5457.

E-mail addresses: r.denaux@leeds.ac.uk (R. Denaux), Glen.Hart@ordnancesurvey.
co.uk (G. Hart), V.G.Dimitrova@leeds.ac.uk (V. Dimitrova), A.G.Cohn@leeds.ac.uk
(A.G. Cohn).

1570-8268/$ - see front matter O 2011 Elsevier B.V. All rights reserved.
doi:10.1016/j.websem.2011.02.001

using available SW tools. Apart from creating an extra layer of
bureaucracy in the development cycle [34], this approach can
hinder the ontology construction process and may have a negative
impact on the quality of the resultant ontology (e.g. poor documen-
tation, inconsistency of terminology used, incorrect or incomplete
knowledge constructs). However, if best practice for ontology construction in large organisations which have successfully adopted
SW technologies is exploited, a step change in the wider deployment of the SW may be achieved.

An example of such an approach  drawn upon extensive experience in creating topographic ontologies at Ordnance Survey, the
national mapping agency for Great Britain [29]  is described here.
The Ordnance Survey is developing a semantic reference system
that includes several foundational domain ontologies and empowers the integration of heterogeneous topographic data and their reuse by third parties [14]. At the heart of ontology development at
Ordnance Survey is the active involvement of domain experts (e.g.
geographers, ecologists, emergency planners) [36]. They construct
the conceptual form of an ontology, hereafter called a conceptual
ontology, which records domain knowledge in an abstract way
which is human understandable and independent from the logical
formalism used at machine level. The conceptual ontology is translated to a machine-interpretable logical form, represented with
appropriate logical formalisms, e.g. description logic [24,27]. The

R. Denaux et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 113127

distinct characteristic of
the proposed ontology engineering
approach is that the development of the logical form of the ontology is not just rooted in, but seamlessly integrated with the construction of
the conceptual ontology. This strengthens the
connection between domain expertise and knowledge engineering,
giving domain experts a direct and more prominent role in ontology authoring.

This paper presents a systematic approach for supporting domain experts to construct the conceptual form of an ontology by
incorporating three essential aspects: (i) a tailored methodology,
called Kanga, for involving domain experts in the ontology authoring process; (ii) a controlled natural language (CNL), called Rabbit,
to enable knowledge encoding in a human understandable form;
and (iii) a user-friendly tool, called ROO, to assist the authoring
of a conceptual ontology in Rabbit and to convert it to a logical
form.

Details of Kanga, Rabbit and ROO, presenting how the approach
was derived and describing specific aspects of it have been published in [13,26,28,36]. This journal paper connects all aspects in
a coherent integrated framework which shows how the different
components fit together, forming a holistic approach for involving
and supporting domain experts in ontology authoring. The novelty
of this paper is the presentation of a birds eye view of the ap-
proach, accompanied by a comprehensive description of how the
different components are linked and complement one another;
crucially, this enables us to assess the capabilities and limitations
of the overall approach, as opposed to the evaluations of separate
aspects in previous publications. Furthermore, the paper examines
and reports the application of the holistic approach, and facilitates
its utilization for making SW technologies usable by people without formal knowledge engineering skills.

The work presented in this paper addresses an important problem (i.e. finding intuitive ways for domain experts to engage in
knowledge engineering) for the realization of the SW, and makes
the following contributions:

- proposing a way to complement controlled natural language
interaction with an appropriate ontology methodology to
develop innovative tools for ontology authoring suitable for
users who lack knowledge engineering background;

- illustrating how intelligent support can be provided to facilitate
domain experts involvement in ontology authoring by both
lessening their effort in defining ontology constructs and
improving their understanding of the knowledge engineering
process;

- identifying benefits and limitations of the approach, based on

user studies;

- drawing implications for the design of tools to support the

involvement of domain experts in ontology authoring.

The next section reviews related work and positions our research in the relevant literature. We outline ontology methodologies which stress the importance of involving domain experts in
ontology engineering, review existing approaches and tools to
facilitate this involvement, and compare our work with relevant
controlled natural language-based approaches. Section 3 presents
our approach to facilitate domain experts involvement in ontology
construction. This provides the reader with an overview of the Ordnance Surveys methodology for engaging domain experts in ontology authoring; introduces the Rabbit controlled natural language;
and presents the ROO tool for authoring OWL ontologies using
Rabbit. We then illustrate the overall approach with example user
interactions following scenarios from ontology authoring tasks at
Ordnance Survey. Section 5 discusses the benefits and limitations
of the approach based on a user study in Geographical domains
(Hydrology and Water Pollution), as well as feedback from the

current usage of the ROO tool. We draw the readers attention to
broader aspects related to supporting domain experts involvement in ontology authoring. The paper concludes by summarizing
the main findings of our work and pointing at future research
directions.

2. Related work

2.1. The role of domain experts in ontology construction methodologies

The active involvement of domain experts in the ontology construction process is one of the main goals of the approach presented in this paper. Promoting this involvement has been an
important trend in recent research efforts in Ontology Engineering.
Our work was inspired by two methodologies which suggest possible ways to achieve the involvement of domain experts in ontology engineering.

The DynamOnt project [25] pointed out that existing methodologies did not support domain experts because they lacked appropriate support for communities and collaboration. The project
aimed at producing a methodology where domain experts could
create lightweight ontological models that could be used as part
of an evolving conceptual model. To achieve this, DynamOnt proposed to reuse existing methodologies (Uschold and King [52] and
CommonKADS [43]) while (i) adding guidance for domain experts
so they can act as knowledge engineers; (ii) encouraging collabora-
tion; and (iii) grounding the resultant ontologies using foundational ontologies such as DOLCE [22].

Similar to DynamOnt, the HCOME methodology [35] argued
that traditional ontology construction methodologies such as
METHONTOLOGY [16], Uschold and King [52], On-to-Knowledge
[47], rely too much on the knowledge engineer for development,
maintenance and evolution of ontologies and minimize the role
of the domain experts. The HCOME methodology proposes to support individual domain experts by enabling them to collaborate in
the construction of ontologies with a community of knowledge
workers.

DynamOnt and HCOME have both pointed out limitations of
traditional ontology construction methodologies and suggested
involving domain experts by (i) considering ontology construction
as a joint process involving both domain experts and knowledge
engineers and (ii) providing domain experts with suitable guidance
to ensure their active involvement in ontology authoring. In prac-
tice, it is very difficult to achieve effective domain experts involvement without appropriate tool support. The approach presented
here embarks on this challenge and illustrates a systematic way
to design an ontology authoring tool geared towards domain experts by integrating an appropriate ontology methodology in the
design of the tool.

In the approach described in this paper we use the Kanga methodology for ontology construction. Kanga has been derived empir-
ically, based on experiences at Ordnance Survey when building
several ontologies in the topographical domain. Kanga adds to
the existing ontology methodologies focusing on domain experts
involvement by clearly identifying the assumptions about domain
experts and distinguishing the phases where domain experts or
knowledge engineers should be involved. Most other methodologies also explicitly include the domain expert. Where Kanga differs
is in the emphasis it places on the domain expert and the central
role that the expert plays. Kanga requires the domain expert to
take the lead role, guided by the knowledge engineer but nevertheless in control. So where Kanga differs in not in that it involves the
domain expert where others do not, but rather the degree to which
it involves them. Additionally, Kanga does not sacrifice the expressivity of the resultant ontologies: it describes how domain experts

can be involved in the construction of highly expressive and interconnected ontologies by using a Controlled Natural Language
interface. Thus Kanga has adapted best practice to place greater
emphasis on the domain expert and therefore the novelty in Kanga
results principally from this shift of emphasis. An overview of Kanga is given in Section 3.1.

2.2. Relevant approaches and tools

Several approaches and tools to involve domain experts in the
ontology construction process have been proposed in recent years.
We discuss these approaches here and position our research with
respect to the relevant work.

Ontology Engineering tools that improve collaboration focus on
supporting a community of people (including domain experts) to
build ontologies, e.g. HCONE [35] and Web Protege [50]. These
tools provide communication and Web 2.0 techniques  such as
discussion forums  to aid users to propose, document and implement changes to the ontology. The main advantage of this approach is that it encourages the formation of a community of
both domain experts and knowledge engineers to collaborate in
building the ontology. These tools improve the communication between domain experts and knowledge engineers, which may motivate domain experts to provide more input into the ontology
construction process. However, the means to edit the ontology
are similar to traditional tools, e.g. Protege, which makes domain
experts heavily dependent on knowledge engineers to formalize
the ontologies. Recent studies explore customised interfaces that
domain experts can be comfortable with (predefined forms or Excel sheets) and can be converted into OWL [42,49]. This clearly has
potential for facilitating domain experts involvement in ontology
engineering, however their participation is currently restricted to
discussions and the population of ontologies with specific instances and subclasses, without being directly involved in the
addition of new formal definitions. Hence, the ontology constructs
are actually composed by a group of knowledge engineers (who
may or may not be domain experts), while the domain experts
without knowledge engineering experience mainly provide the
knowledge sources and are involved in the verification of the
ontology. Further experimental studies are needed to examine
the effectiveness and wider applicability of this approach.

Semantic Wikis [23] are extensions allowing the wiki manager to
define a broad ontology structure that corresponds to wiki pages.
Users then refine the ontology by editing and semantically tagging
wiki pages. The wiki interface hides the ontology formalisms from
the users, in this case domain experts, who can add information to
the ontology model by editing wiki pages. Note that to make the
interaction intuitive, an initial ontology needs to be created with
input from both domain experts and knowledge engineers (e.g.
to create semantic forms in Semantic Media Wiki). Semantic wikis
offer a flexible approach for lightweight ontology engineering.
However, they are inappropriate for heavy weight ontology engineering which requires more expressive logical formalisms, such
as description logic and OWL.

Ontology Maturing [2,5] aims to reuse semi-structured data produced by knowledge workers, such as emails, tags and existing
schemas and classifications, to produce lightweight ontologies
and eventually heavyweight ontologies. This approach looks at
ways for users to add formal semantics to existing data one layer
at a time. Proposed tools, e.g. SOBOLEO [5], for extracting a lightweight ontology based on a set of tags or existing schema provide
intuitive ways for domain experts to encode their knowledge of the
existing data. However, this work is still in progress and more research is required to allow domain experts to insert more complex
relations following the maturing approach. Further experimental

studies are needed to examine how people without knowledge
engineering skills contribute to the ontology maturing process.

Ontology Understanding aims to make it easier to understand
what type of knowledge is represented by a particular ontology,
for example, by extracting the main concepts in an ontology [41]
or by showing relevant metadata [30]. Ontology Visualizations are
also commonly used to show and gain insights into the structure
of ontologies and linked data [40] and to provide visual interfaces
for editing ontologies [37]. Domain experts can benefit from these
approaches by getting a high-level understanding of existing ontologies that they can reuse or extend. However, the reuse of extension of the ontologies requires domain experts to be able to
understand and edit the ontologies at the axiom level, which requires logical modeling skills lacking in many domain experts.

Recently, the use of Controlled Natural Language (CNL) interfaces
[19] to perform ontology engineering has been explored. Because
the approach described here falls in this category, we will review
related tools and CNLs in more detail in the next subsection.

2.3. Controlled natural language tools

A controlled natural language is a subset of natural language
that can be accurately and efficiently processed by a computer,
but is expressive enough to allow natural usage by non-specialists
[18].

CNLs that can be used to view, create and edit ontologies are:
CLOnE [20], SOS [8], ACE [33], and CPL[7]. CLOnE only supports a
small number of OWL constructs  currently, only lightweight
ontologies can be built using CLOnE. We are not aware of any tool
support for the Sydney OWL Syntax (SOS), although there is some
tool support for PENG [46] that forms the basis for SOS.

ACE is the most mature CNL having originally been created to
translate into First Order Logic. A subset of ACE is now used to drive
the ACE View application [32] where the resulting sentences are
translated into OWL and SWRL. ACE View was developed at the same
time as the tool described in this paper. Although ACE View provides
a CNL interface, it still requires users to have some knowledge
engineering background, since there is no guidance through the
ontology construction process. A comparative evaluation study
showed that a CNL interface alone is not enough to properly support
domain experts who lack knowledge engineering experience [13].
Another mature CNL is CPL developed at Boeing and used by the
HALO project [3,7]. HALO improved over other CNL approaches by
providing a more holistic approach: the CNL is provided in conjunction with support of the ontology construction process and
not just as a standalone tool for entering knowledge into the sys-
tem. HALO focuses on query answering and uses a wide variety
of techniques such as information extraction to build the ontology.
As a result, it is difficult to identify to what extent the CNL-based
interaction in HALO facilitates domain experts to abstract the domain and formulate their knowledge in a logical form. Such analysis was possible in our research because the interaction support
was focused around the formulation of ontology constructs in a
CNL.

CNL interfaces have also been proposed for query answering
[3,4,6,48]. These languages make it easier for domain experts to
evaluate an ontology, as they can pose questions that need to be
answered based on the ontology and a set of instances. These languages cannot be used to enter new knowledge into the ontology.
They are mainly used for ontology validation, not ontology con-
struction, which is the focus of the approach presented here.

In our approach, we use Rabbit [28], which has been designed
based on extensive experimental work with domain experts and
focuses on usability. Rabbit aims to resemble English in order to
feel natural for domain experts and enable them to compose
and read Rabbit sentences. At the same time, Rabbit has an

R. Denaux et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 113127

Table 1
Comparison of ROO with existing ontology construction tools.

Collaborative tools

Semantic wikis

Ontology maturing tools

CNL-based tools

No

Yes

No
Formalisation (only for heavyweight
ontologies) and validation
All, but currently more suitable for
lightweight ontologies
Low

No
Validation All stages
(for guidance)
All

Low

No

No
Validation

All

Low

parsed structure to help the user recognize correct sentence patterns [44]; provide a flexible way to parse English sentences using
robust language technologies [20]; automatically translate to OWL
[8,20,33]; use templates to facilitate the knowledge entering process [39]; maintain a text-based glossary describing parsed concepts and relationships [44]; and distribute the CNL tool as a
Protege plug-in [32]. Besides integrating these features, our CNL
tools improves on existing tools by providing intuitive error messages designed to be understood by domain experts building an
ontology (see Section 3.3).

3. Supporting domain experts involvement in ontology
construction

Our main goal is to facilitate the involvement of domain experts
in ontology authoring. We define domain experts as people who
have highly specialized knowledge in a particular domain. They
typically do not have any knowledge engineering experience and
are unwilling to learn technical details about OWL or RDF. They
are not trained in logic formalisms and do not have background
in description logics or programming languages. They are used
for using high level concepts and relationships to make sense of
their domains, and have a vocabulary that they use to communicate with colleagues. This vocabulary is usually informal and
may contain ambiguities or it may not be shared across a large
population of domain experts (not standardized). We assume domain experts are used to using popular software packages such
as web browsers, office packages and more specialist software
packages specific for their own domains, and will be willing to
learn a new software package as long as it is intuitive to use and
does not require extensive training.

In order to enable domain experts to build ontologies with a
training, we propose a holistic approach that

minimum of
combines:

- a methodology that puts the domain expert in the leading role

of the ontology construction process;

- a CNL to make knowledge formalization more intuitive;
- a tool tailored to domain experts that offers guidance through

the methodology and the input of CNL constructs.

This section presents our approach outlining the ontology
methodology (Section 3.1), controlled natural
language (Sec-
tion 3.2), and tool to support domain experts involvement in
ontology editing (Section 3.3).

3.1. Kanga: a methodology for engaging domain experts in ontology
construction

The Ordnance Survey has developed Kanga, a methodology for
authoring ontologies [36] that combines two aspects. The first is

Ontology engineering
tools

Traditional
tools

Ontology engineering

experience required

OWL knowledge required
Stages requiring

knowledge engineer

Supported ontology

expressivity

Yes

Yes
All

All

Yes (at least one
team member)
Yes
All

All

No (but initial
ontology required)
No
Creation of initial
ontology
Lightweight

Learning curve for domain

Very high

High

Low

experts

expressiveness comparable with OWL 2, as there is a one-to-one
correspondence between Rabbit sentences and OWL 2 constructs.
This allows for easy conversion and limits the number of allowed
Rabbit constructs, making it easier to learn. We refer the reader
to [45] for a detailed comparison between ACE, SOS and Rabbit
for OWL editing.

2.4. Design decisions based on related work

Table 1 shows an overview of the major approaches to involve
domain experts in the construction of ontologies. The work presented in this paper promulgates an approach where tool support
enables domain experts, with no knowledge engineering experience and without the direct help of knowledge engineers, to build
both lightweight and heavyweight ontologies. The table shows that
CNL-driven and ontology maturing approaches are good candidates as they do not require ontology engineering experience and
are easy to learn. The CNL-driven approach has the added advantage that the domain expert can produce ontologies that use complex OWL constructs, given appropriate tool support.

Our research intentionally avoids the collaborative aspects of
building ontologies (present in the collaborative and semantic wiki
approaches in Table 1) in order to learn how domain experts can
create ontologies that represent their knowledge of their domain.
Note that the CNL-driven approach presented in this paper is compatible with and could improve collaborative approaches as it allows communities to be less dependent on knowledge engineers
and to produce more complex ontologies if necessary.

The approach presented in this paper improves on other mature
CNL-driven approaches and tools which had the following negative
usability issues: requiring some familiarity with the logical representation used [17,19]; assuming that the user has knowledge engineering skills to complete the ontology authoring process (all tools
suffered from this to an extent); distracting frequently the user from
the main task to enter basic lexicon entries [44]; lack of immediate
feedback and error messages [17,20,44]; requiring installation of
external libraries and appropriate user customization [20].

Our approach is distinguished from existing CNL approaches for
ontology engineering, and makes a contribution to this research, by
providing intelligent support to assist domain experts who may lack
ontology engineering skills to engage in the ontology development
process. Such support, which is based on an ontology construction
methodology developed by the Ordnance Survey (this methodology is outlined in Section 3.1), is a vital improvement as it enables
the domain experts to assume ownership of the ontology construction (from defining the requirements, to documenting and formalising the ontology), instead of merely providing knowledge sources
and verifying the final product.

Building on previous work in CNL, we have developed a novel
tool that overcomes key usability limitations by integrating several
features: a simple version of look ahead to provide suggestions by
guessing what constructs the users might enter [46]; show the

the human-readable or conceptual aspect, which is written by one
or more experts in the relevant domain. The second component is
the computer-parsable or logical aspect, which is created by manual or automatic conversion of the conceptual ontology into a SW
language such as OWL. Kanga is centered on the domain expert,
who, as the human source of the knowledge, should retain control
of the ontology and the authoring process. The authors do not
claim that Kanga introduces much new in terms of methodological
steps or techniques. This was not the point of developing Kanga,
rather it was to emphasise the role of the domain expert and the
central position that this person is given. Hence the development
of Rabbit, which itself is designed to make ontology development
and interpretation more accessible to the domain expert. Indeed
with simple modifications there is no reason to suppose that
ROO could not be adapted to other methodologies, allowing ontologies to be expressed in Rabbit but following a different methodology such as METHONTOLOGY [16].

Using Kanga, ontology engineering is conducted in several iter-

ative steps (see Fig. 1):

- The scope, purpose and other requirements of the ontology are

identified.

- Source knowledge and documents are gathered,

including

ontologies identified for reuse.

- The ontology content is captured in a knowledge glossary.
- Core concepts and relationships between concepts are formally
defined using structured English sentences and then converted
into OWL.

- The ontology constructs are verified and validated.

The first three steps are performed by domain experts, without
involvement of knowledge engineers. The conversion from structured English sentences into OWL in the fourth step and the validation of the ontology of step 5 requires formal knowledge of OWL
and ontology engineering and are usually performed by knowledge
engineers with some assistance from domain experts.

The knowledge glossary consists of lists of core and secondary
concepts, along with relationships and any instances of concepts
necessary to describe the core concepts. Each concept, relationship
and instance is supplied with a natural language description, the
source of which is documented. Core concepts are ones which
are considered to be central to the domain, for example, in a
Hydrology ontology, the concept Waterfall will be considered a
core concept, and will be defined using structured English sentences (as described in Section 3.2). Secondary concepts, for example Cliff in a Hydrology ontology, will only be included in the
ontology in order to describe core concepts, and will not them-

selves be further elaborated on  that is, they are not described
with sentences of their own.

The Ordnance Survey methodology was developed through the
process of authoring two fairly large (approximately 600 concepts
each) and expressive ALCOQ ontologies within topography,
namely Hydrology and Buildings and Places,1 constructed with
the active involvement of domain experts from Ordnance Survey.
Since domain experts did not have knowledge engineering skills,
using the then available ontology construction tools was not appro-
priate. Consequently, the knowledge glossary, which included lists of
concepts and relationships with corresponding textual descriptions
from knowledge sources and structured sentence to define relationships between concepts, was stored in a spreadsheet. The structured
sentences were then manually converted to OWL by a team of
knowledge engineers. This process is now automated using the controlled natural language and the tool described next.

3.2. Rabbit: a controlled natural language for defining ontology
constructs

In response to a need for domain experts to be able to understand and define ontological constructs, a CNL was developed
[26]. Rabbit is designed to be easy for domain experts to understand and produce, allowing them to express what they need to
in order to describe their domain. We have named this language
Rabbit, after Rabbit in Winnie the Pooh, who was actually cleverer
than Owl. To this end, we have involved domain experts from
the outset in the core language design decisions, and modified
our original design based both on the experiences we have had
with domain experts building topographic ontologies in Rabbit,
which are later converted to OWL, and also on experimental results
of testing user understanding of Rabbit sentences.

The main principles underlying the design of Rabbit are:

- To allow the domain expert to express their knowledge as easily
and simply as possible and in as much detail as necessary. It is
designed to appear like writing statements in natural English.
- To have a well defined grammar and be sufficiently formal to
enable those aspects that can be expressed as OWL to be systematically translatable.

- To recognize that the domain expert alone cannot produce an

ontology and that a knowledge engineer is also necessary.

- To be used in conjunction with tools which help to enforce an
authoring method but not to the point where Rabbit is only
readable through tools.

- To be domain independent.

Table 2 gives an overview of some of the main sentence struc-
tures, although Rabbit covers most constructs in OWL 2 [26].2 Ordnance Survey has performed user tests to investigate Rabbit
sentence comprehension and ease of authoring without tool support
[15,28].

3.3. ROO: a tool to augment the ontology construction process

ROO (Rabbit to OWL Ontology authoring) is a novel ontology
construction tool that has been designed specifically to support
the involvement of domain experts in the ontology construction
process. Hence, the target users are domain experts who have limited or no previous experience in ontology building. However, ROO
can also be used by knowledge engineers. ROO compensates for a

Fig. 1. The phases of the Kanga Methodology [36]. The white boxes indicate the
phases performed by domain experts. The formal structuring is done by domain
experts using Rabbit, while the translation to OWL is performed by knowledge
engineers who also complete the last stage  evaluation and verification.

1 Available at http://www.ordnancesurvey.co.uk/ontology.
2 See also http://sourceforge.net/apps/mediawiki/confluence/index.php?title=
Rabbit_Language_Overview for an introduction to the Rabbit language and examples
of all its constructs.

R. Denaux et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 113127

Table 2
Some frequently used Rabbit sentence structures, based on examples from the Ordnance Survey hydrology ontology.

Description

Universal and
Existential
Quantifiers

Defined
Class

Subjunction

Instance
Declaration

Qualified
Cardinality
Restriction

Rabbit

Every River only flows into a Sea.

A Source is anything that:

is a kind of Spring or Wetland;
feeds a River or a Stream.

Every Cataract is a kind of Waterfall.

Somerset is a County.

Every Channel has exactly 2 Banks.

Manchester Syntax

Class River:
SubClassOf:

flowsInto some Sea
and flowsInto only Sea

Class Source:
EquivalentTo:

Spring or Wetland
and feeds some (River or Stream)

Class: Cataract
SubClassOf: Waterfall

Individual somerset:
Types: County

Class Channel:
SubClassOf:

hasBank exactly 2 Bank

lack of knowledge about ontology construction, ontology languages and formal logics, by providing users with appropriate sup-
port, as specified below.

ROO guides the user following an appropriate ontology construction methodology by providing a user interface that reflects the
phases of the Kanga methodology where each tab corresponds to
a phase in the methodology. For example, when a user creates a
new ontology or opens an existing ontology, the first tab is for
Purpose and Scope. The interface components encourage entering an annotation for the ontology purpose and a different annotation for the ontology scope. The annotation URIs have already been
defined for the user as they are not expected to learn the OWL
annotation system. This has the advantage that the encoding of
the purpose is standardized by the tool, so it is easy to check
whether an ontology has defined its purpose and scope. This is
used by ROO s Guide Dog, a component which contains an internal
model of the Kanga methodology to check the progress of the user
in building the ontology. The user can ask the guide dog for advice
regarding building the ontology. When this happens, the guide dog
inspects the state of the ontology to determine the current phase in
the Kanga methodology and suggests a task to the user that is
appropriate for the current phase. The following types of tasks
are suggested: scope and purpose definition, knowledge source
definition, declaration of concepts and relationships (OWL enti-
ties), free text definition of OWL entities, CNL definition of OWL
entities. Examples of Guide Dog suggestions are given in Section 4.
ROO uses a terminology that encourages the creation of a conceptual model and avoids terminology that is OWL specific. ROOs interface shows lists of concepts and relationships as opposed to
Proteges and OWLs terminology of classes, instances and proper-
ties. The rationale behind this is that domain experts are not familiar with OWL and should not be required to learn its terminology.
We assume that the terminology of concepts and relationships is
easily understood by domain experts as they already conceptualize
their domains in order to communicate with colleagues.

ROO provides an intuitive way to describe relevant concepts and
relationships of the domain by using a CNL interface. We have
implemented a parser for the Rabbit CNL, as well as an editor that
provides support for writing correct Rabbit sentences by (i) using
syntax highlighting to show keywords, concepts and relationships;
(ii) providing help files to introduce the Rabbit language, users can
learn about Rabbit by learning about the meaning of the Rabbit
keywords or by seeing examples of Rabbit sentences; (iii) providing a list of Rabbit sentence patterns; (iv) showing invalid Rabbit
sentence error messages explaining errors in terms of missing concepts and relations; (v) detecting ambiguity and showing the user
possible alternatives.

3.4. Technical overview of ROO

ROO is based on Protege 4. Protege 4 is built around a plug-in
architecture based on the OSGiTM dynamic module system for Java3
and can be used for viewing and editing OWL ontologies. Due to the
plug-in architecture of Protege, ROO takes advantage of the available
third party plug-ins (e.g. for ontology visualisation, reasoning, verifi-
cation). This extends the functionality and makes ROO attractive not
only for domain experts (who use ROO features to author the conceptual ontology) but also for knowledge engineers (who benefit
from ROOs features for creating/changing the conceptual ontology
but also have the extensive power of Protege tools for verifying/
altering the ontology generated by ROO).

ROOs user interface reuses some components provided by
Protege, but simplifies the terminology and hides options that require a deep understanding of OWL by choosing default values.
For example, when adding an annotation in Protege 4, the user
has to specify an annotation type, an XML type and a language in
order to enter the annotation. In ROO, we reuse the same GUI component as Protege 4, but we only allow users to use XML String as
the XML type, en as the language code and we choose the annotation type depending on the type of information that the user is
entering (e.g. Natural language definition of a concept or scope of
the ontology).

Note that all the plug-ins that work with Protege can also be
used in ROO, but may be hidden in the default configuration, i.e.
a knowledge engineer can configure ROO to behave in the same
way as Protege. ROO also inherits Proteges functionality to read,
edit and write OWL ontologies using the OWL API. This is used
for converting parsed Rabbit sentences to OWL and to compare
the state of the ontology with the Kanga methodology.

The Guide Dog functionality in ROO is driven by a rule-based
task planner implemented in JBoss Drools. The task planner contains rules for determining when a set of ontology construction
tasks should be carried out according to the Kanga methodology.
Each rule has the form when LHS then RHS, where LHS is a
condition expressed in terms of the ontology being constructed.
Typical conditions are: whether the ontology contains a specific
annotation(e.g. scope annotation); whether the ontology defines
more than a specific number of OWL classes or whether an
OWL entity in the ontology contains a specific annotation (e.g.
related_rabbit_sentence annotation). These LHS conditions
are checked using the OWLAPI to inspect the Ontology, its axioms
and its annotations. The RHS is only triggered when the LHS condi-

3 http://www.osgi.org/.

tion is met. The RHS adds a task to a list of tasks that the Guide Dog
will suggest to the user. As an example, the prescription made by
Kanga that: Users should enter a natural language description
for each concept in the glossary is encoded in the following rule:

rule Enter free-text definition for

Entity X

when

IOntologyWrapper(
hasScope = true,
hasPurpose = true,
numberOfKnowledgeSources > 0)

ew: IOWLEntityWrapper(
hasFreeTextDef = false,
numOfSent:numberOfRabbitSentences)

then

ntc.add(NextTaskSuggestionType.

EnterFreeTextDefinitionForAEntity,
ew);

end

When a user invokes the Guide Dog, all the rules are triggered,
populating the Guide Dog with a list of tasks that can be presented
to the user. Before presenting the tasks to the user, the Guide Dog
sorts the tasks to give priority to tasks that are related to the currently selected OWL Entity.

The Rabbit Language Processor is implemented using GATE4 and
is inspired by the parsing in CLOnE [20]. During parsing, we use
GATE to perform natural language processing tasks such as tokeniz-
ing, sentence splitting and part-of-speech tagging. The NLP processing is vital in order to allow users to write Rabbit sentences that feel
natural because it allows us to recognise that strings with different
morphologies refer to the same OWL entity (e.g.
has part and
have parts). We use the NLP annotations to find the structures defined by the Rabbit grammar using JAPE5 transducers, resulting in a
Rabbit AST(abstract syntax tree). Finally, the Rabbit AST can be used
to give appropriate feedback to the users in case of errors or warnings and it can be used to generate OWL axioms and annotations that
can be added to the ontology. A more detailed description of the
implementation of the Rabbit parser is given in [11].

4. Example interaction with ROO

This section shows a typical user interaction with ROO, illustrating how the combination of the Kanga methodology, Rabbit CNL
and the tool support provided by ROO assist a domain expert to
construct OWL ontologies. We also show how the resultant ontology benefits an experienced user, due to the good readability and
provenance support provided by Kanga and ROO. The interaction
described below combines steps performed by different domain
experts as observed during our evaluation studies (see Section 5),
although we have changed or ommited details in order to give a
narrative that reflects a typical interaction with ROO.6

Hayley is an expert in water pollution at the UK Environment
Agency. She is involved in a team examining the location and likely
impact of pollution events in different parts of the river network in
the UK. Hayley has to describe her knowledge of river networks
and the flow of water in an ontology that can help in pollution
assessment tasks. She has not come across SW technologies before,
is not familiar with ontologies, and has not been previously

4 http://gate.ac.uk.
5 Java Annotation Pattern Engine which provides a language for finding annotation
patterns during the parsing process. It also provides hooks for invoking Java methods
during the parsing of a text. See http://gate.ac.uk/sale/tao/index.html#jape.

6 See the discussion of the benefits and limitations in Section 5 to learn more about

issues not included in this interaction description.

involved in any knowledge engineering tasks. Although Hayley
has extensive domain knowledge, she has no idea how to encode
this knowledge in a computer processable form. Hayley has been
advised to use ROO because the tool can assist her in defining
and documenting the knowledge required for her impact-assess-
ment task, which can in turn help with automating her work and
will enable other organizations to use the same knowledge.

4.1. Creating a new ontology with ROO

Hayley opens ROO7 and is first presented with the ROO welcome
screen. Since Hayley does not know anything about ROO she clicks
on Show me an introduction to ROO which runs a video about
ROO, the user interface and the steps required for building an ontology.
The video also introduces the Guide Dog, which Hayley can use
whenever she is stuck and needs a suggestion about what to do next.
Hayley then starts building her ontology. The first tab in ROO is for
defining the ontology purpose and scope; Hayley learns from the help
documentation what she is supposed to enter here. She enters:

Purpose: To be used for answering questions on
location and likely impact of pollution events in
different part of the river network.
Scope: Describes water pollution in the context of
river networks. Water pollution in oceans and
underground water is not covered.

4.2. Defining knowledge sources

Hayley then selects the next tab in the user interface, which
takes her to the second step in the Kanga methodologygathering
source knowledge and relevant documents. Here, Hayley specifies
sources that provide definitions of general water pollution concepts which she intends to included in the ontology.8 Hayley also
enters her name as a knowledge source since she intends to provide
her own definitions of some specific concepts. Later on, whenever
Hayley needs to refer to more sources, she adds them by coming
back to the Knowledge Sources tab. She will be asked to specify
the corresponding knowledge source every time she enters a natural
language description of an entity. This provides important meta-data
which can be used by other ontology authors for changing/verifying
the conceptual ontology. Hayley becomes aware that specifying
knowledge
step during ontology
construction.

an important

sources

is

4.3. Adding concepts and relationships

Hayley now switches to the Concepts and Relations tab. She
consults the Guide Dog, which suggests to add concepts to the
ontology, prompting Hayley to click on the button to add a new
concept. She sees a dialog with the sentence: A new concept is
a concept. Where A new concept has already been selected.
She types the name of the concept Organophosphate and sees
that the concept is highlighted and that she can click on the
OK button. When she does this, Hayley sees that the concept
Organophosphate now appears on the list of concepts. The Guide
Dog now suggests an ontology construction task that Hayley has
not seen before: to provide a natural language text to describe

7 The current release of ROO (version 1.0.1) is easily installed from a zip file. To run

the tool, the only requirement is that Java 5 is previously installed.

8 For instance, DEFRA (Department for Environment and Rural Affairs) Water
Quality: http://www.defra.gov.uk/environment/water/quality/; Environment Agency
UK Water Quality: http://www.environment-agency.gov.uk/subjects/waterquality/;
UK Rivers Water Pollution in: http://www.ukrivers.net/waterpollution.html; UK
Government Pesticides Safety Directorate Glossary: http://www.pesticides.gov.uk/
appendices.asp?id=744.

R. Denaux et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 113127

the concept Organophosphate. Hayley does this by taking a definition from the UK Government Pesticide Safety Directorate which
she specified as a knowledge resource. At this point the natural
language description of the concept has been filled in.

4.3.1. Describing concepts using Rabbit

After entering more concepts, the Guide Dog starts suggesting a
new task to Hayley: concepts need to be defined using Rabbit sen-
tences. She selects Water Pollution as the concept to define and
sees that it is missing a natural language description. By now, Hayley has learned that this is a step in the ontology creation process,
thus she enters her personal definition (i.e. she indicates herself as
the knowledge source): Water pollution is the contamination of
bodies of water as a result of human activities. Water is polluted when
it can no longer be used for its intended use..9 Because Hayley is not
familiar with Rabbit, she enters the same text in the Rabbit editor.
This results in error messages telling Hayley that the text is not a valid Rabbit sentence and that she should read the documentation
about Rabbit to find a suitable sentence. In the documentation she
sees several examples of valid sentences and Hayley is instructed
to break down long definitions into multiple simple sentences. Using
example sentences, Hayley enters a simplified statement:

Water pollution contaminates bodies of water.

This time, the parser recognises a valid Rabbit sentence and

shows the message:

CONCEPT bodies of water has not been added to the
ontology yet.

INSTANCE water pollution has not been added to the
ontology yet.

RELATIONSHIP contaminates has not been added to
the ontology yet.

Using the templates offered in ROO for adding new concepts

and relationships, Hayley enters:

Bodies of water is a concept.

Contaminates is a relationship.

Hayley is surprised by the suggestion that Water Pollution is
recognised as an instance. She checks the existing Rabbit patterns
and realises that there is a missing word Every at the beginning of
the sentence. She then enters:

Every water pollution contaminates bodies of water.

The sentence is accepted by the parser, and she is allowed to enter it (the OK button for entering the Rabbit construct is allowed).
Hayley moves to the additional part of her natural language sen-
tence. She follows the same Rabbit pattern and enters:

Every water pollution is result of human activities.

The parser recognises the Rabbit construct  Water Pollution
and Human Activities are colored in blue, and is result of is colored in green. Hayley is given a message that the concept Human
Activities and the relationship is result of are not in the ontology yet. She enters Human Activity. When entering the relationship is result of, Hayley realises that she may need to use this
relationship in other sentences and decides to enter is caused by
instead.

As a result of this interaction, Hayley has entered the following
Rabbit sentences corresponding to the first sentence in her natural
language description:

9 This is part of the natural language definition of water pollution given by a

participant in our comparative user study.

Every Water Pollution contaminates Bodies of Water.

Every Water Pollution is caused by Human Activities.

Hayley has more difficulty translating the second sentence of
her natural language description as it contains a self reference
(i.e. its intended use). She solves this by introducing the concept
Contaminated Water. Following the previous Rabbit sentences,
she enters:

Every Water Pollution results in Contaminated Water.

4.3.2. Refining concept definitions

As Hayley defines more concepts (and as she shares her ontology and gets feedback from other people), she gets used to the Rab-
bit
language and starts exploring and using more complex
constructs. She revisits her definition of Water Pollution as
shown in Fig. 3 in order to introduce an equivalence relation. The
Rabbit editor displays Water Pollutant in blue to show that this
is a concept, but underlines the concept using squiggly lines. The
editor also shows a message stating that the concept Water Pollutant is not defined in the ontology. Hayley realizes that she
needs to add concept Water Pollutant and writes the sentence:
Water Pollutant is a concept. This adds the new concept and
the previous error message disappears. Similarly, she adds the relationship causes problems for and accepts the sentence defining
Water Pollution. The appropriate knowledge is encoded and the
new concept and relationship are now added to the glossary.

In a similar way, Hayley enters additional sentences to encode
the knowledge in the textual descriptions of Organophosphate
and Water Pollutant. The Guide Dog reminds Hayley that she
introduced concepts such as Contaminated Water, but she has
not given natural language or Rabbit definitions for that concept.
An example of the ROO interface showing part of the ontology defined by Hayley is given in Fig. 2.

When Hayley exits ROO, she is prompted to save her work. ROO
saves the ontology as an OWL file that can be shared with others
using ROO or any other OWL editor such as Protege. The Rabbit
sentences are stored as annotations which can be presented to
other domain experts for inspection.

As Hayley continues working on her ontology she learns the
capabilities and limitations of Rabbit and is able to write complex
definitions with advanced constructs such as disjoint classes, concept negation, equivalence relations and nested definitions, for
example:

A Rill is anything that:

flows in exactly one Gully;
has part exactly one Gully

that contains Water;

has Linear Form.

Rill and Pools are mutually exclusive.
No Rill has a Circular Current.

4.4. Ontology reuse

The ontology built by Hayley can be reused and extended by
more advanced users. Anjit is an expert at Ordnance Survey
specializing in geographical information systems. He has some
experience of ontologies and SW technologies, having used Protege
to author an ontology to describe his organizations knowledge of
Hydrology and how Ordnance Survey describes the locations of
different water features. His current task is to combine Hayleys
Water Pollution ontology with the Ordnance Surveys Hydrology
ontology, to allow Hayley to access the locations of her pollution
sites using Ordnance Survey topographic data. Anjit
is not
familiar with water pollution, and finds the Rabbit sentences and

Fig. 2. Concept and Relationships tab in ROO shows the Water Pollution ontology, the list of concepts and relationships and the natural language description and Rabbit
sentences of the Organophosphate concept. The user has pressed the Guide Dog button, showing that several concepts related to Organophosphate still need a natural
language description. Concept Ocean already has a natural language description and the guide dog reminds the user to enter Rabbit sentences to define the concept.

Fig. 3. Formalizing knowledge by editing Rabbit sentences in ROO. Appropriate feedback and error messages are provided (on the left) to help the user write correct Rabbit
sentences (on the right).

accompanying free text definitions of the concepts and relationships easier to understand than decoding the OWL. He also finds
it helpful that the ontology is documented and indicates the
knowledge sources used. This enables him to look up the original
documents or contact the original authors of the ontology when
he needs to clarify details of the ontology.

Anjit uses ROO in combination with existing Protege plug-ins to
inspect Hayleys ontology. For example, he can use the OWLViz
plug-in to inspect the water resources Hayley has defined. Anjit decides to import concept Ditch from the Hydrology Ontology into
Hayleys Water Pollution ontology. He does this by entering the following Rabbit sentences:

Use ontology: OS Hydrology from
http://example.com/v1/Hydrology.rbt.
Refer to Ditch [OS Hydrology] as Ditch.

performed by users with background in environmental studies
during an experimental study with ROO. The results of this study,
as well as the regular feedback from current users, enabled us to
identify benefits and limitations of our approach.

5. Evaluation: potential and limitations of the approach

This section discusses how well our approach works based on
user experiences with the ROO tool. We will examine the benefits
of added features in ROO to assist domain experts involvement in
ontology authoring, and will point out how these features can be
integrated in relevant approaches. We will also outline open issues
and point at further development.

5.1. User studies with ROO

Both scenarios given above are based on real ontology engineering tasks at Ordnance Survey and one of their clients  the UK Environment Agency. Hayleys interaction was also part of the tasks

Usability studies were conducted during the development of
ROO while new features were still being added. Three small usability studies (with 34 users) were conducted. Music was chosen as

R. Denaux et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 113127

the domain due to the availability of users with subject knowledge.
Users were asked to build an ontology of musical instruments, corresponding to the material in a UK A-level syllabus.10 All participants had studied this specialized level and had played different
musical instruments. The users did not have experience and knowledge in ontology engineering, their computer background varied
from programming to general computing literacy skills. The sessions
were recorded using CamStudio, and a member of the ROO team
would always be present at the first session for a user to observe
how newcomers would start using the tool. This enabled us to tune
the interface and, most importantly, to polish the user guidance and
the support provided with the CNL interface. It also allowed us to elicit interaction patterns, which were further examined in follow up
studies.

The second source of user experience with ROO is a comparative
user study performed to evaluate (i) the suitability of the tool for
domain experts with no ontology engineering background; (ii)
whether users gain any understanding of ontology construction
and knowledge engineering; and (iii) the quality of the resultant
ontologies. The study followed a task-based, between-subjects
experimental methodology, comparing ROO with a baseline sys-
tem. We chose ACE View [32] as the baseline system, as it is a
CNL-based ontology authoring tool that is similar to ROO. ACE
View is a Protege plugin that provides similar features as ROO:
(i) input in a controlled natural language; (ii) error messages for
sentence composition and (iii) automatic translation into OWL.
ACE View does not provide any other type of support to domain experts besides its CNL interface, making it a suitable baseline system
to gather evidence of the impact of ROOs holistic approach compared to a CNL-only interface.11 We did not choose Protege as the
baseline, because the users in our study did not have knowledge
engineering skills (required for interacting with Protege). The reader
is directed to a previous evaluation study [20] which shows advantages of a CNL-based interface compared to Protege.

The participants in the comparative user study were 16 students (at Bachelor, Master, and PhD levels) and 2 staff members
from the departments of Geography and Environmental Studies
at the University of Leeds. The participants had no previous experience with ontologies, knowledge engineering, or computer pro-
gramming. Their direct interaction with ROO and ACE View was
restricted to a single session of one hour where the participants
built an ontology from scratch. The domain of the ontology was
specified in advance depending on the area of expertise (Water
Pollution or Hydrology) and a list of representative concepts had
been specified (e.g. River, Catchment, Sediment). The participants
were asked to bring free-text definitions of these concepts before
the session, in order to maximise the time of interaction with the
tools. Another hour was used to explain the study, gather questionnaire answers, introduce the domain modeling task, introduce the
tool and CNL to be used, and get feedback about the system. The
interaction records from each session, including video, log files
from ROO, the resultant ontologies and pre- and post-experiment
questionnaires were analyzed to find statistically significant trends
that result from using ROO. A detailed description of this user
study is presented in [13].

As the final source of experience with ROO, several staff members and students at the University of Leeds, UK have used ROO
to build ontologies and have provided their feedback about the tool
and the overall approach. Currently, the ROO distribution provided
at sourceforge has more than 700 downloads.12

10 A specialization which can be chosen in UK upper secondary schools.
11 ACE, the CNL used in ACE View was introduced in Section 2.3.
12 The link to download ROO is available on the project web site http://
www.comp.leeds.ac.uk/confluence/.

Based on the user experiences with ROO, we will address three
questions crucial for research and projects focusing on domain experts involvement in ontology engineering:

- What are the benefits and limitations of CNL-based interaction

for ontology authoring?

- What is the quality of the resultant ontologies?
- How can ontology authoring tools support domain experts

involvement in ontology construction?

5.2. Benefits and limitations of CNL-based interaction for ontology
authoring

5.2.1. Efficiency

The main aim of exploiting CNL-based interaction for ontology
authoring is to allow domain experts to produce ontologies without extensive training beforehand. The user studies with ROO
confirmed this. In the comparative user study domain experts
were able to create simple ontologies after about 10 min introduction to the CNL (Rabbit or ACE) and the corresponding authoring tool (ROO or ACE View) [13]. In one hour, the participants
who used ROO produced an average of 21.9 class definitions
and 8.25 object properties. The one hour included time for initialization tasks, such as the definition of the ontology scope and
purpose and the definition of knowledge sources. Time was also
spent for entering the natural language descriptions for the key
domain concepts (usually copying the definition from a document
or a website). The participants who used the baseline system ACE
View produced an average of 28.1 class definitions and 11.9 object properties. Although this productivity metric is higher than
that of ROO users, note that ACE View did not require users to enter scope, purpose, knowledge sources or natural
language
descriptions.

The results give positive evidence that CNL-based interaction
makes it possible to quickly involve domain experts in ontology
construction. The time users spent familiarizing with CNL interfaces (both in ROO and in ACE View) was relatively short. This
can be due to the fact that CNL sentences are intuitive to understand [15]. Furthermore, CNL provides a unified way for defining
all knowledge constructs, such as entering concepts/relationships,
specifying hierarchical links, and formulating axioms.

Our evaluation does not compare CNL versus non-CNL ontology authoring (e.g. direct editing of OWL statements, using
forms, or visual interfaces). Hence, we have no direct evidence
that CNL-based ontology engineering is faster than non-CNL.
Some work in this direction has been done elsewhere, for example [20] reports that a CNL interface is more efficient for performing simple ontology construction tasks than traditional
tools such as Protege.

5.2.2. Abstraction

Defining ontological constructs in a CNL requires some level of
abstraction, albeit that the CNL reduces the cognitive complexity of
this process. Typical sentences for defining a concept in a natural
language tend to be information rich. For instance, a text definition
of Flood Plain given by a participant in our comparative study is
Flood plain is the area of land surrounding a river, which is usually
flat, and is prone to flooding. The domain expert has to learn that
this text is not suitable for the ontology and has to be broken down
into several sentences. In our study, this was explained at the start
of the session when introducing the tool and CNL. However, participants used help material such as example CNL sentences and the
tool feedback (error messages) to explore the CNL and learn the
limitations of the language. For example, a participant broke
the above natural language definition of Flood Plain into the
following Rabbit constructs:

- Every flood plain is a kind of area of land.
- Every flood plain is around a river.
- Every flood plain is prone to flooding.

Our studies showed that the process of breaking down natural
language definitions into CNL sentences is a crucial step when
using a CNL for building ontologies. In most of the cases, the participants performed this breaking correctly after an initial phase
when they learn the restrictions imposed by the CNL and staying
within those limits while describing concepts from the ontology.
All ROO users, as well as the ACE View users in the comparative
study, were able to quickly decide how to rephrase most natural
language definitions.

Although the reduced complexity of abstraction from natural
language to ontological constructs is a key advantage of CNL-based
ontology authoring, it also brings a crucial limitation. Making the
formulation of ontological statements fairly easy can be mislead-
ing. In our studies, the participants without knowledge engineering
background would focus mainly on the formulation of the CNL con-
structs. None of them questioned what was logically entailed by
what they had entered. In contrast, users with previous ontology
engineering experience not only managed to quickly formulate
Rabbit sentences but were more dubious about the exact meaning
in OWL terms. These users would often open the Protege Class
Description View to check the OWL translation of the entered sen-
tences. This points at the need for offering intuitive ways for ontology validation, as discussed below.

5.2.3. Ambiguity

The syntax and semantics of CNLs are usually clearly defined.
However, to a user the CNL-based interaction resembles natural
language interaction which can hide the logical precision and
may introduce ambiguity. During the user studies with ROO, we
observed three types of ambiguity. Firstly, ambiguity was introduced by some standard constructs embedded in the CNL. Commonly confused were is a kind of and is a in Rabbit  the former
is used to enter subclasses, while the latter is used for defining in-
stances. Users without knowledge engineering background did not
realize the difference, and often mixed instances and classes, see
Section 5.3. Such ambiguity problems can be anticipated and corresponding prompts added. In the most recent version of ROO,
appropriate error messages are added when a possibility for mixing is a kind of and is a is detected (e.g. when a partial pattern of
class definition is recognized but an is a relationship is used, the
user is reminded that is a is used for defining instances).

The second ambiguity type observed was caused by inability of
the CNL parser to determine the part-of-speech for some words,
most commonly when a word could be tagged as either a verb
(hence, corresponding to a relationship) or a noun (hence, corresponding to a concept). For example in a Hydrology domain, a user
stated that Flow is a concept, when trying to describe the Water
Flow of Rivers and other bodies of water in terms of their flow of
water. However, the Rabbit parser (at the time of the study) tagged
Flow as a verb (e.g. to flow) instead of a noun, and the sentence
was not accepted. ACE View had similar problems that were solved
by enabling a user to extend the glossary of terms. A solution for
this ambiguity type is to make the authoring tool aware of such
cases and to allow the user to override the part-of-speech tagger
of the parser at runtime. The tool should help the user realize that
there is a danger of introducing ambiguity, e.g. the user could always state that flow is a relationship, which results in an
ontology having an object property and a class with potentially
the same name. The latest version of ROO includes corresponding
warnings when this type of ambiguity is recognized.

The third ambiguity type observed was associated with parsing
compound noun or verb phrases when the boundary between such

phrases was hard to determine. Consequently, the CNL parsers reported ambiguity and asked the user to re-phrase the sentence. In
ACE View, users worked around the problem by using dashes to get
an ACE sentence accepted (e.g. Catchment is an area-that-
collects-water. where area-that-collects-water is
translated as a single OWL class, which is not further defined).
ROO does not allow this type of merging, which requires the user
to formulate two sentences to relate Catchment to an area and
to things that collect Water. This type of ambiguity was the
most confusing for users without knowledge engineering skills
[13]. The latest release of ROO includes a context-aware Rabbit
sentence parsing which uses the current ontology to recognize
the most probable Rabbit pattern, see [11] for a detailed description of the Rabbit parser.

5.2.4. Coverage

Learning to use a CNL resembles learning a new language 
starting from basic constructs and gradually adding more complex
statements. This was confirmed in the experimental studies with
ROO  most domain experts utilized only a subset of the full set
of Rabbit (and ACE) sentences. The resultant OWL ontologies varied
from ALE to ALCO and ALCOQ. That is, the resultant OWL
statements included definition of subclasses, anonymous classes
(concept union and intersections) universal and existential restrictions and qualified cardinality restrictions. Domain experts rarely
used or did not use at all CNL sentences that translated into dis-
joint, equivalence and negation axioms as well as role hierarchies
and role inclusions. The reason for this might be that users tried
to follow natural language descriptions of concepts  one rarely
describes a concept in terms of what it is not (disjoint classes or
complex concept negation).

These axioms are crucial for the quality of the resultant ontology
as they are vital when using OWL reasoners for automatic classifi-
cation. One possible way to empower the reasoning is to ask knowledge engineers to inspect the ontology and add the connecting
axioms. Another way is to help domain experts use more expressive
sentence types. The Guide Dog feature in ROO would be a good way
to do this. For instance, by scanning the created taxonomy for
suitable candidates it can generate connecting statements that will
enable the reasoning, and can ask the user to confirm or reject these
statements. Furthermore, domain experts can be directed to use a
more systematic approach to combine axioms following ontology
design patterns [21], which can be a crucial feature in ontology
authoring tools geared towards domain experts.

To sum up, our evaluation showed that CNL-based interaction
(in both ROO and ACE View) enabled domain experts to build
ontologies from scratch in a short period of time. This suggests that
CNL-based interaction can reduce the cognitive complexity associated with the move from natural language sentences to formal
ontological statements. CNL shortens the abstraction path by providing an intermediate level of abstraction which helps people without formal logical background to formulate knowledge constructs.
The task of converting CNL constructs to OWL, which requires formal knowledge engineering skills, is performed automatically by
the CNL parsers.

However, we also identified several drawbacks: (i) partial usage
of the full spectrum of CNL sentences; (ii) omission of axioms that
enable reasoning upon the ontology; (iii) existence of ambiguity
which may confuse users with limited logical background. A possible approach to address the drawbacks is to provide intelligent
support embedded in the tools, as discussed in Section 5.4.

5.3. Quality of the resultant ontologies

The comparative user study described in Section 5.1 allowed us
to analyze and compare the quality of the ontologies produced

R. Denaux et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 113127

by domain experts using ROO and ACE View. Here, we summarize
the main findings and explain how we obtained those findings.

5.3.1. Readability

Since both Rabbit and ACE are easy to understand, including
the CNL sentence as annotations improves the readability of the
resultant OWL ontologies; both ROO and ACE View do this. Ontologies created with ROO were slightly more readable because ROO
encourages users to provide natural language descriptions for
both concepts and relationships, which are also included as
annotations.

5.3.2. Fitness-for-purpose

A domain expert who is also a knowledge engineer at Ordnance
Survey produced benchmark ontologies in Hydrology and Water
Pollution, following the purpose and scope of the ontologies defined in the comparative user study. Each ontology produced by
a user in the study was compared to the corresponding benchmark
ontology: 1 point was added for every valid axiom (axiom in the
user ontology which also existed in the benchmark ontology or
was accepted as valid by the analyzing expert), while 1 point
was deducted for any invalid axiom (axiom from the benchmark
ontology missing in the user ontology or a user axiom containing
some modeling error). The overall average score for the ontologies
created with ROO was 2.5, while the average score for the ontologies created with ACE View was 4.25 (MannWhitney U = 9,
p  0.01). ROO out-performed ACE View with marginal significance
which can be attributed to the intelligent support provided in ROO,
see Section 5.4 for a discussion of the main differences between the
tools. Indeed, the analysis showed that ACE View ontologies included a high number of invalid axioms, averaging 8 invalid axioms per person more that ROO users. ACE View ontologies had
an average of 0.4 errors per axiom, compared to 0.13 errors for
ontologies created with ROO.13

None of the ontologies produced during the comparative user
study would have been usable without modification; this is not
surprising because the users in this study had only a limited
amount of time to build the ontologies, had no knowledge engineering background, and may have not realised the importance
of choosing appropriate knowledge sources. Our experimental design followed closely real situations when domain experts enter
knowledge constructs. This, however, led to reliance on domain experts judgement in choosing the natural language definitions and
deciding what statements should be entered. A more controlled
experimental design, e.g. providing the correct natural language
definitions and asking users to formulate corresponding ontological statements, could be used to further examine the effect of the
tools on the ontology quality.

Our observations have key implications for involving domain
experts in ontology authoring. Firstly, having domain experts more
engaged in the ontology authoring process enables key knowledge
constructs to be articulated but does not exclude the involvement of
knowledge engineers who further check the formal meaning and
validate the encoded statements. This gives strong support for collaborative ontology engineering where ontology contributors roles
are identified and the provenance of each ontological statement is
recorded [50].

Secondly, there is a need for intuitive approaches to help domain experts validate to what extent the axioms entered fall in the
ontology scope. While the use of CNL provides feedback about
whether entered knowledge adheres to the OWL standard, it does
not provide feedback about whether the ontology is fit for purpose.

13 See Section 4.3 in [13] for more details about the evaluation of the ontology
quality.

This issue is not directly addressed by the work presented in this
paper; however we are doing some initial work in this direction.
For example, a Verification and Extension Toolkit (VET) for ROO
was developed to help users verify the concepts and relationships
in the ontology they enter against a text corpus [51]. Using text
mining techniques, the Guide Dog was extended to point at redundant concepts and axioms (i.e. concepts and relationships which
are present in the user ontology but are not represented in the text
corpus), as well as to suggest missing concepts and relationships
(i.e. concepts and relationships discovered in the text corpus but
missing in the ontology). Very preliminary tests of VET have been
conducted. However, it has not been evaluated with users, and is
not integrated in the ROO release yet.

Thirdly, domain experts should be made aware of the logical
implications of their assertions. The first author of this paper is
working on a feedback-loop mechanism that will inform the user
about some logical consequences of adding new axioms to the
ontology being built; this feedback mechanism is part of a dialogue
system that aims to capture an OWL representation of ontology
purposes [10]. Other possible approaches include providing explanations about inferences [31] or visualizing what has been stated
[38].

5.4. Intelligent tool support

Based on the findings from the user studies with ROO, we can
identify key features to be embedded in tools that facilitate domain
experts involvement in ontology engineering.

5.4.1. Intuitive interface

The initial usability studies with ROO made it clear that many of
the GUI components used by Protege 4.0 were not suitable for domain experts. Protege provides an interface that enables knowledge engineers to use all the features that OWL provides. For
example, there is a GUI component for adding annotations to
OWL entities. When we asked domain experts to use this compo-
nent, they would be confused by the options provided: the type
of annotation URI (custom, built in, dublin core, etc.); the type of
annotation (constant, individual or property value); if it is a constant annotation, the xsd type and Lang. Our target users are
not interested in learning all the details about OWL, but are interested in building a conceptual model that can be used in tools that
make their jobs easier. The ROO GUI uses terminology that is closer
to the domain experts perspective instead of using OWL terminol-
ogy, e.g. concept instead of OWL class and relation instead of object
property. By providing a simplified GUI, users are able to construct
ontologies without needing to learn about too many OWL specific
details.

5.4.2. Assistance with the ontology authoring process

To compensate for the lack of knowledge engineering skills in
users, ontology authoring tools can incorporate certain knowledge
engineering expertise. In ROO, this was done by following an ontology methodology both in the interface and in the guidance provided via the Guide Dog, see Section 3. The user experience with
ROO confirmed that this was beneficial not only for reducing the
complexity of ontology authoring but, most importantly, for helping users gain an understanding of ontology engineering.

During the comparative study between ROO and ACE View, the
participants answered questions to test their knowledge about
ontologies, concepts, relationships and the steps required to build
conceptual models [13]. The users answered the same questions
before and after their session with ROO and ACE View. Each answer
was marked with: 1 (if the understanding has worsened); 0 (no
change to the users understanding on the questions), +1 (correct
aspects are added but gaps exist), and +2 (the understanding has

improved, and now is correct and complete). The maximum score,
if a user had not had any ontology modeling knowledge and has
become an expert, would have been 12, while the worst score
meaning a user was an expert and became totally confused would
have been 6. The ROO users scored significantly higher (l = 5,
r = 2.78) than the ACE View users (l = 0.38, r = 2.97); U (Mann
Whitney) = 8.5, p  0.01. Hence, with ROO the users understanding in ontology modeling improved significantly more than when
using ACE View.

The positive results are attributed to the main difference between both tools  ROO provides assistance with ontology authoring following an ontology methodology, while ACE View does not
(note that the CNL-based interaction in both tools is fairly similar).
This advocates in favor of using an appropriate methodology to
ground the interaction and to provide intelligent guidance in ontology authoring tools aimed for people with limited knowledge engineering background.

5.4.3. Assistance with CNL-based interaction

The limitations of CNL-based interaction pointed out above can
be addressed with appropriate intelligent support integrated in the
ontology authoring tool. This includes assistance with:

- Composing CNL sentences: the assistance can include intuitive
warnings and error messages, providing appropriate help, offering examples.

- Transition from natural language to CNL: providing help with

breaking a natural language definition into CNL statements.

- Resolving ambiguity: the CNL parsers can recognize possible
between

automatically

disambiguate

patterns

alternatives.

to

- Ontology validation: intuitive ways to help users underimplication of the CNL statements they

stand the logical
have entered.

- Connecting statements: domain experts need to be encouraged to define concepts with sentence patterns that are
required for certain reasoning tasks with ontologies (e.g. define
equivalence relations, disjoint classes), because these constructs are not common in natural language knowledge sources
such as dictionaries, encyclopedia.

- Ontology reuse: domain experts can be enabled to see existing
ontologies in a way they can understand. A bidirectional CNL
approach [9,33], where existing OWL can be automatically converted to Rabbit may help in this respect. Also, high-level sum-
maries, metadata and visualisations as described in Section 2
can help to achieve this.

5.4.4. Individual differences and multiple perspectives

Ontology authors differ based on their domain background, linguistic knowledge, and previous ontology engineering experience.
For example, two participants found the formulation of CNL sentences very challenging. This may be caused by the lack of a good
command of English (neither participants was a native speaker) or
poor linguistic knowledge (these participants found it challenging
to identify potential concepts and relationships in natural language
descriptions).

Ontology purpose is fundamental for taking ontology modeling
decisions and is a key factor in ontology perspectives. It appears
fruitful to seek ways to model purpose and clarify with users that
domain axioms correspond to the intended purpose. Our initial
study in modeling ontology purpose in a Geography domain is presented in [10]. In our current research, we consider modeling multiple perspectives
in ontology authoring and taking these
perspectives into account
for providing individualized user
support.

5.5. Generality and wider applicability

The approach presented here has been followed for the creation
of ontologies in several domains, as part of ongoing projects: mu-
sic,14 water pollution, hydrology, buildings and places,15 dissertation
writing,16 fire and rescue services,17 ontology purpose description.18
In addition, existing ontologies from Protege, such as pizza, wine and
cameras,19 were re-created with ROO. This enabled us to identify
factors which affect the applicability of our approach to a range of
domains, and to identify possible future improvements.

Firstly, the applicability is affected by the CNL-based interface.
Some domains may contain highly specialised terms that standard
parsers cannot identify correctly (i.e. the part-of-speech is identified incorrectly). For example, in the camera domain, camera instances have names such as Canon PowerShot SX200 IS.
Recognizing that the phrase is a possible concept or instance is a
significant challenge for CNL parsers (the ROO parser could not recognise such constructs). In such cases, the interface can be frustrating since the users have to find a way round to enter these
constructs, which make the ontology authoring cumbersome. Fur-
thermore, problems with accurate part-of-speech tagging may
arise when gerunds are used to define concepts such as activities,
actions or processes (e.g. Data Processing, Housing, Trading).
Although we have built some ontologies using this type of con-
cepts, we lack evidence for the robustness of parsing large ontologies where the majority of the concepts describe processes.

Another constraint of CNL-based interaction is that in order to
avoid ambiguity, CNL sentences can be verbose, e.g. when combining more than one conjunction or disjunction with cardinality
restrictions. This may occur in cases where complex relationships
need to be captured between classes in a heavy-weight ontology.
Although such occurrences were noted in our studies, we do not
have enough cases for a systematic analysis of how domain experts
cope with complex CNL constructs. This is a subject of future
research.

Secondly, the wider applicability of our approach, and ROO in
particular, is hindered by technical constraints of the tool. There
is a memory overhead on top of Protege because we keep track
of the CNL sentences on top of the OWL representation. The largest
ontology we have built contains around 250 concepts; we did not
encounter any performance problems with this ontology. However,
much larger ontologies may cause performance problems. Cur-
rently, ROO does not support the translation of an existing OWL
ontology into Rabbit, which means the tool is not suitable for editing existing ontologies. This is not a limitation of the approach, as
other CNL tools such as ACE View and CLOnE provide roundtrip
ontology editing [9].

Finally, the ontology methodology followed, Kanga, does not
provide sufficient description of the collaborative aspect of ontology construction. For instance, Kanga does not specify what collaborative activities several domain experts can be engaged in while
constructing the ontology. Currently, ROO does not provide ways
to collaborate with others or to share the ontology, leaving this
to the users discretion. We plan to extend ROO to include collaborative activities as part of our work on a recently started EU
project.20

14 Used in the initial testing of ROO.
15 http://www.ordnancesurvey.co.uk/oswebsite/ontology/.
16 http://awesome.leeds.ac.uk/.
17 Part of a PhD project that created an intelligent agent for reflective mobile
learning.
18 Part of our ongoing work on multi-perspective ontology engineering.
19 http://protege.stanford.edu/download/ontologies.html.
20 http://www.dicode-project.eu/.

R. Denaux et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 113127

6. Conclusions

This paper has presented our experiences with an approach for
ontology construction that combines a controlled natural language
to formalize conceptualizations and provides guidance through the
ontology construction process by following an ontology construction methodology. This work is fundamental for ontology engineering as it pushes the boundaries of what a single domain
expert with no previous knowledge engineering experience can
do without needing to undergo training on OWL and ontology editing tools. The key novelty of our approach compared to other attempts to actively involve domain experts in ontology authoring
is in offering a holistic way that complements a CNL-based interaction with an appropriate methodology to provide innovative tools
for ontology authoring suitable for users who lack knowledge engineering background.

The paper described the basic building blocks of our approach:
the Kanga ontology construction methodology, the Rabbit controlled natural language and the ROO tool. All three have been designed based on previous experiences on ontology construction
methodologies, CNLs and tools, but focus on the active involvement of domain experts.

Kanga adds to the suite of ontology methodologies tailored for
domain experts. It contributes by showing a methodology that
has been derived empirically, based on extensive experience at a
large organization (the UK mapping agency  Ordnance Survey),
clearly defining the involvement of domain experts and knowledge
engineers, stressing the importance of domain experts in the construction of heavy weight ontologies and showing how this can be
addressed by using a CNL.

Rabbit is a controlled natural

language designed following
extensive experimental work with domain experts at Ordnance
Survey. It looks quite natural and close to English constructs. Its
expressiveness is comparable to OWL 2  there is direct match to
OWL constructs which enables easy conversion to OWL. Rabbit focuses on usability, i.e. how sentences can be composed and read by
domain experts without a formal logics background.

ROO is a novel ontology authoring tool geared towards domain
experts. ROO enhanced the usability of CNL-based interaction,
integrates ontology methodology and compensates for the lack of
knowledge engineering skills by offering intelligent assistance.

Based on experimental studies with ROO, we have discussed the
advantages of using a combined approach that is tailored to domain experts over a CNL-driven approach that provides a layer between Protege and the user. Our experimental studies show that:
(i) ontology authoring with a CNL is easily possible; (ii) providing
CNL tool support results in higher quality ontologies; (iii) providing
ontology construction methodology guidance enables education of
the domain experts in basic knowledge engineering. However, we
also identified pitfalls of CNLs, which indicate that further intelligent tool support is essential. We also pointed out limitations of
our experiences with ROO, such as not having enough data about
users building large complex ontologies with it, so the expressivity
used by domain experts is still limited. These limitations will be
addressed in future work. Finally, our experiences with ROO also
showed that tailoring ROO around Kanga and providing support
based on that methodology improves the users understanding of
the ontology construction process.
