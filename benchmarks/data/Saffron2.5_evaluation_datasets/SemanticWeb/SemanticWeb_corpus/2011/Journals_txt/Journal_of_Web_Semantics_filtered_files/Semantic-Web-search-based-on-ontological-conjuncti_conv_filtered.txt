Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 453473

Contents lists available at SciVerse ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

j o u r n a l h o m e p a g e : h t t p : / / w w w . e l s e v i e r . c o m / l o c a t e / w e b s e m

Semantic Web search based on ontological conjunctive queries q
Bettina Fazzinga a, Giorgio Gianforme b, Georg Gottlob c,d, Thomas Lukasiewicz c,

a Dipartimento di Elettronica, Informatica e Sistemistica, Universita della Calabria, Italy
b Dipartimento di Informatica e Automazione, Universita Roma Tre, Italy
c Department of Computer Science, University of Oxford, UK
d Oxford-Man Institute of Quantitative Finance, University of Oxford, UK

a r t i c l e

i n f o

a b s t r a c t

Article history:
Available online 14 September 2011

Keywords:
Semantic Web search
Ontologies
Description logics
Conjunctive queries
Semantic Web
Web search

Many experts predict that the next huge step forward in Web information technology will be achieved by
adding semantics to Web data, and will possibly consist of (some form of) the Semantic Web. In this
paper, we present a novel approach to Semantic Web search, called Serene, which allows for a semantic
processing of Web search queries, and for evaluating complex Web search queries that involve reasoning
over the Web. More specifically, we first add ontological structure and semantics to Web pages, which
then allows for both attaching a meaning to Web search queries and Web pages, and for formulating
and processing ontology-based complex Web search queries (i.e., conjunctive queries) that involve reasoning over the Web. Here, we assume the existence of an underlying ontology (in a lightweight ontology
language) relative to which Web pages are annotated and Web search queries are formulated. Depending
on whether we use a general or a specialized ontology, we thus obtain a general or a vertical Semantic
Web search interface, respectively. That is, we are actually mapping the Web into an ontological knowledge base, which then allows for Semantic Web search relative to the underlying ontology. The latter is
then realized by reduction to standard Web search on standard Web pages and logically completed ontological annotations. That is, standard Web search engines are used as the main inference motor for ontol-
ogy-based Semantic Web search. We develop the formal model behind this approach and also provide an
implementation in desktop search. Furthermore, we report on extensive experiments, including an
implemented Semantic Web search on the Internet Movie Database.

O 2011 Elsevier B.V. All rights reserved.

1. Introduction

Web search is a key technology of the Web, since it is the primary way to access content in the ocean of Web data. Current
Web search technologies are essentially based on a combination
of textual keyword search with an importance ranking of documents via the link structure of the Web [8]. For this reason, how-
ever, current standard Web search does not allow for a semantic
processing of Web search queries, which analyzes both Web search
queries and Web pages with respect to their meaning, and returns
exactly the semantically relevant pages to a query. For the same
reason, current standard Web search also does not allow for evaluating complex Web search queries that involve reasoning over the
Web.

q This paper is a significantly extended and revised version of a paper that
appeared in: Proceedings FoIKS-2010, LNCS 5956, Springer, 2010, pp. 153172 [25].

 Corresponding author. Tel.: +44 (0) 1865 522566; fax: +44 (0) 1865 273839.

E-mail addresses: bfazzinga@deis.unical.it (B. Fazzinga), giorgio.gianforme@
thomas.

gmail.com (G. Gianforme), georg.gottlob@cs.ox.ac.uk (G. Gottlob),
lukasiewicz@cs.ox.ac.uk (T. Lukasiewicz).

1570-8268/$ - see front matter O 2011 Elsevier B.V. All rights reserved.
doi:10.1016/j.websem.2011.08.003

Many experts predict that the next huge step forward in Web
information technology will be achieved by adding such structure
and/or semantics to Web contents and exploiting them when processing Web search queries. Indeed, the Semantic Web [5,6] as a vision of a more powerful future Web goes in this direction. It is a
common framework that allows data to be shared and reused in
different applications, enterprises, and communities. The Semantic
Web is an extension of the current Web by standards and technologies that help machines to understand the information on the
Web so that they can support richer discovery, data integration,
navigation, and automation of tasks. It consists of several hierarchical layers, where the Ontology layer, in form of the OWL Web
Ontology Language [34,51,4], is the highest layer that has currently
reached a sufficient maturity. Some important layers below the
Ontology layer are the RDF and RDF Schema layers along with the
SPARQL query language. For the higher Rules, Logic, and Proof layers
of the Semantic Web, one has especially developed languages integrating rules and ontologies, and languages supporting more
sophisticated forms of knowledge. During the recent decade, a
huge amount of academic and commercial research activities has
been spent towards realizing the vision of the Semantic Web.
Hence, in addition to the traditional Web pages, future Web data

B. Fazzinga et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 453473

are expected to be more and more organized in the new formalisms of the Semantic Web, and will thus also consist of RDF data
along with ontological and rule-based knowledge.

As an important example of an initiative towards adding structure and/or semantics to Web contents in practice (and thus ultimately also towards the Semantic Web), Googles Rich Snippets1
highlight useful information from Web pages via structured data
standards such as microformats and RDFa. As another example,
OpenCalais2 turns unstructured HTML into semantically marked up
data, ordering data into groups such as people, places, compa-
nies, and more. Other (less general) examples are Freebase,3 which
is a semantically marked up database of structured information similar to Wikipedia, and DBpedia,4 which extracts structured information from Wikipedia and makes that data available on the Web.

The development of a new search technology for the Semantic
Web, called Semantic Web search, is currently an extremely hot to-
pic, both in Web-related companies and in academic research (see
Section 10). In particular, there is a fast growing number of commercial and academic Semantic Web search engines. There are
essentially two main research directions. The first (and most com-
mon) one is to develop a new form of search for searching the
pieces of data and knowledge that are encoded in the new representation formalisms of the Semantic Web, while the second
(and nearly unexplored) direction is to use the formalisms of the
Semantic Web in order to add some semantics to Web search.
The second direction is also a first step towards Web search queries
in (written or spoken) natural language.

In this paper, we follow the second line of research. We aim at
adding ontological structure to Web pages, which then allows for
both analyzing the meaning of Web search queries and Web pages,
and for formulating and processing ontology-based complex Web
search queries that involve reasoning over the Web. Intuitively,
rather than being interpreted in a keyword-based syntactic fash-
ion, the pieces of data on existing Web pages are connected to
(and via) some ontological knowledge base and then interpreted
relative to this knowledge base. That is, the pieces of data on
Web pages are connected to (and via) a much more precise semantic and contextual meaning. This allows for answering Web search
queries in a much more precise way, taking into account the meaning of Web search queries and pages, and it also allows for more
complex ontology-based Web search queries that involve reasoning over the Web, which are also much closer to complex natural
language search queries than current Boolean keyword-based
search queries. The following are some examples of such Web
search queries, which can be appropriately handled in our
Semantic Web search, but not in current standard Web search:

 As for complex Web search queries, when searching for a movie,
one may be interested in movies that were produced by a US
company before 1999 and that had a French director. Similarly,
when buying a house in a town, one may be interested in large
house selling companies within 50 miles of that town, that exist
for at least 15 years, and that were not known to be blacklisted
by a consumer organization in the last 5 years. Such queries are
answered by connecting the information on existing Web pages
relative to some ontological knowledge.

 Suppose next that one is searching for bus (as a means of
transportation for persons) on the Web. Then, one is looking
for buses or synonyms/related concepts, but also for special
kinds of buses that are not synonyms/related concepts, such
as, e.g., passenger vans. Ontological knowledge now allows for

1 http://knol.google.com/k/google-rich-snippets-tips-and-tricks
2 http://www.opencalais.com/
3 http://www.freebase.com/
4 http://dbpedia.org/

obtaining both a collection of contextually correct synonyms/
related concepts and a collection of contextually correct special
kinds of buses.

 Similarly, a Web search for president of the USA should also
return Web pages that contain George W. Bush (who is one
of the presidents of the USA according to some ontological
knowledge). Also, a Web search for the president of the USA
on September 11, 2001 should return Web pages mentioning
George W. Bush (who was the president of the USA on
September 11, 2001).

 On the other hand, when searching for Web pages about the
first president of the USA, Washington, ontological knowledge
allows us to restrict our search to Web pages that are actually
about Washington as the name of the president, and so to
ignore, e.g., Web pages about the state or town.

In our approach, an ontologically enriched Web along with
complex ontology-based search on the Web are achieved on top
of the existing Web and using existing Web search engines. Intui-
tively, standard Web pages are first connected to (and via) an ontological knowledge base, which then allows for formulating and
processing complex ontology-based (conjunctive) search queries
that involve reasoning over the data of the Web. The query processing step is based on new techniques (i) for pre-compiling the
ontological knowledge using standard ontology reasoning techniques and (ii) for translating complex ontology-based Web queries into (sequences of) standard Web queries that are answered
by standard Web search. That is, essential parts of ontological
search on the Web are actually reduced to state-of-the-art search
engines such as Google search. As important advantages, this approach can immediately be applied to the whole existing Web,
and it can be done with existing Web search technology (and so
does not require completely new technologies). Such a line of research aims at adding ontology-based structure and semantics
(and thus in a sense also intelligence) to current search engines
for the existing Web by combining existing Web pages and queries
with ontological knowledge.

The ontological knowledge and annotations that are underlying
our Semantic Web search can be classified according to its origin
and contents. As for the origin, they may be either (a) explicitly defined by experts, or (b) automatically extracted from the Web,
eventually coming along with existing pieces of ontological knowledge and annotations (e.g., from existing ontologies or ontology
fragments, and/or from existing annotations of Web pages in
microformats or RDFa). In the latter case, generating, maintaining,
and updating the ontological knowledge and annotations is done
automatically and much less cost-intensive than in the former
case. As for the contents, (a) the ontological knowledge and annotations may either describe fully general knowledge (such as the
knowledge encoded in Wikipedia) for general ontology-based
search on the Web, or (b) they may describe some specific knowledge (such as biomedical knowledge) for vertical ontology-based
search on the Web. The former results into a general ontologybased interface to the Web similar to Google, while the latter produces different vertical ontology-based interfaces.

The main contributions of this paper and the characteristic features of our approach to Semantic Web search can be briefly summarized as follows:

 We present a novel approach to Semantic Web search, called
Serene, which allows for a semantic processing of Web search
queries relative to an underlying ontology, and for evaluating
ontology-based complex Web search queries that involve reasoning over the Web. We show how the approach can be implemented on top of standard Web search engines and ontological
inference technologies.

 We develop the formal model behind this approach. In particu-
lar, we introduce Semantic Web knowledge bases and Semantic
Web search queries to them. We also define the ObjectRank
ranking for our Semantic Web search.

 We provide a technique for processing Semantic Web search
queries, which consists of an offline inference and an online
reduction to a collection of standard Web search queries. We
prove that this way of processing Semantic Web search queries
is always ontologically correct. Furthermore, we identify a large
class of Semantic Web knowledge bases where it is also
complete.

 The offline inference compiles terminological knowledge into
so-called completed annotations. We prove that these have a
polynomial size and can also be computed in polynomial time.
Furthermore, experimental data show that they are also rather
small in practice, especially since ontological hierarchies in
practice are generally not that deep (a concept has at most a
dozen superconcepts).

 We report on two prototype implementations of our approach
in desktop search. Experiments with more than one million
annotation facts show that the new methods are principally feasible and potentially scale to Web search (which is actually
much faster than desktop search, even with a much larger
search space).

 We also compare our most recent prototype with the Corese
system [17], which is the Semantic Web search system in the
state-of-the-art that is most closely related to our approach,
showing that our system is 18 times quicker than Corese.

 Differently from conventional Boolean keyword-oriented Web
search queries, the proposed Semantic Web search queries
clearly empower the user to precisely describe her information
need for certain kinds of queries, resulting in a very precise
result set and a very high precision and recall for the query
result.

 We show that our approach to Semantic Web search can be
readily applied to existing Web pages, even if they are currently
not (yet) semantically annotated, and that it can be used to perform a vertical ontology-based search. More concretely, we
have used the approach to implement a Semantic Web search
interface for the Web pages of the Internet Movie Database
(IMDB).5

The rest of this paper is organized as follows. In Section 2, we
give an overview of our approach to Semantic Web search. In Section 3, we introduce Semantic Web knowledge bases and Semantic
Web search queries, and we define the ObjectRank ranking. Sections 46 describe how Semantic Web search queries are processed via offline inference and online reduction to standard Web
search. In Sections 7 and 8, we report on two prototype implementations for semantic desktop search, along with extensive experimental results. Section 9 describes the implemented Semantic
Web search on the IMDB. In Sections 10 and 11, we discuss related
work, summarize our main results, and give an outlook on future
research. The basics of the underlying tractable ontology language
are recalled in Appendix A, and detailed proofs of all results are given in Appendix B.

2. System overview

The overall architecture of our Semantic Web search system,
called Serene (Semantic Web search engine), is shown in Fig. 1. It
consists of the Interface, the Query Evaluator (implemented on top
of standard Web Search Engines), and the Inference Engine (blue

5 http://www.imdb.com

Interface

Query

Evaluator

Search
Engine

Inference
Engine

Web

Annotations

Ontology

Fig. 1. System architecture.

parts). Standard Web pages and their objects are enriched by Annotation pages, based on an Ontology.

2.1. Ontology

Our approach to Semantic Web search is done relative to a fixed
underlying ontology, which defines an alphabet of elementary
ontological ingredients, as well as terminological relationships between these ingredients. The ontology may either describe fully
general knowledge (such as the knowledge encoded in Wikipedia)
for general ontology-based search on the Web, or it may describe
some specific knowledge (such as biomedical knowledge) for vertical ontology-based search on the Web. The former results into a
general ontology-based interface to the Web similar to Google,
while the latter produces different vertical ontology-based interfaces to the Web. There are many existing ontologies that can be
used, which have especially been developed in the context of the
Semantic Web, but also in biomedical and technical areas. Such
ontologies are generally created and updated by human experts
in a knowledge engineering process. Recent research attempts
are also directed towards an automatic generation of ontologies
from text documents, eventually coming along with existing pieces
of ontological knowledge [9,22].

For example, an ontology may contain the knowledge that (i)
conference and journal papers are articles, (ii) conference papers
are not journal papers, (iii) isAuthorOf relates scientists and articles,
(iv) isAuthorOf is the inverse of hasAuthor, and (v) hasFirstAuthor is a
functional binary relationship, which is formalized by:
ConferencePaper v Article; JournalPaper v Article;
ConferencePaper v :JournalPaper;
9isAuthorOf v Scientist;9isAuthorOf  v Article;
isAuthorOf  v hasAuthor; hasAuthor v isAuthorOf ;
funct hasFirstAuthor:

2.2. Annotations

As a second ingredient of our approach to Semantic Web search,
we assume the existence of assertional pieces of knowledge about
Web pages and their objects, also called (semantic) annotations,
which are defined relative to the terminological relationships of
the underlying ontology. Such annotations are starting to be
widely available for a large class of Web resources, especially
user-defined annotations with the Web 2.0. They may also be automatically learned from Web pages and their objects (see, e.g., [15]).
As a midway between such fully user-defined and fully automatically generated annotations, one can also automatically extract

B. Fazzinga et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 453473

annotations from Web pages using user-defined rules, as described
in Section 9.

For example, in a very simple scenario relative to the ontology
in Eq. 1, a Web page i1 may contain information about a Ph.D. student i2, called Mary, and two of her papers: a conference paper i3
with title Semantic Web search and a journal paper i4 entitled
Semantic Web search engines and published in 2008. A simple
HTML page representing this scenario is shown in Fig. 2.

There may now exist one semantic annotation each for the Web
page, the Ph.D. student Mary, the journal paper, and the conference
paper. The annotation for the Web page may simply encode that it
mentions Mary and the two papers, while the one for Mary may
encode that she is a Ph.D. student with the name Mary and the
author of the papers i3 and i4. The annotation for i3 may encode
that i3 is a conference paper and has the title Semantic Web
search, while the one for i4 may encode that i4 is a journal paper,
authored by Mary, has the title Semantic Web search engines, was
published in 2008, and has the keyword RDF. The semantic annotations of i1, i2, i3, and i4 are then formally expressed as the following sets of ontological axioms Ai1 , Ai2 , Ai3 , and Ai4 , respectively:
Ai1 14 fcontainsi1; i2; containsi1; i3; containsi1; i4g;
Ai2 14 fPhDStudenti2; namei2; \mary";
isAuthorOfi2; i3; isAuthorOfi2; i4g;

Ai3 14 fConferencePaperi3; titlei3; \Semantic Web search"g;
Ai4 14 fJournalPaperi4; hasAuthori4; i2;

titlei4; \Semantic Web search engines";
yearOfPublicationi4; 2008; keywordi4; \RDF"g:

2.3. Inference Engine

Differently from the ontology, the semantic annotations can be
directly published on the Web and searched via standard Web
search engines. In order to also make it visible to standard Web
search engines, the ontology is compiled into the semantic annota-
tions. More specifically, all semantic annotations are completed in
an offline ontology compilation step, where the Inference Engine
adds all properties (i.e., ground atoms) that can be deduced from
the ontology and the semantic annotations. The resulting (com-
pleted) semantic annotations are then published as Web pages, so
that they can be searched by standard Web search engines.

For example, considering again the running scenario, using the
ontology in Eq. 1, in particular, we can derive from the semantic
annotations in Eq. 2 that the two papers i3 and i4 are also articles,
and both authored by Mary.

2.4. HTML encoding of annotations

The above searchable (completed) semantic annotations of (ob-
jects on) standard Web pages are published as HTML Web pages
with pointers to the respective object pages, so that they (in addition to the standard Web pages) can be searched by standard
search engines. For example, the HTML pages for the completed
semantic annotations of the above Ai1 , Ai2, Ai3 , and Ai4 are shown
in Fig. 3. We here use the HTML address of the Web page/objects
annotation page as an identifier for that Web page/object. The
plain textual representation of the completed semantic annotations allows their processing by existing standard search engines
for the Web. It is important to point out that this textual representation is simply a list of properties, each eventually along with an
identifier or a data value as attribute value, and it can thus immediately be encoded as a list of RDF triples. Similarly, the completed
semantic annotations can be easily encoded in RDFa or
microformats.

2.5. Query Evaluator

The Query Evaluator reduces each Semantic Web search query of
the user in an online query processing step to a sequence of standard Web search queries on standard Web and annotation pages,
which are then processed by a standard Web Search Engine. The
Query Evaluator also collects the results and re-transforms them
into a single answer which is returned to the user. As an example
of a Semantic Web search query, one may ask for all Ph.D. students
who have published an article in 2008 with RDF as a keyword,
which is formally expressed as follows:
Qx 14 9yPhDStudentx ^ isAuthorOfx; y ^ Articley^

yearOfPublicationy; 2008 ^ keywordy; \RDF":

This query is transformed into the two queries Q1 = PhDStudent
AND isAuthorOf and Q2 = Article AND yearOfPublication 2008 AND
keyword RDF, which can both be submitted to a standard Web
search engine. The result of the original query Q is then built from
the results of the two queries Q1 and Q2. Note that a graphical user
interface, such as the one of Googles advanced search, and ultimately a natural language interface (for queries in written or spoken natural language) can help to hide the conceptual complexity
of ontological queries to the user.

3. Semantic Web search

In this section, we introduce Semantic Web knowledge bases,
and we define the syntax and semantics of Semantic Web search
queries to such knowledge bases. We then introduce a ranking
for individuals in our approach, called ObjectRank, which generalizes the standard PageRank ranking of Web pages. Although we
implicitly assume the tractable description logic DL-LiteA
(cf. Appendix A) as underlying ontology language for Semantic
Web knowledge bases and search queries, any other ontology languages may be used as well.

3.1. Semantic Web knowledge bases

Informally, a Semantic Web knowledge base consists of a background TBox and a collection of ABoxes, one for every concrete
Web page and for every object on a Web page. For example, the

Fig. 2. HTML page.

Fig. 3. Four HTML pages encoding the (completed) semantic annotations for the HTML page in Fig. 2 and the three objects on it.

homepage of a scientist may be such a concrete Web page and be
associated with an ABox, while the publications on the homepage
may be such objects, which are also associated with one ABox each.
We assume pairwise disjoint sets D, A, RA, RD, I, and V of atomic data-
types, atomic concepts, atomic roles, atomic attributes, individuals, and
data values, respectively. Let I be the disjoint union of two sets P and O of
Web pages and Web objects, respectively. Informally, every Web page
p 2 P is an identifier for a concrete Web page, while every Web object
o 2 O is an identifier for a concrete object on a concrete Web page. We
assume the atomic roles links_to between Web pages and contains between Web pages and Web objects. The former represents the link
structure between concrete Web pages, while the latter encodes
the occurrences of concrete objects on concrete Web pages.
Definition 1. A semantic annotation Aa for a Web page or object
a 2 P [ O is a finite set of concept membership axioms A(a), role
membership axioms P(a,b), and attribute membership axioms
U(a,v) (which all have the Web page or object a as first argument),
where A 2 A, P 2 RA, U 2 RD, b 2 I, and v 2 V. A Semantic Web
knowledge base KB 14 T ;Aaa2P[O consists of a TBox T and one
semantic annotation Aa for every Web page and object a 2 P [ O.

Informally, a Semantic Web knowledge base consists of some
background terminological knowledge and some assertional
knowledge for every concrete Web page and for every concrete object on a Web page. The terminological knowledge may be an
ontology from some global Semantic Web repository or an ontology defined locally by the user site. In contrast to the terminological knowledge, the assertional knowledge will be directly stored
on the Web (on annotation pages like the described standard
Web pages) and is thus accessible via Web search engines.

Example 1. (Scientific Database contd). Continuing the running
example of Section 2, a Semantic Web knowledge base may specify
some simple information about scientists and their publications.
The sets of atomic concepts, atomic roles, atomic attributes,
individuals, and data values are given as follows:

A 14 fScientist;PhDStudent;Article;ConferencePaper;JournalPaperg;
RA 14 fhasAuthor;isAuthorOf ;hasFirstAuthor;containsg;
RD 14 fname;title;yearOfPublication;keywordg;
I 14 fi1;i2;i3;i4g;
V 14 f\mary";\Semantic Web search";2008;
\Semantic Web search engines";\RDF"g:

The set I is partitioned into the set P = {i1} of Web pages and the set
O = {i2,i3,i4} of Web objects on i1. Then, a Semantic Web knowledge
base is given by KB 14 T ;Aaa2P[O, where the TBox T is given by
the axioms in Eq. 1, and the semantic annotations Aa of the individuals a 2 P [ O are the ones in Eq. 2.

3.2. Semantic Web search queries

As Semantic Web search queries to Semantic Web knowledge
bases, we use unions of conjunctive queries with conjunctive and
negated conjunctive subqueries. We now first define the syntax
of Semantic Web search queries and then the semantics of positive
and general such queries to Semantic Web knowledge bases.

3.2.1. Syntax

Intuitively, using database and description logic terminology,
Semantic Web search queries are unions of conjunctive queries,
which may contain conjunctive queries and negated conjunctive
queries in addition to atoms and equalities as conjuncts.

We first give some preparative definitions as follows. Let X be a
finite set of variables. A term is either a Web page p 2 P, a Web object o 2 O, a data value v 2 V, or a variable x 2 X. An atomic formula
(or atom) a has one of the following forms:

(i) d(t), where d is an atomic datatype, and t is a term;
(ii) A(t), where A is an atomic concept, and t is a term;
(iii) P(t,t0), where P is an atomic role, and t, t0 are terms; and
(iv) U(t,t0), where U is an atomic attribute, and t, t0 are terms.

B. Fazzinga et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 453473

An equality is of the form = (t,t0), where t and t0 are terms. A conjunctive formula $y/(x,y) is an existentially quantified conjunction of
atoms a and equalities = (t,t0), which have free variables among x
and y. We are now ready to define the notion of a Semantic Web
search query as follows.

Definition 2. A Semantic Web search query Q(x) is an expression of
i1419yi/ix; yi, where each /i with i 2 {1, . . ., n} is a
the form
conjunction of atoms a (also called positive atoms), conjunctive
formulas w, negated conjunctive formulas not w, and equalities = (t,t0), which have free variables among x and yi.
Example 2. (Scientific Database contd). The query Q(x) of Eq. 3 is a
Semantic Web search query. Two other Semantic Web search queries are:
Q 1x 14 Scientistx ^ not doctoralDegreex; \oxford university"^

worksForx; \oxford university" _ Scientistx ^ doctoralDegreex;
\oxford university" ^ not worksForx; \oxford university";

Q 2x 14 9yScientistx ^ worksForx; \oxford university" ^ isAuthorOfx; y^

not ConferencePapery ^ not 9z yearOfPublicationy; z:

Informally, Q1(x) asks for all scientists who are either working for
oxford university and did not receive their Ph.D. from that university,
or who received their Ph.D. from oxford university but do not work
for it. Whereas Q2(x) asks for all scientists of oxford university who
are authors of at least one unpublished non-conference paper. Note
that when searching for scientists, the system automatically
searches for all subconcepts (known according to the TBox T of
the underlying Semantic Web knowledge base KB), such as Ph.D.
students or computer scientists.

3.2.2. Semantics of positive search queries

We define the semantics of positive Semantic Web search que-
ries, which are negation-free, in terms of ground substitutions via
the notion of logical consequence.

We first give some preliminary definitions. A Semantic Web
search query Q(x) is positive iff it contains no negated conjunctive
subqueries. A (variable) substitution h maps variables from X to
terms. A substitution h is ground iff it maps to Web pages p 2 P,
Web objects o 2 O, and data values v 2 V. A closed first-order formula / is a logical consequence of a knowledge base KB 14 T ,
Aaa2P[O, denoted KB  /,
iff every first-order model I of
T [
a2P[OAa also satisfies /.

Definition 3. Given a Semantic Web knowledge base KB and a
positive Semantic Web search query Q(x) with free variables x, an
answer for Q(x) to KB is a ground substitution h for the variables x
such that KB  Q(xh).

Example 3. (Scientific Database contd). Consider again the Semantic Web knowledge base KB of Example 1. The Semantic Web
search query Q(x) of Eq. 3 is positive, and an answer for Q(x) to
KB is given by h = {x/i2}.

3.2.3. Semantics of general search queries

We next define the semantics of general search queries by
reduction to the semantics of positive ones, interpreting negated
conjunctive subqueries not w as the lack of evidence about the
truth of w. That is, negations are interpreted by a closed-world
semantics on top of the open-world semantics of description
logics.

Definition 4. Given a Semantic Web knowledge base KB and
search query

Qx 14

9yi/i;1x; yi ^ 			 ^ /i;lix; yi ^ not /i;li1x; yi ^ 			^

i141
not /i;mix; yi;

an answer for Q(x) to KB is a ground substitution h for the variables x
such that KB  Q+(xh) and KB = Q(xh), where the positive search
queries Q+(x) and Q(x) are defined as follows:
9yi/i;1x; yi ^ 			 ^ /i;lix; yi and

Qx 14

i141

Qx 14

9yi/i;1x; yi ^ 			 ^ /i;lix; yi^
i141
/i;li1x; yi _ 			 _ /i;mix; yi:

Informally, a ground substitution h is an answer for the search
query Q(x) to KB iff (i) h is an answer for Q+(x) to KB, and (ii) h is
not an answer for Q(x) to KB, where Q+(x) is the positive part of
Q(x), while Q(x) is the positive part of Q(x) combined with the
complement of the negative one.

Example 4. (Scientific Database contd). Consider the Semantic Web
knowledge base KB 14 T ;Aaa2P[O of Example 1 and the following general Semantic Web search query, asking for Marys unpublished non-journal papers:

Qx 14 9y Articlex ^ hasAuthorx; y ^ namey; \mary"^

not JournalPaperx ^ not 9z yearOfPublicationx; z:

Then, an answer for Q(x) to KB is given by h = {x/i3}. Recall that i3
represents an unpublished conference paper entitled Semantic
Web search. Note that the membership axioms Article(i3) and has-
Author(i2,i3) are not in the semantic annotations Aa, a 2 P [ O, but
they can be inferred from them using the ontology T .

3.3. Ranking answers

As for the ranking of all answers for a Semantic Web search
query Q to a Semantic Web knowledge base KB (i.e., ground substitutions for all free variables in Q, which correspond to tuples of
Web pages, Web objects, and data values), we generalize the PageRank technique [8]: rather than considering only Web pages and
the link structure between Web pages (expressed through the role
links_to here), we also consider Web objects, which may occur on
Web pages (expressed through the role contains), and which may
also be related to other Web objects via other roles. More con-
cretely, we define the ObjectRank of a Web page or object a as
follows:
Ra 14 d 	

b=Nb  1  d 	 Ea;

b2Ba

where (i) Ba is the set of all Web pages and objects o that relate
to a (i.e., o relates to a via some role), (ii) Nb is the number of
Web pages and objects o that relate from b (i.e., b relates to o
via some role), (iii) d is a damping factor, and (iv) E associates
with every Web page and object an initial value, called source
of rank. So, rather than depending only on the link structure between Web pages, the new ranking depends also on the relationships between Web pages and objects, and on the relationships
between Web objects, where the user fixes the roles to be consid-
ered. Note that in some cases, only a subset of all relationships
may be used for specifying ObjectRank. For example, in the Scientific Database, the relationship cites alone between articles produces a very useful ranking on articles.

The ranking on Web pages and objects is then naturally extended to answers (i.e., tuples of Web pages, Web objects, and val-
ues) for Semantic Web search queries to Semantic Web knowledge
bases. For example, the answers can be ordered lexicographically,
or the rank of an answer can be defined as the minimum (or max-
imum) of the ranks of its Web pages and objects, and then ordered
as usual.

4. Realizing Semantic Web search

The main idea behind processing Semantic Web search queries Q
to a knowledge base KB is to reduce them to standard Web search
queries. To this end, the TBox T of KB must be considered when performing standard Web search. There are two main ways to do so. The
first is to compile T into Q, yielding a new standard Web search
query Q0 on the ABox A of KB. The second, which we adopt here, is
to compile T via offline ontology reasoning into the ABox A of KB,
yielding a completed ABox A0, which (being represented on the
Web in addition to the standard Web pages) is then searched online
by a collection of standard Web search queries depending on Q. So,
processing Semantic Web search queries Q is divided into

 an offline ontology reasoning step, where all semantic annotations of Web pages and objects are completed by membership
axioms entailed from KB, and

 an online reduction to standard Web search, where Q is transformed into standard Web search queries whose answers are
used to construct the answer for Q.

Observe that the compilation of the TBox T via an offline ontology reasoning step into the ABox A of KB has several important
advantages over the compilation of T into the query Q. Note that
the latter technique is also applied in the inference and query processing algorithms of DL-Lite; it implies that ontology reasoning is
done online during the query processing step. As a first advantage
of the former, ontology reasoning is done offline (i.e., before and
independently from query processing), and thus its computation
time does not appear in the query processing time. Second, ontology reasoning is done only once, and then used in processing different queries, while the compilation of T into Q requires to
newly perform ontology reasoning for every query, and thus results into many repeated computations. The above two advantages
are especially important for very large knowledge bases, as it is the
case in Semantic Web search. Third, online query processing on the
data resulting from an offline ontology inference step is very close
to current Web search techniques, which also include the offline
construction of a search index, which is then used for rather efficiently performing online query processing. In a sense, offline
ontology inference can be considered as the offline construction
of an ontological index, in addition to the standard index for
Web search. Fourth, our approach reuses existing Web search techniques and can easily be integrated with them, which is generally
not possible for the compilation of T into Q, as this requires a logically correct handling of Boolean operators in search queries; current standard Web search engines, however, are generally lacking
such a handling. Fifth, as another advantage, the compilation of
T into A actually also makes our approach independent from the
underlying ontology language. A disadvantage of our approach to
offline reasoning compared to the compilation of T into Q is that
its result must be updated whenever the TBox T or the ABox A
change, which is, however, less an issue especially when T and
A change only rarely.

In the offline ontology reasoning step, we check whether
the Semantic Web knowledge base is satisfiable, and we compute
the completion of all semantic annotations, i.e., we augment the

semantic annotations with all concept, role, and attribute membership axioms that can be deduced from the semantic annotations
and the ontology. We suggest to use only the so-called simple completion of all semantic annotations, which is sufficient for a large
class of Semantic Web knowledge bases and search queries. It is
important to point out that since ontology reasoning is done offline
(like the construction of an index structure for Web search), its
running time does not contribute to the running time of the actual
online processing of Semantic Web search queries. Thus, the running time used for ontology reasoning can be fully neglected.
Nonetheless, in tractable ontology languages such as DL-LiteA,
checking whether a Semantic Web knowledge base KB is satisfi-
able, and checking whether a membership axiom is a logical consequence of KB for computing the simple completion of KB can both
be done in LOGSPACE in the data complexity, and one can use existing
systems such as QuOnto [12].

In the online reduction to standard Web search, we decompose
a given Semantic Web search query Q into a collection of standard
Web search queries, of which the answers are then used to construct the answer for Q. The standard Web search queries are processed with existing search engines on the Web. Publishing the
completed semantic annotations as standard Web pages, as we
propose here, this standard Web search can be done immediately
with existing standard Web search engines (see Section 7 for an
implementation of semantic desktop search on top of standard
desktop search). Alternatively, we may also keep the completed
semantic annotations in a virtual way only and use them for the
construction of the index structure for Web search only. In that
case, the offline ontology reasoning step can be combined with
the construction of the index structure for Web search.

Note that the terms online and offline are here used in a
computational sense. In the following, we describe the offline
ontology reasoning step in Section 5 and the online reduction to
standard Web search in Section 6. We finally describe the implementation of a semantic desktop search engine in Section 7.

5. Offline ontology compilation

The offline ontology reasoning step compiles the implicit pieces
of terminological knowledge in the TBox of a Semantic Web knowledge base into explicit membership axioms in the ABox, i.e., in the
semantic annotations of Web pages and objects, so that they (in
addition to the standard Web pages) can be searched by standard
Web search engines. This compilation is always correct, also for
other underlying ontology languages different from DL-LiteA. To
also obtain completeness under DL-LiteA, (a) in the case of quanti-
fier-free search queries, (b) when the TBox is equivalent to a Datalog program, and (c) more generally (relative to both) when the
existentially quantified variables in search queries occur only in
safe positions, it is sufficient to add all logically entailed membership axioms constructed from Web pages, Web objects, and data
values. Note that this completeness result assumes that Semantic
Web knowledge bases are defined relative to DL-LiteA.

5.1. Simple completion

We now introduce the notion of simple completion for Semantic Web knowledge bases, which formalizes the compilation of
TBox into ABox knowledge. Informally, for every Web page and object a, all deducible ground membership axioms are collected in a
completed semantic annotation of a. Observe here that the notion
of simple completion of a Semantic Web knowledge base depends
only on its sets of atomic concepts, atomic roles, atomic attributes,
individuals, and data values, and is otherwise independent from
the underlying ontology language.

B. Fazzinga et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 453473

Definition 5. Let KB 14 T ;Aaa2P[O be a satisfiable Semantic Web
knowledge base. The simple completion of KB is the knowledge base
KB0 14 ;;A0
is the set of all concept
membership axioms A(a), role membership axioms P(a, b), and
attribute membership axioms U(a,v)
logically implied by
T [

a2P[OAa, where A 2 A, P 2 RA, U 2 RD, b 2 I, and v 2 V.

aa2P[O where each A0

Example 5. (Scientific Database contd). Consider again the Semantic Web knowledge base KB 14 T ; Aaa2P[O of Example 1. Then,
the simple completion of KB contains in particular the three new
axioms Article(i3), hasAuthor(i3,i2), and Article(i4). The first two are
added to Ai3 and the last one to Ai4.

In general, for any underlying ontology language of a Semantic
Web knowledge base, the simple completion allows for correctly
but not for completely evaluating Semantic Web search queries
(i.e., all answers are correct, but some answers may be missing).
This is because existentially quantified variables in the search
query may refer to incompletely specified existentially quantified
entries in the Semantic Web knowledge base, which thus may
not be connected to concrete individuals and values. Hence, one
case where we easily obtain completeness is when there are no
existential quantifiers in the search query. Towards this result,
the following theorem shows that positive quantifier-free search
queries to a knowledge base KB over DL-LiteA can be evaluated
on the simple completion of KB (which contains only compiled
but no explicit TBox knowledge anymore).

Theorem 1. Let KB be a satisfiable Semantic Web knowledge base
over DL-LiteA, let Q(x) be a positive Semantic Web search query
without existential quantifiers, and let h be a ground substitution for x.
Then, h is an answer for Q(x) to KB iff h is an answer for Q(x) to the
simple completion of KB.

As an immediate consequence, general quantifier-free search
queries to a Semantic Web knowledge base KB over DL-LiteA can
also be evaluated on the simple completion of KB, which is expressed by the following corollary.

Corollary 2. Let KB be a satisfiable Semantic Web knowledge base
over DL-LiteA, Q(x) be a (general) Semantic Web search query without
existential quantifiers, and h be a ground substitution for x. Then, h is
an answer for Q(x) to KB iff h is an answer for Q+(x) but not an answer
for Q(x) to the simple completion of KB.

Another case where we easily obtain completeness is when
there are no incomplete existentially quantified entries in the
Semantic Web knowledge base, which requires to disallow some
concept inclusion axioms, and which actually means that the
knowledge base is equivalent to a Datalog program. This is expressed by the following theorem for positive search queries. Observe that the theorem does not exclude existentially quantified
variables to occur in such queries.

Theorem 3. Let KB be a satisfiable Semantic Web knowledge base
over DL-LiteA such that none of the concept inclusion axioms in KB has
one of the forms B v $P, B v $P, B v d(U), B v $P.C, and B v $P.C.
Let Q(x) be a positive Semantic Web search query, and let h be a
ground substitution for x. Then, h is an answer for Q(x) to KB iff h is an
answer for Q(x) to the simple completion of KB.

It follows immediately that fully general search queries to a
Semantic Web knowledge base KB over DL-LiteA, where the same
concept inclusion axioms are disallowed, can also be evaluated
on the simple completion of KB.

Corollary 4. Let KB be a satisfiable Semantic Web knowledge base over
DL-LiteA such that none of the concept inclusion axioms in KB has one
of the forms B v $P, B v $P, B v d(U), B v $P.C, and B v $P.C. Let
Q(x) be a (general) Semantic Web search query, and let h be a ground
substitution for x. Then, h is an answer for Q(x) to KB iff h is an answer
for Q+(x) but not an answer for Q(x) to the simple completion of KB.

More generally, we also obtain completeness when all existentially quantified variables in search queries occur only in positions
that do not carry any incomplete existentially quantified entry in
the Semantic Web knowledge base. To formalize this result, we
first define the notion of positions and their safeness as follows.
A position F[i] consists of an atomic concept, atomic role, or attribute F and an argument position i = 1, if F is an atomic concept,
and i 2 {1,2}, if F is an atomic role or attribute. A position F[i] is safe
relative to a Semantic Web knowledge base KB 14 T ;Aaa2P[O iff
only Web pages p 2 P, Web objects o 2 O, and values v 2 V occur

in that position in atoms in any universal model of the Datalog
translation of T [
a2P[OAa [11]. The following theorem then formalizes the above idea for the case of positive search queries; it
generalizes Theorems 1 and 3.

Theorem 5. Let KB be a satisfiable Semantic Web knowledge base
over DL-LiteA. Let Q(x) be a positive Semantic Web search query such
that all existentially quantified variables occur only in safe positions,
and let h be a ground substitution for x. Then, h is an answer for Q(x)
to KB iff h is an answer for Q(x) to the simple completion of KB.

As an immediate consequence, this result also carries over to
the case of general search queries to a Semantic Web knowledge
base KB over DL-LiteA. This is expressed by the following corollary,
which generalizes Corollaries 2 and 4.

Corollary 6. Let KB be a satisfiable Semantic Web knowledge base
over DL-LiteA. Let Q(x) be a (general) Semantic Web search query such
that all existentially quantified variables occur only in safe positions,
and let h be a ground substitution for x. Then, h is an answer for Q(x)
to KB iff h is an answer for Q+(x) but not an answer for Q(x) to the
simple completion of KB.

To obtain a sufficient syntactic condition for the safeness of positions relative to KB, we now define the notion of unsafe positions,
which is based on modeling the propagation of existentially quantified entries in the Datalog encoding of KB. Given a Semantic Web
knowledge base KB over DL-LiteA, we define the set of all unsafe positions relative to KB, denoted UKB, inductively as follows:

 UKB contains (1) P[2], (2) P[1], (3) U[2], (4) P[2] and A[1], and (5)
P[1] and A[1] for every concept inclusion axiom B v C in KB such
that C has the form (1) $P, (2) $P, (3) d(U), (4) $P.A, and (5)
$P.A, respectively;

 if UKB contains (1) A1[1], (2) P1[1], (3) P1[2], and (4) U1[1] for B of
the form (1) A1, (2) $P1, (3) 9P
1 , and (4) d(U1), respectively, then
UKB also contains (5) A2[1], (6) P2[1], (7) P2[2], (8) U2[1], (9)
P2[1], and (10) P2[2] for every concept inclusion axiom B v C
in KB such that C has the form (5) A2, (6) $P2, (7) 9P
2 , (8)
d(U2), (9) $P2.A2, and (10) 9P
 if UKB contains (1) P1[1] and (2) P1[2] for Q of the form (1) P1 and
(2) P
1 , respectively, then UKB also contains (3) P2[1] and (4) P2[2]
for every role inclusion axiom Q v R in KB such that R has the
form (3) P2 and (4) P

 if UKB contains (1) P1[1] and (2) P1[2] for Q of the form (1) P

1 and
(2) P1, respectively, then UKB also contains (3) P2[2] and (4) P2[1]
for every role inclusion axiom Q v R in KB such that R has the
form (3) P2 and (4) P

2 :A2, respectively;

2 , respectively;

2 , respectively;

 if UKB contains (1) U1[1] and (2) U1[2] then UKB also contains (1)
U2[1] and (2) U2[2], respectively, for every role inclusion axiom
U1 v U2 in KB.

Informally, the set of all unsafe positions is a superset for the set
of all positions that are not safe, since some of the assumed propagations may not actually occur, because the corresponding rule in
the Datalog program may be inactive due to missing data. This result is formally expressed by the following theorem.

Theorem 7. Let KB be a satisfiable Semantic Web knowledge base
over DL-LiteA. Then, any position that is not unsafe relative to KB is
safe relative to KB.

The following result shows that deciding the satisfiability of
Semantic Web knowledge bases and the logical consequence of
ground atoms can be done in polynomial time in general and in
LOGSPACE in the data complexity.

Theorem 8. Given a Semantic Web knowledge base KB over DL-LiteA,
deciding (a) whether KB is satisfiable and (b) whether a given ground
atom is in the simple completion of KB can both be done in polynomial
time in general and in LOGSPACE in the size of the ABox of KB in the data
complexity.

The next result says that the size of the simple completion of
every semantic annotation in a Semantic Web knowledge base
KB is quadratic in the size of KB in general and linear in the size
of the ABox of KB in the data complexity.
Theorem 9. Let KB 14 T ;Aaa2P[O be a satisfiable Semantic Web
knowledge base over DL-LiteA, let A0, R0
D, P, O, and V0 denote the
sets of all atomic concepts, atomic roles, atomic attributes, Web pages,
Web objects, and values that occur in KB, respectively, and let
KB0 14 ;;A0
aa2P[O be the simple completion of KB. Then, the size of
every A0
Dj 	 jV0j), i.e., it
is quadratic in the size of KB in general and linear in the size of the
ABox of KB in the data complexity.

a with a 2 P [ O is in O(jA0j + jR0

Aj 	 jP [ Oj + jR0

A, R0

It thus follows immediately from the above two theorems that
the simple completion of a Semantic Web knowledge base KB has a
cubic size in the size of KB in general and a quadratic size in the
size of the ABox of KB in the data complexity, and that it can be
computed in polynomial time in the size of KB. These results are
more formally expressed by the following corollary.

Corollary 10. Let KB be a Semantic Web knowledge base over
DL-LiteA. Then, (a) the size of the simple completion KB0 of KB is cubic
in the size of KB in general and quadratic in the size of the ABox of KB
in the data complexity, and (b) computing KB0 can be done in
polynomial time in the size of KB.

In summary, the simple completion of a Semantic Web knowledge base KB has a cubic size in the size of KB in general and a quadratic size in the size of the ABox of KB in the data complexity, and
it can be computed in polynomial time. Furthermore, the simple
completion assures always a correct query processing, and also
guarantees a complete query processing for a large class of Semantic Web search queries. Intuitively, query processing under the
simple completion essentially corresponds to ignoring existentially
quantified entries in the Semantic Web knowledge base that cannot be concretely instantiated by individuals or values, i.e., such
query processing actually only results into a slightly different
semantics of answers. For these reasons, and since completeness
of query processing is actually not that much an issue in an inherently incomplete environment like the Web, we propose to use the
simple completion as the basis of our Semantic Web search.

5.2. HTML encoding

Once the completed semantic annotations are computed, we
encode them as HTML pages, so that they are searchable via standard keyword search. We build one HTML page for the semantic
annotation Aa of each individual a 2 P [ O. That is, for each individual a, we build a page p containing all the atomic concepts whose
argument is a and all the atomic roles/attributes where the first
argument is a.

Observe that this HTML encoding can be done in a way such
that the atomic concepts, atomic roles, atomic attributes, individu-
als, and data values do not mix up with strings that occur on standard Web pages, e.g., by marking the HTML pages that are
representing semantic annotations as such, and considering only
such marked HTML pages during the online processing of Semantic
Web search queries. Alternatively, one can also use a unique identifier (which does not occur elsewhere on the Web) for every ontology as a prefix in the encoding of atomic concepts, roles, and
attributes, as well as individuals and data values.

After rewriting the annotations, also search queries are rewritten to deal with the new syntax of the annotations. Specifically, we
remove all the variables and the brackets. For example, the query
Q(x) = Article(x) ^ yearOfPublication(x,2008) ^ keyword(x,RDF)
is
translated into Article AND yearOfPublication 2008 AND keyword
RDF. In this form, the query can be evaluated by standard Web
search engines, since it is only a conjunction of a keyword and a
phrase.

We rely on the assumption that each Web page and object
a 2 P [ O is associated with an identifier, which uniquely characterizes the individual. Here, we use the HTML address of the
Web pages and objects annotation page as identifier. We employ
the identifiers to evaluate complex queries involving more than
one atomic concept, thus involving several annotations. For
example, consider the query Q(x) of Section 2 and the standard
queries Q 1 14 PhDStudent AND isAuthorOf and Q 2 14 Article AND
\yearOfPublication 2008" obtained from it. To evaluate Q(x), we
submit Q1 and Q2 to a Web search engine, and we collect the results r1 and r2 of the two queries, which are the sets of annotation
pages {i2} and {i4}, respectively. We return the annotation page p
belonging to r1 if there exists an annotation page in r2 that occurs
beside isAuthorOf on p. Since i4 occurs beside isAuthorOf on the
annotation page i2, we thus return i2 as overall query result.

6. Online query processing

We now define simple and safe Semantic Web search queries
and describe how they can be reduced to collections of standard
Web search queries, assuming that each completed semantic annotation of a Web page or object a 2 P [ O is stored on an HTML page
on the Web. We also show how the computation of the ObjectRank
ranking can be reduced to the computation of the standard PageRank ranking.

6.1. Simple search queries

Semantic Web search queries that contain no equalities and
only one free variable, which is the first argument in every atom,
are called simple search queries.

i141Q i;0x ^

Definition 6. A Semantic Web search query is simple iff it has the
form Qx 14
ni
j141not Qi,j(x), where x is a single variable from X, and every Q i;jx 14
i;j with i 2 {1, . . ., n} and
j 2 {0, . . . ,ni} is an equality-free conjunctive formula with x as first
i;j, for
i;jx or /k
argument in all atoms, i.e., either /k
all i 2 {1, . . ., n} and j 2 {0, . . ., ni}.

mj
k141/k
i;j 14 pk

i;j 14 pk

i;jx; tk

B. Fazzinga et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 453473

Example 6. (Scientific Database contd). Query Q1(x) of Example 2 is
simple.

Q 1x 14 9yStudentx ^ not attendsx; y ^ Coursey;
Q 2x 14 9yStudentx ^ not attendsx; y:

Simple Semantic Web search queries can immediately be translated into exactly one variable-free Boolean keyword-based standard Web search query.
Theorem 11. Let KB 14 ;;Aaa2P[O be a Semantic Web knowledge
base. Let Q(x) be a simple Semantic Web search query as in Definition 6.
Then, the set of all answers for Q(x) to KB is given by fh 14 fx=ag j a

i;ja 2 Aa or
i141Ii;0 n 

i;ja; tk
pk

Ii;j 14 fa 2 P [ O j 8k : pk

j141Ii;jg, where

ni

i;j 2 Aag.

Example 7. (Scientific Database contd). Query Q1(x) of Example 2
can be translated into the following variable-free Boolean key-
word-based Web search query:

Scientist ^ not doctoralDegree\oxford university"^
worksFor\oxford university"_
Scientist ^ doctoralDegree\oxford university"^
not worksFor\oxford university":

6.2. Safe search queries

Search queries where all free variables in negated conjunctive
formulas and in equalities also occur in positive atoms are safe
queries. That is, we connect the use of negation to a safeness con-
dition, as usual in databases. They are reduced to collections of
standard atomic Web search queries, one collection for the positive
part, and one for every negative subquery. Due to the safeness, we
retain all results of the positive part that are not matching with any
result of a negative subquery.

Definition 7. A Semantic Web search query Qx 14
i1419yi/ix; yi

is safe iff, for every i 2 {1, . . ., n}, each variable that occurs in an
equality in /i and freely in a negated conjunctive formula also
occurs in a positive atom in /i.

Example 8. (Scientific Database contd). The following Semantic
Web search queries ask for all students who do not attend at least
one existing course (resp., event):

Observe that query Q1(x) is safe, whereas Q2(x) is not, since the variable y does not occur in any positive atom of Q2(x).

We now describe an algorithm for the online reduction of safe
Semantic Web search queries Q to standard Web search queries.
Since such Qs with equalities can easily be reduced to those without (via variable substitutions, if possible at all), we assume w.l.o.g.
that Q is equality-free. Furthermore, we assume w.l.o.g. that Q contains no conjunctive subqueries. So, the algorithm reduces fully
general but safe (and equality-free) Semantic Web search queries
(without conjunctive subqueries) to several standard Web search
queries.
Algorithm SWSearch in Fig. 4 takes as input a Semantic Web
knowledge base KB 14 ;;Aaa2P[O and a safe (equality-free)
Semantic Web search query Q(x), and it returns as output the set
H of all answers h for Q(x) to KB. The main ideas behind it are
informally described as follows. We first decompose the query
Q(x) into the positive subqueries Qi,j(x,yi) with i 2 {1, . . ., n} and
j 2 {0, . . ., ni} whose free variables are among x and yi. Here, Qi,0
(x,yi) stands for the positive part of the i-th disjunct of Q(x), while
the Qi,j(x,yi)s with j > 0 stand for the negative parts of the i-th disjunct of Q(x). We then compute the answers for the positive subqueries Qi,j(x,yi) via Algorithm PositiveSWSearch in Fig. 5 (lines 3 and
5). Thereafter, the result for the i-th disjunct of Q(x) is computed by
removing from the set of all answers for the positive part all tuples
matching with an answer for one of the negative parts (line 6).
Here, t[Ri,j] denotes the restriction of the tuple t to the attributes
of the tuples in Ri,j. Finally, the overall result is computed by projecting the results for all disjuncts onto the free variables x of
Q(x) and unifying the resulting answer sets (line 8).

Algorithm PositiveSWSearch in Fig. 5 computes the set of all
answers for positive Semantic Web search queries. It takes as input
a Semantic Web knowledge base K B 14 ;;Aaa2P[O and a positive
(equality-free) Semantic Web search query Q(x), and it returns as
output the set H of all answers h for Q(x) to KB. We first decompose
the query Q(x) into the subqueries Q ix; y 14
j141/i;j, i 2 {1, . . ., n},
where /i,j = pi,j(ti) or /i,j = pi,j(ti,ti,j), whose free variables are among
x and y. Note that all atoms in such queries have the same term ti
as first argument. We then collect the set Ii of all matching Web
pages and objects in KB for ti as follows. If ti is already a Web page
or object, then Ii = {ti} (line 2), and if ti is a variable, then we collect

ni

Fig. 4. Algorithm SWSearch.

Fig. 5. Algorithm PositiveSWSearch.

in Ii all Web pages and objects a with a matching semantic annotation Aa in KB (line 3). These matching Web pages and objects in Ii
are then used in a look-up step on KB to fill all the matching iden-
tifiers, identifiervalue pairs, and identifieridentifier pairs from
the semantic annotations Aa in KB for all atomic concepts A(ti),
attributes U(ti,ti,j), and roles P(ti,ti,j) in Qi(x,y) into collections of unary and/or binary relations A[ti], U[ti,ti,j], and P[ti,ti,j], respectively
(line 6). These relations are then joined via common variables, indi-
viduals, and values in Qi(x,y), and finally projected to all free variables x in Q(x) (line 8).

The operations in lines 3 and 6 of Algorithm PositiveSWSearch
are realized by a standard Web search and by a look-up on the
Web, respectively. In detail, recall that the semantic annotation Aa
for every Web page and object a 2 P [ O is stored on the Web as
an HTML annotation page. The annotation page for a contains a collection of URIs, namely, the HTML address of as standard Web page,
if a is a Web page, and all standard Web pages mentioning a, if a is a
Web object. In addition, it contains all atomic concepts A such that
KB  A(a), all atomic-attributevalue pairs U v such that
KB  U(a,v), and all atomic-roleidentifier pairs P b such that
KB  P(a,b). Hence, the search in line 3 can be realized by searching
for all the URIs whose pages contain all atomic concepts A, attributes U (resp., U ti,j, if ti,j is a value), and roles P (resp., P ti,j,
if ti,j is an identifier) such that A(ti), U(ti,ti,j), and P(ti,ti,j), respectively,
occur in Qi(x,y), while the operations in line 6 can be realized by
look-ups under the given URIs, collecting all the matching data.

The

following

that

SWSearch

theorem shows

and
PositiveSWSearch in Figs. 4 and 5, respectively, are correct, i.e.,
they return the set of all answers for safe (and equality-free) general and positive Semantic Web search queries, respectively, to
Semantic Web knowledge bases KB 14 T ; Aaa2P[O with T 14 ;.
The theorem holds by the above textual explanations of the two
algorithms.
Theorem 12. Let KB 14 ;;Aaa2P[O be a Semantic Web knowledge
base, and let Q(x) be a safe (and equality-free) Semantic Web search
query. Then, Algorithm SWSearch on KB and Q(x) returns the set of all
answers for Q(x) to KB.

6.3. Ranking answers

The following theorem shows that computing the ObjectRank
ranking can be reduced to computing the PageRank ranking. That
is, using the encoding of semantic annotations as HTML pages on
the Web, the ObjectRank of all Web pages and objects is given by
the PageRank of their HTML pages on the Web.

Theorem 13. Let KB 14 ;;Aaa2P[O be a Semantic Web knowledge
base, let E be a source of rank, and let d be a damping factor. Let the
directed graph GKB = (V,E) be defined by V = P [ O and (u,v) 2 E iff
Pu; v 2 Au. Then, for every u 2 P [ O, the ObjectRank of u relative to
KB is the PageRank of u relative to GKB.

7. Implementation

We have implemented two prototypes (the one described in
[25] and a new one) of Serene for a semantic desktop search engine (in desktop search, it was possible to quickly index 500,000
facts at once). In the following, we report on the new prototype
implementation in the context of this paper. The implementation
is based on the above offline inference technique and a (slightly
more sophisticated) desktop version of
the above online
Semantic Web search (by reduction to standard Web search).
The former uses the deductive database system DLV [38], while
the latter is written in C# (nearly 2000 lines of code) and uses
Microsoft Windows Desktop Search 3.0 (WDS) as external desktop search engine. More concretely, it uses the search index created by WDS, which is queried using an OLE DB connection and
an SQL-like syntax; the template used for an index query is as
follows (as described in the Windows Search 3
 SDK released
by Microsoft):

SELECT System.ItemName
FROM SystemIndex
WHERE freetext(0hpositiveParti0) [AND NOT

freetext(0hnegativeParti0)]w

Here, as usual, the w means that the portion enclosed in squared
brackets is optional and may be repeated as needed.

The prototype uses a slightly more sophisticated version of the
two algorithms in Section 6, yielding improved performances. In
fact, computationally, the most expensive operations are index
queries and the look-up phase. The former are queries over the
index generated by WDS, aimed at finding all the resources containing a set of keywords, while the look-up phase is the scanning
(i.e., parsing) of annotations, needed to search for repeated variables in different annotations to verify the (roles of the) query.
Algorithm SWSearch, in general, recalls PositiveSWSearch a huge
and unnecessary number of times without a precise order, which
causes a huge and unnecessary number of index queries. Further-
more, PositiveSWSearch, after querying the index, during the
look-up phase, performs an exhaustive search of the found anno-
tations, scanning a huge and unnecessary number of annotations.

B. Fazzinga et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 453473

To avoid such a wasting of time, we slightly changed the two
algorithms.

ni

The implemented algorithm first decomposes a given query
Q(x) into the n subqueries Q ix; y 14
j141/i;j, i 2 {1, . . . ,n}, where
/i,j = pi,j(ti) or /i,j = pi,j(ti,ti,j), whose free variables are among x and
y, in the same way as in Algorithm PositiveSWSearch. Note that
also here, all atoms in such queries have the same term ti as first
argument. Then, it divides these subqueries into Qa, containing
the subqueries about concepts for which may be specified attributes but no roles, and Qb, containing the other subqueries, i.e.,
those about concepts for which at least one role is specified. For-
mally, Qa is the set of all Qi such that i 2 {1, . . . ,n} and no pi,j with
j 2 {1, . . . ,ni} is an atomic role, and Qb = {Q1, . . ., Qn}nQa.

Now we can start with the subqueries of Qa, executing index
queries that require a computationally cheap look-up phase: in
fact, these queries can be realized by searching for all the annotations containing all atomic concepts A and attributes U (resp.,
U v, if v is a value) such that A(ti) and U(ti,v), respectively, occur
in a Qi(x,y), thus obtaining one set of resulting annotations for each
executed query. Then, we sort this set on the basis of the sizes of its
elements, and we scan it: for each set element, from resulting
annotations, we extract the URI of the corresponding Web page,
updating, for each annotation the sets of positive and negative
URIs, depending on A (i.e., if it is positive or not). At the end of this
stage, we thus have a partial result with a candidate resulting set of
URIs (i.e., Web pages) and/or forbidden URIs (i.e., those extracted
from resulting annotations corresponding to negated concepts of
Qa).

Regarding the subqueries of Qb, to avoid useless scanning of
annotations, we define the directed graph of Qb as (V,E), where
V is the set of all concepts that are in Qb, and (id1,id2) 2 E iff
Qb contains some p(id1,id2) such that p is an atomic role, and
id1 and id2 are concept identifiers. Here, we assume that the
above directed graph is acyclic, but all the algorithms can be
easily extended to the cyclic case by adding a preprocessing
step, removing some roles and thus the cycles in the query,
and a verification step at the end. Then, exploiting the graph,
we make a new decomposition of the queries of Qb as follows:
component Qb,1 contains the nodes without ingoing edges, and
every component Qb,i with i P 2 contains the nodes with ingoing
edges starting from nodes of Qb,i1.

First, we execute index queries for Qb,1 as described above for
the subqueries of Qa, with the only difference that now the annotations must also contain roles P such that P(id1,id2) occur in Qb,1.
Again, we obtain a set of resulting annotations, which we can sort
on the basis of the sizes of its elements. We scan this set and for
each set element and resulting annotation, we extract the URI of
the corresponding Web page and store it for the next step iff such
URI is between the candidate ones (and/or not between the forbidden ones); if it is the case, we look up the resulting annotations to
find the aforementioned id2s of the P(id1,id2)s of Qb,1.

Then, we update the sets of candidate URIs and forbidden ones,
on the basis of the resulting annotations found, execute index queries for Qb,2 and look up the resulting annotations in order to find
identifiers matching with the id2s. If a resulting annotation has
an identifier that matches with one of the id2s, we store it for
the next step and update the sets of candidate URIs and forbidden
ones. We proceed, in turn, in a similar way for the other partitions
Qb,i. In this way, we improve the performance of the two algo-
rithms, because:

8. Experimental results

In this section, we report on our experimental results with the
two prototype implementations for a semantic desktop search engine (the prototype implementation described in [25] and the new
one), namely, on the size of completed annotations, the running
time of the online query processing step, and the precision and
the recall of our approach to Semantic Web search compared to
Google.

8.1. Size of completed annotations

By Theorem 9, given a Semantic Web knowledge base KB, the
(worst-case) size of every generated completed semantic annotation of KB is in O(jA0j + jR0
A, R0
D,
P, O, and V0 denote the sets of all atomic concepts, atomic roles,
atomic attributes, Web pages, Web objects, and values that occur
in KB, respectively), i.e., it is quadratic in the size of KB in general
and linear in the size of the ABox of KB in the data complexity.

Dj 	 jV0j) (where A0, R0

Aj 	 jP [ Oj + jR0

In practice, since ontological hierarchies are generally not that
deep (intuitively, a concept/role/attribute has generally at most a
dozen superconcepts/-roles/-attributes), the generated completed
semantic annotations are generally even much smaller. To prove
this experimentally, we have measured the size of the generated
completed annotations for some commonly used and standard
benchmark ontologies, namely, for the Adolena, Buildings & Places,
Cell, DOLCE-Lite, Pathway, Pizza, and Zebrafish ontologies from the
TONES Repository,6 for the Finite-State-Machine (FSM), New-Testa-
ment-Names (NTN), Science, and Surface-Water-Model
(SWM)
ontologies from the Protege Ontology Library,7 for the Stock Exchange and Vicodi ontologies [44], for the Finance ontology,8 for
the Lehigh University Benchmark (LUBM) ontology,9 for the SWETO
ontology,10 for the Uniprot (core) ontology,11 and for the University
Ontology Benchmark (UOBM) ontology [41]. We have computed the
completed annotations either for individuals that are already included in the above ontologies, where such (rather realistic) individuals are available (in the majority of cases), or for artificially created
individuals, otherwise. Indeed, the experimental results in Table 1
show that the maximal and average sizes of completed annotations
(i.e., the maximal and average numbers of all ABox axioms, denoted
Max Comp and Avg Comp, respectively) are rather small. Table 1 also
provides for every ontology the DL expressivity (i.e., the underlying
description logic), the size (i.e., the number of all TBox and ABox axi-
oms), the maximal and average depths of property (i.e., role or attri-
bute) hierarchies (denoted Max DPH and Avg DPH, respectively), as
well as the maximal and average depths of concept hierarchies (de-
noted Max DCH and Avg DCH, respectively).

8.2. Efficiency of online query processing

Experiments with our two implemented semantic desktop
search engines (the implementation described in [25] and the
new one) show the principle feasibility of our approach, and that
it scales quite well to very large collections of standard pages,
annotation pages, and background ontologies. The results are summarized in Table 2, which shows in bold the total time (in ms) used
by our new system (including the WDS calls) on a standard laptop
for processing 10 different search queries (Q1, . . ., Q10) on a randomly generated knowledge base (in the context of the running

 we execute a reduced set of index queries;
 we execute a complete scan of an annotation iff it can lead to a

resulting Web page or to a forbidden one;

 we prune the search space whenever possible, updating the sets

of candidate URIs and forbidden ones at each step.

6 http://owl.cs.manchester.ac.uk/repository/
7 http://protegewiki.stanford.edu/index.php/Protege_Ontology_Library
8 http://www.cs.put.poznan.pl/alawrynowicz/financial.owl
9 http://swat.cse.lehigh.edu/projects/lubm/
10 http://knoesis.wright.edu/library/ontologies/sweto/
11 http://dev.isb-sib.ch/projects/uniprot-rdf/owl/

Table 1
Maximal and average sizes of completed annotations for different ontologies.

Ontology

Adolena
Buildings & Places
Cell
DOLCE-Lite
Finance

Pathway
Pizza
Science
Stock Exchange

Uniprot (core)

Vicodi
Zebrafish

DL expressivity
SHI(D)
ALCHIO(D)

ALCHIF
SF (D)

SHIF (D)

ALH(D)
ALCOF (D)
ALCHIOF (D)
SHIN (D)
RDFS(DL)

Size

Max DPH

Avg DPH

Max DCH

Avg DCH

Max Comp

Avg Comp

Scientific Database), consisting of 5000 annotations with 590,027
facts. Notice that this total time (for the decomposition of the
query, for processing all subqueries via WDS, and for the composition of the query results) is very small (all below one second). Table 2 also shows the different numbers of returned pages and
objects. Observe that our new prototype is on the average more
than 130 times quicker than our previous one described in [25]. This
performance increase of the new over the old prototype is due to
several code optimizations, including the direct use of the API of
WDS for querying the search index created by WDS, rather than
a cmdlet script in Microsoft Powershell 1.0. Further dramatic
reductions (even with much larger datasets) can be achieved by
employing a Web search engine (such as Google) rather than a
desktop search engine (since Web search is actually much faster,
even with a much larger search space, especially because it uses
a huge number of processors, differently from desktop search).

The 10 search queries Q1, . . ., Q10 are more concretely given as
follows (where the ais, cis, ois, and uis are either individuals or
values); they ask for all the following individuals (so also yielding
the Web pages containing them):

(1) Professors giving the course c12: Q1(x) = Professor(x) ^ tea-

cherOf (x,c12);
(2) Professors giving the course c12 but not the course c20:
Q2(x) = Professor(x) ^ teacherOf(x,c12) ^ not teacherOf(x,c20);
(3) Scientists working for o12 and authoring a4, or scientists
working for o3 and authoring a25: Q 3x 14 Scientistx^
worksForx; o12 ^ hasWrittenx; a4 _ Scientistx ^ works-
Forx; o3 ^ hasWrittenx; a25;

Table 2
Total time used (in ms) and number of returned URIs for processing the 10 Semantic
Web search queries Q1, . . ., Q10 on a randomly generated Semantic Web knowledge
base with 5000 semantic annotations and 590270 facts.

(4) Scientists working for u but not having a doctoral degree
from u, or scientists having a doctoral degree from u but
Q 4x 14 Scientistx ^ works -
not working
Forx; u ^ not doctoralDegreex; u _ Scientistx ^ doctoral-
Degreex; u ^ not worksForx; u;

for

u:

(5) Professors who are also the head of a department:

Q5(x) = $y(Professor(x) ^ headOf(x,y) ^ Department(y));

(6) Articles with an Italian author

and published in
Q 6x 14 9yArticlex ^ yearOfPublicationx; 2007^

2007:
hasWritteny; x ^ Scientisty ^ nationalityy; italian;

(7) Scientists who are the authors of a journal and a conference
paper published in 2007, or scientists who are the authors of
a book published in 2007: Q 7x 14 9y; zScientistx ^ has -
Writtenx; y ^ JournalPapery ^ yearOfPublicationy; 2007^
hasWrittenx; z ^ ConferencePaperz ^ yearOfPublicationz;
2007 _ 9yScientistx ^ hasWrittenx; y ^ Booky^ yearOf -
Publicationy; 2007;

(8) Italian professors who are not heading any department:
Q 8x 14 Professorx ^ nationalityx; italian ^ not 9yhead -
Ofx; y ^ Departmenty;

(9) Scientists who work for

for no
university from which they have the doctoral degree:
Q 9x 14 9zScientistx ^ worksForx; z ^ Universityz ^ not
9ydoctoralDegreex; y ^ worksForx; y ^ Universityy;

a university, but

(10) Italian scientists who have a non-Italian doctoral degree and
work for an Italian organization, or non-Italian scientists who
have an Italian doctoral degree and work for a non-Italian orga-
nization: Q 10x 14 9y; zScientistx ^ nationalityx; italian^
Universityy^notstatey; italy^ doctoralDegreex; y ^ works-
Forx; z^Organizationz ^ statez; italy_ 9y; zScientistx ^
not nationalityx; italian ^ Universityy ^ statey; italy ^
doctoralDegreex; y ^ worksForx; z ^ Organizationz ^ not
statez; italy:

Query

Total time (ms)

No. URIs

8.3. Efficiency comparison to the Corese system

FoIKS-2010 Prototype

New prototype

Q1(x)
Q2(x)
Q3(x)
Q4(x)
Q5(x)
Q6(x)
Q7(x)
Q8(x)
Q9(x)
Q10(x)

We now compare the running time of query processing in our
new prototype with the running time of query processing in the
Corese system [17], which is the Semantic Web search system in
the literature that is most closely related to our approach. It turns
out that our new prototype is on the average nearly 18 times
quicker than Corese. Note that this difference in the query processing time is partially due to the fact that Coreses ontological inference is performed online (i.e., at query processing time), while the
ontological inference in our approach is done offline. The detailed

B. Fazzinga et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 453473

results are summarized in Table 3, which shows the total time (in
ms) used by Corese and by our new prototype on a standard laptop
for processing ten different search queries (Q1, . . ., Q10) on a randomly generated knowledge base (in the context of the running
Scientific Database), consisting of 5580 annotations with 33,519
facts. In detail, the ten search queries Q1, . . ., Q10 are given as
follows:

(1) Female employees: Q 1x 14 Womanx ^ workx; employee;
(2) Female physicians who are married: Q 2x 14 9yWomanx^

workx; physician ^ hasSpousex; y;

(3) Persons who are not married and have no friends

Q 3x 14 Personx ^ not 9y hasSpousex; y ^ not 9y
hasFriendx; y;

or

female

teachers:

employees:

(4) Employees

(5) Non-married

workx; employee ^ not 9y hasFriendx; y;

workx; employee ^ not 9y hasSpousex; y;

workx; employee _ Personx ^ workx; teacher;

Q 4x 14 Personx^
Q 5x 14 Womanx^
(6) Female employees without friends: Q 6x 14 Womanx^
(7) Bettinas friends: Q 7x 14 Personx ^ hasFriendx; Bettina;
(8) Married persons who have friends and work, but not
as employees: Q 8x 14 9y; z; z0Personx ^ hasSpousex; y^
hasFriendx; z ^ worky; z0 ^ not worky; employee;
(9) Married men who are 40 years old: Q 9x 14 9yManx^
(10) Men who work and have no friends: Q 10x 14 9yManx^

agex; 40 ^ hasSpousex; y;

workx; y ^ not 9z hasFriendx; z:

Note that Corese has its own query syntax, which is based on
RDF(S). For example, the search query Q1(x) is expressed by the following query in Corese:

select ?x display xml
where {?x rdf:type animals:Woman.

?x animals:work ?y
FILTER (?y=employee)}

8.4. Precision and recall of Semantic Web search

Differently from conventional Boolean keyword-oriented Web
search, the proposed Semantic Web search clearly empowers the
user to precisely describe her information need for certain kinds
of queries, resulting in a very precise result set and a very high precision and recall [2] for the query result. In particular, in many
cases, Semantic Web search queries exactly describe the desired
answer sets, resulting into a precision and a recall of 1. Some

Table 3
Total time used (in ms) by Corese and by our new prototype, along with the number
of returned URIs, for processing the Semantic Web search queries Q1, . . ., Q10 on a
randomly generated Semantic Web knowledge base with 5580 semantic annotations
and 33,519 facts.

Query

Total time (ms)

No. URIs

Corese

New prototype

Q1(x)
Q2(x)
Q3(x)
Q4(x)
Q5(x)
Q6(x)
Q7(x)
Q8(x)
Q9(x)
Q10(x)

examples of such Semantic Web search queries (addressed to the
CIA World Fact Book12 relative to the WORLD-FACT-BOOK ontology),13
which have a precision and a recall of 1 in our approach to Semantic
Web search, are shown below, along with corresponding Google
queries:

common border with Austria:
border

(1) Countries having

(2) Countries

Q 1x 14 Countryx ^ borderCountriesx; Austria;
countries Austria;
having

exports
Q 2x 14 Countryx ^ exportsPartnersx; Bulgaria;
partners Bulgaria;

Bulgaria

as

partners:
exports -

(3) Countries

in which

Italian

Countryx ^ languagesx; Italian; languages Italian;

(4) Countries

importing

tobacco:

importsCommoditiesx; tobacco;
tobacco;

is

spoken:

Q 3x 14
Q 4x 14 Countryx^
imports - commodities

(5) Countries exporting tobacco and in which French is spo-
ken: Q 5x 14 Countryx ^ exportsCommoditiesx; tobacco^
languagesx; French; exports - commodities tobacco languages French;
spoken: Q 6x 14
Q 7x 14

Countryx ^ not languagesx; Italian; languages -Italian;

in which Italian is not

(6) Countries

(7) Countries

in which

Countryx ^ languagesx; Arabic; languages Arabic;

spoken:

Arabic

is

(9) Countries

(8) Countries in which Arabic is spoken and not English:
Q 8x 14 Countryx ^ languagesx; Arabic ^ not languagesx;
English; languages Arabic -English;
Q 9x 14
Countryx^importsCommoditiesx;tobacco^importsCommo-
ditiesx;food; imports - commodities tobacco food;
food: Q 10x 14
Countryx ^ importsCommoditiesx; tobacco ^ not imports-
Commoditiesx; food;imports - commodities tobacco -food.

(10) Countries importing tobacco and not

importing

tobacco

food:

and

The precision and the recall of the above 10 Google queries compared to their Semantic Web search queries are shown in Table 4.
Observe that the Google queries often cannot that precisely describe
the desired answer sets, and are thus often resulting into a precision
and a recall much below 1. Note that this lower precision and recall is
due to the limited expressivity of Google queries, and not due to
some incomplete indexing; this is especially obvious for queries that
use the negation: Googles recall is always 1 in positive queries, and
it is less than 1 in queries containing at least one negated predicate
having a keyword as value; the lower recall in the latter case is because Google discards all the pages containing the keyword, including those where the keyword does not refer to the specified
predicate, e.g., when processing the query languages -Italian, Google discards all the pages containing Italian, including those where
Italian does not refer to languages; however, Googles precision
is often 1 for such queries with negated predicates, since all the returned pages are in general also answers to the queries.

9. Semantic Web search on the Internet Movie Database

In this section, we show that our approach to Semantic Web
search can be readily applied to existing Web pages, even if they
are currently not yet semantically annotated. More specifically,
we show how our approach can be used to perform a vertical
ontology-based search on the Web pages of the Internet Movie
Database (IMDB).14 To this end, the necessary semantic annotations

12 http://www.cia.gov/library/publications/the-world-factbook/
13 http://www.ontoknowledge.org/oil/case-studies/
14 http://www.imdb.com

Table 4
Precision and recall of Google vs. Semantic Web search (SWS).

Query

Q1(x)
Q2(x)
Q3(x)
Q4(x)
Q5(x)
Q6(x)
Q7(x)
Q8(x)
Q9(x)
Q10(x)

Results Google

Correct Results

Correct Results Google

Precision Google

Recall Google

Precision SWS

Recall SWS

are automatically constructed from the IMDB Web pages. That is, we
are actually mapping the IMDB Web pages into an ontological
knowledge base, which then allows for processing Semantic Web
search queries in the query language of the underlying ontology.
Intuitively, such an ontological knowledge base can be considered
as an ontological index over the IMDB, against which ontological
conjunctive search queries on the IMDB can be answered. So, our
vertical search on the IMDB works on an ontologically structured
copy of the IMDB without actually changing it.

We considered a sample set of more than 60,000 movies and actors of the IMDB. To avoid a manual annotation of Web pages, we
automatically extracted the annotations for all the movies and actors in our sample set via the wrapper SCRAP [24].

SCRAP is able to extract pieces of information from HTML pages
and to reorganize them into new XML documents. To accomplish
this task, SCRAP uses a set of extraction rules and an extraction sche-
ma. The extraction rules are XPath expressions identifying the portion of the HTML pages to be extracted, while the extraction
schema is a document type definition (DTD) specifying the structure
of the output XML document and associating each element type with
an extraction rule. Thus, an extraction rule is used to define the path
in the input HTML page that locates the text to be returned as the
content of an XML element. In our context, this feature will be
exploited for the purpose of annotation by using XML tag names that
describe the semantics of the pieces of information extracted.

The process of generating the annotations was divided into the
following two phases. In the first phase, by using SCRAP, we extracted the information of the HTML pages to be inserted in the
annotations (such as movie titles, actor names, etc.), and returned
it in the form of XML documents conforming to the specified DTD.
In the second phase, we then further processed these XML documents and translated them into documents conforming to our
annotation syntax.

In more detail, in the first phase, we used a visual tool provided
by SCRAP to define the schemas for extracting the information
about movies and actors. In particular, the extraction schema for
movies consisted of the following DTD:

<!ELEMENT doc (Movie)>
<!ELEMENT Movie (title,director,creator,writer,
genre, language,country?,releaseDate?,awards?,
star)>
<!ELEMENT title (#PCDATA)>
<!ELEMENT director (#PCDATA)>
<!ELEMENT creator (#PCDATA)>
<!ELEMENT genre (#PCDATA)>
<!ELEMENT writer (#PCDATA)>
<!ELEMENT star (#PCDATA)>
<!ELEMENT country (#PCDATA)>
<!ELEMENT releaseDate (#PCDATA)>
<!ELEMENT awards (#PCDATA)>
<!ELEMENT language (#PCDATA)>

The semantics of this extraction schema is that, for every HTML
page describing a movie, an XML document must be generated
containing the title of the movie, the director, the country, the
names of all the actors of the movie, etc. By means of an analogous
extraction schema, suitable for every HTML page describing an
actor, we specified that an XML document must be generated
which contains the name of the actor, the year of birth, the city
of birth, the titles of all the movies starring her, etc. The tag names
and the structure of the extraction schema were defined according
to the IMDB ontology.15 Exploiting SCRAP facilities, we also associated each element type of the extraction schemas with an appropriate extraction rule. For instance, we associated the element type
title of the above-reported DTD with the following XPath extraction rule: html/head/title/text(), which captures the text contained in the specified path of the HTML pages.

After defining the extraction schemas and the associated set of
extraction rules, we ran SCRAP on the sample set from the IMDB,
thus obtaining a set of XML documents, each containing the desired information about a movie or an actor. For instance, consider
the HTML page shown in Fig. 6. The XML document obtained by
running SCRAP on it is shown in Fig. 7.

In the second phase, we took each XML document returned by
SCRAP in the first phase, and automatically transformed it into a final annotation, conforming to our syntax. The final annotation obtained from the HTML of Fig. 6 is shown in Fig. 8.

We point out that, since all the HTML pages describing movies
in the IMDB site share the same structure, the set of extraction
rules was defined looking only at a sample page, and then used
to support the extraction from all the other pages. The same strategy was used for actor pages. This way, we dramatically reduced
the overall human effort needed for the annotation task. In fact,
the only human work to be done was the definition of extraction
schemas and rules, thus not requiring any user to be called for
annotating HTML pages one by one.

The following are some Semantic Web search queries, which we

processed in our Semantic Web search interface for the IMDB:

(1) Comedy movies in English that were nominated for the Oscar:
Q 1x 14 Moviex ^ genrex; Comedy ^ languagex; English ^
awardsx; NominatedforOscar;
(2) Comedy movies directed either by Frank Capra or Woody Allen:
Q 2x 14 Moviex ^ genrex; Comedy ^ directorx; Frank -
Capra _ directorx; WoodyAllen;

(3) Actors born in 1964 or in New York who are still alive:
Q 3x 14 Actorx ^ yearOfBirthx; 1964_cityOfBirthx; NewYork -
City ^ not 9yyearOfDeathx; y;

(4) Crime movies in English with an award and Nicolas Cage as a star:
Q 4x 14 Moviex ^ genrex; Crime ^ languagex; English ^
9y awardsx; y ^ starx; NicolasCage;

15 http://www.csd.abdn.ac.uk/ggrimnes/dev/imdb/IMDB.rdfs

B. Fazzinga et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 453473

Fig. 6. An excerpt of a movie page.

Fig. 7. XML document output of SCRAP.

Fig. 8. Final annotation.

(5) American movies with Julia Roberts and not Clive Owen
Q 5x 14 Moviex ^ countryx; USA ^ starx;

as
JuliaRoberts ^ not starx; CliveOwen;

star:

star:

(6) Movies not directed by Julien Temple and with Monica Bellucci as
Q 6x 14 Moviex ^ not directorx; JulienTemple^

starx; MonicaBellucci;
(7) Actors who played in at least one comedy and no western
movie: Q 7x 14 9yActorx ^ filmx; y ^ Moviey ^ genrey;
Comedy ^ not 9zMoviez ^ genrez; Western ^ filmx; z;
least one movie in Italian:
Q 8x 14 9yActorx ^ filmx; y ^ Moviey ^ languagey;
Italian;
(9) Actors who played in no movie in French: Q 9x 14
Actorx ^ not 9zMoviez ^ languagez; French ^ filmx; z;

(8) Actors who played in at

(10) Drama movies without

genrex; Drama ^ not genrex; Thriller:

thrillers: Q 10x 14 Moviex^

The total processing times (used by the new prototype implementation described in Section 7 on a standard laptop) and the numbers
of returned URIs for the above Semantic Web search queries are
shown in Table 5. Note that most of the search queries had a total
processing time below 1 s; only those with large outputs were taking slightly more time (some few up to six seconds).

It is also important to point out that the precision and the recall
of our approach to Semantic Web search for the above 10 Semantic
Web search queries are both 1, as the search queries describe exactly their natural-language descriptions. That is, in the IMDB
application scenario, our approach to Semantic Web search is both

Table 5
Total time used (in ms) and number of returned URIs for processing the Semantic
Web search queries Q1, . . ., Q10 on a sample set of Web pages extracted from the
IMDB, which included more than 60,000 movies and actors, resulting into more than
one million facts.

Query

Q1(x)
Q2(x)
Q3(x)
Q4(x)
Q5(x)
Q6(x)
Q7(x)
Q8(x)
Q9(x)
Q10(x)

Total time (ms)

No. URIs

sound and complete, and a precision and recall different from 1 for
the above 10 search queries would mean that the prototype implementation contains some errors.

10. Related work

We now discuss related work on Semantic Web search (see
especially [26] for a recent survey), which can roughly be divided
into (1) approaches that are based on structured query languages,
such as [17,28,33,35,42,43,47], and (2) approaches for naive users,
requiring no familiarity with structured query languages.
In
this
category, we distinguish keyword-based approaches,
such as [13,31,32,37,48,49,52], where queries consist of lists of
keywords, and natural-language-based approaches,
such as
[16,21,27,30,39,40], where users can express queries in natural
language. Finally, we also discuss related work on ranking techniques for the Semantic Web.

In order to evaluate user queries on Semantic Web documents,
both keyword-based and natural-language-based approaches need
a reformulation phase, where user queries are transformed into
semantic queries. In keyword-based approaches, query processing generally starts with the assignment of a semantic meaning
to the keywords, i.e., each keyword is mapped to an ontological
concept (property, entity, class, etc.). Since each keyword can
match a class, a property, or an instance, several combinations of
semantic matchings of the keywords are considered, and, in some
cases, the user is asked for choosing the right assignment. Simi-
larly, natural-language-based approaches focus mainly on the
translation of queries from natural language to structured lan-
guages, by directly mapping query terms to ontological concepts
or by using some ad-hoc translation techniques.

In the following, we discuss some approaches based on structured query languages, which are most closely related to ours.
We first focus on some more general approaches [17,33,35] closest
in spirit to ours in that they aim at providing general semantic
search facilities. We then discuss some proposals [28,47,42,43]
that address some specific aspects of semantic search or that are
targeted at specific domains, so that they cannot be strictly viewed
as semantic search engines.

The Corese system [17] is an ontology-based search engine for
the Semantic Web, which retrieves Web resources that are annotated in RDF(S) via a query language based on RDF(S). It is the system that is perhaps closest in spirit to our approach. In a first
phase, Corese translates annotations into conceptual graphs, it
then applies proper inference rules to augment the information
contained in the graphs, and finally evaluates a user query by projecting it onto the annotation graphs. The Corese query language is
based on RDF, and it allows variables and operators.

SHOE [33] is one of the first attempts to semantically query the
Web. It provides the following: a tool for annotating Web pages,

allowing users to add SHOE markup to a page by selecting ontolo-
gies, classes, and properties from a list; a Web crawler, which
searches for Web pages with SHOE markup and stores the information in a knowledge base (KB); an inference engine, which provides
new markups by means of inference rules (basically, Horn clauses);
and several query tools, which allow users to pose structured queries against an ontology. One of the query tools allows users to
draw a graph in which nodes represent constant or variable in-
stances, and arcs represent relations. To answer the query, the system retrieves subgraphs matching the user graph. The SHOE search
tool allows users to pose queries by first choosing an ontology from
a drop-down list and next choosing classes and properties from another list. Finally, the system builds a conjunctive query, issues the
query to the KB, and presents the results in a tabular form.

NAGA [35] provides a graph-based query language to query the
underlying KB encoded as a graph. The KB is built automatically by
a tool that extends the approach proposed in [46] and extracts
knowledge from three Web sources: Wordnet, Wikipedia, and
IMDB. The nodes and edges in the knowledge graph represent entities and relationships between entities, respectively. The query
language is based on SPARQL, and adds the possibility of formulating graph queries with regular expressions on edge labels, but the
language does not allow queries with negation. Answers to a query
are subgraphs of the knowledge graph matching the query graph
and are ranked using a specific scoring model for weighted labeled
graphs.

Comparing the above three approaches to ours, in addition to
the differences in the adopted query languages (in particular, SHOE
and NAGA do not allow complex queries with negation) and underlying ontology languages, there is a strong difference in the queryprocessing strategy. Indeed, Corese, SHOE, and NAGA all rely on
building a unique KB, which collects the information disseminated
among the data sources, and which is suitably organized for query
processing via the adopted query language. However, this has a
strong limitations. First, representing the whole information
spread across the Web in a unique KB and efficiently processing
each user query on the thus obtained huge amount of data is a
rather challenging task. This makes these approaches more suitable for specific domains, where the amount of data to be dealt
with is usually much smaller. In contrast, our approach allows
the query processing task to be supported by well-established
Web search technologies. In fact, we do not evaluate user queries
on a single KB, but we represent the information implied by the
annotations on different Web pages, and evaluate queries in a distributed way. Specifically, user queries are processed as Web
searches over completed annotations. We thus realize Semantic
Web search by using standard Web search technologies as wellestablished solutions to the problem of querying huge amounts
of data. Second, a closely related limitation of query processing
in Corese, SHOE, and NAGA is its tight connection to the underlying
ontology language, while our approach is actually independent
from the ontology language and works in the same way for other
underlying ontology languages.

Swoogle [28] is a crawler-based system for discovering, index-
ing, and querying RDF documents. Swoogle mainly provides a
search for Semantic Web documents and terms (i.e., the URIs of
classes and properties). It allows users to search for all instance
data about a specified class, or on a specified subject, and to specify
queries containing conditions on the document-level metadata
(i.e., queries asking for documents having .rdf as the file exten-
sion), but it does not allow complex queries. Retrieved documents
are ranked according to a ranking algorithm measuring the documents importance on the Semantic Web.

ONTOSEARCH2 [47] is a search and query engine for ontologies
on the Semantic Web. It stores a copy of the ontologies in a tractable description logic and allows SPARQL queries to be evaluated on

B. Fazzinga et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 453473

both the structures and instances of ontologies. The Coraal system
[42] is a knowledge-based search engine for biomedical literature.
Coraal uses NLP-based heuristics to process texts and build RDF triples from them. These RDF triples are integrated with existing domain knowledge and all the collected information can be queried
by the user via a specific query language.

The aim of the approach proposed in [43] is approximately querying RDF datasets with SPARQL [50]. To this end, a SPARQL query
is encoded as a set of triple constraints with variables, and an
approximate answer is a substitution of the variables with data
that may not satisfy all the constraints. The proposed strategy refines the accuracy of the answers progressively, so that the algorithm searching for the answers can be stopped at any time
producing a meaningful result.

There is a plethora of ranking techniques for the Semantic Web;
here, we discuss only some most closely related ones. Also the
ObjectRank approach in [3] adds link semantics to the PageRank
technique, but it requires weights for different forms of links before its application. Such weights are assigned by experts and influence the random walk of users. Beagle++ [14] extends the Beagle
desktop search engine, applying ObjectRank to RDF metadata
about desktop objects for an improved ranking in desktop search.
TripleRank [29] also considers the semantics of links, but it does
not rely on an expert assignment of link weights, and is based on
the HITS technique [36] instead of PageRank. Our ObjectRank tech-
nique, in contrast, is conceptually simpler, and it can be easily reduced to the standard PageRank technique in the context of our
approach to Semantic Web search (cf. Theorem 13).

11. Conclusion

We have presented a novel approach to Semantic Web search,
which allows for a semantic processing of Web search queries relative to an underlying ontology, and for evaluating ontology-based
complex Web search queries that involve reasoning over the Web.
We have shown how the approach can be implemented on top of
standard Web search engines and ontological inference technolo-
gies. We have developed the formal model behind this approach,
and we have also generalized the PageRank technique to our ap-
proach. We have then provided a technique for processing Semantic Web search queries, which consists of an offline ontological
inference step and an online reduction to standard Web search
queries (which can be implemented using efficient relational database technology), and we have proved it ontologically correct (and
in many cases also ontologically complete). The offline inference
compiles terminological knowledge into completed annotations,
which have a polynomial size, can be computed in polynomial
time, and are also rather small in practice. We have reported on
two prototype implementations in desktop search, and provided
very positive experimental results on the running time of the online query processing step, and the precision and the recall of our
approach to Semantic Web search. We have also shown that our
Semantic Web search can be readily applied to existing Web pages
without annotations. More specifically, we have implemented a
Semantic Web search interface for the Internet Movie Database.

In a companion work [1820], we have explored a variant of our
Semantic Web search, which uses inductive reasoning techniques
(based on similarity search for retrieving the resources that likely
have a query property), rather than deductive ones in the offline
ontological inference step. This adds an increased robustness, as
it allows for handling inconsistencies, noise, and incompleteness
in Semantic Web knowledge bases. Furthermore, inductive reasoning allows to infer new (not logically deducible) knowledge (from
training individuals). The main idea behind the inductive approach
in [1820] is also closely related to the idea of using probabilistic

ontologies to increase the precision and the recall of querying databases and of information retrieval in general. However, rather
than learning probabilistic ontologies from data, representing
them, and reasoning with them, the inductive approach directly
uses the data in the inductive inference step.

In the future, we aim especially at extending the desktop implementation to a real Web implementation, using existing Web
search engines, and to more deeply investigate the properties of
the ObjectRank technique. Another interesting topic for future research is to explore how search expressions that are formulated
as plain natural language sentences can be translated into the
ontological conjunctive queries of our approach. It would also be
interesting to investigate whether our approach to Semantic Web
search can be combined with top-k query techniques from databases for a further improved performance. Finally, another interesting topic is to explore the use of probabilistic ontologies rather
than classical ones.

Acknowledgments

This work was supported by the European Research Council under the EUs 7th Framework Programme (FP7/20072013)/ERC
Grant 246858  DIADEM, by the German Research Foundation
(DFG) under the Heisenberg Programme, and by a Yahoo! Research
Fellowship. Georg Gottlob is a James Martin Senior Fellow. The
work was carried out in the context of the James Martin Institute
for the Future of Computing.

We are very grateful to Giorgio Orsi for his generous help in
providing the experimental results shown in Table 1. Many thanks
also to the reviewers of this paper and its FoIKS-2010 abstract for
their useful and constructive comments, which have helped to improve this work.

Appendix A. Description logics

As underlying ontology language, we use the tractable description logic DL-LiteA [45], which adds datatypes to a restricted combination of the tractable description logics DL-LiteF (also called DL-
Lite) and DL-LiteR. All these description logics belong to the DL-Lite
family [12], which are a class of restricted description logics for
which the main reasoning tasks are feasible in polynomial time
in general and some of them even in LOGSPACE in the data complex-
ity. The DL-Lite description logics are fragments of OWL and the
most common tractable ontology languages in the Semantic Web
context. In particular, DL-LiteR provides the logical underpinning
for the OWL 2 profile QL, and it is also able to fully capture the
(DL fragment of) RDF Schema [7], the vocabulary description language for RDF; see [23] for a translation. The DL-Lite description
logics are especially directed towards data-intensive applications,
and they can all be translated into Datalog
0 [11,10]. We now briefly
recall the syntax and the semantics of DL-LiteA. For more details on
description logics in general, we refer the reader to [1].

Intuitively, description logics model a domain of interest in
terms of concepts and roles, which represent classes of individuals
and binary relations between classes of individuals, respectively. A
knowledge base encodes especially subset relationships between
concepts, subset relationships between roles, the membership of
individuals to concepts, and the membership of pairs of individuals
to roles. As an important ingredient, DL-LiteA also allows for datatypes in such pieces of knowledge.

A.1. Syntax

As for the elementary ingredients of DL-LiteA, let D be a finite set
of atomic datatypes d, which are associated with pairwise disjoint

sets of data values Vd. Let A, RA, RD, and I be pairwise disjoint sets of
atomic concepts, atomic roles, atomic attributes, and individuals,
respectively, and let V =
d2DVd. From these, roles, concepts, attri-
butes, and datatypes are then constructed as follows:

 A basic role Q is either an atomic role P 2 RA or its inverse P. A
(general) role R is either a basic role Q or the negation of a basic
role :Q.

 A basic concept B is either an atomic concept A 2 A, or an existential restriction on a basic role Q, denoted $Q, or the domain
of an atomic attribute U 2 RD, denoted d(U). A (general) concept
C is either the universal concept >C, or a basic concept B, or the
negation of a basic concept :B, or an existential restriction on a
basic role Q of the form $Q.C, where C is a concept.

negation of an atomic attribute :U.

 A (general) attribute V is either an atomic attribute U 2 RD or the
 A basic datatype E is the range of an atomic attribute U 2 RD,
denoted q(U). A (general) datatype F is either the universal datatype >D or an atomic datatype.

An axiom is an expression of one of the following forms: (1)
B v C (concept inclusion axiom), where B is a basic concept, and C
is a concept; (2) Q v R (role inclusion axiom), where Q is a basic role,
and R is a role; (3) U v V (attribute inclusion axiom), where U is an
atomic attribute, and V is an attribute; (4) E v F (datatype inclusion
axiom), where E is a basic datatype, and F is a datatype; (5) (functQ)
(role functionality axiom), where Q is a basic role; (6) (functU) (attri-
bute functionality axiom), where U is an atomic attribute; (7) A(a)
(concept membership axiom), where A is an atomic concept and
a 2 I, (8) P(a,b) (role membership axiom), where P is an atomic role
and a,b 2 I; (9) U(a,v) (attribute membership axiom), where U is an
atomic attribute, a 2 I, and v 2 V.
Note that concept inclusion axioms of the form B v >C and datatype inclusion axioms of the form q(U) v >D can be safely ignored,
and that concept inclusion axioms of the form B v $Q.C can be expressed by the two concept inclusion axioms B v $Q.A and A v C,
where A is a fresh atomic concept.

We next define knowledge bases, which consist of a restricted
finite set of inclusion and functionality axioms, called TBox, and
a finite set of membership axioms, called ABox. We also define
queries to such knowledge bases. Formally, a TBox is a finite set
T of inclusion and functionality axioms such that every identifying
property in T is primitive (see [45] for a definition of primitive
identifying properties). An ABox A is a finite set of membership axi-
oms. A knowledge base KB 14 T ;A consists of a TBox T and an
ABox A. A query / is an open formula of first-order logic with
equalities. A conjunctive query is of the form $y/(x,y), where / is
a conjunction of atoms and equalities with free variables among

x and y. A union of
the form
i1419yi/ix; yi, where each /i is a conjunction of atoms and equalities with free variables among x and yi.

conjunctive queries

is of

Example 9. (Scientific Database). Continuing the running example
of Section 2, we use a knowledge base KB 14 T ;A in DL-LiteA to
specify some simple information about scientists and their publi-
cations. The sets of atomic concepts, atomic roles, atomic attri-
butes, individuals, and data values are defined as in Example 9.
Then, a TBox T and an ABox A are given by the axioms in Eqs. (1)
and 2, respectively, while the query Q(x) of Eq. 3 is actually a
conjunctive query.

A.2. Semantics

The semantics of DL-LiteA is defined in terms of standard first-
;	I con-

order interpretations as follows. An interpretation I 14 DI

O; DI

d2D DI

O 
 DI

d, where the DI

V, which is the disjoint
sists of (i) a nonempty domain DI 14 DI
union of the domain of objects DI
O and the domain of values
V 14

d s are pairwise disjoint domains of values for the datatypes d 2 D, and (ii) a mapping 	I that assigns to
each datatype d 2 D its domain of values DI
d, to each data value
v 2 Vd an element of DI
d (such that v  w implies vI wI ), to each
atomic concept A 2 A a subset of DI
O, to each atomic role P 2 RA a
O, to each atomic attribute P 2 RD a subset of
subset of DI
O 
 DI
V , to each individual a 2 I an element of DI

O (such that
a  b implies aI bI ). Note that different data values (resp., indi-
viduals) are associated with different elements of DI
V (resp., DI
O)
(unique name assumption). The extension of 	I to all concepts, roles,
attributes, and datatypes, and the satisfaction of an axiom a in
I 14 DI
;	I, denoted I  a, are defined as usual [45]. The interpretation I satisfies the axiom a, or I is a model of a, iff I  a. The
interpretation I satisfies a knowledge base KB 14 T ;A, or I is a
model of KB, denoted I  KB, iff I  a for all a 2 T [ A. We say
KB is satisfiable (resp., unsatisfiable) iff KB has a (resp., no) model.
An axiom a is a logical consequence of KB, denoted KB  a, iff every
model of KB satisfies a. An answer for a query / to KB is a ground
substitution h for all free variables in / such that /h is a logical consequence of KB.

As shown in [45], in particular, deciding the satisfiability of
knowledge bases in DL-LiteA and deciding logical consequences
of membership axioms from knowledge bases in DL-LiteA can both
be done in LOGSPACE in the size of the ABox in the data complexity
(where only the ABox is variable, but the rest is fixed).

Example 10. (Scientific Database contd). It is not difficult to verify
that the knowledge base KB 14 T ; A of Example 9 is satisfiable,
and that the two axioms JournalPaper v :ConferencePaper and
hasAuthor(i3,i2) are logical consequences of KB.
Informally, KB
implies that no journal paper is a conference paper, and that i3 has
the author i2. Furthermore, the ground substitution h = {x/i2} is an
answer for the conjunctive query Q(x) of Example 9. Informally,
mary is a Ph.D. student who has published an article in 2008 with
RDF as a keyword.

Appendix B. Appendix B: Proofs

Proof of Theorem 1. Recall that the ground substitution h is an answer for Q(x) = q1(x) ^ 			 ^ qm(x) to KB iff Q(xh) is a logical consequence of KB. The latter is equivalent to all qi(xh) with i 2 {1, . . . ,m}
being a logical consequence of KB, which in turn is equivalent to all
qi(xh) with i 2 {1, . . ., m} being in the simple completion of KB. That
is, Q(xh) is a logical consequence of the simple completion of KB.
That is, h is an answer for Q(x) to the simple completion of KB. h

Proof of Theorem 3. As shown in [11], every knowledge base KB in
DL-LiteA can be translated into a program PKB in Datalog. Suppose
that no concept inclusion axiom in KB has one of the forms B v $P,
B v $P, B v d(U), B v $P.C, and B v $P-.C. Then, the resulting PKB is
simply a Datalog program, and any universal model for evaluating
the conjunctive query Q(x) relative to PKB (i.e., also relative to KB) is
given by the simple completion of KB. That is, h is an answer for
Q(x) to KB iff h is an answer for Q(x) to the simple completion of
KB. h

Proof of Theorem 5. Consider the Datalog encoding PKB of KB [11].
Then, (i) h is an answer for Q(x) to KB iff h is an answer for Q(x) to
PKB, (ii) deciding whether h is an answer for Q(x) to PKB can be evaluated on any universal model of PKB, and (iii) only Web pages p 2 P,
Web objects o 2 O, and values v 2 V occur in safe positions in facts
in any universal model of PKB. Hence, deciding whether h is an
answer for Q(x) to PKB (and thus to KB) can actually already be

B. Fazzinga et al. / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 453473

evaluated on the simple completion of KB, which is a subset of
every universal model of PKB. h

Proof of Theorem 7 (sketch). A universal model of the Datalog
encoding PKB of KB [11] can be constructed via the chase, which follows the rules of PKB, and generates and propagates existentially
quantified entries in the same way as described above in the definition of unsafe positions relative to KB. h

Proof of Theorem 8. Immediate by the observation that KB corresponds to a knowledge base in DL-LiteA, and that deciding satisfiability and logical consequences of ground atoms for such
knowledge bases can both be done in polynomial time in general
and in LOGSPACE in the size of the ABox in the data complexity. h

Proof of Theorem 9. The Datalog encoding PKB of KB [11] is safe in
that the Web pages p 2 P, Web objects o 2 O, and values v 2 V in
any universal model must already occur in PKB, i.e., in the ABox
of KB. Hence, every A0
a in the simple completion of KB has at most
jA0j + jR0
Aj 	 jP [ Oj + jR0
Dj 	 jV0j elements. h

i141Q i;0x to KB but not an answer for

i141Ii;0 \ 

i;j 2 Aag.

Proof of Theorem 11. Recall that the ground substitution h = {x/a}
with a 2 P [ O is an answer for Q(x) to KB iff h is an answer

j141Q i;jx
ni
for
to KB. Since the TBox of KB is empty, the latter is equivalent
to a 2
j141Ii;j, where Ii;j 14 fa 2

P [ O j 8k : pk
i141Ii;0n

That

j141Ii;j. Observe also that any ground substitution h = {x/v} with
v 2 V cannot be an answer for Q(x) to KB, since x is the first argument in all atoms of Q(x). h

i141Ii;0 but not a 2

i;ja 2 Aa or pk

i141Q i;0x ^

i;ja; tk

a 2

is,

ni

ni

Proof of Theorem 13. Recall that the PageRank R(u) of a node u 2 V
relative to GKB is defined as follows:
Ru 14 d 	
Rv=Nv  1  d 	 Eu;

v2Bu

where (i) Bu is the set of nodes that point to u, and (ii) Nv is the number of links from v [8]. This already shows that the PageRank of u
coincides with the ObjectRank of u (see Eq. (4)), for all Web pages
and objects u 2 P [ O. h
