Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 99112

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

j o u r n a l h o m e p a g e : h t t p : / / w w w . e l s e v i e r . c o m / l o c a t e / w e b s e m

BNOSA: A Bayesian network and ontology based semantic annotation framework


Quratulain Rajput

, Sajjad Haider

Artificial Intelligence Lab, Faculty of Computer Science, Institute of Business Administration, Karachi, Pakistan

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 2 February 2010
Received in revised form 24 April 2011
Accepted 24 April 2011
Available online 30 April 2011

Keywords:
Ontology
Information extraction
Bayesian network
Machine learning
Semantic annotation

1. Introduction

The paper presents a semantic annotation framework that is capable of extracting relevant information
from unstructured, ungrammatical and incoherent data sources. The framework, named BNOSA, uses
ontology to conceptualize a problem domain and to extract data from the given corpora, and Bayesian
networks to resolve conflicts and to predict missing data. The framework is extensible as it is capable
of dynamically extracting data from any problem domain given a pre-defined ontology and a corresponding Bayesian network. Experiments have been conducted to analyze the performance of BNOSA on several problem domains. The sets of corpora used in the experiments belong to sellingpurchasing websites
where product information is entered by ordinary web users in a structure-free format. The results show
that BNOSA performs reasonably well to find location of the data of interest using context keywords provided as part of the domain ontology. In case of more than one value being extracted for an attribute or if
the value is missing, Bayesian networks identify the most appropriate value for that attribute.

O 2011 Elsevier B.V. All rights reserved.

A large volume of web contents is available in unstructured and
semi-structured format. This includes the contents available on
many online ad portals such as craigslist, ebay, gumtree, etc.
Despite providing a reliable and affordable service to a large cus-
tomer/fan base, these portals contain user-generated posts (data)
written in unstructured and ungrammatical format. Thus, standard
query languages cannot be used to retrieve relevant information
posted on these portals. As a result, users have to rely on key-
word-based searches which do not necessarily retrieve the most
relevant and accurate information [1,2]. During the last few years,
Semantic Web technologies have emerged as a much needed platform that has the potential to turn the dream of data extraction,
integration and annotation into reality [35]. Semantic Web tech-
nologies, such as RDF/XML, RDF Schema and OWL, facilitate in
storing relevant information along with their formal description.
Thus, the Semantic Web can be regarded as an extension of the
current web in which information is given well-defined meaning.
When semantic representation is added to the existing web con-
tents, the process is called semantic annotation [6].

To understand the difference between syntactic and Semantic
Web, consider the following example. Suppose a user is interested
in buying a used Nokia mobile phone from an online ad portal
(such as craigslist) with the following characteristics: memory = 128 MB, price between $200 to $400 and model = 8800. With

 Corresponding author.

E-mail addresses: quratulain.rajput@khi.iba.edu.pk, qrajput@iba.edu.pk (Q.

Rajput), sajjad.haider@khi.iba.edu.pk (S. Haider).

1570-8268/$ - see front matter O 2011 Elsevier B.V. All rights reserved.
doi:10.1016/j.websem.2011.04.002

the existing search interface, the user cannot perform this query
because information is stored in html pages without any metada-
ta/semantic added to it. Thus the user has to rely on keywordbased search results generated by the ad portal. Fig. 1 shows a
sample query results of used mobile phones obtained through
the craigslist website. The user is expected to browse this huge list
of links to identify the relevant information matching his/her crite-
ria. Because of the time taken by manual browsing, users typically
browse only the top few links or select the links randomly by
guessing from the titles of the links. One of the aims of the Semantic Web is to overcome this problem by adding semantics to web
contents and thus automating the task of finding and integrating
relevant information from different sources/pages. Table 1 shows
the result of the same mobile phone query if semantic tags were
associated with data. The first row of the table indicates attribute
names and the last column contains original links of those ads from
which data is extracted. The empty cells show that information is
not available on the corresponding ads.

The full realization of the Semantic Web requires annotation of
web contents with semantics. The development of a new website
with semantics is comparatively easier but the problem lies with
the existing ones as most of the existing web pages do not contain
semantic information. Moreover, a lot of data on those pages is
available in semi-structured/unstructured and ungrammatical format which increases the complexity of the problem. This paper
presents BNOSA which is a semantic annotation framework designed for unstructured and ungrammatical data sources. The work
extends our previous efforts [7,8] by integrating isolated concepts
into a framework and provides a proof of concept by applying it on
sets of online ads belonging to five different domains.

Q. Rajput, S. Haider / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 99112

Fig. 1. Cell phone search results from craigslist website.

Table 1
Cell phone search results in Semantic Web.

Mobile #

Brand

Nokia
Sony Ericson
Motorola

Model

Z550a
L2

Ram (MB)

Price ($)

Color

Black
Silver

BNOSA defines the semantics associated with each class and its
attribute in the annotation section of an ontology. This semantic
data is treated as context words and along with data-types of the
attributes/classes help in extracting relevant values from a web
page. During the extraction process, if multiple values are extracted for an attribute or if a value is found missing then Bayesian
networks are used for conflict resolution or for missing value pre-
diction. This ability of BNOSA to dynamically link an ontology and
the corresponding Bayesian network to extract data from a domain
of interest make it highly extensible.

The rest of the paper is organized as follows. Section 2 provides
an overview of the related work. Ontology and Bayesian networks
are introduced in Section 3 while Section 4 explains BNOSA. Experiments conducted to test BNOSAs performance over different
problem domains are discussed in Section 5. Section 6 compares
BNOSA with two other recently proposed semantic annotation
frameworks. Finally, Section 7 concludes the paper and provides
future research directions.

2. Related work

The process of semantic annotation primarily consists of information extraction, knowledge management, and storage of extracted data in structured format (RDF/OWL/RDBMS) and user
interface design. Due to the complexity of the problem, much of
the research within the field of semantic annotation has been focused on extraction of relevant data using a variety of techniques.
Fiumara [9], Laender et al. [10] and Reev et al. [11] provide a detailed overview of different information extraction techniques
used in semantic annotation. The past few years have also seen a
growing interest in the use of ontology for Semantic Web related
activities. Ontology based tools support automatic/semi-automatic
annotation using domain specific ontologies. Few of them, such as
iASA, MnM and KIM, are designed for unstructured but grammatical texts and also exploit lexical information such as part of speech
tagging and natural language processing. In contrast, tools like
BYU, ontoX, Phoebus (although not exactly ontology based) and
BNOSA are designed for unstructured and ungrammatical data
sources. A brief overview of these tools is given below.

iASA [12] is a semantic annotation tool based on information
extraction and machine learning techniques. It uses annotated documents to construct initial rule set. Once rules are learned, they are
applied on new documents for semantic annotation. If there are
missing instances, iASA uses a classification model to predict them.
MnM [13] is a semi-automatic approach in which users provide
training data by manually annotating documents which are then
used by information extraction system. The Knowledge and Information Management (KIM) [14] primarily focuses on finding/
extracting popular entities (such as name of a person, place, etc.)
in a text document and annotating them.

BYU [1517] performs information extraction with the aid of
extraction ontologies. Object-oriented System Model (OSM) is used
to represent extraction ontology as it allows the specification of
regular expression for constants and context keywords. BNOSA is
similar to BYU in the sense that both approaches construct ontology manually. The similarity, however, ends here and they differ
in the ways conflicts are resolved and regular expressions are gen-
erated. BYU uses a proximity based heuristic to resolve conflicts in
case more than one value is extracted for an attribute. BNOSA, on
the other hand, uses Bayesian networks which is mathematically
a more rigorous and robust approach. In addition, BNOSA exploits
the built-in data type feature of OWL to automatically generate
regular expressions for information extraction. In contrast, BYU
constructs the regular expressions manually which can turn out
to be a time-consuming process.

Phoebus [18,19] exploits an outside collection of known enti-
ties, called reference set, and support vector machines (SVM) to
extract data from unstructured and ungrammatical data sources. It
computes several record-level and field-level similarity metrics for
each post by comparing it with relevant records in the reference
set. The relevant records are then passed through SVM which labels them as match or non-match.

ontoX [20,21] uses OWL to represent extraction ontology. It defines data according to the data types available in OWL 1.0 such as
int, float, string. Ontology is also used to store context keywords.
During the extraction phase, the context keywords of a property
are searched in a document and the values found in the neighborhood of these words are considered the property value. In case of
more than one value being extracted, the final value is selected
by computing a level of evidence. Among all the tools/techniques
mentioned in this paper, ontoX is the most similar to BNOSA. The
main difference lies in the way conflict resolution is handled.
ontoX uses a simple metric level of evidence whereas BNOSA
applies a Bayesian network. Furthermore, BNOSA is also capable
of predicting missing values of any data type (either float or enu-
merated), again with the help of Bayesian networks; a feature
not addressed by any of the mentioned semantic annotation
approaches for unstructured and ungrammatical data sources. A

rather detailed comparison of BNOSA with ontoX and Phoebus is
provided in Section 6.

3. Technical background

This section provides a brief overview of the two core compo-

nents of BNOSA: ontology and Bayesian networks.

3.1. Ontology

Semantic Web,

Ontology can be defined as a formal and explicit specification of
a shared conceptualization. It provides a shared vocabulary to
model a problem domain. This includes types of objects and/or
concepts that exist within the domain, their properties and relationships [22,23]. Ontologies have been used in artificial intelli-
gence,
biomedical
informatics,
library science, and information architecture for
knowledge representation. Fig. 2 shows a simple example that
demonstrates the conceptualization of a university environment.
The dotted lines indicate data type properties, the undirected lines
indicate object properties and the directed lines show subclass
relationships. For example, Student is a subclass of Person and
has a relation enrolledIn with Courses.

software

engineering,

3.2. Bayesian network

A Bayesian network [24] provides a graph theoretic representation to compactly represent the joint probability distribution of
random variables in a problem domain. During the last two dec-
ades, it has become the de-facto standard to model and reason
about uncertain situations. Bayesian networks have been applied
in many fields including fault diagnosis,
information fusion,
forensics, marketing, medical sciences and financial informatics
[2527]. Formally, a Bayesian network consists of two compo-
nents: structure and parameters. The structure is represented as
a directed acyclic graph (DAG) G which consists of a set of vertices
V and a set of edges E that connects these vertices, that is, G =
(V, E). The set of vertices corresponds to random variables in a
problem domain while the set of edges defines certain types of
conditional dependency among these variables. Parameters, on

id 

pid 

name 

pname 

person 

subclassOf 

teacher 

enrolledIn 

subclassOf 

student 

cid 

id 

isTaughtBy 

subclassOf 

courses 

cname

subclassOf 

name 

corecourse 

electivecourse 

Class hierarchy 

Object property  Data type property 

person 
     student 
     teacher 
courses 
     corecourses 
     electivecourses 

isTaughtBy 
enrolledIn 

pid 
pname 
cid 
cname 

Fig. 2. Ontology for university domain.

P(FU=yes) = 0.98 

Fuel 
(FU) 

P(SP=clean) = 0.96 

SparkPlug 

(SP)

FuelGauge 

(FG) 

P(FG=empty |  FU=yes) = 0.01 
P(FG=empty | FU=no) =0.99   

Start 
(ST) 

P(ST=yes| FU=yes, SP=clean) = 0.99 
P(ST=yes| FU=yes, SP=dirty) = 0.01 
P(ST=yes| FU=no, SP=clean) = 0 
P(ST=yes| FU=no, SP=dirty) =0 

Fig. 3. A Bayesian network for car start problem.

the other hand, describe conditional probability distribution of
each variable given its parents in G. These conditional probabilities
together with the Markov property assumption, that is, each variable xiis independent of its non-descendent given its parents, simplifies the computation of joint probability distribution of these
variables. Mathematically,

Px1; x2; :::; xn 14

Pxi j paxi

i141

where paxi represents the set of parents of variable xi.

Fig. 3 shows a sample Bayesian network [28]. It aims to model
the human reasoning process if a car does not start due to unknown reasons. All the nodes in Fig. 3 are assumed to have binary
states: FU={yes, no}, SP{clean, dirty}, FG={empty, nonempty},
ST={yes, no}. According to the model, Fuel and Spark Plug influence Start variable. Fuel also has an influence on Fuel Gauge.
The prior probability of Fuel and Spark Plug, that is, P(FU=yes)
and P(SP=clean), is 0.98 and 0.96, respectively. The conditional
probabilities are: P(FG=EmptyjFuel=yes) = 0.01, P(ST=yesjFU=yes,
SP=clean) = 0.99, and so on.

Traditionally, BNs have been built by acquiring knowledge from
domain experts. In the past few decades or so, a lot of work has
been done on learning the structure and the parameters of a BN
[2932]. Heckerman [33] provides an excellent overview of the
BN learning process.

4. Working of BNOSA

This section explains the proposed semantic annotation frame-
work, BNOSA (Bayesian Network and Ontology based Semantic
Annotation), which aims to annotate unstructured and ungrammatical web data sources. BNOSA is capable of dynamically linking
a domain-specific ontology and a corresponding BN (learned sepa-
rately) to extract relevant information from a domain of interest.
The semantic annotation process performed by BNOSA comprises
of the following three steps which are explained in the sequel:

(A) Ontology development for knowledge representation and

Bayesian network learning.

(B) Information extraction with the help of ontology and Bayes-

ian network.

(C) Semantic tagging of the extracted information using Seman-

tic Web technologies (OWL/RDF).

4.1. Knowledge representation and model learning

4.1.1. Ontology development for knowledge representation

BNOSA conceptualizes a problem domain in the form of an
ontology which stores data type properties of all the relevant

Q. Rajput, S. Haider / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 99112

type:float 

type:string 

processorspeed 

brand 

Laptop 

harddisk 

ram 

price 

type:float 

type:int

type:int

Type 

Comment 

string 

dell , toshiba , ibm , apple , hp , 
compaq , sony 
inch ,  " 

displaysize 

type:float 

Class= Laptop 

Data type 
properties  
brand 

displaysize 

float 

processorspeed  float 

speed,processor,ghz 

harddisk 

ram 

price 

int 

int 

harddrive, harddisk, hdd, gb,gig 

ram, memory,  gb 

float 

$, price 

Fig. 4. Ontology for laptop domain.

classes as well as their context information. This ontology is constructed manually and, due to its support in information extraction
process, is also referred to as extraction ontology. The construction
process first identifies the important features within a particular
domain that are typically sought after by ordinary users and then
incorporates them in the ontology. For example, Fig. 4 shows a possible conceptualization of laptops. The concepts/classes represent
the important features generally searched by users looking to buy
a new or used laptop. The undirected lines in the figure indicate
data type properties. For example,processorspeed, brand, displaysize,
ram and harddisk are data type properties. The data types of these
properties are defined as follows: processorspeed and displaysize as
float, brand as string, and ram and harddisk as integer. It is important
to note that relevant data on online ad portals is not available in a
fixed format. Furthermore, due to the ungrammatical nature of
the text, it is difficult to exploit the grammatical rules of the English
language. Thus, to extract data from unstructured and/or ungrammatical data sources, two issues need to be addressed: (a) finding
the location of relevant data on a web page and (b) defining
patterns for extracting such data. To solve the location problem,
BNOSA stores a list of context words associated with each concept
in the comment section of the ontology (Fig. 4). These words are
used during the extraction phase to mark the neighborhood of the
relevant data items.

BNOSA also defines patterns/rules in the form of regular expressions by utilizing the data-types already specified for each attri-
bute. For instance, ram is defined as int. This implies that all
values which are part of the integer set consist of digits which
themselves are combination of 09. Thus, the rule for int is nd+.
Similarly, the rule for float is nd+n.nd?nd?nd? and the rule for year
is nD(19j20)nd{2}nD. To handle string data type, all possible values of an attribute are specified first. For example, if the brand of a
laptop can be either dell, ibm, toshiba, compaq or hp, then the cor-
delljtoshi-
responding regular expression is generated as
bajibmjcompaqjhp. If the number of possible string values is not
very large than they can be stored in the comment section as
shown in Fig. 4. On the other hand, if the number of possible values
is large, a dictionary needs to be maintained. The specification of
all the relevant attributes, their data types, constraints and the
context information correspond to a fully-specified ontology.

4.1.2. Bayesian network learning

To extract information from ungrammatical and incoherent
data sources, one has to deal with variable size of information.
For instance, if we want to search used cell phones on two different
web sites, we may find one website containing very detailed information (brand, model, camera, condition, battery, display, weight,
etc.) while the other contains very few data elements (brand and
model). Most information extraction systems, when confronted
with such situation, store the unavailable data items as missing.
However, it is possible in many situations that the data is missing
due to some causal reason. In other words, the data is missing because the person who entered the information, in case of online ad
portals, thought that it could easily be predicted from other nonmissing values. If the end-user of the posted information is also
familiar with such assumptions then no ambiguity arises.
However, such assumptions are typically not known to na ve users
and they intend to ignore records having missing values. Further-
more,
in many cases, the relationship between missing and
non-missing values is not deterministically causal, but is probabilistically causal. It is thus highly desirable in situations like these to
predict the missing data to better guide a user in his/her decision
making process. Another major limitation of the existing information extraction systems is their inability to resolve synonym and
polysemy. In some cases, context words can aid in resolving this issue but the situation becomes complicated when relevant context
words are not available in the text or same context words are used
for different attributes. BNOSA applies Bayesian networks to predict missing values and/or to resolve conflicts if multiple values
are extracted for an attribute via ontology based extraction.

Before it can be used for the above mentioned tasks, we need to
learn Bayesian network first. The variables in the Bayesian networks are directly mapped from the extraction ontology developed
in the previous step. All classes/concepts in the ontology are
mapped to variables in the Bayesian network. In addition, values
of string-type variables in the ontology are mapped as states of
the corresponding variable in the Bayesian network. The relationships between variables and their strengths, specified as conditional probability tables (CPTs), however, are learned from the
available data.

The learning involves cleaning and pre-processing of the extracted data. This includes removal of incomplete records from
the training set and then discretizing the continuous data into discrete states. It must be noted that learning a Bayesian network
with good prediction accuracy requires a large data set. Due to
the fact that such data sets are not easily available online, the
ontology-based extraction phase of BNOSA is applied to generate

Extracted data 

ready for 
annotation 

Phase-I 

Extraction via 

ontology 

Phase-II 

Conflict resolution or 

prediction using  
Bayesian network 

Manually 

constructed domain 
specific ontology 

Bayesian network 
learned from data

Fig. 5. Components of BNOSA.

the required training data. The data is further refined by removing
incomplete records and discretizing continuous attributes. The refined data is then used to capture the joint probability distribution
of attributes by learning the structure and the parameters of a
Bayesian network. Together with the ontology constructed in the
previous step, the learned Bayesian network is used for information extraction as explained in the following section.

4.2. Information extraction

The essential components of BNOSA to extract information from
existing web contents are shown in Fig. 5. BNOSA performs information extraction in two phases. In Phase-I, it uses ontology to
extract relevant data while in Phase-II, it uses Bayesian Network
to select the most appropriate value (if more than one value is
extracted) of an attribute or to predict a value (if the value is
missing). The following subsections explain each phase in detail.

4.2.1. Phase-I: extraction via ontology

During the ontology-based extraction phase, BNOSA retrieves
links of information of interest from explicitly provided URL(s).
In an iterative manner, contents of each link are explored to extract
relevant data. BNOSA performs extraction using (a) knowledge
stored in an ontology in the form of concepts, relationships among
concepts, data type properties, and context keywords and (b) generated rules according to the type (int, float, string) of an attribute.
Each context keyword is searched within the ad contents. If a
match is found, this suggests that the corresponding data value
should be present in the neighborhood of this keyword and can
be searched using pre-defined rules/patterns. A fixed threshold of
8 characters is used in BNOSA to define the neighborhood. For
example, if the word GB is matched then 8 characters before
and after it are treated as neighborhood and are searched using
the regular expression defined for harddisk attribute. Besides 8,
several other numbers were also tried, but it was found that 8 provides a good trade-off between efficiency and accuracy. The complete ontology-based extraction process is depicted in Fig. 6
while a formal description is given in Table 2.

It should be mentioned that for humans it is easy to recognize
data by viewing advertisements but this recognition process is
not as simple for machines. For example, if we want to find the size
of a hard disk, our automated tool must have an understanding of

Extracted Data 

Apply rule in the neighbourhood of 

location using context words for 

instance extraction 

Rule generation 
using data types 

Location finding 

using context words 

Relevant link on web 

Jun 25 - DELL 
Inspiron 2650 Laptop 
Wireless Intel Pentium 

Selected ontology 

Context words 
data type properties, 
object properties, etc 

Table 2
Algorithm for ontology based information extraction.

Set T = Null // use to store ad description
Set L = list of ads link
Set O = pre-defined ontology for a domain developed in protege
Set ContextWordList = Null
SetLexiconsOfValue[][] = {{ndndn.nd,{nd+},

{nD(19|20)nd{2}nD},{. . .}}

Step 1: Retrieve all ads links from the specified website.
Step 2: For each ad link L

A. Read ad description text in T
B. For each concept C in ontology O

Set ContextWordList= words in comment section of C

in ontology

Create a new record R
For each datatypeProperty D of C
i. Append words in comment section of D in

ContextWordList

ii. Set TypeOfValue=type of value of D
iii. if (TypeOfValue== float) then Set

Rule = LexiconsOfValue[0]

else if(TypeOfValue== int) then Set

Rule = LexiconsOfValue[1]

else if (TypeOfValue== gYear) then Set

Rule = LexiconsOfValue[2]

. . .

iv. For each context word cw in ContextWordList

if found(cw) in T then

apply Rule in the neighborhood of cw and store

the result in A

if(multiple values in A)or(no value in A)
apply Bayesian network to select best

candidate then store in R.

else if(exactly one value in A) store A in R.

C. Store R in the OWL

all the possible ways hard disks are mentioned in advertisements.
Moreover, it should be able to distinguish among similar values
belonging to different data elements. Thus, the quality of the
extraction depends upon the specification level of the built ontology  the more specific and more detailed the ontology the better
are the extraction results.

4.2.2. Phase-II: missing value prediction and conflict resolution using
Bayesian networks

If there are missing and/or conflicting values in the data set
extracted via ontology, then these issues are resolved in this phase.
It uses an already learned Bayesian network (as described in
Section 4.1.2) to predict missing values and to resolve conflicts.
In case of a missing value, all the non-missing/non-conflicting values are considered as hard evidence and the posterior marginal
probability of the missing attribute is computed. The state with

Final extracted data 

Bayesian inference 

for missing data prediction and for 

conflict resolution 

Learned Bayesian network 

Data preprocessing 

Extracted data via ontology

Fig. 6. Phase-I of information extraction.

Fig. 7. Phase-II of information extraction.

Q. Rajput, S. Haider / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 99112

Table 3
Sample advertisements of selected domains.

HP dv2000 laptop (2gb, 120gb, mouse  $750 (Foggy Bottom)
Ive spent over $1400 on this setup since last year, before I leave DC in a week
 this is a GREAT DEA GWU) to let you test drive the computer before yo
Local sales only and no spamming please.

Here are more detailed specs:
1) HP dv2000t Specs
Windows Vista Home Premium (32-bit)
Intel core 2 Duo T5600 (1.83GHz/2MB L2Cache)
14.100 WXGA BrightView Widescreen (1280 
 800)
128MB NVIDIA GeForce Go 7200
HP Imprint Finish + Microphone + Webcam
2GB RAM (2 Dimm)!!!!!
120GB 5400RPM HDD
99 HONDA ACCORD LX  $3500 (WOODBRIDGE)
1999 Honda Accord 3,500 firm
1 OWNER No Accidents
Green with Tan Lnterior
150k
Automatic Transmission
4 Cylinder Engine great on gas
The car runs and drives great, but it needs a new mass air flow sensor, due the
sensor the car will cut off sometimes....the car is only driven a few miles a
week

Also the paint on the roof and trunk is oxidizing...it just needs a little lic, we

have three other cars dont have time for this one

Nokia 6300 Silver Black Mobile Phone  Sim-Free/Unlocked  $96 (SOMA /

south beach)

Connectivity options: GPRS, Bluetooth, USB
Calling Features: Voice commands, Speakerphone
Physical Design: Form factor Candy bar
Dimensions (W 
 D 
 H): 106.4 
 43.6 
 13.1 mm
Weight w/battery: 91 g
Available colours
LCD display size 2-inch, QVGA screen
Primary Display Color 24 bit
internal memory: 135 MB
Multimedia Built-in digital camera? Yes
Base warranty 1 year

Silver/black

the highest probability replaces the missing value. In case of multiple values belonging to an attribute (conflict resolution), the attributes value is considered as missing and all the non-missing/non-
conflicting values are entered as hard evidences in the BN. The posterior marginal probability of this attribute is computed next.
Among the multiple extracted values, the value/state that has the
highest posterior probability is selected as winner and is assigned
to the corresponding attribute. The process is shown in Fig. 7.

4.3. Semantic annotation in OWL

After successfully completing the above two steps (Sections
4.2.1 and 4.2.2), BNOSA finally populates ontology with the
extracted data. Thus, with the help of RDF/OWL, this step adds
semantics to the extracted data and makes it machine readable.

5. Experiments and results

This section applies BNOSA on three data sets belonging to cars,
laptops and Nokia mobile phones ads posted on the craigslist
website.1 Craigslist is a centralized network of online communities,
featuring free online classified advertisements with sections devoted to jobs, housing, for sale items, and so on. The users post
ads in their own style which in most of the cases are unstructured
and ungrammatical.

Table 3 shows three ads posted on craigslist belonging to each
of the selected domains. It can be easily observed from these ads

that the required information is neither organized in a specific format nor the location of the information is fixed. Thus, extraction of
required information is a big challenge. The following subsections
first describe the challenges faced during the application of BNOSA
on selected domains and then provide experimental settings and
accuracy of the extraction results.

5.1. Challenges of extracting data from online ad portals

Following are the major challenges faced during the application

of BNOSA to extract data from the selected online ad portals.

URL unrecognized: To process the information available on ad
portals, the links of all relevant documents have to be retrieved.
At many times during this retrieval process, links were found to
be removed from the corresponding web sites or were unavailable due to network problems.
Ungrammatical/spelling mistakes: Information posted on online
ad portals is not necessarily available in a proper grammatical
form. Some ads use abbreviations of different terms and some
use different conventions for similar data elements. This leads
to higher chances of typing mistakes.
information: Ads size varies tremendously
Variable Size of
depending upon the level of detail provided by different users.
Some users provide very detailed information including photographs of the item, while some users simply write a phrase
highlighting the most important features of an item.
Appearance: Different data elements with similar appearances
(e.g. 10 GB could be related to ram or to harddisk) and same
data elements with different appearances (e.g. 2 
 512 MB
and 1 GB) lead to identification problems. In car advertise-
ments, mileage appears in different formats such as {87,000
134.000 100k 104xx 170, 45000} and sometimes its context
words (such as mileage) are not used. This makes it difficult to
develop a generalized regular expression as data could belong
to integer in one instance and to string in another. It also
becomes difficult to find location in the absence of context
words.
Unrecognized: Sometimes the required information is available
in a unique format which may not be easily recognizable. For
instance, the possible values of a string type attribute are stored
in the comment section of ontology. Values other than those are
considered as unrecognized. Similarly, the neighborhood area of
context words is pre-defined and a value that occurs beyond
this range is not be recognized.

5.2. Results of extraction via ontology

The first step in the BNOSAs framework is the development of
extraction ontologies. The ontologies for the three selected domains are presented in Figs. 4 and 8. These ontologies are developed using Protege.2 Once an ontology is fully specified, it is
used to extract the required information using the Jena3 ontology
API. For example, consider the laptop ad shown in Table 3. BNOSA
uses context keywords to find values of the corresponding variables in the neighborhood of these keywords. For instance, displaysize uses inch and 00 as context keywords. These words are first
searched by BNOSA and once found their neighborhood is searched
next for a float type value representing display size of the laptop.
As a result of this process, BNOSA finds 14.1 and stores it as the display size. A similar process is applied for all the variables/attributes
and the extracted value(s) are stored for further processing. If

1 http://www.craigslist.org

2 http://protege.stanford.edu
3 http://jena.sourceforge.net/downloads.html

Table 4
Sample extraction results  Phase-I only.

Domain

Laptop

Car

Call phones

Attributes

harddisk
displaysize
price
brand
processorspeed
ram

mileage
transmission
color
make
year
price

memory
color
price
model

Ad 1

#120
#14.1
#750#1400
# hp
#2#1.83
#2

#auto
#green
#honda
#1999
#3500

#135#128
#silver#black
#96
#6300

Ad 2

#250#0#4
#13.3#90
#900
# sony
#2#2.10#4
#250#0#4

#170#155
#auto

#ford
#1998
#1#1500#19

#64

#301
#35#2#8800#8801

Ad 3

Ad 4

#00#100
#14.1
#360
# toshiba
#2#7#2.0#2300
#2#128#100

#40
#auto
#white
#ford

#3500

#2#6#64#70
#black

#6280

#20#1#12
#12.1#1.25
#250
#dell
#2#00#256#1.2
#1024#20#10#100

#143#300
#auto

#toyota
#1996
#3200

#32

#450
#8800#104#8#12

Ad 5

#40

#325
#dell
#1.6#256
#40#256

#3#70
#manual
#red
#toyota

#2#7#7000

#80
#red
#248
#9500

multiple values are extracted then all such values are stored by
BNOSA and the conflicts are resolved in the next step. For instance,
price variable uses $ as a keyword and BNOSA found two values
1400 and 750 within the neighborhood of this word (as can be seen
in the ad of Table 3). Both values are stored by BNOSA at this stage
and this conflict is resolved using an already learned Bayesian network in the next step. The extracted data for 5 sample ads belonging to each of the three selected domain is presented in Table 4.
Ad1 in the table corresponds to the ads shown in Table 3. Nonempty cells in the table show values extracted by BNOSA for the
corresponding attribute while empty cells show either the information was not available in the ad or BNOSA failed to extract it.
If BNOSA finds more than one value for an attribute, it stores all
of them by separating them with # symbol. The correct value is
predicted in the next phase with the help of Bayesian network.
Similarly, a missing value is also predicted with the help of Bayesian network in the next phase.

The accuracy of ontology based extraction process is measured
through recall and precision metrics. The extraction of an attribute
can result in three possibilities: value correctly identified (V), missing value correctly identified (M), and value incorrectly identified
(W). It should be mentioned that in case of multiple values being
extracted, if the correct value exists in the list of values then it is
treated as correct extraction (only at this stage). Mathematically,

VV  VW  VM

Recall V 14
Precision V 14
Recall M 14 MM
MM  MV
Precision M 14 MM

VV  WV  VM

MM  VM


where

 VV is the number of times BNOSA extracted a correct value.
 VM is the number of times BNOSA failed to extract a value.
 VW/WV is the number of times BNOSA extracted a wrong value.
 MV is the number of times BNOSA extracted a value when it was

actually missing in the ad.

 MM is the number of times BNOSA does not extract a value

when it was also missing in the ad.

Table 5
Performance of Phase-I of information extraction.

Attributes

Precision
(V)

Recall
(V)

Precision
(M)

Recall
(M)

Laptop

Car

harddisk
displaysize
price
brand
processorspeed
ram

mileage
transmission
color
make
price
year

Cell

memory

phones

color
price
model

Table 5 presents precision and recall values of Phase-I of information extraction for the selected domains. It can be seen from the
table that Recall (V) is high in laptop ads for all attributes. The precision (V) of all but processor speed attribute is 100%, which suggests that whenever a value is extracted it is extracted correctly.
The recall value of missing elements, Recall (M), is also 100% which
shows that the missing values are extracted as missing very accu-
rately. The recall and precision values for car and cell phone ads are
also very high for most of the attributes. The few exceptions are
mileage and make attributes in car ads which have a decent recall
and precision rate but not as high as other attributes in the exper-
iment. In case of mileage, the reason for a low performance is due
to the fact that mileage sometimes occurs in ads without any context word and is also written in many ways, such as 131.xxx, 90k,
161000 and so on. In case of make, being a string data-type, only
few possible values are stored in the ontology. The actual number
of values which are not part of the ontology is much larger and
thus is not extracted by BNOSA. This has resulted in a low precision
(M) for make attribute.

5.3. Results of Phase-II

It must be noted that VW in Eq. (1) and WV in Eq. (2) correspond

to the same number.

The data extraction in Phase-I was performed with the help of
context words, data types and regular expressions defined in the

Q. Rajput, S. Haider / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 99112

type:string 

type:string 

transmission 

color 

type:float 

mileage 

year 

Car 

price 

model 

type:string 

make 

type:string

type:gYear 

type:float 

Class= Car 

Data type 
properties 
mileage 
transmission 
make 
color 

price 
year 

float 
gYear 

Type  Comment 

float  mileage,miles 
string 
string 
string 

auto,manual 
toyota , honda , ford , nissan 
red,white,champagne,charcoal,grey, 
blue,black,silver,maroon,green,yellow 
$, price 
 ,year 

type:string 

type:int 

color 

memory 

Cell phone

model 

Price 

type:int 

type:float 

Class= Cell Phone 

Data type 
properties  
memory 
model 
color 

Type  Comment 

memory,mb 
make,nokia 

Int 
Int 
String  black , brown , golden , 

price 

Float 

 purple , red , silver 
$, price 

Fig. 8. Ontology for cell phone and car domains.

corresponding ontologies. In some cases, the extraction process extracts multiple values against an attribute. Conversely, in other sit-
uations, no value was extracted. This is due to two possibilities:
either the extraction process failed to extract the value or the value
was not present in the text. In either case, identification of the
missing data could assist a user in her decision making process.
BNOSA uses an already learned Bayesian network (as explained
in Section 4.1.2) to address both issues of conflict resolution and
missing value prediction. For the current experiments, due to
unavailability of any standard dataset, thousands of records corresponding to each domain are extracted from the craigslist website
using BNOSAs ontology based extraction mechanism (Phase-I).
This raw extracted data consisted of several thousand records
posted on craigslist during MayJune, 2009. These records belonged to 10-15 major cities of the USA. Out of this raw data set,
only complete records are retained which consist of several hundred records for each selected domain. Attributes with continuous
values are discretized next. For instance, processorspeed in laptop
is discretized into five states {11.5, 1.52, 22.5, 2.53 and 34},
all in GHz, memory in used mobile phone is discretized into six
states 130; 3165; 6680; 80130; 130200; > 200, all in MB,
and so on. The result data is then used for learning the corresponding Bayesian networks. In this experiment, BN Power Constructor4
is used to learn the structure and the parameters of a Bayesian net-
work. The BNs of laptop, car and cell phones are shown in Figs. 9, 10
and 11, respectively.

Once a Bayesian network is learned, its inference mechanism is
used for conflict resolution and missing value prediction. The paper
uses the SMILE API5 to support this inferencing activity. In case of
multiple values belonging to an attribute (conflict resolution), the
attributes value is considered as missing and all the non-missing/
non-conflicting values are entered as hard evidences in the BN.
The posterior marginal probability of this attribute is computed next.
Among the multiple extracted values, the value with the highest
posterior probability is selected as the winner and is assigned to
the corresponding attribute. For example, consider Ad 1 of laptop
in Table 4. The corresponding columns shows that only one value
belonging to each of harddisk, ram, displaysize and brand is

4 http://www.cs.ualberta.ca/jcheng/bnpc.htm
5 http://genie.sis.pitt.edu/about.html

extracted by BNOSAs Phase-I. Multiple values, however, are extracted for price and processorspeed attributes. Phase-II of BNOSA
is applied in this situation to resolve the conflict (by selecting one
value out of many). First, the non-conflicting values are entered as
hard evidence in the Bayesian network. For discretized variables, evidence is entered into the corresponding interval. For example, the
harddisks value is 120, thus evidence is entered into the discretized
state 90130 GB. Evidence about other discretized variables is entered in a similar fashion. The posterior probability of attributes with
multiple values is computed next, and the state (among the conflicting values) with the highest probability is selected as winner. For
example, two values are extracted for laptop price: 750 and 1400.
The posterior probability computation suggests that 750 has a higher
posterior probability than 1400, and thus is selected as the winner.
The conflict among processorspeed values is resolved similarly.

In case of missing values, all non-missing/non-conflicting values are treated as hard evidence and the evidence is propagated
in the network to compute the posterior probability of the missing
attribute. The value with the highest probability replaces the missing value. For example, consider Ad 5 of laptop in Table 4. There is
one missing value which belongs to displaysize. Only one value is
extracted for harddisk, price and brand attributes while multiple
values are extracted for processorspeed and ram attributes. First
the conflicts are resolved as described above and then all the
non-missing and non-conflicting values are entered as hard evidence in the Bayesian network. Posterior probability of the missing
variable, displaysize, is computed next. The state of displaysize
having the highest posterior probability replaces the missing value.
The performance of BNOSAs conflict resolution in Phase-II is

evaluated as follows:
Accuracy 14

Correct

Correct  Incorrect

where Correct is the number of times a conflict was resolved cor-
rectly. Incorrect is the number of times BNOSA did not perform conflict resolution correctly.

Table 6 shows the accuracy of BNOSAs conflict resolution
mechanism for each of the selected domain. For example, in case
of laptop, all attributes have an accuracy rate of above 70%. In
few cases, the accuracy is more than 90%. Similarly, most of the
attributes in cell phone and car domains also have a very good
accuracy of above 70%. This shows that when multiple values are

Fig. 9. Learned BN for laptop domain.

Fig. 10. Learned BN for car domain.

Q. Rajput, S. Haider / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 99112

Fig. 11. Learned BN for cell phone domain.

Table 6
Performance of conflict resolution in Phase-II.

Laptop

price
processorspeed
displaysize
ram
brand
harddisk

Car

mileage
make
color
year
price
transmission

Cell phones

memory
color
price
model

Table 7
Performance of missing value prediction in Phase-II.

Laptop

price
processorspeed
displaysize
ram
brand
harddisk

Car

mileage
make
color
year
price
transmission

Combined accuracy

Cell phones

memory
color
price
model

extracted by BNOSAs Phase-I, the issue is resolved quite accurately
by the corresponding Bayesian network in Phase-II.

To test the accuracy of BNOSA in missing value prediction, a test
data set (with known values) is passed to the corresponding Bayesian network. Each row of the test data corresponds to a record
containing different attributes within a domain. During the exper-
iment, value of an attribute is considered missing and is predicted
with the help of other known values belonging to the remaining
attributes. For instance, in the laptop example, we assume that
the value of processorspeed is unknown and predict it using other
five known values belonging to the other variables. The known val-

ues are entered as hard evidence in the Bayesian network. The
state having the highest probability is predicted as the missing va-
lue. If the prediction matches with the already known value then
the prediction is correct otherwise it is considered incorrect. The
process is repeated for each attribute in the row by assuming
one attribute missing at a time for all records in the testing data.
The prediction accuracy of the three Bayesian networks corresponding to each of the selected domain is shown in Table 7. The
combined prediction accuracy is 58% for laptop ads, 61% for car
ads and 63% for cell phone ads. It must be stated that data attributes in the selected problem domains are not highly correlated.
Thus, despite having a good prediction accuracy rate in these
experiments, the performance of BNOSA could improve further if
it is applied to a domain where there is a strong correlation among
different attributes within a problem domain.

5.4. Online BNOSA interface

To demonstrate BNOSAs capabilities, its prototype has been
made available online.6 This allows users to search and query used
laptops and cars ads posted on the craigslist website (more domains
and portals are planned to be added in the future). The interface of
BNOSA is very simple and user friendly. Once a user selects a domain
of interest, BNOSA fetches the relevant posts and displays the re-
quired/relevant information in a structured form. The results of a
sample laptop query are shown in Fig. 12. All but the last two columns of the figure describe the laptop schema. Some of the extracted values are enclosed within symbols [] and {}. Values
enclosed in square brackets [] highlight that prediction is made
while values enclosed in curly braces {} identify that a conflict is
resolved.

6 http://ailab.iba.edu.pk/bnosa.html

Fig. 12. BONSA interface.

In addition to displaying query results in a structured form,
BNOSA also provides its users the capability to narrow down
search results. Thus, a user can perform queries of the form find
those post which contain laptop with brand=Apple and proces-
sor-speed=2 to 2.5 Ghz  a feature currently not possible at many
online ad portals including craigslist.

BNOSA also retains actual URLs of the posts and displays them
as part of the search result in case a user likes to see details. The
second-to-last column in Fig. 12 provides the URL of the original
posts, while the last column shows the highlighted view of the
posts. The highlighted view shows how text was actually parsed
and which tokens were extracted by BNOSA during Phase-I. This
view is extremely helpful in analyzing and further improving the
performance of BNOSA as it helps in understanding why BNOSA
failed to pick an attribute or why it picked multiple values for a single attribute.

6. Comparison with related semantic annotation frameworks

This section compares BNOSA with two other recently proposed
semantic annotation frameworks, namely ontoX and Phoebus.
Similar to BNOSA, both ontoX and Phoebus also aim to extract data
from unstructured and ungrammatical data
sources. The
comparison is based on the differences in schema definition and
information extraction mechanism as well as in accuracy of the
extraction results when applied on same data sets.

6.1. BNOSA vs. ontoX

Both ontoX and BNOSA utilize domain ontologies for information
extraction. Different constructs of ontology such as classes, object
properties and data type properties are used to define schema. They
also store additional information in owl:AnnotationProperty ele-
ments. The additional information consists of context keywords
and value constraints. The data type of an attribute, such as int, float,
date, year, etc., defines the range of possible values the attribute can
take. To handle a string type attribute, ontoX defines its range
through xsd:Name property if the attribute has large number of in-
stances. In case of only few instances, it uses the enumeration con-
struct. BNOSA, on the other hand, defines all possible instance values
in the comment section without specifying context keywords.

Fig. 13. Camera extraction results  BNOSA vs. ontoX.

To extract values belonging to each attribute, both approaches
generate rules at runtime. These rules, defined in the form of regular expressions, are generated according to the data types of the
corresponding attributes. Both approaches locate a value within
the neighborhood of the corresponding context keyword. The difference lies in the definition of neighborhood. ontoX defines the
neighborhood of a context keyword as the area ranging from its
previous keyword to its next keyword. BNOSA, on the other hand,
considers a fixed number of characters surrounding the keyword as
its neighborhood. Each attribute can have more than one keywords
associated with it therefore more than one value can be extracted
for an attribute. ontoX resolves such situations by computing a level of evidence metric, while BNOSA resolves it using Bayesian
networks. In addition, if one value is extracted for more than one
attribute then ontoX resolves it using a confidence level metric,
while BNOSA assigns this value to each attribute. BNOSA also uses

Q. Rajput, S. Haider / Web Semantics: Science, Services and Agents on the World Wide Web 9 (2011) 99112

Fig. 14. Car extraction results  BNOSA vs. Phoebus.

attributes (either time-invariant or time-varying) equally but its
way of manually storing all the instances of a string data type in
comment section of an ontology may become inefficient for attributes having a very large number of possible values.

Both Phoebus and BNOSA require sufficient data to learn support vector machines and Bayesian networks, respectively before
these machine learning techniques can be used during the extraction phase. Phoebus learns several support vector machines to
identify the best match of a given post from the reference set
and to align the post tokens with attributes of the schema. BNOSA,
on the other hand, learns Bayesian networks for missing value prediction and conflict resolution.

During the extraction process, Phoebus calculates several
record-level and field-level score metrics and uses support vector
machine (SVM) to find the best match of a post from the reference set. During this process, if more than one members of the
reference set is declared match for a given post then Phoebus picks
the one randomly. Phoebus also claims to handle spelling mistakes.
Although no run-time comparison has been made, it is our belief

Bayesian networks to predict missing values, a function that is not
performed by ontoX.

Recently, the performance of BNOSA and ontoX was compared
[34] on the same data set used by Yaldiz and Miksch [21] to describe ontoX. The data set is a collection of digital camera reviews
available on a retail website.7 Fig. 13 shows the recall and precision
values of both ontoX and BNOSA for different attributes belonging
to the selected domain.

It can be seen from the figure that in general BNOSA performs
better than ontoX and, in a few cases, the difference in the recall
and precision values is much larger. This difference in the performance is primarily due to different heuristics/methods used by both
of these approaches and the way neighborhoods are defined. ontoX,
however, has a more generic mechanism to handle string data types.

6.2. BNOSA vs. Phoebus

Phoebus exploits a collection of known entities and their common attributes, called reference set, and uses support vector machines (SVM) to extract and annotate information available at
unstructured and ungrammatical data sources. It performs the
annotation process in two steps. The first step aligns a post to the
best matching record from the reference set, while the second step
annotates tokens in the post with the corresponding attributes.

The structure of the reference set in Phoebus defines a schema
of the selected domain. This reference set is populated via web
scraping of certain online data sources. For example, Phoebus
builds a reference set for the car domain from edmunds.com which
contains fixed (time-invariant) attributes, such as make, model,
trim and year, of most of the manufactured cars. The main advantage of this approach is that it helps in filling the missing value in a
post as well as in standardizing the annotated values. A drawback,
however, is that time-varying information, such as mileage, color,
price, etc., are not part of the reference set and thus are not predicted by Phoebus during the extraction. Thus, the applicability
of Phoebus is dependent upon the availability and the completeness of the reference set and how much the domain relies on
time-invariant information. BNOSA on the other hand, treats all

7 http://www.steves-digicams.com/

Fig. 15. Hotel extraction results  BNOSA vs. Phoebus.

that the computation of several score metrics and the searching of
a large reference set make Phoebus extraction process slower than
the one adopted by BNOSA (and ontoX) which searches for relevant
data using regular expressions in the neighborhoods of context
keywords.

Furthermore, Phoebus extracts data from the header of a post
and does not scan its body. This increases the chances of missing
out on relevant information as many users do not provide all the
information in the header. In contrast, BNOSA scans the complete
post for the relevant information.

The performance of BNOSA and Phoebus was recently compared
[35] on the same data sets as used by Michelson and Knoblock [18]
to describe the Phoebus framework. The data sets consist of car
posts from craigslist website (www.craigslist.org) and hotel posts
from bidding for travel website (www.biddingfortravel.com). Figs.
14 and 15 show recall and precision values of Phoebus and BNOSA
for the car and hotel domains, respectively.

It can be seen from the figures that extraction results of both
Phoebus and BNOSA are quite similar for most of the attributes.
The difference is primarily due to the way string and numeric attributes are extracted by both frameworks. For numeric data, BNOSA
has a similar performance level (or even better in a few instances)
when compared to Phoebus. For attributes with string data type,
BNOSA has a slightly inferior recall ratio (but better precision).
Phoebus compares the relevant tokens belonging to string attributes with the entries in the reference set and thus has a better recall ratio for such attributes. This advantage, however, may come
at the cost of more computation time as the process requires
searching a large reference database and computing several similarity metrics before suggesting a value. Furthermore, Phoebus fills
the (time-invariant) missing attributes using the information available in the best matched member of the reference set. However, it
does not aim to predict time-varying missing data, such as price,
mileage, etc.; although it extracts this information if it is available
in the post. On the contrary, BNOSA tries to predict all the missing
attributes using the probabilistic relationship captured in the form
of a Bayesian network.

7. Conclusions

The paper presented BNOSA that employs ontology and Bayesian networks to extract and annotate data available at unstructured and ungrammatical data sources. The framework is highly
extensible as it has the capability to extract data from any problem
domain once provided with a domain-specific ontology and the
corresponding Bayesian network. BNOSA extracts the required
information in two phases. In the first phase, it extracts data of
interest with the help of context words and data types already defined in an ontology. During this phase, if some values are found
missing or multiple values for a single attribute are extracted then
Bayesian networks are used in the next phase to predict missing
values and to resolve conflicts. The performance of BNOSA was
tested on three real data sets available online at the craigslist web-
site. The results are very promising with a very high precision and
recall ratio and reasonably well conflict resolution and prediction
accuracy. In addition, BNOSA is also applied on the data sets used
by ontoX and Phoebus frameworks and the results suggest that
BNOSA has similar (or even better) performance in most of the
cases. The future work would focus on the application of BNOSA
on various problem domains specially the ones where there exists
a stronger correlation among different attributes within a problem
domain. The focus would also be on extracting data from multiple
websites belonging to the same problem domain. Handling of
spelling errors as well as handling of string data types with large
number of possible values are other areas of future research.

Acknowledgement

The authors are extremely grateful to anonymous reviewers
and editor for their invaluable feedback which has immensely improved the quality of this work.
