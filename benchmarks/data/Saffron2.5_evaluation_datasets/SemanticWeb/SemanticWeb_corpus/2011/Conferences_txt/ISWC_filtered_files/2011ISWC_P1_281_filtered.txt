Visualizing Ontologies: A Case Study

John Howse1, Gem Stapleton1, Kerry Taylor2, and Peter Chapman1

1 Visual Modelling Group, University of Brighton, UK

{John.Howse,g.e.stapleton,p.b.chapman}@brighton.ac.uk

2 Australian National University and CSIRO, Australia

Kerry.Taylor@csiro.au

Abstract. Concept diagrams were introduced for precisely specifying
ontologies in a manner more readily accessible to developers and other
stakeholders than symbolic notations. In this paper, we present a case
study on the use of concept diagrams in visually specifying the Semantic
Sensor Networks (SSN) ontology. The SSN ontology was originally developed by an Incubator Group of the W3C. In the ontology, a sensor is a
physical object that implements sensing and an observation is observed
by a single sensor. These, and other, roles and concepts are captured vi-
sually, but precisely, by concept diagrams. We consider the lessons learnt
from developing this visual model and show how to convert description
logic axioms into concept diagrams. We also demonstrate how to merge
simple concept diagram axioms into more complex axioms, whilst ensuring that diagrams remain relatively uncluttered.

1 Introduction

There is significant interest in developing ontologies in a wide range of areas,
in part because of the benefits brought about by being able to reason about
the ontology. In domains where a precise (formal) specification of an ontology is
important, it is paramount that those involved in developing the ontology fully
understand the syntax in which the ontology is defined. For instance, one formal
notation is description logic [3], for which much is known about the complexity
of reasoning over its fragments [4].

Notations such as description logics require some level of mathematical training to be provided for the practioners using them and they are not necessarily readily accessible to all stakeholders. There have been a number of efforts
towards providing visualizations of ontologies, that allow their developers and
users access to some information about the ontology. For example, in Prot eg e,
the OWLViz plugin [10] shows the concept (or class) hierarchy using a directed
graph. Other visualization efforts provide instance level information over populated ontologies [11]. To the best of our knowledge, the only visualization that
was developed as a direct graphical representation of description logics is a variation on existential graphs, shown to be equivalent to ACL by Dau and Eklund [7].
However, existential graphs, in our opinion, are not readily usable since their syntax is somewhat restrictive: they have the flavour of a minimal first-order logic

L. Aroyo et al. (Eds.): ISWC 2011, Part I, LNCS 7031, pp. 257272, 2011.
c Springer-Verlag Berlin Heidelberg 2011

J. Howse et al.

with only the  quantifier, negation and conjunction; the variation developed as
an equivalent to ACL uses ? to act as a free variable.

In previous work, Oliver et al. developed concept diagrams (previously called
ontology diagrams) as a formal logic for visualizing ontology specifications [12,13],
further explored in Chapman et al. [5]. Whilst further work is needed to establish fragments for which efficient reasoning procedures can be devised, concept
diagrams are capable of modelling relatively complex ontologies since they are
a second-order logic.

The contribution of this paper is to demonstrate how concept diagrams can be
used to model (part of) the Semantic Sensor Networks (SSN) ontology, in its current version, which was developed over the period February 2009 to September
2010 by an Incubator Group of the W3C, called the SSN-XG [6]. We motivate
the need for accessible communication of ontology specifications in section 2,
ending with a discussion around why visualization can be an effective approach.
Section 3 presents a formalization of some of the SSN ontologys axioms using
concept diagrams and using description logic, contrasting the two approaches.
Section 4 demonstrates how to translate description logic axioms to concept
diagrams and some inference rules. Section 5 concludes the paper.

2 Motivation

Complex ontologies are often developed by groups of people working together,
consistent with their most important application: to support the sharing of
knowledge and data. The most common definition of ontology refers to an
explicit representation of a shared conceptualisation [9]. A shared conceptualisation is usually needed for the purposes for which ontologies are most used: for
representation of data to be shared amongst individuals and organisations in a
community. The sharing is necessary when domain-knowledge capture through
an ontology requires modelling of either commonly-held domain knowledge or
the common element of domain knowledge across multiple domains. This needs
to take account of instances that are asserted to exist in the ontology, and also
instances that might exist, or come into existence when the ontology is applied
to describe some data.

The W3Cs Web Ontology Language (OWL 2.0) is a very expressive but decidable description logic: a fragment of first order predicate calculus. A brief
and incomplete introduction is given here: the reader is referred to [1] for a
complete treatment. In common with all ontology languages, a hierarchical taxonomy of concepts (called classes in OWL) is the primary modelling notion.
Individuals can be members (or instances) of concepts and all individuals are
members of the predefined concept Thing, no individuals are members of the
predefined Nothing. Binary relations over concepts, called roles, are used to relate individuals to others, and concept constructors comprising complex logical
expressions over concepts, roles and individuals are used to relate all these things
together. Most important here are role restrictions: expressions that construct
a concept by referring to relations to other concepts. There are also a range of role
?

?

?
characteristics that can constrain the relations wherever they occur: such as
domain, range, transitive, subproperty and inverse. Two key features of OWL
designed for the semantic web applications is that all entities: classes (concepts),
properties (roles) and individuals are identified by URI (a Web identifier that can
be a URL), and that it has an RDF/XML serialization (commonly considered
unreadable).

In the experience of these authors, when people meet to develop conceptual structures, including models of knowledge intended to become an OWL
ontology, they very quickly move to sketching 2D images to communicate their
thoughts. At the beginning, these may be simple graph structures of labelled
nodes connected by labelled or unlabelled arcs. For example, figure 1 shows the
whiteboard used at the first Face-to-face meeting of the W3C Semantic Sensor
Networks Incubator Group, in Washington DC, USA, November 2009. Unlike
some modelling languages, OWL does not have a heritage in visual representa-
tions, and modellers struggle with different interpretations of the visualizations
used in the group. For example, in OWL, it is very important to know whether a
node represents an individual or a class. In a more advanced example, modellers
need to know whether a specified subsumption relationship between concepts is
also an equivalence relationship. As we shall see, concept diagrams are capable
of visualizing exactly these kinds of definitions.

Fig. 1. Whiteboard used at face-to-face meeting; Photo: M. Hauswirth

The major feature of OWL as a modelling language is also its greatest hindrance for shared development: the formal semantics and capability for reason-
ing. The examples we give later, in our case study, demonstrate that information
that would sometimes need to be inferred is actually explicitly visible on concept

J. Howse et al.

diagrams (so-called free-rides which we explain later). It is commonplace for
ontology developers, as domain experts, to be unaware of the formal semantics underlying OWL, and if they are aware it remains very difficult to apply the knowledge of the semantics in practice while developing in a team environment. For
example, even the simple difference between universal and existential role restrictions are difficult to represent and to assimilate in diagrammatic form. As
another example, the semantic difference between rdfs:domain and rdfs:range constraints on properties, as opposed to local restrictions on those properties in the
context of class definitions, is difficult to represent diagrammatically and very
hard to take into account when studying spatially-disconnected but semanticallyconnected parts of an ontology sketch. There is a need for semantically-informed
sketching tools that help ontology developers to better understand their modelling
in real time.

3 Visualizing the SSN Ontology

In this section we walk through parts of the SSN ontology, showing how to
express it in our concept diagrams. At the end of section 3.2 we will give
a summary of the concept diagram syntax. The SSN ontology is available at
purl.oclc.org/NET/ssnx/ssn and extensive documentation and examples of its
use are available in the final report of the SSN-XG [2]. An alternative visualization of the SSN ontology was created using CMAP from IHMC (see
www.ihmc.us/groups/coe/) and may be compared with the visualisation presented here. The ontology imports, and is aligned with, the Dolce Ultra-Lite
upper ontology [14] from which it inherits upper concepts including Event, Ob-
ject, Abstract, Quality, PhysicalObject, SocialObject, InformationObject, Situation,
Description, Method, and Quality.

3.1 Concept Hierarchy Axioms

To represent the concept hierarchy, concept diagrams use Euler diagrams [8],
which effectively convey subsumption and disjointness relationships. In particu-
lar, Euler diagrams comprise closed curves (often drawn as circles or ellipses) to
represent sets (in our case, concepts). Two curves that have no common points
inside them assert that the represented sets are disjoint whereas one curve drawn
completely inside another asserts a subsumption relationship. In addition, Euler
diagrams use shading to assert emptiness of a set; in general, concept diagrams
use shading to place upper bounds on set cardinality as we will see later.

In the SSN ontology, descriptions of the concepts are given as comments in
the ssn.owl file [2], which we summarize here. The SSN ontology is defined over
a large vocabulary of which we present the subset required for our case study.
At the top level of the SSN hierarchy are four concepts, namely Entity, Feature-
OfInterest, Input, and Output. The concept Entity is for anything real, possible
or imaginary that the modeller wishes to talk about. Entity subsumes five other
concepts which, in turn, may subsume further concepts. The five concepts are:
