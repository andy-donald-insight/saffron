Effectively Interpreting Keyword Queries on

RDF Databases with a Rear View

Haizhou Fu and Kemafor Anyanwu

Semantic Computing Research Lab, Department of Computer Science,

North Carolina State University, Raleigh NC 27606, USA

{hfu,kogan}@ncsu.edu

Abstract. Effective techniques for keyword search over RDF databases
incorporate an explicit interpretation phase that maps keywords in a
keyword query to structured query constructs. Because of the ambiguity
of keyword queries, it is often not possible to generate a unique interpretation for a keyword query. Consequently, heuristics geared toward
generating the top-K likeliest user-intended interpretations have been
proposed. However, heuristics currently proposed fail to capture any
user-dependent characteristics, but rather depend on database-dependent
properties such as occurrence frequency of subgraph pattern connecting
keywords. This leads to the problem of generating top-K interpretations
that are not aligned with user intentions. In this paper, we propose a
context-aware approach for keyword query interpretation that personalizes the interpretation process based on a users query context. Our
approach addresses the novel problem of using a sequence of structured
queries corresponding to interpretations of keyword queries in the query
history as contextual information for biasing the interpretation of a new
query. Experimental results presented over DBPedia dataset show that
our approach outperforms the state-of-the-art technique on both efficiency and effectiveness, particularly for ambiguous queries.

Keywords: Query Interpretation, Keyword Search, Query Context, RDF
Databases.

1 Introduction

Keyword search offers the advantage of ease-of-use but presents challenges due
to their often terse and ambiguous nature. Traditional approaches [1][2][8] for
answering keyword queries on (semi)structured databases have been based on an
assumption that queries are explicit descriptions of semantics. These approaches
focus on merely matching the keywords to database elements and returning some
summary of results i.e., IR-style approaches. However, in a number of scenarios,
such approaches will produce unsatisfactory results. For example, a query like
Semantic Web Researchers needs to be interested as a list of people, many
of which will not have all keywords in their labels and so will be missed by
IR-style approaches. For such queries, each keyword needs to be interpreted
and the entire query needs to be mapped to a set of conditional expressions,

L. Aroyo et al. (Eds.): ISWC 2011, Part I, LNCS 7031, pp. 193208, 2011.
c Springer-Verlag Berlin Heidelberg 2011

H. Fu and K. Anyanwu

i.e., W HERE clause and return clause. It is not often easy to find a unique
mapping, therefore this problem is typically done as a top-K problem with the
goal of identifying the K likeliest user intended interpretations.

Existing top-K query interpretation approaches [11] for RDF databases employ a cost-based graph exploration algorithm for exploring schema and data
to find connections between keyword occurrences and essentially fill in the gaps
in a keyword query. However, these techniques have the limitation of using a
one-size-fits-all approach that is not user-dependent but rather more database-
dependent. The heuristics used are based on the presumption that the likeliest
intended interpretation is the interpretation that has the most frequent support
in the database, i.e., the interpretation is related to classes of high-cardinality.
Unfortunately, since such metrics are not user-dependent, the results generated
do not always reflect the user intent.

In this paper, we address the problem of generating context-aware query interpretations for keyword queries on RDF databases by using information from
a users query history. The rationale for this is that users often pose a series of
related queries, particularly in exploratory scenarios. In these scenarios, information about previous queries can be used to influence the interpretation of a
newer query. For example, given a keyword query Mississippi River, if a user
had previously queried about Mortgage Rates,then it is more reasonable to
select the interpretation of the current query as being that of a financial institution Mississippi River Bank. On the other hand, if a users previous query was
Fishing Techniques, it may make more sense to interpret the current query
as referring to a large body of water: the Mississippi River. Two main challenges that arise here include (i) effectively capturing and efficiently representing
query history and (ii) effectively and efficiently exploiting query history during
query interpretation. Towards addressing these challenges we make the following
contributions:

i) Introduce and formalize the problem of Context-Aware keyword query inter-

pretation on RDF databases.

ii) Propose and implement a dynamic weighted summary graph model that is
used to concisely capture essential characteristics of a users query history.
iii) Design and implement an efficient and effective top-K Context-Aware graph
exploration algorithm that extends existing cost-balanced graph exploration
algorithms, with support for biasing the exploration process based on context
as well as with early termination conditions based on a notion of dominance.
iv) Present a comprehensive evaluation of our approach using a subset of the
DBPedia dataset [3], and demonstrate that the proposed approach outperforms the state-of-the-art technique on both efficiency and effectiveness.

2 Foundations and Problem Definition

Let W be an alphabet of database tokens. An RDF database is a collection
of subject-property-object triples linking RDF resources. These triples can be
?

?

?
represented as a graph GD = VD, ED, D, D, where subject or object is represented as node in VD while property is represented by edge in ED. An object
node can either represent another entity (RDF resource) or literal value. D is a
labeling function D : (VD  ED)  2W that captures the rdfs:label declarations
and returns a set of all distinct tokens in the label of any resource or property in
the data graph. In addition, for any literal node vl  VD, D(vl) returns all distinct tokens in the literal value represented by vl. D is the incidence function:
D : VD  VD  ED.

An RDF schema is also a collection of subject-property-object triples, which
can also be represented as a graph: GS = (VS, ES, S, S, ), where the nodes
in VS represent classes and edges in ES represent properties. S is a labeling
function S : VSES  2W that captures the rdfs:label declarations and returns
a set of all distinct tokens in the label of any class or property in the schema
graph. S is an incidence function: S : VS  VS  ES.  : VS  2VD is
a mapping function that captures the predefined property rdf:type maping a
schema node representing a class C to a set of data graph nodes representing
instances of C. Nodes/edges in a schema can be organized in a subsumption
hierarchy using predefined properties rdfs:subclass and rdfs:subproperty.

We define some special nodes and edges in the schema graph that are necessary

literal nodes representing literal types such as XSD:string);

for some of the following definitions:
 let VLIT ERAL  VS be a set containing all literal type nodes (i.e., a set of
 let VLEAF CLASS  (VS  VLIT ERAL) be a set containing all leaf nodes (i.e.
those nodes representing classes who do not have sub-classes) which are not
literal type nodes;
 let ELEAF P ROP ERT Y  ES be a set containing all leaf edges (i.e. those
 let VLEAF LIT ERAL  VLIT ERAL be a set containing all literal type nodes
who are joined with leaf edges, for example, in Fig 1, literal type node vstring1
is in VLEAF LIT ERAL but vstring2 is not because the edge ename connecting
vP lace and vstring2 is not a leaf edge.

edges representing properties which do not have sub-properties);

We define a keyword query Q = {w1, w2, . . . , wn|wi  W} as a sequence of
keywords, each of which is selected from the alphabet W . Given a keyword
query Q, an RDF schema and data graphs, the traditional problem that is addressed in relation to keyword queries on RDF databases is how to translate an
keyword (unstructured) query Q into a set of conjunctive triple patterns (struc-
tured query) that represents the intended meaning of Q. We call this process
as keyword query structurization/interpretation. To ensure that the
structured query has a defined semantics for the target database, the translation process is done on the basis of information from the data and schema graphs.
For example, given a keyword query Mississippi River Bank, the schema graph
and the data graph shown in Fig 1, we can find a structured query with conjunctive triple patterns listed at the top of Fig 1. Because of the class hierarchy
defined in the schema, there could be many equivalent triple patterns for a given
keyword query. For instance, in the schema graph of Fig 1, Organization is the

H. Fu and K. Anyanwu

Fig. 1. Graph Summarization

super class of Bank. Assuming that only Bank has the property bName,
thus, the two pattern queries:

?x bName river, ?x rdf:type Organization, and
?x bName river, ?x rdf:type Bank
are equivalent because the domain of the property bName requires that the
matches of ?x can only be the instances of Bank. To avoid redundancy and
improve the performance, usually a summary graph structure is adopted that
concisely summarizes the relationships encoded in the subsumption hierarchies
and the relationships between tokens and the schema elements they are linked
to.

Recall that our goal is to enable context-awareness for keyword query interpre-
tation, we would also like this summary graph structure to encode information
about a users query history such as which classes have been associated with
recent queries. This leads to a notion of a context-aware summary graph which
is defined in terms of the concept of Upward Closure:

DEFINITION 1 (Upward closure): Let vC be a node in a schema graph

GS that represents a class C. The upward closure of vC is v
C, which is a set
containing vC and all the nodes representing super classes of C. For example, in
State = {vT hing, vP lace, vState}.

Fig 1, the upward closure of the node vState is: v
The upward closure of an edge eP  ES denoted by e


P is similarly defined.
?

?

?
DEFINITION 2 (Context-aware Summary Graph) : Given an RDF
schema graph GS, a data graph GD and a query history QH: QH = {Q1, . . . ,
QT}, where QT is the most recent query, a context-aware summary graph can
be defined as SG = (VSG, ESG, , SG, SG, ), where
  : VSG  ESG  2(VSES) is an injective mapping function that maps any
 VSG = {vi|u  VLEAF CLASS  VLEAF LIT ERAL such that (vi) = u

node or edge in SG to a set of nodes or edges in GS.

For example, the context-aware summary graph in Fig 1 contains {v1,
v2, v3, v4} four nodes, each of which can be mapped to the upward closure
of one of the leaf nodes in {vstring1, vBank, vState, vstring3} in the schema
graph respectively.

 ESG = {ei|u  ELEAF P ROP ERT Y such that (ei) = u

For example, the summary graph in Fig 1 contains {e1, e2, e3} three
edges, each of which can be mapped to the upward closure of one of the leaf
edges in {ebN ame, elocatedIn, esN ame} respectively.
 v  VSG where (v) = v

 SG is a labeling function: SG : (VSG  ESG)  2W .
rj(vC ) D(rj)},

C and vC  VS representing class C,


S(vi)}  {

SG(v) = {

}.

}.

viv



i.e., union of all distinct tokens in the labels of the super classes of C
and distinct tokens in labels of all instances of C. For example, in Fig 1,

SG(v4) = { Mississippi, North, Carolina};
SG(v3) = { Thing, Place, State}.

 e  ESG where (e) = e
P and eP  ES representing property P ,

S(ei),

SG(e) =
?

?

?
eie


which is a union of all distinct tokens in the labels of all super classes of
P . For example,

SG(e2) = { LocatedIn, In}.


C2, (e) = e


C1, (v2) = v
v

 SG is the incidence function: SG : VSG  VSG  ESG, such that if (v1) =

P , then SG(v1, v2) = e implies S(vC1, vC2) = eP .
  : (QH, VSG  ESG)  R is a query history dependent weighting function
that assigns weights to nodes and edges of SG. For a query history QHT1
and QHT = QHT1 + QT , and m  SG, (QHT1, m)  (QHT , m) if
m  QT .

Note that, we only consider user-defined properties for summary graph while
excluding pre-defined properties. Further, we refer to any node or edge in a
context-aware summary graph as a summary graph element.

DEFINITION 3 (Hit): Given a context-aware summary graph SG and a
keyword query Q, a hit of a keyword wi  Q is a summary graph element
m  SG such that wi  (m) i.e., wi appears in the label of m. Because there
could be multiple hits for a single keyword w, we denote the set of all hits of w
as HIT (w). For example, in Fig 1, HIT (bank) = {v1, v2}.
DEFINITION 4 (Keyword Query Interpretation): Given a keyword query
Q and a context-aware summary graph SG, a keyword query interpretation QI

H. Fu and K. Anyanwu

is a connected sub-graph of SG that connects at least one hit of each keyword
in Q.

For example, the summary graph shown in Fig 1 represents the interpretation
of the keyword query Mississippi, River, Bank which means Returning those
banks in the Mississippi State whose name contains the keyword River . The
equivalent conjunctive triple patterns are also shown at the top of Fig 1. Note
that for a given keyword query Q, there could be many query interpretations due
to all possible combinations of hits of all keywords. Therefore, it is necessary to
find a way to rank these different interpretations based on a cost function that
optimizes some criteria which captures relevance. We use a fairly intuitive cost
function in the following way: cost(QI) =
?

?

?
which defines the cost of an interpretation as a combination function of the
weights of the elements that constitute the interpretation. We can formalize the
context-aware top-k keyword query interpretation problem as follows:

miQI (mi),

DEFINITION 5 (Context-aware Top-k Keyword Query Interpretation
Problem): Given a keyword query Q, and a context-aware summary graph SG,
let [[Q]] = {QI1, . . . , QIn} be a set of all possible keyword query interpretations
of Q, the context-aware top-K keyword query interpretation problem is to find the
top K keyword query interpretations in IS: T OP K = {QI1, . . . , QIK}  [[Q]]
such that

(i.) QIi  T OP K and QIj  ([[Q]]  T OP K), cost(QIi)  cost(QIj).
(ii.) If 1  p < q  k, cost(QIp)  cost(QIq), where QIp, QIq  T OP K.

This problem is different from the traditional top-k keyword query interpretation problem in that the weights are dynamic and are subject to the evolving
context of query history. Because some queries are more ambiguous than others,
keyword query interpretation problem requires effective techniques to deal with
large interpretation space. We propose a concept called Degree of Ambiguity
(DoA) for characterizing the ambiguity of queries: The DoA of the keyword
|HIT (wi)|, which is the number of all
query Q is defined as DoA(Q) =
combinations of keyword matches. It will be used as a performance metric in our
evaluation.
?

?

?
wiQ

Overview of our approach. Having defined the problem, we start with an
overview of our approach. It consists of the following key steps as shown in
Fig 2:

 Find keyword hits using an inverted index for a given keyword query Q.

(Step (1)(3)).

 The query interpreter takes the hits and utilizes a graph exploration algo-

rithm to generate a set of top-K interpretations of Q. (Step (4)(5))

 The top-1 interpretation of the top-K interpretation is passed to a cost model
to update the weights of the context-aware summary graph. (Step (6)(7))
 Steps involved in Fig 2 only capture one of the iteration cycles of the interactions between user and our interpretation system. The new weights of
context-aware summary graph will be used to bias the graph exploration in
the next iteration when user issues a new query
?

?

?
Fig. 2. Architecture and workflow

The cost model will be discussed in the next section and the graph exploration
algorithm will be discussed in section 4.

3 Representing Query History Using a Dynamic Cost

Model

The implementation of the dynamic cost model for representing query history
consists of two main components: i) data structures for implementing a labeled
dynamic weighted graph i.e., the context-aware summary graph; ii) a dynamic
weighting function that assigns weights to summary graph elements in a way that
captures their relevance to the current querying context. To understand what
the weighting function has to achieve, consider the following scenario. Assuming
that we have the following sequence of queries Q1 = Ferrari, price and Q2 =
F1, calendar, with Q2 as the most recent query. If Q1 is interpreted as Car,
the relevance score of the concept Car as well as related concepts (concepts
in their immediate neighborhood such as Auto Engine) should be increased.
When Q2 arrives and is interpreted as Competition, the relevance score for it
should be increased. Meanwhile, since Auto Engine and all the other concepts
that are not directly related to Q2, their relevance scores should be decreased.
Then ultimately, for a new query Q3 = Jaguar, speed, we will prefer the
concept with higher relevance score as its interpretation, for example, we prefer
Car than Mammal.

To achieve this effect, we designed the dynamic weighting function to be based
on a relevance function in terms of two factors: historical impact factor (hif)
and region factor (rf).
Let T indicate the historical index of the most recent query QT , t be the
historical index of an older keyword query Qt, i.e., t  T , and m denote a

H. Fu and K. Anyanwu

T

(m, T ) =

T

d(m,QIt)

summary graph element. Assume that the top-1 interpretation for Qt has already
been generated : QIt. Region factor is defined as a monotonically decreasing
function of the graph distance d(m, QIt) between m and QIt:

rf(d(m, QIt)) =
(rf(d(m, QIt)) = 0 if d(m, QIt)  )  is a constant value, and d(m, QIt)
is the shortest distance between m and QIt, i.e., among all the paths from
m to any graph element in the sub-graph QIt, d(m, QIt) is the length of the
shortest path. Here,  > 1 is a positive integer constant. The region factor
represents the relevance of m to QIt . Historical impact factor captures the
property that the relevance between a query and a graph element will decrease
when that query ages out of the query history. hif is a monotonically decreasing
function: hif(t) = 1/Tt, where  > 1 is also a positive integer constant. We
combine the two factors to define the relevance of m to query interpretation QIt
as hif (t)rf(d(m, QIt)). To capture the aggregate historical and region impacts
of all queries in a users query history, we use the combination function as the
relevance function  :

hif (t)  rf (d(m, QIt)) =

(1/Tt)(1/d(m,QIt))

(1)

To produce a representation of (1) for a more efficient implementation, we rewrite
a function as recursive:

(m, T ) = (m, T  1)/ + 1/d(m,QIT )

(2)
The consequence of this is that, given the relevance score of m at time T  1,
we can calculate (m, T ) simply by dividing (m, T  1) by  then adding
1/d(m,QIT ). In practice, we use d(m, QIT ) <  = 2, so that, only m and the
neighboring nodes and edges of m will be have their scores updated.

Boostrapping. At the initial stage, there are no queries in the query history,
so the relevance score of the summary graph elements can be assigned based on
the T F  IDF score, where each set of labels of a summary graph element m
i.e., SG(m) is considered as a document. User-feedback is allowed at every stage
to select the correct interpretation if the top-1 query interpretation generated is
not the desired one.

Since top-K querying generation is based on finding the smallest cost connected subgraphs of the summary, the definition of our weighting function for
the dynamic weighted graph model is defined as the following function of the
relevance function .

(m, t) = 1 + 1/(m, t)

(3)
This implies that a summary graph element with a higher relevance value will
be assigned a lower weight. In the next section, we will discuss how to find
interpretations with top-k minimal costs.
?

?

?
4 Top-K Context-Aware Query Interpretation

The state of the art technique for query interpretation uses cost-balanced graph
exploration algorithms [11]. Our approach extends such an algorithm [11] with
a novel context-aware heuristic for biasing graph exploration. In addition, our
approach improves the performance of the existing algorithm by introducing an
early termination strategy and early duplicate detection technique to eliminate
the need for duplicate detection as a postprocessing step. Context Aware Graph
Exploration (CoaGe) algorithm shown in Fig 3.

Fig. 3. Pseudocodes for CoaGe and TopCombination

4.1 CoaGe

CoaGe takes as input a keyword query Q, a context-aware summary graph SG
and an integer value K indicating the number of candidate interpretations that
should be generated. In CoaGe, a max binomial heap T OP K is used to maintain
top-K interpretations and a min binomial heap CQ is used to maintain cursors
created during the graph exploration phase (line 1). At the initialization stage,
for each hit of each keyword, CoaGe generates a cursor for it. A cursor originates
from a hit mw of a keyword w is represented as c(keyword, path, cost, topN),
where c.keyword = w; c.path contains a sequence of summary graph elements
in the path from mw to the node that c just visited; c.cost is the cost of the
path, which is the sum of the weights of all summary graph elements in c.path;

H. Fu and K. Anyanwu

c.topN is a boolean value that indicates whether mw is among the top-N hits of
HIT (w). The Top-N hit list contains the N minimum weighted hits of all hits
in HIT (w).

wiQ

Each node v in the context-aware summary graph has a cursor manager CL
that contains a set of lists. Each list in CL is a sorted list that contains a sequence
of cursors for keyword w that have visited v, we use CL[w] to identify the list
of cursors for keyword w. The order of the elements in each list is dependent
on the costs of cursors in that list. The number of lists in CL is equal to the
number of keywords: |CL| = |Q|.During the graph exploration, the cursor with
minimal cost is extracted from CQ (line 5). Let v be the node just visited by this
cheapest cursor (line 6). CoaGe first determines whether v is a root (line 7).

This is achieved by examining if all lists in v.CL is not empty, in other words,
at least one cursor for every keyword has visited v. If v is a root, then, there
|v.CL[wi]| combinations of cursors. Each combination of cursors can
are
be used to generate a sub-graph QI. However, computing all combinations of
cursors as done in the existing approach [11] does is very expensive. To avoid
this, we developed an algorithm T opCombination to enable early termination
during the process of enumerating all combinations. T opCombination algorithm
(line 8) will be elaborated in the next subsection. A second termination condition
for the CoaGe algorithm is if the smallest cost of CQ is larger than the largest
cost of the top-K interpretations (line 9). After the algorithm checks if v is a root
or not, the current cursor c explores the neighbors of v if the length of c.path is
less than a threshold (line 11). New cursors are generated (line 14) for unvisited
neighbors of c (not in c.path, line 13). New cursors will be added to the cursor
manager CL of v (line 17). The cost of new cursors are computed based on the
cost of the path and if c is originated from a top-N hits.

Unlike the traditional graph exploration algorithms that proceed based on
static costs, we introduce a novel concept of velocity for cursor expansion.
Intuitively, we prefer an interpretation that connects keyword hits that are more
relevant to the query history, i.e., lower weights. Therefore, while considering
a cursor for expansion, it penalizes and therefore slows down the velocity of
cursors for graph elements that are not present in the top-N hits (line 16). By
so doing, if two cursors have the same cost or even cursor cA has less cost than
cursor cB, but cB originates from a top-N hit, cB may be expanded first because
the cost of cA is penalized and cA.cost  penalty f actor > cB.cost. The space
|HIT (wi)| is the total
complexity is bounded by O(n  dD), where n =
number of keyword hits, d = (SG) is the maximum degree of the graph and
D is the maximum depth a cursor can explore.
?

?

?
wiQ

4.2 Efficient Selection of Computing Top-k Combinations of

Cursors

The T opCombination algorithm is used to compute the top-K combinations
of cursors in the cursor manager CL of a node v when v is a root. This algorithm avoids the enumeration of all combinations of cursors by utilizing a
notion of dominance between the elements of CL. The dominance relation-
?

?

?
ship between two combinations of cursors Comp = (CL[w1][p1], ...CL[wL][pL])
and Comq = (CL[w1][q1], ...CL[wL][qL]) is defined as follows: Comp dominates
Comq, denoted by Comp  Comq if for all 1  i  L = |Q|, pi  qi and exists
1  j  L, pj > qj. Because every list CL[wi]  CL is sorted in a non decreasing
order, i.e., for all 1  s  L, i  j implies that CL[ws][i].cost  CL[ws][j].cost .
Moreover, because the scoring function for calculating the cost of a combination
Com is a monotonic function: cost(Com) =
ciCom ci.cost, which equals to
the sum of the costs of all cursors in a combination, then we have:
?

?

?
Comp = (CL[w1][p1], CL[w2][p2], ..., CL[wL][pL])

(Comq = CL[w1][q1], CL[w2][q2], ..., CL[wL][qL])
implies that for all 1  i  L,
CL[wi][pi].cost  CL[wi][qi].cost and cost(Comp)  cost(Comq).
In order to compute top-k minimal combinations, given the combination
Commax with the max cost in the top-k combinations, we can ignore all the
other combinations that dominate Commax. Note that, instead of identifying all
non-dominated combinations as in line with the traditional formulation, our goal
is to find top-K minimum combinations that require dominated combinations to
be exploited.

The pseudocodes of the algorithm T opKCombination is shown in Fig 3.
T opKCombination takes as input a max binomial heap T OP K, a cursor manager CL and an integer value K indicating the number of candidate interpretations that should be generated. The algorithm has a combination enumerator
Enum that is able to enumerate possible combinations of cursors in CL (line
1). T L is initialized to contain a list of combinations as thresholds (line 2). The
enumerator starts from the combination

Com0 = (CL[w1][0], CL[w2][0], ..., CL[wL][0]),
which is the cheapest combination in CL. Let
Comlast = (CL[w1][l1], CL[w2][l2], ..., CL[wL][lL]), be the last combination,
which is the most expensive combination and li = CL[wi].length  1, which
is the last index of the list CL[wi].

The enumerator outputs the next combination in the following way: if the

current combination is

Comcurrent = (CL[w1][s1], CL[w2][s2], ..., CL[wL][sL]),
from 1 to L, Enum.N ext() locates the first index i, where 1  i  L such

that si  li, and returns the next combination as Comnext =
(CL[w1][0], ..., CL[wi1][0], CL[wi][si + 1], ..., CL[wL][sL]) ,
where, for all 1  j < i, sj is changed from lj  1 to 0, and sj = sj +
1. For example, for (CL[w1][9], CL[w2][5]), if CL[w1].length equals to 10 and
CL[w2].length > 5, then, the next combination is (CL[w1][0], CL[w2][6]). The
enumerator will terminate when Comcurrent == Comlast.
Each time Enum move to a new combination cur comb, it is compared with
every combination in T L to check if there exists a threshold combination h  T L
such that cur comb  h (line 4). If so, instead of moving to the next combination
using N ext(), Enum.DirectN ext() is executed (line 5) to directly return the

H. Fu and K. Anyanwu

next combination that does not dominate h and has not been not enumerated
before. This is achieved by the following steps: if the threshold combination is
Comthreshold = (CL[w1][s1], CL[w2][s2], ..., CL[wL][sL]),
from 1 to L, Enum.DirectN ext() locates the first index i, where 1  i  L
such that si = 0, and from i + 1 to L, j is the first index such that sj = lj  1 ,
then the next generated combination is Comdirect next =

(CL[w1][0], ..., CL[wi][0], ..., CL[wj1][0], CL[wj][sj + 1], ..., CL[wL][sL])
where for all i  r < j, sr is changed to 0, and sj = sj + 1. For example, for
comthreshold = (CL[w1][0], CL[w2][6], CL[w3][9], CL[w4][2]),
assume that the length of each list in CL is 10, then its next combination

that does not dominate it is

comdirect next = (CL[w1][0], CL[w2][0], CL[w3][0], CL[w5][3]).
In this way, some combinations that could be enumerated by Next() function and will dominate comthreshold will be ignored. For instance, comnext =
N ext(comthreshold) =

(CL[w1][1], CL[w2][6], CL[w3][9], CL[w4][2]),
and the next combination after this one: N ext(comnext) =
(CL[w1][2], CL[w2][6], CL[w3][9], CL[w4][2])
will all be ignored because they dominate comcurrent.
If a new combination is cheaper than the max combination in T OP K, it will
be inserted to it (line 16), otherwise, this new combination will be considered a
new threshold combination, and inserted to T L (line 12) such that all the other
combinations that dominate this threshold combination will not be enumerated.
The time complexity of T opKCombination is O(K k), where K = |T OP K| is the
size of T OP K, k = |Q| is the number keywords. Because, for any combination

com = (CL[w1][s1], ..., CL[wL][sL]), where for all si, 1  i  L, si  K
comK = (CL[w1][K + 1], ..., CL[wL][K + 1])  com
In the worst case, any combinations that dominates comK will be ignored and
K k combinations are enumerated. Consequently, the time complexity of CoaGe
is O(n dD  K k), where n is the total number of keyword hits, d = (SG) is the
maximum degree of the graph, D is the maximum depth. The time complexity
of the approach in [11] (we call this approach T KQ2S) is O(ndD SD1), where
S = |SG| is the number of nodes in the graph.

5 Evaluation

In this section, we discuss the experiments including efficiency and effectiveness
of our approach. The experiments were conducted on a machine with Intel duel
core 1.86GHz and 3GB memory running on Windows 7 Professional. Our test
bed includes a real life dataset DBPedia, which includes 259 classes and over
1200 properties. We will compare the efficiency and effectiveness with T KQ2S.

5.1 Effectiveness Evaluation
Setup. 48 randomly selected college students were given questionnaires to com-
plete. The questionnaire contains 10 groups of keyword queries (To minimize the
?

?

?
cognitive burden on our evaluators we did not use more than 10 groups in this
questionnaire). Each group contains a short query log consisting of a sequence of
up to 5 keyword queries from the oldest one to the newest one. For each group,
the questionnaire provides English interpretations for each of the older queries.
For the newest query, a list of candidate English interpretation for it is given,
each interpretation is the English interpretation representing a structured query
generated by either T KQ2S or CoaGe. Therefore, this candidate interpretation
list provided to user is a union of the results returned by the two algorithm. Then
users are required to pick up to 2 interpretations that they think are the most
intended meaning of the newest keyword query in the context of the provided
query history. A consensus interpretation (the one that most people pick) was
chosen as the desired interpretation for each keyword query.

K

i=1

Metrics. Our choice of a metric of evaluating the query interpretations is to evaluate how relevant the top-K interpretations generated by an approach is to the
desired interpretation. Further, it evaluates the quality of the ranking of the interpretations with respect to their relative relevance to the desired interpretation.
Specifically, we adopt a standard evaluation metric in IR called Discounted cu-
2reli1
mulative gain (DCG) with a refined relevance function: DCGK =
log2(1+i)
, where K is the number of top-K interpretations generated, reli is the graded
relevance of the resultant interpretation ranked at position i. In IR, the relevance
between a keyword and a document is indicated as either a match or not, reli is
either zero or one. In this paper, the relevance between a resultant interpretation QI and a desired interpretation QID for a given keyword query Q cannot be
simply characterized as either a match or not. QI and QID are both sub-graphs
of the summary graph and could have some degree of overlapping, which means
reli  [0, 1]. Of course, if QI == QID, QI should be a perfect match. In this
experiment, we define the relevance between a candidate interpretation QI and
the desired interpretation QID as: reli =
, where QIi is the
interpretation ranked at position i. reli returns the fraction of those overlapping
summary graph elements in the union of the the two sub-graphs. Large overlapping implies high similarity between QIi and QID, and therefore, high relevance
score. For example, if QIi has 3 graph elements representing a class Person,
a property given name and a class XSD:string. The desired interpretation
QID also has 3 graph elements, and it represents class Person, a property
age and a class XSD:int. Therefore, the relevance between QIi and QID is
equal to 1/5 = 0.2 because the union of them contains 5 graph elements and
they have 1 common node.

|QIiQID||QIiQID|

|QIiQID|

On the other hand, we use another metric precision to evaluate the results.
The precision of a list of top-K candidate interpretation is:
P @K = | relevant interpretations in T OP K|/|T OP K|,
which is the proportion of the relevant interpretations that are generated in
T OP K. Because sometimes, when the user votes are evenly distributed, the
consensus interpretation cannot represent the most user intended answer, our
evaluation based on DCG may not be convincing. The precision metric can

H. Fu and K. Anyanwu

Fig. 4. Efficiency and effectiveness evaluation

overcome this limitation by consider the candidate interpretations that over 10%
people have selected as desired interpretations.

Discussion. We compared the DCG of the results returned by our approach
and T KQ2S. Top-8 queries are generated for each algorithm for each keyword
query. The result shown in Fig 4 (a) illustrates the quality of interpreting queries
with varying DoA values. From (a), we can observe that T KQ2S does not generate good quality interpretations for queries with high DoA. The reason is that
they prefer the popular concepts and rank the desired interpretation which is
not popular but is higher relevant to the query history at a low position. It also
does not rank higher relevant interpretations higher. Fig 4 (b) illustrates the
precision of the top-5 queries generated by each algorithm. In most of cases,
T KQ2S generates same number of relevant queries as CoaBe, but it fails to
?

?

?
generate enough relevant interpretations for the last query with DoA = 3364.
For the first query in (b), T KQ2S does not output any relevant interpretations,
therefore, the precision is 0. Fig 4 (c) illustrates how different lengths of query
history will affect results. In this experiment, 4 groups of queries are given, the
ith group contains i queries in the query history. Further, the ith group contains
all the queries in the (i1)th group plus a new query. Given the 4 different query
histories, the two algorithms are to interpret another query Q. (c) illustrates the
quality of interpreting Q given different query histories. We can observe that our
approach will do better with long query history. But for the first group, both
algorithms generate a set of interpretations that are very similar to each other.
Both the DCG values are high because user have to select from the candidate
list as the desired interpretation, even though they may think none of them is
desired. For the third group, that difference in performance is due to a transition
in context in the query history. Here the context of query changed in the 2nd or
3rd query. This resulted in a lower DCG value which started to increase again
as more queries about new context were added.

5.2 Efficiency Evaluation

From the result of the efficiency evaluation in Fig 4 (d)(f), we can see that, our
algorithm outperforms T KQ2S especially when the depth (maximum length of
path a cursor will explore) and the number of top-K interpretations and the
degree of ambiguity DoA are high. The performance gain is due to the reduced
search space enabled by early termination strategy using the T opKCombination
algorithm.

6 Related Work

There is a large body of work supporting keyword search for relational databases
[1][2][7][8] based on the interpretation as match paradigm. Some recent efforts
such as Keymantic[5] QUICK[13], SUITS[6], Q2Semantics[12] and [11] have focused on introducing an explicit keyword interpretation phase prior to answering
the query. The general approach used is to find the best sub-graphs (of the
schema plus a data graph) connecting the given keywords and represent the intended query meaning. However, these techniques are based on fixed data-driven
heuristics and do not adapt to varying user needs. Alternative approaches [6][13]
use techniques that incorporate user input to incrementally construct queries by
providing them with query templates. The limitation of these techniques is the
extra burden they place on users.

Query history has been exploited in IR [4][9][10]. These problems have different challenges from the ones addressed in this work. The similarity measures
are based on mining frequent query patterns. However, we need to exploit the
query history to identify similar search intent, which may not necessarily be the
most frequent query patterns. Our problem requires unstructured queries, their
intended interpretations (structured queries) and the ontology to be managed.

H. Fu and K. Anyanwu

7 Conclusion and Future Work

This paper presents a novel and effective approach for interpreting keyword
queries on RDF databases by integrating the querying context. In addition to
the techniques proposed in this paper, we plan to explore the idea of exploiting
the answers to a priori queries for query interpretation.

Acknowledgement. The work presented in this paper is partially funded by
NSF grant IIS-0915865.
