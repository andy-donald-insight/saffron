Concurrent Classification of EL Ontologies

Yevgeny Kazakov, Markus Krotzsch, and Frantisek Simancik

Department of Computer Science, University of Oxford, UK

Abstract. We describe an optimised consequence-based procedure for
classification of ontologies expressed in a polynomial fragment ELHR+ of
the OWL 2 EL profile. A distinguishing property of our procedure is that
it can take advantage of multiple processors/cores, which increasingly
prevail in computer systems. Our solution is based on a variant of the
given clause saturation algorithm for first-order theorem proving, where
we assign derived axioms to contexts within which they can be used and
which can be processed independently. We describe an implementation of
our procedure within the Java-based reasoner ELK. Our implementation
is light-weight in the sense that an overhead of managing concurrent
computations is minimal. This is achieved by employing lock-free data
structures and operations such as compare-and-swap. We report on
preliminary experimental results demonstrating a substantial speedup
of ontology classification on multi-core systems. In particular, one of
the largest and widely-used medical ontologies SNOMED CT can be
classified in as little as 5 seconds.

1 Introduction

Ontology classification is one of the key reasoning services used in the development of OWL ontologies. The goal of classification is to compute the hierarchical
representation of the subclass (a.k.a. is-a) relations between the classes in the
ontology based on their semantic definitions. Ontology classification, and ontology reasoning in general, is a computationally intensive task which can introduce
a considerable delay into the ontology development cycle.

Many works have focused on the development of techniques to reduce classification times by optimizing the underlying (mostly tableaux-based) procedures
so that they produce fewer inferences. In this paper we study another way of
reducing the classification time, which is achieved by performing several inferences in parallel, i.e., concurrently. Concurrent algorithms and data structures
have gained substantial practical importance due to the widespread availability
of multi-core and multi-processor systems.

Nonetheless, concurrent classification of ontologies is challenging and only few
works cover this subject. Approaches range from generic divide-and-conquer
strategies when the ontology is divided into several independent components
[18] to more specific strategies involving parallel construction of taxonomies [1],
concurrent execution of tableau rules [11,12], distributed resolution procedures
[15], and MapReduce-based distribution approaches [14]. The practical improvements offered by these strategies, however, remain yet to be demonstrated, as

L. Aroyo et al. (Eds.): ISWC 2011, Part I, LNCS 7031, pp. 305320, 2011.
c Springer-Verlag Berlin Heidelberg 2011

Y. Kazakov, M. Krotzsch, and F. Simancik

empirical evaluation of the proposed approaches is rather limited. As of today,
none of the commonly used ontology reasoners, including CEL, FaCT++, Her-
miT, jCEL, Pellet, and RACER, can make use of multiple processors or cores.
In this paper, we consider a consequence-based classification procedure,
which works by deriving logical consequences from the axioms in the ontology.
Such procedures were first introduced for the EL family of tractable description
logics (DLs) [2], which became the basis of the OWL 2 EL profile [13]. Later the
EL-style classification procedures have been formulated for more expressive lan-
guages, such as Horn-SHIQ [7] and ALC [16]. Consequence-based procedures
have several distinguished properties, such as optimal worst-case complexity,
pay-as-you-go behaviour, and lack of non-determinism (even for expressive DLs
such as ALC). In this paper we will demonstrate that such procedures are also
particularly suitable for concurrent classification of ontologies.

The contributions of this paper can be summarised as follows:
(i) We formulate a consequence-based procedure for the fragment ELHR+ of
an OWL 2 EL profile. The procedure does not require the usual axiom
normalisation preprocessing [2] but works directly with the input ontology.
Although normalisation is usually fast and its effect on the overall running time is negligible, the removal of this preprocessing step simplifies the
presentation of the algorithm and reduces the implementation efforts.

(ii) We describe a concurrent strategy for saturation of the input axioms under
inference rules. The strategy works by assigning axioms to contexts in
which the inferences can be performed independently, and can be used
with arbitrary deterministic inference systems.
(iii) We describe an implementation of our concurrent saturation strategy for
ELHR+ within a Java-based reasoner ELK. We demonstrate empirically
that the concurrent implementation can offer a significant speedup in ontology classification (e.g., a factor of 2.6 for SNOMED CT on a 4-core machine)
and can outperform all existing highly-optimised (sequential) reasoners on
the commonly used EL ontologies. The improved performance is achieved
by minimising the overheads for managing concurrency and is comparable
to the embarrassingly parallel algorithm on the pre-partitioned input.

2 Preliminaries
The vocabulary of ELHR+ consists of countably infinite sets NR of (atomic)
roles and NC of atomic concepts. Complex concepts and axioms are defined
recursively using the constructors in Table 1. We use the letters R, S, T for roles,
C, D, E for concepts and A, B for atomic concepts. A concept equivalence C  D
stands for the two inclusions C  D and D  C. An ontology is a finite set of
axioms. Given an ontology O, we write 
O for the smallest reflexive transitive
O S holds for all R  S  O.
binary relation over roles such that R 
ELHR+ has Tarski-style semantics. An interpretation I consists of a non-
I called the domain of I and an interpretation function I that

empty set 
?

?

?
Table 1. Syntax and semantics of ELHR+

Roles:

atomic role
Concepts:

atomic concept
top
conjunction
existential restriction

Axioms:

concept inclusion
role inclusion
transitive role

Syntax

Semantics

I

CI  DI
?

?

?
C  D
R.C {x | y : x, y  RI  y  CI}
C  D
R  S
Trans(T )

T I is transitive

CI  DI
RI  SI

Table 2. Inference rules for ELHR+
C  D1 C  D2

C  D
C  E : D  E  O
C  D1  D2

C  D1
C  D2
C  R.D
D  D
C  C
C   :  occurs in O

R

R
?

?

?
R


R+

R+

R+

: D1  D2 occurs in O

C  D1  D2
C  D
S.C  S.D : S.D occurs in O
D  R.C S.C  E

D  E

D  R.C S.C  E

T.D  E

O S

: R 
R 
O S
Trans(T )  O

O T 

:

I  

I  

I and to each A a set A

I  
I.
assigns to each R a binary relation R
The interpretation function is extended to complex concepts as shown in Table 1.
An interpretation I satisfies an axiom  (written I |= ) if the corresponding
condition in Table 1 holds. If an interpretation I satisfies all axioms in an ontology O, then I is a model of O (written I |= O). An axiom  is a consequence of
an ontology O (written O |= ) if every model of O satisfies . A concept C is
subsumed by D w.r.t. O if O |= C  D. Classification is the task of computing
all subsumptions A  B between atomic concepts such that O |= A  B.
3 A Classification Procedure for ELHR+
Table 2 lists the inference rules of our classification procedure, which are closely
related to the original completion rules for EL++ [2]. The rules operate with
two types of axioms: (i) subsumptions C  D and (ii) (existential) implications
R.C  S.D, where C and D are concepts, and R and S roles. The implications
R.C  S.D have the same semantic meaning as R.C  S.D; we use the
symbol  just to distinguish the two types of axioms in the inference rules.

Y. Kazakov, M. Krotzsch, and F. Simancik

KneeJoint  Joint  isPartOf.Knee

LegStructure  Structure  isPartOf.Leg

Joint  Structure
Knee  hasLocation.Leg

hasLocation  isPartOf

Trans(isPartOf)

Fig. 1. A simple medical ontology describing some anatomical relations

(1)
(2)
(3)
(4)
(5)
(6)



Note that we make a distinction between the premises of a rule (appearing
above the horizontal line) and its side conditions (appearing after the colon),
and use axioms from O as the side conditions, not as the premises.

It is easy to see that the inference rules are sound in the sense that every
conclusion is always a logical consequence of the premises and the ontology,
assuming that the side conditions are satisfied. For all rules except the last one
this is rather straightforward. The last rule works similarly to the propagation
of universal restrictions along transitive roles if we view axioms S.C  E and
T.D  E as C  S
.E respectively. Before we formulate a
suitable form of completeness for our system, let us first consider an example.
Example 1. Consider the ontology O in Fig. 1 expressing that a knee joint is
a joint that is a part of the knee (1), a leg structure is a structure that is a
part of the leg (2), a joint is a structure (3), a knee has location in the leg (4),
has-location is more specific than part-of (5), and part-of is transitive (6).

.E and D  T



Below we demonstrate how the inference rules in Table 2 can be used to prove
that a knee joint is a leg structure. We start with a tautological axiom saying
that knee joint is a knee joint and then repeatedly apply the inference rules:

KneeJoint  KneeJoint
KneeJoint  Joint  isPartOf.Knee
KneeJoint  Joint
KneeJoint  isPartOf.Knee

input axiom,
by R (7) : (1),
by R
by R

 (8),
 (8).

(7)
(8)
(9)
(10)

In the last axiom, we have obtained an existential restriction on knee, which now
allows us to start deriving subsumption relations for this concept thanks to R
 :

Knee  Knee
Knee  hasLocation.Leg

 (10),

by R
by R (11) : (4).

Similarly, the last axiom lets us start deriving subsumptions for leg:

Leg  Leg

by R

 (12).

(11)
(12)

(13)
?

?

?
This time, we can use (13) to derive existential implications using the fact that
hasLocation.Leg and isPartOf.Leg occur in O:

hasLocation.Leg  hasLocation.Leg

isPartOf.Leg  isPartOf.Leg

by R+ (13),
by R+ (13).

(14)
(15)

The last implication, in particular, can be used to replace the existential restriction in (12) using hasLocation 

O isPartOf, which is a consequence of (5):

Knee  isPartOf.Leg

by RH (12), (15).

(16)
O isPartOf 

Similarly, we can derive a new implication using hasLocation 
isPartOf and transitivity of isPartOf (6):
isPartOf.Knee  isPartOf.Leg

by RT (12), (15).

(17)

This implication can now be used to replace the existential restriction in (10):

KneeJoint  isPartOf.Leg

by RH (10), (17).

(18)

Finally, we construct the definition of leg structure (2) using (3) and the fact
that the concept Structure  isPartOf.Leg occurs in O:

KneeJoint  Structure
KneeJoint  Structure  isPartOf.Leg
KneeJoint  LegStructure

by R (9) : (3),
by R+ (18), (19),
by R (20) : (2).

(19)
(20)
(21)

We have thus proved that a knee joint is a leg structure.

In the above example we have demonstrated that a consequence subsumption
KneeJoint  LegStructure can be derived using the inference rules in Table 2
once we start with a tautology KneeJoint  KneeJoint. It turns out that all
implied subsumption axioms with KneeJoint on the left-hand-side and a concept
occurring in the ontology on the right-hand-side can be derived in this way:

Theorem 1. Let S be any set of axioms closed under the inference rules in
Table 2. For all concepts C and D such that C  C  S and D occurs in O we
have O |= C  D implies C  D  S.
Proof (Sketch). Due to lack of space, we can only present a proof sketch. A more
detailed proof can be found in the accompanying technical report [8].
The proof of Theorem 1 is by canonical model construction. We construct an
interpretation I whose domain consists of distinct elements xC, one element xC
for each axiom C  C in S. Atomic concepts are interpreted so that xC  A
I iff
C  A  S. Roles are interpreted by minimal relations satisfying all role inclusion
and transitivity axioms in O such that xC , xD  R
I for all C  R.D  S.

Y. Kazakov, M. Krotzsch, and F. Simancik

Using rules R

 and R

D we have:

 one can prove that for all elements xC and all concepts
C  D  S implies xC  D

(22)
Conversely, using rules R+, R+ , R+ , RH and RT one can prove that for all

.

I.

concepts D occurring in O and all concepts C we have:
I implies C  D  S.

xC  D

I  D

I  E

I by (22). Thus D

I and, since I is a model of O, O |= C  D.

(23)
Properties (22) and (23) guarantee that I is a model of O. To see this, let
D  E  O and xC be an arbitrary element of I. If xC  D
I, then C  D  S
by (23), then C  E  S by rule R, so xC  E
Finally, to complete the proof of Theorem 1, let C and D be such that C 
C  S and D occurs in O. We will show that C  D / S implies O |= C  D.
I by (23). Since C  C  S, we have xC  C
Suppose C  D / S. Then xC / D
?

?

?
by (22). Hence C
It follows from Theorem 1 that one can classify O by exhaustively applying the inference rules to the initial set of tautologies input = {A  A |
A is an atomic concept occurring in O}.
Corollary 1. Let S be the closure of input under the inference rules in Table 2.
Then for all atomic concepts A and B occurring in O we have O |= A  B if
and only if A  B  S .
Finally, we note that if all initial axioms C  D are such that both C and D
occur in O (as is the case for the set input defined above), then the inference
rules derive only axioms of the form (i) C  D and (ii) R.C  E with C, D,
E and R occurring in O. There is only a polynomial number of such axioms,
and all of them can be computed in polynomial time.

4 Concurrent Saturation under Inference Rules

In this section we describe a general approach for saturating a set of axioms
under inference rules. We first describe a high level procedure and then introduce
a refined version which facilitates concurrent execution of the inference rules.

4.1 The Basic Saturation Strategy

The basic strategy for computing a saturation of the input axioms under inference rules can be described by Algorithm 1. The algorithm operates with
two collections of axioms: the queue of scheduled axioms for which the rules
have not been yet applied, initialized with the input axioms, and the set of pro-
cessed axioms for which the rules are already applied, initially empty. The queue
of scheduled axioms is repeatedly processed in the while loop (lines 38): if the
next scheduled axiom has not been yet processed (line 5), it is moved to the set
of processed axioms (line 6), and every conclusion of inferences involving this
?

?

?
Algorithm 1. saturate(input): saturation of axioms under inference rules
Data: input: set of input axioms
Result: the saturation of input is computed in processed

1 scheduled  input;
2 processed  ;
3 while scheduled =  do
?

?

?
axiom  scheduled.poll();
if not processed.contains(axiom) then

processed.add(axiom);
for conclusion  deriveConclusions(processed, axiom) do

scheduled.put(conclusion);

axiom and the processed axioms is added to the queue of scheduled axioms. This
strategy is closely related to the given clause algorithm used in saturation-based
theorem proving for first-order logic (see, e.g., [3]).

Soundness, completeness, and termination of Algorithm 1 is a consequence of

the following (semi-) invariants that can be proved by induction:

(i) Every scheduled and processed axiom is either an input axiom, or is obtained by an inference rule from the previously processed axioms (sound-
ness).

(ii) Every input axiom and every conclusion of inferences between processed
axioms occurs either in the processed or scheduled axioms (completeness).
(iii) In every iteration of the while loop (lines 38) either the set of processed
axiom increases, or, otherwise, it remains the same, but the queue of scheduled axioms becomes shorter (termination).

Therefore, when the algorithm terminates, the saturation of the input axioms
under the inference rules is computed in the set of processed axioms.

The basic saturation strategy described in Algorithm 1 can already be used
to compute the saturation concurrently. Indeed, the while loop (lines 38) can
be executed from several independent workers, which repeatedly take the next
axiom from the shared queue of scheduled axiom and perform inferences with
the shared set of processed axioms. To remain correct with multiple workers, it
is essential that Algorithm 1 adds the axiom to the set of processed axioms in
line 6 before deriving conclusions with this axiom, not after that. Otherwise, it
may happen that two workers simultaneously process two axioms between which
an inference is possible, but will not be able to perform this inference because
neither of these axioms is in the processed set.

4.2 The Refined Saturation Strategy

In order to implement Algorithm 1 in a concurrent way, one first has to ensure
that the shared collections of processed and scheduled axioms can be safely
accessed and modified from different workers. In particular, one worker should

Y. Kazakov, M. Krotzsch, and F. Simancik

Algorithm 2. saturate(input): saturation of axioms under inference rules

Data: input: set of input axioms
Result: the saturation of input is computed in context.processed

1 activeContexts  ;
2 for axiom  input do
?

?

?
for context  getContexts(axiom) do

context.scheduled.add(axiom);
activeContexts.activate(context);

context  activeContexts.poll();
if context = null then break;
loop

axiom  context.scheduled.poll();
if axiom = null then break;
if not context.processed.contains(axiom) then

6 loop
?

?

?
context.processed.add(axiom);
for conclusion  deriveConclusions(context.processed, axiom) do

for conclusionContext  getContexts(conclusion) do

conclusionContext.scheduled.add(conclusion);
activeContexts.activate(conclusionContext);

activeContexts.deactivate (context);

be able to derive conclusions in line 7 at the same time when another worker is
inserting an axiom into the set of processed axioms. The easiest way to address
this problem is to guard every access to the shared collection using locks. But
this will largely defeat the purpose of concurrent computation, since the workers
will have to wait for each other in order to proceed.

Below we describe a lock-free solution to this problem. The main idea is to
distribute the axioms according to contexts in which the axioms can be used
as premises of inference rules and which can be processed independently by the
workers. Formally, let C be a finite set of contexts, and getContexts(axiom) a
function assigning a non-empty subset of contexts for every axiom such that,
whenever an inference between several axioms is possible, the axioms will have
at least one common context assigned to them. Furthermore, assume that every
context has its own queue of scheduled axiom and a set of processed axioms (both
initially empty), which we will denote by context.scheduled and context.processed.
The refined saturation strategy is described in Algorithm 2. The key idea of
the algorithm is based on the notion of an active context. We say that a context is active if the scheduled queue for this context is not empty. The algorithm
maintains the queue of active contexts to preserve this invariant. For every input
axiom, the algorithm takes every context assigned to this axiom and adds this
axiom to the queue of the scheduled axioms for this context (lines 24). Because
?

?

?
the queue of scheduled axiom becomes non-empty, the context is activated by
adding it to the queue of active contexts (line 5).

Afterwards, each active context is repeatedly processed in the loop (lines 6
17) by essentially performing similar operations as in Algorithm 1 but with the
context-local collections of scheduled and processed axioms. The only difference
is that the conclusions of inferences computed in line 14 are inserted into (possi-
bly several) sets of scheduled axioms for the contexts assigned to each conclusion,
in a similar way as it is done for the input axiom. Once the context is processed,
i.e., the queue of the scheduled axioms becomes empty and the loop quits at
line 8, the context is deactivated (line 18).

Similar to Algorithm 1, the main loop in Algorithm 2 (lines 617) can be
processed concurrently by several workers. The advantage of the refined algo-
rithm, however, is that it is possible to perform inferences in line 14 without
locking the (context-local) set of processed axioms provided we can guarantee
that no context is processed by more than one worker at a time. For the latter,
it is sufficient to ensure that a context is never inserted into the queue of active
contexts if it is already there or it is being processed by a worker. It seems that
this can be easily achieved using a flag, which is set to true when a context is
activated and set to false when a context is deactivateda context is added to
the queue only the first time the flag is set to true:

activeContexts.activate (context):
if not context.isActive then
context.isActive  true;
activeContexts.put(context);

activeContexts.deactivate (context):
context.isActive  false;

Unfortunately, this strategy does not work correctly with multiple workers: it
can well be the case that two workers try to activate the same context at the
same time, both see that the flag is false, set it to true, and insert the context
into the queue two times. To solve this problem we would need to ensure that
when two workers are trying to change the value of the flag from false to true,
only one of them succeeds. This can be achieved without locking by using an
atomic operation compare-and-swap which tests and updates the value of the
flag in one instruction. Algorithm 3 presents a safe way of activating contexts.

Algorithm 3. activeContexts.activate(context)
1 if context.isActive.compareAndSwap(false, true) then

activeContexts.put(context);

Deactivation of contexts in the presence of multiple workers is also not as easy
as just setting the value of the flag to false. The problem is that during the time
after quitting the loop in line 8 and before deactivation of context in line 18,
some other worker could insert an axiom into the queue of scheduled axioms for
this context. Because the flag was set to true at that time, the context will not
be inserted into the queue of active contexts, thus we end up with a context
which is active in the sense of having a non-empty scheduled queue, but not

Y. Kazakov, M. Krotzsch, and F. Simancik

activated according to the flag. To solve this problem, we perform an additional
emptiness test for the scheduled axioms as shown in Algorithm 4.

Algorithm 4. activeContexts.deactivate(context)
1 context.isActive  false;
2 if context.scheduled =  then activeContexts.activate(context);

5 Implementation
In this section we describe an implementation of the inference rules for ELHR+ in
Table 2 using the refined concurrent saturation strategy presented in Section 4.2.
There are two functions in Algorithm 2 whose implementation we need to ex-
plain, namely getContexts(axiom) and deriveConclusions(processed, axiom).
Recall that the function getContexts(axiom) is required to assign a set of
contexts to every axiom such that, whenever an inference between several axioms is possible, the premises will have at least one context in common. This is
necessary in order to ensure that no inference between axioms gets lost because
the inferences are applied only locally within contexts. A simple solution would
be to use the inference rules themselves as contexts and assign to every axiom
the set of inference rules in which the axiom can participate. Unfortunately, this
strategy can provide only as many contexts as there are inference rulesnot that
much. To come up with a better solution, note that all premises of the inference
rules in Table 2 always have a common concept denoted as C. Instead of assigning axioms with inference rules, we can assign them with the set of concepts
that match the respective position of C in the rule applications. This idea leads
to the implementation described in Algorithm 5.

// matching the premises of the rules R, R

Algorithm 5. getContexts(axiom)
1 result  ;
2 if axiom match C  D then result.add(C);
3 if axiom match D  R.C then result.add(C);
4 if axiom match S.C  E then result.add(C);
5 return result;

// matching the left premise of the rules RH and RT

 , R

 , R+, R+ , R+

// matching the right premise of the rules RH and RT

To implement the other function deriveConclusions(processed, axiom), we
need to compute the conclusions of all inference rules in which one premise is
axiom and remaining premises come from processed. A naive implementation
would iterate over all possible subsets of such premises, and try to match them
to the inference rules. To avoid unnecessary enumerations, we use index data
structures to quickly find applicable inference rules. For example, for checking
the side conditions of the rule R+ , for every concept D occurring in the ontology
?

?

?
Algorithm 6. deriveConclusions(processed, axiom) for context C
1 result  ;
2 if axiom match C  D then
?

?

?
for D1  (C.subsumptions  D.leftConjuncts) do

result.add(C  D1  D);
?

?

?
for D2  (C.subsumptions  D.rightConjuncts) do

result.add(C  D  D2);

// similarly for rules R, R
 , R+, R+
for S  (C.implications.keySet()  R.superRoles) do

7 if axiom match D  R.C then
?

?

?
for E  C.implications.get(S) do

result.add(D  E);

 , R

// rule R+ , right premise

// rule R+ , left premise

// rule RH, left premise

// similarly for rule RT , left premise
for R  (C.predecessors.keySet()  S.subRoles) do

11 if axiom match S.C  E then
?

?

?
for D  C.predecessors.get(R) do

result.add(D  E);

// rule RH, right premise

// similarly for rule RT , right premise

15 return result;

O we store a set of concepts with which D co-occur in conjunctions:

D.rightConjuncts = {D
D.leftConjuncts = {D

 | D  D
 | D

 occurs in O},
  D occurs in O}.

Similarly, for checking the side condition of the rule RH, for each role R occurring
in O, we precompute the sets of its sub-roles and super-roles:

R.superRoles = {S | R 
R.subRoles = {S | S 

O S},
O R}.

The index computation is usually quick and can be also done concurrently.

To identify relevant premises from the set of processed axioms for the context
associated with C, we store the processed axioms for this context in three records
according to the cases by which these axioms were assigned to C in Algorithm 5:

C.subsumptions = {D | C  D  processed},
C.predecessors = {R, D | D  R.C  processed},
C.implications = {R, E | R.C  E  processed}.

The pairs in the last two records are indexed by the key R, so that for every
role R one can quickly find all concepts D and, respectively, E that occur in the

Y. Kazakov, M. Krotzsch, and F. Simancik

Table 3. Statistics for studied ontologies and classification times on laptop for commonly used EL reasoners and ELK; times are in seconds; timeout was 1 hour

#concepts
#roles
#axioms

FaCT++
jCEL
Pellet
Snorocket
ELK (1 worker)
ELK (2 workers)
ELK (3 workers)
ELK (4 workers)

SNOMED CT
315,489

430,844
13.9
387.1
661.6
509.3
24.5
13.15
7.65
5.66
5.02

23,136

36,547
1.4
timeout
32.5
stack overflow
2.3
1.33
0.90
0.80
0.77

78,977

121,712
0.7
5.4
12.4
88.5
1.6
0.44
0.38
0.39
0.39

19,468

28,897
0.1
7.5
2.9
2.5
0.3
0.20
0.18
0.19
0.19

pairs with R. Given this index data structure, the function deriving conclusions
within a context C can be described by Algorithm 6. Note that the algorithm extensively uses iterations over intersections of two sets; optimizing such iterations
is essential for achieving efficiency of the algorithm.

6 Experimental Evaluation

To evaluate our approach, we have implemented the procedure in the Java-based
reasoner ELK (version 0.1.0) using lock-free data structures, such as Concur-
rentLinkedQueue, and objects such as AtomicBoolean, which allow for compare-
and-swap operations, provided by the java.util.concurrent package.1 The goal of
this section is (a) to compare the performance of ELK in practical situations with
other popular reasoners, and (b) to study the extent in which concurrent processing contributes to the improved classification performance. To evaluate (a)
we have used a notebook with Intel Core i7-2630QM 2GHz quad core CPU and
6GB of RAM, running Microsoft Windows 7 (experimental configuration lap-
top). To evaluate (b) we have additionally used a server-type computer with an
Intel Xeon E5540 2.53GHz with two quad core CPUs and 24GB of RAM running
Linux 2.6.16 (experimental configuration server). In both configurations we ran
Java 1.6 with 4GB of heap space. All figures reported here were obtained as the
average over 10 runs of the respective experiments. More detailed information
for all experiments can be found in the technical report [8].

Our test ontology suite includes SNOMED CT, GO, FMA-lite, and an OWL
EL version of GALEN.2 The first part of Table 3 provides some general
1 The reasoner is available open source from http://elk-reasoner.googlecode.com/
2 SNOMED CT from http://ihtsdo.org/
registration); GO from
from
from

(needs
http://lat.inf.tu-dresden.de/~meng/toyont.html,
http://www.bioontology.org/wiki/index.php/FMAInOwl,
http://condor-reasoner.googlecode.com/

FMA-lite
?

?

?
Fig. 2. Classification time and speedup for n workers on GALEN (left) and
SNOMED CT (right)

statistics about the sizes of these ontologies. We have measured performance on
these ontologies for the reasoners CB r.12 [7], FaCT++ 1.5.2 [19], jCEL 0.15.0,3
Pellet 2.2.2 [17] and Snorocket 1.3.4 [10]. We selected these tools as they provide
specific support for EL-type ontologies. FaCT++ and jCEL, and Pellet were
accessed through OWL API; CB, ELK, and Snorocket were accessed through
their command line interface. The second part of Table 3 shows the time needed
by the tested reasoners to classify the given ontologies in the laptop scenario.
The times are as the reasoners report for the classification stage, which does
not include the loading times.4 The last part of Table 3 presents the result for
ELK tested under the same conditions for a varying number of workers. As can
be seen from these results, ELK demonstrates a highly competitive performance
already for 1 worker, and adding more workers can substantially improve the
classification times for SNOMED CT and GALEN.

The results in Table 3 confirm that concurrent processing can offer improvements for ontology classification on common computing hardware. On the other
hand, the experiments demonstrate that the improvement factor degrades with
the number of workers. There can be many causes for this effect, such as dynamic
CPU clocking, shared Java memory management and garbage collection, hardware bottlenecks in CPU caches or data transfer, and low-level mechanisms like
Hyper-Threading. To check to what extent the overhead of managing the workers
can contribute to this effect, we have performed a series of further experiments.
First, we have investigated classification performance for varying numbers
of parallel workers. Figure 2 shows the results for GALEN and SNOMED CT
obtained for 1  8 workers on laptop and server configurations. The reported
data is classification time (left axis) and speedup, i.e., the quotient of single
worker classification time by measured multi-worker classification time (right
axis). The ideal linear speedup is indicated by a dotted line. On the laptop (4

3 http://jcel.sourceforge.net/
4 Loading times can vary depending on the syntax/parser/API used and are roughly
proportional to the size of the ontology; it takes about 8 seconds to load the largest
tested ontology SNOMED CT in functional-style syntax by ELK.
?

?

?
Y. Kazakov, M. Krotzsch, and F. Simancik
?

?

?
Fig. 3. Classification time for n workers on n copies of GALEN on laptop (left) and
server (right)

cores), there is no significant change in the classification times for more than 4
workers, while the classification times for the server (2  4 cores) can slightly
improve for up to 8 workers.

Next, we measured how our procedure would score against the embarrassingly
parallel algorithm in the situation when an ontology consists of n disjoint and
equal components, which can be classified completely independently. We have
created ontologies consisting of n disjoint copies of our test ontologies, and ran
n independent ELK reasoners with 1 worker on these n copies. We compared
the average classification times of this pre-partitioning approach with the times
needed by ELK when using n workers on the union of these partitions. Both
approaches compute exactly the same results.

The results of this experiment for copies of GALEN are presented in Fig. 3.
As can be seen from these results, the classification time for both scenarios grows
with n (ideally, it should remain constant up to the number of available cores, but
in practice it is not the case because, e.g., the CPU clock speed drops with the
load to compensate for the heat). More importantly, the difference between the
two approaches is not considerable. This proves that, the performance impact of
managing multiple workers is relatively small and interaction between unrelated
components is avoided due to the indexing strategies discussed in Section 5. The
fact that pre-partitioning requires additional time for initial analysis and rarely
leads to perfect partitioning [18] suggests that our approach is more suitable for
the (single machine, shared memory) scenario that we target.

7 Related Works and Conclusion

Our work is not the first one to address the problem of parallel/concurrent
OWL reasoning. Notable earlier works include an approach for parallelization
of (incomplete) structural reasoning algorithms [4], a tableaux procedure that
concurrently explores non-deterministic choices [11], a resolution calculus for
ALCHIQ where inferences are exchanged between distributed workers [15], and
a distributed classification algorithm that can be used to concurrently invoke
?

?

?
(serial) OWL reasoners for checking relevant subsumptions [1]. Experimental
evaluations in each case indicated potential on selected examples, but further
implementation and evaluation will be needed to demonstrate a clear performance advantage of these systems over state-of-the-art systems.
Some other works have studied concurrency in light-weight ontology languages.
Closest to our approach is a distributed MapReduce-based algorithm for EL+
[14]. However, this idea has not been empericaly evaluated. Works dedicated to
OWL RL [13] include an approach for pre-partitioning inputs that inspired the
evaluation in Section 6 [18], and recent MapReduce approaches [6,20].

Many further works focus on the distribution of reasoning with assertional
 (a.k.a. OWL-Horst)
data using weaker schema-level modelling languages pD
and (fragments of) RDFS [5,21,22,9]. These works are distinguished from our
approach by their goal to manage large-scale data (in the range of billions of
axioms), which is beyond the memory capacity of a single machine. Accordingly,
computation is distributed to many servers without memory sharing. Yet, we
can find similarities in term-based distribution strategies [5,21,22,6,20,14] and
indexing of rules [6] with our strategy of assigning contexts to axioms.

In conclusion, we can say that this work does indeed appear to be the first to
demonstrate a compelling performance advantage for terminological reasoning
in OWL through exploiting shared-memory parallelism on modern multi-core
systems. We hope that these encouraging results will inspire further works in this
area, by exploiting existing general techniques for parallel computing, such as
the MapReduce framework, as well as new approaches for parallelization specific
to OWL reasoning, such as consequence-based reasoning procedures.
Acknowledgements. Yevgeny Kazakov and Frantisek Simancik are sponsored
by EPSRC grant EP/G02085X/1. Markus Krotzsch is sponsored by EPSRC
grant EP/F065841/1. Some experimental results were obtained using computing
resources provided by the Oxford Supercomputing Centre.
