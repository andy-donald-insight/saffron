Extending Functional Dependency to Detect

Abnormal Data in RDF Graphs

Yang Yu and Jeff Heflin

Department of Computer Science and Engineering, Lehigh University

19 Memorial Drive West, Bethlehem, PA 18015

{yay208,heflin}@cse.lehigh.edu

Abstract. Data quality issues arise in the Semantic Web because data
is created by diverse people and/or automated tools. In particular, erroneous triples may occur due to factual errors in the original data source,
the acquisition tools employed, misuse of ontologies, or errors in ontology alignment. We propose that the degree to which a triple deviates
from similar triples can be an important heuristic for identifying errors.
Inspired by functional dependency, which has shown promise in database
data quality research, we introduce value-clustered graph functional dependency to detect abnormal data in RDF graphs. To better deal with
Semantic Web data, this extends the concept of functional dependency
on several aspects. First, there is the issue of scale, since we must consider the whole data schema instead of being restricted to one database
relation. Second, it deals with multi-valued properties without explicit
value correlations as specified as tuples in databases. Third, it uses clustering to consider classes of values. Focusing on these characteristics, we
propose a number of heuristics and algorithms to efficiently discover the
extended dependencies and use them to detect abnormal data. Experiments have shown that the system is efficient on multiple data sets and
also detects many quality problems in real world data.

Keywords: value-clustered graph functional dependency, abnormal data
in RDF graphs.

1 Introduction

Data quality (DQ) research has been intensively applied to traditional forms
of data, e.g. databases and web pages. The data are deemed of high quality if
they correctly represent the real-world construct to which they refer. In the last
decade, data dependencies, e.g. functional dependency (FD) [1] and conditional
functional dependency (CFD) [2, 3], have been used in promising DQ research
efforts on databases. Data quality is also critically important for Semantic Web
data. A large amount of heterogeneous data is converted into RDF/OWL format
by a variety of tools and then made available as Linked Data1. During the
creation or conversion of this data, numerous data quality problems can arise.

1 http://linkeddata.org

L. Aroyo et al. (Eds.): ISWC 2011, Part I, LNCS 7031, pp. 794809, 2011.
c Springer-Verlag Berlin Heidelberg 2011
?

?

?
Some works [46] began to focus on the quality of Semantic Web data, but such
research is still in its very early stages. No previous work has utilized the fact
that RDF data can be viewed as a graph database, therefore we can benefit
from traditional database approaches, but we must make special considerations
for RDFs unique features. Since the Semantic Web represents many points of
view, there is no objective measure of correctness for all Semantic Web data.
Therefore, we focus on the detection of abnormal triples, i.e., triples that violate
certain data dependencies. This in turn is used as a heuristic of a potential data
quality problem. We recognize that not all abnormal data is incorrect (in fact,
in some scenarios the abnormal data may be the most interesting data) and thus
leave it up to the application to determine how to use the heuristic.

A typical data dependency in databases is functional dependency [7]. Given a
relation R, a set of attributes X in R is said to functionally determine another
attribute Y , also in R, (written X  Y ), if and only if each X value is associated
with precisely one Y value. An example FD zipCode  state means, for any
tuple, the value of zipCode determines the value of state.

RDF data also has various dependencies. But RDF data has a very different
organization and FD cannot be directly applied because RDF data is not organized into relations with a fixed set of attributes. We propose value-clustered
graph functional dependency (VGFD) based on the following thoughts. First,
FD is formally defined over one entire relation. However RDF data can be seen
as extremely decomposed tables where each table is a set of triples for a single
property. Thus we must look for dependencies that cross these extremely decomposed tables and extend the concept of dependency from a single database
relation to a whole data set. Second, the correlation between values is trivially
determined in a database of relational tuples. But in RDF data, it is non-trivial
to determine the correlation, especially for multi-valued properties. For example,
in DBPedia, the properties city and province do not have cardinality restrictions,
and thus instances can have multiple values for each property. This makes sense,
considering that some organizations can have multiple places. Yet finding the correlation between the different values of city and province becomes non-trivial.
Third, traditionally value equality is used to determine FD. However, this is
not appropriate for real world, distributed data. Consider (1) for floating point
numbers, rounding and measurement errors must be considered. (2) Sometimes
dependencies are probabilistic in nature, and one-to-one value correspondences
are inappropriate. For example, the days needed for processing an order before
shipping for a certain product is usually limited to a small range but not an exact value. (3) Sometimes certain values can be grouped to form a more abstract
value.

In sum, our work makes the following contributions.

 we automatically find optimal clusters of values
 we efficiently discover VGFDs over clustered values
 we use the clusters and VGFDs to detect outliers and abnormal data
 we conducts experiments on three data sets that validate the system

Y. Yu and J. Heflin

The rest of the paper is as follows. Section 2 discusses related work. Section 3
describes how to efficiently discover VGFDs while Section 4 discusses categorizing property values for their use. Sections 5 and 6 give the experimental results
and the conclusion.

2 Related Work

Functional dependencies are by far the most common integrity constraints for
databases in the real world. They are very important when designing or analyzing
relational databases. Most approaches to find FD [810] are mainly based on
the concept of an agree set [11]. Given a pair of tuples, the agree set is all
the attributes for which these tuples have the same values. Since the search
for FDs occurs over a given relation and each tuple has at most one value for
each attribute, then each tuple can be placed into exactly one cluster where all
tuples in the cluster have the same agree set with all other tuples. Agree sets
are not very useful when applied to the extensions of RDF properties, which
are equivalent to relations with only two attributes (i.e. the subject and object
of the triple). Furthermore, many properties in RDF data are multi-valued and
so the correlation between values of different properties becomes more complex.
Finally, since most RDF properties are designed just for a subset of instances in
the data set, an agree set-based approach will partition many instances based
on null values is common.

RDF graphs are more like graph database models. The value functional
dependency (VFD) [12] defined for the object-oriented data model can have
multi-valued properties on the right-hand side, e.g. title  authors. However the
dependencies we envision can have multi-valued properties on both sides and our
system can determine the correlation between each value in both sets. The path
functional dependency (PFD) [13] defined for semantic data models considered
multiple attributes on a path, however the PFD did not consider multi-valued
attributes. FDXML is the FDs counterpart in XML [14] where its left-hand side
is a path starting from the XML document root which essentially is another
form of a record in a database. Hartmann et al. [15] generalized the definitions
of several previous FDs in XML from a graph matching perspective.

As mentioned previously, the basic equality comparison of values used in FD
is limited in many situations. Algebraic constraints [16, 17] in database relations
are about the algebraic relation between two columns in a database and are often
used for query optimization. The algebraic relation can be +,,, /. However
these works are limited to numerical attribute values and the mapping function
can only be defined using several algebraic operators. The reason is that numerical columns are more often indexed and queried over as selective conditions in
databases than strings. In contrast, we try to find a general mapping function
between the values of different properties, both numbers and strings. Addition-
ally, for the purpose of query optimization, they focus on pairs of columns with
top ranked relational significance, the major parts in each of these pairs and the
data related to dependencies that is often queried over, rather than all possible
pairs of properties and all pairs of values existing in the data set.
?

?

?
Data dependencies have recently shown promise for data quality management
in databases. Bohannon et al. [1] focuses on repairing inconsistencies based on
standard FDs and inclusion dependencies, that is, to edit the instance via minimal value modification such that the updated instance satisfies the constraints.
A CFD [2, 3] is more expressive than a FD because it can describe a dependency that only holds for a subset of the tuples in a relation, i.e., those that
satisfy some condition. Fan et. al [2] gave a theoretical analysis and algorithms
for computing implications and minimal cover of CFDs; Cong et al. [3], similar
to Bohannon et al., focused on repairing inconsistent data. The CFD discovery
problem has high complexity; it is known to be more complex than the implication problem, which is already coNP-complete [2]. In contrast to them, we are
trying to both automatically find fuzzy constraints, i.e. those that hold for most
of the data, and report on exceptional data for applications. Our work incorporates advantages from both FD and CFD, i.e. fast execution and the ability to
tolerate exceptions.

With respect to data quality on the Semantic Web, Sabou et al. [4] evaluate
semantic relations between concepts in ontologies by counting the similar axioms
(both explicit and entailed) in online ontologies and their derivation length. For
instance data, previous evaluations mainly focused on two types of errors: explicit
inconsistency with the syntax of the ontologies and logical inconsistency that can
be checked by DL reasoners. However, many Linked Data ontologies do not fully
specify the semantics of the terms defined, and OWL cannot specify axioms that
only hold part of the time. Our work focuses on detecting abnormal semantic
data by automatically discovering probabilistic integrity constraints (IC). Tao et
al. [6] proposed an IC semantics for ontology modelers and suggested that it is
useful for data validation purposes. But the precursor problem of how to discover
these ICs is not addressed. Furber et al. [5] also noticed that using FD could be
helpful for data quality management on the Semantic Web, but do not give an
automatic algorithm to find such FDs and, more importantly, direct application
of FD to RDF data may not capture the unique characteristics of RDF data.

3 Discovering VGFDs

We begin with some definitions.
Definition 1. An RDF graph is G := I, L, R, E, where three sets I, L and
R are instance, literal and relation identifiers and the set of directional edges is
E  I  R  (I  L). Let G be the set of all possible graphs and G  G. Let
 = {r

Definition 2. A Path c in graph G is a tuple I0, r1, I1, ..., rn, In where Ii 
I, ri  R  R
, and i, 0  i < n, if ri  R then (Ii, ri+1, Ii+1)  E or if
ri+1  R
Paths are acyclic and directional, but can include inverted relations of the
form r

 then (Ii+1, ri+1, Ii)  E; j, if i = j then Ii = Ij.

|r  R}.

.

Y. Yu and J. Heflin



  R, Inst(r

 in graph G is r1r2...rn, where
Definition 3. A Composite Property (Pcomp) r
I0, ..., In and I0, r1, I1, ..., rn, In is a Path in G. Let R be all possible Pcomps.
, In|I0, r1, I1, r2, I2, ..., rn, In is a Path
Given r
in G}. Length(r
Definition 4. A Conjunctive Property (Pconj) r+ in graph G is a set

{r1, r2, ..., rn} (written r1 + r2 + ... + rn), where i, ri  R and I
 s.t. 1 
, ri, Ii  Inst(ri, G). Let R+ be all possible Pconjs. Size(r+) =
i  n, I

) = n. r  R, r  R and Length(r) = 1.

, G) = {I0, r


?

?

?
rir+ Length(ri).



, i

  R is also r

|i, r
 
(i, r1) where

) = {i
(i, r
(i, r1), ..., V

  R+.
  R, V
, G)}. Given r+  R+, V +(i, r+) is a tuple V

A Composite Property is a sequence of edges on a Path. The subject and object
of a Pcomp are the first and last objects on the Paths consisting of this sequence
of edges. Every property is a special case of Pcomp. A Conjunctive Property
. Note, each original r  R
groups a set of Pcomps that have a common subject I
is also r  R and each r
Definition 5. Given i  I and r
Inst(r
j, rj  R+.
 returns the objects connected with a subject
Given a Pcomp, value function V
through Pcomp, and given a Pconj, the value function V + returns the set of
objects connected with a subject through Pconj.
Definition 6. Given i, j  I and r
  R, Dependency Equality (DE) be-
) 
tween i and j on r
y  V
), C(x) = C(y)), where C(x) is the dependency cluster of x (dis-
cussed in Section 4). With a slight abuse of notation for DE, given r+  R+,
V +(i, r+) .= V +(j, r+)  rk  r+, V
Definition 7. A value-clustered graph functional dependency (VGFD) s in
graph G is X  Y , where X  R+, Y  R and i, j  I, if V +(i, X) .=
V +(j, X) then V

)  (x  V

(i, rk) .= V

 is: V (i, r

(j, rk).

(i, Y ) .= V

(j, Y ).

(j, r



)

.= V (j, r

(i, r

These definitions state that for all instances, if the values of the left-hand side
(LHS) Pcomp of a given VGFD satisfy Dependency Equality (DE), then there is a
DE on the right-hand side (RHS) Pconj. Note, due to the union rule of Armstrongs
axioms used to infer all the functional dependencies, if    and    hold,
then    holds. Therefore, it is enough to define the VGFD whose right-hand
side (RHS) is each single element of a set of properties. In this work, DE includes
basic equality for both object and datatype property values, transitivity of the
sameAs relation for instances and clustering for datatype values.

Shown in Algorithm 1, this section introduces the VGFD search (line 8-15)
and the next section introduces value clustering (line 2-5) which is used to detect
dependencies. To efficiently discover a minimum set of VGFDs which is a cover
of the whole set of VGFDs, our approach essentially is computed level-wise.
Each level Li consists of VGFDs with LHS of size i (Fig. 1 gives an example).
The computation of VGFDs with smaller sets of LHS properties can be used
?

?

?
Algorithm 1. Search V GF Ds(G, , , ), G = (I, L, R, E) is a graph;  is the
confidence threshold for a VGFD;  is the sampling size;  is the threshold for
pre-clustering.
1: S  , C  
2: for each r  R s.t. r is a datatype property do
groups  P reclustering(Range(r), )
3:
Cr  Optimal Kmeans(Range(r), groups)
4:
C  C  Cr
5:
6: i = 0
7: Li  
8: repeat
9:
10:
11:
12:
13:
14:
15: until Li =  or i >= DEP T H LIM IT
16: return S

i = i + 1
Li  Generate Level with Static P runing(Li1, E)
for each s  Li do

//M is the set of value mappings of s.

if Runtime P runing(s, , , E, C) = F ALSE then
if (M  Compute V GF D(s, , E, C)) =  then

S  S  (s, M )

when computing children VGFDs that have a superset of those LHS properties.
A similar level-wise search was proposed for the Tane algorithm [9], but each
node in Tane corresponds to a subset of our nodes whose LHS is based on single
properties instead of Pcomps. In contrast, our nodes are finer grained which leads
to more opportunities for pruning. Our algorithm starts with level 0. On each
new level, it first generates possible VGFDs on this level based on the results
of previous levels and it also eliminates many potential VGFDs from further
consideration based on some easily computed heuristics (Section 3.1). Then,
a runtime pruning (Section 3.3) and a detailed computation (Section 3.2) are
conducted on each candidate VGFD. The whole process can terminate at a
specified level, or after all levels, although the latter is usually unnecessary and
unrealistic. The process returns each VGFD and its value mappings which is
used for detecting violations.

3.1 Heuristics for Static Pruning

We first define the discriminability for a property as the number of distinct values
divided by the sum of the property extension, and when it is compared between
properties, it is over the instances they have in common. Then, static pruning
heuristics used to eliminate potential VGFDs from further consideration are:
1. insufficient subject overlap between the LHS and the RHS,
2. the LHS or RHS has too high a discriminability,
3. the discriminability of the LHS is less than that of the RHS.

The information for rule 1 can be acquired from an ontology (e.g. using domain and range information) or a simple relational join on data. Here insufficient

Y. Yu and J. Heflin

overlap means too few common subjects, e.g. 20. For rule 2, if the discriminability
is close to one, e.g. 0.95 which means 95%, the property functions like a superkey
in a database. Since such keys identify an individual, they are not useful for
detecting abnormal data. For rule 3, if there is a mapping between two such
properties, some values of the property with smaller discriminability must be
mapped to different values on the RHS which would not be a functional mapping.
In order to apply these heuristics, we make the additional observations:

1. The discriminability of a Pcomp (Pconj resp.) is generally no greater than (no

less than resp.) that of each property involved.

2. A Pconj (Pcomp resp.) cannot be based on two properties that have few

common subjects (objects and subjects resp.).

3. All children of a true VGFD on the level-wise search graph are also true

VGFDs, but are not minimal.

For example, given a Pcomp A  B, its values all come from the values of B
and its extension is a subset of the Cartesian product between objects of A and
subjects of B, then its discriminability, i.e. the distinct values divided by the
usages, should be no greater than that of each component. A similar explanation
applies for Pconj in observation 1. An extension of the observation 2 is that a
Pconj cannot be followed by other properties in a property chain, e.g. (A+B)C,
since its values are tuples (e.g. the values of A + B) as opposed to the instances
and literals that are the subjects of another property (e.g. subjects of C).

A B

A C

A D

B A

B C

B D

... ...

A+C B AC B A+D B AD B A+B C AB C A+D C AD C A+B D AB D

A+C D

(AB)+D C

(AB)D C

(AD)+B C

(AD)+B C

Level 0

Level 1

... ...

AC D
Level 2

Level 3

Fig. 1. An example of level-wise discovering process. We suppose that (1) property A
and B have few common subjects, (2) the discriminability of B is less than that of C
and (3) D has a high discriminability.

Fig.1 is an example showing how these heuristics are useful in the level-wise
searching. Each edge is from a VGFD to a VGFD with an LHS that is a superset
of the parent LHS and the two VGFDs have the same RHS. Note, our current
algorithm does not support the use of composite properties on the RHS. The
VGFDs pruned by the above heuristics are in dotted boxes and dotted lines
pointing to the children pruned. For this example, we make assumptions typical
of real RDF data. For instance, in DBPedia less than 2% of all possible pairs
of properties share sufficient common instances. So following our heuristics, four
VGFDs on level 1 are pruned: A  B is due to heuristic rule 1, B  C is due
to rule 3 and the other two are due to rule 2. Then the children of A  B and
?

?

?
A  D are pruned due to the same reason as their parents. A + B  C on level
2 and (A  D) + B  C on level 3 are pruned due to the first assumption plus
the observation 2. Finally, A+ D  C on level 2 and (A B)+ D  C on level 3
are pruned due to the observation 1 and heuristic rule 2. From this example, we
can see simple conditions can reduce the level-wise search space greatly based
on these heuristics.

3.2 Handling Multi-valued Properties

The fundamental difference between VGFD and FD when computing VGFD
is that we consider multi-valued properties. When finding FDs in databases,
the multi-valued attributes either are not considered (if they are not in the
same relation), or the correlation of their values is given by having separate
tuples for each value. RDF frequently has multi-valued properties without any
explicit correlation of values, e.g. in DBPedia, more than 60% properties are
multi-valued. When computing a VGFD, we try to find a functional mapping

Table 1. The left table is the triple list. The right table is mapping count.

deptNo

deptName

subject object subject object
?

?

?
Candidate Value Mapping Count

1 EE
2 EE
2 CS
1 CS
?

?

?
from each LHS value to an RHS value such that this mapping maximizes the
number of correlations. We consider any two values for a pair of multi-valued
properties that share a subject to be a possible mapping. Then we greedily
select the LHS value that has the most such mappings and remove all other
possible mappings for this LHS value. If multiple RHS values are tied for the
largest number of mappings, then we pick the one that appears in the fewest
mappings so far. Consider Table 1 which analyzes the dependency deptNo 
deptName. The triples are given to the left and each possible value mapping
and its maximal possible count are listed in descending order to the right. The
maximal count of 1  EE is 2, because these two values co-occur for instances
A and B once for each. We first greedily choose mapping 1  EE, because it
has the largest count among all mappings for depN o = 1. After this selection,
the mapping 1  CS is removed since deptN o = 1 has been mapped. Then for
deptN o = 2, to maximize the number of distinct values being matched on both
sides, we choose (2, CS) since CS has been mapped to by fewer LHS values
than EE. Note the basic equality used here is a special case of cluster-based
Dependency Equality. For example, if CS and EE are clustered together, then
the mappings will be 1  EECS and 2  EECS, where EECS is the cluster.

Y. Yu and J. Heflin

Our confidence in a VGFD depends on how often the data agree with it, i.e., the
total matches divided by the sum of the LHSs extension, e.g. the VGFD above
has the confidence of 4/5 = 0.8. In this work, we set the confidence threshold
 = 0.9 to ensure that patterns are significant, while allowing for some variation
due to noise, input errors, and exceptions.

i

3.3 Run-time Pruning
In the worst case, the expensive full scan of value pairs must occur |R
+|  |R
|
times. So we propose to use mutual information (MI) computed over sampled
value pairs for estimating the degree of dependency. In Algorithm 2, given a
candidate VGFD s X  Y , we start by randomly selecting a percentage  of
the instances. In line 2, for each instance i, we randomly pick a pair of values
(i, Y ). Distribution() also applies the clusters CX and
from V +(i, X) and V
?

?

?
CY and returns these pairs in lieu of the actual values. In information theory,
a MI IXY of two random variables X and Y is formally defined as IXY =
j pi,j log (pi,j /pipj), where pi, pj are the marginal probability distribution
functions of X and Y , and pi,j is the joint probability distribution function of
X and Y respectively. Intuitively, MI measures how much knowing one of these
variables reduces our uncertainty about the other. Furthermore, the entropy
coefficient (EC), using MI, measures the percentage reduction in uncertainty
in predicting the dependent variable based on knowledge of the independent
variable. When it is zero, the independent variable is of no help in predicting
the dependent variable; and when it is one, there is a full dependency. The

EC is directional and EC(X|Y ) for predicting the variable X with respect to
variable Y is defined as IXY /EY , where EY is the entropy of variable Y , formally
j pj log 1/pj. Because IXY also can be expressed as EX + EY  EXY which
has a easier form to compute.

Algorithm 2. Runtime P runing(s, , , E, C), s is a candidate VGFD X  Y ;
 is the confidence threshold for a VGFD;  is the sampling size as a percentage;
E is a set of triples. C is a set of cluster sets for each property.
1: I  Sampling Subjects(s, , E)// Sampled subjects shared by the LHS and RHS.
2: {(Xi, Yi)}  Distribution(s, I, E, C)
//A list of value pairs where each pair
|{Xi|Xi=x}|
|{Xi}|
|{Yi|Yi=y}|
|{Yi}|
distinct (x,y){(Xi,Yi)}

3: EX = 
4: EY = 
5: EXY = 

 log |{Xi|Xi=x}|
|{Xi}|
 log |{Yi|Yk =i}|
|{Yi}|
|{(Xi,Yi)|Xi=xYi=y}|
|{(Xi,Yi)}|

consists of two single sampled values of LHS and RHS for the same subject.

 log |{(Xi,Yi)|Xi=xYi=y}|

|{(Xi,Yi)}|

distinct x{Xi}
distinct y{Yi}

6: if (EX + EY  EXY )/EX <   0.2 then
7:
8: return FALSE

return TRUE

We note that Paradies et al. [18] also used entropy to estimate the dependency
between two columns in databases. Since they want to determine attribute pairs
?

?

?
that can be estimated with high certainty, i.e. focusing on precision of the pos-
itives, they need a complex statistical estimator. In contrast, our aim is a fast
filter that is good enough to remove most negatives, i.e. independent pairs, thus
a statistical estimator is not necessary. We can avoid missing positives by setting
a low enough threshold. In our experiments, the difference between EC for a 20%
sample and EC of full data is less than 0.15 on average and the estimated values
typically have higher ECs. For example, it is very rare that a VGFD estimated
lower than 0.7 has an actual value above 0.9. Therefore, a threshold of 0.2 less
than  (line 6) is a reasonable lower bound for filtering out independent pairs.

4 Clustering Property Values

As introduced in Section 1, we must cluster property values in order to discover
dependencies that allow for rounding errors, measurement errors, and distributions of values. For object property values, clustering groups all identifiers
that stand for the same real world object by computing the transitive closure of
sameAs. The rest of this section discusses clustering the values for each datatype
property. This is used to determine Dependency Equality (Definition 6) between
two objects.

4.1 Pre-clustering

The pre-clustering process is a light-weight computation that provides two benefits for finer clustering later: the minimum number of clusters and reserves
expensive distance calculations for pairs of points within the same pre-cluster.
Since the pre-clustering is used for VGFD discovery, there are three thoughts.
First, the values to be clustered are from various properties and have very different features. So the clustering process needs to be generic in two aspects:
(1) a pair-wise distance metric that is suitable for different types of values and
multiple feature dimensions, and (2) suitable for the most common distribution
in real world, i.e. the normal distribution. Second, we prefer a comparatively
larger number of clusters where elements are really close (if not, they may not
be clustered). The reason is that the clusters will be used as class types for detecting dependencies. Larger values of k generate finer-grained class types, which
in turn allow us to generate more precise VGFDs, albeit at the risk of bluring
boundaries between classes and making it harder to discover some dependencies.
This point also makes our approach different from many other pre-clustering ap-
proaches, e.g. [19], because their results of pre-clustering can be overlapped and
rigid clustering later could merge these groups into fewer clusters.

Based on the above thoughts, specifically, given a list of values, the process first
selects a value that is closest to the center (we choose the mean for numeric values
and discuss strings in the next paragraph), and then moves it from the list to be
the centroid of a new group. Second, for each value on the list, if the distance
to this centroid is within the threshold (we use the standard deviation), it will
be moved from the list to a new group. Finally, the above process is repeated

Y. Yu and J. Heflin

if the list is not empty. Thus the process generally finds the cluster around the
original center first, and then the clusters further away from the center. This is
much better than random selection, because if an outlier is selected, then most
instances remain on the list for clustering after this round of computation.

To compute the center and distance of string values, we compute the weight of
each token in a string according to its frequency in values for the property. Then
we pick the string that has the largest sum of weights divided by the number
of tokens in it as the center and the distance between two strings is the sum
of weights of the different tokens in them. The intuition is that by taking these
strings as a class, the most representative one is the one with the most common
words. For example, the property color in DBPedia has values light green,
lime green, etc. Then, the representative of these two strings is the common
word green. For light green, the distance to lime green will be less than
that to light red, since red and green are more common and have larger
weights.

4.2 Optimal k-Means Clustering

There are several popular clustering methods, e.g. k-Means, Greedy Agglomerative Clustering, etc. However most of them need a parameter for the number
of resulting clusters. To automatically find the most natural/best clusters, we
designed the following unsupervised method of finding optimal clusters.

The approach is inspired by the gap statistic [20] which is used to cluster
numeric values with a gradually increasingly number of clusters. The idea is
that when we increase k to above the optimum, e.g. adding a cluster center in
the middle of an already ideal cluster, the pooled within-cluster sum of squares
around the cluster mean decreases more slowly than its expected rate. Thus the
gap between the expectation and actual improvement over different k will be in
a shape with an inflexion which indicates the best k. Our approach improves
upon this idea in three aspects: we start at the number of pre-clusters instead
of 1; in each round of k-Means, the initial centroids are selected according to
pre-clusters; and the distance computation is only made among points within
the same pre-cluster.

Our Optimal kM eans algorithm is presented as Algorithm 3. At first, k is
set to the number of pre-clusters. At each iteration, we increment k and select
k random estimated centroids mi, each of which starts a new cluster ci. Init()
selects the centroids from the pre-clusters in proportion to their sizes. In each
inner loop (line 8-13), every value is labeled as a member of the cluster whose
centroid has the shortest distance to this instance among all centroids that are
within the same pre-cluster as that value (line 10). Then each centroid is recomputed based on the cheap distance metric until the centroid does not change.
After each round of modified k-Means clustering, we compute the difference on
Gap(k) and stop the process if it is an inflexion point. Since the clustering is
used to detect abnormal data in which string values are expected to be caused
by accidental input or data conversion, in this clustering, we use edit distance
as the distance metric for string values as opposed to the above pre-clustering.
?

?

?
k = k + 1, C  tmpC, tmpC  
for each i  k do

Algorithm 3. Optimal kM eans(L, groups), L is a set of literal values; groups
is a set of pre-clustered groups of L.
1: k = |groups|
2: Gap(k) = Gap Statistic(groups)
3: tmpC  groups
4: repeat
5:
6:
7:
8:
9:
10:
11:
12:
until i  k, mi converges
13:
Gap(k) = Gap Statistic(tmpC)
14:
15: until Gap(k) < Gap(k  1)
16: return C

//tmpC is the set of k clusters
Init(mi), ci  ci  mi, tmpC  tmpC  ci //mi is the center of each cluster
for each x  L do
ci  ci  arg minmiGroup(x) Distance(x, mi)
for each i  k do
mi = M ean(ci)

repeat

5 Experimental Results

For our experiments, we selected the Semantic Web Research Corpus2 (SWRC),
DBPedia and RKB3 data sets. All of them are widely used subsets of Linked
Data that cover different domains. Experiments were conducted on a Sun workstation with 8 Xeon 2.8G cores and 6G memory. We observed that there are
few dependencies with an LHS size larger than four and that such dependencies
tend to have less plausible meanings. For this reason, we set the maximal size
of a VGFD to four in our experiments. Based on clusters and VGFDs, abnormal data has two types: one is far away from other clusters and the other is a
violation of VGFDs. Specifically, in this work, a triple is reported as an outlier
if its value is the only element of some cluster whose distance to the nearest
cluster centroid is above twice of the average distances between all clusters for
this property. A triple is reported as abnormal due to violation of VGFDs only
when its value conflicts with a value mapping determined by some VGFD and
this value mapping is confirmed by other triple usages more than twice.

In our first experiment, we compared the overall performance of the system
on three data sets. The sampling size  used in runtime pruning is 20%. In Table
2, we can see that the running time appears to be more heavily influenced by
the number of properties than the data set size. Note that RKB has more triples
but fewer properties than DBPedia, and thus has more triples per property. This
leads to a longer clustering time, but thanks to static and runtime pruning, the
total time to find VGFDs is less.

Table 3 gives some VGFDs from the three data sets and their short descrip-
tions. In DBPedia, among 200 samples out of 2868 abnormal triples, 173 of them

2 http://data.semanticweb.org/
3 http://www.rkbexplorer.com/data/

Y. Yu and J. Heflin

Table 2. System overall performance on SWRC, DBPedia and RKB data sets

SWRC DBPedia RKB
Number of Triples (M) / Properties 0.07 / 112 10 / 1114 38 / 54

Discovered VGFDs on Level 1
Discovered VGFDs on Level 2
Discovered VGFDs on Level 3
Discovered VGFDs on Level 4

Time for Clustering (s)

Time for Level 1 (s)
Time for Level 2 (s)
Time for Level 3 (s)
Time for Level 4 (s)
?

?

?
Total Time (s) / Discovered VGFDs 54 / 51 687 / 721 507 / 9

Reported Abnormal Triples

Table 3. Some VGFDs from the three data sets. The first and second group of VGFDs
are of size 1 and 2. The third group is a set of VGFDs with clustered values.

Description
Organisms in the same genus also have the same family.
A works writer determines the works genre.
The teams with the same owner also have the same chairman.
The works by the same composer have the same media type.
The people of the same military rank also have the same title.
The things on the same location have the same nearest city.
The papers with the same topic have the same primary topic.
The manufacturer and oil system determine the engines
compression ratio.
The publishers country determines the language of that
published work.

genusfamily
writergenre
teamOwnerchairman
composermediaType
militaryRanktitle
locationnearestCity
topicprimaryTopic
manufacturer+oilSystem
compressionRatio
publisher  country
language
article-of-journal+has-volume The volume number of a journal where an article is published
has date
facultybudget
militaryRanksalary
occupationsalary
typeupperAge

determines the published date of this article.
The size of the faculty determines the budget range.
The military rank determines the range of salary.
The occupation determines the range of salary.
A schools type determines the range of upper age.

(86.5%) are confirmed to be true errors in the original data. The correctness of
10 of the remaining triples was difficult to judge. SWRC and RKB have 51% and
62% precision respectively. We believe the lower precision for SWRC is because
it has a higher initial data quality and its properties have a much smaller set of
possible values than those of DBPedia.

We list a number of confirmed erroneous triples in Table 4. For example, the
first triple is reported as an outlier after automatic clustering. The second triple
violates the VGFD that a schools type determines the cluster of its upper age,
because the triples subject is a certain type of school while its value is not in
the cluster of values for the same type of schools.

Next, to check the impact of our pruning algorithms, we performed an ablation study using DBPedia that removes these steps. The left part of Table 5
shows that using static and runtime pruning respectively saves over 62% and
?

?

?
Table 4. Some confirmed erroneous triples in the three data sets, where r, o, i, p, s are
prefixes for http://www.dbpedia.org/resource/, http://www.dbpedia.org/ontology/,
http://acm.rkbexplorer.com/id/,
and
http://data.semanticweb.org/

http://www.aktors.org/ontology/portal/

o:university/undergrad,

194323445>

o:School/upperAge,

2009.0>

o:School/ranking,

2006.0>

o:Work/language,

o:Work/language,
o:Place/coordinates,

1 <r:Shanghai Jiao Tong University,
2 <r:Harrow College,
3 <r:Melbourne Grammar School,
4 <r:Google Maps,
5 <r:Wiktionary,
6 <r:Dembela,
7 <r:Hutt Valley High School,
8 <r:Wake Island,
9 <r:Albuquerque Plaza,
10 <r:varedo,
11 <i:796511,
12 <i:journals/jair/DarwicheP97,
13 <s:person/bastian-quilitz,
14 <s:person/ulf-leser,

o:City/province,
p:has-date,

o:Island/country,

to-10-01>

r:Coverage details of Google Maps>
r:History and development>
coord|N|W>

o:EducationalInstitution/principal,

r:2008>

r:United States Air Force>

o:Building/floorCount,

2221>

r:Province of Milan>

p:has-date,

1998>

s:ns/swc/ontology#affiliation,

research assistant>

s:ns/swc/ontology#affiliation,

professor>

55% of time compared to using neither. Because they utilize different charac-
teristics, using them together saves 85% over neither. When we do not prune,
the few additional VGFDs discovered lead to fewer abnormal triples than those
discovered with pruning (on average 2.2 per VGFD vs. 3.97 per VGFD). Thus
the pruning techniques not only save time but do not affect the abnormality
detection much.

Table 5. The left table is showing the impact of our pruning techniques. The right
table is comparing our preclustering with an alternative called SortSeq on VGFDs
using the clusters and abnormal data found based on these VGFDs.

Time (s) 4047 1529
VGFDs

Abnormal 2923 2915

None Static Runtime Both
?

?

?
Preclustering SortSeq

Time (s)
VGFDs

Abnormal
?

?

?
Besides pruning, we also checked the impact of our pre-clustering. Because our
approach is based on a generic pair-wise distance, we wanted to compare it with
a simpler one based on the linear ordering of values where the distance is just the
difference between numbers. After each iteration of clustering around the mean,
this alternative, referred to as SortSeq, recursively clusters on two remaining
value sets: one is above the mean and the other below the mean. To handle
strings in this approach, we sort them alphabetically and assign each a sequence
number. The right of Table 5 shows that VGFDs and abnormal data that are
based on the baseline clustering are both less than that of our approach. Among

Y. Yu and J. Heflin

the VGFDs not found by the SortSeq, most are for string values. SortSeq finds
fewer VGFDs and less abnormal data, because it naively assumes that the more
common leading characters two strings have, the more similar they are. Thus, our
pre-clustering using cheap and generic computation captures the characteristics
of different property values.

i

)
c
e
s
(
 
e
m

g
n
n
n
u

i

 

level 4

level 3

level 2

level 1

s
?

?

?
d
e
r
e
v
o
o
c
s
i

Time
Time

VGFDs

i

)
c
e
s
(
 
e
m

g
n
n
n
u

i
i
?

?

?
100 200 300 400 500 600 700 800 900 1000

Number of Properties

1% 5% 10% 15% 20% 25% 30% 35% 40% 45% 50%

Sampling Size in Runtime Pruning

Fig. 2. The left is the effect of number of properties on the VGFD searching time. The
right is the effect of sampling size in runtime pruning on the VGFD searching time.

Knowing that pre-clustering and pruning are useful for the system, we systematically checked the trend of system performance, especially time, by using
these techniques. To be comparable on data set size, we picked subsets of properties from DBPedia. For each size, we randomly draw 10 different groups of
this size and average the time over 10 runs. The left of Fig. 2 shows that the
time for every level almost follows a linear trend. The right of Fig. 2 shows the
effect of sampling size  used in runtime pruning on the system. We see that the
running time is in linear proportion to the sampling size. As the VGFD curve
shows,  = 0.2 is sufficient to find most dependencies for DBPedia.

6 Conclusion

We have presented a system to detect Semantic Web data that are abnormal
and thus likely to be incorrect. Inspired by functional dependency in databases,
we introduce value-clustered graph functional dependency which has three fundamental differences with functional dependency in order to better deal with
Semantic Web data. First, the properties involved in a VGFD are across the
whole data set schema instead of a single relation. Second, property value cor-
relations, especially for multi-valued properties, are not explicitly given in RDF
data. Third, using clusters for values greatly extends the detection of dependen-
cies. Focusing on these unique characteristics, our system efficiently discovers
VGFDs and effectively detects abnormal data, as shown in experiments on three
Linked Data sets. In the future we plan to use subclass relationships to further
generalize object property values. We also would like to take into account other
features when clustering, for example the string patterns.
?

?

