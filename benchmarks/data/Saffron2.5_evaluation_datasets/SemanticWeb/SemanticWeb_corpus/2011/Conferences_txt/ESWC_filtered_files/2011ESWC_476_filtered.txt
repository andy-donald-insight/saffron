Whos Who  A Linked Data Visualisation Tool for

Mobile Environments

A. Elizabeth Cano1,, Aba-Sah Dadzie1, and Melanie Hartmann2

1 OAK Group, Dept. of Computer Science, The University of Sheffield, UK

2 Telecooperation Group, Universit at Darmstadt, Germany

{a.cano,a.dadzie}@dcs.shef.ac.uk, melanie@tk.de

Abstract. Reduced size in hand-held devices imposes significant usability and
visualisation challenges. Semantic adaptation to specific usage contexts is a key
feature for overcoming usability and display limitations on mobile devices. We
demonstrate a novel application which: (i) links the physical world with the semantic web, facilitating context-based information access, (ii) enhances the processing of semantically enriched, linked data on mobile devices, (iii) provides an
intuitive interface for mobile devices, reducing information overload.

Keywords: linked data, semantic web, visualisation, mobile devices.

1 Introduction

Mobile devices are increasingly becoming an extension of the lives of humans in the
physical world. The popularity of these devices simplifies in-situ management of the
ordinary end users information needs. Specifically, smart phones embedded devices
(e.g., built-in cameras) allow to build an abstraction of the users environment. Such
abstraction provides contextual information that designers can leverage in adapting a
mobile interface to the users information needs. Further, context can act as a set of
parameters to query the Linked Data (LD) cloud. The cloud connects distributed data
across the Semantic Web; it exposes a wide range of heterogeneous data, information
and knowledge using URIs (Uniform Resource Identifiers) and RDF (Resource Description Framework) [2,6]. This large amount of structured data supports SPARQL
querying and the follow your nose principle in order to obtain facts. We present Whos
Who, a tool that leverages structured data extracted from the LD cloud to satisfy users
information needs ubiquitously. The application provides the following contributions:
1. Exploiting contextual information: Whos Who facilitates access to the LD cloud

by exploiting contextual information, linking the physical world with the virtual.

2. Enhanced processing of Linked Data on mobile devices: Whos Who enables
processing of semantic, linked data, tailoring its presentation to the limited resources of mobile devices, e.g., reducing latency when querying semantic data by
processing triples within a mobile browsers light-weight triple store.

3. Mobile access to Linked Data: Whos Who uses novel visualisation strategies to
access LD on mobile devices, in order to overcome the usability challenges arising
from the huge amount of information in the LD cloud and limited mobile device
display size. This visualisation also enables intuitive, non-expert access to LD.

 To whom correspondence should be addressed.

G. Antoniou et al. (Eds.): ESWC 2011, Part II, LNCS 6644, pp. 451455, 2011.
c Springer-Verlag Berlin Heidelberg 2011

A.E. Cano, A.-S. Dadzie, and M. Hartmann

2 Application Scenario

To illustrate our work consider the following scenario:

Bob is a potential masters student invited to an open day at a university. He will tour
different departments to get to know their facilities and research. To make the best of the
open day, Bob will use his mobile phone as a location guide, allowing him to retrieve
information (encoded as LD) about each department he visits, with the aid of visual
markers distributed around the university. This will allow him to identify researchers he
would like to meet and potential projects to work on.

We demonstrate the approach taken in Whos Who to realise this scenario, i.e., to support user- and context-sensitive information retrieval from the LD cloud using a mobile
device. We exemplify this using the Data.dcs [6] linked dataset, which describes the
research groups in the Department of Computer Science at the University of Sheffield.

3 The Whos Who Application

Whos Who facilitates entry into the LD cloud by exploiting context (section 3.1), reduces potential latency due to resource restrictions on even advanced mobile devices
(section 3.2) and enables seamless management of information load (section 3.3).

3.1 Exploiting Context to Augment Physical Entities

The digital augmentation of physical entities has become a popular feature of many outdoor and indoor mobile applications [4]. For example, outdoors, a users GPS (Global
Positioning System) provides a contextual parameter for retrieving nearby points of
interest. However, GPS is not suited for indoor use, due to among others, poor signal
strength and range accuracy. Alternatives to location-based augmentation for indoor environments are visual detection and RFID (Radio Frequency IDentification) tags, which
can be used to attach hyperlinks to real-world objects. In contrast to RFID readers, visual markers enable widespread use since they can be easily produced by regular print-
ers, and read by standard cameras, which are now integrated into most mobile phones.
To bridge the gap between a physical entitys representation and existing information
regarding it in the LD cloud, we follow the approach illustrated in Fig. 1, which consists
of the following process: (1) a URI encoded in a visual marker represents a physical entity  in our scenario (section 2), a research group housed in the building at the location
in question; (2) this URI translates to a server-side request which queries the LD cloud
to enrich the information related to this entity  in our scenario, with research group
members, their collaborations and publications; (3) this information is processed on the
mobile device and presented to the end user.

3.2 Processing of Linked Data on Mobile Devices

Increments in memory capacity and processing power in mobile devices, particularly
in smart phones, allow semantic processing of large numbers of triples (e.g., the Apple
?

?

?
Fig. 1. The Whos Who application in a nutshell

iPhone 3GS has a 600Mhz CPU and 256MB RAM, and the HTC Hero has a 528MHz
CPU and 288MB RAM; see also benchmarks for semantic data processing in small
devices in [3]). Although it is possible to handle triples in local RDF stores in Androidbased mobiles, this is not possible in other platforms such as the iPhone. An alternative is to use existing lightweight developments such as rdfQuery1, which runs on web
browsers, and HTML 5 features for persisting storage.

However, processing and rendering of semantic resources on mobile web browsers is
still limited by low memory allocation (e.g., 10-64MB in Webkit and Firefox mobile on
iPhone and Android phones). Leaving entirely the processing and rendering of semantic resources to the mobile client improves the user experience by reducing latency due
to multiple requests. However, memory allocation restrictions make this a sub-optimal
option. On the other hand, executing the semantic analysis and data processing entirely
on the server-side results in the execution of continuous calls to the server, which translates to high data latency and a degradation of the responsiveness of the user interface
and interactivity. There must be a compromise between the number of triples handled
by a (mobile device) web browser and visualisation flow performance.

Whos Who follows the mobile and server-side based architecture in Fig. 2. Based on
the parameters encoded in a visual marker, Whos Who queries Data.dcs. The Data.dcs
triples are loaded in-memory via Jena on the server-side, following which SPARQL
queries are executed. The triples retrieved are encoded with JSON  a lightweight
data-interchange format  using JSONLib2, and returned with a JavaScript callback
to the mobile device. On the Whos Who mobile-side, the triples are loaded into an rdfQuery lightweight triple store. Interaction with the visualisation triggers local SPARQL
queries that further filter the information.

The advantages of adopting this approach are that: 1) users need not download the
application in advance (as is the case with applications relying on local RDF storage);
2) users need not know the URI corresponding to the physical entity they want to enrich,
as contextual information is pre-encoded in the visual markers; 3) there is a balance between the triple load handled by the server- and mobile-sides, which translates to more
responsive user interfaces; 4) the mobile-side triple store allows semantic filtering on
the views exposed to the user, reducing latency and improving the interfaces usability.

1 rdfQuery: http://code.google.com/p/rdfquery
2 JSONLib: http://json-lib.sourceforge.net

A.E. Cano, A.-S. Dadzie, and M. Hartmann

Fig. 2. Mobile- and server-side interaction in Whos Who architectural design

3.3 Visualisation

Whos Who supports the user in retrieving information stored in the LD cloud with
visualisations tailored to the application domain. User requests are automatically translated to SPARQL queries executed on the lightweight triple store on the mobile device
itself. If required, additional triples are retrieved from the Whos Who server. Fig. 3 describes the interaction flow for retrieving publications: 1) the user is presented a list of
researchers corresponding to the physical entity encoded in the scanned visual marker;
2) when the user taps on a researcher  in this case Fabio Ciravegna  a SPARQL query
is executed; 3) the publication view is presented, providing an additional filtering layer.
The publication view shows a graph containing the triples resulting from the SPARQL
query  the number of publications per year and the number of collaborators involved
in each publication. In Fig. 3 (3), the user has tapped on the graph bubble corresponding
to the year 2009, which links to two collaborators. The publications are arranged in a
card deck, where the first publication appears in the foreground. The user can traverse
through the publications  where there are multiple  by selecting the next in the deck.

Fig. 3. (1) After selecting a researcher; (2) a SPARQL query is executed; (3) the resulting triples
are presented in the graph in the publication view

4 Related Work

Searching for information about entities and events in a users environment is an oftperformed activity. The state of the art focuses on text-based browsing and querying of
LD on desktop browsers, e.g., Sig.ma [8] and Marbles [1], targeted predominantly at
technical experts (see also [2]). This excludes a significant part of the user population 
non-technical end users  who make use of advanced technology embedded in everyday
?

?

?
devices such as mobile phones. One of the best examples of a visual browser targeted at
mainstream use is DBPedia Mobile [1]; which is a location-aware Semantic Web client
that identifies and enriches information about nearby objects. However it relies on GPS
sensors for retrieving context, which makes it unsuitable for our indoor scenario. Our
approach improves on existing LD browsers for mobile devices in that Whos Who:
1) extracts contextual information encoded in visual markers; 2) hides explicit SPARQL
filters from the user, increasing usability for especially non-technical users.

5 Summary

Whos Who was developed to support especially those end users who may have little to
no knowledge about where to find information on nearby physical entities. It provides
exploratory navigation through new environments, guided by the users context. Studies
(see, e.g., [5,7]) evaluating the utility and usability of tag-based interaction with mobile
device applications illustrate the potential of lowering barriers to LD use.

We have demonstrated the use of a set of visual markers, corresponding to research
groups in a university department, to explore the linked data exposed in Data.dcs, using
a smart phone equipped with a camera and a QRcode scanner. We have also illustrated
how the approach taken in Whos Who simplifies such tasks, by using visualisation
of structured data to extract relevant context and manage information load, to reveal
interesting facts (otherwise difficult to identify), and to facilitate knowledge extraction.

Acknowledgements. A.E. Cano is funded by CONACyT, grant 175203. A.-S. Dadzie
and M. Hartmann are funded by the European Commission (EC) project SmartProducts
(231204). This work was also supported by the EC project WeKnowIt (215453).
