Making Sense of Ratings:

A Common Quantitative Feedback Ontology

Florian Marienfeld

Fraunhofer FOKUS, Berlin
florian.marienfeld@fokus

Edzard Hofig

Fraunhofer FOKUS, Berlin
edzard.hoefig@fokus

Andrea Horch

IAT University of Stuttgart
andrea.horch@iat.uni-

stuttgart.de

Maximilien Kintz

IAT University of Stuttgart

maximilien.kintz@iat.uni-

stuttgart.de

Jan Finzen

Fraunhofer IAO, Stuttgart

jan.finzen@iao

ABSTRACT
This paper proposes a common ontology for ratings,
i.e.
for quantitative user feedback data. Such a framework allows for semantic interoperability of data that adheres to it,
which in turn enables the re-use, by making it independent
from the original system.

In contrast to prior attempts to establish an unambiguous
vocabulary, this approach introduces two components that
are in our view necessary to formally understand what a
users rating actually means. The first is the aspect or facet,
i.e. the viewing angle that was chosen to look at the rated
thing. The second is the meta-model of scales following the
scales of measurement that are widely used in descriptive
statistics. So in plain words, we allow to formally specify
how many out of how many score points something gets and
with regards to what.

We follow the open world assumption of the Web Ontology Language (OWL) and design a vocabulary that is not
specific to any domain. In turn, we rely on the premise that
all domain specific concepts are available as semantic web
resources with appropriate URIs.

Categories and Subject Descriptors
I.2.4 [Artificial Intelligence]: Knowledge Representation
Formalisms and MethodsOntologies, Semantic Networks

General Terms
Standardization, Measurement

Keywords
semantic web, ratings
.fraunhofer.de

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
I-SEMANTICS 2011, 7th Int. Conf. on Semantic Systems, Sept. 7-9, 2011,
Graz, Austria
Copyright 2011 ACM 978-1-4503-0621-8 ...$10.00.

1.

INTRODUCTION

A great variety of applications make use of ratings, i.e.
users explicitly stating their impression of something. These
include recommendation systems, product search, service
selection, information retrieval, and trust management to
mention just the most prominent ones. Many researchers
have explored the question of how to exploit rating data
while fighting attacks/tampering. However, little work has
been carried out to establish a common ontological framework for all rating systems. That is the focus of this article.
The key problem of this state of affairs is that all rating
data are isolated in the very system where they are gener-
ated. That means a specific object may get rated on several
platforms, but the scores can hardly be accumulated into
one average, since all platform hosts have slightly or significantly different concepts regarding their feedback compo-
nent. Thus, there is a need for a formal way to define what
it means that a user picked a certain amount of stars after
her purchase.

The remainder of this paper is organized as follows: Section 2 argues why such an ontology is needed. The actually
proposed concepts are presented and discussed in Section 3.
In Section 4 we argue that our proposal is suitable for real
world domains by showing that popular rating systems can
be modeled adhering to our proposal. We position our work
among similar approaches in Section 5 and conclude in Section 6.

2. MOTIVATION

There are two key motivations for a common rating ontol-
ogy: Interoperability and semantic technologies. In practically all major existing rating systems data is insulated from
other platforms, e.g. a rating on a book at Amazon.com is
no use to customers of any other book seller. This is, of
course, due to economic reasons, but even several web sites
wanted to cooperate, they would have to converge on common semantics in the first place. An example for this would
different open knowledge platforms that allow users to rate
items, like Wikipedia.org. Moreover, current rating data can
only be used in the way it was designed. It is hard to apply
them to fresh ideas or combine them in a new way, as there
is no absolute meaning to most rating data.

A common ontology as presented here would remedy both
problems. All rating data that adheres to it, i.e. that rep-


semantic applications that span rating systems of different
platforms.

The aim of this ontology is to make quantitative opinions
of agents about things formally understandable. Thus, the
guiding question in designing the concepts was What information is necessary to be able to interpret a rating? So
in addition to the straightforward combination of a rater, a
thing rated and a value, we decided it is necessary to define
the aspect which was considered and the scale on which the
value was assigned.

As a consequence of this guiding principle we did not
model qualitative aspects such as free text answers. The
purpose of free text or reviews is precisely to allow for those
answers that a system is not designed to understand, hence
they cannot be exploited in the semantic web in a statistical
fashion, as is the case with quantitative rating. For a bridge
from reviews to the semantic web see [Heath and Motta,
2008] and revyu [Heath and Motta, 2008].

ObjectProperty : about

Domain : Rating
Range :

owl : Thing

ObjectProperty :

submittedBy

Domain : Rating
Range :

Agent

ObjectProperty : h a s A s p e c t

Domain : Rating
Range :
Aspect

ObjectProperty : h a s S c a l e

Domain : Rating
Range :

S c a l e

ObjectProperty : hasValue

Domain : Rating or S c a l e
Range :

Value

DataProperty : c r e a t i o n T i m e

Domain : Rating
Range :

dateTime

3. ONTOLOGY

The ontology was modeled on top of the Web Ontology
Language (OWL) [McGuinness and van Harmelen, 2009]
and is reproduced here in Manchester syntax [Horridge and
Patel-Schneider, 2009] for clarity. Its visualization in Figure 1 is helpful, but does not reflect all details defined in the
code.

Central concept is the class Rating:

Class : Rating

SubClassOf :

about exactly 1 owl : Thing ,
submittedBy exactly 1 Agent ,
c r e a t i o n T i m e exactly 1 dateTime
h a s A s p e c t exactly 1 Aspect ,
h a s S c a l e exactly 1 S c a l e ,
hasValue exactly 1 Value ,
( h a s S c a l e some N o m i n a l S c a l e )
or ( hasValue some N u m er i c al V alu e )

In OWL a subclass is a plain subset of its superclass. So
if a class has multiple superclasses it is a subset of their
intersection. In this case the superclasses are anonymous.
The definition can be read as follows: All instances of the
class, i.e. rating objects, must be in an about relation with
exactly one Thing, Thing being the superclass of all other
classes. Likewise, each instance must have exactly one submitting Agent, one time of creation, one Aspect, one Scale
and one Value. The last two lines denote a union of the
objects that have an OrdinalScale and those that have a
NumericalValue. This constraint makes sure that rating objects cannot have a scale that requires a numerical value but
not actually have a NumericalValue.

For completeness, the respective properties are listed

here, along with their domains and ranges:

No further constraints are imposed on the class Agent.
Anything that is typed as such can be submitter of a rating.
In most cases an instance of Friend-of-a-friends1 class Agent
will be appropriate, but it is no use at this point to enforce
this, as other suitable agents can be thought of.

Class : Agent

The concept Aspect is used to declare with respect to
what the rating was issued. Combined with the recursive
link isRefinementOf, this allows for a hierarchically refined
structure or taxonomy of aspects. Simple examples are
speed of delivery or friendliness of staff of support. This
requires that the referenced resources have valid URIs.
Suitable repositories for that can be Linked Open Data
collections like DBPedia [Auer et al., 2007] or makeshift approaches like disambiguated Wikipedia pages as suggested
in [Milicic, 2008].

Class : Aspect

SubClassOf :

i s R e f i n e m e n t O f max 1 Aspect

ObjectProperty :

i s R e f i n e m e n t O f

Domain : Aspect
Range :
Aspect

The design of the scales is based on the scales of measurements that are elementary in statistics [Stevens, 1946].
A comparison is presented in Table 1. With the help of
the appropriate scale it is possible to state how one rating
value can be compared and aggregated with other values.
To simplify property definitions scales are differentiated into
discrete and continuous:
Class : S c a l e

E q u i v a l e n t T o : C o n t i n u o u s S c a l e or D i s c r e t e S c a l e

Let us consider discrete scales first:

1FOAF: http://xmlns.com/foaf/spec


hasValue

is-a

is-a

is-a
is-a

DiscreteScale

ContinuousScale

Rating

OrdinalScale

NominalScale

RatioScale

IntervalScale

is-a

hasValue
hasValue

is-a

is-a

hasScale

submittedBy

hasAspect

is-a

is-a
is-a

is-a

Value

Scale

Agent
about
is-a

isRefinementOf

Aspect

Thing

Figure 1: Visualization of the Rating Ontology

Class : D i s c r e t e S c a l e

E q u i v a l e n t T o : N o m i n a l S c a l e or O r d i n a l S c a l e
SubClassOf :
DisjointWith : C o n t i n u o u s S c a l e

S c a l e

ObjectProperty : hasValue

Domain : Rating or S c a l e
Range : Value

Class : Value

They have a fixed set of resources to choose from when
rating a thing. Examples for values are URIs for red, very
much, yes, or negative. The two legal subclasses NominalScale and OrdinalScale indicate whether or not the values
are unambiguously comparable. It may be that the NominalScale is rarely used, but it was included nevertheless for
completeness to cover cases like favorite vehicle for a route -
bike, car, walk and situation where only one option is available (like like). The order on an OrdinalScale is ensured
by the exclusively numerical values associated with it. Despite the assigned number, no meaning can be derived from
it other than the ordering. An example of this is accuracy
of description - inaccurate (0), accurate (1), very accurate
(2). Clearly, inaccurate is less than both the other, but it
does not follow that the distance from inaccurate to accurate
is as big as from accurate to very accurate. This difference
intuitively leads to the corresponding legal computations indicated in Table 1. Nominal values only allow for computation of the most frequent element (mode), ordinal values
also imply a middle element (median).

Class : N o m i n a l S c a l e

SubClassOf :

D i s c r e t e S c a l e
hasValue min 1 Value

DisjointWith : O r d i n a l S c a l e

Class : O r d i n a l S c a l e

SubClassOf :

D i s c r e t e S c a l e
hasValue o n l y NumericalValue
hasValue min 2 Value

DisjointWith : N o m i n a l S c a l e

Class : Nu me r i ca lV al ue

SubClassOf :

Value ,
h a s F l o a t exactly 1 f l o a t

In contrast, a ContinuousScale can accommodate any
value in a certain range. Knowing what range a value
was chosen from is essential for interpreting the meaning
of a rating. Obviously, the values have a natural order
and, in addition, distances have a meaning. The crucial
difference between an IntervalScale and a RatioScale is that
on the former ratios are not applicable, but they are on the
latter. In other words, on an IntervalScale only the distance
between values is of meaning, where as on RatioScales also
the distance from zero has a meaning,
i.e. the absolute
value. This difference is also relevant for the semantics of
a feedback value. An example of a RatioScale would be
hours of play time for a computer game.
In this case a
value has an absolute meaning, independent of the range
and an expression like 20% more play time do make sense.


Discontinuous

Continuous

Nominal
Ordinal
Interval
Ratio

applicable expressions

a > b
a > b, a  b
a > b, a  b, a/b

applicable average
mode
median
arithmetic mean
geometric mean

Table 1: Scales of Measurement

Class : C o n t i n u o u s S c a l e

E q u i v a l e n t T o :

I n t e r v a l S c a l e
or R a t i o S c a l e

SubClassOf :

S c a l e ,
lowerBound exactly 1 f l o a t ,
upperBound exactly 1 f l o a t

DisjointWith : D i s c r e t e S c a l e

DataProperty : upperBound

Domain : C o n t i n u o u s S c a l e
Range :

f l o a t

DataProperty :

lowerBound

Domain : C o n t i n u o u s S c a l e
Range :

f l o a t

Class :

I n t e r v a l S c a l e

SubClassOf : C o n t i n u o u s S c a l e
DisjointWith : R a t i o S c a l e

Class : R a t i o S c a l e

SubClassOf : C o n t i n u o u s S c a l e
DisjointWith :
I n t e r v a l S c a l e

Some may feel the urge to define at some point what is
a good and what a bad rating. However, good or bad can
in some cases only be judged by the interpreter of a rating
(consider e.g. the aspect complexity  a high complexity
may be great for thriller movie enthusiasts). Therefore, we
follow the following intuition: the higher the value, that
more applicable is the respective aspect.

4. SUITABILITY FOR USE IN PRACTICE
There are several websites dealing with ratings. One popular example is Amazon, where consumers have the possibility to rate the books or other products they have bought
and sellers rate buyers.

This section will show how things can be rated on different
websites, which rating systems and which scales are used, in
order to argue that all these rating systems can be modeled
using our proposed rating ontology. To support that claim
we looked for a broad variety of different rating systems. The
rating systems on the considered websites were classified and
categorized to a scale of measurement. The results of this
analysis are summarized in Table 2.

In the course of that, we grouped the analyzed rating systems into the following six main rating system types, which
are used on the websites as specialized subtypes (e.g. main
type: star system  subtype: 6 star system). These groups
follow the categorization in [Sparling and Sen, 2010]. We
also identified aspects where the object is rated specifically
by several aspects like kindliness for a craftsman.

1. Unary System

In unary rating systems a rater can only choose
one value for doing the rating.
In most cases unary
rating systems are positive rating systems where a

user or customer can express if he likes a thing (e.g.
Facebooks Like it-Button).

The scale of unary systems is a nominal scale,
because the user can just express if he agrees with a
given value (if he does not agree he will do nothing).
The rating only counts the votes of agreement with
the given rating value, it does not say anything about
the votes of disagreement.

2. Binary System

In a binary rating system the rater can express if he
likes a thing and he also can express if he does not like
it. There are two rating values for doing the rating.
Usually there is a value a rater can choose to express
a linking and one to express a disliking (e.g. Youtube
Top/Flop-Buttons) vote.

These characteristics meet the nature of an ordinal scale, because one can sequence the votes, but one
can not say anything about the distance from a good
to a bad rating value. A sum or percental rate of the
ratings for each value can be built (how many positive
and how many negative ratings are given to a thing).

3. Sentiment System

The sentiment rating system delivers a more extended
view on the raters opinion. Usually it offers three values like negative, neutral and positive(e.g. eBay
rating of sellers). This kind of rating system offers an
easy and fast overview of the sum or percental rate of
positive, neutral and negative ratings given to a
thing.

Sentiment rating systems like binary rating systems also use an ordinal scale to build a sum or
percental rate of the ratings for each given value.

4. Grade Point System

A grade point rating system is inspired by the grade
point system of schools.
In many countries letters
. . . are used to label grades, where A
A, B, C,
is usually the best.
In Germany numbers from 1
to 6 are used instead, where 1 is the best grade.
Some websites like spickmich.de  where teachers and
schools are rated  are using this kind of rating system.

Grade point systems are using an ordinal rating
scale, which can be different between cultural groups,
because they are using different school grade point
systems. To fit these system in our ontology one could
argue that some numeric value with meaning must be
assigned to each label. In the German case even the
grade numbers must be considered labels requiring
a mapping to semantic values. As a matter of fact,


Rated Ob-
ject

Holidaycheck.de Hotels

YouTube.com

Amazon.com

eBay.com

Online-
Videos
products
Sellers
Sellers

Facebook.com

Consumers
Articles/
Facebook-
Sites

maps.google.com Restaurants/

Star System

5 Star System

Interval

1 - 5 stars

MyHammer.de
Branchenbuch

spickmich.de

Compa-
nies/ etc.
Craftsmen/
Orderer

Craftsmen/
Buyer
Teachers/
Teachers
citations/
Schools

Star System

5 Star System

Interval

1 - 5 stars

Sentiment Sys-
tem
Grade
System

Point

Semtiment Sys-
tem
Grades

Ordinal

positive/ neu-
tral/ negative

Ordinal Grades 1 - 6

Table 2: Examples of rating systems

System type

Denotation

Scales

Range

Aspects

Star System

6 Star System

Interval

1 - 6 suns

Slider System

Recommendation
rate

Interval

0 - 100%

Binary System Top-Flop

Sys-

Ordinal

Flop-Top

Star System
Star System
Sentiment Sys-
tem

tem
5 Star System
5 Star System
Sentiment Sys-
tem

Interval
Interval
Ordinal

1 - 5 stars
1 - 5 stars
positive/ neu-
tral/ negative

Unary System
Unary System

Positive System Nominal
Positive System Nominal Like it

positive

hotel,
location,
sport

room,

service,
gastronomy,

accuracy of description,
satisfaction with com-
munication,
delivery
speed, delivery costs

reliability,
kindliness,
craftsmen: quality, buy-
ers: payment behaviour

range of

teachers qual-
schools:
techni-
ity, buildings,
atmo-
cal
equipment,
sub-
sphere,
jects/courses,
adminis-
tration, sport offers, cancellations of classes, dining options, participating in decision taking


score points and grade 6 (fail) to 0 score points,
which is well in line with the semantics of high and
low described in the previous section.

5. Star System

Star rating systems are the most often used rating
systems. The count of stars for the rating differs
between the different websites, but the 5 star rating
system seems to be the most popular. Some examples
are Amazon, Holidaycheck, MyHammer and Google
maps (see Table 2).

Star rating systems are using an interval scale,
because one can accumulate the ratings as well as
build an arithmetic mean; additionally, the distance
from one value of the scale to another are meaningful.

6. Slider System

A slider rating system offers the most possible values to rate a thing or a special aspect of a thing.
Sometimes you can choose values between 0 and
100. One popular website using this rating system is
Holidaycheck were people can rate their hotels after
they have stayed there for there holidays.

Like the star rating system the slider system is
also using an interval scale to be able to accumulate
the ratings and to build arithmetic means.

The above presented rating systems differ in the count
of possible values. That means a unary system allows only
one possible value for the rating, a binary allows two values,
a grade point system in Germany allows six values, a star
system allows zero to less than ten values and a slider system
usually allows zero to 100 values. It is also possible for each
system to rate a thing by rating several aspects of the thing
by using the scale of the system.

We have identified six different types of rating systems
which are using one out of three different scales of measure-
ment. Most of them have a minimum value and a maximum
value for defining the best and the worst possible rating.
None of the systems under investigating presented a concept that is not covered in our proposed ontology.

5. RELATED WORK

There are several publications that have rating interoperability as a theme, but to our knowledge none brings all
insights to the semantic web. We considered strength and
weaknesses of the these to optimize our proposal.

In tvblob [Longo and Sciuto, 2007] the authors present
an ontology for rating. However it can not describe under
what aspect a thing has been rated and what scale was used.
Possibly due to this, tvblob has failed to attract significant
attention.

The Review Vocabulary2 and revyu [Heath and Motta,
2008] are more elaborate but focused rather on reviews, i.e.
qualitative feedback. Quantitative ratings are not modeled
with exhaustive detail.

Another similar ontology is presented in [Sriharee, 2006].
It is however geared towards ratings by agencies. Hence, it
is not intended/suitable to turn user feedback into machine
2http://vocab.org/review

understandable facts. Rather, it allows service providers on
a technical level to incorporate third party classifications in
their profiles.

In [Grinshpoun et al., 2009] cross-domain interoperability

of reputation is discussed but no ontology is presented.

Before the semantic web even became popular a rating
language was formulated3. However, only the syntax was
considered, there is no way to derive absolute meaning.

6. CONCLUSION

We have proposed a common ontology for quantitative
ratings, which is in our view complete, yet universal. Operators who are aware of it may model their rating aspects,
scales and values accordingly. Once the semantics are settled
in this way, they may design their feedback forms accord-
ingly. The resulting user rating can, thus, be understood
by both humans and machines and used in unforeseen fash-
ions. Obviously, this requires that the ratings are available
as open data.

This may not be the final version of the ontology. Rather
this paper can be seen as a Request for Comments of the
semantic web. That means, with the help of interested ontologists a refined and consolidated version may later be
published for a wider adoption.

This work inspires two main lines of further research,
which is intended by the authors. One is the relation between incompatible rating components, i.e. the questions
whether transformation from one scale to another is sensible and how subaspects sum up to more general aspects.
The second line of thought can be phrased as access con-
trol, i.e. how to specify who is allowed to give/read a rating
and privacy issues, including anonymity or pseudonymity of
raters.

7. ACKNOWLEDGMENTS

This work was partially funded by the German THE-
SEUS/TEXO programme and by the openXchange project
of the German Federal Ministry of Economy and Technology under the promotional reference 01MQ09011. It was inspired by the TEXO service ontology rating module by Tom
Kiemes and Daniel Oberle of SAP Research, Karlsruhe.
