Ontology Query Answering on Databases

Jing Mei1,2, Li Ma2, and Yue Pan2

1 Department of Information Science

Peking University

Beijing 100871, China
mayyam@is.pku.edu.cn

2 IBM China Research Lab
{malli, panyue}@cn.ibm.com

Beijing 100094, China

Abstract. With the fast development of Semantic Web, more and more
RDF and OWL ontologies are created and shared. The effective manage-
ment, such as storage, inference and query, of these ontologies on databases gains increasing attention. This paper addresses ontology query
answering on databases by means of Datalog programs. Via epistemic
operators, integrity constraints are introduced, and used for conveying
semantic aspects of OWL that are not covered by Datalog-style rule lan-
guages. We believe such a processing suitable to capture ontologies in
the database flavor, while keeping reasoning tractable. Here, we present
a logically equivalent knowledge base whose (sound and complete) inference system appears as a Datalog program. As such, SPARQL query answering on OWL ontologies could be solved in databases. Bi-directional
strategies, taking advantage of both forward and backward chaining, are
then studied to support this kind of customized Datalog programs, returning exactly answers to the query within our logical framework.

1 Introduction

The Resource Description Framework (RDF) [25] has been recognized as a popular way to represent information in the Semantic Web, accompanying with a
standardized query language, SPARQL [27]. An important vocabulary extension
of RDF is the Web Ontology Language (OWL) [24], whose formalisms rely closely
on the Description Logic (DL) [2]. The Semantic Web Rule Language (SWRL)
[28] arises, when rules are considered to work with OWL in a syntactically and
semantically coherent manner.

Given an RDF document, having information of ontologies (i.e. upgraded to
an OWL document) even rules (i.e. further upgraded to a SWRL document), a
SPARQL query is to extract implicit and explicit RDF data, with query answering as an underlying reasoning service. Compared with weak DL query languages,
SPARQL is expressible for the union of conjunctive queries on RDF triples, not
only concerning traditional DL queries (e.g. instantiation, realization and retrieval [14]), but also allowing predicates (could be DL classes and properties)
being queried as variables.

I. Cruz et al. (Eds.): ISWC 2006, LNCS 4273, pp. 445458, 2006.
c Springer-Verlag Berlin Heidelberg 2006

J. Mei, L. Ma, and Y. Pan

Database (DB) technologies provide a solid support for various data-intensive
applications and could be used for ontology management. So, SPARQL queries
on ontologies possibly work in a similar way as SQL queries on relational data in
databases. Meanwhile, the intention of making the Semantic Web more acceptable to the industry is another underlying promotion to impede the connection
to database communities [9]. However, relational DBs are not rich enough to
capture semantics implied in ontologies. Thus, Datalog is regarded as a suitable
intermedium, and the next section motivates this opinion in detail.
Technically, this paper generalizes a (negation-free) Datalog program P w.r.t.
a given DL KB  =< T ,A >, whose EDB (extensional database) consists of the
inferred DL TBox T  and the original DL ABox A, and whose IDB (intensional
database) is composed of 7 inference rules devised by DL semantics. We state
that P is a syntax variant of an inference system  , and  is proved sound and
, assuming satisficomplete with a specified MKNF-DL [10] KB 
able, extends  with integrity constraints (ICs). By epistemic interpretations,
those ICs exactly reflect the semantic discrepancy between DL and DB, making
query answering in DL accessible to the DB community. As such, we elaborate
a strategy, which benefits from the tractable data complexity of Datalog, to address popular SPARQL queries on databases, provided by RDF data obtained
from OWL documents. Within such a framework, it is possible to futher support SWRL rules which are user-defined Datalog rules sharing predicates with
the ontology classes and properties.

, where 

The paper is organized as follows. Section 2 is our motivation including an
analysis to encountered problems and existing approaches. Section 3 introduces
the preliminaries and notions used in this paper. In Section 4, a so-called DL2DB
 w.r.t. a given DL KB  is defined, along with a natural deduction sysKB 
 to
tem  consisting of DL-driven inference rules. Also, the correspondence of 
, are preits original , as well as the soundness and completeness of  with 
sented. A Datalog program, corresponding to  , is demonstrated in Section 5 for
query answering in . There, we discuss computation strategies and evaluation
approaches as well. Section 6 envisions the desirable extension with user-defined
rules, and shows preliminary experiments in our prototype implementation. Fi-
nally, the conclusion is drawn in Section 7. (Readers can find more details on the
proofs in the paper at http://www.is.pku.edu.cn/~mayyam/appendix.pdf.)

2 Motivation

Our objective is to perform SPARQL query answering on databases which are
used to store RDF data (including OWL ontologies). However, it has been studied in DL-Lite [6] that data complexity of query answering in DL has a LOGSPACE
boundary, above which query answering is not expressible as a first-order logic
formula and hence a SQL query. However, most DLs are more expressive than
DL-Lite. The worst case is an unrestricted combination of OWL and rules, such
as SWRL, leading to the undecidability of interesting reasoning problems [28].
An EXPTIME complexity is reobtained, for query answering in a less expressive
?

?

?
extension to OWL with DL-safe rules [19], but the exponential data complexity
greatly weakens the efficiency.

The discrepancy between DL and DB is actually remarkable. As well-known,
DL is based on an open world assumption (OWA) permitting incomplete information in an ABox, while DB adopts a closed world assumption (CWA) requiring information always understood as complete. The unique name assumption
(UNA) is often emphasized in DB but not in DL. OWL Flight [9], furthermore,
clarifies restrictions in DL and constraints in DB, of which the former is to infer
and the latter to check. When negation comes, DBs prefer to non-monotonic
negation, while DLs rely on monotonic negation.

Facing to these open issues, various proposals have been presented. According
to the engines which do reasoning indeed, existing ontology persistent systems
can be roughly divided into two categories: DL-based and rule-based. The Instance Store [4] is a representative of DL-based systems, where DB serves mainly
for voluminous storage and convenient retrieval, and classical DL tableaux algorithms help make implicit information explicit. In the Instance Store, whatever
queries are (particularly, those queries involving properties and variables can be
handled by rolling-up techniques [14]), a reduction to checking the KB unsatisfiability provides support for query answering. Rule-based approaches are a bit
different, which intuitively translate the meaning of DL constructors into rules.
Due to the expressive power of rules, those DL constructors (e.g. existential
restrictions) are either partially forbidden (as DLP [12] does) or assigned new
meanings (as OWL Flight [9] does). Unlike DL tableaux algorithms, the evaluation of queries adopts strategies by forward chaining or backward chaining.
More tractably, DL-lite [6] is proposed to execute the ABox using a SQL engine,
whose language itself is restrictive while keeping low complexity of reasoning,
namely polynomial in the size of instances in the knowledge base.

Since rule-based approaches are more extensible to SWRL, we make an attempt
to give inference rules as a translation of DL semantics, while RDF data (from
OWL documents) is stored in databases. Concentrating on query answering, this
paper introduces a DL2DB deduction system  and studies which sub-language of
DLs is equivalent to the so-called DL2DB. Surely, DLP (Description Logic Programs [12]) is a nice measure, as cited by most related work. However, inspired
by MKNF-DL (Description Logics of Minimal Knowledge and Negation as Failure [10]), also motivated by bridging DL and DB, we exploit integrity constraints
. Checking satisfiability
to a DL KB , resulting in a special MKNF-DL KB 
of (DB) integrity constraints has been well investigated (e.g. in [5]), and our proposed ICs, in the DL setting, admit of CD (respectively, P.C) being known only
if holding an autoepistemic belief of either C or D (respectively, P and C). That
 as a logically equivalent version of  in the sense of query
is, we contribute 
answering. Particularly, the DL2DB system  proves sound and complete with
 for non-epistemic queries. With a focus on query answering, we assume both

 are satisfiable. Generally speaking, it makes little sense, in classical log-
 and 
ics, when everything is possible to be entailed by inconsistent KBs. Also, checking

J. Mei, L. Ma, and Y. Pan

satisfiability of a DL KB or a MKNF-DL KB has been studied in [2] and [10], but
not scoped in this paper.

We remark this paper does not propose to change or weaken the semantics
of a DL KB. Instead, for keeping DL in classical first-order semantics, we move
to a MKNF-DL world whose unique epistemic model is identical to that of
DL, provided by integrity constraints on demand. In other words, we capture an
epistemic perception for those rule-based approaches, and gain an insight into
integrity constraints for DL constructors, rather than tying to change or weaken
the classical DL semantics.

On the other hand, we do realize nonmonotonic features are gaining increasing
interest in the context of the Semantic Web initiatives [26], and the paper [8]
provides a good survey. A latest work is [18], which proposes hybrid MKNF KBs
integrating decidable DLs with nonmonotonic rules. However, our paper here
belongs to the direction of research for query answering over ontologies relying
over database technologies by making use, in a natural way as a rational agent
does, of well established formalizations and computing mechanisms.

3 Preliminaries
Consider the main layers of the DL family bottom-up [2][13], ALC is a basic
and simple language, permitting class descriptions via C  D, C  D,C,P.C,
and P.C, where C, D are classes and P is a property. Augmented by transitive
properties, ALC becomes ALCR+, denoted by S in the following. SI is an extension to S with inverse properties, followed by SHI with property hierarchies. It
is called SHIF if extending by functional restrictions, SHIN if by cardinality
restrictions, and SHIQ if by qualified number restrictions. Support for datatype
predicates (e.g. string, integer) brings up the concrete domain of D, and using
nominals O helps construct classes with singleton sets.
With the expected pervasive use of OWL, SHIF(D) and SHOIN (D) are
paid more attention: one is the syntax variant to OWL Lite and the other is to
OWL DL. This paper currently takes SHI into account, and more expressive extensions will be explored in our ongoing work. Alternately, DB built-in features,
such as arithmetic operators and aggregate functions, might be considered as a
workaround for F ,N ,Q and D, while list operations for O. In the following, if
not stated otherwise, C, D denote SHI classes and P, Q denote SHI properties.
A DL KB  is defined as a pair  =< T ,A >. The TBox T is a finite set of
class and property subsumptions having the form of C  D and P  Q, resp.
The ABox A is a finite set of class and property assertions having the form of
C(a) and P (a, b). Also, an interpretation I = (,I) consists of a nonempty set
 (the domain of I) and a function I (the interpretation function of I) that
maps every class to a subset of  and every property to a subset of   . An
interpretation I is a model of a DL KB  (denoted as I |= ) iff every sentence
(subsumption or assertion) of  is satisfied in I. For a complete presentation of
other definitions, such as the satisfiability of sentences, we direct readers to the
classical DL handbook [2]. Query answering for q( x) in  attempts to receive
?

?

?
all ground substitutions  t to  x such that  |= q( t), and two KBs  and 
 are
equivalent in the sense of query answering iff those obtained results are identical
in both  and 

 [6].

Next, a more sophisticated language, namely MKNF-DL [10], is sketched. One
epistemic operator K works for minimal knowledge, and the other epistemic operator A plays a role to default assumption. The syntax of MKNF-DL extends
DL with KC, KP, AC and AP , where C is a DL class and P is a DL property.
The semantics of MKNF-DL resorts to epistemic interpretations. An epistemic
interpretation is a triple (I,M,N ) where I is a (first-order) interpretation and
M,N are sets of (first-order) interpretations. Non-epistemic classes and properties are interpreted same as in I, i.e. C
I. The
other semantic conditions state that:

I,M,N = C

I,M,N = P

I and P

I,M,N = ; I,M,N = ; (C)I,M,N = \CI,M,N ;
(C  D)I,M,N = CI,M,N  DI,M,N ; (C  D)I,M,N = CI,M,N  DI,M,N ;
(P.C)I,M,N = {a  |b.(a, b)  P I,M,N and b  CI,M,N};
(P.C)I,M,N = {a  |b.(a, b)  P I,M,N implies b  CI,M,N};
(KC)I,M,N = J M(CJ ,M,N ); (KP )I,M,N = J M(P J ,M,N );
(AC)I,M,N = J N (CJ ,M,N ); (AP )I,M,N = J N (P J ,M,N );

For any property Q being the inverse of P , if (a, b)  P I,M,N then (b, a)  QI,M,N ;
For any transitive property P , if (a, b), (b, c)  P I,M,N then (a, c)  P I,M,N .
A set of interpretations M is an epistemic model for  (denoted as M |= )
iff the structure (M,M) satisfies  and, for each set of interpretations M, if
,M) does not satisfy . A structure (M,N ) satisfies 
M  M then (M
(denoted as (M,N ) |= ) iff each interpretation I  M is such that every sentence (subsumption or assertion) of  is satisfied in the epistemic interpretation
(I,M,N ). The paper of MKNF-DL [10] provides a complete presentation of
other definitions, such as the satisfiability of sentences, and we follow its con-
vention. Notice that, for any (non-epistemic) DL KB , it has one and only one
epistemic model, i.e. the set of all first-order models for , denoted as M().
Finally, this paper adopts assumptions that are suitable for the semantics of
MKNF (cf. [8][10][18]): (1) Every first-order interpretation is over the same fixed,
countably infinite domain; (2) There is a one-to-one correspondence between
individuals in the language and elements in the domain. Thus, the set of all
individuals O is fixed to I, i.e.,  = O, and we denote the interpretation of
a  O simply as a itself, i.e., a
I = a. The assumption of (2) also implies that
two distinct individuals denote two distinct elements, referred as to UNA.

4 Bridging DL and DB

,
Given a DL KB , a specified MKNF-DL KB, namely the DL2DB KB 
 and  in the sense of query
is studied. We guarantee the equivalency of 
answering, for non-epistemic queries. And then, we generalize an inference sys-
tem, namely the DL2DB system  , and this contribution of  is posed by its
soundness and completeness with 

, building a bridge between DL and DB.

J. Mei, L. Ma, and Y. Pan

4.1 The DL2DB KB

Inspired by representing integrity constraints (ICs) in MKNF-DL [10], we propose ICs for some DL constructors to strengthen beliefs, meeting the requirement
that w.r.t. certain information the KB is prone to be complete.
Before introducing definitions, we recall how to formalize ICs using an example
from [10]. For instance, a TBox T : Kemployee AmaleAfemale corresponds
to an IC that each known employee must be known to be either male or female.
Having only one assertion in ABox A: employee(Bob), makes this KB < T ,A >
lack of epistemic models. That is, this IC is violated. Turning to a system level,
we require each known DL class of C  D must be known to be either C or D,
viz. sub1 as defined below, while sub2 is for P.C. Considering the discrepancy
between DL and DB, we believe that ICs bridge them in a semantic pay-as-
you-go manner.
?

?

?
We use clos(C) for the closure of a class C, and clos(C) is the smallest set
containing C that is closed under subclasses and negation (in Negation Normal
Form), while the size of clos(C) is linear in the length of C [13]. Given a set
CM clos(C), the size of clos(M) is polynomial in
of classes M, clos(M) =
the size of M.
Definition 1. Let  =< T ,A > be a DL KB, and C be the set of classes
,A > where T  = T  sub1
occurring in . A DL2DB KB w.r.t.  is 
 sub2,
sub1={K(C  D)  AC  AD | C  D  clos(C)} and
sub2={K(P.C)  AP.AC | P.C  clos(C)}.
The following proposition helps gain insights into the nature of epistemic mod-
 and . Due to space limitation, the detailed proof can be found at
els for 
http://www.is.pku.edu.cn/~mayyam/appendix.pdf.

 =< T 

, (M,M) satisfies .

 be a DL2DB KB w.r.t. . A set of
 iff (1) M is an epistemic model

Proposition 1. Let  be a DL KB, and 
interpretations M is an epistemic model for 
for ; (2) for each subsumption  sub1sub2 in 
As pointed out above, the (non-epistemic) DL KB  has a unique epistemic
model, namely M(), consisting of all first-order models for . This propo-
 has the same unique epistemic model as M() if the
sition indicates that 
structure (M(),M()) satisfies all subsumptions in sub1sub2, otherwise 

 are satisfiable,
is unsatisfiable. Thus, under the assumption that both  and 
the two KBs  and 
 is not, which implies some
of those ICs have been violated. In this case, we fail to returning complete
, although incomplete information in  would not atanswers to queries via 
tack  itself. For example, a DL KB , having a TBox T : maleperson and
femaleperson in addition to an ABox A: malefemale(Bob), is satisfiable
and entails person(Bob) but neither male(Bob) nor female(Bob). Obviously,
the IC of K(malefemale) AmaleAfemale is violated, making this 
 un-
satisfiable. In real DB-based applications, it is highly possible to have complete

 are equivalent in the sense of query answering.

It possibly happens that  is satisfiable but 
?

?

?
 and reobtaining

information on Bobs gender, which leads to person(Bob) in 
the satisfiability of 

.

4.2 The DL2DB System

DL TBox reasoning has been well-developed, but scalable DL ABox reasoning
deserves more investigation. Instead of using classical DL tableaux calculus, we
aim at exploiting an alternative way to ABox reasoning on databases, which are
initialized by an inferred TBox and an original ABox.
Since the complexity of deciding SHI DL class satisfiability is EXPTIMEcomplete [13], computing the closure of the TBox is in coEXPTIME, provided that
C is subsumed by D iff CD is unsatisfiable [2]. Although, such computational
impact might be non-negligible, TBoxes are relatively fixed and certain data
preprocessing at the back end is feasible for applications, to some extent.
So far, a DL2DB system  w.r.t. a DL KB  =< T ,A > is constructed as
below. Starting by initialization, a TBox taxonomy T  = {C  D| |= (C 
D)}  {P  Q| |= (P  Q)}, derived from external DL reasoners, is uploaded
to  together with the original ABox A.
Initialization:
T Box: If   T , then    .
ABox: If   A, then    .
Inference rules:
: If    , then  % .
: If  % C(a) and  % D(a), then  % (C  D)(a).
: If  % P (a, b) and  % C(b), then  % P.C(a).
: If  % P (a, b) and  % P.C(a), then  % C(b).
T : If  % (C  D) and  % C(a), then  % D(a).
P : If  % (P  Q) and  % P (a, b), then  % Q(a, b).
Pi: If  % P (a, b), then  % Q(b, a), where Q is inverse of P .
Pt: If  % P (a, b) and  % P (b, c), then  % P (a, c), where P is transitive.

Symbols of a, b, C, D, P, Q etc. will be instantiated by corresponding individ-
uals, classes and properties in . Classical SHI DL [13] depicts another rule of
+: If  % P (a, b) and  % Q.C(a), where P is transitive and  % (P  Q), then
 % P.C(b). However, support for TBox reasoning gives Q.C  P.(P.C),
because of P  Q where P is transitive. As a result, Rule[] covers the situation
of Rule[+], i.e., if  % P (a, b) and  % P.(P.C)(a), then  % P.C(b).
Applying these inference rules, a sentence  is called derivable if  % , and
its derivation length n counts in the times of applying inference rules. A conflict
is defined in  by the facts that  % C(a) and  % C(a). In this paper,
conflict-free systems are focused, unless otherwise noted.

As far as we know, these above inference rules, more or less, play a role in
most state of the art Semantic Web reasoning engines, in particular for those
which adopt rule-based approaches. Thus, a corresponding KB fit in with this
 is expected, and 

 defined previously happens to be the candidate.

J. Mei, L. Ma, and Y. Pan

 be a satisfiable DL2DB KB w.r.t. ,  be

Theorem 1. Let  be a DL KB, 
a conflict-free DL2DB system w.r.t. , and  be a non-epistemic sentence.
 %  if and only if 
For proofs of this theorem (or called as soundness and completeness), please
refer to the report at http://www.is.pku.edu.cn/~mayyam/appendix.pdf.

 |= .

5 Query Answering

Moving the proposed DL2DB system into practice, a (negation-free) Datalog
program is presented in this section. Also, we discuss rewriting techniques which
make SPARQL queries processable, even those syntactic sugars involving predicates as variables. By implementing a bi-directional strategy of top-down and
bottom-up, we believe that, following some optimization techniques, e.g. Magic
Set [3] and Tabling [1], scalable ontology query answering is hopeful.

5.1 SPARQL Queries
SPARQL is a query language for obtaining information from RDF graphs. An
RDF graph is a set of triples and each triple consists of a subject, a predicate
and an object. From DL perspective, a reserved predicate rdf:type, for ex-
ample, indicates DL class assertions with individuals as the subject and DL
classes as the object. Possibly, a SPARQL query concerns the retrieval of those
objects standing for DL classes, e.g. asking for all types of a specific individual.
Ignoring non-logical constitutions in SPARQL, such as filters, prefixes and so
on, we denote a SPARQL query q( x) by an expression of the form { x|dnf( x,  y)}.
Here,  x are the so-called distinguished variables that will be bound with individuals in the KB, and  y are the non-distinguished variables which are existentially
qualified variables [6]. dnf( x,  y) is a disjunctive normal form of Rel(sub, pre, obj),
and Rel, being a logical predicate, has three parameters: sub, pre, obj, each of
which is either a constant in the RDF DB or a variable in  x or  y.

A question naturally arises, facing to constants in the RDF DB: what the
RDF DB is and what the constants are. Rewriting techniques are introduced to
address the problem. Given a DL KB, C(a) and P (a, b) in the ABox are rewritten by Rel(a, rdf:type, C) and Rel(a, P, b), while C  D and P  Q in the TBox
are rewritten by Rel(C, rdfs:subClassOf, D) and Rel(P , rdfs:subPropertyOf, Q),
where
reserved
predicates. Actually, rewriting not only provides support for SPARQL queries,
but also bridges DL and relational databases. DL constructive classes such as
P.C are not straightforwardly expressible inside of DBs. So, we use unique
IDs to make them recognizable as for participating in sub, pre, obj. Thus, an
RDF DB is a storage of triples w.r.t. a DL KB, where constants are those IDs
representing individuals, DL classes and DL properties.

and rdfs:subPropertyOf

are

rdf:type,

rdfs:subClassOf,

5.2 A Datalog Program
Rewriting techniques, additionally, give our inference rules a new version. For
instance, Rule[]
is depicted as Rel(a, rdf:type, P.C)  Rel(a, P, b),
?

?

?
Rel(b, rdf:type, C). Attention should be paid for variable bindings of a, b, P, C,
and P.C, which impose semantic conditions into the rule body such that a, b are
individuals and P.C is a pending DL class together with its affiliated DL property P and class C. Other rules are processed similarly. For example, Rule[T ]
turns to Rel(a, rdf:type, D)  Rel(C, rdfs:subClassOf, D), Rel(a, rdf:type, C).
As such, given a DL KB  =< T ,A >, the DL2DB system  w.r.t.  appears
as a Datalog program P . The IDB of P consists of those inference rules (except
Rule[]) in  rewritten in triples. The EDB of P is exactly an RDF DB w.r.t.
 storing triples for the original ABox A and the inferred TBox T , driven by
Rule[]. With the help of DL reasoners, we regard the computation of TBoxes
as a preprocessing. It is T  instead of T that plays a role in this program. Since
Datalog has P-complete data complexity [7], query answering in  is polynomial
in size of the KB 
Grounding a Datalog program P on the defined RDF DB needs kN M binding
operations in maximum, where k, M and N are the number of Datalog rules,
variables and constants, resp. In the case of our DL2DB system  , we have k = 7
Datalog rules excluding Rule[], each of which has maximally M = 5 variables
acting as sub, pre, obj. The number N of constants counts those IDs representing
individuals, DL classes and DL properties in . Consequently, interpreting  in
Datalog rules has a computational cost of O(7  n5).

 =< T 

,A >.

5.3 Strategies

Although, a theoretical complexity is tractable, the evaluation strategy in practice is another story. For example, a cyclic DL TBox of P.C  C indicates the
query of C(x) is based on that of P.C(x) which relies on C(y) and P (x, y).
The backward chaining becomes C(x)  P (x, y), C(y), getting entangled in the
recursive retrieval of C. Meanwhile, a pitfall exists when using forward chaining
freely in this example. Thinking about an ABox of P (a, a) and C(a), we receive
an infinite series of P.C(a), P.P.C(a), P.P.P.C(a), and so on. A straightforward top-down implementation can take exponential time using these rules
while a bottom-up approach is faster (generally polynomial), but still wastes
time exploring other rules which are never used in the solution of the query [3].
To improve performance, we can use a procedure which collects subgoals of
a given query top-down firstly, and then evaluates all subgoals bottom-up. Referring to techniques involved in Deductive Databases [21], the repetition of
computing is by all means avoided on the one hand, and the irrelevant goals
may as well be ignored on the other hand.

We are now ready to define algorithms, as shown in Table 1. To improve
legibility, Rule[*] is presented, where * is the placeholder of every inference rule
introduced in the DL2DB system, such as Rule[] and Rule[T ], except for
Rule[]. For computing the answers to a subgoal g over the KB 
,A >,
 =< T 
we first exploit the relational DB to obtain A(g) which means a ground base of
g asserted in A, and then answers are propagated until reaching a fixpoint.

In fact, this proposed strategy, to simulate top-down semantics in a bottom-up
framework, is not new, tracing back to the Magic Set [3] developed in Deductive

J. Mei, L. Ma, and Y. Pan

Algorithm TopDown(q, T )
A query q and
an inferred TBox T 

Input

Table 1. Algorithms of TopDown and BottomUp
ButtomUp(S, T ,A)
A set S of subgoals for q and
an inferred TBox T  and
an original ABox A
A set Ans of answers to subgoals in S
Ans := {A(g)|g  S}
do Ans := Ans

S := {q};
while (top < sizeOf(S)) do

top := 0;

Output A set S of subgoals for q

Steps

for each g in S do

if g matches the consequent
of an instantiated Rule[*]
then top := top + 1; S := S
{those antecedents of Rule[*]}; while Ans = Ans;

with its antecedents in Ans

if an instantiated Rule[*] is settled
then Ans := Ans

{the consequent of Rule[*]}

return S

return Ans

Databases. Not providing a general magic set transformation, we regard the collection of subgoals as a magic set s.t. Ruel[], for example, appears as Rel(a, rdf:type,
P.C)  Magic(P.C), Rel(a, P, b), Rel(b, rdf:type, C). Unless required, such
class expression P...P.C as mentioned above would not be generated by the
Rule[] with Magic.

Meanwhile, there are various algorithms to address the termination of topdown methods, most of which may be considered as variants of OLDT-resolution
[21]. Being a representative and used in XSB Prolog [1], the Tabling method
declares tabled predicates (manually or automatically) whose evaluation is by
means of a so-called SLG resolution, while non-tabled predicates are resolved as
normal, i.e., using the SLD resolution steps, by which the termination reaches
without infinite loops. However, complex and large DL TBoxes expect tabled
predicates declared automatically. Only to exploring all instantiated rules by a
compilation of the TBox, those predicates are detected. It means, a top-down-
like static analysis in Tabling looks similar to the collection of subgoals in our
approach. Besides, we observe that, in Tabling, the table entry associated with
calls to tabled predicates is enriched by inserting new derived answers, step
by step. Such a gradual insertion is executed also similarly in our bottom-up
computation for obtained subgoals. Briefly, running in our strategy seems not
more expensive than in Tabling, equipped with physical optimizations.

So far, we believe that a bi-directional strategy is suitable for query answering
in a DL-driven Datalog program. Specifically, the (indirect) cyclic DL TBox is
legal which leads to the recursion unavoidable, while the superfluous computation (particularly encountering voluminous data in the Semantic Web) has been
cut down by the collection of subgoals. Finally, our bi-directional strategy will
terminate for a given DL KB, provided by the facts that

1. A finite number of subgoals is encountered, whose worst case is the collec-

tion of all DL classes and DL properties appearing in the KB, and

2. Each of these subgoals has a finite number of instances with the maximum

of all individuals in the KB.
?

?

?
5.4 Comparisons

In this section, we summarize and compare systems and approaches for rulebased ontology query answering.

Knowledge compilation, making implicit information explicit in advance, has
been widely used in some systems. For example, OWLIM [15], being a semantic
repository, builds a materialized RDF database with the inferred closure of an
OWL DLP KB. Similarly, Minerva1 performs entailment rules bottom-up for the
DL ABox inferences, plugged with a DL reasoner for precomputing the DL TBox
subsumptions [16][29]. Creating DB views has been preprocessed in DLDB-OWL
[20], of which each view stands for a DL class. Briefly, this kind of tools fills in
DBs with data by a bottom-up precompilation, pushing a direct retrieval to
back-end databases, but at the risk of a whole re-computation when updating.
Querying on-the-fly during reasoning alleviates the suffering caused by updat-
ing. There are two frameworks, KAON2 [19] and DL-lite [6], getting deserved
attention. KAON2 reduces a DL KB to a disjunctive Datalog program, using
the magic set algorithm developed for disjunctive programs. DL-lite is less ex-
pressive, although, any query is expressible as the union of conjunctive queries
in SQL via a KB normalization, making SQL engines responsible to return the
exact answers. Thus, our DL2DB stands in the middle of KAON2 and DL-lite.
Generally speaking, this kind of tools follows a solid logical language, to address
the challenges of expressivity and reasoning power.

It leaves a straightforward way, in which existing (top-down, bottom-up) rule
engines are borrowed as a whole. In this trend, a production rule engine Jess
is used by OWLJessKB2 and OWL2Jess [17], while XSB Prolog is preferred
in FLORA-23 and TRIPLE4. Also, SWI-Prolog5 provides a Semantic Web Library dealing with the RDF data extracted from RDF(S) and OWL documents.
Taking negation into account, deduction in ontologies via ASP (Answer Set
Programming) has been discussed in [23] but regarding classical negation 
as default not, and HEX-Programs [11] provide a hybrid platform integrating
ASP rules with external DL atoms, both of which utilize underlying ASP rule
engines.

6 Discussions

Not surprising, user-defined rules are expected in real applications. Viewing
those inference rules in a system level, we envision a common platform with
support for SPARQL query answering in SWRL. Learning from our preliminary
experiments, performance problems (e.g. running time and memory space) are
discussed below, towards the development of a powerful Semantic Web tool.

1 http://www.alphaworks.ibm.com/tech/semanticstk
2 http://edge.cs.drexel.edu/assemblies/software/owljesskb/
3 http://flora.sourceforge.net/
4 http://triple.semanticweb.org/
5 http://www.swi-prolog.org/

J. Mei, L. Ma, and Y. Pan

6.1 Extensions

SWRL [28] is a combination of OWL DL and unary/binary Datalog rules. Tech-
nically, let S be a SWRL knowledge base, where OC is a set of OWL classes,
OP is a set of OWL properties, and ST is a set of OWL individuals and SWRL
variables. A SWRL rule has the form: h1    hn  b1    bm , where
hi, bj, 1  i  n, 1  j  m are atoms of the form C(t) with C  OC and t  ST ,
or atoms of the form P (t1, t2) with P  OP and t1, t2  ST .
We are more interested in SWRL rules with a unique head atom, and rewriting techniques are still suitable s.t. each SWRL rule appears as: Rel(s0, p0, o0) 
Rel(s1, p1, o1) Rel(sn, pn, on). Facing to a rule body having Rel(a, rdf:type,
P.C), for instance, a rewritten version of Rule[] as mentioned above is applied
to obtain corresponding entailments, also Rule[T ] and others are applicable.
Thus, assuming that a Datalog program works for our DL2DB system, the situation is not aggravated when more rules, in the same style, are involved. Being
a preliminary work, our prototype is towards support for query answering with
SWRL rules, whose semantics appeals, however, for integrity constraints.

A large variety of features have been captured in MKNF-DL [10], such as
default rules and epistemic queries. Our future work includes a more general
formalization concerning non-monotonic logics and latest work in [8] [11] [18] is
well deserved studying.

6.2 Preliminary Experiments

We conducted initial experiments on a DBMS-based OWL repository, Minerva
(DB2 in experiments). Test data sets are from the extended LUBM [16], an
OWL ontology including 69 atomic classes, 39 intersection classes, 10 existential
restrictions, 55 properties, 263 class subsumptions, 16 property subsumptions,
46450 class assertions, 239933 property assertions, and 25461 individuals. These
documents involve one terminology file of size 66 KB, one university file of size
207 KB and twenty department files each of which is about 1300 KB.

Not tangling with complex DL class descriptions, such queries as finding instances typed of Organization receive answers (counting to 229 none of which
is asserted in the ABox) in 0.91 second. However, the query to Chair, defined
as Person   headOf.Department where DepartmentOrganization, requires
the evaluation of Person firstly. There are 35 classes being recognized as the
subclasses of Person, inducing totally 90 subgoals. Finally, it counts to 14995
persons (none exists in the ABox) and 40 chairs (half asserted, half inferred) in
29.73 second. As for a certain individual, who is asserted as Man and Full-
Professor in the ABox, our engine further knows him typed of Professor,
Faculty, Employee, Person and Chair in 21.48 second.

We found that how to process intermediate results obtained from subgoals
becomes a bottleneck of performance, and the worse case reports out of mem-
ory if temporary results are carried out in memory. Inserting them into DBs
is considered, but it takes minutes since various indexes need to be established
and DB needs to write logs. Thus, we turn to declared global temporary tables
(without logging in DB2), in which inferred results are cached. At running time,
?

?

?
top-down and bottom-up procedures proceed, followed by inserting intermediate answers into temporary tables active in a session. SQL engines serve for the
final retrieval, concerning unions of conjunction queries on arbitral RDF triples,
from temporary tables (filled by inferred results) together with other physical
DB tables.

7 Conclusions

On the Semantic Web, SPARQL is for query answering in the RDF community.
An OWL ontology is RDF-based, adopting DL as its logical foundation. Given a
DL KB , by introducing integrity constraints inspired by MKNF-DL [10], we
, in the sense
present its logically equivalent version, namely the DL2DB KB 
of query answering. Meanwhile, an inference system, the DL2DB system  w.r.t.
 for non-epistemic
, takes effect, while preserving sound and complete with 
queries. The appearance of a Datalog program moves  into practice, getting
SPARQL queries solved.

Our proposal, to some extent, is not beyond DLP, where DLP has syntactical
expressive restrictions while DL2DB has semantical integrity constraints. We still
believe this paper, in an epistemic perspective, generalizes those using rules to
perform OWL reasoning. As a preliminary implementation, an engine, coupled
with a scalable OWL storage (e.g. Minerva [29] but not committing its ABox
inferences), is developed. Answers to SPARQL queries are received in seconds
on a data set of a medium size. A better performance is expected by using
more optimization techniques from existing Datalog engines (e.g. [21]) and other
Semantic Web applications (e.g. [22]).
