Web Semantics: Science, Services and Agents

on the World Wide Web 4 (2006) 124143

Semantic Web Mining

State of the art and future directions

Gerd Stumme a,

, Andreas Hotho a, Bettina Berendt b

a Knowledge and Data Engineering Group, University of Kassel, D-34121 Kassel, Germany

b Institute of Information Systems, Humboldt University Berlin, Spandauer Str. 1, D-10178 Berlin, Germany

Received 20 May 2005; accepted 2 February 2006

Abstract

Semantic Web Mining aims at combining the two fast-developing research areas Semantic Web and Web Mining. This survey analyzes the
convergence of trends from both areas: More and more researchers are working on improving the results of Web Mining by exploiting semantic
structures in the Web, and they make use of Web Mining techniques for building the Semantic Web. Last but not least, these techniques can be
used for mining the Semantic Web itself.

The Semantic Web is the second-generation WWW, enriched by machine-processable information which supports the user in his tasks. Given
the enormous size even of todays Web, it is impossible to manually enrich all of these resources. Therefore, automated schemes for learning
the relevant information are increasingly being used. Web Mining aims at discovering insights about the meaning of Web resources and their
usage. Given the primarily syntactical nature of the data being mined, the discovery of meaning is impossible based on these data only. Therefore,
formalizations of the semantics of Web sites and navigation behavior are becoming more and more common. Furthermore, mining the Semantic
Web itself is another upcoming application. We argue that the two areas Web Mining and Semantic Web need each other to fulfill their goals, but
that the full potential of this convergence is not yet realized. This paper gives an overview of where the two areas meet today, and sketches ways
of how a closer integration could be profitable.
 2006 Elsevier B.V. All rights reserved.

Keywords: Web Mining; Semantic Web; Ontologies; Knowledge discovery; Knowledge engineering; Artificial intelligence; World Wide Web

1. Introduction

The two fast-developing research areas Semantic Web and
Web Mining both build on the success of the World Wide Web
(WWW). They complement each other well because they each
address one part of a new challenge posed by the great success of
the current WWW: Most data on the Web are so unstructured that
they can only be understood by humans, but the amount of data is
so huge that they can only be processed efficiently by machines.
The Semantic Web addresses the first part of this challenge by
trying to make the data (also) machine-understandable, while
Web Mining addresses the second part by (semi-)automatically
extracting the useful knowledge hidden in these data, and making
it available as an aggregation of manageable proportions.


Corresponding author. Tel.: +49 561 804 6251.
E-mail addresses: stumme@cs.uni-kassel.de (G. Stumme),

Semantic Web Mining aims at combining the two areas Semantic Web and Web Mining. This vision follows our observation that trends converge in both areas: Increasing numbers of
researchers work on improving the results of Web Mining by
exploiting (the new) semantic structures in the Web, and make
use of Web Mining techniques for building the Semantic Web.
Last but not least, these techniques can be used for mining the
Semantic Web itself. The wording Semantic Web Mining emphasizes this spectrum of possible interaction between both research areas: It can be read both as Semantic (Web Mining) and
as (Semantic Web) mining.

In the past few years, there have been many attempts at
breaking the syntax barrier1 on the Web. A number of them
rely on the semantic information in text corpora that is implicitly
exploited by statistical methods. Some methods also analyze the
structural characteristics of data; they profit from standardized
syntax like XML. In this paper, we concentrate on markup and

1 This title was chosen by S. Chakrabarti for his invited talk at

the

hotho@cs.uni-kassel.de (A. Hotho), berendt@wiwi.hu-berlin.de (B. Berendt)

ECML/PKDD 2004 conference.

1570-8268/$  see front matter  2006 Elsevier B.V. All rights reserved.
doi:10.1016/j.websem.2006.02.001

mining approaches that refer to an explicit conceptualization
of entities in the respective domain. These relate the syntactic
tokens to background knowledge represented in a model with
formal semantics. When we use the term semantic, we thus
have in mind a formal logical model to represent knowledge.

The aim of this paper is to give an overview of where the
two areas of Semantic Web and Web Mining meet today. In our
survey, we will first describe the current state of the two areas
and then discuss, using an example, their combination, thereby
outlining future research topics. We will provide references to
typical approaches. Most of them have not been developed explicitly to close the gap between the Semantic Web and Web
Mining, but they fit naturally into this scheme.

In the next two sections, we give brief overviews of the
areas Semantic Web and Web Mining. Readers familiar with
these areas can skip Section 2 or Section 3, resp. We then go
on to describe how these two areas cooperate today, and how
this cooperation can be improved further. First, Web Mining
techniques can be applied to help create the Semantic Web. A
backbone of the Semantic Web are ontologies, which at present
are often hand-crafted. This is not a scalable solution for a widerange application of Semantic Web technologies. The challenge
is to learn ontologies and/or instances of their concepts, in a
(semi)automatic way. A survey of these approaches is contained
in Section 4.

Conversely, background knowledgein the form of ontologies or in other formscan be used to improve the process and
results of Web Mining. Recent developments include the mining
of sites that become more and more Semantic Web sites and the
development of mining techniques that can tap the expressive
power of Semantic Web knowledge representation. Section 5
discusses these various techniques.

In Section 6, we then sketch how the loop can be closed:
From Web Mining to the Semantic Web and back. We conclude,
in Section 7, that a tight integration of these aspects will greatly
increase the understandability of the Web for machines, and
will thus become the basis for further generations of intelligent
Web tools. We also return to the two notions of semantics and
outline their strengths, weaknesses, and complementarity.

Parts of this substantially revised and extended survey were
presented at the First International Semantic Web Conference
[15].

2. Semantic Web

The Semantic Web is based on a vision of Tim Berners-
Lee, the inventor of the WWW. The great success of the current WWW leads to a new challenge: A huge amount of data
is interpretable by humans only; machine support is limited.
Berners-Lee suggests to enrich the Web by machine-processable
information which supports the user in his tasks. For instance,
todays search engines are already quite powerful, but still too often return excessively large or inadequate lists of hits. Machineprocessable information can point the search engine to the relevant pages and can thus improve both precision and recall.

For instance, today it is almost impossible to retrieve information with a keyword search when the information is spread

Fig. 1. The layers of the Semantic Web.

over several pages. Consider, e.g., the query for Web Mining experts in a company intranet, where the only explicit information
stored are the relationships between people and the courses they
attended on one hand, and between courses and the topics they
cover on the other hand. In that case, the use of a rule stating that
people who attended a course which was about a certain topic
have knowledge about that topic might improve the results.

The process of building the Semantic Web is currently an area
of high activity. Its structure has to be defined, and this structure
then has to be filled with life. In order to make this task feasible,
one should start with the simpler tasks first. The following steps
show the direction where the Semantic Web is heading:

1. Providing a common syntax for machine understandable

statements.

2. Establishing common vocabularies.
3. Agreeing on a logical language.
4. Using the language for exchanging proofs.

Berners-Lee suggested a layer structure for the Semantic
Web. This structure reflects the steps listed above. It follows the
understanding that each step alone will already provide added
value, so that the Semantic Web can be realized in an incremental
fashion.

2.1. Layers of the Semantic Web

Figure 1 shows the layers of the Semantic Web as suggested
by Berners-Lee.2 This architecture is discussed in detail for instance in [126] and [127], which also address recent research
questions.

On the first two layers, a common syntax is provided. Uniform
resource identifiers (URIs) provide a standard way to refer to
entities,3 while Unicode is a standard for exchanging symbols.
The Extensible Markup Language (XML) fixes a notation for
describing labeled trees, and XML Schema allows the definition
of grammars for valid XML documents. XML documents can
refer to different namespaces to make explicit the context (and
therefore meaning) of different tags. The formalizations on these

2 see http://www.w3.org/DesignIssues/Semantic.html.
3 URL (uniform resource locator) refers to a locatable URI, e.g., an
http://... address. It is often used as a synonym, although strictly speaking
URLs are a subclass of URIs, see http://www.w3.org/Addressing.

G. Stumme et al. / Web Semantics: Science, Services and Agents on the World Wide Web 4 (2006) 124143

Fig. 2. The relation between the WWW, relational metadata, and ontologies.

two layers are nowadays widely accepted, and the number of
XML documents is increasing rapidly. While XML is one step in
the right direction, it only formalizes the structure of a document
and not its content.

The Resource Description Framework (RDF) can be seen
as the first
layer where information becomes machine-
understandable: According to the W3C recommendation,4
RDF is a foundation for processing metadata; it provides
interoperability between applications that exchange machineunderstandable information on the Web.

RDF documents consist of three types of entities: Resources,
properties, and statements. Resources may be Web pages, parts
or collections of Web pages, or any (real-world) objects which
are not directly part of the WWW. In RDF, resources are always
addressed by URIs. Properties are specific attributes, charac-
teristics, or relations describing resources. A resource together
with a property having a value for that resource form an RDF
statement. A value is either a literal, a resource, or another
statement. Statements can thus be considered as object-attribute-
value triples.

The middle part of Fig. 2 shows an example of RDF state-
ments. Two of the authors of the present paper (i.e., their
Web pages) are represented as resources URI-GST and URI-

4 http://www.w3.org/TR/REC-rdf-syntax-grammar-20040210/

AHO. The statement on the lower right consists of the resource
URI-AHO and the property cooperates-with with the value
URI-GST (which again is a resource). The resource URISWMining has as value for the property title the literal Semantic Web Mining.

The data model underlying RDF is basically a directed labeled graph. RDF Schema defines a simple modeling language
on top of RDF which includes classes, is-a relationships between
classes and between properties, and domain/range restrictions
for properties. RDF and RDF Schema are written in XML syn-
tax, but they do not employ the tree semantics of XML.

XML and XML schema were designed to describe the structure of text documents, like HTML, Word, StarOffice, or LATEX
documents. It is possible to define tags in XML to carry metadata
but these tags do not have formally defined semantics and thus
their meaning will not be well-defined. It is also difficult to convert one XML document to another one without any additionally
specified semantics of the used tags. The purpose of XML is to
group the objects of content, but not to describe the content.
Thus, XML helps the organization of documents by providing a
formal syntax. This is not semantic in the sense of our survey.
Erdmann [48] provides a detailed analysis of the capabilities
of XML, the shortcomings of XML concerning semantics, and
possible solutions.

The next layer is the ontology vocabulary. Following [62], an
ontology is an explicit formalization of a shared understand-

ing of a conceptualization. This high-level definition is realized
differently by different research communities. However, most of
them have a certain understanding in common, as most of them
include a set of concepts, a hierarchy on them, and relations
between concepts. Most of them also include axioms in some
specific logic. We will discuss the most prominent approaches
in more detail in the next subsection. To give a flavor, we present
here just the core of our own definition [148,22], as it is reflected
by the Karlsruhe Ontology framework KAON.5 It is built in a
modular way, so that different needs can be fulfilled by combining parts.
Definition 1. A core ontology with axioms is a structure O :=
(C,C,R, ,R,A) consisting of
 two disjoint sets C and R whose elements are called concept
 a partial orderC on C, called concept hierarchy or taxonomy,
 a function  : R  C+
is the set
of all finite tuples of elements in C),
 a partial order R on R, called relation hierarchy,
where r1 R r2 implies |(r1)| = |(r2)| and i((r1)) C
i((r2)), for each 1  i  |(r1)|, with i being the projection on the ith component, and

called signature (where C+

identifiers and relation identifiers, resp.,

 a set A of logical axioms in some logical language L.

This definition constitutes a core structure that is quite
straightforward, well agreed-upon, and that may easily be
mapped onto most existing ontology representation languages.
Step by step the definition can be extended by taking into account
lexicons and knowledge bases [148].
As an example, have a look at the top of Fig. 2. The set C
of concepts is the set {Top, Project, Person, Researcher, Lit-
eral}, and the concept hierarchy C is indicated by the arrows
with a filled arrowhead. The set R of relations is the set {works-
in, cooperates-with, name, title}. The relation works-in has
(Person, Project) as signature, the relation name has (Person,
Literal) as signature.6 In this example, the hierarchy on the relations is flat, i.e., R is just the identity relation. (An example
of a non-flat relation hierarchy will be shown below in Fig. 3.)
Up to here, RDF Schema would be sufficient for formalizing
the ontology. But often ontologies also contain logical axioms.
The one in Fig. 2 states for instance that the cooperates-with
relation is symmetric. This will be used for inferencing on the
logic level.

The objects of the metadata level can now be seen as instances
of the ontology concepts. For example, URI-SWMining is an
instance of the concept Project, and thus by inheritance also
of the concept Top.

Logic is the next layer according to Berners-Lee. Today, most
research treats the ontology and the logic levels in an integrated
fashion because most ontologies allow for logical axioms. By
applying logical deduction, one can infer new knowledge from

the information which is stated explicitly. For instance, the axiom given above allows one to logically infer that the person
addressed by URI-AHO cooperates with the person addressed
by URI-GST. The kind of inference that is possible depends
heavily on the logics chosen. We will discuss this aspect in the
next subsection in more detail.

Proof and trust are the remaining layers. They follow the
understanding that it is important to be able to check the validity
of statements made in the (Semantic) Web, and that trust in
the Semantic Web and the way it processes information will
increase in the presence of statements thus validated. Therefore,
the author must provide a proof which should be verifiable by
a machine. At this level, it is not required that the machine of
the reader finds the proof itself, it just has to check the proof
provided by the author. These two layers are rarely tackled in
todays research. Hence we will focus our interest on the XML,
RDF, ontology and logic layers in the remainder of this article.

2.2. Ontologies: Languages and tools

A priori, any knowledge representation mechanism7 can play
the role of a Semantic Web language. Frame Logic (or F-
Logic; [89]) is one candidate, since it provides a semantically
founded knowledge representation based on the frame-and-slot
metaphor. Another formalism that fits well with the structure
of RDF are Conceptual Graphs [136,39]. They also provide a
visual metaphor for representing the conceptual structure.

Probably the most popular framework at the moment are
description logics (DLs). DLs are subsets of first order logic
which aim at being as expressive as possible while still being
decidable. The description logic SHIQ provides the basis for
DAML + OIL, which, in its turn, is a result of joining the efforts
of two projects: The DARPA Agent Markup Language DAML8
was created as part of a research programme started in August
2000 by DARPA, a US governmental research organization. OIL
(Ontology Inference Layer) is an initiative funded by the European Union programme. The latest version of DAML + OIL
has been released as a W3C Recommendation under the name
OWL.9

Several tools are in use for the creation and maintenance of
ontologies and metadata, as well as for reasoning within them.
Ontoedit [151,152] is an ontology editor which is connected to
Ontobroker [53], an inference engine for F-Logic. It provides
means for semantics-based query handling over distributed re-
sources. F-Logic has also influenced the development of Triple
[135], an inference engine based on Horn logic, which allows the
modelling of features of UML, Topic Maps, or RDF Schema.
It can interact with other inference engines, for example with
FaCT or RACER.
FaCT10 provides inference services for the description language SHIQ. In [75], reasoning within SHIQ and its relation-

5 http://kaon.semanticweb.org.
6 By convention, relations with Literal as range are drawn in this way, because

they are in some contexts considered as attributes.

7 See [146] for a general discussion.
8 http://www.daml.org.
9 http://www.w3.org/TR/owl-features/.
10 http://www.cs.man.ac.uk/horrocks/FaCT.

G. Stumme et al. / Web Semantics: Science, Services and Agents on the World Wide Web 4 (2006) 124143

ship to DAML + OIL are discussed. Reasoning is implemented
in the FaCT inference engine, which also underlies the ontology
editor OilEd [12]. RACER [63] is another reasoner for SHIQ,
with emphasis on reasoning about instances.

The Karlsruhe Ontology Framework KAON [22] is an opensource ontology management and learning infrastructure targeted for business applications. It includes a comprehensive
tool suite allowing easy ontology creation supported by machine learning algorithms, ontology management, and building ontology-based applications. The tool suite is also connected to databases to allow working with a large number of in-
stances. Prot eg e-2000 [120] is a platform-independent environment for creating and editing ontologies and knowledge bases.
Like KAON, it has an extensible plug-in structure. Sesame [83]
is an architecture for efficient storage and expressive querying of
large quantities of RDF(S) data. It provides support for concurrency control, independent export of RDF(S) information, and a
query engine for RQL, a query language for RDF. An extensive
overview of ontology tools can be found in [59].

2.3. Related research areas and application areas

One of the many research areas related to the Semantic Web
are databases. In the last few years, most commercial database
management systems have included the possibility of storing
XML data in order to also accommodate semi-structured data.
As the database community has worked on data mining techniques for a long time now, it can be expected that sooner or
later XML mining will become an active research topic. In-
deed, there are first approaches in that direction [100]. From our
point of view, it can be seen as a special case of Semantic Web
Mining.

More generally, several problems (and solutions) within the
database domain are also found within ontology engineering, for
instance schema mapping, or the integration of heterogeneous,
distributed data sources. This is addressed in more detail in Section 4.1, where we also discuss ways of deriving ontologies from
database schemas.

Another related research area are Topic Maps11 that represent the structure of relationships between subjects. Most of
the software for topic maps uses the syntax of XML, just as
RDF does. In fact, Topic Maps and RDF are closely related. In
[8], a formal framework is provided for Topic Maps, which can
also be applied to RDF. Semantic Web Mining with Topic Maps
has for instance been discussed in [60]. Commercial tools like
theBrain12 provide very similar features like named relations,
but without an underlying formal semantics.

Different application areas benefit from the Semantic Web
and a (re-)organisation of their knowledge in terms of ontologies.
Among them are Web Services [52,51,128,124,25] and Knowledge Management (see [144] for a framework and tool suite,
and [98] for an application example). In E-learning, metadata

standards have a long tradition (in particular, Dublin Core13 and
LOM, the Learning Objects Metadata14). They are employed in
educational portals,15 and a general move towards XML and/or
RDF notation can be observed.

Many types of sites can profit from a (re-)organisation as
Semantic Web Sites. Knowledge portals16 provide views onto
domain-specific information on the World Wide Web for helping
their users to find relevant information. Their maintenance can
be greatly improved by using an ontology-based backbone architecture and tool suite, as provided by SEAL [106] and SEAL-II
[77].

While metadata are useful on the Web, they are essential for
finding resources in peer-to-peer networks. Examples include
EDUTELLA [118] (which transfers the educational LOM standard mentioned above to a P2P architecture) and POOL [70].

3. Web Mining

Web Mining is the application of data mining techniques to
the content, structure, and usage of Web resources. It is thus the
nontrivial process of identifying valid, previously unknown, and
potentially useful patterns [50] in the huge amount of these Web
data, patterns that describe them in concise form and manageable
orders of magnitude. Like other data mining applications, Web
Mining can profit from given structure on data (as in database ta-
bles), but it can also be applied to semi-structured or unstructured
data like free-form text. This means that Web Mining is an invaluable help in the transformation from human-understandable
content to machine-understandable semantics.

Three areas of Web Mining are commonly distin-
guished: Content mining, structure mining, and usage mining
[163,96,142]. In all three areas, a wide range of general data
mining techniques, in particular association rule discovery, clus-
tering, classification, and sequence mining, are employed and
developed further to reflect the specific structures of Web resources and the specific questions posed in Web Mining. For
reasons of space, we will introduce Web content, structure, and
usage mining only briefly here; for in-depth overviews of methods and/or applications, see [67,157,66,27,9].

3.1. Content/text of Web pages

Web content mining analyzes the content of Web resources.
Today, it is mostly a form of text mining (for overviews, see
[26,133]). Recent advances in multimedia data mining promise
to widen access also to image, sound, video, etc. content of Web
resources. Multimedia data mining can produce semantic annotations that are comparable to those obtained from text mining;
we therefore do not consider this field further (see [134,161] and
the references cited there). The primary Web resources that are
mined in Web content mining are individual pages.

11 http://www.topicmaps.org/.
12 http://www.thebrain.com/.

13 http://dublincore.org.
14 see http://ltsc.ieee.org/wg12.
15 e.g., http://www.eduserver.de.
16 An example is http://www.ontoweb.org.

Information Retrieval is one of the research areas that provides a range of popular and effective, mostly statistical methods
for Web content mining. They can be used to group, categorize,
analyze, and retrieve documents, cf. [137] for a survey of IR
and [96] for a survey of the relation between IR and Web content mining. These techniques form an excellent basis for more
sophisticated approaches. A prime example is Latent Semantic
Analysis (LSA) [44]. LSA and other factor-analytic methods
have proven valuable for analyzing Web content and also us-
age, e.g. [24,84]. However, LSA refers to a looser notion of
semantic; a lot of effort is needed to identify an explicit conceptualization from the calculated relations.

In addition to standard text mining techniques, Web content
mining can take advantage of the semi-structured nature of Web
page text. HTML tags and XML markup carry information that
concerns not only layout, but also logical structure. Taking this
idea further, a database view of Web content mining [96] attempts to infer the structure of a Web site in order to transfer it
into a database that allows better information management and
querying than a pure IR view.

Web content mining is specifically tailored to the characteristics of text as it occurs in Web resources. Therefore, it focuses
on the discovery of patterns in large document collections and
in frequently changing document collections. An application is
the detection and tracking of topics [5]. This can serve to detect critical events (that become reflected as a new topic in the
evolving document corpus) and trends that indicate a surge or
decline in interest in certain topics.

Further content mining methods which will be used for Ontology learning, mapping and merging ontologies, and instance
learning are described in Section 4.1. In Section 6, we will further set them in relation to the Semantic Web.

3.2. Structure between Web pages

Web structure mining usually operates on the hyperlink structure of Web pages (for a survey, see [27]). Mining focuses on
sets of pages, ranging from a single Web site to the Web as a
whole. Web structure mining exploits the additional information
that is (often implicitly) contained in the structure of hypertext.
Therefore, an important application area is the identification of
the relative relevance of different pages that appear equally pertinent when analyzed with respect to their content in isolation.
For example, hyperlink-induced topic search [91] analyzes
hyperlink topology by discovering authoritative information
sources for a broad search topic. This information is found in
authority pages, which are defined in relation to hubs: Hubs are
pages that link to many related authorities. Similarly, the search
engine Google17 owes its success to the PageRank algorithm,
which states that the relevance of a page increases with the number of hyperlinks to it from other pages, and in particular from
other relevant pages [123].

Web structure mining and Web content mining are often
performed together, allowing the algorithm to simultaneously

17 http://www.google.com.

exploit the content and the structure of hypertext. Indeed, some
researchers subsume both under the notion of Web content
mining [37].

3.3. Usage of Web pages

In Web usage mining, mining focuses on records of the requests made by visitors to a Web site, most often collected in
a Web server log [142,143]. The content and structure of Web
pages, and in particular those of one Web site, reflect the intentions of the authors and designers of the pages and the underlying information architecture. The actual behavior of the users of
these resources may reveal additional structure.

First, relationships may be induced by usage where no
particular structure was designed. For example, in an online
catalog of products, there is usually either no inherent structure
(different products are simply viewed as a set), or one or several
hierarchical structures given by product categories, etc. Mining
the visits to that site, however, one may find that many of the
users who were interested in product A were also interested in
product B. Interest may be measured by requests for product
description pages, or by the placement of that product into the
shopping cart. Such correspondences between user interest in
various items can be used for personalization, for example by
recommending product B when product A has been viewed
(cross-selling/up-selling in E-commerce), or by treating a
visitor according to which customer segment his behavior
indicates. Examples of algorithms and applications can be
found in [115,101,94] and in the recommendations made by
online bookstores and other online shops.

Second, relationships may be induced by usage where a different relationship was intended [36]. For example, sequence
mining may show that many of the users who went from page
C to page D did so along paths that indicate a prolonged search
(frequent visits to help and index pages, frequent backtracking,
etc.). This relation between topology and usage may indicate
usability problems: Visitors wish to reach D from C, but need
to search because there is no direct hyperlink [87], or because it
is hard to find [18]. These insights can be used to improve the
sites information architecture as well as page design.

Third, usage mining may reveal events in the world faster
than content mining. Topic detection and tracking can identify
events when they become reflected in texts, i.e. in Web authors
writing behaviour. However, information seeking often precedes
authoring, and there are more Web users than Web authors. An
example is the detection of the onset of epidemics (or the fear of
epidemics) in the usage of medical information sites [159,71].
Pattern monitoring [10] allows the analyst to go beyond the analysis of simple time series and to track evolutions in more complex access patterns like association rules or sequences.

3.4. Combined approaches

It is useful to combine Web usage mining with content and
structure analysis in order to make sense of observed frequent
paths and the pages on these paths. This can be done by using
a variety of methods. Early approaches have relied on pre-built

G. Stumme et al. / Web Semantics: Science, Services and Agents on the World Wide Web 4 (2006) 124143

taxonomies [162] and/or on IR-based keyword extraction methods [39]. Many methods rely on a mapping of pages into an
ontology; this will be discussed in Sections 5.2 and 6.

In the following section, we will first look how ontologies and
their instances can be learned. We will then go on to investigate
how the use of ontologies, and other ways of identifying the
meaning of pages, can help to make Web Mining go semantic.

4. Extracting Semantics from the Web

The effort behind the Semantic Web is to add machine-
understandable, semantic annotation to Web documents in order
to access knowledge instead of unstructured material. The purpose is to allow knowledge to be managed in an automatic way.
Web Mining can help to learn structures for knowledge organization (e.g., ontologies) and to provide the population of such
knowledge structures.

All approaches discussed here are semi-automatic. They assist the knowledge engineer in extracting the semantics, but cannot completely replace her. In order to obtain high-quality re-
sults, one cannot replace the human in the loop, as there is always
a lot of tacit knowledge involved in the modeling process [23]. A
computer will never be able to fully consider background knowl-
edge, experience, or social conventions. If this were the case, the
Semantic Web would be superfluous, since then machines like
search engines or agents could operate directly on conventional
Web pages. The overall aim of our research is thus not to replace
the human, but rather to provide her with more and more support.

4.1. Semantics created by Content and Structure

4.1.1. Ontology learning

Extracting an ontology from the Web is a challenging task.
One way is to engineer the ontology by hand, but this is expen-
sive. In [105], the expression ontology learning was coined for
the semi-automatic extraction of semantics from the Web. There,
machine learning techniques were used to improve the ontology
engineering process and to reduce the effort for the knowledge
engineer. An example is given in Section 6.

Ontology learning exploits many existing resources including
texts, thesauri, dictionaries, and databases (see [122] as an example of the use of WordNet). It builds on techniques from Web
content mining, and it combines machine learning techniques
with methods from fields like information retrieval [102] and
agents [156], applying them to discover the semantics in the
data and to make them explicit. The techniques produce intermediate results which must finally be integrated in a machineunderstandable format, e.g., an ontology. Mining can supplement existing (Web) taxonomies with new categories (cf. [4] for
an extension of Yahoo18), and it can help build new taxonomies
[95].

A growing number of sites deliver pages that are generated
dynamically in an interaction of an underlying database, information architecture, and query capabilities. For many sites and

18 http://www.yahoo.com.

analysis questions an ontology can be compiled from internal
sources such as database schemas, query options, and transaction
models. This reverse engineering typically involves a large
amount of manual work, but it can be aided by (semi-)automatic
ontology learning schemes. For example, many retailing and
information sites have similarly structured product catalogs
[18,139]. Thus, a tourism site may contain the URL stems
search hotel.html, search yacht club.html,
. . . which allows the deduction of the product categories
hotel, yacht club, etc. 19

4.1.2. Mapping and merging ontologies

The growing use of ontologies leads to overlaps between
knowledge in a common domain. Domain-specific ontologies
are modeled by multiple authors in multiple settings. These ontologies lay the foundation for building new domain-specific
ontologies in similar domains by assembling and extending multiple ontologies from repositories.

The process of ontology merging takes as input two (or more)
source ontologies and returns a merged ontology. Manual ontology merging using conventional editing tools without support
is difficult, labor-intensive, and error-prone. Therefore, several
systems and frameworks for supporting the knowledge engineer in the ontology merging task have recently been proposed
[80,30,119,109]. These approaches rely on syntactic and semantic matching heuristics which are derived from the behavior of
ontology engineers confronted with the task of merging ontolo-
gies. Another method is FCA-Merge, which operates bottomup and offers a global structural description of the process [149].
It extracts instances of source-ontology concepts from a given
set of domain-specific text documents by applying natural language processing techniques. Based on the extracted instances,
it uses the Titanic algorithm [150] to compute a concept lattice.
The concept lattice provides a conceptual clustering of the concepts of the source ontologies. It is explored and interactively
transformed into the merged ontology by the ontology engineer.
Ontology mapping is the assignment of the concepts of one
ontology and their instances to the concepts of another ontology.
This could be useful, for example, when one of several ontologies has been chosen as the right one for the task at hand. The
instances can simply be classified from scratch into the target
ontology; alternatively, the knowledge inherent in the source ontology can be utilized by relying on the heuristic that instances
from one source concept are likely to also be classified together
in one concept of the target ontology [164].

An alternative to merging/mapping ontologies is to simply
collect them in parallel and to select the right one according to
the task at hand. This vision of a corpus of representations
is presented in [65], which opens a new domain of interesting
research questions.

19 This is part of a running example, to be used throughout the paper,
describing a fictitious tourism Web site. It is based on the Getess project
(http://www.getess.de/index en.html), which provides ontology-based access
to tourism Web pages for the German region Mecklenburg-Vorpommern
(http://www.all-in-all.de).

4.1.3. Instance learning

Even if ontologies are present and users manually annotate
new documents, there will still be old documents containing
unstructured material. In general, the manual markup of every
produced document is impossible. Also, some users may need to
extract and use different or additional information from the one
provided by the creator. To build the Semantic Web, it is therefore
essential to produce automatic or semi-automatic methods for
extracting information from Web-related documents as instances
of concepts from an ontology, either for helping authors to annotate new documents or for extracting additional information
from existing unstructured or partially structured documents.

A number of studies investigate the use of content mining to
enrich existing conceptualizations behind a Web site. For exam-
ple, in [114], Mladenic used text categorization techniques to
assign HTML pages to categories in the Yahoo hierarchy. This
can reduce the manual effort for maintaining the Yahoo Web
index.

Information extraction from texts (IE) is one of the most
promising areas of Natural Language Technologies (see, e.g.,
[40]). IE is a set of automatic methods for locating important
facts in electronic documents for subsequent use. IE techniques
range from the extraction of keywords from pages text using
the tf.idf method known from Information Retrieval, via techniques that take the syntactic structures of HTML or natural
language into account, to techniques that extract with reference
to an explicitly modeled target structure such as an ontology (for
a survey, see [97]).

Information extraction is the perfect support for knowledge
identification and extraction from Web documents as it can 
for example  provide support in documents analysis either in
an automatic way (unsupervised extraction of information) or in
a semi-automatic way (e.g., as support for human annotators in
locating relevant facts in documents, via information highlight-
ing). One such system for IE is FASTUS [74]. Another system is
GATE.20 With the rise of the Semantic Web, it has been extended
to ontology support, and in particular for instance learning [20].
The OntoMat Annotizer [68] has been developed directly for the
Semantic Web. It complements IE with authoring functionality.
The approach of Craven et al. [41] is discussed in Section 6.
In [72,73], machine learning techniques have been used for the
semi-automatic annotation of Web services.

4.1.4. Using existing conceptualizations as ontologies and
for automatic annotation

For many sites, an explicit domain model for the generation
of Web pages already exists. These existing formalizations can
be (re-)used for semantic markup and mining.

the path to the product

For example, many Content Management Systems generate Web pages from a product catalog, at URLs that
reflect
in the catalog hierarchy.
In the running example,
lead to URLs like
Hotels/WellnessHotels/BeachHotel.html (simi-
lar URLs can be found in popular Web indices). Classification

this might

by product hierarchy is a commonly used technique for Web usage mining, see e.g. [141,7,54] and the KDD Cup 2000 dataset
available for testing algorithms.21 Alternatively, pages may be
generated from a full-blown ontology and its inference engine
[121,113]. The adaptation of this basic idea to dynamic URLs
is described in Section 5.2.1.

To achieve a common ontology and markup scheme, pages
can be generated centrally by one application server. In the case
of distributed authorship, the use of the common ontology can
be ensured by interactive tools that help individual authors to
mark up their pages. This has proven to be a successful strategy
for developing community-based portals.22

Another way of using existing information is described in
[69]: Deep annotation derives mappings between information
structures from databases. These mappings are used for querying semantic information stored in the database underlying the
Web site. This combines capabilities of conventional Web page
annotation and automatic Web page generation from databases.

4.1.5. Semantics created by structure

As we have discussed in Section 3.2, the results of the analysis
of Web page linkage by Web usage mining create a certain kind
of knowledge, a ranking of relevance. Another kind of knowledge that may be inferred from structure is a similarity between
pages, useful for the popular browser application Find similar
pages (to one that has been retrieved by browsing or search):
Based on the observation that pages which are frequently cited
together from other pages are likely to be related, Dean and
Henzinger [43] propose two algorithms for finding similar pages
based on hyperlink structure. These techniques structure the set
of pages, but they do not classify them into an ontology.

In contrast, the hyperlink structure within pages lends itself
more directly to classification. Cooley, Mobasher, and Srivastava [38], based on [129], propose an ontology of page func-
tions, where the classification of a single page with respect to
this ontology can be done (semi)-automatically. For example,
navigation pages designed for orientation contain many links
and little information text, whereas content pages contain a
small number of links and are designed to be visited for their
content. This can be used to compare intended usage with actual
usage [36]. For example, a content page that is used as a frequent
entry point to a site signals a challenge for site design: First, the
intended entry point, which is probably the home page, should
be made better-known and easier to locate. Second, additional
links for navigation could be provided on the page that is currently the actual entry point. Its content may become a candidate
for a new top-level content category on various head pages.

The structure of within-page markup may also help in extracting page content: Concentrating on page segments identified by
reference to the pages DOM (document object model, or tag
tree) can serve to identify the main content of a page ([27], pp.
228ff.) and to separate it from noise like navigation bars, ad-
vertisements, etc. [158].

20 http://gate.ac.uk/.

21 http://www.ecn.purdue.edu/KDDCUP.
22 See http://www.ontoweb.org and http://www.eduserver.de.

G. Stumme et al. / Web Semantics: Science, Services and Agents on the World Wide Web 4 (2006) 124143

4.2. Semantics created by usage

The preceding discussion has implicitly assumed that content
exists independently of its usage. However, a large proportion of
knowledge is socially constructed. Thus, navigation is not only
driven by formalized relationships or the underlying logic of the
available Web resources. Rather, it is an information browsing strategy that takes advantage of the behavior of like-minded
people ([31], p. 18). Recommender systems based on collaborative filtering have been the most popular application of this
idea. In recent years, the idea has been extended to consider not
only ratings, but also Web usage as a basis for the identification
of like-mindedness (People who liked/bought this book also
looked at . . .; cf. Section 3.3 and [86] for a classic application).
Extracting such relations from usage can be interpreted as a
kind of ontology learning, in which the binary relation is related
to on pages (and thus concepts) is learned. Can usage patterns
reveal further relations to help build the Semantic Web? This
field is still rather new, so we will only describe an illustrative
selection of research approaches.

Ypma and Heskes [160] propose a method for learning content categories from usage. They model navigation in terms of
hidden Markov models, with the hidden states being page cate-
gories, and the observed request events being instances of them.
Their main aim is to show that a meaningful page categorization
may be learned simultaneously with the user labeling and intercategory transitions; semantic labels (such as sports pages)
must be assigned to a state manually. The resulting taxonomy
and page classification can be used as a conceptual model for
the site, or used to improve an existing conceptual model.

Chi et al. [33,32] identify frequent paths through a site. Based
on the keywords extracted from the pages along the path, they
compute the likely information scent followed, i.e. the intended goal of the path. The information scent is a set of weighted
keywords, which can be inspected and labeled more concisely
by using an interactive tool. Thus, usage creates a set of information goals users expect the site to satisfy.23 These goals may
be used to modify or extend the content categories shown to the
users, employed to structure the sites information architecture,
or employed in the sites conceptual model.

Stojanovic, Maedche, Motik, and Stojanovic [145] propose
to measure user interest in a sites concepts by the frequency
of accesses to pages that deal with these concepts. They use
these data for ontology evolution: Extending the sites coverage
of high-interest concepts, and deleting low-interest concepts, or
merging them with others.

The combination of implicit user input (usage) and explicit
user input (search engine queries) can contribute further to conceptual structure. User navigation has been employed to infer
topical relatedness, i.e. the relatedness of a set of pages to a topic
as given by the terms of a query to a search engine (collabo-
rative crawling [2]). A classification of pages into satisfying
the user defined predicate and not satisfying the predicate is

23 An empirical validation showed that this kind of content analysis does indeed
group paths that have the same information goal [34].

thus learned from usage, structure, and content information. An
obvious application is to mine user navigation to improve search
engine ranking [85,88].

Many approaches use a combination of content and usage
mining to generate recommendations. For example, in contentbased collaborative filtering, textual categorization of documents is used for generating pseudo-rankings for every userdocument pair [110]. In [125], ontologies, IE techniques for analyzing single pages, and a users search history together serve
to generate recommendations for query improvement in a search
engine.

5. Using Semantics for Web Mining and mining the
Semantic Web

Semantics can be utilized for Web Mining for different pur-
poses. Some of the approaches presented in this section rely on
a comparatively ad hoc formalization of semantics, while others
can already exploit the full power of the Semantic Web. The
Semantic Web offers a good basis to enrich Web Mining: The
types of (hyper)links are now described explicitly, allowing the
knowledge engineer to gain deeper insights in Web structure
mining; and the contents of the pages come along with a formal
semantics, allowing her to apply mining techniques which require more structured input. Because the distinction between the
use of semantics for Web Mining and the mining of the Semantic
Web itself is all but sharp, we will discuss both in an integrated
fashion.

The first major application area is content mining, i.e., the
explicit encoding of semantics for mining the Web content. The
hyperlinks and anchors in a page are part of that pages text,
and in a semantically marked-up page they are page elements in
the same way that text is. So content and structure are strongly
intertwined (the two fields are sometimes treated as one [37]).
In the Semantic Web, the distinction between content and structure mining disappears completely, as the content of the page is
explicitly turned into the structure of the annotation. However, it
should be noted that the distribution of the semantic annotations
within a page and across pages may provide additional implicit
knowledge.

5.1. Content and structure mining

In [76], ontologies are used as background knowledge during preprocessing, with the aim of improving clustering results.
We preprocess the input data (e.g., text) and apply ontologybased heuristics for feature selection and feature aggregation.
Based on these representations, we compute multiple clustering results using k-Means. Using the ontology, we can select
the result which is most approporiate to our task at hand. In
[79], we demonstrate the improvement in clustering that arises
from the use of WordNet for preprocessing the Reuters cor-
pus. An analogous study showed improvements in classification
[19].

Another current project aims at facilitating the customized
access to courseware material which is stored in a peer-to-peer

network24 by means of conceptual clustering. We employ techniques from Formal Concept Analysis, which have been applied successfully in the Conceptual Email Manager (CEM)
[35]. CEM provides an ontology-based search hierarchy of concepts (clusters) with multiple search paths. A combination of
this approach with text clustering and a visualization method
for analyzing the results are presented in [78].

Knowledge-rich approaches in automatic text summarization
(cf. [107,108,81]) aim at maximizing the information within
a minimal amount of resulting text. They are closely related
to Web content mining using semantics because in both Web
content mining and text summarization, natural language text
needs to be mapped into an abstract representation. This abstract
is often represented in some logic, and it is used to improve
the results of text summarization. We expect that techniques
for automatic text summarization will play an important role in
Semantic Web Mining.

Web structure mining can also be improved by taking content into account. The PageRank algorithm mentioned in Section 3 co-operates with a keyword analysis algorithm, but the
two are independent of one another. So PageRank will consider
any much-cited page as relevant, regardless of whether that
pages content reflects the query. By also taking the hyperlink
anchor text and its surroundings into account, CLEVER [28]
can more specifically assess the relevance for a given query.
The Focused Crawler [29] improves on this by integrating topical content into the link graph model, and by a more flexible way of crawling. The learning Intelligent Crawler [3] extends the Focused Crawler, allowing predicates that combine
different kinds of topical queries, keyword queries, or other
constraints on the pages content or meta-information (e.g.,
URL domain). Ontology-based focused crawling is proposed by
[103].

An important group of techniques which can easily be
adapted to Semantic Web content/structure mining are the approaches discussed as (Multi-)Relational Data Mining (formerly
called Inductive Logic Programming/ILP) [46]. Relational Data
Mining looks for patterns that involve multiple relations in a
relational database. It comprises techniques for classification,
regression, clustering, and association analysis. The algorithms
can be transformed to deal with data described in RDF or by on-
tologies. A starting point for such transformations is described in
[61] that analyzes different logics and develops a new knowledge
representation format closely related to Horn logic, one of the
logics that are common in ILP. Making Relational Data Mining
amenable to Semantic Web Mining faces two major challenges.
The first is the size of the datasets to be processed and the second is the distribution of the data over the Semantic Web. The
scalability to huge datasets has always been a major concern for
ILP algorithms. With the expected growth of the Semantic Web,
this problem increases as well. Therefore, the performance of
the mining algorithms has to be improved by methods like sampling (e.g., [132]). To process distributed data, algorithms have
to be developed that perform mining in a distributed manner,

24 http://edutella.jxta.org.

such that instead of whole datasets, only (intermediate) results
have to be transmitted.

5.2. Usage Mining

Web usage mining benefits from including semantics into the
mining process for the simple reason that the application expert
as the end user of mining results is interested in events in the
application domain, in particular user behavior, while the data
availableWeb server logsare technically oriented sequences
of HTTP requests.25 A central aim is therefore to map HTTP
requests to meaningful units of application events.

In this section, we will first introduce a framework for the
modeling of user behavior, and then discuss how this background
knowledge is used in mining. To illustrate the framework, we
will use it to describe a number of existing studies of Web usage.
We will concentrate on the semantic aspects of the framework.
The studies we describe use a number of different syntactical
conventions for representing the semantics; we expect that in the
future, XML-based (and thus syntactically standardized) notations will allow a better exchange and re-use of these models
[131,92].

5.2.1. Application events

Application events are defined with respect to the application
domain and the site, a non-trivial task that amounts to a detailed
formalization of the sites business/application model (for de-
tails, see [16]). For example, relevant E-business events include
product views and product click-throughs in which a user shows
specific interest in a specific product by requesting more detailed
information (e.g., from the Beach Hotel to a listing of its prices
in the various seasons). Related events include click-throughs
to a product category (e.g., from the Beach Hotel, to the category of All Wellness Hotels), click-throughs from a banner ad,
shopping cart changes, and product purchases or bids.

These events are examples of what we call atomic application events; they generally correspond to a users request for one
page(view). They can be characterized by their content (e.g.,
the Beach Hotel, or more generally All Wellness Hotels or All
Hotels, see Fig. 3) and the service requested when this page
is invoked (e.g., the search hotels by location function) [18].
One page may be mapped to one or to a set of application events.
For example, it may be mapped to the set of all concepts and
relations that appear in its querystring [121]. Alternatively, keywords from the pages text and from the pages linked with it
may be mapped to a domain ontology, with a general-purpose
ontology like WordNet serving as an intermediary between the
keywords found in the text and the concepts of the ontology
[47].

25 Note that this discussion assumes that some other issues affecting data qual-
ity, e.g., the assignment of requests to users and/or sessions, have either been
solved or do not affect the inferences based on the semantics of the requested
Web pages. This is an idealization, see [17] for an investigation of the effect of
sessionization heuristics on mining results. The use of application server logs
can help to circumvent some of these problems [93]. In the following discussion,
we also assume that other standard preprocessing steps have been taken [38].

G. Stumme et al. / Web Semantics: Science, Services and Agents on the World Wide Web 4 (2006) 124143

5.2.2. How is knowledge about application events used in
mining?

Once requests have been mapped to concepts, the transformed
data are ready for mining. We will investigate the treatment of
atomic and of complex application events in turn.

In many applications (cf. the examples in Figs. 3 and 4),
concepts partake in multiple taxonomies. Mining using multiple
taxonomies is related to OLAP data cube techniques: objects (in
this case, requests or requested URLs) are described along a
number of dimensions, and concept hierarchies or lattices are
formulated along each dimension to allow more abstract views
(cf. [162,90,82,147]).

Taxonomic abstraction is often essential for generating meaningful results: First, in a site with dynamically generated pages,
each individual page will be requested so rarely that no regularities may be found in an analysis of navigation behavior.
Rather, regularities may exist at a more abstract level, leading
to rules like people who stay in Wellness Hotels also tend to
eat in restaurants. Second, patterns mined in past data are not
helpful for applications like recommender systems when new
items are introduced into product catalog and/or site structure:
The new Pier Hotel cannot be recommended simply because it
was not in the tourism site until yesterday and thus could not
co-occur with any other item, be recommended by another user,
etc. A knowledge of regularities at a more abstract level could
help to generate a recommendation of the Pier Hotel because
it is a Wellness Hotel, and there are criteria for recommending
Wellness Hotels.

After the preprocessing steps in which access data have been
mapped into taxonomies, subsequent mining techniques can use
these taxonomies statically or dynamically. In static approaches,
mining operates on concepts at a chosen level of abstraction;
each request is mapped to exactly one concept or exactly one set
of concepts (see the above examples). This approach is usually
combined with interactive control of the software, so that the analyst can re-adjust the chosen level of abstraction after viewing
the results (e.g., in the miner WUM; see [18] for a case study).
When the investigated complex application events have a sequential structure, sequence mining is required. This is usually
the case in investigations of searching, shopping, etc. strategies,
as the examples above show.

In dynamic approaches, algorithms identify the most specific level of relationships by choosing concepts dynamically.
This may lead to rules like People who stay in Wellness Hotels tend to eat at vegetarian-only Indian restaurantslinking
hotel-choice behavior at a comparatively high level of abstraction with restaurant-choice behavior at a comparatively detailed
level of description.

For example, Srikant and Agrawal [141] search for associations in given taxonomies, using support and confidence thresholds to guide the choice of level of abstraction. The subsumption hierarchy of an existing ontology is also used for the simultaneous description of user interests at different levels of
abstraction, and this description is used to guide the association
rule and clustering algorithms in methods that link Web pages
to an underlying ontology in a more fine-grained and flexible
way [121,47,113]. When an explicit taxonomy is missing, min-

Fig. 3. Parts of the ontology of the content of a fictitious tourism Web site.

Fig. 4 shows a service ontology for the fictitious example
site, modeled after the one used in a real-world example in [18].
The site shows accommodation-related information at different
levels of detail: As a home (or start) page, on product category
pages (lists of hotels or facilities), and on invidividual product
pages. Search strategies consist of the specification of one or
more of the parameters location, price, and name. Parameters
and their values are specified by choice from a menu or by typing.
In response, the server generates a category page with all hotels
or facilities that satisfy the given specifications.

Atomic application events are usually part of larger meaningful units of activities in the site, which we call complex application events. Examples include (a) directed buying events in
which a user enters an E-commerce store, searches for a product,
views that product, puts it into the shopping cart and purchases
it, and then leaves, or (b) knowledge building events, in which
a user browses and searches repeatedly through categories and
product views, and usually leaves without purchasing (but may
use the knowledge built to return and acquire something later)
[140]. Complex application events are usually described by regular expressions whose alphabet consists of atomic application
events [140], or by an order structure on atomic application
events [14].

Fig. 4. Parts of the ontology of the services of the fictitious example site.

ing can provide aggregations towards more general concepts
[42].

Semantic Web Usage Mining for complex application events
involves two steps of mapping requests to events. As discussed
in Section 5.2.1 above, complex application events are usually
defined by regular expressions in atomic application events (at
some given level of abstraction in their respective hierarchies).
Therefore, in a first step, URLs are mapped to atomic application events at the required level of abstraction. In a second
step, a sequence miner can then be used to discover sequential
patterns in the transformed data. The shapes of sequential patterns sought, and the mining tool used, determine how much
prior knowledge can be used to constrain the patterns identified.
They range from largely unconstrained first-order or k-th order
Markov chains [21], via combinations of Markov processes and
content taxonomies for a data-driven modelling of content [1], to
regular expressions that specify one or a set of atomic activities
[138,11].

Examples of the use of regular expressions describing
application-relevant courses of events include search strategies [18], a segmentation of visitors into customers and noncustomers [139], and a segmentation of visitors into different
interest groups based on the customer buying cycle model from
marketing [140].

To date, few commonly agreed-upon models of Semantic
Web behavior exist. The still largely exploratory nature of the
field implies that highly interactive data preparation and mining
tools are of paramount importance: They give the best support
for domain experts working with analysts to contribute their
background knowledge in an iterative mining cycle. A central
element of interactive tools for exploration is visualization. In
the STRATDYN tool [14,13], we propose a semantic Web usage visualization that enables the analyst to detect visual patterns that can be interpreted in terms of application domain
behaviors.

With the increasing standardization of many Web applica-
tions, and the increasing confluence of mining research with
application domain research (e.g., marketing), the number of
standard courses of events is likely to grow. Examples are the
predictive schemes of E-commerce sites (see the example from
[112] mentioned in Section 5.2.1 above), and the description of
browsing strategies given by [116].

The representational power of models that capture user behaviour only in terms of a sequence of states identified by page
requests is limited. In the future, we expect more explorations of
the meaning of viewing time (e.g., [55,11]) and of the transitions
between states [14].

In the analysis and evaluation of user behavior, it must be
kept in mind that different stakeholders have different perspectives on the usage of a site, which leads them to investigate different processes (complex application events) and also makes
them consider different user actions correct or valuable. Re-
cently, frameworks have been proposed for capturing different
processes [99,154,6] and perspectives [111].

In summary, a central challenge for future research in Semantic Web Usage Mining lies in the development, provision,
and testing of ontologies of application events.

6. Closing the loop

In the previous two sections, we have analyzed how to establish Semantic Web data by data mining, how to exploit formal
semantics for Web Mining, and how to mine the Semantic Web.
In this section, we sketch one out of many possible combinations
of these approaches. The example shows how different combinations of Semantic Web and Web Mining can be arranged in a
feedback loop.

Our goal is to take a set of Web pages from a site and to
improve them for both human and machine users: (a) to generate
metadata that reflect a semantic model underlying the site, (b) to
identify patterns both in the pages text and in their usage, and,
based on these insights, to improve information architecture and
page design. To achieve these goals, we will proceed through
several steps in which we
 employ mining methods on Web resources to generate semantic structure (steps 1 and 2: Learning and filling the ontology),
 employ mining methods on the resulting semantically structured Web resources to generate further structure (steps 3 and
4),
 at the end of each step, feed these results back into the content
and design of the Web pages themselves (visible to human
users) and/or their metadata and the underlying ontology (vis-
ible to machine users).

We will only give a rough sketch in order to illustrate our
ideas, using the running example of the fictitious tourism Web
site used throughout this paper.

One may split the first step, ontology learning, into two sub-
steps. First a concept hierarchy is established using the OTK
methodology for modeling ontologies [153]. It may be supported by the formal ontology modeling method OntEx (Ontol-
ogy Exploration, [57]) which relies on the knowledge acquisition technique of Attribute Exploration [56] as developed in the
mathematical framework of Formal Concept Analysis [58]; and
guarantees that the knowledge engineer considers all relevant
combinations of concepts while establishing the subsumption
hierarchy. OntEx takes as input a set of concepts, and provides
as output a hierarchy on them. This output is then the input to
the second sub-step, together with a set of Web pages. Maedche
and Staab [104] describe how association rules are mined from
this input, which lead to the generation of relations between the
ontology concepts (see Fig. 5). The association rules are used
to discover combinations of concepts which frequently occur
together. These combinations hint at the existence of conceptual
relations. They are suggested to the analyst. As the system is
not able to automatically generate names for the relations, the
analyst is asked to provide them.

In the example shown in the figure, automatic analysis has
shown that three concepts frequently co-occur with the concept
area. Since the ontology bears the information that the concept wellness hotel is a subconcept of the concept hotel,
which in turn is a subconcept of accommodation, the inference engine can derive that only one conceptual relation needs
to be inferred based on these co-occurrences: The one between

G. Stumme et al. / Web Semantics: Science, Services and Agents on the World Wide Web 4 (2006) 124143

Fig. 5. Step 1: Mining the Web for learning ontologies.

accommodation and area. Human input is then needed to
specify a meaningful name such as hasLocation for the generalized conceptual relation.

In the second step, the ontology is filled. In this step, instances
are extracted from the Web pages, and the relations from the ontology are established between them using techniques described
in [41] (see Fig. 6), or any other technique described in Section
4.1.3. Beside the ontology, the approach needs tagged training
data as input. Given this input, the system learns to extract instances and relations from other Web pages and from hyperlinks.

In the example shown in the figure, the relation belongsTo
between the concepts golf course and hotel is instantiated by
the pair (SeaView, Wellnesshotel), i.e., by the fact derived from
the available Web pages that the golf course named SeaView
belongs to the Wellness Hotel.

After the second step, we have an ontology and a knowledge base, i.e., instances of the ontology concepts and relations
between them. These data are now input to the third step, in
which the knowledge base is mined. Depending on the purpose,
different techniques may be applied. One can for instance com-

Fig. 6. Step 2: Mining the Web for filling the ontology.

Fig. 7. Step 3: Using the ontology for mining again.

pute relational association rules, as described in detail in [45]
(see Fig. 7). Another possibility is to conceptually cluster the
instances [150].

In the example shown in Fig. 7, a combination of knowledge about instances like the Wellnesshotel and its SeaView
golf course, with other knowledge derived from the Web pages
texts, produces the rule that hotels with golf courses often have
five stars. More precisely, this holds for 89% of hotels with golf
courses, and 0.4% of all hotels in the knowledge base are fivestar hotels owning a golf course. The two values are the rules
confidence and support, standard measures for mining association rules.

The resulting semantic structure can now be used to better understand usage patterns. In our example, the clustering of user
sessions may identify a cluster of users who visit and closely
examine the pages of the Wellnesshotel, the Schlosshotel,
and the Hotel Mecklenburg. While this information, on its
own, may be sufficient to generate a dynamic recommendation
You might want to also look at the Castle Hotel at the Lake for
new users who visit the Wellnesshotel and the Hotel Meck-
lenburg, it remains unclear why this cluster of hotels may be
interesting to a sizeable group of users. This problem can be
solved by using our ontology to compute domain-level usage
profiles [42]: We find that all these hotels are characterized by
having a golf course.

This understanding of usage patterns can help us to achieve
our initial goal (b), the generation of recommendations for site
re-design. We propose to introduce a new category golf hotels
into both the sites ontology and its information architecture and
page design. Instance learning for this category is simple: All
hotels for which there is a golf course that belongs to the
hotel, and only these, become instances of the new category.
Site and page design could, for example, be modified by adding

a new value golf hotel for the search criterion hotel facilities
in the sites search/browse navigation bar. Also, when new hotels with golf courses are entered into the knowledge base, these
may be dynamically recommended to visitors of the pages of
the Wellnesshotel, the Schlosshotel, and the Hotel Meck-
lenburg.

Our initial goal (a), the generation of a semantic model and
metadata that reflect this model, has also been achieved. Among
other benefits, this allows a page of the site that describes the
Fischerhotel in the town of Zingst to be returned in answer
to a a search engine query for accommodation in Ahrenshoop,
because hotels are known to be a subclass of accommodation and the towns Ahrenshoop as well as Zingst are known
to be located on the Fischland-Dar peninsula. The former
piece of knowledge is taken from our ontology (see Fig. 5); the
latter is retrieved by the search engine from a general-purpose geography ontology available from another semantically enriched
site.

As we have seen, the results of steps 3 and 4 may lead to
further modifications of the ontology and/or knowledge base.
When new information is gained, it can be used as input to
the first steps in the next turn of the site and ontology life
cycle.

Of course, ultimate quality control, the decision to maintain
or drop concepts from the ontology, and the transformation of
ideas obtained from interpreting usage patterns into site design
changes, remain a human responsibility. Nonetheless, in the
achievement of both our initial goals, the combination of Semantic Web and Web Mining methods has saved a considerable
amount of manual effort that is necessary when both goals are
worked on in isolation: The work of creating and instantiating an
ontology of tourism facilities from a huge number of dynamic
page templates, database schemas, and raw HTML, as well as

G. Stumme et al. / Web Semantics: Science, Services and Agents on the World Wide Web 4 (2006) 124143

the work of interpreting patterns of co-occurring URLs found in
user sessions.

6.1. Semantic Web Mining and other feedback loops

The feedback loop described in this section shares a number
of features with approaches to discovering knowledge from the
Web that rely on a looser notion of semantics. A prime example
of the latter is the recently proposed KnowItAll system [49].
It is based on a bootstrapping approach similar to the one we
have described in this paper: Instances of concepts and relations
are extracted from the Web, and the reliability of these instances
is then judged by the amount of support that these assertions
receive from the Web. Due to the size of the Web, fully automated approaches like this one seem to be the premier route for
gaining instantaneous access to the knowlegde implicit in the
whole Web, in particular its fast-changing, ad hoc parts.

However, such syntax-based systems rely on the massive redundancy of the Web and can therefore gain access only to information that can be found in a large number of Web pages
(and that can be identified by the necessarily limited naturallanguage templates used for information extraction). In the
nineties, Voorhees [155] claimed that, as a matter of principle,
Artificial Intelligence (and in particular NLP) is inapt to provide significantly better IR results than such pure syntactical ap-
proaches. Since then, however, progress has been made. For in-
stance, [117] won with significant distance the TREC contest in
the open domain question answering task in 2002 by combining NLP with logical knowledge representation and reasoning.
We therefore expect a preference for Semantic-Web-Mining
solutions when the knowledge sought must cover as many as possible (or all) information items available and cannot rely on the
redundancy and the majority votes implicit in mining schemes
like KnowItAll or PageRank. Because of the extra work required at least by authors, participants in suitable application
areas should be dedicated to information quality, a dedication
induced by high intrinsic or extrinsic motivation. Application
contexts with a higher-than-usual tolerance for being observed
by usage mining will benefit from the added advantages of Semantic Web usage mining. Prominent examples of application
areas that exhibit this combination of features are science (where
exhaustive literature lists are important), voluntary communities
joined by common interests, and business (where transaction
costs have to be minimized). Currently, systems for leveraging
these application areas need for coverage and information quality range from WWW-based architecture proposals [130] via operational P2P networks [64] to long-established frameworks that
might profit tremendously from being transferred from proprietary systems to the open architecture of the (Semantic) Web (for
instance the EDI system of modeling business transactions26).
Besides coverage and quality, the form of semantics described
in this paper has two further advantages that make it suitable
for high-commitment domains. Both advantages derive from

26 EDI (Electronic Data Interchange) is a standard format for exchanging business data, internationally standardized in ISO 9735.

the differences in opaqueness between syntactical and semantic
information processing approaches. First, the information processing of statistical methods that operate exclusively on syntactic tokens remains opaque to most human users, in particular
when proprietary algorithms are employed. There is usually no
way to explain, in user-understandable terms, why an algorithm
arrived at a particular result. In contrast, an explicit conceptualization enables people and programs to explain, reason, and
argue about meaning and thus rationalize their trust, or lack of
trust, in a system. Second, their relative opaqueness forces purely
statistical-syntactical methods to rely on the individual users
sense-making abilities. Experience shows that users do make
sense of the results, but generally in an ad hoc manner that does
not encourage reflection or externalization. In contrast, Semantic
Web Mining supports the development of principled feedback
loops that consolidates the knowledge extracted by mining into
information available for the Web at large.

7. Conclusion and outlook

In this paper, we have studied the combination of the two fastdeveloping research areas Semantic Web and Web Mining. We
discussed how Semantic Web Mining can improve the results
of Web Mining by exploiting the new semantic structures in the
Web; and how the construction of the Semantic Web can make
use of Web Mining techniques. The example provided in the last
section shows the potential benefits of further research in this
integration attempt.

Further investigating this interplay will give rise to new research questions and stimulate further research both in the Semantic Web and in Web Miningtowards the ultimate goal of
Semantic Web Mining: a better Web for all of its users, a
better usable Web. One important focus is to enable search
engines and other programs to better understand the content of
Web pages and sites. This is reflected in the wealth of research
efforts that model pages in terms of an ontology of the content,
the objects described in these pages.

We expect that, in the future, Web Mining methods will increasingly treat content, structure, and usage in an integrated
fashion in iterated cycles of extracting and utilizing semantics,
to be able to understand and (re)shape the Web. Among those
iterated cycles, we expect to see a productive complementarity
between those relying on semantics in the sense of the Semantic
Web, and those that rely on a looser notion of semantics.
