Web Semantics: Science, Services and Agents

on the World Wide Web 4 (2006) 243262

Using Bayesian decision for ontology mapping


, Juanzi Li, Bangyong Liang, Xiaotong Huang, Yi Li, Kehong Wang
Jie Tang
Department of Computer Science and Technology, 10-201, East Main Building, Tsinghua University, Beijing 100084, PR China

Received 22 June 2004; accepted 2 June 2006

Abstract

Ontology mapping is the key point to reach interoperability over ontologies. In semantic web environment, ontologies are usually distributed
and heterogeneous and thus it is necessary to find the mapping between them before processing across them. Many efforts have been conducted
to automate the discovery of ontology mapping. However, some problems are still evident. In this paper, ontology mapping is formalized as a
problem of decision making. In this way, discovery of optimal mapping is cast as finding the decision with minimal risk. An approach called Risk
Minimization based Ontology Mapping (RiMOM) is proposed, which automates the process of discoveries on 1:1, n:1, 1:null and null:1 mappings.
Based on the techniques of normalization and NLP, the problem of instance heterogeneity in ontology mapping is resolved to a certain extent.
To deal with the problem of name conflict in mapping process, we use thesaurus and statistical technique. Experimental results indicate that the
proposed method can significantly outperform the baseline methods, and also obtains improvement over the existing methods.
 2006 Elsevier B.V. All rights reserved.

Keywords: Ontology mapping; Semantic web; Bayesian decision; Ontology interoperability

1. Introduction

Ontologies, as the means for conceptualizing domain knowl-
edge, have become the backbone to enable the fulfillment of the
semantic web vision [3,21]. Many ontologies have been defined
to make data sharable, for example, Cyc Ontology [17], Enterprise Ontology [38], Bibliographic-data Ontology [14], Biological and Chemical Ontology (BAO) [25], and Bio-Ontologies
[43]. See [45] for more ontologies.

Unfortunately, ontologies themselves are distributed and het-
erogeneous. Ontologies have two kinds of heterogeneities: metadata heterogeneity and instance heterogeneity [4,16]. Specifi-
cally, entities (entity represents concept, relation, or instance)
with the same meaning in different ontologies may have different label names and the same label name may be used for
entities with different intentional meanings; instances in different ontologies may have different representations; and different
ontologies may have different taxonomy structures.


Corresponding author. Tel.: +86 10 627 81461; fax: +86 10 627 89831.
E-mail addresses: j-tang02@mails.tsinghua.edu.cn (J. Tang),

ljz@keg.cs.tsinghua.edu.cn (J. Li), liangby97@mails.tsinghua.edu.cn
(B. Liang), x.huang@keg.cs.tsinghua.edu.cn (X. Huang),
yi-li@mails.tsinghua.edu.cn (Y. Li), wkh@keg.cs.tsinghua.edu.cn (K. Wang).

1570-8268/$  see front matter  2006 Elsevier B.V. All rights reserved.
doi:10.1016/j.websem.2006.06.001

In order to achieve semantic interoperability over ontolo-
gies, it is necessary to discover ontology mapping as the
first step [1]. This is exactly the problem addressed in this
paper.

Many efforts have been conducted to deal with the problem.
However, the following problems still exist. First, the type of
cardinalities that can be processed is limited. Most of the work
was focusing on only 1:1 mapping [9,10,15,18,22,23,26,29]
despite of the fact that approximately 2250% of mappings
are beyond this cardinality by statistics on real-world examples
[11,32]. Secondly, ontology mapping has been done mainly on
metadata heterogeneity, not on instance heterogeneity. In natural language processing, text normalization has been studied
[34]. But before adapting the method to deal with the problem
of instance heterogeneity, many efforts are still required. The
existing methodologies proposed in the previous work can be
used in ontology mapping. However, they are not sufficient for
solving all the problems.

At present, questions arise for ontology mapping: (1) how
to formalize the problem so that it can describe different kinds
of mapping cardinalities and heterogeneities, (2) how to solve
the problem in a principled approach, and (3) how to make an
implementation.

In this paper, we tried to solve the above problems and have

done the following work:

J. Tang et al. / Web Semantics: Science, Services and Agents on the World Wide Web 4 (2006) 243262

(1) We formalize ontology mapping as that of decision making.
Specifically, discovery of optimal mapping is cast as finding
the decision with minimal risk.

(2) We propose an approach called Risk Minimization based
Ontology Mapping (RiMOM) to conduct ontology mapping
by running several passes of processing: first multi-strategy
execution in which each decision find the mapping indepen-
dently; and then strategy combination in which the mappings output by the independent decisions are combined;
thirdly mapping discovery in which some mechanisms are
used to discover the mapping based on the combined results.
Mapping process can take place iteratively until no new
mappings are discovered. In each iteration, user interaction
can be used to refine the obtained mappings.

(3) We make an implementation for the proposed approach. For
each available clue in ontologies, we propose an independent decision for finding the mappings. We also make use of
the representation normalization and NLP techniques in the
mapping process. We combine the results of the independent
decisions by a composite method.

We tried to collect heterogeneous ontologies from different
sources. In total, 28 ontologies from five different sources were
gathered. Five data sets were created with the 28 ontologies.
Our experimental results indicate that the proposed method performs significantly better than the baseline methods for mapping
discovery. We also present comparisons with existing methods.
Experimental results indicate improvements over them.

The rest of the paper is organized as follows. Section 2
describes the terminologies used throughout the paper. In Section 3, we formalize the problem of ontology mapping and
describe our approach to the problem. Section 4 explains one
possible implementation. The evaluation and experiments are
presented in Section 5. Finally, before concluding the paper with
a discussion, we introduce related work.

2. Terminology

This section introduces the basic definitions in the mapping
process and familiarizes the reader with the notations and terminologies used throughout the paper.

2.1. Ontology

The underlying data models in our process are ontologies. To
facilitate further description, we briefly summarize the major
primitives and introduce some shorthand notations. The main
components of an ontology are concepts, relations, instances
and axioms [7,37].

A concept represents a set or class of entities or things within

a domain. The concepts can be organized into a hierarchy.

Relations describe the interactions between concepts or properties of a concept. Relations fall into two broad types: Taxonomies that organize concepts into sub- or super-concept hier-
archy, and Associative relationships that relate concepts beyond
the hierarchy. The relations, like concepts, can also be organized
into a hierarchy structure. Relations also have properties that can

Table 1
Mappings from O1 to O2

Ontology O1

Object
Washington course
Asian Studies
College of Arts and Sciences
Linguistics

Ontology O2

Thing
Cornell course
Asian Languages and Literature
College of Arts and Sciences
Linguistics LING

describe the characteristics of the properties. For example, the
cardinality of the relationship, and whether the relationship is
transitive.

Instances are the things represented by a concept. Strictly
speaking, an ontology should not contain any instances, because
it is supposed to be a conceptualization of the domain. The
combination of an ontology with associated instances is what is
known as a knowledge base. However, deciding whether something is a concept or an instance is difficult, and often depends
on the application. For example, Course is a concept and Linguistics is an instance of that concept. It could be argued that
Linguistics is a concept representing different instances of
Linguistics courses, such as French Linguistics Course and
Spanish Linguistics Course. This is a well known and open
question in knowledge management research.

Finally, axioms are used to constrain values for classes or
instances. In this sense, the properties of relations are kinds of
axioms. Axioms also, however, include more general rules, such
as a course has at least one teacher.
For facilitating the description, we denote a concept by c
and a set of concepts by C (c  C), respectively. We use r to
denote relation and use R to denote a set of relations (r  R). We
also respectively denote instance and a set of instances by i and
I (i  I). Axioms are denoted by A0.

2.2. Heterogeneity of ontology

In order to reach interoperability over heterogeneous ontolo-
gies, two problems must be dealt with: metadata heterogeneity
and instance heterogeneity [4,16]. Metadata heterogeneity concerns the intended meaning of described information. There are
two kinds of conflicts in metadata heterogeneity: structure conflict and name conflict. Structure conflict means that ontologies
defined for the same domain may have different taxonomies.
Name conflict means that concepts with the same intended
meaning may use different names and the same name may be
used to define different concepts.

Fig. 1 shows an example of metadata heterogeneity. Two
ontologies O1 and O2 respectively represent college courses at
Washington University and Cornell University.1 The dashed line
in the figure represents a reasonable mapping between them.
Table 1 lists the mappings.

In the example, the concept Asian Studies in Ontology
O1 has the same meaning as the concept Asian Lanugages

1 http://anhai.cs.uiuc.edu/archive/summary.type.html.

Fig. 1. Example of two heterogeneous ontologies and their mappings.

and Literature in ontology O2. But they have different names.
On the other hand, the concept Linguistics is defined in both
O1 and O2. However they represent different meanings. In
ontology O1, Linguistics denotes a linguistics course which
focuses on the basic analytic methods of several subfields of
linguistics, such as phonetics, phonology, morphology, syntax,
semantics, and psycholinguistics. While in ontology O2, Linguistics is referred to as a taxonomy of linguistic courses,
including four sub-classes: French linguistic course, Romance
linguistic course, Spanish linguistic course, and Linguistics
course.

Instance heterogeneity concerns the different representations
of instances. Information described by the same ontology can be
represented in different ways. This is also called representation
conflict. For example, a date can be represented as 2004/2/27
and also can be represented as February, 27, 2004; person
name can be represented as Jackson Michael or Michael,
Jackson, etc. Instance heterogeneity makes it necessary to do
normalization before ontology interoperation [40].

So far, many efforts have been placed on the problem of metadata heterogeneity and few efforts are concerned with instance
heterogeneity, to the best of our knowledge. Moreover, most of
the existing work was focusing on 1:1 mapping.

2.3. Ontology mapping

Ontology mapping takes two ontologies as input and creates a semantic correspondence between the entities2 in the two
ontologies [32].

In this paper, we define ontology mapping as a directional
one. Given a mapping from ontology O1 to O2, we call ontology

2 In this paper, to facilitate the description, we use entities to denote concepts,

properties and relations.

O1 as source ontology and O2 as target ontology. We call the
process of finding the mapping from O1 to O2 as (ontology)
mapping discovery or mapping prediction.

Formally, ontology mapping function Map can be written in

the following way:
Map({ei1}, O1, O2) = {ei2}
with ei1  O1,
ei2  O2 :
{ei1} Map{ei2}. {ei1} or {ei2}
denotes a collection of entities, and ei1  CO1  RO1. The target
entity collection can contain one entity, multiple entities or null.
Here null means that there is no mapping for {ei1} in O1.
To facilitate the description, we usually leave out O1 and O2
and write the function as Map({ei1}) ={ei2}. Moreover, we use
the notation Map(O1,O2) to denote all entity mappings from O1
to O2.

There are six kinds of mapping cardinalities: 1:1, 1:n, n:1,
1:null, null:1, and n:m. Table 2 shows examples of the cardinal-
ities.

Among these kinds of cardinalities, existing mapping methods was mainly focusing on 1:1 mapping. This paper has investigated the problem of mappings with 1:1, n:1, 1:null, and null:1.
The kind of n:m mapping is more complicated and is not the
focus of this paper. For 1:n mapping, we consider it in a bidirectional process of mapping discovery, that is, we find 1:n mapping
by making use of both the mapping from O1 to O2 and the mapping from O2 to O1. In this paper, we confine ourselves to the
single directional mapping and focus on the 1:1, n:1, 1:null and
null:1 mappings.
Once a mapping Map({ei1},{ei2}) between two ontologies
O1 to O2 is discovered, we say that entities {ei1} is mapped
onto entities {ei2}. For each pair of entity sets ({ei1}, {ei2}),
we call it a candidate mapping. We make the assumption that an
entity in the source ontology can only participant into at most
one mapping.

J. Tang et al. / Web Semantics: Science, Services and Agents on the World Wide Web 4 (2006) 243262

Table 2
Mapping cardinality examples

Cardinality

O1

Faculty
Name
Cost, Tax ratio

1:1
1:n
n:1
1:null
null:1
n:m

O2

Mapping expression

Academic staff
First name, last name
Price

O1. Faculty = O2 Academic staff
O1. Name = O2. First name + O2. Last name
O1. Cost*(1 + O1. Tax ratio) = O2. Price

Book title, Booka No., Publisher No., Publisher name

Book, publisher

O1. Book title + O1 Booka No. + O1. Publisher
No. + O1. Publisher name = O2. Book + O2. Publisher

3. Ontology mapping modeling

In this section, we first briefly introduce the Bayesian decision
theory and its use in RiMOM, and then describe the mapping
process, finally illustrate the sub-decisions that are exploited to
determine the mappings.

3.1. Bayesian decision theory

Bayesian decision theory provides a solid theoretical foundation for thinking about problems of action and inference under
uncertainty [2]. In Bayesian decision theory, the observations
are a set of samples X, in which each sample is denoted as x.
Let y  Y be a class. Each sample x can be classified into one
class. Let p(y|x) denote the conditional probability of the sample x belonging to class y. Let A ={ai, a2, . . ., an} be a set of
possible decisions (actions). Actions are defined according to
the specific application. For each action ai, Bayesian decision
theory associate a loss function L(ai,y) to indicate the loss of
classifying the sample x to class y.

Given Y and A, the Bayesian risk of each sample x is defined

by:
R(ai|x) =

L(ai, y)p(y|x) dy

The solution to the Bayesian problem is to find an action ai

which minimizes the risk.
 = arga min R(a|x)
Classification is a special case of Bayesian decision problem
where the set of action A and the set of classes Y coincide with
each other. An action then means to classify sample x to class y.
For example, in Na ve Bayes classification, to find the action ai
with minimal risk means to classify the sample x to class y with
the highest probability (inversely minimal loss).

3.2. RiMOM

In terms of Bayesian decision theory, we formalize the ontology mapping problem as that of decision making. This section
presents an ontology mapping model, called RiMOM.
In our case, our observations are all entities in the two ontologies O1 and O2. Entities {ei1} in O1 are viewed as samples and
entities {ei2} in O2 are viewed as classes. Each entity ei1 can
be classified to one class ei2. This also means that entity ei1

is mapped onto entity ei2. We use p(ei2|ei1) to denote the conditional probability of the entity ei1 being mapped onto entity
ei2. We then define actions as all possible mappings (i.e. all candidate mappings). In this way, finding the optimal mapping is
formalized as finding the action with minimal risk.

We denote the loss function as L(ai, ey, O1, O2, ex). For entity

ex in O1, the Bayesian risk is given by
R(ai|ex, O1, O2)

L(a, ey, O1, O2, ex)p(ey|ex, O1, O2)d(ey), ex  O1

ey

We include O1 and O2 in the conditional probability p(ei2|ei1,
O1, O2), which means that not only the information of ex and ey
themselves but also the global information in O1 and O2 will be
considered for calculating the mapping risk.

We employ a commonly used loss function, log loss function,

which is defined as:
L(ai, ey, O1, O2, ex) = log(p(ey|ex, O1, O2))

Finally, based on the Bayesian decision theory, the sufficient
and necessary condition for minimal Bayesian risk is to find
minimal risk for each sample. Thus, the risk of mapping from
O1 to O2 is defined as:
R =

R(ai|ex, O1, O2)d(ex),

ex  O1

(1)

ex

3.3. Process

Eq. (1) is a general formula to view ontology mapping as
a decision problem. There are many methods to implement it.
In mapping discovery, different information can be exploited,
e.g. instance, entity name, entity description, taxonomy struc-
ture, and constraint. We designed a sub-decision for each of the
available clues. Every sub-decision can be used independently
to discover the mappings from O1 to O2. The discovered mappings by these sub-decisions are then combined into the final
mappings. In this paper, we also call the implementation of each
decision as strategy.

Fig. 2 illustrates the mapping process in RiMOM with two
input ontologies, one of which is going to be mapped onto the
other. It consists of five phases:

1. User interaction (optional). RiMOM supports an optional
user interaction to capture information provided by the user.

Fig. 2. Mapping process in RiMOM.

The information can be used to rectify the existing mappings
or create new mappings. The targeted user interaction can be
used to improve the mapping accuracy.

2. Multi-strategy execution. The crucial process in mapping
iteration is the execution of the multiple independent mapping strategies. Every strategy determines a predicting value
between 0 and 1 for each possible candidate mapping. The
output of the mapping execution phase with k strategies, m
entities in O1 and n entities in O2 is a k*m*n cube of predicting values, which is stored for later strategy combination.

3. Strategy combination. In general, there may be several predicting values for a pair of entities, e.g. one is the prediction
by their name and another one is by their instances. This
phase is to derive the combined mapping results from the
individual decision results stored in the predicting cube. For
each candidate mapping, the strategy-specific predicting values are aggregated into a combined predicting value.

4. Mapping discovery. This phase uses the individual or combined predicting values to derive mappings between entities
from O1 to O2. In existing literature, mechanisms include
using thresholds or maximum values for mappings prediction [26], performing relaxation labeling [11], or combining
structural criteria with similarity criteria.

5. Iteration. Mapping process taking place in one or more iterations depends on whether an automatic or interactive determination of mapping is to be performed. In interactive mode,
the user can interact with RiMOM in each iteration to specify the mapping strategies (selection of mapping strategies),
to correct mistake mappings, to create new mappings, or to
accept/reject mappings from the previous iteration. In automatic mode, the strategies perform iteration over the whole
process. Outputs of the iteration can be used in the next iter-
ation. Each iteration contains two parts: one is to discover
concept mappings and the other is to discover relation map-
pings. Iteration stops until no new mappings are discovered.

Eventually, the output is a mapping table. The table includes
multiple entries, each of which corresponds to a mapping. An
entry in the mapping table contains two entity sets. One set is
the source entity set in O1 and the other is the target entity set
in O2. Table 1 shows an example of the mapping table.

3.4. Multiple decisions in RiMOM

In this section, we first present the sub-decisions for each
available clue. Then we combine the results from these sub-
decisions.

3.4.1. Name based decision

The most intuitive method may be that of exploiting entity
name to discover the mapping. Several approaches have been
proposed to conduct the mapping discovery by making use of
the entity name. For example, Madhavan et al. use VSM (Vector
Similarity Model) by casting the problem as that of information
retrieval [19]; Bouquest et al. propose to employ Edit Distance
to compute the similarity of entity names [4]; and Doan et al.
utilize machine learning methods to make prediction [11]. How-
ever, all the approaches may have some troubles. Specifically,
information retrieval methods usually result in unsatisfactory
results. Edit distance defines the strings similarity by the minimum number of insertions, deletions, and substitutions required
to transform one string into the other. It ignores that two entity
names with similar meaning might be absolutely differently
spelled. Moreover, classifier usually is effective on long text
content, but not effective on short text content. Entity name is
often represented by short text.

We propose to conduct name based decision by combining
thesaurus method with statistical technique. Formally, we can
define the similarity between words w1 and w2 as:
(simd(w1, w2) + sims(w1, w2))

where simd(w1,w2) denotes the similarity between w1 and w2
according to thesaurus. As the thesaurus, we use Wordnet, one
of the most popular thesauruses. sims(w1,w2) is the statistical
similarity which will be described later.

Wordnet is a semantic network of word senses, in which each
node is a synset. A synset contains words with same sense and
a word can occur in different synsets indicating that the word
has multiple senses. Lin et al. define the similarity between two
senses in Wordnet [30] as:
simd(s1, s2) =

2  log p(s)

log p(s1) + log p(s2)

where p(s) = count(s)/total, is the probability of a randomly
selected word occurring in the synset s or any sub synsets of
it. Total is the number of word in Wordnet and count(s) is the
number of word in s and sub synsets of it. The synset s is the
common hypernym of synsets s1 and s2 in WordNet.
Let s(w1) ={s1i|i = 1,2, . . ., m} and s(w2) ={s2i|i = l,2, . . .,
n} denotes the senses of w1 and w2, respectively. We define
the similarity of two words by the maximum similarity between
their senses. It is written as:
simd(w1, w2) = max(simd(s1i, s2j))

s1i  s(w1), s2j  s(w2)

J. Tang et al. / Web Semantics: Science, Services and Agents on the World Wide Web 4 (2006) 243262

For calculating the statistical similarity we use a statistical
similarity dictionary. Lin constructs a thesaurus, in which similarities between words are calculated based on their distribution
in the documents [30]. We obtain the value of sims(w1,w2) by
directly looking up the dictionary.

It is necessary to do preprocessing before calculating the
name similarity. The preprocessing includes: text tokenization
for deriving a bag of tokens, e.g. Earth-and-Atmospheric-
Sciences{earth, and, atmospheric, sciences}.

The name based strategy then computes similarity matrix for
the two word sets. Each value in the matrix denotes the similarity
of a pairwise of words. Specifically, for two entity names, name1
and name2, they are pre-processed into two token sets {w1i} and
{w2j}. Then for each w1i, we select the highest similarity as the
similarity between w1i and name2, i.e. sim(w1i, name2). Finally,
the similarity of name1 and name2 is defined as
sim(name1, name2) =

sim(w1i, name2)

i=1...n

where n is the number of word in name1.

By comparison of existing methods, this method works well
not only on similar names, but also on different names with
semantic relationship.

3.4.2. Instance based decision

This strategy makes use of text classification techniques
to find entity mappings. The inputs are all entities and their
instances in the two ontologies.

An entity can have instances. An instance typically has a
name and a set of properties together with their values. We treat
all of them as the textual content of the instance. We also take
the documents that related to the instance as a kind of source
to its textual content. For example, in the Course ontology of
AnHais data1, we can take the web pages that related to the
instance as its text content. In this way, we create a document
for each instance and a document set for each enetity.

This strategy exploits the word frequencies in the textual
content of each instance to discover mappings. It formulates
ontology mapping as a classification problem. Given two ontologies O1 and O2 with a set of entities{ei1} and{ei2}, respectively,
and each entity ei1 with a set of instances Ii1 ={ii1k}, the decision takes {ei2} as classes, instances in O2 as training samples
and instances in O1 as test samples, so that the mapping can be
automatically discovered by predicting the class of the test sam-
ples. The textual content of each instance is processed into a bag
of words, which are generated by word tokenizing, stop-word
removing and word stemming. Let ii1k ={w} be the content of
an input instance where w is a word.

We employ Na ve Bayesian (NB) classifier. NB tries to generate a model from training samples that can be applied to classify
test samples. Given test instances Ii1, NB predicts its class by
arg maxei2 p(ei2|Ii1). The posterior probability p(ei2|Ii1) is calculated by:
p(ei2|Ii1) = p(Ii1|ei2)p(ei2)

p(Ii1)

In the equation, p(Ii1) can be ignored because it is just a con-
stant. p(ei2) is estimated as the probability of training instances
that belong to ei2. To compute p(Ii1|ei2), we make the assumption that words appear in instances Ii1 independently of each
p(Ii1|ei2) = 
other for the given ei2. Thus, p(Ii1|ei2) can be computed by
p(w|ei1). Finally, we are able to rewrite
w Ii1
p(ei2|Ii1) as:

p(ei2|Ii1) =
(2)
where p(w|ei2) is estimated by n(w,ei2)/n(ei2). n(ei2) is the total
number of words in the instances of ei2, and n(w,ei2) is the
number of times that word w appears in the instances of ei2.
For each possible candidate mapping of ei1, the strategy computes the probability p(ei2|Ii1), and predicts the mapping by
arg maxei2 p(ei2|Ii1).

p(w|ei2)p(ei2)

w Ii1

Instance based decision works well on long text contents. It

seems less effective on short contents.

3.4.3. Description based decision

Entity usually has comment or description (for short, we use
description hereafter) and description is often expressed by natural language and is also one kind of valuable information for
ontology mapping. Typically, it reflects more semantic of the
entity than entity name itself.

We use text classification method to find mapping with the
information of entity description. Specifically, we use word frequencies in entity descriptions of the target ontology to construct
a Bayesian classifier. Then we exploit words in entity descriptions of the source ontology for prediction. The principle of this
decision is similar to that of instance based decision except that
in instance based decision the words are from instance textual
content while in description based decision the words are from
the entity description.

3.4.4. Taxonomy context based decision

Taxonomy structure describes the taxonomy context for the
entity. The strategy is derived from the intuition that entities
occurring in the similar contexts tend to be matchable, e.g. two
concepts may match if their sub-classes match. A concepts taxonomy context includes its super-class, sub-classes, properties
and relations. A relations taxonomy context includes its subject,
object, super-relation, sub-relations, and constraints. Thus, the
taxonomy-context similarity of two entities can be defined by
aggregating similarities of the respective entities in their con-
texts. The similarities are obtained from the other strategies,
such as name based decision and instance based decision. In
our current implementation we only consider the entities in the
immediate context.3

3.4.5. Constraints based decision

Constraints are often used to restrict concepts and properties

in ontology. They are also useful for mapping discovery.

3 However, indirectly related entities will be considered in the future work.

We utilized the constraints by defining heuristic rules for

refining the learned mappings. Examples of such rules are:

- datatypeproperty with range Date can only be mapped to

the datatypeproperty with range Date->confidence: 1.0.

- datatypeproperty with range float may be mapped to one
with range string->confidence: 0.6. Rules are also defined
similarly for nonNegativeInteger, boolean, etc.

- concepts that have the same properties but the properties have
different cardinalities may not be mapped to each other->
confidence: 0.3. Here, for the same properties, we mean two
properties that are proposed as a mapping by the other deci-
sions. Rules are also defined similarly for maxCardinality
and minCardinality.

- concepts that have the same number of properties tends to be

mapped to each other->confidence: 0.3.

Each constraint is assigned with a confidence (e.g. 1.0 and
0.6) to extend the traditional Boolean constraint (i.e. yes or no).
The confidences are specified manually. By far, we totally define
12 rules according to the constraints in ontology language and
the domain knowledge.

3.4.6. Using NLP to improve the decision

Information processing on plain text usually meets the problem of data sparseness. Data sparseness makes the classifier
over-fitting the training examples, thus affects its effectiveness
on unseen cases. In the processing of mapping discovery, we also
observed the problem: lack of common instances. For example,
instances of concept telephone number in two ontologies can
have few common ones. The problem depresses the performance
of instance based decision and description based decision. We
propose to deal with the problem by making use of NLP tech-
nique.

Existing NLP techniques can be used to associate additional

linguistic knowledge to each word.

The NLP techniques include: morphological analyzer, POS
tagging, name entity recognizer, user-defined dictionary, etc.
We employ Part of Speech (POS) and Name entity recognition results as the additional knowledge. An example is
shown in Table 3 (we conducted NLP analysis by using GATE
[3]).

Table 3
Instances of concept Address with NLP knowledge

Instance with NLP knowledge

Index

Word

Knowledge
Engineering
Group

Tsinghua
University

China
100084

Noun
Noun
Noun

Noun
Noun

Noun
Number

Name entity

Organization

Fig. 3. The sigmoid function.

(a1
+ a3

w Ii1

p(POS|ei2)

With the additional knowledge, Bayesian classifier can learn
the model not only by the bag of words but also by their POSs
and name entities. Then, Eq. (2) becomes:

p(w|ei2) + a2
ne Ii1

POS Ii1
p(ne|ei2))  p(ei2)
a1 + a2 + a3

p(ei2|Ii1) 
where p(POS|ei2) is the conditional probability of POS given
entity ei2; p(ne|ei2) is the conditional probability of name entity
ne given entity ei2. Parameters a1, a2, and a3 are weights
used to tune the preferences to word, POS and name entity,
respectively.

3.4.7. Combination of multi-decision

Outputs of the strategies need to be combined. There are two
most popular approaches for combination: the hybrid or composite approach [9,11]. Hybrid method is usually used when
multiple algorithms are integrated into a single algorithm. Composite method is used when multiple algorithms results need to
combination. We employed the composite method and combine
the strategies by:
Map(ei1, ei2) =

k=1...nwk(Mapk(ei1, ei2))

wk

where wk is the weight for an individual strategy, and  is
a sigmoid function. Sigmoid function makes the combination
emphasize high individual predicting values and de-emphasize
low individual predicting values. Function  is defined as:
(x) =

5(x)

1 + e

where x is a individual predicting value. We tentatively set 
as 0.5. The general shape of the sigmoid function is shown in
Fig. 3.

University

4. Implementation

Country
Zipcode

In this section, we consider one implementation of RiMOM.
We focus on two phases in ontology mapping: Preprocessing

J. Tang et al. / Web Semantics: Science, Services and Agents on the World Wide Web 4 (2006) 243262

4.2. Discovery

Discovery consists of four stages: entity mapping, mapping
combination, mapping discovery, and mapping refinement. First,
concept mapping and relation mapping are performed independently by the decisions as described in Section 3. Secondly,
we combine results from the multiple decisions and obtain a
composite result. After that, we employ several strategies to
determine the 1:1, n: 1, 1:null, and null: 1 mappings. Finally, we
refine the generated mappings.

Multiple decisions and the combination algorithm are presented in Section 3. In this section, we mainly discuss the
discovery process and the refinement method.

4.2.1. Mapping discovery process

In mapping discovery process, RiMOM computes the
Bayesian risk for each possible mapping, and then searches
the whole space to find the mapping with minimal risk. The
algorithm of mapping discovery is shown in Fig. 4. prepro-
cess() is the preprocessing procedure as described in Section 4.1.
NamePrediction(), InstancebasedPrediction(), DescriptionPre-
diction(), TaxonomyContextPrediction() and ConstraintDeci-
sion() are five sub-decisions. DecisionCombination() is the function to combine the results of the multiple decisions. For each
concept, PreConceptDecision() first outputs top ranked three

and Discovery. We will not focus on mapping representation.
In this paper, it is expressed by XML (Section 4.2 will give
an example). See [20] and [33] for details about mapping
representation.

4.1. Preprocessing

Before mapping process, textual contents of instances, entity
names and entity descriptions need to be preprocessed. The
preprocessing includes tokenization, stop-word removing, word
stemming, POS tagging, name entity recognition, and normal-
ization. In our implementation, we use a general toolkit (viz.
GATE [6]) to perform the preprocessing. GATE integrates many
tools for NLP, including morphological analyzer, POS tagger,
user-defined dictionary, and name entity recognizer (which can
recognize person name, dates, number, organization names, etc).
We process the textual content of each instance and store the
result for later processing.

The same instance may have various expressions (also called
instance expression conflict). In natural language processing,
Sproat et al. have investigated normalization of non-standard
words in text processing [34]. They define a taxonomy of nonstandard words and apply n-gram language models, decision
trees, and weighted finite-state transducers to the task of nor-
malization. But in ontology, the instance expression may not be
in natural language, and thus the n-gram based method may not
work well.

We formalize the problem as that of instance normalization.
We conduct the normalization as follows. We first use GATE
to identify the name entities as candidates for normalization
(including: time, date, year, percentage, money, person name,
etc). We then defined hard-rules for normalizing time, date,
year, percentage, money, and person name. For example, for
date we transform the different formats into a unique form:
year-month-day, e.g. 2004-3-1 and March 1, 2004 are both
transformed into the format 2004 March 1; for person name,
we normalize its format into firstname lastname, e.g. Jackson Michael and Michael, Jackson are both normalized into
Jackson Michael.4 We also merge the person names like J.
Michael and Jackson Michael. Rules for other type of name
entities are also defined in this way. We omit the details due to
space limitation.

It seems reasonable to conduct

the normalization for
instances in this way. The rules defined work well in most of
the cases. By a preliminary analysis on the 28 ontologies, we
have found that more than 85.5% of the instance expression
conflicts come from time, date, year, percentage, money, and
person name.

The other task in this phase is to normalize the entity name for
facilitating the name based decision. For example, given a concepts name company information, we need to tokenize it into
{company, information}. For relation name hasEmployee, we
tokenize it into {has, Employee}.

4 GATE can recognize the first name and last name by name entity recognizer

and by a user-defined dictionary.

Fig. 4. The flow in mapping discovery.

mappings. And then for all concepts, ConceptMappingDeci-
sion() determines the final concept mappings by using the outputs of PreConceptDecision(). PruneConceptMapping() uses
domain knowledge to prune the wrong mappings and to discover
1:null mappings. We employ the same procedure to find mappings of properties. After that, we conduct a mapping refinement
procedure. In this procedure, we refine the concept mapping
and property mapping by making use of their results for each
other. We should also take into consideration of other kinds of
mappings. For example, since it is not necessary disjoint for
concepts, properties, and instances, there should also include
mappings of concept to instance, instance to concept, property to concept, etc. In this paper, we confine ourselves to
the mapping of concept to concept and property to property.
Because we have observed few other mapping types available in
our data.

1:1 mapping is the simplest and also the most common map-
ping. The task of finding 1:1 mapping is accomplished by selecting the corresponding entity with minimal risk from O2 for each
entity in O1. The selection is determined by the combination of
decisions described in Section 3.

4.2.1.1. n:1 mapping. n:1 may exist when multiple entities in
O1 are mapped to one entity in O2. The discovery of n:1 mapping
consists of two steps: mapping entities discovery and mapping expression discovery. In mapping entities discovery, we
are aimed at finding whether there are multiple source entities
mapped onto one target entity. In mapping expression discov-
ery, we try to search for a function for combining the source
entities so that the source entities can be best matched by
the target entity. For example, the source entities are firstname and lastname and the target entity name is person name,
then the expression function can be concatenation of the two
source entities: concat(firstname, lastname) (also written as firstname + lastname).

After predicting mapping for each entity of the source ontol-
ogy, RiMOM search all the mappings to see whether there exist
multiple source entities mapped onto the same target entity.
If exist, RiMOM triggers a combination process, which automatically searches for the expression function. Now, we use an
example to illustrate the process.

For example, when three concepts Address, Zipcode and telephone are all mapped onto one concept contract infomation,
RiMOM triggers a special function to search for the possible
mapping expression. By mapping expression, we mean how the
concepts from the source ontology should be organized so that
they can be exactly mapped onto the target concept. Formal
description of the mapping expression is

F (f (eAddress), f (eZipcode), f (etelephone))
= f (econtract information)

where f(e) is a function of e, such as left(e, length), lowercase(e).
Function F is a composition function of the input parameters.
Currently, for both function F and f, we only take the type of
string into consideration. For function f, we define five functions
including: left, right, mid, lowercase, uppercase, and capitalize.

Fig. 5. An example of output by n:l mapping.

For function F, we define the function as string concatenation
by different orders of the input parameters.

Fig. 5 shows an output of n:1 mapping by using only instance
based decision. This is a concept mapping with the source
concepts address, zipcode and telephone and the target concept contract information. Each concept is assigned
with an id (e.g. #addr), which is used in the expression
upcase(#addr) + #zip + #tele = #ci. The expression means that
the concatenation of uppercase form of address and original form of zipcode and telephone is mapped onto contact information. Each candidate mapping is labeled with a
score. The highest scored one is proposed as the mapping and
the other two top scored are followed as candidates.

4.2.1.2. 1:null mapping. 1:null is a special case. We perform
1:null mapping discovery by using heuristic rules. Table 4 shows
some examples of the rules.

4.2.1.3. null: 1 mapping. The discovery of null: 1 mapping is
straightforward. When there is no entities mapped to ei2, we say
that for entity ei2, there is a null: 1 mapping.

4.2.2. Mapping refinement

In mapping refinement, we focus on refining the generated

mapping by utilizing rules.

In this step, we aim to remove the top ranked but unreasonable mappings. We also tried to highlight the mappings that
are low ranked but seem reasonable mappings. We use following four example cases to explain how we refine the generated
mappings.

Case 1: Concept ei1 has a mapping to concept ei2, and both

its super-concept e
i1 have mappings to the
super-concept e
i2 of ei2. The three mappings are contradictive.
There might exist a mistake mapping. We define the rule in this
case that the mapping es

i1 and sub-concept es

i1 to e

i2 is a mistake mapping.

J. Tang et al. / Web Semantics: Science, Services and Agents on the World Wide Web 4 (2006) 243262

Table 4
Examples of rules for 1:null mappings

Categorization

Examples

Threshold

Taxonomy

For ei1, if none of its candidate mappings has the predicting value exceeding the threshold , then we infer that entity ei1 has a 1:null
mapping. In our experiments,  is assigned as 0.2.
For ei1, if all sub-decisions propose different mappings, i.e. the top ranked mappings of them are different, and none of them has the
predicting value exceeding threshold  (we tentatively set it as 0.3), then we infer that entity ei1 has a 1:null mapping.

For ei1, if both its super-entity and sub-entities can be mapped to the corresponding entities in O2, and in O2 there is no entity between the
target super-entity and target sub-entities, then we can infer that ei1 has a 1:null mapping. See Fig. 6(a) for an example, for the concept
car, its super-concept transport and sub-concepts cab and police car have mapping concepts in O2. But in O2, there is no concept
between the concept vehicle and the sub concepts taxi and prowl car. Then we say that concept car has a 1:null mapping.
If entity ei1 has a corresponding entity ei2 in O2, and the number of sub-concepts of ei1 is greater than that of ei2, then we infer that there
might be 1:null mappings for the sub-concepts of ei1. See Fig. 6(b) for an example, concept Asian languages has a mapping to Asian
studies, and Asian languages has four sub-concepts but Asian studies only has three sub-concepts, then there might be a 1:null
mapping for one sub-concept of Asian languages. We use the combined predicting value as the metric to judge which entity has the 1:null
mapping. The lower predicting value the entity has, the higher probability it has a 1:null mapping.

Case 2: For concept pair ei1 and ei2, its super concept e

i1 and
sub concept es
i1, respectively, has a mapping to the super concept

i2 of concept ei2. But ei1 does not have a
i2 and sub concept e

top ranked mapping to ei2. It has a mapping to concept ek2 that
is scored higher than the mapping to ei2. Then if the difference
of their scores is slight and is under a threshold, we switch to
propose the mapping of ei1 to ei2 rather than ei1 to ek2.

Case 3: We make use of property mapping to refine concept
mapping. For each generated concept mapping ei1 to ei2, we
checks mappings of their properties. The idea is to give a penalty
for those concept mappings when their properties do not have
mappings. We calculate a score that indicates the percentage
of correspondingly mapped properties in all of their properties.
After that, we multiply the combined predicting value of mapping ei1 to ei2 by the score. Finally, we re-rank mappings for
each concept.

we again check whether their objects are concepts, and then
check whether the concepts have a mapping. Next, we calculate
a score multiply the combined predicting value of mappings
ei1 to ei2 by the score. Finally, we re-rank mappings for each
property.

We also exploit the rules defined for constraint based deci-
sion. Details of the rules defined for constraint based decision
can refer to Section 3.4.

5. Experiments and evaluation

In this section, we first present our experiment design. Next,
we give the experimental results on five data sets. After that, we
compare RiMOM with existing methods. The implementation
of RiMOM was coded in Java.

Case 4: We make use of concept mappings to refine property
mappings. For each property mapping ei1 to ei2, we check its
domain and range. We check whether their domains are
the same or whether there is a concept mapping between their
domain concepts (in most cases, the domain of a property is
concept). We also check whether their range have the same
type (such as data type or object type). For data type, we again
check whether they are the same data type. For object type,

5.1. Experiment design

5.1.1. Evaluation measures

In the experiments of mapping, we conducted evaluations
in terms of precision and recall. The measures are defined as
follows:

Precision(P): It is the percentage of correct discovered map-

pings in the discovered mappings.

Fig. 6. Examples of l:null mappings.

Recall(R): It is the percentage of correct discovered map-

pings in the correct mappings.

P = |ma

mm|

, R = |mm

|ma|

|mm|

ma|

where ma are mappings discovered by RiMOM and mm are
mappings assigned manually (we view the manually assigned
mappings as the correct mappings).

However, we note that it is difficult to directly port them to
our scenario because n:1 mapping should not be judged by only
correct or incorrect. Therefore, allowing for n:1 mapping, we
extend the precision and recall as:
P =

mm

ma

imi  fp
|ma|

imi  fc
|mm|

, mi 
, mi 

ma

mm

R =
where fp =|mai  mmi|/|mai| is the proportion of correct items in
the discovered mapping mai. fc =|mai  mmi|/|mmi| denotes
the proportion of correctly discovered items in the correct
mapping mmi. For
correct mapping is
Location + Zipcode + Email Address and the discovered mapping is Location + Department + Phone + Email
Address. Then we obtain fp = 2/4 = 0.5, fc = 2/3 = 0.667.

example,

the

5.1.2. Data sets

We tried to collect heterogeneous ontologies from different

sources. Totally, we collected five data sets.

Course Catalog ontology I. It describes courses at Cornell
University and Washington University. The ontologies of Course
Catalog I have 3439 concepts, and are similar to each other.

Company Profile. It uses ontologies from Yahoo.com and
The Standard.com and describes the business of the two com-
panies.

Employee Ontology. It describes employee information.

Instances of the two ontologies have little overlap data.

Sales Ontology. It describes sales information. Instances of

the two ontologies have some overlap data.

EON. It includes 19 ontologies. The ontologies are about

domain of Bibliographic reference.

Course Catalog I and Company Profile are designed by
Doan [11], and were downloaded from http://anhai.cs.uiuc.edu/
archive/summary.type.html. EON is from the 2004 Evaluation of Ontology-based Tools workshop at http://co4.inrialpes.
fr/align/Contest/. We also created two data sets from real-world
databases: Employee Ontology and Sales Ontology. For each
database, we created two heterogeneous ontologies according
to the schema, and then translate records from the database into
instances of the two ontologies.

Except for EON data set, the other four data sets respectively
contain two heterogeneous ontologies, and thus the task is to
map them onto each other. In EON, there are 26 ontologies
used for the evaluations in the 2004 Evaluation of Ontologybased Tools workshop. One of the ontologies is chosen as target
ontology (also called reference ontology in the EON workshop).
The task is to map all the other 25 ontologies onto the reference

one. In the final evaluation, however, only 19 mapping tasks
are tested. On one ontology, we met the problem of parsing
error. Then we left it out from the data set. Finally, we included
the 18 source ontologies and the target ontology in EON data
set.

The entity names defined in the two ontologies of Course Catalog I are similar to each other and those in Company Profile are
not. The two data sets are used to test the effectiveness of name
based decision. Instances of the two ontologies in Employee
Ontology have little overlap data and those in Sales Ontology
have some overlap. The two data sets are used to test the effectiveness of instance based decision. EON has 19 ontologies and
18 mapping tasks. It is designed to test many different kinds of
mapping tasks. See [44] for details.

We manually created mappings for Employee Ontology and
Sale Ontology. Course Catalog I, Company Profile, and EON
include the correct mappings in the data sets.

Table 5 shows the statistics on the data sets. The columns
represent, respectively, data set, ontologies in the data sets,
number of concepts, properties, manual mapping, and instances
in the ontologies. Course Catalog I, Company Profile, and
EON are designed only for 1:1 mapping evaluation. So, our
evaluation and comparison focus on the 1:1 mapping on
them.

We see that in the first four data sets, the concept numbers
of the two ontologies are significant different, in particular in

Table 5
Statistics on data sets (%)

Data set

Ontology

Concept

Property Manual

Instance

Course
Catalog I

Company
Profiles

Employee

Sales
Ontology

Cornell
Washington

Standard.com
Yahoo.com

Ontology 1
Ontology 2

Ontology 1
Ontology 2
