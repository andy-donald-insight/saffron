Web Semantics: Science, Services and Agents

on the World Wide Web 3 (2005) 340365

Learning domain ontologies for semantic

Web service descriptions

Marta Sabou a,

, Chris Wroe b, Carole Goble b, Heiner Stuckenschmidt a

a Department of Artificial Intelligence, Vrije Universiteit Amsterdam, De Boelelaan 1081a, 1081 HV Amsterdam, The Netherlands

b Department of Computer Science, University of Manchester, UK

Abstract

High quality domain ontologies are essential for successful employment of semantic Web services. However, their acquisition
is difficult and costly, thus hampering the development of this field. In this paper we report on the first stage of research that
aims to develop (semi-)automatic ontology learning tools in the context of Web services that can support domain experts in the
ontology building task. The goal of this first stage was to get a better understanding of the problem at hand and to determine which
techniques might be feasible to use. To this end, we developed a framework for (semi-)automatic ontology learning from textual
sources attached to Web services. The framework exploits the fact that these sources are expressed in a specific sublanguage,
making them amenable to automatic analysis. We implement two methods in this framework, which differ in the complexity
of the employed linguistic analysis. We evaluate the methods in two different domains, verifying the quality of the extracted
ontologies against high quality hand-built ontologies of these domains.

Our evaluation lead to a set of valuable conclusions on which further work can be based. First, it appears that our method,
while tailored for the Web services context, might be applicable across different domains. Second, we concluded that deeper
linguistic analysis is likely to lead to better results. Finally, the evaluation metrics indicate that good results can be achieved using
only relatively simple, off the shelf techniques. Indeed, the novelty of our work is not in the used natural language processing
methods but rather in the way they are put together in a generic framework specialized for the context of Web services.
 2005 Elsevier B.V. All rights reserved.

Keywords: Ontology learning; Semantic Web services; Ontology learning evaluation

In the last few years the Web encountered two revolutionary changes which aim to transform it from
a static document collection in an intelligent and
dynamic data integration environment. First, the Web
service technology allowed uniform access via Web


Corresponding author. Tel.: +31 20 598 7752;

fax: +31 20 598 7653.

E-mail address: marta@cs.vu.nl (M. Sabou).

standards to software components residing on various platforms and written in different programming
languages. As a result, software components providing a variety of functionalities (ranging from currency
conversion to flight booking or book buying) are now
accessible via a set of Web standards and protocols.
Naturally, the real value of Web services is in their
composition which allows creating new and complex
functionalities from the existing services. The second

1570-8268/$  see front matter  2005 Elsevier B.V. All rights reserved.
doi:10.1016/j.websem.2005.09.008

novel Web technology, the Semantic Web, developed
techniques for augmenting existing Web data with logics based formal descriptions of their meaning. This
semantic markup is machine processable and therefore
facilitates access and integration of the vast amount of
Web data.

A major limitation of the Web services technology
is that finding and composing services still requires
manual effort. This becomes a serious burden with the
increasing number of Web services. To address this
problem, semantic Web researchers advanced the idea
of augmenting Web services with a semantic description of their functionality that could facilitate their
discovery and integration. More precisely, Web services are described in terms of concepts provided by a
domain ontology. These concepts denote entities in the
domain of the Web service (e.g., Food, Hotel) as well as
functionalities that can be performed by services in the
given domain (e.g., OrderFood, BookHotel). To ensure
high quality reasoning on these semantic Web service
descriptions, it is essential that they rely on the use
of quality domain ontologies, i.e., ontologies that have
a broad coverage of their domains terminology. This
would allow many (if not all) services to use the same
or a small number of different ontologies thus reducing
the need of mappings at reasoning time. Note that in
practice different types of ontologies are used ranging
from catalogs of domain concepts to formal domain
models.

Despite their importance, few domain ontologies for
Web service descriptions exist and building them is
a challenging task. One of the problematic aspects is
that for building a high quality domain ontology one
ideally needs to inspect a large number of Web services in that domain. Since many domains witnessed a
rapid increase in the number of available Web services
to several hundreds (1000+ in bioinformatics), tools
that support ontology curators to build a Web service
domain ontology from these large and dynamic data
sets become crucial.

the problem of

Our work addresses

(semi-)
automatically learning Web service domain ontologies.
We report on the first stage of this work in which we
aim to get a better understanding of the ontology learning task in the context of Web services and to identify
potentially feasible technologies that could be used.
Early in our work we learned that the context of Web
services raises several issues that constrain the devel-

opment of an ontology learning solution. We designed
a framework for performing ontology learning in the
context of Web services which addresses these issues
in two ways. First, it exploits the particularities of Web
service documentations to extract information used for
ontology building. In particular, the sublanguage characteristics of these texts lead to the identification of a set
of heuristics. These heuristics are implemented as pattern based extraction rules defined on top of linguistic
information. Second, the learned ontologies are suited
for Web service descriptions as they contain both static
and procedural knowledge.

We implemented two learning methods that follow
the basic principles of the framework but use different linguistic knowledge. The first method uses basic
Part-of-Speech (POS) information and was developed
and tested in the context of the WonderWeb project1
[52]. The second method uses deeper dependency parsing techniques to acquire linguistic knowledge. It was
designed and tested on data sets provided by the myGrid
project2 [55]. In this paper we present both methods and
compare them by applying and evaluating them in the
context of both projects.

This paper is structured as follows. We first present
some introductory notions about semantic Web service
technology, concluding the important role that Web
service domain ontologies play as well as some requirements that they should fulfill (Section 1). Then, we analyze why it is difficult to build such domain ontologies.
We do this by describing the process of building domain
ontologies in the context of the two research projects
that served as case studies for developing and evaluating our framework (Section 2). We conclude Section 2
with an overview of the issues that constrain the development of an ontology learning solution in the Web
services context. Then, we present an ontology learning framework that deals with these constraints and
the two concrete implementations of this framework
in Section 3. Implementation details and some considerations about the usability of the extraction tools
are provided in Section 4. In Section 5 we present an
overview of existing ontology learning evaluation practices and show how they were adapted for our work. In
this section we also detail our experimental results. We

1 http://wonderweb.semanticweb.org/.
2 http://www.mygrid.org.uk/.

M. Sabou et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3 (2005) 340365

finally discuss our major findings and point out future
work in Section 6.

1. Semantic Web services

The advent of Web service technology brought a
new dynamic aspect to the Web. The W3C Web Services Architecture Working Group defines a Web service as a software application identified by an URI,
whose interfaces and bindings are capable of being
defined, described and discovered as XML artifacts. A
web service supports direct interactions with other software agents using XML-based messages exchanged
via Internet-based protocols [22]. Typically a Web
service interface is described using the XML based
Web Service Description Language (WSDL) and registered in UDDI, a registry that permits Universal
Description Discovery and Integration of Web ser-
vices. Web service exchange messages are encoded
in the XML based SOAP messaging framework and
transported over HTTP. By relying on these standards,
Web services hide any implementation details increasing cross-language and cross-platform interoperability.
The WSDL language specifies the functionality and
message formats of the service only at a syntactic level.
While these descriptions can be automatically parsed
and invoked by machines, the interpretation of their
meaning is left for a human programmer. This lack of
semantics limits the use of WSDL descriptions to facilitate invocation of the correct service.

The semantic Web community addressed this limitation by augmenting the service descriptions with a
semantic layer in order to achieve their automatic dis-
covery, composition and execution. A semantic Web
service description relies on two major constituents as
shown by the following hotel booking service descrip-
tion.

proceeded by the GO (i.e., generic ontology) namespace prefix. A first major initiative in this direction of
establishing a standard generic terminology for Web
service description is the development of the OWL-
S ontology [37,38]. Second, the description template
built with these generic terms is filled in with domain
specific concepts provided by a Web service domain
ontology (see the concepts proceeded by the DO (i.e.,
domain ontology) prefix). A Web service domain ontology typically specifies two types of domain knowl-
edge. On one hand it contains concepts that describe
functionalities offered in a domain (e.g., BookHotel,
BuyTicket). On the other it specifies domain concepts
that often appear as parameters of Web services (e.g.,
Hotel, Ticket). Web service domain ontologies differ from existing domain specific ontologies used in
the semantic Web by the fact that besides specifying
domain concepts (i.e., static knowledge) they also specify functionality types (i.e., procedural knowledge).

Several Web service tasks can be automated by using
semantic descriptions. For example, service offers
and requests can be matched automatically [44]. This
matchmaking is flexible because it allows retrieving
services that only partially match a request but are still
potentially interesting. For example, the hotel booking
service will be considered a match for a request for
Accommodation booking services, if the used domain
ontology specifies that Hotel is a kind of Accommo-
dation. This matchmaking is superior to the keyword
search offered by UDDI registries.

A basic requirement for being able to perform
complex reasoning with multiple semantic Web service descriptions is that (many of) these descriptions
should use concepts of the same (or a small number of)
domain ontology. If each Web service description uses
a different domain ontology then a mapping between
these ontologies has to be performed before any reasoning task can take place. However, ontology mapping

First, the semantic description uses the vocabulary
defined by a generic Web service description language
to specify the main elements of the service (e.g., inputs,
outputs). The generic concepts used in our example are

itself is a difficult and largely unsolved problem in the
Semantic Web. Therefore, a quality domain ontology
will reflect a high percentage of the domains terminology (i.e., offer a high coverage of the domains

terminology) so that many Web services in that domain
can be described with its concepts. This requirement
makes the building of the domain ontologies difficult
as it is evident in the next section where we present an
analysis of the ontology building process in two concrete research projects.

2. Building Web service domain ontologies

The creation of semantic Web service descriptions is
a time consuming and complex task whose automation
is desirable, as signaled by many researchers in this
field, for example [59]. This task can be broken down
in two smaller tasks. First, acquiring a suitable Web
service domain ontology is a prerequisite when creating
semantic Web service descriptions. This is required to
create a shared terminology for the semantic service
descriptions. Second, the actual process of annotating
Web services with the concepts provided by the domain
ontology (i.e., creating their semantic descriptions) has
to be performed.

To our knowledge, two research teams concentrate
on the Web service annotation task. Hess and Kushmerick [28] employ Naive Bayes and SVM machine learning methods to classify WSDL files (or Web forms)
in manually defined task hierarchies. Patil et al. [45]
employ graph similarity techniques to select a relevant
domain ontology for a given WSDL file from a collection of ontologies. Then they annotate the elements of
the WSDL file with the concepts of the selected ontol-
ogy. Both teams use existing domain ontologies and
acknowledge that their work was hampered by the lack
of such ontologies. The work presented in this paper
is complementary, since we address the acquisition of
Web service domain ontologies.

In the rest of this section we describe the ontology building process as it took place in the context
of two research projects: WonderWeb and myGrid.
These projects offered realistic requirements, data
sets and evaluation standards for our work. In both
cases we detail (1) the kind of data sources used
for ontology building and the (2) resulting manually
built domain ontologies. These manually built ontologies serve as Gold Standards when evaluating the
automatically learned ontologies. We also (3) highlight the difficulties encountered during building these
ontologies.

The benefit of this analysis is twofold. First, these
projects reveal some of the major aspects that make
Web service ontology building difficult. These aspects
prompt at the need of automating (at least to some
extent) the acquisition of domain ontologies. The second benefit of the analysis is an overview of a set
of issues that constrain the development of ontology
learning methods in the context of Web services. These
constraints, discussed in Section 2.3, guided us in the
design of the ontology learning framework described
in Section 3.

2.1. Case study 1: WonderWeb RDF(S) storage
tools

2.1.1. Project description

The EU-funded WonderWeb research project aimed
to develop an infrastructure for large-scale deployment of ontologies on the Semantic Web. The projects
engineering infrastructure was provided by the KAON
Application Server [34], a semantic middleware system which facilitates the interoperability of Semantic
Web tools [54]. Ontologies that describe the functionality of Semantic Web tools and services are core to
the architecture of this middleware. As RDF(S) storage and query facilities are essential components of
any Semantic Web application they were the first ones
to be integrated with KAON and thus required a domain
ontology that would describe this domain. Besides
WonderWeb, the ontology for describing RDF(S) storage functionality was also used in the AgentFactory
project which performs configuration of semantically
described Web services using agent-based design algorithms [49,50].

2.1.2. Data sources

While there are many tools offering ontology storage (a major survey [18] reported 14 such tools), only
few are available as Web services (two, according to
the same survey). Therefore, it is problematic to build
a quality domain ontology by analyzing only the available Web services. However, since Web services are
simply exposures of existing software to Web acces-
sibility, there is a large overlap (often one-to-one cor-
respondence) between the functionality offered by a
Web service and that of the underlying implementation.
Based on this observation, the domain ontology was
manually built by analyzing the APIs of three RDF(S)

M. Sabou et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3 (2005) 340365

storage tools (Sesame [5], Jena [39], KAON RDF API
[34]).

The data source used during ontology building consisted of the javadoc documentation of all methods
offered by these APIs. A javadoc documentation contains a general description of the methods functional-
ity, followed by the description of its parameters, result
types and exceptions to be thrown. See, for example,
the javadoc documentation of the add method from the
Jena API.

add

Add all the statements returned by an

iterator to this model.

Parameters:

iter  An iterator which returns the

statements to be added. Returns:this
model Throws: RDFException - Generic RDF
Exception

2.1.3. Manually built ontology

The manually built ontology contains 36 concepts
distributed in two main hierarchies. The first hierarchy contains concepts that denote a set of functionalities offered by the analyzed APIs. These concepts
are grouped under the Method concept which is similar in meaning to the OWL-S Profile concept (see a
snapshot of the ontology in Fig. 1). This hierarchy contains four main categories of methods for: adding data
(AddData), removing data (RemoveData), retrieving
data (RetrieveData) and querying (QueryMethod). Nat-
urally, several specializations of these methods exist.
For example, depending on the granularity of the added

data, methods exist for adding a single RDF statement
(AddStatement) or a whole ontology (AddOntology).

Besides the Method hierarchy, the ontology also
contains the elements of the RDF Data Model (e.g.,
Statement, Predicate, ReifiedStatement) and their hier-
archy, grouped under the Data concept. The ontology
is rich in knowledge useful for several reasoning tasks.
For example, the methods are defined by imposing
restrictions on the type and cardinality of their parame-
ters, describing their effects and types of special behavior (e.g., idempotent). Note that the hierarchy encoded
by this ontology reflects a certain conceptualization
and is not unique. Furthermore, the building of this
manual ontology was a good indication that API documentations are rich enough to allow building domain
ontologies.

2.1.4. Encountered problems

The major impediment in building a domain ontology for describing RDF(S) storage tools was the choice
of data sources from which to build the domain ontol-
ogy. Once the decision taken, it took three weeks (for
one person) to build the ontology. This time includes the
time to read and understand the API documentations as
well as the time to identify overlapping functionalities
offered by the APIs and to model them in an ontology.

2.2. Case study 2: myGrid bioinformatics services

2.2.1. Project description

myGrid is a UK EPSRC e-Science pilot project
building semantic grid middleware to support in silico experiments in biology. The experimental protocol
is captured as a workflow, with many steps performed
by Web services. Core to the infrastructure is an ontology for describing the functionality of these services
and the semantics of the manipulated data. A key role
of the ontology is to facilitate user driven discovery of
services at the time of workflow construction.

2.2.2. Data sources

The ontology was built manually initially using the
documentation for 100 services as a source of relevant
terms. These services are part of the European Molecular Biology Open Software Suite (EMBOSS) service
collection3 and are further referred to as EMBOSS

Fig. 1. RDF(S) storage ontology snapshot.

3 http://www.hgmp.mrc.ac.uk/Software/EMBOSS/Apps/.

services. Each service has a detailed description
containing (among others) a short description of the
service, detailed information about its command line
arguments, examples of the input/output file formats
and its relation with other services in the collection.

2.2.3. Manually built ontology

The manually built myGrid ontology is much larger
and more complex than the RDF(S) ontology. It contains over 550 concepts distributed over a set of distinct
subsections covering the domains of molecular biol-
ogy, bioinformatics, informatics and generic tasks, all
under a common upper level structure. Several relations are defined between these concepts and multiple
inheritance is often used. However, currently only a
part of this ontology (accounting for 23% of its con-
cepts) provides concepts for annotating Web service
descriptions in a forms-based annotation tool. The so
obtained semantic Web service descriptions are used
for facilitating service discovery [59]. The rest of the
ontology contains concepts from the domain of biology
that are too generic for describing the existing services
(e.g., organism), or concepts that are used to define
orthogonal views on the existing services (see more on
this in Section 5.4.3). The myGrid ontology contains
only a small number of concepts denoting functionality (23) (see a snapshot of this part of the ontology in
Fig. 2). Also, a different modelling principle is used
here compared to the RDF(S) ontology. Namely, the
functionality concepts simply denote generic actions
that can be performed in bioinformatics without being
linked to the involved data structures. A possible explanation for this choice is that in bioinformatics one

Fig. 2. myGrid ontology snapshot.

can perform these operations on a multitude of data
structures and thus, enumerating all these combinations
would be impractical.

2.2.4. Encountered problems

Several factors hampered the building of this ontol-
ogy. First, ontology building in itself is time consuming.
The ontology was initially built with two months of
effort from an ontology expert with four years experience in building description logic based biomedical
ontologies. A second impediment is the dynamic nature
of the field. The exponential rise in the number of bioinformatics Web services over the past year required a
further two months effort to maintain and extend the
ontology. However, its content currently lags behind
that needed to describe the 1000+ services available
to the community. Thirdly, lack of tools hampered
the process. At the time of development, tool support
for handling separate ontology modules was minimal,
hence the existence of a single substantial ontology.
A fourth impediment was the lack of guidelines on
how to build the domain specific ontology, or indeed
how to relate it to upper level ontologies. Since at that
time DAML-S (the predecessor of OWL-S) was still
under development, the ontology curator devised their
own generic Web service description schema based
on DAML-S but much simplified to reflect narrower
requirements. Lacking guidance from the Web services
field, the curator relied on design principles employed
in other large open source biomedical ontologies such
as openGALEN [47] and the TAMBIS ontology [2].

2.3. Ontology learning characteristics

The analysis of the ontology building process presented in the previous two subsections lead to two
major conclusions. First, ontology building was experienced as a difficult process in both projects. This
prompts to the need of an automated solution for this
problem. Second, the Web service context presents a set
of constraints that have to be taken into account when
designing an automatic solution. In this subsection we
detail both conclusions.

1. Ontology building is difficult and should be auto-
mated. Both case studies agree on a set of problematic factors that hampered the ontology building
activity. First, the ontology curators had to analyze

M. Sabou et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3 (2005) 340365

(read, understand and identify common concepts
in) a high number of textual documents (over hundred in both cases) to ensure the quality of their
ontologies. The number of analyzed documents is
likely to increase as many domains witness a rapid
increase in the number of available Web services
to several hundreds. A second impediment was the
lack of guidelines on what knowledge such ontologies should contain and what design principles they
should follow. This resulted in different groups
building different ontologies to describe Web services in the same domain, as reportedly happened
in bioinformatics [31]. These factors make ontology building a time consuming activity creating a
demand for tools that support ontology curators to
extract ontologies from large and rapidly changing
textual data collections.

2. Several constraints have to be taken into account
when building an automated ontology learning
solution. We conclude that the two ontology building activities differ in several aspects. First, the
application domains are different: computer science
versus biology related. Second, different kinds of
data sources are used as a basis for ontology build-
ing: javadoc descriptions of several tool APIs in
case study 1 and detailed service documentations
in case study 2. These sources also differ in their
grammatical quality, the descriptions used in case
study 1 having a lower quality from this perspective.
The manually built ontologies are also different.
The myGrid ontology is much larger and more complex than the one about the RDF(S) domain. This is
not necessarily an advantage since experience has
shown that only a small fraction of the ontology was
usable for Web service annotation.

This diversity of domains, existing data sources and
requirements for the extracted ontologies raises the
challenge to build an ontology learning method which
can deal with these variances. Namely, the learning
method should be applicable on a wide range of texts
from different domains and offer a configurable set of
modelling principles for conceptualizing the extracted
knowledge. However, there are some important characteristics that both case studies exhibit and which guided
us in setting up the ontology learning framework (see
Section 3). These relate to the quality of the analyzed
data sets and the requirements for the learned Web ser-

vice domain ontology. We discuss each of them in a
separate subsection.

2.3.1. Low grammatical quality

The natural language descriptions associated with
Web services are mostly comments written by their
developers. Therefore, they have a low grammatical
quality. Punctuation is often completely ignored and
several spelling mistakes are present. Naturally, services that have a larger user base expose a better documentation while some less-used services barely contain
snippets of abbreviated text.

The evident drawback of this low grammatical quality of the analyzed texts is that they are difficult to
process with off the shelf NLP tools. These tools were
trained on high quality newspaper corpora but even the
best quality documentations are often under the standard quality of newspaper texts. For example, some rule
based Part-of-Speech taggers are sensitive to the capitalization of words considering most capitalized words
as nouns. A possible remedy is to use preprocessing,
e.g., capitalization of the first words of the sentence,
adding some punctuation.

There are also two advantages of working with such
documentations. First, these texts usually employ simple sentences instead of using complicated phrases.
This reduced ambiguity allows using deeper linguistic
analysis. For example, dependency parsers work better on short sentences than on complex phrases. The
second advantage is that these texts use natural language in a specific way. This characteristic makes them
amenable to automatic analysis as discussed in the next
subsection.

2.3.2. Sublanguage characteristics

Software documentation in general, and Web service descriptions in particular, employ natural language
in a specific way. They belong to what is defined as a
sublanguage in [21]. A sublanguage is a specialized
form of natural language which is used within a particular domain or subject matter and characterized by
a specialized vocabulary, semantic relations and syntax (e.g., weather reports, real estate advertisements).
Harris, one of the first researchers to study the use of
natural language in restricted domains, introduced the
notion of sublanguage word classes defined as sets of
words that are acceptable in the same context within a
sublanguage [25]. An intuitive example from the med-

ical domain is that in the context{space} revealed a
tumor we might find words such as X-ray, film, scan.
These words belong to the MEDICAL TESTs sublanguage word class. There are several constraints on the
co-occurrences of word classes in a sublanguage. For
example, many valid sentences in the medical sublanguage have the form MEDICAL TEST revealed DISEASE while sentences of the form DISEASE revealed
MEDICAL TEST are meaningless in this sublanguage
(even if grammatically valid). These constraints are
called selectional constraints.

Several word classes and selectional constraints can
be determined in the Web service sublanguage. For
example, by considering EXT VB a word class of verbs
that indicate an extraction process (e.g., extract, get,
retrieve) a frequently occurring pattern which involves
this word class and the preposition from can be used
to easily determine the output and the source of the
action.

Selectional Constrain (Pattern):

EXT VB OUTPUT from SOURCE.

Examples:

Extract data from aaindex.
Extract cds, mrna and translations from

feature tables.

Get data from cutg.
Retrieve features from a sequence.

Knowledge about word classes and their selectional
constraints in a certain sublanguage can support several Natural Language Processing tasks, such as Information Extraction [19]. Using sublanguage analysis
techniques has also a direct applicability in Ontology Learning since word classes often denote semantic
classes. Selectional constraints can help to determine
the members of a word class given some knowledge
about the members of other word classes involved in
the restrictions.

One of the major problematic aspects of sublanguage analysis is that determining the interesting word
classes and their selectional constraints is a time consuming process. There has been promising research
on (partly) automating this process [20,51]. However,
our intention was, for the first implementation of our
framework, to focus on few but frequently occurring
sublanguage features that do not need laborious work
to be identified. Such are patterns that do not rely on
lexical information but only on syntactic structures. For

example, one of the straight-forward observations was
that, in this sublanguage, almost any verb indicates an
action performed by a Web service. So, a word class
ACTION would include any identified verbs. Also,
noun phrases that appear after an action verb denote
a participant in the action, forming the word class
ACTION PARTICIPANT. These word classes can be
easily identified by relying only on a minimal linguistic analysis. Any co-occurrence of these word classes
identifies a Web service functionality and therefore
provides the basic material for our ontology learning
algorithm.

2.3.3. Ontologies of procedural knowledge

Ontology learning has to be adapted not only to deal
with the characteristics of the input data but also to produce ontologies that are fit for the task of describing
Web services. Web service domain ontologies contain
both static (i.e., domain entity concepts) and procedural
knowledge (i.e., functionalities offered by Web ser-
vices). Existing ontology learning efforts, to our knowl-
edge, have only focused on deriving static knowledge.
One of the contributions of our work is to extend these
techniques to the acquisition of procedural knowledge
as well.

The ontologies we learn are, in the terminology
introduced by Guarino in [23], application ontologies,
i.e., ontologies that contain both domain concepts and
problem solving knowledge useful in a particular appli-
cation. However, the OWL-S coalition has coined the
term domain ontology as referring to ontologies that
provide any kind of domain knowledge, both static and
procedural. In this paper we use the term domain ontology in the sense used by the Web services community.

3. The Web service ontology learning
framework

In the previous section we identified a set of particularities that condition ontology learning when performed in the context of Web services. These particular
characteristics require the adaptation of existing ontology learning methods. Our literature study yielded that
the ontology learning field offers a wide range of different approaches to ontology acquisition. However, while
most work is targeted on specific domains we are not
aware of any efforts that analyze software documenta-

M. Sabou et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3 (2005) 340365

tion style texts. Several generic ontology learning tools
exist, most prominently Text-To-Onto [36], OntoLearn
[41] or OntoLT [7], but they are either not available
for experimenting or they are workbenches of generic
methods that can be fine-tuned for a certain domain.

In this section we present an ontology learning
framework which is tailored to address the particularities of the Web services domain. We first describe the
learning framework as a whole (Section 3.1), then we
detail each of its steps.

3.1. Overview of the framework

The ontology learning framework consists of several
steps, as depicted in Fig. 3. We briefly describe these
steps and show how the characteristics of the Web services context influenced their design.

1. Term extraction: In the first step we identify words
in the corpus that are relevant for ontology build-
ing. A word or a set of words that are identified as
useful for ontology building form a term. Term
extraction is done in two steps. First, in a linguistic
analysis phase the corpus is annotated with linguistic information. Then, a set of extraction rules are
applied on this linguistic information to identify the
potentially interesting terms.

The characteristics of the Web services domain
influenced our design choices in several ways. First,
to overcome the limitations of the poor grammatical
quality of the texts we employed linguistic analysis of different complexity. As it is evident from
the results of our experiments, more complex analysis led to better results. Then, the small size of the
corpus and its sublanguage features facilitated the

Fig. 3. The ontology learning framework.

use of a rule-based solution. Namely, the sublanguage features of the corpora allowed us to easily
observe a few heuristics for identifying important
information and implement them in our extraction
rules.

2. Ontology building: The identified terms are central-
ized, analyzed and transformed in corresponding
concepts and their hierarchical
relations. The
ontology building phase derives both static and
procedural knowledge in the form of a hierarchy of
frequent domain concepts and a hierarchy of Web
service functionalities. The strong sublanguage
features of the analyzed corpora allow extracting
terms that are highly relevant for ontology building.
Therefore, it suffices to use simple ontology learning techniques and to adapt them to the requirements
of the domain (e.g., extract procedural knowledge).
3. Ontology pruning: The low grammatical quality of
the corpus and its sublanguage characteristics cause
a suboptimal functioning of the used linguistic
tools. Therefore, some of the derived concepts
do not have any domain relevance. The pruning
stage filters out
these potentially uninteresting
concepts.

In the next subsections we detail all three steps of

the learning framework.

3.2. Step 1: Term extraction

The term extraction phase identifies (sets of) words
(terms) in the corpus that are relevant for ontology
building. This phase can be implemented in different
ways.

First, linguistic analysis of different complexity can
be used. In this paper we report on two concrete implementations of the framework which use two different
kinds of linguistic knowledge. The first implementation discussed in Section 3.2.1, M POS, uses basic
Part-of-Speech (POS) information. The second implementation presented in Section 3.2.2, M DEP, uses
deeper dependency parsing techniques.

Second, the different linguistic information require
implementing different extraction patterns: surface
patterns in the first case and syntactic patterns in the
second. While the technical implementation of these
pattern based rules differ (as described in Sections 3.2.1
and 3.2.2), the heuristics behind them remain the same.

Independently of the technical implementation, we distinguish two major categories of rules according to the
type of information they derive.

Rules for identifying domain concepts rely on the
observation that domain concepts correspond to nouns
in a corpus. Given the small size of the corpora and
the concise style of the Web service documentations
the majority of nouns denote potentially interesting
domain concepts. We extract entire noun phrases where
a noun phrase consists of a head noun preceded by an
arbitrary (zero or more) number of modifiers (nouns or
adjectives).

Rules for identifying functionalities implement the
previously described sublanguage characteristics, i.e.,
that verbs and related nouns are good indicators of Web
service functionality.

In the next two subsections we detail the technical
details of two concrete implementations of the term
extraction step.

3.2.1. Method 1: Part-of-Speech based extraction

The first implementation of the framework relies
on Part-of-Speech tags (POS). We use the Hepple POS
tagger [27] to perform the linguistic analysis phase. The
tagger assigns each word in the sentence a corresponding POS tag. For example, in the sentence below, the
tagger identifies a verb (i.e., find), two nouns (i.e., sites,
proteins), an adjective (i.e., antigenic) and a preposition (i.e., in).

Find(VB) antigenic(JJ) sites(NN)

in(Prep) proteins(NN).

Following the general steps described by the frame-
work, a set of extraction rules are applied on the derived
linguistic information. The extraction patters which
form the right hand side of the rules are implemented
as surface patterns which, besides POS tag linguistic information, rely on surface knowledge such as the
order of words in the sentence.

1. Identifying domain concepts: We stated above that
extraction patterns are written to extract both static
(domain concepts) and procedural (service func-
tionalities) knowledge. The surface pattern that
extracts noun phrases implements the heuristic
observation described above. This rule is specified
in JAPE [13], a rich and flexible regular expression
based rule mechanism offered by the GATE framework [12].

The pattern in the left hand side of the rule (i.e.,
before ) identifies noun phrases. Noun phrases
are word sequences that start with zero or more
determiners (identified by the (DET)* part of the
pattern). Determiners can be followed by zero or
more adjectives, nouns or possession indicators in
any order (identified by the (ADJ|NOUN|POS)*
part of the pattern). A noun phrase mandatorily
finishes with a noun, called head noun ((NOUN)).
DET, ADJ, NOUN and POS are macros and act as
placeholders for other rules identifying terms that
are part of these categories. These macros rely on
the actual POS tag information. For example, the
ADJ macro has the following definition:

The macro contains the disjunction of three pat-
terns. This means that the macro fires if a word
satisfies any of these three patterns. Each of these
three patterns identifies words which were assigned
one of the JJ, JJR and JJS POS tags. POS tags are
assigned in the category feature of a Token annota-
tion.

Any word sequence identified by the left hand
side of a rule can be referenced in its right hand
side. The text snippet identified by a (part of) a pattern is associated to a variable which can then be
reused in the right hand side. For example, np identifies all noun phrases. This string is then used in
the right hand side of the rule which specifies that
strings denoted by np should be annotated with the
NP annotation.

In the example, our pattern identifies antigenic
sites4 (ADJ NOUN) and proteins (NOUN) as
noun phrases.

2. Identifying functionalities: One surface pattern
identifies pairs of verbs and following noun phrases
as potential functionality information to be added to

4 We use this notation convention to present terms extracted from

the corpus.

M. Sabou et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3 (2005) 340365

the domain ontology. Having identified and annotated noun phrases (NP) and verbs (VB) with two
previous rules, the JAPE rule for identifying and
annotating functionalities is straightforward.

In

the

the

example,

identifies
find antigenic site as a verb phrase denoting a
possible functionality in bioinformatics.

pattern

3.2.2. Method 2: Dependency relation based
extraction

In a second implementation, M DEP, we experimented with richer linguistic information than POS
tags, i.e., dependency relations. Dependency parsing
offers a deeper linguistic analysis than POS tagging
being a commonly used method in computational
linguistics. A dependency relation is an asymmetric
binary relation between a word called head and a word
called modifier.

We use Minipar [30], a state of the art dependency
parser with a reported high performance (88% precision and 80% recall). As an example, we list in Table 1
Minipars analysis for our example sentence. For each
word, the following information is provided: (i) its position in the sentence; (ii) its form as it appears in the
sentence; (iii) its lemma; (iv) its Part-of-Speech; (v)
the name of the dependency relation between this word
and the head (e.g., obj) and (vi) the position of the head
word modified by the current word. In the example antigenic is an adjective which modifies the noun sites, and
sites is the object of the verb find.

The benefit of using richer linguistic information
is that the potentially interesting information can be
extracted in an easier way. While the same heuristics are
used, the extraction patterns must be re-implemented.
These patterns are defined on the syntactic relations
within the sentences and therefore called syntactic pat-
terns.

1. Identifying domain concepts: The first category
of patterns, those that identify domain concepts,
explore the nn (noun modifier of a noun) and
mod (adjective modifier of a noun) dependency
relations to detect noun phrases. When such relations are identified, the head noun together with
its modifiers are annotated as being a noun phrase.
Regular expressions are not enough to encode these
more complex patterns (they do not allow variables).
We use extra java code on the right hand side of the
JAPE extraction rules to accomplish this.

2. Identifying functionalities: The pattern for functionality identification relies on the obj relationship
and identifies pairs of verbs and their objects. If the
object is the head of a noun phrase then the whole
noun phrase is extracted. This pattern relies on the
output of the previous NP extraction pattern.

This pattern captures the desired information in the
majority of cases with a few exceptions. One of the
exceptions occurs when several verbs in a sentence
refer to the same object. For example, the sentence
Replace or delete sequence sections suggests that both
replace sequence section and delete sequence
section are valid functionalities in this domain that
we wish to extract. However, Minipars output does
not directly encode the verb-object relation between
delete and section (see Table 2). On the other hand,
the analysis denotes that there is a dependency relation
between the two verbs of the sentence. Whenever two
or more verbs are related by a logical operator they
should be bound to a single noun (the object of one
of the verbs). One of our extraction patterns identifies
cases when several verbs are related via the lexdep
or conj relations. These relations denote cases when
verbs are related via logical operators such as or,
and (e.g., Reverse and complement a sequence) or
,. Often there are cases when the logical dependency
between more than two verbs is partially specified and

Table 1
An example Minipar output

Table 2
Verb dependency example

Pos.

Word

find
antigenic
sites
in
proteins

Lemma

find
antigenic
site
in
protein

Prep

Relation


mod
obj
mod
pcpmp-n

Head

Position Word

Lemma

Relation

Head

replace
or
delete
sequence
sections

replace
or
delete
sequence
section

lex-mod
lex-dep
nn
obj

Table 3
Noun dependency example

Position Word

Lemma

POS Relation Head

pick
pcr
primers
hybridization
oligos

pick

pcr
primer

hybridization N
oligos


nn
obj
nn
conj

we have to explicitly define all dependents based on the
transitivity of this relation (e.g., if dependency(v1,v2)
and dependency(v2,v3) then dependency(v1,v3)).

Another exception is when several objects are in
a conjunctive relation. For example, from Pick pcr
primers and hybridization oligos we wish to extract
both pick pcr primer and pick hybridization
oligos functionalities. However,
the Minipar output specifies only the first verb-object relation (see
Table 3). Nevertheless, knowing that there is a conjunctive relation between primers and oligos we can deduce
that oligos also plays an object relation with respect to
the verb pick. Just as with verbs, we wrote a pattern that
identifies conjunctive NPs and deduces the additional
knowledge. The patterns that identify dependency of
verbs and objects are performed before the pattern that
identifies functionalities.

3.2.3. Related work on pattern based techniques

The term extraction phase has a major importance
in our framework since the quality of this extraction
directly influences the final ontology. We present an
overview of related work on using pattern based techniques to derive semantic relations.

Pattern based techniques are widely used in several
natural language processing applications. Notably they
have been used to derive semantic relations from large
corpora. A pioneer in this direction of research was
the work of Hearst which introduced the idea of learning hyponymy relations using lexico-syntactic patterns
[26]. Lexico-syntactic patterns are defined on both lexical and basic syntactic information (POS tags). As
such, they allow extracting relations after shallow text
processing only. For example, the hyponymy relationship suggested by Bruises, wounds, broken bones or
other injuries could be extracted using the NP, NP*,
or other NP pattern [26]. As a follow up of this work,
Charniak developed a set of lexico-syntactic patterns
that identify meronymy (partOf) relations [3]. In both

cases, the identified semantic relations were used to
enlarge WordNet.

Naturally, such patterns have a clear relevance
for ontology learning. Indeed, Hearst-style patterns
are used in the work of Cimiano [10] and in the
CAMELEON tool which incorporates over 150 generic
patterns for the French language [1,56]. While such
generic patterns work well in general corpora they often
fail in small or domain specific corpora. In these cases
domain-tailored patterns provide a better performance
[1]. Besides using domain tailored patterns one can
enlarge the extraction corpora. For example, World
Wide Web data can be used for pattern based learning [8]. In our work and in several ontology learning
approaches pattern based extraction is just a first step
in a more complex process [7,15,32]. In these cases
patterns identify potentially interesting terms in the corpus and then the next processing steps derive relevant
semantic structures from these terms.

Summarizing Section 3.2, note that the sublanguage
nature of the Web service specific corpora allowed us
to extract sufficient material for ontology building by
using only relatively simple, off the shelf natural language processing techniques. There are several advantages of using these simple extraction methods. First,
they are fast. Second, they rely on off the shelf, thoroughly researched and high-performance techniques
(POS tagging, dependency parsing). Finally, the pattern based extraction rules can be adjusted or extended
by the users of the system according to the needs of
their particular data sets.

3.3. Step2: Ontology building

The ontology building step collects the results of the
pattern based extraction. Noun phrases are a basis for
deriving a data structure hierarchy and the functionality information is used for building a functionality
hierarchy. We employ the lemma (i.e., base form) of
the extracted terms for ontology building.

3.3.1. Building the data structure hierarchy

We observed that many of the terms mentioned in the
analyzed corpora (and especially in the bioinformatics
corpus) have a high level of compositionality, in the
sense that they incorporate other meaningful terms as
proper substrings. Our observation is confirmed by a
recent study of the Gene Ontology terms which proved

M. Sabou et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3 (2005) 340365

either of these modelling styles, i.e., creating verbnoun phrase (e.g., Delete SequenceSection) or only
verb (e.g., Deleting) based concepts.

3.4. Step3: Ontology pruning

The first two main steps of the framework, term
extraction and ontology building, result in an initial
ontology. These steps only rely on our initial heuristics
to select the potential concepts. However, even if they
capture strong sublanguage characteristics, our heuristics are not perfect and some of the derived concepts
are not domain relevant. The pruning module filters out
these irrelevant concepts.

Maedche describes two major strategies for performing ontology pruning [33]. First, the baseline
pruning strategy is based on the assumption that frequent terms in a corpus are likely to denote domain
concepts. Conversely, concepts that are based on low
frequency terms should be eliminated from the ontol-
ogy. The second pruning strategy, relative pruning, is
based both on the frequency of the terms in the analyzed corpus and in an independent reference corpus.
Only concepts that are frequent in both analyzed and
relative corpora are maintained in the ontology.

We use a baseline pruning strategy in our current
implementations. We consider the average frequency
(Freq) of the n learned concepts as a threshold value
and prune all concepts that have a lower frequency than
this value.
Freq =

freq(concepti)

i=1

Another heuristic for the pruning is based on the observation that noun phrases included within a functionality annotation by our rules are more likely to denote
domain concepts. Therefore, if a low frequency data
structure concepts lexicalization is identified within a
functionality annotation and the corresponding Functionality concept is not pruned then the data structure
concept is not pruned either.

4. Implementation

We provide two concrete implementations6 of the
above presented framework. One of the goals of our

6 Available at http://www.cs.vu.nl/marta/experiments/extraction.
html.

Fig. 4. The site concept.

that 63.5% of all terms in this domain are compositional
in nature [43]. Another observation, also proved by this
study, is that compositionality indicates the existence
of a semantic relationship between terms. If a term t1
is obtained by adding a modifier to another term t2
then t1 is more specific than t2. This translates in the
ontological subsumption relationship.

The hierarchy building algorithm reflects these
observations. If a concept As lexicalization is a proper
substring ending another concept Bs lexicalization
(e.g., Site in AntigenicSite) then A is more generic than
B and the corresponding sub-sumption relationship is
added to the ontology. Also, if the lexicalizations of
two concepts B and C end with the same substring we
speculate that this substring represents a valid domain
concept (even if it does not appear as a stand alone term
in the corpus) and add it as a parent concept for B and
C. As an example, Fig. 4 depicts the Site data structure hierarchy. Such compositionality based hierarchy
building has also been used in other ontology learning
approaches [7,58].

3.3.2. Building the functionality hierarchy

There are no clear guidelines in the field of semantic Web services about how functionality hierarchies
should look like. The OWL-S/IRS [40]/WSMO5 style
of modelling functionalities includes both the verb of
the action and a directly involved data element in the
functionality (e.g., BookTicket). This modelling style
was followed in case study 1 (see Fig. 1). On the
other hand, in the bioinformatics domain ontology
developed in case study 2, functionalities are concepts
denoting action (e.g., Aligning) without any connection
to the data structures (see Fig. 2). We provide modules that produce functionality hierarchies fulfilling

5 http://www.wsmo.org/.

implementation was to ensure a high usability of the
extraction tool. We achieved this in two ways. First, we
aimed for a modular, easy to run and understand implementation that can be easily modified and adapted to
new situations. We achieved this by using the intuitive
user interface offered by the GATE framework. Second,
we used visual techniques to improve the presentation
of the learned ontology. Further, we discuss these two
aspects.

4.1. Using GATE for implementation

Many of the off the shelf techniques on which
our framework relies were readily offered by GATE.
For example,
the Linguistic Analysis step of the
M POS implementation was entirely performed
using processing resources offered by GATE: a
tokenizer (ANNIE English Tokenizer), a sentence
splitter (ANNIE Sentence Splitter) and the Hepple
POS tagger [27] (available as the ANNIE POS Tagger
processing resource) (Fig. 5). In the case of M DEP
we performed the linguistic preprocessing external to
GATE. For both approaches, the Extraction patterns
were implemented using the JAPE regular expression
based rule mechanism which is part of GATE. The
two steps are jointly performed by a single
final
module
(OntologyBuilding&Pruning) which was
implemented as a GATE Processing Resources and
therefore it is usable from the GATE GUI. The data
used by our methods (such as the linguistic information
or the structures identified by patterns) is represented
as annotations on the analyzed texts. Both patterns
and individual modules operate on these annotations
and represent their output as annotations.

We greatly benefitted from the support of GATE
during implementation. To briefly mention the most
important benefits, first, we could reuse several existing
libraries for document management and ontology rep-
resentation. Second, by declaring parts of our method
as GATE Processing Resources and using the offered
annotation based system as a data transfer mechanism
between these parts, we can run and manage our tool
via the GATE GUI. It is now possible (1) to build and
configure modular applications by visually selecting
existing Processing Resources (our own or provided by
GATE), (2) to select different corpora or (3) to inspect
the annotation based output of each processing module.
All these allow easy debugging and make the extraction process transparent to the end users thus increasing
the usability of the tool. Finally, we have used the data
storage and evaluation facilities of GATE during the
development and fine-tuning stages of our prototype.

4.2. Using visualization techniques for ontology
presentation

Another approach to enhance the usability of our
implementation was the use of visual techniques to
present the learned ontology. There is an increasing
awareness in the ontology learning community that the
results of the extraction methods must be easily understandable by the domain engineer who needs to further
refine the extracted ontologies. Therefore, some evidence about the provenance of the learned concepts
or their intended meaning should be supplied. Usability was addressed differently by the existing ontology
learning tools. The importance of an intuitive user
interface was advocated by the developers of ASIUM
[15]. In OntoLearn a natural language description of
the automatically learned formal concepts is generated based on WordNet glosses. These explanations
support the domain expert in evaluating (and under-
standing) the learned concepts [41]. In OntoLT for each
derived concept one can inspect all its appearances
in the analyzed corpus [7]. Text-to-Onto employs a
TouchGraph7 based ontology visualization technique
to depict the extracted concepts and their (taxonomic)
relations [24,36]. While this visualization allows for
easy browsing of the conceptual structure, no explanation is provided of why a certain concept was extracted.

Fig. 5. The GATE implementation of M POS.

7 http://www.touchgraph.com/.

M. Sabou et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3 (2005) 340365

Fig. 6. Visual ontology inspection.

We use the Cluster Map visualization technique,
developed by the Dutch software company Aduna,8
to present the extracted ontologies. The Cluster Map
technique differs from structural visualizations predominantly used in Semantic Web tools by the fact
that besides depicting ontology concepts and their taxonomic relations it visualizes the instances of a number of selected classes from a hierarchy, organized by
their classifications [17]. The highly interactive GUI
in which the Cluster Map is integrated is used for presenting the extracted ontologies. In Fig. 6 a part of the
extracted bioinformatics ontology is shown. The left
pane of the GUI presents the concepts and their hierarchical relations. By selecting a concept, all its instances
(i.e., all the documents from where it was extracted)
are visualized in the right pane. Each small yellow
sphere represents an instance (i.e., one document).
The concepts are represented as rounded rectangles,
stating their names and cardinalities (the cardinality
denotes the number of documents from where the concept was extracted). Each instance is visualized only
once and balloon-shaped edges connect it to its most

8 http://aduna.biz.

specific concept(s). For example, Fig. 6 shows that one
document lead to extracting three concepts (Finding,
Maintaining, Removing). This document is placed in
between these concepts thus visualizing them close to
each other on the diagram. An attractive property of
this visualization is that visual closeness of concepts
often denotes a semantic closeness.

By using this visualization it is easy to access documents from which a concept was extracteda mouse
click on any cluster results in a list of document
instances. A more important benefit is that by visually
analyzing the extracted concepts and their relations to
the underlying corpus one can derive relations between
them that are not explicitly stated in the corpus and
therefore were not extracted by the learning method.
For example, in Fig. 6 two larger groups of interconnected functionalities emerge. Each group represents
functionalities that are often offered simultaneously
by Web services. At a closer look we observe that
the first group contains functionalities that search or
modify content, while in the second group we find
functionalities concerned with input/output operations
such as Reading or Writing. The domain expert can easily access (with a simple mouse click) and inspect the

documents that interrelate these concepts and decide
if it is the case to set up new abstract concepts (e.g.,
ContentServices and InputOutputServices). For more
examples on the use of information visualization in
the context of ontology learning we refer the interested
reader to [53].

5. Evaluation

In previous work we verified the performance of the
first implementation of the framework (M POS) on the
data sets provided by case study 1 [52] and we evaluated the second implementation (M DEP) in the context of case study 2 [55]. The goal of the experiments
reported in this paper is to compare the performance
of the two extraction methods by applying and evaluating them on data drawn from both case studies.9 The
ontology building algorithm was adjusted to follow the
modelling principle employed by each Gold Standard
ontology (i.e., producing compound functionality concepts for case study 1 and simple action verb based
concepts for case study 2). In order to get an insight in
the efficiency of our pruning heuristics we evaluated
both a pruned (i.e., completing all processing steps
in Fig. 3) and an un-pruned (i.e., excluding the last
processing step from Fig. 3) version of the extracted
ontologies.

Since ontology learning evaluation is a non-trivial
task, we start this section by giving an overview of some
evaluation practices used by the community (Section
5.1). We use a subset of these practices and describe
them in details in Section 5.2. A description of the used
experimental corpora (Section 5.3), our experimental
results (Section 5.4) and an attempted comparison with
other ontology learning tools (Section 5.5) form the rest
of this section.

5.1. Ontology learning evaluation practices

Evaluation of ontology learning is a very important
but largely unsolved issue, as reported by papers in a
recent workshop [6]. Two evaluation stages are typically performed when evaluating an ontology learn-

9 All experimental data (corpora, extracted and gold standard
ontologies) can be downloaded from http://www.cs.vu.nl/marla/
experiments/extraction.html.

ing method. First, term level evaluation assesses the
performance of extracting terms relevant for ontology
learning from the corpus. Naturally, the quality of term
extraction has a direct influence on the quality of the
built ontology. This evaluation can easily be performed
by using the well-established recall/precision metrics.
Second, an ontology quality evaluation stage assesses
the quality of the learned ontology. Two different ontology evaluation approaches were identified by Maedche
[33] depending on what is considered a quality ontol-
ogy.

In an application specific ontology evaluation the
quality of an ontology is directly proportional to the
performance of an application that uses it. Several
papers report on successfully using ontologies in various tasks such as text clustering and classification tasks
[29,4] or information extraction [16]. However, initial
considerations on task-based ontology evaluation are
only reported in [46]. Two problematic issues surface
for such evaluations. First, it is often difficult to asses
the quality of the supported task or the performance of
the application (e.g., search). Second, an experimental
environment needs to be created where no other factors but the ontology influences the performance of the
application.

In a Gold Standard based ontology evaluation the
quality of the ontology is expressed by its similarity
to a manually built Gold Standard ontology. In some
cases the authors use a Gold Standard ontology which
was extracted from different corpora than used by the
learning method [48]. Other authors use Gold Standard
ontologies extracted strictly from the automatically
analyzed corpora [9,11]. One of the difficulties encountered by this approach is that comparing two ontologies
is rather difficult. According to [35], one of the few
works on measuring the similarity between ontologies,
one can compare ontologies at two different levels: lexical and conceptual. Lexical comparison assesses the
similarity between the lexicons (set of labels denoting concepts) of the two ontologies. At the conceptual
level the taxonomic structures and the relations in the
ontologies are compared.

the Gold Standard ontology contains all

The Gold Standard evaluation approach assumes
that
the
extractable concepts from a certain corpus and it contains only those. In reality though, Gold Standards
omit many potential concepts in the corpus and introduce concepts from other sources (such as the domain

M. Sabou et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3 (2005) 340365

knowledge of the expert). The evaluation results are
influenced by these imperfections of the Gold Standard.
To compensate for these errors, a concept-per-concept
evaluation by a domain expert can be performed. Such
an evaluation is presented in [42]. Expert evaluation
can be performed also in cases when a Gold Standard
is not available and its construction is too costly just
for the sake of the experiment.

5.2. Chosen evaluation criteria

We employ a combination of these evaluation strategies to assess and compare the quality of the implemented learning methods. We first assess the performance of the term extraction algorithm (marked 1 in
Fig. 7). To evaluate ontology quality, we first rely on the
domain experts concept per concept based evaluation
(2). The domain experts in both case studies are the
curators of the corresponding Gold Standard ontolo-
gies. Then, we compare the extracted ontologies to the
Gold Standard ontologies provided by each case study
(3). In what follows, we present the methodology and
metrics for performing each type of evaluation.

1. Term extraction: This evaluation stage is only concerned with the performance of the term extraction
modules. To measure the performance of these modules we manually identified all the relevant terms
to be extracted from the corpus. Misspelled terms

are not considered for extraction. Then, using the
Benchmark Evaluation Tool offered by GATE, we
compared this set of terms with the ones that were
identified through pattern based extraction. We use
Term Recall (TRecall) to quantify the ratio of (man-
ually classified) relevant terms that are extracted
from the analyzed corpus (correctextracted) over all
terms to be extracted from the corpus (allcorpus).
Term Precision (TPrecision) denotes the ratio of
correctly extracted terms over all extracted terms
(allextracted). We also compute the Fmeasure of the
extraction by assigning an equal importance to both
precision and recall.

TRecall = correctextracted

allcorpus

TPrecision = correctextracted

allextracted

Fmeasure = 2  TPrecision  TRecall
TPrecision + TRecall

2. Expert evaluation: In this evaluation stage the
domain expert performs a concept per concept evaluation of the learned ontology. Ontology precision
represents the percentage of domain relevant concepts in the extracted ontology (OPrecision). We
observed that manually built Gold Standards often
omit several concepts from the corpus or introduce concepts that are not named in the corpus.
Therefore, for this evaluation step, a domain expert
distinguishes between two categories of relevant
concepts: existent in the Gold Standard (correct)
and omitted by the Gold Standard (new). Irrelevant
concepts are marked spurious.
OPrecision =

correct + new

correct + new + spurious

the taxoWe also evaluate the quality of
the numnomic relations. For this we count
ber of taxonomic relations established between
domain relevant (i.e., correct and new) concepts
(allRelsRelevant). Then an expert assesses how many
of these taxonomic relations express indeed an isA
relation (allRelsCorrect). The TaxoPrecision metric
is the ratio of correctly identified isA relations over
all taxonomic relations between domain relevant

Fig. 7. Chosen evaluation strategies.

concepts that were automatically discovered.
TaxoPrecision = allRelsCorrect
allRelsRelevant

The comments of the experts are useful sideresults of the expert evaluation. This qualitative evaluation provided valuable ideas for further
improvements.

3. Ontology comparison: In the final evaluation stage
we compare each extracted ontology to the manually built Gold Standard ontologies in the corresponding domain. For the lexical comparison, our
first metric denotes the shared concepts between
the manual and extracted ontology. This metric was
originally defined in [35] as the relative number of
hits (RelHit), then renamed in [11] to Lexical Overlap (LO). Let LO1 be the set of all domain relevant
extracted concepts (correct and new) and LO2 the
set of all concepts of the Gold Standard. The Lexical
Overlap is the ratio between the number of concepts
shared by both ontologies (i.e., the intersection of
these two sets) and the number of all Gold Standard
concepts (noted all). Intuitively, this metric is equivalent to recall while the previously defined OPrecision represents precision. If two or more correctly
extracted concepts are equivalent to a single concept
in the Gold Standard (e.g., AddModel, LoadOntology are equivalent to AddOntology) then only one
 LO2 contains
of them is counted. Therefore, LO1
only individual concepts (noted correcti).
= correct

LO(O1, O2) = |LO1

 LO2

|LO2

all

The extracted ontology can often bring important
additions to the manual ontology by highlighting concepts that were ignored during its creation. We are not
aware of any previously defined metric for measuring these additions. Therefore, we define Ontological
Improvement (OI) as the ratio between all domain
relevant extracted concepts that are not in the Gold
Standard (noted new) and all the concepts of the Gold
Standard.

OI(O1, O2) = |LO1
\LO2

| = new
|LO2

all

For comparing the taxonomic structures of the Gold
Standard and the extracted ontology we employ a similar strategy as during the Expert Evaluation stage.

We first count the number of taxonomic relations that
were identified between two Gold Standard concepts
(allRelsRelevantGS). Then we count the number of relations that are qualified as isA relations by the Gold
Standard (allRelsCorrectGS). The TaxoPrecisionGS is
the ratio of these two numbers.
TaxoPrecisionsGS = allRelsCorrectGS
allRelsRelevantGS

This taxonomic comparison is simpler than the cotopy
based comparison introduced by Maedche [33]. How-
ever, it is feasible to be performed because the compared hierarchies are not too deep and the overlap
between them is quite small.

5.3. Experimental corpora

The experimental corpora were provided by the two

research projects.

Case study 1: RDF(S) tools: The first corpus,
C RDFS, contains 112 documents extracted from the
documentation of the tools used to build the manual
ontology (51 documents from Jenas API, 37 from the
KAON RDF API and 24 from Sesames API). Each
document in the corpus contains the javadoc description of one method. Previous work showed that the
short textual descriptions of these methods contain the
most information and other javadoc elements such as
the method syntax and the description of the parameters
introduce a lot of noise severely diminishing the performance of the extraction [52]. Therefore, we only use
these short descriptions in these experiments. Also, we
exclude the syntax of the method because it introduces
irrelevant technical terms such as java, com, org.

Case study 2: Bioinformatics services: The corpus for this domain (C BIO) consisted of 158 individual bioinformatics service descriptions as available
at the EMBOSS web site. We worked only on the
short method descriptions since they are significant
for Web service descriptions in general being similar
to descriptions found in online Web service repositories such as XMethods.10 The detailed descriptions of
the EMBOSS services present a specific layout which
makes extraction much easier. However, using it would
have biased our extraction methods towards this particular kind of documentation.

10 http://www.xmethods.net.

M. Sabou et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3 (2005) 340365

Table 4
Term extraction results for both case studies

C RDFS

C BIO

M POS

M DEP

M POS

M DEP

correctextr
allcorpus
allextr

TRecall (%)
TPrecision (%)
F-measure

5.4. Results

In this section we present an evaluation and comparison of the two implementations of the framework. We
ran both implementations on both corpora and used the
evaluation criteria described in Section 5.2 to evaluate
them.

5.4.1. Term extraction

The results of the first evaluation stage (see Table 4)
indicate a better performance of M DEP in both
domains,
the F-measure being higher for M DEP
than for M POS. In each corpus M DEP resulted in
higher recall than M POS even if the precision slightly
decreased. However, in the context of ontology learn-
ing, in general, and for Web service domain ontology
learning in particular, recall is often more important
than precision: the domain experts prefer deleting a few
concepts rather than missing some important concepts.
The errors are mostly due to mistakes in the output of
the linguistic analysis tools. These tools perform worse
on these specialized corpora than on newspaper style
texts used for their initial training. For example, verbs
at the beginning of the sentence are often mistaken for
nouns thus causing a lower recall. It is likely that these
performance values will remain in this range unless
we train the linguistic analysis tools for this specific
sublanguage. Note also that the performance of dependency parsing is sensitive to the length and complexity
of the analyzed texts. Fortunately, the majority of sentences in our corpora are simple and allow a correct
analysis. This partially explains the better performance
of M DEP.

A second source for errors are spelling and punctuation mistakes. The RDF(S) corpus C RDFS has, from
this perspective, a lower quality than the bioinformatics

Table 5
Evaluation results for the RDF(S) domain, case study 1, in the expert
evaluation and ontology comparison phases

Not pruned

Pruned

correct(i)
new
spurious

M POS

29 (23)

OPrecision (%)
LO (%)
OI (%)

M DEP

35 (27)

M POS

M DEP

24 (20)

29 (21)

corpus C BIO and, indeed, this affects the TPrecision
of the extracted set. This leads to the conclusion that
textual sources from Web service catalogs should be
preferable to low quality code documentation.

5.4.2. Expert evaluation

The results of the expert evaluation for the extracted
ontologies are shown in Table 5 (for the RDF(S)
domain) and Table 6 (for the bioinformatics domain).
For both data sets the second method resulted in a
slightly decreased ontology precision as a direct consequence of a lower term extraction precision (more
incorrect extracted terms lead to more incorrect con-
cepts). The pruning mechanism increased the ontology
precision in both domains and for both methods leading to precisions in the range of 57% and 74%. This
means that more than half of the concepts of all pruned
ontologies are relevant for the analyzed domain.

The taxonomic evaluation results (see Tables 7
and 8) show that both methods identified a similar number of taxonomic relations per corpus (18/17
for C RDFS and 78/73 for C BIO). Naturally, C BIO
resulted in more taxonomic relations given the high

Table 6
Evaluation results for the bioinformatics domain, case study 2, in the
expert evaluation and ontology comparison phases

Not pruned

Pruned

M POS

M DEP

M POS

M DEP

correct
new
spurious

OPrecision (%)
LO (%)
OI (%)

Table 7
Taxonomy evaluation results for the RDF(S) domain, case study 1

cision was already high without pruning, we might have
adopted a lower value for the overall pruning threshold.

Not pruned

Pruned

M POS M DEP M POS M DEP

allRelsRelevant
allRelsCorrect
TaxoPrecision (%)
allRelsRelevantGS
allRelsCorrectGS
TaxoPrecisionGS (%)

level of compositionality of bioinformatics concepts.
For both corpora and both extraction methods all
extracted taxonomic relations were correct (TaxoPreci-
sion = 100%). This indicates that the hierarchy building
algorithm that we used, even if simple, performs well.
Note that besides correctly discovering the taxonomic
relations existing in the Gold Standards, many new
taxonomic relations were discovered as well (either
between Gold StandardGold Standard concepts, Gold
Standardnew concepts or newnew concepts).

The effect of the pruning mechanism on the taxonomic structures is different for the two corpora.
Namely,
in C BIO more than half of the correct
taxonomic relations disappear after pruning (unlike
C RDFS where the effect of pruning is not so rad-
ical). One of the major reasons for this behavior is
that, in bioinformatics, due to the compositionality of
terms, deep data structure concept hierarchies are created where the frequency of the concepts decreases with
their generality. These low frequency specialized concepts are often pruned even if important, thus many
taxonomic relations being deleted with them. The pruning threshold should be decreased when advancing
deeper into the hierarchy. Also, since the ontology pre-

Table 8
Taxonomy evaluation results for the bioinformatics domain, case
study 2

Not pruned

Pruned

M POS M DEP M POS M DEP

allRelsRelevant
allRelsCorrect
TaxoPrecision (%)
allRelsRelevantGS
allRelsCorrectGS
TaxoPrecisionGS (%)

5.4.2.1. Qualitative evaluation. Besides our quantitative results, we collected useful comments from the
domain experts who performed the evaluation.

5.4.2.2. Recall versus precision. It seams that
the
cleanness of the ontology is not of major importance
for the ontology engineer. Often even concepts that are
not included in the final ontology are useful to give
an insight in the domain itself and to guide further
abstraction activities. We should therefore concentrate
on increasing the recall of the term extraction process
even at the expense of its precision.

5.4.2.3. Synonymy. During the evaluation, the expert
recognized several potential synonym sets such as:
{find, search, identify, extract, locate, report, scan},
{fetch, retrieve, return}, {pick, select}, {produce, cal-
culate} or {reverse, invert}. Synonymy information is
an important piece of knowledge for semantic Web
services. Especially search and matchmaking algorithms would benefit from knowing which concepts
are equivalent. The ontology engineer can decide to
include synonymy in his ontology in different ways.
For example, he can maintain all these concepts and
describe their synonymy via an explicit mapping (e.g.,
owl:equivalentClass). Alternatively, he can choose to
maintain one single concept per synonym set and link
all lexical synonyms to this concept.

5.4.2.4. Abstractions. The experts often redistributed
the extracted domain concepts according to their
domain view. For example, two subclasses identified
for Protein belong to different domains, molecular biology and bioinformatics, and have to be placed in the
corresponding hierarchies accordingly. Such abstractions need to be still manually created according to the
ontology engineers view on the domain. However, the
abstraction step is considerably supported if the expert
has an overview of relevant domain concepts.

curators

5.4.2.5. Support. The
the
extracted ontologies as a useful start for deriving a
domain ontology. Several complex structures could
be directly included in a final ontology (e.g., the Site
hierarchy in Fig. 4), or provided helpful hints on how

considered

M. Sabou et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3 (2005) 340365

certain concepts interrelate. The most appreciated contribution was that the learned ontologies even suggested new additions for the manually built ontologies.

5.4.3. Ontology comparison

The unpruned RDF(S) ontology extracted with
M DEP contains more than half of the concepts existing in the manually built ontology and many new potential concepts (see Table 5). These values are lower for
the unpruned ontology derived with M POS. Lexical
overlap was computed based on the individual correctly
extracted concepts (correcti) shown within brackets.
The behavior of the pruning mechanism was satisfac-
tory. While pruning has almost doubled the ontology
precision (from 38% to 60% in M POS and from 35%
to 57% in M DEP) it only slightly affected the lexical
overlap. Ontological improvement was more affected
(halved) because many of the newly identified concepts
possibly have a low domain relevance. Therefore, our
pruning distinguishes between important domain concepts and less important concepts.

The comparison with the bioinformatics Gold Standard (the ontology currently used in Web service
description), shows the same trend but registers less
success than the RDF(S) case study (see Table 6). The
unpruned ontologies cover only 2022% of the manual
ontology for both methods, even if they suggest many
new possible concepts. Pruning behavior is less satisfactory in this case: it reduces both the lexical overlap
and the ontology improvement to half while resulting
in a low ontology precision increase. This behavior is
also explained by the fact that low frequency specialized terms are pruned even if important (see Section
5.4.2).

We tried to understand why is the lexical overlap so
low. We concluded that the major cause for ontological losses was that the curator also included concepts
about the fields of biology, bioinformatics and informatics that are not present in the corpus. For this he
relied on his expert knowledge and other ontologies in
the domain (see Section 2). For example, the ontology
contains the 10 level deep organism-primate hierarchy
as well as a hierarchy of measurement units. Further,
the curator relied on a set of compound concepts to
define different views on services. For example, a set
of concepts define different kinds of services based on
their com-positionality or the types of inputs and out-
puts. These concepts that define views represent 18%

of all concepts in the Gold Standard. Our algorithm is
not able to learn such views, however, it is feasible to
extend it in this direction.

We also investigated the causes of the high ontological improvement. Our results suggest that the ontology
curator worked rather independently from the given
corpus during the building of the Gold Standard as
he missed many concepts named in the corpus. Postexperimental interviewing revealed that the examination of the corpus was not meticulous. He used just
in time ontology development: concepts were added
if needed for describing a certain service. Note also
that he worked on a subset of our analyzed corpus
(100 descriptions instead of 158 analyzed by us). Name
changes could also account for the mismatch. The curator expanded abbreviations or created a preferred term
for several synonyms (e.g., Retrieving for fetch, get,
return). In fact, he acknowledged that the automatic
approach leads to a more faithful reflection of the terms
the community uses to describe their services.

5.5. Comparison with other ontology learning
tools

While the primary goal of this research was that of
adapting existing techniques to the Web services context rather than developing new ones, we still attempted
to compare our tool with other existing tools. This task
was hampered by several factors. First, few ontology
learning tools are publicly available for download and
experimentation. We only know about TextToOnto and
OntoLT. However, these tools are essentially ontology learning workbenches that provide a set of generic
techniques and allow their customization to the users
needs. For example, both tools offer a way to encode
domain specific extraction patterns. Therefore, after
tuning these tools to execute our patterns (which are
the elements that make our tool tailored for Web ser-
vices) we expect to have similar results. The differences
would be caused by the performance of the underlying
language processing tools (e.g., all tools use different
POS taggers).

To compensate for not being able to perform a comparison with an existing tool, we searched for performance results that could give an insight in how our tool
compares to others. This is again difficult because many
ontology learning related papers do not report any evaluation results or use metrics that are incompatible with

ours. We found that [11] (and other papers related to
this work) reported on a formal concept analysis based
ontology learning algorithm where the lexical overlap
reached a maximum value of 27.71%. We are aware
that this result is not enough to give an overview of
the state of the art performance in ontology learning.
In fact, the ontology learning community just reached
the stage when an effort is made towards establishing
evaluation benchmarks and metrics to be adopted by
the whole community with the purpose of being able
to compare the different efforts.

6. Discussion

The work presented in this article is motivated by the
observation that, while domain ontologies are of major
importance for semantic Web services, their acquisition
is a time-consuming task which is made more difficult with the increasing number of Web services. The
ultimate goal of our research is to build tool support
for (semi-)automatic ontology learning in the context
of Web services. In this article, we presented the first
stage of the research which pioneered ontology learning for Web services. Ontology learning in the context
of Web services raises several non-trivial questions
while inheriting some unsolved issues from ontology
learning in general. The aim of the work was to better
understand the problem at hand and to investigate what
technologies might be feasible to use. We addressed a
set of fundamental issues such as: data source selection,
choosing the appropriate learning algorithms, deciding
on evaluation methodologies and metrics, considering
usability. The contribution of our work lies in identifying and tackling these issues as well as offering (partial)
solutions to them. We discuss our findings in this section and point out future work that can be based on our
findings so far.

6.1. Selecting data sources

Traditional ontology learning is predominantly
focused on learning ontologies that describe a set of textual documents. In this case the data sources for ontology learning are those textual sources. However, there
are several possible data sources that could be used for
learning Web service ontologies. First, resources connected to the underlying implementation might provide

useful knowledge about the functionality of the Web
service since Web services are simply web accessible
software products. Such are the source code, its textual
documentation or existing UML diagrams. Second, one
could use Web service specific data sources such as
associated WSDL files or activity logs.

During our work, we observed that Web services
are almost always accompanied by a short textual
description of their functionality which helps a user
to quickly understand what the service does. Such
descriptions exist in on-line Web service repositories
such as XMethods and also in javadoc style code docu-
mentations. Besides being the most available sources,
short textual descriptions of Web services (1) are characterized by a low grammatical quality and (2) use a
specific sublanguage that makes them amendable to
automatic processing. In our work we only considered
these textual documentations. Current experiments, not
reported here, show that WSDL documents also contain valuable information about the service that they
describe being more detailed than the short textual
descriptions we have considered. As future work, we
will combine these two sources.

6.2. Choosing learning techniques

The goal of our work was to adapt existing ontology learning techniques for this specific domain rather
than developing novel ones. The choice of these techniques depends on the kind of data sources considered.
For example, UML diagrams would require semantic
mapping techniques [14] which are essentially different
from natural language processing techniques used for
textual sources. We designed a framework for learning
ontologies from textual Web service descriptions and
implemented two methods within this framework that
use natural language processing techniques of different complexity. During the design and evaluation of
this framework we derived the following observations:
 Simple techniques work fine in well-defined contexts.
Despite our methods are based on relatively simple,
off the shelf term extraction and ontology building techniques, the learned ontologies have a good
quality (as we argue in the evaluation part of this dis-
cussion). One explanation of this phenomenon is that
we are considering a well-defined ontology learning
task and work on specialized texts with strong sub-

M. Sabou et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3 (2005) 340365

language characteristics. Our context differs from
efforts to design generic ontology learning methods
which have to run on any kind of textual sources
and build only generic ontological structures. There-
fore, generic ontology learning methods are harder to
build and they rely on more complex techniques. We
believe that since Semantic Web technology is used
in a variety of specialized domains, tools that allow
easy adaptation of basic ontology learning methods
will have an increased practical importance. We consider this work as a first step towards context directed
ontology learning.
 Deeper linguistic analysis seems to increase perfor-
mance. The dependency relationship based method
performs better than the POS tag based method.
First, it increases the recall of the term extraction
from the corpus with little impact on the term extraction precision. Second, while the extracted ontologies have a lower precision this is compensated by
higher values for ontological overlap and improvement (but experts consider precision less important
than domain coverage). Another argument for the
use of dependency parsing is that the richer dependency information makes it much easier to write and
establish new syntactic patterns than surface ones.
 It should be possible to build a domain independent tool. The sublanguage features on which our
methods build can be identified in Web service
descriptions written for various domains. Therefore,
we believe that it is feasible to build an ontology
learning tool that is tailored to the context of Web
service descriptions but which is applicable across
different application domains. A good indication is
that both our methods perform similarly in two different domains. However, corpus particularities can
influence the extraction performance. For example,
punctuation and spelling mistakes lead to a low term
extraction precision, and consequently, to a less precise ontology.

We envision several improvements for the basic
framework presented here. First, we wish to extend the
method with more fine-grained extraction patterns to
complement the current high coverage patterns. There
are a considerable number of sublanguage patterns that
were not used in this iteration but could provide rich
input for the ontology building. We also wish to exploit
pronoun resolution and prepositional phrases. Machine

learning techniques could help in discovering some
new patterns. It is interesting to investigate if these
fine-grained lexical-based patterns would still make our
framework applicable across different domains. Sec-
ond, we want to enhance the ontology building step.
Use of synonymy information during the conceptualisation step is an important development possibility.
Further, we would like to concentrate on strategies
that enrich the basic extracted ontology. For example,
defining different views on a set of domain concepts
or providing a set of patterns that act on the already
extracted semantic information.

6.3. Evaluation

To achieve the goal of this first stage of research, that
of understanding the applicability of ontology learning
techniques in the context of Web services, our evaluation was directed towards getting an insight in the
performance of the learning methods. A possible extension of our evaluation would be to test the robustness
of the methods, i.e., to see how their performance is
affected when applied on incrementally enlarged data
sets in the same domain. One of the major future tasks
is to perform a task-based evaluation of the extracted
ontologies in a Web service context, e.g., by powering Web service tasks such as search, matchmaking,
etc. This would complement the current evaluation and
indicate the appropriateness of the learned ontologies
for Web service tasks. However, we believe that the
current evaluation is sufficient to encourage the continuation of this line of work.

One of our major observations is that the evaluation of ontology quality is difficult because the Gold
Standards do not faithfully represent the knowledge
in the corpus: the domain experts omit several concepts because it is not feasible to read and analyze
all available documents in a reasonable time frame.
Complementary, our methods extract ontologies that
contain a high percentage of domain relevant concepts
from a corpus. The amount (and domain relevance)
of extracted concepts can be influenced by tuning the
pruning algorithm.

The quality of the ontologies, even if extracted by
using simple methods, was encouraging. We state this
based on the fact that similar work on ontology learning
in open domain reports on a maximum lexical overlap
of 27.71% while we reach 54% procent in some cases.

Further, during the qualitative evaluation, the experts
indicated that the extracted ontologies represent more
faithfully the knowledge in the corpus and that they
provide a useful start for building a domain ontology.
Indeed, providing ontology curators with ontologies
that contain half of the extractable concepts is a considerable help for this time consuming task.

6.4. Usability

An important lesson from the ontology learning
research is that there are no generic (one-size-fits-all)
methods and therefore any learning method will need
some degree of adaptation to a new domainor even
data sets within the same domain. It is important that the
user of the ontology learning method can understand
(1) each step of the extraction (to eventually adapt it
to his needs) and (2) the learned ontology. To fulfill
the first requirement we used the GATE framework to
create a modular, easy to adapt implementation which
gives insight in the working of each extraction mod-
ule. The second objective was reached by presenting
the extracted ontology using visual techniques. Future
work will need to evaluate the efficiency of our usability measures through user based case-studies.

Acknowledgments

This work was carried out in the context of Wonder-
Web, an EU Information Society Technologies (IST)
funded project (EU IST 2001-33052). We thank the
GATE group for their support in the development of
the prototype and G. Mishne, F. van Harmelen and S.
Schlobach for their comments on the earlier versions of
this paper. Finally, we are grateful to the three anonymous reviewers that provided insightful comments for
the improvement of this article.
