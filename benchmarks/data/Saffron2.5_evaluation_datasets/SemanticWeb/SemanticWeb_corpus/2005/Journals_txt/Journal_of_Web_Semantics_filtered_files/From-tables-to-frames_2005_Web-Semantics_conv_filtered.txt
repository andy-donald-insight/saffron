Web Semantics: Science, Services and Agents

on the World Wide Web 3 (2005) 132146

From tables to frames

A. Pivka,b,

, P. Cimianob, Y. Sureb

a Jozef Stefan Institute, Department of Intelligent Systems, Ljubljana, Slovenia

b Institute AIFB, University of Karlsruhe, Karlsruhe, Germany

Received 1 June 2005; accepted 2 June 2005

Abstract

Turning the current Web into a Semantic Web requires automatic approaches for annotation of existing data since manual
approaches will not scale in general. We here present an approach for automatic generation of F-Logic frames out of tables which
subsequently supports the automatic population of ontologies from table-like structures. The approach consists of a methodology,
an accompanying implementation and a thorough evaluation. It is based on a grounded cognitive table model which is stepwise
instantiated by our methodology.
 2005 Published by Elsevier B.V.

Keywords: Table structure; Table modeling; Knowledge frame; Ontology generation; Web mining

1. Introduction

Turning the current Web into a Semantic Web
requires automatic approaches for annotation of
existing data since manual annotation approaches such
as presented in [9] will not scale in general. More
scalable (semi-)automatic approaches known from
ontology learning (cf. [16]) deal with extraction of
ontologies from natural language texts. However, a
large amount of data is stored in tables which require
additional efforts.


Corresponding author. Tel.: +386 1 447 3380;

fax: +386 1 425 1038.

E-mail addresses: aleksander.pivk@ijs.si (A. Pivk),

cimiano@aifb.uni-karlsruhe.de (P. Cimiano),
sure@aifb.uni-karlsruhe.de (Y. Sure).
URL: http://dis.ijs.si/sandi.

1570-8268/$  see front matter  2005 Published by Elsevier B.V.
doi:10.1016/j.websem.2005.06.003

We here present an approach for automatic generation of F-Logic frames [14] out of tables which
subsequently supports the automatic population of
ontologies from table-like structures. Even successful
search engines on the Web currently do not make the
content of tables searchable to users. Applying our
approach e.g. allows for querying over a heterogeneous
set of table-like structures.

Our approach consists of a methodology, an accompanying implementation and a thorough evaluation. It
is based on a grounded cognitive table model which is
stepwise instantiated by our methodology. In practice
it is hard to cover every existing type of a table. We
identified a couple of most relevant table types which
were used in the experimental setting during the
evaluation of our approach.

In this paper we use HTML tables as examples.
We would like to point out that the methodology is in

Our approach builds on the model described above.
However, we will not consider the graphical dimension
as no image processing will be necessary. Regarding
the physical dimension, we process the tables encoded
in HTML format in order to get a physical model
of the table. In principle it can be seen as a graph
describing the cells being connected together. In order
to capture the structural dimension of the table, further
processing is necessary (i) to determine the orientation
of the table, i.e. top-to-down or left-to-right, and (ii) to
discover groups of cells building logical units. When
talking about the function of a table, Hurst [12] distinguishes between two functional cell types access and
data. Cells of type data are the ones we are interested
when reading a table and which contain the actual
information, while cells of type access determine the
path to follow in the table in order to find the data cell
of interest. Further, he distinguishes local (looking
for one specific data cell) from global (comparing the
value of different data cells) search in a table. In our
approach we describe the functional dimension of a
table in order to support local search. Such a functional
description requires (i) finding all the data cells in a
table as well as (ii) all the access cells to reach a given
data cell of interest. In terms of database terminology,
we need to find the keys for a certain field in the table.
In our approach we distinguish between two functional
types of cells: A(ttribute)-cells and I(nstance)-cells.
A-cells describe the conceptual nature of the instances
in a column or a row. I-cells represent instances of the
concepts represented by a certain A-cell. I-cells can
have the two functional roles described by Hurst, i.e.
they can play the role of data or access cells.

Regarding the semantic description we follow a
different paradigm as Hurst. Instead of adopting the
relational model [2], we describe the semantics of a
table in terms of F-Logic frames [14]. F-Logic combines the intuitiveness of modeling with frames with
the expressive power of logic. Furthermore, existing
F-Logic inference engines such as Ontobroker [5]
allow later on e.g. for processing and query answering.
Therefore it was our primary choice as representation
language.

We briefly introduce our running example. As input
we use table in Fig. 1 which is taken from the tourism
domain and is (roughly speaking) about room prices.
The ideal description in terms of an F-Logic frame of
this table, i.e. the output after applying our approach,

Fig. 1. Example of a possible table, found in [1].

general independent of the incoming document type
(i.e. text, pdf, excel) and can be applied to any table
equivalent structure. The implementation is almost
as generic as the methodology. To apply it for other
formats than HTML one would only need to adapt the
implementation for the first methodological step (cf.
Fig. 2).

The paper is structured as follows. In the next Section 2 we first introduce the grounding table model
which forms the base for our stepwise approach to generate frames out of tables. Subsequently we explain
each step in detail and show relevant substeps. In Section 3 we present a thorough evaluation of the accompanying implementation. Before concluding and giving
future directions, we present related work.

2. Methodological approach

Linguistic models traditionally describe natural language in terms of syntax, semantics and pragmatics.
There also exist models to describe tables in similar
ways (cf. [11,12]) where tables are analyzed along
the following dimensions: (i) graphicalthe image
level description of the pixels, lines and text or other
content areas, (ii) physicalthe description of intercell relative location, (iii) structuralthe organization
of cells as an indicator of their navigational relation-
ship, (iv) functionalthe purpose of areas of the tables in terms of data access, and (v) semanticthe
meaning of text in the table and the relationship between the interpretation of cell content, the meaning of structure in the table and the meaning of its
reading.

A. Pivk et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3 (2005) 132146

could look as follows:
Tour[TourCode => ALPHANUMERIC;

ValidityPeriod => DATE;
Price(PersonType,RoomType,PriceType) => LARGE NUMBER].
By resorting to F-Logic we are thus able to describe
the semantics of tables in a model-theoretic way.
Furthermore, as required by Hurst, the frame makes
explicit: (i) the meaning of cell contents, (ii) the functional dimension of the table, and (iii) the meaning
of the table abstracting from its particular structure.
In this line, different tables with different structures
but identical meaning would be described by one and
the same frame. In what follows we describe how
we process the table in order to yield intermediate
descriptions of a table along the dimensions described
above as well as how as a last step the table is translated
into an F-Logic frame.

As depicted in Fig. 2 our methodology consists of
four main steps. For each building block of the table model there exists a corresponding methodological
step to create this part of the table model. In the following subsections we will describe all steps in detail.

2.1. Cleaning and normalization

Web documents are often very noisy in a sense that
their syntactic structure is incorrect. Web browsers (i.e.
Opera) are capable of dealing with such poorly struc-
tured, inconsistent, and incomplete documents. In order to remove this noise we perform the following two
steps: (a) syntactic correction/cleaning, and (b) normal-
ization.

First, we assume that documents are represented
with the DOM1 (Document Object Model). A DOM
tree is an ordered tree, where each node is either an
element or a text node. An element node includes an
ordered list of zero to many child nodes, and contains
a string-valued tag (such as table, h1 or title) and zero
to many string-valued attributes (such as href or src).
A text node normally contains a single text string and
has no child nodes.

In the two substeps we construct an initial table
model out of an updated DOM tree. In order to clean
the code and make it syntactically correct, we employ
the CyberNeko HTML Parser.2 The normalization step

1 http://www.w3.org/DOM/.
2 http://www.apache.org/andyc/neko/doc/html.

Fig. 2. Building blocks of the methodology.

of the rendered table element node in the DOM tree is
only necessary, when an explicit child nodes attribute,
such as rowspan or colspan, indicates multiple row or
column spanning cells, where the actual total number
of rows or columns is lower than the attribute value. In
both steps our system updates the corresponding DOM
subtrees accordingly.

Table in Fig. 3 shows the final reformulation of the
example in table in Fig. 1, where cleaning has been
performed and copies of cells with rowspan and colspan
attributes have been properly inserted into the matrix
structure.

2.2. Structure detection

2.2.1. Assignment of functional types and
probabilities to cells

In the initial pass over the table element node of the
DOM tree, we convert a subtree into a matrix structure,
which is populated by cells according to its layout information as shown in Fig. 3. During this step the text
of each cell is tokenized, and each token is assigned a
token type according to the hierarchy tree leaves presented in Fig. 4. At the same time, we assign each cell
in the rendered table a functional type and the probability for the type. By default, a cell is assigned either
no functional type (probability value equals zero), or
I-cell type, in case it includes only/mostly tokens, recognized as dates, currencies, or numerical values. Its
probability is then calculated based on the portion of
these relevant tokens. Finally, we assume that the cell in
the lower-right corner is always an I-cell, and the cell in
the upper-left corner is an A-cell. Therefore we assign
those two cells the types, regardless of their content,
with probability one.

In the structure detection block several assump-
tions, regarding languages and reading orientations,
are made. These assumptions are based on the fact
that the prototypical implementation can deal only
with documents encoded in (western) languages, that

Fig. 3. Table, shown in Fig. 1, after cleaning and normalization step.

read from left-to-right, i.e. English. Eastern languages,
reading from right-to-left, are not considered here,
but it is straightforward to extend our approach to
properly cover them. In particular, each extracted
table from a document encoded in such a language
is altered into the matrix structure, which requires an
additional transformation, i.e. mirroring, rotation, or
transposition, depending on the language in question.
These transformations enable a suitable representation
that our approach can handle in the following steps.

2.2.2. Detecting table orientation

One problem related to the interpretation of a
table is that its logical orientation is a priori not
clear. In fact, when performing a local search on a
table, the data of interest can be either ordered in
a top-to-down (vertical orientation) or left-to-right
manner (horizontal orientation). For example, in Fig. 1
the relationship Tour Code, DP9LAX01AB reads
left-to-right, but price values of an attribute Economic
Class appear top-to-down. When trying to determine
the table orientation we rely on the similarity of cells.

The intuition here is that, if rows are similar to each
other, then orientation is vertical and on the contrary, if
columns are similar, then interpretation is horizontal.
In order to calculate the differences among rows
and columns of the table, we need first to define how
to calculate the difference between two cells. For this
we represent a cell as a vector c of token types of all
the tokens in the cell. Henceforth, ci will denote the
ith component of the vector c, corresponding to the
token type of the ith token in the cell. Furthermore, |c|
will denote the size of the vector. The token types we
consider are given in Fig. 4. They are ordered hierarchically thus allowing to measure the distance  between
two different types as the number of edges between
them. For example, the numeric type is hierarchically
divided into categories that include range information
about numbers, i.e. large num (10.000), med num
(100  n < 10.000),
small num (<100). This
representation is very flexible and can be extended to
include domain specific information. In particular, the
composed node includes two nodes containing such
information.

Fig. 4. Hierarchy of token types.

A. Pivk et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3 (2005) 132146

Now when comparing the vectors of two cells, we
compare the token types with same indices in case the
vectors have equal length; otherwise, we calculate the
distance for the left-side tokens (tokens aligned at the
head) and for the right-side tokens (tokens aligned at
the tail). The distance is in both cases also normalized.
(cPi , cQi) + (cP|cP|i+1

cells(cP , cQ) :=


i=1

i=1

where u = min(|cP|,|cQ|), v = max(|cP|,|cQ|) and
w = v  u + 1. Now given a table with r rows and
s columns, the total distance (cols) between columns
is calculated by summing up the distance between the
last column and each of the preceding m  1 columns,
where m = min(r, s), i.e.

(cPi , cQi)

cols(colsi, cols)

cols = m1
cols(colp, colq) = r

i=1

i=r

cells(ci,p, ci,q)

where cx,y is the cell in row x and column y, and
:=

r  m + 1

if r > m
otherwise

The total distance (rows) between rows is by analogy
calculated by summing up the distance between the last
row and each of the m  1 preceding rows:

rows = m1
rows(rowp, rowq) = s

i=1

rows(rowri, rowr)

cells(cp,i, cq,i)

i=s

where
:=

s  m + 1

if s > m
otherwise

Here we only compare equal number of rows and
columns, starting at the lower-right corner, thus optimizing the number of comparisons (not all the rows

and columns to the top of the table need to be com-
pared). Finally, to determine the orientation of the
table, we compare both results. If the distance between columns is greater than among rows (cols >
rows), orientation is set to vertical (top-to-down).
On the other hand, if the distance between columns

if u = v

otherwise

(1)

, cQ|cQ|i+1)

is lower than among rows (cols < rows), then orientation is set to horizontal (left-to-right). In the last case,
where the two results are equal, orientation is assigned
the default, i.e. vertical.
In our example, we determine m to be five (m =
min(9, 5) = 5). The orientation is calculated by comparing first m  1 columns to the column m and last
m  1 rows to the last row. The difference among
columns is greater than among the rows, hence the orientation is set to vertical.

2.2.3. Discovery of regions

Definition 1 (Logical unit). A logical unit is a part of a
table produced by a horizontal split in case of vertical
orientation or by a vertical split in case of horizontal
orientation.

Definition 2 (Region). A region is a rectangular area
of a table consisting of cells with the same functional
type. Regions cannot extend over multiple logical
units and can therefore only appear within one logical
unit.

Here we will present a step-by-step algorithm for discovery of regions in tables. Pseudocode of the algorithm is given in Fig. 5.

(1) Initialize logical units and regions. Note that a table is by definition one logical unit. First, the system tries to split a table into several logical units.
In particular, when table orientation is columnwise (vertical), the horizontal split is done at every
row containing cells spanning multiple columns,

(2)

(3)

(4)

(5)

(6)

(7)

over data strings within regions, which, at the end
of table transformation, reflects a generalization
of possible concept instance values.

The patterns are of two types: the first represents the content of cells from left-to-right
(forward) and the second from right-to-left
(backward). The pattern FIRST UPPER Room
for example covers the cells Single Room and
Double Room. In our example, the coverage of
this pattern is 1/3 for the region extending over the
last six rows of the third column. For the purpose
of pattern construction we have implemented the
DATAPROG algorithm, which is described in [15]
together with a detailed pattern learning process.
In case there are not enough examples (less than
20) to statistically choose the most significant
patterns, only the most specific (having their
coverage over the threshold value) are chosen.

Before entering the loop (see algorithm in
Fig. 5), the system checks the uniformity of every
logical unit. In our case, a logical unit is uniform
when it consists of logical sub-units and each
sub-unit includes only regions of the same size
and orientation. Only the units that are not uniform
are further processed within the following steps
of the loop.

(3) Select the best coherent region. The best region
is used to propagate and normalize neighboring
(non-uniform)
regions and consequently the
logical unit itself. The best region rmax is selected
= maxr in l r,l,
according to the formula rmax
which is calculated by the following equation:

r,l : =


|r|
|l| + P(r)

(9)

|r|  |Pr|

covers(p, r)

pPr

where l denotes a logical unit, r denotes a region in
the unit, c denotes cells in the region, and Pr is the
set of significant string (forward and backward)
patterns for the region as described above. The
function covers(p, r) returns a number of cells
covered by pattern p in region r. According to the
above formula, the selected region maximizes the
sum of averaged region size (1st operand of the

Fig. 5. Algorithm for discovery of regions.

or when dealing with row-wise (horizontal) orien-
tation, vertical split is done at every column containing cells spanning multiple rows. Consecutive
logical units may then be merged if their layout
structure is equal. Over-spanning cells of type I-
cell also represent a sign for a split.

For example, table in Fig. 1 has three logical
units. The first logical unit is extending over the
first two rows, the second one over the third row,
and the third one over the rest of the table. The
first two rows have an over-spanning cell with
functional type I-cell and are grouped into one
logical unit because their layout structure is equal.
A third row has a cell spanning multiple columns,
and the rest is grouped into one logical unit.

Once splitting is over, the region initialization
step begins. The system starts at a lower-right
corner and moves according to its orientation towards upper-left corner over all logical units, thus
generating all distinct initial regions. The cell cN
is added to a region r if the following conditions
apply (otherwise a new region is created):
(a) the cell cN is within the same logical unit as

other cells in r,

(b) its size is equal to the size of cells already in

r, and

(c) it keeps the distance among cells in r within

a threshold value:
cells(cN , c1(r))  2
where the value of 2 reflects a significant
token type change according to the hierarchy
in Fig. 4.

(8)

(2) Learn string patterns for regions. For each region r
we learn a set Pr of significant patterns, which are
sequences of token types and tokens, describing
the content of a significant number of cells. The
positive property of a pattern is that it generalizes

A. Pivk et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3 (2005) 132146

sum), averaged cell probabilities (2nd operand)
and averaged pattern coverage over a particular
region (3rd operand).

(4) Level neighboring regions within the logical unit.
The intuition here is to use the best region as a
propagator for other regions in their normalization
process. First, the system selects (based on the ori-
entation) all neighboring regions, i.e. those that appear in the same rows (left/right) for column-wise
orientation, or in same columns (up/down) for rowwise orientation. Now, two possibilities exist: (a)
neighboring regions within a common column/row
(orientation dependent) do not extend over the
boundaries of the best region. In this case, the solution is straightforward, because the new region is
extended in a way to cover all common column/row
regions. (b) neighboring regions within a common
column/row do extend over the boundaries of the
best region. In this case, the best region is extended
accordingly, and the whole step repeated.

The logical unit is being processed within the
loop as long as the system is not able to divide
it
into logical sub-units, where each sub-unit
includes only regions of the same size and orientation (uniformity condition). Note that string
patterns, probabilities and functional
types of
normalized regions are also updated in every itera-
tion. Finally, in this way all logical units are being
normalized and prepared for further processing.

2.3. Building of a functional table model

The key step of translating a table into a frame is
building a model of the functional dimension of the
table. This model is called Functional Table Model
(FTM) and essentially arranges regions of the table in
a tree, whereby the leaves of the tree are all the regions
consisting exclusively of I-cells. Most importantly, in
the FTM these leaves are assigned their functional role,
i.e. access or data, and semantic labels as described in
Section 2.4.1.

The construction of the FTM proceeds bottom up:
we start with the lowest logical unit in the table and
proceed with further logical units towards the top. For
each logical unit in question we first determine its type.
There are three possibilities: (a) the logical unit consists only of A-cells, in which case all its regions will be
turned into inner nodes of the tree and thus connected

to some other nodes in the tree, (b) it consists only of
I-cells, in which case they will constitute leaves and
will be connected to appropriate inner nodes, and (c)
it consists of I-cells and A-cells, in which case we determine the logical separation between them by taking
the uniformity condition into account.

In some cases a special connection node (see Fig. 6)
needs to be inserted into the tree. This occurs when
we encounter a logical unit that reflects a split in the ta-
ble, in particular when a previous logical unit contained
only A-cells, but the present logical unit again contains
I-cells. In such cases, we check (described later in this
paragraph) if reading orientation of the present logical
unit is different from the previous one and needs to be
changed. If this is true, the logical unit needs to be re-
calculated, as described in Section 2.2.3. For example,
the first logical unit (first two rows) in table in Fig. 1
has four regions (each logical cell) and there is no
logical unit on top of it. So, if the orientation was vertical (i.e. like in lower logical unit), there would be no
inner node (consisting of A-cells) to connect the I-cells
to. Thus orientation has to be changed from vertical to
horizontal for this logical unit.

As already mentioned above, each region in a leaf
position is assigned its corresponding functional role.
The role access is assigned to all consecutive regions
(starting at the left subnodes of a subtree) together
forming a unique identifier or key in the database ter-
minology. The rest of the leaf nodes in the subtree get
assigned the role data.

When all logical units have been processed, we connect the remaining unconnected nodes to a root node.
For example, the FTM constructed out of our running
example is depicted in Fig. 6.

After the FTM is constructed, we examine if there
are any multi-level (at least two levels of inner A-cell
nodes) subtrees that might be merged. The candidate
subtrees for merging must have the same tree structure (same number of levels and nodes on each level)
and at least one level of matching A-cells. If there are
any candidates that fulfill the requirements, we perform a process called recapitulation, where we merge
the nodes at same positions in both subtrees. As we
only require one level of matching A-cells, there might
be some A-cells that do not match. For every such case,
the following steps are taken: (a) find a semantic label
of a merged A-cell node (described in Section 2.4.1),
(b) connect the merged A-cell to a new leaf node,

Fig. 6. The final Functional Table Model (FTM) of the running example (table in Fig. 1) where square components represent I-cells and rounded
A-cells.

which is populated by the A-cell contents of merged
nodes, and (c) assign the functional role of the new
leaf node to access. In this way we check and merge
all possible multi-level subtrees of an FTM and finalize the construction process. In our case, two nodes
are merged into the Extension node, hence a new leaf
node is created with functional role access and two
respective values, i.e. Economic and Extended, as
obvious from Fig. 6.

2.4. Semantic enriching of the functional table
model

2.4.1. Discovery of semantic labels

In order to find semantic labels for each table region
(node), we resort to the WordNet lexical ontology [8]
to find an appropriate hypernym covering all tokens in
the cells contained in the region. Furthermore, we also
make use of the GoogleSets3 service to find synonyms
for certain tokens. For example, the left-most leaf in
Fig. 6 consists of tokens adult and child, for which
WordNet suggests the hypernym person. However, the
tokens are not always so pure, therefore we stepwise
remove words in the cells by the following transformations and consult WordNet after each step to yield a
suitable hypernym:

3 http://labs.google.com/sets.

(1) punctuation removal,
(2) stopword removal,
(3) compute the IDF measure (where the documents
are cells in our case) for each word and filter out
the ones with value lower than the threshold,

(4) select words that appear at the end of the cells as

they are more significant,4

(5) query GoogleSets with the remaining words in order to filter words which are not mutually similar.

The process of semantic label discovery prototypically covers English language only, but the extensions
for other languages can be easily incorporated. Each
language extension should include its formalized lexical knowledge, and should modify the existing approach according to the language properties.

2.4.2. Map functional table models into frames

In order to define how to transform an FTM into a
frame, we first give a formal definition of a method and
a frame:
Definition 3 (Method). A method is a tuple M :=
(nameM , rangeM , PM), where (i) nameM is the name of
the method, (ii) rangeM is a string describing the range

4 The intuition here is that for nominal compounds the nominal

head is at the end.

A. Pivk et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3 (2005) 132146

of the method and (iii) PM is a set of strings describing
the parameters of the method.

Price(PersonType, RoomType) 
The method
NUMBER would for example be formally represented as the tuple (Price,NUMBER,{PersonType,
RoomType}). Further, a frame is defined as follows:
(Frame). A Frame F is a pair F :=
Definition 4
(nameF , MF ) where nameF is the name of the frame
and MF is a set of methods as described above.

Now when generating a frame, we create one
method m for every region with functional role data
with all the regions of type access as parameters of
this method. This parameters must either be located on
the same level within the same subtree or on a parent
path to the root node. Here it is crucial to find appropriate names for the method (nameM) and parameter
identifiers p  PM, which was described in a previous
section. The semantic label for each identifier is a combination of a region label and parent A-cell node labels,
as can be observed from bold labels of the final FTM,
shown in Fig. 6. For better understanding, compare the
FTM tree and the example of the generated frame given
below. Further, we also set the range rangeM of the
method m to the syntactic token type of the region with
functional role data for which the method was gener-
ated. Finally, the frame for the running example, generated by the system, looks as follows:
Tour [Code => ALPHANUMERIC;

DateValid => DATE;
Extension (PersonClass,PriceClass,RoomClass,ExtensionType) =>

NUMBER].

3. Evaluation

In order to evaluate our approach, we compare the
automatically generated frames with frames manually
created by two different subjects in terms of Preci-
sion, Recall and F-Measure. In particular, we considered 21 different tables in our experiment and asked
14 subjects to manually create a frame for three different tables such that each table in our dataset was
annotated by two different subjects with the appropriate frame (14  3 = 21  2 = 42). In what follows we first describe the dataset used in the exper-

iments. Then we describe the evaluation methodology and present the actual results of the experiment.
The definition of the task as well as the instructions
for the annotators can be found at http://www.aifb.uni-
karlsruhe.de/WBS/pci/FromTables2Frames.ps.

3.1. Table classes

We have identified three major table classes according to their layout that appear frequently on the web:
1-Dimensional (1D), 2-Dimensional (2D), and Complex tables. The first two classes are more simple and
also appear more often compared to the last class. A
similar classification into classes has also been introduced in [24].

1-Dimensional tables: This class of tables has at
least one row of A-cells above the rows of I-cells. If
there are more than one row of A-cells then we assume
that they are hierarchically connected. The content of
the I-cells in different columns represent instances of
the A-cells above. An example of this type is given in
Fig. 7(a).

2-Dimensional tables: This class has a rectangular
area of I-cells appearing within columns. This class has
at least one row of A-cells above the rectangular area,
and at least one column of A-cells on the left side of
the rectangular area. Discovering and handling of this
class is hard as it is difficult for a system (without any
other knowledge) to decide if the first column consists
of A-cells or I-cells. Our solution here is to interpret
the leading column as A-cells only if its first row cell
is a non-spanning cell with an empty label or a label
containing a character /. An example for this type of
table is given in Fig. 7(b).

Complex tables: This class of tables shows a great
variety in layout structure. Therefore a table might have
the following features:
 Partition data labels: Special over-spanning data
labels between the data and/or attribute labels can
make several partitions of the table. Each partition
shares the same attributes, such as in Fig. 7(c). In
this case the relation among attribute and data value
cannot be obtained directly.
 Over-expanded labels: Some entries might expand
over multiple cells. There are two options: (a) data
values span over multiple rows in the same column
or (b) an attribute label spans over multiple columns.

Fig. 7. Examples of tables.

An example of this class is the part of table in Fig. 1
consisting in the lower seven rows.
 Combination: Large tables might consist of several
smaller, simpler ones. For example, table in Fig. 1
consists of two structurally independent tables.

In our experiment, we have gathered 21 tables, each
belonging to at least one class. Since the first two
classes are a bit less interesting, we used only three
different tables for each class, but for each complex
subclass we used five different tables. All tables were
gathered from two distinctive sources: one from tourist
domain and another from a source dealing with food
research. Domains were quite different, and also tables
were selected from different sources in a way that their
distribution over classes is uniform.

3.2. Evaluation methodology

We evaluated our approach by considering the
well-known information retrieval measures Precision,

Recall and F-Measure. In particular, for each table we
evaluated the frame automatically generated for it by
the system with respect to the two frames manually
created by two different subjects along the following
lines: Syntactic Correctness, Strict Comparison, Soft
Comparison, Conceptual Comparison.

In order to assess how similar two strings are,
we will
introduce a string comparison operator
 : String  String  [0 . . . 1]. In particular, in our
evaluation we use a string comparison operator based
on a combination of a TFIDF weighting scheme with
the Jaro-Winkler string-distance scheme. Cohen et al.
[3] showed that the operator produces good results in
such tasks.

The Syntactic Correctness measures how well the
frame captures the syntactic structure of the table, i.e.
to what extent the number of arguments matches the
number of parameters as specified by the human annotator for a given method. In what follows we define
three functions Syntactic giving the syntactic correctness between two methods as well as a method and a
frame, respectively.

SyntacticMM(m1, m2) :=

|Pm1
|Pm2

| > 0
| = |Pm2

if |Pm2
if |Pm1
otherwise

| = 0

(10)

SyntacticMF (m, f ) = SyntacticMM(m, m

),

(11)

A. Pivk et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3 (2005) 132146

where m  fM which maximizes 
SyntacticMM(m, m

).

(m, m

Note that the above measures are directed; they will
be used in one direction to obtain the Precision and in
the other direction (achieved by replacing object iden-
tifiers) to obtain the Recall of the system.

Strict Evaluation then checks if the identifier for the
method name, the range and the parameters are identi-
cal. We also define a corresponding functions Strict
again defined on two methods and a method and a
frame, respectively:
StrictMM(m1, m2) :=

= namem2

if namem1
otherwise

StrictMF (m, f ) = maxmMf StrictMM(m, m

(12)

(13)

The Soft Evaluation also measures in how far the
identifiers for the method name, the range and the parameters match, but makes use of the string comparison
operator defined above:
SoftMM(m1, m2) = (namem1
SoftMF (m, f ) = max
mMf

, namem2)
SoftMM(m, m

(14)

(15)

Further, we have a modified string comparison 
which returns 1 if the string to compare are equivalent
from a conceptual point of view and  otherwise, i.e.

(s1, s2)


(s1, s2) :=

if s1 and s2 are conceptually

equivalent

otherwise

(16)

The Conceptual measure was introduced to check
in how far the system was able to learn the frame for
a table from a conceptual point of view. In order to
assess this, two of the authors compared the frames
produced by the system and the ones given by the human subjects and determined which identifiers can be
regarded as conceptually equivalent. In this line Re-
gionType, Region and Location can be regarded as conceptual equivalent. Here are the formal definitions of

the corresponding functions:
ConceptualMM(m1, m2) = 

(namem1

, namem2)

(17)

ConceptualMF (m, f )
= maxmMf ConceptualMM(m, m
For all the above measures we compare two frames

(18)

as follows:
XFF (f, f 

) =

mfM

XFF (m, f 
|fM|

(19)

where X stands either for Syntactic, Strict, Soft or Con-
ceptual.

In our evaluation study, we give results for Precision,
Recall and F-Measure between the frame FS produced
by the system and the frames F1, . . . , Fn (in our case
n = 2) produced by the human annotators. In partic-
ular, we will consider the above evaluation functions
Syntactic, Strict, Soft and Conceptual in order to calculate the Precision, Recall and F-Measure of the sys-
tem. Thus, in the following formulas, X stands either
for Syntactic, Strict, Soft or Conceptual:
PrecAvg,X(FS ,{F1, . . . , Fn}) =

1in X(FS , Fi)

And Recall is defined inversely, i.e.
RecAvg,X(FS ,{F1, . . . , Fn}) =

(20)

1in X(Fi, FS)

(21)

Obviously, according to the definitions of the measures,
the following equations hold:
PrecStrict  PrecSoft  PrecConceptual
RecStrict  RecSoft  RecConceptual
Furthermore, we also give the value of the precision and recall for the frame which maximizes these
measures, i.e.
Precmax,X(FS ,{F1, . . . , Fn}) = max

X(FS , Fi)

(22)

(23)

and

And Recall is defined inversely, i.e.
Recmax,X(FS ,{F1, . . . , Fn}) = max

X(Fi, FS)

(24)

Table 1
Results of the different evaluation measures (in %)

Average

Syntactic

Precision
Recall
F-Measure

Strict

Soft

Conceptual

Maximum

Syntactic

Strict

Soft

Conceptual

Obviously, here the following equations hold:
PrecX  Precmax,X and RecX  Recmax,X

(25)

The reason for calculating precision and recall
against the frame given by both annotators which maximizes the measures is that some frames given by the
annotators were not modelled correctly according to
the intuitions of the authors. Thus, by this we avoid
to penalize the system for an answer which is actually correct. As a byproduct of calculating RecallX and
Recallmax,X we can also indirectly judge how good the
agreement between human subjects is.

Finally, as is usual we balance Recall and Precision
against each other by the F-Measure given by the for-
mula:
FX(PX, RX) = 2PXRX
PX + RX

(26)

The system is now evaluated by calculating the
above measures for each automatically generated
frames and the corresponding frames given by the human annotator.

3.2.1. Discussion of results

Table 1 gives the results for the Precision, Recall and
F-Measure as described above. The first interesting observation is that the values for the maximum evaluation
are quite higher than the ones of the average evalua-
tion, which clearly shows that there was a considerable
disagreement between annotators and thus that the task
we are considering is far from being trivial.
The results of the Syntactic comparison are an
F-Measure of Favg,Syntactic = 49.60% for the average evaluation and Fmax,Syntactic = 65.11%. The values show that the system is interpreting the table to
a satisfactory extent from a syntactic point of view,
i.e. it is determining the number of parameters correctly in most of the cases. Regarding the naming
of the methods,
their range and their parameters,
the results vary considerable depending on the mea-

sure in question. For the average evaluation the results are: Favg,Strict = 37.77%, Favg,Soft = 46.27% and
Favg,Conceptual = 57.22%. These results show that the
system has indeed problems to find the appropriate
name for methods, their ranges and their parameters.
However, as the conceptual evaluation shows, most
of the names given by the system are from a conceptual point of view equivalent to the ones given by
the human annotator. For the maximum evaluation we
have: Fmax,Strict = 50.29%, Fmax,Soft = 60.05% and
Fmax,Conceptual = 74.18%. Thus, we can conclude that
from a conceptual point of view the system is getting
an appropriate name in almost 75% of the cases and it
is getting the totally identical name in more than 50%
of the cases. Actually, our system only failed in processing two of the 21 tables, such that in general we
conclude that our results are certainly very promising.

4. Related work

A very recent systematic overview of related work
on table recognition, transformation, and inferences
can be found in [32]. Several conclusions can be drawn
from this survey. Firstly, only few table models have
been described explicitly. Apart from the table model
of Hurst which we applied in our approach [11,12] the
most prominent other model is Wangs [25]. However,
the model of Hurst is better suited for our purpose since
it is targeted towards table recognition and transformation whereas Wang is targeted towards table generation.
A table model for recognition must support two tasks:
the detection of tables, and the decomposition of table
regions into logical structure representation. This models tend to be more complex than generative models,
since they must define and relate additional structure
for recovering the components of generative models
[32].

Secondly, research in table recognition, transfor-
mation, and inferences so far addressed several types

A. Pivk et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3 (2005) 132146

of document encodings. The most work was done on
plain text files, images, OCR and HTML documents
[32]. Work performed on textual tables, OCR documents and images was mainly oriented towards table
recognition [6,10,18,26,29], row labeling [10,17,20],
and cell classification [10,17,20], where the work on
web tables was extended to indexing relation detection [1,22,31] and cell/row/table merging or splitting
[30]. Other approaches aim at the deep understanding
of table structure, applying different techniques such
as cell cohesion measures [13,27], deriving regular expressions [18], edit distance [18], graphs [10,21] as well
shallow parsing of the content. Knowledge engineering techniques employing certain heuristics based on
the formatting cues and machine learning techniques
like decision trees [18,26], Expectation Maximization
[30], Hidden Markov Models [17,26], and conditional
random fields [20] have been previously explored. Table extraction methods have also been applied in the
context of question answering [19,24], and ontology
learning [7,23].

The work done on the task of table detection was
performed by [1,10,27,28]. As evident from this work,
heuristics and machine learning based approaches have
been generally used to perform this task. The documents containing both real tables and tables used for
layout formatting serve as an input to a table detection
system. The table detection task involves separating
tables that contain logical and relational information
from those that are used only for layout purposes. As
the output the system returns tables classified in two
categories: real table and non-real table. Usually this
is a pre-step in table extraction process but it could
also be combined with the extraction algorithm. In con-
trast, we assume that tables are already harvested, and
we provide a methodology and implementation which
completely instantiates a table model and additionally
closes the gap to formal semantics provided by ontolo-
gies.

Chen et al. [1] present work on table detection and
extraction on HTML tables. The table detection algorithm uses string, named entity and number category
similarity to decide if it is a real or non-real table.
Based on cell similarity the table extraction algorithm
identifies whether the table is to be read row wise or
column wise. They split the cells which span over multiple cells into individual cells. The table extraction
algorithm presented in this work is simple and works

only if spanning cells are used for nested labels. The
paper did not provide evaluation results for their table
extraction algorithm.

The problem of merging different tables which are
about the same type of information has been addressed
in [30]. The merging task as defined in this paper considers combining different tables into one large table.
They define different structures for tables based on the
arrangement of the labels and use Expectation Maximization to classify the tables to one of the defined
structures. The structure recognition task is similar
to the classification task. However structure recognition or merging does not solve the table extraction
problem.

Table extraction by wrapper learning has been explored in [4]. Wrappers learn rules based on examples.
The rules are composed of tokens made of HTML tags
or the content itself. The rules tend to be specific and
can be applied only to those documents whose structure
is similar to the training documents. The use of tokens
to compose rules makes it difficult to generalize across
distributed websites. Hence wrappers learned for one
website cannot be used on another website. No clear
evaluation for table extraction has been described in
this work.

Conditional random fields for table extraction from
text tables were described in [20]. However the system
described does not perform a complete table extraction task; it only classifies the rows of the table into
a type such as datarow, sectionheader or super-
header. They used a set of 12 table types (classes) and
achieve a precision of 93.5% for the classification task.
Work presented in [10,17] also focused on the task of
classifying table rows.

Tijerino et al. [23] presented a vision for a system
that would be able to generate ontologies from arbitrary tables or table-equivalents. Their approach consists of a four step methodology which includes table
recognition and decomposition, construction of mini-
ontologies, discovery of inter-ontology mappings, and
merging of mini-ontologies. For the purpose of semantics discovery the approach is multifaceted, meaning
they use all evidence at their disposal (i.e. Wordnet, data
frames, named entities, etc.). Since the paper presents
only a vision, no evaluation is provided.

We conclude thus in this section that our approach
is indeed novel in the sense that it is the first approach
addressing the whole process of transforming a table

into a form reflecting its inherent meaning at a struc-
tural, functional and semantic level. Further, as far as
we know our method is also original in being the first
complete instantiation of a formal table model such as
the one described by Hurst [11,12].

5. Conclusion

We have presented an approach which stepwise instantiates a formal table model consisting of Physical,
Structural, Functional and Semantic components. The
core steps of the methodology are (i) Cleaning and Nor-
malization, (ii) Structure Detection, (iii) Building of
the Functional Table Model (FTM) and (iv) Semantic
Enriching of the FTM. We have further demonstrated
and evaluated the successful automatic generation of
frames from HTML tables. Additionally, our experimental results show that from a conceptual point of
view the system is getting appropriate names for frames
in almost 75% of the cases and it is getting the totally
identical name in more than 50% of the cases. These
results are certainly very promising.

In general, an approach such as presented in this
paper is crucial for the Semantic Web to become fea-
sible. Actually it is a generally accepted fact that without the help of tools reliably harvesting information
from databases, images, tables or texts the Semantic
Web as an extension of the current World Wide Web
is doomed to failure. Though our work can still be improved in many ways, we regard it as a first and important step towards extracting knowledge from table
structures available in the web which brings us a little
bit closer to the vision of a semantic one.

Acknowledgments

This work has been supported by the IST-projects
Dot.Kom (Designing adaptive infOrmation exTraction from text for KnOwledge Management, IST-2001-
34038) and SEKT (Semantically Enabled Knowledge
Technologies, IST-2004-506826), sponsored by the EC
as part of the frameworks V and VI, respectively. During his stay at the AIFB, Aleksander Pivk has been
supported by a Marie Curie Fellowship of the European Community program Host Training Sites and
by the Slovenian Ministry of Education, Science and

Sport. Thanks to all our colleagues for participating in
the evaluation of the system as well as to the reviewers
for useful comments on the paper.
