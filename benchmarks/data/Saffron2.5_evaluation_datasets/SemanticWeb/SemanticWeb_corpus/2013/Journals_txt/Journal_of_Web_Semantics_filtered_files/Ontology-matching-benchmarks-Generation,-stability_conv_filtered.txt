Web Semantics: Science, Services and Agents on the World Wide Web 21 (2013) 3048

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

journal homepage: www.elsevier.com/locate/websem

Ontology matching benchmarks: Generation, stability,
and discriminability
Jerome Euzenat a,, Maria-Elena Rosoiu a, Cassia Trojahn b,1

a INRIA & LIG, Grenoble, France
b IRIT & UTM2, Toulouse, France

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 2 March 2012
Received in revised form
6 May 2013
Accepted 20 May 2013
Available online 27 May 2013

Keywords:
Ontology matching
Matching evaluation
Test generation
Semantic web

1. Introduction

The OAEI Benchmark test set has been used for many years as a main reference to evaluate and compare
ontology matching systems. However, this test set has barely varied since 2004 and has become a
relatively easy task for matchers. In this paper, we present the design of a flexible test generator based
on an extensible set of alterators which may be used programmatically for generating different test
sets from different seed ontologies and different alteration modalities. It has been used for reproducing
Benchmark both with the original seed ontology and with other ontologies. This highlights the remarkable
stability of results over different generations and the preservation of difficulty across seed ontologies,
as well as a systematic bias towards the initial Benchmark test set and the inability of such tests to
identify an overall winning matcher. These were exactly the properties for which Benchmark had been
designed. Furthermore, the generator has been used for providing new test sets aiming at increasing
the difficulty and discriminability of Benchmark. Although difficulty may be easily increased with the
generator, attempts to increase discriminability proved unfruitful. However, efforts towards this goal raise
questions about the very nature of discriminability.

 2013 Elsevier B.V. All rights reserved.

Heterogeneity is inherent to open environments like the
semantic web. Thus, ontology matching is very important to
overcome ontology heterogeneity and many systems have been
proposed for this purpose [1]. However, matching ontologies is not
a deductive task for which it is possible to check if a matcher is
compliant with a specification, like for SPARQL querying or OWL
reasoning. Hence, evaluation of ontology matchers is of paramount
importance.

Evaluating ontology matching systems may be achieved in different ways. The most common one consists of providing matchers
with two ontologies and comparing the returned alignment with a
reference alignment [2]. However, this raises the issue of the choice
of ontologies and the validity of the reference alignments.

Since 2004, the Ontology Alignment Evaluation Initiative
(OAEI)2 makes available a collection of test sets for evaluating

 This paper improves over [15], in particular by providing results based on more
extensive test bases and in generating difficult and discriminant test sets. It has been
 Corresponding author. Tel.: +33 1111111.
thoroughly revised.

E-mail addresses: Jerome.Euzenat@inria.fr (J. Euzenat),

rosoiu.maria@gmail.com (M.-E. Rosoiu), Cassia.Trojahn@irit.fr (C. Trojahn).
1 Work performed when this author was with INRIA.
2 http://oaei.ontologymatching.org/.

1570-8268/$  see front matter  2013 Elsevier B.V. All rights reserved.
http://dx.doi.org/10.1016/j.websem.2013.05.002

matching systems. One such test set is Benchmark (we use the
term Benchmark with a capital B for referring to this test set). It
is a well-defined set of tests in which each test is composed of two
ontologies and a reference alignment. The tests are based on one
particular ontology, from the bibliographic domain, and systematic alterations of this ontology, e.g., removing classes, renaming
properties.

The Benchmark test set was designed with the aim of covering
the various situations in which a matcher may be (called problem
space). However, this test set can be criticised on three main
aspects: (a) lack of realism: tests are mechanically generated,
(b) lack of variability: it always uses the same seed ontology
altered in the exact same way, and (c) lack of discriminability:
the tests are not difficult enough to discriminate well matchers.
While the drawback (a) has been overcome by other test sets made
available by OAEI, in this paper we are particularly interested in
investigating (b) and (c).

To that extent, we have developed a test generator that may
be used with any seed ontology and that allows for fine tuning
the input parameters, as well as randomised modifications over
the ontology entities. The generator framework is extensible  it
is possible to add new alteration capabilities  and flexibleit can
be used through a clearly defined API to generate various kinds of
tests.

This generator enables us to evaluate the relevance of the
Benchmark test set: by reproducing this test set and using it to

evaluate different matchers, under the same conditions, we can
assess how much the obtained results were dependent on the
particular seed ontology or on the particular matcher.

We ran a set of matchers on the generated tests, which sheds
light on the results obtained so far with Benchmark. Concerning
Benchmark, we establish that:
 results obtained by different generations of Benchmark are very
stable, i.e., their standard deviation is less than 1% point on
average;
 the difficulties encountered by a particular matcher at a test are
preserved across seed ontologies, even if the difficulty of each
test set is different;
 a bias towards the original Benchmark and its domain can be
observed;
 it is not possible to identify a general order between matchers
consistent across seed ontologies.

These points support the initial design goals of Benchmark: to be
able to identify strengths of matchers over the whole problem
space and not to find which matcher is the best one. The last point
may however be related to the lack of discriminability of the test
set: matchers obtain very close scores.

Furthermore, we have extended the framework in order to
generate tests of increasing difficulty and discriminability. The
test generator can easily generate test sets of increasing difficulty.
However, such tests do not provide increased discrimination.
Increasing discrimination has proved challenging. This may be
due to two main factors: matchers may be very similar, and
discriminability is not precisely defined. We use the test generator
to discuss and illustrate these issues.
In summary, as main contributions, this paper:
 provides a very versatile benchmark generator (Section 3),
 uses this generator to measure meaningful properties of
ontology matchers and to assess the validity of Benchmark
principles and its stability (Section 4),
 shows how new tests may be generated on the same model for
evaluating scalability, difficulty and discriminability (Section 5).
The remainder of the paper is structured as follows.
In
Section 2, we present the state-of-the-art in ontology matching test
generation. In Section 3, we present the architecture of our test
generator and the strategy to reproduce the Benchmark test set.
In Section 4, we experimentally establish properties of Benchmark
using new generated test sets. Section 5 investigates different ways
to use the test generator in order to generate more difficult and
discriminant benchmarks. Finally, conclusions and future work are
presented in Section 6.

Fig. 1. Test generation (plain lines), ontology matching (dotted) and evaluation
(dashed). The test generator provides two ontologies (o and o) and a reference
alignment (R). The matcher outputs an alignment (A) that the evaluator compares
to the reference alignment in order to compute a measure (m).

put on features of the ontologies, e.g., labels, structure, instances,
or switches indicating which part of the program may be activated,
e.g., for fast computation or for emphasising recall.

In this paper, a matcher is a particular program performing
the ontology matching operation together with its resources and
parameters, so it is always the exact same piece of program run
under the same conditions.
An alignment can be defined as a set of correspondences.
A correspondence between two ontologies o and o is a triple
e, r, e, such that e is an entity belonging to the first ontology,
e is an entity belonging to the second ontology, and r is a relation,
e.g., equivalence or subsumption, between them.

A matcher can be evaluated by comparing its output alignment (A) with a reference alignment (R) using some measure
(Fig. 1, dashed lines). Typical measures are precision, recall and
F-measure [2]. Thus, in order to evaluate a matching system, one
has to generate test sets in which a test is composed of two ontologies to be matched (o and o) and a reference alignment (R). The
test generation considered here starts with a seed ontology (o) for
which the test generator provides both an altered ontology (o) and
the corresponding reference alignment.

In order to abstract from actual ontology matching problems,
we distinguish between the problem space, the solution space
and the alteration space. The problem space corresponds to the
space of various manifestations of ontology heterogeneity which
require matching ontologies. The solution space defines the space
of ontology matching problems that can be solved by matchers.
The alteration space is a space of ontology matching problems
that may be obtained by altering ontologies (as in Fig. 2). We use
the term space because these situations are not unrelated and
their proximity is expected to be meaningful. This is illustrated in
Section 3.4.

2. Ontology matching evaluation and test generation

2.2. The Benchmark test set

In this section, we briefly present the current setting of
ontology matching evaluation (Section 2.1), the Benchmark test
set (Section 2.2) and its limitations (Section 2.3). We survey
the state-of-the-art in alignment test generators (Section 2.4).
The interested reader can find a broader overview of ontology
matching evaluation in [2].

2.1. Evaluating ontology matching systems

Ontology matching [1] can be seen as an operation that takes
as input two ontologies (o and o), a set of parameters, a possibly
empty partial alignment (A) and a set of resources, and outputs
an alignment (A) between these ontologies (Fig. 1, dotted lines).
A resource may be a particular lexicon used for matching terms or
an ontology that may be used as an intermediate ontology between
the ontologies to be matched. Parameters may be different weights

The Benchmark test set aims at assessing the strengths and the
weaknesses of matching systems, depending on the availability of
ontology features, i.e., the availability of instances, properties or
labels in the ontology. This test set has 111 tests, requiring to match
an ontology written in OWL-DL with another one:
 Tests 1xxcompare the original ontology with itself, a random
one and its generalisation in OWL-Lite.
 Tests 2xxcompare the original ontology with the ontology
obtained by applying the following set of modifications to it
(Fig. 2):
 names (naming conventions: synonyms, random strings,

different generalisations, translation into other language),

 comments (no comment),
 hierarchy (flattened hierarchy/expanded hierarchy/no spe-

cialisation),

 instances (no instance),

J. Euzenat et al. / Web Semantics: Science, Services and Agents on the World Wide Web 21 (2013) 3048

Fig. 2. The Benchmark semi-latticethe higher the test in the hierarchy, the easier it is. Dashed lined tests were not generated in the experiments reported here (see
Section 4).

 properties (no property, no restriction),
 classes (flattened classes/expanded classes).
 Test 3xxcompare the original ontology with real ones found
on the web.

Since 2004, Benchmark has been generated from the same seed
ontology through the same set of XSLT stylesheets. This means,
in particular, that no random modification is applied to these
ontologies: the same 20% of classes are renamed and this renaming
is always the same, i.e., editor is always renamed as dzajj. This
has advantages for studying the evolution of the field, because the
test is strictly the same, but it has drawbacks as well, because the
alterations are known in advance by evaluation participants.

2.3. Benchmark limitations

The Benchmark test set can be criticised on three main aspects:

lack of realism Benchmark is not realistic because it covers a
in reality, a

whole systematic alteration space and,
matcher is not faced with such a space.

lack in variability Benchmark is always the same test set, hence it
is not variable. This covers three slightly different kinds of
problems: (a) it can only be used with one seed ontology,
(b) it always applies the same transformations (to the
same entities), instead of applying them randomly, and
(c) it is not flexible in the sense that it is not possible
to produce an arbitrary test (such as 12% renaming, 64%
discarding properties).

lack of discriminability Benchmark seems, in general, easy enough
to OAEI participants so that it does not really allow them
to make progress and it is not discriminant enough to
compare them [3]. This is because many of the proposed
tests are easy and only a few of them are really difficult.
Our goal is to address the variability and discriminability by
producing a test generator (a) independent from the seed ontology,
(b) with random modifications, and (c) which allows for fine
tuning parameters in order to cover the alteration space with any
precision. With such a test generator, it is also possible to generate
different tests than Benchmark, focusing on particular application
profiles or particularly difficult cases.

We do not address the lack of realism because Benchmark has
been designed to cover the problem space and not to offer one
realistic profile. One may argue that we currently consider an
alteration space, instead of a problem space, and that the term
problem space assumes some realism, i.e., that these problems
actually occur. This is right, but this alteration space is our attempt
to cover the problem space, and not to represent the problem space
itself. Other initiatives, such as other tracks of OAEI and other
generators, address the realism issue [4,2].

Some general purpose requirements for semantic web know-
ledge-based system benchmarks are scalability, openness to
new participants, platform independence, meaningful metrics, and
reproducibility [5]. These criteria are those which are targeted by
a new Benchmark generation: having a variable and flexible test
generator allows for scalability and reproducibility (as will be
shown in Section 4); using standardised input and output formats
ensures openness; platform independence is the goal of both the
test generator and the SEALS platform3; finally, we think that
Section 4 shows that Benchmark indeed measures permanent and
meaningful properties of matchers.

Moreover, five properties were found essential (though not
exhaustive, but good enough) for ontology matching evaluation
test sets [3]. These are: complexity: that it is hard for state-of-
the-art matching systems; incrementality: that it is effective in
revealing weaknesses of the state-of-the-art matching systems;
discriminability: that it discriminates sufficiently among the
various matching solutions; monotonicity: that the matching
quality measures calculated on the subsets of the test set do not
differ substantially from the measures calculated on the whole test
set; and correctness: that it can be considered as a correct tool to
support the improvement of matching solutions.

Of those, complexity, incrementality and discriminability resort
to what we identified as lack of difficulty and discriminability. Later
(Section 5), we will consider both discriminability and difficulty
in an incremental way. Monotony is not sufficiently precisely
defined for qualifying. Obviously, if a test set like Benchmark
offers various configurations, it will not be monotonous with any
subset. However, we will show in Section 4 that Benchmark is
indeed monotonous in different ways. Finally, under this informal

3 http://seals-project.eu.

definition of correctness, there are good reasons to consider
Benchmark as correct (it is widely used by matcher developers,
participating or not to OAEI).

Three different properties have also been considered [4]:
consistency: each test should evaluate a particular feature;
completeness: there is a test for any (important) feature; and
minimality: there is no more than one test per feature. As soon as a
combination of alterations can be considered a feature, Benchmark
can be considered as following these rules.

2.4. Related work

Many efforts took inspiration from the original OAEI Benchmark
in generating test sets through alteration. Both IIMB [6] and
ONTOBI [7] benchmarks have been developed for testing instancebased ontology matching systems. On the basis of Benchmarklike tests, they add a large amount of instances based on external
sources (the internet movie database or wikipedia data). These
efforts publish the resulting test set and not the generator.

The Swing approach [8], based on the experience of IIMB, is
a further effort to generate instance matching test sets. Swing
classically distinguishes between the TBox, i.e., the vocabulary, and
the ABox, i.e., the data. It works in two steps: a data acquisition
step collects data to be matched and induces a TBox from this
data. It then adds expressiveness to this TBox both automatically
and manually. In the test generation step, from that ontology
which remains the same over all tests, different ABoxes (test
cases) are generated by altering data values, e.g., token addition,
name style abbreviation, changing data format or number format,
and properties, e.g., adding or suppressing properties, changing
property depth.

In XML schema matching, STBenchmark [9] offers a schema
generator (SGen) which generates tests (pairs of schemas) by
altering a source schema based on the specification of 11 scenarios
(base alterators). It does not generate any reference results. The
scenarios are defined through a set of input parameters which
include the characteristic parameter, e.g., nesting depth, number
of sub-elements, length of joint paths, the standard deviations
to be applied for sampling each characteristic, and a repetition
parameter. Schema instances can also be included in the generated
schemas. Like Benchmark, STBenchmark uses XSLT.

XBenchMatch [10] provides a way for benchmarking schema
matching systems, but is not a test generator. However, it provides
a testbed involving a large schema corpus that can be used to
benchmark new schema matching algorithms. It aims at providing
a systematic way for analysing if a schema matching tool is
appropriate in a given context. XBenchMatch takes as input sets
of correspondences or integrated XML schemas produced by
matchers and applies a set of metrics for evaluating the quality
of the input and performance of the matching tool, for instance.
XBenchMatch can be extended programmatically.

Another automatic generator inspired by Benchmark [11] takes
as seed ontology a random tree which is computed using a
Gaussian distribution of the number of children per node. The
second ontology is obtained from the first one by applying a set of
alterations, similar to the ones used in Benchmark, such as label
replacement, word addition or removal in labels, node deletion
and node child addition and children shuffling. Then, these two
generated ontologies are used to generate alignments between
them. The aim of generating the original ontology is to perform
realistic tests and to allow a wider coverage of variations in their
structure. The same technique has been used for testing web
service matching in the geographical domain [12].

An automatic generator aiming at realism has been proposed
in [13]. This generator satisfies two requirements: (a) to generate
the structure and the instances of two taxonomies, and (b) to

generate an alignment between these two generated taxonomies.
Both taxonomies must have a fixed size and a Boltzmann sampler is
used to achieve this. The probabilistic model used ensures an equal
probability of appearance of a tree having a given size. Therefore,
the input data is controlled using this sampler. The number of
child nodes is controlled as well. Then, the alignment between the
two taxonomies is generated in a way that does not contradict
the generated data. To achieve this goal, three constraints were
enforced: the alignment must not introduce a cycle in the newly
obtained graph (the alignment and the two given taxonomies),
the alignment must not contradict the knowledge of the two
taxonomies and they must not entail each other.

For directory matching, the TaxMe test set [3] is built from
existing web directories with the aim of providing large scale
and realistic test sets for matching evaluation. However, this test
set only contains a subset of the reference alignment and the
generation mechanism matches concepts which are indexing
documents in common. It is thus comparable to a matching
technique and thus prone to bias towards similar techniques.

Table 1 compares the different work. Some of them have not
been made available as generators (ONTOBI and TaxMe); some
are dedicated to XML (or database) schemas (STBenchmark and
XBenchMatch). Swing is dedicated to the alteration of the ABox
part of the ontologies. In fact, the approach developed here is
complementary to that of Swing and could take advantage of its
ABox alterators. The most suitable systems for ontology matching
test generation are the systems described in [11,13] which were
not available to reuse and offer little flexibility.

In conclusion, none of the available generators is both able
to generate OAEI Benchmark and go beyond it by offering the
flexibility that the original Benchmark lacked as discussed in
Section 2.3. Hence, we developed the test generator that is
described hereafter.

3. A modular benchmark test generator

The OAEI Benchmark test set suffers from lack of variability
and discriminability. These problems affect the relevance of
Benchmark as a good basis for comparing matchers, but not
for matcher developers themselves. In order to overcome these
problems, we developed a test generator in Java, based on the
Jena API4 and integrated within the Alignment API.5 We present
the principles of the generator (Section 3.1) and illustrate them
through an example of generation (Section 3.2). We present
preliminary tests (Section 3.3) and show how generating a test set
allows for characterising matchers (Section 3.4).

3.1. Generator principles

The main principle of the test generator is that, from one
ontology, it can generate an altered one. The input ontology may
be a regular ontology or an artificially generated one: this may be
useful for generating scalability tests.

Because the alteration is known, if an input alignment is
provided it can be altered in the same exact way, providing a new
reference alignment. If no input alignment is given, an implicit
alignment between all named entities of the input ontologies is
assumed.

We designed the Alterator interface taking as input an
ontology and an alignment between this ontology and the
seed ontology. This module outputs an altered ontology and its
alignment with the seed ontology (Fig. 3). This allows for applying
alterations in sequence and retrieve a test at each step.

4 http://jena.sourceforge.net/ontology/index.html.
5 http://alignapi.gforge.inria.fr.

J. Euzenat et al. / Web Semantics: Science, Services and Agents on the World Wide Web 21 (2013) 3048

Table 1
Comparison of the different test generators.

Input
TBox alteration
ABox alteration
Variable ontology
Extensible alterators
Parameterisable alt.
Variable pattern
Available generator

ONTOBI [7]

owl


OAEI [2]

owl
del


Swing [8]
owl


STBenchmark [9]
xml


XBenchMatch [10]
xml


[11]

owl


[13]

owl


TaxMe [3]


This paper

owl

del


Fig. 3. Modular structure of test generators (Alterator).

Fig. 4. One-shot test generation by composing alterators (TestGenerator).
Subscripts on the arrows illustrate the evolution of the alignment which always
refers to the input ontology (o).

Thus, extending the test generator is easy since it is sufficient
to implement new alterators and to declare them to the
AlteratorFactory.
Elementary alterators. Elementary alterators are implementation
of the Alterator interface that apply one particular alteration
to an ontology. In order to assess the capability of matchers with
respect to particular ontology features, we consider the following
alterations: remove percentage of classes; remove percentage of
properties; remove percentage of comments; remove percentage
of restrictions; remove all classes from a level; rename percentage
of classes; rename percentage of properties; add percentage of
classes; add percentage of properties; add a number of classes
to a specific level; flatten a level; remove individuals. A detailed
description of elementary alterators is provided in Appendix C.

Alterators may be composed and manipulated programmati-
cally, so as to dynamically adapt the degree of alteration for in-
stance.
Generating a test case. To modify an ontology according to a set
of parameters we have defined TestGenerator as illustrated in
Fig. 4. It receives as input the seed ontology and the parameters
which represent the alterations to be applied. The output is the
modified ontology and the reference alignment. The implementation of the TestGenerator composes Alterators in a serial
manner. This can produce an arbitrary test.
Generating a test set. The test generator framework can also be
used to reproduce test sets such as Benchmark. For that purpose,
we designed the TestSet abstraction, in which test sets are
described as applications of TestGenerator. The program will
either generate all the required tests independently, by running
TestGenerator each time from the beginning (Fig. 5), or
generate them sequentially, as the initial Benchmark test set, by
using a previous test and altering it further (Fig. 6). In the latter
case, this corresponds to selecting paths in the semi-lattice of Fig. 2
which cover the whole test set.

Fig. 5. Random test set generation by combining test generators (TestSet).

Fig. 6. Continuous test set generation (forcing monotony) (TestSet).

The proposed approach is extensible because it allows for
implementing new alterators as very simple components that
can be exploited in generating test sets. It is also flexible, as it
can be used to generate complete test sets covering the whole
alteration space with a varying degree of precision (incrementing
the alteration proportion by 50% or by 2%), or more specific test
sets aiming at identifying specific problems (see Section 5).

3.2. Example of test case generation

As an example, consider the generation of test #258-4 from
the classic Bibliography ontology. Fig. 7 presents a simplified
view of the different generated ontologies and alignments. This
test renames 40% of the classes, suppresses one class level and
suppresses all instances. For this purpose, it uses four different
alterators which, at each step, generate a well-identified test:

Empty

does not modify the ontology but generates an alignment
expressing all equivalences between named entities of
the ontologies. This outputs test #101 (see Section 2.2).
Rename classes, provided with a parameter of 40%, will randomly
rename that amount of entities, in Fig. 7, InProceedings
is renamed into Culbrdeo and creator is renamed into
ovxylty. This may affect individuals because they are
described through properties and classes. The alignment
is altered in the same way by renaming the same entity
URIs. This outputs test #202-4.

Fig. 7. Test generation example (classes are in rounded boxes, properties are in italics, instances in rectangular boxes). The upper part shows the evolution of the ontology;
the lower part that of the alignment (always between the 101 ontology and the target ontology).

Flatten level, with level 1 as parameter, will suppress all classes
at level 1 in the hierarchy. All constraints applying to
these classes will be attached to their subclasses (which
will be attached to their superclasses) and all individuals
attached to these classes will remain attached to their
superclasses. Correspondences involving the suppressed
entities are suppressed from the alignment. This outputs
test #251-4.

Remove instances, with parameter 100%, will simply suppress all
individuals from the ontology. Since the individuals are
not present in the alignment, nothing is changed in it
(except using the URIs of the newly produced ontology).
This outputs test #258-4.

3.3. Preliminary observations

Before evaluating matchers in the large, we proceeded with
a preliminary evaluation of matchers. We report some of the
observation from this first use of the test generator, because they
may be useful to others. We first noted that matcher behaviour
was often not monotonic with respect to the expected difficulty
of tests. This did not occur in the original Benchmark because tests
were generated from one another in increasing difficulty order. We
validated this interpretation by adding the opportunity to generate
more continuous test sets (Section 3.3.1). We also noticed that
randomly generated tests may have variable results depending
on the order in which alterators were applied and we adopted a
suitable generation order (Section 3.3.2).

3.3.1. Random vs. continuous policies

Contrary to expected, matchers did not show a continuous
degradation of their performances as more alterations were
applied: it may happen that a matcher had better results at a
particular test than at a less altered one. This may be caused
by two different sources: (i) matchers have a non deterministic
behaviour, i.e., they use randomisation methods as do systems
based on evolutionary computation. Such matchers usually find
a local optimum depending on the initial alignment. So they

can, by chance, return a better result to a more difficult test.
Other matchers analyse the ontologies to match and run different
matching components depending on their characteristics. They
may have a non homogeneous behaviour because the generated
ontologies have different characteristics. Finally, other matchers
use resources on the web changing over time, such as search
engines, so that the results are dependent of the query context.
(ii) because tests are generated independently from each others
(Fig. 5), it may happen by chance that a test with more alteration is
easier than a test with less alteration. In the former case, matcher
developers are used to the behaviour of their systems, but the latter
case will affect all matchers and puzzle matcher developers.

This behaviour is only observable locally, i.e., on one test set,
and only for a few matchers. When averaging several test set
results, matcher behaviours are, on average, continuous. However,
this made difficult for tool developers to read one Benchmark test
result.

We countered this effect by generating continuous tests (Fig. 6)
the same way as the initial Benchmark was generated. In this case,
new tests are generated from the previous ones with the modular
architecture of the generator. However, we also observed that the
averaged results are the same with either random or continuous
generation (see Section 4.3). In the results reported below, unless
stated otherwise, the results are generated randomly.

3.3.2. Modification dependencies

We observed that test difficulty may not be the same across
tests supposed to have the same amount of alteration. This is
explained by the dependency between alterations. For instance, if
one wants to remove 60% of classes and to rename 20% of classes,
three extreme cases may happen (as illustrated in Fig. 8):
 rename 20% of classes and then remove 60% of classes, including
all renamed classes. In this situation, the test is easier than
expected because all renamed classes have been removed;
 rename 20% of classes and then remove 60% of classes, including
a part of renamed classes. In this situation, the test is as hard
as expected because the required proportion of the renamed
classes has been removed;

ReferenceReferenceReferenceReferenceReferenceReferenceReferenceReferenceMiscPartPartHthpyzCulbrdeoCulbrdeoCulbrdeoInProceedingsInBookInBookInBookInBookArticleArticleArticleArticleMiscPartPartHthpyzInProceedingsCulbrdeoCulbrdeoCulbrdeo36

J. Euzenat et al. / Web Semantics: Science, Services and Agents on the World Wide Web 21 (2013) 3048

Fig. 8. Test dependency: depending on which entities are affected by changes, the
test difficulty may vary.

Fig. 9. Matcher performance display on a grid preserving as much as possible the
topology of tests (left). Each cell corresponds to the availability of some features in
the test (l = labels and comments, p = properties, i = instances, h = hierarchy). A
sample display of F-measure (the darkest the best) generated by GroupOutput of
the Alignment API (right).

 rename 20% of classes and then remove 60% of classes, without
removing any of the renamed classes. In this situation, the test
is harder than expected because none of the renamed classes
has been removed.
Hence, a random disposition of parameters might reduce the really hard cases. As can be seen from the example, the nominal expected case may be restored by removing 60% of the classes before
renaming 20% of the remaining ones. Therefore, we established a
relevant order for parameters: remove classes, remove properties,
remove comments, remove restrictions, add classes, add proper-
ties, rename classes, rename properties. In this way, we obtained
the expected results. This order helps determining the paths in
Fig. 2 used for generating Benchmark. Such an order was not previously observed in the Benchmark test set because the value of
parameters, but rename resources, was always 100%.

3.4. Characterising matchers

Benchmark has been created for characterising matchers,
i.e., indicating in which situation they perform adequately. This
may, of course, be obtained by running the test corresponding
to the actual situation, e.g., #258-4, and compare the results of
matchers. This may also be obtained by considering a more detailed
view of the whole Benchmark, instead of an aggregated figure such
as average F-measure. Indeed, the more natural way of doing it
would be to display Fig. 2 with an indication, e.g., colour, of matcher
performances. This would be still uneasy to understand because
there are many tests and their horizontal ordering is quite random.
Fig. 9 provides a display that synthesises the obtained results by
aggregating in one cell a group of tests in which the ontologies to
compare share a common set of characteristics, namely, that the
same set of features has been altered [14]. Each cell is presented
with a colour representing the average of the F-measure in each
of these tests. The darker the cell, the better the algorithm. These
diagrams are topologically correct, i.e., if a cell touches another cell
on one side, they differ from only one feature, but not complete,
i.e., it is not possible to present all these connections in a planar
way.

Fig. 10. Matcher F-measure signature on the bibliography benchmark (matchers
are those presented in Section 4).

Such diagrams may be obtained for any measure that can be

rendered in colour: we use them with F-measures here.

Their use for an application developer consists of characterising
the cell in the diagram which corresponds to the application
data (by the presence/absence of labels, properties, instances or
hierarchy) and to select the best matcher with regard to this cell.
For instance, Fig. 10 displays the grid for each matcher that will
be considered below on the newly generated Benchmark test set.
It seems that matcher 7 is only able to provide results if labels
are preserved and is not useful otherwise. It is also surprising that
matcher 5 works incorrectly when nothing has been altered. In
fact, only in this case and only for biblio, this matcher outputs
peculiar results which are syntactically incorrect. Finally, it seems
that matcher 8 is better at compensating the alteration of both
instances and labels than matcher 4 or 6.

4. Benchmark assessment

In order to test the validity of Benchmark principles, we used
the test generator to reproduce it with different characteristics.
Then, we used 10 different matchers in order to compare the
results obtained with the different tests.

Our goal was to explore the properties of Benchmark with
respect to what they measure. All our results are given with
respect to F-measure. We used F-measure because, given the
number of evaluation modalities (random/continuous, four test
sets, difficulty, etc.), considering both precision and recall would
have complexified the analysis. In particular, the strategy of
matchers with respect to trading recall in favour of precision
does not help having an independent analysis. We provide in the
Appendix and in the associated material, everything necessary for
conducting the same analysis with other measures.

We presented preliminary results in [15] using a limited set
of matchers, a reduced set of seed ontologies and a different set
of tests. This gave different results from what we obtain now:
we still observe strong robustness of results and preservation of
difficulty for a matcher across test sets. However, the results allow
for identifying a bias in favour of the initial Benchmark test set
and the order of matchers across seed ontologies does not seem
preserved.

In order to remove the possibility that the obtained results
are an artefact of generated tests, we ran the tests five times for
each method (continuous and random) and then we computed
the average among the obtained results. Likewise, the tests are

Table 2
Progression of the various test sets.

Table 3
Matchers used in the experiments.

Photo

Biblio

Seed
Ontology size
Classes + Prop

Instances

Entities
Triples

Test set generation time (user time on 5 runs)

Random

Continuous
Difficult5

Difficult7

Difficult9
Triple/s

Prov

Finance

21 979

the same at each run (these tests are 101, 201202, 221225,
228, 232233, 236241, 246254, 257262, 265266, so 94 tests).
We decided not to reproduce the ones in which the labels are
translated into another language or the ones in which the labels
are replaced with their synonyms, because the corresponding
alterators are not sufficiently good. The same algorithms with the
same parameters have been used for all tests (the tests with the
original Benchmark have been run again and restricted to these
same tests).

4.1. Test sets

In order to assess the accuracy of the results obtained over the
years with the Benchmark test set, we have generated new test
sets from different seed ontologies with exactly the same structure
and modalities, and compared the results on F-measure. These
modalities were chosen because we assume that, since participants
have had the opportunity to test their systems with the original
Benchmark, their results may be higher on this test set.

New benchmark test sets have been generated from the same
seed ontology (bibliography or biblio) and different ontologies of
various sizes6:
photography (photo): http://www.co-ode.org/ontologies/photo

graphy/photography.owl

provenance (prov):

http://lsdis.cs.uga.edu/projects/glycomics/

propreo/ProPreO-060506.owl

finance : http://www.fadyart.com/ontologies/data/Finance.owl
The characteristics of these seed ontologies are summarised
in Table 2. Seed ontologies may be selected on the basis of their
size, e.g., number of classes, properties, axioms, their domain,
e.g., biomedical and tourism ontologies have different vocabular-
ies, or their expressivity. The main criterion for these ones was size.
The test sets have been generated using a MacBook Pro Intel
Core2Duo 2.66 GHz running MacOS X.6.8 (timing performed with
the Unix time command).

Table 2 shows that there is a strong constant correlation
between the time taken to generate a test set and the number of
triples in the test set (and not the number of entities). This is partly
due to Jena which forces to go through all triples when renaming
entities and when changing the namespace of entities. This
performance could be improved, but the linear time is reasonable.
The continuous generation of tests needs around 20% less time
than the random generation. This is a benefit of having a modular
generation framework allowing for optimising the generation
process (we will see in Section 4.3 that the continuous and random
tests provide the same results).

6 All the ontologies have been retrieved from the web (on 12/2011) and used
without any alteration.

System
Anchor-
Flood [16]
Aroma [17]
CSA [18]
Falcon-AO [19]
Lily [20]
LogMap [21]
MapEVO [22]
MapSSS [23]
Rimom [24]
SOBOM [25]

Version


(2011)

2011-0.2

(2006)
(2009)

http://www.kde.ics.tut.ac.jp/~hanif/

http://aroma.gforge.inria.fr/

http://ws.nju.edu.cn/falcon-ao/
http://cse.seu.edu.cn/people/pwang/lily.htm
http://www.cs.ox.ac.uk/isg/projects/LogMap/
http://sourceforge.net/projects/mappso/files/

http://keg.cs.tsinghua.edu.cn/project/RiMOM/
http://gudu-yebai.appspot.com/Download.jsp

4.2. Matchers

The results reported below have been achieved with 10 matchers selected for two reasons:
 they were available to us, either because they participated to
OAEI evaluation under the SEALS platform or because they were
generally available;
 they were the fastest systems among a panel of 20 systems,
allowing us to run more tests.

Incidentally, these matchers cover 3 out of the top-5 OAEI 2011
performers in terms of F-measure on Benchmark as well as matchers participating in previous OAEI campaigns. These matchers are
presented in Table 3. They have been anonymised in the remainder
of the paper, because our purpose is to evaluate the test generation
and not these systems. Systems however, keep the same numbers
across all experiments.

In that sense, we will not provide individual timing of matchers.
However, the time taken for running these tests evolve with the
size of the test set to the extent that they can be differentiated on
this basis. For bibliography, the time taken to run the 94 tests in
a test set varies from 58 s to 48 min, while for finance, it varies
from 9 min to 75 h with different matchers being the fastest and
the slowest (one matcher times out at finance).

4.3. Robustness

The first result that can be observed with the test generator is its
robustness. Regardless the seed ontology and independently from
the generation modality, matchers usually have the same results.
All the tests reported here (but those on the original Benchmark)
have been obtained over five different generations of each test.
However, this is barely useful because the results always reach very
close F-measure.

This applies even for non deterministic matchers which may
have large variations: these variations do not show up at that
level, because these F-measures correspond to the average over 94
ontology matching tasks.

Table 4 shows that the maximum variation that we observe
overall on F-measure is 0.02 on random bibliography. We chose
the maximum variation which is much larger than the standard
deviation (always less than 1%). This 0.02 variation occurs in only
one instance for bibliography and one instance for finance. If we
consider it on precision or recall, the variation does not exceed 0.02
on random tests (see Appendix B).

The continuous test sets have more variation: up to 0.04 in one
instance on bibliography and 0.02 on average. The reason is that,
since tests are not independent, any deviation occurring early in
the test generation process will be preserved throughout the test
set with little opportunity to be compensated.

The variation between random and continuous test sets is so
small that it justifies that we concentrate on the random test sets.

J. Euzenat et al. / Web Semantics: Science, Services and Agents on the World Wide Web 21 (2013) 3048

Table 4
Maximum F-measure variation across 5 runs for the different seed ontologies. Empty results correspond to syntax errors explained in Appendix B.

Biblio

Photo

Prov

Finance

Matcher
Random
Continuous
Rand  cont
Random
Continuous
Rand  cont
Random
Continuous
Rand  cont
Random
Continuous
Rand  cont


+0.01


+0.01


+0.01


+0.01


Avg

Table 5
Conservativeness of test difficulty.

Matcher (m)

m(biblio)

m(photo)

m(prov)

m(finance)


Avg


In fact, observing large variations (0.05) was a sign that
something went wrong in the test execution (usually testified by
error messages or by the absence of alignments). This is what
happened in some of the cancelled matcher results (see Appendix B
for details).

4.4. Conservativeness

We noticed that the difficulties encountered by a matcher at a
test are preserved across seed ontologies. What we mean is that by
looking at the results, test by test, the relative performance at a test
for a matcher is preserved across seed ontologies.

So, in order to assess the stability of differences across seed
ontologies for a matcher, we have designed a specific indicator. It
is relative to a matcher m and a set S of test sets which corresponds
to test sets with the same structure generated from different seed
ontologies (in our case, S contains biblio, photography, provenance
and finance). The indicators are expressed with respect to a
particular evaluation measure M: M(m, Ti) is the measure obtained
by matcher m on test Ti. Here we consider F-measure for M.
These F-measures are averaged across values obtained with five
independently generated tests for each seed ontology. We still do
not use standard deviation because of the difficulty to read it with
values between 0 and 1.


First, we define, for each particular test Ti, the average of results

M(m, Ti)
|S|

m(i) across all seed ontologies:
m(i) =

This is a level of difficulty of a particular test (Ti) for a particular
matcher (m). Then, for each test set, we compute the average
difference to this average:
M(m, Ti)  AS

m(T ) =

This is a signed difference: it is positive if the test set is easier
and negative if it is harder. This is a theoretical measure of the test

|T|

|T|

m(i)

i=1

i=1

|T|

set T difficulty for a particular matcher m (relative to the others
in S).

We observe on Table 5 that the ES

m values seem to induce a
general agreement that photography and provenance are harder
than bibliography and finance. Indeed, no matcher, but 7, found
bibliography more difficult than average, and no matcher but 9
found finance more difficult than average.
Finally, the expected value for m at a particular test Ti will be
m(i) + ES

m(T ). For measuring the globally observed deviation that

a matcher has with these expected measures, we compute:

m =

The result is a cumulative difference because of the absolute
value: positive and negative differences are not compensated. A
non cumulative difference (without the absolute value) would
yield 0. This is a very strong measure which means that the
observed value is on average at S
We can indeed observe the low level of difference between
the observed and the expected results (). All the values are well
below 0.1 variation.

m/2 of the expected one.

|M(m, Ti)  (AS
|S|  |T|

m(i) + ES

m(T ))|

This supports the observation that the results at a test set would
provide a signature for a particular matcher, not in the sense that
it allows for distinguishing a matcher among others  see the
discussion about discriminability in Section 5.2 , but in the sense
that difficulty always evolves in the same way for that matcher.

4.5. Systematic bias

Because the original Benchmark has been used for many years
with the same ontologies and public reference alignments, it is to
be expected that matchers have been particularly trained for this
test set.

We compared the results obtained with the original Benchmark
to those obtained on newly generated test sets with the same
seed ontology, either continuously or randomly, and other seed
ontologies. This comparison only covers the 94 tests which are
reproduced in the generated test sets (so the results are different
than those already published in OAEI results).

Table 6
Difference between results obtained through standard benchmarks (reduced to the relevant tests) and newly generated tests.

Matcher

Continuous
0  continuous
Random
0  random
Photo
0  photo
Prov
0  prov
Finance
0  finance

+0.01

+0.08

+0.03

+0.02

+0.15

+0.11

+0.04

+0.08

+0.07

+0.18

+0.06

+0.02

+0.02

+0.15

+0.18

+0.08

+0.18

+0.31

+0.17

+0.09

+0.04

+0.04

+0.02

+0.02

+0.12

+0.10

+0.05

+0.03

+0.03

+0.15

+0.28

+0.21

+0.13

+0.13

+0.20

+0.14

+0.08

Avg

+0.03

+0.03

+0.13

+0.14

+0.07

tests, all the matchers evolve very closely (except for the black
outlier). Hence, if one wants to draw conclusions for comparing
matchers, it is necessary to use more discriminant tests.

4.7. Conclusion

This study revealed several properties of the Benchmark setting

for evaluating ontology matchers:

Fig. 11. Average F-measure of each system with different seed ontologies. Seed
ontologies have not been ordered by size but by average F-measure.

Results are synthesised in Table 6. Two of the 10 systems (2 and
3) have many errors in their results which lowers the results they
obtain on Benchmark. Only one of the systems (6) has better results
on the original Benchmark than on the generated bibliography test
sets. Matcher 7, as already observed in Section 4.4, is the only
system finding the bibliography test set more difficult than the
others.

These results correspond to a general bias towards the original
Benchmark, since matchers obtain, nearly systematically, better
results with it.

There does not seem to be an overfitting to the actual precise
tests since the average difference with newly generated test sets
from the same seed ontology is only 0.03, while with other
newly generated tests, the advantage is much larger. Hence,
matchers seem biased towards the bibliographic ontology rather
than towards the initial Benchmark test set itself.

4.6. Order preservation

Similarly, we wanted to know if the order between matchers,
in terms of F-measure, would be preserved across seed ontologies.
Fig. 11 shows the evolution of F-measure. Seed ontologies are
ordered from the easier to the more difficult (based on average
F-measure).

If globally matchers evolve in the same direction, decreasing F-
measure with difficulty, the global result obtained with one seed
ontology cannot be transposed to another. So, from Fig. 11, the
order between matchers does not seem preserved. This may be
amended when considering the bias for the initial Benchmark test
set. Indeed, the three other test sets provide ordered clusters of
matchers (8, 2, 10, then 3, 6, 1, then 7, 5, 4 then 9). But it is
very difficult to evaluate precisely this order preservation, because
matcher results are often very close. This is related to the lack of
discriminability between matchers. Fig. 11 shows that for these

Robustness: the results will be the same for a matcher across
different randomly generated test sets from the same
ontology. This is also true when the test set is generated
in a random or continuous way. This provides confidence
in the generator and in the initial Benchmark test set.

Conservativeness: benchmark is good for studying matcher be-
haviour, and even relative matcher behaviour, because
matcher profiles with respect to test difficulty is pre-
served. Hence, independently from the seed ontology, the
difficult cases for a matcher will remain the same tests.
it is better to compare systems on the basis of seed
ontologies different from the original one. Arguably,
matchers are biased towards this particular test set or
domain albeit to different extents.
the order between matchers is not preserved across seed
ontologies. Hence, Benchmark is not good for identifying
the best matcher. However, having the ability to generate
a test set from an ontology allows for selecting the one
suited for a particular test set.

Order:

Bias:

This shows that Benchmark is indeed adequate for finding the
profile of a matcher, but not for comparing matchers. Matcher
comparison will depend on the particular situation, e.g., the seed
ontology.

In addition, it is difficult to have a definitive judgement about
the preservation of matcher order across different seed ontologies,
because the tests do not allow for sufficient discrimination. Hence,
we consider this problem in the remainder.

5. Difficulty and discriminability

One of the drawbacks of the initial Benchmark test set is its lack
of difficulty and discriminability (Section 1). A new test generator
allows for addressing such drawbacks in an interesting way. As
mentioned, Benchmark, being focussed on paving the problem
space regularly, contains many easy tests. We present here two
ways to use the test generator in order to generate more difficult
(Section 5.1) and discriminant (Section 5.2) test sets.

5.1. Increasing difficulty

The first way to increase difficulty has consisted of altering
differently the labels in the test sets. Instead of having a regular

J. Euzenat et al. / Web Semantics: Science, Services and Agents on the World Wide Web 21 (2013) 3048

Table 7
Percentage of altered labels depending on each of the modalities (the first modality
does not follow the p (i) formula).
i \ 

0%
20%
40%
60%
80%
100%

0%
70%
91%
97%
99%
100%

0%
90%
100%
100%
100%
100%

0%
50%
75%
87%
94%
97%

alteration of 20, 40, 60, 80 and 100% of the labels, we decided to
modify the benchmark generator so that it alters more radically
the labels depending on the required test difficulty. Hence, the
alteration p(i) at level i for difficulty  is:
p(i) = 1  (1  )i.

The corresponding values are given in Table 7.
This kind of modification in the test generation process could
be applied to any type of alteration (see Section 5.2), but this way
preserves the structure of the original Benchmark, so we used it for
comparison purposes.

5.1.1. Setting

We have generated three new test sets, based on the same
ontologies, with three new difficulty levels (0.5, 0.7 and 0.9)
using the random generation modality. This has been possible by
simply introducing a parameter in the BenchmarkGenerator
class which generates the Benchmark test set (the implementation
of Figs. 5 and 6).

The same matchers as above (Section 4.2) have been used on

these tests.

5.1.2. Results

Fig. 12 shows that the proposed technique indeed results in
more difficult tests. All systems have decreasing F-measure as
difficulty increases. This is correctly observed across all seed
ontologies.

This shows that the difficulty of tests can be systematically and
uniformly controlled by the test generation process. However, this
does not increase discrimination: matchers remain as close as in
the initial test set. Other techniques are necessary for increasing
discriminability.

5.2. Increasing discriminability

Looking at results on bibliography (iteration 0 of Fig. 14), it
seems clear that the tests discriminate between matchers 5, 1, 10,
7, and 9. These are also clearly distinguishable from the group of
matchers 6, 8, 4, 3 and 2. But among this latter group, the marchers
are very close to each others. In particular, 8 and 4 are barely
distinguishable. We describe a general use of the generator for
increasing discriminability of a given test set.

If we know what is discrimination between two matchers (their
F-measure is at a large distance), we do not know what it is
for several matchers at once. Hence, if discrimination seems a
desirable property of a test set, the notion of discrimination put
forward in [3] would gain in being formally defined so that we
could measure that test sets indeed discriminate.

We thus designed a measure based on the spread of values
which we call ideal spread distance. It is represented by the
difference between the actual spread of values and a uniform
spread, so for results of a set S of matchers:
isd(i) =

|M((k), Ti)  k/(|S|  1)|

|S|1

k=0

Fig. 12. Average F-measure of each system with increasing difficulty (0.0, 0.5, 0.7,
0.9) tests on the bibliography (top) and photography (bottom) seed ontologies.

with  a function ordering the matchers by increasing F-measure.
This value goes from|S|/2 (5 in our case) when all measures are the
same to 0. When they are equi-distributed over the unit interval.
The ideal spread distance is used here as a possible discriminability
indicator, other definitions may be retained.

One may argue that it is possible to increase discrimination
by just spreading evenly the result values to the unit interval,
preserving the order. This would indeed, reduce the ideal spread
distance to 0., but then (a) discrimination would be useless since
it would only amplify a measure that was already available
(no information gain), and (b) the obtained values would be
meaningless (these would be ordinals).

This is why we try to proceed by adding new tests which
are chosen on the basis of discrimination expectation and not an
artificial guarantee of discrimination. The difficulty is to find such
tests.

The process is the following:

1. generate the full alteration space on all (n) dimensions with the
simplest parameters: 0% and 100% alteration. This is N0 = 2n
tests;

2. run a set of matchers and compute a value, e.g., average

F-measure, for each tests which has no such value;

3. select the adjacent pair of tests with the highest difference

between these values, if a threshold is reached, then exit;
4. generate an intermediate test with the following parameters: if
the two tests are characterised by parameter vectors p1, . . . ,
pn and p
n, then generate the test with parameters
1, . . . , p
 p1+p
, . . . , pn+p

Fig. 13.
Iterative execution of the adaptive test generation (3 iterations). Each
vertex is labelled by average F-measure from which difference at each edge is
computed.

5. optionally, complete (close) the set of tests so that for any pair
n, there existsmin(p1, p
1),
n) within
1), . . . , max(pn, p

1, . . . , p
n) and max(p1, p

of testsp1, . . . , pn andp
. . . , min(pn, p
the test set;
6. go to Step 2.

Fig. 13 provides an example of running such a process.
This process can itself be parameterised in several manners in
addition to the seed ontology and the set of matchers:
 the number of dimensions;
 the type of values computed: this may be average F-measure
or other measures, such as standard deviation or the difference
between the highest and the lowest F-measure in the test;
 the output threshold may be a number of iterations, a maximum
or minimum value for the previous measure or a global
evaluation of difficulty or discriminability on the test set;
 the completion or not of the test set;
 the continuous or random generation of the new tests.

5.2.1. Setting

Such an adaptive process can be implemented with the given
test generation architecture. The difficulty of this particular
generator is that generation depends on test results. Hence it
is necessary to combine generation, matching and evaluation.
Although the Alignment API allows for such a combination, we
implemented the computation by hand, i.e., we generated a priori
all the necessary tests and used the relevant ones at each iteration.
We generated test sets of 125 tests corresponding to 5 levels of
alteration on three dimension (53 = 125), already present in the
initial Benchmark tests:
1. property removal;
2. comment removal;
3. label scrambling.

Fig. 14. Average F-measure at different iterations of the discriminability procedure
with the bibliography seed ontology when difficult tests are added on a whole
dimension at once (top: with maximum difference, bottom: with standard
deviation). Standard deviation stops after two iterations because it always selects
the same dimension to further develop.

Five such test sets based on the bibliography seed ontology were
generated. The initial iteration (0) of Fig. 14 corresponds to the use
of 8 tests offering either 0% or 100% alteration on each dimension.
Increasing discrimination was attempted by adding new tests.

We ran the same 10 matchers as above (Section 4.2) on these

new test sets and simulated the effect of the above process.

5.2.2. Results

The computations necessary for deciding which dimension to
further develop were initially based on two different criteria:
 largest difference in the difference between the maximum and
the minimum F-measure (across all systems);
 largest difference in standard deviation (between F-measures).
We compared the results obtained by selecting the dimension
based on the largest value between two vertices or the largest value
on average between all vertices in which the alteration matters.
This provided the same result.

In each case, we iterated three times. In most cases, the selected

dimension was the label scrambling dimension.

The results, provided in Fig. 14, show that this approach
does not allow us to discriminate much more the matchers than
the initial tests. The ideal spread distance of the initial state
(iteration 0) is 2.78, the resulting values for maxdiff and stdev are
respectively 2.94 and 2.72. Hence the proposed procedure does not
succeed in significantly increasing discrimination.

We tried to directly use the ideal spread distance in order to
identify the most promising dimension to develop. Indeed, it is

J. Euzenat et al. / Web Semantics: Science, Services and Agents on the World Wide Web 21 (2013) 3048

Table 8
Increased discrimination between two systems based on standard deviation by
adding the single most discriminating dimension (left) or the most discriminating
test (right) at each iteration.

st.dev.

Iteration

st.dev.

is a problem with putting the requirement of discrimination above
other requirements such as the ability to measure something. As
we have already mentioned, some systems are so close to each
other, not only in terms of performance but in terms of design, that
discriminating between them is not informative.

6. Conclusion

In this paper, we have brought three contributions to ontology
matching evaluation:
 an ontology alignment test generator which is extensible and
flexible;
 extensive experiments with the Benchmark modality used in
OAEI, showing that it indeed measures permanent properties
of matchers;
 Benchmark-like tests with improved variability and difficulty.
We have developed a test generator which follows a simple
modular architecture and API. This generator does not depend on
the seed ontology. It allows for different modifications at each run
of the program and the set of input parameters can be adjusted
in order to cover the problem space with any precision. It is also
extensible and flexible: it can be extended by adding new ontology
alterators and it can be used for generating individual tests with
controlled characteristics as well as full test sets.

This generator has been used for generating test sets on the
model of Benchmark with different seed ontologies and different
modalities. This largely improved the variability of generated tests.
Some of these generated tests have been used in OAEI 2011. From
these new generated test sets, we have been able to assess (a)
the robustness of evaluation results which remains the same for
differently generated test sets and (b) the preservation of the
relative difficulty across test sets generated with different seed
ontologies. On the other hand, we also identified (c) an apparent
bias towards the initial Benchmark test set and seed ontology, and
(d) a poor preservation of the order between matchers across seed
ontologies.

These observations confirm that the Benchmark test set is
suited for finding the strengths and weaknesses of matchers, but
not suited for ranking matchers universally. This remark may also
be related to the relative proximity of results provided by matchers
at such evaluations. It may be due to the intrinsic proximity of
matchers or to the lack of discriminability of the proposed test sets.
We have used the same test generator to address the problems
of difficulty and discriminability by generating specifically designed test sets. This has shown that increasing test difficulty is not
a problem, but increasing discriminability proved to be more difficult than expected. Indeed, improving discriminability through the
addition of new tests in a test set would require a precise definition
of what is expected, which would lead to a practical evaluation
measure. All those that we tried (standard deviation, maximum
span, average distance) did not provide convincing results.

Fig. 15. Average F-measure at different iterations of the discriminability
procedures with the bibliography seed ontology when difficult tests are added one
by one (top: with standard deviation; bottom: with the spread difference measure).

possible to compute the ideal spread distance for any single test
and when the difference between this measure is high for two
adjacent tests, an intermediate test may raise the overall spread.
When applying the ideal spread distance to the tests with either
0% or 100% alteration, the obtained value is maximal (4.83) when
nothing has been altered and minimal (1.65) when everything
has been altered. However, there are lower values in the other
generated tests.

Using the ideal spread distance with the closing modality
provides comparable results to those of Fig. 14 for the maximum
difference criterion (top). Fig. 15 displays the results of the
procedure when tests are added one by one, instead of dimension
per dimension (closing modality) for standard deviation and ideal
spread distance difference. In both cases, the final ideal spread
distance was 2.88, failing again to improve on this measure.

We performed the same tests in order to better discriminate
between only two matchers. In this case, using the maximum
difference or the standard deviation of average F-measure gave the
same results. We chose to discriminate between the two matchers
with the closest average F-measure, i.e., where discrimination
is needed. We obtained a better discrimination as is shown in
Table 8 by the increase of average standard deviation. However,
this increase in discrimination brought a difference in which
system is best. When working specifically with these two systems,
adding blindly the test that increases standard deviation makes
that 4 has a higher F-measure than 8 at iteration 1, then 8 has a
higher F-measure than 4 at iteration 2. In fact, this could change
at further iterations. Similar observations were possible with the
other discrimination experiments.

Indeed, modifying tests for the sole purpose of increasing
discriminability does not guarantee that results are preserved. This

Moreover, discrimination, taken as an absolute property may
lead to twist results so that they discriminate more instead of
providing an objective assessment of matcher quality. Hence, we
think that discriminability should not be taken as an absolute
criterion for test sets. It is more important to have test sets
providing meaningful and interpretable results than discriminant
ones.

In conclusion, a test generator as the one presented in this paper
is a valuable tool for introducing variability in benchmarks. In
addition, assessing the properties of generated tests is important to
understand what is evaluated and exposing benchmarks to critical
discussion. This can be achieved theoretically or experimentally. In
the latter case, a flexible test generator is precious.

This test generation framework has been used for generating
new test cases OAEI 2011.5 and OAEI 20 012 campaigns, focusing
in particular on scalability. We also consider extending it for
generating networks of ontologies instead of pairs of ontologies,
and further analysing the notion of test hardness applied to the
ontology matching problem.

Acknowledgement

This work has been partially supported by the SEALS project

(IST-2009-238975).

Appendix A. Additional material

A.1. Directory structure

In order to allow for reproducibility and result analysis, we

provide the following resources:

ontologies/ The seed ontologies used for generating these tests

are those of Section 4.1.
The original Benchmark test set.

origin/
dataset/seed/modality/run/test The different generated test sets
(onto.rdf and refalign.rdf) and results provided by
matchers: seed is the ontology name, modality is one
of benchmark, continuous, disc, difficult 5, 7, or 9, run
is the number of the generated test set (0 is the initial
bibliography test), and test the particular test number.

results/ The results computed and aggregated (in LaTeX, html and

as spreadsheets).

Various scripts for regenerating or manipulating these data are

also available.

These test sets and results are made available at ftp://ftp.inrial
pes.fr/pub/exmo/datasets/bench2012.zip (the zip file is around
700 MB).

DIR=dataset/$SEED

/bin/rm -rf $DIR
mkdir $DIR

# Generates 5 benchmarks in random mode
mkdir $DIR/benchmarks
for i in 1 2 3 4 5
do
mkdir $DIR/benchmarks/$i
$JAVA -Xmx1200m -cp $JAVALIB/procalign.jar

fr.inrialpes.exmo.align.cli.TestGen
-u http://oaei.ontologymatching.org/2011/benchmarks/$SEED/$i
-t fr.inrialpes.exmo.align.gen.BenchmarkGenerator
-o $DIR/benchmarks/$i ontologies/$SEED.owl

# Generates 5 benchmarks in continuous mode
mkdir $DIR/continuous
for i in 1 2 3 4 5
do
mkdir $DIR/continuous/$i
$JAVA -Xmx1200m

-cp $JAVALIB/procalign.jar
fr.inrialpes.exmo.align.cli.TestGen
-u http://oaei.ontologymatching.org/2011/continuous/$SEED/$i
-t fr.inrialpes.exmo.align.gen.BenchmarkGenerator
-Dcontinuous=1
-o $DIR/continuous/$i ontologies/$SEED.owl

done

done

# Generates 5 benchmarks in difficult fashion
for j in 5 7 9
do
mkdir $DIR/difficult$j
for i in 1 2 3 4 5
do
mkdir $DIR/difficult$j/$i
$JAVA -Xmx1200m

-cp $JAVALIB/procalign.jar
fr.inrialpes.exmo.align.cli.TestGen

-u http://oaei.ontologymatching.org/2011/difficult$j/$SEED/$i

-t fr.inrialpes.exmo.align.gen.BenchmarkGenerator
-Dincrement=.$j -Dmodality=mult
-o $DIR/difficult$j/$i ontologies/$SEED.owl

done
done
if [ $SEED == biblio ]
then
cp -r origin $DIR/continuous/0

# Generates 5 discriminant tests
mkdir $DIR/disc

for i in 1 2 3 4 5
do
mkdir $DIR/disc/$i
$JAVA -Xmx1200m -cp $JAVALIB/procalign.jar

fr.inrialpes.exmo.align.cli.TestGen
-u http://oaei.ontologymatching.org/2011/disc/$SEED/$i
-t fr.inrialpes.exmo.align.gen.DiscriminantGenerator
-o $DIR/disc/$i ontologies/$SEED.owl

done
fi
done

A.2. Test generation

A.3. Matching

The test generator for generating new independent test sets is
available in the Alignment API 4.3 (http://alignapi.gforge.inria.fr).
Tests may be generated from command lines with the following

script:
#!/bin/sh

JAVALIB=/Java/alignapi/lib
JAVA=/usr/bin/java
CP=

ontology="onto.rdf"

/bin/rm -rf dataset
mkdir dataset

We do not provide the matchers because we do not necessarily
have the right to do so; we indicated in Table 3 where they could
be found.

When a matcher is available under the Alignment API, instruc-

tions for running the matcher are:
$JAVA -cp $JAVALIB/procalign.jar

fr.inrialpes.exmo.align.util.GroupAlign
-o edna -n file://$TESTDIR/101/onto.rdf
-i fr.inrialpes.exmo.align.impl.method.StringDistAlignment
-DstringFunction=levenshteinDistance -Dnoinst=1
Not all matchers are available under this API. We ran the

evaluations through the SEALS platform client.7

for SEED in biblio photography provenance finance
do

7 http://oaei.ontologymatching.org/2011.5/seals-eval.html.

J. Euzenat et al. / Web Semantics: Science, Services and Agents on the World Wide Web 21 (2013) 3048

Table A.9
Number of tests on which this study is based: 712  94 + 50  125 = 73 178 matching tests.

diff = 0.5
5  10
5  7
2  10
2  9

diff = 0.7
5  10
5  7
2  10
2  9

diff = 0.9
5  10
5  7
2  10
2  9

Disc
5  10

Total

Random
5  10
5  7
5  10
5  9

Continuous
6  10
5  7
5  10
2  9

Biblio
Photography
Provenance
Finance
Total

A.4. Evaluation

We also provide the evaluation results (precision, recall,
F-measure) computed from these results in the results directory.
These computation can be achieved through the Alignment API by:

#!/bin/sh

TESTDIR=pwd
JAVALIB=/Java/alignapi/lib
JAVA=/usr/bin/java

FILES=1,2,3,4,5,6,7,8,9,10

/bin/rm -rf results
mkdir results

for ONTO in biblio photography provenance finance
do
for TEST in benchmarks continuous difficult5 difficult7

difficult9 disc

do
for RUN in 0 1 2 3 4 5
do

if [ -d dataset/$ONTO/$TEST/$RUN ]
then

cd dataset/$ONTO/$TEST/$RUN
$JAVA -Xmx1200m -cp $JAVALIB/procalign.jar

fr.inrialpes.exmo.align.cli.GroupEval
-f pfr -t tex -l $FILES
> $TESTDIR/results/html/$ONTO-$TEST-$RUN.tex

cd $TESTDIR

fi
done
done
done

A.5. Statistics

Table A.9 summarises the different result sets on which
this paper is based. This represents 712 test sets containing
94 elementary matching tasks, plus 5 discriminant test sets of
125 matching tasks, hence 73 178 elementary matching tasks. In
reality, we run far more tests than this number: cancelled results
from the following table have been computed (and are provided
in the zip file) and many more tests have been performed in the
preparatory phase.

Appendix B. Aggregated precision/F-measure/recall per test
suite

Tables B.10B.13 provide aggregated precision, F-measure and
recall for all generated test sets. We tried to ensure that we had
as much results as possible but the tests taking so long in some
instances, it was not possible to run them as many times as we had
wished for. This is the reason why the largest tests were not run 5
times.

There have been difficulties with several systems, the effect of
which has been cancelled. In particular, we had problems with:
 one system (1) failing completely to run on finance due to time
out;
 two systems (1 and 6) failing to run on photography due to
incorrect URIs in the ontologies (ignored by other systems);

 one system (3) delivering in some cases only, significantly
different results on photography due to the same incorrect URIs
(definitely a bug);
 two systems (2 and 3) failing to run on bibliography from
test 224 due to a Jena error: Property not found exception:
rdf:rest.

In the first three cases, we decided to ignore the results (they are
provided in the zip). They were due to our methodology to not alter
ontologies and the late discovery of incorrect URIs (with fragments
like f/22 or 24x36) which were silently but incorrectly parsed by
Jena.

Appendix C. Description of elementary alterators

We provide a description of the elementary alterators currently
implemented in the Alignment API. Further details about how to
use them are available at http://align http://api.gforge.inria.fr/
testgen.html. A more elaborate description of how they were used
in Benchmark is available at http://oaei.ontologymatching.org/
tests/.

The functional description is given at the level of OWL, however,
the actual implementation largely differs by taking advantage of
different internal structures referring to an actual ontology model.
Hence, sometimes modifications are applied on the OWL structure,
and sometimes they are applied at the level of triples. The final
model and alignments are generated at the end of the process. This
is only at the end of the process that the entities are assigned a final
URI.

Parameters are keyvalue pairs defining the degree or modalities of alteration. The priority is expressed by a level such that modifications of lower level must be applied before modifications of
higher level. All classes are in the fr.inrialpes.exmo.align.
gen.alt package.

EmptyModification

Class: EmptyModification
No parameters
Result: generates a test in which the initial ontology is not altered.
This test is used in test sets, for generating an initial identity
alignment.
Functional description: for each named class and each named
property in the input ontology, creates a correspondence with
equivalence between this entity and itself.
Priority: 0

RemoveClasses

Class: RemoveClasses
Parameters: removeClasses = p
Result: removes p% of classes.
Functional description: selects randomly p% of classes to
suppress from the ontology. For each of these classes, attaches,
directly, its subclasses to its superclasses (including constraints,

Table B.10
Bibliography results with 2012 test set (P = precision, F = F-measure, R = Recall).
Algo
Test

P.

P.

F.

R.

F.

R.

F.

R.

P.

P.

P.

F.

R.

F.

P.

R.

P.

F.

R.

P.

P.

F.

R.

P.

F.

R.

F.

R.

F.

R.

Random
0.95 0.76 0.63 1.0 0.82 0.69 0.99 0.78 0.64 0.97 0.76 0.63 0.99 0.84 0.74 0.86 0.78 0.72 0.74 0.56 0.45 0.90 0.82 0.76 0.52 0.37 0.29 0.95 0.74 0.60

0.96 0.75 0.62 1.0 0.82 0.70 0.98 0.78 0.64 0.98 0.76 0.62 0.99 0.84 0.73 0.86 0.78 0.72 0.73 0.55 0.44 0.90 0.82 0.76 0.50 0.37 0.29 0.95 0.74 0.61

0.95 0.75 0.61 1.0 0.82 0.69 0.98 0.78 0.64 0.97 0.76 0.63 0.99 0.84 0.73 0.86 0.79 0.72 0.74 0.56 0.45 0.91 0.83 0.76 0.51 0.37 0.29 0.95 0.74 0.61

0.95 0.75 0.62 1.0 0.82 0.69 0.98 0.78 0.64 0.97 0.76 0.62 0.99 0.84 0.73 0.85 0.77 0.71 0.74 0.56 0.45 0.91 0.83 0.77 0.51 0.37 0.29 0.95 0.74 0.60

0.96 0.76 0.62 1.0 0.82 0.69 0.98 0.78 0.64 0.98 0.76 0.62 0.99 0.84 0.73 0.86 0.78 0.71 0.74 0.56 0.45 0.90 0.82 0.76 0.52 0.37 0.29 0.94 0.73 0.60

Avg
0.95 0.75 0.62 1.0 0.82 0.69 0.98 0.78 0.64 0.97 0.76 0.62 0.99 0.84 0.73 0.86 0.78 0.72 0.74 0.56 0.45 0.90 0.82 0.76 0.51 0.37 0.29 0.95 0.74 0.60
Maxvar 0.01 0.01 0.02 0.00 0.00 0.01 0.01 0.00 0.00 0.01 0.00 0.01 0.00 0.00 0.01 0.01 0.02 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.02 0.00 0.00 0.01 0.01 0.01
Continuous
0.96 0.76 0.63 0.99 0.84 0.72 0.97 0.85 0.75 0.95 0.78 0.66 0.98 0.84 0.73 0.83 0.76 0.70 0.74 0.60 0.51 0.93 0.84 0.78 0.55 0.40 0.32 0.99 0.87 0.78

0.95 0.76 0.63 1.0 0.82 0.69 0.99 0.78 0.65 0.97 0.77 0.64 0.99 0.85 0.74 0.85 0.78 0.72 0.74 0.56 0.45 0.88 0.81 0.75 0.51 0.37 0.30 0.94 0.73 0.60

0.97 0.78 0.65 1.0 0.81 0.68 0.98 0.77 0.64 0.97 0.76 0.62 0.99 0.85 0.74 0.86 0.78 0.72 0.74 0.56 0.45 0.91 0.83 0.77 0.52 0.37 0.29 0.94 0.74 0.61

0.95 0.75 0.62 1.0 0.82 0.69 0.98 0.77 0.63 0.97 0.75 0.61 0.99 0.83 0.72 0.86 0.79 0.72 0.74 0.55 0.44 0.91 0.83 0.76 0.50 0.36 0.29 0.93 0.74 0.61

0.96 0.75 0.62 1.0 0.82 0.69 0.98 0.78 0.64 0.98 0.77 0.64 0.99 0.85 0.75 0.88 0.79 0.71 0.74 0.56 0.45 0.89 0.82 0.76 0.52 0.38 0.30 0.95 0.74 0.60

0.92 0.74 0.62 1.0 0.80 0.67 0.98 0.77 0.63 0.98 0.76 0.63 0.99 0.83 0.72 0.86 0.78 0.71 0.74 0.56 0.46 0.88 0.81 0.74 0.51 0.37 0.29 0.93 0.73 0.60

Avg
0.95 0.76 0.63 1.0 0.81 0.68 0.98 0.77 0.64 0.97 0.76 0.63 0.99 0.84 0.73 0.86 0.78 0.72 0.74 0.56 0.45 0.89 0.82 0.76 0.51 0.37 0.29 0.94 0.74 0.60
Maxvar 0.05 0.04 0.03 0.00 0.02 0.02 0.01 0.01 0.02 0.01 0.02 0.03 0.00 0.02 0.03 0.03 0.01 0.01 0.00 0.01 0.02 0.03 0.02 0.03 0.02 0.02 0.01 0.02 0.01 0.01
Difficult5
0.96 0.63 0.46 1.0 0.73 0.58 0.97 0.68 0.52 0.98 0.67 0.51 0.99 0.76 0.61 0.86 0.76 0.68 0.65 0.41 0.30 0.84 0.71 0.62 0.39 0.26 0.20 0.88 0.60 0.46

0.96 0.63 0.47 1.0 0.74 0.58 0.98 0.68 0.52 0.98 0.66 0.50 0.99 0.76 0.62 0.86 0.76 0.68 0.65 0.41 0.30 0.84 0.72 0.63 0.40 0.26 0.20 0.89 0.61 0.46

0.96 0.61 0.45 1.0 0.73 0.58 0.97 0.67 0.52 0.98 0.66 0.50 0.99 0.76 0.62 0.86 0.75 0.67 0.66 0.41 0.30 0.84 0.72 0.63 0.39 0.26 0.20 0.91 0.61 0.46

0.96 0.61 0.45 1.0 0.74 0.59 0.98 0.68 0.52 0.98 0.66 0.50 0.99 0.76 0.62 0.85 0.74 0.66 0.65 0.41 0.30 0.84 0.72 0.63 0.40 0.26 0.20 0.87 0.60 0.46

0.96 0.63 0.46 1.0 0.73 0.58 0.98 0.68 0.52 0.98 0.67 0.51 0.99 0.76 0.62 0.85 0.75 0.66 0.65 0.41 0.30 0.85 0.72 0.63 0.38 0.26 0.19 0.90 0.61 0.46

Avg
0.96 0.62 0.46 1.0 0.73 0.58 0.98 0.68 0.52 0.98 0.66 0.50 0.99 0.76 0.62 0.86 0.75 0.67 0.65 0.41 0.30 0.84 0.72 0.63 0.39 0.26 0.20 0.89 0.61 0.46
Maxvar 0.00 0.02 0.02 0.00 0.01 0.01 0.01 0.01 0.00 0.00 0.01 0.01 0.00 0.00 0.01 0.01 0.02 0.02 0.01 0.00 0.00 0.01 0.01 0.01 0.02 0.00 0.01 0.04 0.01 0.00
Difficult7
0.97 0.55 0.39 1.0 0.68 0.51 0.97 0.62 0.46 0.99 0.61 0.44 0.99 0.70 0.54 0.87 0.70 0.59 0.58 0.32 0.22 0.79 0.67 0.58 0.31 0.20 0.15 0.84 0.52 0.38

0.97 0.55 0.38 1.0 0.68 0.52 0.97 0.62 0.46 0.99 0.61 0.44 0.99 0.70 0.54 0.86 0.69 0.58 0.58 0.32 0.22 0.79 0.67 0.58 0.31 0.20 0.15 0.89 0.52 0.37

0.97 0.54 0.38 0.99 0.68 0.52 0.97 0.62 0.46 0.99 0.61 0.44 0.99 0.69 0.54 0.87 0.70 0.59 0.58 0.32 0.22 0.79 0.67 0.58 0.31 0.20 0.15 0.87 0.52 0.37

0.96 0.55 0.38 0.99 0.68 0.51 0.97 0.62 0.46 0.99 0.61 0.44 0.99 0.69 0.53 0.87 0.70 0.59 0.58 0.32 0.22 0.79 0.67 0.58 0.31 0.20 0.14 0.83 0.51 0.37

0.97 0.55 0.38 0.99 0.68 0.52 0.97 0.62 0.46 0.99 0.61 0.45 0.99 0.70 0.54 0.86 0.69 0.58 0.58 0.32 0.22 0.79 0.67 0.58 0.31 0.20 0.15 0.86 0.52 0.37

0.97 0.55 0.38 0.99 0.68 0.52 0.97 0.62 0.46 0.99 0.61 0.44 0.99 0.70 0.54 0.87 0.70 0.59 0.58 0.32 0.22 0.79 0.67 0.58 0.31 0.20 0.15 0.86 0.52 0.37
Avg
Maxvar 0.01 0.01 0.01 0.01 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.01 0.01 0.01 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.06 0.01 0.01
Difficult9
0.97 0.51 0.34 0.99 0.64 0.47 0.97 0.59 0.42 0.99 0.57 0.40 0.99 0.65 0.48 0.88 0.67 0.54 0.52 0.26 0.17 0.77 0.66 0.57 0.26 0.16 0.11 0.83 0.46 0.32

0.96 0.50 0.34 1.0 0.64 0.47 0.97 0.59 0.42 0.99 0.57 0.40 0.99 0.65 0.48 0.87 0.67 0.54 0.53 0.26 0.18 0.75 0.65 0.57 0.26 0.16 0.12 0.81 0.44 0.31

0.96 0.51 0.35 0.99 0.64 0.47 0.97 0.59 0.42 0.99 0.57 0.40 0.99 0.65 0.48 0.87 0.67 0.54 0.52 0.26 0.18 0.76 0.65 0.57 0.25 0.16 0.12 0.82 0.46 0.32

0.96 0.51 0.34 0.99 0.64 0.47 0.97 0.59 0.42 0.99 0.57 0.40 0.99 0.65 0.48 0.88 0.66 0.53 0.52 0.26 0.17 0.75 0.64 0.56 0.26 0.16 0.12 0.83 0.46 0.31

0.97 0.51 0.34 0.99 0.64 0.47 0.97 0.59 0.42 0.99 0.57 0.40 0.99 0.65 0.48 0.88 0.67 0.55 0.52 0.26 0.17 0.76 0.65 0.57 0.26 0.16 0.12 0.84 0.46 0.31

Avg
0.96 0.51 0.34 0.99 0.64 0.47 0.97 0.59 0.42 0.99 0.57 0.40 0.99 0.65 0.48 0.88 0.67 0.54 0.52 0.26 0.17 0.76 0.65 0.57 0.26 0.16 0.12 0.83 0.46 0.31
Maxvar 0.01 0.01 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.01 0.02 0.01 0.00 0.01 0.02 0.02 0.01 0.01 0.00 0.01 0.03 0.02 0.01

i.e., anonymous Restriction classes); attaches, directly, its
instances to its superclasses; converts all restrictions involving
it (allValuesFrom, someValueFrom) with its superclasses;
converts all property restrictions involving it (domain, range)
with its superclasses; removes the class. Updates the alignment by
suppressing all correspondences involving the suppressed classes.
Priority: 1

RemoveComments

Class: RemoveComments
Parameters: removeComments = p
Result: removes p% of comments.
Functional description: selects, randomly and independently
from the ontology, p% of classes, properties and individuals whose
comments will be suppressed. Removes the comments of each
selected entity. The alignment is not modified.
Priority: 2

RemoveIndividuals

Class: RemoveIndividuals
Parameters: removeIndividuals = p
Result: removes p% of individuals.
Functional description: selects randomly p% of individuals to
suppress from the ontology. Suppresses from the ontology model
all statements using these individuals, either as subject or object.
The alignment is not modified, because it does not contain
individuals.
Priority: 2

RemoveProperties

Class: RemoveProperties
Parameters: removeProperties = p
Result: removes p% of properties.

J. Euzenat et al. / Web Semantics: Science, Services and Agents on the World Wide Web 21 (2013) 3048

F.

R.

P. F. R. P.

F.

R.

F.

R.

P.

0.95 0.63 0.47 0.96 0.66 0.51
0.95 0.63 0.47 0.96 0.66 0.51
0.95 0.63 0.47 0.96 0.66 0.51
0.95 0.63 0.47 0.96 0.66 0.51
0.94 0.63 0.47 0.96 0.66 0.51
0.95 0.63 0.47 0.96 0.66 0.51
0.01 0.00 0.00 0.00 0.00 0.00

P. F. R.

P.

0.92 0.69 0.54
0.92 0.68 0.54
0.93 0.69 0.55
0.92 0.68 0.55
0.92 0.69 0.55
0.92 0.69 0.55
0.01 0.01 0.01

0.92 0.69 0.55
0.92 0.69 0.55
0.92 0.69 0.55
0.92 0.69 0.55
0.92 0.69 0.55
0.92 0.69 0.55
0.00 0.00 0.00

Table B.11
Photography results with 2012 test set (P = precision, F = F-measure, R = Recall).
Algo
Test
Random

Avg
Maxvar
Continuous

Avg
Maxvar
Difficult5

Avg
Maxvar
Difficult7

Avg
Maxvar
Difficult9

Avg
Maxvar

0.92 0.49 0.33
0.91 0.48 0.33
0.91 0.49 0.33
0.91 0.49 0.33
0.91 0.49 0.33
0.91 0.49 0.33
0.01 0.01 0.00

0.91 0.43 0.28
0.91 0.43 0.28
0.91 0.43 0.28
0.91 0.43 0.28
0.91 0.43 0.28
0.91 0.43 0.28
0.00 0.00 0.00

0.92 0.57 0.41
0.92 0.57 0.42
0.92 0.57 0.41
0.92 0.57 0.41
0.92 0.57 0.42
0.92 0.57 0.41
0.00 0.00 0.01

0.90 0.40 0.26 0.95 0.41 0.26
0.90 0.40 0.26 0.94 0.41 0.27
0.90 0.40 0.26 0.95 0.41 0.26
0.90 0.40 0.26 0.95 0.41 0.26
0.90 0.40 0.26 0.95 0.42 0.27
0.90 0.40 0.26 0.95 0.41 0.26
0.00 0.00 0.00 0.01 0.01 0.01

0.89 0.34 0.21 0.97 0.34 0.21
0.89 0.34 0.21 0.97 0.34 0.21
0.89 0.34 0.21 0.96 0.34 0.21
0.89 0.34 0.21 0.97 0.34 0.21
0.89 0.34 0.21 0.96 0.34 0.21
0.89 0.34 0.21 0.97 0.34 0.21
0.00 0.00 0.00 0.01 0.00 0.00

0.94 0.63 0.47 0.95 0.66 0.50
0.95 0.63 0.48 0.96 0.66 0.50
0.94 0.63 0.47 0.96 0.67 0.52
0.95 0.64 0.48 0.95 0.67 0.52
0.95 0.63 0.48 0.97 0.67 0.52
0.95 0.63 0.48 0.96 0.67 0.51
0.01 0.01 0.01 0.02 0.01 0.02

0.92 0.49 0.34 0.94 0.52 0.36
0.92 0.50 0.34 0.93 0.52 0.36
0.92 0.49 0.34 0.93 0.52 0.36
0.92 0.49 0.34 0.93 0.52 0.36
0.92 0.49 0.34 0.93 0.52 0.36
0.92 0.49 0.34 0.93 0.52 0.36
0.00 0.01 0.00 0.01 0.00 0.00

P. F. R.

P.

F.

R.

P.

F.

R.

P.

F.

R.

P.

F.

R.

0.66 0.50 0.84 0.72 0.63 0.45 0.25 0.18 0.93 0.67 0.52

0.66 0.50 0.83 0.71 0.63 0.44 0.25 0.17 0.92 0.67 0.52

0.66 0.49 0.83 0.72 0.63 0.44 0.25 0.17 0.92 0.67 0.52

0.66 0.50 0.83 0.71 0.63 0.45 0.25 0.17 0.93 0.67 0.52

0.66 0.50 0.84 0.72 0.63 0.45 0.25 0.17 0.92 0.67 0.52

0.66 0.50 0.83 0.72 0.63 0.45 0.25 0.17 0.92 0.67 0.52
0.00 0.00 0.01 0.01 0.01 0.00 0.01 0.00 0.01 0.01 0.00 0.00

0.66 0.49 0.83 0.72 0.63 0.45 0.25 0.17 0.93 0.66 0.52

0.66 0.49 0.84 0.72 0.63 0.45 0.25 0.17 0.92 0.66 0.52

0.66 0.49 0.84 0.73 0.64 0.46 0.25 0.18 0.93 0.67 0.52

0.66 0.50 0.84 0.73 0.65 0.46 0.25 0.18 0.92 0.67 0.52

0.66 0.49 0.84 0.73 0.64 0.45 0.24 0.17 0.93 0.67 0.52

0.66 0.49 0.84 0.73 0.64 0.45 0.25 0.17 0.93 0.67 0.52
0.00 0.00 0.01 0.01 0.01 0.02 0.01 0.01 0.01 0.01 0.01 0.00

0.50 0.33 0.76 0.58 0.47 0.34 0.17 0.12 0.88 0.53 0.38

0.50 0.33 0.76 0.58 0.48 0.34 0.18 0.12 0.88 0.52 0.37

0.50 0.33 0.76 0.58 0.47 0.34 0.18 0.12 0.88 0.53 0.38

0.50 0.33 0.75 0.58 0.47 0.34 0.18 0.12 0.88 0.53 0.38

0.50 0.33 0.75 0.58 0.47 0.34 0.17 0.12 0.89 0.53 0.37

0.50 0.33 0.76 0.58 0.47 0.34 0.18 0.12 0.88 0.53 0.38
0.00 0.00 0.00 0.01 0.00 0.01 0.00 0.01 0.00 0.01 0.01 0.01

0.39 0.25 0.66 0.49 0.40 0.27 0.13 0.09 0.84 0.44 0.30

0.39 0.25 0.67 0.50 0.40 0.27 0.14 0.09 0.83 0.44 0.30

0.39 0.25 0.67 0.50 0.40 0.28 0.14 0.09 0.85 0.44 0.30

0.39 0.25 0.67 0.50 0.40 0.28 0.14 0.09 0.84 0.44 0.30

0.39 0.25 0.67 0.50 0.40 0.27 0.14 0.09 0.84 0.44 0.30

0.39 0.25 0.67 0.50 0.40 0.27 0.14 0.09 0.84 0.44 0.30
0.00 0.00 0.00 0.01 0.01 0.00 0.01 0.01 0.00 0.02 0.00 0.00

0.33 0.2
0.33 0.2
0.33 0.2
0.33 0.2
0.33 0.2
0.33 0.2

0.60 0.45 0.37 0.23 0.11 0.07 0.79 0.38 0.25

0.60 0.45 0.36 0.23 0.11 0.07 0.81 0.38 0.25

0.60 0.45 0.36 0.23 0.11 0.07 0.79 0.38 0.25

0.60 0.46 0.37 0.23 0.12 0.08 0.78 0.38 0.25

0.59 0.45 0.36 0.23 0.11 0.07 0.81 0.38 0.25

0.60 0.45 0.36 0.23 0.11 0.07 0.80 0.38 0.25
0.00 0.00 0.00 0.01 0.01 0.01 0.00 0.01 0.01 0.03 0.00 0.00

Functional description: selects randomly p% of properties to
suppress from the ontology. For each of these properties, removes
all restrictions (allValuesFrom, someValues From) involving
it, removes all assertions involving it. Updates the alignment
by suppressing all correspondences involving the suppressed
properties.
Priority: 2

RemoveRestrictions

Class: RemoveRestrictions
Parameters: removeRestrictions = p
Result: removes p% of restriction.
Functional description: selects randomly p% of property restrictions to suppress from the ontology. For each of these restrictions,
removes it from the model. The alignment is not modified.
Priority: 3

RenameClasses

Class: RenameClasses
Parameters: renameClasses=p, renameMethod=m

Result: renames p% of classes depending on the requested method
m (random strings, translation, synonyms) [default: random]
Functional description: selects randomly p% of classes from the
ontology to rename. For each of these classes, creates a new name
depending on the selected m and replace its rdfs:label by
the new name. For each triple of the model, all URI fragments
are replaced if necessary by the corresponding new name. Each
correspondence of the alignment involving a renamed class is
replaced by the corresponding new URI.
Priority: 4

RenameProperties

Class: RenameProperties
Parameters: renameProperties=p, renameMethod=m
Result: renames p% of properties depending on the requested
method m (random strings, translation, synonyms) [default:
random]
Functional description: selects randomly p% of properties from
the ontology to rename. For each of these properties, creates a new
name depending on the selected m. For each triple of the model, all
URI fragments are replaced if necessary by the corresponding new

Table B.12
Provenance results with 2012 test set (P = precision, F = F-measure, R = Recall).
Algo
Test

P.

P.

F.

R.

F.

R.

F.

R.

F.

R.

P.

P.

P.

P.

R.

P.

F.

R.

P.

P.

F.

R.

P.

F.

R.

F.

R.

F.

R.

F.

Random
0.92 0.68 0.54 0.96 0.73 0.58 0.89 0.67 0.54 0.77 0.60 0.49 0.85 0.53 0.39 0.92 0.67 0.52 0.99 0.66 0.50 0.82 0.74 0.67 0.21 0.12 0.08 0.92 0.73 0.61

0.92 0.68 0.54 0.96 0.72 0.58 0.89 0.67 0.54 0.78 0.60 0.49 0.85 0.53 0.38 0.90 0.67 0.54 0.99 0.66 0.50 0.82 0.74 0.67 0.21 0.12 0.08 0.92 0.73 0.61

0.92 0.68 0.54 0.97 0.73 0.58 0.90 0.68 0.54 0.78 0.60 0.49 0.85 0.53 0.39 0.90 0.67 0.53 0.99 0.66 0.50 0.82 0.74 0.67 0.21 0.12 0.08 0.92 0.73 0.61

0.92 0.68 0.54 0.97 0.73 0.58 0.89 0.67 0.54 0.77 0.60 0.49 0.85 0.53 0.39 0.91 0.67 0.53 0.99 0.66 0.50 0.82 0.74 0.67 0.21 0.12 0.08 0.92 0.73 0.61

0.91 0.68 0.54 0.97 0.73 0.58 0.89 0.67 0.54 0.78 0.60 0.49 0.85 0.53 0.38 0.90 0.67 0.53 0.99 0.66 0.50 0.82 0.74 0.67 0.21 0.12 0.08 0.92 0.73 0.60

Avg
0.92 0.68 0.54 0.97 0.73 0.58 0.89 0.67 0.54 0.78 0.60 0.49 0.85 0.53 0.39 0.91 0.67 0.53 0.99 0.66 0.50 0.82 0.74 0.67 0.21 0.12 0.08 0.92 0.73 0.61
Maxvar 0.01 0.00 0.00 0.01 0.01 0.00 0.01 0.01 0.00 0.01 0.00 0.00 0.00 0.00 0.01 0.02 0.00 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01
Continuous
0.92 0.68 0.54 0.97 0.73 0.59 0.90 0.67 0.54 0.78 0.60 0.49 0.85 0.52 0.38 0.90 0.67 0.53 0.99 0.66 0.50 0.82 0.74 0.67 0.21 0.12 0.08 0.92 0.73 0.60

0.92 0.68 0.54 0.97 0.73 0.59 0.89 0.67 0.54 0.78 0.61 0.50 0.86 0.53 0.39 0.91 0.67 0.53 0.99 0.66 0.50 0.82 0.74 0.67 0.21 0.12 0.08 0.92 0.73 0.61

0.92 0.68 0.54 0.97 0.73 0.58 0.90 0.68 0.54 0.79 0.61 0.50 0.85 0.52 0.38 0.91 0.67 0.53 0.99 0.66 0.50 0.82 0.74 0.67 0.21 0.12 0.08 0.92 0.73 0.61

0.92 0.68 0.54 0.96 0.72 0.58 0.89 0.67 0.54 0.77 0.60 0.49 0.85 0.53 0.38 0.91 0.67 0.54 0.99 0.66 0.50 0.82 0.74 0.67 0.21 0.12 0.08 0.92 0.73 0.61

0.91 0.68 0.54 0.97 0.73 0.59 0.89 0.67 0.54 0.78 0.61 0.50 0.85 0.53 0.38 0.94 0.67 0.52 0.99 0.66 0.50 0.82 0.74 0.67 0.21 0.12 0.08 0.92 0.73 0.60

Avg
0.92 0.68 0.54 0.97 0.73 0.59 0.89 0.67 0.54 0.78 0.61 0.50 0.85 0.53 0.38 0.91 0.67 0.53 0.99 0.66 0.50 0.82 0.74 0.67 0.21 0.12 0.08 0.92 0.73 0.61
Maxvar 0.01 0.00 0.00 0.01 0.01 0.01 0.01 0.01 0.00 0.02 0.01 0.01 0.01 0.01 0.01 0.04 0.00 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01
Difficult5
0.92 0.54 0.38 0.95 0.60 0.44 0.82 0.53 0.39 0.71 0.49 0.38 0.83 0.46 0.31 0.87 0.54 0.39 0.99 0.50 0.34 0.76 0.63 0.53 0.15 0.08 0.06 0.86 0.61 0.47

0.91 0.53 0.38 0.95 0.60 0.44 0.82 0.53 0.39 0.71 0.49 0.38 0.83 0.46 0.31 0.89 0.54 0.39 0.99 0.50 0.34 0.76 0.63 0.53 0.16 0.08 0.06 0.86 0.61 0.48

Avg
0.91 0.53 0.38 0.95 0.60 0.44 0.82 0.53 0.39 0.71 0.49 0.38 0.83 0.46 0.31 0.88 0.54 0.39 0.99 0.50 0.34 0.76 0.63 0.53 0.15 0.08 0.06 0.86 0.61 0.47
Maxvar 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.01
Difficult7
0.93 0.45 0.30 0.93 0.52 0.36 0.76 0.45 0.31 0.67 0.43 0.31 0.82 0.40 0.26 0.85 0.46 0.32 0.99 0.40 0.25 0.71 0.55 0.45 0.12 0.06 0.04 0.82 0.53 0.40

0.93 0.45 0.29 0.94 0.52 0.36 0.76 0.44 0.31 0.67 0.43 0.31 0.82 0.40 0.26 0.88 0.45 0.31 0.99 0.40 0.25 0.70 0.55 0.45 0.12 0.07 0.04 0.82 0.53 0.39

Avg
0.93 0.45 0.29 0.93 0.52 0.36 0.76 0.44 0.31 0.67 0.43 0.31 0.82 0.40 0.26 0.86 0.45 0.31 0.99 0.40 0.25 0.70 0.55 0.45 0.12 0.06 0.04 0.82 0.53 0.39
Maxvar 0.00 0.00 0.01 0.01 0.00 0.00 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.03 0.01 0.01 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.01
Difficult9
0.94 0.40 0.25 0.92 0.46 0.31 0.72 0.39 0.27 0.64 0.38 0.27 0.81 0.36 0.23 0.88 0.40 0.26 0.99 0.33 0.20 0.63 0.51 0.42 0.10 0.05 0.04 0.79 0.48 0.34

0.94 0.40 0.25 0.92 0.46 0.31 0.72 0.39 0.27 0.64 0.38 0.27 0.81 0.36 0.23 0.86 0.40 0.26 0.99 0.33 0.20 0.63 0.51 0.43 0.10 0.05 0.04 0.79 0.48 0.34

Avg
0.94 0.40 0.25 0.92 0.46 0.31 0.72 0.39 0.27 0.64 0.38 0.27 0.81 0.36 0.23 0.87 0.40 0.26 0.99 0.33 0.20 0.63 0.51 0.43 0.10 0.05 0.04 0.79 0.48 0.34
Maxvar 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00

name. Each correspondence of the alignment involving a renamed
property is replaced by the corresponding new URI.
Priority: 4

AddClasses

Class: AddClasses
Parameters: addClasses=p
Result: adds p% classes.
Functional description: selects randomly p% of classes from the
ontology to add a subclass to. For each of these classes, generates
randomly a new name; creates a new subclass of the selected class
with this name. The alignment is not modified.
Priority: 5

AddProperties

Class: AddProperties
Parameters: addProperties=p
Result: adds p% properties.
Functional description: selects randomly p% of properties from
the ontology to add. For each of these properties, generates
randomly a new name. For half of these new properties, creates
an ObjectProperty: selects randomly a domain and range
among the classes and assign them to this property. For the
other half of these new properties, creates a DatatypeProperty:

selects randomly a domain among the classes and assign it to
this property. range is set to xsd:string. The alignment is not
modified.
Priority: 5

FlattenLevel

Class: FlattenLevel
Parameters: levelFlattened = n
Result: removes all classes of level n (their subclasses are directly
connected to their superclasses).
Functional description: for each class of that level, attaches,
directly, its subclasses to its superclasses (including constraints,
i.e., anonymous Restriction classes); attaches, directly, its
instances to its superclasses; converts all restrictions involving
it (allValuesFrom, someValueFrom) with its superclasses;
converts all property restrictions involving it (domain, range)
with its superclasses; removes the class. Updates the alignment by
suppressing all correspondences involving the suppressed classes.
Priority: 6

SuppressHierarchy

Class: SuppressHierarchy
No parameter
Result: suppresses all subclasses relations (but to Thing)

J. Euzenat et al. / Web Semantics: Science, Services and Agents on the World Wide Web 21 (2013) 3048

Table B.13
Finance results with 2012 test set (P = precision, F = F-measure, R = Recall).
Algo
Test

P. F. R. P.

P.

P.

P.

R.

R.

F.

F.

R.

F.

F.

P.

R.

P.

P.

P.

P.

F.

R.

F.

R.

F.

R.

F.

R.

F.

R.

Random

Avg
Maxvar
Continuous

Avg
Maxvar
Difficulty = 0.5

Avg
Maxvar
Difficulty = 0.7

Avg
Maxvar
Difficulty = 0.9

Avg
Maxvar

0.99 0.80 0.67 0.93 0.79 0.69 0.90 0.70 0.57 0.90 0.67 0.53 0.92 0.77 0.66 0.94 0.64 0.49 0.93 0.79 0.69 0.34 0.19 0.13 0.97 0.79 0.67
0.99 0.80 0.67 0.93 0.79 0.69 0.89 0.70 0.57 0.90 0.67 0.53 0.92 0.77 0.67 0.94 0.64 0.49 0.93 0.79 0.69 0.34 0.19 0.13 0.97 0.79 0.67
0.99 0.79 0.67 0.93 0.79 0.69 0.90 0.69 0.57 0.90 0.67 0.53 0.92 0.77 0.66 0.94 0.64 0.49 0.93 0.79 0.69 0.33 0.18 0.13 0.97 0.79 0.67
0.99 0.79 0.67 0.93 0.79 0.69 0.90 0.70 0.57 0.90 0.67 0.53 0.92 0.77 0.66 0.94 0.64 0.49 0.93 0.79 0.69 0.34 0.19 0.13 0.96 0.79 0.67
0.99 0.80 0.67 0.93 0.79 0.69 0.89 0.69 0.57 0.90 0.67 0.53 0.92 0.77 0.66 0.94 0.64 0.49 0.93 0.79 0.69 0.34 0.19 0.13 0.97 0.79 0.67
0.99 0.80 0.67 0.93 0.79 0.69 0.90 0.70 0.57 0.90 0.67 0.53 0.92 0.77 0.66 0.94 0.64 0.49 0.93 0.79 0.69 0.34 0.19 0.13 0.97 0.79 0.67
0.00 0.01 0.00 0.00 0.00 0.00 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.01 0.00 0.01 0.00 0.00

0.99 0.80 0.67 0.93 0.79 0.69 0.90 0.70 0.57 0.90 0.66 0.52 0.91 0.78 0.68 0.95 0.64 0.49 0.93 0.79 0.69 0.34 0.19 0.13 0.96 0.78 0.66
0.99 0.80 0.67 0.93 0.79 0.69 0.90 0.70 0.57 0.90 0.66 0.53 0.92 0.77 0.66 0.94 0.64 0.48 0.92 0.78 0.68 0.34 0.19 0.13 0.96 0.79 0.67
0.99 0.80 0.67 0.93 0.79 0.69 0.90 0.70 0.57 0.90 0.66 0.53 0.92 0.78 0.67 0.95 0.64 0.49 0.93 0.79 0.69 0.34 0.19 0.13 0.96 0.79 0.67
0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.01 0.01 0.02 0.01 0.00 0.01 0.01 0.01 0.01 0.00 0.00 0.00 0.00 0.01 0.01

0.99 0.51 0.34 0.91 0.71 0.58 0.89 0.61 0.46 0.89 0.61 0.46 0.86 0.68 0.57 0.92 0.50 0.34 0.91 0.69 0.55 0.24 0.13 0.09 0.94 0.69 0.55
0.99 0.52 0.35 0.91 0.71 0.58 0.89 0.61 0.46 0.89 0.61 0.46 0.87 0.69 0.57 0.92 0.50 0.34 0.92 0.69 0.56 0.25 0.13 0.08 0.94 0.69 0.55
0.99 0.52 0.35 0.91 0.71 0.58 0.89 0.61 0.46 0.89 0.61 0.46 0.87 0.69 0.57 0.92 0.50 0.34 0.92 0.69 0.56 0.25 0.13 0.09 0.94 0.69 0.55
0.00 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.01 0.00 0.00 0.00 0.00 0.01 0.00 0.01 0.01 0.00 0.01 0.00 0.00 0.00

0.97 0.63 0.46 0.90 0.66 0.52 0.88 0.55 0.40 0.89 0.57 0.42 0.86 0.58 0.44 0.91 0.41 0.26 0.89 0.63 0.48 0.19 0.10 0.06 0.92 0.63 0.48
0.97 0.63 0.46 0.90 0.66 0.52 0.89 0.55 0.40 0.89 0.58 0.43 0.86 0.58 0.44 0.91 0.41 0.26 0.89 0.63 0.48 0.19 0.09 0.06 0.92 0.63 0.47
0.97 0.63 0.46 0.90 0.66 0.52 0.89 0.55 0.40 0.89 0.58 0.43 0.86 0.58 0.44 0.91 0.41 0.26 0.89 0.63 0.48 0.19 0.10 0.06 0.92 0.63 0.48
0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.01

0.97 0.57 0.40 0.89 0.63 0.49 0.89 0.51 0.36 0.90 0.55 0.40 0.85 0.52 0.37 0.90 0.35 0.22 0.85 0.60 0.46 0.15 0.07 0.05 0.91 0.59 0.43
0.97 0.57 0.41 0.89 0.63 0.49 0.89 0.51 0.36 0.90 0.55 0.40 0.86 0.50 0.35 0.90 0.35 0.22 0.85 0.60 0.47 0.15 0.08 0.05 0.91 0.59 0.43
0.97 0.57 0.41 0.89 0.63 0.49 0.89 0.51 0.36 0.90 0.55 0.40 0.86 0.51 0.36 0.90 0.35 0.22 0.85 0.60 0.47 0.15 0.08 0.05 0.91 0.59 0.43
0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.02 0.02 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.01 0.00 0.00 0.00 0.00

Functional description: suppresses all subClassOf assertions
towards other named classes (assertions towards Restrictions
remain). The alignment is not modified.
Priority: 7
