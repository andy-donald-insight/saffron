Web Semantics: Science, Services and Agents on the World Wide Web 21 (2013) 12

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

journal homepage: www.elsevier.com/locate/websem

Editorial
On a steady path to semantic technology evaluation

Semantic technologies have become a well-established field of
computer science. However, the field is continuously evolving:
the number of semantic technologies is constantly increasing,
standards evolve and new ones are defined. In this context,
the problem of how to compare and evaluate the various
approaches becomes crucial. The consistent evaluation of semantic
technologies is critical not only for future scientific progress, by
identifying research goals and allowing a rigorous examination of
research results, but also for their industrial adoption, by allowing
objective measurement and comparison of these technologies and
enabling their certification.

Semantic technology evaluation must, on the one hand, be
supported by strong methodological approaches and relevant
test data and, on the other hand, satisfy the differing needs
of developers, researchers and adopters by addressing those
quality characteristics that are relevant to each target group.
Nevertheless, numerous issues must be faced when evaluating
semantic technologies.

For example,

the rapid evolution of

the semantic field
necessitates the adaptation and extension of existing evaluation
methods or, indeed, the creation of new ones. However, the cost
of defining new evaluation methods or reusing existing ones can
be prohibitive, so facilitating the understanding of such methods
or their automated processing becomes highly significant.

This special issue brings together the latest research on semantic technology evaluation. The selection of papers is representative
of the actual trend in semantic technology evaluation in which,
while evaluation is mainly driven by research, there is increasing
interest in determining whether semantic technologies can cope
with industrial requirements and, conversely, whether industry is
ready for the adoption of such technologies (which is the topic of
one of the papers).

It is apparent that the semantic technology community is
working towards creating sets of standardised evaluations. As in
more mature fields (such as Information Retrieval or Automatic
Speech Recognition), a common approach is the organisation of
open evaluation campaigns which promote wider participation,
distribute the effort of evaluation execution and persist over time.
Four papers stem from such evaluation campaigns and, even
if they cover a limited set of the broad semantic technology
spectrum, they provide food for thought for any evaluators in the
field.

Evaluating Question Answering over Linked Data (by Lopez,
Unger, Cimiano, and Motta) describes the QALD evaluation
campaigns, which aim to evaluate natural language based question
answering systems over linked data. In these campaigns, systems
are asked to return,
language question
and a RDF data source, a list of correct answers (resources or
literals). Two different closed datasets (DBpedia and MusicBrainz)

for a given natural

1570-8268/$  see front matter  2013 Published by Elsevier B.V.
http://dx.doi.org/10.1016/j.websem.2013.08.003

were selected,
in order to minimise resource consumption
and support reproducibility, and,
for each dataset, different
hand-crafted natural language questions were defined, which
are annotated with keywords and their corresponding SPARQL
queries. In addition, participants were provided with an evaluation
infrastructure to support executing the evaluation and assessing
their results. The paper presents the results both of the seven tools
participating in the two formal QALD evaluations and of another
four tools using the QALD evaluation resources independently. The
campaigns highlighted the interest of participants in evaluating
against heterogeneity (in terms of schema and domain) and that
evaluation results depend on the characteristics of the queries, of
the data, and of the ontology used to describe such data.

Repeatable and Reliable Semantic Search Evaluation (by
Blanco, Halpin, Herzig, Mika, Pound, Thompson, and Tran)
describes an evaluation framework for semantic search where
systems are asked to retrieve either entities or sets of matching
entities from RDF crawled from the Web. This framework uses
both simple and complex real-world queries from actual query
logs (from Yahoo! Search and Microsoft Live Search), exploits the
Billion Triples Challenge 2009 dataset, and has been used in the
Semantic Search Challenge 2010 and 2011 evaluation campaigns,
for which the paper presents detailed results. Acknowledging
the high cost of defining good evaluation data and inspired
by crowdsourcing-based evaluations already performed in other
fields, the authors significantly reduce the cost of defining a gold
standard for their evaluation framework by crowdsourcing its
creation. Furthermore, after applying this approach in the two
evaluation campaigns, the authors check that this novel approach
for the gold standard definition is not only affordable, but also
reliable and repeatable.

Ontology matching benchmarks: generation, stability, and
discriminability (by Euzenat, Rosoiu, and Trojahn) describes a
generator of ontology matching test data that is based on an extensible set of permutators which may be used programmatically
for generating different test data from different seed ontologies.
The motivation behind this generator is to increase the difficulty
and discriminability of the Benchmark test data, which has been
used in the Ontology Alignment Evaluation Initiative since its first
edition. The paper shows the importance of analysing the different
characteristics of the test data generation process and of the generated data, as well as the correlations between them; the authors
observe the stability of results over generations and the preservation of difficulty across seed ontologies. Furthermore, they check
how decisions made when defining test data (or its generation pro-
cess) affect its characteristics and intended use; e.g., the Benchmark test data and the generator presented in the paper are useful
for characterising matchers in terms of the problem space but are
not suitable for discriminating between different matchers.

Editorial / Web Semantics: Science, Services and Agents on the World Wide Web 21 (2013) 12

Evaluation of Instance Matching Tools: The Experience of
OAEI (by Ferrara, Nikolov, Noessner, and Scharffe) describes the
methodology used for instance matching and data linking evaluation that has been applied in the instance matching sub-track of
the Ontology Alignment Evaluation Initiative. The paper provides
a comprehensive view on the evolution of this track during three
consecutive evaluation campaigns (OAEI 2009, 2010, and 2011)
and analyses in detail the relevant characteristics of test data for
instance matching evaluation (both real-world and automatically-
generated) as well as future requirements for such test data. In the
paper we can see how the evolution of test data requires taking into
account new metrics in the evaluation and imposes new requirements over systems. Specifically, it shows that when using larger
test data, efficiency and scalability become relevant and that systems need to be capable of coping with such data volumes. Further-
more, the authors discuss the difficulty of defining (or selecting) a
gold standard and ensuring its quality and of how to achieve a fair
comparison between systems, either by restricting their behaviour
or by letting them perform at their best (e.g., by using domainspecific knowledge as mentioned in the paper).

Unlike the four preceding papers, Towards Savvy Adoption
of Semantic Technology: From Published Use Cases to CategorySpecific Adopter Readiness Models (by Nekvasil and Svatek)
is not based on a particular evaluation campaign. Instead, it
aims to define a layered adopter readiness model (inspired by
Capability Maturity Models) that allows potential adopters of
semantic technologies to assess their readiness to benefit from
these technologies. To this end, the authors define a lightweight
method that, starting from a collection of use cases (those
collected by the W3C Semantic Web Education and Outreach 
SWEO  Interest Group), categorises semantic applications by
first describing them according to a set of categorisation criteria
and then clustering them. Then, critical success factors in the
development and deployment of such applications are defined
and later assessed by the developers of the use cases through
questionnaires. These application categories and critical success
factors are the input for defining the adopter readiness model
that can support organisations in discovering whether they satisfy
the success factors in a certain application category. The authors
also evaluate the application clustering approach (by comparing
to a previous analysis of the SWEO use cases) and the adopter
readiness model (by re-applying the method to the last use cases
that appeared in the SWEO catalog).

Current evaluation initiatives show a firm progress towards
maturity in the semantic technology field by moving from onetime activities to collaborative evaluation campaigns. However,

the field still demands further standardised evaluations and there
is still room for improvement in the current ones, as we can see in
this special issue.

The community needs new evaluations, metrics and test data
and special care must be taken to analyse their characteristics
(e.g., representativeness, reproducibility or discrimination are
analysed in these papers). New evaluations should be grounded
under common principles and terminology, facing new challenges
beyond those purely coming from the semantic field (e.g.,
multilingualism).

The creation or selection of test data is vital in any evaluation
effort. Since the definition of good test data from scratch is costly,
in this set of papers we can observe different approaches followed
by evaluators: either to reuse existing datasets, to implement data
generators, or to crowdsource its creation. However, even if there
are datasets or generators available, the shortest path is not always
the best one. The characteristics of the test data used must not
be underestimated: test data are focused on specific problems and
must evolve to ensure that they remain challenging for state of the
art systems. Therefore, evaluations should be supported by a rich
corpus of test data that can be used for different purposes.

It is evident that defining and performing evaluations is
expensive and evaluators need to minimise costs. These papers
exemplify different ways of doing so by joining efforts across
institutions, building upon previous evaluation efforts, automating
evaluations, and sharing evaluation resources: test data (and data
generators), evaluation infrastructures, and evaluation results.

We would like to thank our team of reviewers for their
dedication to producing this special issue during the whole review
process. Thanks are also due to Steffen Staab and Silke Werger for
their timely support.

Raul Garcia-Castro
Universidad Politecnica de Madrid, Spain
E-mail address: rgarcia@fi.upm.es.

Stuart N. Wrigley
University of Sheffield, UK

Jeff Heflin
Lehigh University, USA

Heiner Stuckenschmidt
University of Mannheim, Germany
