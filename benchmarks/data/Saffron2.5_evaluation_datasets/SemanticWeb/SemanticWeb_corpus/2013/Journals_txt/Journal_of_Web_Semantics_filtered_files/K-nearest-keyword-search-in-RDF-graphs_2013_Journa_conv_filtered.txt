Web Semantics: Science, Services and Agents on the World Wide Web 22 (2013) 4056

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

journal homepage: www.elsevier.com/locate/websem

k-nearest keyword search in RDF graphs
Xiang Lian, Eugenio De Hoyos, Artem Chebotko, Bin Fu, Christine Reilly

Department of Computer Science, University of Texas - Pan American, Edinburg, TX 78539, USA

h i g h l i g h t s
 We propose a novel and useful k-nearest keyword (k-NK) query over RDF data graphs.
 We design effective pruning strategies for k-NK queries with and without schema.
 We present effective indexing mechanisms to greatly facilitate pruning strategies.
 We integrate our pruning and indexing methods into efficient k-NK query procedure.
 We conduct extensive experiments to confirm the efficiency of our approaches.

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 16 October 2012
Received in revised form
5 May 2013
Accepted 7 August 2013
Available online 2 September 2013

Keywords:
RDF graph
Nearest keyword search
Semantic Web

Resource Description Framework (RDF) has been widely used as a W3C standard to describe the resource
information in the Semantic Web. A standard SPARQL query over RDF data requires query issuers to fully
understand the domain knowledge of the data. Because of this fact, SPARQL queries over RDF data are not
flexible and it is difficult for non-experts to create queries without knowing the underlying data domain.
Inspired by this problem, in this paper, we propose and tackle a novel and important query type, namely
k-nearest keyword (k-NK) query, over a large RDF graph. Specifically, a k-NK query obtains k closest pairs
of vertices, (vi, ui), in the RDF graph, that contain two given keywords q and w, respectively, such that ui
is the nearest vertex of vi that contains the keyword w. To efficiently answer k-NK queries, we design
effective pruning methods for RDF graphs both with and without schema, which can greatly reduce
the query search space. Moreover, to facilitate our pruning strategies, we propose effective indexing
mechanisms on RDF graphs with/without schema to enable fast k-NK query answering. Through extensive
experiments, we demonstrate the efficiency and effectiveness of our proposed k-NK query processing
approaches.

 2013 Elsevier B.V. All rights reserved.

1. Introduction

Resource Description Framework (RDF) is a W3C standard [1]
to capture resource information in real-world applications such
as social networks, biological databases, and the data analysis in
the Semantic Web [1]. Specifically, RDF data can be represented
by triples in the form of (subject, predicate, object). For example,
Fig. 1(a) shows a set of ten RDF triples, showing the organization
and department in a university, as well as their relationships. The
first RDF triple (university,hasBand,band) indicates that this
university has a band organization, where university, hasBand,
and band are subject, predicate, and object, respectively.

 Corresponding author. Tel.: +1 9566652472.

E-mail addresses: lianx@utpa.edu (X. Lian), ejdehoyos@utpa.edu (E. De Hoyos),

chebotkoa@utpa.edu (A. Chebotko), bfu@utpa.edu (B. Fu), reillycf@utpa.edu
(C. Reilly).

1570-8268/$  see front matter  2013 Elsevier B.V. All rights reserved.
http://dx.doi.org/10.1016/j.websem.2013.08.001

Equivalently, RDF triples can be represented by a graph which
links data from various domains of human knowledge in the world.
In the previous university example, RDF triples in Fig. 1(a) can be
described as a graph shown in Fig. 1(b), where subjects/objects
are labels of vertices, and predicates refer to labels of edges. In
particular, RDF triple (university,hasBand, band) in Fig. 1(a)
can be transformed to an edge with label hasBand (in Fig. 1(b)),
having its 2 ending vertices labeled by university and band,
respectively.

In real applications such as DBpedia [2] and YAGO [3], RDF
graphs constitute the backbone of the Semantic Web, and feature
billions of interconnected facts (or RDF triples) that are published
on the Web. Thus, it is very useful and important to study efficient
answering of various queries over such RDF graphs.

SPARQL [4,5] is a standard language for querying RDF data. One
example of a SPARQL query is given below, which aims to obtain an
organization and its member such that this organization published
an album called Sun, and its members name contains John. In

(a) RDF triples.

(b) An RDF graph of Fig. 1(a).

Fig. 1. An example of RDF triples and RDF graph.

query. With the k-NK query, users can identify the relationship between vertices (or keywords) in the RDF graph by specifying only
two query keywords q and w (without knowing the domain knowl-
edge). In particular, a k-NK query returns k closest pairs of vertices (vi, ui) in the RDF graph such that vertices vi and ui contain
keywords q and w, respectively, and ui is the nearest vertex of vi
among all vertices containing keyword w, where the distance between two vertices is the shortest path distance in the RDF graph.
In the previous example, if we want to explore the reason for
the popularity of John, we can issue a k-NK query with two query
keywords John (= q) and Sun (= w) over the RDF graph in
Fig. 1(b), where k = 2. In the figure, the painting Sun and album
Sun are nearest neighbors of John Green and John Smith
(both with the shortest path length 2), respectively. Therefore, the
2-NK query returns 2 pairs with paths below:
 (John Green hasName prof  paints Sun); and
 (Sun hasAlbum band hasMember John Smith).
Note that, in the example above, we only consider the nearest
keyword Sun of those vertices containing John. The intuition is
that, the nearest Sun has the closest relationship with John. For
example, the band member John Smith in Fig. 1(b) is more related
to the album Sun (with distance 2), than the painting Sun (with
distance 5). The case of professor John Green is similar.

Fig. 2. A query graph Q for the SPARQL query.

the where cause of the SPARQL below, regex(string, keyword)
is a filtering function which returns true if string contains keyword (false, otherwise).

select
where {?organizationhasMember?member.

?organization ?member
?organizationhasAlbum Sun.
FILTER regex(?member, John)}

In the context of RDF graphs, SPARQL can be equivalently
translated to a subgraph matching query [68] over a large RDF
data graph. Fig. 2 shows a query graph Q that is transformed from
the SPARQL query above. As a result, the SPARQL query can be
conducted by retrieving those subgraphs in the RDF data graph
(e.g., Fig. 1(b)) that match with the query graph Q .

Although SPARQL is powerful enough to retrieve any query answer we want, it has two disadvantages. First, in order to compose
SPARQL queries, one has to know the ontology and vocabulary of
RDF data very well. In other words, the query issuers should be familiar with the graph structure (e.g., the one in Fig. 1(b)) and labels
of vertices/edges (e.g., hasAlbum and hasMember in Fig. 2).
However, this requirement is extremely difficult for those nonexpert users who do not have the domain knowledge about RDF
graphs. This is especially true when the RDF graph is of large scale.
For example, a typical W3C Linking Open Data project, DBpedia [2],
involves around one billion triples extracted from Wikipedia,
which would produce a large RDF graph with a large number of
vertex/edge labels. Thus, it is infeasible, if not impossible, for common users to express queries in this graph using SPARQL.

Second, SPARQL provides no or limited support for queries that
aim to discover relationships between known resources. As an ex-
ample, assume that we know someone called John is very popular in the university recently, and people are talking about him with
the hot (frequent) word Sun. In this case, we may want to explore
the relationship between John and Sun in the university data
of Fig. 1(b). However, without knowing the ontology/vocabulary
information or possible path lengths between John and Sun,
we cannot even compose a SPARQL query graph. Another possible
alternative is to manually explore an RDF graph to find paths between John and Sun. Nonetheless, with billions of edges, this
does not seem to be an efficient and feasible option either.

Inspired by the aforementioned problem that cannot be effectively solved via SPARQL, in this paper, we formulate and tackle
a flexible and useful query, namely the k-nearest keyword (k-NK)

The k-NK query is also useful in many other applications. For ex-
ample, when a biologist studies the relationship between dog and
other species, he/she can perform a k-NK query with keywords, say
dog and wolf, over the biology RDF graph. This way, one can
find k closest relationships between different types of dogs and
wolves. Furthermore, in social networks (e.g., Facebook), we can
model the entire network as a graph, where each person corresponds to a node, and two nodes are connected by an edge if their
corresponding persons are friends. Moreover, in this graph, each
node (person) contains some annotated keywords extracted from
their profile, posts, or comments. To discover social relationships
between people with different features, we can issue a k-NK query
with two feature keywords, and do a case study to pairs of people
returned from social networks (i.e., k-NK query answers).

Since the k-NK query involves the nearest keyword neighbor
search in RDF graphs, it is different from traditional keyword
search problem [912], which obtains a subgraph that contains all
keywords and has the highest ranking score. Even in the case where
two keywords are specified and the ranking function considers the
shortest path distance, the resulting keyword-search answer might
be a graph (rather than a path between two keywords), or contain
duplicate keywords (e.g., 2 Johns and 1 Sun in Fig. 1(b)), which
is not our desired k-NK answer.

To the best of our knowledge, this is the first work that considers the retrieval of the nearest keyword neighbors in RDF graphs,
and existing techniques cannot be directly applied to solve our
k-NK problem. In order to efficiently answer k-NK queries, in this
paper, we explore efficient query answering techniques which

X. Lian et al. / Web Semantics: Science, Services and Agents on the World Wide Web 22 (2013) 4056

utilize effective pruning strategies and indexing mechanisms.
In particular, we propose three approaches, pre-computation,
schema-based, and non-schema-based approaches, which are used
in different scenarios (e.g., over RDF graphs with or without
schema). We also confirm the effects of our proposed approaches
on various real/synthetic RDF data sets through experiments.

Specifically, in this paper, we make the following contributions.
1. We formulate the problem of the k-nearest keyword (k-NK)
query over RDF data graphs in Section 2, which has practical
applications in the Semantic Web and cannot be easily solved
by traditional SPARQL query engine for RDF.

2. We design effective pruning strategies in Sections 4.1 and 5.2
to reduce the search space of the k-NK query over RDF graphs
with and without schema, respectively.

3. We propose effective indexing mechanisms to organize RDF
graphs in Sections 4.3 and 5.3, and facilitate efficient k-NK
query answering over RDF graphs with and without schema, in
Sections 4.4 and 5.4, respectively.

4. We demonstrate through extensive experiments the efficiency
and effectiveness of our proposed k-NK query processing
approaches in Section 6.
In addition, Section 7 briefly overviews previous works on
RDF graphs, keyword search, and nearest neighbor search. Finally,
Section 8 concludes this paper.

2. Problem definition

2.1. Data model for RDF graphs

RDF triples. RDF can be used to model data in the Semantic Web,
and this data can be represented by triples as defined below.

Definition 2.1 (RDF Triples). Denote pairwise disjoint infinite sets
of Internationalized Resource Identifiers (IRIs), blank nodes, and
literals as IRI, BN, and L, respectively. An RDF triple, ti, is a tuple
(s, p, o)  (IRI  BN)  IRI  (IRI  BN  L), where s, p, and o are
subject, predicate, and object, respectively. 

RDF graph databases. We can equivalently represent a set of RDF
triples, (s, p, o), by a directed RDF graph, in which vertices correspond to subjects (s) or objects (o), and edges are labeled by predicates (p), connecting from vertices s to o. Here, s, p, or o must
contain at least one keyword.

Formally, an RDF graph database D consists of RDF graphs G

defined as follows.

Definition 2.2 (RDF Graph). An RDF graph, G, is a triple (V (G),
E(G), (G)) such that:
 V (G) is a finite set of vertices vi, each of which is associated with
a set of keywords K (vi);
 E(G) is a finite set of directed edges eij, each of which is
associated with a keyword set K (eij); and
 (G) is a mapping from V (G)  V (G) to E(G), which contains
(vi, vj)  eij, indicating that edge eij is connecting vertices from
vi to vj. 
In Definition 2.2, V (G) is a set of vertices vi associated with
keyword sets, K (vi), appearing in subjects or objects of RDF triples;
similarly, E(G) is a set of directed edges, eij, having keyword sets,
K (eij) in predicates of RDF triples.

2.2. Definition of k-nearest keyword search over RDF graphs

We next present the formal definition of our k-nearest keyword

(k-NK) queries.

Fig. 3.

Illustration of the k-NK problem.

Definition 2.3 (k-Nearest Keyword Query in RDF Graphs, k-NK
Query). Given an RDF graph G, two query keywords q and w, and
a user-specified integer k, a k-nearest keyword (k-NK) query in G
retrieves k pairs of vertices, (u1, v1), (u2, v2), . . . , and (uk, vk) in
V (G)  V (G), as well as the shortest paths between vertex pairs,
such that:
 vertices ui and vi (for 1  i  k) contain query keywords q and
w, respectively;
 vi is the nearest vertex of ui that contains keyword w; and
 the shortest path distance from ui to vi is smaller than that
from u (with keyword q) to its nearest vertex v containing
keyword w, for u  (V (G)  {u1, u2, . . . , uk}). Formally, for
any vertex u  {u1, u2, . . . , uk} with keyword q  K (u) and its
nearest vertex v with w  K (v), we always have dist(ui, vi) 
dist(u, v) (1  i  k),

where function dist(x, y) outputs the shortest path length from
vertex x to vertex y in the RDF graph G. 

In Definition 2.3, the k-NK query obtains k pairs of vertices ui
and vi in the RDF graph, where ui contains keyword q, and vi is
the nearest neighbor of ui that has keyword w. Intuitively, the
k-NK query returns k vertices vi containing keyword w that have
the closest relationships with vertices ui with keyword q.
The order of k-NK query keywords. Note that, if we swap the two
given keywords q and w, the resulting k-NK query answers might
be different. This difference is not simply to swap the roles of nodes
from (ui, vi) to (vi, ui). The reason for this is that, we consider the
nearest keyword w of a query keyword q (i.e., the two keywords
are ordered). In other words, although the nearest keywords w
of keyword q (in node ui) is in node vi, the nearest keyword q of
keyword w (in node vi) may not be in node ui.

We use an example in Fig. 3 to illustrate the k-NK query results
with different orders of query keywords, where the numbers
associated with the dashed edge indicate the shortest path length
between any two vertices in the RDF graph and k = 2. We can see
that, if we change the order of the two query keywords q and w,
we can obtain two different answer sets, that is,{(u1, v1), (u2, v1)}
and {(v1, u1), (v2, u1)}. Thus, the answer sets are different if we
change the order of query keywords.
Differences from the top-k closest pair problem and the existing
keyword search problem. A top-k closest pair query obtains k
pairs of vertices that contain 2 query keywords, respectively, and
have the smallest distances. In contrast, our k-NK problem is a
different query type and considers the condition of the nearest
neighbor/keywords in the RDF graph. In the previous example
of Fig. 3, when k = 2, our k-NK query returns the answer set
{(u1, v1), (u2, v1)}, however, the top-k closest pair query returns
{(u1, v1), (u1, v2)} (since they have the two shortest path lengths).
Therefore, they are two different query types, with different
returned answer sets.

Moreover, a general keyword search (KWS) problem obtains k
subgraphs (Steiner trees or graphs) that contain all query keywords
w1, w2, . . . , and has k smallest summed shortest path lengths
in the RDF data graph. The differences between k-NK and KWS
problems are two folded. First, the 2 query keywords, q and w,
in the k-NK query have a strict order, whereas KWS does not

Table 1
Symbols and descriptions.

Symbol

vi or ui
eij
K (vi) or K (eij)

q, w
dist(u, v)
lb_dist(u, v)
ub_dist(u, v)

Description
An RDF graph
A vertex in the vertex set V (G) of RDF graph G

A directed edge vivj in the edge set E(G) of RDF graph G
A set of keywords associated with vertex vi or edge eij
An RDF graph database containing RDF graphs
An ontology of RDF data
An ontology graph of RDF data
Two query keywords specified by k-NK queries
The shortest path distance between vertices u and v
The lower bound distance between vertices u and v
The upper bound distance between vertices u and v

require such an order (i.e., multiple query keywords have equal
importance). Second, the query predicates of k-NK and KWS differ
from each other. That is, k-NK considers the nearest neighbor
w of keyword q (other vertices with keywords w have longer
distance to q), whereas KWS only takes into account the shortest
path length between any two query keywords. Even in the case
where both queries specify 2 query keywords, the answer sets
of the two problems might be different. In the example of Fig. 3,
the 2-NK query would return {(u1, v1), (u2, v1)}, whereas a top-2
KWS problem returns 2 subgraphs that contain query keywords
q and w and have the smallest summed path lengths, that is,
{(u1, v1), (u1, v2)} (or even subgraphs {(u1, v1, v2), (v1, u1, u2)}
with duplicate keywords). Thus, k-NK and KWS problems return
different answer sets.

In this paper, we will simply consider querying keywords in
vertices (i.e., K (ui)). The case of querying keywords in edges, eij, can
be easily extended by conducting k-NK queries over a modified RDF
graph (i.e., adding a vertex v between vertices ui and uj on edge eij,
and associating v with keyword set K (eij)).
Challenges. One straightforward method to solve the k-NK problem
is to enumerate each vertex pair (ui, vi) that contains keywords q
and w, respectively, conduct a single-source search in RDF graphs
to obtain the shortest path between ui and vi, and retrieve k closest
pairs (ui, vi) such that vi is the nearest vertex of ui that contains
keyword w (i.e., k-NK query answers). However, this method can
be very inefficient in RDF graphs. This is because, in a large RDF
graph, there can be many vertices ui with keyword q, and their
nearest keywords w (in vertices vi) might be far away from ui. In
this case, we have to refine many candidates by traversing a large
portion of the graph inefficiently. What is worse, in RDF graphs,
some vertices near ui (with keyword q) may typically have high
in-/out-degrees (i.e., hot-spots). Thus, during the graph traversal,
we have to access many unnecessary vertices through hot-spots
(even if answer paths do not contain hot-spots), which makes the
straightforward method rather inefficient.

Inspired by the aforementioned challenges, we aim to efficiently obtain k-NK query answers by designing effective pruning
strategies. Furthermore, we will propose effective indexing mechanisms to encode RDF graphs and facilitate efficient k-NK query an-
swering. Table 1 summarizes the symbols that are commonly used
in this paper, along with their descriptions.

3. Pre-computation approach

In this section, we first present a pre-computation approach (PC),
which offline pre-calculates the nearest keywords for all keyword
pairs (q, w), and inserts them into a B+ tree index for efficient
online retrieval. Specifically, the PC approach starts from every
vertex u in RDF graph G, and obtains all of its nearest keywords
w in vertices v. Then, assuming keyword q is in vertex u, we can
have keyword pairs (q, w) (held in vertex pairs (u, v)), as well as
their in between distances d (= dist(u, v)).

Next, we define a linear order to triples (q, w, d), where q, w,
and d form an ordered composite key. We insert triples with
composite keys into a B+-tree (i.e., when two triples have the same
keywords q and w, the one with smaller d has lower rank than that
with larger d).
Complexity analysis. For the offline pre-computation of triples, we
can start from each vertex/edge, and traverse the entire graph in
a breadth-first manner to retrieve k-NK answers. The time and
space complexities are O((|V| + |E|)  k  (|K (V )| + |K (E)|))
and O(k  (|K (V )| + |K (E)|)2), respectively, where |K (V )| (or
|K (E)|) is the number of distinct keywords in vertices (or edges)
of G. Moreover, since parameter k is online given, we have to
pre-compute data and construct B+-tree index for each k value.
Therefore, such a pre-computation is time- and space-inefficient,
in addition to being prohibitively infeasible and unscalable for
large RDF graphs. Nonetheless, the online k-NK retrieval via the
B+-tree is very efficient, with the O(logF U + k) time complexity,
as given in Theorem 3.1.
Theorem 3.1. A B+-tree can support insertion, and deletion in
O(logF U) time, and the k-NK retrieval in O(logF U + k) time, where
F is the average fanout of B+-tree, U is the total number of precomputed triples, and k is the parameter in k-NK queries.
Proof. Please refer to Appendix A. 
Road map. Since the PC approach has low query cost but high
space/pre-computation costs for large RDF graphs, we next propose approaches to make the trade-off between space and querying time costs. In particular, we will present an effective pruning
strategy to filter out false alarms, based on space-efficient offline
pre-computations. Then, according to the special features of RDF
graphs, we consider two scenarios, RDF graphs with and without
schema. Correspondingly, we fully utilize this feature, and design
two indexing mechanisms, respectively, for searching k nearest
keywords in RDF graphs with/without schema.

4. Schema-based k-NK query answering

In this section, we propose an efficient approach for answering k-NK queries in RDF graphs with schema (WS-k-NK). Specifically,
Section 4.1 illustrates the basic idea of our pruning methods to
reduce the k-NK search space. Then, to enable the pruning, Section 4.2 designs cost-model-based synopses for RDF graphs, over
which an schema-based index is built in Section 4.3. Finally, Section 4.4 discusses efficient WS-k-NK query answering through the
constructed index.

4.1. Pruning strategy

We first present the rationale of our k-NK pruning strategy,
which is fundamental for RDF graphs both with and without
schema. Assume that we can somewhat quickly compute (either
offline or online) lower and upper bounds of the distance between
any two vertices in the RDF graph. Then, our k-NK pruning method
will utilize these bounds to rule out false alarms, and greatly reduce
the k-NK search space.

Without loss of generality, denote lb_dist(x, y) and ub_dist(x, y)
as lower and upper bounds of distance between two vertices x
and y in the RDF graph G, respectively. We have the k-NK pruning
strategy in the following lemma.

Lemma 4.1 (k-NK Pruning Strategy). Let  be the k-th smallest upper
bound of distances dist(u, v) for vertex pairs, (u, v), we have seen
so far, where vertex u contains query keyword q, and vertex v is the
nearest neighbor of u that has keyword w. Then, any vertex pair
(u, v) (containing keywords q and w, respectively) that satisfies
lb_dist(u, v) >  can be safely pruned.

X. Lian et al. / Web Semantics: Science, Services and Agents on the World Wide Web 22 (2013) 4056

Proof. According to the lemma assumption, let (u1, v1), (u2, v2),
. . . , and (uk, vk) be k vertex pairs with the smallest distance upper
bounds we have seen so far. Then, we have ub_dist(ui, vi)   , for
any 1  i  k. Since it holds that dist(ui, vi)  ub_dist(ui, vi), by
the inequality transition, we obtain dist(ui, vi)   .
Therefore, for any vertex pair (u, v) with lb_dist(u, v) >
it holds that dist(u, v) 
 , by the inequality transition,
lb_dist(u, v) >   dist(ui, vi), where 1  i  k. In other
words, the distance between u and v is greater than that of at least
k vertex pairs (ui, vi). Hence, vertex pair (u, v) cannot be the k-NK
query result, and thus can be safely pruned, which completes the
proof. 

Intuitively, Lemma 4.1 utilizes distance lower/upper bounds
between vertices u and v (containing keywords q and w,
respectively), and filters out those false alarms that can never be
k-NK query answers (i.e., those vertex pairs with long distances).
Note that, the idea of pruning using distance bounds was also
used in the literature of spatial databases such as nearest neighbor
(NN) search [13], where Euclidean distance between objects is
considered. In contrast, our work focuses on a different query
type (i.e., k-NK query) which involves the nearest keyword search
(with the shortest path distances) in RDF graphs (rather than
spatial data). Thus, to achieve high pruning power, one critical yet
challenging issue is how to design efficient techniques, specific to
our k-NK query, and compute tight lower/upper distance bounds
in RDF graphs, which will be described below.

4.2. Derivation of distance bounds

As mentioned in Section 4.1, to facilitate the k-NK pruning,
we need to derive lower/upper bounds of distances between two
keywords q and w (in vertices u and v, respectively). In particular,
we will design a cost-model-based encoding technique to record
keywords and distances (between keywords) in RDF graphs, which
can help obtain distance bounds between keywords.
Synopses. Specifically, to facilitate the bound derivation, we first introduce a synopsis, namely keyword-distance bitmap (KD-Bitmap),
to encode keywords in vertices, as well as their distance infor-
mation. In particular, for each vertex u  V (G), we offline
pre-compute a synopsis, KD-Bitmap(u), which contains a 2-
dimensional bit matrix as shown in Fig. 4(a). The l-th column of
the bit matrix KD-Bitmap(u) represents a bit vector, BVl(u), that encodes keywords (e.g., w) with distance to vertex u equal to l. Each
bit vector BVl(u) is a hashing table of size B. The i-th bit position,
BVl(u)[i], is set to 1, if there exists at least one keyword w hashed
to this position; otherwise, we have BVl(u)[i] = 0. As an example,
in Fig. 4(a), keyword w is hashed to the fourth position (from top
down), and it is in a vertex with distance to u equal to 3. Thus, the
position BV3(u)[3] is set to 1.
Note that, to reduce the chance of conflict, the KD-Bitmap only
hashes those keywords that are nearest to vertex u. Moreover, here
we consider the pre-computation of KD-Bitmap(u) with width (i.e.,
x-dimension) ranging from 0 to a system parameter r, and height
B  (|K (V )| + |K (E)|).

To pre-compute the synopsis KD-Bitmap(u), we can start from
vertex u, and traverse the RDF graph in a breadth-first manner.
On each traversal level l (0  l  r), we map any newly encountered keyword w (never appearing on levels < l) to a position,
BVl(u)[H(w)], via a hashing function H() (i.e., set this position to
1). The pre-computation continues until the r-th level is reached.
Computation of distance lower bound. With the KD-Bitmap synopses
discussed above, given a vertex u and any keyword w, we can
immediately obtain a lower bound of the nearest distance from
vertex u to keyword w. Specifically, we sweep along the x-axis of
KD-Bitmap from left to right, starting from distance 0, and obtain

the first bit vector BVl(u) whose H(w)-th position is equal to 1.
In this case, index l of bit vector BVl(u) indicates the lower bound
distance from a vertex v (containing keyword w) to vertex u. Here,
l is a lower bound of the actual distance dist(u, v), due to possible
conflicts with other keywords in this position. That is, multiple
keywords may be hashed into the same position in the KD-Bitmap,
and thus we may underestimate the distance from u to w when
we look up the nearest w through the KD-Bitmap. In a special case
where all (r + 1) bit vectors have the H(w)-th position equal to
0, the lower bound distance lb_dist(u, v) is set to (r + 1).
KD-Vector. We observe that, to retrieve distance lower bound from
KD-Bitmap, we need to scan multiple bit vectors, which may not be
efficient. In fact, the only information we look up in the KD-Bitmap
is the (lower bound) distance w.r.t. a specific keyword. Therefore,
to reduce the search time in the KD-Bitmap, we can, instead, utilize
a vector, namely KD-Vector, of size B to encode such keyworddistance information for a vertex u, where each entry of KD-Vector
records the smallest distance to vertex u for all keywords hashed
to that entry.

As an example, Fig. 4(b) illustrates a KD-Vector corresponding
to the KD-Bitmap shown in Fig. 4(a). In particular, the fourth
(top-down) position contains value 3, indicating that all keywords
mapped to this position have the smallest distance 3 to vertex u.
In the case where no keywords (within r-levels from vertex u) are
mapped to a position, this position would store the value (r + 1).
Computation of distance upper bound. Similar to the distance lower
bound, we can also pre-compute the distance upper bound by
using another KD-Vector. That is, for each vertex u, we store a
KD-Vector, which is a vector of positions that store the longest
distances from the nearest keywords (hashed to those positions)
to vertex u. Without loss of generality, we denote KD-Vectorlb
and KD-Vectorub as KD-Vectors that store lower and upper bound
distances, respectively.
KD-Map. To reduce the confliction rate of keywords and obtain
tighter bounds, for each vertex u, we use multiple (m) KD-Vectors,
KD-Vectorz
m(u), to store
lower/upper distance bounds, which adopt m hashing functions,
H1(x), H2(x), ..., and Hm(x), respectively, where z can be either lb
or ub. We say that these m KD-Vectors form a KD-Map.

2(u), . . . , and KD-Vectorz

1(u), KD-Vectorz

(2)

(1)

For any query keyword w, we first obtain the hashed positions
via these m hashing functions, that is, H1(w), H2(w), . . . , and
Hm(w). Then, we retrieve lower/upper bound distances from these
hashed positions in m KD-Vectors, respectively, and take the
maximum/minimum values as the distance lower/upper bound.

KD-Vectorlb[Hi(w)]
KD-Vectorub[Hi(w)]

Formally, we have:
lb_dist(u, v) = mmax
i=1
ub_dist(u, v) = m
min
i=1
Cost model. Next, we present a cost model for measuring the
tightness of the distance bounds via KD-Maps. Specifically, due to
different distributions of keywords near any vertex u, we should
choose appropriate parameters of KD-Maps, such as the size of
KD-Vector, B, and the number of hash functions, m, in order to
achieve high pruning power. Our goal is to design a cost model for
the expected distance lower bound (as given in Eq. (1)), and tune
parameters B and m to maximize the lower bound (i.e., having high
pruning power).

In particular, we consider the cost model of the expected lower
bound distance given in Eq. (1). Denote X as a random variable
of lower bound distance stored in a position of a KD-Vector (via
one hashing function). Moreover, assume that for a breadth-first
search starting from vertex u, on the l-th level, there are nl newly
encountered keywords (0  l  r).

(a) KD-Bitmap.

(b) KD-Vector.

Fig. 4.

Illustration of KD-Bitmap and KD-Vector.

As a result, the probability that X is equal to a value d is:

d1

l=0


1  1

nl

nd

1  1

Pr{X = d} =

(3)

where nd is the number of newly encountered keywords on the
d-th level of the breadth-first search.
In Eq. (3), the first term is the probability that keywords are
not mapped to that position within (d  1) levels from vertex u,
whereas the second term is the probability that at least one (newly
encountered) keyword is hashed to that position on the d-th level.
Moreover, we can also obtain the cumulative probability that
X  d holds as follows.

nl
Pr{X  d} = 1  Pr{X > d}
1  1

= 1  d
= 1 

l=0 nl

(4)

l=0
1  1

Let X1, X2,

..., and Xm be random variables of lower bound
distances in KD-Vectors (of KD-Map) using m hash functions
H1(x), H2(x), ..., and Hm(x), respectively. Note that, these m variables follow the same probabilistic distributions as variable X,
given by Eqs. (4) and (5). Moreover, denote Xmax as the random
variable of lower bound distance in Eq. (1) (i.e., taking the maximum value among X1  Xm). That is, Xmax = maxm

i=1{Xi}.

Based on order statistics, we have:

Pr{Xmax = d} = m

  Pr{X = d}  Pr{X  d}m1

= m  Pr{X = d}  Pr{X  d}m1.

Thus, the expected value, Xmax, of variable Xmax is given by:

(5)

(6)

Xmax = r

d=0

d  Pr{Xmax = d}.

According to the cost model in Eq. (6), we can find appropriate
values of parameters B and m, such that Xmax is as high as possible.
We assume that the pre-computed KD-Map is constrained by a
budget of available memory, M. Thus, we require B  m  M.
To obtain good parameter values, we will enumerate value pairs
(B, m) under the M-constraint, and select a pair with the largest
estimated Xmax value (as given in Eq. (6)).

4.3. Index construction over RDF graph with schema

In this subsection, we assume that the data schema of RDF graph
G is known in advance. We will construct an index I over graph G
with schema for answering k-NK queries. In particular, this index
is built based on the ontology information (i.e., data schema) of

RDF data, and can be used to reduce the k-NK search space via our
proposed pruning methods.
Background of ontology-based RDF graph. In many real applications,
RDF data often follow a data schema, called ontology, which is a
summary of the underlying RDF data. As an example, the RDF triple,
(department-of-art,hasProf,prof), in Fig. 1(a), can be summarized by meta-data (department,hasProf,prof), where
department-of-art is an instance of the class department.

We give formal definitions of ontology and RDF ontology graph

below.

Definition 4.1 (Ontology). An ontology O is a tuple (C, l, P, dom,
rng), such that:
 C is a finite set of classes or concepts;
 l is a special concept representing literal;
 P is a finite set of predicates;
 dom : P  C is a function that determines the domain of a
property; and
 rng : P  C  {l} is a function that determines the range of a
predicate. 
In Definition 4.1, classes/concepts C, such as department, can
have multiple possible instances in real RDF data, for example,
department-of-art and department-of-computer-science.

j  V (G); and
j )  eO
i to vO

Similar to RDF triples, RDF ontology can be represented by a
graph structure, namely RDF ontology graph, described as follows.
Definition 4.2 (RDF Ontology Graph). Given an ontology O = (C, l,
P, dom, rng), an RDF ontology graph of O is a directed labeled graph
G = (V (G), E(G), (G)), such that:
 V (G) is a finite set of nodes, and each node vO
i  V (G) represents a class ci  C or literal l, which is a meta node that may
have one or multiple vertex instances in RDF data graphs;
ij  E(G) repre-
 E(G) is a finite set of directed edges, and each eO
sents a predicate pij  P, connecting from vertices vO
j , for
vO
i , vO
 (G) is a mapping from V (G)  V (G) to E(G), which contains
(vO
i , vO
ij is connecting vertices
j  rng(eO
from vO
Note that, in practice, an RDF ontology graph can be extracted
from an ontology or vocabulary defined in RDF Schema (RDFS) [14]
or Web Ontology Language (OWL) [15,16]. While OWL is more
expressive than RDFS and includes a number of advances features,
only the RDFS-subset of OWL is required for our RDF ontology
graph definition. Traditional definition of the ontology graph [17]
does not include nodes of literals. Nonetheless, to enable effective
filtering in our k-NK problem, we relax this requirement by adding
those edges connecting with literals back to the ontology graph.
Thus, when we refer to the ontology graph below, we always mean
the variant, that is, the ontology graph with literal nodes given in
Definition 4.2.

ij, indicating that edge eO
ij ) and vO

i  dom(eO

j , for vO

ij ). 

i to vO

X. Lian et al. / Web Semantics: Science, Services and Agents on the World Wide Web 22 (2013) 4056

Relationship between RDF ontology graph and data graph: In Definition 4.2, nodes in an RDF ontology graph, G, are associated with
classes ci, rather than keywords in real RDF data graphs. Therefore,
the ontology graph is a summary of RDF data graphs, indicating
the connectivity rules (and edge predicates) among vertices of different classes in data graphs. On the other hand, RDF data graphs
can be also considered as materialized ontology graph, by replacing
classes in nodes with their corresponding keywords. Thus, an RDF
data graph consists of multiple subgraph instances of the ontology
graph.
Index structure. Next, we will illustrate the tree index structure
over the ontology-based RDF graph. Intuitively, since RDF graphs
follow the data schema (i.e., the ontology graph), we can utilize this
ontology graph to obtain RDF graphs (graph instances defined later)
that are structurally similar to the ontology graph. Then, we group
these graphs with similar keywords in vertices, and recursively
construct super nodes (index nodes, associated with aggregated
synopses and data) on a higher level in the tree index. The reason
that we group graphs with similar vertex keywords is that, the
resulting super nodes (tree nodes) can achieve high pruning power.
Through this index, we can apply our proposed pruning strategies
to each index node we encounter (via synopses and aggregates),
and filter out false alarms of index nodes, thus avoiding the access
cost of those RDF graphs under the pruned nodes. In the sequel, we
will first present the structure of our tree index, followed by details
of the index construction.
Leaf nodes: To explain the details of the index, we first define the
notion of the graph instance:

Definition 4.3 (Graph Instance). For any subgraph g within RDF
data graph G, if g is a materialized ontology graph G (by replacing
classes in G with some keywords), then we say that g is a graph
instance of ontology graph G.

According to Definition 4.3, each graph instance contains
vertices/edges mapping to nodes/edges in the ontology graph. An
RDF data graph G usually consists of multiple graph instances,
which are materialized ontology graphs, and these graph instances
will be used as basic building blocks (units) in our index structure.
Specifically, as illustrated in Fig. 5, our index I is a tree structure,
in which leaf nodes on the bottom level exactly correspond to RDF
graph instances in RDF graph G. In order to enable the filtering,
within graph instances, we also store KD-Maps in their vertices
u, which encode keywords (within r levels from vertices u) and
distance lower/upper bounds.
Non-leaf nodes: Starting from leaf nodes, we can iteratively
construct non-leaf (intermediate) nodes of the tree I. In particular,
a non-leaf node N contains a set of entries Ni, as well as pointers
pointing to child tree nodes. Each entry Ni is a supergraph of those
graph instances under Ni, and the structure of the supergraph is
stored in the form of an adjacency list.

Moreover, each vertex of this supergraph is also associated with
synopses (i.e., KD-Vectors in KD-Maps), summarizing keywords at
this vertex position in all graph instances under Ni. Specifically,
each KD-Vector, KD-Vector(uO) (in KD-Map(uO)), of a vertex
uO in an entry Ni summarizes all vertices u in children of Ni
(mapped to uO in the ontology graph G). That is, each position
in KD-Vectorlb(uO) (or KD-Vectorub(uO)) takes the minimum
(maximum) value from KD-Vectorlb(u) (or KD-Vectorub(u)) for all
u in uOs children. That is, we have:
KD-Vectorlb(uO)[j] = min
uS(uO)

KD-Vectorlb(u)[j],

and
KD-Vectorub(uO)[j] = max
uS(uO)

KD-Vectorub(u)[j],

Fig. 5.

Indexing structure for ontology-based RDF graph.

where vertex uO is in entry Ni, and S(uO) is a set of vertices in
children of Ni mapping to uO.

Note that, as illustrated in Fig. 5, since there might be edges
across graph instances in different leaf nodes, edges may also
connect vertices in different intermediate nodes. Moreover, selfedges may exist in intermediate tree nodes, when vertices in
graph instances (under tree nodes) are connected and condensed
in intermediate nodes. Thus, adjacency lists in intermediate nodes
also need to record such information.
Index construction. The construction of the index (as shown in
Fig. 5) is in a bottom-up manner. That is, given those graph
instances (each corresponds to a leaf node) in G on the leaf level
(0-th level), we start to build non-leaf nodes on 1-st, 2-nd, ..., and
H-th levels in order, where H is the height of the tree.

Fig. 6 shows the pseudo code of constructing the index for WS-
k-NK query answering. Specifically, from bottom up, procedure
WS_k-NK_Index_Constructor creates non-leaf nodes on the (h +
1)-th level from nodes on the h-th level, by grouping/clustering
lower-level nodes via procedure Parent_Selection and obtaining
higher-level nodes (line 5), where 0  h  H  1.
Details of procedure WS_k-NK_Index_Constructor: Initially, there
are n(0) graph instances corresponding to n(0)
leaf nodes in
Child_Set (lines 12). Then, assuming that the current level is h with
n(h) tree nodes in Child_Set, the (h+ 1)-th level is expected to have
n(h+1) = n(h)/F nodes (line 4), where F is the fanout of the tree.
The process of obtaining parent nodes in Par_Set on the (h+ 1)-th
level is achieved by invoking procedure Parent_Selection (line 5).
Once we have parent nodes, we can create additional synopses for
those parent nodes in Par_Set (line 6), and let Child_Set be Par_Set
to prepare for constructing the next index level (lines 78). Finally,
the index construction stops when less than F groups are obtained,
that is, the root of the tree is obtained (line 3).
Details of procedure Parent_Selection:
In procedure Parent_Selection (lines 926), we group nodes on the
h-th level, based on the criterion of enhancing the pruning power
during the k-NK query answering.

Specifically, we first randomly select n child nodes from set
Child_Set, and form the parent set Par_Set (line 11). Then, we
assign other child nodes to their nearest nodes in Par_Set, by
using a distance measure (which will be discussed later in this
subsection; line 12). As a result, each node in Par_Set is associated
with an initial group of child nodes. Moreover, we also evaluate
the summed distance, local_dist, of all initial groups (line 13).
Next, we start to randomly swap a child node Nc that is not in
Par_Set with a random one in Par_Set (lines 1617). After the
swapping, we evaluate the new grouping strategy (with parent set
Par_Setnew), and obtain the summed distance, distnew (line 18). In
the case where new grouping strategy has smaller (better) distance
(i.e., distnew < local_dist), we will let the new grouping strategy
be the best-so-far strategy (lines 1921). The random swapping
terminates when a maximum number of iterations, max_local_cnt,
is reached (line 15). Furthermore, in order to avoid local optimum

Fig. 6.

Index construction for WS-k-NK query answering.

of the selected parent nodes, we repeat the above process for
max_global_cnt times (line 10), and obtain the grouping strategy
with the lowest distance measure (lines 2325).
Measure of the grouping strategy: Up to now, the only remaining
issue is how to measure the goodness of a grouping strategy,
which is used in lines 13 and 18 of procedure Parent_Selection.
Our intuition of the grouping is as follows. If we group those child
nodes with similar KD-Maps together, then the aggregated KDMaps in their parent node would expect to have high pruning
power. Therefore, we will measure the goodness of our grouping
strategies by the summed L1-norm distance between KD-Maps in
a parent node and that in its child nodes.

As an example, assume that KD-Map KD-Map(u) is KD-Map in
vertex u of a parent node N, whereas KD-Map(v) is KD-Map at
the same vertex position v in a child node Ni. Then, the L1-norm
distance between KD-Map(u) and KD-Map(v) is given by:
L1-norm(KD-Map(u), KD-Map(v))

|KD-Vectori(u)[j]  KD-Vectori(v)[j]|.

= m

B1

i=1

j=0

Intuitively, smaller L1-norm distance may lead to more similar
KD-Maps, and tighter distance bounds (in turn, higher pruning
power).

To calculate the measure, dist, for the entire group, we thus have

the following formula:

dist =


L1-norm(KD-Map(u), KD-Map(v)),

Ni

uNvNi

where u and v are vertices at the same vertex position in parent N
and child Ni, respectively.
Inverted index: In addition to the tree index, to help efficient
WS-k-NK query answering, we also maintain an inverted index,
where each entry of the inverted index corresponds to a keyword
q, and stores pointers pointing to vertices (that contain q) in the
root of index I. This inverted index can quickly find those vertices
that may contain some keywords.

Fig. 7. k-NK query answering over RDF graphs with schema.

4.4. WS-k-NK query answering procedure

In this subsection, we present the algorithm for processing
WS-k-NK queries, namely WS_k-NK_Processing, in Fig. 7. This
query procedure takes the index I over the RDF graph database
D, two query keywords q and w, and parameter k as the input, and
aims to obtain k-NK query answers (i.e., k vertex pairs).
Basic idea of WS-k-NK Query Answering: Intuitively, procedure
WS_k-NK_Processing traverses the index I, and prunes those
nodes on both leaf and non-leaf levels whose underlying graphs
cannot contain k-NK answers. In particular, let  be the k-th
smallest distance upper bound among candidates we have seen
so far. Then, we can safely prune those nodes whose lower bound
distance greater than  . This way, the cost of accessing those graph
instances under pruned nodes can be greatly saved.
Details of procedure WS_k-NK_Processing: Specifically, in order to
obtain k-NK query answers, we traverse the index I by maintaining
a minimum heap H (line 1). Entries in the heap H are in the form
(N, u, key), where N is a tree node (i.e., an intermediate or leaf
node), u is a candidate vertex that may contain query keyword q,
and key is the sorting key of the heap equal to the lower bound
distance from u to its nearest vertex v with keyword w. At each
step of the index traversal, an entry with the minimum key, key,
is popped out from the minimum heap H. Intuitively, when the
lower bound distance (i.e., key) is small, its corresponding node N
is more likely to contain k-NK query answers. Moreover, we also
keep a candidate set (initially empty), Scand, to record candidate
vertex pairs, and a threshold  (with initial value+) for the k-NK
pruning (line 2).

Given a k-NK query, we first look up the inverted index, and
find out those vertices uO in entries of root, root(I), that contain
query keyword q (line 3). Then, for each candidate vertex uO, we
obtain its distance lower and upper bounds, lb_dist(uO, vO) and
ub_dist(uO, vO), respectively, from KD-Map(uO) (line 4). Next, we
can set the threshold  to the k-th largest upper bound among candidate vertices uO (in case less than k candidate vertices are found,
set  to the largest upper bound; line 5). With  , we can prune those
entries with distance lower bound greater than threshold  . Only
for the remaining candidate vertices uO in entries Ni (with lower

X. Lian et al. / Web Semantics: Science, Services and Agents on the World Wide Web 22 (2013) 4056

bound lb_dist(uO, v) smaller than or equal to threshold  ), we insert them into heap H in the form (Ni, uO, lb_dist(uO, vO)) (lines
68).

During the index traversal, each time we pop out an entry
(N, uO, key) from the heap H with the minimum key. If key is
greater than threshold  , it indicates that all the remaining entries
in the heap have their distance lower bounds greater than  , and
thus can be safely pruned. Therefore, in this case, the loop can be
terminated (lines 911). Otherwise, we will further check children
in entry N below.

When the popped entry (N, uO, key) corresponds to a leaf node,
we will check each graph instance g in N (lines 1216). That is, we
compute the lower/upper bounds, lb_dist(u, v) and ub_dist(u, v),
for candidate vertices u in graph g (lines 1214). Then, we apply
the k-NK pruning method (in Lemma 4.1) to rule out those vertices
u with lower bounds lb_dist(u, v) greater than  ; the remaining
vertices u are candidates to be added to set Scand, and meanwhile
the threshold  is updated with the k-th smallest upper bound in
Scand (lines 1516).

) and ub_dist(uNi

Similar to the leaf node, when the popped entry (N, uO, key)
from the heap H is an intermediate node, we visit each child entry
Ni of tree node N, and via KD-Maps compute distance lower/upper
bounds, lb_dist(uNi
) (lines 1719). By
using the k-NK pruning (in Lemma 4.1), we can also safely filter
out those entries Ni satisfying lb_dist(uNi
) >  . The remaining
entries are candidates, and are thus inserted into heap H in the
form (Ni, uNi
The index traversal stops when either the heap H is empty (line
9) or the condition in line 11 holds. After the index traversal, we
need to refine candidates in the candidate set Scand, by using the
single-source search to compute the actual shortest path distances
dist(u, v) (line 22). Finally, k vertex pairs (ui, vi) with the smallest
distances are returned as the k-NK query answers (line 23).

)) (lines 2021).

, lb_dist(uNi

, vNi

, vNi

, vNi

, vNi

5. k-NK query answering without the schema

In this section, we propose an efficient approach for k-NK query

answering in RDF graphs without schema (WoS-k-NK).

5.1. Highlights of differences between WS-k-NK and WoS-k-NK

Practically, not all RDF data have the ontology (graph) available.
Although there are some existing works [8] that can construct a
summary graph (i.e., similar to the ontology graph) automatically
from an RDF graph without the data schema, the accuracy of the
resulting summary graph cannot be guaranteed. Thus, in the case
where we do not have (accurate) schema information on RDF
graphs, we need to design an efficient approach, called WoS-k-NK,
to handle the k-NK query answering over RDF graphs without any
schema.

The differences of WoS-k-NK from WS-k-NK (Section 4) are
twofold. First, WS-k-NK offline maintains synopses such as KD-
Maps, in order to compute distance bounds for the online pruning.
In contrast, WoS-k-NK not only utilizes synopses, but also uses
the selected vertices (pivots) to compute distance lower/upper
bounds between keywords (discussed later in Section 5.2), which
is expected to obtain tighter distance bounds (or higher pruning
power).

Second, the WS-k-NK approach is specifically designed for RDF
graphs that follow an ontology graph, and can fully use the ontology structure to construct the index (as discussed in Section 4.3).
As a result, for those RDF graphs without schema, this indexing
structure proposed for ontology-based RDF graphs can no longer
be applied. In contrast, the WoS-k-NK approach (including indexing mechanism), to be proposed in this section, can exactly answer
k-NK queries directly over RDF graphs, without requiring the ontology graph. Therefore, the WoS-k-NK approach is more general
and suitable for any RDF graph without knowledge of the schema.

Fig. 8. An example of upper bound derivation for WoS-k-NK.

5.2. Derivation of distance bounds for WoS-k-NK

Distance upper bound for WoS-k-NK. We first illustrate the basic
idea of computing the distance upper bound between any two
keywords q and w (in vertices u and v, respectively), by using the
example in Fig. 8. Without loss of generality, assume that we can
select two vertices (or called pivots), p1 and p2, from an RDF graph
G, and know their shortest path distance dist(p1, p2). Moreover, the
distance from a vertex v to p1 is upper bounded by an integer n, and
similarly, that from a vertex u to p2 is also bounded by n. That is,
we have dist(v, p1)  n and dist(u, p2)  n. Then, by applying the
triangle inequality in RDF graphs, we have the following lemma for
the upper bound, ub_dist(u, v), of distance dist(u, v).

Lemma 5.1 (Distance Upper Bound). Given vertices p1, p2, u, and v
in an RDF graph, if dist(p1, p2) is the exact shortest path length between vertices p1 and p2, dist(v, p1)  n, and dist(u, p2)  n, then
we have ub_dist(u, v) = dist(p1, p2) + 2  n.
Proof. Since the triangle inequality holds for the shortest path in
the RDF graph, according to the lemma assumption, we have the
following derivation:
dist(u, v)  dist(v, p1) + dist(p1, u)

 n + dist(p1, p2) + dist(p2, u)
 dist(p1, p2) + 2  n
= ub_dist(u, v).

Hence, the lemma holds. 
From Lemma 4.1, we can utilize the selected pivots to obtain a
distance upper bound between u and v. In particular, from the RDF
graph, we can choose a subset of vertices as pivots (e.g., p1 and p2),
such that each vertex in the graph has the shortest path distance
to its nearest pivot smaller than or equal to n (e.g., dist(v, p1)  n
and dist(p2, u)  n). This way, by offline pre-computing pairwise
shortest path distances among pivots, we can calculate the distance
upper bound between keywords for online k-NK queries.
Distance lower bound for WoS-k-NK. For distance lower bounds, we
will adopt the similar synopses discussed in Section 4.2. The only
difference is that the KD-Map is constructed with r  2  n. This is
because, the upper bound given in Lemma 5.1 is at least 2  n, and
we can only prune a candidate pair if their lower bound ( r + 1)
is greater than the distance upper bound ( 2  n).
In addition, we can also obtain a distance lower bound from
pivots. In particular, we have:
lb_dist(u, v) = dist(p1, p2)  dist(v, p1)  dist(p2, u).
The proof of the inequality above is similar to that of upper bound
derivation, using the triangle inequality.

Then, combined with the lower bound via synopses in Section 4.2, we will take the larger (tighter) one between the two
lower bounds (from synopses and pivots), and set it as the lower
bound of dist(u, v).

5.3. WoS-k-NK indexing

Until this point, we have discussed the basic idea of pruning
with pivots for WoS-k-NK queries on RDF graphs without schema.
Next, we will illustrate how to build an index to facilitate the WoS-
k-NK query processing.

Pivot selection. One important yet challenging issue remains, that
is, how to select the pivots to enable pruning.

From Lemma 5.1, we want to choose a small subset of pivots such that, each vertex has distance to its closest pivot not exceeding a parameter n. Note that, we call this subset of pivots
n-dominating set defined as follows.

Definition 5.1 (n-Dominating Set, n-DS). Given an RDF graph G and
an integer n, an n-dominating set, nDS(V (G)), contains a minimum
number of vertices in V (G), such that for any vertex u  V (G), there
exists a vertex v  nDS(V (G)) satisfying dist(u, v)  n.
Note that, in a special case where n = 1, the n-DS problem in
Definition 5.1 is exactly the classical dominating set problem [18],
which is NP-complete. When n > 1, the n-DS problem is also a hard
problem. Please refer to Appendix B for the detailed discussion.
Approximation approach: Since the time complexity of finding the
best (optimal) n-DS is rather high (e.g., intractable for n = 1),
we, instead, propose an efficient approximation approach below
to solve the n-DS problem with polynomial time complexity.
Specifically, to obtain an n-DS set, we first conceptually convert
the original RDF graph G into a new graph G by adding edges eij if
ui and uj have the shortest path length  n in G. Then, each step of
our approximation algorithm will remove a vertex u  V (G) with
the highest degree and its connected vertices/edges from G, and
add u to the n-DS set. This process of the vertex removal repeats,
until G becomes an empty graph. This greedy algorithm provides
a factor (1+ log|V|)-approximation of a minimum dominating set
in G. Raz and Safra [19] showed that no algorithm can achieve an
approximation factor better than c log|V| for some c > 0 unless
P = NP.

In the algorithm above, since in each step we remove a dominating vertex that can dominate the most other vertices, the resulting
n-dominating set is expected to have small size.
Index construction. After introducing the n-dominating set prob-
lem, we now focus on the construction of an index for the RDF
graph (without schema). Our basic idea is to build a hierarchical
tree structure, in which the i-th level of the tree index corresponds
to an i-dominating set of vertices in the original RDF graph, where
1  i  H, and H is the height of the tree.

Specifically, we construct the tree index in a top-down manner.
That is, we first generate an H-DS set for the root, root(I), of the
index I. Then, we continue to produce (H-1)-DS, (H-2)-DS, ..., and
1-DS, for levels (H-1), (H-2), ..., and 1, respectively. To enable the
pruning with bounds via pivots (as mentioned in Section 5.2), our
index should guarantee that the i-DS set (on the i-th level) should
be a superset of (i+ 1)-DS set (on the (i+ 1)-th level). Thus, when
we select vertices (pivots) in i-DS, we will use (i + 1)-DS as the
initial set, and then obtain the remaining pivots in i-DS, using the
approximation approach mentioned above.
In particular, on the (i + 1)-th level of the tree index (1  i 
H  1), once we have the (i + 1)-DS set, denoted as nDSi+1, we
will assign each vertex u  V (G) to its nearest pivot (i.e., (i + 1)-
DS vertex). This way, we can divide vertices of the RDF graphs into
|nDSi+1| partitions, each of which contains a pivot p(i+1)  nDSi+1,
having distances to other vertices in the same partition (i+1). As
a result, we can treat each partition as an entry in an intermediate
node on the (i + 1)-th level in the tree index.
Next, we will construct the i-th level of the index, with the i-DS
set nDSi. Similar to the (i + 1)-th level, we can also obtain |nDSi|
partitions of vertices containing pivots p(i). Let Pari and Pari+1 be
partitions on the i-th and (i + 1)-th levels with pivots pi and pi+1,
respectively. Then, if pivots p(i) (on Level i) fall into partition Pari+1
(on Level (i + 1)), we will group their corresponding partitions
(entries), Pari, together as one intermediate/leaf node, Nj, on the
i-th level.

Fig. 9. k-NK query answering over RDF graphs without the schema.

Within each tree node Nj on the i-th level, for each pivot p(i),
we store the pre-computed distance dist(p(i), p(i+1)) from (child)
pivot p(i) to its parent pivot p(i+1). Moreover, for p(i), we also
compute a synopsis KD-Maplb(p(i)) (as mentioned in Section 4.2) to
summarize distance lower bounds for the nearest keywords. Note
that, for an intermediate node, we calculate the KD-Map by taking
the minimum values among all KD-Maps from its children.

In addition, on Level H (i.e., root), we not only store the
information mentioned above, but also the pairwise distances in
the original RDF graph between pivots p(H). These distances would
be used to facilitate pruning with distance lower/upper bounds.
Furthermore, similar to the WS-k-NK indexing (in Section 4.3), we
also maintain an inverted index, where each entry corresponds to
a keyword, and points to a list of root entries, under which the leaf
nodes contain that keyword.

5.4. WoS-k-NK query processing

WoS-k-NK query procedure. We illustrate the k-NK query proce-
dure, namely WoS_k-NK_Processing, for RDF graph without the
schema in Fig. 9. Different from WS-k-NK (in Fig. 7), procedure
WoS_k-NK_Processing is conducted over the tree index I constructed via n-dominating sets (discussed in Section 5.3), rather
than the schema.

In Fig. 9, the index traversal is achieved by using a heap H
with entry (Nq, Nw, key), where Nq and Nw are two tree nodes,
under which keywords q and w may reside, respectively, and key
is defined as the lower bound distance between q and w under
nodes (line 1). The general traversal steps are very similar to that
of procedure WS_k-NK_Processing in Fig. 7. The only difference is
that the computation of lower/upper bounds utilizes the selected
pivots (in n-DSs) and synopses stored in tree nodes (lines 4, 14, and
19), as mentioned in Section 5.2.

Specifically, at each step an entry (Nq, Nw, key) is popped out
from heap H. When Nq and Nw are leaf nodes, we consider all
possible candidate pairs (u, v) (for u  Nq and v  Nw), use
lower/upper bounds for filtering (lines 1214). If candidate pairs
(u, v) cannot be pruned, then we add it to the candidate set
Scand (lines 1516). Similarly, when Nq and Nw are non-leaf nodes,

X. Lian et al. / Web Semantics: Science, Services and Agents on the World Wide Web 22 (2013) 4056

Table 2
The experimental settings.

Parameters

Keyword distribution
No. of keywords per vertex

|V|

Settings
4, 5, 6, 7
100, 200, 300, 400, 500
1, 2, 3, 4, 5
Uniform, Gaussian, Zipf
2, 3, 5, 8, 10
2, 4, 6, 8, 10
10 K, 20 K, 30 K, 40 K, 50 K

we also compute distance lower/upper bounds for all node pairs,
(Ni, Nj), where Ni  Nq and Nj  Nw (lines 1819). If candidate
) in (Ni, Nj) cannot be pruned via distance
vertex pairs (uNi
bounds, then we insert entry (Ni, Nj, lb_dist(uNi
)) into heap H
for further checking (lines 2021).

, vNj

, vNj

After visiting the tree index (lines 921), we can obtain a
candidate set Scand containing vertex pairs. Then, we will refine
these candidate pairs by calculating the actual shortest path
distances via the single-source search (line 22). Finally, we return
k actual k-NK answers with the smallest distances (line 23).
Discussions on k-NK query variant. Up to now, we have discussed
the k-NK query, which obtains k pairs of vertices (ui, vi) from
the RDF graph such that vertex vi with keyword w is closely
related to ui that contains keyword q. In practice, we can extend
the k-NK problem which involves only two query keywords q
and w to the one with L keywords (namely, the kL-NK query).
Specifically, we may be interested in retrieving k groups of vertices,
(ui, v(1)
), which contain L keywords, q, w1, . . . , and
wL1, respectively, and vertices v(1)
are the group
nearest neighbor (GNN) [20] of vertex ui (i.e., satisfying that the
i ) of k groups are the smallest
among all possible groups). Intuitively, we want to obtain those
vertices v(1)
that have close relationships with
vertex ui in the RDF graph, which are also very useful in real
applications, such as the Semantic Web and social networks, where
multiple interested features/keywords are considered.

summed distancesL1

, . . ., and v(L1)

, . . . , and v(L1)

j=1 dist(ui, v(j)

, . . . , v(L1)

bounds of the summed distanceL1

To tackle the kL-NK problem, we can apply the pruning strategy similar to Lemma 4.1. Instead of computing distance bounds
of dist(u, v), for the kL-NK query, we can compute lower/upper
i ), by underestimating or overestimating the bounds of dist(ui, v(j)
i ) via KD-Map
synopses (Section 4.2) or pivots (Section 5.2). Then, the kL-NK
query can be answered by traversing our constructed indexes (as
mentioned in Sections 4.3 and 5.3), applying the pruning strategy
to filter out false alarms, and finally refining the remaining candidate groups.

j=1 dist(ui, v(j)

6. Experimental evaluation

In this section, we test the performance of our proposed approaches to process k-NK queries on both real and synthetic data
sets. Specifically, we use two real data sets, DBpedia [2] and WordNet [21], and one synthetic data set, LUBM [22].
 DBpedia1 is one of the largest real-life RDF data sets that contain
structured information extracted from Wikipedia.
 WordNet2 is a lexical database for the English language, which
organizes English words into synonym sets according to part
of speech (e.g., noun, verb, etc.), and enumerates linguistic
relations between these sets.

 Lehigh University Benchmark (LUBM)3 is a popular benchmark
for RDF databases that includes the OWL university ontology,
RDF data generator, and a set of test queries.
For real data sets, we extract a connected subgraph with |V|
vertices by starting from a random vertex and expanding the subgraph (including all edges with both ending vertices in the sub-
graph) until the number of vertices is reached, and then obtain
keywords from text/abstract associated with each vertex in real
data. For synthetic data set, LUBM, we use the data generator to obtain graphs with |V| vertices, and keywords of vertices (each keyword corresponds to an integer) are randomly generated within a
range,[Kmin, Kmax) ([0, 1000) by default), following Uniform, Gaussian (with mean Kmin+Kmax
), or Zipf (with
skewness 0.8) distribution.

and variance KmaxKmin

In order to evaluate the k-NK query performance, we randomly
generate keywords from the keyword set, and obtain 50 keyword
pairs (q, w). We report the experimental results in terms of CPU
time and the number of candidates. In particular, the CPU time
is the average time cost (over 50 runs) to traverse the index and
retrieve k-NK candidates (including the I/O cost); the number of
candidates is computed by running 50 queries with 50 keyword
pairs and taking the average over all the 50 numbers of k-NK
candidate pairs after pruning through the index.

To the best of our knowledge, there is no prior work on answering k-NK queries over RDF graphs with/without the schema.
Therefore, one straightforward method is to explore every vertex
pair (u, v) that contains keyword pair (q, w), and perform a singlesource or bidirectional search, which is rather inefficient. In con-
trast, our work applies an effective pruning strategy to quickly
filter out many false alarms, and greatly reduce the search space.
Our experiments show that, our approaches usually incur much
fewer (e.g., 7150) candidate pairs to be refined (i.e., conducting
the costly single-source or bidirectional search) than this straightforward method (e.g., about 22K candidates) by 23 orders of mag-
nitude. Thus, our approaches can outperform the straightforward
method by orders of magnitude. For the sake of clearly illustrating
the trends of our approaches, in the sequel, we will only present the
results of our proposed 3 approaches, PC, WS-k-NK, and WoS-k-NK.
Table 2 summarizes the experimental settings, where the numbers in bold font are default values. For each set of experiments, we
will vary one parameter at a time, while setting other parameters
to their default values. All our experiments are conducted on a PC
with Intel Xeon E5645 2.4 GHz CPU with 64G memory.

6.1. Cost model verification and parameter tuning

In this subsection, we first verify the correctness of our proposed cost model in Section 4.2. Specifically, based on our cost
model, we want to choose a (B, m)-pair with high pruning power,
or tight distance lower bound Xmax, where B is the size of KD-Vector
and m is the number of KD-Vectors in a KD-Map. Thus, to evaluate the cost model, we will compare the estimated Xmax (denoted
as est) given in Eq. (6) with the actual distance lower bound (de-
noted as act). Fig. 10 shows the comparison results over LUBM data,
where B varies from 100 to 400, m = 1, and other parameters are
set to their default values. From the figure, we can see that the estimated values on LUBM data set are very close to the actual ones,
and moreover their trends are the same. The verification results
on the other 2 data sets, DBpedia and WordNet, are similar, which
confirms the effectiveness of our cost model. According to the cost
model and the empirical study (note: similar experimental results
by varying parameters B and m), in subsequent experiments, we
will set default values of (B, m) to (300, 3) for WS-k-NK.

1 DBpedia, http://dbpedia.org.
2 WordNet, a lexical database for English, http://wordnet.princeton.edu.

3 Lehigh University Benchmark, http://swat.cse.lehigh.edu/projects/lubm/.

because larger m value leads to more space cost and in turn
more computation cost to perform the filtering. Nevertheless, due
to the pruning effect of KD-Map with larger size, the remaining
candidates become fewer for larger m (as confirmed by Fig. 12(b)).

6.2. k-NK query performance

k-NK query performance vs. data sets. Fig. 13(a) reports the k-NK
query performance of our proposed 3 approaches over both real
and synthetic data sets, where parameters are set to default values.
In the figure, the PC approach incurs the smallest CPU time (about
0.1 ms), since PC has pre-computed every nearest-keyword pair,
and only need to retrieve the k-NK results directly through B+-
tree. In contrast, WS-k-NK and WoS-k-NK require about 1100 ms
for the filtering through the index, which is also small. Since WoS-
k-NK utilizes pivots to achieve higher pruning power, WoS-k-NK
needs smaller CPU time than WS-k-NK. This phenomenon holds
for other experimental results as well.

Although the PC approach has the lowest time cost, it is not
space efficient with respect to the graph size. As illustrated in
Fig. 13(b), with a graph only containing 30 K vertices, the space
used for PC indexing (B+-tree) is almost 1 TB over all the 3 data
sets. Moreover, as shown in Fig. 13(c), the time cost of constructing
the index (including pre-computations) for PC can be up to 10 h.
In contrast, indexes for WS-k-NK and WoS-k-NK only require
hundreds to thousands of Megabytes, and their index construction
times are at most 9 and 39 min, respectively. Therefore, for larger
graph with more vertices, PC is not very scalable or feasible, in
terms of both space and pre-computation time efficiency. Thus, in
the sequel, we will only present the results for scalable WS-k-NK
and WoS-k-NK.

Fig. 10. Cost model verification of Xmax in Eq. (6) (LUBM).

Moreover, we also evaluate the performance of the WS-k-NK
approach over 3 data sets in Fig. 11, by varying parameter r from 3
to 7, where other parameters are set to default values. Since the
distance lower bounds are expected to be tighter (higher) with
larger r value, the pruning power becomes higher. Thus, when r
increases, the CPU time slightly decreases in Fig. 11(a), and the
number of remaining candidates also decreases (due to higher
pruning power) in Fig. 11(b). For all the three data sets, their
numbers of candidates are similar, since we fixed the graph to the
same size |V| = 30 K. In the following experiments, we will test
the effect of other parameters by fixing r = 5.

In Fig. 12, we present the CPU time and the number of remaining
candidates for the WS-k-NK approach with different m values
(the number of KD-Vectors used in each KD-Map) from 1 to 5,
where other parameters have their default values. From Fig. 12(a),
we can see that with larger m, the CPU time increases. This is

(a) CPU time.

(b) No. of candidates.

Fig. 11. k-NK query performance vs. r.

(a) CPU time.

(b) No. of candidates.

Fig. 12. k-NK query performance vs. m.

X. Lian et al. / Web Semantics: Science, Services and Agents on the World Wide Web 22 (2013) 4056

(a) CPU time.

(b) Index size.

(c) Index construction time.

Fig. 13. k-NK query performance/index size/index construction time vs. data sets.

(a) CPU time.

(b) No. of candidates.

Fig. 14. k-NK query performance vs. keyword distributions (LUBM).

k-NK query performance vs. keyword distributions. Fig. 14 illustrates
the experimental results on LUBM data with different keyword
distributions. From figures, for all the 3 tested distributions, WoS-
k-NK consistently performs better than WS-k-NK. That is, WoS-
k-NK has lower CPU time and fewer candidates to be refined
than WS-k-NK. Nonetheless, both approaches achieve very low
CPU time (i.e., 1.697.8 ms), and small number of candidates
(i.e., 15150.2).
k-NK query performance vs. no. of keywords per vertex. For both
real/synthetic data, we extract/generate various numbers of
keywords for each vertex (keywords of real data are retrieved
from abstracts associated with vertices). Fig. 15 presents the query
performance of WS-k-NK and WoS-k-NK, by varying the number
of keywords per vertex from 2 to 10, where other parameters are

by default. Similar to previous results, WoS-k-NK outperforms WS-
k-NK by about 12 orders of magnitude. With more keywords
per vertex, the pruning power with synopses becomes lower
(due to hashing conflictions), and there are more candidates that
should be retrieved and refined. Thus, both approaches require
more filtering time, and incur more candidates. Nonetheless, the
CPU time remains low (i.e., 0.35109.39 ms), and the number of
candidates is small (7.13312.81).
k-NK query performance vs. k. Fig. 16 evaluates the effect of
parameter k on the k-NK query performance, where k varies from
2 to 10, and other parameters are set to default values. From
figures, we can see that, when k increases, the CPU time and the
number of candidates of WS-k-NK and WoS-k-NK slightly increase.
Nonetheless, both measures are not very sensitive to the increasing

(a) CPU time.

(b) No. of candidates.

Fig. 15. k-NK query performance vs. the number of keywords per vertex.

(a) CPU time.

(b) No. of candidates.

Fig. 16. k-NK query performance vs. k.

(a) CPU time.

(b) No. of candidates.
Fig. 17. k-NK query performance vs. |V|.

k, which indicates the robustness of our proposed pruning method
and k-NK query processing approaches.
k-NK query performance vs. |V|. Finally, we report the scalability of
our approaches, with respect to the graph size, that is, the number
of vertices |V| in the RDF graph. Fig. 17 shows the experimental
results, where |V| is varying from 10 K to 50 K, and other
parameters are set to their default values. When |V| increases, the
CPU time of both WS-k-NK and WoS-k-NK approaches becomes
higher linearly. Nevertheless, the CPU time remains low, that
is, 0.31164.04 ms. Similarly, the number of candidates linearly
increases with larger |V|, but remains low (7.11263.59), which
confirms the scalability of our proposed k-NK query answering
approaches.

Furthermore, we also conduct a set of experiments on our
WoS-k-NK approach over DBpedia data with graph size |V| =
500 K, by varying k from 2 to 10, and illustrate the results in Fig. 18.
Similar to results with default graph size |V| = 30 K in Fig. 16,
with larger k values, the CPU time and the number of candidates
increase. This is reasonable, since larger graph sizes require more
effort (time cost) to explore and lead to more possible k-NK answer
candidates. Nevertheless, the results confirm that our approach
can achieve low CPU time (less than 9 ms) and small number of
candidates (i.e., 1539), even against graph size |V| as large as
500 K.
We also did experiments by varying other parameters (e.g.,
fanout of indexes, B, etc.), and the trends of the query performance
are similar.

X. Lian et al. / Web Semantics: Science, Services and Agents on the World Wide Web 22 (2013) 4056

(a) CPU time.

(b) No. of candidates.

Fig. 18. Scalability test for the k-NK query performance vs. k (WoS-k-NK; |V| = 500 K; for default value k = 6, the index construction time is 5.79 h, and the index size is
23,960 MB).

Summary. From the experimental results, we can see that, our proposed cost model can closely and effectively estimate the performance trend of our approach, and thus provide a good measure to
guide the parameter tuning. Moreover, on different real/synthetic
data sets, both WoS-k-NK and WS-k-NK approaches can achieve
good performance, including low CPU time and small number of
candidate pairs. While the WoS-k-NK approach often achieves better query efficiency than WS-k-NK, WS-k-NK sometimes requires
smaller index space on some data sets (as shown in Fig. 13). Most
importantly, our proposed k-NK approaches are scalable against
different graph sizes, as confirmed by Fig. 17.

7. Related work

In this section, we overview previous work on RDF data, key-

word search, and nearest neighbor search.
RDF data. RDF data are widely used in the Semantic Web, and they
can be equivalently represented by several data models. For exam-
ple, triple store [2325] models RDF data by triples; C-Store (or column store) [2628] organizes each column (e.g., subject, predicate,
or object) of RDF data in a separate table; property tables [29,30]
store RDF data in several tables, each containing one or multiple
attributes; and RDF data can be also represented by graphs [8,31].
To issue queries over RDF data, a standard SPARQL query is often used, which is a SQL-like language. The existing works usually
aim to speed up the SPARQL query processing, by designing effective indexes or join plans, for example, C-Store [27], RDF-3X [32],
MonetDB [28], and Hexastore [23]. However, as discussed in Section 1, in order to compose SPARQL, users should know the RDF
schema (including ontology and vocabulary), which is not feasible
for non-expert users. Moreover, since our k-NK problem cannot be
effectively converted into SPARQL, previous techniques proposed
for answering SPARQL queries are not able to answer k-NK queries
efficiently.
Keyword search. In the literature of RDF graphs, previous works
[8,33] considered the keyword search over RDF graphs, which
returns RDF subgraphs that contain keywords and have the highest
ranking scores. In graph databases [8,12,3436], the existing works
usually studied how to obtain meaningful subgraphs that contain
query keywords, and thus proposed different semantics of ranking
subgraphs [912,34,35] such as the shortest path length [37],
IR-score, and so on. For example, Ladwig and Tran [36] returns
Steiner subgraphs that contain query keywords and the minimum
summed shortest path lengths in the data graph. Variant is also
considered, which restricts that the length of any direct path in the
returned answers should be smaller than or equal to a threshold d.

Although the keyword search (KWS) problem also involve
query keywords (similar to our k-NK problem), they are fundamentally two different query types in twofold. First, the 2 query
keywords, q and w, in the k-NK query have a strict order (since w
should be the nearest neighbor of q). In other words, if we swap the
two query keywords q and w and use them as the input of the k-NK
query, the results may be different. In contrast, in the KWS problem (given a ranking semantic), all the specified query keywords
have equal importance, and for any order of these keywords, the
resulting KWS answers are the same. Second, the query predicates
of k-NK and KWS differ from each other. That is, k-NK considers the
nearest neighbor w of keyword q (i.e., other vertices with keywords
w have longer distance to q), whereas KWS only takes into account
the shortest path length between any two query keywords (note:
the returned subgraphs may contain duplicate keywords). As a re-
sult, even in the case where both queries specify 2 query keywords,
the answer sets of the two problems can be different.

While all the aforementioned works above aim to tackle the
KWS problem by finding paths between vertices, the focus of our
k-NK problem is on how to find the nearest neighbor of a query
keyword q (rather than simply subgraphs containing keywords),
which is thus more challenging. Since prior works on the KWS did
not consider query predicates related to nearest keywords, their
proposed techniques are not directly applicable to our k-NK query
processing. Inversely, since our proposed synopses (i.e., KD-Map)
or pivot-based techniques can compute lower/upper distance
bounds between keywords in the graph, they may help the KWS
problem where the ranking semantics involve the shortest path
lengths between vertices in the graph. However, this is out of the
scope of this paper.
Nearest neighbor search. In spatial databases, the nearest neighbor
(NN) query is a classical query which retrieves the nearest data
point of a query point, where the similarity metric is measured
by Euclidean distance. Indexes, such as R-tree [38,39], are usually
constructed, and the NN query is processed by traversing the
index in either depth-first [13] or best-first [40] manner. Moreover,
Papadias et al. [41] studied the nearest neighbor search in a
spatial road network (planar graph), which utilizes the relationship
between spatial distance and network distance to enable the
pruning. In contrast, our work considers a different k-NK problem,
which involves the shortest path distance in RDF graphs (rather
than Euclidean distance or in spatial planar graph), and searches
the nearest keywords instead of nearest objects (note: not all
objects contain the given keywords). Further, RDF graphs are not
road networks, thus, the pruning with spatial information in [41]
cannot be used. Therefore, the existing NN approaches cannot be
directly applied to our k-NK problem.

With the linear order, we can build up a B+-tree with all the
triples (p, w, d) saved in tree nodes on the leaf level. We also build
up a linked list among the leaves in the B+-tree (standard B+-tree
structure). Thus, both insertions and deletions are standard operations in our constructed B+-tree. For the k-NK retrieval, we just
spend O(logF U) time in finding the triple (q, w, d) with the least
d, and following the linked list to obtain the next k triples with the
smallest distances. 

Moreover, Wu et al. [42] assumed that each object in the spatial
database is associated with its location and a set of keywords. They
tackled the problem of spatial keyword queries, which obtain k
objects that contain query keywords and are the closest to a query
point. The proposed pruning/indexing techniques utilized the
spatial information to reduce the search. The problem studied in
[42] differs from our problem in both data model and query types.
That is, the data model used in [42] was under the Euclidean space
(rather than RDF graphs in our work), whereas the spatial keyword
and k-NK problems have different query predicates (i.e., the spatial
keyword query fixes the query point, and the k-NK query only
specify the query keyword q which may exist in any vertices in the
RDF graph). Therefore, we cannot directly borrow the techniques
in [42] to solve our k-NK problem.

In the context of XML databases, Tao et al. [43] studied a
problem that retrieves the nearest keyword of a given node in
an XML document. In particular, the authors utilized an interval
encoding approach to obtain the pre-computed data from the XML
tree with linear space (w.r.t. database size), which can be used for
answering the nearest keyword search query. Our k-NK problem is
different from [43] in that k-NK is conducted in RDF graphs (rather
than XML trees), and it obtains k closest nearest-keyword pairs
(q, w), instead of 1-nearest keyword of a specified query node. As
a result, the interval encoding approach proposed for trees in [43]
is not applicable to that for RDF graphs, and thus their proposed
techniques cannot be directly used for answering our k-NK query.

8. Conclusions

In this paper, we formulate and tackle an important query,
namely k-nearest keyword (k-NK) query, over a large RDF graph. To
efficiently answer k-NK queries, we present three approaches, pre-
computation, schema-based, and non-schema-based approaches.
In particular, we present an effective pruning strategy, which rules
out false alarms of k-NK answers over RDF graphs with and without
schema. Then, we design effective indexes to facilitate efficient k-
NK query processing. Finally, we demonstrate through extensive
experiments the efficiency and effectiveness of our proposed
approaches over real/synthetic RDF data sets.

Acknowledgments

This research is supported in part by NSF HRD-1137764 and NSF

Early Career Award 0845376.

Appendix A. Proof of Theorem 3.1

Theorem A.1. A B+-tree can support insertion and deletion in
O(logF U) time, and the k-NK retrieval in O(logF U + k) time, where
F is the average fanout of B+-tree, U is the total number of precomputed triples, and k is the parameter in k-NK queries.
Proof. Based on our PC approach, in the B+-tree, the set of triples
(q, w, d) has a linear order using q, w, and d as a composite key,
where q and w are two keywords, and d is the distance between
the two vertices holding the two keywords, respectively, in the
graph G.
Each composite key is assigned an integer. For two triples (q,
w, d) and (q, w, d), we say (q, w, d) < (q, w, d) if one of the
following conditions is true:
1. q < q,
2. q = q, and w < w,
3. q = q, w = w, and d < d

Appendix B. Discussions on the hardness of n-DS problem

From Definition 5.1, we want to select a minimum subset
of vertices from the RDF graph G, satisfying the n constraint.
Note that, it is a hard problem to select an optimal (minimum)
n-dominating set (n-DS). In particular, the NP-hardness for 1-
DS follows from the NP-hardness of the classical dominating set.
Moreover, the n-DS problem (for n > 2) is also NP-complete, which
can be proved by the following theorem.

Theorem B.1 (NP-Completeness of the n-DS problem). Assume that
n is a positive integer parameter greater than 2.
1. The n-dominating set problem is NP-hard.
2. Assume that (|V|) is a nondecreasing function.

If there is
time (|V|)-approximation algorithm for the
a polynomial
dominating set problem, there is a polynomial time (|V|)-
approximation algorithm for the dominating set problem.

3. Assume that (|V|) is a nondecreasing function.

If there is
time (|V|)-approximation algorithm for the
a polynomial
n-dominating set problem, there is a polynomial time (n|V|2)-
approximation algorithm for the dominating set problem.

Proof. Statement 1. It is well known that dominating set problem
is NP-complete. We just reduce the dominating set problem to
n-dominating set. Let G(V , E) with parameter t be a graph for the
classical dominating set problem. A graph G(V, E) is constructed
such that each edge in E is added n  1 new nodes in the middle.
Assume that H is a dominating set of G. We also have that H
forms a n-dominating set for G.
Assume that H is an n-dominating set of G. If there u  H such
that u is a new node added to an edge (v1, v2)  E, then remove all
nodes that have distance at most n with u in G. We will show by
an induction that G has an n-dominating set of size h if and only
if G has an n-dominating set of size h, and all nodes in it is from
the nodes in G. The basis is trivial when G has one or two vertices.
Assume that it is true when G has |V| nodes. Now we consider the
case that G has |V| + 1 nodes.
Let G be the new graph after removing those nodes from G.
Clearly, G has an n-dominating set with size at most|H| 1. Thus,
G has an n-dominating set H that has size |H|  1, and only
consists of the nodes in the original graph G. Assume that u is closer
to v1 than v2. All the neighbors of v2 should be in H. Thus, H{u1}
is a n-dominating set of G.
Therefore, G has a dominating set of size h if and only if G
has an n-dominating set of size h. Therefore, n-dominating set is
NP-complete.
Statement 2. Assume that A is an (|V|)-approximation
algorithm for the classical dominating set problem. Let G(V , E) be a
graph the for n-dominating set problem. Construct graph G(V , E)
such that there is an edge (u, v)  E if and only if the distance
between u and v in G is at most n. Apply A to G to get an (|V|)-
approximation algorithm for the n-dominating set.
Statement 3. It follows from the proof of Statement 1. Note that
in the proof of Statement 1, for a graph G, we construct a graph G
that has at most (n  1)|E|  n2|V|2 nodes. 
