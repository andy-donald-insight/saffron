Web Semantics: Science, Services and Agents on the World Wide Web 21 (2013) 6174

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

journal homepage: www.elsevier.com/locate/websem

Towards savvy adoption of semantic technology: From published use
cases to category-specific adopter readiness models
Marek Nekvasil a, Vojtech Svatek b,

a Adastra Business Consulting, Karolinska 654/2, 186 00, Praha 8 - Karlin, Czech Republic
b University of Economics, Prague, Nam. W. Churchilla 4, 130 67, Praha 3, Czech Republic

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 15 March 2012
Received in revised form
9 May 2013
Accepted 20 May 2013
Available online 27 May 2013

Keywords:
Information systems
Semantic technology
Clustering
Critical success factors
Readiness

1. Introduction

The decision of organizations to invest (or not) into a semantic application is, currently, often based
on vague considerations and personal feelings. What is lacking is a model that would help determine
whether semantic approaches would be adequate, given the aspects of the particular business and
concrete adopter. Such a model would however need to take into account the heterogeneity of
different applications that exhibit semantic features. We present a thorough exercise, and a prototypical
methodology abstracted from it, for proceeding in multiple steps, from loosely sorted and purely textual
descriptions of semantic applications to structured and instructive adopter readiness models. The whole
process relies on expert-level manual analysis of textual descriptions, automatic cluster analysis (leading
to plausible categories of semantic applications), critical factor analysis, questionnaire survey addressing
the developers of applications, and adaptation of principles known from building multi-layer Capability
Maturity Models. Although the overall approach relies to a large degree on (potentially subjective) manual
analysis, very lightweight quantitative evaluation was also made for relevant steps in the process.

 2013 Elsevier B.V. All rights reserved.

During the last decade, a constantly growing amount of semantic applications and tools has been moving from research labs
towards possible industrial exploitation. However, for a semantic
application to be feasible in the commercial environment it is necessary to justify the investments tied with it. As in other areas of
information technology, the costs of such investment can be identified with a certain effort, but quantifying the gains is very difficult,
if not impossible. This is caused by the fact that the main advantages of such systems are of non-monetary in nature and there are
many views on what the actual gains are. Much more essential in
assessing the investment is estimating (or defending) the overall
feasibility of the application and determining the necessary conditions under which the whole project will not cause monetary
losses. These necessary conditions could be summarized as a kind
of adopter readiness and expressed in terms of critical success factors (CSFs). If an organization does not satisfy a significant number
of CSFs, it should consider first moving to a higher level of readiness
and only then endeavoring to implement a semantic approach.
One of obstacles to formulating this kind of models is, however,

 Corresponding author. Tel.: +420 224095495.

E-mail addresses: marek.nekvasil@adastra-abc.com (M. Nekvasil),

svatek@vse.cz (V. Svatek).

1570-8268/$  see front matter  2013 Elsevier B.V. All rights reserved.
http://dx.doi.org/10.1016/j.websem.2013.05.003

the fact that the nature of semantic applications is very diverse
from project to project: from applications that integrate data from
different sources, support the search in a diverse range of data,
and derive new relationships across heterogeneous databases, including the application support of social networking; management
decision-making; annotating and indexing of any content; up to
information extraction from unstructured sources; and even socalled Business Intelligence 2.0 [1]. In a sense, the broad coverage
of semantic technologies is not caused by the omnipotence of a sin-
gle, coherent set of technological principles (like in the relational
database technology) but by the fact that there is no generally accepted definition of what is a semantic application and what is not.
As a lightweight compromise, we can adopt the definition according to [2], postulating that any application that stores data separately from the meaning and content files, and at the same time
does not have the meaning hard-wired into the application code,
can be called a semantic application. This concept includes the use
of semantic data and ontology languages (such as RDF, RDFS, OWL,
etc.), rule-based systems and many other possibilities. Although
the standards developed for the Semantic Web dominate this field,
the concept of semantic technologies (or even semantic web tech-
nologies) is far from being exclusive to the web environment.

Hence, the conditions of feasibility and potential profitability
cannot be set generally but it is necessary to identify some
categories of knowledge-based applications in the first place. Once
these categories have been identified, it becomes possible to
formulate the requirements on each of the substantially different

M. Nekvasil, V. Svatek / Web Semantics: Science, Services and Agents on the World Wide Web 21 (2013) 6174

kinds of semantic applications. Identification of the most common
categories of semantic applications is thus the first, successive, goal
of the project underlying this paper. We hypothesize that it will
be possible to isolate some of the substantial properties of each
category. At this level of abstraction it will then be more adequate
to establish relevant sets of critical success factors (in the sense
of relation to Key Performance Indicators [3]). The second aim of
this work is thus to identify some of the most important CSFs of
deploying (and developing) the semantic applications. Moreover,
once the set of critical success factors has been verified, it will be
possible to take this deliberation one step further and formulate
the adopter readiness models for deployment of knowledge-based
applications (inspired by Capability Maturity Models [4]). The
required verification of CSF validity can be carried out by means of
a questionnaire survey addressing the authors and users of current
semantic applications.

Published reports on existing semantic applications, such as
those maintained by the (now closed) W3C Semantic Web Education and Outreach (SWEO) Interest Group, are obviously meant to
spread out experience on working solutions that could influence
wider adoption of the underlying technology. However, such textbased reports offer only a limited possibility for learning lessons
with regard to new applications, for the following reasons:
 The individual use cases, having been compiled by different
people at different times and without comprehensive guide-
lines, lack metadata that would allow their systematic/focused
exploration.
 It is not obvious to what degree (if at all) the application can be
considered successful in the longer term, and what factors were
important for such success.
 Insufficient information is provided regarding the prior characteristics of the organization in which the application was de-
ployed.
The presented research outlines a methodology allowing a
technology analyst to proceed from a list of verbal use cases of
semantic (or, possibly, other information) technology applications
to a structured model that provides insights into the eligibility
conditions for particular application categories; to a limited
degree, the model should be able to operationally predict the
readiness of a certain organization for an application category.
In brief, the contributions of the presented research are
 the methodology itself
 exploration (though tentative rather than rigorously system-
atic) of detailed computational techniques applicable within
the methodology, such as cluster number estimation or solidity calculation
 actual results of the analysis and synthesis, obtained in the
given study based on the SWEO catalog: types (clusters) of
applications; critical success factors; readiness models.
The paper is structured as follows. First, the overall methodology is briefly outlined (Section 3). Next, a survey of related research
is provided (Section 2). Then the design of categorization criteria is
explained, and the result of applying the criteria to the existing set
of use cases is described (Section 4). The clustering process, which
exploits the values of categorization criteria for each application, is
then described and its results shown (Section 5). This is followed by
an explanation of the design of critical success factors, and the description of the questionnaire that builds on these CSFs; the questionnaire results then allow us to partially associate CSFs with application clusters (Section 6). Then the last component type of the
model inventory, the readiness models, is presented (Section 7). An
attempt to quantitatively evaluate both the clustering model and
the readiness models follows (Section 8). The paper concludes with
a summary discussion and conclusions, showing the possibilities
for future work (Section 9).

This research is based on the first authors Ph.D. Thesis, published as [5], although most of the text has been completely rewritten and new evaluations have been added. A very early phase of the
research (manual analysis of the use cases) has also been published
as the workshop paper [6].

2. Related research

The principles of evaluation valid for complex, fielded semantic
applications currently seem rather underexplored. In the following
we provide an overview of relevant efforts, divided into four
categories.

2.1. Systematic analysis of deployed use cases

In 2007 the W3C Semantic Web Education and Outreach
(SWEO) Interest Group initiated the collection and publishing of
case studies for existing applications and potential use cases that
take advantage of semantic technologies in practice.1 The main
criterion for acceptance in this catalog was that the submission
should describe a real project, possibly deployed in practice or at
least under evaluation for deployment. This provided an unprecedented pool of materials for subsequent analysis, allowing the
semantic technology community to gain insights into the current
position of semantic applications and their usability it the industrial environment. Although the extent of this catalog is, after the
initial burst, growing very slowly (of the 46 case studies to date,
only 6 were contributed in 2010 or later) and thus gradually becoming obsolete, it still seems unmatched when considering its
size in combination with the degree of elaboration of the studies.
The first comprehensive analysis of this catalog was probably
that presented by Sauermann [7], who attempted to identify the
benefits addressed by the individual applications using manual
analysis of all studies to date. More recently, Della Valle et al. [8]
used the catalog for a questionnaire-based survey aiming to assess
the industrial need for a methodology to be used in designing semantic web applications, as well as barriers for semantics adop-
tion, including individual components such as difficult-to-use
tools.2 Three domains of semantic applications were explicitly
defined (probably as a rule of thumb): Knowledge Management,
Enterprise Information Integration and Information Retrieval.

Independent of the SWEO catalog, an analysis of semantic
technology use cases was also addressed in the VALUE-IT project
[9,10], which relied on both a questionnaire survey and interviewing approach. However, VALUE-IT mapped the potential of
semantic technologies in a blanket fashion, hence few of the questionnaire responses actually corresponded to successful adopters
of semantics. Even for the interviews already targeting promising
use cases, the descriptions are rather scanty. This contrasts with
the elaborate textual descriptions of publicly announced use cases
in the SWEO catalog, which allowed the authors of [8]3 to formulate a very targeted questionnaire only meant for true adopters.

The approach taken by Joo [11] differs from the mentioned
approaches by the research method: quantitative analysis of
questionnaires was completely omitted and in-depths interviews
with adopters and suppliers were carried out instead. The research
findings, related to semantic web technology adoption, were then

1 http://www.w3.org/2001/sw/sweo/public/UseCases/.
2 Our research described in this paper started earlier than that in [8] and therefore
we could not take inspiration from this interesting and well-founded approach. We
see the research goals of that project, based on the same source documents, as rather
complementary to ours, and plan to build on their results in the next iteration of our
research.
3 As well as us in the research presented further in this paper.

formulated using the techniques of grounded theory [12]. While
such kind of research is definitely very valuable, as it eliminates
some of the bias introduced by the closed setting of questionnaires,
it does not easily lead to operational models.

Yet another (though probably overlapping) resource of experiences on semantic technology adoption is provided by submissions
to regularly organized semantic web challenges. This resource
(in particular, several editions of the ISWCs Semantic Web Challenge and ESWCs Scripting for the Semantic Web challenge) was
used by Heitmann et al. [13] to assess the setting of components
needed for a semantic web application. Rather than specific categories (considered, to some degree, in the SWEO-catalog-based
research mentioned above), the last-mentioned research aimed at
framing a mainstream type of such an application.

2.2. Semantic application categorization

While individual semantic components are often arranged into
categorized lists, such as that maintained at the main Semantic web portal,4 categorization of deployed applications (which
obviously requires a somewhat different approach) is much less
covered. We have already mentioned the benefit-based categorization by Sauermann [7] and the three categories (with uncertain
provenance) from [8]. It is also worth mentioning that in the field
of ontology-based application, which pre-dated the semantic web
field, a general but thorough analysis was carried out by Jasper and
Uschold [14]. On the other hand, beyond the semantic technology
field there are established category systems such as that by Gartner
Group,5 which we refer to in Section 5.3.

2.3. Benchmarking of tools

Semantic technology evaluation campaigns, for example those
undertaken in EU projects such as Knowledge Web6 or SEALS,7
typically address a specific type of back-end software tool such as
an RDF store, a description logic reasoner or an ontology matching
tool. However, in a business setting, only complex applications that
include end-user functionality are meaningful. Such applications
typically rely on a combination of multiple software tools and are
not a priori labeled with a neat category, besides that of semantic
(web) application.

2.4. Cost-benefits models

Pioneering research was carried out by Hepp [15] concerning
the motivation to develop, and especially maintain, rich ontological models by practitioners. A framework allowing a certain kind of
quantification, again for the narrow topic of ontology design, was
suggested and implemented in the ONTOCOM project [16]. Over-
all, this kind of research focused on a single topic and, with the
exception of [16], only treated it at the theoretical level, without
an empirical survey. The problem of costs and benefits of semantic
technology adoption was also addressed by preliminary marketoriented analyses such as [17], ignited by the increased discussion between the semantic web and (mainstream) web developer
communities. In the research community itself, a nice follow-up
analysis has been carried out by Cobden et al. [18], who identify
six research challenges that are critical for adoption of semantic
(web) technology in the corporate environment, where the fully

4 http://semanticweb.org/wiki/Tools.
5 http://www.gartner.com/it-glossary/enterprise-application-software/.
6 http://knowledgeweb.semanticweb.org.
7 http://www.seals-project.eu/.

open data model often cannot work. These analyses are however
restricted to the monetization problem of publishing semantic data
on the web and do not cover the whole range of intra- and interorganizational problems related to provision, collection and consumption of such data.

3. Methodology outline

The loose methodology we applied on the available repository
of semantic application use cases can be summarized in twelve
steps, arranged into four phases:
Application categorization.

1. Collection of the set of candidate application categorization

criteria.

2. Selection of most important criteria.
3. Description of the applications by the unified set of criteria.
4. Clustering of the applications with respect to the criteria.
5. Characterization of the clusters, making them meaningful

categories.

Critical success factors analysis.

6. Collection of potential critical success factors (CSFs) from the

individual use cases.

7. Alignment of CSFs to the application clusters/categories,
through a questionnaire survey of the individual application
representatives.

8. Analysis of CSF importance (solidity) for each application
category, and re-description of the categories based on the CSFs.

Readiness model design.

9. Building a (multi-grade) readiness model for each application

category.

Evaluation of the models.

10. Evaluation of clustering (i.e., category system) quality based on

CSF distribution.

11. Evaluation of clustering quality based on alignment to external

resources.

12. Evaluation of readiness model in predictive setting.

We applied the methodologywhile evolving some of its steps
on the flytaking the 41 W3C SWEO use cases that existed at the
moment (plus one additional) as a starting point. The whole process took about a year; most steps were carried out by a single
person (the first author), although step 1 and part of step 2 also
involved five other in-house experts in various kinds of semantic
technologies, and step 7 benefited from the feedback obtained via
a questionnaire survey. The following four sections describe the
course of the four above-mentioned phases in detail (the subsections mostly correspond to one, or two tightly-coupled, steps).

It should be noted in advance that our actual process slightly
deviated from the (partially ex-post-formulated) methodology, as
it involved manual formulation of application archetypes (i.e., logically a part of step 5) prior to clustering (step 4). This seems to
have had a positive effect, as it allowed us to check that the number of clusters proposed in the course of hierarchical clustering was
plausible. However, manual formulation of archetypes might not
be possible if the whole process of analysis were even more distributed (e.g., if the number of described applications were much
higher). Therefore we did not include it in the methodology outline.

M. Nekvasil, V. Svatek / Web Semantics: Science, Services and Agents on the World Wide Web 21 (2013) 6174

4. Applications categorization

4.1. Process overview

As previously mentioned, semantic applications cannot be considered a compact area of interest, as they correspond to very
heterogeneous uses of individual technologies. Applications differ
from each other in many aspects, and their categorization is a multidimensional problem. The particular dimensions (i.e., categorization criteria) first have to be identified. In our project, a large list of
possible differences between individual semantic applications was
generated by a team of experts on both Semantic Technology and
IT Assessment, in the course of several brainstorming sessions. Afterwards this list was checked against the real cases in the SWEO
catalog,8 stripped down to the most notable dimensions, and organized into two groups of four criteria each (by a single person, to
maintain consistency). The number of categorization criteria had
to be reasonably small for this size of case-study dataset so the following analyses would not over-fit the classification structure. This
organized list of criteria was then again reviewed and approved by
the fellow experts. For the actual description of individual cases,
the team approach was tried at first, too. However, the results were
partly inconsistent due to ambiguous interpretation of some criteria by different people; for this reason, they had to be eventually
consolidated by a single person. The individual criteria values were
binarized, and all the cases were evaluated according to them. This
way the resulting values are as consistent as possible.

4.2. Categorization criteria

The categorization criteria we found can be arranged into two
groups. The first set of the criteria is characteristic for applications
that encompass semantics:
Character of information sources. The semantic character of considered applications directly implies that at least one knowledge
model (ontology or taxonomy) has to be used. Apart from that, the
applications can also use other data of various kinds. Knowledgebased applications can be divided according to whether they
primarily process structured (generalized) knowledge, structured
data or unstructured data.
Data source provenance. Semantic applications can be distinguished
according to whether the information they work with arises in
other systems (or is already available in a structured form) or
whether it is created specifically for this system. If the data are
created exclusively for the semantic system the cases for which
this is done manually, automatically from other sources or as a
side effect of other activities (such as normal user behavior) can
be further distinguished.
Accuracy of inputs and outputs. Different approaches to transforming inputs to outputs occurred throughout the example case stud-
ies. Here the applications can be divided, e.g., into those firmly relying on full precision of data, applications that expect that the data
may be incomplete but do not expect them to be inconsistent and
do not work with uncertainty, and finally, applications that include
treatment of uncertainty.
Subject of operation. From the analysis of case studies we managed
to identify several main types of activities within semantic applica-
tions, namely, data indexing, data integration and reasoning. How-
ever, in most cases these activities are the means rather than the

8 In addition, a semantic application in which several members of the group
participated, was also considered: the AQUA tool for evaluation and semantic
labeling of medical websites, created within the EU MedIEQ project, http://www.
medieq.org.

purpose of the activity (an exception is the integration of heterogeneous data, which provides a durable asset in its own right).
Starting from these, several other activities that support the main
purpose of the application can be derived, for example, better
searching capabilities (indexing + integration), heterogeneous
database browsing and navigation in the domain (integration+ in-
dexing), recommending new relationships among entities (applied
reasoning) and independence on the source systems data structures (data integration).

Apart from the semantics-specific criteria, semantic applications also differ in dimensions shared with traditional systems.
These general criteria also judge the application from the point of
view of deployment and reuse. The identified general classification
criteria are as follows:
Number and kind of users. Users of semantic applications may
consist of common users, domain experts, knowledge technology
experts, and management staff. Applications can also be distinguished according to whether they are intended for individuals,
working groups or thousands of random users (esp. in social net-
works).
User vs. provider relationship. We identified several expected
options for operating the applications: the user is an individual
and operates the application for his/her own use; the users are
the customers of the provider; and, finally, the users are the
employees of the provider. For the last two possibilities, cases can
be distinguished where either the operation of the application is
the core business of the company or where it is only a supporting
process and can therefore be considered a possible target for
outsourcing. The cases when the operation is ensured by the
community can be further broken down according to whether the
operation is centralized or decentralized.
Frequency of access to the application and its availability. Applications may be used continuously (24/7), at random, regularly or by
a single opportunity. Furthermore, a distinction must also be made
by the availability of such applications: either the application must
be available constantly, in defined intervals, or on demand (reac-
tive manual start of the application).
Domain-specificity and reusability. Because of the separation of
data from their meaning, semantic applications should be much
less domain-dependent than conventional solutions. Even here,
however, there are exceptions which include, for example, specific
interfaces tailored to a specific domain or particular treatment of
data on the application level.

4.3. Criteria value frequencies

By sorting the considered case studies we can estimate how
often some specific values of the proposed criteria are seen in realworld applications. The rightmost column in Fig. 1 displays the
relative frequencies of the distinct values for individual criteria.
Note that the values are mostly not disjoint, and therefore the
relative frequencies do not sum up to 100%.

4.4. Tentative categoriesarchetypes

The initial sorting enabled us to intuitively identify some basic
archetypes of semantic applications, based on the distributional
characteristics of the criteria. While this manual process was not
backed up by any rigorous method and has not become part of
our proposed methodology, it had an advantage that the identified
archetypes could be named and verbally characterized (leveraging
on prior familiarization with the case studies descriptions):
Improved search engine. These applications focus on indexing
the data, often associated with integrating data from various

Fig. 1. Characteristics of clusters.

other systems where the data are generated automatically. Such
automatically acquired data are also often augmented by manual
annotation. Applications of this type work with both structured
and unstructured data (using automatic filters and wrappers).
The main benefit of these applications is to enable searching in
heterogeneous data and creating complex queries without the
need for prior knowledge of the data structures. In other aspects,
however, they can vary greatly; so such different applications can
be classified here as support for annotating and searching files
on a personal computer, a public portal for searching findings in
Chinese medicine, and management of sound recordings archives
by a radio station.
Data-browsing interface. These applications follow the abilities
of the previous archetype, but enhance not only the possibility
of displaying diverse content (videos, articles, chemical formulas,
etc.), but also the possibility of visual browsing of data regardless
of their structure. These applications are mainly focused on
use by professionals and are operated either commercially for
internal use or non-profitably to support a professional community
(while simultaneously promoting the technology). Examples of
this archetype can be systems for aggregating of medical data,
whether used to facilitate the treatment of patients or achieve
savings in the development of new drugs, or a portal for association
of programming knowledge.
Recommending system. The nature of these applications is the
derivation of new relationships between entities. Moreover, apart
from all other types of source data, these applications often utilize
data that are automatically generated as a side effect of normal
user activity, which enables them, inter alia, to propose new
relationships on the basis of the current users context. The user
is often a customer of the provider, whether the application is run
as a paid service, a public service (e.g., designing of city tours on
behalf of a municipality) or to personalize targeted advertizing in
conjunction with the provision of services (such as recommending
services to users of mobile devices). Applications in this category
often work with uncertainty (in the classical expert system style).
Data interchange framework. Operations of applications in this
category are (because of their nature) distributed; they typically
allow unifying the structure of data exchanged between multiple

participants, regardless of content. This content can evolve over
time and be adapted to the needs of a particular bilateral exchange
relationship and yet be transmitted in a standardized format. An
example is the initiative for the establishment of semantic data
interchange in the worldwide oil and gas industry.

There are also applications that cannot be assigned to any of
these archetypes, as well as some that lie in between two or more.

5. Applications clusters

5.1. Cluster building

While the manual formulation of the archetypes provides us an
initial framework of discernment, in the next phase we proceeded
to standard, automatic cluster analysis [19] of the case-study data.
All of the studied cases were entered into a database. The
categorization criteria were transformed to binary attributes to
facilitate the possibility of multiple values per case. Even with
such a small database it was possible to perform cluster analysis
with sensible results, but the parameters had to be carefully
stipulated. We used the common k-means algorithm. The choice
of the right number of clusters is always a compromise between
oversimplification and over-fitting of the clustering model. To
this end we deployed several methods starting from a heuristic
2 ), followed by an estimate
based on equalizing the actual cluster sizes for the smallest k
possible, an estimate based on minimizing the marginal increase
in cluster centroid variance with increasing k, and also computing
the clustergram [20,21] showing the PCA-weighed means of the
centroids in the sense of dimensionality reduction for different ks.
All of these methods suggested that the optimal number of clusters
would be close to 4.

estimate based on dataset size (k  n

The relative frequencies of individual attributes values are
displayed in Fig. 1. The individual clusters found are labeled
C1C4 due to our lack of any preliminary knowledge of their
interpretation. In the individual columns the principal frequencies
of occurrences are listed; this view only shows the actual status
of the clusters and does not significantly manifest the reasons for

M. Nekvasil, V. Svatek / Web Semantics: Science, Services and Agents on the World Wide Web 21 (2013) 6174

Fig. 2. Cluster discrimination factors.

selection of the clusters. For this we needed to investigate the effect
of attribute values on cluster discrimination.

The discrimination factors of clusters in the resulting clustering
model are shown in Fig. 2. They are represented by coefficients of
the principal component analysis over all predictors. The numbers
in individual cells indicate the likelihood of whether an observation that embodies the concerned value is a member of the particular cluster, where a positive number indicates a positive impact on
the likelihood and a negative number indicates a negative impact.
Higher numbers in a row indicate a higher resulting total weight
of an attribute. This view captures the very essence of the findings
of the clustering model and enables interpretation of the clusters.
Thanks to these two views it is possible to describe the

individual clusters as follows:
Cluster C1. The first cluster (11 cases) is characterized by the
following: The application is used by more than a hundred
users who are mostly customers of the application provider. The
application is not used for data integration but the results the
application yields are fully accurate; it uses solely structured data.
The deployment of such an application is mostly case-specific.
Cluster C2. Applications in this cluster (also 11 cases) are
characterized by their ability to treat uncertainty and work with
partial accuracy, and they also use structured knowledge for this
purpose. These characteristics are mainly used to apply inference
to the knowledge bases rather than to index the data. These
applications are used by several to many users that are, most often,
employees of the provider, and irregular use of the application is
expected.
Cluster C3. Applications in this cluster (10 cases) are typically
designed for the integration of data from various sources; they
mainly use (as specific for this cluster only) unstructured data that
are processed both automatically and manually but with full 
or occasionally partial  accuracy. These applications are used by
hundreds of users and their functionality is independent of any
particular domain.
Cluster C4. These applications (9 cases) are typically only used by a
few users (or even a single user) who are at the same time providers

of the system. These applications mostly do not utilize structured
knowledge and do not treat uncertainty. The main purposes of
these applications are data integration and indexing, while other
aspects are not so definite and differ within the cluster.

5.2. Cluster characterization

The cluster analysis has introduced four large clusters of
Semantic Applications that can be partially aligned with the
previously intuitively identified archetypal use-cases; however,
these two structures will be overlapping only to a point. The other
clusters can of course be named and constituted as additional usecase archetypes.
Cluster C1 characterization. A typical use case of applications in
this cluster can be named a knowledge management system
because it is mainly used for indexing the data manually entered
by many providers employees. It is very similar to the proposed
improved search engine archetype, and it extends it just by
targeting employees instead of general users. Typical examples are
various semantics-aware wiki systems such as Faviki.9
Cluster C2 characterization. These applications fit almost exactly
the structure of the proposed recommending system archetype
due to deployment of automated inference over the structured
knowledge and its being opened to many users. An example is
SurveyorHealth,10 which recommends reductions of drugs based
on known interactions.
Cluster C3 characterization. This cluster resembles the databrowsing interface archetypal use case. Such applications are targeted at integrating and displaying various data of heterogeneous
structures. Some applications of the data interchange framework
would also be part of this cluster and will be differentiated from
the former by the absence of visualization capabilities. Examples
of browsing interfaces for semi-structured data are the Oracles

9 http://www.w3.org/2001/sw/sweo/public/UseCases/Faviki/.
10 http://www.pharmasurveyor.com/.

BI system Siderean or the Bankinter system for management of
ideas.11
Cluster C4 characterization. This cluster consists of applications that
can be characterized as Personal desktop indexers because they
are characterized by a small number of intended (as well as actual)
users and their orientation on data indexing and integration. This
group has no counterpart in the aforementioned archetypes, but
the small number of users is the only feature that distinguishes it
clearly from the other clusters. A well-known example from this
category is the Nepomuk semantic desktop system.12

The cluster analysis has thus introduced one additional kind of
semantic application not identified within the first, manual try.
There is no pretension that this list is complete; it is inevitable
that other use cases exist or will emerge in the future, however
they are not significantly represented in the dataset we have used.
Therefore, more archetypal use cases will be discovered through
analyzing additional case studies. A starting point could be the
positive cases from the in-depth interviews carried out within the
VALUE-IT project with 50 European company directors regarding
their use of and attitude towards semantic technologies [9].

From now on we will interchangeably designate the clusters as

categories where appropriate.

5.3. Alignment with enterprise application software

Despite the specific features of semantic applications, it might
also make sense to loosely align the discovered categories with
industrial categorizations of mainstream enterprise software.
We chose the Enterprise Application Software categorization in
the Gartner IT Glossary13 as sufficiently representative. It lists
seven categories: Content, communication and collaboration,
CRM (customer relationship management), DCC (digital content
creation),
Office suites,
PPM (project portfolio management) and SCM (supply chain
management).

ERP (enterprise resource planning),

We hypothesize that knowledge management systems (C1), including collaborative approaches such as semantic wikis, will most
likely overlap with Content, communication and collaboration;
recommending systems (C2) are likely to appear as components
of ERP, CRM or SCM solutions; a data-browsing interface (C3) may
align with Digital content creation tools, or possibly PPM; and personal desktop indexers (C4) clearly fit Office suites, possibly with
further integration with C1. However, a new empirical study, possibly similar to the present one, would be needed to confirm such
hypotheses.

6. Critical success factor analysis

6.1. Critical success factor formulation

By synthesizing the risks mentioned in the individual case
studies we can draft some of the most frequent critical success
factors (CSFs) in the deployment of the semantic applications and
their deployment into the production environment. Methods for
determining the CSFs have been developed in several areas, but
sadly not for Semantic Technologies. The best way to establishing
a complete framework of CSFs thus seemed to lie in gathering
a list of individual success factors in particular case studies, as
exhaustive as possible, by walking through them one by one
and assessing the CSFs expertly, and then aligning them to the

discovered clusters (or, typical use cases) of semantic applications
discovered previously. Finally, for the framework to be approved
the results had to be evaluated with the authors, sponsors and
users of the particular applications. The critical success factors are,
of course, of two sorts: firstly there are general factors that mostly
depend on the application use case, and secondly factors that are
more specific to the use of semantic technologies. The first sort
includes factors like strategy and planning, resource availability,
deployment quality, implementation or maintenance, which are
already covered by widely-used assessment methods like those
included in COBIT.14 Of more interest for this work is the second
sortthe semantics-specific CSFs. The CSFs of this kind identified
thus far can be grouped as follows:
CSFs tied with the business case.
 Effectible benefit. Or, in other words, the potential of possible
benefits to compensate for the temporary reduction in productivity during implementation and learning (as well as operating
costs). The benefits of the applications are very diverse and often rather vague (in contrast with conventional solutions) and
thus can hardly be estimated (and quantified) at the time of the
deployment of the system. Operating costs are mostly comparable to conventional applications, but in the phase of deployment it is necessary to allow for a temporary decrease in users
productivity. For an application to be successful, this temporary
decrease should not be so serious that it would overshadow the
potential benefits.

the more crucial

CSFs of the used knowledge model.
 Correctness of the core ontology/taxonomy. This factor holds for
most knowledge-based applications, and the more complicated
and less volatile the used model,
its
correctness. Achieving this success factor entails the need for
recruitment of high-quality analysts and knowledge engineers
in the phase of development and deployment of the application,
which incurs considerable costs. The quality and reach of the
used ontologies is not limitless; apart from the costs of creation
it also has other more structural constraints (see [15]).
 Willingness and discipline of all parties to use a common knowledge model. When the operation of the application is dis-
tributed, it is necessary that all interested parties use a central
shared knowledge model. There is therefore a potential risk associated with the need to negotiate on its form and content.
 Synchronized distribution of central ontology. Gradually, there
may be modifications of the central knowledge model that arise
subsequently, and if the operation is distributed, it is necessary
that these changes are properly disseminated among interested
parties to avoid inconsistencies. While these changes and modifications take place, the previous item still holds.

CSFs of users behavior.
 Sufficiently steep learning curve of end-users and user-friendliness.
This applies to applications that have individual end users.
Semantics used in this type of application entails quite atypical
methods of control compared to standard applications, and the
learning curve is rising very slowly. Not only is a comprehensive
and intuitive user interface of the system a must, but clarity and
accuracy of the outputs and results are also vital for the users
work.
 Sufficient number of users. If the respective semantic application
is based on social data, its success is conditioned by the
existence of a large enough number of users that produce these
data. The risk in this case occurs in the form of necessary
expenses for the promotion of the emerging system.

11 http://www.w3.org/2001/sw/sweo/public/UseCases/Bankinter/.
12 http://nepomuk.semanticdesktop.org/nepomuk/.
13 http://www.gartner.com/it-glossary/enterprise-application-software/.

14 http://www.isaca.org/Knowledge-Center/COBIT.

M. Nekvasil, V. Svatek / Web Semantics: Science, Services and Agents on the World Wide Web 21 (2013) 6174

 Users motivation. This critical factor occurs at two levels. The
first is at the time the new application is introduced, when the
user experiences negative stimulation due to the slow rise of
the learning curve. The user thus lacks the motivation to learn
how to deal with the system from the first moment. Moreover,
the user that does the work is not always the one who benefits
from it (discussed in [15]) which can be a further burden. The
second is in the actual phase of operation; a common source of
data for the semantic systems of all sizes is manual annotation,
whose creation is up to a certain point very labor-intensive for
the users. A partial source of motivation may be the potential
benefit of better results or facilitation of work in the future. In
addition the user can be motivated by the possibility of later
using the gained experience elsewhere, since even the most
different semantic applications often use similar technologies
(e.g., SPARQL querying).

CSFs of data treatment.
 Sufficient supply of data. For applications that rely on reasoning,
having a sufficient data source is essential
for providing
beneficial results (i.e., utilizing the added value of semantics).
Even for applications based on data indexing, having enough
data is critical to success, because for small volumes of data
they give similar results as traditional methods but with higher
initial costs.
 Diversity of sources and forms of data. The richer the knowledgemodeling language is (namely, its part actually used in the
application), the more beneficial results can be produced by
applications based on the derivation of new relationships.
Likewise, the greater the diversity of data content is the more
useful the results are given by applications performing semantic
integration. The use of semantic technologies on trivial systems
is therefore not likely to pay off.
 Data consistency. This is essential for maintaining at least the
same accuracy of results as in the sub-systems, if there are any.
Applications that integrate data of some source systems are at
risk of inconsistency occurring in the aggregated results. The
functionality of semantic applications is typically not exposed
to only consistent data, but the possible inconsistency should
already be expected in the design of such applications. Good
estimation of the data source reliability is thus crucial at this
point.
 Reliability of parsers and wrappers. If the application handles unstructured data, it is dependent on the output of parsers and
wrappers of various content and, where appropriate, the natural language processing systems. Here again the same reservation applies as in the previous point, namely, it is necessary to
correctly estimate the reliability of the information obtained in
such a way.

Of course, these critical factors will be weighted differently
within the scope of different applications. If, for example, the
source application collects data automatically and passes the outputs to the user in almost natural language and in an appropriate
context, one can expect a relatively steep learning curve, and thus
the period of reduced productivity is quite minimal and as a result
it will be compensated enough even by minor benefits. These critical success factors can only be taken as a starting point before aligning them with individual use cases. The CSFs found in this way are
thus not valid universally but more likely their relevance depends
on the kind of semantic application we are dealing with.

6.2. Questionnaire survey

To map how the actual projects from the dataset of available
case studies perform in a real-life environment, we contacted
the most competent sources  people directly involved in the
development and deployment of the applications of concern  via
a survey aligned to the previously identified CSFs (see column
Respective Questions in Fig. 3). For achieving the maximum
response rate possible, the questionnaire was lightweight, split
into sections of few questions, and gave the respondent an
opportunity to express his/her opinions and experiences at the
level of detail s/he wished. The complete list of questions is in
the Appendix. We received a total of 18 responses out of the 41
respondents contacted. Some of the participants agreed to provide
their responses on the condition of publishing only the aggregate
results. This is understandable as the survey was addressed to
employees of important organizations like NASA, Audi, Renault
or Vodafone, and in the responses they had to share parts of
their know-how. Luckily enough, we received responses evenly
on applications from all of the four identified clusters (all clusters
yielding 5 or 4 responses); although this is just a coincidence, it
ensures that the results are as general as other conditions allow.
The questions in the survey (apart from the few formal open
questions that aimed at identifying the respondent and collecting
his/her feelings) addressed the areas associated with the identified
CSFs, namely:
 General questions about the feeling of the projects success rate
and cost level, asked to gather some relevant statistics about
these factors.
 Questions about the users behavior, asked to gain insight into
the possible special requirements from and constrains on the
users.
 Questions about the used knowledge model, asked to uncover
the requirements on the ontologies used.
 Data-related questions that covered the success factors with
respect to the treatment of inconsistencies and uncertainty.

Some of the answers were conceived as positive and some as negative w.r.t. the criticality of a CSF for the given case, as elaborated
in the next subsection. Negative answers were both those expressing explicit negation of criticality of a CSF (e.g., correctness of the
core ontology is not critical) and those declaring irrelevance of the
CSF (e.g., there is no core ontology). Similarly, negative answers
were both those declaring independence of the expected benefits
of the given parameter (e.g., size of data) and those indicating negative dependence (e.g., with growing data size the benefits are de-
creasing), provided the CSF was uniquely focused on the positive
dependence.

By means of the individual parts of the survey it was possible
to back up the individual CSFs and grade them for the particular
semantic application clusters.

6.3. CSFs solidity calculation

For the purpose of aligning the survey results to the structure
of Critical Success Factors we defined the measure of CSF solidity
in each cluster as the proportion of such answers to the relevant
question that indicate criticality of the particular success factor;
if there are more than one question relevant for a particular CSF
then the average of the partial solidities is computed over all such
questions:
Sf (C) = qiQf Sqi
f (C)
|Qf|
f }|
f (C) = |{aqi
j  PAqi
j }|
|{aqi

Sqi

Fig. 3. CSF solidity evaluation by cluster.

is the answer to question qi in the j-th questionnaire
is the set of possible answers to question qi that indicate
for the application described by a

where
 f is a CSF, and C is a collection of questionnaires (representing
the analyzed cases; a collection can thus represent a cluster of
cases in our study)
 Qf is the set of all questions relevant for the CSF f
 aqi
 PAqi
criticality of the CSF f
questionnaire in which this answer is used.
For this purpose the survey was designed with a clear mapping
of individual questions to success factors. For example, if the
responding representatives of Cluster 2 applications answered at
25% that the users understanding of the underlying semantic
features is critical and in another 25% answered that some degree
of training with the user interface is required (the last two answers
to question 5 as in the Appendix), then the User-friendliness CSF
was graded 50% for Cluster C2.

The solidity rate, as an approximate measure of criticality of
particular success factors, is shown for all clusters and all critical
success factors in Fig. 3, along with the list of respective survey
questions that are relevant for determining the CSFs. The question
numbers refer to the original numbering in the questionnaire.

6.4. Using CSFs for re-describing the clusters

These results are of course dissipated due to a relatively small
number of cases available in the survey, and the measures of different CSFs are not scaled because of the unavoidable psychological
bias of individual questions. On the other hand, these results still
show a remarkable bias in the attitudes to different problems by
different application clusters:
 Cluster C1Applications from this cluster, the knowledge
management systems, show the greatest need of all for the
initial correctness of the core ontology used. Surveyed users
from this group also show above-average concern about the
sufficient number of users, the quality of user interface and the
consistency of data provided. This is also the type of semantic
application that should have the reachability of benefits evaluated most carefully because its development and deployment
is often more expensive than in the case of traditional alterna-
tives.
 Cluster C2A recommending system can often be run dis-
tributed, which leads to focus on the feasibility of synchronization of the models used. They also often make extensive use of
unstructured data; thus, the reliability of parsers and wrappers
is essential. Finally, the more users use it the more relevant results the application can provide.

 Cluster C3In order to allow the users to make use of the benefits of data-browsing interfaces a sufficient supply of heterogeneous data is required; otherwise the system cannot best a
traditional solution. The correct mapping of several knowledge
models is essential to index the data, and the way of presenting
them to the users also impacts the final success of the project.
 Cluster C4Most of personal desktop indexers also rely on
sufficient supply of heterogeneous data that often come from
unstructured sources.

This basic view on success factors may help IT managers assess
the Semantic Application projects in a structured way, and on the
other hand may also help the promoters of Semantic Technologies
to support their arguments for their deployment where relevant.

7. Adopter readiness model design

Although the CSF weights computed for different application
clusters already provide some guidance, they may still not be
instructive enough. What is needed is a structured model that
grades the readiness of the organization to successfully adopt the
given type of technology. For this purpose, lessons can be learned
from the so-called Capability Maturity Models [4]. Capability
Maturity Models have been developed over the past two decades
within initiatives such as CMMI15 in order to enable the assessment
of the enterprise processes readiness to implement some kind
of structural investment, and most commonly they are used in
the deployment of any IT applications such as CRM systems, ERP
and Business Intelligence. In our opinion it makes sense on the
basis of the above aspects of categorization and the associated
critical success factors to establish a similar kind of model for the
deployment of semantic technologies. As it might be too strong to
say that an enterprise not satisfying the conditions for successful
adoption of semantic technology is immature, we opted for the
notion of adopter readiness model.

In the terminology of CMMI, the models that can be constructed
under current conditions for semantic technologies will be
without factual content and without a target statement (because
there are no formalized best practices of the semantic technologies
deployment on a particular structure of business processes) but
can be formulated based on the CSF solidity for each application
type. They will thus make it possible to set a certain level of
requirements for an enterprise that should be met in order to
consider the feasibility of the solution. As for the method, there is

15 http://www.sei.cmu.edu/cmmi/.

M. Nekvasil, V. Svatek / Web Semantics: Science, Services and Agents on the World Wide Web 21 (2013) 6174

Fig. 4. Adopter readiness model for recommending system.

no best practice for how to construct a maturity model based on the
CSF evaluation and therefore we used our own, simple yet novel,
approach.

Starting with the set of CSFs evaluated for each semantic
application cluster and ranked by their solidity, it is possible
to establish certain readiness levels and define them by the
enterprises ability to satisfy those CSFs that exhibit (at least) the
respective solidity for the cluster. The readiness levels themselves
represent the threshold for the CSF satisfaction; in other words,
the greater the enterprise readiness is for the application type,
the greater should also be its ability to satisfy even the least solid
(yet not completely irrelevant) CSFs. To determine the individual
thresholds we assumed six readiness levels (as is common for
maturity models) traditionally ranging from M0 to M5, where in
the case of M0 no CSFs are satisfied and the threshold is 100%
solidity (exclusive), and in the case of M5 even the least significant
CSFs need to be covered, therefore the threshold is 0% solidity
(again exclusive). The thresholds of the in-between levels can be
set individually for each cluster by taking the quintiles of the
ordered array of unique CSF solidity values that came out of the
evaluation. In this way it is ensured that, with sufficiently distinct
solidity values, there will always be an incremental difference
among the two consecutive readiness levels. This structure will
also enable the evaluation of enterprise readiness by assessing its
ability to satisfy the individual CSFs via interviews or other possible
means.

An example visualization of the readiness model for the
Recommending System type is shown in Fig. 4. We see that for
achieving, e.g., level M2, an adopter should have at least four CSFs
satisfied; one of them is a sufficient number of users, which was
not yet required for level M1.

It is also possible to describe the requirements for an enterprise
in a verbal way:
 If the enterprises user support system uses a single source of
data and a proprietary data structure, then the enterprise is
unprepared for the introduction of this kind of system as it
would not bring any additional benefits.
 If there is a large supply of data (not necessarily consistent) from
several data sources, then there may be some new relationships
in the data, identifiable through induction, and may yield
some added benefit to the current databases in the form of
discovering and recommending new links between sources.
 If all the data in the systems are consistent and there is a
demand from the user side to ease the search and navigation in
enterprise catalogs, semantics may provide an elegant solution
by introducing the recommending power to users searches. If
there are many users producing data and the systems make use
of these social data, then the use of semantic technology is at
hand and can be recommended.

Note that unlike for the CMMs, which reflect the maturity of
processes in the organization, here the transition to higher levels
is not always obvious, as the non-satisfaction of some CSFs may be
an inherent consequence of the particular business.

8. Evaluation of the method

Although most steps of the methodology are of a soft nature,
we have attempted to set up a partial framework for evaluating
the plausibility of the approach. The evaluation concerns two
components resulting from the automated part of the process: the
clustering model (i.e., category system) and the adopter readiness
model.

8.1. Evaluation of the clustering

To some degree, the added value of the application clustering
can be justified merely based on the cluster characteristics. The
difference in individual success factors solidity between the four
identified clusters that came out of the survey is clearly visible
in Fig. 3 and indicates that the classification covers at least a
part of the differentiation between semantic applications that
is needed in todays business environment. Rigorous statistical
evaluation should obviously be applied if the total number of
records (applications) were high enough.

Furthermore, in order to leverage on the prior work on the
same input resource, we can compare our results to those obtained
by [7], who single-handedly formalized the benefits of applications
from the subset of the same SWEO catalog we worked with. As
this benefit formalization considers the applications covered by
our categorization, it is straightforward to examine its results
with respect to individual clusters. This alignment is displayed in
Fig. 5. The figures show the relative frequency of the benefit in
a given cluster, as well as the degree of variance and its p-value,
i.e., the probability of obtaining the given degree of variance under
the null hypothesis assumption (which is, here, non-correlation
between our and Sauermanns clusterings). The p-value is always
below 0.1, and for a majority of cases (i.e., benefits) it is even
below 0.05 as the common threshold of statistical significance level
(despite the very small size of the data we operate on). It thus
seems that this independent scoring on the same set of applications
returned results that confirm the differences among the clusters
we identified.

For the dependence comparison of categorical variables (in
our case the Sauermanns benefit explanation vs. our cluster
alignment), statistical metrics such as weight of evidence or mutual
information are most often used. However, because of some
combinations of zero probabilities in our sample, these metrics
would result in infinite values and therefore would not provide

Fig. 5. Alignment with Sauermanns benefit model.

objective results. For practical reasons, in such cases the selectionweighted normalized variance (i.e., normalized by n 1 where n is
the cardinality of a predictor) is used in the business environment
as a proxy for the mutual explanation of variables. To quantify the
correlation of the two sets of measures, Fig. 5 thus lists both the
standard variance and its normalized version (explanation).

8.2. Evaluation of the readiness model

Since the same data about use cases were used both as input
for the manual design of categorization criteria and CSFs on the
one hand and as actual data to be processed by the method on
the other hand, we found it useful to also apply the models on
previously unseen data, in a predictive manner. Unfortunately,
the inflow of new use case descriptions is not very fast; only four
new descriptions appeared in the SWEO portal from the beginning
of 2010 to mid-2011: one in the media domain (BBC), one in the
biomedical domain, and two in the publishing domain (technical
standards and legislation, respectively, both from Korea). We
applied the method to them, starting from the analysis of
categorization characteristics, through automated assignment to
categories, choice of readiness model (corresponding to cluster)
and application of the model to prediction of application success.
Categorization of new cases was carried out using a common
scoring method. First, all of the new cases were graded in terms
of the categorization criteria (by the same author as that of the
original sample, to maintain the same subjective bias). Then the
clustering model was applied on these criteria without re-learning
the parameters, such that the new cases could be aligned with the
previously described clusters.

The categories of the new applications were determined as
Data Browsing Interface (for BBC), Recommending System (for
the biomedical application), and (twice) Knowledge Management
System, respectively. Since we had not yet received any survey
response from the authors of the new applications, the solidity
rates for CSFs were set up by us, based on the textual description
only. Finally, an (again, manual, text-only-based) assessment of the
success of the application was made.

Although such an extremely soft evaluation process surely
cannot be taken as guarantee of method plausibility, its outcomes
at least did not go against intuition. While the media application,
which is fully deployed into commercial practice, reached the
maximum (M5)
the biomedical
application (looking like a promising semi-deployed prototype)

level of adopter readiness,

reached the M2 level, and the remaining two (assessed as purely
academic research) only reached M1 and M0.

Obviously, application of the models in a real-world corporate
(or, e.g., government) setting would by far be their best kind
of evaluation. Although this has not happened to date, we see
such a scenario quite plausible in the future, be it for the
current models or for other models built using our methodology,
be it by us or others (possibly with improved data sources
and enhanced computational techniques). The strong point of
readiness models is indeed their similarity to the widely used
Capability Maturity Models. For example, in the consulting practice
of the second authors organization, very similar models have been
successfully used on several dozens of projects in the banking and
telco industries. Maturity models are mostly used for assessing
readiness for certain process tools in credit underwriting, debt
collections, and targeting of marketing campaigns. If our proposed
kind of adopter readiness models were assumed to be used in the
same style, then, first, a realistic deployment plan for these models
would have to be adopted by consulting companies and IT suppliers,
and used for assessing the risk tied with software development
and deployment projects, as most of these projects in commercial
area are driven by the sales success of these suppliers. Therefore, a
realistic scenario would be, for the supplier of semantic solutions
1. to walk through the criteria of the readiness model with

2. assess the risks tied with the deployment of the respective

potential customers

(semantic) solutions

3. and then focus the sales activities on the most mature clients,
i.e., the clients with the lowest risk and thus biggest potential
in terms of revenue.

Secondly, these models should by adopted by IT departments
of companies interested in deploying the semantic solutions,
for them to be able to realistically assess the meaningfulness
of these desires. This can help prevent capital expenditures on
development projects for which the company is not mature
enough.

9. Discussion and conclusions

The described research attempts to pave the way from textual
descriptions of semantic applications, collected in various surveys
and on professional portals, to directly usable models allowing the
potential adopters to assess their readiness to benefit from such
an application. The ultimate adopter readiness models borrow in

M. Nekvasil, V. Svatek / Web Semantics: Science, Services and Agents on the World Wide Web 21 (2013) 6174

shape from the Capability Maturity Models (widely known among
practitioners), while being derived from the original documents
and distributed questionnaires in a relatively systematic fashion.
The approach can be considered a lightweight methodology
applicable by future R&D projects on ICT adoption, not necessarily
in the semantic technology field. A prerequisite is, however, the
availability of relatively comprehensive application descriptions
as well as of contact information to the maintainers of the
applications, allowing for a questionnaire survey. The actual
readiness models (available in [5]), as concrete artifacts of the
study, can presumably be applied (obviously, with caution, given
the very tiny practical evaluation so far) as one of the guidance
materials for organizations considering (or offered by an external
provider) the adoption of a technological solution labeled as
semantic. The third, certainly least important, contribution of the
presented research is the exploration of concrete computational
measures in the context of the methodology, such as solidity
calculation or cluster number estimation. These measures are very
likely to be replaced by more systematically selected and tuned
ones in the future.

There are various limits of the work done so far, as far as
the concrete study is concerned. The first and probably most
substantial one is the size of the available dataset. Even though
we based our observation on probably the best sample available,
it might be too small for all qualities that semantic applications
might manifest. It is possible that when our procedure is repeated
on a sample larger by an order of magnitude, new clusters would
emerge that would bring a qualitative improvement. For now, the
statistical error would definitely be large; with a larger sample the
results would also be improved in quantitative precision.

A second limit is a certain bias of the study towards semantic
web technologies in the narrow sense. While this issue is shared
with other (mostly even more biased) relevant projects mentioned
in Section 2, it should be noted that the coverage of computing
approaches that can also be characterized as semantic-oriented,
such as information extraction from text or the use of linguistic
thesauri, could possibly have been more extensive and finer-
grained. Moreover, even in the semantic web context, the study did
not look ahead towards the most ambitious kind of applications:
those robust enough to consume completely unforeseen data
sources and schemas. While the current industrially deployed
applications (aside generic components focused on semantic
specialists, such as editors or heavy-weight reasoners) do not
exhibit such capability to significant extent, it is likely that the
landscape will change in the future, with the growing amount
of available (linked) data sources and vocabularies, which will
eventually make manual pre-selection and adaptation of whole
datasets too tedious. The cultural readiness of the organization
to absorb manually unchecked information sources, for instance,
might then become an important factor to be considered.

The third limit of the method is of a more inherent nature.
When determining the CSFs solidity from the survey results,
we faced a dilemma considering its quantification. This was a
problem of opting between the notions of frequency vs. average;
we chose the first approach and directly derived the CSFs solidity
as the frequencies of their absolute criticality. It is possible that,
should we have chosen the latter (i.e., defined the solidity as
the average perceived severity), the results would have differed.
Further experiments in this direction would be desirable.

Aside from the mentioned more systematic choice of measure,
follow-up studies based on the same or similar methodology will
address other available textual resources such as those resulting
from the VALUE-IT project. In addition to such broad-area surveys,
we will also pay attention to more specialized types of semantic
applications that are likely to soon acquire their own use-case
catalog. For example, in the context of the EU projects LOD2

and LATC, a collection of fielded applications of the Linked Data
technology is fostered under the name of PubLink16; a catalog of
textual descriptions is very likely to soon result from it.

As the adopter readiness models presented here are rather qualitative and oriented towards the respective organization, they deserve (in particular, for the businesses attached to the web envi-
ronment) to be complemented by monetization models that would
also inspect the broader market setting, as outlined by [17,18].

On the other hand, while the readiness models themselves
are qualitative, the research leading to their formulation relied,
pre-dominantly, on quantitative research. As already shown by
Joo [11], qualitative analysis of interviews could reveal some less
intuitive aspects of the complex landscape of semantic technology adoption. However, the higher costs for the human subjects
involved would have to be taken into account, in particular, considering that the case studies are geographically very dispersed;
additional incentives for the interviewees would thus likely need
to be implemented in order to keep the coverage at a comparable
level. Additionally, since the discussions about the semantic web
are heavily taking place at various online fora, netnography techniques [22]  qualitative study of online communities  could be
applied.

Acknowledgments

This research was supported by the CSF project no. P202/10/0761,
Web Semantization. We are grateful to Ota Novotny for his inspiring feedback on maturity models and to Ivan Herman for his clarifications regarding the SWEO catalog.

Appendix. Questions from the questionnaire

Note: the accompanying motivating text with contact information

of the questionnaire authors is omitted; it can be found in [5].

General questions (1...4 of 15)

2. Would you say that your project was a success?

Please especially answer the second question; any other can be
skipped if you wish to. However the more questions you answer,
the more helpful your participation will be.
1. What application or project are you answering about? If
this field is pre-filled and seems correct, please do not edit.
Otherwise put in some identification of your project.
 Yes, definitely, all works as supposed to and our application
brings the desired benefits.
 The application works but the benefits are not as large as
expected.
 The application was implemented but has never been used in
the scale originally intended.
 No, we have never gotten past the proposal stage.
 Other.
3. What are the main benefits the use of semantics brought to
your application? Please describe briefly, keywords are also
sufficient.

4. Were the costs of development/acquisition and operation of
your application high as compared to a non-semantic one?
Please consider financial and time costs.
 Lower, as inclusion of semantics eases the development
and/or operation.
 Approximately the same.
 Higher, because of the subsidiary inclusion of semantics.
 Higher, because our solution has many more features
compared to any non-semantic one.
 Incomparable, as a similar application would not be possible
without semantics.

16 http://lod2.eu/Article/Publink.html.

Questions about users behavior (5...8 of 15)

Data-related questions (12...15 of 15)

Please answer only those questions that you consider relevant.
If you do not understand any question, just skip it. Skip this entire
page if your application has no end users.
5. How noticeable is the presence of semantics in the end user

interface?
 Not at all, the end user is not aware of the semantics.
 The user can add some semantics optionally (e.g., tagging is
optional but not required).
 The user has to learn some interface specifics before using all
features.
 Understanding the semantic nature of metadata is critical for
the use of our application.
6. Are the end users producing some semantic data that they can
benefit from?
 Yes, there are some data produced automatically by the users
activity.
 Yes, users provide some data manually (e.g., tags).
 No, all data arise automatically from other sources.
7. Is a higher number of users beneficial for the operation of your
system?
 Yes, the more users provide their data, the more they benefit
from the results.
 No, the functionality is not dependent on the amount of data
provided by users.
 No, the more users provide their data, the more errors they
introduce.
 The users do not provide any data.
8. What is the main motivation of the end user for using
all features of your semantic application? Please especially
consider the features related to semantics.
 The user has no alternative and so using this application is a
necessity.
 This application provides better results as-is, compared to
traditional systems.
 This application provides the better results the more the user
uses it.
 The user interface is more user friendly than would be
possible without semantics.

Questions about the used knowledge model (9...11 of 15)

Again, please answer only those questions that you consider

relevant. If you do not understand any question, just skip it.
9. How critical is the initial correctness of the main ontol-
ogy/taxonomy in your application?
 Highly, all data processed are critically dependent on the
central ontology.
 Moderately, the data are dependent on the ontology but can
be adjusted to any changes.
 Little, the core ontology is considered volatile in its nature.
 There is no core ontology/taxonomy in our application.
10. If your system is distributed, how critical is that all parties use
the same knowledge model?
 Absolutely critical, running on different knowledge models
would not be possible.
 It is desirable, the parties would benefit from using the same
model.
 Not needed, our system does not depend on using the same
knowledge model.
 Our application is not distributed.
11. If more parties are using the same central ontology, how
critical is its timely synchronization?
 Very critical, the ontology is volatile and needs to be updated
in real-time.
 The central ontology is static and every change in it is very
problematic.
 There is no central ontology.

Yet again, please answer only those questions that you consider

relevant. If you do not understand any question, just skip it.
12. Does your application require a large supply of data?

 No, our application provides results of the same quality
regardless of the quantity of data processed.
 No, too much data can reduce the accuracy/reliability of the
results.
 Yes, the application can benefit from more source data.
 Yes, large scale of data processed is substantial to providing
interesting results.

13. Does your application integrate any heterogeneous data

sources?
 Yes, it is a part of the main functionality.
 Yes, as a positive side-effect of having semantics included.
 No, using more data sources is not possible in this kind of
application.
 No, there are no data sources to integrate.

14. Are there any inconsistencies in the data your application

uses?
 No, our data are completely consistent.
 Some inconsistencies may occur, but they are not relevant.
 The inconsistencies are expected and are handled properly.
15. Does your application process any non-structured documents?
(e.g., parsing of textual documents, HTML pages and/or natural
language)
 No, all inputs are structured in their nature.
 Yes, they are annotated manually.
 Yes, the data are processed automatically and our algorithms
are 100.
 Yes, we process the data automatically and expect some
degree of errors.
