Web Semantics: Science, Services and Agents on the World Wide Web 19 (2013) 121

Contents lists available at SciVerse ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

journal homepage: www.elsevier.com/locate/websem

Improving habitability of natural language interfaces for querying ontologies
with feedback and clarification dialogues
Danica Damljanovic a,, Milan Agatonovic b, Hamish Cunningham a, Kalina Bontcheva a

a University of Sheffield, Department of Computer Science, Sheffield, United Kingdom
b Fizzback, London, United Kingdom

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 25 January 2011
Received in revised form
8 February 2013
Accepted 20 February 2013
Available online 5 March 2013

Keywords:
Natural language interfaces
Ontologies
Learning
Clarification dialogues
User interaction
Feedback

1. Introduction

Natural Language Interfaces (NLIs) are a viable, human-readable alternative to complex, formal query
languages like SPARQL, which are typically used for accessing semantically structured data (e.g. RDF and
OWL repositories). However, in order to cope with natural language ambiguities, NLIs typically support
a more restricted language. A major challenge when designing such restricted languages is habitability
 how easily, naturally and effectively users can use the language to express themselves within the constraints imposed by the system. In this paper, we investigate two methods for improving the habitability
of a Natural Language Interface: feedback and clarification dialogues. We model feedback by showing the
user how the system interprets the query, thus suggesting repair through query reformulation. Next, we
investigate how clarification dialogues can be used to control the query interpretations generated by the
system. To reduce the cognitive overhead, clarification dialogues are coupled with a learning mechanism.
Both methods are shown to have a positive effect on the overall performance and habitability.

 2013 Elsevier B.V. All rights reserved.

Recent years have seen a tremendous increase in structured
data on the Web, with public sectors such as UK and USA
governments opening their data to the public,1 and encouraging
others to build useful applications on top. At the same time,
the Linked Open Data (LOD) project2 continues to promote the
authoring, publication and interlinking of new RDF graphs with
those already in the LOD cloud [1]. In March 2009, around 4
billion RDF statements were available while in September 2010
this number increased to 25 billion, and continues to grow. This
massive amount of data requires effective exploitation, which
is now a great challenge largely due to the complexity and
syntactic unfamiliarity of the underlying triple models and the
query languages built on top of them. Natural Language Interfaces
(NLIs) to rich, structured data, such as RDF and OWL repositories,
are a viable, human-readable alternative.

The main challenges related to building NLIs are centred around
solving the Natural Language understanding problem, the data that
 Corresponding author. Tel.: +44 1142221931.

E-mail addresses: danica.damljanovic@gmail.com, danica@kuatostudios.com

(D. Damljanovic), milan.agatonovic@gmail.com (M. Agatonovic),
h.cunningham@dcs.shef.ac.uk (H. Cunningham), k.bontcheva@dcs.shef.ac.uk
(K. Bontcheva).
1 http://data.gov.uk and http://www.data.gov.
2 http://linkeddata.org.

1570-8268/$  see front matter  2013 Elsevier B.V. All rights reserved.
http://dx.doi.org/10.1016/j.websem.2013.02.002

is being queried, and the user, and the way in which the users
information need is verbalised into a question.

Solving the Natural Language understanding problem includes
grammar analysis, and solving language ambiguity and expressive-
ness, e.g. [2]. Ambiguity can be avoided through the use of a Controlled Natural Language (CNL): a subset of Natural Language (NL)
that includes a limited vocabulary and grammar rules that must be
followed. Expressiveness can be improved by extending the system
vocabulary with the use of external resources such as WordNet [3]
or FrameNet [4].

The second group of challenges is related to the data that is
being queried, and building portable systemsthose that can be
easily ported from one domain or ontology to another without
significant effort. According to [5], a major challenge when building
NLIs is to provide the information the system needs to bridge the
gap between the way the user thinks about the domain of discourse
and the way the domain knowledge is structured for computer
processing. This implies that in the context of NLIs to ontologies, it
is very important to consider the ontology structure and content.
Two ontologies describing identical domains (e.g., music) can use
different modelling conventions. For example, while one ontology
can use a datatype property artistName of class Artist, the
other one might use instances of a special class to model the artists
name.3

3 See for example how the class Alias is used in the Proton System Module
ontology: http://proton.semanticweb.org/.

D. Damljanovic et al. / Web Semantics: Science, Services and Agents on the World Wide Web 19 (2013) 121

Ontologies can be constructed to include sufficient lexical
information to support a domain-independent query analysis
engine. However, due to different processes used to generate
ontologies, the extracted domain lexicon might be of varying
quality. In addition, some words might have different meanings
in two different domains. For example, How big might refer to
height, but also to length, area, or populationdepending
on the question context, but also on the ontology structure. This
kind of adjustments  or mappings from words or phrases to
ontology concepts/relations  is performed during customisation of
NLIs.

The third group of challenges is centred around the users
and how they translate their information need into questions.
While NLIs are intuitive, having only one text query box can
pose difficulties for users, who need to express their information
need through a natural language query effectively [6]. In order to
address this problem, several usability enhancement methods have
been developed with the aim to either assist users with query
formulation, or to communicate the systems interpretation of the
query to the user. In other words, the role of these methods is
to increase the habitability of the system. Habitability refers to
how easily, naturally and effectively users can use language to
express themselves within the constraints imposed by the system.
If users can express everything they need for their tasks, using the
constrained system language, then such a language is considered
habitable.

Our focus is on building portable systems that do not require
a strict adherence to syntaxthe supported language includes
both grammatically correct and ill-formed questions, but also
question fragments. We look at improving the habitability of
such NLIs to ontologies through the application of feedback and
clarification dialogues. We first discuss habitability and the four
different domains that it covers in Section 2. We then describe how
we model feedback relative to the specific habitability domains,
and evaluate it in a user-centric, task-based evaluation (Section 3).
Further on, in Section 4 we look at clarification dialogues and
whether they can improve the specific habitability domains, by
making the process of mapping a NL question into a formal query,
transparent to the user. We combine the dialogue with a light
learning model in order to reduce the users cognitive overhead and
improve the systems performance over time. We then examine
the approach we have taken, which combines clarification dialogues
with learning, in the controlled evaluation using the Mooney
GeoQuery dataset.

2. Habitability

According to Epstein [7], a language is habitable if:
 Users are able to construct expressions of the language which
they have not previously encountered, without significant
conscious effort.
 Users are able to easily avoid constructing expressions that are
not part of the language.
Another way of viewing habitability is as the mismatch between
user expectations and the capabilities of an NLI system [8].
Ogden and Bernick [9] describe habitability in the context of four
domains [9]:
 The conceptual domain of the language supported by the system
describes the area of its coverage, and defines the complete set
of objects and the actions which are covered. In other words,
the conceptual domain determines what can be expressed by
the system. Consequently, this domain is satisfied if the user
does not ask about concepts which cannot be processed by
the system. To cite the example from [9], the user could not
ask What is the salary of John Smiths manager? if there is

no information about managers in the system. The conceptual
domain of the language can be expanded to inform the user that
there is no information about managers in the system.
 The functional domain determines how a query to the system
can be expressed. Natural language allows different ways of
expressing the same fact, especially taking into account the
knowledge of the listener and the context. The functional
domain is determined by the number of built-in functions
or knowledge the system has available. If, for example, the
answer to a question requires combining several knowledge
sources, the system itself might not be able to answer it and
would require the user to ask two questions instead of one. A
habitable system provides the functions that the user expects.
Note that this is different from rephrasing the question due to
unsupported grammar constructions, which is related to the
syntactic domain.
 The syntactic domain of a language is determined by the
number of paraphrases of a single command that the system
understands. For example, to cite again the example from [9],
the system might not be able to understand the question What
is the salary of John Smiths manager? but, could be able to
process a rephrased one such as What is the salary of the
manager of John Smith?.
 The lexical domain is determined by the words available in
the lexicon. For example, in order to improve the coverage,
many systems extend their lexicon through the use of external
sources for finding synonyms.
For an NLI to be considered habitable, it should cover all
four domains. Habitability is an important aspect of a system to
measure because it can affect the usability of NLIs. By identifying
why systems fail to be habitable, we can identify the ways to
improve them [10].

One way to increase habitability is to use usability enhancement
methods such as feedback and clarification dialogues. We first look
at how feedback can improve the users experience with an NLI,
thus having an effect on habitability (Section 3). Further on, we look
at using clarification dialogues to improve the habitability domains
and make the process of mapping an NL question onto the formal
query transparent; this gives the users control as they can influence
the full interpretation of the query (Section 4).

3. Feedback

Showing the user the systems interpretation of the query
in a suitably understandable format is called feedback. Feedback
increases the users confidence and in the case of failures, helps
the user understand which habitability domain is affected. Several
early studies [11,12] show that after receiving feedback, users are
becoming more familiar with the systems interpretations and the
next step is usually that they try to imitate the systems feedback
language. In other words, returning feedback to the user helps
them understand how the system interprets queries, therefore
motivating them to use similar formulations and create queries
that are understandable to the system.

Showing feedback can be useful for communicating the message
between the user and computer clearly. This is comparable to
humanhuman communication, where participants usually try
to establish that the message they are trying to communicate
is properly understood. This process is called groundingas the
users try to ground what is being said [13]. As pointed out by
Clark and Brennan [13], humans seek evidence of understanding,
which can either be positive or negative. Negative evidence is
the evidence that they have not been understood, or heard, and
if they find any, they attempt to repair it. If they fail to find
any negative evidence, the assumption is that the other human
understood the message correctly. However, often people search

for a positive evidence as well, such as acknowledgements, or
inclusion of the relevant next turn. Using negative evidence to repair
in humanhuman conversation is studied in humancomputer
interaction such as in [14] where the authors conducted a study
with novice users who deal with a database system called SAMi.
The study reveals that 30% of the users time is spent in repair.
In comparison to humanhuman conversation, repair seems to be
more significant in humancomputer interaction, as it becomes the
primary medium for learning the actions required by the system
and also the method of correcting ineffective input [14].

In the following section we describe how we model feedback,
which is used to communicate the message between the user and
the system clearly, and suggest repair if necessary.

3.1. Baseline

To model and test feedback, we used a Natural Language Interface for querying ontologies, which we developed in our previous
work [15], as the baseline. The baseline system automatically generates a set of ranked query interpretations from an NL query or
its fragment, and then shows the answer of the best ranked option,
which returns a non-empty result to the user. Both failure to generate query interpretations, and generated interpretations that result in no answer, produce the same outputthe system shows the
message No answer found. Hence, the user does not receive any
feedback from the system in terms of how the query is interpreted
the message No answer found, does not provide any additional
information on whether this was due to system failure or due to a
non-existing knowledge. In other words, it does not become clear
to the user whether repair in a form of query reformulation might
or might not help in answering the question.

3.2. Extending baseline with feedback

We extend the baseline by implementing feedbackshowing
the user all possible query interpretations and the systems
rankings, so that the user then can modify the answer by choosing
the correct interpretation. We modelled feedback having in mind
four previously discussed habitability domains, and using repair to
improve them where appropriate. Given an NL query as input, the
system can produce the following output:
 Successthe query is successfully parsed and the query interpretation is correct. Showing feedback is a positive evidence of
understanding the query:
 A non-empty answer: if the users query is correctly inter-
preted, and the system returns a non-empty answer, feedback can increase the users confidence that the answer is
indeed correct and can also make the user familiarise himself
with the queried knowledge structure.

 An empty answer: the answer is not found although the system successfully parsed the question. As curating the knowledge is outside the scope of the topic discussed in this paper,
we consider this case to be a negative answer, and therefore
categorise it under success. The role of feedback is to communicate this message to the user effectively so that the user can
conclude with confidence that the answer is negative.
 Failurethe query is not successfully parsed or the query interpretation is incorrect. Showing feedback is negative evidence of
understanding the query and should suggest a repair. Ideally,
the system should be able to detect which habitability domain
is affected, and the feedback should be used to make the user
aware of the reasons why the failures happened. We distinguish
two kinds of failures based on whether or not repair in the form
of query reformulation could be used to correctly answer the
question:

 Encourage query reformulation: The question could be answered if reformulated, and the feedback should help the user
to reformulate the query to conform to the lexical, functional
or syntactic domain of the system.

 Encourage change of topic: The answer is not found because
the system could not find the information about the required
conceptsthe question could not be answered if reformu-
lated. The user should be able to conclude based on feedback
that the question is outside the conceptual domain of the sys-
tem.

3.2.1. Hiding complexities

One challenge when modelling feedback is showing the systems interpretation bearing in mind that NLIs are intended to be
used by users not necessarily familiar with ontologies. NLIs to ontologies usually translate a natural language query into some intermediate interpretation such as a set of triples or a formal query
such as SPARQL. Hence, the most natural way from the point of
view of the systems developer, would be showing either triples or
the SPARQL query. However, as our intention is to develop methods which are suitable for casual users as well as for semantic web
experts, our initial design sought to simplify the systems interpre-
tation, and hide complexities as much as possible. Therefore, the
following decisions were taken:
 Show labels instead of URIs.
 Show the linear list of elements (instead of triples) in the order
in which they appear in the question.
 Show relations between the elements by rendering a tree-like
structure.

3.2.2. Identified context and tree-based view

Implementing these decisions resulted in the Web interface
shown in Fig. 1. After the user posts a question, the system first
generates the table with two columns: Identified context, which
shows query interpretations as a linear list of elements (recognised
concepts and relations between them as found in the ontology),
and Our score, which shows the score by which the interpretations
are ranked. The system automatically selects the first option, and
the results are rendered using the tree-based view.

The user has the option to select any of the Identified contexts by
clicking on the radio button in the desired row. The results for the
selected interpretation will be rendered upon clicking. Further on,
the user can explore the tree-based view by selecting its nodes, for
example Country in Fig. 1, and the instances will be shown in the
right hand side pane.

In cases where the system recognises concepts in a query, but
does not find any results, the query interpretation (e.g. a set of
concepts) will be shown in the Identified context, and on selection
the message reading No relation found within this context is
displayed in the area for displaying the tree-based view.

3.2.3. Linearised list of concepts

The query interpretation is shown as a set of recognised con-
cepts, which follows the order in which they appear in the ques-
tion. However, due to the presence of properties in each query
interpretation (as properties are crucial to get the correct interpretation and consequently the answer), this can lead to the not
so natural effect; for an example, see Fig. 2. The Identified context
is shown including the has runtime parameter relation. The
interpretation as such is not understandable without an additional
explanation to the userusers must be trained to understand the
role of the property in between the recognised concepts. The other
option which we could consider is to reverse the order and show
the interpretation to read:

D. Damljanovic et al. / Web Semantics: Science, Services and Agents on the World Wide Web 19 (2013) 121

Fig. 1. Feedback: showing the user the systems interpretation of the query capitals of countries in Europe.

Fig. 2. Feedback and results for runtime parameters of RASP parser.

rasp parser (language analyzer) has runtime
parameters resource parameter
However, for more complex queries, this approach would require
modelling triples. For example, if we look back at the example in
Fig. 1, the first interpretation reads:
capital has capital country sub region of
Europe (continent)
To make this interpretation more natural, we would have to show:
country has capital capital;
country sub region of Europe (continent)
However, this makes it harder to follow which question term
refers to which ontology concept, and from where the relations
were derived. Therefore, we used the linearised representation, but
decided to model the tree-like view (see the lower left part of the
Fig. 2), so that it is indeed clear to the user that according to the
knowledge structure the RASP parser has runtime parameters ...
rather than the other way around.

When no results were found, the user was prompt with the

message No relations found within this context, see Fig. 3.

3.3. Evaluation

3.3.1. Training

Participants listened to the talk about Natural Language
Interfaces to Ontologies for 20 min, where they were given a short
overview of how the system works and the language supported by
the system. They were given a five minute demo on how to use the
Web-based interface.4

3.3.2. Evaluation measures

At the beginning of the experiment, we asked participants
to complete the questionnaire about their background (age,
gender, profession, knowledge of semantic technologies). They
then completed four tasks after each they were asked to answer
several questions. We then measured the following based on the
answers:
 Effectiveness: whether they could finish the tasks successfully.
 Feedback: whether the feedback was helpful or not for the
particular task.
 Difficulty of the supported language: whether or not it was easy
to formulate the queries for the task.
The subjects were offered a predefined set of answers, with an
option to add additional comments in free-text. After finishing all

In order to test feedback we organised a task-based evaluation
with the participants from the GATE Summer School in July 2009.

4 Slides are available from http://gate.ac.uk/sale/talks/gate-course-july09/slides-
pdf/questio.pdf.

Fig. 3. Feedback and results for init parameters of RASP parser.

the tasks, subjects were asked to complete the SUS questionnaire
as a standard user satisfaction measure.

In addition, we measured efficiencythe time each user spent

on each task, and also the number of queries they used.

3.3.3. Dataset

We initialised the system with two domain ontologies. The
first one covers GATE components,5 while the second one is the
Mooney GeoQuery ontology that covers the geography of the
United States.6

Subjects were asked to perform four tasks. For each task, they
had the opportunity to choose between the two ontologies. If they
were not confident in their knowledge about GATE, we hoped they
would choose the task relating to US geography. The task pairs
covering two domains were of the same complexity.

3.3.4. Tasks

Our intention was to see whether users could make the correct
conclusions based on the systems feedback, and therefore conclude correctly whether the systems response resulted in a success
or failure, and in case of failures whether it could successfully suggest repair. We designed four tasks, each one to assess a specific
part of the feedback discussed at the beginning of this section:
 Successa non-empty answer: an NL question is expected to be
successfully parsed, the query interpretation correct, and the
answer to the question found and returned to the user. Based
on feedback, the user should conclude that the answer is correct
and terminate the task (Task 1).
 Successnegative answer: an NL question is expected to be
successfully parsed, but the answer to the question is negative.
Based on feedback, the user should conclude that the query
interpretation is correct, as the knowledge about concepts
exists, but the lack of relations between the concepts indicates
that the answer is negative. Based on feedback, they should
conclude that they successfully finished the task and they
should terminate it (Task 2).
 Failureencourage repair through query reformulation: an NL
question is parsed but the resulting output leads to an incorrect
(often empty) answer. Based on feedback, the subjects need to
decide that there is knowledge about what they are searching
for in the system, but the query they are likely to type in first is

5 http://gate.ac.uk/ns/gate-kb.
6 The Mooney geography dataset is available from http://www.ifi.uzh.ch/ddis/
research/talking-to-the-semantic-web/owl-test-data/.

too complex and needs reformulating, rather than concluding
there is no answer. A successfully finished task results in
Successa non-empty answer (Task 3).
 Failureencourage change of topic: an NL question cannot be
parsed or the query interpretation is incorrect or partially
correct (e.g., the output leads to a more generic answer as
some concepts are skipped during query processing as they
are not in the knowledge base). Based on feedback, the user
should conclude that the query reformulation cannot help as
the knowledge is not available in the system (Task 4).
The task pairs were as follows:
Task 1:
 Task 1a: Find part of speech taggers which exist in GATE. Find
out which parameters exist for the POS Tagger of your choice.
 Task 1b: Find mountains which exist in the United States. Find
out in which state the mountain of your choice is located.
Task 2:
 Task 2a: Imagine that you are a GATE developer who needs to
extend the RASP Parser. Your task is to find out the names of init
parameters.
 Task 2b: Find out which states border Hawaii.
Task 3:
 Task 3a: What are the parameters of the PRs which are included
in the same plugin as the Morpher?
 Task 3b: Which rivers flow through the state in which the
Harvard mountain is located?
Task 4:
 Try exploring the knowledge available in the system. Either
search for various components of GATE such as PRs, plugins, LRs,
VRs, or explore the geography of the United States by enquiring
about: cities, states, rivers, mountains, highways, etc.. Then ask
some questions in order to connect these concepts such as
Which states border Georgia? or Which rivers flow through
states which border California?. Input as many queries as you
like.

3.3.5. Participants

Participants were all external to Sheffield University, and were
not known to us before they registered to attend the GATE Summer
School. They were almost evenly distributed across researchers,
software developers and students, as well as across gender. We
measured their expertise in ontologies, ontology editors and

D. Damljanovic et al. / Web Semantics: Science, Services and Agents on the World Wide Web 19 (2013) 121

Fig. 5. Task difficulty based on the success rate per task: finished with ease (0),
finished with difficulty (1), not finished (2).

Fig. 4. Expertise in using ontologies, ontology editors and SPARQL for 30 participants (M = 60.79, SD = 22.97).

SPARQL, using the Likert scale.7 Their knowledge of the semantic
web technologies was neither basic, nor advanced (see Fig. 4),
although did lean towards a more advanced level (M = 60.80,
SD = 22.98).

3.3.6. Results

While the number of participants at the GATE Summer School
was 50, the participation in the evaluation was on a voluntary basis,
and many did not complete all required tasks or all questionnaires.
We therefore disregarded all
leaving 30
participants who had completed the background questionnaire
and at least the first three tasks. 11 out of these 30 participants
finished Task 4, while 19 completed the SUS questionnaire.
However, all of them have previously finished at least three tasks
and therefore we can make conclusions about the user satisfaction
based on these records.

incomplete records,

Effectiveness. Fig. 5 illustrates the task difficulty based on the
mean success rate across four tasks. Based on the average value
of the task difficulty across all participants, Task 1 was the easiest,
while Task 3 was the most difficult to finish. This is because the
subjects usually followed the wording of the description of the
task carefully, and hence, for Task 1, as the description is given
in two sentences (e.g., Find mountains which exist in the United
States. Find out in which state is the mountain of your choice
located.) they typed in a query per sentence. Also the second
sentence asks about the mountain/the POS tagger of your choice,
which indicates that they first needed to find out the names of
mountains/POS taggers using the first query, and then follow up
with another query formulated using the result from the first. For
Task 3, they struggled more as their first attempt was to finish
the task with one complicated query (often following the exact
wording of the task, which was given in one sentence), before
they decided to reformulate the query. This was in line with our
expectations as Task 3 was designed to test whether feedback can

7 Expertise was calculated as a linear combination of these three, and then
normalised on the scale from 0 to 100 similar to how the SUS score is calculated.
The scores were normally distributed.

help users to resolve failures by suggesting repair through query
reformulation.

However, looking into the distribution of different difficulty
levels per task, as shown in Fig. 6, only Tasks 2 and 3 had failures
(23.33% and 20% respectively). Task 1 was completed successfully
by all participants, with only four subjects reporting difficulty. The
log shows that the queries of those who reported difficulty were
very similar to the queries of the subjects who reported that they
finished the task with ease. However, instead of terminating the
task after determining the answer, the former group of subjects
usually followed up with an additional set of queries, some of
which could not be answered by the system. For example, after
asking What part-of-speech taggers are there in GATE?, a subject
followed up with What parameters are there for the Hepple
tagger?. Both queries were correctly answered by the system,
however, the subject followed up with Can you give me any
more detailed information?. The system returned no answer, and
the subject terminated the task and reported difficulty. Another
subject tried 14 queries, all of them being very similar, but just
reworded versions enquiring about POS taggers in GATE. In some
cases, this caused the system failure such as in What are the PRs
for POS tagging? where POS tagging was not recognised due
to the failure of the morphological analyser, and after the user
reformulated the question to What are the PRs for POS taggers?
the results were returned.

Interestingly, if participants managed to finish Task 2 success-
fully, they did not experience any difficulties. This is because
most subjects managed to formulate the query that was correctly
parsed by the system immediately, and then the subjects either
understood the systems message (that the answer is negative),
or they did not and attempted to follow up with many query
reformulationshowever this did not help and they eventually terminated the task reporting that they could not finish it.

Task 3 was not finished in 20% of the cases. In comparison to
Task 2, this is slightly better, however, a large portion of those
who completed Task 3 (37.5%) reported difficulty in doing so. This
is because the query reformulation that was part of this task was
not easy for the majority of participantsit took them 5.5 queries
on average to successfully complete the task, although the optimal
number of queries that was necessary was 2.

Task 4 was finished by only 11 participants, the majority of
whom reported that they finished it with ease. Based on the query
logs, the reason seems to be that the majority of subjects used
queries that were successfully answered in and similar to those
used in the previous tasks. Hence, they did not experience any

Fig. 6. Frequency of different success rates per task.

failures and we could not test the case when the queries fell outside
of the conceptual domain of the supported language (Failure
encourage change of topic).
User satisfaction. With regard to the SUS score, the result (M =
66.97, SD = 15.38) can be interpreted to be in between OK and
good: according to [16] the score of 50.9 is the lower limit for OK,
and 71.4 is the lower limit for good (see Fig. 7).

Subjective measures of user satisfaction. Fig. 8 shows the distribution of the subjects subjective judgment on the Identified context.
The exception is Task 2 for which we did not ask subjects about
Identified context explicitly. Instead we asked them whether it was
clear that the answer was negativethere were no states or no init
parameters of the RASP parser.

Task 1: Successa non-empty answer. A large percentage of
subjects (43.33%) found the Identified context confusing or neutral
when doing Task 1, although all of them successfully finished
the task. Six subjects who found the Identified context confusing,
reported that several of the generated examples were confusing
or non-sensical e.g. state -- is mountain of -- rainer.
The reason for this was that the system showed the recognised
elements of the query in the order in which they appeared in the
query. A more natural way of showing this to the user would be:
rainer -- is mountain of -- state. However, this kind
of interpretation is a step towards showing triples to the end-user,
and for more complex queries, these would need to be multiplied.

Fig. 7. Distribution of SUS scores for 19 participants (M = 66.97, SD = 15.38). A
large portion of the participants (42.1%) rated the system usability as good (in the
range from 70 to 80).

D. Damljanovic et al. / Web Semantics: Science, Services and Agents on the World Wide Web 19 (2013) 121

Fig. 8. Clarity of feedback for all tasks.

As we have previously discussed (see Section 3.2.3), our intention
was to mark question terms as recognised without going deeply
into the complexities of ontology structure; the tree-based view
was meant to correctly show the relations and that in fact rainer
-- is mountain of -- state, and not the other way around.
Indeed, all subjects had positive comments on the tree-based view
component of feedback.

Task 2: Successa negative answer. With regard to seven subjects who failed to complete Task 2, this happened due to the following reasons:
 28.57% said the System provided confusing output so they could
not determine what to do.
 71.43% said the System provided no output so they could not
determine what to do.
The last group can be classified as system failure, and therefore
we conclude that the remaining 28.57% of failures happened due
to the users struggling to understand the feedback.

Looking at the results of the 23 participants who claimed that
they finished task 2 with ease (see Fig. 9):
 For 34.78% it was not clear that there were no bordering
states/no init time parameters for RASP parser for that specific

Fig. 9. Clarity of feedback for Task 2 considering only the participants who finished
the task with ease.

task based on feedback, but they could successfully finish the
task by looking at the results for some other queries. For
example, some of them said that they determined that the
system meant that there were no bordering states by querying
another state with others bordering it.

Fig. 10. The subjects perception about the difficulty of the supported language.

that the system provided confusing output: could not manage
to find out how to formulate the query; tried several ones by
refinement. Upon further investigation of this users query log, we
found out that he tried 18 different queries, most of which gave
some results, however, they were either too generic (e.g., PRs), or
too specific and long, and also very similar to the wording of the
actual task, for example creole plugin PRs parameters that are the
same as the parameters of GATE morphological analyser.

 13.04% experienced the system failure, which they recognised
as repair and reformulated the query in order to finish the task
successfully.
 For 52.17% of participants, the feedback shown by the system
was clear enough to immediately draw conclusions that there
was no answer.
Overall, one third of subjects struggled to understand the systems feedback, however four fifths of those found an alternative
way to solve the task usually by trying similar queries which returned a non-empty result.

From Task 2, we conclude that the Identified context coupled
with the message No relation found within this context was not
useful even though 76.67% of subjects found a way to complete
the task successfully. Hence, those queries for which the answer
is negative, showing the user that the system knows about certain
concepts, but does not find any answer due to the missing relations,
resulted in a large number of subjects being confused. Some of
them reported that they would rather see the message There
are no states or There are no parameters instead of the list of
recognised concepts and a generic message No relations found.
Task 3: Failureencourage repair through query reformulation.
Among 20% of subjects who failed to complete Task 3, one reported

The majority of subjects tried to input the exact wording of
Task 3 into the system and then, since the system showed the
recognised concepts but no answer, a large number recognised the
need to repair and reformulated the query. This resulted in 80% of
subjects successfully finishing the task, while 20% gave up.

Difficulty of the supported query language. Fig. 10 illustrates the
difficulty of the supported language as perceived by participants,
per task. According to these results, subjects struggled most with
formulating queries for Task 3. This is because the majority
tried to solve the task using one complicated query, which they
reformulated several times before deciding to split it in two, which
successfully completed the task.

Table 1 illustrates the optimal number of queries that was
required for successfully finishing the first three tasks, and the
average number of queries used across all subjects. The lowest

D. Damljanovic et al. / Web Semantics: Science, Services and Agents on the World Wide Web 19 (2013) 121

Table 1
Number of queries per task across all subjects.

Table 2
Difficulty of the supported language as perceived by subjects in the two studies.

Task

Optimal
#queries

Avg.
#queries

Avg. #queries
(tasks finished)

Avg. #queries
(tasks not finished)
n/a

number of queries was for Task 2. However, it seems that the
subjects who finished Tasks 2 and 3 successfully, used fewer
queries. The logs reveal that the majority of subjects who failed to
complete the task, could have finished it even after the first query,
given that they could understand feedback. While the highest
number of average number of queries per participant is for Task 1,
this is not related to the task difficulty, as the majority finished this
task with ease. In contrast, the query log shows that the subjects
could finish Task 1 usually after the second query, however, they
decided to try several other similar queries before marking that
they had finished the task successfully.

One of the subjects stated that [our system] is a nice tool
but can easily be fake i.e. try state mountains in the States or
state apple, monkeys, bananas, mountains in the USA. This is
an interesting observation, and is indeed true. Our system would
indicate that state at the beginning of the query is recognised as
geo:State, and the user, knowing this is not true, would need to
reformulate the query (i.e. use similar words such as give me or
show or list instead of state at the beginning of the query).
Comparison with baseline. The baseline system described in
Section 3.1 was tested in the baseline usability study through
measuring effectiveness, efficiency, and user satisfaction with 12
participants, and 4 tasks, covering the GATE domain. The goal and
the scope of that evaluation goes beyond the goals of this paper
(see [17] for more details), however, when designing the feedback
usability study presented here, we repeated some tasks deliberately
(Tasks 1 and 4), in order to make a comparison where appropriate.
Task 1 was intended to test the difference between the effectiveness and the efficiency of the baseline with feedback in comparison to
the baseline without feedback. The aim was to answer the question
of whether feedback improves usability of the baseline system or not
for those tasks for which the answer exists in the knowledge base.8
 Effectiveness. In the baseline usability study the tasks equivalent
to Task 1 in the feedback usability study resulted in a task difficulty of 0.67, while in the feedback usability study the same
task resulted in a task difficulty of 0.13. This result shows that
the task seemed easier in the baseline with feedback in comparison to the baseline without feedback. We tested the significance
of this difference using Chi-Square test of independence. Our
null hypothesis was that there is no relation between the system
used (independent variable) and effectiveness measured through
the task difficulty (dependent variable). Based on the results we
can reject the null hypothesis, leading us to the conclusion that
the difference in effectiveness in using the two systems is sig-
nificant,  2(2, N = 42) = 8.31, p = .016. This indicates that
feedback had a positive impact on effectiveness as the task seemed
significantly easier when performed using the baseline with
feedback in comparison to the same task performed using the
baseline only.

8 As in the feedback usability study where the first task was equivalent to the two
tasks (Task 1 and 2) in the baseline usability study, we first merged the results of
these two into one. For effectiveness, in case the success score differed for the two
tasks in the previous study, the higher one was picked as the representative. For
example, if one of the tasks was marked as task completed with ease (0), and the
other failed to complete (2), the overall assigned score was failed to complete (2). For
efficiency, measured through the time spent on the task, we summarised the time
for Tasks 1 and 2 into one value.

Query language
Easy (%) Neutral (%) Difficult (%)

Baseline (defined task)
Baseline with feedback (defined task)
Baseline (undefined task)
Baseline with feedback (undefined task)
 Efficiency. With regard to efficiency, in the baseline usability
study it took subjects 180.5 s on average to finish the task
equivalent to Task 1 in the feedback usability study. In the
feedback usability study the same task took 155.27 s on aver-
age, which indicates that the subjects were faster when using
the baseline with feedback system. To test the significance of
this difference we used a 2-tailed independent t-test, which
revealed that this difference is not significant (t(40) = 0.19,
p = .85 with equal variances assumed), and thus we retain the
null hypothesis and conclude that there is no relation between
the system used (independent variable) and efficiency measured through the time spent on task (dependent variable). This
indicates that feedback did not have a significant influence on how
quickly subjects could finish the task.
Further on, as we assessed the difficulty of the supported language in the baseline usability study, we can now compare the results of the two studies for the tasks that we could then have
repeated in the feedback usability study. In other words, we compared the perception of the difficulty of the supported language in
the baseline usability study and in the feedback usability study for
the equivalent tasks.

As we described in Section 3.1, the baseline without feedback
and the baseline with feedback support the same query language.
Hence, the comparison of the users perception of the difficulty of
the supported language will help us reveal whether there is any
effect of some other factors (such as user-interaction features of the
tree-based view, and the feedback delivered through the Identified
context) that influence the perception of the difficulty of the query
formulation. Our null hypothesis is that there is no relation between
the system used, and the difficulty of the supported language. We used
non-parametric Fisher Exact Test to assess this.9

Table 2 shows the distribution of the answers, indicating that
there were more positive answers in the feedback usability study in
comparison to the baseline usability study for both defined and undefined tasks. For defined tasks, Fishers Exact test reveals that this
difference is not significant (F = 5.25, p = .07) hence there is no
evidence to reject the null hypothesis (that there is no difference
in how the two groups of subjects perceived the query formulation
for defined tasks). For the undefined task, this difference is significant (F = 8.02, p = .015) indicating that subjects had the impression that the query language in the feedback usability study was
easier than the one required by the baseline system in the baseline
usability study. This indicates that feedback had a positive effect on
the users perception of the difficulty of the query language and
helped boost the users experience.

3.4. Summary and discussion

In this section we presented one possibility of designing feedback in Natural Language Interfaces for querying Ontologies, and a

9 Fisher exact test is the exact version of Chi-square, which is usually used
for testing 2-by-2 tables, particularly for small samples. As Chi-square is an
approximation, it is not as trustworthy as the exact test on the data with expected
counts less than 5.

task-based evaluation with 30 subjects. This was conducted in order to assess whether using feedback has any effect on the usability
of such systems and hence could help in building habitable NLIs.

As a baseline we used a Natural Language Interface for querying
ontologies that we developed and evaluated in our previous work.
We first extended the baseline by implementing feedback using the
following two elements:
 The Identified context table showing all query interpretations
to the user, where each interpretation is a linear combination
of the concepts and relations between them. The order of the
recognised concepts follows the order in which they appear in
the question.
 The tree-based view shows the concepts and their relations to
any selected Identified context.
We designed feedback to test four different aspects:
 Successa non-empty answer was tested through Task 1.
 Successa negative answer was tested through Task 2.
 Failurerepair through query reformulation was tested through
Task 3.
 Failureencourage change of topic was intended to be tested
through Task 4. However, the subjects did not experience
this kind of failure at all and hence this aspect is not further
discussed.
In the baseline usability study described elsewhere (see [17]),
among other tests, we measured effectiveness and efficiency per
task, and also the perception of difficulty of the supported query
language per user. In the feedback usability study presented here,
we repeated two tasks (Tasks 1 and 4) in order to make a
comparison, where appropriate, of the two systems: the baseline
with feedback and the baseline without feedback.

In the feedback usability study all subjects completed Task 1
although four of them did so with difficulty. This result is significantly better (p = .01) than the results for the same task in the
baseline usability study, indicating that the tasks for which the answer exists in the knowledge base are more easily successfully finished with the baseline with feedback in comparison to the same
kind of tasks performed with the baseline system. However, although the subjects finished the task more quickly than in the previous study, this difference is not significant (p >= .78).

Identified context was not well received even for Task 1, which
was the easiest. For Task 2, the Identified context was not key to
success. Instead of understanding that there were no relations
within the identified context as it was stated by the system, the
subjects reformulated the initial query many times, and tried
similar ones in order to understand the answer. The average
number of queries per task for the tasks completed successfully
is much lower than for those that were not completed, indicating
that the subjects who did not understand the systems messages
believed that they needed to reformulate the query; they did this
many times until eventually giving up. This is specifically the
case for Tasks 2 and 3. For Task 3, the feedback which combines
Identified context and the tree-based view was quite successful in
suggesting repair and 80% of subjects managed to successfully
reformulate their initial queries and finish the task successfully.

Overall, our conclusion is that feedback can help to build habitable NLIs through showing the user how the system interpreted the
query. By looking at the interpretations, the users can better understand if the query they formulated is too complex for the system,
and if they need to reformulate it in order to receive the answer.
More specifically:
 Feedback had a positive impact on the overall effectiveness of
the system, but no significant effect on efficiency.
 Feedback had a positive impact on the subjects perception of
the difficulty of the supported query language.

 The Identified context showing the linearised list of concepts
was not well accepted, especially for the cases when the answer
to the question was negative. In other words, showing that
the system knows about certain concepts, but cannot find any
relations between them was not clear and the subjects disliked
the generic message No relation found.
 The tree-based view and especially its interactive feature was
well accepted. This indicates that showing context from which
the answer was derived can increase user confidence.
 For complex queries, feedback was useful to suggest repair
through query reformulation.
We attempted to render feedback in a user-friendly manner and
our eventual goal is to make the vast amount of structured information available to casual users. However, based on the evaluation
presented in this section, we can only make claims about the population represented by our sample which largely included computational linguists, computer scientists, and software developers, who
were familiar with semantic web technologies even if not on an advanced level.

While feedback can be useful to train the user towards formulating queries that are supported by the system thus improving
habitability, this method does not offer the user to be involved or
to anyhow modify potential query interpretations. In other words,
the user can either:
 Choose from an already existing list of query interpretations, or
 Recognise repair and reformulate the query for which the
system will generate a new set of interpretations.
In the next section, we look at clarification dialogues as a method
that allows the user to be involved in modifying or generating
query interpretations. In other words, the user can supervise the
process of mapping an NL question into a formal query in order to
produce the correct answer. Habitability in this case is expected to
be improved by extending the existing habitability domains of the
language through the dialogue, and repair is expected to happen
during the process of mapping.

4. Clarification dialogues

Using clarification dialogues is a common way of solving the
ambiguity problem in NLIs to ontologies (e.g., Querix [18], AquaLog [19]), and involves engaging the user in a dialogue whenever the system fails to solve the ambiguities automatically. This
method is especially effective for large knowledge bases with a
huge number of items with identical names, but also when the
question is ambiguous. For example, if the user asks How big is
California?, the system might discover ambiguity when trying to
map big into state population or state area. Hence, using
clarification dialogues will allow the user to specify the meaning of
big. Another way to solve ambiguities is to show all possible answers to the user, however, this might not be feasible when there
are too many alternatives. Clarification dialogues can help to define
the information need precisely, thus resulting in a system with a
higher precision.

We extend the application of clarification dialogues to support
not only solving ambiguities but rather to make the whole process
of mapping an NL question into the form that would lead to the
correct answer transparent to the user. We combined clarification
dialogues with a light learning model in order to improve the
habitability domains of our NLI. We consider several aspects of
habitability domains previously discussed in Section 2, and look
at how we can improve them.

The lexical domain of NLIs to ontologies is usually bound to the
ontology lexicalisations, and then extended from various resources

D. Damljanovic et al. / Web Semantics: Science, Services and Agents on the World Wide Web 19 (2013) 121

to include synonyms, hypernyms and hyponyms. The most common resources are WordNet [3], FrameNet [4], and OpenCyc.10 A
few systems use resources accessible through the Semantic Web
via the owl:sameAs relation (e.g., PowerAqua [20]), or even personalise the system by including user-centric vocabulary. We derive
the domain lexicon from the ontology, enrich it with synonyms
from WordNet, and then further enrich it from dialogues with the
user.

The most obvious way to improve the conceptual domain is to
add a new knowledge about the non-existing concept to the on-
tology. This goes beyond the scope of our research, as we are interested in querying the existing data without any intention to
give feedback on quality (e.g. by adding, updating or deleting statements from the ontology). Hence, instead of improving this domain
towards extending the systems knowledge, we focus on communicating the failure to the user through the dialogue that shows
what the coverage of the system is. The dialogue contains suggestions listing only the available conceptsthose that the system
knows about. For example, if the ontology is about geography,
and the user asks List actors from Hollywood, the system
will not know about actors, but knows about Hollywood (a
district in Los Angeles). In this case, it will prompt the
users with a dialogue, asking them to map actors to properties
such as population, area and classes such as state, which
are clearly not related to actors at all. In other words, the dialogue will assist the user to receive the answer about all concepts/
relations related to Hollywood, however, the lack of suggestions
mentioning actors indicates that the query falls beyond the coverage of the supported conceptual domain.

The functional domain can be improved by extending the list
of functions available in the system, given that those extensions
are in line with user expectations. However, for flexible supported
languages, such as ours, which do not have strictly defined rules
to be followed, issues with unsupported functions can arise from
the system incorrectly interpreting a question term. We solve this
problem by allowing different configurations of the dialogue. For
example, one possible configuration (a force dialogue mode) is to
model a dialogue for any attempt to map a question term onto an
Ontology Concept. In this case it is possible to extend the dialogue
allowing users a flexible mapping of a NL query to the formal
query. For example, we added a None element to each dialogue
to allow users to ignore attempts to perform mappings that would
cause a system failure. In this way, instead of asking the user to
reformulate the question, we are asking the system to ignore the
specific question term. In addition, the suggestions shown to the
user hold some of the functions that are used to map a NL query
into the SPARQL. This is especially the case for datatype properties
of type number, where, in addition to mapping a word/phrase onto
a datatype property, it is possible to map it to a function such as
minimum, maximum, and sum, which would communicate to the
system to apply the special function to the result set before it shows
the answer to the user.

The syntactic domain of any supported language can be improved by allowing as many grammatical constructions as possi-
ble. When the system fails to answer the question that falls out of
the scope of the syntactic domain, an ideal solution would be to
encourage the user to reformulate the query. We improve this domain by making the process of mapping a NL query into SPARQL
transparentthe user controls the mapping through the dialogue.
In some situations the dialogue might reveal that certain syntactic
structures are not supported. For example, if the system attempts
to map as to an ontology concept, it shows the user that the system is not aware of the semantic meaning of the word, and the
question should be reformulated.

10 http://www.opencyc.org/.

Fig. 11. FREyA workflow: from a Natural Language question to the answer.

The involvement of the user into the dialogue is empowered
by a light learning model, in order to improve the habitability
domains (and hence the systems performance) over time. We
tested the combination of clarification dialogues and the learning
model through the implementation of the FREyA system to which
we now turn.11

4.1. FREyA workflow

FREyA12 is an interactive Natural Language Interface for
querying ontologies, which combines syntactic parsing with
the ontology-based lookup in an attempt to precisely answer
questions. If the system fails to automatically generate the answer
(or when it is configured to work in the force dialogue mode, see
Section 4.7) it models the clarification dialogue. The suggestions
shown to the user are found through ontology reasoning and
are initially ranked using a combination of string similarity and
synonym detection. The system then learns from user selections,
and improves its performance over time.

Fig. 11 shows the workflow starting with a Natural Language
question (or its fragment), and ending when the answer is found.
The syntactic parsing and analysis generates a parse tree (using
the Stanford Parser [21]) and then uses several heuristic rules in
order to identify Potential Ontology Concepts (POCs). POCs refer to
question terms/phrases which can, but not necessarily have to,
be linked to Ontology Concepts (OCs). POCs are chosen based on
the analysis of the syntactic parse tree, however this analysis does
not require strict adherence to syntax and works on ill-formed
questions and question fragments as well as on grammatically
correct ones. For example, nouns, verbs, or WH-phrases such
as Where, Who, When, How many are expected to be found by
our POC identification algorithm. This algorithm is based on the
identification of prepreterminals and preterminals in the parse
tree, as well as on their part-of-speech tags (see [22]).

The ontology-based lookup links question terms to logical forms
in the ontology, which we call Ontology Concepts (OCs), without
considering any context or grammar used in the question (apart
from morphological analysis, see [15]). Ontology Concepts refer to
instances/individuals, classes, properties, or datatype property values
such as string literals. By default, the system assumes that the
rdfs:label property is used to name the specific Ontology

11 Note that this system is different from the systems described in Section 3.
12 More information about the system and the source code is available from:
https://sites.google.com/site/naturallanguageinterfaces.

Concept. However, for ontologies which use different naming
conventions (such as using dc:title inside the MusicBrainz
dataset), it is possible to predefine which properties are used
for names. This will enable the system to make the distinction
between making a datatype property value element and an instance
element. This distinction is important for determining the semantic
meaning of the question terms.

The consolidation algorithm aims at mapping existing POCs to
OCs automatically. If it fails, the user is engaged in a dialogue. For
instance, in the query Give me all former members of the Berliner
Philharmoniker, the POC identification algorithm will find that the
Berliner Philharmoniker is a POC, while the ontology-based lookup
will find that Berliner Philharmoniker is an OC, referring to an
instance of mm:Artist. As the only difference in the POC and
the OC text is the determiner (the), the consolidation algorithm
will resolve this POC automatically by removing it, thus verifying
that this noun phrase refers to the OC with dc:title Berliner
Philharmoniker.

When the system fails to automatically generate the answer,
it will prompt the user with a dialogue. There are two kinds of
clarification dialogues in FREyA:
 Disambiguation dialogues involve users in resolving all identified
ambiguities.
 Mapping dialogues involve users in mapping a POC to one of the
suggested OCs.
While the two types of dialogues look identical from the users
point of view, there are differences which we will highlight here.
Firstly, we give a higher priority to disambiguation dialogues in
comparison to mapping dialogues. This is due to our assumption
that question terms which exist in the graph (OCs) should be
interpreted before those which do not (POCs). Note that FREyA
does not attempt to interpret the whole question at once, but
rather one pair at a time. In other words, each resolved dialogue
can be seen as a pair of two OCs: an OC to which a question term
is mapped, and the neighbouring OC (context). Secondly, the way
the suggestions are generated for the two types of dialogues differ.
Disambiguation dialogues include only suggestions with Ontology
Concepts that are the result of ontology-based lookup. Mapping
dialogues, in contrast, show suggestions that are found through
ontology reasoning. This ensures that any suggestion that is shown
to the user will generate an answer.

Finally, the sequence of disambiguation and mapping dialogues
themselves are controlled differently for these two kinds of dia-
logues:
 Disambiguation dialogues are driven by the question focus or the
answer type, whichever is available first: the closer the OC to be
disambiguated to the question focus/answer type, the higher the
chance that it will be disambiguated before any other. The question focus is the term/phrase which identifies what the question
is about, while the answer type identifies the type of the question (such as Person in the query Who owns the biggest department store in England?). The focus of this question would
be the biggest department store (details of the algorithm for
identifying the focus and the answer type are described in [23]).
After all ambiguities are resolved, the workflow continues to resolve all POCs through mapping dialogues.
 Mapping dialogues are driven by the availability of OCs in the
neighbourhood. We calculate the distance between each POC
and the nearest OC inside the parse tree, and the one with the
minimum distance is the one to be used for the dialogue, before
any other.

4.2. Disambiguation dialogues

Table 3
Generating suggestions based on the type of the nearest OC.

Type of the closest OC
Class or instance

Datatype property of type number

Object property
Datatype property value

Suggestions
All classes connected to the OC by exactly
one property, and all properties defined
for this OC
Maximum, minimum and sum function
of the OC
All domain and range classes for the OC
Suggestions for the instance to which this
value belongs

specific meaning. Disambiguation dialogues consist of an ambiguous
term and a list of OCs. The user is then asked:

I struggle with [ambiguous term]. Is
[ambiguous term] related to:

OC1
OC2
...
OCn

While it is possible to automatically disambiguate the meaning
depending on the question context and using ontology reasoning
(e.g. ontology relations), this option could be expensive, but
also insufficient. Our approach suggests that any automatic
disambiguation could be corrected by involving the user in a
dialogue. For example, if someone is enquiring about Mississippi,
we might not be able to automatically derive whether the query
refers to geo:River,13 or geo:State, because we do not
have enough context for effective disambiguation. However, if
the question is Which rivers flow through Mississippi?, the
context can help automatically derive that the question is about
Mississippi state, due to the existing relation in the ontology such
as geo:River -- geo:flowsThrough -- geo:State.

4.3. Mapping dialogues

For all POCs that could not be automatically resolved to an OC,
mapping dialogues were initiated. They consisted of an unknown/
POC term and a list of suggestions. The user was then asked:

I struggle with [POC term]. Is [POC term]
related to:

suggestion 1 (OC1)
suggestion 2 (OC2)
...
suggestion n (OCn)

Note that while the OCs in Disambiguation dialogues are found
by ontology-based lookup, the OCs (suggestions) in Mapping
dialogues are found by ontology reasoningthey are derived based
on the closest OC to the POC term. The closest OC is found by walking
through the syntax tree. Based on the type of the closest OC, rules
for generating suggestions vary (see Table 3). For the closest OC
X, we identified its neighbouring concepts which were shown to
the user as suggestions. Neighbouring concepts include the defined
properties for X, and also its neighbouring classes. Neighbouring
classes of class X are those that are defined to be:
 The domain of the property P where range(P) = X, and
 The range of the property P where domain(P) = X.
Option none (None Element) is always added to the list of
suggestions (see Table 4), unless FREyA is configured differently
(see Section 4.7 for different modes). This allows the user to

For ambiguous OCs that are identified through ontology-based
lookup, dialogues are modelled so that the user disambiguates the

13 For clarity of presentation, we use prefix geo: instead of http://www.mooney.
net/geo# in all examples.

D. Damljanovic et al. / Web Semantics: Science, Services and Agents on the World Wide Web 19 (2013) 121

Table 4
Sample queries and generated suggestions for the identified POCs.

Query
Population of
cities in
California

Population

Closest OC
geo:City

Suggestions
1. City population
2. State
3. Has city
4. Is city of
5. None

Population of
California

Population

geo:California 1. State population
2. State pop density
3. Has low

n. None

Which city has
the largest
population in
California

Largest
population

geo:City

1. max (city population)
2. min (city population)
3. sum (city population)
4. None

ignore suggestions if they are irrelevant thus improving FREyAs
functional domain. That is, the system assumes that the POC in
the dialogue should not be mapped to any suggested OCs, and
therefore the system learns that this POC is either: (1) incorrectly
identified, or (2) cannot be mapped to any OC as the ontology does
not contain the relevant knowledge. While this option will not be
of significant benefit to end-users, it is intended to identify flaws
in the system and encourage improvements.

The task of creating and ranking suggestions before showing
them to the user is quite complex, and this complexity increases
with the size of the queried knowledge source.

4.4. Ranking suggestions

The initial ranking of suggestions is based on string similarity
between a POC term and the suggestions, and also based on
synonym detection:
 String similarity. We combined Monge Elkan14 metrics with the
Soundex15 algorithm. When comparing two strings the former
gives a very high score to those which are exact parts of the
other. For example, if we compare population with city pop-
ulation, the similarity would be maximised as the former is
contained in the latter. The intuition behind this is that the
ontology concepts are usually named using camelCased names,
and are more explicit than how they are usually referred to
using natural language, e.g., cityPopulation, stateArea,
projectName, and the like. The Soundex algorithm compensates for any spelling mistakes that the user makesthis algorithm gives a very high similarity to two words which are spelt
differently but pronounced similarly.
 Synonym detection. We used WordNet [3] in order to rank the
synonyms of a POC higher. For example, if a question is What
is the highest peak in the US?, although there is no mention of
US in the ontology, WordNet would list The States as a synonym
for US. This would match with the geo:State in the ontology
and therefore, this option would be ranked very high.
When ambiguous OCs and all POCs have been resolved, the
query is interpreted as a set of OCs. At this point, there is enough
information to identify the answer type. Unlike other approaches
which start by identifying the question type, followed by the
identification of the answer type, our approach interprets the
majority of the question before it identifies the answer type. The
reason for this is that our approach does not require a strict

adherence to syntax, and it heavily relies on ontology-based lookup
and the definitions in the RDF structure. Hence, it can only identify
the answer type after all relevant mappings and disambiguations
are performed. Note however, that there are cases when the
answer type is identified before the whole question is interpreted,
and in this case it is used to drive the remaining mappings, if any
(as described above in Section 4.1).

4.5. Combining Ontology Concepts into triples and generating SPARQL

The list of Ontology Concepts was prepared to conform to the
structure that was suitable for generating triples. As the triples are
in a form

SUBJECT
CLASS/INSTANCE - PROPERTY
LITERAL

- PREDICATE - OBJECT

- CLASS/INSTANCE/

we first inserted any potential joker elements in between OCs, if
necessary. Jokers are wildcards or variables used instead of classes,
instances, literals or properties to generate query interpretations
in a triple format. At the time of generating these interpretations
it was not known what kind of elements could be expected, and
hence jokers were used. The rules for inserting joker elements are
as follows:
 If the first or the last element is a property, then we add a Joker
element at the beginning or at the end of the list, respectively;
a joker here is a variable representing a class, an instance, or a
datatype property value (literal).
 If any two classes, instances, or datatype property values in the
list of OCs are next to each other, we insert the Joker element
representing a property between them.
 If any two properties in the list of OCs are next to each other,
insert a Joker element representing a class/datatype property
value between them.

For example, if the first two OCs derived from a question are
referring to a property and a class respectively, one joker class
would be added before them. For instance, the query What is
the highest point of the state bordering Mississippi? would be
translated into the following list of OCs:

isHighestPointOf State border
PROPERTY

CLASS PROPERTY

mississippi
INSTANCE

These elements are transformed into the following:

JOKER PROPERTY1

isHighestPointOf State border

mississippi
CLASS1 PROPERTY2 INSTANCE

The next step is generating a set of triples from OCs, taking into
account the domain and the range of the properties. For example,
from the previous list, two triples would be generated16:

? - geo:isHighestPointOf - geo:State;
geo:State - geo:borders - geo:mississippi
(geo:State);

The last step is generating the SPARQL query. Sets of triples are
combined and based on the OC type, relevant parts are added to
the SELECT and WHERE clauses. Following the previous example,
the SPARQL query would look like the following:

14 http://sourceforge.net/projects/simmetrics/.
15 http://en.wikipedia/wiki/Soundex.

16 Note that if geo:isHighestPointOf had geo:State as a domain, the triple
would look like:geo:State -- geo:isHighestPointOf -- ?;.

as well. The first dialogue in Figs. 13 and 14 is a disambiguation
dialogue, whereas the second one is a mapping dialogue.

While clarification dialogues give full control to the user when
mapping a NL into the formal query language to formulate the
answer, they can also be seen as a cognitive overhead. Therefore,
we enhance them through a learning mechanism that is expected
to reduce this overhead over time, which increases system
performance as well as habitability for end-users.

4.6. Learning

Supervised learning requires a set of questions with the
right answers in order to achieve satisfactory performance.
Unfortunately, as noted by Belew [24], there are many situations
where we do not know the correct answers. In supervised learning
every aspect of the learners actions can be contrasted with
corresponding features of the correct action. On the other hand,
semi-supervised approaches such as Reinforcement Learning (RL)
aggregate all these features into a single measure of performance.
Therefore, reinforcement seems to be much better for users as
there is less cognitive overhead.

We decided to use a semi-supervised approach for several
reasons. Firstly, supervised learning goes in line with the automatic
classification of the question, where each question is usually
identified as belonging to one predefined category. Our intention
is to avoid this automatic classification and allow users freedom
to enter queries of any form. Secondly, we want to minimise the
manual work required when mapping some parts of the query
to the underlying structure. For example, we want the system to
suggest that Where should be mapped to a specific part of the
ontology concept such as Location, rather than the application
developer browsing the ontology structure in order to place this
mapping.

Our learning algorithm is inspired by a pure delayed reward
reinforcement function [25], which is defined to be zero after the
user selects the option, except when an action results in a win
(satisfying answer) or a loss (wrong answer or no answer), in
which case there would be a +1 reinforcement for a win, and a
1 reinforcement for a loss.
We initialise the value function based on string similarity and
synonym detection, as described in Section 4.4. When the user
changes the selection (selects an option other than the first one
suggested by the system), the system will learn that the previous
rankings were not correct, and will recalculate its value function.
We assume that the action selected by the user is the one which
is desired, and therefore we give a reinforcement of +1 to such
an action, while we give 1 to all the others. Therefore, if the
initial ranking was wrong, there is a strong chance that this is
corrected after only one user choosing the right option, due to
the fact that the initial ranking is in the range from 0 to 1. For
example, if the question was How many people live in Florida?
the closest OC to the POC people is geo:florida, which is a
state. Our ranking mechanism would place the correct suggestion
(geo:statePopulation) in the 14th place. This is because
there is no significant similarity between people and state
population, at least according to our initial ranking algorithm.

Fig. 15 shows the values of the initial states, the reinforcement
received after the user selected geo:statePopulation, and
finally the rankings after recalculation.18

4.6.1. Generalisation of the learning model

We use the ontology as a source for designing a generic
learning model. When an OC is related to another concept with

Fig. 12. Validation of potential ontology concepts through the user interaction.

prefix rdf:<http://www.w3.org/1999/02/22-rdf-
syntax- ns#>
prefix geo: <http://www.mooney.net/geo#>
select ?firstJoker ?p0 ?c1
where { {?firstJoker ?p0 ?c1 .

?i3

?p2

filter (?p0=geo:isHighestPointOf) .

?c1 rdf:type geo:State .
?c1 ?p2 ?i3 .
filter (?p2=geo:borders) .
?i3 rdf:type geo:State .
filter (?i3=geo:mississippi) .

An Example. Fig. 12 shows the syntax tree for the query What
is the population of New York?. As New York is identified as
referring to both geo:State and geo:City, we first asked the
user to disambiguate (see Fig. 12(a)). If they selected, for example,
geo:City, we start iterating through the list of remaining POCs.
The next one(population) is used together with the closest
OC geo:City, to generate suggestions for the mapping dialogue.
Among them there will be geo:cityPopulation and after the
user selects this from the list of available options, population
is mapped to the datatype property geo:cityPopulation (see
Fig. 12(b)). Note that if the user selected that New York refers
to geo:State, the suggestions would be different, and following
the users selection,
population would be mapped to refer
to geo:statePopulation, because the closest OC would be
geo:State.

An example of the generated suggestions for the same query is
shown in Fig. 13. The suggestions are made based on geo:City
(city), which is the closest OC. If the user selected geo:State
(state), the list of suggestions would contain different options
starting with geo:statePopulation (state population)
(see Fig. 14). We can see the difference in the generated suggestions
in the cases when the user selects that New York means the
city, and the state, respectively.17 The following answer differs

17 Note that the system can also work in the automatic mode where it would
simulate user selection of the best ranked options, without the need to engage the
user into a dialogue. This is discussed later in Section 4.7.

18 For the sake of clarity, we only show a subset of the generated suggestions in
Fig. 15.

D. Damljanovic et al. / Web Semantics: Science, Services and Agents on the World Wide Web 19 (2013) 121

Fig. 13. Generated suggestions and the result for city population of the New York city.

Fig. 14. Generated suggestions and the result for state population of New York state.

(a) Initial ranking.

(b) Reinforcement based on the user selecting
geo:statePopulation.

(c) Ranking after the user selects
geo:statePopulation.

Fig. 15. Mapping How many people to geo:statePopulation in the ontology.

a subClassOf relation, that concept is used to learn the model.
For example, if the features are extracted for the OC of type
classgeo:Capital, the same features would be applicable for
the OC geo:City, because geo:Capital rdfs:subClassOf
geo:City.

In addition, we do not update our learning model per question,
but per combination of a POC and the closest OC. We also
preserve a function over the selected suggestion such as minimum,
maximum, or sum (applicable to datatype property values). In this
way, we extract several learning rules from one single question,
so that if the same combination of POC and OC appears in another

question, we can reuse it. Table 5 shows several sample questions
and its derived features, which are used to learn the model.

4.7. Combining clarification dialogues with learning through modes

The role of learning is to improve the ranking of the suggestions
shown to the user so that after sufficient training the system
can automatically generate the answer by selecting the best
ranked options. In addition, while the intention behind clarification
dialogues is to control the process of mapping a NL question, the

Table 5
Features used for learning the model.

Correct rank

Context

geo:City

Function

What is the smallest city in the US?
Smallest
What is the population of Tempe, Arizona?
Population

What is the population of the capital of the smallest state?
Population
Smallest
What state has the smallest population?
Population
Smallest

geo:State
geo:statePopulation

geo:Capital
geo:State

geo:City


min


min

min

geo:cityPopulation

geo:cityPopulation

geo:cityPopulation
geo:statePopulation

geo:statePopulation
geo:statePopulation

approach is applicable to any ranking mechanism. Our assumption
is that no ranking will be perfect and therefore should be
corrected by involving the user, thus improving the performance
of the system.

An interesting question is how to decide whether or not the
existing ranking is good. When using a new dataset the best option
is to use clarification dialogues as much as possible in order to check
the system interpretations and correct them if necessary. In that
regard, there are several modes that can be used:
 Automatic mode: the system will generate the answer by
simulating the selection of the best ranked option. This mode
is used when confidence is high that the ranking is effective, or
that the system has been trained enough.
 Force dialogue mode: the system will generate a clarification
dialogue for each attempt to map a question term or phrase into
an ontology concept. This mode operates on two levels:
1. Ignoring the systems attempt to perform the mapping by
adding a None element. As previously discussed, this element is used to ignore the systems attempt to map a question term to an OC.

2. Extending the disambiguation dialogue: This option extends
the disambiguation dialogue by adding more suggestions,
in addition to the OCs identified through Ontology-based
Lookup. This option is important to be used when the knowledge base has a large number of names (e.g., MusicBrainz)
so that any question would be a rich set of Ontology Con-
cepts, while the underlying grammar would be somewhat
ignored. For example, in the question Which members of
the Beatles are dead?, due to a huge number of string literals dead appearing in the ontology, this element would
be annotated to refer to several OCs (such as instances of
rdf:type mm:Album) while in this context it needs to be
mapped to the property endDate.

4.8. Evaluation

While we tested feedback and its effect on habitability in the
user-centric study (Section 3), we tested clarification dialogues
in a controlled experiment on the well-known and widely used
Mooney GeoQuery dataset.19 This is because it is important to use
feedback in order to communicate the right message to the end-
user. The supported language does not change with feedback as it
is only used to show the systems interpretations of the query to the
user and lets them decide whether the answer is correct, or a query
reformulation is required. In contrast, clarification dialogues, as
described in this section, are used to improve habitability domains
of the supported language. This improvement can be measured by

Fig. 16. The distribution of the number of dialogues for 202 correctly answered
questions.

comparing the performance of our system against other similar
systems, that have used the same ontology and the same set
of questions in their evaluation. Another important aspect of
clarification dialogues is that they are combined with a learning
model and hence in our evaluation we also look at how the learning
mechanism improves the systems performance over time.

We used 250 questions from the Mooney GeoQuery dataset.
Although the ontology contains a relatively small portion of
knowledge about the geography of the United States, the questions
are quite complex and the system must have a good understanding
of their semantic meaning in order to correctly answer them.

We evaluated the correctness of the overall approach, as well as

the learning and ranking algorithms.

4.8.1. Correctness

We report correctness of the overall approach in terms of
precision and recall, which are measures adapted from Information
Retrieval. Precision measures the number of questions correctly
answered then divided by the number of questions for which
some answer is returned [26,27]. Recall is defined as the number
of correctly produced answers, divided by the total number of
questions.

Recall and precision values are equal, reaching 94.4%. This is
due to the fact that our approach always attempts to generate a
dialogue, and return an answer, although partial or incorrect. 34
questions were answered correctly without requiring any dialogue
with the user, while the remaining 202 required at most four
dialogues in order to correctly return the answer (see Fig. 16).
The system failed to answer 14 questions (5.6%), five out of which
are not supported by the system as they are outside the syntactic
and functional domain of the system, e.g. negation or comparison
Which states have points higher than the highest point in
Colorado?. The remaining nine were interpreted incorrectly.

Although FREyA required quite a significant input from the user,
its performance compares favourably to other similar systems.
PANTO [28] is a similar system which was evaluated on the Mooney
geography dataset of 877 questions (they removed duplicates from
the original set of 879). They reported precision and recall of 88.05%
and 85.86% respectively. NLP-Reduce [29] was evaluated with
the original dataset, reporting 70.7% precision and 76.4% recall.
Kaufmann et al. [18] selected 215 questions which syntactically
represented the original set of 879 queries. They reported the
evaluation results over this subset for their system Querix with
86.08% precision and 87.11% recall. Our 250 questions were those
released for public from the original source and syntactically
represent the original dataset.20

19 The ontology and the questions can be downloaded from: http://www.ifi.uzh.
ch/ddis/research/talking-to-the-semantic-web/owl-test-data/.

20 see http://www.cs.utexas.edu/users/ml/nldata/geoquery.html.

D. Damljanovic et al. / Web Semantics: Science, Services and Agents on the World Wide Web 19 (2013) 121

In order to test the statistical significance of our results, we
calculated the 95% confidence interval for precision and recall. As
we only have one test set, we used the bootstrapping sampling
technique, which is used in the CoNLL-03 competition (see [30] and
also [31]).

The 95% confidence interval with 1000 samplings ranges from
91.6% to 97.2%. As the lower range is still higher than the best
previously evaluated system (88.05% for recall of PANTO [28], and
87.11% precision of Querix [18]), we conclude that precision and
recall values obtained with our approach were significantly better
(p = 0.05) than the precision and recall of other systems trialed
with the same dataset. It should be noted, however, that this
high performance of our system engaged the user into dialogues.
Querix also relies on dialogues, while PANTO answers questions
automatically.

What makes our approach outstanding is the possibility to put
the user in control through the dialogue, in order to improve
the performance incrementally with each users new question, by
boosting the rankings through learning from the users choices.
In the next section, we describe the evaluation of our learning
mechanism and its effect on performance.

4.8.2. Learning

We evaluate our learning algorithm using cross-validation on
202 questions, which are a subset of the above 250those that
were answered correctly and required a dialogue.

Cross-validation is a statistical method used to evaluate and
compare learning algorithms by dividing the data into two
segments: one used to train a model and the other used to test
it. In typical cross-validation, the training and validation sets must
cross-over in successive rounds such that each data point is tested
against [32]. The basic form of cross-validation is k-fold cross-
validation, where the data is first partitioned into k equally (or
nearly equally) sized folds. Subsequently k iterations of training
and validation are performed such that within each iteration a
different fold of the data is held-out for testing while the remaining
(k  1) folds are used for training.
We have performed 10-fold evaluation using the subset of the
Mooney GeoQuery questions, which were answered correctly in
the earlier evaluation (Section 4.8.1):
 5 questions were not supported by the system, and they have
been removed due to no possibility of mapping them to the
relevant ontology concepts and formulating the correct answer,
 8 questions were misinterpreted by the system,
 35 could be answered automatically so they were removed.
This resulted in 202 questions requiring 343 dialogues in total.
In 10 iterations, 181/182 questions were used for training the
model, while the remaining 21/20 were used for testing it. Before
executing the test, we generated a gold standard in two steps:
 We ran FREyA in the automatic mode where for any required
dialogue the system chose the first available option, saved the
learning items and carried forward to the next question. The
results were saved and used as a baseline.
 We then manually examined the output and corrected invalid
entries; if we had to change the entries we marked those as
incorrect.
The goal of this evaluation was to test whether the learning
algorithm can improve the performance of the system. In order to
assess this, we compare the precision of the trained system with
the performance of the baseline. The results are shown in Table 6
and also in Fig. 17.

The average precision for the system trained with 9/10 of
questions was 0.48, which is 0.23 higher than the baseline.
While this is a good improvement over the baseline model, the
performance is not outstanding. Looking into the questions, which
could not be answered using our trained system, the reasons are:

Table 6
Precision for the questions evaluated using 10-fold cross-validation.

Fold
Baseline
Learning

0.15 0.2

0.65 0.4

0.65 0.4

0.24 0.55 0.5 0.6

0.25 0.24 0.3

Avg
0.3 0.35 0.15 0.19 0.25
0.35 0.48 0.48

Fig. 17. Precision for the learned vs. baseline using 10-fold cross-validation.
 Ambiguity. 30 questions were incorrectly answered due to
ambiguity. The advantage of our learning model is its simplicity:
it is based on a very few features, which ensures that questions
with similar word pairs benefit from the training with similar
and not necessarily the same questions. However, this is, at
the same time, a drawback as it can introduce ambiguities. For
example, if the system learns from What is the highest point
of Nebraska? the point refers to geo:HiPoint, whenever
it appears in the context of geo:Country, then, for similar
albeit drastically different questions, the system would use the
knowledge which might be wrong. For the question What
point is the lowest in California? the system would find
the previously learned mapping and it will associate point
with geo:HiPoint. The correct mapping is, however, the
geo:LoPoint. This indicates that we should extend the context
of our learning model to consider the whole phrase in which an
unknown term appears, so that for the above example whenever
point appears in the context of geo:Country
 AND highest, map it to geo:HiPoint.
 AND lowest, map it to geo:LoPoint.
 Sparsity. 65 questions contained a learning item, which was
seen only once across all questions.
While the performance of the baseline is quite low, we should
note here that this figure does not take into consideration the cases
when an unknown or ambiguous term can be mapped to more
than one ontology concept. In addition, the question is marked as
correct if all dialogues had the correct ranking placed as number 1.
However, for some cases it is very difficult to automatically judge
which suggestion to place as number one. It is very likely that
different users would select different suggestions for questions
phrased in the same way. This emphasises the importance of the
dialogues when modelling NLI systems. To assess this, we evaluate
suggestion ranking in isolation.

4.8.3. Ranked suggestions

We use Mean Reciprocal Rank (MRR) to report the performance
of our ranking algorithm. MRR is a statistic for evaluating any
process that produces a list of possible responses (suggestions

in our case) to a query, ordered by probability of correctness.
The reciprocal rank of a suggestion is the multiplicative inverse of
the correct rank. The mean reciprocal rank is the average of the
reciprocal ranks of results for a sample of queries (see Eq. (1)).
MRR = 1
|Q|

(1)

ranki

i=1

We have manually labelled the correct ranking for suggestions
which have been generated when running FREyA on the above set
of 202 questions. This was the gold standard against which our
ranking mechanism achieved MRR of 0.76. However, the median
and mode were both 1, indicating that the majority of the rankings
were correct. Indeed, in 69.7% of the cases the correct ranking was
placed as number 1, while in 87.5% of the cases the correct ranking
was among the top 5.

From the above set of 343 dialogues, we randomly selected
103, then ran our initial ranking algorithm and compared results
against the manually labelled gold standard. MRR was 0.72. We
then grouped 103 dialogues by OC, and then randomly chose
training and evaluation sets from each group. We repeated this
twice. These two iterations are independentthey have both been
performed starting with an untrained system. Overall MRR (for
all 103 dialogues) increased from 0.72 to 0.77. After training the
model with 47 items during iteration 2, overall MRR increased to
0.79. Average MRR after running these two experiments was 0.78,
which shows the increase of 0.06 in comparison to MRR of the
initial rankings. Therefore, we conclude that for the selection of
103 dialogues from the Mooney GeoQuery dataset, our learning
algorithm improved our initial ranking by 6% (further details about
the benefits of our learning mechanism can be found in [22]).

4.9. Summary and discussion

In this section we discussed how a combination of clarification
dialogues and learning can be used to improve certain aspects of the
habitability of NLIs to ontologies. As discussed earlier, the NLIs that
we are interested in are those that are portable and that support a
non-controlled and flexible query language. The flexibility of the
supported language has a trade-off related to the fact that it is not
trivial for the user to translate the information need into a question.
Hence, we examined how clarification dialogues can:
 Improve precision by asking the user to disambiguate. The system then improves the disambiguation for the next user/ques-
tion.
 Improve recall through vocabulary extension: generating a dialogue for any question term that is considered important for
understanding the semantic meaning of the question. This
question term typically does not exist in the domain vocabulary
(derived from the semantic resources and enriched from Word-
Net). The system will then learn the new term for the next user.
The vocabulary extension improves the lexical domain of
habitability, as defined in Section 2. Moreover, the lexical domain is
improved due to our ranking algorithm, which relies on Soundex
a state-of-the-art algorithm that assigns a very high similarity
to words which are spelt differently, but pronounced similarly.
Soundex is combined with the Monge Elkan string similarity
algorithm, which assigns a high similarity to words where one
is contained within other (e.g. a question term population is very
similar to the ontology lexicalisation state population according to
Monge Elkan). Combining the two algorithms gives the possibility
of going beyond the existing lexicalisations attached to semantic
resources, and understand words which are either misspelled or
expressed differently in comparison to how they are verbalised in

the semantic repository. The ranking mechanism showed a good
performance as well (see Section 4.8.3).

The functional domain of habitability is defined by algorithms
which are used to identify Potential Ontology Concepts (candidate
question terms that are identified as being important for understanding the semantic meaning of the question) and generate suggestions for the dialogue. These consider adding maximum,
minimum and sum function to the datatype property values, so that
adjectives which modify nouns (as in the largest city) can be
mapped to different functions applied to datatype property values and attached to classes. In the largest city example, this
means that, once city is mapped to the class geo:City, the dialogue attempting to map largest will model suggestions by looking at the defined properties for City, and if any of them is a
datatype property of type number, it will add the additional functions so that it is possible to map largest to maximum value of
geo:cityPopulation.

Our approach of combining clarification dialogues with learning
is evaluated on the GeoQuery Mooney dataset, to enable us to
compare our results against other NLI systems, which use different
approaches. The overall precision and recall with this dataset
reached 94.4%, which is significantly better than other similar
systems evaluated on the same dataset.

We also evaluated the individual algorithms. MRR for the initial
ranking, using 250 questions from the Mooney GeoQuery set,
yielded 0.76.

The learning algorithm showed an improvement over the base-

line model of 0.23.

The combination of clarification dialogues and learning is envisaged to be used in two steps which correspond to two different,
albeit easily interchangeable, modes of the system:
 The force dialogue mode is used to train the system towards a
reasonable performance. The system will generate a dialogue
for any attempt to map a question term into an Ontology
Concept, when its confidence to automatically resolve this
mapping is below 100%.
 The automatic mode: the system will return the answer automatically by simulating selection of the best ranked options.
Note that for true ambiguities the automatic mode might not be
the best choice even in a perfectly trained system. For instance, if
somebody asks How big is New York state? we might be unable
to decide whether How big refers to state area or state
population automatically. In this situation, as the system learns
from the users selections, the automatic mode would work in
favour of the majority of users. However, if the majority of users
refer to state area when mentioning size, the minority still have
a chance to get the correct answer by switching to the force dialogue
mode and mapping big to state population.

Upon initial inspection, the two types of modes described above
seem like a perfect match for the two types of users of an NLI:
ideally application developers can use an NLI in the force dialogue
mode until they are satisfied with the systems interpretations
of the questions. At that point, the end-users can take over the
system and use it in the automatic mode to ask questions. However,
the real scenario might be completely different. The mode can
be changed easily so if the user discovers non-satisfying results
in the automatic mode, they can immediately switch to the force
dialogue mode in order to investigate the mappings. Their input will
then improve the system for the next user/subsequent questions
of the same user. The easy switching between modes makes our
approach suitable to be used by both end-users and application
developers. In fact, the border between the customisation of the
system performed by application developers, and the customised
version of the system used by the end-users is not strict. Hence,
the role of the two types of users is, to some extent, blurred, which

D. Damljanovic et al. / Web Semantics: Science, Services and Agents on the World Wide Web 19 (2013) 121

allows end-users to control the answers to their questions or to,
at least, understand how Natural Language queries are mapped
to the formal queries. This leaves us with the same question that
we asked in the previous section about end-users. Who are they?
For the current state of the methods and algorithms, the endusers probably do not need to know about semantic technologies
if the system works within a narrow domain, such as the Mooney
GeoQuery ontology. As soon as we move towards large scale data
(such as parts of the Linked Open Data cloud e.g. DBPedia), and
datasets which are characterised by a large amount of redundant,
duplicate, and often false data, our approach becomes suitable to
be used by semantic web experts who can explore the available
knowledge by asking questions and being engaged into dialogues
(see [33] for details on how our approach is evaluated with
DBPedia). The force dialogue mode applied to the low quality data
can be used not only to get familiarised with the dataset, but also to
discover existing inconsistencies. It is left for future work to further
develop and test mechanisms that will use our system in this kind
of scenario, and also in the scenarios where these large knowledge
bases are queried by end-users who are not familiar with semantic
technologies at all.

5. Related work

While research has been active in testing usability of various
semantic search interfaces (see [29,34]), little work has been done
in the area of testing the usability of NLIs to ontologies themselves.
There are evaluation campaigns of SEALS project21 [35], which
partially address this problem, however there is a little emphasis
on testing individual usability enhancement methods and their
effect on habitability as well as the overall performance and
usability of NLIs to ontologies. While these methods have been
extensively researched in Information Retrieval for example, the
challenge with NLIs to ontologies is the underlying structure of the
knowledge, which is considered complex for casual users. In this
paper, we fill that gap by designing and testing feedback in NLIs
to ontologies, in order to emphasise their importance in building
habitable NLIs which can be considered a user-friendly way of
bringing the structured information in the form of ontologies to
the casual users.

Building habitable NLIs for querying ontologies is a difficult
task, and many different systems have been developed in recent years. While NLI systems with a good performance require
customisation through specific software (such as in the case of
ORAKEL [36]), several systems have been developed for which customisation is not mandatory (e.g., PANTO [28], Querix [29], AquaLog [19]), NLP-Reduce [29], QuestIO [15]. However, as reported
in [19] the customisation usually improves recall and hence helps
in improving the specific habitability domains. For example, as
customisation usually requires mapping WH-phrases to Ontology
Concepts, it can improve the syntactic domain of habitability, while
in the case of mapping verbs to Ontology Concepts this can improve
the lexical domain. However, performing customisation manually
(as in AquaLog or PANTO) when querying large ontologies might
be impractical if not impossible as it implies that the application developers who customise the system also know the ontology
structure very well. Our approach does not require any mandatory
customisation, however, the specific habitability domains are improved by making the process of mapping an NL to Ontology Concepts transparent to the user. We do this by combining clarification
dialogues with learning. The role of learning is to improve the initial
ranking that exists in the dialogues, which removes the cognitive
overhead for the users.

21 http://www.seals-project.eu/.

The majority of existing NLIs to ontologies are portable in a
sense that all that is required to port the system to work with
a new domain is the ontology URIthe system automatically
generates the domain lexicon by reading and processing ontology
lexicalisations. Indeed, most of the mentioned systems rely on
the ontology lexicalisations and WordNet [3]. AquaLog [19] and
PowerAqua [37] are capable of learning the users jargon in order
to improve the lexical domain of habitability and hence the user
experience. Their learning mechanism is good in a way that it uses
ontology reasoning to learn more generic patterns, which can then
be reused for the questions with similar context. However, the
clarification dialogues in AquaLog are used for resolving ambiguities
only, and also learning from the user jargon applies only to
ontology relations. Our approach is more generic as our definition
of clarification dialogues is wider, and also we model context
differently.

Querix [18] is another ontology-based question answering
system which relies on clarification dialogues in case of ambiguities,
but in comparison to AquaLog it does not implement the learning
mechanism and hence its lexical domain is bound to the ontology
lexicalisations enriched by synonyms from WordNet.

Our approach of combining clarification dialogues with the
learning mechanism is different in that it shares the input from all
users. This is influenced by the recent emergence of social net-
works, which have shown the advantages of collaborative intel-
ligence. In addition, the role of clarification dialogues in our case
is not only to resolve ambiguities but rather to control the whole
process of mapping an NL question (including WH-phrases) to the
formal query, hence allowing the user to define or change the specific lexical, syntactic or functional habitability domain.

Moreover, while other existing approaches start by generating linguistic triples from a question (even if in an iterative
fashion) and then attempting to generate ontology triples in
a form of Subject-Predicate-Object, our approach operates on a pair of Ontology Concepts at one time, which can
be Subject-predicate or predicate-Object or Subject-
Object. In that sense our approach is more flexible as it operates
on a unit smaller than a triple, where each unit can be validated or
changed through the clarification dialogue.

6. Conclusion

The NLIs to ontologies that we discuss in this paper are those
that are portable and also, those with a flexible supported language
so that not only grammatically correct questions, but also question
fragments and ill-formed questions are supported. In particular,
we discussed the application of feedback and clarification dialogues
and how they can affect the habitability of such Natural Language
Interfaces to Ontologies.

First, we looked at the effect of modelling feedback by showing
users the systems interpretations of the query. The method was
tested with users and our results reveal that feedback can increase
habitability and thus usability of an NLI system. More specifically,
it can improve the effectiveness of the system, while it does not
significantly improve efficiency. In addition, feedback has a positive
effect on the users perception of the difficulty of the supported
language.

Next, we examined how the existing habitability domains of
the language can be extended and improved through dialogues.
Here we are not concerned with showing the user previously
generated query interpretations (as in feedback), but rather with
involving the user in the process of generating the correct
query interpretation through clarification dialogues. To reduce
the cognitive overhead, clarification dialogues are coupled with a
learning mechanism, so that the users input is used to improve
the system through training. This method is tested in a controlled

evaluation using the Mooney GeoQuery dataset,
in order to
make comparisons against other similar approaches. Our approach
obtained very high precision and recall, outperforming other state-
of-the-art systems. While the reason for such a good performance
is partially in its subsequent modules, such as the learning and
ranking algorithms, the most important aspect of our approach is
adding the users into loop, allowing them to control the output and
supervise the querying process through dialogue. The question of
whether this level of involvement is acceptable from an end-users
point of view is a subject of our future work.

Acknowledgements

We would like to thank Abraham Bernstein and Esther Kaufmann from the University of Zurich, for sharing with us the Mooney
dataset in OWL format and J. Mooney from the University of Texas
for making this dataset publicly available. Grateful acknowledgement for proofreading and correcting the English edition go to Amy
Walkers from Kuato Studios.

This research has been partially supported by the EU-funded

TAO (FP6-026460) and LarKC (FP7-215535) projects.
