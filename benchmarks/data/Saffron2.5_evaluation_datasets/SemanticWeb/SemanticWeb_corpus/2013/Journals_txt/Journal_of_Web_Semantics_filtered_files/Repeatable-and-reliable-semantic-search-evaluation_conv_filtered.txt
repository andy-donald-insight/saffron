Web Semantics: Science, Services and Agents on the World Wide Web 21 (2013) 1429

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

journal homepage: www.elsevier.com/locate/websem

Repeatable and reliable semantic search evaluation
Roi Blanco b, Harry Halpin c, Daniel M. Herzig a,, Peter Mika b, Jeffrey Pound d,
Henry S. Thompson c, Thanh Tran a
a Institute AIFB, Karlsruhe Institute of Technology, Karlsruhe, Germany
b Yahoo! Research, Barcelona, Spain
c University of Edinburgh, Edinburgh, UK
d David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Canada

a r t i c l e

i n f o

a b s t r a c t

An increasing amount of structured data on the Web has attracted industry attention and renewed research interest in what is collectively referred to as semantic search. These solutions exploit the explicit semantics captured in structured data such as RDF for enhancing document representation and retrieval, or
for finding answers by directly searching over the data. These data have been used for different tasks and
a wide range of corresponding semantic search solutions have been proposed in the past. However, it has
been widely recognized that a standardized setting to evaluate and analyze the current state-of-the-art in
semantic search is needed to monitor and stimulate further progress in the field. In this paper, we present
an evaluation framework for semantic search, analyze the framework with regard to repeatability and reli-
ability, and report on our experiences on applying it in the Semantic Search Challenge 2010 and 2011.

 2013 Elsevier B.V. All rights reserved.

Article history:
Received 12 March 2012
Received in revised form
27 April 2013
Accepted 20 May 2013
Available online 27 May 2013

Keywords:
Semantic search evaluation
Semantic search
Structured data
Web data

Web search

1. Introduction

There exist a wide range of semantic search solutions targeting
different tasksfrom using semantics captured in structured
data for enhancing document representation (and document
retrieval [14]) to processing keyword search queries and natural
language questions directly over structured data (data retrieval
[57]).

In general, the term semantic search is highly contested, primarily because of the perpetual and endemic ambiguity around the
term semantics. While search is understood to be some form of
information retrieval, semantics typically refers to the interpretation of some syntactic structure to another structure, the semantic
structure, that more explicitly defines the meaning that is implicit
in the surface syntax. Already in the early days of information retrieval (IR) research, thesauri capturing senses of words in the form
of concepts and their relationships were used [8]. More recently,
the large and increasing amount of structured data that are embedded in Web pages or available as publicly accessible datasets

 Corresponding author. Tel.: +49 721 608 46108.

E-mail addresses: roi@yahoo-inc.com (R. Blanco), H.Halpin@ed.ac.uk

(H. Halpin), herzig@kit.edu, daniel@herzig.it (D.M. Herzig), pmika@yahoo-inc.com
(P. Mika), jpound@cs.uwaterloo.ca (J. Pound), ht@inf.ed.ac.uk (H.S. Thompson),
ducthanh.tran@kit.edu (T. Tran).

1570-8268/$  see front matter  2013 Elsevier B.V. All rights reserved.
http://dx.doi.org/10.1016/j.websem.2013.05.005

constitute another popular type of semantic structure. The advantage here is that these data are commonly represented in RDF
(Resource Description Framework), a standard knowledge representation formalism recommended by the W3C. RDF is a flexible graph-structured model that can capture the semantics
embodied in information networks, social networks as well as
(semi-)structured data in databases. Data represented in RDF is
composed of subjectpredicateobject triples, where the subject is
an identifier for a resource (e.g. a real-world object), the predicate
an identifier for a relationship, and the object is either an identifier
of another resource or some information given as a concrete value
(e.g. a string or data-typed value). As opposed to the wide range
of proprietary models that have been used to capture semantics in
the past, RDF provides a standardized vehicle for representation,
exchange and usage, resulting in a large and increasing amount
of publicly and Web-accessible data that can be used for search
(e.g. Linked Data).

The explicit semantics captured by these structures have been
used by semantic search systems for different tasks (e.g. document and data retrieval). More specifically, it can be used for enhancing the representation of the information needs (queries) and
resources (documents, objects). While this helps in dealing with
the core task of search, i.e., matching information needs against
resources, it has been shown that semantics can be beneficial
throughout the broader search process [9], from the specification

of the needs in terms of queries to matching queries against resources and ranking results, to refining the information needs and
up to the presentation and analysis of results.

While there is active research in this field of semantic search,
it has been concluded in plenary discussions at the Semantic
Search 2009 workshop that the lack of standardized evaluation has
become a serious bottleneck to further progress in this field. One
of the principle reasons for the lack of a standardized evaluation
campaign is the cost of creating a new and realistically sized goldstandard dataset and conducting annual evaluation campaign was
considered too high by the community.

In response to this conclusion, we elaborate on an approach for
semantic search evaluation that is based on crowdsourcing. In this
work we show that crowdsourcing-based evaluation is not only affordable but in particular, it satisfies the criteria of reliability and
repeatability that are essential for a standardized evaluation frame-
work. We organized public evaluation campaigns in the last two
years at the SemSearch workshops and tested the proposed evaluation framework. While the main ideas behind our crowdsourcingbased evaluation may be extended and generalized to the general
case (i.e., other search tasks), the kind of semantic search we have
focused on in the last two campaigns were the keyword search
over structured data in RDF. We were motivated by the increasing
need to locate particular information quickly and effectively and in
a way that is accessible to non-expert users. In particular, the semantic search task of interest is similar to the classic ad-hoc document retrieval (ADR) retrieval task, where the goal is to retrieve a
ranked list of (text) documents from a fixed corpus in response to
free-form keyword queries. In accordance with ADR, we define the
semantic search task of ad-hoc object retrieval (AOR) [10], where
the goal is to retrieve a ranked list of objects (also referred to as resources or entities) from a collection of RDF documents in response
to free-form keyword queries. The unit of retrieval is thus individual entities and not RDF documents, and so the task differs from the
classic textual information retrieval insofar as the primary unit is
structured data rather than unstructured textual data. In particular,
we focus on the tasks of entity search, which is about one specific
named entity, and list search, which is about a set of entities.

This paper provides a comprehensive overview of work on semantic search evaluation we did in the last three years and reports
on recent progress on semantic search as observed in the evaluation campaigns in 2010 and 2011. It builds on the first work towards this direction on AOR [10], which provided an evaluation
protocol and tested a number of metrics for their stability and discriminating power. We instantiated this methodology in the sense
of creating a standard set of queries and data (Section 3) which we
execute the methodology using a crowdsourcing approach (Sec-
tion 4). A thorough study on the reliability and repeatability of the
framework has been presented in [11]. Lastly, we discuss the application of this framework and its concrete instantiation in the Semantic Search Challenge held in 2010 and 2011 (Section 5). Details
on these campaigns can be found in [12,13], respectively.

Outline. This paper is organized as follows. In Section 2 we discuss different directions of related work. In Section 3, we present
the evaluation framework, discuss its details and the underlying
methodology. How the evaluation framework can be instantiated
is detailed in Section 4, where we also examine its reliability and
repeatability. In Section 5, we report on two evaluation campaigns,
the Semantic Search Challenge, held in 2010 and 2011 and show
the applicability of our evaluation framework in the real-world. Fi-
nally, we conclude in Section 6.

2. Related work

We discuss related work from the perspectives of crowd-
sourcing-based evaluation, semantic search evaluation and search
evaluation campaigns.

2.1. Crowdsourcing-based evaluation

The main difference in using crowdsourcing to gold standard
evaluation dataset creation in campaigns like TREC [14] is that
human judges are no longer a relatively small group of professional expert judges who complete an equal-sized number of as-
sessments, but large group of non-experts who may complete
vastly differing numbers of assessments and may not actually have
the required skill-set (such as command of English) to complete
the task or be completing the task honestly. Earlier work in using crowdsourcing for information retrieval demonstrated quick
turn-around times and the ability to have a much higher number of judges than previously thought possible [15]. This has led
to a rapidly-expanding number of applications of crowdsourcing
evaluation datasets to a wide range of information retrieval tasks
such as XML-based retrieval [16]. Crowdsourcing has also been
expanded successfully to related areas, such as machine translation [17].

In this vein, our primary contribution is in demonstrating the
repeatability of crowdsourcing judgments in creating evaluation
datasets, even when entirely different sets of judges are used on
the same task over long periods of time, a necessary feature for running large-scale campaigns for novel information retrieval tasks
on an annual basis. Previous work on crowdsourcing evaluation
campaigns, such as work on replicating image labeling in ImageCLEF [18], has focused on determining the reliability of the judges
over small subsets of the original campaign, but has not tested
whether the evaluation campaign is repeatable over large time
intervals (i.e., months or years), only inspecting differences over
small amounts of time (4 days) and not comparing the judges performance over time to each other, but aggregating all judgments.
Previous work [15,18] in general has focused on comparing
crowdsourcing judgments to that of experts on the existing campaigns with well-known gold standards, not boot-strapping new
evaluation campaigns for new search tasks where there are multiple competing but unevaluated search systems, such as in semantic search. Another goal of our work is to demonstrate the use of
crowdsourcing for a large-scale evaluation campaign for a novel
search task, which in our case is ad-hoc object retrieval over RDF.
Many semantic search systems of this type, such as [57], have appeared in the past few years, but none have been evaluated against
each other except on a very small scale. Semantic search systems
are a subset of information retrieval systems, and thus it would be
natural to apply the existing IR benchmarks for their evaluation in
a large-scale campaign.

It is of course possible to use crowd-sourcing for evaluationtype tasks without paying for participation, for example as part of
a game [19] or as a side-effect of robot-blocking [20]. The short
turn-around time required for our exercise, in the context of a
competition, plus its scale, ruled this kind of approach out in this
instance.

2.2. Semantic search evaluation

Especially through the series of SemSearch workshops, we observed a strong need for a standardized evaluation framework. To
the best of our knowledge, we are the first to propose an evaluation framework and methodology as well as organizing the campaigns for participants to evaluate their semantic search systems.
There are two difficulties in applying the ad-hoc document retrieval methodology directly to semantic search and the object retrieval problem in particular, as identified in [10]. The first and
most apparent problem is that not all semantic search engines
perform document retrieval, but rather retrieve knowledge that is
already encoded in RDF, where factual answers may be found by
aggregating or linking knowledge across RDF data, e.g. [21]. This is

R. Blanco et al. / Web Semantics: Science, Services and Agents on the World Wide Web 21 (2013) 1429

a clear difference to entity search tracks such as the TREC Entity
Track [14] or the INEX Entity Ranking Track [22]. With respect to
addressing keyword retrieval on structured data, there is also the
existing work in the database literature (e.g., [23]), but this field
of research has not produced a common evaluation methodology
that we could have adapted. Second, in semantic search the unit
of retrieval and thus the way to evaluate the results is dependent
on the type of query. In turn, the types of queries supported may
vary from search engine to search engine. By reducing the broad
problem of semantic search to that of keyword-based ad-hoc object retrieval (i.e. retrieving objects given in RDF with relevant factual assertions connected as a property by a single link), we could
invite multiple systems to our campaign, as most semantic search
systems have this baseline feature. More complex query and result
processing relies upon first retrieving a baseline of relevant objects,
and so this baseline should be evaluated first.

2.3. Evaluation campaigns

The Semantic Search Challenge differs from other evaluation
campaigns on entity search. In comparison to the TREC 2010 Entity Track [24], the SemSearch Challenge searches over structured
data in RDF rather than text in unstructured web-pages and features more complex queries. Likewise, in comparison to the INEX
Entity-Ranking task [25], SemSearch focuses on RDF as opposed to
XML as a data-format, and searches for relevance over entire RDF
descriptions, not passages extracted from XML. Unlike the QALD-
1 Question Answering over Linked Data [26] task, our queries
were not composed of hand-crafted natural language questions
built around particular limited datasets such as DBPedia and MusicBrainz (i.e. RDF exports of Wikipedia and music-related infor-
mation), but of both simple and complex real-world queries from
actual query logs. The use of queries from actual Web search logs is
also a major difference between our competition and all aforementioned competitions such as TREC and INEX. Keyword search over
structured data gets also more attention in the database community [27] and an evaluation framework was recently proposed [28],
but an standardized evaluation campaign is not yet available.

3. Evaluation framework

In the Information Retrieval community the Cranfield methodology [29,30] is the de-facto standard for the performance
evaluation of IR-systems. The standardized setting for retrieval
experiments following this methodology consists of a document
collection, a set of topics and relevant assessments denoting which
documents are (not) relevant for a given topic. We adapted this
methodology to semantic search. In this section, we describe the
data collection used in our evaluation framework and the query
sets, which we developed for the Semantic Search Challenge in
2010 and 2011. How we obtained relevance assessments will be
described in detail in Section 4.

3.1. Data collection

A standard evaluation data collection should be not biased
towards any particular system or towards a specific domain, as
our goal is to evaluate a general purpose entity search over the
RDF data. Therefore, we needed a collection of documents that
would be a realistically large approximation to the amount of
RDF data available live on the Web and that contained relevant
information for the queries, while simultaneously of a size that
could be manageable by the resources of a research groups. We
chose the Billion Triples Challenge (BTC) 2009 dataset, a dataset

Table 1
Statistics on the data collection.

Billion Triple Challenge 2009 dataset
1.4 billion
RDF triples
247 GB uncompressed
Size
http://km.aifb.kit.edu/ws/dataset_semsearch2010
Download
Description
http://vmlion25.deri.ie/

created for the Semantic Web Challenge [31] in 2009. The dataset
was created by crawling data from the Web as well as combining
the indexes from several semantic web search engines. The raw
size of the data is 247 GB uncompressed and it contains 1.4B
RDF statements describing 114 million entities. The statements are
composed of quads, where a quad is a four tuple comprising the
four fields subject, predicate, object, as is standard in RDF, but also
a URI for context, which basically extends a RDF triple with a new
field giving a URI that the triples were retrieved from (i.e. hosted
on). There was only a single a modification necessary for using
this dataset for entity search evaluation which was to replace RDF
blank nodes (an existential variable in RDF) with unique identifiers
so that they can be indexed. Details of the dataset are given in
Table 1.

3.2. Real-world web queries

As the kinds of queries used by semantic search engines vary
dramatically (ranging from structured SPARQL queries to searching
directly for URI-based identifiers), it was decided to focus first
on keyword-based search. Keyword-based search is the most
commonly used query paradigm, and supported by most semantic
search engines. The type of result expected varies and thus the way
to assess relevance depend on the type of the query. For example,
a query such as plumbers in mason ohio is looking for instances
of a class of objects, while a query like parcel 104 santa clara is
looking for information for one particular object, in this case a
certain restaurant. Pound et al. [10] proposed a classification of
queries by expected result type, and for our evaluation we have
decided to focus on object-queries, i.e. queries demonstrated by
the latter example, where the user is seeking information on a
particular object. Note that for this type of queries there might be
other objects mentioned in the query other than the main object,
such as santa clara in the above case. However, it is clear that the
focus of the query is the restaurant named parcel 104, and not the
city of Santa Clara as a whole.

We were looking for a set of object-queries that would be unbiased towards any existing semantic search engine. First, although
the search engine logs of various semantic search engines were
gathered, it was determined that the kinds of queries varied quite a
lot, with many of the query logs of semantic search engines revealing idiosyncratic research tests by robots rather than real-world
queries by actual users. Since one of the claims of semantic search
is that it can help general purpose ad-hoc information retrieval
on the Semantic Web, we have decided to use queries from actual
users of hypertext Web search engines. As these queries would be
from hypertext Web search engines, they would not be biased towards any semantic search engine. We had some initial concerns if
within the scope of the dataset it would be possible to provide relevant results for each of the queries. However, this possible weakness also doubled as a strength, as the testing of a real query sample
from actual users would determine whether or not a billion triples
from the Semantic Web realistically could help answer the information needs of actual users, as opposed to purely researchers [32].

Table 2
Examples queries from the 2010 Entity Query Set.

james caldwell high school
44 magnum hunting
american embassy nairobi
city of virginia beach
laura bush
pierce county washington
university of north dakota
kaz vaporizer
david suchet
fitzgerald auto mall chambersburg pa
mst3000

3.2.1. Queryset 2010

In order to support our evaluation, Yahoo! released a new
query set as part of their WebScope program,1 called the Yahoo!
Search Query Log Tiny Sample v1.0, which contains 4500 queries
sampled from the companys United States query log from January,
2009. One limitation of this dataset is that it contains only queries
that have been posed by at least three different (not necessarily
authenticated) users, which removes some of the heterogeneity of
the log, for example in terms of spelling mistakes. While realistic,
we considered this a hard query set to solve. Given the well-known
differences between the top of the power-law distribution of
queries and the long-tail, we used an additional log of queries from
the Microsoft Live Search containing queries that were repeated by
at least 10 different users.2 We expected these queries to be easier
to answer.

We have selected a sample of 42 entity-queries from the Yahoo!
query log by classifying queries manually as described in [10]. We
have selected a sample of 50 queries from the Microsoft log. In this
case we have pre-filtered queries automatically, eliminating the
ones where no entities were found with the Edinburgh MUC named
entity recognizer [30], a gazetteer and rule-based named-entity
recognizer that has shown to have a very high precision in competi-
tions. Both sets were combined into a single, alphabetically ordered
list, so that participants were not aware which queries belonged to
which set, or in fact that there were two sets of queries. The 2010
query set is available at http://km.aifb.kit.edu/ws/semsearch10/
Files/finalqueries. Ten random queries of the set are shown in
Table 2.

3.2.2. Querysets 2011

In 2011, the Semantic Search Challenge comprised two tracks.
The Entity Search Track is identical in nature to the 2010 challenge.
However, we created a new set of queries for the entity search task
based on the Yahoo! Search Query Tiny Sample v1.0 dataset. We
selected 50 queries which name an entity explicitly and may also
provide some additional context about it, as described in [10].

In the case of the List Search Track, the second track of the 2011
challenge, we hand-picked 50 queries from the Yahoo query log as
well as from TrueKnowledge recent queries.3 The queries describe
a closed set of entities, have a relatively small number of possible
answers (less than 12) which are unlikely to change.

Although many competitions use queries generated manually
by the participants, it is unlikely that those queries are representative of the kinds of entity-based queries used on the Web. There-
fore, we manually selected queries by randomly selecting from the
query logs and then manually checked that at least one relevant
answer existed on the current Web of linked data.

1 http://webscope.sandbox.yahoo.com/.
2 This query log was used with permission from Microsoft Research and as the
result of a Microsoft Beyond Search award.
3 http://www.trueknowledge.com/recent/.

Table 3
Examples queries from the 2011 Entity Query Set (left) and 2011 List Query Set
(right).

08 toyota tundra
Hugh Downs
MADRID
New England Coffee
PINK PANTHER 2
concord steel
YMCA Tampa
ashley wagner
nokia e73
bounce city humble tx
University of York

gods who dwelt on Mount Olympus
Arab states of the Persian Gulf
astronauts who landed on the Moon
Axis powers of World War II
books of the Jewish canon
boroughs of New York City
Branches of the US military
continents in the world
standard axioms of set theory
manfred von richthofen parents
matt berry tv series

Table 3 shows examples from the query sets for both tracks. The

entire query sets are available for download.4

4. Reliability and repeatability of the evaluation framework

Advances in information retrieval have long been driven by
evaluation campaigns using standardized collections of datasets,
query workloads, and most importantly, result relevance judg-
ments. TREC (Text REtrieval Conference) [33] is a forerunner in IR
evaluations, but campaigns also take place in specialized forums
like INEX (INitiative for the Evaluation of XML Retrieval) [22] and
CLEF (Cross Language Evaluation Forum). The main premises of
these campaigns is that a limited and controlled set of human experts decide the correctness of a given set of results, which will be
used as a ground truth for evaluating the performance of different systems [33]. Early evaluation campaigns targeted relatively
narrow domains and used small collections, where evaluations using a small number of queries provided robust results. Moving to
the open domain of the Web resulted in significantly larger heterogeneity of data sources and an increase in the potential information needs (and so diverse tasks) that need to be evaluated.
Current research in campaigns (like TREC) and information retrieval evaluation in general focus primarily on the following
goals:

Repeatabilityas observed by Harter [34], there can be substantial variation among different expert judges performing the
same task. If evaluation is to drive the next generation of search
technologies, it is important to validate that relevance assignment
is a repeatable process. This fundamental requirement exacerbates
the scalability problem, because the agreement between assessors
needs to be tested not only for each new search task, but also for
each set of judges that have been employed (agreement is a measure of the extent to which judges are interchangeable). However,
outsiders who would like to validate an experiment will typically
not have access to the original judges (or those judges may not be
available or willing to repeat experiments at later times).

Reliabilitythe expert judges employed by campaigns such as
TREC [35] are expected to be sufficiently reliable to produce a
ground truth for evaluation. However, setting up new tracks for
novel search tasks is often not feasible or expedient, due to the time
and effort it takes to set up such tracks and the limited resources
of the organizers. In such cases, researchers need to set up their
own evaluation and seek replacements for experts, training others
to be judges of their work, where training is often nothing more
than providing a description of the task.

How can researchers create repeatable and reliable evaluation campaigns that scale over the number of new tasks brought
about by the Web? An increasingly popular way of evaluating

4 http://semsearch.yahoo.com/datasets.php.

R. Blanco et al. / Web Semantics: Science, Services and Agents on the World Wide Web 21 (2013) 1429

Fig. 1. A sample HIT for semantic search evaluation.

novel search tasks is the approach known as crowdsourcing. Crowdsourcing is a method of obtaining human input for a given task
by distributing that task over a large population of unidentified
human workers. In the case of building a search evaluation col-
lection, crowdsourcing means distributing relevance judgments
of pooled results over this crowd. The advantage of the crowd
is that it is always available, it is accessible to most people at
a relatively small cost, and the workforce scales elastically with
increasing evaluation demands. Further, platforms such as Amazon Mechanical Turk5 provide integrated frameworks for running
crowdsourced tasks with minimal effort. We show how crowdsourcing can help execute an evaluation campaign for a search
task that has not yet been sufficiently addressed to become part
of a large evaluation effort such as TREC: ad-hoc Web object
retrieval [10], for which we created a standard dataset and queries
for the task of object retrieval using real-world data, and the way
we employed Mechanical Turk to elicit high quality judgments
from the noise of unreliable workers in the crowd. The queries, index used, and results of the evaluation campaign are also publicly
available for use in the evaluation of web-object retrieval systems.6
There are two research questions that must be answered for
crowdsourcing to be used systematically in evaluation campaigns.
First, are evaluation campaigns with crowdsourced workers re-
peatable, such that the resulting ranking of systems is the same
for different pools of crowdsourced judges over a period of time?
Second, are crowdsourced workers reliable, such that differences
between experts and crowdsourced workers do not change the resulting ranking of the systems? As our primary contribution, we
experimentally demonstrate the repeatability of our search system
evaluation experiment using crowdsourcing. We also test the reliability of judges who are not task or topic-experts, which has been
questioned in the previous work [36], as crowdsourced workers do
not have access to the original information need and may lack specialized training or background knowledge possessed by experts.
The case of Mechanical Turk provides an extreme where the judges
are not only likely to be untrained and non-expert, but they also
sign up for payment and so have an incentive to cheat in order to

5 http://www.mturk.com.
6 http://semsearch.yahoo.com.

gain monetary reward. Therefore, we repeat our evaluation and assess whether the results from the original campaign can be reproduced after six months with a new set of crowdsourced judges, and
whether those results correspond to what we would have obtained
using a more traditional methodology employing expert judges.
We also explore the effect of different numbers of judges per result
on the quality of judgments. Finally, we analyze the robustness of
three popular information retrieval metrics under crowdsourced
judgments. The metrics studied are discounted cumulative gain
(NDCG), mean average precision (MAP), and precision at k (P@k).
To the best of our knowledge, we are the first to analyze the repeatability of crowdsourcing in a real-world evaluation campaign.

4.1. Crowdsourcing judgments

In this section, we report how we used the Amazon Mechanical Turk to assess the relevance of search results and describe the
different sets of assessments we obtained for the evaluation. Using
Mechanical Turk, tasks  called Human Intelligence Tasks (HITS)
 are presented to a pool of human judges known as workers
who do the task in return for very small payments. Amazon provides a web-based interface for the workers that keeps track of
their decisions and their payments. Because anyone can sign up
to be a worker, we had to present each result for judgment in a
way comprehensible to non-expert human judges. It was not an
option to present the data in the native syntactic format of RDF
such as RDF/XML or N-Triples, because they are too complex for
average users, especially with the use of URIs as opposed to natural
language terms for identifiers in RDF. In practice, semantic search
systems use widely varying presentations of search results, sometimes tailored to particular domains. However, the rendering of results could possibly affect the valuation given by a judge. Allowing
each participant to provide their own rendering would make it difficult to separate the measurement of ranking performance from
effects of presentation, and would also eliminate the ability to pool
results which reduces the total number of judgments needed.

For the purpose of evaluation, we have created a rendering algorithm to present the results in a concise, yet human-readable
manner without domain-dependent customizations (see Fig. 1).
First, for each subject URI, all properties and objects were re-
trieved. Then the last rightmost hierarchical component of the
property URI, often referred to as the local name, was used as the

label of the property after tokenization. For example, the property http://www.w3.org/1999/02/22-rdf-syntax-ns/type was presented to the judge simply as type. A maximum of twelve object
properties were displayed to the judge, based on previous experience that fitting the whole task on a single page improved participation rates. Preference was given to a few well-known property
types defined in the RDF and RDF Schema namespaces, followed by
custom-defined properties presented in the order retrieved from
the dataset. In order to keep the amount of information given constant across judges and facilitate timely completion of the task, the
URIs were not clickable and the judges were instructed to assess
using only the information rendered, as to make the task of ad-hoc
object retrieval directly comparable to tasks such as ad-hoc document retrieval. During the evaluation, we encountered the problem
that some of the retrieved URIs only appear as objects, resulting in
an empty display. Of the 6158 URIs, a small minority of URIs (372)
had triples only in the object position. For the current evaluation,
we have ignored these results. Workers were given three options
to judge each result: Excellentdescribes the query target specifically and exclusively, Not badmostly about the target, and
Poornot about the target, or mentions it only in passing. Note
that we used the human-friendly labels Excellent, Not bad and
Poor for relevant, somewhat relevant and irrelevant results. We
did not provide instructions to emphasize any particular properties (such as the categories in Fig. 1), leaving the judgment to
be based on the general purpose judgment combining background
knowledge about the entities and all of the displayed information.
In the following, any grade higher than Poor will be considered
as Relevant for metrics that compute performance values over
binary relevance judgments (MAP and P@10).

4.2. Quality assurance and costs of evaluation

In order to ensure quality in the presence of possible lowquality workers, each HIT consisted of 12 query-result pairs for relevance judgments. Of the 12 results, 10 were real results drawn
from the participants submissions, and 2 were gold-standard results randomly placed in the list of results. These gold-standard
results were results from queries distinct from those used by the
workers and have been manually judged earlier by an expert in
RDF and information retrieval as being obviously relevant or ir-
relevant. For each HIT, there was both a gold-standard relevant
and gold-standard irrelevant result included. These gold-standard
results enabled the detection of workers who were not properly
doing their task, as can be done by monitoring the average performance of judges on the gold-standard results hidden in their HITs.
It is a common occurrence when using paid crowdsourcing systems for bogus workers to try to game the system in order to gain
money quickly without investing effort in the task, either by using automated bots or simply answering uniformly or randomly.
Note that while we chose our gold-standards manually since we
were evaluating a new task, one could in future campaigns use results with high inter-annotator agreement as new gold standards
or apply machine learning techniques to predict spammers [37].
Amazon Mechanical Turk allows payment to be withheld at the
discretion of the creator of the HIT if they believe the task has not
been done properly.

Before publishing the final tasks, we had done small-scale experiments with varying rewards for the workers. Mason and Watts
have already determined previously that increased financial incentives increase the quantity, but not the quality, of work performed
by participants [38]. Thus our approach was to lower the payment
to workers down to the price where the speed of picking up the
published tasks was still acceptable. When our results were published via Amazon Mechanical Turk, workers were paid $0.20 per

HIT. In the first experiment reported here 65 workers in total participated in judging a total of 579 HITs or 1737 assignments (3
assignments per HIT), covering 5786 submitted results and 1158
gold-standard checks. (Note that of these only a subset of 4209
results and 842 checks is relevant here, being those which were
also evaluated in MT2 and EXP, see below). Three workers were
detected to be answering uniformly or randomly, and their work
(a total of 95 assignments) was rejected and their assignments returned to the pool for another worker to complete. Two minutes
were allotted for completing each HIT. On average the HITs were
completed in 1 min, with only two complaints that the allotted
time was too short. This means that workers could earn $6$12 an
hour by participating in the evaluation. The entire competition was
judged within 2 days, for a total cost of $347.16. We consider this
both fast and cost-effective. Given that this cost includes not only
payments to judges, but also the provision of the entire testing in-
frastructure, this compares very favorably with the likely cost of re-
cruiting, managing and paying graduate students for the same task.
To study repeatability of our evaluation campaign we have reevaluated the relevance of the search results returned by our test
systems using a second set of workers. This second experiment has
been performed six months after the initial evaluation using the
exact same procedure. In the following, we will refer to the original
set of assessments as MT1 and the repeated set of assessments as
MT2. For MT1 there were 64 judges in total. The top four judges did
131 HITs and did not differ from the experts on the gold-standard
items, with the overall percentage of mistakes over the 2176 goldstandard items in those 1088 HITs was 3.2%. For MT2 there were 69
judges in total. The top five judges did 165 HITs and did not differ
at all from experts on the gold-standard items, and the overall
percentage of mistakes with regards the 1662 gold-standard items
in those 831 HITs was 4.5%. For future campaigns items with a
high inter-annotator reliability could be used to chose more goldstandard items.

To study the reliability of our crowdsourced judgments, we also
created an expert set of relevance judgments over standard HITs
that were not gold-standard items. Unlike repeatability, reliability
concerns the ability of Mechanical Turk to reproduce a ground
truth provided by experts. In our case, the authors of this paper
have provided the ground truth by re-evaluating the same subset
used in MT2. As this is a significant effort, we have used only one
judge per HIT for re-evaluating the entire set of 4209 results, in
421 HITs of 10 results (leaving out the known-good and knownbad gold-standard check items). The resulting dataset is referred
to as EXP herein.

For all of MT1, MT2, and EXP, we report here on the exact same
set of queries and results. Some participants submitted more than
one set of results (outputs from their system in differing config-
urations), of which we used the best submission of each of the
competitor systems for testing repeatability. In total there were 6
competing systems with one submission each, which will be described in Section 5.1.1. Each result of every submission was judged
by 3 crowdsourced workers, with systems results being judged to
a depth of 10, given that it was a new unstudied task. We broke
ties by taking the majority vote, except where the three judges
each gave a different judgment, in which case we chose the middle,
Not Bad assessment. In EXP, as mentioned above, each result was
judged by a single expert, but a subset of 30 results were judged by
three experts to determine intra-expert reliability.

Although the procedure for MT2 was the same as for MT1, the
intervening six months appear to have seen a significant change
in the worker pool: monitoring worker time-to-complete and performance on the known-good and known-bad gold-standard results revealed a total of 14 bogus workers for MT2, who completed
a total of 1471 assignments between them before they were detected and blocked and their assignments returned to the pool. This

R. Blanco et al. / Web Semantics: Science, Services and Agents on the World Wide Web 21 (2013) 1429

Fig. 2. Workers ordered by decreasing number of items assessed.

change from 5% of assignments rejected in MT1 to 54% of assignments rejected in MT2 may indicate a significant increase in the
number of bogus workers, and underlines the importance of including known-good and known-bad data in every HIT.

4.3. Analysis of results

We seek to answer the following in our experiments:
 Repeatability. Are judges really interchangeable?
 Can we expect anonymous crowdsourced workers to agree

on judgments?

 Can we expect repeated experiments to produce the same
results in terms of relevance metrics and the rank-order of
the evaluated systems?

This requires also confirming previous results [16]:
 Reliability. Can crowdsourced workers reliably reproduce the
results we would have obtained if we were using expert
judges?
 Are the same items scored similarly by workers and experts?
 Can worker evaluations produce the same results in terms
of our relevance metrics and the rank-order of the evaluated
systems?

We will use as parameters both the evaluation metric, the
number of assessors per item and the relevance scale used. In
particular, we would like to find out the following:
 Which of our three evaluation metrics (MAP, NDCG, P@10) are
more robust to changing the pool of workers, and when replacing experts with workers?
 Do we obtain better results with increasing number of assessments per item?
 Do our results hold for both binary and ternary scale assess-
ment?

4.3.1. Repeatability

As previously discussed, in IR evaluation the notion of repeatability is tied to measuring the extent to which judges are inter-
changeable. The argument being that if we show judges from a
particular pool of assessors are interchangeable, the experiment
can be repeated with any subset of judges from the pool: the judges
will agree on the relevancy of items to be judged, which will be reflected in the metrics to be computed, and the eventual ranking of
the competing systems.

The most common measures of inter-annotator agreement in
IR evaluations are Cohens  for the case of two judges, and Fleisss
 for the case of multiple judges, which has a free-marginal version [18]. While we report inter-annotator agreement, we note that
the applicability of standard metrics to the case of crowdsourced
workers can be questioned. The reason is that although we have

Fig. 3. Agreement between workers.

a fixed number of workers for each HIT, in the crowdsourcing scenario the workers select the tasks, and thus they are not necessarily
the same workers who assess each item. Fig. 2 shows the number of
items judged by each worker in our first experiment with Mechanical Turk. In the case of traditional expert-based evaluation, this distribution would be flat as each expert would assess the same items.
In our case, each worker may assess a different number of the total
set of HITs. Some workers assess a large number of HITs, with the
most diligent worker going through 273 HITs, while a long tail of
workers worked on a single task only. This long tail is especially
problematic since there is much less data about these workers on
which to base reliability tests.

Based on our knowledge of the related work, it seems that there
is not yet consensus as to how to account for this deficiency [39]
and the question of reliability is sometimes ignored altogether [40].
We believe the most prudent way to proceed is to report the
distribution of Fleisss  values considering all HITs as individual
assessments of a small number of 12 items. In Fig. 3 we show
this distribution for our first and second experiment. As the Figure
shows, the level of agreement is very similar. The average and
standard deviation are 0.36  0.18 for the first experiment (MT1)
versus 0.36  0.21 for MT2. In fact, the difference between the
average agreement appears at the fourth digit, strongly supporting
the idea of a homogeneous pool of workers. We achieve slightly
higher levels of agreement for binary relevance (with somewhat
relevant and relevant judgments counted both as relevant),
0.44  0.22 and 0.47  0.25. There is thus no marked difference
between a three-point scale and a binary scale, meaning that it was
feasible to judge this task on a three-point scale.

Agreement numbers are not easy to interpret even in the context of related work, and agreement is only a proxy for a repeatable
evaluation: what we are ultimately after is whether different pools
of workers used in different experiments lead to the same results
in terms of evaluation metrics, and ultimately the same ordering of
the evaluated systems. Fig. 4 shows Mean Average Precision (MAP)
scores for the different systems using the two different evaluation
sets obtained via Mechanical Turk (MT1 and MT2). The results are
also included in Table 5. We can see that the scores are close in
value, and in fact there is no change to the rank-order of the sys-
tems. The result holds for both binary and ternary scale, and for
both MAP, P@10 and NDCG. Broadly, this confirms our hypothesis that crowdsourced ad-hoc evaluation is repeatable. The relative
change in scores across the two sets, for all systems in average, is
7.85% for MAP, 4.24% for NDCG and 6.87% for P@10. This gives us
a first indication that two systems would need to be very close in
performance in order to change places in the ranking produced by
repeated experiments.

In fact, Mechanical Turk gives surprisingly robust results with
just a single assessment per item. We have tested this by subsam-
pling, i.e. selecting randomly a single assessment for each item

Table 4
Scoring patterns in different evaluation sets.
Irrelevant

Total items

Set
MT1
MT2

Somewhat R.

Relevant

Fig. 4. Mean average precision (MAP) for the systems using different test sets.

Fig. 6. Mean average precision (MAP) for the systems using different test sets and
three workers.

Fig. 5. Mean average precision (MAP) for the systems using different test sets and
a single worker.

from the six assessments we have collected in total. We have repeated this 100 times and computed the min, max, mean and standard deviation of our metrics. Fig. 5 shows the min, max, and the
range of one standard deviation from the mean for each system,
using MAP as the metric. This figure furthermore shows that even
one standard deviation intervals provide different ranges for the
different systems and effectively separate them. Though the score
of a system in a particular sample may surpass the score of an overall inferior system, such cases would be rare. Note that there is
a particular robustness to Mechanical Turk. Though conventional
wisdom would certainly be against running an evaluation with a
possibly unreliable single judge, in the case of crowdsourcing the
assessments will come from not a single expert judge for all the
results, but multiple workers. These workers may be individually
unreliable, but each will judge a small number of items. When considering three judges, see Fig. 6, the intervals around the mean get
even tighter.

The decrease of standard deviation around the mean is also
shown in Fig. 7. This Figure shows the standard deviation on the y-
axis, for different numbers of workers (x-axis), and using different
metrics. We see that P@10 benefits the most from increasing the
number of workers and that adding more workers decreases the
standard deviation between workers.

4.3.2. Reliability

Repeatable evaluations require that each evaluation be reliable,
and while work such as Alonso et al. [16] has shown that crowdsourced judges can be reliable in information retrieval tasks, we
should show that this reliability holds over repeated experiments.
We measured the agreement between expert judges on a subset of

Fig. 7. Average standard deviation around the mean for different numbers of
workers and using different metrics.

the items (30 HITs). In this case, the average and standard deviation of Fleisss  for the two- and three-point scales are 0.57 0.18
and 0.56 0.16, respectively. The level of agreement is thus higher
for expert judges, with comparable deviation. For expert judges,
there is practically no difference between the two- and three-point
scales, meaning that expert judges had much less trouble using the
middle judgment.

Moving on to comparing expert reliability with crowdsourced
judgments from MT1 and MT2, Table 4 shows that again different
sets of workers behave very similarly, though different from the
experts on the whole. Fleisss  is similar with 0.412 between MT1
and experts, and 0.417 between MT2 and experts. In particular,
experts are more pessimistic in their scoring, marking irrelevant
many of the items that the workers would consider somewhat rel-
evant.

This effect is also visible in Fig. 8, which shows the assessments
of the two worker sets compared to the assessments of the experts
for the three assessment options. Whereas the two worker sets
display similar behavior compared to each to other, the difference
towards more positive assessments compared to the experts can be
observed. This may suggest that crowdsourced judgments cannot
replace expert evaluations. Based on the comments and the data,
the source of this effect is likely the fact that experts understood
describes the query target specifically and exclusively to be much
of a more sharp distinction about objects than workers. An expert

R. Blanco et al. / Web Semantics: Science, Services and Agents on the World Wide Web 21 (2013) 1429

Table 5
Evaluation results using different evaluation sets and metrics.

System

L3S

MT1

MT2

MT1

MT2

P@10
MT1

MT2

Fig. 8. Assessments of the two workers sets compared to the experts assessments
for the three assessment options.

Fig. 9. Average deviation of sample means from the expert assessments.

would note that the IMDB article about a movie featuring actor
David Suchet would not be considered relevant, while workers
would often judge that result as relevant if the query asked for
David Suchet.

Fig. 10. Kendalls Tau between workers and experts for different number of
assessments per item.

As in the case of repeatability, we might ask whether crowdsourced assessments become more reliable when adding more
judges. We have already shown in Fig. 7 that increasing the number of workers decreases their standard deviation and increases
the reliability of workers, and this trend seems to continue beyond 6 workers. Fig. 9 shows the deviation resulting from using
the workers assessments instead of the expert assessments, in particular the average relative change in our metrics for subsamples,
for different numbers of workers. We can see a clear benefit to using three workers instead of 1 or 2 workers, but there is comparatively less benefit from employing more than three judges. Fig. 10
shows the same for MAP and NDCG using the average values of
Kendalls  between the subsamples of worker judgments and the
expert assessments. This value of  is already very close to one for
three judges independent of the metric. While intra-worker reliability increases as the number of workers increase, adding more
than three workers will lead to a higher number of disagreements
with expert judges.

4.4. Conclusions on reliability and repeatability

With the advent of crowdsourcing platforms like Amazon
Mechanical Turk, creating a gold standard evaluation dataset of

Looking at agreement rate in other settings, such a  of 0.55 at
TREC 2005 on sentence relevance at TREC 2004 Novelty Track [41],
our experts are clearly reliable, with agreement ratings of 0.57
(binary scale) and 0.56 (ternary scale). The reliability of non-expert
crowdsourced judges of 0.36 in our experiment then appears to
be less than ideal. However, does it change the ranking of the
systems? This would be the ideal test of how far reliability has to
degrade in order to impact an evaluation campaign.

Even if the level of agreement is higher amongst expert judges,
if the ranking of the systems does not change when non-experts are
employed, then a crowdsourcing approach is still reliable enough
for the task (even if their reliability is strictly speaking relatively
lower than expert judges). The relative change in scores when
going from experts to workers (moving from EXP to three-samples
of MT1 and MT2), for all systems on average, and using three
judgments, is 1.8% for MAP, 3.5% for NDCG and 12.8% for P@10 (see
also Table 5). These are comparable changes to what we have seen
when moving from one worker set to another, but the changes are
mostly positive, with notable increases in P@10 when changing
from experts to workers. In particular, the increase in somewhat
relevant scores explains the increase of the binary P@10 measure.
Somewhat relevant results (counted as relevant for the binary
measures) that are coming in at lower ranks boost P@10 more than
MAP and NDCG, which are less sensitive to changes in the lower
ranks. While the reliability of non-expert judges is lower than
expert judges, the reliability of non-expert judges is still sufficient
for ranking systems in the evaluation.

Fig. 4 illustrates the performance values for MAP for the different systems using the two MT evaluation sets and the expert judg-
ments. The values are not only close, but in fact again the obtained
values for the experts produce the same rank-order of the systems
as with any of the MT evaluation sets.

relevance judgments for new kinds of search tasks is now cheap,
scalable, and easy to deploy. We have shown how to quickly bootstrap a repeatable evaluation campaign for a search task that has
not previously been systematically evaluated, such as the object
information retrieval task in semantic search, using Mechanical
Turk. However, are such crowdsourced evaluation campaigns
trustworthy? Are the relevance judgments of crowdsourced judges
both reliable compared to experts and can such judgments be
repeated with entirely different crowdsourced judges over time?
Regarding the repeatability of such crowdsourced judgments,
we have shown that the level of agreement is the same for two
pools of crowdsourced judges even when the evaluation is repeated after six months. Repeating an evaluation using crowdsourcing after six months led to the same result in evaluation
metrics and the rank-order of the systems being unchanged. Concerning the reliability of crowdsourced judgments, we have observed that experts in general rate more results negative than
crowdsourced judges. This is likely due to the object retrieval task
and the time pressure on workers, as experts were more adept at
discriminating between queries exclusively about an object to ones
simply mentioning an object given time limits. However, the rank
ordering of systems does not change when moving from experts
to crowdsourced workers. Three judges seems to be a sufficient
number and, surprisingly, increasing the number of crowdsourced
judges has little effect unless the systems are particularly close.
As regards evaluation metrics, P@10 is more brittle than measures
such as MAP and nDCG and so benefits most from collecting additional judgments.

We have successfully shown how a number of real-world and
research semantic search systems can be evaluated in a repeatable
and reliable manner via creating a new evaluation campaign using
crowdsourcing. While the study here has focused on agreement
between judges and workers over time and holding the items
(queries and results) constant, future research needs to study
the agreement between judges and workers on a per-item basis.
For example, how does the ambiguity of entity queries affect
reliability and repeatability? Future work should also take into
account if these results hold over different kinds of entity queries
or different kinds of tasks that vary in the levels of ambiguity.
So far, the Semantic Search evaluation campaign focused on the
case of entity search. It will be broadened to deal with new
kinds of semantic search tasks such as relational keyword search
and complex question answering featuring more expressive and
complex queries beyond keyword-based entity search queries. The
methodology demonstrated in this work should be repeated for
these new tasks because the differences in ambiguity may have
impact on the reliability and repeatability of the results.

5. Semantic Search Challenge

We applied the evaluation framework in the Semantic Search
Challenge 2010 and 2011, which were held as part of the
Semantic Search Workshop at WWW2010 and WWW2011. The
main difference between the challenges is that 2011 challenge
comprised also a List Search Track in addition to the Entity Search
Track.

5.1. Semantic Search Challenge 2010

In the following, we describe the participating systems and discuss the results of the Semantic Search Challenge 2010 as reported
in [12].
Entity Search Track. The Entity Search Track aimed to evaluate a typical search task on the web, keyword search where the keyword(s)
is generally the name of the entity. Entities are ranked according to
the degree to which they are relevant to the keyword query. This
task was part of the Semantic Search Challenge 2010 and 2011.

5.1.1. Participating systems 2010

For the evaluation campaign, each semantic search engine was
allowed to produce up to three different submissions (runs), to
allow the participants to try different parameters or features. A
submission consisted of an ordered list of URIs for each query. In
total, we received 14 different runs from six different semantic
search engines. The six participants were DERI (Digital Enterprise
Research Institute), University of Delaware (Delaware), Karlsruhe
Institute of Technology (KIT), University of Massachusetts (UMass),
L3S, and Yahoo! Research Barcelona (Yahoo! BCN).

All systems used inverted indexes for managing the data. The
differences between the systems can be characterized by two major aspects: (1) the internal model used for representing objects
and (2), the kind of retrieval model applied for matching and rank-
ing. We will now first discuss these two aspects and then discuss
the specific characteristics of the systems and their differences.

For object representation, RDF triples having the same URI as
subject have been included and that URI is used as the object iden-
tifier. Only the DERI and the L3S deviate from this representation,
as described below. More specifically, the object description comprises attribute and relation triples as well as provenance infor-
mation. While attributes are associated with literal values, relation
triples establish a connection between one object and one another.
Both the attributes and the literal values associated with them are
incorporated and stored on the index. The objects of relation triples
are in fact identifiers. Unlike literal values, they are not directly
used for matching but this additional information has been considered valuable for ranking. Provenance is a general notion that
and can include different kinds of information. For the problem of
object retrieval, participated systems used two different types of
provenances. On the one hand, RDF triples in the provided dataset
are associated with an additional context value. This value is in
fact an identifier, which captures the origin of the triples, e.g. from
where it was crawled. This provenance information is called here
the context. One the other hand, the URI of every RDF resource is
a long string, from which the domain can be extracted. This kind of
provenance information is called domain. Clearly, the domain is
different to the context because URIs with the same domain can be
used in different contexts. Systems can be distinguished along this
dimension, i.e., what specific aspects of the object they took into
account.

The retrieval model, i.e. matching and rankings [42], is clearly
related to the aspect of object representation. From the descriptions of the systems, we can derive three main types of approaches:
(1) the purely text based approach which relies on the bag-of-
words representation of objects and applies ranking that is based
on TF/IDF [43], BM25 [44], or language models [45]. This type of
approach is centered around the use of terms and particularly,
weights of terms derived from statistics computed for the text cor-
pus. (2) weighting properties separately is done by approaches that
use models like BM25F [46] to capture the structure of documents
(and objects in this case) using a list of fields or alternatively, using mixture language models, which weight certain aspects of an
object differently. Since this type of approach does not consider
objects as being flat as opposed to the text-based ones but actually decompose them according to their structure, we call them
structure-based. (3) for the last approach, the structured information is used for ranking results for a specific query, there are
also approaches that leverage the structure to derive query independent scores, e.g. using PageRank. We refer to them as queryindependent structure-based (Q-I-structured-based) approaches.
To be more precise, the three types discussed here actually capture
different aspects of a retrieval model. A concrete approach in fact
uses a combination of these aspects.

R. Blanco et al. / Web Semantics: Science, Services and Agents on the World Wide Web 21 (2013) 1429

Table 6
Feature overview regarding system internal object representation and retrieval model.

Participant

Object
representation

Retrieval
model

Run

Attribute
values
Relations
Context
(+)/domain
()
Text based
Structure-
based
Q-I-
Structure-
based

Delaware
sub28-
Okapi


sub28-
Dir


sub28-


sub27-
dpr

sub27-
dlc

sub27-
gpr

sub32


L3S
sub29


UMass
sub31-
run1


sub31-
run2


sub31-
run3


Yahoo! BCN
sub30-
RES.1


sub30-
RES.2


sub30-
RES.3

Based on the distinction introduced above, Table 6 gives an
overview of the systems and their characteristics. A brief description of each system is given below, and detailed descriptions are
available at http://km.aifb.kit.edu/ws/semsearch10/#eva.

Delaware: Object representation: the system from Delaware
took all triples having the same subject URI as the description of
an object. However, the resulting structure of the object as well
as the triple structure were then neglected. Terms extracted from
the triples are simply put into one bag-of-words and indexed
as one document. Retrieval model: three existing retrieval models
were applied for the different runs, namely Okapi for sub28-Okapi,
language models with Dirichlet priors smoothing sub28-Dir, and
an axiomatic approach for sub28-AX.

DERI: Object representation: the Sindice system from DERI applied a different notion of objects. All triples having the same subject and also the same context constitute one object description.
Thus, the same subject that appears in two different contexts might
be represented internally as two distinct objects. Further, the system considered relations to other objects, context information, and
URI tokens for the representation of objects. Retrieval model: the
context information, as well as the relations between objects are
used to compute query independent PageRank-style scores. Different parameter configurations have been tested for each run, resulting in different scores. For processing specific queries, these
scores were combined with query dependent TF/IDF-style scores
for matches on predicates, objects and values.

KIT: Object representation: the system by KIT considered literal
values of attributes and separately those of the rdfs:label attribute
as the entity description. All other triples that can be found in the
RDF data for an object were ignored. Retrieval model: the results
were ranked based on a mixture language model inspired score,
which combines the ratio of all query terms to the number of term
matches on one literal and discounts each term according to its
global frequency.

L3S: Object representation: the system by L3S takes a different
approach to object representation. Each unique URI, appearing as
subject or object in the dataset, is seen as an object. Only information captured by this URI is used for representing the object.
Namely, based on the observation that some URIs contain useful
strings, a URI was split into parts. These parts were taken as a
bag-of-words description of the object and indexed as one doc-
ument. Thereby, some provenance information is taken into ac-
count, i.e., the domain extracted from the URI. Retrieval model:
a TF/IDF-based ranking combined with using cosine similarity to
compute the degree of matching between terms of the query and
terms extracted from the object URI was used here.

UMass: Object representation: all triples having the same subject
URI were taken as the description of an object. For the first two
runs, sub31-run1 and sub31-run2, the values of these triples

are just seen as a bag-of-words and no structure information
was taken into account. For the third run, sub31-run3, the object
representation was divided into four fields, one field containing
all values of the attribute title, one for values of the attribute
name, a more specific one for values of the attribute dbpedia:title
and one field containing the values for all the attributes. Retrieval
model: existing retrieval models were applied, namely the query
likelihood model for sub31-run1 and the Markov random field
model for sub31-run2. For sub31-run3, the fields were weighted
separately with specific boosts applied to dbpedia:title, name, and
title.

Yahoo! BCN: Object representation: every URI appearing at the
subject position of the triples is regarded as one object and is represented as one virtual document that might have up to 300 fields,
one field per attribute. A subset of the attributes were manually
classified into one of the three classes important, neutral, and unimportant and boosts applied respectively. The Yahoo! system took
the provenance of the URIs into account. However, not the context
but the domain of the URI was considered and similarly to the at-
tributes, it was classified into three classes. Relations and structure
information that can be derived from them were not taken into ac-
count. Retrieval model: The system created by Yahoo! [47] uses an
approach for field-based scoring that is similar to BM25F. Matching
terms were weighted using a local, per property, term frequency as
well as a global term frequency. A boost was applied based on the
number of query terms matched. In addition, a prior was calculated
for each domain and multiplied to the final score. The three submitted runs represent different configurations of these parameters.

5.1.2. 2010 Entity Track evaluation results

Only the top 10 results per query were evaluated, and after
pooling the results of all the submissions, there was a total of 6158
unique query-result pairs. Note this was out of a total of 12,880
potential query result pairs, showing that pooling was definitely
required. Some systems submitted duplicate results for one query.
We considered the first occurrence for the evaluation and took
all following as not relevant. Further, some submissions contained
ties, i.e. several results for one query had the same score. Although
there exist tie-aware versions of our metrics [48], the trec_eval
software7 we used to compute the scores cannot deal with ties
in a correct way. Therefore we broke the ties by assigning scores
to the involved result according to the order of occurrences in the
submitted file.

Table 7 shows the evaluation results for the submitted runs. The
third run submitted by Yahoo!, together with the third run of the
UMass system, gave the best results.

7 http://trec.nist.gov/trec_eval/.

Table 7
Results of submitted Semantic Search engines.

Participant
Yahoo! BCN
UMass
Yahoo! BCN
UMass
Yahoo! BCN
Delaware
Delaware
UMass

Delaware

L3S

Run
sub30-RES.3
sub31-run3
sub30-RES.2
sub31-run2
sub30-RES.1
sub28-Okapi
sub28-AX
sub31-run1
sub27-dpr
sub27-dlc
sub28-Dir
sub27-gpr
sub29
sub32

P@10

Fig. 11. Average NDCG for queries from the Microsoft dataset.

Figures show the boundary of the first and third quartiles using
error bars. It is noticeable that the Yahoo! set is indeed more
difficult for the search engines to process, with larger variations
of NDCG across both queries and across systems. The performance
on queries from the Microsoft log, which are more frequent
queries, shows less variation among queries and between systems
processing the same queries. This confirms that popular queries are
not only easier, but more alike in difficulty.

5.1.3. Discussion of the 2010 challenge

The systems submitted to the evaluation represent an array
of approaches to semantic search, as shown by the diversity of
results. Most participants started with well-known baselines from
Information Retrieval. When applied to object retrieval on RDF
graphs these techniques yield workable results almost out-of-the-
box, although a differential weighting of properties has been key to
achieving top results (see the runs from Yahoo! BCN and UMass).
Besides assigning different weights to properties, the use of
semantics or the meaning of the data has been limited. All the
participating systems focused on indexing only the subjects of
the triples by creating virtual documents for each subject, which
is understandable given the task. However, we would consider
relations between objects as one of the strong characteristics of the
RDF data model, and the usefulness of graph-based approaches to
ranking will still need to be validated in the future. Note that in
the context of RDF, graph-based ranking can be applied to both
the graph of objects as well as the graph of information sources.
Similarly, we found that keyword queries were taken as such, and
despite our expectations they were not interpreted or enhanced
with any kind of annotations or structures. The possibilities for
query interpretation using the background knowledge (such as
ontologies and large knowledge bases) or the data itself is another
characteristic of semantic search that will need to be explored in
the future.

The lack of some of these advanced features is explained partly
by the short time that was available, and partly by the fact that this
was the first evaluation of this kind, and therefore no training data
was available for the participants.

5.2. Semantic Search Challenge 2011

As described in Section 5.1 the evaluation in 2010 was centered
around the task of entity search. This choice was driven by the
observation that over 40% of queries in real query logs fall into this
category [10], largely because users have search engine relevance
decreases with longer queries and have grown accustomed to
reducing their query (at least initially) to the name of an entity.
However, the major feedback and criticism of the 2010 SemSearch
Challenge was that by limiting the evaluation to keyword search
for named entities the evaluation excluded more complex searches
that would hypothetically be enabled by semantic search over RDF.
Therefore, the 2011 SemSearch competition introduced a second
track, the List Search track, that focused on queries where one or
more entities could fulfill the criteria given to a search engine.

The Semantic Search Challenge 2011 comprised two different
tracks, the Entity Search Track, just like in 2010, and the List Search
Track as reported before in [13].

5.2.1. Participating systems in the Entity Track 2011

Four teams participated in both tracks. These teams were University of Delaware (UDel), Digital Enterprise Research Institute
(DERI), International Institute of Information Technology Hyderabad (IIIT Hyd), and Norwegian University of Science and Technology (NTNU). Dhirubhai Ambani Institute of Information and
Communication Technology (DA-IICT) participated additionally in
the List Search Track.

Fig. 12. Average NDCG for queries from the Yahoo! dataset.

It was interesting to observe that the top two runs achieved
similar levels of performance with retrieving very different sets
of results. The overlap between these two runs as measured by
Kendalls  is only 0.11. By looking at the results in detail, we see
that sub31-run3 has a strong prior on returning results from a
single domain, dbpedia.org, with 93.8% of all results from this do-
main. DBpedia, which is an extraction of the structured data contained in Wikipedia, is a broad-coverage dataset with high quality
results and thus the authors have decided to bias the ranking towards results from this domain. The competing run sub30-RES3
returns only 40.6% of results from this domain, which explains the
low overlap. The performance difference is also visible in Fig. 13,
which shows the NDCG per query for both runs. Also we can observe that sub30-RES3 exceeds sub31-run3 for 40 of 92 queries.
Fig. 11 shows the per-query performance for queries from the
Microsoft and Fig. 12 for the queries from the Yahoo! log. Both

R. Blanco et al. / Web Semantics: Science, Services and Agents on the World Wide Web 21 (2013) 1429

Fig. 13. Comparison between runs sub30-RES3 and sub31-run3 in terms of NDCG per query for the Entity Track 2010.

Table 8
Feature overview regarding system internal entity representation and retrieval model.

Entity representation

Retrieval model

Run

attribute-value
relations
domain
context
Text-based
Structure-based
Q-I-structure

UDel


Prox


Each team was allowed to enter up to three different submissions per track, in order to experiment with different system con-
figurations.

In total, 10 runs were submitted for the Entity Search Track and

11 runs for the List Search Track.

In the following sections, we briefly describe and characterize
the systems for each track and report on their performance. Detailed system descriptions are available at the challenge website.8
In order to categorize the systems and illustrate their different
approaches to the entity search task, two major aspects can be
distinguished: (1) the internal model for entity representation, and
(2) the retrieval model applied for matching, retrieval, and ranking.
Before, we characterize the systems, we discuss these two major
aspects.
Entity representation. Teams used a quad having the same subject
URI as the representation of an entity. Only DERI deviated from
this representation and took all quads having the same subject
and their contexts as the representation of an entity. The applied
representations of an entity can be characterized by four aspects,
which describe how the specifics of the data are taken into account.
The RDF data model makes a distinction between object and data
type properties. Data type properties can be seen as attribute-value
pairs, where the value is a literal value, usually a text string. In con-
trast, object properties are typed relations in the form of attributeobject pairs, where the object is the URI identifier of another entity
rather than a literal value. Since URIs are used as identifiers, each
URI has a domain name, which can be seen as one kind of prove-
nance. Another provenance aspect is the context, which describes
the source of the triple in the BTC dataset. The domain is different from the context because URIs with the same domain can be
used in different contexts. Whether these aspects are considered,
is illustrated in Table 8 as follows:
 Attribute-value: are the attribute-values of the triples used in
the entity representation (yes +/no )?

8 http://semsearch.yahoo.com.

Olav


Harald

Godfrid

 Relations: are the relations to other entities considered (yes
+/no )? The relations are potentially exploitable for ranking,
because they form the data graph by linking to other entities. If
this information is not taken into account, the relations usually
treated as additional attribute-value pairs.
 Domain: is the domain information used (yes +/no )? Entities
of a certain domain are some times boosted, because certain
domains are considered a-priori as relevant or of high quality.
Often entities from dbpedia.org are considered for a-priori
boosting.
 Context: is the context information included in the entity representation (yes +/no )? This information can be used as well
to favor certain sources.

Retrieval model. All participating systems used inverted indexes to
manage their data. Still, the different approaches can be characterized by three main aspects introduced in Section 5.1.1 Table 8 gives
an overview of the systems based on the characteristics introduced
above.

5.2.2. Overview of evaluated systems Entity Track 2011
UDel:

Entity representation: all quads having the same subject
URI constituted one entity. Terms extracted from these
quads are simply put into one bag-of-words and indexed
as one document.
Retrieval model: an axiomatic retrieval function was applied by University of Delaware [49]. For run UDel-Prox,
query term proximity was added to the model, which favors documents having the query terms within a sliding
window of 15 terms. The third run UDel-VO promotes entities whose URI has a direct match to a query term.
Entity representation: in contrast to the other systems,
the Sindice system from DERI took all quads having the
same subject and the same context as the description
of an entity. Only entity descriptions comprising more
than 3 quads were considered. This entity description
is internally represented as a labeled tree data model
with an entity node as the root, and subsequent attribute

DERI:

IIIT Hyd: did not provide a system description.
NTNU:

and value nodes. In addition, run DERI-3 used the entire graph structure, so exploiting the relationships of any
given entity when ranking.
Retrieval model: BM25MF, an extension of BM25F, which
allows fields to have multiple values was used by
Sindice to rank entities for all runs. The second and
winning run, DERI-2, applied additionally query specific weights, namely query coverage and value coverage.
These weights indicate how well the query terms are covered by a root node, respectively value node, in the internal data model. The more query terms are covered by a
node, the more weight is contributed to this node. In ad-
dition, query independent weights were assigned to at-
tributes, whose URI contain certain keywords, e.g. label,
title, sameas, and name. Run DERI-3 used additionally the
relations to compute query independent scores based on
the graph structure.

Entity representation: NTNU used the DBPedia dataset in
addition to the BTC to represent entities. An entity is
represented by three sub-models, the first comprises all
name variants of this entity in DBPedia, the second considers several attributes from DBPedia for this entity, and
the third uses the data from BTC about this entity. On
the syntactic level, all triples having the same subject
URI were used for the models based on DBPedia. For run
NTNU-Olav, the model based on the BTC used only literal
objects and regarded them as one flat text representation.
For the runs NTNU-Harald and NTNU-Godfrid, the model
had two fields, the name field which contained values of
attributes that mentioned the name of the entity, while
all other attributes were put into the content field.
Retrieval model: mixture language models were used to
incorporate the different entity models in the retrieval
function, while weights were applied for specific attributes of DBPedia. Run NTNU-Godfrid used sameAs (an
equivalence link on the Semantic Web) relations to propagate scores, in order to rank directly related entities
higher.

Table 9
Results of the 2011 Entity Search Track.

Participant

UDel

UDel

IIIT Hyd
IIIT Hyd

Run

Prox
Harald
Godfrid
Olav

P10

P5

5.2.4. 2011 List Search Track evaluation

The List Search Track comprised queries that describe sets of en-
tities, but where the relevant entities were not named explicitly in
the query. This track was designed to encourage participating systems to exploit relations between entities and type information of
entities, therefore raising the complexity of the queries. The information need was expressed by a number of keywords (minimum
three) that describe criteria that need to be matched by the returned results. The goal was to rank higher the entities that match
the criteria than entities that do not match the criteria. Examples
of the queries used in the two tracks are shown in Table 3 and described in the Section 3.2.2.

For the List Search Track, the workers were presented additionally with a reference list of correct entities in addition to the criteria
itself, which was obtained through manual searching by the orga-
nizers. This was done as the queries were of such difficulty that
many assessors may not know the answers themselves.

In general the teams participated with the same systems in the
List Search Track and adapted them only slightly to this new task,
although the most high-performing system was specially designed
for the List Track. The adaptions were mostly on the query analysis
and interpretation, because the queries were not just keywords
but more complex descriptions in natural language, as described
in Section 3.2.2. The modifications as well as the additional system
were described in the next section followed by the results for this
track.

5.2.3. 2011 Entity Track results
Discussion of the 2011 Entity Search Track. The semantic search
task of finding entities in an large RDF graph has been addressed
by a spectrum of different approaches in this challenge as shown
by the diversity of the results (see Table 9). The basis for most
systems was well known Information Retrieval techniques, which
yielded acceptable results. However, the winning system from
DERI was a specialized system, which adapted IR methods and
tailored them to RDF. The key feature for success, shared by
the two top ranked systems in the 2011 challenge, was to take
the proximity or coverage of query terms on individual attribute
values into account. This was a consequent development step
over the 2010 challenge, where weighting properties individually
was the key feature for success. The general observation was
that considering the particular pieces of the structured data
yields higher performance over unstructured text-based retrieval
methods.

Similar to 2010, one of the main and promising features of
the RDF data model, namely the ability to express and type the
relations between entities was only used by one run from DERI,
which did not exceed the other runs. Whether relations are actually
not helpful for entity search on large scale datasets or whether
the usage of the relations is not yet understood remains to be
investigated in the future. The List Search Track was designed with
the intention in mind to get the systems to consider the relations
as well. How the systems addressed this task is described in the
next section.

DERI:

5.2.5. Participating systems in the List Search Track
Delaware: the team from Delaware applied an NLP parser to process the queries for run UDelRun1, in order to find the
target type of the entities. Only entities belonging to
this type were considered as results. For the runs UDelRun2 and UDelRun3 the type information was manually expanded, because the automatic processing failed
in some cases. Instead of the axiomatic retrieval func-
tion, model-based relevance feedback was applied for
run UDelRun3 [50].
DERI participated with an identical system configuration
in the List Search Track.

NTNU: NTNU participated with a system especially designed for
this track. The system used only the Wikipedia dataset
and mapped the results to entities in the BTC collec-
tion. The queries were analyzed and potentially reformulated using the Wikipedia Miner software [51], in order
to find the primary entity of the query. The query was
run against an index of Wikipedia abstracts to get a candidate list of Wikipedia articles. The outgoing links from
these articles were expanded and the resulting articles
were also added to the candidate list. Scores are added if
an article occurs multiple times and articles with a direct
relation to the principal entity are boosted. In contrast to
run NTNU-1, the runs NTNU-2 and NTNU-3 used an additional boosting for articles belonging to a Wikipedia set

R. Blanco et al. / Web Semantics: Science, Services and Agents on the World Wide Web 21 (2013) 1429

Table 10
Results of the List Search Track.

Participant

UDel
UDel
IIIT Hyd
IIIT Hyd
DA-IICT

Run

P10

P5

that had more than a certain fraction of its set of members
in the candidate list. Run NTNU-3 also applied an additional boost based on sameAs links.

DA-IICT: the system by DA-IICT used a text-based approach build
on Terrier [52] which favored entities according to the
number of query terms present in their textual descrip-
tion. Due to data loss, the queries were only run against
a part of the BTC data collection.

5.2.6. List Search Track results

The retrieval performance for the submitted runs are shown in
Table 10. The metrics were computed the same ways as for the
Entity Track. There are on average 13 relevant entities per query
with a standard deviation of 12.8. The participating systems could
not find relevant entities for 6 queries. These were the queries with
numbers q15, q23, q27, q28, q45 and q48, for example q15: henry
iis brothers and sisters.
Discussion of the 2011 List Search Track. The List Search Track proved
to be a hard task and may require different techniques compared
to the Entity Search Track. Since this track was new, most teams
participated with their systems built for the Entity Search Track
and adapted to the task mainly by analyzing and interpreting
the query. Still, the performances showed that solutions can be
delivered, although there was still room for improvement. The
winning system by NTNU did not use the BTC data collection, but
was built on the Wikipedia corpus and exploited the links between
articles, demonstrating that the plain links between articles are
a valuable resource for search. Ideally, such algorithms could
eventually be adopted to more general-purpose RDF structured
data outside that of Wikipedia.

5.2.7. Discussion of the 2011 Semantic Search Challenge

The Semantic Search Challenge started in 2010 with the task
of (named) entity retrieval from RDF data crawled from the Web.
Though this task was seemingly simple, because the query contains
the name of the entity, it features many of the problems in
semantic search, including the potential ambiguity of short-form
queries, the varying degrees of relevance by which an entity can
be related to the one named in the query and the general quality
issues inherent to Web data. The List Search Track introduced in
2011 presented an even harder problem, i.e. queries that do not
explicitly name an entity, but rather describe the set of matching
entities.

The general direction of our work will continue towards exploring the search tasks of increasing difficulty. In addition, there are a
number of open questions that may impact the end-user benefits
of semantic search engines and would still need to be investigated.
For example, the retrieval engines above did not attempt to remove
duplicates, and may return different, redundant descriptions of the
same entity multiple times. A semantic search engine should remove such duplicates or merge them. Similarly, the user experience was largely impacted by the explanations given by the search

engines. Similar to how the current text search engines generate
summaries and highlight keyword matches, a semantic search engine should attempt to summarize information from an RDF graph
and highlight why a particular result is an answer to the users
query.

6. Conclusion

The topic of semantic search has attracted large interests both
from industry and research, resulting in a variety of solutions that
target different tasks. There is however no standardized evaluation
framework that helps to monitor and stimulate the progress in this
field. We define the two standard tasks of entity search and entity list search, which are commonly supported by semantic search
systems. Starting with these tasks, we run evaluation campaigns
organized in the context of the series of SemSearch workshops
to assess the state-of-the-art in semantic search with respect
two these basic tasks. Aiming at affordable, repeatable and reliable evaluation, we provide a crowdsourcing-based evaluation
methodology alongside with a semantic search evaluation framework consisting of real-world queries and datasets. This work discusses the tasks, the framework, the performances achieved by the
systems that participated in the campaigns, and the repeatability
and reliability of the proposed evaluation methodology. Throughout these two years, we have observed that not only was the evaluation reliable and repeatable but also, the experiments could be
performed at an acceptable cost.

So far, the methodology has been tested only with respect to
two tasks. We are planning to extend the Semantic Search Challenge to cover other retrieval scenarios such as search for consolidated objects (data integration and search), search for documents
with embedded RDF (semantic document retrieval) and search
for relations between objects (relational search). We consider the
provided evaluation framework as a basis platform, which invites
researchers to participate and extend towards these and other
semantic search scenarios.

Acknowledgments

We acknowledge Yahoo! Research for making available under
license the Yahoo! Search Query Log Tiny Sample, version 1.0
dataset as part of the WebScope program. We also thank Evelyne
Viegas and Microsoft Research for allowing a portion of the Microsoft Live Query Log to be used in the 2010 campaign. In par-
ticular, we would like to thank Amazon and the NAACL workshop
on using the Mechanical Turk for providing the initial funding for
the 2010 evaluation, and would like to thank the European PASCAL project for the rest of the support. Of course, none of this
work would have been possible without the groups that submitted search engines for evaluation at the Semantic Search Challenge 2010 and 2011. Harry Halpin was partially supported by a
Microsoft Research grant on Beyond Search. We acknowledge Ya-
hoo! Labs for hosting the Semantic Search Challenge 2011 website
and sponsoring the prizes. The costs of the 2011 evaluation have
been sponsored by the European SEALS project, http://www.seals-
project.eu.
