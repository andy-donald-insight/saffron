Web Semantics: Science, Services and Agents on the World Wide Web 20 (2013) 117

Contents lists available at SciVerse ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

journal homepage: www.elsevier.com/locate/websem

Relatedness between vocabularies on the Web of data: A taxonomy and an
empirical study
Gong Cheng, Yuzhong Qu

State Key Laboratory for Novel Software Technology at Nanjing University, Nanjing 210023, PR China

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 24 July 2012
Received in revised form
22 January 2013
Accepted 14 February 2013
Available online 26 February 2013

Keywords:
Graph analysis
Ontology
Relatedness
Vocabulary

1. Introduction

Given thousands of vocabularies published and used on the Web of data, the sociology of vocabulary
creation and application is receiving increasing attention, which studies the statistical features of and the
relations between vocabularies from various sources. In this article, we tackle a taxonomy of relatedness
between vocabularies, comprising declarative, topical and distributional perspectives, which are derived
from the structural description, textual description and context of use of a vocabulary, respectively. We
characterize each perspective by using a graph model representing vocabularies and their relatedness,
and implement it over a data set containing 2996 vocabularies and 4.1 billion RDF triples, based on which
we perform degree, connectivity and cluster analysis. We also discuss the correlation between different
perspectives. The results and findings are expected to be useful for future research and development on
vocabularies.

 2013 Elsevier B.V. All rights reserved.

A vocabulary (a.k.a. an ontology) defines a collection of classes
and properties, representing concepts (i.e. types of resources) and
attributes of or relations between resources, respectively. In the
Web era, a major language for writing vocabulary description
is RDF(S), when some more expressive vocabularies also feature
constructs provided by OWL. Vocabularies play a key role on the
emerging Web of data, since they function as shared schemas of
Web data, and thus form a basis that supports the integration of
Web data at the semantic level.

So far, major libraries have collected hundreds of vocabularies
even in a particular domain [1]; search engines such as Swoogle1
claimed to have discovered thousands of vocabularies. Whereas
different vocabularies may be published from not a single but
many different sources, certain kinds of relations may be found
between them. For example, one vocabulary may extend another
by creating more specific classes; two vocabularies could actually
be different copies of the same underlying conceptualization. Such
relations, being either explicitly stated or hidden to be mined,
make vocabularies on the Web not isolated from each other but
connected into a network.

Analyzing the relatedness between vocabularies has shown
its great importance in many applications. In a multi-vocabulary
management system such as PROMPT [2], a core task is to find
 Corresponding author. Tel.: +86 0 25 89680923; fax: +86 0 25 89680923.

E-mail addresses: petercheng456@gmail.com, gcheng@nju.edu.cn (G. Cheng),

yzqu@nju.edu.cn (Y. Qu).
1 http://swoogle.umbc.edu/.

1570-8268/$  see front matter  2013 Elsevier B.V. All rights reserved.
http://dx.doi.org/10.1016/j.websem.2013.02.001

all kinds of relations between vocabularies, e.g. finding similar
vocabularies and computing mappings between them, detecting
different versions of the same vocabulary and identifying the
changes. In a vocabulary search engine, leveraging the relatedness
between vocabularies to develop ranking algorithms has proven
to be effective [3]. Besides, extensive analysis of relatedness over
a representative sample of real-world vocabularies could help find
some laws of the Semantic Web, to provide empirical support for
existing applications and to direct future research.

In light of this, the main contribution of this article is an empirical study of the relatedness between 2996 vocabularies obtained
from a large data set consisting of 4.1 billion RDF triples crawled
from the Web. Considering the various kinds of relations between
vocabularies, we divide relatedness into three perspectives:
 declarative relatedness, which captures the semantic relations
between vocabularies explicitly declared in their structural
descriptions,
 topical relatedness, which measures the overlap of topics
between vocabularies reflected in their textual descriptions,
and
 distributional relatedness, which characterizes the similar contexts on the Web of data where vocabularies are often used.
For each perspective, a powerful tool adopted in our study
is graph analysis [4]. Specifically, we represent vocabularies and
their relatedness by a graph, and compute several standard metrics
including degrees, connected components, clustering coefficients
and cluster structure. Each of these metrics has a specific indication
with regard to each specific perspective, based on which we are

G. Cheng, Y. Qu / Web Semantics: Science, Services and Agents on the World Wide Web 20 (2013) 117

able to comprehensively characterize the network of related vo-
cabularies. Besides analyzing each perspective separately, we also
discuss their correlation.

Compared with our earlier effort [5] which mainly defines several measures of relatedness between vocabularies, in this extended article we shift the focus, by combining these measures
with concepts in graph theory, to a systematic analysis of relatedness between vocabularies in the real world. Hence, instead of
leveraging relatedness to implement a specific kind of application,
e.g. recommender systems as in [5], our primary goal here is to
draw, from extensive experiments, interesting conclusions that are
useful for better understanding the connections between vocabularies and accordingly improving all kinds of applications.

The remainder of the article is structured as follows. Section 2
gives preliminaries on vocabulary and graph theory. Section 3
describes the acquirement and statistics of the data set used in
this study. Sections 46 define and analyze declarative, topical
and distributional relatedness between vocabularies, respectively.
Section 7 presents their correlation. Finally, Section 8 discusses
related work before we conclude in Section 9.

2. Preliminaries

In this section, we informally introduce the terms we will use

in subsequent sections.

2.1. Vocabulary

An RDF document is a Web document that serializes an RDF

graph to RDF/XML, Turtle, XHTML + RDFa, etc.

A term is a URI that identifies a class or a property in the RDF
document obtained via dereferencing the URI (called its authoritative document); that is, whereas the denotation of a URI may
vary with data source on the Web (e.g., denoting a class in one RDF
document but denoting a person in another), we only trust the document at the provenance of the URI. When we mention the description of a term (or any resource identified by a URI), we shall refer to
its RDF description provided by its authoritative document.

Terms sharing a common namespace URI constitute a vocabu-
lary, which is identified by the namespace URI. When we mention
the description of a vocabulary, we shall refer to all of its constituent
terms descriptions; when we mention the meta-description of a vo-
cabulary, we shall treat the vocabulary as a resource identified by
its namespace URI, and refer to the description of this resource.

The above definitions indicate that we are only interested in
dereferenceable RDF vocabularies that are published on the Web by
applying best practice [6], i.e., (part of) the RDF description of a
vocabulary will be returned when its URI and/or the URIs of its
constituent terms are dereferenced. However, it is worth noting
that for a vocabulary published in a distributed fashion (i.e., its
constituent terms are defined in multiple RDF documents), we may
fail to find all of its constituent terms because the data set we use
may fail to include certain relevant RDF documents.

2.2. Graph theory

Graph comprises nodes and edges.
A self-loop is an edge that connects a node to itself. Parallel edges
are two or more edges incident to the same (ordered) pair of nodes.
A simple graph contains no self-loop and no parallel edges.

The degree of a node is the number of edges incident to it; a node
with degree 0 is called an isolated node. When edges are directed,
we distinguish between the indegree and outdegree of a node. The
indegree of a node is the number of edges directed to it; a node
with indegree 0 is called a source node. The outdegree of a node is
the number of edges directed from it; a node with outdegree 0 is

called a sink node. In a digraph, the degree of a node is equal to the
sum of its indegree and outdegree.

A strongly connected component (SCC) of a digraph is a maximal
subgraph where there is a path from each node to every other
node; a digraph can be decomposed into SCCs in a unique way. A
trivial SCC is an SCC that contains a single node; a nontrivial SCC
contains more than one node.

A weakly connected component (WCC) of a digraph is a maximal
subgraph where there is an undirected path (in which the edges
not necessarily go the same direction) from each node to every
other node; a digraph can be decomposed into WCCs in a unique
way. A trivial WCC is a WCC that contains an isolated node; a
nontrivial WCC contains more than one node.

A connected component (CC) of an undirected graph is a maximal
subgraph where there is a path between every pair of nodes; an
undirected graph can be decomposed into CCs in a unique way.
A trivial CC is a CC that contains an isolated node; a nontrivial CC
contains more than one node.

In a simple graph, the clustering coefficient of a node is defined
as the ratio of the number of actual edges between its adjacent
nodes to the number of all possible edges between them; in
other words, it quantifies to what extent the subgraph induced
by all the adjacent nodes of a node approximates a complete
graph. For nodes with degree zero (i.e. isolated nodes) and one
(i.e. leaf nodes), we define their clustering coefficients to be zero.
The clustering coefficient of a graph is given by the average of the
clustering coefficients of all the nodes in the graph.

3. Data set

In this section, we describe how we obtained  via crawling  a
sample of the Web of data as the data set analyzed in our study, and
present a number of its statistical features to show its large scale
and diversity. Then we report statistics on the vocabularies found
in the data set.

3.1. RDF documents

The data set analyzed in this work is a snapshot of the data set
maintained by the Falcons search engine2 in May 2011, comprising
a crawled portion of the real-world Web of data; it is also the one
used in our earlier work [5]. The URI pool of the crawler was initialized by a collection of seed URIs consisting of: (a) URIs offered
by several online repositories such as pingthesemanticweb.com
and schemaweb.info, (b) URIs of sample resources and/or entry
points of many data sets published in the Linking Open Data cloud,
and (c) URIs returned by the Google search engine (restricted
by filetype:rdf OR filetype:owl) and the Swoogle search
engine against a collection of keyword queries automatically
generatedeach corresponding to one or several keywords randomly extracted from the category names at the top-three levels
of the Open Directory Project.3 Then, a parallel crawler which follows the Robots Exclusion Protocol4 repeatedly picked a URI at random from the pool and dereferenced it with content negotiation
to only accept RDF/XML or XHTML + RDFa document. Every URI
met in a downloaded RDF document and not seen before would be
added to the pool, to be crawled in the future.

We are aware that the Web of data might be too large to be
exhaustively crawled since new URIs are continually added to the
pool. So we only expect our data set to be sufficiently large and
diverse so that our empirical study would be statistically significant

2 http://ws.nju.edu.cn/falcons/.
3 http://www.dmoz.org/.
4 http://www.robotstxt.org/.

Table 1
Distribution of RDF documents in PLDs.

hi5.com
l3s.de
geonames.org
dbpedia.org
freebase.com
fu-berlin.de
bibsonomy.org
livejournal.com
zitgist.com
dbtune.org
Others

Table 2
Distribution of vocabularies in TLDs.

org
edu
com
eu
uk
fr
net
de
Others

Number of RDF documents
1,074,814
1,072,874
1,029,254
1,022,667

6,908,606

Number of vocabularies

Fig. 2. Distribution of the number of constituent terms of a vocabulary (on a loglog
scale).

59,695 properties (13.1%) and 173 defined as both classes and
properties; all the terms clustered into 2996 vocabularies. How-
ever, out of the 5805 PLDs in our data set that offer RDF documents,
only 261  a very small portion (4.5%)  publish their own vocab-
ularies; that is, most Web data publishers choose to reuse existing
vocabularies, whereas vocabulary creation has become the responsibility of a small community. Further, these 261 PLDs belong to 33
top-level domains (TLDs). As shown in Table 2, org and edu hold
the most vocabularies, dominating with 44.5% and 31.6%, respec-
tively, followed by com and several country code TLDs. This distribution basically agrees with the one reported by Swoogle five years
ago [8]; that is, most vocabularies are still published by noncommercial organizations and educational institutions. Besides, based
on the metadata of each data set in the Linking Open Data cloud
(provided by datahub.io) about the vocabulary prefixes used and
the mapping from prefix to URI (provided by prefix.cc), we found
that the entire Linking Open Data cloud used 127 vocabularies, of
which 57 (44.9%) are included in our data set, showing a considerable coverage.

Looking inside vocabularies, firstly, as plotted in Fig. 2 on a
loglog scale, their sizes  in terms of the number of constituent
terms  differ widely from one to another. On the one hand,
there are ten large vocabularieseach containing more than ten
thousand terms, including different versions of the well-known

Fig. 1. Distribution of the number of RDF documents crawled from a PLD (on a
loglog scale).

and typical. The following statistical results will verify our
expectation. Our data set comprises 15.9 million RDF documents,
which were crawled from 5805 pay-level domains (PLDs)5 and
collectively contain 4.1 billion RDF triples. By comparison, at the
time of writing, Swoogle reported 3.8 million RDF documents and
1.1 billion RDF triples6; the well-known Billion Triple Challenge
2011 data set (BTC 2011) consists of 2.1 billion RDF triples crawled
from 791 PLDs7; the entire Linking Open Data cloud published 31.6
billion RDF triples.8 That is, in terms of the number of RDF triples,
our data set has covered 13.0% of the known portion of the Web
of data, and is larger than Swoogle and BTC 2011; in terms of the
number of PLDs, our data set is contributed by even more sources
than BTC 2011.

As plotted in Fig. 1 on a loglog scale, in our data set, RDF
documents are not evenly distributed among PLDs. The initial part
of the distribution approximates a power law. To be specific, let f (x)
be the number of PLDs that have exactly x RDF documents being
crawled; then f (x) basically has the form:
f (x)  x,
(1)
where  > 1, so that on a loglog plot it will appear as a straight
line with slope . We are not the first to observe this powerlaw phenomenon in content creation on the Web; it has been
extensively reported in the literature, e.g., on two crawled sets of
RDF documents [8] and on two crawled sets of webpages [9]. As we
will see later in the article, we have adapted our means of analysis
to such extremely uneven distribution when necessary.

In particular, the distribution has a long tail, which corresponds
to large PLDs containing far more RDF documents than others.
Table 1 presents the ten largest PLDs in our data set. We found
that the top-nine PLDs contribute more than half of all the RDF
documents in our data set. In fact, the distribution would probably
have become more skewed if we had not stopped our crawler
from crawling the PLDs having contributed more than roughly
one million RDF documents. This protected our data set from
potentially being dominated by only a few PLDs to an unacceptable
degree.

3.2. Vocabularies

Based on the strategy described in Section 2.1, we found
455,718 terms in our data set, comprising 395,850 classes (86.9%),

5 A pay-level domain is a domain that requires payment at a (country code) toplevel domain [7]. For instance, the URI http://ws.nju.edu.cn/falcons/ belongs to the
pay-level domain nju.edu.cn.
6 http://swoogle.umbc.edu/index.php?option=com_swoogle_stats&Itemid=8.
7 http://km.aifb.kit.edu/projects/btc-2011/.
8 http://www4.wiwiss.fu-berlin.de/lodcloud/state/.

G. Cheng, Y. Qu / Web Semantics: Science, Services and Agents on the World Wide Web 20 (2013) 117

YAGO vocabulary, the Cyc vocabulary, etc. On the other hand, most
vocabularies (69.3%) are rather smalleach containing twenty
terms or fewer. Secondly, the Pearsons correlation coefficient
between the number of classes in a vocabulary and the number
of properties it contains is 0.045, implying basically no linear
correlation. As extreme cases, a version of YAGO and other 589
vocabularies (19.7%) only comprise classes, whereas some other
970 (32.4%) only comprise properties. To summarize, we indeed
observed a great diversity of vocabularies in our data set.

To conclude the section, we have obtained, from a representative sample of the Web of data, a large number of real-world vocabularies in various constitutions. This forms a solid foundation
for the following empirical study to produce significant results. In
particular, we have made our data set online accessible9 so that one
can repeat all our experiments.

4. Declarative relatedness

A vocabulary can be entirely created from scratch; alternatively,
it can be developed based upon existing vocabularies, which may
substantially save time and cost. In the latter case, for example,
when we intend to develop a vocabulary describing electric
vehicles, and we are aware of an existing vocabulary describing
vehicles, we only need to focus on the development of the electric
part, since for the vehicle part we can reuse descriptions of terms
in the existing vocabulary. In particular, it is not necessary to copy
and paste these descriptions; rather, the existing vocabulary (or
its constituent terms) can be simply referenced in the description
of the new vocabulary, just as in HTML a hyperlink is created
to reference a webpage. Such reference relates one vocabulary
to another, and indicates a certain kind of relatedness between
them, which we term declarative relatedness since it is declared by
vocabulary publishers.

In this section, firstly we define and classify references between
vocabularies. Then we represent vocabularies and their references
by graph models, and report the results of an empirical graph
analysis based on our data set.

4.1. Vocabulary references

Depending on whether a reference is declared at the vocabulary
level or at the term level, we distinguish between two kinds of
vocabulary references: explicit reference and implicit reference.

4.1.1. Explicit references

Major vocabulary languages such as OWL offer properties for
describing a vocabulary itself (rather than its constituent terms),
which constitute the meta-description of the vocabulary. Some
of these properties connect a vocabulary with other vocabular-
ies. For instance, owl:imports indicates that the referencing
vocabulary needs to access terms in the referenced vocabulary;
owl:priorVersion declares that the referenced vocabulary is a
prior version of the referencing vocabulary. Since such references
are explicitly attached to vocabularies, we term it explicit reference.
In other words, vocabulary v1 explicitly references vocabulary v2 if
v2 is referenced in v1s meta-description.

4.1.2. Implicit references

In the description of a vocabulary, a term may be defined
based upon other terms, e.g. via rdfs:subClassOf or complex
OWL constructs; that is, the description of a term may reference
other terms. When the referencing term and a referenced term
belong to different vocabularies, such term-level reference implies
a reference between vocabularies, which we term implicit reference
since it is attached to not vocabularies but their constituent terms.

9 http://ws.nju.edu.cn/njvr/.

It is worth noting that the observation of an implicit reference
does not necessarily indicate the existence of a corresponding
explicit reference, and vice versa; that is, even if some terms in
vocabulary v2 are referenced in vocabulary v1s description, v2 may
or may not be referenced in v1s meta-description, and vice versa.
For this reason, both explicit and implicit references deserve to be
studied.

4.2. Graphic representation

We intend to analyze the relatedness between vocabularies
on the Web induced by declared references. We observed that
the situation here is quite similar to the hyperlink analysis of the
Web (e.g. [1012]); that is, vocabularies and references between
vocabularies correspond to webpages and hyperlinks between
webpages, respectively. In traditional Web analysis, a commonly
adopted approach is to model the Web as a graph, where nodes
correspond to webpages, and directed edges correspond to hyper-
links. Analogously, we can use graph model to characterize references between vocabularies; one extra benefit is that we can
compare our results with those of traditional Web analysis.

To be specific, given V , a collection of vocabularies, the first
graph we construct, denoted by GE, is a digraph where nodes
correspond to vocabularies, and a directed edge connects v1  V to
v2  V if v1 explicitly references v2. In a similar way, we construct
the second digraph, denoted by GI, to capture implicit references.
Finally, when both explicit and implicit references are considered,
we obtain the third digraph denoted by GE+I. It is trivially true that
these three graphs have exactly the same number of nodes (viz.
|V|), and there is an edge connecting v1  V to v2  V in GE+I if
and only if there is an edge connecting v1 to v2 in GE, in GI, or in
both.

Here, GE , GI and GE+I are processed to be edge-unlabeled simple
graphs. They are edge-unlabeled because we are only interested
in whether one vocabulary references another but disregard of
which type such reference is; for example, given v1, v2, v3, v4 
V , v1 owl:imports v2, and v3 owl:priorVersion v4, we will
not distinguish between these two different types of references,
but merely represent them by two unlabeled edges in GE. The
graphs we consider are also simple. It is possible (and actually quite
common) that a vocabulary references itself, leading to a self-loop;
for example, one of two terms in v  V is referenced in the others
description via rdfs:subClassOf, implying a self-reference of v,
which corresponds to a self-loop on v in GI. However, for simplicity
of analysis, self-loops are removed.

In particular, three language-level vocabularies, namely RDF,
RDFS and OWL, and all the references involving them, are too trivial
to attract our interest. They are excluded from the three graphs and
the following analysis.

4.3. Graph analysis

There are many reasons for studying hyperlinks between webpages as a graph [11], most of which also apply to studying references between vocabularies as a graph. For example, it might help
to design crawling strategies [13], it is relevant to algorithms for
ranking vocabularies that operate on the idea of reference analysis [3], it is essential to the sociology of vocabulary creation on the
Web [14], etc. Therefore, we intend to analyze GE , GI and GE+I constructed based upon our data set, by using standard measures for
graph analysis. Because we have three comparable graphs, our results will be mostly reported in a comparative manner.

4.3.1. Size analysis

Each of GE , GI and GE+I contains exactly 2993 nodes; however,
they differ in the number of edges, i.e. in the number of ordered
referencingreferenced pairs of vocabularies. As shown in Table 3,
GE contains more edges than GI; that is, more references were given
explicitly.

(a) GE.

(b) GI.

(c) GE+I.

Fig. 3.

Indegree distributions of GE , GI and GE+I (on a loglog scale).

Table 3
Statistical properties of GE , GI and GE+I.

Nodes
Edges
Average degree
Highest degree
Isolated nodes
Isolated nodes (%)
Highest indegree
Source nodes
Source nodes (%)
Highest outdegree
Sink nodes
Sink nodes (%)
Trivial SCCs
Nontrivial SCCs
Maximum SCC size
Nontrivial WCCs
Maximum WCC size
Isolated and leaf nodes
Clustering coefficient

GE+I

Recall that GE+I was constructed as a union of GE and GI;
thereby, we can infer from Table 3 that both an explicit reference
and an implicit reference were observed on 1113 pairs of vocab-
ularies, leaving 1803 explicit references (61.8%) not accompanied
with any term-level reference and 988 implicit references (47.0%)
not captured by the corresponding meta-descriptions of vocab-
ularies. The latter is more interesting because it shows that the
meta-description of a vocabulary often fails to reflect its reference to another vocabulary which actually exists in its description,
indicating that meta-descriptions are not reliable in this respect.
Therefore, it will be insufficient to only leverage meta-descriptions
to perform reference analysis for carrying out tasks such as vocabulary crawling and ranking.

4.3.2. Degree analysis

Degree is usually treated as an indicator of the centrality of
a node within a graph, i.e. the importance of the node in terms
of the graph structure. For example, in our context, a vocabulary
with a high indegree in GE , GI or GE+I is deemed a popular and
probably fundamental vocabulary since it is referenced by many
other vocabularies on the Web. The indegree of a vocabulary can
hardly be controlled by its publisher, but rather, is given by the
number of references received from other vocabularies, thereby
being collaboratively contributed by all the vocabulary publishers
on the Web. By contrast, a vocabulary with a high outdegree

exhibits its publishers strong desire to reuse existing vocabularies,
and indeed, useful vocabularies can be found on the Web.

As shown in Table 3, although on average a vocabulary is only
involved in 2.61 references, or 1.95 explicit and 1.40 implicit ref-
erences, the highest degrees observed in GE , GI and GE+I are all as
high as several hundreds, mainly contributed by indegrees, showing extremely uneven distributions. Fig. 3 depicts the indegree distributions of GE , GI and GE+I; each of them has an initial part that
approximates a power law, and a relatively long tail corresponding
to several singular vocabularies having very high indegrees. For in-
stance, in GE, the well-known DC vocabulary10 has the highest indegree of 786, i.e., it is explicitly referenced by 26.3% of the other
vocabularies; IRIDL11 takes the second place because it is a fundamental vocabulary that supports many other vocabularies underlying a common data set. IRIDL also leads in GI with indegree
683, followed by FOAF.12 On the other hand, most vocabularies
(90.2% in GE, 85.8% in GI and 83.7% in GE+I) are source nodes, i.e.
not referenced by any other vocabulary.

By comparing our results with those of hyperlink analysis of
the Web [1012], we found that power-law indegree distribution
seems to be a universal phenomenon in link creation on the Web;
there are always some items (e.g. vocabularies, webpages) that are
relatively much more popular on the Web, being linked to from
many others, when most of the rest are rarely referenced. Besides,
the IRIDL case reflects that leveraging the indegree centrality to
solve certain tasks such as ranking will give inadequate accuracy
because the indegree of a node only measures its centrality based
on its neighborhood in the graph, thereby failing to consider the
global situation. As a result, it cannot differentiate between a
vocabulary that is only locally popular within a small community
(such as IRIDL) and a globally prominent one (such as DC); one
can also easily hack such a solution (e.g. a ranking algorithm) by
injecting a lot of vocabularies that all point to the vocabulary to
be promoted. Better results may be obtained by using a global
measure (e.g. PageRank), which we will test in future work.

Compared with indegrees, as shown in Table 3, the highest outdegrees observed in GE , GI and GE+I are much loweras low as
14, 17 and 17, respectively. Most vocabularies either have no explicit reference or have no implicit reference to any other vocabu-
lary, thereby becoming sink nodes in GE or GI; in particular, 46.1%
have neither. Fig. 4 depicts the outdegree distributions of the three
graphs on a loglog scale. In GE, 2959 vocabularies (98.9%) are with
outdegree 3 or lower; in GI and GE+I, the situations are similar,
comprising 97.7% and 96.8% of the vocabularies, respectively. To

10 http://purl.org/dc/elements/1.1/.
11 http://iridl.ldeo.columbia.edu/ontologies/iridl.owl.
12 http://xmlns.com/foaf/0.1/.

G. Cheng, Y. Qu / Web Semantics: Science, Services and Agents on the World Wide Web 20 (2013) 117

(a) GE.

(b) GI.

(c) GE+I.

Fig. 4. Outdegree distributions of GE , GI and GE+I (on a loglog scale).

sum up, vocabulary publishers are generally not active in referencing other vocabularies. Besides, the outdegrees in GE and GE+I
do not follow a power-law distribution, since we found a maximal
value at outdegree 3 rather than 1. It is mainly due to a group of
vocabularies having a common pattern of meta-description, which
references exactly 3 vocabularies.

Table 3 also tells that GE+I contains 1214 isolated nodes, i.e.,
40.6% of the vocabularies neither reference nor are referenced by
any other vocabulary. Considering that more references between
vocabularies indicate greater possibilities of (cross-domain) semantic data integration, the present coverage is far from sufficient.
By comparison, some hyperlink analysis of the Web [1012] observed only less than 10% of the webpages being isolated, demonstrating that publishers on the Web are capable of creating links
broadly. Both sides above jointly give rise to a call for a Web of linked
vocabularies, as an achievable complement to the emerging Web of
linked data.

We complete our degree analysis by studying the correlation
between the indegree and outdegree of a vocabulary. In terms
of the Pearsons correlation coefficient, the indegreeoutdegree
correlation in GE , GI and GE+I are 0.01, 0.04 and 0.01, respectively,
implying no linear correlation. However, by excluding long-tail
vocabularies with indegree higher than 100, the correlation in
GE , GI and GE+I became 0.08, 0.16 and 0.10, respectively. That is, an
increase in correlation was found over a range of small indegrees.
To conclude, more popular vocabularies slightly tend to (implicitly,
in particular) reference a higher number of other vocabularies,
whereas the most popular ones do not necessarily reference a
lot. This phenomenon, conforming with the one reported in a
hyperlink analysis of the Web [10], is caused by the different
nature of indegree and outdegree: the indegree of a popular
vocabulary, which is contributed from outside the vocabulary
(by other vocabularies on the Web), has no upper bound when
new vocabularies emerge continually, whereas the outdegree of a
vocabulary, which is determined by the inside of the vocabulary, is
usually limited to a human-manageable level.

4.3.3. Connectivity analysis

We divide the next set of experiments into two parts: on

strongly and on weakly connected components.

As to GE , GI and GE+I, a path from vocabulary v1 to vocabulary
v2 tells that v1 references v2either directly via a single edge or
indirectly via a sequence of other vocabularies. In this sense, an SCC
captures a family of vocabularies that directly or indirectly reference
each other, thereby indicating their strong association.

As shown in Table 3, GE is decomposed into 2958 SCCs, in
which 2943 are trivial and 15 are nontrivial; that is, only 50
vocabularies (1.7%) are involved in nontrivial SCCs. The largest SCC

contains only 7 vocabularies. This situation is noticeably different
from the results of hyperlink analysis of the Web such as [1012],
where a single large SCC was consistently identified, covering
28%86% of the webpages (depending on the data set). We tend
to attribute such difference to the different nature of edges. As to
webpages, the size of an SCC characterizes the navigability of the
Web, considering that a user starting from any webpage in an SCC
is able to reach every other node in the SCC only via following edges
which correspond to hyperlinks. As to vocabularies, in addition to
navigability, because each reference is with a specific type which is
owl:imports in most cases as we have reported in [5], an SCC in
this context further represents a bunch of vocabularies in mutual
dependence, which probably share a common authorship. This has
been verified by our finding that in every nontrivial SCC observed,
not only all the vocabularies were crawled from the same PLD but
also their URIs share a common, significantly long prefix, e.g. an
SCC consisting of Food13 and Wine.14 Therefore, if the types of
references are still largely limited to owl:imports, we believe
that it is unlikely to see a dominant SCC in GE because that would
actually give rise to a single large vocabulary according to the
semantics of owl:imports.

By comparison, there are relatively more types of references at
the term level so that one may expect to see larger SCCs in GI. This
is indeed the case but the difference is not significant. In GI, we
observed 33 nontrivial SCCs, which collectively cover 109 vocabularies (3.6%) and of which the largest contains 13 vocabularies. One
notable finding is that in 2 SCCs we saw vocabularies from different
PLDs: one consisting of FOAF, Geo15 and Contact,16 and the other
consisting of four vocabularies on DBpedia and Cyc. The former is
still due to the overlap of the authors of the vocabularies, but the
later may truly represent a community effort.

Finally, by considering both explicit and implicit references, in
GE+I we identified 41 nontrivial SCCs covering 139 vocabularies
(4.6%), and the largest one consists of 14 vocabularies. However,
no more cross-PLD SCCs could be found.

To summarize, the absence of large SCCs on the one hand indicates the very weak navigability of the Web of linked vocab-
ularies. As a result, to explore and find vocabularies, one has to
refer to external services such as a vocabulary library or a vocabulary search engine. Interestingly, when a traditional crawler
(such as [13]) feeds its URI pool primarily with those met in the
contents previously downloaded, the weak navigability here is
forcing crawler developers to seek more effective solutions to

13 http://www.w3.org/TR/2003/PR-owl-guide-20031209/food.
14 http://www.w3.org/TR/2003/PR-owl-guide-20031209/wine.
15 http://www.w3.org/2003/01/geo/wgs84_pos.
16 http://www.w3.org/2000/10/swap/pim/contact.

Fig. 5. Decreasing curve of the size of the largest WCC of GE+I when repeatedly
removing a node with the highest degree (on a loglog scale).

enlarging their indexes. On the other hand, in consideration of the
specific semantics of references, we see that vocabularies on the
Web are, on average, loosely coupled in terms of reference. So positively speaking, by treating the Web of linked vocabularies as a
whole, the current situation might also be interpreted as a neatlyperformed modularizationif not disconnected at all, which motivated the following analysis.

As shown in Table 3 and discussed in Section 4.3.2, GE , GI and
GE+I contain 1741, 1339 and 1214 trivial WCCs (i.e. isolated nodes),
covering 58.2%, 44.7% and 40.6% of the vocabularies, respectively.
By comparison, hyperlink analysis of the Web (e.g. [1012])
observed less than 10% being isolated. That is, the Web of linked
vocabularies appears far more fragmented. So the true situation
is that a substantial fraction of publishers have only made their
vocabularies accessible on the Web, but have not considered
establishing connections to improve interoperability via the Web.
As a result, even if these isolated vocabularies are prominent
in their respective domains, they still make difficulties in data
integration in a wider scope such as in cross-domain applications.
Despite many isolated nodes, in GE , GI and GE+I we observed
a single dominant WCC consisting of 843, 1202 and 1332 nodes,
or 28.2%, 40.2% and 44.5% of the vocabularies, respectively, when
other nontrivial WCCs comprising only dozens of or fewer nodes.
Taking GE+I
for instance, the vocabularies in its largest WCC
were crawled from 97 PLDs, showing a relatively wide variety of
sources. However, further observation found that this widespread
connectivity is poor in resilience, i.e., the connectivity is mainly
attributed to a few nodes with high degrees functioning as hubs
that connect many other nodes. To demonstrate it, we repeatedly
removed from GE+I a node with the highest degree, thereby
monotonically decreasing the size of the largest WCC of GE+I, until
no nontrivial WCC could be found. This process is characterized by
the curve in Fig. 5, which approximates a straight line on a loglog
scale, i.e., the size of the largest WCC decreases polynomially fast.
For instance, after removing 3 and 10 nodes, the size of the largest
WCC decreases from 1332 to 526 and 190, by 60.5% and 85.7%,
respectively. By comparison, a resilient graph, such as the one
under a hyperlink analysis of the Web [11], still contains a giant
WCC comprising 29.1% of the nodes even after removing all the
nodes with indegree 5 or higher. Therefore, in view of the large
but fragile WCC as well as the large number of isolated nodes, we
conclude that the Web of linked vocabularies is in its youthbeing
only partially and sparsely linked.

Last but not least, from Table 3 we derive that although GI
contains fewer edges than GE, its connectivity seems better since
it contains fewer isolated/source/sink nodes, and more and larger
nontrivial SCCs/WCCs. The implication here is that in GE and GI,
edges are distributed in very different manners, which we will
investigate in the next set of experiments.

Fig. 6. Cumulative distributions of clustering coefficients of the nodes in GE , GI and
GE+I (on a loglinear scale).

4.3.4. Cluster analysis

Leaf nodes have no positive contribution to the clustering
coefficient of a graph, but they may notably influence the
connectivity. According to Table 3, GE contains 1741 isolated nodes,
more than 1339 in GI; on the other hand, GI contains 1186 leaf
nodes, much more than 250 in GE. In fact, out of the isolated nodes
in GE, 327 (18%) become leaf nodes in GI. In particular, 220 of
these leaf nodes are members of the largest WCC of GI. Mainly for
this reason, GI exhibits higher connectivity (e.g. containing more
and larger nontrivial WCCs) than GE, despite comprising fewer
edges. That is, GI is more economical in terms of the connectivity
gained versus the edges paid, which suggests that in tasks such as
crawling, term-level references are preferable.

Fig. 6 depicts the cumulative distributions of clustering coefficients of the nodes in GE , GI and GE+I. GI and GE+I are quite similar
in this regard, and they go almost consistently above GE, i.e., the
nodes in GI (as well as GE+I) tend to cluster more tightly than the
nodes in GE. In fact, the clustering coefficient of GI is 0.034, roughly
3 times higher than 0.012the one of GE. Therefore, by merely
looking at references provided by meta-descriptions of vocabular-
ies, one may significantly underestimate the degree to which vocabularies tend to cluster at the term level.

Although the three curves in Fig. 6 decrease rapidly (which
are on a loglinear scale), we still argue that the nodes in each
of these graphs are highly clustered. To demonstrate this, we
leveraged the well-known ErdosRenyi model [15] to separately
generate 10,000 random graphs having exactly the same number
of nodes and edges as GE , GI and GE+I. We found that on average,
the clustering coefficients of these random graphs are 0.00019,
0.00010 and 0.00032, which are 63340 times lower than the ones
of GE , GI and GE+I, respectively.

Since vocabularies tend to cluster together, we performed
graph clustering on GE+I to reveal its cluster structure. Recall
that GE+I comprises a single dominant WCC (denoted by C max
E+I )
and many other small WCCs, so we focus on C max
E+I . For simplicity,
we transformed C max
into an undirected graph by ignoring edge
E+I
directions and merging parallel edges. We applied to C max
E+I a
widely adopted graph clustering algorithm introduced in [16]. To
be specific, we repeatedly removed from C max
E+I an edge with the
highest betweenness, thereby partitioning C max
into more and
E+I
more CCs, the nodes in each forming a cluster. The betweenness of
an edge is defined as the number of shortest paths between pairs
of nodes in the graph that go through it. After this process, we
actually obtained a series of clusterings, of which we intended to
find the best one. We followed [16] to compute the modularity of
each clustering, which is a measure of the quality of a clustering
and we refer the reader to [16] for its definition; in short, higher
modularity indicates better clustering. The results for the initial

GEGIGE+18

G. Cheng, Y. Qu / Web Semantics: Science, Services and Agents on the World Wide Web 20 (2013) 117

Fig. 7. Modularity of clusterings of C max
is 12.

E+I , which peaks when the number of clusters

Fig. 8. Cluster structure of C max
E+I , where each node represents a cluster whose radius
is proportional to the logarithm of the number of its constituent nodes in C max
E+I , and
the width of an edge is proportional to the logarithm of the number of cross-cluster
edges in C max
E+I .

phase of the process are depicted in Fig. 7, which peak when the
number of clusters is 12, implying that it is best to divide the nodes
in C max

E+I into 12 clusters.
Accordingly, Fig. 8 depicts the best cluster structure of C max
E+I ,
which was generated by using the Pajek program.17 Nodes in this
graph correspond to clusters, and the size of a node indicates
the number of members of the corresponding cluster. An edge
connecting two clusters represents cross-cluster edges in the
original graph, and its width characterizes the number of crosscluster edges, indicating the strength of the association between
clusters. We saw two large, strongly associated clusters located
at the center of the structure, namely Cluster #6 and #11, which
contain DC and FOAF, respectively, as well as vocabularies that
reference them. Among other non-leaf nodes in Fig. 8, Cluster
#5 basically consists of vocabularies for describing Web services;
Cluster #8 comprises a collection of vocabularies (including
Contact) published by W3C; Cluster #10 mainly contains several
vocabularies underlying the IRI data library.18 It is worth noting
that in each of the above three clusters, most vocabularies
were crawled from the same PLD, namely daml.org, w3.org and
columbia.edu, respectively.

To summarize, despite the noticeable isolation, most vocabularies in the real world still tend to gather together via references
particularly those at the term level. As a result, the dominant connected component they form shows a dual-core (viz. DC and FOAF)
structure, which connects several other communities. We consider

17 http://pajek.imfm.si/.
18 http://iridl.ldeo.columbia.edu/.

Fig. 9. Distribution of the number of labels of a term (on a loglinear scale).

that DC and FOAF are two natural choices for the hub of the Web
of linked vocabularies, because the former is a fundamental vo-
cabulary, and the later is for describing peoplea domain that is
probably the most relevant to human users. However, solely relying on such small and highly focused vocabularies to directly bridge
most of the other vocabularies for establishing semantic interoperability is not the best situation because many domain-specific
connections between vocabularies cannot be conveyed via them.
This gives rise to a call for not only a Web of linked vocabularies
but also one that goes beyond fully centralized, e.g. containing more
peer-to-peer references or being hierarchically structured.

5. Topical relatedness

Complementary to the structural aspect embodied in the references between terms, the description of a vocabulary also contains textual parts. For instance, a term is usually annotated with
one or more labels that give its human-readable name(s), which
associate machine-friendly formal models with human-friendly
real-world domains. In practice, it is possible that multiple vocabularies describe common or overlapping domains, and thus are re-
lated; whereas their relatedness may or may not be materialized
in the form of references between them, their textual descriptions
should exhibit a certain degree of overlap, e.g. containing common
labels of terms. We term such kind of relatedness derived from
common textual descriptions topical relatedness since it indicates
to what extent vocabularies share common topics.

In this section, firstly we analyze the use of labels in real-world
vocabularies, based on which we define a measure of topical relatedness between vocabularies. Then we characterize vocabularies
and their topical relatedness by using a graph model, and present
an empirical study of the graph structure formed based on our data
set.

5.1. Vocabulary labels

Textual descriptions of a term can be given in various ways, e.g.
as a concise human-readable label or a detailed human-readable
comment. Biased towards removing noise to achieve more precise
results of analysis, we only focused on the labels of terms. When
publishing a vocabulary, a standard way of annotating a term with
a label is by using the rdfs:label property. In our data set,
rdfs:label could be found in the descriptions of 289,455 terms
(63.5%), distributed in 1052 vocabularies (35.1%). This unsatisfying
coverage motivated us to consider more properties having similar
functions. As a result, by leveraging rdfs:label, dc:title and
84 inferred subproperties thereof (e.g. skos:prefLabel), Fig. 9
depicts the distribution of the number of labels found in the
description of a term on a loglinear scale. Unfortunately, there
are still 36.3% of the terms annotated with no label. The rest of

the terms (63.7%) are distributed in 36.2% of the vocabularies.
Specifically, exactly one label could be found for 58.2% of the terms;
only 5.4% of the terms are annotated with multiple labels, and the
largest number of labels that annotate a term is 28.

A recent analysis of the Web of data [17] estimated that
only 38.2% of the (non-information) resources are annotated with
labels. By comparison, here when restricted to the schema level
(i.e. terms), the situation is much better. However, because the
absence of label was still commonly observed, we considered to
also employ the local name of each term as a label, which is basically
a suffix of the URI of the term obtained by removing the namespace
part. Finally, for each term t, let Lbl(t) denote the set comprising
the local name and all the labels of t, to be used in the experiments.

5.2. Relatedness measurement

We measure the topical relatedness between two vocabularies
by adding up, in a sophisticated way, the lexical similarities
between their constituent terms. Firstly, we leverage a widely used
string metric [18] (denoted by ISub) for the comparison of two
labels of different terms (denoted by li  Lbl(tm) and lj  Lbl(tn)),
whose range is inside the interval [1, 1]. We normalize it to be
inside the interval [0,1] by defining:
ISubNorm(li, lj) = ISub(li, lj) + 1

(2)
Considering that a term may be annotated with multiple labels
including its local name, we take the maximum ISubNorm value
between all pairs of the respective labels of two terms (denoted
by tm and tn) as their lexical similarity:
LexSim(tm, tn) =
We argue that a set of terms (denoted by Tp) has a wide coverage
of the topics of another set of terms (denoted by Tq) if for each
term tn  Tq there is some tm  Tp that is lexically similar to tn.
Accordingly, we define the topical coverage of Tq by Tp as follows:
Cov(Tp, Tq) = 1
|Tq|

liLbl(tm),ljLbl(tn)

LexSim(tm, tn),

ISubNorm(li, lj).


max
tmTp

max

(3)

(4)

tnTq

which is an asymmetric metric. Two sets of terms are topically
related if they have a wide topical coverage of each other. To
implement this idea, we employ the harmonic mean to summarize
the two topical coverage values as the topical relatedness:
RT (Tp, Tq) = 2Cov(Tp, Tq)Cov(Tq, Tp)
Cov(Tp, Tq) + Cov(Tq, Tp)
Since it is senseless to compare a class with a property, we apply
the above metric not directly to two vocabularies (denoted by vs
and vt) but separately to their constituent classes (denoted by
Clss(vs) and Clss(vt )) and properties (denoted by Props(vs) and
Props(vt )), and then take a linear combination of the results:
RT (vs, vt ) = C
C + P
+ P
C + P

RT (Props(vs), Props(vt )),

RT (Clss(vs), Clss(vt ))

(5)

(6)

where RT (Clss(vs), Clss(vt )) and RT (Props(vs), Props(vt )) are given
by Eq. (5), and

C =0
P =0

if Clss(vs) = Clss(vt ) = ,
otherwise.
if Props(vs) = Props(vt ) = ,
otherwise.

(7)

Finally, in the experiments we employed Eq. (6) to measure the
topical relatedness between every pair of vocabularies in our data
set, whose range is inside the interval [0, 1], and a larger value
indicates higher relatedness.

Fig. 10. Decreasing curve of the number of edges in GT when increasing the topical
relatedness threshold (on a loglinear scale).

5.3. Graphic representation

We intend to analyze the topical relatedness between vocabularies in our data set by also using a graph model, as we have
done in the analysis of declarative relatedness. Firstly, we represent each vocabulary by a node within a graph (denoted by GT ).
Then, because the topical relatedness between two vocabularies is
given in the form of a numerical value, in order to consistently analyze unweighted graphs in this article, we need to find a threshold
(denoted by T  [0, 1]) so that an undirected edge will be added
to GT to connect two nodes if the topical relatedness between their
corresponding vocabularies is not lower than T . As a result, GT will
be an edge-unlabeled simple undirected graph.

Our choice of T is based on the distribution of topical relatedness values. Fig. 10 depicts on a loglinear scale how many edges
will appear in GT under different settings of T from 0 to 1 with
0.01 increments. Whereas generally the number of edges decreases
rapidly when increasing T , there is a noticeable stable interval
from 0.54 to 0.77 during which when increasing T by 0.01, the
number of edges decreases by less than 1%; that is, relatively few
pairs of vocabularies in our data set have topical relatedness inside
this interval. This wide interval naturally divides (most) pairs of vocabularies into two groups: highly related ones (i.e. RT  0.77) and
lowly related ones (i.e. RT  0.54). Therefore, we set T = 0.655,
the median of this interval.

5.4. Graph analysis

The graph GT constructed based on our data set comprises
2996 nodes corresponding to vocabularies and 98,052 undirected
edges that connect every pair of nodes (i.e. vocabularies) whose
topical relatedness given by Eq. (6) is not lower than 0.655. In the
following, we analyze this graph by using standard measures for
graph analysis to study how vocabularies in the real world are
related in terms of describing common topics.

5.4.1. Degree analysis

In GT , the degree of a node indicates the number of other
vocabularies that are, to a great extent, topically related to the
corresponding vocabulary. In our data set, the average degree
observed is 65.46, which tells that for a vocabulary on average,
surprisingly several dozens of other vocabularies about largely
overlapping topics could be found. However, further analysis
revealed that the median is only 3.0, and the high average is caused
by a few nodes with very high degrees. For example, the highest
degree observed is 300, shared by 301 nodes; another 293 nodes
are with degree 292. Interestingly, all of these central nodes were
crawled from the same PLD, namely columbia.edu; we will return
to these nodes later in the article.

G. Cheng, Y. Qu / Web Semantics: Science, Services and Agents on the World Wide Web 20 (2013) 117

Fig. 11. Degree distribution of GT (on a loglog scale).

Fig. 13. Cumulative distribution of clustering coefficients of the nodes in GT .

By looking into the largest CCs, we found that C 1

T , the situation is much the same, and the nodes in C 2

T consists of
near-duplicate vocabularies that offer properties for annotating
some data sets in the IRI data library; some of them actually
represent different versions of the same vocabulary, and show no
significant difference. As a result, these vocabularies exhibit very
high topical relatedness between each other, making C 1
T a complete
graph. They are also exactly the nodes with the highest degree
(i.e. 300) in GT , as mentioned in the previous degree analysis. As
to C 2
T are
exactly those with the second highest degree (i.e. 292) in GT . This
observation confirms the necessity of near-duplicate detection
in applications like a vocabulary search engine, and suggests
our metric for a suitable solution. By comparison, C 3
T shows a
wide variety of sourcescontaining vocabularies crawled from
41 PLDs. Most of these vocabularies describe academic concepts
such as publication and university, including different variants
of a bibliographic vocabulary used in the OAEI campaigns19 for
evaluating ontology matchers. It reflects that academic matters
have received the most widespread attention from vocabulary
publishers.

5.4.3. Cluster analysis

Since we chose a high value for T , in this setting, if a vocabulary
is topically related to more than one vocabulary, it is most likely
that these neighbors in GT are also topically related to each other,
leading to a high clustering coefficient. This has been verified by the
cumulative distribution of clustering coefficients of the nodes in GT
as shown in Fig. 13. We observed 1066 isolated and 305 leaf nodes,
whose clustering coefficients are defined to be zero. However, out
of the remaining 1625 nodes, the clustering coefficients of 1595
(98.2%) are 0.5 or higher; in particular, for 1329 nodes (81.8%), their
neighborhoods are all complete graphs, resulting in a clustering
coefficient of 1. Hence, the clustering coefficient of the entire
graph is 0.52, which is much higher than 0.022the average of the
clustering coefficients of 10,000 random graphs generated by the
ErdosRenyi model that have exactly the same number of nodes
and edges as GT .

To reveal the global cluster structure of vocabularies, under a
given T , we can obtain a natural clustering by treating each CC
of GT as a cluster. We have performed this under T = 0.655 as
discussed in Section 5.4.2. Further, by setting T to different values
from 0 to 1 with 0.01 increments, we obtained different variants of
GT and correspondingly a series of clusterings. Fig. 14 characterizes
this process by presenting the increase of the number of CCs and
the decrease of the size of the largest CC of GT , which can be
divided into three phases. In the first phase, when increasing T

19 http://oaei.ontologymatching.org/.

Fig. 12. Size distribution of the CCs of GT (on a loglog scale).

On the other hand, GT contains 1066 isolated nodes, constituting around one third of all (35.6%); being isolated implies that
each of these vocabularies is about a relatively distinctive topic. We
found that several popular vocabularies such as DC, FOAF and Geo
fall into this category. One possible explanation is that these vocabularies are prominent in their respective domains so that no further attempt has been made to develop other vocabularies about
the same topics.

To elaborate on the extremely uneven degree distribution of
GT , Fig. 11 depicts it on a loglog scale. To summarize at a coarsegrained level, apart from one third of the nodes being isolated,
about one third (31.4%) are with low degrees between 1 and
10, and another one third (33.0%) are with degrees higher than
10. However, we were not able to identify the specific model
underlying this distribution.

5.4.2. Connectivity analysis

In the context of GT , a nontrivial CC captures a group of vocabularies concerning overlapping topics. In particular, setting T to
a value as high as 0.655 in our experiments requires considerable
overlap of topics among connected vocabularies.

We identified 1302 CCs from GT , of which 1066 are trivial and
236 are nontrivial ones. Such a large number of CCs suggests a
remarkable diversity of the topics covered by existing vocabularies
on the Web. Fig. 12 depicts the size distribution of these CCs, which
approximates a power law but with a long tail. On the one hand,
most of the nontrivial CCs (89.8%) comprise fewer than 10 nodes.
So for most topics, there are only a few (usually one) relevant
vocabularies that can be found on the Web. On the other hand,
given C k
T contain
301, 293 and 167 nodes, constituting 10.0%, 9.8% and 5.6% of the
nodes in GT , respectively. That is, a small number of hot topics have
attracted noticeably more interests, but no single dominant topic
was observed.

T denoting the k-th largest CC of GT , C 1

T , C 2

T and C 3

structure is the hub Cluster #3, which is relatively large and about
broad topics. Connected with it, there are not only single clusters
about relatively distinctive topics such as Cluster #8 on pizza, but
also groups of interrelated clusters such as Cluster #6 and #37 on
bibliography and Cluster #7 and #18 on geography. We also observed topic shifts along paths, e.g., from the hub to Cluster #9 on
people and further to Cluster #39 on people and pets. On the other
hand, several noticeably large clusters are disconnected from the
main body of the structure, which mainly comprise vocabularies
underlying the IRI data library crawled from columbia.edu. Some
of these clusters such as Cluster #2 become isolated, when some
others such as Cluster #1 and #4 gather together into a small com-
munity. Therefore, although the vocabularies contributed by this
single source constitute a considerable portion of our data set, they
have been identified as outliers in the cluster structure by our
approach.

6. Distributional relatedness

Vocabularies are defined to conceptualize real-world domains,
and then, are used (a.k.a. instantiated) as the schema level of Web
data. The openness of the Web enables data publishers to freely
choose vocabularies. In consideration of the complexity and diversity of practical tasks, multiple vocabularies may be instantiated
in a single application, indicating a degree of proximity between
them, e.g. describing related domains. We term it distributional
relatedness between vocabularies, inspired by the research on linguistics [19] which studies the relatedness between words by investigating their co-occurrence in text. Distributional relatedness
comes from the use of vocabularies, different from declarative and
topical relatedness which are computed based on the (structural
or textual) contents of vocabularies defined by their publishers.

In this section, firstly we analyze the use of vocabularies over
the Web, based on which we define a measure of distributional
relatedness between vocabularies. Then we employ a graph model
to represent vocabularies and their distributional relatedness, and
empirically study the graph structure obtained from our data set.

6.1. Vocabulary instantiation

Instantiating a term is to create its instance. In RDF, an RDF
triple instantiates its object which is a class when its predicate
is rdf:type; every RDF triple instantiates its predicate which
is a property. A vocabulary is instantiated if at least one of its
constituent terms is instantiated.

The effectiveness of applying distributional relatedness measures to vocabularies primarily depends on how many vocabularies have been instantiated. Distributional measures would fail
if vocabularies rarely had instances. In our data set, instantiation
has been found for 115,707 classes (29.2%) and 25,963 properties
(43.4%); that is, most terms have no instance on the Web of data.
However, instantiated terms are distributed in 1874 vocabularies (62.6%); that is, most vocabularies contain instantiated terms,
which supports the applicability of distributional measures. It is
worth noting that we only considered instantiation within explicitly stated RDF triples; more instantiations would be found if reasoning were performed to infer more RDF triples.

Further, we are interested in the extent to which term and
vocabulary instantiations are distributed, and measured this by
counting PLDs that offer such instantiations. We computed at
the level of PLD instead of RDF document or RDF triple in order to avoid bias raised by the uneven distribution of RDF documents as reported in Section 3.1. Fig. 16(a)(c) plot on a loglog
scale the distributions of the number of PLDs offering instances
of a class, a property and a vocabulary, respectively. All the three

Fig. 14.
the largest CC of GT when increasing the topical relatedness threshold.

Increasing curve of the number of CCs and decreasing curve of the size of

Fig. 15. Cluster structure of GT under T = 0.59, where each node represents a CC
of GT whose radius is proportional to the logarithm of the size of the CC, and edges
connect CCs that are parts of the same CC under T = 0.46. CCs comprising fewer
than five nodes are omitted.

from 0 to 0.46, the size of the largest CC decreases slowly from
2996 to 2332, still constituting 77.8% of the nodes, accompanied
by other 503 CCs, the largest of which comprises only 10 nodes.
That is, a single dominant cluster remains when hundreds of
small clusters about topics far away from the center has been
split. This dominant cluster consists of vocabularies crawled from
221 PLDs, constituting 84.7% of the 261 PLDs that were found to
have contributed vocabularies to our data set, still holding a wide
variety of sources. In the second phase, when continuing increasing
T from 0.46 to 0.59, the size of the largest CC decreases quickly
from 2332 to 310, constituting only 10.3% of the nodes, all of which
were crawled from the same PLD (viz. columbia.edu), and GT is
further fragmented into 1128 CCs. It is implied that no dominant
cluster remains, and the vocabularies are distinctly grouped by
topics. Finally, in the third phase, when further increasing T , more
and more CCs emerge but the size of the largest CC changes slightly.
We interpret this phase as a subdivision of some topically related
vocabularies according to subtopics.
Based on the above analysis, the most interesting clustering
seems to be the one under T = 0.59. Fig. 15 depicts the corresponding cluster structure, where each node represents a cluster
(i.e. a CC of GT ). Since GT is decomposed into 1128 CCs in this set-
ting, all the CCs comprising fewer than five nodes are omitted for
better presentation, leaving 67 CCs of considerable size. Besides,
to characterize the topical relatedness between clusters, we connected two clusters by an edge if they are parts of the same CC
under T = 0.46. We chose this value because it separates the first
and the second phases as discussed previously, i.e., isolating distant
topics but keeping major topics connected. As shown in Fig. 15, basically the clusters form a star-shaped structure. At the center of the

G. Cheng, Y. Qu / Web Semantics: Science, Services and Agents on the World Wide Web 20 (2013) 117

(a) Class.

(b) Property.

(c) Vocabulary.

Fig. 16. Distributions of the number of PLDs offering instances of (a) a class, (b) a property and (c) a vocabulary (on a loglog scale).

distributions are extremely uneven. According to Fig. 16(a), although instances of two classes (viz. foaf:Person and rdf:Seq)
were observed in more than 1 thousand PLDs, 96.7% of the instantiated classes have instances offered by only one PLD. For
properties, as presented in Fig. 16(b), the situation is similar,
containing 95.9% of the instantiated properties being instantiated in only one PLD; on the other hand, popular properties having instances distributed in more than 1 thousand PLDs include
rdf:type, dc:creator, foaf:name, dc:date, rdfs:label,
foaf:homepage and foaf:mbox_sha1sum. As to the vocabulary level depicted in Fig. 16(c), 92.6% of the instantiated vocabularies are instantiated in only one PLD; in particular, 87.7% of the
instantiated vocabularies are only used by their publishers, that is,
each of these vocabularies and all of its instances were crawled
from the same PLD. To conclude, most vocabularies on the Web
have not been used by data publishers other than their own pub-
lishers, when a few ones have been widely adopted.

6.2. Relatedness measurement

In computational linguistics, distributional measures [19] define closely related words as those used in similar contexts, e.g. cooccurring with many common words in a corpus. We follow this
concept to define distributional relatedness between vocabularies
on the Web. Specifically, given V , a collection of vocabularies, a
distributional profile (denoted by DP(v)) is created for each v  V ,
which is a|V|-dimensional vector characterizing the strength of association between v and every vocabulary in V in terms of the frequency of co-occurrence. Given D, a collection of RDF documents,
we define the i-th component of DP(v) as the following fraction,
which represents a conditional probability:

DPi(v) = |{PLD(d)| d  D and v, vi  IV(d)}|
|{PLD(d)| d  D and v  IV(d)}|

(8)

where PLD(d) returns the PLD where d was crawled, and IV(d)
comprises all the vocabularies instantiated in d. The denominator
is the number of PLDs that offer RDF documents instantiating
v, and the numerator is the number of PLDs that offer RDF
documents instantiating both v and vi (i.e. the co-occurrence of v
and vi). Finally, we measure the distributional relatedness between
vocabularies by calculating the cosine similarity between their
distributional profiles; the range is inside the interval [0, 1], and
a larger value indicates higher relatedness.

In our experiments, the three language-level vocabularies were
removed from V prior to processing, because on the Web of data,
their constituent terms which are widely and trivially instantiated
can be compared to the function words in natural language.

Fig. 17. Decreasing curve of the number of edges in GD when increasing the
distributional relatedness threshold (on a loglinear scale).

6.3. Graphic representation

Analogous to the analysis of topical relatedness, we represent vocabularies and their distributional relatedness by an edgeunlabeled simple undirected graph, denoted by GD, where each
node represents a vocabulary and an edge connects two vocabularies if their distributional relatedness is not lower than a threshold,
denoted by D  [0, 1]. In our experiments, after excluding the
1122 vocabularies that have no instance and the three languagelevel vocabularies, GD contains 1871 nodes.

To help find an appropriate value for D, Fig. 17 depicts, on a
loglinear scale, the number of edges appearing in GD under different settings of D from 0 to 1 with 0.01 increments. Different
from Fig. 10, the distribution here contains no stable interval in the
middle of [0, 1] (where every increase of 0.01 in D decreases the
number of edges by less than 1%) that is noticeably long (i.e. not
shorter than 0.10) to gracefully divide pairs of vocabularies into
highly and lowly related ones. Therefore, we simply set D to 0.5,
indicating a medium level of relatedness.

6.4. Graph analysis

The graph GD constructed based on our data set comprises
1871 nodes representing vocabularies and 292,497 undirected
edges connecting vocabularies that have distributional relatedness
not lower than 0.5. In the following, we analyze this graph by
using standard measures for graph analysis to study vocabulary cooccurrence and the induced relatedness.

6.4.1. Degree analysis

In GD, the degree of a node represents the number of other
vocabularies that have similar distributional characteristics. In our

Fig. 18. Degree distribution of GD (on a loglog scale).

Fig. 19. Size distribution of the CCs of GD (on a loglog scale).

data set, the average degree is as high as 312.66, and the median
is 130.0. In particular, 737 vocabularies are with degree equal
to or higher than 659, and the highest degree observed is 737.
Surprisingly, all such vocabularies were crawled from the same
PLD, namely columbia.edu. The reason they have quite similar
distributional profiles to each other is that basically they cooccur with IRIDL and several other common vocabularies. Not
like the three language-level vocabularies, the constituent terms
of these common vocabularies are only locally instantiated and
thus did not draw our particular attention; for example, in our
data set, all the RDF documents instantiating IRIDL were crawled
from columbia.edu. Therefore, it remains a question whether
such vocabularies should be removed prior to processing, but it
still motivates future work on more sophisticated distributional
measures than the standard one we adopted, e.g. penalizing
the relatedness brought by the co-occurrence with common
vocabularies that are frequently met.

We also found 394 isolated nodes (21.1%) in GD having no considerably similar vocabulary in terms of distributional character-
istics. In fact, 390 of these vocabularies never co-occur with any
other vocabulary (except for the language-level ones). Their instantiations are distributed in RDF documents crawled from 92 PLDs,
constituting only 1.9% of the 4906 PLDs that offer instantiations of
non-language-level vocabularies. In other words, most data publishers have used a combination of multiple vocabularies, thereby
demonstrating the practical motivation of studying vocabulary co-
occurrence.

Finally, Fig. 18 depicts the degree distribution of GD on a loglog
scale, which is similar to the degree distribution of GT shown in
Fig. 11 in the sense that both of them basically decrease in the
initial phase but reach the maximum value in the end. Modeling
their distributions will be a major work in the future.

6.4.2. Connectivity analysis

A nontrivial CC of GD captures a group of distributionally related
vocabularies, which in practice are used in quite similar contexts
and form a community, although they do not necessarily share
overlapping topics as a CC in GT .

In our experiments, GD is decomposed into 444 CCs, including
394 trivial ones corresponding to isolated nodes and 50 nontrivial
ones. Fig. 19 depicts their size distribution. Despite two very large
ones, other nontrivial CCs are relatively very smallcontaining 30
or fewer vocabularies.

The largest CC, denoted by C 1

D, consists of 738 vocabularies,
all of which were crawled from columbia.edu. They are basically
the nodes with the highest degrees in GD and are connected to
each other, as we have discussed in Section 6.4.1, thereby falling
into the same CC. Interestingly, IRIDL is not a member of C 1

because it co-occurs with hundreds of the vocabularies in C 1
D so

that its distributional profile looks quite different from theirs.
This reminds that direct co-occurrence is a different concept from
distributional relatedness, since the latter also considers the cooccurrence with third-party vocabularies. In addition, although
the vocabularies from columbia.edu constitute a considerable
portion of our data set, they are rarely used by the majority of
Web data publishers and were successfully separated from others
by our approach, since C 1
D contains no vocabulary from outside
columbia.edu.

By comparison, the second largest CC, denoted by C 2

D, exhibits
a wide variety of sources, which consists of 509 vocabularies
crawled from 106 PLDs, including many well-known vocabularies
such as DC, FOAF, Geo and those on DBpedia. Considering that
other CCs either sourced from a single PLD (e.g. C 1
D) or are of
an insignificant size, C 2
D is believed to capture the mainstream
vocabularies adopted by the majority of Web data publishers. We
will particularly look into this CC later in the article.

Last but not least, those trivial and nontrivial but small CCs are
not negligible, which collectively constitute 33.4% of the instantiated vocabularies. However, their instantiations are distributed in
RDF documents crawled from 112 PLDs, constituting only 2.3% of
the PLDs offering instantiations of non-language-level vocabular-
ies. That is, a very small portion of data publishers have promoted
the use of a considerable portion of vocabularies that are rarely
used in other contexts.

6.4.3. Cluster analysis

We have defined the distributional relatedness between vocabularies as the similarity between their distributional profiles. Ac-
cordingly, if a vocabulary is distributionally related to more than
one vocabulary, these neighbors of a common node in GD are probably distributionally related to each other since their distributional
profiles are also similar to a certain degree. Therefore, the nodes
in GD are expected to have high clustering coefficients, as demonstrated by the cumulative distribution of clustering coefficients of
the nodes in GD shown in Fig. 20. Except for the 440 isolated or leaf
nodes whose clustering coefficients are zero, 96.9% of the remaining nodes have a clustering coefficient equal to or higher than 0.5,
including 85.4% at the level of 0.9 or higher. The clustering coefficient of the entire graph is 0.72; by comparison, the average of the
clustering coefficients of 10,000 random graphs generated by the
ErdosRenyi model that have exactly the same number of nodes
and edges as GD is only 0.167. That is, the nodes in GD are highly
clustered.

In Section 6.4.2 we have partially revealed the cluster structure
of GD, which comprises two large CCs (viz. C 1
D) and
hundreds of small ones. To better characterize it, we intend to
explore whether any weak distributional relatedness can be found
between these components. Fig. 21 depicts the number of CCs and
the size of the largest CC of GD under different values of D from 0 to

D and C 2

G. Cheng, Y. Qu / Web Semantics: Science, Services and Agents on the World Wide Web 20 (2013) 117

Fig. 20. Cumulative distribution of clustering coefficients of the nodes in GD.

Fig. 21.
the largest CC of GD when increasing the distributional relatedness threshold.

Increasing curve of the number of CCs and decreasing curve of the size of

0.5 with 0.01 increments. Unfortunately, when decreasing D from
0.5 to 0.01, few of CCs are merged and the number of CCs varies
slightly. That is, basically no notable distributional relatedness
can be found between these CCs to connect them. However, in
this process, the size of the largest CC increases sharply when
decreasing D to 0.47, which is mainly caused by the merger of C 1

and C 2
D, showing a considerable distributional relatedness between
them. To sum up, at a coarse-grained level, GD consists of two fairly
coupled communities (viz. C 1
D) and a number of isolated
small communities.

D and C 2

In consideration of the simple constitution of C 1

D (all crawled
from the same PLD), our further investigation will be confined to
D. By choosing higher values for D, C 2
the internal structure of C 2

is likely to be decomposed into multiple CCs, each being treated as
a cluster. To find an appropriate value for D to identify clusters,
Fig. 22 depicts the number of CCs and the size of the largest CC of
C 2
D under different values of D from 0.5 to 1 with 0.01 increments.
Different from Fig. 14 where the process can be clearly divided
into three phases based on the slopes of the curves (from which
we identified an interesting cluster structure), here the number of
CCs (resp. the size of the largest CC) increases (resp. decreases) at
stable rates, expect for two singular points when increasing D to
0.62 and to 0.72 where the largest CC breaks down to a lot of pieces.
Therefore, we could not infer any remarkable clustering from the
two curves. On top of this, Fig. 22 also depicts the size of the second
largest CC of C 2
D. When increasing D, the largest CC is consistently
much larger (4.481.3 times) than the second largest one before
both of them go down to very small sizes when D  0.76. That
is, a unique dominant cluster is observed at all times except for
extremely high values of D. All the above findings inspire a singlecore structure of C 2
D, where relatedness values decrease from the
interior to the exterior along with the increase of the distance from

Fig. 22.
the largest CC of C 2

Increasing curve of the number of CCs and decreasing curve of the size of

D when increasing the distributional relatedness threshold.

the core (just as the air density of the Earths atmosphere). When
increasing D, outer vocabularies are peeled off progressively;
finally under D = 0.75, the remaining vocabularies in the core
include FOAF, Geo, Contact and dozens of other famous ones.

7. Relatedness correlation

We have discussed three kinds of relatedness between vocabularies from different perspectives. One natural question arises as
to whether these perspectives are independent or exhibit a degree
of correlation. To answer it, we require every relatedness measure
to define a numerical value for every pair of vocabularies, based on
which we can obtain a ranking of all pairs of vocabularies. Then we
employ the well-known Spearmans rank correlation coefficient
(denoted by   [1, 1]), which assesses the degree of agreement
between two rankings, to compare the ranked relatedness values
given by different measures. The sign and absolute value of  indicate the direction and strength of association between rankings,
respectively.

Our measures of topical and distributional relatedness (denoted
by RT and RD, respectively) exactly produce numerical values.
As to declarative relatedness, our analysis was based on the
references between vocabularies, from which binary relatedness
values can be derived (i.e. 1 for those having reference and 0 for
the others). However, the absence of reference between a pair of
vocabularies should not be simply interpreted as a zero relatedness
value because they may be indirectly connected by a sequence of
references, which can be regarded as a kind of weak relatedness.
To implement this idea, taking explicit reference as an example,
we define an edge-weighted undirected simple graph, denoted by

E , where each node corresponds to a vocabulary and every pair of
nodes (denoted by vi and vj) are connected by an undirected edge
weighted by a numerical value denoted by w(vi, vj), which equals
1 if vi and vj explicitly reference each other, 2 if either explicitly
references the other, or + if neither explicitly references the
other; then, the explicit declarative relatedness (denoted by RE)
between two vocabularies is defined as the reciprocal of the weight
of a shortest path between their corresponding nodes in GW
E . That
is, higher relatedness values are given to those having mutual
references and to those having a shorter distance in the reference
graph. When considering implicit reference and both (explicit and
implicit), we can define relatedness measures (denoted by RI and
RE+I, respectively) in an analogous manner.

We applied the above five related measures (viz. RE , RI , RE+I ,
RT and RD) to all pairs of vocabularies in our data set excluding
those involving the three language-level ones. Table 4 presents
the  values between the ranked relatedness values given by
these measures, all of which are positive, showing that all our
measures are positively correlated. In particular, strong correlation

Table 4
Spearmans rank correlation coefficients between the ranked relatedness values
given by different measures.

RE+I

RE+I

was observed between RI and RE+I ( = 0.88), and between RE
and RE+I ( = 0.53), which are not surprising since RI and RE are
actually parts of RE+I. An interesting finding is the very high value
between RE and RD ( = 0.66), indicating that explicitly referenced
vocabularies are also likely to be instantiated in similar contexts,
and vice versa. On the other hand, the lowest  values belong to
those involving RT ; that is, topical relatedness is relatively far away
from the others. Based on our analysis in Section 5.4.2, we attribute
this to the fact that many topically related vocabularies are either
independent copies of the same underlying conceptualization or
developed by different publishers to describe a common domain;
in both cases, they are neither likely to be referenced to each other
nor to be used in the same context.

8. Related work

8.1. Vocabulary relatedness

The origin of the study of vocabulary relatedness can be traced
back to computational linguistics, where researchers seek measures of relatedness between words in natural language. Budanitsky et al. [20] compared a spectrum of measures that employ a
semantic network such as WordNet by mapping words to concepts
in the network and calculating their distance in the network, the
depth of their least common ancestor in the is-a hierarchy, etc. The
Semantic Web community have extended these ideas to measure
the relatedness between terms in a vocabulary [21,22], by treating
the description of a vocabulary as a semantic network comprising terms and their relations (e.g. is-a, part-of). Another line of research [19] derives distributional relatedness between words from
the common contexts where they are used, by analyzing the cooccurrence of words in a corpus. Recently, researchers tended to
regard the Web as a corpus, and access it by exploiting Web search
engines [23,24]. This idea has been applied to measuring the similarity between terms in different vocabularies [25].

From a more coarse-grained perspective, research effort (in-
cluding this article and its preceding work [5]) has also been
directed towards vocabulary-level relatedness. Ding et al. [3] distinguished between several types of references between vocabularies,
and gave them different weights in the proposed random surfer
model for ranking vocabularies. Allocca et al. [26] detected several types of relations between vocabularies such as similar-to and
is-previous-version-of, based on which they grouped the search results returned by a vocabulary search engine.

Maedche and Staab [27] specifically focused on the similarity between vocabularies. They proposed two methods: the first
one looking at the lexical similarities between constituent terms,
which is close to our topical relatedness measure, and the second
one comparing the conceptual structures of vocabularies, in particular the class hierarchies. Dellschaft and Staab [28] followed a
precision/recall-like style to aggregate term-level lexical similari-
ties. David and Euzenat [29] evaluated twelve similarity measures
including both textual and structural ones. David et al. [30] represented vocabularies and the mappings between their constituent
terms as a graph, where the distance between vocabularies indicates their similarity.

Different from the above work, in this article we tackled a
new taxonomy of relatedness measures. It covers not only the

structural and textual aspects of vocabularies (as in existing work)
via declarative and topical relatedness, respectively, but also, for
the first time, considers the use of vocabularies in practice via
distributional relatedness. It is worth noting that the employed
distributional relatedness is different from the notion of instancebased similarity (e.g. [31]), since the former operates on cooccurrence of term instantiations in a certain context (e.g. an RDF
document) when the later strictly requires similar terms to share
common instances.

8.2. Vocabulary analysis

Extensive work has been found in the literature on analyzing various aspects of vocabularies in various manners. Providing
statistics is a simple but effective strategy. As an early effort, Magkanaraki et al. [32] reported the sizes, depths and branch factors of
the class and property hierarchies of 28 RDFS vocabularies. Besides
the shape of class hierarchy, Want et al. [33] analyzed the use of
OWL constructs and expressivity of nearly 1300 RDFS and OWL vo-
cabularies. In a similar fashion, Abdul Manaf et al. [34] studied 478
SKOS vocabularies. Ding and Finin [8] distinguished their work by
counting the instantiations of terms.

A more powerful technique is to model and analyze the description of a vocabulary as a graph. Hoser et al. [35] represented
terms by nodes, connected subclasses to superclasses by edges, and
connected each property with its domain and range. Then the authors compared the graphic representations of SWRC and SUMO
in degree centrality, betweenness centrality, density, diameter and
eigenvectors. Theoharis et al. [36] separated the domain and range
relations from class hierarchy, and constructed two graphs for each
of the 250 vocabularies they collected. They investigated degree
distributions, self-loops and parallel edges in these graphs, and
studied the morphology of class hierarchies.

Recently, attention has been directed towards relations between
vocabularies. Cheng et al. [14] discussed term-level dependence,
which corresponds to implicit reference in this article. The
authors implemented a digraph model with more than 1 million
terms, and analyzed its degree distributions, reachability and
connectivity. Prior to this article, in a preliminary work [5] we
reported statistical properties of GE , GI and GE+I, and presented the
pairs of vocabularies that co-occur the most frequently. dAquin
et al. [37] clustered the vocabularies provided by the Watson
search engine based on their textual similarity (which is similar
to our topical relatedness), detected different versions of the same
underlying vocabulary based on their namespace URIs, and studied
the topology of the graphic representation of the owl:imports
relation (which is close to explicit reference in this article).

Ghazvinian et al. [38] focused on the mappings between vo-
cabularies. They constructed a digraph containing two hundred
nodes representing vocabularies and edges connect vocabularies
weighted in proportion to the number of mappings between the
constituent terms thereof. The authors studied the degree distributions and connectivity of this graph. Hu et al. [39] applied the same
idea to a larger data set consisting of four thousand vocabularies.
Compared with existing analysis of relations between vocabu-
laries, the work in this article is distinguished by a taxonomy of
three kinds of relatedness, which has addressed most of the previous work; in particular, we reported their correlation. We also employed a systematic collection of measures for graph analysis, including not only degree and connectivity but also cluster analysis.
Last but not least, we have made the data set used in this article
publicly available, so that the reader can validate our results or
conduct new experiments. Alternatively, as surveyed by dAquin
and Noy [1], there are many other online repositories where
existing vocabularies can be found.

G. Cheng, Y. Qu / Web Semantics: Science, Services and Agents on the World Wide Web 20 (2013) 117

9. Conclusions and future work

We have tackled a taxonomy of relatedness between vocab-
ularies, comprising declarative (explicitly and implicitly), topical
and distributional perspectives. Overall, they exploit not only the
structural and textual description of a vocabulary but also its use
on the Web of data. This taxonomy is defined based on the different forms of data on a vocabulary. Thereby, it is orthogonal
to many specific types of relations between vocabularies. For in-
stance, the similarity between vocabularies can be measured from
all the above three perspectives. This motivates an interesting future work on examining the correlation between our taxonomy
and those well-known relations such as similar-to and is-previous-
version-of as examined in [26]. We also intend to extend our taxonomy to include more perspectives derived from other sources of
information, e.g. the provenance of a vocabulary, and to explore the
applications of these relatedness measures apart from vocabulary
recommendation [5].

We have performed an empirical study of relatedness between
thousands of real-world vocabularies based on a large data set,
which shows to be a representative sample of the Web of data.
This ensures the significance of our experimental results. For each
perspective, we have constructed a graph model and employed
standard measures for graph analysis. Degree, connectivity and
cluster analysis lead to many high-level findings, which characterize vocabularies in the real world from various angles, and provide empirical evidence and inspiration to both researchers and
practitioners. As we have shown, although the three perspectives
are positively correlated, they exhibit a noticeable difference in
the graph structure, which is attributed to their different nature.
It demonstrates the necessity of splitting the graph analysis into
three facets. However, it would also be interesting to combine different perspectives into a single relatedness measure and analyze
the induced graph structure as future work. Another direction is to
validate our findings by repeating our experiments on other data
sets, to perform a comparative analysis.

We have linked some of our findings to their impacts on practical applications such as vocabulary crawling, ranking and match-
ing. However, this should not restrict the value of our experimental
results only to these areas. Actually, the essential conclusions we
have provided are just the results of performing graph analysis
by using standard measures, based on which we (hopefully) have
revealed certain laws of the Semantic Web. It is the readers responsibility to interpret, select and utilize these results in her own
context of research and development. In future work, exploring
their value for search applications will be our focus.

Acknowledgments

This work was supported in part by the NSFC under Grants
61100040, 61223003 and 61170068, in part by the JSNSF under
Grant BK2012723, and in part by the SSFC under Grant 11AZD121.
We thank Saisai Gong for his contribution to our earlier work, and
thank three anonymous reviewers for their helpful comments on
earlier drafts of this article.
