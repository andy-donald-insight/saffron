TRM  Learning Dependencies between Text
and Structure with Topical Relational Models

Veli Bicer1, Thanh Tran2, Yongtao Ma2, and Rudi Studer2

1 IBM Research, Smarter Cities Technology Centre

Damastown Industrial Estate, Dublin, Ireland

velibice@ie.ibm.com

2 Institute AIFB, Karlsruhe Institute of Technology
Geb. 11.40, KIT-Campus S ud, Karlsruhe, Germany
{duc.tran,yongtao.ma,rudi.studer}@kit.edu

Abstract. Text-rich structured data become more and more ubiquitous on the Web and on the enterprise databases by encoding heterogeneous structural information between entities such as people, locations,
or organizations and the associated textual information. For analyzing
this type of data, existing topic modeling approaches, which are highly
tailored toward document collections, require manually-defined regularization terms to exploit and to bias the topic learning towards structure
information. We propose an approach, called Topical Relational Model, as
a principled approach for automatically learning topics from both textual
and structure information. Using a topic model, we can show that our
approach is effective in exploiting heterogeneous structure information,
outperforming a state-of-the-art approach that requires manually-tuned
regularization.

Introduction

We study the problem of learning on text-rich structured data that shares these
main characteristics: it describes interconnected objects (relational ) of different types (heterogeneous) that are associated with textual attributes (text-rich).
Examples include graph-structured RDF data forming connected resource descriptions and relational database records containing textual values that are
connected via foreign keys.

Topic modeling (TM) approaches have shown to be effective for dealing with
text, which recently, have also been extended to deal with the combination of
textual and structured data [19]. However, when dealing with the text-rich
structured data as we consider as an input in this paper (as shown Fig. 1),
two problems mainly arise: First, such data mostly consists of a heterogeneous
structure such as many classes and relations each of which has varying effects
on different topics. Thus entities having different structure information in the
data need to have different topic distributions. For example, any Company entity
having product relation is more related to a topic about manufacturing than
another Company entity of being a movie distributor. Usually such correlations

H. Alani et al. (Eds.): ISWC 2013, Part I, LNCS 8218, pp. 116, 2013.
c Springer-Verlag Berlin Heidelberg 2013

V. Bicer et al.

between the topics and these structural elements (i.e. classes and relations) are
not one-to-one, but the latter can be correlated to one or more topics with
different proportions. Previous TM approaches are not well suited to handle such
complex correlations between the structure and text since they either consider
a homogeneous structure (e.g. social networks [5], citation networks [7] or Web
links [8]) or networks with a few types of relations [9, 7]. An alternative solution
to handle such complex correlations can also be applying Statistical Relational
Learning (SRL) techniques [10] which are naturally formulated as instances of
learning a probabilistic model (e.g. Markov Logic Network (MLN) [11] or (non-
)linear link functions [12]) from the data. However the main problem of directly
applying the SRL techniques into our setting is about handling textual data since
they consider the input data to be available in a sort of structured form (e.g. a
user-movie matrix indicating whether a user likes a movie). Only few works exist
that utilize SRL to learn from text-rich structured data [13, 12] but mainly suffer
from a large and complex structured network required for textual data. At this
point, using the latent topics as low-dimensional representation of textual data
provides a better way to incorporate textual information into the SRL models.
Yet another problem also arises due to the sparsity of these correlations between the topics and the structure. For example, in Fig. 2 the class City is highly
correlated to the topic t4 but not to the other three topics. Such sparsity is not
well addressed by the previous work, especially the ones focusing on heterogeneous networks (e.g. [9, 7]) which aims the topic smoothness instead of sparsity.
One exception to this is the focused topic model [14] which utilizes latent feature
models to control sparsity but it is an unsupervised approach and does not take
the structure information into account.
?

?

?
Fig. 1. An example data graph
?

?

?
In this paper, we propose Topical Relational Model (TRM) that uses relational information in structured data for topic modeling (text analysis tasks),
and also allows the learned topics to be employed as a low-dimensional representation of the text to capture dependencies between structured data objects.
The main novel aspects of this model are: (1) compared to previous SRL works,
TRM employs hidden topic variables to reduce model complexity. TRM uses
latent topics to represent textual information, targeting the specific case of textrich structured data. In this way, it provides a systematic way to learn lowdimensional features from text (i.e., topics) that can be used for SRL-related
tasks. We show in experiments that topics learned by TRM outperform the
features manually specified in previous SRL works [12, 11]. (2) Compared to
existing TM approaches, TRM is able to exploit the richness in relational information available in structured data. With TRM, the learning of topics recognizes
the heterogeneity of classes and relations associated with entities. While some
related TM work [9] requires manually defined regularization terms to exploit
this heterogeneous structure information, TRM captures correlations between
structure and topics through dedicated latent feature model parameter in order
to better handle the sparsity of the correlations. In experiments, we show that
leveraging relational information this way, TRM improves the performance of
state-of-the-art TM approaches [9, 15]. (3) TRM is unique in its hybrid nature:
it is a topic model that incorporates structure in addition to textual information;
at the same time, it is also a Bayesian network capturing relational dependencies
between text-rich objects that can be employed for SRL tasks.

Structure. We present the main ideas behind TRM in Sec. 2.1, TRM variables
in Sec. 2.2 and their dependencies in Sec. 2.3. Sec. 2.4 describes the generative
process, which is reversed in Sec. 2.5 for learning TRM. Experimental results
are presented in Sec. 3, followed by conclusions in Sec. 4.

2 Topical Relational Models

TRM supports different types of graph-structured data including relational, XML
and RDF data. The focus lies on text-rich data describing objects through textual attributes. More formally, the data is a directed graph G = (V,R) (see
Fig.1), where V is the disjoint union V = VC VE representing classes and enti-
ties, respectively, and R stands for binary relations between entities. The set of
classes an entity e belongs to is denoted C(e) = {c | type(e, c)} (type is a special
relation that connects an entity node with a class node), while the relations e
  VE}. In our text-rich
is involved in is R(e) = {r | r(e, e
data setting, every entity also has some textual attribute values such as name,
comment and description. To incorporate this, every entity node e  VE is
treated as a document, i.e., modeled as a bag of words e = {w1, . . . , w|e|}, which

, e)  R  e, e
?

?

?
), r(e

contains all words in the textual values of e.

V. Bicer et al.

t1

Organization(2.3)

Company(1.9)

s
e
s
s
a

l

Airline(1.2)
Person(0.5) C
company
employ
industry
business
associate
automotive
incorporation

s
d
r
o
w
p
o

merger
operate

corporation
headquarter

airline

keyPerson (5.2)

owningCompany(2.3)

owningCompany (0.9)

t2

s
e
s
s
a

l

s
d
r
o
w
p
o

Person(3.2)
Company(1.1)

Organization(0.8)

Scientist(0.3)

ceo

manager
engineer
university

work

president
founder
member
software

chief

company
trustee

product(1.7)

headquarter(1.8)

t3

t4

Automobile(4.6)

MoT(3.1)

Company(0.4) C

l

s
e
s
s
a

City(4.6)
Place(3.9)

s
e
s
s
a

Settlement(3.3) C

l

brand
product

car

automobile

market

manufacturer

motor
drive
bmw
engine
door
audi

s
d
r
o
w
p
o

population

city

location
large
people
capital

munich
urban
europe
center
million
state

s
d
r
o
w
p
o

successor (1.1)

successor (3.0)

manufacturer(3.1)

Fig. 2. An excerpt of the TRM result for DBpedia. It captures four TRM topics and
and their top ranked words. Further, classes that are highly correlated with individual
topics and relations that are highly correlated with pairs of topics are shown (strength
of correlation shown in brackets).

2.1 TRM
?

?

?
TRM is a topic model representing G through a set of topics T = {ti, . . . , tK}.
wV p(w | t) = 1
Each t  T is a probabilistic distribution {p(w | t)}wV , where

and V is the vocabulary of words. However, the context from which topics are
derived is not made up of plain words but also includes entities, classes and relations in G. This is reflected in the TRMs output, which includes topics as well as
their strength of correlation w.r.t. words, classes and relations. Fig. 2 illustrates
this. For instance, we can see that the related words employ and merger form
the topic t1. Further, t1 is not only drawn from words but also, correlates with
entities of the types Organization and Company. This topic (and its associated
words) correlates with other topics (words) such as t3 (brand and engine) in
the context of the manufacturer relation.

Because TRM also captures dependencies between structure elements (classes
and relations), it can also be seen as a SRL approach. However, it captures them
only indirectly using topics, which act as a low-dimensional abstraction of the
text. We now present the TRMs Bayesian network representation to show these
relational dependencies conditioned on topics.

2.2 Template-Based Representation of TRM

A template-based representation of Bayesian networks [16] is used to define TRM
as a set of template attributes and template factors.

Let X denotes some of the random variables, X = {X1, ..., Xn}, where each
Xi  X can be assigned a value from the range V al(Xi). Then, a template attribute (or attribute hereafter) is a function A(1, ..., k), whose range is V al(A),
and each argument i is a placeholder to be instantiated with values of a particular type. For example, there are the template attributes Company(1) and
headquarter(1, 2). The values used to instantiate i are drawn from the data,
?

?

?
called the object skeleton, O(A). Given O(A), the variables instantiating A is
XO(A) = {A(o) | o  O(A)}, where V al(Xi) = V al(A) for each Xi  XO(A).
For example, from Company(1) and O(Company) = {c1, c2, c3}, we obtain the
random variables XO(Company) = {Company(c1), Company(c2), Company(c3)}.

Template factors are used to define probability distributions over random
variables. They are templates because instead of ground variables, template attributes are taken as arguments. They return real numbers for assignments of
variables instantiated from template attributes, i.e., given a tuple of attributes

A special template factor is the conditional probability distribution, which splits

A1, ..., Al, a template factor f is a function from V al(A1)  ...  V al(Al) to R.
the attributes into two groups, Ac, AP a  {A1, ..., Al}, called child and parent
attributes.
Observed Variables. Elements in G are used to instantiate three template
attributes defined for observed variables, namely the class c, the relation r
and the entity-word assignment w. Their object skeletons are entities belonging to the type c, relations of the type r, and words in the entities bag-of-words
)  R} and
representation, i.e., O(c) = {e|c  C(e)}, O(r) = {(e, e
O(w) = {(e, v) | e  VE  v  e}. These templates are binary-valued func-
?

?

?
)|r(e, e
?

?

?
tions, indicating whether an entity, a relation instance or an entity-word assignment exists or not. For example, the variables Company(c1), product(c1, p1)
and w(c1, car) obtained for these templates model whether there is an entity c1
that is a company, p1 is a product of c1 and car is a word associated with c1,
respectively.
?

?

?
Observe that some entity-word assignments are dependent on a particular re-
lation. For instance, the probability of observing the assignment w(e, munich)
) indicating BMW, the company e, has its
is very high, given headquarter(e, e
headquarter in Germany, the country e
. Further, such dependencies may exist
only for some particular entities  e.g. not every entity that has its headquarter
in Germany contains the word munich but some other words representing other
cities in Germany. Instead of modeling dependencies between all observable variables (words and structure elements) directly, TRM models variables as being
dependent on hidden topic-related variables.
?

?

?
Hidden Topic-Related Variables. The hidden topic variables are captured
by the template attributes topic indicator b, topic proportion  and topic-word
assignment z. For these templates, we need object skeletons that also range
over the topics T . In particular, b is instantiated with entities and topics in the

skeletons O(b) = {(e, t)|e  VE, t  T}. It is a binary-valued function such that
for an entity e, bt(e) = 1 indicates that t is a topic of e. The vector of all topic
indicator variables of e is b(e) = bt1 (e), . . . , btK (e).

While b(e) is useful to determine which topics are present for an entity e,
it does not capture sufficient information to model the probabilities of entity
words. Following the tradition of topic modeling,  is introduced for modeling
entity words through a distribution of topics. While  is defined over the same
skeletons, it is different to b in that it is real-valued: for an entity e, t(e) returns a

V. Bicer et al.

Company(c1)









&

<

b(l1)

headquarter(c1,l1)

b(c1)

product(c1,p1)

b(p1)



d

 (c1)

z(c1,operate)

...

z(c1,car)

...

z(c1,bmw)

w(c1,operate)

...

w(c1,car)

...

w(c1,bmw)



nj







d





nj

(a)





U



G



G

<

G

G



(b)

Fig. 3. (a) Template-based representation around the entity c1 with observed variables
(dark) and hidden topic-related variables (light). (b) The generative process for two
entities shown in plate notation.
?

?

?
real number and (e) = t1 (e), . . . , tK (e) defines a per-entity topic distribution

such that

tT t(e) = 1.

The semantics of the per-entity topic-word assignment is the same as in LDA.
To capture this, we define a template attribute z that has the same skeleton as
w. However, instead of a binary value, it returns a topic for an entity-word pair,
i.e., z(e, v) = t indicates that the word v associated with e belongs to topic t.

Fig. 3-a depicts variables obtained by instantiating the templates with infor-

mation about the entity c1.

2.3 Probabilistic Dependencies in TRM

Central to TRM is the assumption that given the topic indicator vector of an
entity, all its random variables derived from class and relation attributes are
conditionally independent. That is, instead of capturing dependencies between
these structure variables directly, we propose to use hidden topics. We want
to capture that when entities exhibit structural resemblances, their topics shall
also be similar. Vice versa, given some topics, some structure elements are more
likely to be observed than others. We introduce the model parameters  and 
to capture the quantity of how much a particular structure variable depends on
a topic (pair of topics).

First, we consider that the probability of observing an entity e belonging to a
class c depends on its topic indicator vector b(e). We model this as a template
factor captured by a logistic sigmoid function defined over a linear combination
of topic indicators, i.e.,

p(c(e) | b(e)) = (T

c b(e)) = 

bti (e) ci

(1)




tiT



?

?

?
(2)
?

?

?
Topical Relational Models

1+ex is the logistic sigmoid mapping values from [, +] to
where (x) = 1
[0, 1] and  is a global parameter represented as a |VC|K matrix. Each element
ci in the vector c represents the strength of dependency between the class c
and topic ti.

Similarly, the probability of observing a relation r(e1, e2) is modeled via logistic regression over the topic indicator vectors b(e1) and b(e2). A template
factor over r(e1, e2), b(e1) and b(e2) is defined as

tk,tlT btk (e1)btl(e2)rkl and r is a K  K matrix.

where b(e1)T rb(e2) =
For any given two entities e1 and e2, where e1 has the topic indicator tk and e2
has tl, the weight of observing a relation r between these two entities is given as
the value of the cell (k, l) of the matrix r denoted as rkl.

Further, we employ the topic parameters  and z to bring words into this
picture. We want to capture that given some topics, some words are more likely
than others. This part essentially follows the idea of topic modeling behind LDA.
The only difference is that while LDA defines the topic proportion (e) over all
topics, (e) here is defined only over the topics captured by the corresponding
topic indicator vector b(e), i.e., topics not in b(e) have no density in (e) (this
creates a sparsity of topics similar to focused topic modeling [14]). To capture
this, we introduce the template factor p((e) | b(e)).

This is an important design decision: On one hand, in order to handle the
sparsity of dependencies that occur between the structure variables and the top-
ics, the topics indicated in b(e) determines the probability of observing structure
for the entity. On the other hand, the topic proportions of the entity in (e) is
governed by the topic indicator vector b(e) which in turn is determined by the
structure around the entity.

2.4 Generative Process

We specify the full joint distribution for TRM and the generative process so
that we can infer hidden variables from observed data in G. First, we start with
the vector b that specifies binary topic indicators for each entity. We use a
prior distribution over the possible values in b in order to capture our initial
uncertainty about the parameters. Obtaining such a prior is possible with the
Indian Buffet Process (IBP), which is a non-parametric Bayesian process used to
generate latent features via a Beta-Bernoulli distribution [17]. IBP assumes that
each entity e possesses a topic t with probability t, and that topic indicators
in b(e) are then generated independently. Under this model, the probabilities of
the topics are given as  = {1, ..., K}, and each t follows a Beta distribution
with hyperparameter , i.e., p(t | ) = Beta(/K, 1). Then, for an entity e,
each topic indicator value is sampled from a Bernoulli distribution as p(bt(e) |
t) = Bernoulli(t). IBP can be utilized for both finite and infinite number
of topics [18]. Using infinite number of topics dynamically has great benefits in
the case of an unsupervised topic learning so that number of topics gets larger
whenever the need arises. However, in our case we are mostly interested in the

V. Bicer et al.

words of the entity to be distributed only to those topics selected for the classes
and relations that entity has (no matter how many words the entity has, because
we want to bias the topics according to structure). Thats why, for the purpose of
this work, we set the number of topics to a fixed K so that variational inference
can be applied to learn the model in the exponential family [18]. For (e), we
set a Dirichlet prior over the topics just like in LDA. However, instead of using a
uniform hyperparameter, we parametrize the Dirichlet with topic indicators b(e):
p((e) | b(e), ) = Dirichlet(b(e)). As the number of selected topics varies
according to b(e), each entity will have a different density of topic proportions.
For the topic index z and word attribute w, the same process as defined for LDA
involving the hyperparameter  is used. We arrive at the following generative
process (see Fig. 3-b):

1. For each topic t = 1, 2, .., K:

(a) Draw t |   Beta(/K, 1).

2. For each entity e:

(a) For each topic t:

i. Draw bt(e) | t  Bernoulli(t)

(b) For each class c of entity e:
i. Draw c(e) using Eq. 1
(c) Draw (e) |   Dir(b(e))
(d) For each word v of entity e:

i. Draw topic index z(e, v) | (e)  M ult((e))
ii. Draw word w(e, v) | z(e, v), 1:K  M ult(z(e,v))
?

?

?
3. For each pair of entities e, e
?

?

?
(a) For each relation r  {r | r(e, e
?

?

?
:

i. Draw r(e, e

) or r(e

, e) using Eq. 2
?

?

?
), r(e
?

?

?
, e)  R}:

2.5 Learning

We propose to learn the posterior distribution of the hidden topic-related variables b, ,  and z conditioned on the observed variables w, c and r via variational Bayesian learning [19]. Intuitively, the variational method approximates
the posterior distribution of p by another simpler distribution q. In particu-
lar, through mean field approximation [19], we have q as a distribution that is
fully-factorized over the hidden variables indexed by the free variational parameters , ,  and  for Bernoulli, Dirichlet, Multinomial and Beta distribution,
respectively:

q(b, , , z | , , ,  ) =


q((e) | (e))
?

?

?
eVE

(e,v)O(w)

K

t=1
?

?

?

q(t | t1, t2)



?

?

?
eVE

q(bt(e) | t(e))



q(z(e, v) | (e, v))

(3)

These variational parameters are then fit such that q is close to the true
posterior of p, where closeness is measured by the KL-divergence, KL(q  p).
Because the decomposition log p(c, r, w) = KL(q  p) + L(q) and KL(q  p)  0
?

?

?


+

t2 = 1 + |O(b)| 

t(e)

t(e)
?

?

?
(e,t)O(b)

(e,t)O(b)

(5)

(6)

r2 + 

(7)

Topical Relational Models

hold [19], minimizing the KL-divergence is equivalent to maximizing the term
L(q), the variational lower bound on the log marginal likelihood. The learning
problem can then be expressed as optimizing
{, , , } = arg max
{,,,}

L(q)

(4)

For this, we use the 2-steps variational Bayesian EM algorithm. It takes the
fixed hyperparameters  and  and an initial choice of the model parameters , 
and  as inputs. Then, it iteratively updates the variational parameters , , 
and  until convergence in the E-step. Then, for fixed values of the variational
parameters, the model parameters ,  and  are iteratively computed in the
M-step. Thus, parameters are updated until convergence within the two steps,
and both steps are run until convergence in the outer loop of the EM.

Variational E-Step. Update equations for this step can be obtained by setting

the derivative of L(q) equal to zero. For each topic t  K, we compute t1 and
t2 of the Beta distribution as

The update of t(e) is given as t(e) =

t(e) =  +

c +
?

?

?
cC(e)
?

?

?
1+et(e) where
r(e,e)R
r(e,e)R

r1 +

The update in Eq. 7 has five different parts. The contribution from the Beta prior
can be computed by  =  (t1)   (t2) where  () is the digamma function.
For each class c  C(e) the contribution to the update is given by

c = (1  (T

c (e)))ct

If the entity is the source of a relation, i.e., we have r(e, e

r1 = (1  ((e)T r(e
?

?

?
)))rt.(e
?

?

?
)

or if it is the target, i.e. r(e
?

?

?
, e), the contribution is

r2 = (1  ((e
?

?

?
)T r(e)))r.t(e
?

?

?
)

and  is updated by

 =  ( (t(e))   (
?

?

?
t

t (e)))

The variational Dirichlet parameter t(e) is
?

?

?
t(e) = t(e) +

t(e, v)

(e,v)O(w)

(8)
?

?

?
), the contribution is

(9)

(10)

(11)

(12)
?

?

?
(1  (T

c (e)))t(e)
?

?

?
eVE
?

?

?
r(e,e)R

(14)

(15)

V. Bicer et al.

The contribution to the update in Eq. 12 includes the variational multinomial
t(e, v) and also the variational parameter t of the corresponding topic indicator
bt, i.e., q(bt(e) | t(e). This is the direct result of parameterizing the Dirichlet

distribution with the topic indicator vector of each entity instead of using a
non-informative prior  as in LDA.

The updates for the variational multinomial t(e, v) is identical to that in

variational inference for LDA [15]:

t(e, v)  exp{logtv +  (t(e))   (

t (e))}

(13)
?

?

?
t

where t(e, v) = q(z(e, v) = t).

Variational M-Step. The update for the topic parameter  is the same as in
LDA because also here, the words are conditionally dependent on  and z.

In order to fit the parameters  and  of the logistic regression defined by Eq.
1 and 2, respectively, we employ gradient-based optimization. At each iteration,
we perform updates using the gradient

for each class c and topic t and

rtt =

(1  ((e)T r(e
?

?

?
)))rttt (e
?

?

?
)

for each relation r and topics t and t
?

?

?
.

These gradients cannot be used directly since they are only calculated for
positive observations of classes and relations. For the unobserved cases (r(e, e) =
0) a regularization penalty is applied so the updates decreases  and  in each
iteration for the topics controlled by the Beta-Bernoulli prior of b. This also
introduces sparsity of the weights in  and  according to the topics selected
in b. In particular, let  be the number of observations (e.g. entities) for which
the class membership is unknown such that c(e) = 0 and  be a topic-indicator
vector set to be the mean of the Beta-distributed variable ,  =  1
. Then,
the regularization for c is Rc = ((T
)). Similarly, let  be the number
) = 0). Then,
the regularization term for r is Rr = ((T r).

of observations where a particular relation is unknown (i.e. r(e, e

 1+ 2
?

?

?
c

3 Experiments

First, we aim to obtain an initial understanding of the (1) quality of the topic
model produced by TRM. Then, we provide a quantitative analysis of TRM by
comparing its performance to state-of-the-art TM and SRL approaches w.r.t. the
(2) object clustering and (3) link predication tasks, respectively.

Datasets. We use a subset of DBpedia containing 20,094 entities described by
112 distinct classes and 49 different types of relations. All attribute values are
?

?

?
treated as textual information and put into bags of words. The resulting vocabulary comprises 26,109 unique words after stop word removal. We also employ the
DBLP 1 dataset. The abstract and title of the papers are treated as textual data.
In addition, authors and conferences and their relations to papers are taken into
account. We use a subset of papers that belong to the fields of database, data
mining, information retrieval and artificial intelligence. In total, there are 28,569
paper, 28,702 author and 20 conference entities, and a vocabulary comprising
11,771 unique words.

3.1 Topic Analysis
?

?

?
logtv  1

t logtv

A useful application of TRM is to understand the data. Fig. 2 displays the top
words of four selected topics using the learned  parameter. Words are ranked by
score(v, t) = tv
, which intuitively, assigns high scores
to those words that are characteristic for a topic, relative to all other topics.
We can clearly observe that structure elements (classes and relations) have an
influence on the topics and the words that are ranked high for these topics. For
example, t1 has top words from entities of the type organization whereas t2
captures words related to person. In particular, t1 and t2 have top words from
those organization and person entities that are involved in the keyPerson rela-
tion. In fact, TRM not only exploits structure information for topic modeling but
also explicitly models the strength of dependencies between topics and structure
elements through the  and  parameters.

3.2 Link Prediction

Note that TRM captures the joint distribution over variables representing topics
and structure elements. Thus, not only are topics dependent on structure elements but also vice versa, the existence of certain topics (and their words) can
be used to infer that some structure elements are more likely than others. Here,
we evaluate the effectiveness of using TRM topics for link prediction  based
on the design and implementation used for the previous C3 experiment [12]. We
created training data using the author relations between papers and authors
in DBLP and the starring relations between movies and actors in DBpedia.
Then, this data is divided into a training and test set, with test data set to be
2, 4 and 3

4 times the amount of training data.

However, it should be noted that the task of link prediction in SRL is different
from LP for documents (e.g. [4, 5, 8]) in which topic similarity of documents
is only distinctive feature. In the former each relation (e.g.starring,author) is
characterized differently by its weights to features in a linear model like SVM.
Thats why supervised TMs are not directly applicable on this task. Thus, in this
experiment we show how the topic features based on  of a relation are distinctive
for link prediction beyond some base features such as the ones employed in C3.
Also note that, unlike other supervised TMs, TRM distinguishes different types
of relations and a separate  matrix is used for every relation, which assigns

1 http://www.informatik.uni-trier.de/ley/db/

V. Bicer et al.

cross-topic weights. Link direction is also considered as the matrix omega is
asymmetric.

Methods. We compare TRM against MLN [11] and C3 [12]. To train the MLN,
we use the open source Alchemy2 implementation and adopt the rules as described in the Alchemys tutorial for link predication. In order to predict links
between two entities, C3 employs SVM along with a set of features including Jacccard similarity computed from textual values of the two entities, words shared
by the entities and adjacent nodes connected to the entities. LibSVM3 is used
to train a nu-SVM with a RBF kernel. The value of nu is set experimentally between 0.1 and 0.5. To use TRM for link prediction, we consider the combination
of C3 and the topics inferred by TRM. Namely, TRM provides two additional
features that are then used by C3. The first feature is a topic-based similarity
vVM p(v | t) rtt p(v | t ) where VM is
score defined as simr(e1, e2) =
the set of words e1 and e2 have in common. Instead of using these shared words
only, we also use all the words in e1 and e2 to calculate a second feature using
the formula above.
?

?

?
t,t

Table 1. Precision, recall and accuracy results for link prediction on DBLP and DBpedia

DBLP(1-4)

DBLP(1-2)

DBLP(3-4)

Prec. Rec. Acc. Prec. Rec. Acc. Prec. Rec. Acc.
MLN 50.46 74.75 50.05 49.53 71.33 49.23 51.9 76.14 52.31
C 3
55.51 76.92 57.69 56.09 71.87 55.14 58.13 78.12 59.80
TRM 66.03 84.84 67.34 65.98 83.87 69.53 68.83 85.54 71.25

DBpedia(1-4)

DBpedia(1-2)

DBpedia(3-4)

Prec. Rec. Acc. Prec. Rec. Acc. Prec. Rec. Acc.
MLN 50.72 81.7 51.08 50.4 83.05 50.5 51.45 79.61 52.03
57.14 66.67 54.95 56.52 66.10 54.04 55.88 65.51 54.95
TRM 72.71 78.43 69.04 70.58 74.38 67.84 71.68 74.20 67.28

C3

Results. We present the overall performance in Table 1. Also considering true
negatives as depicted in Fig.4, we observe that while MLN can provide high
recall for positive labeled data (achieves best recall for DBpedia in one setting),
it does not perform well for negative labeled data. C3 performs better than
MLN in terms of precision and accuracy and also, achieves higher true negative
rate. However, C3s performance could clearly be improved when using TRM
features in additional: TRM outperforms both baselines in terms of precision
and accuracy and achieves average recall comparable to MLN. Also, it is more
superior than these baselines in handling negative labeled data. The second
feature provided by TRM captures the topical correlation between all words of
the entities. It was particularly helpful in eliminating false negatives, i.e., two
entities not correlated topic-wise w.r.t. r are mostly not linked. The topic-based
similarity calculated from matching words further helps to find true positives.

2 http://alchemy.cs.washington.edu/
3 http://www.csie.ntu.edu.tw/~cjlin/libsvm/
?

?

?
Fig. 4. True negative rate for DBLP and DBpedia

Table 2. Precision and normalized mutual information (NMI) results for object clustering on DBLP and DBpedia

Method/
Metric

Paper(%)
Author(%) Venue(%)
Acc. NMI Acc. NMI Acc. NMI
47.22 15.97









TMBP-RW 69.11 45.24 74.67 65.61 65.79 67.48
TMBP-Reg 78.21 58.42 88.55 71.17 75.02 64.01
TRM 89.35 65.51 93.44 78.16 87.73 75.11

(a) DBLP

Method/
Metric

Movie(%)
Acc. NMI
58.71 22.29
TMBP-RW 57.10 27.65
TMBP-Reg 62.33 31.84
TRM 71.57 44.28

(b) DBpedia

3.3 Object Clustering

For DBPedia, we use entities of the type movie. Following the experiment performed previously [9], we use the six labels in DBLP representing various computer science fields as clusters. The clustering result is evaluated by comparing
the label of each paper with the topic learned from the data.

Methods. We compare TRM to three TM approaches. As the most relevant
baselines, we use two methods for learning topics from heterogeneous networks
[9]: one model is learned with biased random walk (TMBP-RW) and the other
results from biased regularization (TMBP-Reg). TMBP-RW propagates topic
probabilities through the network via random walk, while TMBP-Reg achieves
topic propagation through regularizing a statistical topic model with two generic
terms. A previous experiment [9] has already shown that incorporating heterogeneous structure information as performed by these baselines helps to outperform
clustering results of several existing (TM) approaches. For brevity, we thus include only the results of the standard LDA model [15]. Since LDA cannot be
directly applied to heterogeneous information networks, we only use the bag-of-
words representation of entities and ignore structure information.

Results. Table 2 shows the average results obtained from 10 test runs. TMBPRW outperforms LDA on DBLP and is comparable to LDA on DBPedia. TMBPReg slightly outperforms TMBP-RW on both datasets. This suggests that
exploiting structure information as supported by TMBP-RW and TMBP-Reg,
leads to better results than LDA, which only considers word co-occurrences.

V. Bicer et al.

TRM leads to further improvements on both datasets by incorporating the effects of specific classes and relations on topics. For DBpedia for instance, TRM
automatically infers that the structure elements distributor and country have
a strong discriminative effect on assigning objects to the correct clusters.

4 Related Work

Related to our work, there are TM approaches proposed for homogeneous networks such as NetPLSA [2], Pairwise-Link-LDA [3], Nubbi [5], author-topic models [1], latent topic models for hypertext [6], citation networks [7] and relational
topic models [8]. The major distinction between these models and TRM is that
they consider a homogeneous network structure with only few types of entities
and relations. In addition, more related to TRM, there are approaches over heterogeneous networks [9, 7] which utilize specific regularization functions to fit the
topics to the underlying network structure. In general, as the network becomes
more heterogeneous (i.e. more than two types of relations), more complex topic
models are needed to capture complex correlations between the topic and structural variables. TRM mainly addresses this in a principled way by introducing
sparsity of topics via topic indicators to create specific bias of topics towards
structure information, i.e., classes and relations. In fact these approaches can
be regarded as the extension of previous supervised topic models (e.g. [2023])
to the networks in which observed variables are the relations instead of some
tags or annotations. Also one similar work to ours is Type-LDA [24] which aims
to discover the clusters of observed relation tuples and their associated textual
expressions. However, that work only has a narrow focus on relation extraction
in NLP and does not address the discovery of the correlations occurring in the
whole data graph.

SRL works such as probabilistic relational models (PRM) [10] and MLN [11]
learn graphical models using relational information. As discussed, this training is
costly when the dependency structure is complex and the number of variables is
high  which is particularly the case when a large amount of text is involved. We
propose the use of hidden topic variables to reduce this complexity. To combine
SRL with topic models, FoldAll [25] uses constraints given as first-order rules in a
MLN to bias the topics by training a MRF. Although biasing the topics according
to structure information can be accomplished through MLN, this approach does
not capture correlations between topics and structure elements (e.g. predicate of
rules). In addition, the number of groundings in the MLN rules poses a problem
for FoldAll, since each grounding is represented as an indicator function in the
corresponding topic model. TRM is unique in terms of using the topics as a
low-dimensional abstraction to capture the correlations between the topics and
classes/relations (i.e  and ).

5 Conclusion

We presented TRM, a novel combination of TM and SRL to learn topics from
text-rich structured data. It captures dependencies between words in textual
?

?

?
and structured data through hidden topic variables in a template-based model
constructed according to the underlying data structure. It represents a novel approach for automatically using heterogeneous structure information for learning
topics as well as using topics to perform SRL tasks. In experiments, we show
that compared to existing TM approaches, TRM is more effective in exploiting
structure information. It reveals and exploits varying level of dependencies between topics and specific classes and relations, resulting in higher performance
for both object clustering and link prediction.

As future work we plan to explore the extension of TRM to even richer generative models, such as time-varying and hierarchical topic models. In addition,
potential application areas of TRM in the field of text-rich databases are many-
fold. In particular, for selectivity estimation of structural queries comprising
string predicates, TRM provides a synopsis of the database by capturing the
topics and their correlations with classes and relations. This way, any structural
query can be interpreted as a probability distribution, from which the query result size is estimated. We also consider TRM as being useful for keyword search
on structured data. In existing work, keywords are mapped to database elements
and connections between these keyword elements are discovered based on the
relations given in the schema to compute structured results. The ranking of
these results is separated from that computation. Instead of using the schema
for discovering connections and a separate model for ranking, TRM can serve as
a probabilistic schema, capturing connections that are most probable. Hence,
it can be used as a holistic model both for result computation and ranking based
on their probability.

Acknowledgments. This research is partially supported by Siemens / DAAD
Postgraduate Program under grant number A/10/94300.
