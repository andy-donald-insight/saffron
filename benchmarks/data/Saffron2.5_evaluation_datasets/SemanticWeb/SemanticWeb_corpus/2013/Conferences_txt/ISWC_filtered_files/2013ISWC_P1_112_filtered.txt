Empirical Study of Logic-Based Modules:

Cheap Is Cheerful

Chiara Del Vescovo1, Pavel Klinov2, Bijan Parsia1,

Ulrike Sattler1, Thomas Schneider3, and Dmitry Tsarkov1

{delvescc,bparsia,sattler,tsarkov}@cs.man.ac.uk

1 University of Manchester, UK

2 University of Ulm, Germany

pavel.klinov@uni-ulm.de

3 Universit at Bremen, Germany

tschneider@informatik.uni-bremen.de

Abstract. For ontology reuse and integration, a number of approaches
have been devised that aim at identifying modules, i.e., suitably small
sets of relevant axioms from ontologies. Here we consider three logically
sound notions of modules: MEX modules, only applicable to inexpressive
ontologies; modules based on semantic locality, a sound approximation of
the first; and modules based on syntactic locality, a sound approximation
of the second (and thus the first), widely used since these modules can
be extracted from OWL DL ontologies in time polynomial in the size of
the ontology.

In this paper we investigate the quality of both approximations over
a large corpus of ontologies, using our own implementation of semantic locality, which is the first to our knowledge. In particular, we show
with statistical significance that, in most cases, there is no difference
between the two module notions based on locality; where they differ,
the additional axioms can either be easily ruled out or their number is
relatively small. We classify the axioms that explain the rare differences
into four kinds of culprits and discuss which of those can be avoided
by extending the definition of syntactic locality. Finally, we show that
differences between MEX and locality-based modules occur for a minority of ontologies from our corpus and largely affect (approximations of)
expressive ontologies  this conclusion relies on a much larger and more
diverse sample than existing comparisons between MEX and syntactic
locality-based modules.

Introduction

Some notable examples of ontologies describe large and loosely connected do-
mains, as it is the case for SNOMED CT, the Systematized Nomenclature Of
MEDicine, Clinical Terms,1 which describes the terminology used in medicine
including diseases, drugs, etc. Users often are not interested in a whole ontology

1 http://www.ihtsdo.org/snomed-ct/

H. Alani et al. (Eds.): ISWC 2013, Part I, LNCS 8218, pp. 84100, 2013.
c Springer-Verlag Berlin Heidelberg 2013
?

?

?
O but rather only in a limited part of it which is relevant to their application.
One recently explored technique for addressing this situation is to use modules,
i.e., suitably small subsets of O that behave for specific purposes like the original
ontology over a given signature , i.e., a set of terms (classes and properties).

Using a module rather than a whole ontology aims at improving performance
since only information that is relevant to a restricted vocabulary is processed.
However, the correctness of the outcome can be guaranteed only if the used modules satisfy certain well-defined properties. For example, reasoning-based tasks
require the modules to provide coverage for O over , i.e., preserve all the entailments of O over  (they are called logical modules [9,4]). Applications of
logical modules include reuse of (a part of) well-established ontologies, ontology integration, and computing justifications to debug ontologies [11]. In these
scenarios, though, a stronger notion of logical module is required that satisfies
also two additional properties [15,19]: self-containment and depletion. The former means that the module preserves entailments over all terms that occur in
the module (not just those used to extract the module). The latter means that
O\M does not entail any non-tautological axioms over . In this paper we will
analyze only depleting and self-contained logical modules.

Interestingly, a minimal depleting and self-contained module for a signature
 is, under some mild conditions, uniquely determined [15]. Extracting such
modules is, unfortunately, computationally hard or even undecidable for expressive ontology languages [10,17,18]. In order to identify notions of modules whose
extraction is feasible we can follow two alternative strategies. The first one consists of restricting the expressivity of the ontology language, as in the case of the
MEX approach [14]: the MEX system allows for the extraction in polynomial time
of the minimal self-contained and depleting module from acyclic ELI terminolo-
gies. The second strategy consists of looking for practical sufficient conditions to
guarantee the properties of logical modules without imposing minimality on the
module M, as it is the case for the family of logical modules known as localitybased modules (LBMs) [3]; these modules can be extracted from ontologies as
expressive as SROIQ, are self-contained and depleting, but can contain axioms
that are not relevant to preserve any entailment over the given .

The family of LBMs consists of module notions that are parameterized according to two features: (1) the technique used for identifying which axioms need to
be included in the module (semantic or syntactic); (2) the kind of placeholder(s)
used for those terms not included in the signature (bottom, top, or nested ). In
the next two paragraphs we provide an intuitive discussion of the meaning of
these two features.

The extraction of semantic LBMs requires entailment checks against an empty
ontology and thus involve reasoning, which makes the computation as hard as
reasoning. Moreover, the kind of reasoning service used is rather unusual for DL
reasoners.2 Hence, although algorithms for extracting semantic LBMs are known,
until now and to the best of our knowledge they had not been implemented.

2 DL reasoners usually classify an ontology: test it for consistency and all concept

names for satisfiability/mutual subsumption.

C. Del Vescovo et al.

In contrast, the extraction of syntactic LBMs involves only parsing the axioms
of the ontology. Algorithms for the extraction of syntactic LBMs are known that
run in time polynomial in the size of the ontology (thus much cheaper than
reasoning), and are implemented in the OWL API.3

The kind of placeholder(s) used for semantic and syntactic LBMs gives a
flavour of the different module notions. The bottom variants of LBMs provide a
view of O from  upwards since they contain all named superclasses of class
names in ; the top variants instead provide a view of O from  downwards
since they contain all named subclasses of class names in ; finally, the nested
variants provide a view of O within  since they still provide coverage for
 as the other variants, but they do not necessarily contain all the sub- or
super-classes of the classes in .

This paper empirically studies the seven module notions depicted in Fig. 1
which summarizes their notations and their inclusion relations. Each node represents a module notion; the one for the MEX module is shadowed because this
method can be used only for ELI acyclic ontologies. The MEX notion is in the
same column as the nested versions because MEX modules provide a similar view
of O within .

bottom

top

nested

method

Semantic
locality

Syntactic
locality

N.A.



















N.A.




?

?

?


Fig. 1. Inclusion relations between the 7 notions of modules investigated

As shown in Fig. 1, the MEX module for a signature  is a subset of the
nested semantic LBM, and for each variant bottom, top, and nested, the semantic LBMs are contained in the corresponding syntactic ones. Hence, syntactic
locality can be seen as an approximation of semantic locality which, in turn, is
an approximation of MEX modules. This gives rise to the question of how good
these approximations are: how much larger are the modules extracted by the
approximations, and how much faster is the extraction?

This paper provides emprical answers to these questions by comparing different modules systematically extracted from a large corpus of real-life ontolo-
gies. Specifically, semantic LBMs are compared with syntatic LBMs and with
MEX modules (for acyclic ELI ontologies). This paper substantially extends
3 http://owlapi.sourceforge.net/
?

?

?
the previous experiments reported in [14] where MEX modules were compared
with syntactic bottom modules on a sample of 5000 random signatures and the
SNOMED CT ontology. We perform our study on a larger corpus (not restricted
to ELI), compare more notions of logical modules, and also provide rigorous
statistical significance results.

The main contributions of this paper are summarized as follows:

 We show with statistical significance that, for almost all members of a large
corpus of existing ontologies, there is no difference between any syntactic
LBM and its semantic counterpart. In the few cases where differences occur,
those are extremely modest so that it is questionable whether extracting
semantic LBMs is worth the increased computational cost.

 We isolate four culprits, i.e., patterns of axioms that completely explain those
rare differences. One includes simple tautologies that can be removed in a
straightforward preprocessing step.

 Our results show that the extraction of semantic LBMs, which is in principle hard, is feasible in practice: on average, it is between 3 times (for top-
modules) and 15 times (for bottom- and nested-modules) slower than the
extraction of syntactic LBMs, and both only take milliseconds to seconds
for most ontologies below 10K axioms.

which, to the best of our knowledge, is the first ever to be implemented.

 To obtain these results, we use our own implementation of semantic locality
 We modify the original corpus to obtain for each ontology an acyclic EL
version suitable for the use with the MEX system. We then compare MEXmodules and the nested-variants of LBMs, and find differences in only 27%
of the corpus. We explain one reason for the largest differences observed.

2 Preliminaries
We assume the reader to be familiar with Description Logic languages (e.g.
SROIQ [1,13]), and aim here at fixing the notations and at defining the key
notions around module extraction, with a focus on locality-based modules [3]
and MEX modules [14].

Let O denote an ontology, NC a set of class names, and NR a set of property
axiom X, we call the set of terms in X the signature of X, denoted X. Given a
names. A signature is a set   NC  NR of terms. Given a class, property, or
SROIQ ontology O, a set M  O of axioms from O, and a signature , we say
axioms  with   , it holds that O |=  if and only if M |= . O is a model
that O is a deductive -conservative extension (-dCE ) of M if, for all SROIQ-
-conservative extension (-mCE ) of M if {I| |I |= O} = {I| |I |= M}.
Dually, M is a dCE-based module of O for  if O is a -dCE of M, and it is
an mCE-based module for  if O is a -mCE of M. All dCE-based modules
are also mCE-based modules, whilst the converse is not always true. A module
M  O for  is called depleting if there is no non trivial entailment  over 
Since M  O the monotonicity of SROIQ implies that every entailment
 over  derivable from M is also derivable from O. Deciding the converse

such that O \ M |= ; M is called self-contained if M is a module for  = M.

C. Del Vescovo et al.

direction is in general computationally hard, or even undecidable for expressive
DLs [10,17,18]. Since we do not need to find all the subsets of O that are a
module for , we can use easier conditions which guarantee that a set of axioms
M  O is a module for .
Let  be a signature and O be an ontology. Let x  {MEX,, ,,} be a
notion of module. For each such notion, an oracle x-check can be defined that
determines whether an axiom  may be involved in preserving an entailment 
of O over . Then, the x-module x-mod(,O) for  in O can be computed by
performing Algorithm 1.

Algorithm 1. Extraction of an x-module for 

Input: Ontology O, seed signature , oracle x-check
Output: x-module M of O w.r.t. 
M  ; O  O
repeat

changed  false
for all   O
if the x-check for  against   M is positive then
do
M  M  {}; O  O \ {}; changed  true

until changed = false
return M
Algorithm 1 is a special case of the one in [3, Figure 4], and its output M does
not depend on the order in which the axioms  are selected [3].

Due to space limitations, we can just briefly sketch the intuition behind the
definition of each oracle and the corresponding results of interest for this paper.
We refer the interested reader to [3,14] for further details.
The MEX System. In [14], the notion of a MEX-module is defined for acyclic
terminologies, i.e., ontologies that satisfy two conditions: (1) they only contain
axioms of the form A  C or A  C where A is a class name and C is a complex class;
(2) for each A, there is at most one axiom with A on the left-hand side; if one such
axiom  exists, then A is said to be defined, and to be directly dependent on all the
terms X that occur on the right-hand side of  (denoted A  X). The MEX method
requires to determine for each defined class A the set dependO(A) of all the terms
X in O such that the pair (A, X) belongs to the transitive closure of . Intuitively,
then, the MEX-check for an axiom  against a signature  tests whether either 
O\M, or if every term on which A depends only via -axioms is used to define4

defines a class A  M and uses 4at least one term X  dependO(A)(M) in
some term in   M. The authors prove that, if O is an acyclic ELI ontology,

then using the oracle MEX-check in Algorithm 1 generates the minimal depleting
self-contained module for a signature  in polynomial time.
Semantic Locality. In [3], the authors define a family of notions of locality with
different parameters, the prominent notions being those where the placeholder

4 The expressions use and used to define are high-level intuitive descriptions of the two
conditions given in [14, Fig. 4], to which we refer the reader since a formal definition
goes beyond the scope of this paper.
?

?

?
x belongs to {, }. These two notions of locality can be intuitively described
as follows: a SROIQ axiom  is -local (resp. -local ) w.r.t. signature  if 
?

?

?
obtained by replacing all terms in  \  with  (resp. ) is a tautology, in
axioms are -local) w.r.t.   M, then M is an mCE-based (and hence dCE-

which case the x-check returns negative. This treatment of  independently of
the remaining axioms distinguishes the - and -check (as well as the - and
-check introduced in the next paragraph) from the MEX-check; hence the name
local. The authors of [3] prove that, if all axioms in O \ M are -local (or all
based) module of O for . Since deciding - or -locality requires tautology

checks, this problem is as hard as standard reasoning. In some cases, 
is not a
SROIQ axiom, so standard reasoners need to be extended.
Syntactic Locality. In order to achieve tractable module extraction, the two
syntactic notions of x-locality for x  {,} have been defined in [3]. Similarly
to semantic locality, the x-check for an axiom  against a signature  operates

obtained by replacing all terms not in  with
on the transformed axiom 
the placeholder x. However, rather than invoking a reasoner, the x-check of 
against  makes use of a simple syntactic test [3, Sec. 5.5]. For example,   C
is clearly a tautology for each class C. If the x-check is negative,  is said to be -
or -local w.r.t. . The x-check used in syntactic LBMs is sound in identifying
non-tautological axioms, but it may fail to spot a tautology, i.e., every -local
(-local, resp.) axiom w.r.t.  is also -local (-local, resp.) w.r.t. , but not
vice versa. Thus, also - and -modules are mCE- and dCE-based modules for
. Applying the syntactic rules requires polynomial time, hence the extraction of
this kind of modules is performed in time polynomial in the size of the ontology.
Modules based on syntactic (semantic) locality can be made smaller by iteratively nesting - and -extraction (- and -extraction), again obtaining mCEand dCE-based modules [3,19], called 

- and 

-modules.

Algorithm 1 guarantees that the module notions considered here are selfcontained and depleting: self-containment holds because of the iteration until
the signature of M remains unchanged; depletion holds because the axioms left
out of M are those whose x-check against the enlarged signature is negative.

3 Research Questions and Experimental Design

A natural question arising is whether syntactic and semantic LBMs differ in
practice, and, if yes, by how much. A second question is whether semantic module
extraction is noticeably more costly: the x-check has to be carried out often
once per axiom and signature that the algorithm goes through and it is hard
to predict the feasibility of semantic LBM extraction. Altogether, we want to
know whether syntactic LBMs are a good approximation of semantic LBMs, and
how much they differ in cost. Similarly, for acyclic ELI ontologies the analogous
question arises: how good an approximation of MEX modules are LBMs?

An answer to these questions will allow for a more informed choice of which
module extraction technique to select. One can always construct ontologies with
huge differences in size and time between syntactic and semantic LBMs and
between LBMs and MEX modules. Here, we are interested in these differences

C. Del Vescovo et al.

in currently available ontologies, and thus we need to design, run, and analyse
suitable experiments.

Selection of the Corpus. For our experiments, we have built a corpus con-
taining: (1) all the ontologies from the NCBO BioPortal ontology repository,5
version of November 2012; (2) ontologies from the TONES repository6 which
have already been studied in previous work on modularity [6]: Koala, Mereology,
University, People, miniTambis, OWL-S, Tambis, Galen. From this corpus, we have
removed ontologies that cannot be downloaded, whose .owl file is corrupted or
impossible to parse, or which are inconsistent. Furthermore, we have excluded
those large ontologies (exceeding 10K axioms) where the extraction of a semantic
LBM repeatedly took more than 2 minutes: for each such ontology, the estimated
time needed to perform our experiments could have exceeded 300 hours. How-
ever, to include at least one case of a huge ontology, we have kept in the corpus
NCI, an SH(D) ontology with 123,270 axioms.
This selection results in a corpus of 242 ontologies, which even beside NCI
greatly vary in expressivity (from AL to SROIQ(D)) and in size (1016,066
axioms, 1016,068 terms) [12]. For a full list of the corpus, please refer to [5].
As mentioned above, it is not possible for some ontologies to test -locality
(and thus for extracting - and 
-modules) using standard DL reasoners, see
[5] for details. To cover these cases, we have extended the reasoner FaCT++ to
cover the use of the -role as required by the semantic locality tests.
Since MEX handles only acyclic ELI ontologies, we created an ELI version
ELI(O) of each ontology O in our corpus by filtering unsupported axioms and
breaking terminological cycles. A principled way of doing this is beyond the
scope of this paper, and we have used the heuristic described in [5]. The resulting
corpus contains 239 ontologies since 3 were left empty after the ELI-fication.
Comparing Modules and Locality. In order to compare syntactic and semantic locality, as well as LBMs and MEX modules, we want to understand
(1) whether, for a given seed signature , it is likely that there is a difference
between the syntactic, the semantic, and the MEX modules for ; if so, the size
of the difference;7 and (2) how feasible the extraction of semantic LBMs is.
For this purpose, we compare (a) -semantic and -syntactic locality, -
semantic and -syntactic locality, (b) - and -modules, - and -modules,
- and 

Due to the recursive nature of Algorithm 1, our investigation is both on a
per-axiom-basis: given axiom  and signature , is it likely that  is -local

-modules, (c) MEX modules and 

-modules.

per-module basis: given a signature , is it likely that

(-local, resp.) w.r.t.  but not -local (-local, resp.) w.r.t. ?
 -mod(,O) = -mod(,O), or
 -mod(,O) = -mod(,O), or

5 http://bioportal.bioontology.org
6 http://owl.cs.manchester.ac.uk/repository/
7 Recall: the MEX module is always a subset of the semantic -module, which is

always a subset of the syntactic -module.
?

?

?
 
 
If yes, is it likely that the difference is large?

-mod(,O) = 
-mod(,O), or
-mod(,O) = MEX-mod(,O)?

if m = #O, there are 2m possible seed signatures, so that testing axioms for

Clearly we need to pick, for each ontology in our corpus, a suitable set of
signatures, and this poses a significant problem. A full investigation is infeasible:
locality against all the signatures is already impossible for m  100. One could
assume that comparing modules is easier since many signatures can lead to
the same module. However, previous work [6,8] has shown that the number of
modules in ontologies is, in general, exponential w.r.t. the size of the ontology.
Still, different seed signatures can lead to the same module, which makes it hard
to extract enough different modules.

We will consider seed signatures of two kinds: genuine seed signatures and

random seed signatures.
Genuine Seed Signatures. A module does not necessarily show an internal co-
herence: e.g., if we had an ontology O about the domains of geology and philoso-
phy, we could extract the module for the signature  = {Epistemology, Mineral}.
That module is likely to be the union of the two disjoint modules for 1 =
{Epistemology} and 2 = {Mineral} [7].
In contrast, genuine modules can be said to be coherent: they are those modules that cannot be decomposed into the union of two -uncomparable mod-
M = x-mod(,O). As a consequence, there are only linearly many genuine
ules. Interestingly, a module M is genuine iff there exists an axiom  such that
modules in the size of O, and extracting one module per axiom is enough for
obtaining all of them. Moreover, all modules of O are composed from genuine
modules [7]. Thus, genuine modules are of special interest, and we can investigate
all of them, together with the corresponding genuine signatures.

Random Seed Signatures. Since a full investigation of all the signatures is
impossible, we compare localityboth on a per-axiom and per-module basisas
well as LBMs and MEX modules on a random signature , which we select by
setting each named entity E in the ontology to have probability p = 1/2 of being
included in . This ensures that each  will have the same probability to be
chosen. This approach has a clear setback: the random variable size of the seed
signature generated follows a binomial distribution, so a random seed signature
is highly likely to be rather large and to contain half the terms of the ontology.
However, we do not yet have enough insight into what typical seed signatures are
for module extraction, so biasing the selection of signatures to, for example, those
of a certain size has no rationale. In contrast, selecting random seed signatures
avoids the introduction of any bias. Moreover, this choice is complementary to
the selection of all the genuine signatures, which are in general small.

With this in mind, we will analyze the modules obtained by random signatures
with p = 1/2, and we will see in Section 4 that the module sizes obtained do
allow for a reliable statement about the differences observed.
How many seed signatures do we have to sample from a given ontology O
in order to obtain statistically significant statements about modules determined

C. Del Vescovo et al.

by the real population of all signatures from O? We apply the usual statistical
model of confidence intervals [20], aiming at a confidence level of 95% that the
true proportion of differences between modules  i.e., the proportion of seed
signatures that lead to different modules  lies in the confidence interval (5%) of
the observed proportion. Then we can generalize the conclusions for the random
sample to the full population because the probability that the proportion of
differences among modules for all seed signatures differs by no more than 5%
from the proportion observed in the sample (and reported in Section 4) is 95%.
In order to reach this confidence level, we need a sample size of at least 385
elements, independently of the size of the full population: for a two-sided test
to detect a change in the proportion defective of size  in either direction, the
minimum sample size is

N  p(1  p)

2

z2
1/2 ,

where p is the observed proportion,  the significance level, and z1/2 the critical value of the underlying distribution [2]. Here, we use the normal distribution
as an approximation of the binomial distribution which is usually assumed for
proportions in random sampling; hence the significance level of  = 0.05 leads
to z1/2  1.96. Furthermore, although we do not know the value p in advance,
it is clear that p(1  p)  0.25 because 0  p  1. The confidence interval of
5% determines the error of  = 0.05. Therefore, we obtain

N  0.25
0.052

 1.962  384.16,

that is, a representative sample for these parameters needs at least 385 elements,
and this number is independent of the population size. For ontologies with at
least 9 elements in the signature, we will therefore draw a sample of size 400.
For all other ontologies, we will look at all of the  400 signatures.
Summary. We compare, for every ontology O in our corpus,
(T1) for random seed signatures  from O,

(a) for each axiom  in O, is 

(b) is

 -local w.r.t.  but not -local w.r.t. ?
 -local w.r.t.  but not -local w.r.t. ?
 -mod(,O) = -mod(,O)?
 -mod(,O) = -mod(,O)?
 
 

-mod(,O) = 
-mod(,ELI(O)) = MEX-mod(,ELI(O))?

-mod(,O)?

(T2) the same questions (a) and (b), with  ranging over all the genuine

signatures   for   O.

Our sample selection includes large as well as small seed signatures: the random
seed signatures created to answer T1 will tend to contain around half the terms in
the ontology, while the signatures used to answer T2 will range over all signatures
of single axioms and therefore tend to be small.
?

?

?
4 Results of the Experiments

4.1 Semantic Versus Syntactic Locality

No Differences in Locality. The main result of the experiment is that, for
the vast majority of the ontologies in our corpus, no difference between syntactic
and semantic locality is observed, for all three variants  vs. ,  vs. , and


. More precisely, for 209 out of 242 ontologies, we obtain that:

vs. 

(T1) for random seed signatures, there is no statistically significant difference

(a) between semantic and syntactic locality of any kind,
(b) between semantic and syntactic LBMs of any kind;

(T2) given any genuine signature, there is no such difference.

More specifically, for all randomly generated seed signatures and all genuine
signatures, the corresponding bottom-modules (and the corresponding top- and
nested-modules, respectively) agree, and every axiom is either - and -local, or
none of both (and either - and -local, or none of both).
unusually large -modules [3,8].

The 209 ontologies include Galen and People, which are renowned for having

In most cases, extracting a semantic and syntactic LBM each took only a few
milliseconds, so a performance comparison is not meaningful. For some ontolo-
gies, the semantic LBM took considerably longer to extract than the syntactic:
up to 5 times for nested-modules in Molecule Role, and up to 34 times in Galen.
Differences in Locality. We have observed differences between syntactic and
semantic locality for 33 ontologies in our corpus. We call the axioms that cause
these differences culprits  patterns of axioms which are not -local (-local,
respectively) w.r.t. some signature , but which are -local (-local, respec-
tively) w.r.t. . We have identified four types of patterns, ad , and we describe
them in the following. Sometimes, culprit axioms pull additional axioms into the
syntactic LBM, due to signature extension during module extraction.


?

?

?
and C

We denote class names by A, B, complex classes by C, D, properties by r, s, . . . ,
nominals by a, non-empty data ranges (e.g., int or int0..9) by R, possibly with
indices.  denotes a signature for which a module is extracted or against which
an axiom is checked for locality. Terms outside  are overlined; we further use
notation C
to denote classes that are bottom- or top-equivalent due to
the grammar defining syntactic locality in [3, Fig. 3] and the analogous grammar
for semantic locality.
Culprits of Type a are simple tautologies that accidentally entered the in-
ferred view (closure under certain entailments) of an ontology. These axioms do
not occur in the original asserted versions and could, in principle, be detected
in a simple preprocessing step. Type-a culprits occur in 10 ontologies of the
above 33 and are of the kinds A  A or r  (r

. Each such tautology is trivially -local and -local w.r.t. any , but not always - or -local: if  contains
all terms in that tautology, then both sides of the subsumption (equivalence) are
neither - nor -equivalent.


)

C. Del Vescovo et al.

Differences Caused Not Solely by Culprits of Type a have been observed
for 27 ontologies. In only 6 of these cases, the differences affect modules; in the
remaining 20, they only affect locality of single axioms (tests T1 a and T2 a). We
will focus on the former 6, listed in Table 1, and refer to [5] for details on all 27.

Table 1. Ontologies that exhibit differences in modules

Ontology
MiniTambis-repaired
Tambis-full
Bleeding History Phenotype
Neuro Behavior Ontology
Pharmacogenomic Relationsh...
Terminological and Ontological... TOK

Abbreviation DL expressivity #axioms #terms
MiniT

Tambis
?

?

?
PhaRe
?

?

?
SHIN (D)
ALCIF(D)

ALCHIF(D)
SRIQ(D)
?

?

?
1,925
1,314
?

?

?
According to Table 1, differences between modules occur for ontologies of
medium to large size and medium to high expressivity. Differences in locality
alone additionally affect small ontologies such as Koala (42 axioms) and Pilot
Ontology (85 axioms), as well as large ontologies such as Galen (4,735 axioms)
and Experimental Factor Ontology (7,156 axioms). The number of axioms causing
these differences (i.e., matching the culprit patterns) in the affected ontologies is
small except for Galen, and most of the observed differences are relatively small.
Table 2 gives a representative selection of the differences in modules observed,
plus the relative sizes of modules extracted for (T1) and (T2). For a complete
overview, including differences in locality of single axioms, see the table in [5].

Table 2. Overview of observed differences between modules

Ontol. Types

#diffs

size of diffs

affected

#axs

(rel.)

size of 

miniT bot, nested 1425%
17
Tambis bot, nested 3257% 241c
BHOa
17% 112
NBOa

PhaRea top, nested

0600%b 4879
162%c 7588
0300% 5572
0200% 6478
18% 1326d 06,520%d 5070
09% 4868

top, nested 49100%

nested
nested

17

3%

T1

-modules

(%) T2

culprit
type
range avg. range avg. + freq.
c

c

b 31
d

d 10
d
?

?

?
917 10

08
034
031
03
08
?

?

?
adifferences only for genuine modules
bdifferences > 5% only for genuine modules
cdifferences > 11 axioms (> 2%) only for genuine modules
ddifferences > 13 axioms (> 1,300%) only for top-modules
The columns show: ontology name (abbreviations: see Table 1); type of modules af-
fected; relative number of module pairs with differences; number of axioms in the
differences (absolute and relative to the - or - or 
-case); type of culprit present
and number of axioms of this type involved in differences.
?

?

?
Table 2 shows small absolute differences for miniT, BHO, NBO, and TOK. In
Tambis, large differences occur only for genuine modules. Finally, in PhaRe, large
differences occur only for top-modules.

Culprit-b axioms affect genuine modules of BHO, and (only) locality of single

For all these ontologies, a single syntactic or semantic module was extracted
within only a few milliseconds, making module extraction times roughly equal.
Culprits of Type b are axioms with an -restriction on a set of nominals or
a non-empty data range on the right-hand side, such as A  r.{a1, . . . , an} or
A  r.R. These axioms are -local w.r.t. a signature that does not contain r
because they become tautologies if r is replaced by . However, they can never
be -local unless A is replaced by some C

axioms for 4 more ontologies. We observed a variant A  C
Culprits of Type c are axioms  that contain a class description C such that
(a) C becomes equivalent to  (or ) if all terms outside  are replaced by
 (or ); (b) this causes  to be semantically -local (or -local); but (c)
). For
the grammars for syntactic locality do not detect C to be a C
example, C = r.A  r. becomes -equivalent if A is replaced by ; the same
holds with cardinality restrictions in place of . Consequently, axioms such as
  Br.C
s.{a} =3 r., (taken from Koala) are -local but not -local.
We found this pattern in 8 ontologies. Only in miniT and Tambis, it affects a
large proportion of bottom- and nested-modules, with additional axioms pulled
in. Still, the size of the differences is modest, as argued above. Some of the
remaining 6 ontologies contain different kinds of complex classes that cause differences in top-locality of single axioms.

  r.R.

.


?

?

?
(or C

Culprits of Type d are axioms where a class (or property) name from the lefthand side occurs on the right-hand side together with a top-equivalent property
(or class), causing differences in top-modules. The simplest such axiom is A 
r.A, which is -local because replacing r with  makes it a tautology. The
axiom is only -local if  contains neither r nor A. We have found further, more
complex, examples in Adverse Event Reporting Ontology and Galen; see [5].

We have observed culprits of type d in 17 ontologies, see the detailed overview

in [5]. Only in 3 cases (NBO, PhaRe, and TOK) are modules affected.

Galen contains 121 culprit-d axioms, but they only affect locality of single
axioms. The time differences for Galen are remarkable:checking all axioms for
-locality takes up to 70 times longer than checking them for -locality.
Module Sizes. The selection of the signatures for the experiment was designed
to allow for the analysis of two, complementary, kinds of modules: 1) genuine
modules, which constitute a base of all modules, extracted from generally small
axiom signatures; 2) a statistically significant amount of random modules, obtained from random, unbiased signatures which are likely to contain half the
terms of the ontology. We argue in what follows that it is neither the case that
genuine modules are so small to be almost irrelevant sets to investigate, nor that
random modules are so big to leave no space for differences to be observed. We
will focus on syntactic modules which contain the other kinds of modules.

C. Del Vescovo et al.

During the experiment we have computed and analyzed a high number of
genuine modules: more than 380K for the -notion, more than 40K for the -
notion, and more than 440K for the 
-notion of locality. As we mentioned
above, these modules tend to be quite small. However, they are not of irrelevant
size:  8% of the genuine -modules,  11% of the genuine -modules, and
 5% of the genuine 
-modules contain more than 20% of the axioms of the
corresponding ontology. So the low number of differences observed is not due to
checking only against very small modules.

With a similar and complementary discussion, we argue that the modules
obtained through random, big signatures do not necessarily contain almost all
of the ontology: e.g., 39% of all random 
-modules, and 28% of all random
-modules, contain less that 60% of the axioms of the corresponding ontology.
To sum up, the lack of differences between the modules is not due to too small

or to too big sizes of the modules selected.
Discussion. All culprits hardly ever cause significant differences in modules.
Only for PhaRe are differences between semantic and syntactic modules not
negligible, but we were able to relativize them, see [5].
Table 1 may suggest that culprits occur only in expressive ontologies. However,
patterns a, c, d can, in principle, already occur in simple terminologies in EL
and ALC, respectively. Evidently, type-a culprits can easily be filtered out in
a preprocessing step. For types c and d , there is no hope for an exhaustive
extension to locality because they can (and do) occur in arbitrarily complex
shapes and contexts. For this reason, the identification of culprits can only be
done on demand, i.e., by observing the differences in the modules of given
ontologies.

Patterns of type b rely on nominals or datatypes  but they are repairable
by a straightforward extension to the definition of syntactic locality: one can
extend the locality definition to distinguish - and -distinct classes, by adding
appropriate grammars to the definition of syntactic locality, and adding more
cases of - and -equivalent classes to the existing grammars. However, from
the small numbers of differences observed, we doubt that such an extension of
syntactic locality will have any significant effects in practice.

4.2 LBMs vs MEX Results

The results of the experimental comparison of syntactic/semantic LBMs and
MEX modules are summarized in Table 3. They show that MEX modules smaller
than the corresponding LBMs can be found in 27% of the preprocessed on-
tologies, for either random or axiom-based seed signatures. At the same time,
unsurprisingly, syntactic and semantic LBMs do not differ at all for these simple
ELI ontologies.

In experiments with random seed signatures, it can be seen that for those
ontologies where there are differences (most notably, Galen), they occur in many
tests. Thus, the difference appears to be caused by features of the ontology, not
some particular seed signatures. Also, the difference sometimes comes out large
in certain tests, also for genuine modules. For example, for the signature of the
?

?

?
Table 3. Differences between MEX and LBMs (

, 

)

Experiment

Random signatures
Axiom signatures

#ontol. % tests avg size of diffs
rel.
013%
080%

with diffs. with diffs. #axs
026
013

84%
12%
?

?

?
The results from the third column on are averaged over all ontologies with differences
LBMMEX in at least one module. For example, the last two columns show the average
min and max absolute (resp. relative) difference between LBMs and MEX modules.

following axiom in Galen, both 
the MEX-module only contains the axiom itself:8 RICF  ICF  ISFO.RSH.

-mod and 

-mod contain 127 axioms while

We analyzed whether the differences observed correlate with the size of the
original ontology, its expressivity or the extent of the modification done in the
ELI-fication. There is no correlation with size but, as is to be expected, with
the other two features, which are closely connected to each other. Table 4 illustrates the observations by dividing the 239 ontologies tested into four groups.
The ontologies in Group 1 are in a format MEX can handle, so they have not
been modified. The others required more or less heavy modifications (Groups
24). Differences between MEX and LBMs as described above occur only for
ontologies that required heavy modifications (Group 4).

Table 4. Overview of MEX experiment

Group

2 little-changed ontologies

1 unchanged ontologies
no diff.  \ MEX
no diff.  \ MEX
no diff.  \ MEX
with diff.  \ MEX

3 largely-changed ontologies

4 largely-changed ontologies

#axioms #ontologies ontology size (avg.)
removed

33 (14%) 1916,066 (2,176)

128

36 (15%) 13 6,587

(466)

317,836
(avg. 884)
3012,185
(avg. 1,001)

104 (44%) 5113,153 (2,373)

66 (27%) 4212,344 (1,843)

As expected, the expressivity among Groups 1 and 2 is generally low: only 21
ontologies in Group 2 use expressivity above ALE (up to SHIF(D), which is an
outlier). However, the size of some ontologies in Group 1 is already considerable:
22 out of 33 have > 100 axioms; 10 have > 1, 000 axioms. In contrast, the
ontologies in Group 4 have almost always high expressivity, for example 27 out
of 66 contain nominals.

8 The acronyms denote RightIneffectiveCardiacFunction, IneffectiveCardiacFunction,

isSpecificFunctionOf, RightSideOfHeart.

C. Del Vescovo et al.

Despite the correlation between the impact of the ELI-fication and the differences observed between MEX- and 
-modules, we cannot claim that there is a
causation between the two events. Indeed, we have investigated the reasons for
the differences observed between the two kinds of modules, and we have noticed
that in all the cases the culprit is the proliferation of equivalence axioms. For
example A  B will end up in the 
-mod for any seed signature containing
either A or B. It is, however, an mCE of  w.r.t. to either {A} or {B}.

The experimental results in view of this insight are summarized as follows:

Random-modules experiment: the 66 ontologies where differences between
-modules were observed, coincide exactly with those

random MEX- and 
where equivalences occur in the ELI-TBox.
Genuine-modules experiment: all 61 ontologies where differences between
genuine MEX- and 
-modules were observed contain equivalence axioms.
We conjecture that the low expressivity of the ELI-language reduce the possibility of MEX- and 
-modules to differ only to the presence of equivalences.
In addiction to the empirical evidence for such a claim, we plan to investigate
further this aspect in future work.

5 Conclusion and Outlook

Summary. We obtain three main observations from our experiments. (1) In
general, there is no or little difference between semantic and syntactic locality.
Hence, the computationally cheaper syntactic locality is a good approximation of
semantic locality. (2) In most cases, there is no or little difference between LBMs
and MEX modules. (3) Though in principle hard to compute, semantic LBMs
can be extracted rather fast in practice. Still, their extraction often takes considerably longer than that of syntactic LBMs. We cannot make any statement
about MEX module extraction times because we use the original MEX imple-
mentation, which combines loading and module extraction. Due to results (1)
and (2), hardly any benefit can be expected from preferring potentially smaller
modules (MEX or semantic LBMs) to cheaper syntactic LBMs. For the ontologies Galen and People, which are renowned for having disproportionately large
modules, syntactic and semantic LBMs do not differ. Only for Galen are MEX
modules considerably smaller than LBMs.

Not only does our study evaluate how good the cheap syntactic locality approximates semantic locality and model conservativity, it also required us to
provide the first implementation for extracting modules based on semantic lo-
cality. Furthermore, we have been able to fix bugs in the existing implementation
of syntactic modularity. A complete report of bugfixes is beyond the scope of
the paper; as an example, early runs of the experiment led us to correcting the
treatment of reflexivity axioms by the locality checker in the OWL API.
Future Work. Two issues are interesting for future work: (1) Sampling seed
signatures so that all sizes of signatures are equally likely to be sampled; (2) Comparing LBMs to other types of conservativity-based modules.
?

?

?
As for (1), the current sampling causes small and large signatures to be un-
derrepresented. One might argue that, for big ontologies, the typical module
extraction scenario does not require large seed signatures  but it does sometimes require relatively small seed signatures, for example, when a module is
extracted to efficiently answer a certain entailment query of typically small size.
We therefore plan to conduct a similar experiment using other sampling methods.
Concerning (2), one could include, for example, the technique based on reduction to QBF for the OWL 2 QL profile [16] when an off-the-shelf implementation
becomes available.
