ORCHID  Reduction-Ratio-Optimal Computation

of Geo-spatial Distances for Link Discovery

Axel-Cyrille Ngonga Ngomo

Department of Computer Science

University of Leipzig

Johannisgasse 26, 04103 Leipzig

ngonga@informatik.uni-leipzig.de

http://limes.sf.net

Abstract. The discovery of links between resources within knowledge bases is
of crucial importance to realize the vision of the Semantic Web. Addressing this
task is especially challenging when dealing with geo-spatial datasets due to their
sheer size and the potential complexity of single geo-spatial objects. Yet, so far,
little attention has been paid to the characteristics of geo-spatial data within the
context of link discovery. In this paper, we address this gap by presenting Orchid,
a reduction-ratio-optimal link discovery approach designed especially for geospatial data. Orchid relies on a combination of the Hausdorff and orthodromic
metrics to compute the distance between geo-spatial objects. We first present two
novel approaches for the efficient computation of Hausdorff distances. Then, we
present the space tiling approach implemented by Orchid and prove that it is optimal with respect to the reduction ratio that it can achieve. The evaluation of our
approaches is carried out on three real datasets of different size and complexity.
Our results suggest that our approaches to the computation of Hausdorff distances
require two orders of magnitude less orthodromic distances computations to compare geographical data. Moreover, they require two orders of magnitude less time
than a naive approach to achieve this goal. Finally, our results indicate that Orchid
scales to large datasets while outperforming the state of the art significantly.

Keywords: Link discovery, Record Linkage, Deduplication, Geo-Spatial Data,
Hausdorff Distances.

1 Introduction

The Linked Open Data Cloud (LOD Cloud) has developed to a compendium of approximately 300 datasets over the last few years. Currently, geographic data sets contain
approximately 6 billion triples and make up 19.4% of the triples in the LOD Cloud.
Projects such as LinkedGeoData1 promise an increase of these numbers by orders of
magnitude in the near future. However, only 7.1% of the links between knowledge
bases in the LOD Cloud currently connect geographic entities. This means that less
than 1% of triples within the geographic datasets of the LOD Cloud are links between

1 See http://linkedgeodata.org. Last access: January 11th, 2013.

H. Alani et al. (Eds.): ISWC 2013, Part I, LNCS 8218, pp. 395410, 2013.
c Springer-Verlag Berlin Heidelberg 2013

A.-C. Ngonga Ngomo

knowledge bases.2 This blatant lack of links is partly due to two factors: First, it is due
to the large number of geo-spatial entities available on the Linked Open Data Cloud.
Moreover, the geo-spatial resources are often described as (often ordered) sets of points
which describe geometric objects such as (multi-) polygons or (multi-) polylines. This
way of describing resources differs considerably from the approach followed for most
Linked Data resources, which are commonly easiest identified by the means of a label.
Consequently, such descriptions have not yet been payed much attention to in the field
of link discovery (LD).

We address this gap by presenting Orchid, a reduction-ratio-optimal approach for
LD. Orchid assumes the LD problem as being formulated in the following way: Given
a set S of source instances, a set T of target instances and a distance threshold , find the
+ such that (s, t)  . Given this assumption, the
set of triples (s, t, (s, t))  S  T  R
idea behind Orchid is to address the LD problem on geographic data described as (or-
dered) sets of points by two means. First, Orchid implements time-efficient algorithms
for computing whether the distance between two polygons s and t is less or equal to a
given distance threshold . Moreover, Orchid implements a space tiling algorithm for
orthodromic spaces which allows discarding yet another large number of unnecessary
computations.

The rest of this paper is structured as follows: In Section 2, we present the core
notation used throughout this paper as well as some formal considerations underlying
our approach. Section 3 presents two approaches that allow computing the Hausdorff
distance between two polygons efficiently.3 Subsequently, we present the space discretization approach implemented by Orchid and show that it is optimal with respect
to its reduction ratio. We then present a thorough evaluation of our approach on three
datasets of different sizes and complexity. We also compare our approach with a state-
of-the-art LD framework which implements the orthodromic distance. We conclude the
paper with a brief overview of related work (Section 6) and a discussion of our results
(Section 7). The approach presented here was integrated in the LIMES framework.4 Due
to space restrictions, we had to omit some details of the approaches presented herein.
These can be found in the corresponding technical report on the project webpage.

2 Preliminaries

The formal specification of LD adopted herein is tantamount to the definition proposed
in [11]: Given a set S of source resources, a set T of target resources and a relation
R, our goal is to find the set M  S  T of pairs (s, t)  S  T such that R(s, t). If
R is owl:sameAs, then we are faced with a deduplication task. Given that the explicit
computation of M is usually a very complex endeavor, M is usually approximated by
+ : (s, t)  }, where  is a distance function and
a set  M = {(s, t, (s, t))  S  T  R
  0 is a distance threshold. For geographic data, the resources s and t are described
2 See

http://wifo5-03.informatik.uni-mannheim.de/lodcloud/state/

for

an

overview of the current state of the Cloud. Last access: January 11th, 2013.

3 The Hausdorff distance can be used to compare the distance between any two sets of ordered
points located in a space where a distance function is defined. Thus, while we focus on polygons in this paper, our approach can be used for all sets of points.

4 http://limes.sf.net
?

?

?
by using single points or (ordered) sets of points, which we regard as polygons. Given
that we can regard points as polygons with one node, we will speak of resources being
described as polygons throughout this paper. We will use a subscript notation to label
the nodes that make up resources. For example, if s had three nodes, we would denote
them s1, s2, and s3. For conveniences sake, we will write s = {s1, s2, s3} and si  s.

While there are several approaches for computing the distance between two poly-

gons [2], a common approach is the use of the Hausdorff distance [14] hd:

hd(s, t) = max
sis

{min
t jt

{(si, t j)}},

(1)

where  is the metric associated to the affine space within which the polygons are de-
fined. We assume that the earth is a perfect ball with radius R = 6378 km. Then, 
is the orthodromic distance and will be denoted od in the rest of this paper. Given
these premises, the LD task we investigate in this paper is the following: Find the set
 M = {(s, t, hd(s, t))  S  T : hd(s, t)  } where si  s t j  t (si, t j) = od(si, t j). It
is important to notice that the orthodromic distance is known to be a metric, leading to
the problem formulated above being expressed in a metric space.

Two requirements are central for the approaches developed herein. First, the approaches have to be complete (also called lossless [11]), which simply means that they
+ for which hd(s, t)  
must be able to compute all triples (s, t, hd(s, t))  S  T  R
holds. This characteristic is not fulfilled by certain blocking approaches, which trade
runtime efficiency for completeness. In addition to developing a complete approach,
we aim to develop a reduction-ratio-optimal approach [10]: Let A be an algorithm for
computing  M and  be the vector that contains all parameters necessary to run A. More-
over, let |A()| be the number of computations of hd carried out by A when assigned
the vector of parameters . We call A() reduction-ratio-optimal when

r < 1  |  M|

|S||T|  : 1  |A()|

|S||T|  r.

(2)
Naive approaches to computing  M have two drawbacks: First, they require |s||t| calls
of od to compute hd(s, t). Moreover, they carry out |S||T| computations of hd to find all
elements of  M. Addressing the time complexity of LD on geographic data thus requires
addressing these two quadratically complex problems. Our approach addresses the time
complexity of the first problem by making use of the Cauchy-Schwarz inequality, i.e.,

od(x, y)  od(x, z) + od(z, y),

(3)

and of bounding circles for approximating the distance between polygons. The second
problem is addressed by the means of a reduction-ratio-optimal tiling approach similar
to the HR3 algorithm [10].

3 Efficient Computation of Hausdorff Distances

Several approaches have addressed the time-efficient computation of Hausdorff distances throughout literature (see [14] for a good overview). Yet, so far, these approaches

A.-C. Ngonga Ngomo

have not been concerned with the problem of only finding those triples (s, t, hd(s, t))
with hd(s, t)  . In the following, we present several approaches for achieving this
goal. These approaches are later evaluated in Section 5. For space reasons, we omit the
pseudo-code for the first two approaches. These can be found in the technical report.

3.1 Naive Approach

The naive approach for computing hd(s, t) would compare all elements of the polygon
s  S with all elements of the polygons t  T by computing the orthodromic distance
between all si  s and t j  t. Let  S be the average size of the polygons in S and  T be
the average size of the polygons in T . The best- and worst-case runtime complexities of
the naive approach are then O(|S||T|  S  T ).

3.2 Bound Approach
A first idea to make use of the bound hd(s, t)   on distances lies in the observation
that

si  S : min
t jt

{od(si, t j)} >   hd(s, t) > 

(4)

This insight allows terminating computations that would not lead to pairs for which
hd(s, t)   by terminating the computation as soon as a si is found that fulfills Eq.
(4). In the best case, only one point of each s  S is compared to all points of t  T
before the computation of hd(s, t) is terminated. Thus, the best-case complexity of the
approach is O(|S||T|  T). In the worst case (i.e., in the case that the set of mappings
returned is exactly S  T ), the complexity of the bound approach is the same as that of
the naive approach, i.e., O(|S||T|  S  T ).

3.3 Indexed Approach

The indexed approach combines the intuition behind the bound approach with geometrical characteristics of the Hausdorff distance by using two intuitions. The first intuition
is that if the minimal distance between any point of s and any point of t is larger than
, then hd(s, t) >  must hold. Our second intuition makes use of the triangle inequality
to approximate the distances od(si, tk). In the following, we present these two intuitions
formally. We dub the indexed approach which relies on the second intuition alone CS
while we call the indexed approach that relies on both intuitions BC + CS .

Intuition 1: Bounding Circles. Formally, the first intuition can be expressed as
follows:

{od(si, t j)} >   hd(s, t) > .

min
sis, t jt

(5)
Finding the two points si and t j which minimize the value of od(si, t j) requires O(|s||t|)
computations of od, i.e., O(|S||T|  S  T ) overall. However, a lower bound for this minimum
for all pairs (s, t)  S  T can be computed efficiently by using encompassing circles:
Let C(s) resp. C(t) be the smallest circles that fully encompass s resp. t. Moreover, let
?

?

?
r(s) resp. r(t) be the radius of these circles and (s) resp. (t) be the centers of the circles
C(s) resp. C(t). Then,

{od(si, t j)} > od((s), (t))  (r(s) + r(t)) = (s, t).

(6)

min
sis,t jt

Figure 1 displays the intuition behind this approximation graphically. Note that this
equation also holds when the circles overlap (in which case od((s), (t))(r(s)+r(t)) <
0 as od((s), (t)) < (r(s) + r(t)).

s 
(s) 
r(s) 

(s,t) 

t 
(t) 

r(t) 

Fig. 1. Lower bound of Hausdorff distances based on circles

Computing the smallest circle that encompasses any polygon x can be carried out in

O(|x|2) by simply computing od(xi, xk) for all (xi, xk)  x2. Then,

r(x) =

max
xix,xkx

od(xi, xk)

while

(x) =



x+ + x

where (x+, x



) = arg max
xix,xkx

od(xi, xk).

(7)

(8)



) = arg max
xix,xkx

are at most at a distance 2r

The proof that the radius r(x) must have the value shown in Equation 7 is as follows:

of each other.
The points within a circle with radius r
 < r(x) cannot contain both elements of the
Consequently, any circle with radius r
pair (x+, x
od(xi, xk). Thus, the smallest possible radius of a circle that
encompasses x fully must be the maximal distance between points which belong to x.
This is exactly the value of r(x). Now the only way to ensure that a circle with radius
r(x) really encompasses all points in x is to have x+ and x
to be diametrically opposite.
Thus, (x) must be exactly in the middle of x+ and x
The runtime complexity of this approximation is O(|S|  S 2 +|T|  T 2 +|S||T|). O(|S|  S 2 +
|T|  T 2) computations of od are required to determine the circles and their radii while
O(|S||T|) computations are required to compare the circles computed out of S with
those from T . Note that for large problem  S 2 resp.  T 2 are very small compared to |S|
resp. |T|, leading to O(|S|  S 2 + |T|  T 2 + |S||T|)  O(|S| + |T| + |S||T|)  O(|S||T|).
?

?

?




.

Intuition 2: Distance Approximation Using the Cauchy-Schwarz Inequality. Now
given that we have computed all distances between all pairs (t j, tk)  t2, we can reuse
this information to approximate distances from any si to any tk by relying on the
Cauchy-Schwarz inequality in a fashion similar to the LIMES algorithm presented

A.-C. Ngonga Ngomo

in [12]. The idea here is that we can compute an upper and a lower bound for the
distance od(si, tk) by using the distance od(si, t j) previously computed as follows:

|od(si, t j)  od(t j, tk)|  od(si, tk)  od(si, t j) + od(t j, tk).

(9)

od(tx, tk)

txt

For each si, exploiting these pre-computed distances can be carried out as follows:
For all tk for which od(si, tk) is unknown, we approximate the distance from si to tk by
finding a point t j for which

t j = arg min

(10)
  t is the set of points tx of t for which od(si, tx) is known. We call the
holds, where t
point t j an exemplar for tk. The idea behind using one of points closest to tk is that it
gives us the best possible lower bound |od(si, t j)  od(t j, tk)| for the distance od(si, tk).
Now if |od(si, t j)  od(t j, tk)| > , then we can discard the computation of the distance
od(si, tk) and simply assign it any value  > . Moreover, if |od(si, t j)  od(t j, tk)| is
larger than the current known minimal distance between si and points in t, then we can
also discard the computation of od(si, tk). If such an exemplar does not exist or if our
approximations fail to discard the computation, then only do we compute the real value
of the distance od(si, tk).
The best-case complexity of this step alone would be O(|S||T|  S ) while in the worst
case, we would need to carry out O(|S||T|  S  T ) computations of od. The overall complexity of the indexed approach is O(|S|  S 2 + |T|  T 2 + |S||T|) (i.e., that of the bounding
circles filter) in the best case and O(|S|  S 2 + |T|  T 2 + |S||T| + |S||T|  S  T ) in the worst case.
The overall algorithm underlying the indexed approach is shown in Algorithm 1.

4 ORCHID

Although the indexed method presented above can significantly reduce the number of
computations carried out to compare S and T , it still needs at least |S||T| comparisons.
For example, imagine our source and target data sets were all geo-spatial entities on
the portion of the surface of the planet shown in Figure 2. If Oslo (which has the coordinates (59?5658 N, 10?4523 E)) was the resource to link via dbp:near, then
the approaches above would compare it with each of the other elements of the dataset.
The idea behind Orchid is to reduce the number of comparisons even further while remaining complete and being reduction-ratio-optimal. To achieve this goal, Orchid uses
a space discretization approach and only compares polygons t  T which lie within
a certain range of s  S . An example of the discretization generated by Orchid is
shown in Figure 2. Instead of comparing Oslo with all other elements of the dataset,
Orchid would only compare it with the geo-spatial objects shown in the gray cells.
In the following, we present Orchid formally and prove that it is both complete and
reduction-ratio-optimal.

4.1 Preliminaries

Explaining the approach implemented by Orchid prerequisites the explication of a set
of characteristics of the orthodromic distance od. Given a polygon s, finding all points
?

?

?
e = exemplar(t j)
if e   then

approx = |od(si, e)  od(e, t j)|
if approx >   approx > min then

return 
max  0
for si  s do
min  
for t j  t do

Algorithm 1. Implementation of the BC + CS Hausdorff distance computation. The
implementation of CS lacks lines 1,2,3 and 28.
1: if (od(c(s), c(t))  r(s)  r(t) > ) then
2:
3: else
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28: end if

end for
if max >  then

return 

d(si, t j) = od(si, t j)

end if
min = min(min, d(si, t j))

end for
max = max(max, min)

else

end if

return max

else

end if

else

d(si, t j) =  + 1

d(si, t j) = od(si, t j)

t such that hd(s, t)   requires being able to find all points y for which od(x, y)  
given a point x. In general, a point x on the surface of the planet can be characterized by
two values: its latitude lat(x) and its longitude long(x). These two values are bound as
/2  lat(x)  /2 and   lon(x)   always hold.5 We write x = (lat(x), lon(x))
to denote points. Now, given a point y with lon(x) = lon(y), then od(x, y) = R|lat(x) 
lat(y)|. Yet, if lat(x) = lat(y), then od(x, y) = R|lon(x)  lon(y)|cos(lat(x)). This difference between latitude and longitude is central when finding all points y for which
od(x, y)  . Formally, it means that we can create a discretization in which we treat the
latitude values independently from the longitude values but not the other way around.
This particular characteristic of latitude and longitude values lies at the heart of Orchid.

4.2 Discretization for Geo-Spatial Points
The idea behind Orchid is to make use of the values of latitude and longitude being
bound to first create a grid on the surface of the planet. We call   N the granularity
5 All angles in this paper are assumed to be in radian unless stated otherwise.

A.-C. Ngonga Ngomo

64 

62 

60 

58 

56 

Hamer 

Oslo 

Mora 

Stavanger 

Friederikstad 

Stockholm 

Boras 

2 

4 

6 

8 

10 

12 

14 

16 

18 

20 


Fig. 2. Example of tiling for  = 1 and  = 222.6km (i.e., R = 2
Oslo. The gray cells are the elements of A(Oslo).

). Here, the resource to link is

parameter of Orchid. Given the premises described in Section 4.1, we can infer that for
x  s  S and t  T  T

od(x, y)    |lat(x)  lat(y)|  /R = R.

(11)

and clon

Based on this equation, we can create a grid such that width and height of each cell of
the grid is R = R/. For each cell ci, two whole numbers clat
exist such that
i
ci contains only points x for which
  lat(x) < (clat
, clon

(12)
) the coordinates of ci. Moreover, we write x  ci if Equation 12
holds. We call (clat
i
holds for x. We also write ci(x) to signify the cell to which x belongs. In our example
(Figure 2), the cell which contains Oslo has the coordinates (29, 5). Given this definition
of a grid, the set  M(x) of y with od(x, y)   is clearly a subset of all y for which |lat(x)
lat(y)|  R holds. With respect to our grid, we can infer the following inequality:

+ 1))  (clon

  lon(x) < (clon

+ 1))

i

i

i

(clat
i

i

i

x  ci  y  c j  |clat

i

 clat

j

| >   y   M(x).

(13)

We call the set of all cells which abide by this inequation LAT (x). Finding a similar
equation for longitudes is more demanding, as the equation depends on the latitude of
 M(x) of y with od(x, y)   is clearly a subset of all
cells ci and c j. Formally, the set
y for which |lat(x)  lat(y)|  R/ min{cos(lon(x), lon(y))} holds. Consequently, we can
derive the following equation:
?

?

?
x  ci  y  c j  |clon

i

 clon

j

| >



mincos(ci, c j)

 y   M(x)

(14)

where

mincos(ci, c j) = min{cos(ci), cos((ci + 1)), cos((c j)), cos((c j + 1)).

(15)

We call this set LON(x). Now, if one the minimal cosine values in Equation 15 is 0, then
Equation 14 is not well-defined. This happens when one of the cells ci or c j is adjacent to
?

?

?


mincos(ci,c j)

= 0. This assumption has the simple
one of the poles. In this case, we assume
consequence that we select all cells c j at the poles to contain potential y with od(x, y) 
 M(x) by computing the intersection
. We can now generate a first approximation of
of all y that abide by Equations 13 and 14. We call this set A(x) = LAT (x)  LON(x).
Note that  M(x)  A(x). An example of such a set is shown in Figure 2 where A(Oslo) is
depicted as a set of gray squares. Note that given that  = 1, we only need to consider
 30. Yet, given that cos(60?) = 0.5, the number of cells that
the cells with 28  clat
have to be considered in longitude grows from 5 to 7 when crossing the 60th north
parallel.

i

4.3 Optimality of Orchid for Points
 M(x)  A(x), it is possible that A(x) contains grid cell c
While it is guaranteed that
with y  c, (x, y, od(x, y))   M.6 Such cells must be eliminated from A(x) as they lead
to unnecessary comparisons. Achieving this goal can be carried out by measuring the
minimal distance from the cell c(x) which contains x and all other cells c  A(x). Let us
assume that c is at the north east of c(x) (for reasons of symmetry, the argumentation can
be extended to all other cells). In our example, such a cell would be that which contains
Mora. Then the most north eastern point of ne(c(x)) has the coordinates (clat
i (x) +
1, clon
(x) + 1) while the most south western point of sw(c) of c has the coordinates
i
(clat
, clon
i

). Consequently, the minimal distance from points in c(x) to points in c is

i

(c(x), c) = od((clat

min
od

i (x) + 1, clon
i

(x) + 1), (clat

i (x), clon
i

)).

We thus define the set OPT (x)  A(x) as

OPT (x) = {y  A(x) : min

(c(x), c(y))  }.

od

(16)

(17)

This set is guaranteed not to contain any cell with which elements of c(x) should be
compared. Consequently, it is the set of points x and all other elements of c(x) are
compared to by Orchid.

OPT (x) is optimal in the sense that

+ OPT (x) =  M(x).
lim

(18)
This is simply due to   + leading to   0. In this case, c(x) = {x} and c(y) = {y}.
Thus, minod(c(x), c(y)) = od(x, y) which allows to infer that OPT (x) =  M(x) from
Equation 17. Note that this proof shows that Orchid fulfills a necessary and sufficient
condition to be reduction-ratio-optimal on single points in the sense of [10]. In our
example A(Oslo) = OPT (Oslo).

4.4 Comparing Polygons with Orchid

The extension of OPT (x) to polygons is based on the following observation: Given the
definition of Hausdorff distances,

hd(s, t)    si  s t j  t : od(si, t j)  

(19)

6 Note that od(x, y) = hd(x, y) for |x| = |y| = 1.

A.-C. Ngonga Ngomo

holds. Consequently, OPT (s) =
?

?

?
sis

for polygons follows from its reduction-ratio-optimality for points.

OPT (si). The reduction-ratio optimality of Orchid

5 Evaluation

The goal of the evaluation was to assess the performance of our approaches with respect
to their runtime and the number of computation of the orthodromic distance that they
carried out. To achieve this goal, we first compare the naive, bound, CS and BC+CS implementations of the computations of bound Hausdorff distance on samples from three
different datasets. Note that we refrained from using the whole datasets because the
runtime of the naive approach would have been impracticable. In the second part of our
evaluation, we study the combination of Orchid and our Hausdorff implementations.

5.1 Experimental Setup

We selected three publicly available datasets of different sizes for our experiments. The
first dataset, Nuts, contains a detailed description of 1,461 specific European regions.7
The second dataset, DBpedia, contains all 731,922 entries from DBpedia that possess
a geometry entry.8 Finally, the third dataset, LGD, contains all 3,836,119 geo-spatial
objects from LinkedGeoData that are instances of the class Way.9 An overview of the
distribution of the polygon sizes in these datasets is given in Figure 3. In addition, we
used a dataset that consists of all points which have the wgs84:geometry property10
from DBpedia for the comparison with SILK.11 The 732,224 entities in this dataset are
single points on the surface of the planet. We used this dataset because SILK 2.5.3 does
not yet support the Hausdorff distance but implements the orthodromic distance.
?

?

?
(a) Nuts
?

?

?
(b) DBpedia
?

?

?
(c) LGD

Fig. 3. Distribution of polygon sizes

All experiments were carried out on a 32-core server running JDK 1.7 on Linux
10.04. The processors were 8 quadcore AMD Opteron 6128 clocked at 2.0 GHz. Unless stated otherwise, each experiment was assigned 10GB of memory and was ran 5

7 We used version 0.9.1 as available at http://nuts.geovocab.org/data/
8 We used version 3.8 as available at http://dbpedia.org/Datasets
9 We used the RelevantWays dataset (version of April 26th, 2011) of LinkedGeoData as avail-

able at http://linkedgeodata.org/Datasets

10 wgs84 stands for http://www.w3.org/2003/01/geo/wgs84_pos#
11 The dataset was extracted from the RelevantNodes dataset (version of April 26th, 2011) of

DBpedia as available at http://linkedgeodata.org/Datasets
?

?

?
times. The time-out for experiments was set to 3 hours per iteration. The granularity
parameter  was set to 1. In the following, we present the minimal runtime of each of
the experiments.

5.2 Results
Hausdorff Implementations.
In the first part of our evaluation, we measured the
runtimes achieved by the three different implementation of the Hausdorff distances
on random samples of the Nuts, DBpedia and LGD data sets. We used three different thresholds for our experiments, i.e., 100 m, 0.5 km and 1 km. In Figure 4, we present
the results achieved with a threshold of 100 m. The results of the same experiments for
0.5 km and 1 km did not provide us with significantly different insights. All exact values
can be found on the project website. As expected the runtime of all three approaches
increases quadratically with the size of the sample. There is only a slight variation in the
number of comparisons (see Figure 4) carried by the three approaches on the DBpedia
dataset. This is simply due to most polygons in the dataset having only a small number
of nodes as shown in Figure 3. With respect to runtime, there is no significant difference
between the different approaches on DBpedia. This is an important result as it suggests
that we can always use the CS or BC + CS approaches even when the complexity of
the polygons in the datasets is unknown.

On the two other datasets, the difference between the approaches with respect to both
the number of comparisons and the runtime can be seen clearly. Here, the bound implementation requires an order of magnitude less comparisons than the naive approach
while the indexed implementations need two orders of magnitude less comparisons. The
runtimes achieved by the approaches reflect the observations achieved on the compar-
isons. In particular, the bound approach is an order of magnitude faster than the naive
approach. Moreover, the BC + CS approach outperforms the bound approach by approximately one further order of magnitude. Note that up to approximately 1.07% of
the comparisons carried out by BC + CS are the result of the indexing step.

Deduplication. In our second series of experiments, we deduplicated the three datasets
at hand by using four different thresholds between 100 m and 2 km. We compared the
combination of Orchid ( = 1) and of all different implementations of the Hausdorff
distance. The rationale behind this experiment was to measure whether the bound and
indexed implementations were of any use even within the smaller sub-problems generated by Orchid. The results achieved show that using these implementations can indeed
lead to significant improvements in both runtime and comparisons (see Figure 5). In
particular, the indexed distance profits from the fact that it can discard a large number
of computations that would lead to distance below and above the distance threshold.
Thus, it requires over than two orders of magnitude less computations than the bound
and naive versions on the Nuts dataset. Given the small size of the index that it generates for Nuts, the indexed approach is also two orders of magnitude faster across all the
thresholds. On the LGD dataset, the indexed approach is the only one that terminated
within the set time of 3 hours. Due to the topology of the DBpedia data, the runtimes
on DBpedia are comparable for all approaches. Here, it is important to note that for
smaller thresholds, the indexed approach still requires close to an order of magnitude
less comparisons than the naive approach.

A.-C. Ngonga Ngomo
?

?

?
(a) Comparisons on Nuts
?

?

?
(c) Comparisons on DBpedia
?

?

?
(e) Comparisons on LGD
?

?

?
(b) Runtime on Nuts
?

?

?
(d) Runtime on DBpedia
?

?

?
(f) Runtime on LGD
?

?

?
Fig. 4. Number of comparisons and runtimes on samples of the datasets

Scalability. We were also interested in knowing how our approach performs with growing dataset sizes. We thus ran Orchid in combination with BC with randomly selected
slices of LinkedGeoData and DBpedia and computed the runtime against the size of
the data slices. The similarity threshold was set to 0.1 km as in the previous experiment.
The results on DBpedia and LinkedGeoData are shown in Table 1. We omitted Nuts
because it is too small for scalability experiments. The runtimes and number of comparisons on DBpedia suggest that the approach behaves in a quasi-linear fashion on lowdimensional and sparsely distributed data. Note that the number of mappings because
partly larger than the number of computations on this dataset is simply due to items with
the same URI being found in both source and target and thus not necessitating any comparisons for deduplication. This is more rarely the case in the LinkedGeoData dataset.
The runtimes on LinkedGeoData yet suggest that both the number of computations and
the runtime required of our approach grow sub-linearly with the number of mappings
to be computed when the number of points per polygon grows. This can be explained
?

?

?
(a) Comparisons on Nuts
?

?

?
(c) Comparisons on DBpedia
?

?

?
(e) Comparisons on LGD
?

?

?
 
?

?

?
 
?

?

?
(b) Runtime on Nuts
?

?

?
(d) Runtime on DBpedia
?

?

?
(f) Runtime on LGD
?

?

?
Fig. 5. Number of comparisons and runtime of Orchid

by our approach making effective use of existing data to discard computations and reduce the ratio of number of computations to mappings with growing data size. Thus,
our approach promises to scale well to even larger data sets.

Comparison with Other Approaches. SILK12 [6] is of the few other LD framework
which implements the orthodromic distance. To the best of our knowledge, no other LD
framework implements the Hausdorff distance. Thus, we compare Orchid in combination with the naive implementation of the Hausdorff distance to SILK on all 732,224
points from DBpedia that contain longitude and latitude information. The results of
four different distance thresholds are shown in Figure 6. Our results clearly show that
Orchid outperforms SILK by more than one order of magnitude in all settings.

12 Throughout our experiments, we used SILK 2.5.3.

A.-C. Ngonga Ngomo

Table 1. Scalability results. The top section shows the results on DBpedia while the lower section
shows the results on LinkedGeoData.

Sample Size od computations Runtime (ms) Mappings

2  105
4  105
7.3  105

2  105
4  105
8  105
16  105

34,959
97,798
341,986
1,035,222

5,703,683
11,734,609
24,844,435
55,212,459
131,405,064

2,936
5,783
10,423
20,727

103,428
215,096
459,681
932,848

77,003
42,437
159,878
57,935
342,477
153,174
411,248
777,826
819,636 1,902,803
?

?

?
Fig. 6. Comparison of runtime of SILK and Orchid

6 Related Work

The work presented herein is related to record linkage, deduplication, LD and the efficient computation of Hausdorff distances. An extensive amount of literature has been
published by the database community on record linkage (see [7,4] for surveys). With
regard to time complexity, time-efficient deduplication algorithms such as PPJoin+ [19],
EDJoin [18], PassJoin [8] and TrieJoin [17] were developed over the last years. Several
of these were then integrated into the hybrid LD framework LIMES [11]. Moreover,
dedicated time-efficient approaches were developed for LD. For example, RDF-AI [15]
implements a five-step approach that comprises the preprocessing, matching, fusion,
interlink and post-processing of data sets. [12] presents an approach based on the
Cauchy-Schwarz that allows discarding a large number of unnecessary computations.
The approaches HYPPO [9] and HR3 [10] rely on space tiling in spaces with measures
that can be split into independent measures across the dimensions of the problem at
hand. Especially, HR3 was shown to be the first approach that can achieve a relative
less or equal to any given relative reduction ratio r > 1. Standard
reduction ratio r
blocking approaches were implemented in the first versions of SILK and later replaced
with MultiBlock [6], a lossless multi-dimensional blocking technique. KnoFuss [13]
also implements blocking techniques to achieve acceptable runtimes.
?

?

?
Hausdorff distances are commonly used in fields such as object modeling, computer
vision and object tracking. [1] presents an approach for the efficient computation of
Hausdorff distances between convex polygons. While the approach is quasi-linear in
the number of nodes of the polygons, it cannot deal with non-convex polygons as commonly found in geographic data. [5] presents an approach for the comparison of 3D
models represented as triangular meshes. The approach is based on a subdivision sampling algorithm that makes use of octrees to approximate the distance between objects.
[16] present a similar approach that allows approximating Hausdorff distances within
a certain error bound while [3] presents an exact approach. [14] present an approach
to compute Hausdorff distances between trajectories using R-trees within an L2-space.
Note that our approach is tailored to run in orthodromic spaces. Still, some of the insights presented in [14] may be usable in an orthodromic space. To the best of our
knowledge, none of the approaches proposed before address the problem of finding
pairs of polygons (A, B) such that hd(A, B)   in an orthodromic space.

7 Conclusion and Future Work

In this paper, we presented Orchid, a LD approach for geographic data. Our approach
is based on the combination of Hausdorff and orthodromic distances. We devised two
approaches for computing bound Hausdorff distances and compared these approaches
with the naive approach. Our experiments showed that we can be more than two orders
of magnitude faster on typical geographic datasets such as Nuts and LinkedGeoData.
We then presented the space tiling approach which underlies Orchid and proved that
it is reduction-ratio-optimal. Our most interesting result was that our approach seems
to be sub-linear with respect to the number of comparisons and the runtime it requires.
This behavior can be explained by the approach making use of the higher data density to
perform better distance approximations and thus discarding more computations of the
orthodromic distance. In addition to comparing different parameter settings of Orchid
with each other, we also compared our approach with the state-of-the-art LD framework
SILK. Our results show that we outperform the blocking approach it implements by
more than one order of magnitude. In future work, we will extend our approach by
implementing it in parallel and integrating it with a load balancing approach.

Acknowledgement. The work presented in this paper was financed by the EU-FP7
Project GeoKnow (Grant Agreement No. 318159).
