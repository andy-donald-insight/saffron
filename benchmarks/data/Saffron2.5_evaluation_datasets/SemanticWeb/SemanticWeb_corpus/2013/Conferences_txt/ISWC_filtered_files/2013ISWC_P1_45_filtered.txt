A Confidentiality Model for Ontologies

Piero A. Bonatti and Luigi Sauro

Dept. of Electrical Engineering and Information Technologies

Universit`a di Napoli Federico II

Abstract. We illustrate several novel attacks to the confidentiality of knowledge
bases (KB). Then we introduce a new confidentiality model, sensitive enough to
detect those attacks, and a method for constructing secure KB views. We identify
safe approximations of the background knowledge exploited in the attacks; they
can be used to reduce the complexity of constructing secure KB views.

1 Introduction

There is ample evidence of the need for knowledge confidentiality measures. OWL
and the LOD paradigm are increasingly being used to encode the private knowledge of
companies and public organizations. Linked open government data include potentially
sensitive information, e.g. related to health. Medical records are annotated with semantic metadata based on SNOMED. FOAF assertions and other semantic description of
social networks may affect the privacy of individuals. In all of these cases, semantic web
techniques help in linking different knowledge sources and extract implicit information,
thereby increasing security and privacy risks. Even the authors of public ontologies may
want to hide some axioms to capitalize on their formalization efforts. See [8] for further motivations. In order to tackle the confidentiality requirements arising from these
scenarios, several approaches have been proposed. The most popular security criterion
is that the published view of the knowledge base should not entail any secret sentence
(we call it simple confidentiality model). However, there exist attacks that cannot be
blocked this way. The user may exploit various sources of background knowledge and
metaknowledge to reconstruct the hidden part of the knowledge base. This paper contributes to the area of knowledge base confidentiality in several ways:

(i) It highlights some vulnerabilities of the approaches that can be found in the liter-

ature, including attacks based on meta-reasoning (Sec. 3).

(ii) It introduces a stronger confidentiality model that takes both object-level and
meta-level background knowledge into account (Sec. 4), and it defines a method for
computing secure knowledge views (Sec. 5) that generalizes some previous approaches.
(iii) It proposes a safe approximation of background metaknowledge (Sec. 6 and 7).
(iv) It investigates the computational complexity of constructing secure knowledge

base views with our methodology (Sec. 7).

The paper is closed by a discussion of related work (Sec. 9), and conclusions. Some

proofs are omitted due to space limitations.

H. Alani et al. (Eds.): ISWC 2013, Part I, LNCS 8218, pp. 1732, 2013.
c Springer-Verlag Berlin Heidelberg 2013

P.A. Bonatti and L. Sauro

2 Preliminaries on Description Logics

We assume the reader to be familiar with description logics, and refer to [1] for all definitions and results. We assume a fixed, denumerable signature  specifying the names
of concepts, roles, and individuals. Our framework is compatible with any description
logic DL that enjoys compactness (needed by Theorem 6) and has decidable reasoning
problems (e.g., ALC, EL, SHIQ, etc.). We simply assume that our reference logical
language L is generated from  by the grammar of the selected logic DL. By axioms,
we mean members of L, unless stated otherwise. A knowledge base is any subset of L.1
Recall that axioms are expressions of the form C  D, R  S , C(a), and R(a, b)
where C, D are concept expressions, R, S are role expressions, and a, b are individual
constants. In some DL, an individual constant a may occur also in a nominal, that is,
a concept expression {a} denoting the singleton containing a. The axioms involving 
are called inclusions (or subsumptions), while C(a) and R(a, b) are called assertions. In
the simplest case, C and R are first order predicates and assertions are actually standard
first-order atomic formulae. Inclusions are syntactic variants of logical implications.
The notion of logical consequence is the classical one; for all K  L, the logical
consequences of K will be denoted by Cn(K) (K  Cn(K)  L).

3 A Simple Confidentiality Model

The most natural way of preserving confidentiality in a knowledge base KB is checking
that its answers to user queries do not entail any secret. Conceptually, the queries of a
user u are answered using us view KBu of the knowledge base, where KBu is a maximal
subset of KB that entails no secret. In order to illustrate some possible attacks to this
mechanism, let us formalize the above simple confidentiality model (SCM).2 It consists
of: the knowledge base KB (KB  L); a set of users U; a view KBu  KB for each
u  U; a set of secrecies S u  L for each u  U. Secrecies are axioms that may or may
not be entailed by KB; if they do, then they are called secrets and must not be disclosed
to u. Revealing that a secrecy is not entailed by KB is harmless [4]. For example, there
is no need to protect the information that someone is not having chemotherapy.
A view KBu is secure iff Cn(KBu)  S u = . A view KBu is maximal secure if it is
secure and there exists no K such that KBu  K  KB and Cn(K)  S u = .
Attacks Using Object-Level Background Knowledge. Frequently, part of the domain
knowledge is not axiomatized in KB, therefore checking that Cn(KBu)S u =  does not
suffice in practice to protect confidentiality. For example, suppose that there is one secret
S u = {OncologyPatient(John)} and KBu = {SSN(John, 12345), SSN(user123, 12345),
OncologyPatient(user123)}. KBu does not entail OncologyPatient(John), so according
to the SCM model KBu is secure. However, it is common knowledge that a SSN uniquely
identifies a person, then the user can infer that John = user123, and hence the secret.

In other examples, the additional knowledge used to infer secrets may be stored in a

public ontology or RDF repository, and confidentiality violations may be automated.

1 Real knowledge bases are finite, but this restriction is not technically needed until Sec. 7.
2 This usage of term model is common in Security & Privacy.
?

?

?
Attacks to Complete Knowledge. Suppose the attacker knows that KB has complete
knowledge about a certain set of axioms. Then the attacker may be able to reconstruct
some secrets from the I dont know answers of a maximal secure view KBu.

Example 1. Consider an organizations knowledge base that defines a concept
Employee and a role works for that describes which employees belong to which of
the n departments of the company, d1, . . . , dn. The KB consists of assertions like:

Employee(e)

(1)

works for(e, di)

(2)

where we assume that each employee e belongs to exactly one department di. A user
u is authorized to see all assertions but the instances of (2) with i = n, because dn is a
special department, devoted to classified projects. So S u (the set of secrecies for u) is
the set of all assertions works for(e, dn).

Note that there is one maximal secure view KBu. It consists of all instances of (1),
plus all instances of (2) such that i  n. Clearly, KBu is secure according to SCM
(because Cn(KBu)  S u = ). However, observe that works for(e, dn)  Cn(KB) iff
Employee(e)  Cn(KBu) and for all i = 1, . . . , n, works for(e, di)  Cn(KBu) (that is, the
members of dn are all the employees that apparently work for no department). Using this
property (based on the knowledge that for each employee e, KB contains exactly one
assertion works for(e, di)) and the knowledge of the protection mechanism (i.e. maximal secure views), that we assume to be known by attackers by Kerchoffs principle, a

smart user can easily identify all the members of dn.

In practice, it is not hard to identify complete knowledge. A hospitals KB is expected
to have complete knowledge about which patients are in which ward; a companys KB
is likely to encode complete information about its employees, etc.

Some approaches filter query answers rather than publishing a subset of KB [7,
14, 16]. We call our abstraction of this method simple answer confidentiality model
(SACM). It is obtained from the SCM by replacing the views KBu  KB with answer
views KBa
u is not required to be a subset of KB
u
andconceptuallyKBa

 Cn(KB). The difference is that KBa

u)  S u = .

u may be infinite. KBa

u is secure iff Cn(KBa

The reader may easily verify that the SACM is vulnerable to the two kinds of attacks
illustrated for the SCM. It is also vulnerable to a third kind of attacks, illustrated below.

Attacks to the Signature. Suppose the user knows the signature of KB well enough to
identify a symbol  that does not occur in KB. First assume that  is a concept name.
It can be proved that:

Proposition 1. If KBa
u is a maximal secure answer view and  is a concept name not
occurring in KB, then for all secrecies C  D  S u, KBa
|= C    D iff KB |= C  D.
The problem is that although C    D does not entail the secret inclusion C  D,
still a smart user knows that the former inclusion cannot be proved unless KB entails
also the latter (then maximal secure answer views generally fail to protect secrets). This
attack can be easily adapted to the case where  is a role name. In practice, it is not
necessary to be sure that  does not occur in KB. The attacker may make a sequence of
educated guesses (say, by trying meaningless long strings, or any word that is clearly

u

P.A. Bonatti and L. Sauro

unrelated to the domain of the KB); after a sufficient number of trials, the majority of
answers should agree with the real answer with high probability. Rejecting queries
whose signature is not contained in KBs signature mitigates this kind of attacks but it
leaks KBs signature and it does not provide a complete solution: Any  occurring in
KB that is logically unrelated to C and D can be used for a similar attack.

4 A Meta-safe Confidentiality Model

In this section we introduce a confidentiality model that makes the vulnerabilities illustrated above visible, by taking into account object- and meta-level background knowl-
edge. A bk-model M = KB, U, f,S u, PKBu, BKuuU consists of a knowledge base
KB  L, a set of users U, plus:
 a filtering function f : (L)  U  (L), mapping each knowledge base K and
each user u on a view f (K, u)  Cn(K);
 for all u  U:
 a finite set of secrecies S u  L;
 a set of axioms BKu  L, encoding the users object-level knowledge;
 a set of possible knowledge bases PKBu  (L) (users metaknowledge).3

The view of KB released to a user u is f (KB, u). We adopt PKB because at this stage
we do not want to tie our framework to any specific metalanguage. PKB represents the
knowledge bases that are compatible with the users metaknowledge.
Definition 1. A filtering function f is secure (w.r.t. M) iff for all u  U and all s  S u,
there exists K  PKBu such that:
f (K, u) = f (KB, u);
1.
2. s  Cn(K  BKu).

Intuitively, if f is safe according to Def. 1, then no user u can conclude that any secret
s is entailed by the KB she is interacting withenhanced with the object-level background knowledge BKufor the following reasons: By point 1, KB and K have the
same observable behavior, and K is a possible knowledge base for u since K  PKBu;
therefore, as far as u knows, the knowledge base might be K. Moreover, by point 2, K
and the object-level background knowledge BKu do not suffice to entail the secret s.
In the rest of the paper we tacitly assume that no secret is violated a priori, that is,
for all secrets s  S u there exists K  PKBu such that s  Cn(K  BKu).4 Moreover,
in order to improve readability, we shall omit the user u from subscripts and argument
lists whenever u is irrelevant to the context.

The attacks discussed in Section 3 can be easily formalized in this setting; so, in

general, the maximal secure views of SCM are not secure according to Def. 1.

3 In practice, bk-models are finite, and filterings computable, but no such assumption will be

technically needed until Sec. 7.

4 Conversely, no filtering function can conceal a secret that is already known by the user.
?

?

?
Example 2. Example 1 can be formalized in our model as follows: The set of secrecies
S is the set of all assertions works for(e, dn); BK =  and PKB is the set of all the
knowledge bases K that consist of assertions like (1) and (2), and such that for each
axiom Employee(e), K contains exactly one corresponding axiom works for(e, di) and
viceversa. The filtering function f maps each K  PKB on the maximal subset of K that
entails none of S s members, that is, f (K) = K \ S (by definition of PKB).

Note that f is injective over PKB, so condition 1 of Def. 1 is satisfied only if K = KB.
So, if KB contains at least one secret, then the conditions of Def. 1 cannot be satisfied,
that is, maximal secure SCM views are not secure in our model. Indeed, KB can be
reconstructed from the secure view by observing that KB = f (KB)  {works for(e, dn) |
Employee(e)  f (KB)  i = 1, . . . , n, works for(e, di)  f (KB)}.

Similarly, the formalizations of the other attacks yield injective filtering functions (the
details are left to the reader).

5 A Meta-secure Query Answering Mechanism
In this section we introduce a secure filtering function. It is formulated as an iterative process based on a censor, that is a boolean function that decides for each axiom
whether it should be obfuscated to protect confidentiality. The iterative construction
  (L)  (L) that represent a meta constraint on possible
manipulates pairs X+, X
knowledge bases: we say that a knowledge base K satisfies X+, X
 iff K entails all the
 = ).
sentences in X+ and none of those in X

Let PAX (the set of possible axioms) be the set of axioms that may occur in the
. Let  =
knowledge base according to the users knowledge, i.e. PAX =
|PAX| + 1 if PAX is finite and  =  otherwise; let 1, 2, . . . , i, . . . be any enumeration
of PAX (i < ).5 The secure view construction for a knowledge base K in a bk-model
M consists of the following, inductively defined sequence of pairs K+
 K+

(formally, Cn(K)  X+ and Cn(K)  X
?

?

?
 = , , and for all 1  i <  , K+

KPKB K



i

i+1


i+1

i0 :
 is defined as follows:
, K
, i+1) = true then let K+

, K
, i+1) = f alse and K |= i+1 then
i+1
 {i+1}, K
;

 = K+


i

, K
, K
i+1
i
i
 =

i , and fM(K, u) = K+ .
i< K

 {i+1} .


, K
i
 ;

 = K+


, K
i

i+1

i

i


, K
 if censorM(K+


, K
 if censorM(K+

i
, K
 = K+
K+

i
 otherwise let K+
i+1

i+1
i< K+
i , K

, K

i+1

i

i

Finally, let K+ =

Note that the inductive construction aims at finding maximal sets K+ and K
that
(i) partly describe what does / does not follow from K (as K satisfies K+, K
 by
construction), and (ii) do not trigger the censor (the sentences i+1 that trigger the censor
are included neither in K+ nor in K

, cf. the induction step).



In order to define the censor we need an auxiliary definition that captures all the sentences that can be entailed with the background knowledge BK and the meta-knowledge
 analogous to those adopted in the iterative
PKB enriched by a given constraint X+, X
construction: Let CnM(X+, X

) be the set of all axioms   L such that



for all K

  PKB such that K
?

?

?
satisfies X+, X

,   Cn(K

  BK) .

(3)

5 We will show later how to restrict the construction to finite sequences, by approximating PAX.

P.A. Bonatti and L. Sauro

Now the censor is defined as follows: For all X+, X

  L and   L,

censorM(X+, X

, ) =

if there exists s  S s.t. either s  CnM(X+  {}, X
or s  CnM(X+, X

  {});



)

(4)

 true

false otherwise.

)  



In other words, the censor checks whether telling either that  is derivable or that  is
, restricts the
not derivable to a user aware that the knowledge base satisfies X+, X
set of possible knowledge bases enough to conclude that a secret s is entailed by the
knowledge base and the background knowledge encoded by BK and PKB.

Note that the censor obfuscates i+1 if any of its possible answers entail a secret,
independently of the actual contents of K (the two possible answers yes and no
correspond to conditions s  CnM(X+  {}, X
  {}), respec-
tively). In this way, roughly speaking, the knowledge bases that entail s are given the
same observable behavior as those that dont. Under a suitable continuity assumption
on CnM, this enforces confidentiality:

) and s  CnM(X+, X



i< CnM(KB+
i

Theorem 1. If CnM(KB+, KB

i ), then fM is secure w.r.t. M.

, KB
Proof. Let u and s be arbitrary members of U and S u, respectively. We have to show
that there exists a K  PKBu satisfying the two conditions of Def. 1. Let KB+
i<

, KB
i
be the sequence underlying the construction of fM(KB, u). By construction, for all i <

) 


, s  CnM(KB+
i ). Moreover, by the continuity hypothesis, CnM(KB+, KB
, KB


i
i ), where KB+ =
, KB
i . It follows that
). Then, by definition of CnM, there exists K  PKBu such that:

s  CnM(KB+, KB

i< CnM(KB+
i

i and KB

i< KB+

i< KB

 =
?

?

?
i

Cn(K)  KB+

(5)

Cn(K)  KB

 =  (6)

s  Cn(K  BKu) .

(7)

Since (7) is the second condition of Def. 1, we are only left to show the first one, that
is, fM(K, u) = fM(KB, u). It suffices to prove by induction on i that for all i < ,

K+

i

 = KB+

i


i


, KB
i

 .

, K

(8)

, K


i1

The base case is trivial. Induction step (i > 0): By induction hypothesis, (8) holds for
i  1, therefore censorM(K+
, i). If the censors are
i1
true, then (8) follows directly from the induction hypothesis. If the censor is false, then
i belongs to KB+KB
; note that K and KB agree on these formulae, by (5) and (6), so
both knowledge bases insert i into the same element of the i-th pair and (8) holds. 
Examples of the behavior of fM are deferred until Sec.7.

, i) = censorM(KB+
i1


, KB
i1



6 Approximating Background Knowledge

Of course, the actual confidentiality of a filtering f (KB, u) depends on a careful definition of the users background knowledge, that is, PKBu and BKu. If background knowledge is not exactly known, as it typically happens, then it can be safely approximated by
overestimating it. More background knowledge means larger BKu and smaller PKBu,
which leads to the following comparison relation k over bk-models:
?

?

?
u

iff

, f

, U

, PKB
?

?

?
, BK
u
u

, U = U

Definition 2. Given two bk-models M = KB, U, f,S u, PKBu, BKuuU and M =
KB
,S
uU, we write M k M
?

?

?
, and S u = S
, f = f
1. KB = KB
2. for all u  U, PKBu  PKB
u and BKu  BK

The next proposition proves that a bk-model M can be safely approximated by any M
such that M k M
Proposition 2. If f is secure w.r.t. M
Consequently, a generic advice for estimating BK consists in including as many pieces
of relevant knowledge as possible, for example:

, then f is secure w.r.t. M.

u (for all u  U);
?

?

?
and M k M
?

?

?
u.

:

(i) modelling as completely as possible the integrity constraints satisfied by the data,

as well as role domain and range restrictions and disjointness constraints;

(ii) including in BK all the relevant public sources of formalized relevant knowledge

(such as ontologies and triple stores).

While object-level background knowledge is dealt with in the literature, the general
metaknowledge encoded by PKB is novel. Therefore, the next section is focussed on
some concrete approximations of PKB and their properties.

7 Approximating and Reasoning about Possible Knowledge Bases

In this section, we investigate the real world situations where the knowledge base KB is
finite and so are all the components of bk-models (U, S u, BKu, PKBu); then we focus
on PKBu that contain only finite knowledge bases. Consequently, fM will turn out to be
decidable and we will study its complexity under different assumptions.

(n  0, m  0) ,

A language for defining PKB is a necessary prerequisite for the practical implementation of our framework and a detailed complexity analysis of fM. Here we express PKB
as the set of all theories that are contained in a given set of possible axioms PAX 6 and
satisfy a given, finite set MR of metarules like:
1, . . . , n  1 | . . . | m

(9)
where all i and  j are in L (1  i  n, 1  j  m). Informally, (9) means that if
KB entails 1, . . . , n then KB entails also some of 1, . . . , m. Sets of similar metarules
can be succintly specified using metavariables; they can be placed wherever individual
constants may occur, that is, as arguments of assertions, and in nominals. A metarule
with such variables abbreviates the set of its ground instantiations: Given a K  L,
let groundK(MR) be the ground (variable-free) instantiation of MR where metavariables
are uniformly replaced by the individual constants occurring in K in all possible ways.

Example 3. Let MR =
, where X is a metavariable, and let K =

R(a, b)
If r denotes rule (9), then let body(r) = {1, . . . , n} and head(r) = {1, . . . , m}. We say
r is Horn if |head(r)|  1. A set of axioms K  L satisfies a ground metarule r if either
body(r)  Cn(K) or head(r)  Cn(K)  . In this case we write K |=m r.
6 Differently from Sec. 5, here PKB is defined in terms of PAX.
?

?

?
. Then groundK(MR) =

(R.{a}  A(a)), (R.{b}  A(b))

R.{X}  A(X)
?

?

?
.

P.A. Bonatti and L. Sauro

Example 4. Let A, B, C be concept names and R be a role name. The axiom set K =
{A  R.B, A  C} satisfies A  R  A  B | A  C but not A  R  A  B.

Moreover, if K satisfies all the metarules in groundK(MR) then we write K |=m MR.
Therefore the formal definition of PKB now becomes:

PKB = {K | K  PAX  K |=m MR} .

(10)

In accordance with Prop. 2, we approximate PAX in a conservative way. We will

analyze two possible definitions:

1. PAX0 = KB (i.e., as a minimalistic choice we only assume that the axioms of KB
are possible axioms; of course, by Prop. 2, this choice is safe also w.r.t. any larger
PAX where at least the axioms of KB are regarded as possible axioms);

2. PAX1 = KB 

rgroundKB(MR) head(r).

Remark 1. The latter definition is most natural if metarules are automatically extracted
from KB with rule mining techniques, that typically construct rules using material from
the given KB (then rule heads occur in KB).

Example 5. Consider again Example 1. The users metaknowledge about KBs completeness can be encoded with:

Employee(X)  works for(X, d1) | . . . | works for(X, dn) ,

(11)



where X is a metavariable. First let PAX = PAX1 . The secure view fM(KB) depends on
the enumeration order of PAX. If the role assertions works for(e, di) precede the concept assertions Employee(e), then, in a first stage, the sets KB+
j are progressively filled

with the role assertions with di  dn that belong to KB, while the sets KB
j accumulate
all the role assertions that do not belong to KB. In a second stage, the sets KB+
j are
further extended with the concept assertions Employee(e) such that e does not work for
dn. The role assertions works for(e, dn) of KB and the corresponding concept assertions
Employee(e) are neither in KB+ nor in KB
. Note that the final effect is equivalent to
removing from KB all the axioms referring to the individuals that work for dn. Analo-
gously, in [7] the individuals belonging to a specified set are removed from all answers.
Next suppose that the role assertions works for(e, di) follow the concept assertions
Employee(e), and that each works for(e, di) follows all works for(e, dk) such that k < i.
Now all the assertions Employee(e) of KB enter KB+, and all axioms works for(e, di)
with i < n  1 enter either KB+ or KB
, depending on whether they are members
of KB or not. Finally, the assertions works for(e, di)  Cn(KB) with i  {n  1, n}

are inserted neither in KB+ nor in KB
, because the corresponding instance of (11)
with X = e has the body in KB+ and the first n  2 alternatives in the head in KB

,
therefore a negative answer to works for(e, dn1) would entail the secret works for(e, dn)
by (11). This triggers the censor for all assertions works for(e, dn1). Summarizing, with
this enumeration ordering it is possible to return the complete list of employees; the
members of dn are protected by hiding also which employees belong to dn1.



Finally, let PAX = PAX0 . In this case, all possible knowledge bases are subsets of
KB; the latter contains exactly one assertion works for(e, di(e)) for each employee e.
?

?

?
Then, in order to satisfy (11), every K  PKB containing Employee(e) must contain
also works for(e, di(e)). It follows that fM must remove all references to the individuals

e that work for dn, as it happens with the first enumeration of PAX1.
Definition 3. A bk-model M is canonical if for all users u  U, PAXu is either PAX0
or PAX1 and PKBu is defined by (10) for a given MRu. Moreover, M is in a description
logic DL if for all u  U, all the axioms in KB, PKBu, BKu, and S u belong to DL.

The size of PAX0 and PAX1

7 is polynomial in the size of KB  MR, therefore PKB is
finite and exponential in the size of KB MR. Finiteness implies the continuity hypothesis on CnM of Theorem 1, and hence (using Theorem 1 and Prop. 2):
Theorem 2. If M is canonical, then fM is secure with respect to all M k M.
Proof. Since M is canonical, for all u  U, PKBu is finite and  = |PAXu| + 1 < . By
?

?

?
i in the sequence KB+
i< grow monotonically

construction, the sets KB+
i and KB


1. Moreover, CnM is monotonic in
i< KB+
= KB+
1 and
with i, so
i< KB

i
?

?

?
i ) = CnM(KB+
i< CnM(KB+
, KB
1
both arguments, so
i

, KB
,
1) =
i< KB


, KB
i


1). It follows that

i< CnM(KB+
, KB
i ) ,
i


i ) = CnM(KB+
1

CnM(

i< KB+
i


i

= KB

i

, KB

that is, the continuity hypothesis of Theorem 1 is satisfied. Then, by Theorem 1, fM is
secure with respect to M, and by Prop. 2, fM is secure with respect to all M k M. 
First we analyze the complexity of constructing the secure view fM(KB) when the

underlying description logic is tractable, like EL and DL-lite for example.
Lemma 1. If the axioms occurring in MR and K are in a DL with tractable subsumption and instance checking, then checking K |=m MR is:
1. in P if either MR is ground or there exists a fixed bound on the number of distinct

variables in MR;

2. coNP-complete otherwise.

Proof. Point 1: K |=m MR can be checked as follows: For each r  groundK(MR) and
all axioms   body(r)  head(r) check whether K |=m r by verifying whether there
exists either   body(r) such that   Cn(K), or   head(r) such that   Cn(K). The
cost of each test K |=m r is polynomial in the size of MR and K since membership in
Cn(K) is in P by hypothesis. The number of iterations is polynomial in the size of MR
and K, too, because the hypothesis that the number of variables in r is bounded implies
that |groundK(MR)| is polynomial in the size of MR and K.
Point 2: (Membership) The complementary test K |=m MR can be carried out in
two steps: first guess an r  MR and a substitution  that maps each metavariable
in r on an individual constant occurring in K; second, check whether K |=m r does
not hold. Checking K |=m r is in P (cf. point 1), so K |=m MR can be checked in
nondetermistic polynomial time, and hence the original problem (K |=m MR) is in coNP.
7 We assume here and in the following complexity results that axiom setsand hence KBs

have a natural encoding as strings that determine their size.

P.A. Bonatti and L. Sauro

Hardness follows by reducing to K |=m MR the clause subsumption problem: Given two
clauses (i.e. two sets of literals) G and H, is there a substitution  such that G  H?
(if the answer is yes then G subsumes H). The problem is still NP-complete if all
literals are positive, terms are function-free, and predicate arity is bounded by 2. Let
G = {p1, . . . , pn} and H be two clauses satisfying these assumptions. Let K = H (i.e.
K is a set of assertions whose concept names and role names are the unary and binary
predicates of H, respectively, and whose individual constants are the terms occurring in
H). Let MR = {p1, . . . , pn }, where the terms occurring in G and not in H are regarded
as metavariables. Now G subsumes H iff there exists a substitution  such that G  H,

iff there is an instance r  groundK(MR) such that K |=m r, iff K |=m MR.
With Lemma 1, one can prove the following two lemmas.
Lemma 2. Let M range over canonical bk-models. If M, s, X+, and X
are in a DL
with tractable subsumption/instance checking, and the number of distinct variables in
MR is bounded by a constant, then checking whether s  CnM(X+, X
1. in P if MR is Horn and PAX = PAX1;
2. coNP-complete if either MR is not Horn or PAX = PAX0.
Proof. Point 1: By standard logic programming techniques, a minimal K  PAX satisfying MR and entailing X+ can be obtained with the following PTIME construction:

) is:





K0 = X+ , Ki+1 = Ki 

{ head(r) | r  groundKi (MR)  body(r)  Cn(Ki)} .
?

?

?
(12)


)



Point 2: Membership in coNP is straightforward (s  CnM(X+, X

This sequence reaches its limit after at most |PAX| iterations. Then s  CnM(X+, X
holds iff either s  K|PAX| or K|PAX|  X
  . Both tests are in P since K|PAX|  PAX.
) can be checked
by guessing a K  PAX that satisfies X+, X
 and such that s  Cn(K  BK)). To
prove hardness first assume that PAX = PAX0. For each given 3-SAT instance, encode
its n propositional variables and their negation with 2n concept names Pi and  Pi, re-
spectively. Introduce a concept name Ck for each clause ck = lk,1  lk,2  lk,3. Let KB
consist of all the inclusions A  Pi and A   Pi (1  i  n), plus all Lk, j  Ck s.t. Lk, j
is the encoding of lk j ( j = 1, 2, 3). Let s = (A  B), BK =  and let MR consists of all
the rules (A  Pi, A   Pi ), ( Lk, j  Ck), ( A  Ck). MR is Horn, and the given
clause set is satisfiable iff there exists K  KB = PAX0 such that K |=m MR. For all such
K, s  Cn(K) because B does not occur in KB. Then the given clauses are satisfiable iff
s  CnM(,). This proves that checking whether s  CnM(X+, X

) is coNP-hard.



We are left to show a similar result under the assumption that MR is not Horn and
PAX = PAX1. Let KB, s, and BK be defined as before. Let MR be the set of all rules
(A  Pi, A   Pi ), ( A  Pi | A   Pi), (A   Lk,1, A   Lk,2, A   Lk,3  s), where
each  Lk j is the encoding of the complement of lk j. Clearly, the given set of clauses is
satisfied iff there exists K  PAX1 such that K |=m MR and s  Cn(K); this is equivalent
to s  CnM(,). The theorem follows immediately.

Lemma 3. Let M be a canonical bk-model. If M, s, X+, and X
are in a DL with
tractable entailment problems, and there is no bound on the number of variables in the
metarules of MR, then checking s  CnM(X+, X

) is:




?

?

?
1. in PNP if MR is Horn and PAX = PAX1;
2. in  p

2 if either MR is not Horn or PAX = PAX0.

Proof. To prove Point 1, we use the same algorithm used for Lemma 2.(1), based on
the bottom-up construction defined by (12). However, due to the lack of bounds on
metavariables, groundKi(MR) can be exponentially large. Then the complexity of each
iteration in (12) is determined with a different, nondeterministic algorithm: For each
possible ground instance of a rule head (quadratically many due to arity bounds) use
the NP oracle to guess an instance of the rule body and check (in polynomial time)
whether it is entailed by Ki. The deterministic algorithm then runs in polynomial time
using an NP oracle.
Point 2 can be proved with the naive nondeterministic algorithm that guesses a K 
  Cn(K) = , and (iii)
PAX and checks whether (i) K  PKB, (ii) X+  Cn(K) and X
s  Cn(K  BKu). Condition (i) can be verified by checking whether K |=m MR; this
test is NP-complete by Lemma 1.(2). Conditions (ii) and (iii) are in P by hypothesis. So
the whole nondeterministic algorithm runs in polynomial time using an NP oracle. 
, ) can be computed straightforwardly by iterating the tests
The value of censor(X+, X
s  CnM(X+ {}, X
 {}) for all secrets s  S . Since the set of
secrets is part of the parameter M of the filtering function, the number of iterations is
polynomial in the input and the complexity of the censor is dominated by the complexity
of CnM(). The latter is determined by Lemma 2 and Lemma 3, so we immediately get:
Corollary 1. Let M be a canonical bk-model and suppose that M, X+, X
, and  are
in a DL with tractable entailment problems. If the number of distinct variables in MR is
bounded by a constant, then computing censor(X+, X

) and s  CnM(X+, X





, ) is:

 in P if MR is Horn and PAX = PAX1;
 coNP-complete if either MR is not Horn or PAX = PAX0.

If there is no bound on the number of variables in the metarules of MR, then computing
censor(X+, X

, ) is:

 in PNP if MR is Horn and PAX = PAX1;
 in  p

2 if either MR is not Horn or PAX = PAX0.

We are now ready to analyze the complexity of filtering functions:

Theorem 3. If M is a canonical bk-model in a DL with tractable entailment problems,
then computing fM(KB) is:

1. in P if the number of distinct variables in the rules of MR is bounded, MR is Horn,

and PAX = PAX1;

2. PNP-complete if the number of distinct variables in MR is bounded, and either MR

is not Horn or PAX = PAX0;

3. in PNP if the variables in MR are unbounded, MR is Horn, and PAX = PAX1;
4. in p

3 if MR is not restricted and PAX  {PAX0, PAX1}.

P.A. Bonatti and L. Sauro

Proof. Point 1 follows easily from Corollary 1: use the straightforward algorithm that
iterates over all i in the enumeration of PAX, and for each of them computes the censor
and checks whether KB |=  (if needed); since the number of iterations is polynomial
in the input, the overall complexity is dominated by the complexity of evaluating the
censor and KB entailments (both are tractable).
?

?

?
i

 =  and i  KB+ iff censorM(KB+
?

?

?
n+1i (the rest of the ordering is not relevant).

Point 2: Assume that PAX = PAX0 and MR is Horn, the other case where PAX =
PAX1 and MR is not Horn can be proved with the techniques adopted in Lemma 2.(2).
Membership in PNP is straightforward, by the same argument applied in Point 1. Hardness is proved by a reduction of the maximum satisfying assignment problem which,
given a set of clauses C = {c1, . . . , cm} in the variables p1, . . . , pn, consists in finding the
lexicographically maximum assignment msa  {0, 1}n that satisfies C, or 0 if C is unsat-
isfiable. We extend KB and MR defined at point 2 in Lemma 2 as follows: first, we add
i, with 1  i  n. Secondly, we replace the
to KB A  C and the set of inclusions A  P

rules  A  Ck, 1  k  m, with A  C  A  Ck and add the rules A  P
 A  Pi,
with 1  i  n. Finally, consider an ordering of PAX where 1 = A  C and, for each
1  i  n, i+1 = A  P
The inclusion A  C plays the role of a satisfiability checker for C, that is if C is
not satisfiable, then for all K  KB, A  C  Cn(K). Consequently, A  C  fM(KB).
Assume now that C is satisfiable. First of all, since for all 0  i  n KB |= i, then
,, i) is false. Now, note that the i are

not forced to be entailed by any rule, therefore for each K  PKB also K \ {i} 
,, i) is false iff there exists a K  PKB such that
PKB. Consequently, censorM(KB+
 {i}. In particular, this ensures that A  C  KB+. Now, since PKB
Cn(K)  KB+
i
satisfies the rules A  P
 A  Pi and encodes with the inclusions A  Pi all
,, i) is false (i.e.
possible assignments  that satisfy C, this means that censorM(KB+
i  KB+) iff there exists an assignment  such that i = 1 and for all 1  j < i,  j = 1
i
iff A  P
 KB+. Finally, from the fact that most significant A  P
?

?

?
i are processed first,
we have that if C is not satisfiable then A  C  fM(KB), otherwise A  C  fM(KB)
j
and A  P
 fM(KB) iff msa

Points 3, 4 are straightforward by the same argument for membership in Point 1. 
i
Theorem 4. Computing fM(KB) over canonical M in a DL with ExpTime entailment
(e.g. ALCQO, ALCIO, ALCQI, SHOQ, SHIO, SHIQ), is still in ExpTime.
Proof. Consider any test s  CnM(X+, X
) in the construction of fM(KB, u) (there are

two such tests for each censor evaluation). Carrying out the test for given X+, X
, and
s  S u can be done by brute force, iterating over all the exponentially many K
  PAX

(which is either PAX0 or PAX1 whose size is polynomial in KB). For each such K
, we
|=m MR; this can
have to verify whether it belongs to PKB, by checking whether K
be done in ExpTime by iterating over all the (ground) instances r  groundK(MR) and
 |=m r. Then, for all K
  PKB, three ExpTime
checking in polynomial time whether K
problems must be solved (X+  Cn(K
  BKu)).

), X
If they all succeed, s  CnM(X+, X
); otherwise the algorithm continues with the next
  PAX. So the total cost of each censor call is exponential in the size of KB, MR,

and BKu. In order to compute fM(KB), this cost is iterated for all combinations of secrets and axioms in PAX; moreover, for each iteration where the censor is false, an

= 1.

  Cn(K
?

?

?
) = , and s  Cn(K
?

?

?
i

i
?

?

?
i

i




?

?

?
additional ExpTime entailment problem is solved (KB |= i+1). It follows that comput-

ing fM(KB, u) is exponential in the size of KB, MR, BKu, and S u.
Theorem 5. Computing fM(KB) over canonicalM in SROIQ(D) is in coNPN2ExpT ime.

Proof. (Hint) Use the same brute-force algorithm used in Theorem 4.

8 Relationships with the SCM

Here we show that the meta-secure framework is a natural generalization of the SCM.
The main resultroughly speakingdemonstrates that the SCM model can be essentially regarded as a special case of our framework where PKB  (KB) and BK = . In
this case fM is secure even if M is not assumed to be canonical.
Theorem 6. Let M = KB, U, fM,S u, PKBu, BKuuU. If PKB = (KB), BK = , and
KB is finite, then

1. CnM(KB+, KB
2. For all enumerations of PAX, the corresponding fM(KB, u) is logically equivalent
to a maximal secure view KBu of KB according to the SCM; conversely, for all
maximal secure view KBu of KB (according to the SCM) there exists an enumeration
of PAX such that the resulting fM(KB, u) is logically equivalent to KBu.
uU such
fM is secure w.r.t. M and w.r.t. any M = KB, U, fM,S u, PKB

, BK
u
that PKB

  (KB) and BK

i< CnM(KB+
i

 = .


i ).
?

?

?
, KB

) =

3.
?

?

?
u

Proof. By the first hypothesis, PAX = KB. As a first consequence, for all   PAX,
i<, we have
  Cn(KB), and hence, by definition of the inductive sequence KB+
= . As a second consequence, for all X+  KB, we have
that all for all i < , KB
X+  PKB = (KB). Therefore X+ is also the least K  PKB (up to logical equivalence)
such that Cn(K)  X+ and Cn(K)   = . This fact and the second hypothesis imply
(by definition of CnM) that


, KB
i


i

i

CnM(X+,) = Cn(X+) .


i ) = Cn(KB+
i< Cn(KB+
i ); then Point 1 follows by:

, KB

i ), for all i < . Moreover, by

(13)
?

?

?
i< KB+

As a special case, we get CnM(KB+
i

compactness, Cn(


i ) =

) = Cn(KB+) = Cn(

CnM(KB+, KB
?

?

?
KB+

i ) =

Cn(KB+

i ) =

CnM(KB+
i

, KB


i ).

i<

i<

i<

Now let 1, 2, . . . , i, . . . be any enumeration of PAX. By induction on i, it is easy to
i+1 and the censor) that for all i < , i  K+
prove (using (13) and the definitions of K+
i
i< KB+
iff either Cn(K+
i1
i
is logically equivalent to a maximal subset KBu of KB that does not entail any secret.
By definition, the same holds for fM(KB, u) =

{i})S   or i  Cn(Ki1). It follows immediately that
?

?

?
i< KB+
i .

P.A. Bonatti and L. Sauro
?

?

?
i< KB+
i

i+1 iff either Cn(K+
i

i . This completes the proof of Point 2.

n is logically equivalent to KBu, and for all i > n, KB+
i

Conversely, let KBu be any maximal subset of KB that entails no secret, and let n =
|KBu|. Let 1, 2, . . . , i, . . . be any enumeration of PAX such that KBu = {1, . . . , n}
(i.e. the sentences in KBu precede those in KB \ KBu). As in the above paragraph, it
 {i})  S   or i 
can be verified that for all i < , i  K+
Cn(Ki). It follows that KB+
=

is logically equivalent to KBu, and so is fM(KB, u) =
KBn. Consequently,
i< KB+
Point 3: fM is secure w.r.t. M by Theorem 1, whose hypothesis is satisfied by Point 1.
It follows by Proposition 2, that fM is also secure for all M
such that M k M, which
includes all M
that are identical to M with the exception of their possible knowledge

bases PKB
Remark 2. Theorem 6 applies to every canonical M such that MR = BK = , because
MR =  implies that PAX0 = PAX1 = KB and hence PKB = (KB). This shows
that the SCM can be regarded as a special case of our framework where the user has
no background knowledge. Moreover, by this correpondence, one immediately obtains
complexity bounds for the SCM from those for PAX1 and Horn, bounded-variable MR.
?

?

?
, and such that PKB

  PKB = (KB).

9 Related Work

Baader et al. [2], Eldora et al. [12], and Knechtel and Stuckenschmidt [14] attach security labels to axioms and users to determine which subset of the KB can be used by
each subject. These works are instances of the SCM so they are potentially vulnerable
to the attacks based on background knowledge; this holds in particular for [14] that
pursues the construction of maximal secure views. Similar considerations hold for [16].
Moreover, in [2, 12] axiom labels are not derived from the set of secrets; knowledge engineers are responsible for checking ex post that no confidential knowledge is entailed;
in case of leakage, the view can be modified with a revision tool based on pinpointing.
Our mechanism produces automatically a secure view from the secrets, instead, and
decides secondary protection, i.e. which additional axioms shall be hidden for security.
Chen and Stuckenschmidt [7] adopt an instance of the SACM based on removing
some individuals entirely. In general, this may be secure against metaknowledge attacks
(cf. Ex. 5). However, no methodology is provided for selecting the individuals to be
removed given a target set of secrets.

In [3], KB is partioned into a visible part KBv and a hidden part KBh. Conceptually,
this is analogous to axiom labelling, cf. the above approaches. Their confidentiality
methodology seems to work only under the assumption that the signatures of KBv and
KBh are disjoint, because in strong safety they do not consider the formulae that are
implied by a combination of KBv and KBh. Surely the axioms of KBh whose signature
is included in the signature of KBv cannot be protected, in general. A partition-based
approach is taken in [10], too. It is not discussed how to select the hidden part KBh
given a set of target secrets (which includes the issue of deciding secondary protection).
Similarly, in [15] only ex-post confidentiality verification methods are provided. In
their model the equivalent of PKB is the set of all knowledge bases that include a
given set of publicly known axioms S  KB; consequently, their verification method is
?

?

?
vulnerable to the attacks to complete knowledge based on conditional metaknowledge
(cf. Example 2 and Example 5) that cannot be encoded in their framework.

Cuenca Grau and Horrocks [9] investigate knowledge confidentiality from a probabilistic perspective: enlarging the public view should not change the probability distribution over the possible answers to a sensitive query Q that represents the set of
secrets. In [9] users can query the knowledge base only through a pre-defined set of
views (we place no such restriction, instead). A probability distribution P over the set
of knowledge bases plays a role similar to metaknowledge. However, their confidentiality condition allows P to be replaced with a different P
after enlarging the public
view, so at a closer look P does not really model the users a priori knowledge about the
knowledge base (that should remain constant), differently from our PKB.
?

?

?
Our method is inspired by the literature on controlled (database) query evaluation
(CQE) based on lies and/or refusals ([4, 5, 6] etc). Technically we use lies, because
rejected queries are not explicitly marked. However, our censor resembles the classical
refusal censor, so the properties of fM are not subsumed by any of the classical CQE
methods. For example (unlike the CQE approaches that use lies), fM(KB, u) encodes
only correct knowledge, and it is secure even if users initially know a disjunction of
secrets. Unlike the refusal method, fM can handle cover stories because users are not
told which queries are obfuscated; as an additional advantage, our method needs not to
adapt existing engines to handle nonstandard answers like refusals (mum).

10 Discussion and Conclusions

We identified some novel vulnerabilities of those confidentiality preservation methods
that do not take background knowledge into account. The new confidentiality model of
Sec. 4 can detect these vulnerabilities, based on a generic formalization of object- and
meta-level background knowledge. A general mechanism for constructing secure views
(the filtering fM) is provably secure w.r.t. this model under a continuity assumption,
and generalizes a few previous approaches (cf. Thm. 6 and Ex. 5). In order to compute
secure views in practice we introduced a safe, generic method for approximating background knowledge, and a specific rule-based metalanguage. In this instantiation of the
general framework fM is always secure and its complexity can be analyzed.

If the underlying DL is tractable, then in the simplest case fM can be computed in
polynomial time. The number of variables in metarules and the adoption of a more
secure approximation (PAX0) may increase complexity up to PNP = p
2 and perhaps
p
3. The complexity of non-Horn metarules, however, can be avoided by replacing each
non-Horn r with one of its Horn strengthenings: body(r)   such that   head(r).
This approximation is safe (because it restricts PKB), and opens the way to a systematic
use of the low-complexity bk-models based on PAX1 and Horn metarules.

For the many ExpTime-complete DL, secure view computation does not increase
asymptotic complexity. So far, the best upper complexity bound for computing secure
views in the description logic underlying OWL DL (i.e. SROIQ(D)) is coNPN2ExpT ime.
We plan to refine these complexity results and investigate different tradeoffs between
information availability and computational complexity. Moreover, the idea of mining
metarules from KB is particularly intriguing: it would be the first automated support to
background knowledge approximation.

P.A. Bonatti and L. Sauro

We are investigating implementations of the low-complexity frameworks (based on
PAX1 and Horn metarules) using the incremental engine versions available for Pellet
and ELK to avoid repeated classifications in the iterative construction of fM. Metarule
bodies can be evaluated with SPARQL. Answer set programming technologies (e.g.
DLV-Hex [11]) provide interesting alternatives. Secure views are constructed off-line,
so no overhead is placed on user queries, that can be answered with any standard engine.
For these reasons, our approach is expected to be applicable in practice.
