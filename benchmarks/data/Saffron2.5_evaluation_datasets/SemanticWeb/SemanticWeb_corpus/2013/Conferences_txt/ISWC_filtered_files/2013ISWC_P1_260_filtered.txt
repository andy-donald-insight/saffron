Incremental Reasoning in OWL EL

without Bookkeeping

Yevgeny Kazakov and Pavel Klinov

The University of Ulm, Germany

{yevgeny.kazakov,pavel.klinov}@uni-ulm.de

Abstract. We describe a method for updating the classification of ontologies expressed in the EL family of Description Logics after some axioms have been
added or deleted. While incremental classification modulo additions is relatively
straightforward, handling deletions is more problematic since it requires retracting logical consequences that are no longer valid. Known algorithms address this
problem using various forms of bookkeeping to trace the consequences back to
premises. But such additional data can consume memory and place an extra burden on the reasoner during application of inferences. In this paper, we present a
technique, which avoids this extra cost while being very efficient for small incremental changes in ontologies. The technique is freely available as a part of
the open-source EL reasoner ELK and its efficiency is demonstrated on naturally
occurring and synthetic data.

1 Introduction and Motivation
The EL family of Description Logics (DLs) are tractable extensions of the DL EL featuring conjunction and existential restriction. Despite a limited number of constructors,
EL became the language of choice in many applications, especially in Biology and
Medicine, which require management of large terminologies. The DL EL++ [1]an
extension of EL with other features such as complex role inclusion axioms, nominals,
and datatypesbecame the basis of the OWL EL profile [2] of the Web ontology language OWL 2 specifically aimed at such applications.
Ontology classification is one of the main reasoning tasks. It requires computing all
entailed (implicit) subsumption relations between atomic concepts. Specialized EL rea-
soners, such as CEL [3], ELK [4], jcel [5], and Snorocket [6] are able to compute the
classification for ontologies as large as SNOMED CT [7] with about 300,000 axioms.
Classification plays the key role during ontology development, e.g., for detecting modeling errors that result in mismatches between terms. But even with fast classification
procedures, frequent re-classification of ontologies can introduce significant delays in
the development workflow, especially as ontologies grow over time.

Several incremental reasoning procedures have been proposed to optimize frequent
ontology re-classification after small changes. Most procedures maintain extra information to trace conclusions back to the axioms in order to deal with axiom deletions (see
Section 2). Although managing this information typically incurs only a linear overhead,
it can be a high cost for large ontologies such as SNOMED CT. In this paper, we propose an incremental reasoning method which does not require computing any of such

H. Alani et al. (Eds.): ISWC 2013, Part I, LNCS 8218, pp. 232247, 2013.
c Springer-Verlag Berlin Heidelberg 2013
?

?

?
information. The main idea is to split the derived conclusions into several partitions.
We identify partitions containing affected consequences (those that could be invalidated by deletion) using a simple forward chaining procedure, and then re-compute
all conclusions in these partitions. This way, we avoid storing any bookkeeping information for checking whether the affected consequences still follow from other conclu-
sions. Our hypothesis is that, if the number of partitions is sufficiently large, changes
are relatively small, and most inferences happen within individual partitions, the recomputation of affected partitions will not be too expensive. We describe a particular
partitioning method for EL that has this property, and verify our hypothesis experimen-
tally. Our experiments demonstrate that for large ontologies, such as SNOMED CT,
incremental classification can be 1040 times faster than the (already highly optimized)
full classification, thus making re-classification almost instantaneous.
In this paper we focus on the DL EL+, which covers most of the existing OWL EL
ontologies, and for simplicity, consider only additions and deletions of concept axioms,
but not of role axioms. Although the method can be extended to changes in role axioms,
it is unlikely to pay off in practice, because such changes are more likely to cause a
significant impact on the result of the classification.

2 Related Work

Directly relevant to this work are various extensions to DL reasoning algorithms to
support incremental changes.
Incremental classification in EL modulo additions implemented in the CEL system,
comes closest [8]. The procedure works, essentially, by applying new inferences corresponding to the added axioms and closing the set of old and new conclusions under all
inference rules. Deletion of axioms is not supported.

Known algorithms that support deletions require a form of bookkeeping to trace
conclusions back to the premises. The Pellet reasoner [9] implements a technique called
tableau tracing to keep track of the axioms used in tableau inferences [10]. Tracing
maps tableau elements (nodes, labels, and relations) to the responsible axioms. Upon
deletion of axioms, the corresponding elements get deleted. This method is memoryintensive for large tableaux and currently supports only ABox changes.

The module-based incremental reasoning method does not perform full tracing of
inferences, but instead maintains a collection of modules for derived conclusions [11].
The modules consist of axioms in the ontology that entail the respective conclusion,
but they are not necessarily minimal. If no axiom in the module was deleted then the
entailment is still valid. Unlike tracing, the method does not require changes to the
reasoning algorithm, but still incurs the cost of computing and storing the modules.

The approach presented in this paper is closely related to the classical DRed (over-
delete, re-derive) strategy for incremental maintenance of recursive views in databases
[12]. In the context of ontologies, this method was applied, e.g., for incremental updates
of assertions materialized using datalog rules [13], and for stream reasoning in RDF
[14]. Just like in DRed, we over-delete conclusions that were derived using deleted
inferences (to be on the safe side), but instead of checking which deleted conclusions are
still derivable using remaining inferences (which would require additional bookkeeping
information), we re-compute some well-defined subset of broken conclusions.

Y. Kazakov and P. Klinov

Table 1. The syntax and semantics of EL+

Syntax

Semantics

Roles:

atomic role
Concepts:

atomic concept
top
bottom
conjunction
existential restriction

Axioms:
?

?

?

C  D
R.C
C  D
R  S

concept inclusion

role inclusion

role composition R1  R2  S R
?

?

?


I  D

{x | y  C
I  D

I  S

1  R
2  S
?

?

?
: x, y  R

I}

3 Preliminaries
3.1 The Description Logic EL+
In this paper, we will focus on the DL EL+ [3], which can be seen as EL++ [1] without
nominals, datatypes, and the bottom concept . It is defined w.r.t. a vocabulary consisting of countably infinite sets of (atomic) roles and atomic concepts. Complex concepts
and axioms are defined recursively in Table 1. We use the letters R, S for roles, C, D, E
for concepts, and A, B for atomic concepts. An ontology is a finite set of axioms. Given
an ontology O, we write 
O for the smallest reflexive transitive binary relation over
roles such that R 
called the domain of I and an
An interpretation I consists of a nonempty set 

interpretation function I
I 
I  
?

?

?
. This assignment is extended to

complex concepts as shown in Table 1. I satisfies an axiom  (written I |= ) if
the corresponding condition in Table 1 holds. I is a model of an ontology O (written
I |= O) if I satisfies all axioms in O. We say that O entails an axiom  (written
O |= ), if every model of O satisfies . The ontology classification task requires to
compute all entailed subsumptions between atomic concepts occurring in O.

O S holds for all R  S  O.

that assigns to each role R a binary relation R

, and to each atomic concept A a set A

I  

3.2 Inferences and Inference Rules

Let Exp be a fixed countable set of expressions. An inference over Exp is an object inf
which is assigned with a finite set of premises inf.Premises  Exp and a conclusion
inf.conclusion  Exp. When inf.Premises = , we say that inf is an initialization
inference. An inference rule R over Exp is a countable set of inferences over Exp; it is
an initialization rule if all these inferences are initialization inferences. The cardinality
of the rule R (notation ||R||) is the number of inferences inf  R. In this paper, we view
an inference system as one inference rule R representing all of their inferences.
?

?

?
R0

R

: C occurs in O

C  C
C   : C and  occur in O
R C  D
C  E
R
?

?

?
C  D1  D2

: D  E  O

C  D1 C  D2

R+ C  D1 C  D2
C  D1  D2
R E  R.C C  D

: D1  D2 occurs in O
S.D occurs in O
R 
R E  R1.C C  R2.D

E  S.D

:

O S
S1  S2  S  O
R1 
R2 

O S1
O S2

:

E  S.D

Fig. 1. The inference rules for reasoning in EL+

We say that a set of expressions Exp  Exp is closed under an inference inf if
inf.Premises  Exp implies inf.conclusion  Exp. Exp is closed under an inference
rule R if Exp is closed under every inference inf  R. The closure under R is the
smallest set of expressions closed under R. Note that the closure is always empty if R
does not contain initialization inferences.
We will often restrict inference rules to subsets of premises. Let Exp  Exp be a
set of expressions, and R an inference rule. By R(Exp) (R[Exp]) we denote the rule
consisting of all inferences inf  R such that inf.Premises  Exp (respectively Exp 
inf.Premises). We can combine these operators: for example, R[Exp1](Exp2) consists
of those inferences in R whose premises contain all expressions from Exp1 and are a
subset of Exp2. Note that this is the same as R(Exp2)[Exp1]. For simplicity, we write
R(), R[], R(exp), and R[exp] instead of R(), R[], R({exp}), and R[{exp}] respectively.
Note that R[] = R and R() consists of all initialization inferences in R.

3.3 The Reasoning Procedure for EL+
The EL+ reasoning procedure works by applying inference rules to derive subsumptions between concepts. In this paper, we use the rules from EL++ [1] restricted to
EL+, but present them in a way that does not require the normalization stage [4].
The rules for EL+ are given in Figure 1, where the premises (if any) are given above
the horizontal line, and the conclusions below. Some rules have side conditions given
after the colon that restrict the expressions to which the rules are applicable. For exam-
ple, rule R+ contains one inference inf for each C, D1, D2, such that D1  D2 occurs
in O with inf.Premises = {C  D1, C  D2}, inf.conclusion = C  D1  D2. Note
that the axioms in the ontology O are only used in side conditions of the rules and never
used as premises of the rules.
The rules in Figure 1 are complete for deriving subsumptions between the concepts
occurring in the ontology. That is, if O |= C  D for C and D occurring in O,
then C  D can be derived using the rules in Figure 1 [1]. Therefore, in order to
classify the ontology, it is sufficient to compute the closure under the rules and take the
derived subsumptions between atomic concepts. The following example illustrates the
application of rules in Figure 1 for deriving the entailed subsumption relations.
?

?

?
Example 1. Consider the following EL+ ontology O:
(ax1): A  R.B (ax2):H.B  C (ax3): R  H
(ax4): B  S.A
The subsumptions below can be derived via rules in Figure 1:

(ax5): S.C  C

A  A
B  B
C  C

R.B  R.B
S.A  S.A
H.B  H.B
S.C  S.C
A  R.B
B  S.A
R.B  H.B
H.B  C
S.C  C

(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)
(11)
(12)
(13)
(14)
(15)
(16)
(17)
(18)
(19)
The subsumptions (1)(19) are closed under these rules so, by completeness, A  A,
B  B, C  C, A  C, B  C are all atomic subsumptions entailed by O.

by R0 since A occurs in O,
by R0 since B occurs in O,
by R0 since C occurs in O,
by R0 since R.B occurs in O,
by R0 since S.A occurs in O,
by R0 since H.B occurs in O,
by R0 since S.C occurs in O,
by R to (1) using (ax1),
by R to (2) using (ax4),
by R to (4) and (2) using (ax3),
by R to (6) using (ax2),
by R to (7) using (ax5),
by R to (8) and (2) using (ax3),
by R to (10) using (ax2),
by R to (13) using (ax2),
by R to (5) and (15),
by R to (9) and (15),
by R to (16) using (ax5),
by R to (17) using (ax5).

R.B  C
A  C

A  H.B

S.A  S.C
B  S.C

S.A  C
B  C

3.4 Computing the Closure under Inference Rules

Computing the closure under inference rules, such as in Figure 1, can be performed
using a well-known forward chaining procedure presented in Algorithm 1. The algorithm derives consequences by applying inferences in R and collects those conclusions
between which all inferences are applied in a set Closure and the remaining ones in a
queue Todo. The algorithm first initializes Todo with conclusions of the initialization
inferences R()  R (lines 23), and then in a cycle (lines 49), repeatedly takes the
next expression exp  Todo, if any, inserts it into Closure if it does not occur there,
and applies all inferences inf  R[exp](Closure) having this expression as one of the
premises and other premises from Closure. The conclusions of such inferences are then
inserted back into Todo.
?

?

?
Algorithm 1. Computing the inference closure

: R: a set of inferences
: Closure: the closure under R

input
output
1 Closure, Todo  ;
2 for inf  R() do
4 while Todo =  do

Todo.add(inf.conclusion);
exp  Todo.takeNext();
if exp / Closure then
Closure.add(exp);
for inf  R[exp](Closure) do
Todo.add(inf.conclusion);

/* initialize */

/* close */

10 return Closure;

The following example illustrates the execution of Algorithm 1 for computing the

deductive closure under inferences in Figure 1.

Example 2 (Example 1 continued). The conclusions (1)(19) in Example 1 are already
listed in the order in which they would be inserted into Todo by Algorithm 1. When a
conclusion is inserted into Closure, all inferences involving this and the previous conclusions are applied. For example, when (10) is inserted, the previous conclusions (1)
(9) are already in Closure, so (14) is derived and added into Todo after (11)(13).
?

?

?
) for the result Closure

because every inference inf  R(Closure

Note that Algorithm 1 performs as many insertions into Todo as there are inferences
) is evenin R(Closure
tually applied, and an inference cannot apply more than once. Therefore, the number of
inferences performed by Algorithm 1 is exactly ||R(Closure
)||. The time complexity of
the algorithm depends highly on the representation of the inference rules. If the initialization inferences inf  R() in line 2 and matching inferences inf  R[exp](Closure) in
line 8 can be effectively enumerated, the algorithm runs in O(||R(Closure

)||).
?

?

?
4 Incremental Deductive Closure Computation

In this section, we discuss algorithms for updating the deductive closure under a set of
inferences after the set of inferences has changed. Just like in Section 3.4, the material
in this section is not specific to any particular inference system, i.e., does not rely on
the EL+ classification procedure described in Section 3.3.

The problem of incremental computation of the deductive closure can be formulated
be sets of inferences, and Closure the closure under R.
)  R+, using

as follows. Let R, R+ and R
The objective is to compute the closure under the inferences in (R \ R
Closure, R, R+, or R

, if necessary.






?

?

?
: Closure: the closure under R  R+

Y. Kazakov and P. Klinov

Algorithm 2. Update modulo additions

input
output
1 Todo  ;
2 for inf  (R+ \ R)(Closure) do
Todo.add(inf.conclusion);
4 R  R  R+;
5 while Todo =  do

/* initialize */

/* close */

exp  Todo.takeNext();
if exp / Closure then
Closure.add(exp);
for inf  R[exp](Closure) do
Todo.add(inf.conclusion);

11 return Closure;

4.1 Additions Are Easy



= ), the closure under R  R+ can be computed by
If there are no deletions (R
Algorithm 2. Starting from Closure, the closure under R, the algorithm first initializes
Todo with conclusions of new inferences inf  (R+ \ R) applicable to Closure, and
then processes this queue with respect to the union of all inferences R  R+ as it is
done in Algorithm 1. Note that Algorithm 1 is just a special case of Algorithm 2 when
Closure =  and the initial set of inferences R is empty.
the set obtained in
the output. Intuitively, the algorithm applies all inferences in (R  R+)(Closure
) that
are not in R(Closure) because those should have been already applied. If, in contrast,
we compute the closure from scratch using Algorithm 1, we would need to apply all
inferences in (R  R+)(Closure
). Note that it is essential that Algorithm 2 starts with
the closure under R. If we start with a set that is not closed under R, we may lose some
conclusions because no inference in R(Closure) is applied by the algorithm.

Let Closure be the set in the input of Algorithm 2, and Closure
?

?

?
4.2 Deletions Are Difficult
Let us now see how to update the closure under deletions, i.e., when R+ = . Consider Algorithm 3, which works analogously to Algorithm 2, but removes conclusions
instead of adding them. In this algorithm, the queue Todo is used to buffer conclusions
that should be removed from Closure. We first initialize Todo with consequences of
the removed inferences inf  R
(Closure) (lines 23), and then remove such elements
from Closure together with the conclusions of inferences from Closure in which they
participate (lines 510). Note that in this loop, it is sufficient to consider only consequences under the resulting R = R\ R
are already
added into Todo during the initialization stage (lines 23).
expressions that are still derivable in R \ R

because all consequences under R
Unfortunately, Algorithm 3 might not produce the closure under R\R

. For example,

: it may delete
for
the input








?

?

?
Algorithm 3. Update modulo deletions (incomplete)
: R, R
: Closure: a subset of the closure under (R \ R

: sets of inferences, Closure: the closure under R





input
output
1 Todo  ;
2 for inf  R

4 R  R \ R
5 while Todo =  do



;

(Closure) do

Todo.add(inf.conclusion);

)(Closure)

/* initialize */

/* close */

exp  Todo.takeNext();
if exp  Closure then

for inf  R[exp](Closure) do
Todo.add(inf.conclusion);

Closure.remove(exp);

11 return Closure;

R = {/a, a/b, b/a} (x/y is an inference with the premise x and conclusion y), R
=
{b/a}, and Closure = {a, b}, Algorithm 3 removes both a since it is a conclusion of
(Closure), and b since it is a conclusion of (R \ R


)[a](Closure), yet both a and b

= {/a, a/b}.
are still derivable by the remaining inferences R \ R




A common solution to this problem is to check which of the removed expressions are
conclusions of the remaining inferences in R(Closure), put them back into Todo, and
re-apply the inferences for them like in the main loop of Algorithm 2 (lines 510). This
is known as the DRed (over-delete, re-derive) strategy in logic programming [12]. To
check whether an expression is a conclusion of some inference from Closure, however,
one either needs to record how conclusions where produced, or build indexes that help
to identify matching premises in Closure by conclusions. Storing this information for
everything derived can consume a lot of memory and slow down the inference process.
Note that it makes little sense to simply re-apply all inferences in R to the set
Closure produced by Algorithm 3. This differs little from running Algorithm 1 from
scratch, which applies exactly the same inferences anyway. Most of the inferences are
likely to be already applied to Closure, so, even if it is not fully closed under R, it may
be almost closed. The main idea behind our method presented in the next section, is
to identify a large enough subset of expressions Closure1  Closure and a large enough
subset of inferences R1  R, such that Closure1 is already closed under R1. We can
then re-compute the closure under R incrementally from Closure1 using Algorithm 2
for R+ = R \ R1. As has been shown, using this approach we can avoid applying the
already applied inferences in R1(Closure1).

Let Closure be the set in the input of Algorithm 3, and Closure

the set obtained in
the output. Similarly to Algorithm 2, Algorithm 3 applies all inferences in R(Closure)
except for those in (R \ R
). Indeed, during initialization (lines 23) the
(Closure), and in the main loop (lines 510) it
algorithm applies all inferences in R
applies each inference in (R \ R

)exactly
)(Closure
those inferences that have at least one premise in Closure \ Closure

. The conclusion of

)(Closure) that is not in (R \ R
?

?

?
)(Closure





?

?

?
every such inference is removed from Closure, i.e., it is an element of Closure\Closure

.
Although, as has been pointed out, the output Closure
is not necessarily the closure
under R \ R
, it is, nevertheless, a subset of this closure:
Lemma 1. Let Closure be the set in the input of Algorithm 3, and Closure

obtained in the output. Then Closure

is a subset of the closure under (R\R

the set
).

)(Closure




?

?

?




)(Closure

)  Closure

  Closure and Closure

fore, inf.conclusion  Closure \ Closure

be the closure under (R \ R

Proof. Let Closure
). We need to prove that
  Closure. Define
  Closure

. Clearly, Closure
Closure
  Closure. We claim that Closure1 is
Closure1 := (Closure \ Closure

closed under R. Indeed, take any inf  R(Closure1). Then there are two cases possible:
1. inf  R(Closure)\ (R\ R
): Then inf was applied in Algorithm 3. There-
  Closure1 
2. inf  (R \ R
  Closure1 and
is closed under inf.

, we have inf  (R \ R


Closure
is closed under (R \ R


Closure
Therefore inf.conclusion  Closure
Now, since Closure1  Closure is closed under R and Closure is the smallest set
closed under R, we have Closure1 = Closure. Therefore,  = Closure \ Closure1 =

). Since Closure
)(Closure

)(Closure1), then Closure
  Closure1.

): Since inf  R(Closure1) and Closure

  Closure1.

)(Closure

)(Closure


?

?

?
Closure

 \ Closure
?

?

?
, and so, Closure

  Closure
?

?

?
, as required.




?

?

?
Note that Lemma 1 claims something stronger than just that Closure
. It is, in fact, a subset of the closure under (R\R


. Not every subset of the closure under R \ R

the closure under R\R
R \ R
property means that every expression in Closure
using only expressions in Closure
important for correctness of our method.

is a subset of
) 
)(Closure
has this property. Intuitively, this
can be derived by inferences in R\ R

as intermediate conclusions. This property will be


?

?

?
4.3 Incremental Updates Using Partitions

Our new method for updating the closure under deletions can be described as follows.
We partition the set of expressions in Closure on disjoint subsets and modify Algorithm 3 such that whenever an expression is removed from Closure, its partition is
marked as broken. We then re-apply inferences that can produce conclusions in broken
partitions to repair the closure.
Formally, let Pts be a fixed countable set of partition identifiers (short partitions),
and every expression exp  Exp be assigned with exactly one partition exp.partition 
Pts. For an inference rule R and a set of partitions Pts  Pts, let RPts be the set of
inferences inf  R such that inf.conclusion.partition  Pts and exp.partition / Pts for
every exp  inf.Premises. Intuitively, these are all inferences in R that can derive an
expression whose partition is in Pts from expressions whose partitions are not in Pts.

We modify Algorithm 3 such that whenever an expression exp is removed from
Closure in line 10, we add exp.partition into a special set of partitions Broken. This set
is then used to repair Closure in Algorithm 4. The goal of the algorithm is to collect in
the queue Todo the conclusions of inferences in R(Closure) that are missing in Closure.
?

?

?
Algorithm 4. Repair of over-deletions

input

: R: a set of inferences, Closure: a subset of the closure under R (Closure),
Broken: a set of partitions such that if inf  R(Closure) and
inf.conclusion / Closure then inf.conclusion.partition  Broken
: Todo: the conclusions of inferences in R (Closure) that do not occur in Closure

/* initialize */

/* close */

output
1 Todo, ToRepair, Repaired  ;
2 for inf  RBroken(Closure) do

if inf.conclusion / Closure then
Todo.add(inf.conclusion);

else

ToRepair.add(inf.conclusion);

7 while ToRepair =  do

exp  ToRepair.takeNext();
if exp / Repaired then

for inf  R[exp](Closure) do

if inf.conclusion.partition  Broken then

if inf.conclusion / Closure then
Todo.add(inf.conclusion);

else

ToRepair.add(inf.conclusion);

Repaired.add(exp);

17 return Todo;

This is done by applying all possible inferences inf  R(Closure) that can produce such
conclusions. There can be two types of such inferences: those whose premises do not
belong to any partition in Broken, and those that have at least one such premise. The
inferences of the first type are RBroken(Closure); they are applied in initialization
(lines 26). The inferences of the second type are applied in the main loop of the algorithm (lines 716) to the respective expression in Closure whose partition is in Broken.
Whenever an inference inf is applied and inf.conclusion belongs to a partition in
Broken (note that it is always the case for inf  RBroken(Closure), see also line 11),
we check if inf.conclusion occurs in Closure or not. If it does not occur, then we put the
conclusion into the output Todo (lines 4, 13). Otherwise, we put it into a special queue
ToRepair (lines 6, 15), and repeatedly apply for each exp  ToRepair all inferences
inf  R[exp](Closure) of the second type in the main loop of the algorithm (lines 716).
After applying all inferences, we move exp into a special set Repaired (line 16), which
is there to make sure that we never consider exp again (see line 9).
Lemma 2. Let R, Closure, and Broken be the inputs of Algorithm 4, and Todo the
output. Then Todo = {inf.conclusion | inf  R(Closure)} \ Closure.
Proof. Let Closure
that Todo = Closure
sufficient to prove that Closure

= {inf.conclusion | inf  R(Closure)}. We need to demonstrate
 \ Closure. Since Todo  Closure
and Closure  Todo = , it is

 \ Closure  Todo.
?

?

?
Y. Kazakov and P. Klinov

First, note that Closure
?

?

?
= Closure

.

  Closure
?

?

?
is the closure under R(Closure). Indeed, if Closure

is the
closure under R(Closure), then Closure  Closure

by the assumption of Algorithm 4.
Hence, for every inf  R(Closure)  R(Closure
), we have inf.conclusion  Closure
?

?

?
.

is closed under R(Closure), we
, and since Closure
Therefore, Closure
have Closure
Let Closure1 = {exp  Closure | exp.partition / Broken}. Then it is easy to see
from Algorithm 4 that for every inf  R(Closure1Repaired), we have inf.conclusion 
Closure1  Repaired  Todo. Indeed, if inf.conclusion.partition / Broken then by
assumption of Algorithm 4, since inf  R(Closure) and inf.conclusion.partition /
Broken, we must have inf.conclusion  Closure, and thus inf.conclusion  Closure1.
If inf.conclusion.partition  Broken, there are two cases possible. Either inf 
R(Closure1), thus, inf  RBroken(Closure). In this case inf is applied in Algorithm 4
during initialization (lines 26). Or, otherwise, inf has at least one premise in Repaired,
and hence, it is applied in the main loop of Algorithm 4 (lines 716). In both cases the
algorithm ensures that inf.conclusion  Repaired  Todo.
Now, since Closure1  Repaired Todo is closed under R(Closure1  Repaired) and
ClosureTodo = , it is also closed under R(Closure) (if inf  R(Closure) is applicable
to Closure1RepairedTodo then inf  R(Closure1Repaired)). Since Closure

is the
= Closure1RepairedTodo 
closure under R(Closure), we therefore, have Closure
Closure  Todo. Hence, Closure

 \ Closure  Todo, as required.
?

?

?
After computing the repair Todo of the set Closure using Algorithm 4, we can compute the rest of the closure as in Algorithm 2 using the partially initialized Todo. The
correctness of the complete incremental procedure follows from Lemma 1, Lemma 2,
and the correctness of our modification of Algorithm 2 when Todo is initialized with
missing conclusions of R(Closure).

Algorithm 4 does not impose any restrictions on the assignment of partitions to ex-
pressions. Its performance in terms of the number of operations, however, can substantially depend on this assignment. If we assign, for example, the same partition to all
expressions, then in the main loop (lines 716) we have to re-apply all inferences in
R(Closure). Thus, it is beneficial to have many different partitions. At another extreme,
if we assign a unique partition to every expression, then RBroken would consist of all
inferences producing the deleted expressions, and we face the problem of identifying
such inferences in lines 26. Next, we present a specific partition assignment for the
EL+ rules in Figure 1, which circumvents both of these problems.
5 Incremental Reasoning in EL+
In this section, we apply our method for updating the classification of EL+ ontologies
computed using the rules in Figure 1. We only consider changes in concept inclusion
axioms while resorting to full classification for changes in role inclusions and com-
positions. We first describe our strategy of partitioning the derived subsumptions, then
discuss some issues related to optimizations, and, finally, present an empirical evaluation measuring the performance of our incremental procedure on existing ontologies.
?

?

?
5.1 Partitioning of Derived EL+ Subsumptions
The inferences R in Figure 1 operate with concept subsumptions of the form C  D.
We partition them into sets of subsumptions having the same left-hand side. Formally,
the set of partition identifiers Pts is the set of all EL+ concepts, and every subsumption
C  D is assigned to the partition corresponding to its left-hand side C. This assignment provides sufficiently many different partitions, which could be as many as there
are concepts in the input ontology. It also has the advantage that the inferences RPts
for any set Pts of partitions can be easily identified. Indeed, note that every conclusion
of a rule in Figure 1, except for the initialization rules R0 and R, has the same lefthand side as one of the premises of the rule. Therefore, RPts can only contain those
initialization inferences in R0 and R for which C  Pts.

5.2 Optimizations
Let us discuss a few optimizations that are specific to the EL+ inference rules.
Rule Optimizations: The approach described in Section 4 can be used with any EL+
classification procedure that implements the inference rules in Figure 1 as they are.
Existing implementations, however, include several optimizations to avoid unnecessary
applications of some rules. One of such optimizations in ELK prevents applying rule
R
 to conclusions of R+ , and rules R and R if its left premise was obtained by
R [15]. Even though the closure computed by Algorithm 1 does not change under
such optimizations (the algorithm just derives fewer duplicate conclusions), if the same
optimizations are used for deletions in Algorithm 3, some subsumptions that are no
longer derivable may remain in Closure. Intuitively, this happens because the inferences
for deleting conclusions in Algorithm 3 can be applied in a different order than they
were applied in Algorithm 1 for deriving these conclusions. Please refer to the technical
report [16] for an extended example of this situation.

To fix this problem, we do not use rule optimizations for deletions in Algorithm 3. To
repair the closure using Algorithm 4, we also need to avoid optimizations to make sure
that all expressions in broken partitions of Closure are encountered, but it is sufficient
to insert only conclusions of optimized inferences into Todo.
Subsumptions That Cannot Be Re-Derived: When Algorithm 3 deletes an expression
exp from Closure, we mark exp.partition as broken because this expression could be
re-derived. In some situations this is not possible. One property of the EL+ rules in
Figure 1, is that they derive only subsumptions of the form C  D or C  R.D where
C and D occur in the ontology. So, if a deleted subsumption is not of this form for the
ontology after deletion, we know that it cannot be re-derived. For example, consider
the following ontology O: (ax1) A  B, (ax2) B  C, from which (ax2) is deleted.
When the previously derived conclusion A  C is deleted, there is no need to mark the
partition of A as broken since C does not occur in the ontology after the deletion.
Structural Rules: When we apply our incremental procedure for the EL+ in Figure 1,
to be the inferences that are no longer valid after deletion of axioms. An
we take R
inference by a rule in Figure 1 is not valid when its side condition is not satisfied. For
example, for the rule R, the subsumption D  E may be removed from the ontology,



Y. Kazakov and P. Klinov

or for the rule R+ , the conjunction D1  D2 does not occur in the ontology any more.
But the impact of these two inferences is different: the conclusion of R may be not
correct if the side condition does not hold, but the conclusion of R+ is always correct,
but may be just irrelevant. This distinction between the rules can be used in our next
optimization. We call the rules R0, R, R+ , R
 , and R structuralthese rules use
only the structure of the concepts; they are sound even if their side conditions are not
satisfied. Avoiding application of some structural rules during deletions may result in
fewer broken partitions as shown in the next example.
Consider an ontology O: (ax1) A  B, (ax2) A  C, (ax3) (B  C)  D  E. The

rules in Figure 1 derive the following conclusions (with the partition A):

A  A
A  B
A  C
A  B  C

by R0 since A occurs in O,
by R to (20) using (ax1),
by R to (20) using (ax2),
by R+ to (21) and (22) using (ax3).

(20)
(21)
(22)
(23)
Now, assume that (ax3) is deleted from O. Normally, we should revert the inference
producing (23) by R+ using (ax3) in the deletion stage, which would then mark the
partition of A as broken. We can, however, leave this rule applied (because it is still
sound), which not only makes the partition of A unaffected, but also prevents further
deletion of subsumptions A  B and A  C by rule R

 applied to (23).

5.3 Experimental Evaluation
We have implemented the procedure described in Section 4.3 in the OWL EL reasoner
ELK v.0.4.0,1 and performed some experiments to evaluate its performance.
We used three large OWL EL ontologies which are frequently used in evaluations
of EL reasoners [36]: the Gene Ontology GO [17] with 84, 955 axioms, an EL+-
restricted version of the GALEN ontology with 36, 547 axioms,2 and the official January 2013 release of SNOMED CT with 296, 529 axioms.3

The recent change history of GO is readily available from the public repository.4 We
took the last (as of April 2013) 342 changes of GO (the first at r560 with 74, 708 axioms
and the last at r7991 with 84, 955 axioms). Each change is represented as sets of added
and deleted axioms (an axiom modification counts as one deletion plus one addition).
Out of the 9 role axioms in GO, none was modified. Unfortunately, similar data was
not available for GALEN or SNOMED CT. We used the approach of Cuenca Grau et.al
[11] to generate 250 versions of each ontology with n random additions and deletions
(n = 1, 10, 100). For each change history, we classified the first version of the ontology
and then classified the remaining versions incrementally. We used a PC with Intel Core
i5-2520M 2.50GHz CPU, running Java 1.6 with 4GB of RAM available to JVM.
1 In fact, the incremental procedure in ELK supports many other features outside of EL+, such
as assertions, disjointness axioms, and restricted use of nominals and datatype restrictions, see
http://elk.semanticweb.org for the full release notes.

2 http://www.co-ode.org/galen/
3 http://www.ihtsdo.org/snomed-ct/
4 svn://ext.geneontology.org/trunk/ontology/
?

?

?
Table 2. Number of inferences and running times (in ms.) for test ontologies. The results for
each incremental stage are averaged (for GO the results are only averaged over changes with a
non-empty set of deleted axioms). Time (resp. number of inferences) for initial classification is:
GO (r560): 543 (2,224,812); GALEN: 648 (2,017,601); SNOMED CT: 10,133 (24,257,209).

Ontology

GO (r560)

(EL+ version)

SNOMED CT
(Jan 2013)

1+1

84+26 62,384
3,444
10+10 68,794
100+100 594,420
4,022
10+10 42,026
100+100 564,004

1+1

Repair

Deletion

Changes
Addition
add.+del. # infer. |Broken| time # infer. time # infer. time
?

?

?
48 17,628

4,321
66 37,583
4,508 214 314,666

8,343
3,577 662 138,633

64 120
251 420

Total
time
# infer.

8 58,933
138,945
?

?

?
3,055
10,820

17 49,662
156,039

96 426,462 168 1,335,548

7,331
?

?

?
4 31,966 349

82,335
56 414,255 545 1,116,892 1,376

2,886

The results of the initial and incremental classifications are given in Table 2. For
GO we have only included results for changes that involve deletions (otherwise the
averages for deletion and repair would be artificially lower). First note that in each
case, the incremental procedure makes substantially fewer inferences and takes less
time than the initial classification. Unsurprisingly, the difference is most pronounced
for larger ontologies and smaller values of n. Also note that the number of inferences in
each stage and the number of partitions |Broken| affected by deletions, depend almost
linearly on n, but not the running times. This is because applying several inferences
at once is more efficient than separately. Finally, observe that the repair stage takes a
relatively small fraction of the total time.

In order to compare our method to the module-based approach of [11] (the only
implemented incremental reasoning procedure for DLs which works for TBox additions
and deletions that we are aware of) we classified the same history of GO changes using
the implementation included in the standard distribution of Pellet 2.3.2.5 Pellet provides
a consequence-based procedure for EL classification which was used for re-classifying
the affected parts of the ontology. Unfortunately the same experiment was not possible
for the other two ontologies due to time-outs (10 hours). The results for GO are as
follows: initial classification together with module extraction takes 126 seconds, the
average incremental classification 101 seconds, the average numbers of re-computed
modules are 634 (when processing deletions) and 672 (for additions).
Abstracting from the much worse time results,6 which are likely due to a naive implementation of the module-based incremental procedure and/or the EL algorithm in Pel-
let, it is interesting to compare the average number of modules which are re-computed
during the deletion stage with the average number of broken partitions reported by our
algorithm. Intuitively, both of these metrics characterise the number of named concepts
for which subsumers need to be re-computed upon an axiom change. The number of
modules (634) is greater than the number of broken partitions (560).
Interestingly,
this relationship is of general nature. We prove in the technical report [16] that if a

5 http://clarkparsia.com/pellet/
6 Note that the times in Table 2 are in milliseconds, not in seconds.
?

?

?
subsumption C  D is deleted by our (optimized) incremental algorithm as a result of
deleting some axiom , then  is contained in the locality-based module for C and thus
the module must be re-computed. Simply put, the generic module-based approach may
not incur less overhead than our method for EL+.
In general, this relationship does not hold in the other direction since modules can
contain more axioms than used in derivations. For example, consider the ontology O
containing A  R.B and B  C. The rules in Figure 1 derive only A  A and
A  R.B in the partition for A, thus removing B  C will not break the partition for
A. On the other hand, the locality-based module for A contains all axioms in O, and
thus, it has to be re-computed after the deletion. The difference between the number of
re-extracted modules and the number of broken partitions is likely to be greater for more
complex ontologies, e.g., GALEN. The structure of the anatomical part of GALEN is
known to induce very large locality-based modules [11].

Finally, we have evaluated the effectiveness of the two optimizations from Section 5.2 that can reduce the set of broken partitions when some concepts get deleted
from the ontology. Avoiding applications of structural rules during deletion gives the
most improvement. It reduces the set Broken by roughly 10%, e.g., 498 vs 560 on average for GO. This leads to reduction of the total number of rule applications also by
10%. The time difference is most visible for smaller change sizes, e.g. 1 and 10 for
GALEN and SNOMED CT. Please see the technical report [16] for detailed results.

6 Summary and Future Research
In this paper we have presented a new method for incremental classification of EL+ on-
tologies. It is simple, supports both additions and deletions, and does not require deep
modification of the base reasoning procedure. Our experiments, though being preliminary due to the shortage of revision histories for real-life EL ontologies, demonstrate
that the reasoning results can be obtained almost instantly after small changes. Potential applications of the method range from background classification of ontologies in
editors to stream reasoning and query answering. The method could also be used to
handle ABox changes (via a TBox encoding) or easily extended to consequence-based
reasoning procedures for more expressive Description Logics [18, 19].
The main idea of our method is that we can benefit from knowing the exact rules of
EL+, which is not possible in the general DRed setting. In particular, we can exploit the
granularity of the EL+ procedure, namely that subsumers of different concepts can be
often computed independently of each other. A similar property is a corner stone for the
concurrent EL classification algorithm used in ELK where contexts are similar to our
partitions [4]. In the future, we intend to further exploit this property for on-demand
proof generation (for explanation and debugging) and distributed EL reasoning.
