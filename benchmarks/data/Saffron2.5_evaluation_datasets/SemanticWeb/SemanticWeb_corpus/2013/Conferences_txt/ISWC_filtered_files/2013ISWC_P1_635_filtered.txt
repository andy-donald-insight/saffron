A Graph-Based Approach to Learn Semantic

Descriptions of Data Sources

Mohsen Taheriyan, Craig A. Knoblock, Pedro Szekely, and Jos e Luis Ambite

University of Southern California

Information Sciences Institute and Department of Computer Science

{mohsen,knoblock,pszekely,ambite}@isi.edu

Abstract. Semantic models of data sources and services provide support to automate many tasks such as source discovery, data integration,
and service composition, but writing these semantic descriptions by hand
is a tedious and time-consuming task. Most of the related work focuses
on automatic annotation with classes or properties of source attributes
or input and output parameters. However, constructing a source model
that includes the relationships between the attributes in addition to their
semantic types remains a largely unsolved problem. In this paper, we
present a graph-based approach to hypothesize a rich semantic description of a new target source from a set of known sources that have been
modeled over the same domain ontology. We exploit the domain ontology
and the known source models to build a graph that represents the space
of plausible source descriptions. Then, we compute the top k candidates
and suggest to the user a ranked list of the semantic models for the new
source. The approach takes into account user corrections to learn more
accurate semantic descriptions of future data sources. Our evaluation
shows that our method produces models that are twice as accurate than
the models produced using a state of the art system that does not learn
from prior models.

Keywords: semantic description, source modeling, source description,
semantic model, Semantic Web.

Introduction

Today, information sources such as relational databases and Web services provide
a vast amount of structured data. A common approach to integrate sources
involves building a global model and constructing source descriptions that specify
mappings between the sources and the global model [8]. In the traditional data
integration approaches, source descriptions are specified as global-as-view (GAV)
or local-as-view (LAV) descriptions. In the Semantic Web, what is meant by a
source description is a semantic model describing the source in terms of the
concepts and relationships defined by an ontology. This semantic model can be
viewed as a graph with ontology classes as the nodes and ontology properties as
the links between the nodes.

H. Alani et al. (Eds.): ISWC 2013, Part I, LNCS 8218, pp. 607623, 2013.
c Springer-Verlag Berlin Heidelberg 2013

M. Taheriyan et al.

The first step in building a source description of a source is to determine the
semantic types. That is, each source attribute is labeled with a class or a data
property of the domain ontology. However, simply annotating the attributes is
not sufficient. For example, consider a data table with two columns: name, which
is mapped to the class P erson, and place, which is mapped to the class City.
Unless the relationship between the two columns is explicitly specified, we do not
know whether the city is the birth place of the person or it is the place where she
currently lives. A precise source description needs a second step that determines
the relationships between attributes in terms of properties in the ontology.

Writing source descriptions by hand requires significant effort and expertise.
Although desirable, generating these descriptions automatically is a challenging
problem [1, 7, 10, 17, 20]. Most of the proposed approaches on the Semantic Web
focus on the first step of the modeling process. In our previous work [13], we presented an algorithm to construct semantic models of data sources by computing
a Steiner tree in a graph derived from the ontology and the semantic types. If
the suggested tree is not the correct model of the data, the user interactively
imposes constraints on the algorithm through a graphical user interface to build
the correct model. However, the system does not learn the refinements done by
the user and always suggests a random minimal tree as the initial model of the
new sources.

In this work, we present algorithms to improve the quality of the automatically
generated models by using the already modeled sources to learn the patterns that
more likely represent the intended meaning of a new source. The insight of our
approach is that different sources in the same domain often provide similar or
overlapping data. Thus, it should be possible to exploit knowledge of previously
modeled sources to learn descriptions for new sources. First, we construct a
graph whose main components are subgraphs corresponding to the known source
models. We use the domain ontology to infer the possible paths connecting the
nodes across different subgraphs. This graph models the space of plausible source
descriptions. Next, we label each source attribute with a semantic type and try
to find a set of candidate mappings between these semantic types and the nodes
in the graph. For each resulting set of the nodes, we compute the minimal tree
that connects them and consider this tree as a plausible source model. Then,
we score the models to prefer the ones formed with more coherent and frequent
patterns. Finally, we generate a ranked list of possible semantic models. We can
put users in the loop by allowing them to select the correct model or refine one
of the suggested models. The correct model will be added to the graph as a new
component yielding more accurate models in the future.

Our work provides a basis to learn the source descriptions of information
sources. The main contribution of our work are the techniques to leverage attribute relationships in known source descriptions to hypothesize attribute relationships for new sources, and capturing them in source descriptions. Such
source descriptions are beneficial to automate tasks such as source discovery,
information integration, and service composition. They also make it possible to
convert sources into RDF and publish them in the Linked Data cloud [23].
?

?

?
: subclass
: object property
: data property

name

birthDate

Person

livesIn

bornIn

worksFor

organizer

location

ceo

Event

location

nearby isPartOf

Place

name

postalCode

name

phone

Organization

state

email

startDate endDate

title

City

State

Fig. 1. The ontology that we use to model the sources in the example

2 Motivating Example

In this section, we explain the problem of learning source descriptions by giving
a concrete example that will be used throughout the paper to illustrate our
approach. In this example we have five data sources whose definitions are as
follows:
s1 = personalInf o(name, birthdate, city, state, workplace)
s2 = getCities(state, city)
s3 = businessInf o(company, ceo, city, state)
s4 = getEmployees(employer, employee)
s5 = postalCodeLookup(zipcode, city, state)

s1 is a dataset providing information about people; s2 is a Web service that
takes as input a U.S. state and returns all the cities of that state; s3 is a dataset
providing information about businesses such as their name; s4 is a list of U.S.
companies and their employees; and s5 is a Web service that returns the list of
all the cities in a ZIP code. We use the ontology shown in Figure 1 to build a
source description for each source. These descriptions are illustrated in Figure
2. Now, suppose that the first three sources (s1, s2, and s3) have already been
modeled and the other two (s4 and s5) are new sources not modeled yet. The
goal is to automatically infer the source descriptions for s4 and s5 given the
ontology and the models for s1, s2, and s3.

Automatically building a source description of an unknown source is difficult.
First, the system must map the source attributes to classes in the ontology.
Considering source s4 in Figure 2, attribute employer should be mapped to name
of Organization and attribute employee should be mapped to name of Person.
Next, the system needs to infer the relationships between the classes used to
model the attributes. Our sample ontology has two links between Person and
Organization, namely worksFor and ceo. The system needs to select worksFor,
which captures the intended meaning of s4. The problem is more complicated in
cases when the relevant classes are not directly connected in the ontology and
there exist multiple paths connecting them to each other.

M. Taheriyan et al.

s1

bornIn

Person

worksFor

state

City

State

name

birthDate

name

name

Organization

name

m : model 

Person . name

Person . birthdate

City . name

State . name

Organization . name

personalInfo( name,       birthdate,       city,       state,          workplace)

(cid:113): attribute mapping 
      function
s : source

s2

state

State

City

s3

location

ceo

Organization

Person

isPartOf

City

State

name

name

name

name

name

name

State . name

City . name

Organization . name

Person . name

City . name

State . name

getCities(  state,        city  )

businessInfo( company,         ceo,        city,        state  )

s4

worksFor

s5

Organization

Person

name

name

Place

name

isPartOf

state

City

State

name

name

Organization . name

Person . name

Place . name

City . name

State . name

getEmployees( employer,       employee  )

postalCodeLookup( zipcode,         city,           state )

s(x,y)

: data source

X . y

: data node

: class node

: attribute mapping function

: object property

: data property

Fig. 2. Source descriptions of sample data sources according to the introduced ontology

In this work, we exploit already modeled sources to build semantic models that
are more accurate in terms of the relationships between the source attributes.
One of the metrics helping us to build our models is the link popularity, neverthe-
less, simply using link and node popularity would lead to myopic decisions that
select nodes and links that appear frequently in other models without taking into
account how these nodes are connected to other nodes in the given models. The
main idea of our approach is taking into account the coherency of the patterns
and this is much harder to do. Suppose that we have one model containing the
link organizer between Event and P erson and the link location between Event
and P lace. We also have several models including P erson and P lace (but not
Event) connected by the relation bornIn. If the new source contains P erson,
P lace, and Event, just using the link popularity yields to an incorrect model.

3 Problem Formulation

Having given the example above, we state the problem of learning source
descriptions of sources formally.

A source is a n-ary relation s(a1, , an), with a set of attributes a1, , an,

denoted as Attributes(s).

A semantic model m is a directed graph containing two types of nodes, class
nodes and data nodes. Class nodes (ovals in Figure 2) correspond to classes in
?

?

?
the ontology and are labeled with class URIs (if v is a class node, uri(v) is
URI of the associated class). Data nodes (rectangles in Figure 2), correspond
to the range of data properties and are labeled with a pair of URIs: one is the
URI of a data property and the the other is the URI of one of the domains
of that property e.g., P erson, name or City, name. The links in the graph
correspond to ontology properties and are labeled with property URIs (if e is a
link, uri(e) represents the URI of the property). In general, a semantic model
may contain multiple nodes and links labeled with the same URI.
An attribute mapping function :Attributes(s)N odes(m) is a mapping from
the attributes of source s to a subset of the nodes in m. It can be a partial
mapping, i.e, only some of the attributes are connected to the nodes in m.

A source description is a triple (s, m, ) where s is a source, m is a semantic
model,  is an attribute mapping function that connects the source to the model,
and m can be written as a conjunctive query over the predicates of the domain
ontology O (in this work, we do not deal with source descriptions involving more
complex constructs such as aggregation, union, or negation).

Figure 2 shows the source descriptions for our five example sources. In the fig-
ure,  is represented using the inverted arrows symbols ( ) connecting attributes
in the source to nodes in the model.

The problem of inferring source descriptions can be stated as follows. Let O

be a domain ontology and S = {(s1, m1, 1) , (sk, mk, k)} a set of source

descriptions. Given a new source s, we want to compose a semantic model m
and an attribute mapping function  such that (s, m, ) is an appropriate source
description. Clearly, many triples (s, mi, i) are well-formed source descriptions,
i.e., mi and i are well defined, but only one or a few capture the intended meaning of the data contained in s. Our goal is to automatically compute (s, m, )
such that it minimizes the graph edit distance to a source description that the
user would deem correct. We evaluate our approach by computing the graph edit
distance from (s, m, ) to a user-defined source description.

4 Learning Semantic Descriptions

The approach we take to learn the source description of a new source aims to
characterize a source in terms of the concepts and properties in the domain
ontology. In general, the ontology defines a large space (sometimes infinite) of
possible source descriptions and without additional information, we do not know
which one describes the source more precisely. The main idea here is that data
sources in the same domain usually provide overlapping data. Therefore, we can
leverage the knowledge of previously described sources to limit the search space
and get some hints to hypothesize more plausible candidates.

Our approach has four steps. First, we construct a graph whose main components are the semantic models of the known source descriptions. We use the domain ontology to enrich the graph by adding the nodes and the links that connect
these components. Second, we label the source attributes with semantic types.
This step is not the focus of this paper and we use an existing technique [12]

M. Taheriyan et al.

to annotate the attributes. Third, we find all possible mappings between the
assigned semantic types and nodes of the graph and select the k most promising
mappings. We compute a semantic model m and an attribute mapping function
 for each mapping to construct the top k source descriptions (s, m, ). Finally,
we define metrics to rank the generated candidates.

4.1 Building a Graph from Known Semantic Models

The central component of our method is a directed weighted graph G built on
top of the known semantic models mi and expanded using the domain ontology
O. Similar to the graph of semantic models, G contains both class nodes and data
nodes and links corresponding to properties in O. However, the links in G are
weighted and they also store a list of the model identifiers, called support models.
Algorithm 1 explains how we create G from the known models.

Before we describe the algorithm, we need to define the functions closure(c)
and relations(ci, cj). For every class c in O, we define closure(c) as the set
of classes that either are superclasses of c or can reach c or one of its superclasses by a directed path whose links are object properties. For example,
closure(P erson) = {Organization, Event} because there are the links ceo
and organizer from Organization and Event to P erson. As another exam-
ple, closure(City) = {P lace, State, P erson, Organization, Event}. P lace is
in the set because it is a superclass of City and the other classes have a path
to either P lace or City. We define relations(ci, cj) between two class nodes as
the properties connecting ci to cj. It includes the subClassOf relation and also
the properties inherited from the class hierarchy. For instance, relations(P erson,
City) = {bornIn, livesIn} and relations(City, P lace) = {subClassOf , nearby,
isP artOf}.

The algorithm has three main parts. In the first part (lines 4-17), we add a
component to G for each semantic model mi that is not a subgraph of the existing
components, i.e., mi introduces a new pattern. If mi is subsumed by some of the
components, we just update the support models of the corresponding links in
those components. That means if a pattern appears in k models, all the links of
that pattern will have k elements in their support models. Figure 3 illustrates

the graph built over M = {m1, m2, m3}. Although we have three known models,
two components are added to G since m2 is a subgraph of m1 and we only need
to update the support models of the common links between m1 and m2.

Next (lines 18-20), we find all the classes that are connected to the current
nodes through a path in the ontology. To do this, for every class node v, we
calculate closure(uri(v)). Then, for each class uri in the resulting set, if G does
not already include a node with the same label, we add a new node marked with
class uri. In our example in Figure 3, applying this step adds nodes 9 and 11 to
the graph because Event  closure(City)  closure(State)  closure(P erson)
and P lace  closure(City)  closure(State). In the final part (lines 21-25),
we use the ontology properties to connect different components of the graph.
We compute relations(vi, vj) to find all the possible links between the two class
?

?

?
Component C1
(models m1 and m2)

Person . name

Person . birthDate

birthDate
{m1}

{m1}
name

Person

City . name

{m1, m2}

name

bornIn
{m1}

City

state
{m1, m2}

State

Event

location
w=17 -  1 

Place

organizer

w=17

worksFor

{m1}

Organization

{m1}

name

Organization . name

location
w=17 -  1 

Component C2

(model m3)

worksFor
w=16

ceo
w=16

Organization

{m3}

name

Organization . name

ceo
w=16

worksFor
w=16

Person

ceo
{m3}

location

{m3}

City

isPartOf
{m3}

isPartOf

w=17 -  1 

isPartOf

w=17 -  1 

{m3}

name

City . name

{m1, m2}

name

State . name

bornIn
w=17 -  1 

bornIn
w=16

livesIn
w=17

Person . name

name
{m3}

State

{m3}

name

State . name

Fig. 3. The graph constructed from M = {m1, m2, m3} (semantic models of s1, s2,
and s3) and the domain ontology O

nodes vi and vj that are not both in the same component. For legibility, we only
show some of the links in Figure 3.

Assigning weights to the links of the graph is fundamental in our algorithm.
We assign a very low weight wmin = 	 to all the links inside a component
associated with semantic models (black links in Figure 3). For all other links
(blue links in Figure 3), we assign a high weight wmax. The intuition behind
this decision is to produce more coherent models later when we compute the
minimal tree. In our example, wmax = 17 because the total number of links in
the set of known models (M ) is 17. Regarding the links that do not belong to
any component of G (their support models is empty), we use a simple counting
mechanism to prefer the more popular ones (lines 42-44). For example, the weight
of the link worksF or from node 13 to node 7 is 16 because we have seen the
same link with the same domain and range in m1 (the link from node 4 to node
7). As another example, the link bornIn from node 13 to node 6 has a weight of
(17  1
17 ). The reason is that although there exist one link with the same label
in m1, the target of the links do not match each other.

4.2 Semantic Labeling of Source Attributes

When presented with a new source s whose source description is unknown, the
first step is recognizing semantic types of its data. We formally define a semantic
type to be either an ontology class or a pair consisting of a data property and its

M. Taheriyan et al.

Algorithm 1. BuildGraph(M, O)

Input: A set of known semantic models M = {m1,  , mn} and the domain ontology O
Output: A weighted directed graph G that will be used later in learning semantic
descriptions of new sources

|Edges(mi)|

 wmax = total number of the links in M
 subgraphs of G corresponding to the semantic models

n
i=1

1: wmin  	
2: wmax  
3: components  {}
4: for each mi  M do
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16: end for
17: G =

end for

end if

else
?

?

?
cicomponents

ci

 add the known semantic models to the graph

if mi is a subgraph of c  components then

  c be a subgraph of c that matches mi

let c
add mi to the support models of the links in c

create a new component ci by cloning mi
for each link e  ci do
support models(e)  mi
weight(e)  wmin

components  components  ci

 traverse the ontology O to find the classes that do not map to any node in the graph
but are connected to them through a path in the ontology

 add disjoint components to the graph G

 use the properties defined in O to join the disconnected components

if vi, vj are both class nodes but do not belong to the same pattern then
?

?

?
 mi introduces a new pattern

end if

AddClosure(v, G)

AddLinks(vi , vj , G)

18: for each class node v  G do
19:
20: end for
21: for vi, vj  G do
22:
23:
24:
25: end for
26: return G
27: procedure AddClosure(v, G)
28:
29:
30:
31:
32:
33:
34:
35: end procedure
36: procedure AddLinks(vi , vj , G)
37:
38:
39:
40:
41:
42:

end for

uri(u)  class uri
add a new node u to G

end if

43:
44:
45:
46: end procedure

end for

ClosureSet  closure(uri(v))
for each class uri  ClosureSet do

if there is no node in G labeled with class uri then

RelationSet  relations(uri(vi), uri(vj ))
for each property uri  RelationSet do

add a new link e from vi to vj
uri(e)  property uri
support models(e)  empty
c1  
and target match e}
c2  
weight(e)  min(wmax  c1, wmax  c2/wmax)

|support models(e
|support models(e

eE1

eE2
?

?

?
)| where E1={links of G whose label, source,
)| where E2={links of G labeled with uri(e)}
?

?

?
domain. We use a class as a semantic type for attributes whose values are URIs
for instances of a class and for attributes containing automatically-generated
database keys that can also be modeled as instances of a class. We use a data
property/domain pair as a semantic type for attributes containing literal val-
ues. For example, the semantic type for the first attribute of s4, employer, is
Organization, name.

We employ a supervised machine learning technique introduced in our previous work [12] to learn semantic types. To achieve high accuracy, we use a Conditional Random Field (CRF) [15] method that uses features extracted both from
the attribute names and their values. The CRF is trained with user-specified assignments of attributes to semantic types, specified when the source descriptions
for sources s1 to sn were constructed.

Applying this method on a new source s yields a set of candidate semantic
types, each with a confidence value. Our algorithm then selects the top m semantic types for each attribute as an input to the next step of the process. To make
the description of the source description construction algorithm simpler, we describe the case of m = 1 and then explain how our algorithm can be generalized
to handle m > 1 (the general case is interesting because it enables the algorithm
to cope with situations when the top ranked semantic type is incorrect). Thus,

if the new source s has n attributes denoted by Attributes(s) = {a1, , an},
the output of the semantic labeling step is Labels(s) = {l1, , ln} where li is
the semantic type of ai. For example, for s4 with Attributes(s4) = {employer,
employee} we will have Labels(s4) = {Organization, name, P erson, name}.

4.3 Generating Candidate Models

So far, we have annotated the source attributes with semantic types. To build
a complete source description we still need to determine the relationships between the attributes. For example, after labeling s4s attributes, even though
Organization, name and P erson, name are assigned as the semantic types,
the relationship between the attributes is not fully determined. It is not clear
whether s4 describes organizations and their employees (using worksFor property from P erson to Organization) or organizations and their CEOs (using
ceo property from Organization to P erson). As we can see, even for simple
sources like s4 having few attributes, the problem of learning an accurate semantic description is a difficult problem. What we propose here is to leverage
the knowledge of the known semantic models to discover the most popular and
coherent patterns connecting the semantic labels of a new source s.

The inputs to this step of our algorithm, Labels(s), are the semantic types
assigned to the new source s and the graph G, which includes the known semantic
models, M , and is expanded using the domain ontology O. The output is a set of
candidate source descriptions for the new source s where each candidate (s, m, )
contains the model m along with the mapping  from the source attributes to
the nodes of m. Algorithm 2 shows the steps of our approach.

M. Taheriyan et al.

Algorithm 2. GenerateCandidates(Labels(s), G)

Input: A set of semantic types Labels(s) = {l1,  , ln} and the graph G
Output: A set of candidate source descriptions (s, m, )

 mapping semantic types to the nodes of the graph

matched(li)  all the nodes in G whose label match li
if matched(li) is empty then

else

if li represents a class then

1: Candidates  {}
2: for each li  Labels(s) do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19: end for
20: M atchedSet  {{v1,  , vn}|vi  matched(li)}

AddLinks(vi , vj , G)

end for

end if

add a new class node u to G and uri(u)  li
matched(li)  u
add a new data node v to G and uri(v)  li
matched(li)  v
add a new class node u to G and uri(u)  domain(li )
add a new link e from u to v and uri(e)  data property(li)

end if
AddClosure(u, G)
for class nodes vi, vj where either vi or vj  new nodes do

 add new node(s) if no node matches li

 li is in form of domain, data property

 compute the closure of the new node u

 connect the new added nodes to the other nodes

 for each possible mapping from the semantic types to the nodes, we compute the minimal
tree that connects those nodes

 find the tree with minimal cost

21: for each {v1,  , vn}  M atchedSet do
SteinerN odes  {v1,  , vn}
22:
m  SteinerTree(G, SteinerN odes)
23:
  {a1, v1,   ,an, vn}
24:
Candidates  Candidates  (s, m, )
25:
26: end for

return Candidates

In the first part of the algorithm (lines 2-20), we find all the mappings from the
semantic types to the nodes of the graph. Since it is possible that a semantic type
maps to more than one node in G, more than one mapping might exist from the
semantic types to the nodes. For example, if we look into the graph shown in Figure 3, the semantic type Organization, name maps to nodes 10 and 15 and the
semantic type P erson, name maps to nodes 2 and 14. Thus, for s = s4, we have
four mappings from the semantic types to the graph nodes, r1 = {Organization,
name  node 10, P erson, name  node 2}, r2 = {Organization, name
 node 10, P erson, name  node 14}, r3 = {Organization, name 
node 15, P erson, name  node 2}, and r4 = {Organization, name 
node 15, P erson, name  node 14}. If a semantic type does not map to any
node in the graph, we add a new node to the graph. We use the procedure AddClosure to add the related nodes and then call AddLinks to connect the
newly-added nodes to the rest of the nodes in G.

In the next step (lines 21-26), for each set of nodes in each mapping, we find
the minimum-cost tree connecting these nodes. Given an edge-weighted graph
and a subset of the vertices, called Steiner nodes, the goal is to find the minimumweight tree that spans all the Steiner nodes. Because the Steiner tree problem
?

?

?
is NP-complete, we use an approximation algorithm [14] with an approximation
ratio bounded by 2(1  1/l), where l is the number of leaves in the optimal
Steiner tree. One problem with this algorithm is that if there is large number
of mappings from the semantic types to the nodes of the graph, the algorithm
becomes inefficient, because we will get a large number of sets as the possible
Steiner nodes and we need to run the Steiner tree module over all of them. To
solve this problem, we perform an optimization step right after computing the
possible mappings. We use a blocking method to eliminate the mappings that are
unlikely to generate higher ranked models. This step is not shown in Algorithm
2 to make the algorithm more readable, but we explain it here.

As we will see in the next section, one of the factors to rank the candidate
models is their cost. While it is true that the exact cost cannot be calculated
until we compute the Steiner tree, the way the links are weighted in G enables
us to estimate which sets of Steiner nodes generate lower-cost models. As previously described, all the links inside a known pattern have a very low weight (	).
Consequently, sets of Steiner nodes containing larger number of nodes belonging
to the same pattern are more likely to yield Steiner trees with lower cost. We
apply this idea by sorting all sets of Steiner nodes (M atchedSet in line 20) based
on how coherent each set is. For example, considering the four possible mappings
we showed earlier for s4, r1 and r4 will be ranked higher than r2 and r3, as their
matched nodes are part of the same pattern. Once all the node sets are sorted,
we pick the top k ones and compute the Steiner tree algorithm only over these
sets to generate k candidate models for the new source s. Figure 4 illustrates
the top two candidate models for s4 and s5. The inverted arrows ( ) depict the
mappings from the source attributes to the nodes of the models ().

The blocking method to reduce the number of mappings also supports generalizing our algorithm to handle the case where each attribute is labeled with a
set of possible semantic types rather than only one semantic type (case m > 1
discussed in the previous section). To handle this case, we compute the set of
all permutations of the semantic types and for each permutation, we find the

^
m 4
candidate 1

worksFor

Organization

Person

name

name

^
m 4
candidate 2

ceo

Organization

Person

name

name

Organization . name

Person . name

Organization . name

Person . name

getEmployees(   employer,       employee  )

getEmployees(   employer,       employee  )

^
m 5
candidate 1

Place

name

isPartOf

state

City

State

name

name

^
m 5
candidate 2

isPartOf

isPartOf

City

State

name

name

Place

name

Place . name

City . name

State . name

Place . name

City . name

State . name

postalCodeLookup( zipcode,         city,           state )

postalCodeLookup( zipcode,         city,           state )

Fig. 4. Top two candidate models generated for s4 and s5

M. Taheriyan et al.

possible mappings from the semantic types to the nodes of the graph. Once the
mappings are calculated, we apply the blocking technique to get the k most
promising node sets. Then, we run the Steiner tree algorithm for each node set
to generate k candidate models.

4.4 Ranking Source Descriptions
?

?

?
The final step in learning the semantic description of a source is ranking the
candidates produced in the previous step. We define two metrics to rank source
descriptions, coherence and cost. The cost of a candidate (s, m, ) is the sum
e m weight(e). The goal of defining the coherence is to
of the link weights,
give more priority to the models containing larger segments from the known
patterns. Let Ep = {e|e  m |support models(e)|> 0} be the links in model m
coming from an observed pattern. We partition Ep to create groups of links that
belong to the same pattern. More concretely, we define a list I = (x1, y1,  ,
xn, yn), where xi is the size of a group of links sharing a model identifier in
their support models (links seen in the same pattern), yi is the number of model
i=1 xi = |Ep|.
identifiers shared between all of the links in that group, and
In Figure 4, both candidates of m4 have I = {3, 1}, for the first candidate of
m5 (at the left) I = {3, 2}, and for the second candidate of m5 (at the right),
I = {3, 1}. Note that in m5, Ep does not include the links connecting P lace to
?

?

?
n

the other nodes because P lace does not exist in any of the observed patterns.

We sort the candidate source descriptions first based on their coherence and
then based on their cost. To compare two models regarding the coherence, we
compare their coherence lists. We sort the elements of each list descending and
then compare the elements one by one. The inequality equation between two

elements z1 = xi, yi and z2 = xj, yj can be defined as [z1 > z2; if (xi >
xj)  (xi = xj  yi > yj)]. For example, for s5 the first candidate will be ranked
higher than the second one and for s4, both candidates will be ranked in the
same place since they have the same coherence list and the same cost.

5 Evaluation

We evaluated our approach using 17 data sources containing overlapping data
(the first column in Table 1 shows the signatures of these sources). We created
source descriptions for them manually using the DBpedia, FOAF, GeoNames,
and WGS84 ontologies, and used these source descriptions as the gold standard
to evaluate our approach. We then used our algorithm to learn a source description for each source given the manually created source descriptions of the
other sources as training data (The original source descriptions and the results
are available at: http://isi.edu/integration/data/iswc2013). Since the semantic labeling is not the focus of this paper, we assume that Algorithm 2 is
given the correct semantic type for each attribute (we evaluated the semantic
labeling in our previous work [13, 24]).

We used k = 50 in the third step of our approach to generate 50 candidate
source descriptions and measured the graph edit distance (GED) between the
?

?

?
top ranked description and the manually created one. The results are shown in
the second column in Table 1. The value of GED is the (minimum) sum of the
costs of the edit operations needed to convert one graph to another. The edit
operations include node insertion, node deletion, edge insertion, edge deletion
and edge relabeling. Edge relabeling means that we substitute a link between two
nodes with another link with the same direction but another URI. We assigned
a cost of one to each edit operation.

We compared the results of our approach with the results of Karma [13], a
data integration tool that allows users to semi-automatically create source descriptions for sources and services. To make the Karma results comparable to
our approach, we also gave Karma the correct semantic types for each attribute.
We measured GED between the source description that Karma generates automatically (i.e., without user adjustments) and the gold standard. The results are
shown in the third column of Table 1.

The results show that our algorithm generates source descriptions that are
more than twice as accurate than Karma-generated ones. Karma learns to assign semantic types, but in this evaluation we gave it the correct semantic types,
so Karma was not leveraging any knowledge learned from other source descrip-
tions. Our approach outperformed Karma on all the sources except one. The
one incorrect choice is not unexpected since there are cases for which there is
no prior evidence or the evidence is misleading. Both systems use a Steinertree algorithm to compute their models, so the results show that the learning

Table 1. Evaluation results for learning the semantic descriptions of sample data
sources. The second column is the graph edit distance between our hypotheses and
the correct source descriptions and the third column is the edit distance between the
Karma-generated source descriptions and the correct ones.

Source Signature

GED of

GED of

our models

Karma models

nearestCity(lat,lng,city,state,country)
findRestaurant(zipcode,restaurantName,phone,address)
zipcodesInCity(city,state,postalCode)
parseAddress(address,city,state,zipcode,country)
companyCEO(company,name)
personalInfo(firstname,lastname,birthdate,brithCity,birthCountry)
citiesOfState(state,city)
restaurantChef(restaurant,firstname,lastname)
capital(country,city)
businessInfo(company,phone,homepage,city,country,name)
findSchool(city,state,name,code,homepage,ranking,dean)
ocean(lat,lng,name)
employees(organization,firstname,lastname,birthdate)
education(person,hometown,homecountry,school,city,country)
postalCodeLookup(zipcode,city,state,country)
country(lat,lng,code,name)
administrativeDistrict(city,province,country)

Total
?

?

?
M. Taheriyan et al.

algorithms presented here enable the system to produce more accurate source
descriptions.

We also evaluated our approach on five museum datasets modeled using the
Europeana EDM, SKOS and FOAF ontologies. The models were created by domain experts for the purpose of creating an integrated dataset. The preliminary
results show a 30% improvement, which we believe can be improved further. This
improvement is not as large as the improvement on the other test dataset, but
it shows that the method works with large, real-world datasets and ontologies.

6 Related Work

The problem of describing semantics of data sources is at the core of data integration [8] and exchange [3]. The main approach to reconcile the semantic
heterogeneity among sources consists in defining logical mappings between the
source schemas and a common target schema. The R2RML W3C recommendation [6] captures this practice for Semantic Web applications. Although these
mappings are declarative, defining them requires significant technical expertise,
so there has been much interest in techniques that facilitate their generation.

The mapping generation problem is usually decomposed in a schema matching
phase followed by schema mapping phase [4]. Schema matching [20] finds correspondences between elements of the source and target schemas. For example,
iMAP [7] discovers complex correspondences by using a set of special-purpose
searchers, ranging from data overlap, to machine learning and equation discovery techniques. We use our previous work on semantic labeling [12], which
considers attributes that map to the same semantic type as potential matches.
Schema mapping defines an appropriate transformation that populates the target schema with data from the sources. Mappings may be arbitrarily procedures,
but of greater interest are declarative mappings expressible as queries in SQL,
XQuery, or Datalog. These mapping formulas are generated by taking into account the schema matches and schema constraints. There has been much research in schema mapping, from the seminal work on Clio [10], which provided a
practical system and furthered the theoretical foundations of data exchange [11]
to more recent systems that support additional schema constraints [17]. Alexe
et al. [1] generate schema mappings from examples of source data tuples and
the corresponding tuples over the target schema. Karma [13] and An et al. [2]
generate mappings into ontologies, suggested by exploring low-cost Steiner trees
that connect matching semantic types within a graph derived from the target
ontology. Karma allows the user to correct the mappings interactively.

Our work in this paper is complementary to these schema mapping techniques.
Instead of focusing on satisfying schema constraints, we analyze known source
descriptions to propose mappings that capture more closely the semantics of the
target source, in ways that schema constraints could not disambiguate. For ex-
ample, suggesting that a worksFor relationship is more likely than ceo in a given
domain. Moreover, following Karma, our algorithm can incrementally refine the
mappings based on user feedback and improve future predictions. Carman and
?

?

?
Knoblock [5] also use known source descriptions to generate a LAV mapping
for an unknown target source. However, a limitation of that work is that their
approach could only learn descriptions that were conjunctive combinations of
known source descriptions. By exploring paths in the domain ontology, in addition to patterns in the known sources, we can hypothesize target mappings that
are more general than previous source descriptions or their combinations.

Semantic annotation of services [21, 22] and more recently of web tables [16,
18, 25] has also received attention. Most of this work learns types for services
parameters or table columns, but is limited in learning relationships. Limaye et
al [16] generate binary relationships leveraging the Yago ontology.

Ontology alignment [9] usually considers alignments between individual classes,
so it is more applicable to the matching phase. However, Parundekar et al. [19] use
an extensional approach to discover alignments between conjunctions and
disjunctions of classes from linked data ontologies.

7 Discussion

We presented a novel approach to automatically learn the semantic description
of a new source given a set of known semantic descriptions as the training set
and the domain ontology as the background knowledge. The learned semantic
descriptions explicitly represent the relationships between the source attributes
in addition to their semantic types. These precise descriptions of data sources
makes it possible to automatically integrate the data across sources and provides
rich support for source discovery.

In our approach we build a graph whose main components are the known
semantic descriptions expanded using the domain ontology. Next, we use a machine learning technique to label the attributes of the new source with classes
and properties of the ontology. We find the possible one-to-one mappings from
the semantic types to the nodes of the graph and calculate the top k promising
mappings. Then, we build a tree over each mapping to generate k candidate
models. Finally, we score the candidates to output a ranked list of the most
plausible semantic models. The evaluation results showed that our algorithm
generates models that are more accurate than Karma, a state of the art tool to
semi-automatically model data sources.

The graph construction in the presented algorithm is an incremental process,
i.e., we augment the graph with a new component when a new known model is
presented to the system. The algorithm that we use to compute the Steiner tree
is an approximation algorithm whose complexity is O(|S||V |2) where V is the
set of nodes in the graph and S is a subset of the nodes (size of S is equal to the
number of the new source attributes). Thus, computing the candidate models
might be challenging when the size of the graph is very large in terms of the
number of the nodes, even though we are using a polynomial time algorithm. In
future work, we plan to investigate the idea of creating a more compact graph
by consolidating the overlapping segments of the known semantic models. This
will reduce the number of nodes added to the graph when a new pattern is given
to the system. We also plan to integrate our approach into Karma in order to

M. Taheriyan et al.

suggest more accurate semantic models to users. This will make it possible to
automatically produce source descriptions with minimal user input.

Acknowledgements. This research is based upon work supported in part by
the National Science Foundation under award number IIS-1117913 and in part
by the NIH through the following NIGMS grant: U24 GM104203 Bio-Informatics
Research Network Coordinating Center (BIRN-CC). The views and conclusions
contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or
implied, of NSF or any person connected with them.
