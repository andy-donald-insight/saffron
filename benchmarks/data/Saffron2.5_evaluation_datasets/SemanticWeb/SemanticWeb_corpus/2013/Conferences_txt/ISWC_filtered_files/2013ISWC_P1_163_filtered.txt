Real-Time RDF Extraction from Unstructured

Data Streams

Daniel Gerber, Sebastian Hellmann, Lorenz Buhmann, Tommaso Soru,

Ricardo Usbeck, and Axel-Cyrille Ngonga Ngomo

Universitat Leipzig, Institut fur Informatik, AKSW,

Postfach 100920, D-04009 Leipzig, Germany

{dgerber,hellmann,buehmann,tsoru,usbeck,ngonga}@informatik.uni-leipzig.de

http://aksw.org

Abstract. The vision behind the Web of Data is to extend the current document-oriented Web with machine-readable facts and structured
data, thus creating a representation of general knowledge. However, most
of the Web of Data is limited to being a large compendium of encyclopedic knowledge describing entities. A huge challenge, the timely and
massive extraction of RDF facts from unstructured data, has remained
open so far. The availability of such knowledge on the Web of Data would
provide significant benefits to manifold applications including news re-
trieval, sentiment analysis and business intelligence. In this paper, we
address the problem of the actuality of the Web of Data by presenting
an approach that allows extracting RDF triples from unstructured data
streams. We employ statistical methods in combination with dedupli-
cation, disambiguation and unsupervised as well as supervised machine
learning techniques to create a knowledge base that reflects the content of
the input streams. We evaluate a sample of the RDF we generate against
a large corpus of news streams and show that we achieve a precision of
more than 85%.

1 Introduction

Implementing the original vision behind the Semantic Web requires the provision
of a Web of Data which delivers timely data at all times. The foundational
example presented in Berners-Lee et als seminal paper on the Semantic Web [3]
describes a software agent who is tasked to find medical doctors with a rating of
excellent or very good within 20 miles of a given location at a given point in time.
This requires having timely information on which doctors can be found within
20 miles of a particular location at a given time as well as having explicit data on
the rating of said medical doctors. Even stronger timeliness requirements apply
in decision support, where software agents help humans to decide on critical
issues such as whether to buy stock or not or even how to plan their drive
through urban centers. Furthermore, knowledge bases in the Linked Open Data
(LOD) cloud would be unable to answer queries such as Give me all news of the
last week from the New York Times pertaining to the director of a company.

H. Alani et al. (Eds.): ISWC 2013, Part I, LNCS 8218, pp. 135150, 2013.
c Springer-Verlag Berlin Heidelberg 2013

D. Gerber et al.

Although the current LOD cloud has tremendously grown over the last years [1],
it delivers mostly encyclopedic information (such as albums, places, kings, etc.)
and fails to provide up-to-date information that would allow addressing the
information needs described in the examples above.

The idea which underlies our work is thus to alleviate this current drawback
of the Web of Data by developing an approach that allows extracting RDF from
unstructured (i.e., textual) data streams in a fashion similar to the live versions
of the DBpedia1 and LinkedGeoData2 datasets. The main difference is yet that
instead of relying exclusively on structured data like LinkedGeoData or on semistructured data like DBpedia, we rely mostly on unstructured, textual data to
generate RDF. By these means, we are able to unlock some of the potential
of the document Web, of which up to 85% is unstructured [8]. To achieve this
goal, our approach, dubbed RdfLiveNews, assumes that it is given unstructured
data streams as input. These are deduplicated and then used as basis to extract
patterns for relations between known resources. The patterns are then clustered
to labeled relations which are finally used as basis for generating RDF triples.
We evaluate our approach against a sample of the RDF triples we extracted from
RSS feeds and show that we achieve a very high precision.

The remainder of this work is structured as follows: We first give an overview
of our approach and give detailed insights in the different steps from unstructured
data streams to RDF. Then, we evaluate our approach in several settings. We
then contrast our approach with the state of the art and finally conclude.

2 Overview
We implemented the general architecture of our approach dubbed RdfLiveNews
according to the pipeline depicted in Figure 1. First, we gather textual data
from data streams by using RSS feeds of news articles. Our approach can yet be
employed on any unstructured data published by a stream. Since input streams
from the Web can be highly redundant (i.e., convey the same information), we
then deduplicate the set of streams gathered by our approach. Subsequently,
we apply a pattern search to find lexical patterns for relations expressed in
the text. After a refinement step with background knowledge, we finally cluster
the extracted patterns according to their semantic similarity and transform this
information into RDF.

2.1 Data Acquisition

Formally, our approach aims to process the output of unstructured data sources
Si by continuously gathering the data streams Di that they generate. Each data
[t,t+d] be the
stream consists of atomic elements di
portion of Di that was emitted by Si between the times t and t + d. The data
[t,t+d].
gathering begins by iteratively gathering the elements of the streams Di
from all available sources Si for a period of time d, which we call the time
1 http://live.dbpedia.org/sparql
2 http://live.linkedgeodata.org/sparql

j (in our case sentences). Let Di
?

?

?
Fig. 1. Overview of the generic time slice-based stream processing

slice duration. For example, this could mean crawling a set of RSS feeds for a
duration of 2 hours. We call Di
[t,t+d] a slice of Di. We will assume that we begin
[k.d,(k+1).d] with k  N. The data
this process at t = 0, thus leading to slices Di
gathered from all sources during a time slice duration is called a time slice. We
apply sentence splitting on all slices to generated their elements.

2.2 Deduplication

The aim of the deduplication step is to remove very similar elements from slices
before the RDF extraction. This removal accounts for some Web data streams
simply repeating the content of one of several other streams. Our deduplication approach is based on measuring the similarity of single elements si and
sj found in unstructured streams. Elements of streams are considered to be
different iff qgrams(si, sj) < , where   [0, 1] is a similarity threshold and
qgrams(si, sj) measures the similarity of two strings by computing the Jaccard
similarity of the trigrams they contain. Given that the number of stream items
to deduplicate can be very large, we implemented the following two-step ap-
j within
proach: For each slice Di
[k.d,(k+1)d] = {di
[k.d,(k+1)d]. This results in a duplicate-free data stream i
Di
j :
j) 
[k.d,(k+1)d])  (si
 Di
(di
k, di
)  (di
j
j, di
k
[k.d,(k+1)d]
are then compared to all other elements of the w previous deduplicated streams
[(kw).d,(kw+1)d], where w is the size of the deduplication win-
i
[k.d,(k+1)d] is used for further processing. To ensure the scalability
dow. Only i
of the deduplication step, we are using deduplication algorithms implemented in
the LIMES framework [18]. Table 2 gives an overview of the number of unique
data stream items in our dataset when using different deduplication thresholds.

di
[k.d,(k+1)d] qgrams(si
j) < )}. The elements of i

[k.d,(k+1)d], we first deduplicate the elements si

[(k1).d,kd] to i

 Di

 i

k

[k.d,(k+1)d] qgrams(di

[k.d,(k+1)d]
k, di

 i

j

D. Gerber et al.

2.3 Pattern Search and Filtering

In order to find patterns we first apply Named Entity Recognition (NER) and
Part of Speech (POS) tagging on the deduplicated sentences. RdfLiveNews can
use two different ways to extract patterns from annotated text. The POS tag
method uses NNP and NNPS 3 tagged tokens to identify a relations subject
and object, whereas the Named Entity Tag method relies on Person, Location,
Organization and Miscellaneous tagged tokens. In an intermediate step all consecutive POS and NER tags are merged. An unrefined RdfLiveNews pattern p is
now defined as a pair p = (,S), where  is the natural language representation
(NLR) of p and S = {(si, oi) : i  N; 1  i  n} is the support set of , a set of
the subject and object pairs. For example the sentence:
David/NNP hired/VBD John/NNP ,/, former/JJ manager/NN of/IN ABC/NNP ./.
would result in the patterns:

p1 = ( [hired],{(David, John)} and
p2 = ([, f ormer manager of ],{(John, ABC)}).

After the initial pattern acquisition step, we filter all patterns to improve their
quality. We discarded all patterns that did not match these criteria: The pattern
should (1) contain at least a verb or a noun, (2) contain at least one salient
word (i.e. a word that is not a stop word), (3) not contain more than one non-
alpha-numerical character (except ",  ") and (4) be shorter than 50 characters.
Since the resulting list still contains patterns of low quality, we first sort it by
the number of elements of the support set S and solely select the top 1% for
pattern refinement to ensure high quality.

2.4 Pattern Refinement
?

?

?
The goal of this step is to find a suitable rdfs:range and rdfs:domain as well
as to disambiguate the support set of a given pattern. To achieve this goal we
first try to find an URI for the subjects and objects in the support set of p by
matching the pairs to entries in a knowledge base. With the help of those URIs
we can query the knowledge base for the classes (rdf:type) of the given resources
and compute a common rdfs:domain for the subjects of p and rdfs:range for
the objects respectively. A refined RdfLiveNews pattern pr is now defined as a
quadruple pr = (,S
, , ), where  is the natural language representation, S

the disambiguated support set,  the rdfs:domain and  the rdfs:range of pr.
To find the URIs of each subject-object pair (s, o)  S we first try to complete
the entity name. This step is necessary and beneficial because entities usually get
only written once in full per article. For example the newly elected president of
the United States of America might be referenced as President Barack Obama
in the first sentence of a news entry and subsequently be referred to as Obama.
In order to find the subjects or objects full name, we first select all named
entities e  Ea of the article the pair (s, o) was found in. We then use the
3 All POS tags can be found in the Penn Treebank Tagset.
?

?

?
longest matching substring between s (or o) and all elements of Ea as the name
of s or o respectively. Additionally we can filter the elements of Ea to contain
only certain NER types. Once the complete names of the entities are found, we
can use them to generate a list of URI candidates Curi. This list is generated
with the help of a query for the given entity name on a list of surface forms
(e.g. U.S. or USA for the United States of America), which was compiled
by analyzing the redirect and disambiguation links from Wikipedia as presented
in [14]. Each URI candidate c  Curi is now evaluated on four different features
and the combined score of those features is used to rank the candidates and
choose the most probable URI for an entity. The first feature is the Aprioriscore a(c) of the URI candidate c, which is calculated beforehand for all URIs
in the knowledge base by analyzing the number of inbound links of c by the
following formula: a(c) = log(inbound(c) + 1). The second and third features are
based on the context information found in the Wikipedia article of c and the
news article text (s, o) was found in. For the global context-score cg we apply
a co-occurrence analysis of the entities Ea found in the news article and the
entities Ew found in the Wikipedia article of c. The global context-score is now
computed as cg(Ea,Ew) = |Ea  Ew| / |Ea  Ew|. The local context-score cl is the
number of mentions of the second element of the pair (s, o), o in the case of s
and vice versa, in Ew. The last feature to determine a URI for an entity is the
maximum string similarity sts between s (or o) and the elements of the list of
surface forms of c. We used the qgram distance4 as the string similarity metric.
We normalize all non-[0, 1] features (cg, cl, a) by applying a minimum-maximum
normalization of the corresponding scores for Curi and multiply it with a weight
parameter which leads to the overall URI score:

c(s, o, uri) =

a
amax

+

cg
cgmax

cl
clmax

+

+ sts

If the URIs score is above a certain threshold   [0, 1] we use it as the URI for
s, otherwise we create a new URI. Once we have computed the URIs for all pairs
(s, o)  S we determine the most likely domain and range for pr. This is done
by analyzing the rdf:type statements returned for each subject or object in S
from a background knowledge base. Since the DBpedia ontology is designed in
such a way, that classes do only have one super-class, we can easily analyze its
hierarchy. We implemented two different determination strategies for analyzing
the class hierarchy. The first strategy, dubbed most general, selects the highest
class in the hierarchy for each subject (or object) and uses the most occurring
class as domain or range of pr. The second strategy, dubbed most specific,
works similar to the most general strategy with the difference that it uses the
most descriptive class to select the domain and range of pr.

4 http://sourceforge.net/projects/simmetrics/

D. Gerber et al.

2.5 Pattern Similarity and Clustering
In order to cluster patterns according to their meaning, we created a set of
similarity measures. A similarity measure takes two patterns p1 and p2 as input
and outputs the similarity value s(p1, p2)  [0, 1]. As a baseline we implemented
a qgram measure, which calculates the string similarity between all non stop
words of two patterns. Since this baseline measure fails to return a high similarity
for semantically related, but not textually similar patterns like s attorney ,
and s lawyer , we also implemented a Wordnet measure. As a first step the
Wordnet similarity measure filters out the stop words of p1 and p2 and applies
the Stanford lemmatizer on the remaining tokens. Subsequently, for all token
combinations of p1 and p2, we apply a Wordnet Similarity metric (Path [20],
Lin [13] and Wu & Palmer [25]) and select the maximum of all comparisons as
the similarity value s(p1, p2). As a final similarity measure we created a Wordnet
and string similarity measure with the help of a linear combination from the
before-mentioned metrics. In this step we also utilize the domain and range of
pr. If this feature is enabled, a similarity value between two patterns p1 and p2
can only be above 0, iff {p1, p1
The result of the similarity computation can be regarded as a similarity graph
G = (V, E, ), where the vertices are patterns and the weight (p1, p2) of the
edge between two patterns is the similarity of these patterns. Consequently,
unsupervised machine learning and in particular graph clustering is a viable way
of finding groups of patterns that convey similar meaning. We opted for using
the BorderFlow clustering algorithm [19] as it is parameter-free and has already
been used successfully in diverse applications including clustering protein-protein
interaction data and queries for SPARQL benchmark creation [15]. For each node
v  V , the algorithm begins with an initial cluster X containing only v. Then,
it expands X iteratively by adding nodes from the direct neighborhood of X
to X until X is node-maximal with respect to the border flow ratio described
in [15]. The same procedure is repeated over all nodes. As different nodes can
lead to the same cluster, identical clusters (i.e., clusters containing exactly the
same nodes) that resulted from different nodes are subsequently collapsed to one
cluster. The set of collapsed clusters and the mapping between each cluster and
the nodes that led to it are returned as result.

} \ {p2 , p2

} = .

2.6 Cluster Labeling and Merging
Based on the clusters C obtained through the clustering algorithm, this step
selects descriptive labels for each cluster ci  C, which can afterwards be used to
merge the clusters. In the current version, we apply a straightforward majority
voting algorithm, i.e. for each cluster ci, we select the most frequent natural
language representation  (stop words removed) occurring in the patterns of ci.
Finally, we use the representative label of the clusters to merge them using a
string similarity and WordNet based similarity measure. This merging procedure
can be applied repeatedly to further reduce the number of clusters, but taking
into account that those similarity measures are not transitive, we are currently
only running it once, as were more focused on accuracy.
?

?

?
2.7 Mapping to RDF and Publication on the Web of Data
To close the circle of the round-trip pipeline of RdfLiveNews, the following prerequisite steps are required to re-publish the extraction results in a sensible way:
1. The facts and properties contained in the internal data structure of our tool

have to be mapped to OWL.

2. Besides the extracted factual information several other aspects and meta
data are interesting as well, such as extraction and publication data and
provenance links to the text the facts were extracted from.

3. URIs need to be minted to provide the extracted triples as linked data.
Mapping to OWL. Each cluster ci  C represents an owl:ObjectProperty
propci. The rdfs:domain and rdfs:range of propci is determined by a majority voting algorithm with respect to  and  of all pr  C. The skos:prefLabel 5 of propci
is the label determined by the cluster labeling step and all other NLRs of the patterns in ci get associated with propci as skos:altLabels. For each subject-object
pair in S
 we produce a triple by using propci as predicate and by assigning
learned entity types from DBpedia or owl:Thing.
Provenance Tracking with NIF. Besides converting the extracted facts from
the text, we are using the current draft of the NLP Interchange Format (NIF)
Core ontology6 to serialize the following information in RDF: the sentence the
triple was extracted from, the extraction date of the triple, the link to the source
URL of the data stream item and the publication date of the item on the stream.
Furthermore, NIF allows us to link each element of the extracted triple to its
origin in the text for further reference and querying.

NIF is an RDF/OWL based format to achieve interoperability between language
tools, annotation and resources. NIF offers several URI schemes to create URIs for
strings, which can then be used as subjects for annotation. We employ the NIF
URI scheme, which is grounded on URI fragment identifiers for text (RFC 51477).
NIF was previously used by NERD [21] to link entities to text. For our use case, we
extended NIF in two ways: (1) we added the ability to represent extracted triples
via the ITS 2.0 / RDF Ontology8. itsrdf:taPropRef is an owl:AnnotationProperty
that links the NIF String URI to the owl:ObjectProperty by RdfLiveNews. The
three links from the NIF String URIs (str1, str2, str3) to the extracted triple (s,
p, o) itself make it well traceable and queryable: str1  s, str2  p, str3  o, s 
p  o . An example of NIF RDF serialization is shown in Listing 1. (2) Although
[21] already suggested the minting of new URIs, a concrete method for doing so
was not yet researched. In RdfLiveNews we use the source URL of the data stream
item to re-publish the facts for individual sentences as linked data. We strip the
scheme component (http://) of the source URL and percent encode the ultimate
part of the path and the query component9 and add the md5 encoded sentence to
produce the following URI:
5 http://www.w3.org/2004/02/skos/
6 http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core#
7 http://tools.ietf.org/html/rfc5147
8 http://www.w3.org/2005/11/its/rdf#
9 http://tools.ietf.org/html/rfc3986#section-3

D. Gerber et al.

1 @b ase <h ttp : / / r d f l i v e n e w s . aksw . org / e x t r a c t i o n /www. necn . com/07/04/12/
S c i e n t i s t s d i s c o v e rnewsubatomicp a r t i c / l a n d i n g . html%3FblockID
%3D735470 %26feed ID %3D4213/8 a 1 e 5 9 2 8 f 6 8 1 5 c 9 9 b 9 d 2 c e 6 1 3 c f 2 4 1 9 8 #>.

l i n k i n g

.

d b p ed i a :CERN a owl : Thing ;

s k o s : a l t L a b e l " , d i r e c t o r o f " ;

r l n o : d i r e c t o r O f a owl : O b jectProp erty ;

s k o s : p r e f L a b e l " d i r e c t o r o f " ,
owl : e q u i v a l e n t P r o p e r t y dbp : d i r e c t o r

r l n r : Rolf_Heuer a dbo : Person ;
r d f s : l a b e l " R o l f Heuer "@en ;
r l n o : d i r e c t o r O f d b p ed i a :CERN .

2 ## p r e f i x e s : p l e a s e u se h ttp : / / p r e f i x . cc , e . g . h ttp : / / p r e f i x . cc / r l n o
3 ## e x t r a c t e d p r o p e r t y + r e s u l t o f
?

?

?
7 ## e x t r a c t e d f a c t s :
?

?

?
13 ## p roven an ce t r a c k i n g wi th NIF :
14 <ch ar =0,10>

16 <ch ar =14,18> i t s r d f : t a I d e n t R e f d b p ed i a :CERN .
17 <ch ar =11,24> n i f : anchorOf

.
19 ## d e t a i l e d NIF ou tp u t wi th con text ,
20 <ch ar =0,> a n i f : S t r i n g , n i f : Context , n i f : RFC5147String ;

i t s r d f : taProp Ref r l n o : d i r e c t o r O f

i t s r d f : t a C l a s s R e f dbo : Person ;

" , d i r e c t o r o f "^^xsd : s t r i n g ;

r d f s : l a b e l "CERN"@en .

i n d i c e s and anchorOf

r l n r : Rolf_Heuer .

i t s r d f : t a I d e n t R e f

n i f : i s S t r i n g " R o l f Heuer
d i s c o v e r e d p a r t i c l e
c l a i m i n g o u t r i g h t
e x t r e m e l y f i n e d i s t i n c t i o n . " ;
newsubatomicp a r t i c / l a n d i n g . html ? b l ockID =735470& feed ID =4213>;

n i f : s o u r c e U r l <h ttp : / /www. necn . com/07/04/12/ S c i e n t i s t s d i s c o v e r

s a i d th e newly
th e H i ggs boson i t s e l f  an

, d i r e c t o r o f CERN ,
i s a boson , but he stop p ed j u s t

shy o f

t h a t

i s

i t

d cterms : c r e a t e d "20130509T18 : 2 7 : 0 8 + 0 2 : 0 0 "^^xsd : dateTime .

23 ## e x t r a c t i o n d ate :

25 ## p u b l i s h i n g d ate :
26 <h ttp : / /www. necn . com/07/04/12/ S c i e n t i s t s d i s c o v e rnewsubatomic

p a r t i c / l a n d i n g . html ? b l ockID =735470& feed ID =4213>

d cterms : c r e a t e d "20120815T14 : 4 8 : 4 7 + 0 2 : 0 0 "^^xsd : dateTime .

28 <ch ar =0,10> a n i f : S t r i n g , n i f : RFC5147String ;
?

?

?
n i f : r e f e r e n c e C o n t e x t <ch ar =0, >; n i f : anchorOf " R o l f Heuer " ;
n i f : b e g i n I n d e x "0 "^^xsd : l o n g ; n i f : endIndex " 10 "^^xsd : l o n g ;

Listing 1. Example RDF extraction of RdfLiveNews

http://rdflivenews.aksw.org/extraction/ + example.com:8042/over/ +

urlencode(there?name=ferret) + / + md5(sentence)

extracted

of RDF. The

Republication
on:
http://rdflivenews.aksw.org. The data for individual sentences is crawlable
via the file system of the Apache2 web server. We assume that source URLs only
occur once in a stream when the document is published and the files will not be
overwritten. Furthermore, the extracted properties and entities are available as
linked data at http://rdflivenews.aksw.org/{ontology|resource}/$name and they
can be queried via SPARQL at http://rdflivenews.aksw.org/sparql.

hosted

triples

are

2.8 Linking

The approach described above generates a set of properties with several labels.
In our effort to integrate this data source into the Linked Open Data Cloud,
we use the deduplication approach proposed in Section 2.2 to link our set of
properties to existing knowledge bases (e.g., DBpedia). To achieve this goal, we
?

?

?
consider the set of properties we generated as set of source instances S while
the properties of the knowledge base to which we link are considered to be a set
of target T . Two properties s  S and t  T are linked iff trigrams(s, t)  p,
where p  [0, 1] is the property similarity threshold.

3 Evaluation

The aim of our evaluation was to answer four questions. First, we aimed at
testing how well RdfLiveNews is able to disambiguate found entities. Our second goal was to determine if the proposed similarity measures can be used
to cluster patterns with respect to their semantic similarity. Third, we wanted
to evaluate the quality of the RDF extraction and linking. Finally, we wanted to
measure if all computational heavy tasks can be applied in real-time, meaning
the processing of one iteration takes less time than its compilation.

For this evaluation we used a list of 1457 RSS feeds as compiled in [10]. This
list includes all major worldwide newspapers and a wide range of topics, e.g.
World, U.S., Business, Science etc. We crawled this list for 76 hours, which
resulted in a corpus, dubbed 100% of 38 time slices of 2 hours and 11.7 million
sentences. The average number of sentences per feed entry is approximately 26.5
and there are 3445 articles on average per time slice. Additionally we created
two subsets of this corpus by randomly selecting 1% and 10% of the contained
sentences. All evaluations were carried out on a MacBook Pro with a quad-core
Intel Core i7 (2GHz), a solid state drive and 16 GB of RAM.

3.1 URI Disambiguation

To evaluate the URI disambiguation we created a gold standard manually. We
took the 1% corpus, applied deduplication with a window size of 40 (contains
all time slices) and a threshold of 1 (identical sentences), which resulted in a
set of 69884 unique sentences. On those sentences we performed the pattern
extraction with part of speech tagging as well as filtering. In total we found
16886 patterns and selected the Top 1%, which have been found by 1729 entity
pairs. For 473 of those entity pairs we manually selected a URI for subject and
object. This resulted in an almost equally distributed gold standard with 456
DBpedia and 478 RdfLiveNews URIs. We implemented a hill climbing approach
with random initialization to optimize the parameters (see Section 2.4). The
precision of our approach is the ratio between correctly found URIs for subject
and object to the number of URIs above the threshold  as shown in Equation 1.
The recall, shown in Equation 2, is determined by the ratio between the number
of correct subject and object URIs and the total number of subjects and objects
in the gold standard. The F1 measure is determined as usual by: F1 = 2 
PR
P +R. We optimized our approach for precision since we can compensate a lower
recall and could achieve a precision of 85.01% where the recall is 40.69% and
the resulting F1 is 55.03%. The parameters obtained through the hill-climbing
search indicate that the Apriori-score is the most influential parameter (1.0),
followed by string-similarity (0.78), local-context (0.6), global context (0.45) and

D. Gerber et al.

a URI score threshold of 0.61. If we optimize for F1, we were able to achieve a
F1 measure of 66.49% with a precision of 67.03% and a recall of 65.95%.

For 487 out of the 934 URI in the gold standard no confident enough URI
could be found. The most problems occured for DBpedia URIs which could
not be determined in 305 cases, in comparison to 182 URIs for newly created
resources. Additionally, for 30 resources RdfLiveNews created new URIs where
DBpedia URIs should be used and in 0 cases a DBpedia URI was used where a
new resource should be created. The reason for those mistakes are tagging errors,
erroneous spellings and missing context information. For example Wikipedia has
97 disambiguations for John Smith which can not be disambiguated without
prior knowledge.

We used AIDA [11] to compare our results with a state-of-the-art NED algo-
rithm. We configured AIDA with the Cocktailparty setup, which defines the recommended configuration options of AIDA. AIDA achieved an accuracy of 0.57,
i.e. 57% of the identifiable entities were correctly disambiguated. The corpus
described above provides a difficult challenge due to the small disambiguation
contexts and is limited to graphs evolving from two named entities per text.
AIDA tries to build dense sub-graphs in a greedy manner in order to perform
correct disambiguation. This algorithm would profit from a bigger number of
entities per text. The drawback is AIDA needs 2 minutes to disambiguate 25
sentences. Overall, AIDA performs well on arbitrary entities.

|suric| + |ouric|
|suri| + |ouri|

P =

(1)

|suric| + |ouric|

2  |GS|

R =

(2)

3.2 Pattern Clustering

To evaluate the similarity generation as well as the clustering algorithm we relied
on the measures Sensitivity, Positive Predictive Value (PPV) and Accuracy. We
used the adaptation of those measures as presented in [4] to measure the match
between a set of pattern mappings10 from the gold standard and a clustering
result. The gold standard was created by clustering the patterns as presented in
the previous section manually. This resulted in a list of 25 clusters with more
than 1 pattern and 54 clusters with 1 pattern. Since cluster with a size of 1
would skew our evaluation into unjustified good results, we excluded them from
this evaluation.
Sensitivity. With respect to the clustering gold standard, we define sensitivity
as the fraction of patterns of pattern mapping i which are found in cluster j.
In Sni,j = Ti,j/Ni, Ni is the number of patterns belonging to pattern mapping
i. We also calculate a pattern mapping-wise sensitivity Snpmi as the maximal
fraction of patterns of pattern mapping i assigned to the same cluster. Snpmi =
j=1Sni,j reflects the coverage of pattern mapping i by its best-matching
maxm
cluster. To characterize the general sensitivity of a clustering result, we compute

10 A pattern mapping maps NLRs to RDF properties.
?

?

?
.

n

i=1 Ni
?

?

?
n
i=1 NiSnpmi

n
i=1 Ti,j = Ti,j/T.j

a clustering-wise sensitivity as the weighted average of Snpmi over all pattern
mappings: Sn =
Positive Predictive Value. The positive predictive value is the proportion of
members of cluster j which belong to pattern mapping i, relative to the total
number of members of this cluster assigned to all pattern mappings. P P Vi,j =
Ti,j/
T.j is the sum of column j. We also calculate a cluster-wise positive predictive
value P P Vclj , which represents the maximal fraction of patterns of cluster j
i=1P P Vi,j reflects
found in the same annotated pattern mapping. P P Vclj = maxn
the reliability with which cluster j predicts that a pattern belongs to its bestmatching pattern mapping. To characterize the general PPV of a clustering
result as a whole, we compute a clustering-wise PPV as the weighted average of
P P Vclj over all clusters: P P V =

m

j=1 T.j P P Vclj
?

?

?
m
j=1 T.j

.

Accuracy. The geometric accuracy (Acc) indicates the tradeoff between sensitivity and positive predictive value. It is obtained by computing the geometrical
mean of the Sn and the P P V : Acc =


Sn  P P V .

We evaluated the three similarity measures with respect to the underlying
WordNet similarity metric (see Section 2.5). Furthermore we varied the clustering similarity threshold between 0.1 and 1 with a 0.1 step size. In case of the
qgram and WordNet similarity metric we performed a grid search on the WordNet and qgram parameter in [0, 1] with a step size of 0.05. We achieved the best
configuration with the qgram and WordNet similarity metric with an accuracy
of 82.45%, a sensitivity of 71.17% and a positive predictive value of 95.51%.
The best WordNet metric is Lin, the clustering threshold 0.3 and the qgram
parameter is with 0.45 significantly less influential than the WordNet parameter
with 0.75. As a reference value, the plain WordNet similarity metric achieved
an accuracy of 78.86% and the qgram similarity metric an accuracy of 69.1% in
their best configuration.

3.3 RDF Extraction and Linking
To assess the quality of the RDF data extracted by RdfLiveNews, we sampled the
output of our approach and evaluated it manually. We generated five different evaluation sets. Each set may only contain triples with properties of clusters having at
least i = 1 . . . 5 patterns. We selected 100 triples (if available) randomly for each
test set. As the results in Table 1 show, we achieve high accuracy on subject and
object disambiguation. As expected, the precision of our approach grows with the
threshold for the minimal size of clusters. This is simply due to the smaller clusters
having a higher probability of containing outliers and thus noise.

The results of the linking with DBpedia (see Table 3) showed the mismatch
between the relations that occur in news and the relations designed to model
encyclopedic knowledge. While some relations such as dbo:director are used
commonly in news streams and in the Linked Data Cloud, relations with a more
volatile character such as rlno:attorney which appear frequently in news text
are not mentioned in DBpedia.

D. Gerber et al.

Table 1. Accuracy of RDF Extraction
for subject (S), predicates (P) and objects
(O) on 1% dataset with varying cluster
sizes Ei

Ei

Table 2. Number of non-duplicate sentences in 1% of the data extracted from
1457 RSS feeds within a window of 10
time slices (2h each). The second column
shows the original number of sentences
without duplicate removal.

SAcc
PAcc
OAcc
|Ei|

0.81 0.88 0.86 0.857 0.804
0.86 0.89 0.90 0.935 1.00
0.93 0.91 0.90 0.948 0.941
T otalAcc 0.86 0.892 0.885 0.911 0.906

|P|  |Ei| 28

100 100
?

?

?
Time
Slice

No dedu-
plication

 = 1.0  = 0.95  = 0.9
?

?

?
Table 3. Example for linking between RdfLiveNews and DBpedia

RdfLiveNews-URI DBpedia-URI
rlno:directorOf
rlno:spokesperson dbo:spokesperson [, a spokeswoman for], [spokesperson],

Sample of cluster
[manager], [, director of], [, the director of]

dbo:director

[, a spokesman for]
[s attorney ,], [s lawyer ,], [attorney]

rlno:attorney



3.4 Scalability

In order to perform real-time RDF extraction, the processing of the proposed
pipeline needs to be done in less time than its acquisition requires. This also
needs to be true for a growing list of RSS feeds. Therefore, we analyzed the time
each module needed in each iteration and compared these values between the
three test corpora. An early approximation of this evaluation implied that the
pipeline indeed was not fast enough, which led to the parallelization of the pattern refinement and similarity generation. The results of this evaluation can be
seen in Figure 2. With an average time slice processing time of about 20 minutes
for the 100% corpus (2.2 minutes for 10% and 30s for 1%), our approach is clearly
fit to handle up to 1500 RSS and more. The spike in the first iteration results
out of the fact that RSS feeds contain the last n previous entries, which leads to
a disproportional large first time slice. The most time consuming modules are
the deduplication, tagging and cluster merging. To tackle these bottlenecks we
can for example parallelize sentence tagging and the deduplication.

The results of the growth evaluation for patterns until iteration 30 can be seen
in Figure 3. The number of patterns grows with the factor of 3 from 1% to 10%
and 10% to 100% corpora. Also, the number of patterns found by more than one
subject-object pair increases approximately by factor 2. Additionally we observed
| > 1) and 100% showing
a linear growth for all patterns (also for patterns with |S
the highest growth rate with a factor 2.5 over 10% and 4.8 over 10%.


?

?

?
60s

45s

30s

15s

0s

300s

225s

150s

75s

0s

2000s

1500s

1000s

500s

0s

Deduplication

Tagging

Refinement

Merging

RDF Extraction

Other

Fig. 2. Runtimes for different components and corpora (1% left, 10% middle, 100%
right) per iteration

106

105

104

103

102

Patterns  @ 1%
Patterns+ @ 1%

Patterns  @ 10% 
Patterns+ @ 10%

Patterns  @ 100%
Patterns+ @ 100%

103

102

Cluster  @ 1%
Cluster+ @ 1%

Cluster  @ 10% 
Cluster+ @ 10%

Cluster  @ 100%
Cluster+ @ 100% 

Fig. 3. Number of patterns (log scale) and
| > 1 (Patterns+) for itpatterns with |S
erations and test corpus



Fig. 4. Number of clusters (log scale) and
clusters with |C| > 1 (Cluster+) for iterations and test corpus

The results of the growth evaluation for clusters can be seen in Figure 4. The
evaluation shows that the number of clusters increases by a factor of 2.5 from
1% to 10% and 10% to 100%. Moreover, approximately 25% of all cluster have
more than 1 pattern and the number of clusters grows linear for 1% and 10%
but for the 100% corpus it seems to coverage to 800. The same holds true for
clusters with more then one pattern, as they stop to grow at around 225 clusters.

4 Related Work

While Semantic Web applications rely on formal, machine understandable languages such as RDF and OWL, enabling powerful features such as reasoning and
expressive querying, humans use Natural Language (NL) to express semantics.
This gap between the two different languages has been filled by Information Extraction (IE) approaches, developed by the Natural Language Processing (NLP)
research community [23], whose goal is to find desired pieces of information, such
as concepts (hierarchy of terms which are used to point to shared definitions), entities (name, numeric expression, date) and facts in natural language texts and

D. Gerber et al.

print them in a form that is suitable for automatic querying and processing. Ever
since the advent of the Linked Open Data initiative11, IE is also an important key
enabler for the Semantic Web. For example, LODifier ([2], [6]) combines deep semantic analysis with named entity recognition, word-sense disambiguation and
controlled Semantic Web vocabularies. FOX [17] uses ensemble learning to improve the F-score of IE tools. The BOA framework [9] uses structured data as background knowledge for the extraction of natural language patterns, which are subsequently employed to extract additional RDF data from natural language text.
The authors of [16] propose a simple model for fact extraction in real-time taking
into account the difficult challenges that timely fact extraction on frequently updated data entails. A specific application for the news domain is described in [24],
wherein a knowledge base of entities for the French news agency AFP is populated.
State-of-the-art open-IE systems such as ReVerb automatically identify and
extract relationships from text, relying on (in the case of ReVerb) simple syntactic constraints expressed by verbs [7]. The authors of [5] present a novel
pattern clusters method for nominal relationship classification using an unsupervised learning environment, which makes the system domain and language-
independent. [22] shows how lexical patterns and semantic relationships can be
learned from concepts in Wikipedia.

5 Conclusion and Future Work

In this paper, we presented RdfLiveNews, a framework for the extraction of
RDF from unstructured data streams. We presented the components of the RdfLiveNews framework and evaluated its disambiguation, clustering, linking and
scalability capabilities as well as its extraction quality. We are able to disambiguate resources with a precision of 85%, cluster patterns with an accuracy of
82.5% and extract RDF with an total accuracy of around 90% and handle two
hour time slices with around 300.000 sentences within 20 min on a small server.
In future work, we will extend our approach to also cover datatype properties.
For example from the sentence . . . , Google said Motorola Mobility contributed
revenue of US$ 1.25 billion for the second quarter. the triple dbpedia:Google
rlno:says Motorola Mobility contributed revenue of US$ 1.25 billion for the second quarter can be extracted. Additionally we plan to integrate DeFacto [12],
which is able to verify or falsify a triple extracted by RdfLiveNews. Finally, we
will extend our approach with temporal logics to explicate the temporal scope
of the triples included in our knowledge base.
