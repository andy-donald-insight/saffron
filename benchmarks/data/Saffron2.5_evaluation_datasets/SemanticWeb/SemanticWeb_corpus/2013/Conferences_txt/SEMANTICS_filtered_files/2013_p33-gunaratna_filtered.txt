A Statistical and Schema Independent Approach to Identify

Equivalent Properties on Linked Data

Kalpa Gunaratna

Kno.e.sis Center

Wright State University

Dayton OH, USA

kalpa@knoesis.org

Krishnaprasad
Thirunarayan
Kno.e.sis Center

Wright State University

Dayton OH, USA

tkprasad@knoesis.org

Prateek Jain

IBM T J Watson Research

Center

Yorktown Heights NY, USA

jainpr@us.ibm.com

Amit Sheth
Kno.e.sis Center

Wright State University

Dayton OH, USA

amit@knoesis.org

Sanjaya Wijeratne

Kno.e.sis Center

Wright State University

Dayton OH, USA

sanjaya@knoesis.org

ABSTRACT
Linked Open Data (LOD) cloud has gained significant attention in the Semantic Web community recently. Currently
it consists of approximately 295 interlinked datasets with
over 50 billion triples including 500 million links, and continues to expand in size. This vast source of structured information has the potential to have a significant impact on
knowledge-based applications. However, a key impediment
to the use of LOD cloud is limited support for data integration tasks over concepts, instances, and properties. Efforts to address this limitation over properties have focused
on matching data-type properties across datasets; however,
matching of object-type properties has not received similar
attention. We present an approach that can automatically
match object-type properties across linked datasets, primarily exploiting and bootstrapping from entity co-reference
links such as owl:sameAs. Our evaluation, using sample
instance sets taken from Freebase, DBpedia, LinkedMDB,
and DBLP datasets covering multiple domains shows that
our approach matches properties with high precision and
recall (on average, F measure gain of 57% - 78%).

Categories and Subject Descriptors
I.2.6 [Artificial Intelligence]: Learning

Keywords
Linked Open Data, Property Alignment, Relationship Iden-
tification, Statistical Equivalence

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from Permissions@acm.org.
ISEM 13, September 04 - 06 2013, Graz, Austria
Copyright 2013 ACM 978-1-4503-1972-0/13/09$15.00.
http://dx.doi.org/10.1145/2506182.2506187

1.

INTRODUCTION

About 6 years ago, Sir Tim Berners-Lee introduced the
idea of openly publishing RDF based datasets. This idea
was based on four simple rules as described in [2]. The ultimate objective was to promote interlinking between datasets
and lay the foundation for Web of Data. He advocated
the use of unique URIs to identify and distinguish things on
the Web. Use of different namespaces and URIs frees the
dataset publishers from worrying about name conflicts with
existing resources of the same kind on the LOD. This step
was significant from the point of view of encouraging RDF
publication as, in the span of 6 years, we have approximately
295 datasets interlinked with each other on the LOD cloud1.
These datasets cover numerous domains such as entertain-
ment, life sciences, and governmental legislations. Further-
more, this interlinked collection of diverse and structured
datasets has the potential to be used and exploited for numerous tasks such as enhanced browsing and search. Search
using LOD datasets can be very significant in practice as it
can provide structured representation of entities and important information associated with them returned as results.
Thus, a search for term Tim Berners-Lee will not only return Web pages of him, but also aggregate information from
his FOAF and DBpedia profiles.

Some of the existing LOD search engines such as Sig.ma
[15] provide these capabilities by integrating data from different data sources. These search engines utilize the owl:same
As relationships created by tools like SILK [16] and LIMES
[8]. Such search engines are an important step towards presenting and using information on LOD.

However, there are certain areas where these systems can
be improved. For example, a quick glance at the search
results on Sig.ma points to redundant information about
entities. For Tim Berners-Lee, the two properties named
birth place and born in list similar values2. Here, the birth
place property comes from DBpedia and born in comes from
Yago. These two property names can be aligned to present
an unified view under one property name or used to improve coverage and organization. Therefore, property align-
1http://linkeddata.org
2As of 23-03-2013

                                               33ment is imperative for data integration and consuming tasks
over LOD. Furthermore, properties capture the meaning of
RDF triples and understanding the interconnection between
them is considered to be important in the Semantic Web
context[12].

In this work, we present an approach to align properties
between two different linked datasets. Our approach relies
on utilizing the entity co-reference relationships (ECR) such
as those formalized using owl:sameAs and skos:exactMatch.
Our approach analyzes occurrences of equivalent subject and
object values across datasets to align properties. E.g., given
two matching subject and object pairs that are connected
by ECR links, we check whether the associated property
names have the potential to be equivalent. This is done
in a robust manner by analyzing the aggregated statistical
results related to matching subject-object pairs for a given
pair of properties. Using these results, we show how existing
entity co-reference links between resources in LOD cloud can
be used to align properties. The main contributions of the
paper are as follows:

 It introduces an efficient approach that utilizes property extensions and resource inter-links for property
alignment.

 It uses the notion of Statistical Equivalence to approx-

imate owl:equivalentProperty.

The rest of the paper is organized as follows. In Section
2, we discuss the state of the art techniques used in various property alignment tasks. Section 3 elaborates on the
problem and the proposed algorithm. Section 4 presents
the results obtained and shows that Statistically Equivalent
matches (defined later) outnumber coincidental matches as
the algorithm is run on a large number of instances. Section
5 discusses interesting matching patterns and facts relevant
to the matching process, while Section 6 concludes with our
findings and future work.

2. BACKGROUND

Property alignment, which is crucial for data integration
tasks, has not been addressed adequately compared to concept and instance alignment. In contrast with instance and
concept alignment, properties exhibit complex structure and
meaning. Current techniques used for property alignment
(including object-type) fall into three categories: (i) Syntac-
tic/ dictionary-based, where predicate similarity is garnered
via string matching or using WordNet, (ii) Schema depen-
dent, and (iii) Schema independent, where instance level
information is utilized. Some use a mix of these techniques.
There are a few efforts for matching data-type properties in ontologies [10][14]. Nunes et al. [10] utilized mutual
information present at the instance level using genetic al-
gorithms. They discuss matching of one to many complex
relationships in different ontologies, but are limited to datatype properties. [14] discusses a cluster based similarity aggregation methodology for ontology matching where they
focus on four different similarity measures to align proper-
ties. They calculate string, WordNet, profile, and instance
similarities based on the domains and ranges of the prop-
erties. Even though [14] tries to match object-type prop-
erties, they mention that the results are not strong enough
to distinguish matching and non-matching property names.

[13] incorporates a density estimation approach using Kernel Density Estimation (KDE) to map opaque properties
(properties conveying the same meaning irrespective of their
names) in ontologies. However, they transform values into
numeric form to be compatible with KDE. This transformation is not easy in the LOD context where each instance
contains many triples.

Zhao et. al [17] presented a graph based ontology analysis complementing their previous work related to building a
mid-level ontology utilizing WordNet and string based similarity measures to group properties. The approach is not
suitable for identifying equivalent properties since it groups
any two related properties (using object value similarity)
and does not take into account the effect of coincidental
matches in initial grouping. SLINT [9] is an instance matching system that uses an IR based heuristic to calculate object
values overlap. Both these approaches are coarse grained
and hence not suitable for identifying equivalent properties
as they aggregate properties on the overlap (not on the individual subject pairs). E.g., they can confuse conceptually different predicates placeOfBirth and placeOfDeath.
These approaches are also different from ours in the sense
that they use only object values and their overlap whereas
we strictly try to match the property extensions minimizing false positives. TripleRanks effort [4] in faceted browsing computes latent predicate similarity (i.e., similar properties within a dataset) indirectly as a byproduct of SVD
and it is hard to verbalize the results in terms of exten-
sions. Furthermore, it does not provide an evaluation on
intra/inter dataset property alignment in terms of precision
and recall. The analysis of owl:sameAs networks and their
implications for detecting schema-level inconsistencies and
ontology alignment are discussed in [3]. Some of the alignment techniques and applications have used these networks
showing their effectiveness in practice [11][17]. We also use
a similar link traversal network model in our approach to
match property extensions.

3. APPROACH

Property alignment is a non-trivial research problem in
the Semantic Web domain whose accomplishment can lead
to significant advancements in data integration tasks. The
objective of property alignment is to identify equivalent or
sub-property relationships between a property pair P1 and
P2, which may be in the same or different datasets. Since
property names in different datasets have independent origin and relationships capture complex meaning in a triple
[12], calculating string similarity or synonym based measurements on property names alone does not suffice. To solve
this problem, our extensional approach determines related
properties (P ) by finding similar triple patterns across datasets
by matching subject (S) and object (O) values in triples of
the form (SP1O) and (SP2O).
3.1 Property alignment between datasets

OWL [1] defines the concept of equivalent property (owl:
equivalentProperty) as two properties having the same ex-
tension. For example, if property P is defined by triples
{ a P b, c P d, e P f } and property Q is defined by triples
{ a Q b, c Q d, e Q f }, then they are equivalent properties
because they have the same extension {{a, b}, {c, d}, {e, f}}.
Since it is hard to expect exactly the same property extensions in real datasets, we approximate it by a signifi-

                                               34cant overlap in matching subject-object pairs. For this pur-
pose, we define statistical equivalence of properties on linked
datasets. For example, if property P is defined by triples
{ a P b, c P d, e P f } and property Q is defined by triples
{ a Q b, c Q d, g Q h }, then property extensions are not the
same, but P and Q have matching subject and object values two times out of three providing statistical evidence in
support of equivalence. When we utilize evidence for extension matching, we need to overcome the potential problem of
incorrect matches in complex data representation contexts.
We first determine the relatedness between a property
pair to decide a match, which also reduces the search space.
Note that while SKOS[7] is a formal specification of concept
relatedness in ontologies, there is no such specification for
properties. We now present some notions to help represent
property alignment on linked data. We first define the notion
of candidate match between two properties.

The following statement is true for all the definitions in the
paper. Let S1P1O1 and S2P2O2 be two triples in two different datasets D1 and D2 respectively representing relations
P1(S1, O1) andP 2(S2, O2). ECR are the entity co-reference
links described in Section 1.

Definition 1: Candidate Match
The two properties P1 and P2 are a candidate match iff S1
ECR* S2 and O1
connected by an ECR
the instances using ECR links (where,
notation).

link if there is a link path between
is the Kleene star

ECR* O2. We say two instances are





Candidate matches can provide supportive evidence for
property alignment. But there can be coincidental (spu-
rious) matching of properties. Consider the following two
triples in the datasets DBpedia(d) and Freebase(f):

d:Arthur Purdy Stout d:place of birth d:New York City
f:Arthur Purdy Stout
f:place of death f:New York City

Arthur Purdy Stout is a person (in fact, a surgeon and
pathologist in real life) who lived in New York City. Given
that d:Arthur Purdy Stout is the same as f:Arthur Purdy
Stout and d:New York City is the same as f:New York City,
d:place of birth and f:place of death properties are a candidate match according to the definition. But clearly these
two properties should not be treated as equivalent because
they have different intentional semantics. Therefore, this
coincidental match is not an equivalent match.

To minimize mis-identification of coincidental matches as
equivalent (ideally eliminating them), our approach aggregates additional evidence in support of a statistical match,
to approximate equivalent match (defined formally using ex-
tensions). Therefore, we keep track of some statistical measures along with the candidate matches to compute statistical equivalence. For a candidate matching property pair
(P1,P2), Match Count (P1,P2) andCo-appearance Count
(P1,P2) can be defined as follows.

Match Count (P1,P2) is the number of triple pairs for P1
and P2 that participate in candidate matches. That is,

(P1, P2) = |{S1P1O1  D1 |  S2P2O2  D2
ECR* O2}|

ECR* S2  O1

 S1

(1)

Step 1 

Dataset 1 

Dataset 2 

owl:sameAs 

d2:theodore_harold_maiman


I1 

I1 

I1 

triple 1 

d1:Theodore_Maiman 

P1=d1:doctoralStudent 
triple 2 

triple 3 

P2=d2:education. 
academic.advisees 
triple 4 

Step 3 

matching resources  


I2 

I2 

triple 5 

I1=d1:Willis_Lamb  

I2 =d2:willis_lamb 

property P1 and property P2 are a candidate match 

Figure 1: Process of Candidate Matching. Matching
resources are in the same color/pattern.

Co-appearance Count (P1,P2) is the number of triple
pairs for P1 and P2 that have matching subjects. That is,

(P1, P2) = |{S1P1O1  D1 |  S2P2O2  D2

 S1

ECR* S2}|

(2)

Statistical equivalence in this work is measured by analyzing candidate matches over co-appearances of a property
pair, which provides statistical evidence, i.e., it will have
many matching subject-object pairs over common subjects.
Therefore, the number of matching subject-object pairs in
the property extensions and co-appearances of a property
pair directly influence the decision function F (defined be-
low) for selection (captured using a confidence threshold ).
Also, a property pair must co-appear enough times (sup-
porting evidence) to be picked as a match, to overcome coincidental matches, by achieving a sufficient match count .
Therefore, this minimum number of match count  should
be greater than a constant k. This constant k filters out
many incorrect random candidate matches. Now, statistically equivalent property pairs can be defined as follows.

Definition 2: Statistically Equivalent Properties
The pair of properties P1 and P2 are statistically equivalent
to degree (, k) iff

F = (P1, P2)/(P1, P2)  
where, (P1, P2)  k, and 0 <  1, k > 1

(3)

Note that, a list of statistically equivalent properties can,

strictly speaking, consist of equivalent properties, sub-properties
and incorrectly mapped coincidental matches.
3.2 Resource matching and generating prop-

erty alignments

Our algorithm is based on exploiting entity co-reference
links that exist between instances in linked data for candidate matching of property pairs. This process is further
illustrated in Figure 1 by matching an instance I1 (Willis
Lamb) with I2 using owl:sameAs as the co-reference (ECR)
link. This is achieved in three steps.
In step 1, the corresponding instance for I1 is identified as I2 by following

                                               35resources  set S1 

resources  set S2 

f:theodore_harold_maiman 
d:Theodore_Maiman 

d:Theodore_H._Maiman 
d:Theodore_Harold_Maiman 

d:Theodore_Maiman 

d:Ted_Maiman 

Dataset1 = DBpedia(d) 

Dataset2 = Freebase(f) 

owl:sameAs 

owl:sameAs 

d:Willis_Lamb 

P1 = doctoralStudent 

P2 = education. 
academic.advisees 

f:Willis_Lamb 

d:Theodore_Maiman 

f:theodore_harold_maiman 

Matching 

D2:Dataset 2 

D1:Dataset 1 

Subject Pairs 

S1  doctoralStudent  O1   S1
S1  birth_place  O2 
S1
S2  doctoralStudent  O4   S2
S2
S2  birth_place  O5 
S2
S3  doctoralStudent  O6   S3
S3
S3

Matching 
Object Pairs 
| ) 
( O1  , O1
--- 
( O4  , O4
( O5  , O5
( O5  , O5
( O6  , O6
--- 
S3  birth_place  O8 
( O8  , O8
Generated Candidate Matching Property Lists (cid:177) Matches selected are in Boldface 
[D1:doctoralStudent] 
[D1:birth_place] 

| academic.advisees  O1
| place_of_birth  O3 
| influenced  O4

|  place_of_birth  O5
|  place_of_death  O5

|  academic.advisees  O6
| influenced  O7 
|  place_of_birth  O8

[ [D2:academic.advisees, 2:2] , [D2:influenced, 1:2] ]        list1 
[ [D2:place_of_birth, 2:3] , [D2:place_of_death, 1:1] ]      list2 

( S1  , S1
( S1  , S1
( S2  , S2
( S2  , S2
( S2  , S2
( S3  , S3
( S3  , S3
( S3  , S3

| ) 
| ) 
| ) 
| ) 
|  ) 
| ) 
| ) 
|  ) 

| ) 
| ) 
| ) 
| ) 

| ) 

Figure 2: Property matching with overlapping sets
of resources

Property Pair 

MatchCount  Co-appearanceCount 

[D1:doctoralStudent, D2:academic.advisees] 
[D1:birth_place, D2:place_of_birth] 


In step 2, the subject
an owl:sameAs link from I1 to I2.
instances are expanded using triples they consist of.
I.e.,
dataset 1 has three triples for I1 as the subject and dataset
2 has two triples for I2 as the subject. In step 3, since all subject values are matching for the triples of I1 and I2, finding
two matching object values leads to a candidate match, i.e.,
in triples 2 and 4, object values can be matched by following
an owl:sameAs link between them. Therefore both subject
and object values are matching for triples 2 and 4 (also an
extension match) leading to a candidate match for doctoralStudent and education.academic.advisees properties. This
process is further outlined in Algorithm 1.

Algorithm 1 GenerateCandidateMatches(X, namespace)
Input: X, namespace
Output: ,  for each property pair

1: for i = 1  Size(X) do
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23: end for

end for

end for

end if

end if

else

p matches q

subject S1  X[i]
subject S2  GetCorsEntity(subjectS1, namespace)
map l1  ExtractP O(S1)
map l2  ExtractP O(S2)
for each property p  l1 do

object o1  l1(p)
Set o1 ecr  GetECRLinks(o1)
for each property q  l2 do
if p exactmatch q then

update (p, q)
object o2  l2(q)
Set o2 ecr  GetECRLinks(o2)
isM atch  M atch(o1 ecr, o2 ecr)
if isM atch then
update (p, q)





Comparison of the object values of triples is computed
by checking for an overlap of ECR links, which includes
. This is further illustrated in Figure 2
checking for ECR
with only direct linking between them (one level deep path)
for clarity. Further, this overlap based computation allows
) to arrive (converge) at a
us to have link chains (ECR
decision. The process of candidate matching is presented
in Algorithm 1 where the input to the procedure is a set
X of subject instances of the first dataset and namespace
identifier of the second dataset. The GetCorsEntity procedure returns the relevant corresponding entity in the second dataset for a given subject instance in the first dataset
(by following an ECR link and namespace identifier). The
ExtractP O procedure is for extracting all the propertyobject pairs for a given subject instance and returns a map
data structure. The map data structure has object values
stored for each property. The GetECRLinks procedure re-

Figure 3:
appearanceCount values

Calculating MatchCount

and Co-

turns all the available corresponding entities without considering any namespaces.
If two property names can be
matched exactly (except namespaces), then they are considered as matched, as outlined in line 11 of the algorithm.
As the process outlined in Algorithm 1 is performed on a
sample instance set taken from dataset 1, candidate matches
are found with  and  counts. Then applying function F
with  and k in equation 3 for aggregated  and  for each
property pair in the whole instance set will yield statistically equivalent property pairs for the two datasets. Note
that GetCorsEntity and GetECRLinks procedures can be
configured to use a specific ECR link or multiple types of
ECR links (like owl:sameAs, skos:exactMatch, etc).

Figure 3 shows an example for calculating  and . In this
example, the algorithm used =0.5 and k=2 to decide on
matches from each list of candidate matching pairs and eliminate coincidental matches. Recall that  is the minimum
fraction of matching triples over co-appearances and k is the
minimum number of matching triples required to be considered for equivalence. The need for k together with  can
be further explained using list1 & list2 in Figure 3, where
doctoralStudent matched to influenced and birth place
matched to place of death with satisfying . Thus use
of k avoids such coincidental (incorrect) matches.
It also
illustrates the nature of our bootstrapping algorithm that
decides on a matching property pair after analyzing the evidence from all matching subject-object pairs of the property
extensions.
3.2.1 Complexity Analysis
Let the average number of properties for an entity be x
and average number of objects for a property be j. Then,
Algorithm 1 has to compare j  j  x  x object value pairs
for an entity pair. For n number of subjects, it would be
n  j  j  x  x. To extract property-object pairs, it requires
2  n operations3. x and j are independent of n as number
of properties per instance and number of object values per
property do not change on average for larger or smaller n.
Since n > j and n > x, O(n  j2  x2 + 2n) isn . The
inherent parallel nature of Algorithm 1 allows us to adapt it

3If comparisons are not restricted this way to each subject,
a naive algorithm would need n2  j2  x2 + 2n comparisons.

                                               36to Map-Reduce paradigm to improve efficiency.

Implementation details

The algorithm is implemented using Java, Apache Hadoop
and Jena framework for processing RDF triples. Datasets
are replicated locally using Virtuoso triple store except for
Freebase dataset for which we used their public APIs.



In this experiment, we mainly used owl:sameAs and skos:
exactMatch links as ECR (owl:sameAs was the dominant
link of the two for the selected datasets). When searching
for ECR links between entities, we investigated one level
deep paths in this experiment (see Figure 2) since it was
sufficient for the experimental datasets. But this can be extended further by examining more than one level deep link
) for enhanced coverage. We also used exact
paths (ECR
matching of rdfs:labels of the two objects when comparing
ECR links for object equivalence. This is used as an approximation to ECR links for the comparison of object values
(line 16 of Algorithm 1) to improve coverage in absence of
ECR links. This is because, we are looking for exact matching of labels for objects that belong to the same entity in
two datasets. It is a good approximation as the subject (en-
tity) is guaranteed to be the same, since only ECR links are
used for subject matching. This approximation also tries to
compensate for sparseness of ECR links.

The process presented in Algorithm 1 requires a lot of
comparisons and could grow for larger instance sets. But the
algorithm was easily adapted to Map-Reduce framework by
distributing subject instances among mappers. The specific
implementation is explained below:

 Map phase

 Let the number of subject instances in dataset 1
(D1) be X and namespace of dataset 2 (D2) be
ns. For each subject i  X, start a mapper and
call GenerateCandidateMatches(i, ns) outlined in
Algorithm 1.
 Each mapper outputs (key,value) pairs as (p:q,
(p,q):(p,q)) to reducer, where property pD1
and property qD2.

 Reducer phase

 Collects output from all mappers and aggregates

(p,q) and (p,q) values for each key p:q.

The process can be parallelized since computation of one
subject instance is independent of the others. We implemented the algorithm in Hadoop Map-Reduce 1.0.3 framework and achieved significant improvements in running time
on a 14 node cluster (speedup of 833% compared to the desktop version on average). Moreover, we may achieve faster
times when more resources are available to paralleize. Further detailed discussion on this is out of scope of this paper.
When the algorithm is run for a sample set of instances
starting from dataset D1, we can generate candidate matches
of property pairs. One property may be matched to many
other properties because of similar extensions, as explained
in Section 3.1. After recording candidate matches, each
property in D1 has been mapped to a list of properties that
are in dataset D2 with match counts (see Figure 3). The
most probable matching properties will have higher match
counts in the list and also have higher values for function F .

We sort each list in descending order based on match counts
and then test the first property pair from each sorted list to
see whether it satisfies threshold  and k for a match4. Applying these thresholds and sorting the lists remove many
of the coincidental matches and leave probable matches.
The algorithm outputs these as statistically equivalent. In
this experiment, we were able to remove 76%, 87%, 67%,
83%, and 25% coincidental matches respectively from Experiments 1 to 5 5. These statistics also reveal that simple
object value overlap or grouping mechanisms [9][17] are not
adequate for finding equivalent properties in the LOD con-
text.

4. RESULTS AND EVALUATION

The objective of this evaluation is to show that statistical equivalence of properties can successfully approximate
owl:equivalentProperty in the LOD context and the developed algorithm performs considerably better than the existing techniques used for property alignment, which includes string similarity and external hierarchies (i.e., Word-
Net) based approaches. To prove our claim, we chose DB-
pedia6, Freebase7, LinkedMDB8, DBLP L3S9, and DBLP
RKB Explorer10 datasets and extracted sample instance sets
for the following reasons. (1) They have more or less com-
(2) They are well interplete information for instances.
(4)
linked.
They cover several dimensions of our evaluation as multidomain to multi-domain, specific-domain to multi-domain
and specific-domain to specific-domain dataset property align-
ment.

(3) They have complex/opaque11 properties.

Our property alignment results are presented in Table 1.
We randomly selected 5000 subject instances for each exper-
iment, numbered 1 to 5 in Table 1. The experiments cover
object-type properties in person, film, and software domains
between DBpedia and Freebase, films between DBpedia and
LinkedMDB, and articles between DBLP RKB explorer and
DBLP L3S datasets. DBLP RKB explorer and DBLP L3S
are two separate datasets for DBLP that use two different
ontologies. Furthermore, the DBLP RKB explorer project
had different requirements, where it needed to achieve high
precision and recall on search services compared to DBLP
L3S dataset. DBpedia and Freebase are both multi-domain
huge data hubs in LOD with overlapping information but
have non-trivial differences in their schema. Freebase uses
more blank nodes, increasing its complexity compared to
DBpedia. LinkedMDB is a specialized dataset for movies
and has its own schema which is completely different from
DBpedia. Therefore, the datasets selected for the experiments from LOD are different from each other in both complexity and variety of data representation.

4Assuming both datasets do not have duplicate properties.
In our experiments, DBpedia had duplicate properties.
5From 732, 355, 221, 255 and 4 generated candidate property pairs respectively.
6http://dbpedia.org/
7http://freebase.com/
8http://linkedmdb.org/
9http://dblp.l3s.de/d2r/
10http://dblp.rkbexplorer.com
11Complex or opaque properties are semantically the same
but have different word selections in describing the relation-
ship.

                                               37Measure
Type

Precision
Recall
F measure
Precision
Recall
F measure
Precision
Recall
F measure
Precision
Recall
F measure

DBpedia-
Freebase
(Person)1

0.8089*
0.8410*

0.4777*
0.6000*

0.5350*
0.5978*

0.4140*
0.4609*

DBpedia-
Freebase
(Film)2


DBpedia-
Freebase
(Software)3


DBpedia-
LinkedMDB
(Film)4


DBLP RKBDBLP L3S
(Article)5


Average


Our
Algorithm

Dice
Similarity

Jaro
Similarity

WordNet
Similarity

Table 1: Alignment results of object-type properties. Experiments are numbered 1 to 5.

Experiment 1 

Experiment 2 

Precision
Recall
F1
k=14 

Experiment 5 

alpha 

alpha 

Experiment 3 

alpha 

Precision
Recall
F1
k=2 

Experiment 4 

Precision
Recall
F1
k=2 

alpha 

alpha

Precision
Recall
F1
k=6 

Precision
Recall
F1
k=2 

Figure 4: Precision, Recall and F measures for varying  values

4.1 Deciding  and k values

Estimating the values for  and k was done based on the
following facts. (1) Data in different datasets on LOD are
not complete and contain similar but not identical informa-
tion. (2) Representation of these data is not uniform due
to multiple authors and naming preferences. (3) A resource
is not guaranteed to be linked to all matching resources.
Because of these reasons, the same property in two different datasets cannot be expected to have close values for 
and  for a higher F value closer to 1. Therefore, based
on the above observations and our empirical evaluation, a
threshold value closer to 0.5 for  seemed to be appropriate
considering F measure. Figure 4 further clarifies this claim
in our empirical evaluation showing precision, recall, and F
measure for varying  with the chosen k values. Results presented in Table 1 are using 0.5 for  except for experiment
2 where it is 0.7 for optimal F measure results.

The constant k is also affected by the above reasons and
we followed a data driven approach to approximate a suitable value for k as follows. First the algorithm output is
filtered using =0.5 and k=2 which means, the lowest possible matching with a positive confidence level12. Then we get
the  values for property pairs matched using exact string
matching, which are not identified by the algorithm and get
the average of the counts as constant k. Our analysis over
k suggests that most of the time, optimal performance for
the algorithm is achieved by a value closer to this approximation of k (default set to 2). Following this approach, we
used 14, 6, 2, 2, and 2 for k in the experiments numbered

as 1, 2, 3, 4, and 5 respectively in Table 1. Increasing both
these thresholds  and k yields high confidence matches as
they demand a higher number of extension matching.
4.2 Experiment setting

We compared our bootstrapping based algorithm with
two string matching algorithms and WordNet13 similarity
suggested by [17]. We calculated WordNet similarity14 by
searching for at least one matching object value [17] and
then applying WordNet similarity on pre-processed terms
of the properties. We tokenized and removed stop words
from properties to calculate WordNet similarity and added
Porters stemming algorithm for string similarity. The results shown for these similarities are the optimal ones considering F measure metric. The threshold values used for string
similarity and WordNet similarity algorithms were 0.92 and
0.80 respectively. For experiment 5, Dice and Jaro similarity didnt show any matching properties even for a threshold
value of 0.5.

We used three independent reviewers to evaluate the match-

ing of the properties and we chose the majority vote to determine the correct matches. The evaluators were given some
sample matches (2-3), and if the meaning of the properties were hard to understand by their name (which is the
case for most properties), they were provided with queries
to execute and explore details about them, like instances
they connect to and their domain and range. They were not
asked specifically to distinguish between an exact match and
a sub-property match in the alignment. For the experiment
numbered 1 in Table 1, we could not find all possible correct
matches because the total number of combinations is large
for a manual evaluation( 39k pairs). Therefore, we gave
evaluators all the mappings found by the algorithm without any threshold applied (extracted from property lists as
shown in Figure 3). Hence, recall and F measure are just
approximations and they are marked with an * in Table 1
for experiment 1. Among the above mentioned comparable
techniques, in every case, our bootstrapping based approach
showed higher recall and F measures.
4.3 Types of properties identified

Our algorithm identified different kinds of matching property pairs. It can potentially subsume approaches that use
techniques such as string similarity and synonym checking
as shown in Table 2 using extensional matching. The sample pairs listed in Table 2 are all correctly identified by our
algorithm. To explain example matching pairs in Table 2,
consider the following observations.

String Similarity Matches can be identified using string

12<0.5 is a negative confidence level since less than half of
subject-object pairs got matched from common subjects.

13http://wordnet.princeton.edu/
14http://www.sussex.ac.uk/Users/drh21/

                                               38Matching
Category
String
Similarity

Synonymous

Complex
Matches

Dataset 1

Dataset 2

db:nationality
db:religion
db:occupation
db:battles
db:screenplay
db:doctoralStudents

fb:nationality
fb:religion
fb:profession
fb:participated in conflicts
fb:written by
fb:advisees

Table 2: Sample of matching properties under different categories. namespaces: db for DBpedia and
fb for Freebase.

similarity measures such as Dices Coefficient or simple character comparison on property names. Synonymous Matches
are mappings that can be identified by analyzing synonyms
of the property names. For example, occupation and profession can be mapped using a dictionary, but interestingly the
WordNet approach failed to match this pair for the provided
threshold (showed very low similarity value of 0.5). In fact,
both pairs were missed by the WordNet approach. Complex Matches are the last category shown in Table 2 and are
harder to identify. All of the approaches outlined in the evaluation missed this mapping except our algorithm. This is
because our approach exploits property extensions in aligning properties. There are also false positives in our result set.
One such property pair is http://dbpedia.org/property/issue
and http://rdf.freebase.com/ns/people.person.children, which
happens to have similar extensions but has different inten-
tions.

5. DISCUSSION

The results show that our approach can effectively identify equivalent object properties by utilizing different statistical parameters presented above. It performs well even
when complex properties exist, like in experiments 1 and 5
in terms of F measure metric. Our algorithm also performs
well in terms of precision when datasets contain more literally similar property names as in experiments 2, 3, and 4,
and discovers more interesting matches in every case, showing higher recall, e.g., experiment 5 is about aligning properties in the same domain (publication) with high overlapping
information, but represents data using two completely different ontologies. These two ontologies do not have similar
word selection or synonyms in their schema exemplifying
complex data representation typically found on LOD. Because of this, string similarity based approaches do not perform well and synonym based approaches also show poor
coverage over terms. Therefore, novel techniques such as
ours for discovering equivalent properties on Linked Data
(i.e., Complex Matches in Table 2) are indispensable15.

We selected DBpedia, Freebase, LinkedMDB, and DBLP
for our evaluation because of the existence of many entity co-reference links between their entities, specifically,
DBpedia and Freebase provide diverse and complex property sets. Alignment between DBpedia and Freebase evaluates multi-domain property alignment whereas DBpedia
and LinkedMDB covers multi-domain and specific-domain
dataset alignment. The DBLP alignment evaluates our algorithm between specific-domain datasets. Therefore, our
evaluation covers property alignment between different types

15More details can be found at http://wiki.knoesis.org/
index.php/Property_Alignment

of datasets that can arise in the LOD domain. When analyzing results of matching frequency for property pairs, it was
observed that some matching properties do not appear frequently enough. Consequently, the algorithms confidence
in picking them as a match is low. This is mainly due to
the nature of properties, as discussed in Section 3. When a
random sample set is selected, it contains instances belonging to various types, e.g., a person type can have instances
belonging to athlete, artist, etc., which have rare proper-
ties. We can run the algorithm iteratively for more subject
instances that have these less frequent property pairs to improve precision. Our algorithm assumes that there are no
duplicate properties in both datasets being aligned. If both
datasets have duplicate properties, it requires additional inferencing mechanism to identify missing pairs, since only one
matching pair from each candidate list is selected.

We observed that certain properties that are mapped as
equivalent properties are actually sub-properties. This happens mainly because we do not distinguish between equivalent properties and sub-properties. For an example, http://
dbpedia.org/property/mother and http://rdf.freebase.com/ns
/people.person.parents are two such properties matched as
equivalent, while, in fact, the first is a sub-property of the
second. We would like to distinguish sub-properties in our
future experiments for more fine grained matches. Also,
links are useful in general, where many
we believe ECR
datasets are linked to central hubs like DBpedia and the algorithm will be able to discover more connections between
resources by finding link paths via these hubs.



We believe the uniqueness of our approach has resulted in
high quality results compared to existing approaches because
it, (1) does extension matching using entity co-reference
links, (2) bootstraps from candidate matches and aggregates
the results, and (3) makes final alignments using statistical
measures analyzing aggregated confidence of the matching
pairs. While, string similarity and synonymous matching
have been shown to be effective in the past (primarily for
literals), they do not have sufficient coverage for resources
as shown in our evaluation. We use owl:sameAs as an entity co-reference link but its use to link two semantically
equivalent instances seems to be controversial in the LOD
community [5][6]. In spite of well-known shortcomings, (like
equating London to Greater London), our algorithm is
expected to be sufficiently robust for property alignment,
because it is based on aggregating information from a large
number of entity equivalence assertions. In other words, we
believe that the effect of a few misused links will not affect
the final result much, because our algorithm does not decide on a property match by analyzing a single matching
triple. Even though the interlinking (entity co-reference)
relationships are small compared to the size of similar instances between datasets today, we expect that these links
will become prevalent as the datasets evolve and are maintained as part of the linked data life cycle (with projects
such as http://latc-project.eu/ and http://stack.lod2.eu/).
Moreover, we can see other recent successful efforts in using
owl:sameAs networks in the LOD context[11][17] in spite of
these known issues.

We also identified some property pairs which are matched
but have incorrect meaning in the property name. For exam-
ple, http://dbpedia.org/property/issue and http://rdf.freebase
.com/ns/people.person.children property pair between DBpedia and Freebase. This happened because of mixed values

                                               39present in the DBpedia property http://dbpedia.org/property
/issue, that has both integer values and names of children
as object values.
In this case, we can regard this kind of
a property as ambiguous and having noise, which has a
negative effect on the matching process. Another instance
where the algorithm can go wrong is when it encounters
special cases where the two datasets have enough facts for
the process to identify two properties as a match, but actually they are not. For an example, for film domain, it maps
http://dbpedia.org/ontology/distributor to http://rdf.freebase
.com/ns/film.film.production companies. This mainly happens because a number of production companies who produce also distribute films. Thus, they have multiple roles
and the extensional equality may choose to match one of
the many roles (extensions match but different intentions).

6. CONCLUSION AND FUTURE WORK

We have developed and evaluated an extension-based approach to match object-type properties on linked datasets.
We have defined a computable concept of statistical equivalence of properties to approximate owl:equivalentProperty
by leveraging entity co-reference links. Our algorithm is
unique and novel in how it computes extensional equivalence iteratively by building candidate matches in parallel.
This approach ultimately determines statistically equivalent
property pairs. The algorithm is easily parallelizable as evidenced by our straight forward Map-Reduce implementa-
tion. The empirical evaluation shows that our approach
is superior to the state of the art using F measure metric
(on average, F measure gain is in the range 57% - 78%).
This suggests that it is possible to align object-type properties on LOD datasets effectively. Our approach subsumes
conclusions arrived at by string-based and WordNet-based
similarity metrics and discovers more hidden and interesting matches well-suited for the LOD cloud. However, we
can use string-based or WordNet-based approaches to further improve confidence in our results to minimize coincidental matches. In fact, our data driven approach can also
be adapted to align data-type properties by using similarity metrics for object values in triples. Another beneficial
side effect of object-type property alignment is that it may
be used to generate or discover potential co-reference links
between instances. Furthermore, we expect to test the algorithm on different levels (strength) of entity co-reference
links. In future, we will extend the coverage to more types
of data and discover domains in which properties exist for
better alignment. The alignment problem can be further
refined to determine sub-properties in the future that will
help with fine grained data integration tasks.
Acknowledgements
This work was supported by the National Science Foundation under award 1143717 III: EAGER Expressive Scalable
Querying over Linked Open Data. Any opinions, findings,
and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect
the views of the National Science Foundation.
