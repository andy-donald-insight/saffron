User-driven Quality Evaluation of DBpedia

Amrapali Zaveri*

zaveri@informatik.uni-

leipzig.de

Lorenz Buhmann*

buehmann@informatik.uni-

leipzig.de

Dimitris Kontokostas*

kontokostas@informatik.uni-

leipzig.de

Mohamed Morsey*

morsey@informatik.uni-

leipzig.de

Mohamed A. Sherif*
sherif@informatik.uni-

leipzig.de
Soren Auer+

auer@cs.uni-bonn.de

Jens Lehmann*

lehmann@informatik.uni-

leipzig.de

* AKSW/BIS, Universitat Leipzig

PO Box 100920, 04009

Leipzig, Germany

+ CS/EIS, Universitat Bonn
Romerstra 164, 53117 Bonn

Bonn, Germany

ABSTRACT
Linked Open Data (LOD) comprises of an unprecedented
volume of structured datasets on the Web. However, these
datasets are of varying quality ranging from extensively curated datasets to crowdsourced and even extracted data of
relatively low quality. We present a methodology for assessing the quality of linked data resources, which comprises of
a manual and a semi-automatic process. The rst phase includes the detection of common quality problems and their
representation in a quality problem taxonomy. In the manual process, the second phase comprises of the evaluation
of a large number of individual resources, according to the
quality problem taxonomy via crowdsourcing. This process
is accompanied by a tool wherein a user assesses an individual resource and evaluates each fact for correctness. The
semi-automatic process involves the generation and verication of schema axioms. We report the results obtained by
applying this methodology to DBpedia. We identied 17
data quality problem types and 58 users assessed a total of
521 resources. Overall, 11.93% of the evaluated DBpedia
triples were identied to have some quality issues. Apply-

Permission to make digital or hard copies of all or part
of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for
prot or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for
components of this work owned by others than the author(s)
must be honored. Abstracting with credit is permitted. To
copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specic permission and/or a
fee. Request permissions from Permissions@acm.org. ISEM
13, September 04 - 06 2013, Graz, Austria Copyright is
held by the owner/author(s). Publication rights licensed
to ACM. ACM 978-1-4503-1972-0/13/09$15.00. http://dx.
doi.org/10.1145/2506182.2506192

ing the semi-automatic component yielded a total of 222,982
triples that have a high probability to be incorrect. In par-
ticular, we found that problems such as object values being
incorrectly extracted, irrelevant extraction of information
and broken links were the most recurring quality problems.
With this study, we not only aim to assess the quality of
this sample of DBpedia resources but also adopt an agile
methodology to improve the quality in future versions by
regularly providing feedback to the DBpedia maintainers.

Keywords
Evaluation, DBpedia, Data Quality, RDF, Extraction

INTRODUCTION

1.
The advent of semantic web technologies, as an enabler of
Linked Open Data (LOD), has provided the world with an
unprecedented volume of structured data currently amounting to 50 billion facts represented as RDF triples. Although
publishing large amounts of data on the Web is certainly a
step in the right direction, the data is only as usable as its
quality. On the Data Web, we have varying quality of information covering various domains. There are a large number
of high quality datasets (in particular in the life-sciences
domain), which are carefully curated over decades and recently published on the Web. There are, however, also many
datasets, which were extracted from unstructured and semistructured information or are the result of some crowdsourcing process, where large numbers of users contribute small
parts. DBpedia [1, 17] is actually an example for both - a
dataset extracted from the result of a crowdsourcing process.
Hence, quality problems are inherent in DBpedia. This is
not a problem per se, since quality usually means tness for
a certain use case [12]. Hence, even datasets with quality
problems might be useful for certain applications, as long as
the quality is in the required range.

In the case of DBpedia, for example, the data quality is
perfectly sucient for enriching Web search with facts or
suggestions about common sense information, such as entertainment topics. In such a scenario, where the DBpedia
background knowledge can be, for example, used to show the
movies Angelina Jolie was starring in and actors she played
with it is rather neglectable if, in relatively few cases, a

                                               97movie or an actor is missing. For developing a medical ap-
plication, on the other hand, the quality of DBpedia is probably completely insucient. Please note, that also on the
traditional document-oriented Web we have varying quality of the information and still the Web is perceived to be
extremely useful by most people. Consequently, a key challenge is to determine the quality of datasets published on
the Web and make this quality information explicit. Other
than on the document Web where information quality can be
only indirectly, (e.g. via page rank, or vaguely) dened, we
can have much more concrete and measurable data quality
indicators for structured information, such as correctness of
facts, adequacy of semantic representation or degree of cov-
erage.

In this paper, we devise a data quality assessment method-
ology, which comprises of a manual and a semi-automatic
process. We empirically assess, based on this methodology,
the data quality of one of the major knowledge hubs on the
Data Web { DBpedia. The rst phase includes the detection
of common quality problems and their representation in a
comprehensive taxonomy of potential quality problems. In
the manual process, the second phase comprises of the evaluation of a large number of individual resources, according
to the quality problem taxonomy, using crowdsourcing in
order to evaluate the type and extent of data quality problems occurring in DBpedia. Here we would like to clarify
the use of crowdsource used in this paper. Crowdsourcing involves the creating if HITs (Human Intelligent Tasks),
submitting them to a crowdsourcing platform (e.g. Amazon Mechanical Turk1) and providing a (nancial) reward
for each HIT [11]. However, we use the broader-sense of the
word as a large-scale problem-solving approach by which a
problem is divided into several smaller tasks (assessing the
quality of each triple, in this case) that can be independently solved by a large group of people. Each represented
fact is evaluated for correctness by each user and, if found
problematic, annotated with one of 17 pre-dened quality
criteria. This process is accompanied by a tool wherein a
user assesses an individual resource and evaluates each fact
for correctness.

The semi-automatic process involves the generation and verication of schema axioms, which yielded a total of 222,982
triples that have a high probability to be incorrect. We
nd that while a substantial number of problems exists, the
overall quality is with a less than 11.93% error rate relatively
high. With this study we not only aim to assess the quality
of DBpedia but also to adopt a methodology to improve the
quality in future versions by regularly providing feedback to
the DBpedia maintainers to x these problems.

Our main contributions are:

assessment (Section 2),

 a crowdsourcing based methodology for data quality
 a comprehensive quality issue taxonomy comprising
common knowledge extraction problems (Section 3),
 a crowdsourcing based data quality assessment tool
 an empirical data quality analysis of the DBpedia dataset

(Section 4),

performed using crowdsourcing (Section 5) and

http://mturk.com

 a semi-automated evaluation of data quality problems

in DBpedia (Section 5).

We survey related work in Section 6 and conclude with an
outlook on future work in Section 7.

class

2. ASSESSMENT METHODOLOGY
In this section, we describe a generalized methodology for
the assessment and subsequent data quality improvement of
resources belonging to a dataset. The assessment methodology we propose is depicted in Figure 1. This methodology
consists of the following four steps: 1. Resource selection,
2. Evaluation mode selection, 3. Resource evaluation and
4. Data quality improvement. In the following, we describe
these steps in more detail.
Step I: Resource selection. In this rst step, the resources
belonging to a particular dataset are selected. This selection
can be performed in three dierent ways:
 Per Class: select resources belonging to a particular
 Completely random: a random resource from the dataset
 Manual: a resource selected manually from the dataset
Choosing resources per class (e.g. animal, sport, place etc.)
gives the user the exibility to choose resources belonging to
only those classes she is familiar with. However, when choosing resources from a class, the selection should be made in
proportion to the number of instances of that class. Random selection, on the other hand, ensures an unbiased and
uniform coverage of the underlying dataset.
In the manual selection option, the user is free to select resources with
problems that she has perhaps previously identied.
Step II: Evaluation mode selection. The assignment of
the resources to a person or machine, selected in Step I, can
be accomplished in the following three ways:

 Manual: the selected resources are assigned to a person (or group of individuals) who will then proceed to
manually evaluate the resources individually.

 Semi-automatic: selected resources are assigned to a
semi-automatic tool which performs data quality assessment employing some form of user feedback.

 Automatic: the selected resources are given as input
to an automatic tool which performs the quality assessment without any user involvement.

For the semi-automatic evaluation, machine learning can be
applied as shown in [4] and provided by the DL-Learner
framework [16, 19], where the workow can be as follows:
(1) based on the instance data, generate OWL axioms which
can also be seen as restrictions2, e.g.
learn characteristics
(irreexivity, (inverse) functionality, asymmetry) of properties as well as denitions and disjointness of classes in the
knowledge base; (2) ask queries via SPARQL or a reasoner
for violations of theses restrictions, e.g.
in case of an irreexive property, triples where subject and object are the
same would indeed violate the characteristic of the irreex-
ivity. In the automatic case, a possible approach is to check
for inconsistencies and other modelling problems as, e.g.,
described in [18].
2A local Unique Name Assumption is used therefore, i.e.
every named individual is assumed to be dierent from every
other, unless stated explicitely otherwise

                                               98along with their categories and sub-categories. We indicate
whether the problems are automatically detectable (column
D) and xable (column F). The ones marked with a 4in column D refer to those categories that can be automatically
identied such as invalid datatypes ("1981-01-01T00:00:00
+02:00"^^xsd:gYear), irrelevant properties (dbpprop:image
Caption) or dead links. The column F refers to those categories that can be automatically amended, like xing an
invalid datatype ("1981"^^xsd:gYear) or removing triples
with irrelevant properties and dead links.
If the problem
is xable, we determined whether the problem can be xed
by amending the (i) extraction framework (E), (ii) the mappings wiki (M) or (iii) Wikipedia itself (W). Moreover, the
table species whether the problems are specic to DBpedia
(marked with a 4) or could potentially occur in any RDF
dataset. For example, the sub-category Special template not
properly recognized is a problem that occurs only in DBpedia
due to the presence of specic keywords in Wikipedia articles
that do not cite any references or resources (e.g. ffUnreferenced stubjauto=yesgg). On the other hand, the problems that are not DBpedia specic can occur in any other
datasets. In the following, we provide the quality problem
taxonomy and discuss each of the dimensions along with its
categories and sub-categories in detail by providing exam-
ples.

Accuracy.
Accuracy is dened as the extent to which data is correct,
that is, the degree to which it correctly represents the real
world facts and is also free of error [23]. We further classify
this dimension into the categories (i) object incorrectly ex-
tracted, (ii) datatype problems and (iii) implicit relationship
between attributes.
Object incorrectly extracted. This category refers to those
problems which arise when the object value of a triple is
awed. This may occur when the value is either (i) incorrectly extracted, (ii) incompletely extracted or (iii) the
special template in Wikipedia is not recognized:
 Object value is incorrectly extracted, e.g.:

dbpprop:map

dbpedia:Oregon_Route_238
"238.0"^^http://dbpedia.org/datatype/second.
This resource about state highway Oregon Route 238
has the incorrect property map with value 238.
In
Wikipedia the attribute map refers to the image name
as the value: map=Oregon Route 238.svg. The DBpedia property only extracted the value 238 from his
attribute value and gave it the datatype second assuming it is a time value, which is incorrect.
 Object value is incompletely extracted, e.g.:

dbpprop:dateOfBirth

dbpedia:Dave_Dobbyn
"3"^^xsd:integer. In this example, only the day of
birth of a person is extracted and mapped to the dateofBirth property when it should have been the entire
date i.e. day, month and year. Thus, the object value
is not completely extracted.

 Special template not properly recognized, e.g.:

"yes"@en.

dbpprop:auto

dbpedia:328_Gudrun
Certain article classications in Wikipedia (such as
\This article does not cite any references or sources.")
are performed via special templates (e.g. ffUnreferenced stubjauto=yesgg). Such templates should be
listed on a black-list and omitted by the DBpedia extraction in order to prevent non-meaningful triples.

Figure 1: Workow of the data quality assessment
methodology.

Step III: Resource evaluation. In case of manual assignment of resources, the person (or group of individuals) evaluates each resource individually to detect the potential data
quality problems. In order to support this step, a quality
assessment tool can be used which allows a user to evaluate
each individual triple belonging to a particular resource. If,
in case of Step II, the selected resources are assigned to a
semi-automatic tool, the tool points to triples likely to be
wrong. For example, domain or range problems are identied by the tool and then assigned to a person to verify the
correctness of the results.
Step IV: Data quality improvement. After the evaluation
of resources and identication of potential quality problems,
the next step is to improve the data quality. There are at
least two ways to perform an improvement:

 Direct: editing the triple,

identied to contain the

problem, with the correct value

 Indirect: using the Patch Request Ontology3 [13] which
allows gathering user feedbacks about erroneous triples.

3. QUALITY PROBLEM TAXONOMY
A systematic review done in [23] identied a number of different data quality dimensions (criteria) applicable to Linked
Data. After carrying out an initial data quality assessment
on DBpedia (as part of the rst phase of the manual assessment methodology cf. Section 5.1), the problems identied
were mapped to this list of identied dimensions. In particu-
lar, Accuracy, Relevancy, Representational-consistency and
Interlinking were identied to be problems aecting a large
number of DBpedia resources. Additionally, these dimensions were further divided into categories and sub-categories.
Table 1 gives an overview of these data quality dimensions

http://141.89.225.43/patchr/ontologies/patchr.

ttl#

Resource Selection[Per Class][Manual][Random]ResourceEvaluation mode selectionResource Evaluation[Manual]Triples[Semi-automatic][Automatic]List of invalid factsData QualityImprovementPre-selection of triplesPatch Ontology                                               99Datatype problems. This category refers to those triples
which are extracted with an incorrect datatype for a typed

literal. Datatype incorrectly extracted, e.g.:

dbpedia-owl:activeYears-

dbpedia:Stephen_Fry
StartYear "1981-01-01T00:00:00+02:00"^^xsd:gYear.
In this case, the DBpedia ontology datatype property
activeYearsStartYear has xsd:gYear as range. Although the datatype declaration is correct, it is formatted as xsd:dateTime. The expected value is "1981"^^
xsd:gYear.

Implicit relationship between attributes. This category
of problems may arise due to (i) representation of one fact in
several attributes, (ii) several facts encoded in one attribute
or (iii) an attribute value computed from another attribute
value in Wikipedia.

 One fact is encoded in several attributes, e.g.:

dbpprop:postalCodeType

dbpedia:Barlinek
"Postal code"@en. In this example, the value of the
postal code of the town of Barlinek is encoded in two
attributes postal code type = Postal code and postalcode = 74-320. DBpedia extracts both these attributes
separately instead of combining them together to produce one triple, such as: dbpedia:Barlinek
dbpprop:postalCode

 Several facts are encoded in one attribute, e.g.:

"74-320"@en.

dbpedia-owl:synonym

dbpedia:Picathartes
"Galgulus Wagler, 1827 (non Brisson, 1760:
preoccupied)"@en. In this example, even though the
triple is not incorrect, it contains two pieces of infor-
mation. Only the rst word is the synonym, the rest of
the value is a reference to that synonym. In Wikipedia,
this fact is represented as \\synonyms = \Galgulus"
small Wagler, 1827 (\non" [[Mathurin Jacques Bris-
sonjBrisson]], 1760: [[Coraciasjpreoccupied]])/=small"".
The DBpedia framework should ideally recognize this
 Attribute value computed from another attribute value,
and separate these facts into several triples.

e.g.:
dbpedia:Barlinek
dbpprop:populationDensityKm
"auto"@en. In Wikipedia, this attribute is represented
as \population density km2 = auto". The word \auto"
is an indication in Wikipedia that the value associated
to that attribute should be computed \automatically".
In this case, the population density is computed automatically by dividing the population by area.

Relevancy.
Relevancy refers to the provision of information which is
in accordance with the task at hand and important to the
users query [23]. The only category Irrelevant information
extracted of this dimension can be further sub-divided into
the following sub-categories: (i) extraction of attributes containing layout information, (ii) image related information,
(iii) redundant attribute values and (iv) other irrelevant in-
formation.
 Extraction of attributes containing layout information,

e.g.:
dbpedia:Lalsyri dbpprop:pushpinLabelPosition
"bottom"@en. Information related to layout of a page
in Wikipedia, such as the position of the label on a
pushpin map relative to the pushpin coordinate marker,
in this example specied as "bottom", is irrelevant when
extracted in DBpedia.

 Image related information, e.g.:

dbpedia:Three-banded_Plover dbpprop:imageCaption
"At Masai Mara National Reserve, Kenya"@en. Extraction of an image caption or name of the image is
irrelevant in DBpedia as the image is not displayed for
any DBpedia resource.

 Redundant attributes value, e.g.:

The resource dbpedia:Niedersimmental_ District contains the redundant properties
dbpedia-owl:thumbnail, foaf:depiction,
dbpprop:imageMap with the same value "Karte Bezirk
Niedersimmental 2007.png" as the object.

 Other irrelevant information, e.g.:

dbpedia:IBM_Personal_Computer
dbpedia:Template:Infobox_information_appliance
"type"@en. Information regarding a templates infobox
information, in this case, with an object value as \type"
is completely irrelevant.

Representational-consistency.
Representational-consistency is dened as the degree to which
the format and structure of information conforms to previously returned information and other datasets. [23] and has
the following category:

 Representation of number values, e.g.:

dbpprop:seating

dbpedia:Drei_Flsse_Stadion
Capacity "20"^^xsd:integer. In Wikipedia, the seating capacity for this stadium has the value \20.000",
but in DBpedia the value displayed is only 20. This
is because the value is inconsistently represented with
a dot after the rst two decimal places instead of a
comma.
Interlinking.
Interlinking is dened as the degree to which entities that represent the same concept are linked to each other [23]. This
type of problem is recorded when links to external websites
or external data sources are either incorrect, do not show
any information or are expired. We further classify this dimension into the following categories:
 External websites: Wikipedia usually contains links to
external web pages such as, for example, the home
page of a company or a music band. It may happen
that these links are either incorrect, do not work or are
unavailable.

 Interlinks with other datasets: Linked Data mandates
interlinks between datasets. These links can either be
incorrectly mapped or may not contain useful infor-
mation. These problems are recorded in the following sub-categories: 1. links to Wikimedia, 2. links to
Freebase, 3. links to Geospecies, 4. links generated via
Flickr wrapper.

4. A CROWDSOURCING QUALITY ASSESS-

MENT TOOL

In order to assist several users in assessing the quality of a
resource, we developed the TripleCheckMate tool4 aligned
with the methodology described in Section 2, in particular
with Steps 1 { 3. To use the tool, the user is required to
authenticate herself, which not only prevents spam but also
4available at http://github.com/AKSW/TripleCheckMate

                                               100Dimension

Category

Sub-category

Accuracy

Relevancy

Triple
incorrectly
extracted
Datatype problems
Implicit
relationship
between
attributes

Irrelevant
information
extracted

Represensati-
onal-Consistency

Representation of number
values
External links

Interlinking

Interlinks with
other datasets

Object value is incompletely extracted
Object value is incompletely extracted
Special template not properly recognised
Datatype incorrectly extracted
One fact encoded in several attributes
Several facts encoded in one attribute
Attribute value computed from another attribute value

Extraction of attributes containing layout information
Redundant attribute values
Image related information
Other irrelevant information
Inconsistency in representation of number values

External websites
Links to Wikimedia
Links to Freebase
Links to Geospecies
Links generated via Flickr wrapper


E +

DBpedia
specic


Table 1: Data quality dimensions, categories and sub-categories identied in the DBpedia resources. Detectable (column D) means problem detection can be automised. Fixable (column F) means the issue is
solvable by amending either the extraction framework (E), the mappings wiki (M) or Wikipedia (W). The
last column marks the dataset specic subcategories.

helps in keeping track of her evaluations. After authenticating herself, she proceeds with the selection of a resource
(Step 1). She is provided with three options: (i)per class,
(ii)completely random and (iii)manual (as described in Step
I of the assessment methodology).

After selecting a resource, the user is presented with a table
showing each triple belonging to that resource on a single
row. Step 2 involves the user evaluating each triple and
checking whether it contains a data quality problem. The
link to the original Wikipedia page for the chosen resource
is provided on top of the page which facilitates the user
to check against the original values. If the triple contains
a problem, she checks the box is wrong. Moreover, she is
provided with a taxonomy of pre-dened data quality problems where she assigns each incorrect triple to a problem.
If the detected problem does not match any of the existing
types, she has the option to provide a new type and extend
the taxonomy. After evaluating one resource, the user saves
the evaluation and proceeds to choosing another random resource and follow the same procedure.

Another important feature of the tool is to allow measuring of inter-rater agreements. That is, when a user selects a
random method (Any or Class) to choose a resource, there
is a 50% probability that she is presented with a resource
that was already evaluated by another user. This probability
as well as the number of evaluations per resource is cong-
urable. Allowing many users evaluating a single resource not
only helps to determine whether incorrect triples are recognized correctly but also to determine incorrect evaluations
(e.g.
incorrect classication of problem type or marking
correct triples as incorrect), especially when crowdsourcing
the quality assessment of resources. One important feature
of the tool is that although it was built for DBpedia, it is
parametrizable to accept any endpoint and, with very few
adjustments in the database back-end (i.e. ontology classes
and problem types) one could use it for any Linked Data
dataset (open or closed).

5. EVALUATION OF DBPEDIA DATA QUAL-

5.1 Evaluation Methodology
Manual Methodology
We performed the assessment of the quality of DBpedia in
two phases: Phase I: Problem detection and creation of taxonomy and Phase II: Evaluation via crowdsourcing.

Phase I: Creation of quality problem taxonomy. In the rst
phase, two researchers independently assessed the quality of
20 DBpedia resources each. During this phase an initial list
of data quality problems, that occurred in each resource,
was identied. These identied problems were mapped to
the dierent quality dimensions from [23]. After analyzing
the root cause of these problems, a renement of the quality
dimensions was done to obtain a ner classication of the
dimensions. This classication of the dimensions into subcategories resulted in a total of 17 types of data quality
problems (cf. Table 1) as described in Section 3.

Phase II: Crowdsourcing quality assessment. In the second
phase, we crowdsourced the quality evaluation wherein we
invited researchers who are familiar with RDF to use the
TripleCheckMate tool (described in Section 4). First, each
user after authenticating oneself, chooses a resource by one
of three options mentioned in Section 2. Thereafter, the extracted facts about that resource are shown to the user. The
user then looks at each individual fact and records whether
it contains a data quality problem and maps it to the type
of quality problem.

Semi-automatic Methodology
We applied the semi-automatic method (cf. Section 2),
which consists of two steps: (1) the generation of a particular set of schema axioms for all properties in DBpedia
and (2) the manual verication of the axioms.

Step I: Automatic creation of an extended schema. In this
step, the enrichment functionality of DL-Learner [4] for

                                               101SPARQL endpoints was applied. Thereby for all properties
in DBpedia, axioms expressing the (inverse) functional, irreexive and asymmetric characteristic were generated, with
a minimum condence value of 0.95. For example, for the
property dbpedia-owl:firstWin, which is a relation between
Formula One racers and grand prix, axioms for all four mentioned types were generated: Each Formula One racer has
only one rst win in his career (functional), each grand prix
can only be won by one Formula One racer (inverse func-
tional). It is not possible to use the property
dbpedia-owl:firstWin in both directions (asymmetric), and
the property is also irreexive.

Step II: Manual evaluation of the generated axioms. In the
second step, we used at most 100 random axioms per axiom type and manually veried whether this axiom is ap-
propriate. To focus on possible data quality problems, we
restricted the evaluation data to axioms where at least one
violation can be found in the knowledge base. Furthermore,
we tried to facilitate the evaluation by taking also the target
context into account, i.e. if it exists we consider the deni-
tion, domain and range as well as one random sample for a
violation. When evaluating the inverse functionality for the
property dbpedia-owl:firstWin, we can therefore make use
of the following additional information:

Domain : dbpedia - owl : F o r m u l a O n e R a c e r Range : dbpedia -

owl : G r a n d P r i x
Sample V i o l a t i o n :

dbpedia : F e r n a n d o _ A l o n s o dbpedia - owl : f i r s t W i n dbpedia

:2003 _ H u n g a r i a n _ G r a n d _ P r i x .

dbpedia : W i k i P r o j e c t _ F o r m u l a _ O n e dbpedia - owl : f i r s t W i n

dbpedia :2003 _ H u n g a r i a n _ G r a n d _ P r i x .

5.2 Evaluation Results
Manual Methodology.
An overview of the evaluation results is shown in Table 25.
Overall, only 16.5% of all resources were not aected by
any problems. On average, there were 5.69 problems per
resource and 2.24 problems excluding errors in the dbprop
namespace 6 [17]. While the vast majority of resources have
problems,
it should also be remarked that each resource
has 47.19 triples on average, which is higher than in most
other LOD datasets. The tool was congured to allow two
evaluations per resource and this resulted to a total of 268
inter-evaluations. We computed the inter-rater agreement
for those resources, which were evaluated by two persons by
adjusting the observed agreement with agreement by chance
as done in Cohens kappa7. The inter-rater agreement results { 0.34 for resource agreement and 0.38 for triple agreement { indicate that the same resource should be evaluated
more than twice in future evaluations. To assess the accuracy of the crowdsourcing evaluation, we took a random
sample of 700 assessed triples (out of the total 2928) and
evaluated them for correctness based on the formula in [15]
intended to be a representative of all the assessed triples.
Additionally, we assumed a margin of 3.5% of error, which
is a bound that we can place on the dierence between the
estimated correctness of the triples and the true value, and a
95% condence level, which is the measure of how condent
5Also available at: http://aksw.org/Projects/DBpediaDQ

http://dbpedia.org/property/
http://en.wikipedia.org/wiki/Cohen%27s_kappa

Total no. of users
Total no. of distinct resources evaluated
Total no. of resources evaluated
Total no. of distinct resources without problems
Total no. of distinct resources with problems
Total no. of distinct incorrect triples
Total no. of distinct incorrect triples in the dbprop
namespace
Total no. of inter-evaluations
No. of resources with evaluators having dierent
opinions
Resource-based inter-rater agreement (Cohens
Kappa)
Triple-based
Kappa)
No. of triples evaluated for correctness
No. of triples evaluated to be correct
No. of triples evaluated incorrectly
% of triples correctly evaluated
Average no. of problems per resource
Average no. of problems per resource in the dbprop
namespace
Average no. of triples per resource
% of triples aected
% of triples aected in the dbprop namespace

inter-rater

agreement

(Cohens


Table 2: Overview of the manual quality evaluation.

we are in that margin of error8. From these 700 triples, 133
were evaluated incorrectly resulting in about 81% of triples
correctly evaluated.

Table 3 shows the total number of problems, the distinct resources and the percentage of aected triples for each problem type. Overall, the most prevalent problems, such as
broken external links are outside the control of the DBpedia extraction framework. After that, several extraction and
mapping problems that occur frequently mainly aecting ac-
curacy, can be improved by manually adding mappings or
possibly by improving the extraction framework.

When looking at the detectable and xable problems from
Table 1, in light of their prevalence, we expect that approximately one third of the problems can be automatically detected and two thirds are xable by improving the DBpedia
extraction framework. In particular, implicitly related attributes can be properly extracted with a new extractor,
which can be congured using the DBpedia Mappings Wiki.
As a result, we expect that the improvement potential is that
the problem rate in DBpedia can be reduced from 11.93%
to 5.81% (calculated by subtracting 7.11% from 11.93% reported in Table 2). After revising the DBpedia extraction
framework, we will perform subsequent quality assessments
using the same methodology in order to realize and demonstrate these improvements.

Semi-automatic Methodology.
The evaluation results in Table 4 show that for the irreexive case all 24 properties that would lead to at least one
violation should indeed be declared as irreexive. Applying the irreexive characteristic would therefore help to nd
overall 236 critical triples, for e.g. dbpedia:2012_Coppa_
Italia_Final
dbpedia:2012_Coppa_Italia_Final, which is not meaningful as no event is the following event of itself. For asymme-

dbpedia-owl:followingEvent

http://research-advisors.com/tools/SampleSize.htm

                                               102Criteria
Accuracy
Object incorrectly extracted
Object value is incorrectly extracted
Object value is incompletely extracted
Special template not recognized
Datatype problems
Datatype incorrectly extracted
Implicit relationship between attributes
One fact is encoded in several attributes
Several facts encoded in one attribute
Value computed from another value
Accuracy unassigned
Relevancy

Irrelevant information extracted
Extraction of layout information
Redundant attributes value
Image related information
Other irrelevant information
Relevancy unassigned

Representational-consistency

Representation of number values
Representational-consistency unassigned

Interlinking
External websites (URLs)
Interlinks with other datasets (URIs)
Links to Wikimedia
Links to Freebase
Links to Geospecies
Links generated via Flickr wrapper
Interlinking unassigned


AT %


Table 3: Detected number of problem for each of
the dened quality problems. IT = Incorrect triples,
DR = Distinct resources, AT = Aected triples.

try, we got 81 approved properties, for example, containing
dbpedia-owl:starring with domain Work and range Actor.
Compared with this, there are also some properties where
asymmetry is not always appropriate, e.g.
dbpedia-owl:influenced.

Functionality, i.e. having at most one value of a property,
can be applied to 76 properties. During the evaluation, we
observed invalid facts such as, for example, two dierent values 2600.0 and 1630.0 for the density of the moon Himalia.
We spotted overall 199,480 errors of this type in the knowledge base. As the result of the inverse functionality eval-
uation, we obtained 13 properties where the object in the
triple should only be related to one unique subject, e.g. there
should only be one Formula One racer which won a particular grand prix, which is implicit when using the property
dbpedia-owl:lastWin.

6. RELATED WORK
Web data quality assessment frameworks. There are a number of data quality assessment dimensions that have already
been identied relevant to Linked Data, namely, accuracy,
timeliness, completeness, relevancy, conciseness, consistency,
to name a few [2]. Additional quality criteria such as unifor-
mity, versatility, comprehensibility, amount of data, validity,
licensing, accessibility and performance were also introduced
to be additional means of assessing the quality of LOD [7].
Additionally, there are several eorts in developing data
quality assessment frameworks in order to assess the data
quality of LOD. These eorts are either semi-automated [7],
automated [8] or manual [3, 20].

Even though these frameworks introduce useful methodologies to assess the quality of a dataset, either the results are
dicult to interpret, do not allow a user to choose the input
dataset or require a considerable amount of user involve-
ment. In our experiment, we used crowdsourcing to perform
the evaluation because (1) none of the frameworks provided
the granularity of quality criteria that we identied to be
quality problems in DBpedia resources and (2) we were interested in whether it was possible to use crowdsourcing to
assess and thus improve the quality of a dataset.

Concrete Web Data quality assessments. An eort to assess
the quality of web data was undertaken in 2008 [5], where
14.1 billion HTML tables from Googles general-purpose web
crawl were analyzed in order to retrieve those tables that
have high-quality relations. Additionally, there have been
studies focused on assessing the quality of RDF data [9] to
report the errors occurring while publishing RDF data and
the eects and means to improve the quality of structured
data on the web. As part of an empirical study [10] 4 million RDF/XML documents were analyzed, which provided
insights into the level of conformance in these documents
with respect to the Linked Data guidelines. Even though
these studies accessed a vast amount of web or RDF/XML
data, most of the analysis was performed automatically and
therefore the problems arising due to contextual discrepancies were overlooked. Another study aimed to develop a
framework for the DBpedia quality assessment [14]. In this
study, particular problems of the DBpedia extraction framework were taken into account and integrated in the frame-
work. However, only a small sample (75 resources) were
assessed in this case and an older DBpedia version (2010)
was analyzed.

Crowdsourcing-based tasks. There are already a number of
eorts which use crowdsourcing focused on a specic type
of task. For example, crowdsourcing is used for entity linking or resolution [6], quality assurance and resource mangement [22] or for enhancement of ontology alignments [21]
especially in Linked Data. However, in our case, we did
not submit tasks to the popular internet marketplaces such
as Amazon Mechanical Turk or CrowdFlower9. Instead, we
used the intelligence of a large number of researchers who
were particularly conversant with RDF to help assess the
quality of one of the important and most linked dataset,
DBpedia.

7. CONCLUSION AND OUTLOOK
To the best of our knowledge, this study is the rst comprehensive empirical quality analysis for more than 500 resources of a large Linked Data dataset extracted from crowdsourced content. We found that a substantial number of
problems exist and the overall quality, with a 11.93% error
rate, is moderate. Moreover, the semi-automatic analysis
revealed more than 200,000 violations of property charac-
teristics.
In addition to the quality analysis of DBpedia,
we devised a generic methodology for Linked Data quality
analysis, derived a comprehensive taxonomy of extraction
quality problems and developed a tool which can assist in
the evaluation. All these contributions can be reused for
analyzing any other extracted dataset (by domain experts).

http://crowdflower.com/

                                               103Characteristic

#Properties

Total Violated

Irreexivity
Asymmetry
Functionality
Inverse Functionality


Correct


#Violations

Min.

Max.

Avg.

Total


199480

Table 4: Results of the semi-automatic evaluation. The table shows the total number of properties that have
been suggested to have the given characteristic by Step I of the semi-automatic methodology, the number of
properties that would lead to at least one violation when applying the characteristic, the number of properties
where the characteristic is meaningful (manually evaluated) and some metrics for the number of violations.

The detailed analysis of data quality problems allows us to
devise and implement corresponding mitigation strategies.
Many of the problems found can be rstly automatically
detected and secondly avoided by (1) improving existing ex-
tractors, (2) developing new ones (e.g. for implicitly related
attributes) or (3) improving and extending mappings and
extraction hints on the DBpedia Mappings Wiki.

With this study, we not only aim to assess the quality of
this sample of DBpedia resources but also adopt an agile
methodology to improve the quality in future versions by
regularly providing feedback to the DBpedia maintainers to
x these problems. We plan to improve the DBpedia extraction framework along these detected problems and periodically revisit the quality analysis (in regular intervals) in
order to demonstrate possible improvements.
