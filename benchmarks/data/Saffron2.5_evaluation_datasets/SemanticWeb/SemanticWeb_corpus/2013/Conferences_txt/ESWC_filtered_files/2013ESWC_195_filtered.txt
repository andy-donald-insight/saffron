Logical Linked Data Compression

Amit Krishna Joshi, Pascal Hitzler, and Guozhu Dong

Kno.e.sis Center, Wright State University, Dayton, OH, U.S.A.

{joshi35,pascal.hitzler,guozhu.dong}@wright.edu

Abstract. Linked data has experienced accelerated growth in recent
years. With the continuing proliferation of structured data, demand for
RDF compression is becoming increasingly important. In this study, we
introduce a novel lossless compression technique for RDF datasets, called
Rule Based Compression (RB Compression) that compresses datasets
by generating a set of new logical rules from the dataset and removing
triples that can be inferred from these rules. Unlike other compression
techniques, our approach not only takes advantage of syntactic verbosity
and data redundancy but also utilizes semantic associations present in
the RDF graph. Depending on the nature of the dataset, our system is
able to prune more than 50% of the original triples without affecting
data integrity.

Introduction

Linked Data has received much attention in recent years due to its interlinking
ability across disparate sources, made possible via machine processable nonproprietary RDF data [18]. Today, large number of organizations, including gov-
ernments, share data in RDF format for easy re-use and integration of data by
multiple applications. This has led to accelerated growth in the amount of RDF
data being published on the web. Although the growth of RDF data can be
viewed as a positive sign for semantic web initiatives, it also causes performance
bottlenecks for RDF data management systems that store and provide access
to data [12]. As such, the need for compressing structured data is becoming
increasingly important.

Earlier RDF compression studies [3,6] have focused on generating a compact
representation of RDF. [6] introduced a new compact format called HDT which
takes advantage of the powerlaw distribution in term-frequencies, schema and resources in RDF datasets. The compression is achieved due to a compact form representation rather than a reduction in the number of triples. [13] introduced the
notion of a lean graph which is obtained by eliminating triples which contain blank
nodes that specify redundant information. [19] proposed a user-specific redundancy
elimination technique based on rules. Similarly, [21] studied RDF graph minimization based on rules, constraints and queries provided by users. The latter two approaches are application dependent and require human input, which makes them
unsuitable for compressing the ever growing set of linked datasets.

In this paper, we introduce a scalable lossless compression of RDF datasets
using automatic generation of decompression rules. We have devised an algorithm

P. Cimiano et al. (Eds.): ESWC 2013, LNCS 7882, pp. 170184, 2013.
c Springer-Verlag Berlin Heidelberg 2013
?

?

?
to automatically generate a set of rules and split the database into two smaller
disjoint datasets, viz., an Active dataset and a Dormant dataset based on those
rules. The dormant dataset contains list of triples which remain uncompressed
and to which no rule can be applied during decompression. On the other hand,
the active dataset contains list of compressed triples, to which rules are applied
for inferring new triples during decompression.

In order to automatically generate a set of rules for compression, we employ frequent pattern mining techniques [9,15]. We examine two possibilities
for frequent mining - a) within each property (hence, intra-property) and b)
among multiple properties (inter-property). Experiments reveal that RB compression performs better when inter-property transactions are used instead of
intra-property transactions. Specifically, the contribution of this work is a rulebased compression technique with the following properties:
 The compression reduces the number of triples, without introducing any new
 The set of decompression rules, R, can be automatically generated using
 The compression is lossless.
A very preliminary and limited version of this paper appeared in [14].

subjects, properties or objects.

various algorithms.

2 Preliminaries

2.1 Frequent Itemset Mining

The concept of frequent itemset mining [1] (FIM) was first introduced for mining transaction databases. Over the years, frequent itemset mining has played an
important role in many data mining tasks that aim to find interesting patterns
from databases, including association rules and correlations, or aim to use frequent itemsets to construct classifiers and clusters [7]. In this study, we exploit
frequent itemset mining techniques on RDF datasets for generating logical rules
and subsequent compressing of RDF datasets.

Transaction Database. Let I = {i1, i2, . . . , in} be a set of distinct items. A
set X = {i1, i2, . . . , ik}  I is called an itemset, or a k-itemset if it contains

k items. Let D be a set of transactions where each transaction, T = (tid, X),
contains a unique transaction identifier, tid, and an itemset X. Figure 1 shows
a list of transactions corresponding to a list of triples containing the rdf:type1
property. Here, subjects represent identifiers and the set of corresponding objects
represent transactions. In this study, we use the following definitions for intraand inter-property transactions.

Intra-property transactions. For a graph G containing a set of triples, an intraproperty transaction corresponding to a property p is a set T = (s, X) such that
s is a subject and X is a set of objects, i.e. (s, p, ox) is a triple in graph G; ox is
a member of X.
1 rdf:type is represented by a.

A.K. Joshi, P. Hitzler, and G. Dong

Inter-property transactions. For a graph G containing a set of triples, an interproperty transaction is a set T = (s, Z) such that s is a subject and each member
of Z is a pair (pz, oz) of property and object, i.e. (s, pz, oz) is a triple in graph G.

s4 a 22.

s1 a 125 s4 a 125.
s1 a 22.
s1 a 225. s4 a 225.
s4 a 60.
s1 a 60.
s6 a 90.
s6 a 22.
s5 a 125. s5 a 22.
s2 a 225. s2 a 125.
s2 a 22.
s3 a 22.

s3 a 81.

TID rdf:type
S1
S2
S3
S4
S5
S6

125,22,225,60
125,22,225
81,22
125,22,225,60
125,22
90,22

(a) Triples

(b) Transactions

Fig. 1. Triples and corresponding transactions

Item (k) Frequent Patterns (Fk)
{([22, 225], 525786)}

{([22, 225, 60], 525786)}

{([22, 227, 83, 189], 60194)}

{([22, 227, 83, 189, 213], 60194)}

{([22, 103, 26, 304, 173], 57772)}

{([22, 70], 56372),

([22, 103, 26, 304, 173, 70], 31084),
([22, 202, 42, 70], 25288)}
{([22, 225, 60, 174, 13], 53120)}
{([22, 225, 60,174,235],52305),
([22, 225, 60, 202,42, 174,235],480)}
{([22, 191, 97, 222, 126], 49252)}
?

?

?
(a) Frequent Patterns

Item Object

owl:Thing
227 dbp:Work
189 dbp:Film
213 schema:Movie
103 dbp:Person

304 foaf:Person
173 dbp:Artist
225 dbp:Place

schema:Place

schema:Person

(b) object mappings

Fig. 2. Sample frequent patterns generated for DBpedia Ontology Types dataset. Each
item represents a numerically encoded object. An item can be associated with multiple
frequent patterns as seen for item 70.

Support and Frequent Itemset. The support of an itemset X, denoted by
(X), is the number of transactions in D containing X. Itemset X is said to be

frequent if (X)  min (min is a minimum support threshold).

Itemset Mining. A frequent itemset is often referred to as a frequent pattern.
Numerous studies have been done and various algorithms [1,2,9,22,23] have been
proposed to mine frequent itemsets. In this study, we use the FP-Growth [9] algorithm for generating frequent itemsets. We represent the output of FP-Growth
as a set of pairs (k, Fk), where k is an item, and Fk, a set of frequent patterns
corresponding to k. Each frequent pattern is a pair of the form (v, v). v is an
itemset of a frequent pattern and v is a support of this frequent pattern.
?

?

?
Definition 1. Let D be a transaction database over a set I of items, and min
a minimum support threshold. The set of frequent itemsets in D with respect to
min is denoted by F (D, min) := {X  I|(X)  min}

Figure 2(a) shows several frequent patterns for DBpedia Ontology Types dataset
containing only the rdf:type property.2 To generate such frequent patterns, we
first create a transaction database as shown in Figure 1 and then use parallel
FP-Growth to compute frequent patterns. Please refer to [9,15] for details about
the FP-Growth algorithm and its implementation. Figure 3 shows the list of
inter-property frequent patterns for one of the linked open datasets.

Item Frequent Patterns
6:114 {([1:101, 5:113, 6:114],748384),
5:102 {([1:101, 5:102],1042692),
5:176 {([1:101, 5:176],1695814),
6:109 {([1:101, 5:108, 6:109],2792865),

([1:101, 11:8912626, 5:113, 6:114],230746)}
([1:101, 11:8912626, 5:102],225428)}
([1:101, 11:8912626, 5:176],1044079)}
([1:101, 5:108, 6:109, 11:8912626],166815)}

Fig. 3. Frequent patterns generated for the Geonames dataset. Each item is a pair of
property and object (p : o).

2.2 Association Rule Mining

Frequent itemset mining is often associated with association rule mining, which
involves generating association rules from the frequent itemset with constraints of
minimal confidence (to determine if a rule is interesting or not). However, in this
study, we do not require mining association rules using confidence values. Instead,
we split the given database into two disjoint databases, say A and B, based on
the frequent patterns. Those transactions which contain one or more of the top
N frequent patterns are inserted into dataset A while the other transactions are
inserted into dataset B. Compression can be performed by creating a set of rules
using top N frequent patterns and removing those triples from the dataset which
can be inferred by applying rules to some other triples in the same dataset.

Multi-dimensional Association Rules. Although association mining was
originally studied for mining transactions for only one attribute (ex:Product),
much research has been performed to extend it across multiple attributes
[16,17,28,29]. In this study, RDF datasets are viewed as multi-dimensional transaction databases by treating each property as an attribute and a subject as an
identifier. Similar to intra-transaction and inter-transaction associations [17], we
define intra-property and inter-property associations for RDF datasets. Intraproperty association refers to an association among different object values for

http://downloads.dbpedia.org/preview.php?file=3.7 sl en sl instance
types en.nt.bz2

A.K. Joshi, P. Hitzler, and G. Dong

a given property while inter-property association refers to association between
multiple properties.

3 Rule Based Compression

In this section, we introduce two RB compression algorithms - one using intraproperty transactions and the other using inter-property transactions. In addi-
tion, we provide an algorithm for delta compression to deal with incremental
compression when a set of triples needs to be added to existing compressed
graphs. Specifically, we investigate how to
 generate a set of decompression rules, R
 decompose the graph G to GA and GD, such that the requirements of RB
 maximize the reduction in number of triples

compression holds true

Fig. 4. Rule Based Compression, G = GD  R(GA)

Figure 4 depicts the high level overview of Rule Based Compression technique.
We consider an RDF Graph G containing |G| non-duplicate triples. Lossless
compression on graph G can be obtained by splitting the given graph G into
an Active Graph, GA, and a Dormant Graph, GD, such that: G = R(GA) 
GD where R represents the set of decompression rules to be applied to the
active graph GA during decompression. R(GA) is the graph resulting from this
application.

Since the compression is lossless, we have |G| = |R(GA)| + |GD|.

Definition 2. Let G be an RDF graph containing a set T of triples. An RB

compression is a 3-tuple (GA, GD, R), where GD  G is a dormant graph containing some triples TD  T , GA is an active graph containing TA  T  TD
triples and R is a set of decompression rules that is applied to GA (denoted by
R(GA)) producing a graph containing exactly the set T  TD of triples.

GD is referred to as dormant since it remains unchanged during decompression

(no rule can be applied to it during decompression).
?

?

?
Logical Linked Data Compression

Algorithm 1 follows a divide and conquer approach. For each property in a
graph G, we create a new dataset and mine frequent patterns on this dataset.
Transactions are created per subject within this dataset. Each transaction is a
list of objects corresponding to a subject as shown in Figure 1. Using frequent
patterns, a set of rules is generated for each property and later aggregated. Each
rule contains a property p, an object item k, and a frequent pattern itemset v
associated with k. This rule will be used to expand compressed data given in
GA as follows:

x.triple(x, p, k)  n

i=1

triple(x, p, vi)

where, v = v1, v2, ..., vn

Algorithm 1. Intra-property RB compression
Require: G
1: R  , GD   , GA  
2: for each property, p that occurs in G do
3: create a transaction database D from a set of intra-property transactions. Each
transaction (s, t) contains a subject s as identifier and t a set of corresponding
objects.

for all (k, Fk) do

select vk such that
(vk) = argmaxv{(v)|v occurs in Fk,|v| > 1}
R  R  (k  vk)

end for
for each (s, t)  D do
if t  vk = vk then

4: generate {(k, Fk)} set of frequent patterns
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21: end for

for each (k  vk)  R do
GA  GA  (s, p, k)
t  t  vk

end for
for each o  t do

GD  GD  (s, p, o)

end for

end for

end if

 add a new rule

 add single triple

For illustration, heres one such decompression rule we obtained during an

experiment on DBpedia dataset:
x.triple(x, rdf:type, foaf:Person) 

triple(x, rdf:type, schema:Person)
 triple(x, rdf:type, dbp:Person)
 triple(x, rdf:type, owl:Thing)

This triple is attached to the active graph GA so that all triples that can be
inferred from it are removed. Other triples which cannot be inferred, are placed

A.K. Joshi, P. Hitzler, and G. Dong

in dormant graph GD. The process is repeated for all properties, appending
results to already existing rules R, active graph GA and dormant graph GD.

3.2 Inter-property RB Compression

In Algorithm 2, we mine frequent patterns across different properties. Transactions used in this algorithm are created by generating a list of all possible pairs
of properties and objects for each subject. Thus, each item of a transaction is a
pair (p : o). We follow similar approach as before for generating frequent patterns
and rules. Each rule contains a key pair (pk, ok) and a corresponding frequent
pattern v as a list of items (p : o).

transaction, (s, t) contains a subject s as identifier and t a set of (p, o) items.

(vk) = {argmaxv(v)|v occurs in Fk,|v| > 1}

Algorithm 2. Inter-property RB compression
Require: G
1: R  , GD   , GA  
2: create a transaction database D from a set of inter-property transactions. Each
3: generate {(k, Fk)} set of frequent patterns
4: for all (k, Fk) do
select vk such that
5:
6:
R  R  (k  vk)
7:
8: end for
9: for each (s, t)  D do
10:
if t  vk = vk then
11:
12:
13:
14:
15:
16:
17:
18:
19: end for

for each (k  vk)  R do
GA  GA  (s, pk, ok)
t  t  vk

 add single triple

end if

end for
for each (p, o)  t do
GD  GD  (s, p, o))

end for

 add a new rule

The procedure is similar to one described in 3.1 once frequent patterns and

rules are generated.

x.triple(x, pk, ok)  n

triple(x, pi, oi)

For illustration, heres one such decompression rule we obtained during an

i=1

experiment on Geonames dataset:
x.triple(x, geo:featureCode, geo:V.FRST) 
triple(x, rdf:type, geo:Feature)
 triple(x, geo:featureClass, geo:V)
?

?

?
3.3 Optimal Frequent Patterns

In this section, we describe optimal rule generation strategy for achieving better
compression. In Algorithm 1 and Algorithm 2, we generate frequent patterns and
keep only one frequent pattern v per k. By selecting only one frequent pattern
per item, its guaranteed that no circular reference or recursion occurs during
decompression. As such, for any given triple in a compressed graph, only one
rule can be applied.

The choice of v for k is determined based on whether v has the maximum
support. In this section, we present our findings for optimal v pattern selection
based on both support value and itemset length. To illustrate this finding, please
consider a sample FP-Growth output obtained by mining one of the datasets as
shown in Figure 2(a) in section 2.1. If we look at frequent pattern sets for k = 70,
we have:

1. (v1, 1) = ([22, 70], 56372)
2. (v2, 2) = ([22, 103, 26, 304, 173, 70], 31084)
3. (v3, 3) = ([22, 202, 42, 70], 25288)

The following rule can be applied to select the optimal frequent pattern: select

the pattern vi that maximizes (|vi| 1) i). We call (|vi| 1) i), denoted by
(vi), the Redundant Triple Density, signifying the total number of triples that
can be removed by using a rule: (k  vk). It is apparent that selecting v2 during
rule generation leads to higher compression than selecting v1 or v3.
We call (|vi|)  i) the Triple Density signifying the total number of triples

that are associated with this rule.

3.4 Delta Compression

One of the important properties of RB compression is that incremental compression can be achieved on the fly without much computation. Lets say, we
consider an RDF graph G, which has undergone RB-Compression resulting in
GA active graph, GD dormant graph and a set R of decompression rules. If a
new set of triples corresponding to a subject s, denoted by Ts, needs to be
added to graph G, delta compression can be achieved by using the results from
the last compression. Each delta compression updates the existing active and
dormant graphs. Hence, there is no need for full RB-Compression every time a
set of triples is added.

Algorithm 3 provides a delta compression algorithm when Ts needs to be
added. The algorithm can be extended to include a set of subjects, S. It should
be noted that we do not create new rules for a new set of triples. As such, the
compressed version might not be optimal. A full compression is recommended
if a large number of new triples needs to be added or if large number of delta
compression have already been performed.

If a triple needs to be removed, an extra check needs to be performed to see
if the removal violates any existing rules. Such removal might require moving
some of the inferred triples from the active graph to the dormant graph.

A.K. Joshi, P. Hitzler, and G. Dong

Algorithm 3. Delta Compression
Require: GA, GD, R , Ts
1: Extract all triples, TD, corresponding to s subject from GD
2: T  TD  Ts
3: for all t  T do
4:
5:
6:
7:
8: end for
9: for all t  T do
GD  GD  t
10:
11: end for

if R(t)  T then
GA  GA  t
T  T  R(t)

end if

 insert into active graph

 insert into dormant graph

4 Decompression

Decompression can be performed either sequentially or in parallel. Sequential
decompression requires applying R decompression rules to triples in GA active
graph and merging these inferred triples with the triples in GD dormant graph.
Since each triple in a compressed graph can belong to at most one rule, its
complexity is O(|R|.|GA|). The number of rules is negligible compared to the

number of triples in the active graph.

For parallel decompression, an active graph can be split into multiple smaller
graphs so that each small dataset can perform decompression. This allows generation of inferred triples in parallel. Since rules are not ordered, inferred triples
can be added to an uncompressed graph whenever they are generated. Finally,
all triples of the dormant graph are merged into this uncompressed graph.

5 Experiments

This section shows experimental results of the compression performed by our
system. Our experiment is conducted on several linked open datasets as well as
synthetic benchmark datasets of varying sizes. The smallest dataset consists of
130K triples while the largest dataset consists of 119 million triples.

5.1 RB Compression - Triple Reduction

Table 1 shows a comparison between the outputs of the two algorithms we discussed in Section 3 for nine different linked open datasets. The compression
ratio, r is defined as the ratio of the number of triples in compressed dataset to
that in uncompressed dataset. It is evident from the results that compression
based on inter-property frequent patterns is far better than compression using
intra-property frequent patterns. Details including the number of predicates and
transactions derived during experiments are also included in the table. It can be
seen that the best RB compression (inter-property) can remove more than 50%
of triples for the CN datasets and DBpedia rdftype dataset.
?

?

?
Table 1. Compression ratio for various linked open datasets

Dataset

triples predicate transaction

(K)

(K)

compression ratio

intra-property inter-property

Dog Food

CN 2012

ArchiveHub

Jamendo

LinkedMdb 6147

rdftypes

RDF About 17188

Geonames 119416
?

?

?
0.98

0.82

0.92

0.99

0.97

0.19

0.97

0.96
0.97

0.82

0.43

0.71

0.82

0.75

0.19

0.84

0.86
0.71

5.2 RB Compression - Performance

In addition to the compression ratio, the following metrics are measured to
evaluate the performance of the system: a) time it takes to perform RB compression and b) time it takes to perform full decompression. Figure 5 shows the

Compression
Decompression

s
d
n
o
c
e
s

Dogfood

Lah

Jamendo

Lmdb

Rdftype

DBLP Geonames

Datasets

Fig. 5. Compression vs Decompression time for various linked open datasets

comparison between total time required for compression and the total time required for the full decompression. In general, RB compression time increases
with the increase in triple size. However, if the total number of predicates in
a dataset is very low, as in the case of DBpedia rdftypes dataset, compression
time could be significantly lower. Decompression is faster by several order of
magnitudes compared to the compression. This can be attributed to the fact
that each triple is associated with a maximum of one rule and the number of
rules are very few compared to the triple size. In addition, we apply rules only
to triples in the Active Graph.











A.K. Joshi, P. Hitzler, and G. Dong

5.3 RB Compression on Benchmark Dataset

In this experiment, we ran RB Compression against one of the mainstream
benchmark datasets, LUBM [8]. LUBM consists of a university domain ontology
and provides a method for generating synthetic data of varying size.

Table 2 provides details on various LUBM datasets3 we used for the exper-
iment. Not surprisingly, these results show that compression time on dataset
increases with the increase in dataset size. However, the compression ratio remained nearly constant for all the synthetic dataset. Decompression time proved
to be far lesser than the time required for compression as seen in Figure 6. It took
only 200 seconds for the decompression of the LUBM 1000 dataset compared to
11029 second for the compression.

Table 2. Compression ratio and time for various LUBM datasets

Dataset

triples transaction compression Time

ratio

sec

(K)

(K)

LUBM 50

LUBM 100 13405

LUBM 200 26696

LUBM 500 66731

LUBM 1000 133573

0.763

0.757

0.757

0.757

0.757

Compression
Decompression

s
d
n
o
c
e
s
?

?

?
LUBM50

LUBM100

LUBM200

LUBM500

LUBM1000

LUBM Datasets

Fig. 6. Compression vs Decompression time for various LUBM datasets

5.4 Comparison Using Compressed Dataset Size

In addition to evaluating our system based on triple count, we examine the
compression based on the storage size of the compressed datasets and compare it
against other compression systems. This is important since none of the existing
compression systems has the ability to compress RDF datasets by removing
triples. [5] compared different universal compressors and found that bzip24 is
one of the best universal compressors. For this study, we compress the input

3 LUBM datasets created with index and seed set to 0.

http://bzip2.org







?

?

?
dataset (in N-Triples format) and the resulting dataset using bzip2 and provide
a quantitative comparison (see Table 3). An advantage of semantic compression
such as RB Compression is that one can still apply syntactic compression (e.g.
HDT) to the results. HDT [6] achieves a greater compression for most of the
datasets we experimented on. Such high performance can be attributed to its
ability to take advantage of the highly skewed RDF data. Since any generic
RDF dataset can be converted to HDT compact form, we ran HDT on the
compressed dataset resulting from RB Compression. The experimental results
are shown in Table 3. We see that this integration does not always lead to a
better compression. This is due to the overhead of header and dictionary that
HDT creates for both active and dormant dataset5.

Table 3. Comparison of various compression techniques based on dataset size

Dataset

Size

compressed

compressed size using bzip2

HDT inter-property HDT + inter-

DogFood

CN 2012

23.4 MB

1.5 MB 1088 K

1492 K

17.9 MB

488 K

164 K

Archive Hub

71.8 MB

2.5MB

1.8 MB

Jamendo

143.9 MB

6 MB

LinkedMdb

850.3 MB 22 MB

DBpedia rdftypes 1.2 GB

45 MB

4.4MB

16 MB

11 MB

296 K

1.9 MB

5.6 MB

22.6 MB

17.9 MB

7.5 GB

265 MB 201 MB

239 MB

Geonames

13 GB

410 MB 304 MB

380 MB

1106 K

144 K

1.7MB

4.6 MB

14.5MB

10.1 MB

205 MB

303 MB

6 Soundness and Completeness

Although it should already be rather clear from our definitions and algorithms
that our compression is lossless in the sense that we can recover all erased triples
by using the newly introduced ruleslet us dwell on this point for a little while.
First of all, it is worth mentioning that we cannot only recreate all erased
triples by exhaustive forward-application of the rulesa fact that we could reasonable refer to as completeness of our approach. Rather, our approach is also
sound in the sense that only previously erased triples are created by application
of the rules. I.e., our approach does not include an inductive component, but is
rather restricted to detecting patterns which are explicitly and exactly represented
in the dataset. Needless to say, the recreation of erased triples using a forwardchaining application of rules can be rephrased as using a deductive reasoning
system as decompressor.
It is also worth noting that the rules which we introduce, which are essentially
of the form triple(x, p, k)  triple(x, p, v), can also be expressed in the OWL [10]
5 If both these graphs are merged and HDT is performed, the resulting size will be

always lesser than that obtained when only HDT is used for compression.

A.K. Joshi, P. Hitzler, and G. Dong

Web ontology Language. Indeed, a triple such as (x, p, k) can be expressed in
OWL, e.g., in the form6 k(x) if p is rdf:type, or in the form p(x, k) if p is
a newly introduced property. The rule above then becomes k  v for p being
rdf:type, and it becomes p.{k}  p.{v} in the case of the second example.
The observation just made that our compression rules are expressible in OWL.
From this perspective, our approach to lossless compression amounts to the
creation of schema knowledge which is completely faithful (in the sound and
complete sense) to the underlying data. I.e., it amounts to the introduction of
uncontroversial schema knowledge to Linked Data sets. It is rather clear that
this line of thinking opens up a plethora of exciting follow-up work, which we
intend to pursue.

7 Related Work

To the best of our knowledge, this is the first work that investigates practical
rule based logical compression of RDF datasets which removes triples to achieve
compression. Most of the existing compression techniques focus on compact representation of RDF data as a means of compression. Turtle, a sub-language of
N3, is one such compact and natural text representation for RDF data. [5] has
explored various compression techniques for RDF datasets and observed that
most RDF datasets are highly compressible due to its power-law distribution in
term-frequencies, schemas and resources. [6] introduced a more compact representation format, HDT, by decomposing an RDF data source into Header, Dictionary and Triples. A specific compressed version of HDT, HDT-compressed,
outperforms most of the universal compressors [6]. [19,21] studied the problem of
redundancy elimination on RDF graphs in the presence of rules, constraints and
queries. [24] uses distributed dictionary encoding with MapReduce to compress
large RDF datasets.

Work on frequent itemset mining [1,9,15,26,20,27] provides a foundation for
our algorithms. [4] explored pattern mining based compression schemes for web
graphs specifically designed to accomodate community queries. [25] used association rule mining techniques for generating ontology based on rdf:type statements.

8 Conclusion

In this paper, we have introduced a novel lossless compression technique called
Rule Based Compression that efficiently compresses RDF datasets using logical
rules. The key idea is to split the original dataset into two disjoint datasets
A and B, such that dataset A adheres to certain logical rules while B does
not. Dataset A can be compressed since we can prune those triples that can be
inferred by applying rules on some other triples in the same dataset. We have
provided two algorithms based on frequent pattern mining to demonstrate the
compression capability of our rule based compression. Experimental results show

6 We use description logic notation for convenience, see [11].
?

?

?
that in some datasets, RB Compression can remove more than half the triples
without losing data integrity. This finding is promising and should be explored
further for achieving better compression. In future work, we will investigate the
use of RB Compression in instance alignment and automated schema generation.

Acknowledgments. This work was supported by the National Science Foundation under award 1143717 III: EAGER  Expressive Scalable Querying over
Linked Open Data and 1017225 III: Small: TROn  Tractable Reasoning with
Ontologies.
