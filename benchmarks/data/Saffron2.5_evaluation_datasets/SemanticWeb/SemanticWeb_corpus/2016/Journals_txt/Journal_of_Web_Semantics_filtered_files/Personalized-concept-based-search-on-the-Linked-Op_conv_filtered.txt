Web Semantics: Science, Services and Agents on the World Wide Web 36 (2016) 3257

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

journal homepage: www.elsevier.com/locate/websem

Personalized concept-based search on the Linked Open Data
Melike Sah a,, Vincent Wade b

a Department of Computer Engineering, Near East University, Nicosia, North Cyprus, via Mersin 10, Turkey
b Centre for Global Intelligent Content, Knowledge and Data Engineering Group, Trinity College Dublin, Dublin, Ireland

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 16 January 2014
Received in revised form
31 August 2015
Accepted 26 November 2015
Available online 17 December 2015

Keywords:
Categorization
Concept-based search
Fuzzy retrieval model
Personalized search/exploration
Linked Open Data

In this paper, we present a novel personalized concept-based search mechanism for the Web of Data
based on results categorization. The innovation of the paper comes from combining novel categorization
and personalization techniques, and using categorization for providing personalization. In our approach,
search results (Linked Open Data resources) are dynamically categorized into Upper Mapping and Binding
Exchange Layer (UMBEL) concepts using a novel fuzzy retrieval model. Then, results with the same
concepts are grouped together to form categories, which we call concept lenses. Such categorization
enables concept-based browsing of the retrieved results aligned to users intent or interests. When the
user selects a concept lens for exploration, results are immediately personalized. In particular, all concept
lenses are personally re-organized according to their similarity to the selected lens. Within the selected
concept lens; more relevant results are included using results re-ranking and query expansion, as well as
relevant concept lenses are suggested to support results exploration. This allows dynamic adaptation of
results to the users local choices. We also support interactive personalization; when the user clicks on
a result, within the interacted lens, relevant lenses and results are included using results re-ranking and
query expansion.
Extensive evaluations were performed to assess our approach: (i) Performance of our fuzzy-based
categorization approach was evaluated on a particular benchmark (10,000 mappings). The evaluations
showed that we can achieve highly acceptable categorization accuracy and perform better than the vector
space model. (ii) Personalized search efficacy was assessed using a user study with 32 participants in a
tourist domain. The results revealed that our approach performed significantly better than a non-adaptive
baseline search. (iii) Dynamic personalization performance was evaluated, which illustrated that our
personalization approach is scalable. (iv) Finally, we compared our system with the existing LOD search
engines, which showed that our approach is unique.

 2015 Elsevier B.V. All rights reserved.

1. Introduction

1.1. Motivations

With the adoption of the Linked Open Data (LOD) by a
wider Web community, large volumes of semantic data are being
generated. The challenge now is finding and exploring relevant
information on the Web of Data (WoD). This is crucial for the
uptake of the LOD by applications in order to support both ordinary
Web and Semantic Web users. Thus, LOD search engines play
a vital role for providing efficient access mechanisms. However

 Corresponding author.

E-mail addresses: melike.sah@neu.edu.tr (M. Sah), Vincent.Wade@scss.tcd.ie

(V. Wade).

http://dx.doi.org/10.1016/j.websem.2015.11.004
1570-8268/ 2015 Elsevier B.V. All rights reserved.

current approaches such as Swoogle [1], Sindice [2], Watson [3],
OKKAM [4] use keyword-based search and ranked result lists
presentation of traditional information retrieval (IR), which is not
very effective with the growing LOD [5]. Like the traditional Web,
keyword queries perform well if the user has specific information
needs. If the users aim is to gather or explore information in
unfamiliar domains, then the existing approaches provide little
support. Although some approaches allow results filtering by
class, predicate, etc., users are required to know technical details
(i.e. URIs, SPARQL queries). This means these features are only
available to Semantic Web experts. Some form of categorization
can also be useful and OKKAM organizes the results into seven
concepts (e.g. location, organization, artifact instance, artifact type,
event and other). However, the provided concepts are too broad to
be real value. There is little research investigating search problems
on the WoD.

Another search paradigm for the LOD is faceted search/browsing
systems, which provide facets (concepts) for interactive search and
browsing [6]. Facets assist results filtering and exploration. How-
ever, the main limitation of faceted search is that facet creation
depends on specific data and schema properties of underlying
metadata and it can be difficult to generate useful facets to large
and heterogeneous WoD [712].

Traditional IR has been investigating efficient search mechanisms for decades; results clustering and personalized search are
two popular methods for enhancing search effectiveness. In clustering search, results are organized into categories for assisting
users in results exploration and in disambiguation of the query
(Snaket [13], Vivisimo.com, carrot2.org). Results categorization is
also widely used, such as Google categories, Yahoo Directories
and Open Directory Project (ODP). Although clustering search and
faceted search seem similar, the latter filters results based on
schema/metadata, whereas the former clusters results based on
their meaning (language model).

Alternatively, personalized search aims to improve retrieval
efficiency by adapting results to context/interests of individual
users; thus the user can explore personally relevant results [14].
It is a popular research and commercial interest (i.e. Google).
However, personalized search gained very little focus on the
Semantic Web [15]. This could be because of isolated and low
volumes of metadata created in early linked data initiatives. As the
size of LOD increases, personalized search and interactions become
more crucial.

Our objective is to improve current search mechanisms on the
LOD with a novel personalized concept-based search. In order to
achieve this, we address the following key challenges: (1) Categorization of LOD resources under a conceptual structure. (2) Providing a scalable system architecture for dynamic categorization of
LOD resources. (3) Supporting a novel search interface based on
concept-based results exploration. (4) Introducing a model such
that results categorization can be used as a tool for adaptive results
presentation. In addition, personalization should be non-intrusive
and scalable. (5) Representing users search intents using the available semantic and syntactic information. (6) Introducing a scalable
similarity score which includes both semantic and syntactic similarity measures for results adaptation. All of the mentioned features were implemented and an online demo is publicly available
as we discuss in Section 2.

1.2. Contributions

We innovatively combined results categorization and personalized IR to introduce a novel personalized concept-based search
mechanism for the WoD. In our approach, users can use keyword or URI queries and results are organized into concepts using
UMBEL conceptual vocabulary [16]. Here, the concept can be
thought as a class, topic or category. To avoid any confusion, we
will use the term concept in the rest of the paper. For categorization
of results, we introduce a novel retrieval model, which works on
any linked dataset, scalable and reasonably accurate (F-Measure
of 87% on 10,000 mappings). On the client-side, results with the
most confident UMBEL concept categorization are grouped to form
concept lenses. To clearly define, a concept lens corresponds to a
UMBEL concept and the search results belonging to this concept.
Similarly, when we say concept lenses, we mean a set of UMBEL
concepts and the search results belonging to these concepts. There-
fore, concept lenses depend on the actual search query. The aim of
concept lenses is to aid results exploration. Then, based on user interactions with the results, we apply personalization in two cases:
(i) When a user selects a concept lens. All concept lenses are reorganized based on conceptual similarity. In addition, within the

selected lens, immediately more relevant results are included using results re-ranking and query expansion as well as similar lenses
are suggested for exploration. (ii) When a user clicks onto a result,
then we support interactive personalization. Last N clicks of the
user within the search session are used to add relevant results to
the interacted lens and relevant concept lenses are suggested for
exploration. In our approach, users search intents are represented
as a vector of concepts (semantic) and terms (syntactic). Thus a
combined similarity measure can be used for results adaptation.

Please note that our personalization approach is non-intrusive,
privacy preserving and scalable, since it does not require an explicit
user login and the personalization is implemented at the client-
side. In addition, our approach is adaptable and can be plugged
on top of any linked data search engine; in this paper, we use
Sindice [2]. It only requires categorizations of resources using
UMBEL, which can be achieved by number of methods such as our
retrieval model [17].
Our contributions can be summarized as follows:
 We introduce a novel personalized concept-based search and
exploration mechanism for the WoD. To the best of our
knowledge, no such previous work exists.
 We introduce a novel and robust categorization algorithm using
a novel fuzzy retrieval model. In our fuzzy retrieval model,
first we extract terms from search results (LOD resources).
Then, relevancy of the extracted terms on UMBEL concepts is
calculated using a fuzzy function. In our fuzzy function, we
calculate membership scores for each semantic part of a UMBEL
concept using an extended tf  idf model. Then, these scores
are combined and an ontological relationships driven voting
algorithm is applied in order to categorize the search result into
a specific UMBEL concept.
 Categorizations are utilized for concept-based results presen-
tation. Results with the same most confident categorization are
grouped together and presented, which we call concept lenses.
In this way, users can explore results by clicking on concepts.
 We suggest the use of results categorization as a tool for personalized concept lenses re-ranking, results re-ranking, query expansion and lenses suggestion. To the best of our knowledge, we
are the first to combine these four personalization techniques
based on concept-based search results presentation and user
click histories. In related works [13,14], ontology is utilized for
results re-ranking and query expansion. However, results are
presented as traditional ranked lists. On the other hand, in our
concept-based personalization approach, a vocabulary (UMBEL
in this case) is the key element of categorization, concept-based
presentation and personalization of results. In particular, we
track user clicks on both concept lenses and individual search
results. We develop personalization strategies and personalized
ranking models to rank both concept lenses and results based
on user actions. In addition, we predict relevant concept lenses
based on ontological relationships between concepts. Finally,
we use user clicks and UMBEL concept labels for query expan-
sion. The evaluations have indicated that the use of our personalization strategies and lenses approach provides significantly
better results than a non-adaptive baseline search systems.

A part of this work has been presented in [1719]. In this paper,
we extend the explanations and evaluations of the framework.
In particular, we introduce two new evaluations: Compare our
approach with the existing LOD search engines and present a new
user study using 32 participants. The user study consisted of a
task-based evaluations based on real life user information needs
regarding to a tourist domain. In the user study, we compared our
personalized concept-based search with a non-adaptive baseline
search system in order to assess the added value of categorization
and personalization.

M. Sah, V. Wade / Web Semantics: Science, Services and Agents on the World Wide Web 36 (2016) 3257

Fig. 1. System components process flow.

The rest of the paper is organized as follows: Section 2
explains the system architecture of the personalized conceptbased search framework. Section 3 introduces the categorization
algorithm, which involves; term extraction from LOD resources; a
novel semantic indexing model; and a novel retrieval model for
categorization of LOD resources into UMBEL concepts. Section 4
introduces search results personalization that involves; contextbased user data gathering; user profile representation; and
adaptation algorithms based on personalized IR techniques.
Section 5 presents extensive evaluations. Specifically, our retrieval
model was evaluated on a particular benchmark (10,000
mappings) for its categorization performance, which achieved
high categorization accuracy (90%) and outperformed the vector
space model (33%). This is crucial for the correct formation of
concept lenses. Moreover, time evaluations were performed to
test system categorization performance. Personalization efficacy
was evaluated with user studies, which showed that our approach
performs significantly better than a non-adaptive baseline search
system. Dynamic personalization efficiency (time overhead) and
retrieval performance was evaluated. Evaluations showed that
our personalization approach is effective in terms of scalability
and retrieval performance. Finally, we compared our search
mechanism with the existing LOD search engines, which showed
that our approach is unique on the LOD. Section 6 discusses the
related work.

2. System architecture of the personalized concept-based
search framework

The system architecture of the personalized concept-based
search mechanism is shown in Fig. 1. An online demo of the
personalized concept-based search is also available.1 The online
demo uses our user study domain, which is tourism in Killarney,

1 Online demo of the proposed search is available at http://phaedrus.scss.tcd.
ie/personalenses/demo/index.html. Please note that the demo works on Google
Chrome browser.

Ireland. Therefore, users can use the provided search tasks [20]
as a guide to explore and test our search system. Please note that
our system can work on any linked data domain since our system
architecture is designed independent of linked data domains.
Specifically, our categorization and personalization algorithms are
domain independent. Here, the demo just showcases how our
system operates in a tourism domain. The demo can be adapted
to a new domain by only changing the indexed linked data URIs.

As can be seen, the system architecture has two main parts;
(i) using a user query, the system searches the LOD and categorizes
the retrieved search results at the server-side as we discuss in
Section 3. (ii) Based on user interactions, at the client-side the
search results are personalized to individual users as we explain
in Section 4. In our approach, users can provide keyword or URI
based queries to the system. Using these input queries, our system
searches the WoD by utilizing Sindice search API [2] and initial
search results from the Sindice search are presented to users
with no categorization. Note that any LOD search API can be
used and currently we use Sindice. Then, we need to categorize
each result into UMBEL concepts where a detailed categorization
workflow is depicted in Fig. 2. For each search result (LOD URI),
parallel requests are sent to the server for categorization of
LOD resources under UMBEL concepts. First, RDF description of
the LOD resource is cached to a Jena model using the Sindice
Cache (i.e. http://any23.org/). Using the RDF description, terms
from different semantic parts of the LOD resource are mined.
Subsequently, the extracted terms are weighted according to their
importance (see Section 3.1). In order to match the extracted
terms to UMBEL concepts, we represent UMBEL concepts using
a semantic indexing model. Specifically, depending on where the
terms appear in the UMBEL concept hierarchy has a particular
importance during the indexing process (see Sections 3.2 and 3.3).
Finally, the obtained terms from the LOD resource are mapped
to the inverted concept index by a fuzzy retrieval model (see
Section 3.4) and categorized LOD resources are sent back to
the client. In particular, we categorize a result into 13 UMBEL
concepts and find their top level concepts (super type concepts in
UMBEL). Specifically, we use the whole 5-depth UMBEL hierarchy

Fig. 2. Categorization workflow.

(a) Non-adaptive results presentation; the user selects a
concept lens for exploration.

(b) Results are immediately personalized; the order of lenses are adapted, more relevant results are added by results re-ranking and query
expansion, also relevant concept lenses are suggested.

Fig. 3. Personalized concept-based search for the query killarney sightseeing. A demo of the interface can be found at [31].

(25,000 concepts); a LOD resource may match to any concept,
which is different than many personalized IR methods. Generally
the top level or top 2 levels of the ontology (200 concepts of ODP)
are used to represent search results. However, such an approach
can only model general user interests. Finally, categorized LOD
resources (results) are sent back to the client and the results with

the same concepts are grouped to form concept lenses. Specifically,
the most confident categorization is used for creating concept
lenses and the rest is used for the purpose of personalization and
semantic similarity comparisons. Since dynamic categorization can
be time consuming, search results are shown incrementally by
using Asynchronous JavaScript (AJAX). In addition, for a scalable

M. Sah, V. Wade / Web Semantics: Science, Services and Agents on the World Wide Web 36 (2016) 3257

performance, categorized LOD resources are cached to a local index
at the server-side. Dynamic categorization is applied if a LOD
resource has not been processed before. In summary, results are
presented with concept lenses rather than a flat list of ranked
results. Thus, users can explore the search results from different
conceptual perspectives, which aid results exploration.

In Fig. 3, a screen shot of the concept-based search interface
is presented for query Killarney sightseeing. Users are first
presented with a list of concepts according to the results
categorization (see Fig. 3(a)) and required to select a concept lens
in order to start results exploration. When the user clicks onto
a concept lens, results are immediately personalized as shown
in Fig. 3(b). In this particular case, the user selected the Tourist
Attraction concept lens. Subsequently, the following adaptation
is applied: (i) All concept lenses are re-organized based on a
combined semantic and syntactic similarity to the selected concept
lens, which is Tourist Attraction concept (see Section 4.3 for
details). (ii) Within the selected lens, more relevant results are
included using results re-ranking (see Section 4.4) and query
expansion (see Section 4.5). In the example, using re-ranking,
relevant results are added under more results caption, such Gap
of Dunloe resource (see Fig. 3(b)). For query expansion, we use
UMBEL alternative labels to re-formulate the query. In UMBEL, the
Tourist Attraction concept contains the following alternate labels;
tourist site, tourist attractions, attraction and tourist destination.
We add these alternative labels to the original query and perform
a new search query in order to add more relevant results into
the interacted concept lens. In the example, five more relevant
tourist attractions are added into the results list. (iii) Within the
selected lens, relevant concept lenses are suggested for exploration
(see Section 4.4). In our example here, the user is interested in
tourist attractions at Killarney region. As shown in Fig. 3(b), our
system suggests horseback riding, tourism, golf, mountain, walking
and national park concepts for exploration. These are very good
suggestions, since natural places and nature related activities are
the most popular tourist attractions in Killarney region. Finally,
when the user interacts with the results (i.e. clicks onto a result),
we support interactive personalization. We use the last N clicks of
the user to apply result re-ranking, query expansion and concept
lenses suggestions.

Using our approach, results and concept lenses are adapted to
the local choices of the user. Specifically, we support personalized
and conceptual result exploration, which is especially useful in
complex information needs, such as information gathering in
unfamiliar domains. With our approach, users are not required
to log in for personalization, since we only use the click data
within the current search session. The system is able to cope with
changes of search domain by using categorization as we discuss in
Sections 4.1 and 4.2.

The framework is implemented using the following software
tools. Given that indexing and caching of WoD is very expensive,
our approach uses Sindice search API for querying the WoD and
Sindice Cache for retrieving RDF descriptions of LOD resources [2].
Lucene IR framework is utilized for indexing of concepts and at
the implementation of the retrieval model. The server side is
implemented with Java Servlets and uses Jena for processing of
RDF. The client side is written using Javascript and AJAX. To support
non-intrusive user modeling and adaptation, personalization is
completely implemented at the client-side. Thus, it does not
require user login since only users click data within the current
search session is used. Client-side personalization is also scalable
and computationally efficient since the workload is distributed to
the clients and network traffic is significantly reduced.

3. Categorization of LOD resources using UMBEL

In this section, we explain the categorization process in detail.
Section 3.1 explains why UMBEL is selected for categorization.
Section 3.2 discusses which LOD resource features are extracted
for the categorization. Section 3.3 explains the semantic indexing
of UMBEL concept descriptions. Section 3.4 introduces our retrieval
model for the categorization.

3.1. UMBEL and vocabularies for concept-based IR

In concept-based IR, existence of a conceptual structure for
representing context of an information object and query is crucial.
The conceptual structures can range from simple thesaurus,
dictionaries to more complex semantically rich ontologies. Yahoo
Directory,2 ODP,3 Proton,4 SUMO5 and Sensus6 are examples that
have been utilized for concept-based search. SUMO and Proton
have relatively sparse subject concepts and their penetration into
general Web is quite limited. Sensus is a concept ontology derived
from WordNet. However, Sensus does not include much semantic
information, which can be very useful for concept-based IR. Yahoo
and ODP are by far the most commonly used taxonomies for
concept-based IR [14,21,22]. However, with the increasing LOD
ontologies, usage of Yahoo and ODP has significantly decreased.
Therefore, we looked for candidates in LOD vocabularies such as
OpenCyc,7 UMBEL [16], DBpedia [23] and Yago [24].

OpenCyc is an upper ontology generated manually over the
last twenty years. It captures common sense knowledge and a
broad number of concepts. In addition, OpenCyc is purposefully
created to support inferencing such as it uses WordNet for concept
disambiguation and it captures subject relationships between
concepts to enable reasoning. However, OpenCycs top ontology
concepts are obscure and contain many domain-specific concepts
that are developed for project purposes.

UMBEL is a cleaner and simpler sub-set of OpenCyc with
the specific aim of promoting interoperability with external
linked datasets. UMBEL contains 25,000 concepts and classified
into a taxonomy using super and sub-concepts. Each UMBEL
concept description contains preferred label and alternative labels.
Alternative labels usually include synonyms, quasi-synonyms,
lexical variations, plural, verb derivations and semantic related
words. In addition, UMBEL concepts are organized under 32 toplevel super type classes (i.e. Events, Places), which makes them
easier to reason, search and browse. Moreover, a key benefit of
using UMBEL is that it is connected to other linked datasets such as
DBpedia, Geonames, schema.org and OpenCyc, which we can use
to enrich results display.

On the other hand, DBpedia and Yagos named entity coverage
is good. However, their category graph is noisy and not well
structured in a sense that obtaining a useful hierarchy from these
datasets can be challenging.8 This makes them difficult to use and
reason for concept-based search.

Finally, recently Google, Yahoo, Bing and Yandex started to use
a markup vocabulary, schema.org, for annotating Web pages, so

2 http://dir.yahoo.com/.
3 http://www.dmoz.org/.
4 http://proton.semanticweb.org/.
5 http://www.ontologyportal.org/.
6 http://www.isi.edu/natural-language/projects/ONTOLOGIES.html.
7 http://www.opencyc.org/.
8 DBpedia and Yago provide rich interconnected instance data, as well as,
instances are linked to categories. However DBpedia and Yago categories are not
structured well; structural relationships between categories are not consistent and
their coverage is not balanced.

generic (i.e. rdfs:label, rdf:type, foaf:name, etc.) and do not provide
information about the context. To overcome this, we compiled a
list of generic property names and if a property name matches any
of these, it is not accepted. On the other hand, type (rdf:type and
dc:type) and subject (dc:subject) provides the most discriminative
features about the context of a LOD resource. For instance, type
and subject values can provide useful knowledge about concepts
(general notion or idea) of the resource for correct categorization.
For instance, for label ocean the context is not clear. But if we
know type is album, then we can understand that ocean is a label
of an album.

that search engines can improve presentation of search results. The
vocabulary contains broad concepts, which can be useful if they
publish the training data in future. In addition, UMBEL is mapped
to schema.org concepts, which can be utilized directly.

Based on current available data, UMBELs broad concept
coverage, rich representation of concept descriptions and powerful
reasoning capabilities stand out among other LOD conceptual
ontologies. Thus, we decided to utilize UMBEL for concept-based
search.

On the other hand, one can argue that a tool like DBpedia Spotlight can be used to recognize entities [25] and then DBpedia categories of the recognized entities can be used instead of UMBEL
concepts. However, DBpedia categories have the following two
drawbacks: (1) According to the website,9 DBpedia contains unnecessary internal categories. Many of the categories are for
(mostly) internal administration purposes and they do not actually represent true knowledge structure of the DBpedia knowledge base. Even the cleaned DBpedia categories reach to 80,000
concepts. This is a problem for user modeling using DBpedia cat-
egories, since the user data may be noisy as well as user information may be scattered (i.e. very sparse). (2) The second main issue
is that DBpedia categories do not have consistent structural relationships between them. For example, some categories may have
heavy tree structures and some categories are just leaf nodes. This
is particularly a problem during semantic similarity computation
of results using sub-concept or super-concept relationships. On the
other hand, UMBEL provides a light-weight ontology structure and
standard set of subject concepts (25,000) and relationships. The
UMBEL vocabulary was carefully constructed and verified by human subjects in order to promote a lightweight vocabulary for data
sharing. In addition, all UMBEL concepts are categorized under 32
super type concepts which allow us to infer general (top-level) user
interests or more specific user interests by navigating in the UMBEL hierarchy.

3.2. Recognizing context of LOD resources

In order to generate concept-based search results, first the
retrieved LOD resources from the Sindice search need to be
categorized under UMBEL concepts. To achieve this, the concepts
of LOD resources should be understood, where lexical information
about LOD resources can be used to mine such knowledge.
One option is to extract all lexical information from the URI,
labels, properties and property values of the LOD resources that
are retrieved by Sindice search. However, in such a process,
many misleading words may also be extracted. For example, a
LOD resource about a TV broadcasting organization may include
information about broadcasting network but it may also include
other information about programs, coverage, etc., which may lead
to an incorrect categorization. Thus, the challenge is to identify
important features of LOD resources for correct categorization.
Term extraction and enrichment. We chose the following
common features of LOD resources that may be used to represent
context of the resourceURI (u), label (l), type (t), subject (s) and
property names (p). We chose these features for the following
reasons: the URI of a resource may contain keywords relevant to
the context of the resource. Titles (dc:title), names (foaf:name)
and labels (rdfs:label), so called label features usually include
informative information about the resource. property names
typically provide further knowledge about what type of resource
is. For example, birth place, father, mother, spouse are properties
associated with persons. However, some property names are

In Algorithm 1, the term extraction procedure is shown. From
each LOD resource, keywords are extracted from the features as
explained above. Then, qualifiers and propositions are removed
from the extracted keywords, to enhance categorization accuracy.
For instance, for the context hockey games in Canada, the main
concept is hockey games and not the country Canada. Thus,
we remove qualifiers/propositions and the words after them for
better term extraction. Qualifier removal is based on keyword
matching of from, in, of and has. To ensure correct matching, there
must be a space before/after the qualifiers, e.g. words in italic
are removed after the qualifiers: people from Alaska, reservoirs in
Idaho, mountains of Tibet, chair has four legs. This has the effect of
generalizing the concepts, which is perfectly reasonable for our
purpose of categorizing and browsing based on higher level of
concepts.

On the other hand, after initial experiments, we observed
that many LOD resources do not have information about label,
type and subject. The following statistics illustrate this problem.
According to LODStats10 that provides statistics about 2122 LOD
datasets, currently the LOD contains a total of 1,773,446,198
triples and 522,060,120 entities. Among these entities, 57.78%
contain type information (combination of rdf:type, dcterms:type,
dc:type and europeana:type), 53.75% contain label (combination
of foaf:name, dc:title and rdfs:label) and 5.63% contain subject
information. To improve lexical data mining, we apply a semantic
enrichment technique, where more lexical data is gathered from
the linked data graph of the resource by traversing owl:sameAs
and dbpedia:WikiPageRedirect links. For owl:sameAs links, we

9 www.umbel.org/specifications/annexes/annex-h.

10 http://stats.lod2.eu/stats.

M. Sah, V. Wade / Web Semantics: Science, Services and Agents on the World Wide Web 36 (2016) 3257

only follow to the directly connected entities (i.e. we do not
follow owl:sameAs links in a transitive way). If a resource has
such links, first we obtain RDF description of these resources
and apply the feature extraction techniques explained above.
Finally, from the obtained and enriched terms, stop words are
removed and stemming is applied, where we obtain the final
terms, which we call LOD terms. It is important to note that in our
benchmark dataset of 10,000 mappings from DBpedia, we used
320 dbpedia:WikiPageRedirect links and 22,836 owl:sameAs links
for the enrichment. This figure shows that DBpedia extensively
connects similar resources. Using our enrichment technique,
we improved the categorization performance 16% (please see
Section 5.1.2 for details).
LOD term weights based on features. Since different LOD terms
have comparative importance on the context of the LOD resource,
terms are weighted. For example, type and subject features provide
more discriminative terms. Therefore terms which appear in these
features should be weighted higher. To achieve this, we divided
LOD resource features into two groups: Important features (I) and
Other features (O). For important features, terms from type and
subject features are combined to form a vector. For other features,
terms from URI, label and property name are combined to form a
vector. We use the normalized term frequency (tf ) of these features
for term weighting as given below,

if t  I, w(t) = 0.5 + 0.5  tf (t)I


max(tf I )

(1)

if t  O, w(t) = tf (t)O
max(tf O)

where w(t) is weight of term t, and tf (t)I and tf (t)O is term
frequency of term t in important and other features respectively.
max(tf (t)I ) and max(tf (t)O) represent maximum term frequency
in those features. For terms that appear in important features (I), a
minimum weight threshold of 0.5 is used to encourage syntactic
matches to these terms within UMBEL concept descriptions for
categorization. On the other hand, inverse document frequency
(idf ) can also be used together with term frequency. However,
idf calculation for each LOD term is expensive. For dynamic idf
calculations, dynamic search on the LOD is required for each term,
which is computationally expensive. For offline calculations, we
need to continuously index the LOD, which is a resource-intensive
task. Thus, we did not use idf.

3.3. Semantic indexing of UMBEL concept descriptions

UMBEL provides highly structured descriptions of a broad
number of concepts, which can be used to represent the context
of LOD resources.
Semantic indexing model. The formal representation of concept
descriptions plays a crucial role in the effectiveness of the
IR system. In general, concepts representations are based on
extraction of keywords and the usage of a weighting scheme
in a vector space model (tf  idf ). This provides a simple but
robust statistical model for quick retrieval [26]. For indexing of
UMBEL concept descriptions, tf  idf weighting scheme is used
similar to other concept-based IR models [14,21,22]. Typically
these IR models utilize vector space representations of categories
(concepts), where the terms inside the concept description and the
terms inside sub-concepts are indexed and retrieved using tf 
idf scheme. However, such formal representations are extremely
simple and do not discriminate the terms that are semantically
more important to the concept based on the semantic structure of
concept hierarchy.

In our opinion, structured concept descriptions of UMBEL can
be indexed more efficiently by exploiting the semantic structure

of the concept descriptions. For instance, where the term appears
in a structured concept description (i.e. in a URI label, preferred
labels, alternative labels, sub-concepts labels or super-concepts
labels), should have a certain impact on the associated weight
of the term to the concept. Therefore, to produce more effective
concept representations, we propose a semantic indexing model
based on the different semantic parts of the concept.

In our approach, we divided concept descriptions into different
parts: concept URI labels (uri), concept labels (cl), sub-concept labels
(subl), super concept labels (supl) and all labels (al). A uri contains
terms that appear in the URI of the concept, where terms that
appear in a uri can be particularly important to the concept.
cl contain terms that appear in the preferred and alternative
labels of the concept. Hence most lexical variations of the concept
description are captured by cl. In concept-based IR, typically subconcept labels are also accepted as a part of the concept [14,21,22].
For instance, for the concept sports, sub-concepts baseball,
basketball, football, etc. may provide further relevant lexical terms
about sports. Instead of accepting sub-concepts as a part of
the concept, we separately index sub-concepts labels as subl. The
subl include terms that occur in all inferred sub-concepts URIs,
preferred and alternative labels. In addition, we observed that
many LOD resources contain links to super-concepts. For example,
a resource about a writer also contains information that writer
is a person. Thus, we index super-concept labels for more robust
descriptors, where supl contain terms that occur in all inferred
super-concepts URIs, preferred and alternative labels. Finally, al
contain all the terms that appear in all parts.

We use the vector space representation while indexing
different semantic parts, which allows scalable performance. This
provides a simple way of encoding key semantic knowledge
into IR retrieval model. We use only hierarchical relationships in
UMBEL as the ontology does not contain the semantic relatedness
relationships between the concepts. We alleviate this issue to
an extent by extracting data from subjects of LOD resources. For
example, semantically related LOD resources may share common
subjects, e.g. Pope and Vatican might share Christianity and
Catholic subjects. To include semantically related concepts into
the categorization, we associate each LOD resources to 3 UMBEL
concepts. We use the most confident concept (categorization with
the highest score) for lenses creation (e.g. Pope) and the rest
for semantic similarity comparison. Moreover, textual content
is extracted from abstract/labels of resources to generate term
vectors for combined semantic and syntactic similarity. Combining
semantic and syntactic similarity provides better results [27] when
the data is incomplete or poor quality (i.e. varying LOD quality).

For semantic indexing, we run the Algorithm 2. In particular,
UMBEL is formatted in RDF N-triple format and we load the triples
into a triple store (Jena persistent storage using Mysql DB) to
extract the terms from UMBEL concepts. Each concept is divided
into semantic parts of uri, cl, subl, supl and al using SPARQL queries,
where concept descriptions are extracted from each semantic part.
From the concept descriptions, stop words are removed, as they
have no semantic importance to the description, and words are
stemmed into their roots using the Porter stemmer. The resultant
words are accepted as concept terms. Finally, the extracted concept
terms from the semantic parts are indexed. To do this, we consider
each concept as a unique document and each semantic part is
separately indexed as a term vector under the document (concept)
using Lucene IR framework. In addition, the maximum normalized
term frequency and inverse document frequency term value of
each semantic part is calculated (which is subsequently used by our
retrieval model) and indexed together with the concept for quick
retrieval. The inverted concept index is used for categorization of
LOD resources.

Definition 1. The membership degree of the term, t, to each part,
p = [cl, subl, supl, uri], is a function , (t, c, p)  [0, 1], which
is based on tf  idf model. First, we calculate normalized term
frequency (ntf ) of t in cl, subl and supl of the concept c,

ntf (t, c, cl) = 0.5 + 0.5  tf (t, c, cl)


subl, supl  p,

max(tf (c, cl))
ntf (t, c, p) = tf (t, c, p)
max(tf (c, p))

(2)


where tf (t, c, p) represents term frequency of the term t in the
part p of the concept c and max(tf (c, p)) represents maximum
term frequency in the part p of the concept c. We calculate
local normalized term frequencies for each semantic part, rather
than calculating normalized term frequency using all terms of the
concept in all semantic parts. In this way, term importance for
a particular semantic part is obtained and frequent terms have
higher value. For cl a minimum threshold value of 0.5 is set, since
cl contains preferred/alternative terms of the concept, which is
important for the context of the concept c. Then, for each part, p,
we calculate the idf value of t in p,
cl, subl, supl  p,
(3)
here, n : t  p is the number of semantic parts that contain the
term t in p (i.e. n : t  cl) and C is the total number of concepts in
the collection. Again idf of a term in a particular semantic part is
calculated instead of idf of a term in the whole corpus. Rare terms
in a particular semantic part are assigned with higher values and
they are more important for the semantic part p. Next, tf idf value
of the term t in the semantic part p is computed,
cl, subl, supl  p,
tf  idf (t, c, p) = ntf (t, c, p)  idf (t, c, p).
(4)
Finally, the membership degree of the term t to each part p is
calculated as follows:

idf (t, c, p) = log

(n : t  p) + 1

cl, subl, supl  p, (t, c, p) = tf  idf (t, c, p)
max (tf  idf (c, p))
(5)
where (t, c, p)  [0, 1] equals normalized tf  idf value of
the term t in the part p. In this way, a fuzzy relevancy score
is generated, where the term that has the maximum tf  idf
value in the part p, (t, c, p) = 1 and (t, c, p) reduces as the
term importance decreases. As we discussed earlier, the maximum
tf  idf value for each semantic part is calculated and indexed
during the semantic indexing for better algorithm performance.

Last, (t, c, uri)  [0, 1] equals normalized term frequency,
(t, c, uri) = 0.5 + 0.5 

tf (t, c, uri)

tf (ti, c, uri)


(6)

i=1

here, term frequency of t in uri is divided by total number of terms
in the uri. A minimum threshold value of 0.5 is set, since uri terms
are important. If uri contains one term, (t, c, uri) = 1, means the
term is important for uri. The term importance decreases as the
number of terms in the uri increases.
Definition 2. Relevancy of the term t to the concept c, is calculated by, (t, c), where membership degrees of the term t to the
parts uri, cl, subl and supl are combined, (t, c) is given in Box I,
where, (t, c)  [0, 1] and, wuri, wcl, wsubl and wsupl are constant
coefficients that aid to discriminate features obtained from different parts. For example, the terms obtained from the uri and cl can
be weighted higher than subl and supl. The parameter values were
experimentally determined as we discuss later in the evaluations
section.

It should be also noted that concept descriptions can be enhanced with lexical variations using WordNet. However, typically UMBEL descriptions include such word variations in
alternative labels. This is the advantage of UMBEL being built
upon on OpenCyc since OpenCyc contains rich lexical concept descriptions using WordNet. In addition, UMBEL includes synonyms,
quasi-synonyms, lexical variations, plural, verb derivations and semantic related words of a concept in alternative labels. For the UMBEL concept http://umbel.org/umbel/rc/Automobile for instance,
preferred label is car and alternative labels are auto, automobile, au-
tomobiles, autos, cars, motorcar and motorcars. This demonstrates
a rich set of apparent lexical variations. Since these rich lexical
descriptions are available in UMBEL, we did not use other lexical
enhancement techniques because accuracy of the automated enhancements may affect categorization performance.

3.4. Categorization of LOD resources using a novel retrieval model

In this step, we match the extracted LOD terms (as we discuss
in Section 3.1) to UMBEL concept descriptions (as we discuss in
Section 3.2). In traditional IR, tf  idf is used to retrieve relevant
concepts, i.e. each term in a concept has an associated value of
importance (i.e. weight) to that concept and important terms are
those that frequently occur inside a text but infrequent in the
whole collection (tf  idf ) [26]. Since we represent each UMBEL
concept as a combination of different semantic parts (rather than
one document), we need to calculate term relevancy to each
semantic part. Then individual part relevancies can be used for a
final relevancy score calculation. For this purpose, we propose a
fuzzy retrieval model, where relevancy of a term, t, on concept, c,
is calculated by a function, (t, c)  [0, 1], using semantic parts of
the concept and an extended tf  idf model.
Fuzzy retrieval model. First, UMBEL concept candidates are
retrieved by searching LOD terms in all labels (al) of concepts. Then,
for each LOD term, t, a relevancy score to every found UMBEL
concept, c, is calculated by using a fuzzy function, (t, c)  [0, 1],
on uri, cl, subl and supl semantic parts (since different parts have
relative importance on the context of the concept c). Thus, (t, c)
shows the degree of membership of the term t to all semantic parts
of the concept c; where high values of (t, c) show that t is a good
descriptor for the concept c and (t, c) = 0 means that the term t
is not relevant for c. For membership degree calculation of (t, c),
first membership degree of the term, t, to each part, p, should to be
computed.

M. Sah, V. Wade / Web Semantics: Science, Services and Agents on the World Wide Web 36 (2016) 3257

(t, c) = wuri  (t, c, uri) + wcl  (t, c, cl) + wsubl  (t, c, subl) + wsupl  (t, c, supl)

wuri + wcl + wsubl + wsupl

(7)

Box I.

Definition 3. Finally, relevancy of all LOD terms, T = {t1, . . . , tm},
to the concept c is,

i=1

(T , c) =

((ti, c)  w(ti))

w(ti)

i=1

(8)

where, (T , c)  [0, 1] and w(ti) is term importance of LOD term
ti, which is calculated by Eq. (1). Using term weights, w(ti), term
matches to the important LOD terms are encouraged (i.e. terms
with higher weights). In addition, concepts that have a good
coverage of LOD terms especially in important semantic parts
(which is determined by coefficients in Eq. (7) in Box I), will have a
higher relevancy score.

Definition 4. Finally, the concept with the maximum (T , c) is
selected as the main categorization of the LOD resource. In cases
where there are two or more concepts that have the maximum
value, the main categorization is decided based on an ontological
relationships driven voting algorithm. The algorithm is as follows:
For each concept with the maximum (T , c); (1) we find the
number of sub-concepts (n), (2) sub-concepts (sc) count, k, that
have non-zero relevancy value to LOD terms, (T , sc) > 0, (3) total
membership degree, , of all sub-concepts that (T , sc) > 0. The
voting score (v) of the concept is computed as, v =   (k/n),
which means we encourage the concept whose sub-concepts have
higher scores as well as the concept with a greater number of subconcept matches. After the voting, the concept with the maximum
v is accepted as the categorization. If there is still more than one
maximum, we accept all concepts as categorizations, since a LOD
resource may belong to one or more concept, e.g. Kyoto is a city as
well as a district.

4. Search results personalization

After the categorization, results with the same most confident
categorization are grouped together and presented with concept
lenses as shown in Fig. 3. Lenses are presented static on the page,
thus users can access the lenses even if they scroll the results page.
Using the lenses, users can explore the search results from different
conceptual perspectives. This is particularly useful for searching or
information gathering in unknown domains. Users can get a sense
of what type of concepts the results belong to and can narrow
down the results. In addition to this, we adapt the presentation of
results to individual users based on their interactions with concept
lenses and results (i.e. click onto a lens or result). This involves
adding more relevant results into the concept lens that the user
is currently browsing; re-ordering lenses according to similarity;
suggesting relevant concept lenses for further exploration. In order
to achieve this, first we need to gather user data from system
interactions and represent this information in some form of user
profile as we discuss in Sections 4.1 and 4.2 respectively. Then, we
explain the different personalization features that we support in
Sections 4.34.5.

4.1. Context-based user data gathering

In our approach, we use context-based user modeling rather
than background knowledge. Only click data within the current

search session is used to adapt to users local choices. Search
session starts is when the user opens the retrieval interface and
session ends when she closes it. The system is able to cope
with changes of search domain from categorization. Suppose the
user refined a query; it is probable that similar concepts/super
types will occur in new search results. However, if the search
topic changes completely, categorization in 25,000 UMBEL
concepts will not be the same, thanks to the use of whole
concepts. Fortunately, super types can be used to understand users
general interests even if search topic change. Our approach is
complemented by categorization and interactive personalization
as follows: Suppose the user is interested in concept x and clicked
onto a promising result in this lens. After a quick investigation,
she deems the result irrelevant. However, this negative feedback
is still very valuable thanks to categorization. By analyzing the
last N clicks of the user on concepts/supertype concepts and the
system can find similar LOD resources that share related concepts.
In addition, we developed an interactive personalization where
any feedback can be used. On click to a concept lens or a result,
immediate personalization is supported such as lenses re-ranking,
results re-ranking and query expansion.

4.2. User profile representation

For user profiling, we track: (i) clicks onto concept lenses and

(ii) clicks onto last N results.
User concept lens choices: When a user clicks onto a concept lens,
the results are adapted based on users local choices. In particular,
conceptually more relevant lenses are pushed on top of the list.
Hence users can explore results that have similar concepts to the
selected lens. To achieve this, accurate personalized re-ordering of
lenses is important. Thus, a robust and efficient similarity measure
is essential for personalized lenses re-ranking. In our approach,
we use a hybrid similarity measure by combining hierarchical
structure of the UMBEL vocabulary and shared statistical data
between resources. We adopted vector space representation of
the ontology [2830] that allows efficient and scalable similarity
compared to more complex description logic-based approaches.
Since, performance is vital for on-time personalization.

Definition 5. To represent user interests on concept lenses, first
information about all results under a concept lens are combined
to represent concept lens with; (a) vector of UMBEL concepts,
i.e. user interests to specific concepts in a 5-depth hierarchy
of 25,000 concepts. Unlike general approach in personalized IR,
we represent results with very specific UMBEL categorizations.
(b) vector of super type concepts, i.e. top-level concepts to represent
broad user interests. (c) vector of terms, i.e. terms extracted from
results snippets such as title, url keywords and descriptions of the
concept lens. Stop words are removed and terms are stemmed for
comparison. Users interest for a concept lens is represented with
three vectors. Suppose search results contain m concept lenses, l.
Each concept lens, l, contains n results, r. Each result is represented
with up to k UMBEL concepts, c, and their associated super types, sc
(k = 3 in experiments). The vector of concepts, and vector of super

type concepts, of lenses are calculated as follows:

ricj  Vc(lz ) = (w(c1, lz ), . . . , w(ct , lz ))

lz = n

(9)

z=1

i=1

j=1

lz = n

riscj  Vs(lz ) = (w(sc1, lz ), . . . , w(scv, lz )) (10)

i=1

j=1

z=1
where it is the sum of all concepts and super type concepts of all
results under a concept lens. Here, each dimension of w(c1, lz ) or
w(sc1, lz ) corresponds to a separate UMBEL concept or super type
concept and its weight. We use concept/super type frequency as
weight, i.e. if a concept does not occur in the concept lens, the
value is 0. Generally term frequency, inverse document frequency
(tf  idf ) is used for weighting. However, our studies show
that the frequency, tf , works better tf  idf . idf weights rare
terms (or concepts) higher. This works well for retrieval, but not
for similarity comparison as we compute the shared information
between the lenses. In the same manner, results snippets are
combined to form a vector of terms of the concept lenses as shown
in Eq. (11). Each dimension corresponds to a unique term and its
weight. Again we use the term frequency as weight.

lz = n

z=1
 Vt (lz ) = (w(t1, lz ), w(t2, lz ), . . . , w(ts, lz ))

i=1

rit

Definition 6. To represent user interests, we track the users
information need from the clicked results, which are then used
for results re-ranking and query expansion. For this purpose, we
track the last M clicks of the user, u, and generate three types of
vectors to represent the users information need: vector of concepts
(specific interests), vector of super types (broad interests), and
vector of terms (language model). In this case, vectors are extracted
from the last M results clicks as shown below:

rick  Vc(u) = (w(c1, u), . . . , w(ctt , u))

risck  Vsc(u) = (w(sc1, u), . . . , w(scvv, u))

j=1
rit  Vt (u) = (w(t1, u), . . . , w(tss, u))

(12)

(13)

(14)


j=1

i=1


i=1

i=1

4.3. Re-organization of concept lenses and concept lenses suggestion

Dynamic adaptation of results to the users local choices is the
most innovative personalization provided by our system, which
moves conceptually relevant concept lenses to the top of the list.

Definition 7. For concept lenses re-ranking, we compare the
similarity of the selected concept lens to other concept lenses using
the cosine similarity of concept, super type and term vectors:

sim(l1, l2) = V1(l1)  V2(l2)
| V1(l1)|| V2(l2)|
(15)
where sim(l1, l2)  [0, 1], numerator is the inner product of the
vectors and the denominator is the multiplication of the vector
magnitudes. We generate three similarity scores for each concept
lens, namely c_sim, s_sim and t_sim according to their similarity
to the selected concept lens, ls. The concept similarity (c_sim)
compares similarity of shared specific concepts, i.e. if lenses share
more specific concepts, it is more likely that they are relevant.
The supertype similarity (s_sim) computes shared broad concepts.
For example, mountain and lake concept lenses have the same

M. Sah, V. Wade / Web Semantics: Science, Services and Agents on the World Wide Web 36 (2016) 3257

supertype concept and they broadly related. Finally, the term
similarity (t_sim) allows comparing language models of the lenses.
This information can be noisy since different resources may share
similar meanings but may use different terms. However, still term
similarities can be used to guarantee some level of similarity
between lenses.

Our evaluations on a benchmark dataset [19] showed that
the concept similarity of lenses provided the best precision @top
N concept lenses compared to super type and term similarities.
In addition, when different similarity scores were combined,
precision was improved (see Eq. (16)). In particular, when the
influence of the c_sim was weighted higher than the s_sim and
t_sim, the best precision @top N concept lenses was obtained [19].
Especially the best results were obtained when  = 2,  = 1 and
 = 1. Please refer to [19] for more information about precision
@top N for different similarity measures and personalization
strategies.

sim(l, ls) =   c_sim(l, ls) +   s_sim(l, ls) +   t_sim(l, ls)

. (16)

 +  + 

(11)

Finally, concept lenses are re-ranked in decreasing sim(l, ls)
order. By default, the selected lens is pushed on top of the list since
cosine similarity of a vector to itself is 1.

Definition 8. To suggest concept lenses, lenses similarity is uti-
lized. In particular, if a concept lens similarity is above a certain
threshold, such as sim(l, ls) > 0.2 in the experiments, the concept
lens is suggested for exploration as shown in Fig. 3. In this way, conceptually relevant results can be found under the suggested concept lenses, which can aid results exploration. Lenses suggestions
are presented static at the right hand side of the screen. Thus users
can access these suggestions, even if they scroll down the page.

4.4. Results re-ranking and concept lenses suggestion

For results re-ranking, each result is represented with a
vector of concepts, super types and terms as follows: Vc(r) =
(w(c1, r), . . . , w(cx, r)), Vsc(r) = (w(sc1, r), . . . , w(scw, r)) and
Vt(r) = (w(t1, r), . . . , w(ty, r)).
Definition 9. Results re-ranking is applied in two cases: (a) when
the user selects a concept lens from the results list for exploration
and (b) when the user clicks onto a result (LOD resource) within
a concept lens. In both cases, the re-ranked results are included in
the context of the interacted concept lens. This allows in context
results exploration thanks to the use of concept lenses. In case (a),
we compare concept vector, of the selected concept lens (ls) with
the top K results using Eqs. (15) and (17);

sim( Vc(ri), Vc(ls)).

(17)

i=1

We compare concept vectors since results matching at specific
UMBEL concepts are more likely to be relevant compared to super
type or term similarities (since we only have users interest for
a concept lens). In our experiments, K = 100, for a scalable
performance. Results, where sim( Vc(r), Vc(ls)) > , are re-ranked
in decreasing order and added into the interacted concept lens.
 = 0; any match was considered because of specific concept
vectors comparison. Later,  can be determined experimentally.

In case (b), we use the click history of the user within the
current search session to re-rank results. In particular, from the
last M results clicks of the user, users specific concept, super
type and term interests are represented as vectors as shown in

i=1

M. Sah, V. Wade / Web Semantics: Science, Services and Agents on the World Wide Web 36 (2016) 3257

  sim( Vc(ri), Vc(u)) +   sim( Vsc(ri), Vsc(u)) +   sim( Vt(ri), Vt(u))

 +  + 

Box II.

(18)

Eqs. (12)(14). These vectors are compared with top K result
vectors using Eqs. (15) and (18) (Eq. (18) is given in Box II), where
three similarity scores are combined for re-ranking of the results in
decreasing order. Especially,  = 2,  = 1 and  = 1 give better
results. Again, a threshold can be used to select relevant results
conservatively, i.e. higher thresholds.

Definition 10. During results re-ranking,
if a relevant result
belongs to another concept, then the concept lens is suggested for
exploration within the interacted lens.

4.5. Query expansion using concept labels

Definition 11. Query adaptation is applied in two cases: (i) when
the user selects a concept lens from the results list for exploration
and (ii) when the last two consecutive result clicks share the same
concept. In both cases, we assume that the user is interested in this
concept and we expand the original query with the concept label
that the user is interested. It is a simple approach, but works well
since UMBEL categorizations provide very specific concept names
and it can be used to clarify the meaning of the query with the user
feedback. In both cases, more results are included in the context
of the interacted concept lens, so that the user can explore more
relevant results in context.

4.6. Discussion of personalized search limitations

One can argue that results re-ranking is applied to the top K
results and only those concepts lenses belonging to the top K
results. In personalized search field, using the top K results for
results re-ranking is an accepted methodology. Since, relevancy of
the results to the user query drops significantly beyond a certain
threshold. The popular search engines also use this methodology;
they present the top K results (e.g. 20) in the first page and the
user needs to click on the next set of results for viewing. Since,
relevancy to the query decreases significantly beyond a certain
threshold, processing all results does not add much value to the
personalization as well. Therefore, in our approach, we use the top
K results for scalability. In evaluations section, we assess effects of
increasing K values (K = 50, K = 100 and K = 200) on the system
performance.

On the other hand, one can argue that our personalization
approach is strongly dependent on an already good performance of
the underlying retrieval model to retrieve reasonable top K results
for re-ranking. We agree that results re-ranking performs well if
the top K results are good. However, we do not just apply results reranking for personalization. Instead, we develop personalization
strategies to combine various personalization techniques; first
according to the user click on a concept lens, all concept lenses
are re-ranked. This means that on one click of the user, we rerank all concept lenses hence all results according to relevancy
to the selected concept lenses. Then, we apply results re-ranking
within the selected concept lens. In our case of results re-ranking,
the user already specified that s/he is interested in this concept by
clicking on the concept lens. If the search results are good, results
re-ranking can push relevant results within the selected concept
lens on top of the list. If the search results are poor, then results
re-ranking cannot improve. However, please note that we also
immediately apply query expansion by using the concept labels

of the selected lens. Thus, the latter case can be improved even
the search results are poor. UMBEL vocabulary provides rich set of
synonyms and related words from WordNet, which are utilized for
retrieving more relevant results using query expansion.

In summary, we are not just re-ranking results or concept
lenses. Instead, we developed personalization strategies and aimed
to alleviate limitations of different personalization methods by
intelligently combining various personalization techniques using
a concept-based results presentation and interaction approach.

5. Evaluations

This section discusses the experiments that were undertaken to
test the performance of our approach. Section 5.1 discusses categorization performance evaluations since incorrect categorizations
significantly affects system usability and personalization efficacy.
For this purpose, a particular benchmark was created to eval-
uate: (1) categorization performance of different LOD features,
(2) categorization accuracy of our retrieval model against the vector space model, (3) categorization efficiency of system perfor-
mance. Section 5.2 explains the user studies that we undertook in
order to assess system performance and user satisfaction with the
personalized concept-based search interface compared to a
non-adaptive baseline search system. The user study consisted of
a task-based evaluation using real life user information needs regarding to a tourist domain. Section 5.3 discusses personalization
performance evaluation. For the practical applicability of our ap-
proach, it is important to evaluate dynamic personalization effi-
ciency. Section 5.4 presents a comparison of our approach with the
existing LOD search engines.

5.1. Categorization performance

5.1.1. Setup

The performance of the concept-based search depends on
the accuracy of the categorization algorithm, because incorrect
categorizations can degrade user experience with the search
mechanism. Thus, categorization accuracy is crucially important
and it needs to be evaluated.
Dataset. Fortunately, many DBpedia LOD resources (900,000)
have mappings to UMBEL concepts and DBpedia publishes these
links in RDF N-Triple format (http://dbpedia.org/Downloads).
According to UMBEL website,11 most of the UMBEL to DBpedia
mappings were manually generated, which was assisted by various
NLP systems and scripts. The mapping process was as follows:
Firstly extraneous internal DBpedia categories were cleaned by
using specific patterns and formats, which resulted in 80,000
cleaned DBpedia categories to represent true knowledge structure.
Subsequently, class-level mappings were performed and verified
by hand. Verification was completed via testing with reasoners
(Pellet, Fact++ or other) for consistency against the UMBEL
supertypes restrictions. Instance level mappings were performed
manually or semi-automatically using the following: (1) Instances
inherit DBpedia ontology class mappings. (2) DBpedia categories
were cleaned further using the so called semantic vectors

11 www.umbel.org/specifications/annexes/annex-h.

features and models. In evaluations, if the predicted categorization
directly matches to any of UMBEL concept mappings or the
predicted categorization is a super-concept of a UMBEL concept
mapping, then the categorization is accepted as correct. Superconcept mappings are also accepted as correct since it is intuitively
and logically true. For example, if x is a umbel:Cyclist, it is also true
that x is a umbel:Athlete or if x is a umbel:Bird, it is also true that x
is a umbel:Animal.
Proposed fuzzy retrieval model. Fig. 4 shows precision and recall
of our retrieval model: (1) with different LOD resource features
and (2) with and without the semantic enrichment technique.
The results show that among all LOD resource features, the type
feature alone gave the best F-Measure of 71.94% and 87.50%
without and with the enrichment respectively. This is because
most resources contain type feature, which provides knowledge
about the context of resources. subject feature performed an
F-Measure of 61%, uri (23%) and label (21%) features alone
did not perform well and property names performed the worse
accuracy. Among combinations of different LOD resource features,
type + uri and type + label provided the best accuracy without
the semantic enrichment with an F-Measure of 85.32% and 86.28%
respectively. Other combinations did not improve the overall
accuracy despite more LOD terms being used in the categorization.
Another interesting outcome is that the semantic enrichment
technique did not have a significant impact on the categorization
accuracy (1% improvement) except the type feature. In the type
feature, the enrichment technique improved the F-Measure16%.
In addition, we noticed that in some cases all possible mappings
from DBpedia to UMBEL are not included, e.g. a volcano mountain
is mapped as umbel:Mountain, but not as umbel:Volcano. Besides,
DBpedia uses more general mappings, for example, a science
fiction writer is mapped as umbel:Writer, despite the existence of
umbel:ScienceFictionWriter. This could be because of human error
since manual mapping process12 is involved, which can be error-
prone. Although these particular cases affected categorization
accuracy, our retrieval model achieved high accuracy on the
benchmark. Especially high performance is achieved by using the
type feature and the type + uri and type + label features (with
and without the enrichment). The results are promising because
typically LOD resources contain data about type and labels of the
resource, which can be used to provide high quality categorization.
Vector space model. Since our retrieval model extends tf  idf
with a fuzzy relevancy score calculation using semantic structure
of concepts, we compared the categorization accuracy against the
tf  idf model. In vector space model, concept descriptions can be
represented in a number of ways; using (1) only uri, (2) uri + cl,
(3) uri + cl + subl, and (4) uri + cl + subl + supl. On the concept
representation alternatives, we applied tf  idf retrieval model on
the benchmark. For fair comparison, the same clean-up steps are
applied to the vector space model (i.e. stemming, stop word and
qualifier removal) and the same voting algorithm is used if there
is more than one maximum categorization. In contrast, concept
weights to all LOD terms are calculated using the tf  idf scheme.
In Fig. 5, the best results are shown, which is achieved by the
type feature. Results show that the vector space model did not
perform well. The best results are obtained by using uri+cl with an
F-Measure of 38.04% without the semantic enrichment and with an
F-Measure of 39.09% with the semantic enrichment. When using
all semantic parts, the F-Measure of the vector space is decreased
to 21.54% compared to 87.50% F-Measure of the proposed retrieval
model, which uses all semantic parts.

dbpediaOntology.n3.

http://umbel.googlecode.com/svn/trunk/v100/External%20Ontologies/

Fig. 4. Categorization accuracy of the proposed retrieval model with respect to
different LOD resource features and the semantic enrichment technique.

approach, and then from the cleaned categories, mappings were
performed by hand. (3) A script analyzes the DBpedia category
structure and suggests candidate UMBEL concepts. Subsequently
mappings were performed by hand. (4) Finally, existing hand
inspected OpenCyc to DBpedia mappings were used from the
OpenCyc knowledge base. As a summary, DBpedia mappings were
rigorously validated by hand in order to generate high quality
mappings and this dataset provide a reasonable ground truth for
our evaluations.

Since our aim is to test performance of various features and
different algorithms, the use of whole mappings is computationally expensive. Instead, we created a particular benchmark of
10,000 mappings from the provided DBpedia links. The selection procedure for the benchmark was as follows: We randomly
selected from different types of UMBEL concept mappings (e.g. um-
bel:HockeyPlayer, umbel:Plant, etc.) and those DBpedia resources
that are cached by the Sindice Cache. This resulted with 10 227
benchmark resources. Then, RDF descriptions of 10,227 benchmark resources and resources that are linked from those resources
using owl:sameAs and dbpedia:WikiPageRedirect links (2 per re-
source) were cached to a local disk for context extraction. In addi-
tion, during the feature extraction, links to UMBEL concepts were
ignored. More information about this dataset such as the used DBpedia URIs, ground truth mappings and generated mappings using
various semantic features (with and without the semantic enrichment technique) can be found at [31].
Optimizing parameters. In the experiments, the parameter values
of wuri = 2, wcl = 2, wsubl = 1 and wsubl = 1, as well as,
the fixed weight of 0.5 in Eqs. (2) and (6), were experimentally
determined to achieve the best results. In particular, we randomly
selected 500 mappings from the benchmark and chose the values
that gave the best precision/recall. With these parameter values,
URI and concept labels have the highest impact and, sub-concept
and super-concept labels contribute moderately.

5.1.2. Categorization accuracyprecision, recall, F-measure

By accepting the DBpedia to UMBEL mappings as ground truth,
precision (P), recall (R) and F-Measure (F) of the automatically
generated categorizations are calculated. Precision equals to the
number of correctly predicted (CPr) divided by total number of
predictions (Pr), P = CPr/Pr. Recall equals to the number of unique
correct predictions (UPr) per resource (since our algorithm may
predict one or more categorizations for each resource) divided by
total number of mappings (T = 10 227), R = UPr/T. F-Measure
is the weighted harmonic mean of precision and recall, F = (2 
P  R)/(P + R). We used F-Measure for the comparison of different

M. Sah, V. Wade / Web Semantics: Science, Services and Agents on the World Wide Web 36 (2016) 3257

Fig. 5. Comparison of categorization performance.

Discussion of results. Our retrieval model performs outstandingly
better than the vector space model for the following reason. tf idf
is a robust statistical model, which works well with good training
data. Traditional concept-based IR systems [14,21,22] use the top
23 levels of a concept hierarchy (few hundred concepts) with
hundreds of training documents. In contrast, we use the whole
25,000 UMBEL concepts. Moreover each concept contains few
lexical information in different semantic parts of the concept, such
as in URI, preferred/alternative labels and super/sub-concept(s)
labels to describe that concept. tf  idf cannot discriminate
terms only using combined terms and often few LOD terms are
matched to many concepts (sometimes hundreds) with the same
tf  idf scores. We propose a more intuitive approach, where
our retrieval model extends tf  idf with a fuzzy relevancy score
calculation based on semantic structure of concepts, i.e. terms from
the concept, sub-concept(s) and super-concept(s) have certain
importance in retrieval. Besides, relevancy scores are combined
according to their importance to the concept. Hence, this more
intuitive approach performs astoundingly better than tf  idf ,
which do not discriminate term importance based on semantic
structure of a concept hierarchy.

5.1.3. Computational efficiency of dynamic categorization

In addition to high accuracy, dynamic categorization performance is an important factor for the concept-based search. To provide fast categorizations, each search result (resource) is processed
in parallel using AJAX. In addition, for scalability, categorizations
are indexed at the server side. To give an idea of dynamic (online)
categorization times, we measured average processing times based
on number of LOD terms per resource using a laptop with Windows
7 operating system, 4 GB RAM, Intel Core 2 Duo CPU (2.53 GHz) and
54 Mbps Internet connection. Without the enrichment, average
processing times vary between 1 and 1.5 s for our approach compared to 0.10.5 s of the vector space model. With the enrichment,
processing times increase for both model, because of dynamic
caching from LOD graphs. We found that an average of twelve LOD
terms are extracted from the benchmark resources, which means
we can perform categorization within 1 s and 1.5 s with and
without the enrichment respectively (see Fig. 6).

5.2. Personalized vs. non-adaptive retrieval performance

One of the main questions that should be investigated is if the
provided personalized concept-based search performs better than

Fig. 6. Dynamic algorithm performance with respect to number of LOD terms.

other retrieval algorithms and semantic search approaches?. In IR
community, precision at top N results is used to assess this retrieval
performance. Since lazy users usually browse the top results and
improvements at the top of the results lists are encouraged. Thus,
we adopted this approach.

In order to assess personalized search efficacy, we measured
precision @top M concept lenses and @top N results for the personalized concept-based search. Then, we compared these results
to three ranking/retrieval techniques: (a) non-adaptive conceptbased search; (b) non-adaptive term-based retrieval (tfidf ); and
(c) non-adaptive ad-hoc retrieval method BM25F [32]. In this way,
we evaluated the retrieval performance of our approach against the
related work. In particular, in our approach we apply term-based
retrieval using tfidf and categorize the search results accordingly.
Therefore, our personalized search approach can be compared with
the non-adaptive ranked list scoring using tfidf. In addition, another non-adaptive search approach is concept-based search without the added personalization, which can be compared with the
personalized case. Here, the non-adaptive concept-based search
presents the results without adaptation to the selected concept
lens, i.e. there is no lenses re-ordering, results re-ranking and query
expansion. Finally, in IR community, for scoring and retrieval of
structured documents, BM25F ranking is used [32]. BM25F is a
parametric scoring function, which uses different boosting factors
and parameters in order to include various semantic fields into the
scoring function. Thus, the retrieval algorithm benefits from the
semantics of the structured documents. As a result, BM25F represents a semantic search approach.

Dataset and experimental setup. We used a tourism domain to
create a benchmark dataset using LOD resources. We selected
a tourism domain since it suits well for data gathering and
informational queries; the user generally has a vague idea about
queries and gradually refines queries to gather/explore more
information. In addition, the tourism domain can be used to find
direct answers to specific search queries (i.e. users looking for a
specific resource).

Our dataset is about tourism in Killarney Ireland and it was
created as follows: First, we investigated popular search queries
about the domain from Google search trends. Then, the popular
search queries were used to query WoD with Sindice to gather
data about available URIs. Particularly 500 URIs from DBpedia,
GeoNames, Trip Advisor and ookaboo domains were selected.
Initially, we selected 30 queries, which do not have a direct answer,
i.e. navigational queries were not selected, such as Killarney
Victoria Hotel. Top concept lenses and results returned by the
queries were manually assigned relevant or irrelevant by three
independent judges. Judges were academics (two post-docs and
a professor) in the field of IR. Among 30 queries, we selected 20
queries for the evaluation; where the inter-rater agreement of
judges was 100% for the best concept lens, as well as, we manually
selected the results where the inter-rater agreement was 100%. The
dataset and results can be found at [31]. Although 20 queries is a
small sample set, such sizes have been used before to determine
indicative results in semantic search [28,33].

We used Lucene for implementing tfidf and BM25F scorings.
As suggested by authors [32], we use the following boost factors
and parameter values for BM25F: text = 1, title/url = 3, inlinks =
2, obj = 2 and type = 2, K1 = 1.7, btitle = 0.4, binlinks = 0.4,
btype = 0.4, bobj = 0.4 and btext = 0.4. Both tfidf and BM25F
rankings were presented as ranked lists without any adaptation.

5.2.1. Performance of personalized concept-based search

In the experiments, personalization was performed after the
users concept lens selection following a query. In the evaluations,
three judges decided on the best concept lens, which represented
the search query. To evaluate the efficiency of personalized lenses
re-ranking, first precision at top M concepts was measured, which
was adopted by [13]. Precision at top M concepts is: P@M =
R@M/M, where R@M is the number of concept lenses which have
been manually tagged relevant among top M concept lenses. For
ambiguous concept lenses, if the majority of results under the
lens were relevant, then the judges assigned relevant. We use
P@1, P@3, P@5, P@10 and P@15, since lazy users browse the
top concept lenses. The results in Fig. 7 show that for lenses
re-ranking, lenses concept vector similarly provided the best
precision compared to supertype and term vector similarities.
When various similarity scores were combined, the precision was
improved (in the experiment, influence of concept similarity is
higher than others, e.g.  = 2,  = 1 and  = 1). The
best personalized lenses re-ranking was obtained when concept,
supertype and term vector similarities were combined.
In a similar manner, we measured precision at top N results
for different personalization strategies: P@N = R@N/N, where
R@N is the number of results which have been manually tagged
relevant among top N results as shown in Fig. 8. The results showed
that lenses re-ranking significantly improve precision @top N
results. Combined lenses re-ranking with results re-ranking or
query expansion improve lenses re-ranking performance. This also
shows that personalized re-ranking of results and query expansion
with concept lens label work well. When all personalization
strategies were combined, the best results were obtained, where
100% precision at P@3, P@5 and P@7 were obtained on the tested
20 queries.

Fig. 7. Precision @top N concept lenses for all queries.

Fig. 8. Precision @top N results for all queries.

5.2.2. Comparison with non-adaptive concept-based search and nonadaptive retrieval methods

We compared the retrieval performance of personalized search
performance against non-adaptive concept-based, non-adaptive
tfidf based ranked results list and non-adaptive BM25F ranked
results list. Here the ranked result lists uses the original rank of
the result and present it without categorization. First, we evaluated
non-adaptive concept lenses ordering since lenses can be ordered
in two different ways; (a) the minimum result rank within a lens,
or (b) average of all results ranks within it. Results in Fig. 9 show
that both cases provide similar results. However, for the minimum
rank order, P@1 is slightly better than the average rank. Thus, we
used the minimum order for comparison with personalized lenses
re-ranking. The personalized re-ordering of lenses significantly
improved precision @top M concept lenses compared to the nonadaptive concept lenses as shown in Fig. 10.
Finally, we compared our personalized search on precision @top N
results against non-adaptive concept-based search, non-adaptive
tfidf based ranked results list and non-adaptive BM25F ranked
results list (Fig. 11). The results showed that our personalized
search outperforms precision at all levels compared to all nonadaptive results presentations. We also noticed that the semantic
search approach, BM25F, which uses type, title, url, etc. in the
retrieval model, only slightly improved the retrieval performance
at P@15 and P@20 against the tfidf scoring. In addition, nonadaptive concept-based search performed similar to non-adaptive
ranked results lists at P@3, P@5, P@7 and P@10. However, its
precision dropped significantly as more results were added. This
shows that concept-based search alone did not perform well and
that concept-based search is effective if it is personalized, which
was an interesting finding.

M. Sah, V. Wade / Web Semantics: Science, Services and Agents on the World Wide Web 36 (2016) 3257

information as quickly as possible. The hypotheses regarding the
task assistance are as follows:
 H1: Personalized concept-based search system better assists a
users search for information than a non-adaptive baseline search
system.
 H1.1: The personalized concept-based search system allows
users to be more efficient in terms of user effort for task
completion. We use task completion times, number of issued
queries and users overall page view counts to test this
hypothesis.
 H1.2: The personalized concept-based search system allows
users to be more effective for task completion than a nonadaptive baseline system. We use the users measured and
perceived task accuracy to test this hypothesis.

The null hypothesis for H1.1 and H1.2 is that there are no differences between the personalized concept-based search system and
the non-adaptive baseline search system.
User satisfaction can be viewed as the perceived usability of
various functionalities provided by the personalized conceptbased search system. The assumption is that users perceive the
personalized concept-based search system to be more helpful
for completing the given tasks than the non-adaptive baseline
search system. In addition, users recognize and value the various
functionalities of the system. The hypotheses regarding the user
satisfaction are as follows:
 H2: Users are more satisfied with a personalized concept-based
information composition system than a non-adaptive baseline
search system.
 H2.1: The personalized concept-based search system outperforms the non-adaptive baseline search system in usabil-
ity. Application specific and usability questionnaires [34] are
used to assess this hypothesis.
 H2.2: Users recognize and value the categorization and personalization aspects of the system. Usability questionnaires and
user interactions with personalized concept lenses/results
are used to test this hypothesis.
 H2.3: Users find the personalized concept-based search
system to be more motivating and engaging than the nonadaptive baseline search. Questionnaires are used to test this.
The null hypothesis for H2.1H2.3 is that there are no differences
between the personalized concept-based search system and the
non-adaptive baseline search system.

5.3.2. Comparison to baseline

In order to test our hypothesis, a baseline search system is
required for a comparison. Generally linked data search engines
provide users with keyword-based query interfaces such as
Sindice, Swoogle, OKKAM and Watson. These systems present
the results as a ranked list based on relevance to the users
query.
In addition, none of the current LOD search engines
provide adaptation. Therefore, as a baseline, a purpose-built
non-adaptive baseline search system was generated to compare
with the personalized concept-based search system. For a fair
and competitive comparison with the personalized system, the
baseline system (i) uses the exact same underlying indexing and
retrieval models (both tfidf ranking), (ii) operates across the same
content base (benchmark dataset as described below), (iii) allows
free-text queries as our search system and (iv) results are ranked
according to relevancy to the query as with our system, hence
stimulating the real life Web search conditions. In Fig. 12, a screen
shot of the baseline system is shown. The only difference is that
personalized concept-based search system categorizes and adapts
the results to the users current browsing history and the baseline
system does not.

Fig. 9. Precision @top M concepts for non-adaptive lenses ordering.

Fig. 10. Precision @top M concepts; personalized vs. non-adaptive lenses ordering.

Fig. 11. Precision @top N results; personalized vs. non-adaptive search.

5.3. User studies

In order to evaluate the personalized concept-based search ap-
proach, a user study was performed using task-based information
seeking tasks in a tourism domain. In particular, our approach was
evaluated in terms of (i) the ability to support users in completing
the tasks and (ii) the usability from the users perspective (i.e. user
satisfaction). In the study, our approach was compared with a baseline non-adaptive search system in order to assess the added value
of the personalized concept-based search paradigm.

5.3.1. Hypothesis

In the user study, the aim is to test the following hypothesis:

Task assistance is the systems ability to assist a users search
for information. In particular, how effective and efficient is the
system in solving search tasks. It is desirable that a system requires
users to invest the least amount of effort in order to find relevant

Fig. 12. Baseline search system for the query killarney national park.

5.3.3. Experimental setup
Dataset. Our evaluation is based on task-based information
seeking tasks in our tourism dataset, namely tourism in Killarney
Ireland. Particularly 500 URIs from DBpedia (6.80%), GeoNames
(12.04%), Trip Advisor (72.77%) and ookaboo (8.37%) domains were
selected. Please note that in this dataset, we specifically selected
resources from different LOD dataset, thus we can illustrate that
our approach can work on different domains. RDF descriptions
of the URIs, their UMBEL and super type categorizations were
indexed offline by our retrieval model (as we explained in the
previous section) to carry the experiments. In the experiment,
we re-directed DBpedia URIs to their respected Wikipedia pages
for human friendly presentations. Trip Advisor, Geonames and
ookaboo domains had human friendly HTML pages thus we did not
alter URIs. Furthermore, in the experiment, K is set to 100; meaning
that we use the top 100 results and the concept lenses belonging
to those results in order to apply results re-ranking and concept
lenses re-ordering.
Tasks. The search tasks were inspired from the popular search
queries.
In particular, tasks are related to available tourist
attractions in the area, particular type of attractions that are close
proximity to a certain location and food/dining choices. This is a
typical real-life search scenario, since users generally search for
attractions and food/dining choices before going to a vacation.

Different type of tasks may affect users perceptions about the
personalized concept-based search and the baseline search. In
particular, it is known that if the user has specific information
needs (i.e. know what they are looking for) then traditional ranked
list based presentation works well for such fact finding tasks.
On contrary, if the user has a vague idea about what they are
looking for, then categorization and personalization may assist
users better in solving the search tasks. Therefore, we created 4
different search tasks with varying level of specificity according to
the popular search queries from Google search trends. In particular,
we had fact finding tasks (asking very specific information), open
questions (finding an answer to a question with no clues) and
tasks that contained mixture of specific and open questions. Each
task contained 34 sub-tasks and users were asked to write
down answers of each question. In order to generate task scores,
the questions were deliberately very specific such as asking a

specific resource or type of resource. For example, name 5 tourist
attractions in Killarney. This enabled users to determine if they
were satisfied with the answer and if they were happy to complete
the task. We analyzed the results based on different tasks and as
an overall (average) to assess if our approach can improve the
traditional baseline search. The tasks were as follows (the actual
tasks can be accessed at [20]):
Task 1: In Task 1, we asked questions related to tourist attractions
in Killarney, Ireland. For instance, naming tourist attractions in the
region, different types of attractions that are available (e.g. sports,
historical, etc.), a multiple choice question about which attraction
is not in the Killarney National Park and naming specific lakes
in the region. An example tasks is name 5 tourist attractions in
Killarney. In this task, there were open questions and questions
that have specific clues (i.e. users knew which attractions they
were looking for).
Task 2: Task 2 was related to activities that can be done in Killarney
National Park and Killarney town. An example task is Please
name 5 activities that can be done in Killarney National Park. The
questions were open ended, since no clues were given.
Task 3: In Task 3, we asked users to name tourist attractions
related to nature. In particular, we asked about trails, walking and
hiking options. An example task is name 2 trails that are available
in Killarney region. This task contained a mixture of open and
specific information gathering queries.
Task 4: Task 4 was related to food options in Killarney town. For
example, we asked open ended questions about available cuisines,
restaurants serving seafood and places for a drink. All of the
questions did not contain any clue about the answer, which is a
good example of an information gathering task. An example task is
name 5 different cuisines that are available in Killarney town.
Experiment. Since the search tasks were performed on the
adaptive and the non-adaptive version of search systems, we
need a fair comparison strategy. For this purpose, we divided
participants into two groups such as Group A and Group B.
Group A users performed either Task 1 or Task 3 using the
baseline system first and then they performed Task 2 or Task 4
using the personalized search system (aka proposed). Similarly,
Group B users performed Task 1 or Task 3 using the personalized

M. Sah, V. Wade / Web Semantics: Science, Services and Agents on the World Wide Web 36 (2016) 3257

system first and then they performed the Task 2 or Task 4 using
the baseline system. In this way, all tasks were equally tested
by both personalized and baseline search systems. In addition,
the learning effect was removed by swapping adaptive and nonadaptive versions. Please also note that users did not know which
system was adaptive or which system was the baseline system
since the systems were anonymously presented as system A or
system B. During the experiment, user actions were anonymously
tracked throughout the search session in order to analyze users
system interactions. In addition, task completion times were
tracked between the first display of a tasks questions until a users
submission of the task answer.

The experiment was carried out in the following order.

Pre-questionnaire: First, users were asked to fill a prequestionnaire about their Web search experiences.
Experiment with system A: Users were anonymously presented
with either the personalized or the baseline search system. To
balance effect of bias, system order was randomized to ensure
that overall the personalized and baseline system appeared equally
as often as the first system (system A) or second system (system
B). In addition, tasks were equally tested by the adaptive and
the baseline system. First, we gave a quick demo of the search
system A using an example query before giving the actual search
task. Then, users performed the given search task using System
A and asked to write down the answers. After task completion,
users were given; (i) an application specific post-questionnaire (to
assess search performance and user satisfaction) and (ii) a usability
questionnaire (to assess system design usability). We used a
5-point Likert scale for the questionnaires (1 = strongly disagree,
2 = disagree, 3 = not sure, 4 = agree, 5 = strongly agree).
Experiment with system B: Users were anonymously presented
with the system B (the opposed system). Again, an example query
was issued and an interface demo was presented before the actual
search task. Then, we asked the users to complete the randomized
search task using the system B and wrote down answers. Then,
they were given the same application specific post-questionnaire
and the usability questionnaire.
Comparative post-questionnaire: Finally, after completing all
tasks with both systems, we directly asked users to compare
both systems (e.g. system A and system B) using a comparative
questionnaire.
Participants. We recruited 32 participants from the School of
Computer Science and Statistics of Trinity College Dublin using
email invitations to the research group. After completing the
experimental process, users were entered into a random draw for
the chance to win an electronic device. Participants had different
background (1 master student, 20 Ph.D. students, 9 post-docs and
2 academics), and aged between 20 and 50. It is important to
note that participants did not know that they were performing
the search tasks using LOD resources. In particular, we used LOD
resource descriptions for creating the benchmark dataset and
categorizations. However, users were presented with the human
friendly HTML pages of particular LOD datasets for better user
experience because none of the participants were Semantic Web
experts. The aim of the user study was to assess the added
value of the categorization and personalization as opposed to the
traditional LOD search; assuming even non expert Semantic Web
users can explore the LOD resources effectively.

5.3.4. Results

The pre-questionnaire revealed some background information
about users search experiences. 93.75% and 6.25% of the participants said they use Web search engines several times in a day
or several times in a week respectively to search for informa-
tion. This answer shows that Web search is an important part of

daily Web activity and users are very familiar with traditional Web
search engines. Thus it can be difficult to persuade users with new
search and interaction interfaces like the one we propose in this
paper. To differentiate between navigational queries (searching for
a Website) and real information queries, we asked users how often they use Web search engines for information gathering search
tasks (i.e. finding answer to a particular question); 81.25% use several times in a day and 18.75% use several times in a week.
This answer shows that users usually perform fact finding queries.
Since the search tasks were in the tourism domain, we asked users
how often they search for information in a tourism domain (i.e. va-
cation, events, activities);15.62% users search several times in a
week, 46.87% participants search several times in a month and
37.5% users search several times in a year about tourism. These
figures show that users generally search for tourism related information and performing a user study in a tourism domain is appro-
priate. Finally, we asked users if they used adaptive systems in the
past, where 93.75% of the users said they used adaptive systems
such as Amazon, LinkedIn, Facebook (product or people sugges-
tions). Among these users, we asked how often they use recommendations from the adaptive systems; 21.87% and 40.62% of the
users said they use recommendations very often or often re-
spectively. These findings show that more than half of the users
heavily uses adaptive recommendations from Websites.
Results of task assistance (Hypothesis H1). As stated in hypothesis H1, the goal of the personalized concept-based search system is
to better assist a users search for information than a non-adaptive
baseline search system. The results from the task completion times
revealed that the personalized search system outperformed the
baseline search system with an average of 6.50 (m:ss) versus 10.48.
Moreover, t-tests confirm that the results are indeed significant for
each task (for Task 1 p = 0.037, for Task 2 p = 0.047, for Task 3
p = 0.03 and for Task 4 p = 0.003). Fig. 13 shows the average task
completion times across tasks and reveals that users were
consistently faster using our system. It is also shown that Task 2
and especially Task 4 took considerably longer to complete using
the baseline system compared to our system. The reason for this
could be explained by the nature of these two tasks. Task 2 contained mixture of fact finding and open ended questions and Task
4 contained open ended questions, compared to fact finding questions in Task 1 and Task 3. Especially users spent more time in reformulating queries for Task 4 using the baseline search system.
Perceived task difficulty also proves this point as shown in Fig. 14.
When asked, users thought Task 2 and Task 4 was more difficult
than the other two tasks.

Similarly, users formulated fewer queries using the personalized search system in order to find their information (see Fig. 15)
across all tasks. Again t-tests confirm that the results are significant
(for Task 1 p = 0.045, for Task 2 p = 0.031, for Task 3 p = 0.017
and for Task 4 p < 0.001). On average, users issued 6.46 queries
using the baseline system to complete the tasks, whereas the personalized system required an average of 3.03 queries. Again in information gathering tasks like Task 4, users required an average of
9.25 queries using the baseline system compared to an average of
2.12 queries using our system.

Another aspect of task assistance is the number of viewed
pages. It is desirable that the search system provides the best
information resources in the top results, so that users can reach
the desired information with few page views. This can be measured
by comparing click histories across systems and tasks. The results
showed that users consistently required more page views across
all tasks using the baseline system in order to reach the desired
information (Fig. 16). T-tests also confirms the significance of the
results for each task (for Task 1 p = 0.048, for Task 2 p =
0.048, for Task 3 p = 0.038 and for Task 4 p = 0.013). On
average, users required 10.56 page views using the baseline search

Fig. 13. Task completion times.

Fig. 16. Number of page views.

Fig. 14. Users perceived task difficulty.

Fig. 15. Number of queries.

system, whereas they viewed an average of 5.34 pages using the
personalized search system. This shows that users reached the
desired information more easily using our approach. This gain were
mainly obtained by the personalization; results were re-ordered
and re-ranked as well as more relevant results were automatically
pushed on top of the search list using concepts/results re-ranking
and query expansion techniques using our approach.

Shorter task completion times, fewer queries and fewer page
views clearly point towards the validation of hypothesis H1.1, as
they reduce the required user effort. T-tests also confirm that
the results are significant and are not obtained by chance. These
findings are also backed up by post-questionnaire questions. Users
agreed with an average of 3.0 for the statement I had to search
a lot before I found interesting content using the baseline search
and disagree for the personalized search system (an average of 1.5)
as shown in Fig. 17 (p < 0.001). In addition, users agreed with an
average of 4.37 for the statement I spent less time querying and
more time browsing for our approach, whereas baseline system
received an average of 3.03 (p < 0.001). Moreover, users agreed
for the statement I was less exposed to irrelevant content with
an average of 4.37 for our approach and disagree for the baseline
search (average of 2.59) (p < 0.001).

Fig. 17. Users perceptions of search, result presentation and personalization
(average of all tasks, 1 = strongly disagree, 2 = disagree, 3 = not sure, 4 = agree,
5 = strongly agree).

Comparative post-questionnaire also supports this hypothesis
(see Fig. 22). When we asked users directly if they had to search
more than the other system, users strongly agreed for the baseline system with an average of 4.31 and strongly disagree for the
personalized system with average of 1.59 (p < 0.001). In addi-
tion, users strongly agreed that the personalized system returned
relevant content more prominently compared to the baseline system with an average of 4.31, whereas they disagree for the baseline
system with an average of 2.03 (p < 0.001). Furthermore, for the
sentence I found the results structure and content was more help-
ful, the majority of users agreed/strongly agreed (37.5%/59.37%)
with our system (average of 4.5) and disagreed/strongly disagreed
(34.37%/53.12%) with the baseline system (average of 1.65) (p <
0.001) (Fig. 23).

One interesting finding is that user perceptions about both systems varied based on different tasks. Figs. 1821 show users perceptions according to different tasks. As can be seen, for fact finding
tasks such as Task 1 and Task 3 (Figs. 18 and 20), user ratings for
our approach were slightly better than the traditional ranked list
based baseline system. It is known that traditional keyword based
queries work well if the user knows what s/he is looking for. This
finding revealed that our approach was at least as effective as the
baseline system in fact finding tasks while users perceptions about
our system was still better than the baseline system. On the other
hand, when the users had vague information needs, such as Task 2

M. Sah, V. Wade / Web Semantics: Science, Services and Agents on the World Wide Web 36 (2016) 3257

Fig. 18. Users perceptions of search, result presentation and personalization (average of Task 1).

Fig. 19. Users perceptions of search, result presentation and personalization (average of Task 2).

Fig. 20. Users perceptions of search, result presentation and personalization (average of Task 3).

Fig. 21. Users perceptions of search, result presentation and personalization (average of Task 4).

Fig. 22. Comparative questionnaire about users perceptions of the proposed
system and the baseline search; in the user studies both systems were anonymously
shown as either System A or System B.

Fig. 24. Standard usability scale (SUS) Questionnaire.

Fig. 23. Users perceptions about the results structure and content of the proposed
and the baseline systems.

and Task 4 that contained information gathering tasks, the personalized system outperformed the baseline search (Figs. 19 and 21).
In information gathering/exploration tasks, users felt more guided
and considerably more satisfied with our approach compared to
the baseline search. Since, our approach helped them to complete
tasks more efficiently. These findings are supported by task completion times, number of issued queries and number of page views.
Overall, the hypothesis for task assistance (H1) clearly shows
the benefits of our approach. The personalized system performed
better than the baseline search system.
In particular, users
completed tasks faster, issued fewer queries and had to search less
for the required information (less page views) in order to complete
tasks. These results are backed up by various questionnaire
questions, such as users felt our search system returned relevant
content more prominently, they had to search less for the
information and the results structure was more helpful for solving
the tasks.
Results of user satisfaction (Hypothesis H2). User satisfaction is
aiming to identify users appreciation and satisfaction regarding
usability and the various functionalities of the personalized search
system.

SUS is an independent usability questionnaire, which can
be used to compare different systems independent of their
application domain, design and functionality [34]. We used SUS
to determine overall usability of both systems (hypothesis H2.1).
The personalized approach achieved an average SUS score of 88.90,

Fig. 25. Overall user satisfaction about the proposed and the baseline systems.

whereas the baseline search system scored an average of 75.15
(see Fig. 24). This is a very encouraging result for such a novel
system, especially considering that all of the users were very
familiar with the traditional ranked list based search (baseline
system) and it can be very difficult to influence users with novel
search and interaction paradigms like the one proposed in the
paper. Moreover, these results even more encouraging since with
personalization features like re-ranked lenses, re-ranked results
and concept lenses suggestions, users thought that our system
was easy to use and better than the baseline system in terms of
usability.

This finding is further supported by answers of the postquestionnaire questions. Users confirmed that I am satisfied with
the system performance, guidance and assistance with an average
of 4.34 for the personalized system compared to 2.96 of the
baseline system (p < 0.001). When asked directly to compare both
systems, users gave even strong evidence for the overall usability
of our system (Fig. 25). The majority of users agreed/strongly
agreed (40.62%/50%) that they were more satisfied with our system
(average of 4.37) and disagreed/strongly disagreed (37.5%/40.62%)
that they were more satisfied with the baseline system (average of
1.90) (p < 0.001).

In addition to more general usability questions, users were
asked about the various composition and personalization aspects
of the system (hypothesis H2.2). With these questions, we aimed

M. Sah, V. Wade / Web Semantics: Science, Services and Agents on the World Wide Web 36 (2016) 3257

Fig. 26. Lens click histories according to tasks.

to more specifically assess if the users recognized and valued the
various functionalities provided by our system, such as categorization and personalization. Questions Q7Q12 in Fig. 17 summarize these results. The results clearly indicate user preference
towards the presentation, categorization and adaptation of the
personalized system for all questions (with a statistical significance
of p < 0.001 for Q7Q12). This is again very encouraging considering that the users familiarity with the baseline search system.
Questions 8, 11 and 12 highlight in particular the strengths of our
approach. In particular, users valued and strongly recognized (av-
erage of 4.43) the categorization aspect of our system, where they
were strongly agreed for the statement I found the categorization
and grouping of search results helpful compared to the baseline
system (average 1.78) (p < 0.001). Categorization of results is the
highest rated feature of our system in the questionnaires, which is
backed up by the open question which feature did you liked most
(shown below). 23 out of 32 participants said they liked the categorization feature most. Moreover, users valued and recognized
personalization aspects of our system. In particular, answers to the
following two statements confirmed these findings; for the personalized system users agreed I felt guided to relevant results
with an average of 4.34 versus 2.37 (p < 0.001) and the system
guided me towards more personally relevant content with an average of 3.96 versus 1.68 (p < 0.001). By monitoring click histories of the users on lenses and recommended concept lenses, some
interesting insights about personalization features can also be re-
vealed. In Fig. 26, the users lens click histories for different tasks
are presented. Initial represents the initial lens selection since
we deliberately asked users to select a lens for results exploration.
As shown in Fig. 15, for our approach, users had higher number of
queries for the Task 1 and Task 2. Hence initial lens clicks for the
Task 1 and Task 2 are higher than the other two tasks in Fig. 26.
For the Task 3 and Task 4, users used lens navigation and the
recommended concept lenses more heavily. Here, by navigation
we mean the user did not re-formulate a query but used the existing lenses to navigate between the results. According to lens clicks
observations (Fig. 26) and query re-formulation statistics (Fig. 15),
we came to this conclusion; our observation is that users used lens
navigation instead of query re-formulations since they could narrow down the search space by selecting the appropriate lens. This
action (lens selection), also triggered personalization, such as automatically the results were re-ranked and more relevant results
were added using query expansion, as well as, relevant concept
lenses were suggested in order to assist users in results explo-
ration. It is also clear that users used the recommended concept
lenses for all tasks. In summary, both user answers to questionnaires and lens click histories supported the hypothesis H2.2 that
users valued and recognized categorization and personalization aspects of the proposed system as opposed to the baseline system.

In order to gain more insights about which features were particularly useful, users were asked what features/ characteristics

Fig. 27. User motivation, engagement and fun.

did you like most about the system. For the baseline system, the
dominant responses were its simplicity such as uncluttered results presentation (mentioned by 17 users), users familiarity with
such system systems (6 users), relevant results (4 users) and its
speed (2 users). For our system, users particularly liked the categorization of the results (mentioned by 23 users), ranked concept lenses/results (5 users), simplicity such as uncluttered results
presentation (3 users) and recommended concept lenses (2 users).
These results showed that users liked the categorization and personalization aspects of our approach. In turn, we asked what fea-
tures/characteristics did you least like about the system. For the
baseline system, users mentioned the lack of guidance/ recommendations (5 users), lack of grouping or structuring of results (4 users)
and time consuming to investigate results (1 user). For our sys-
tem, users mentioned that no results were shown initially (4 users),
some users found it difficult to change search habits (2 users) and
recommended concept lenses on right were confusing (1 user).
These answers are certainly promising since they show general acceptance of the personalized concept-based search, while leaving
certain amount of work to be done in terms of system design. For
example, we can show the initial search results and start adaptation only when the user clicks on a concept lens.

Finally, an important goal of categorization and personalization
of search results is providing users with a motivating and engaging search interface (Hypothesis H2.3). Answers to questionnaire
questions Q13Q15 (Fig. 27) confirmed that users were more motivated (average of 3.84 versus 2.31) (p < 0.001), more engaged (av-
erage of 4.03 versus 2.28) (p < 0.001) and had fun (average of 3.81
versus 2.03) (p < 0.001) with our system compared to the baseline system. If asked directly (see Fig. 28), users agreed/strongly
agreed (46.87%/46.87%) that they found the overall experience of
using the personalized system more stimulating than the baseline
system (average of 4.37) and they disagreed/strongly disagreed
(43.75%/46.87%) that the baseline system was more stimulating
than the personalized system (average of 1.65) (p < 0.001). These
results are very encouraging since it shows that our system helped
users to motivate and engage more with the search system.
Discussion of results. The results of the user study clearly
showed that we provided an interface that enabled users to be
efficient and effective at performing both information gathering
and fact finding tasks.
In particular, users were consistently
faster, issued less number of queries and viewed less number
of pages in order to reach the desired information. This finding
also shows that concept-based results presentation can be used
in quick results finding as well as information exploration tasks.
These results have been contributed by both categorization
and personalization features of our approach. In particular, the
proposed approach was able to assist users to relevant content
through personalized lenses/results re-ranking, query expansion

Fig. 28. Overall user experience.

and concept lenses recommendation, which was all supported by
results categorization.

In addition to user effectiveness and efficiency, user satisfaction metrics have shown that users recognized and valued
categorization and personalization aspects. In particular, general
usability has been rated higher for our system compared to the
more established ranked-list based search paradigm. This result is
very encouraging since even with added personalization features,
such as lenses/results re-ranking, query expansion, concept lenses
suggestion, users thought the system was easy to use and even better than the traditional ranked-list based search system in terms of
usability. Application-specific questions also confirmed these results that users appreciated the provided results presentation and
personalization.

Results from t-tests also revealed that the results are indeed
significant for all metrics. Task assistance metric achieved p values of 0.048 or smaller. In addition, application specific and comparative questionnaires achieved a strong statistical significance of
p  0.001 for all questions. This confirms that the results have not
occurred by chance and that our approach has consistently performed high on user effectiveness, efficiency and satisfaction. In
addition, users were not aware of the tasks and systems before the
experiment. Hence there was no incentive for users to be biased
towards either of the systems. Moreover, users were anonymously
presented with either of the systems and system order was randomized in order to remove bias.

5.4. Personalization time overhead

Dynamic personalization performance is vital for the practicability of the proposed personalization. We measured the personalization time overhead compared to a non-adaptive concept-based
search to assess this. In non-adaptive concept-based search, results
are categorized into concepts and presented with no adaptation.
Please note that results categorization can be applied offline during
the indexing of LOD resources and in this section we only focused
on personalization efficiency.
It is important to remind that in our approach, results re-ranking
is applied to the top K results and only those concepts lenses
belonging to the top K results are re-organized during concept
lenses re-organization. In our search interface, therefore results
are displayed pages of K results and if the user wants to explore
more search results, then s/he can click to the next button.
Similarly, within the next K results, personalization is applied
accordingly. This approach of processing only K results during
the personalization enables to run the adaptation algorithms
efficiently. Therefore, even we used a small dataset to assess the
personalization performance, these results are applicable to larger
datasets. Since only K results are processed during personalization
and the performance does not dependent on the dataset size. In
general, personalized search approaches aim to improve results

Fig. 29. Non-adaptive and personalized concept-based search response times.

at the top of the list and they process the top results such as
explained in [35]. Because beyond the top results relevancy to
the query decreases significantly and processing all results does
not add much value to the personalization. In the experiment, we
assess effects of increasing K values on the system performance.
Evaluation setup. The evaluation setting was as follows: We used
the tourism dataset and 20 sample queries from the benchmark
dataset [31]. For each query, we computed the average time
required to generate personalized results such as lenses re-
organization, results re-ranking and query expansion following a
lens selection. Similarly, for non-adaptive search, we computed
the average time required to generate results using two ranking
models, traditional tdfidf scoring and semantic search approach
BM25F. For each query, an average of 5 runs was used. The results
were run on Windows 7 computer, 2.2 GHz CPU and 7.90 GB RAM.
Since personalization performance depends on the top K results,
we tested three different K values as follows: K = 50, K = 100
and K = 200.
Results. Results showed that personalized results were obtained
within an average of 0.19, 0.26 and 0.53 s for K = 50, K = 100 and
K = 200 respectively (Fig. 29). When the top 200 results were used
for the personalization, adaptation was applied in a reasonable
0.53 s. It is also shown that the non-adaptive results were obtained
faster; tfidf was the fastest among all algorithms; BM25F was
slightly faster than the personalized search since it also needs to
include various semantic features into the ranking model, which
takes time. However, BM25F only slightly improve the retrieval
performance as we discussed in Section 5.2. Our personalization
is scalable thanks to two factors: Client-side implementation and
vector space representation of the results and interests.
Analysis of client-side implementation:
In the client-side
implementation of the personalization, extra network connection
with the server was kept to a minimum (i.e. only one additional
HTTP request for query expansion). One can argue that a serverside solution may be more appropriate. However,
in search
systems, client requests can be overwhelming even without the
added personalization. Please note that a search system with an
interactive personalization such as ours, the number of requests
that are sent to the server can easily scale up. It means that
in server-side implementation, every user interaction (e.g. lens
click, results click, navigation), requires another HTTP request
to the server. In our approach, by keeping the relatively simple
vector-space calculations at the client-side (through Javascript),
we eliminate the more costly HTTP server requests/responses,
which potentially can add more delays depending on the clients
internet speed. Evaluations proved that our client-side solution is
scalable.
Analysis of vector space representation of interests and results:
Moreover, we adopted the scalable vector space representation of
the ontology and terms. For instance, concept lenses, search results

M. Sah, V. Wade / Web Semantics: Science, Services and Agents on the World Wide Web 36 (2016) 3257

Table 1
Comparison of LOD search approaches.

Swoogle

Watson

Ranked list

Ranked list

Sindice

Ranked list

Ranked list

Keyword
search
Results
organization
Results
filtering

By three defined
categories:
Ontology, document,
term

An advanced search
interface is provided,
where users can manually
specify particular words,
classes, properties,
individuals or scope (name,
label, comment or literal
value)


Manual sorting options for
filtering by date, format,
ontology, class predicate,
domain, triples, word

By 7 pre-defined entities:
Any, Location, Person,
Artifact Instance,
Organization, Artifact type,
Other and event

Results
sorting

Manual soring
according to
ontology rank, date
or triple
Personalized

search/exploration

Manual sorting according
to term relevance or date


Proposed method

Concept-based visualization
(concept lenses)
By UMBEL concepts
(categorization under any
25,000 UMBEL concepts)

Automatic sorting according
to personalized and semantic
relevance

Lenses and results
re-ranking, query expansion,
lenses recommendation

and user interests are represented with vector of UMBEL concepts
and vector of terms at the client-side. This enables efficient
computation of
intensive similarity comparisons (e.g. cosine
similarity of vectors) during the runtime as shown by the results.

results within the interacted lens using results re-ranking and
query expansion as well as suggest the relevant concept lenses
that have similar results for exploration. This analysis shows that
the proposed approach provide a unique concept-based search and
personalized results exploration experience on the LOD.

5.5. Comparison with LOD search engines

Finally, in order to assess the added value of our system to
the LOD search engine literature, we compared our approach with
the existing approaches such as Swoogle, Watson, Sindice and
OKKAM as shown in Table 1. First, we identified the following
common features of these approaches; keyword search, results
organization, results filtering, results sorting and personalization.
We use these features as a basis for the comparison. All LOD search
approaches support keyword queries. Except from our method,
existing approaches use simple ranked result lists presentations.
Whereas in our method, we use a novel concept-based results
organization, which can support results exploration based on
diverse concepts drown from UMBEL (25,000). Because of the
simplicity of keyword queries, all approaches do support some
form of results filtering in order to allow narrowing of the search
results. In particular, many LOD engines support filtering by class,
predicate, triples, ontology and so forth. However, all of these
filtering designs require technical semantic knowledge, which
means that these features are only available to Semantic Web
experts. For example, there is no guidance on URI domains or
common predicates. In addition, such features can only be used
if a Semantic Web expert knows what s/he is looking for. If the
search intent is to explore and gather information in an unfamiliar
domain, then such filtering is not useful. Alternatively, OKKAM
allows results filtering by seven predefined concepts. However,
these concepts are too general/vague (e.g. Artifact, Other) to be real
value to users. On contrary, in our approach, results are organized
based on 25,000 conceptual concepts that can support complex
results exploration. Another feature is results sorting supported
by Swoogle, Sindice, Sig.ma and our approach. In our method,
automatic sorting is achieved based on personal and semantic
similarities, whereas the rest uses manual sorting. Finally, none
of the existing LOD search engines support personalization. This
is the unique contribution of our approach to the WoD, which
allows results personalization using results categorization and
user interaction. In particular, users are not required to login to
get benefit of the personalization, since we only use the current
user interactions within the search session. In particular, we
adapt results organization using lenses re-ranking, personalize the

6. Related work

6.1. Concept-based and clustering search systems

Most of the current search engines utilize keyword-based
search algorithms (i.e. full-text search) for information retrieval.
Although this method is simple and fast, it often produces the
problem of high recall and low precision, because it mainly finds
documents that contain query keywords. Clustering or conceptbased IR aims to improve retrieval effectiveness by organizing
search results based on their meaning [13]. Open Directory Project
and Yahoo Directory for instance use manual categorization, which
is not scalable. Conversely, automatic clustering of results is
scalable but challenging. Approaches usually use data mining, NLP
and statistical techniques (e.g. k-means clustering) to calculate
document similarities, form/label clusters and present flat or
hierarchical result categories ([13], vivisomo.com, carrot2.org).
On the other hand, since it is expensive and difficult to create
broad conceptual structures, concept-based search approaches
use existing conceptual structures, such as Yahoo Directory [21]
and Open Directory Project (ODP) [14,22]. With the increasing
number of LOD ontologies and metadata, interests in using these
taxonomies are decreasing.

6.2. Search mechanisms on the WoD

Faceted search is an example exploratory search method,
which uses facets for interactive filtering of results based on
shared schema properties. Generally, faceted search uses labeled
graph [15,36] or textual overviews (semantic properties as browsable facets). In both cases, usability and efficiency decreases as the
complexity of information space increases. To increase usability
of information visualization on huge repositories, [37] describes
overview first, zoom and filter, then details on-demand fashion. [36]
uses both statistical knowledge and graph structure (subject and
broader concepts) to estimate resource popularity for graph
presentation in DBpedia. [38] provides a formal model of facet navigation for heterogeneous RDF vocabularies. In addition, they develop a technique for automatic facet ranking using facet metrics

and predicate frequency. In this way, more useful and important
facets can be presented first. Whereas, [15] utilizes clustering and
personalization in a multimedia domain to decrease visualization
complexity. Faceted search is typically applied in closed domains
since it requires high data completeness and consistent markup
across the whole corpus. In addition, applying dynamic conjunctive clauses on large datasets significantly increases complexity of
faceted search. Considering the varying data quality and heterogeneous vocabularies of the WoD [811], it can be challenging
to generate consistent facets for the whole LOD. Alternatively, to
overcome this issue, our approach can be combined with the
faceted search. For example, initially LOD resources can be categorized using UMBEL concepts so that useful facets can be
presented to the heterogeneous LOD resources. Subsequently,
common metadata properties of LOD resources can be utilized
to provide more browsable facets whereas available. Similarly,
this combination can benefit our approach as well; using common metadata properties of LOD resources as browsable facets, we
can offer more options for information exploration. To summarize,
the main difference of our concept-based search from the faceted
search is that it works on open corpus of LOD resources thanks to
the use of our retrieval method and UMBEL [17].

Other related works that uses categorization as a form of
search results presentation are [39], Semantic GrowBag [40] and
SearchPoint search engine add-on [41].

In [39], authors propose a categorization method for Semantic
Web query results presentation. In this approach, query is in the
form of a conjunctive query and the returned search results are
grouped by taking into account the subsumption hierarchy of
the underlying ontology. Thus, the results can be showed and
navigated similarly to a faceted search. Although some proof-of-
concept experimental evaluation is reported, it is not clear how the
underlying ontology will affect the categorized results. In addition,
this method is designed for Semantic Web experts for querying
semantic knowledge bases.

The Semantic GrowBag Project13 uses the keywords provided
in metadata annotations of digital objects collections to automatically create light-weight topic categorization of search results [40].
Using such available metadata annotations enables an alternative
way to filter large result sets according to the objects content
without the need to manually classify all objects with respect to
a pre-specified vocabulary. In order to generate topic catego-
rizations, the Semantic GrowBag algorithm uses a biased PageRank algorithm and the higher-order co-occurrence of metadata
annotations of digital objects (e.g. documents, images, etc.) in a
corpus. Specifically, the higher-order co-occurrence method takes
into account the keywords co-occur together. Finally, the generated topic categorizations are utilized for providing faceted search
interfaces. The main drawback of this approach is that it is best applied on a specific dataset with consistent metadata annotations,
such as creating topic facets for DBLP [42] or generating topic facets
in a medicine domain [43]. It cannot be applied to heterogeneous
datasets.

SearchPoint search engine add-on14 processes the results
returned by a Web search engine and presents the results along
with topics graphs [41]. These topic graphs are generated either
from the top 150 results using K-means clustering or from an
ontology (dmoz.org) or by utilizing both methods. Authors argue
that cleaner topic graphs can be generated with an ontology
comparing to K-mean clustering method. In SearchPoint, as the
user drags the red focus point on different topics on the graph, the

13 Semantic GrowBag Project is available at http://www.l3s.de/growbag/index.
php.
14 SearchPoint search engine is available at http://searchpoint.ijs.si.

search results ranking is changed; at each position of this focus,
a different ranking is returned. For example, if the user clicks on
one topic, results are ordered mostly by the sense of that topic. If
the focus is moved to a position between two topics, results that
share similarity with both topics will tend to be ranked higher.
This approach combines both clustering and ontology-based topic
creation. In addition, the search results are re-ranked based on the
selected topic by the user. Developers of SearchPoint, suggest that
this work is best applied to closed corpus domains, such as for
corporate search engines.

Finally considering the large body of work on clustering or
faceted search, current WoD search mechanisms (Swoogle [1],
Sindice [2], Watson [3], OKKAM [4]) use keyword queries and
ranked lists and they are not focusing on data exploration
problems. Users cannot understand what the resource is about
without opening and investigating the LOD resource, since
title/triples are not informative enough. Sig.ma [5] attempts to
solve this issue using querying, rules, machine learning and
user interaction. However, Sig.mas focus is on data aggregation.
Another relevant aspect of semantic search is the way users
express their information needs. Keyword queries are the simplest
and widely used [14,28]. Natural
language queries increase
expressiveness such as linguistic analysis can be applied to extract
syntactic information [27]. Controlled natural language queries are
also utilized, where query can be expressed by values/properties
of an ontology [29]. Finally, the most formal systems use ontology
query languages (i.e. SPARQL), which demands high expertise
and impractical from usability point of view. A trade-off between
expressivity and usability should be achieved.

6.3. Personalized search

Personalized IR is a popular topic in traditional Web. Generally,
personalized IR comprises of: (1) User data gathering, (2) user
profile representation and (3) personalization techniques. In this
section, we discuss the related work in these areas.
User data gathering. User profiles can be generated from
relevance feedback, implicit relevance feedback, desktop data,
social Web or users context [44,45]. Generally the effectiveness
of relevance feedback is limited since users are often reluctant to
manually provide information. Implicit relevance feedback thus
uses interactions with the system such as previous browsing
activity, time spent on pages, etc. as an indication of implicit user
interests [12]. In both cases, the system needs time to gather
enough information about users all past interests. To overcome
this issue, some approaches utilize desktop data or recently social
web data [14], which often contains enough information about
general user interests. However, relying on all past user interests
is tricky and often a correct subset of past interests needs to
be identified, which can be very challenging. This is because
not all past interests may be important in the current context
and an incorrect personalization may annoy the user experience,
e.g. a user looking for hotels in Florence will not be interested
to get Florence hotels in results after booking a hotel. Thus,
approaches based on all past interests require fine-tuning, such
as threshold selection for similarity/time decay, which may differ
for various users or search scenarios. Moreover, in long-term
user profiling, generally users activities with the retrieval system
are continuously monitored for results adaptation (i.e. Google,
amazon). This means users are required to register and login
to get benefit of the personalization, which often raises privacy
issues. An alternative to this approach is, no login/no storage
or client storage. Client-side storage has its own issues; users
may have multiple access mechanisms to the internet (especially
with growing mobile access devices). Thus the user profile may
be dislocated to multiple devices and the user may get different

M. Sah, V. Wade / Web Semantics: Science, Services and Agents on the World Wide Web 36 (2016) 3257

personalization experience based on the device s/he used. Finally,
in the context-based user modeling, only the current available
information within the current search context is utilized (i.e. query,
query context, clicked results, etc.). A benefit is system only deals
with few number of interests hence performance is scalable. The
drawback is past user interests are lost but not all past interests
are useful or identification of related interests can be challenging
as we mentioned.

In our approach, we only use the current search context, hence
it does not require user login. As a result, our approach provides
personalization according to local choices of the user, which is
based on results categorization. A similar work is [13], which
uses hierarchical page snippet clustering for personalized search.
Results are categorized into hierarchical folders using gapped
sentences and ODP based on current user interaction. In this
approach, the user need to select a list of relevant labels related
to his/her information needs. Then relevant results are filtered,
and the query is expanded. In our approach, all relevant lenses are
implicitly re-organized when the user selects a concept lens. This is
different from the approach in [13], as it requires explicit selection
of all relevant labels.
User profile representation. General user profile representation
methods in personalized IR are: weighted keywords, semantic
network of terms and semantic network of concepts [14].
The simplest model is weighted vector of keywords. However,
keyword-based representation does not capture semantics of
related terms. Ontology-based profile representation techniques
try to overcome this problem. [14] utilizes the entire ontology
for representing user profiles. Extracted keywords from the
browsed pages are matched to ontology concepts and concepts
are represented as weighted vector of keywords. Generally user
profiles are utilized for results re-ranking. In contrast to the general
approach, our personalized search approach is driven by results
categorization and we represent user interests using combined
ontology-based and keyword-based vectors. Usually either one of
these representations is used.

On the other hand, similarity measures play an important role
in IR, such as measuring relevance between the users keyword
query and set of pages. A majority of these measures are statistical
or linguistic models for unstructured text documents. With the
Semantic Web, semantic similarity measures are proposed to
compare concepts and/or concept instances. They can be classified
into structure and information based approaches. The structurebased methods use ontology hierarchical structure, such as edge
distance between concepts. Information based methods use the
shared content between concept features, e.g. comparing concepts
textual data using cosine similarity. Hybrid approaches combine
both methods. For semantics-based IR, appropriate similarity
measures depend on many factors, such as concepts representation
(e.g. bag of words, logic predicates, etc.), search context and
concept expressivity [46]. Description logic based approaches
allow full expressivity but complexity can be high [47]. Overall,
similarity measures depend on the application area.
In our
approach, we use a hybrid semantic measure for lenses and results
re-ranking.
Personalization techniques. The following personalized IR techniques are common [44]: Query disambiguation, query expansion,
result re-ranking, results filtering, hybrid methods and collaborative adaptation. Two popular techniques are query expansion and
results re-ranking. Query expansion methods augment the query
with terms that are extracted from interests/context of the user so
that more personally relevant results can be retrieved. A general
limitation is that if expansion terms are not selected carefully, it
may degrade the retrieval performance. Conversely, in result reranking (rank biasing), the initial set of results are retrieved and

the results are re-ranked based on a user profile (i.e. profile similarity [14]). The aim is to push personally relevant results up in
the result list. [13] applies results re-ranking based on explicit user
selection on hierarchical folders. Different from [13], we present
results with concept lenses rather than ranked lists. Despite ongoing research in traditional personalized search, there are few examples of personalized search approaches in semantic search such
as [15]. [15] uses clustering and personalization in a multimedia
domain in order to decrease visualization complexity. We believe
that with the growing volume of LOD, personalized search and exploration become more crucial. We directly address this issue by
introducing a personalized concept-based search interface for the
LOD.

7. Conclusions and future work

We have presented a novel personalized concept-based search
mechanism for the Web of Data (WoD) based on results
categorization. In our approach, the search results (LOD resources)
are dynamically categorized into UMBEL concepts using a novel
retrieval model. Then, results with the same concepts are grouped
together to form categories that we call concept lenses. Such
categorization enables concept-based browsing of the retrieved
results aligned to users intent or interests. When the user interacts
with the concept lenses, results are immediately personalized.
Specifically concept lenses are re-ranked according to their
similarity to the selected lens. Within the selected concept
lens; more relevant results are included using results re-ranking
and query expansion, as well as relevant concept lenses are
suggested to support results exploration. This innovative approach
allows dynamic adaptation of results to the users local choices.
Moreover, we use the last N user clicks on results to support
interactive personalization. In particular, within the interacted
lens, immediately more relevant results are included using results
re-ranking and query expansion as well as relevant concept lenses
are suggested. Moreover, our personalization approach does not
require login hence it is non-intrusive and privacy preserving.

Extensive evaluations have revealed the feasibility and novelty
of our approach. In particular, the proposed retrieval model was
evaluated on a particular benchmark (10,000 mappings). The
evaluations showed that our approach achieved highly acceptable
categorization accuracy (90%) and outperformed the vector
space model. In addition, a scalable system architecture was
proposed for dynamic categorization of LOD resources.

Personalized search efficacy was assessed using a user study,
which consisted of a task-based evaluation regarding a tourist
domain. The results showed that our approach performed
significantly better than a non-adaptive baseline search.
In
particular, our approach enabled users to be efficient and effective
at performing both information gathering and fact finding tasks.
Users were consistently faster, issued less number of queries
and viewed less number of pages in order to reach the desired
information. In addition, user satisfaction metrics have shown that
users recognized the categorization and personalization features
of our search system. Moreover, general usability has been rated
higher for our system compared to the more established rankedlist based search paradigm. This result is very encouraging since
even with added personalization features, users thought the
system was easy to use and even better than the traditional rankedlist based search system. Results from t-tests also revealed that the
results are indeed significant for all metrics. Finally, comparison
with the existing LOD search engines showed that our approach is
unique on the WoD.

In future, we will investigate how the system design can
be improved. For example, investigating what is the best way
to present the initial search results and the adapted results
from different personalization features. Moreover, we can explore
where the categorization performance can be improved further
with more sophisticated IR models, such as BM25F or PL2F.

Acknowledgment

This research is supported by the Science Foundation Ireland
(Grant 12/CE/I2267) as part of the Centre for Global Intelligent
Content (www.cngl.ie) at Trinity College Dublin.
