Web Semantics: Science, Services and Agents on the World Wide Web 36 (2016) 2331

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

journal homepage: www.elsevier.com/locate/websem

Domain adaptation for ontology localization
John P. McCrae a,b,, Mihael Arcan b, Kartik Asooja b,c, Jorge Gracia c, Paul Buitelaar b,
Philipp Cimiano a
a Cognitive Interaction Technology, Center of Excellence, Universitat Bielefeld, Inspiration 1, 33615 Bielefeld, Germany
b Insight Centre for Data Analytics, National University of Ireland, Galway, IDA Business Park, Galway, Ireland
c Ontology Engineering Group, Universidad Politecnica de Madrid, Campus de Montegancedo, 28660 Boadilla del Monte, Spain

h i g h l i g h t s
 Detailed description of an architecture and methodology for machine translation of ontologies.
 Methodology for extracting domain terminology from several resources.
 Statistical methods for the domain adaptation of machine translation systems according to ontologies.
 Detailed evaluation showing improvement in translation quality for a number of ontologies.

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 27 March 2014
Received in revised form
22 December 2015
Accepted 22 December 2015
Available online 30 December 2015

Keywords:
Ontology localization
Statistical machine translation
Domain adaptation

Ontology localization is the task of adapting an ontology to a different cultural context, and has been
identified as an important task in the context of the Multilingual Semantic Web vision. The key task
in ontology localization is translating the lexical layer of an ontology, i.e., its labels, into some foreign
language. For this task, we hypothesize that the translation quality can be improved by adapting a
machine translation system to the domain of the ontology. To this end, we build on the success of
existing statistical machine translation (SMT) approaches, and investigate the impact of different domain
adaptation techniques on the task. In particular, we investigate three techniques: (i) enriching a phrase
table by domain-specific translation candidates acquired from existing Web resources, (ii) relying on
Explicit Semantic Analysis as an additional technique for scoring a certain translation of a given source
phrase, as well as (iii) adaptation of the language model by means of weighting n-grams with scores
obtained from topic modelling. We present in detail the impact of each of these three techniques on the
task of translating ontology labels. We show that these techniques have a generally positive effect on the
quality of translation of the ontology and that, in combination, they provide a significant improvement in
quality.

 2016 Elsevier B.V. All rights reserved.

1. Introduction

The vision of a Multilingual Web of Data in which knowledge
is represented in a language-independent fashion and users
can access this knowledge in their own language, has attracted
the attention of research efforts in the area of the Semantic
Web recently [1,2]. In fact, the Web of Data is moving from a

 Corresponding author at: Insight Centre for Data Analytics, National University

of Ireland, Galway, IDA Business Park, Galway, Ireland.

E-mail addresses: john@mccr.ae (J.P. McCrae), mihael.arcan@insight-centre.org

(M. Arcan), kartik.asooja@insight-centre.org (K. Asooja), jgracia@fi.upm.es
(J. Gracia), paul.buitelaar@insight-centre.org (P. Buitelaar),
cimiano@cit-ec.uni-bielefeld.de (P. Cimiano).

http://dx.doi.org/10.1016/j.websem.2015.12.001
1570-8268/ 2016 Elsevier B.V. All rights reserved.

monolingual landscape (in English) towards hosting an increasing
amount of multilingual content. For instance, the number of
multilingual RDF datasets on the Web doubled from January 2012
to December 2012 [3]. However, realizing the Multilingual Web
vision according to which users can access semantic information in
any natural language requires the localization of the vocabularies
that the information is described with. The task of translating
ontological vocabularies into other languages is thus at the core of
the Multilingual Semantic Web Vision, and high-quality translation
approaches are required [4]. This task involves the translation of
ontology labels and, as manual translation of existing vocabularies
is a time-intensive and costly process, automatic techniques, such
as the one proposed in this paper, are needed. Furthermore,
these labels are frequently only fragments of text, instead of

J.P. McCrae et al. / Web Semantics: Science, Services and Agents on the World Wide Web 36 (2016) 2331

full sentences as typically handled by state-of-the-art machine
translation systems.
Indeed, off-the-shelf machine translation
systems are not designed to translate the short labels that typically
occur as labels of ontology elements in SW ontologies, but typically
require more context (i.e., a full sentence) to yield satisfactory
translation results. Our goal is to develop methods that factor in
the ontological context of a label into the translation task, making
standard SMT systems also applicable to the task of localizing
ontologies. With ontological context we refer to the semantic
neighbourhood of a given concept within an ontology, in particular
the neighbours in the graph occurring within a fixed distance
from the ontology element the label of which is to be translated.
In this line, in this paper we investigate the impact of multiple
domain adaptation techniques with respect to the task of ontology
localization. In this paper we handle smaller ontologies for which
the context of a label can be considered to be the whole ontology.
However, for very large ontologies such as DBpedia [5], techniques
to identify the more immediate context should be applied.

Our approach to domain adaptation takes three complementary
paths as extension to a state-of-the-art and off-the-shelf statistical
machine translation (SMT) system such as Moses [6], which
relies on a probabilistic model learned from a parallel corpus
coupled with a monolingual language model acquired from a larger
monolingual corpus to score the plausibility of a translation.

Firstly, we consider enriching the phrase table used by the
machine translation system by translation candidates that are
specific to the domain. In this case, we use the labels in the ontology
to bootstrap this process and extract translation candidates from
Wikipedia and other resources.

Our second approach involves the direct incorporation of the
semantic context of the ontology label into the translation model.
This is achieved by incorporating a feature which describes how
semantically similar a potential translation is to the ontology, by
means of a score computed by Cross-Lingual Explicit Semantic
Analysis (CL-ESA) [7,8].

Finally, our third approach consists in adjusting the translation
model itself in response to the domain of the translation. We
achieve this by means of updating the language model with new
probabilities that are learnt by weighting each document in the
corpus individually by way of its similarity to the ontology as a
whole.

We quantify the impact of all these domain adaptation
techniques on the task of ontology localization using a state-of-
the-art statistical machine translation system as baseline [6]. We
show that all individual domain adaptation techniques lead to
some improvement. The impact actually comes from using all
domain adaptation techniques in combination, which yields an
improvement of up to 30 points in BLEU score [9] according to our
experiments for the financial domain.

The paper is structured as follows: Section 2 discusses the
framework and architecture of the system we propose and which
builds on a state-of-the-art statistical machine translation (SMT)
framework. In Sections 35 we present in more detail the three
domain adaptation techniques examined. Section 6 reports on
our experiments on 2 ontologies: the IFRS ontology and a public
service ontology that were used as use cases in the FP7 Monnet
Project. We describe the datasets we use in more detail as well
as the evaluation metrics used. We first present and discuss the
results of the single components with respect to a baseline system
and then move to discuss results of applying the mentioned
domain adaptation techniques in combination. Before concluding,
we discuss some related work in Section 7.

Fig. 1. An example of constructing a translation by phrase-based statistical
machine translation.

2.1. Statistical machine translation

We base our approach on the statistical approach to machine
translation [10], where we wish to find the translation that
maximizes some function such that the best translation, t, of a
foreign label, f, is given by a log-linear model combining some set

of features {i(t|f)}:
t = arg max

= arg max

exp(wi (t|f))
wi i(t|f).

(1)

The translation that maximizes the score of the log-linear model
is obtained by searching in the space of possible translations via
a so called decoder. The decoder is essentially a search procedure
that computes the sentence in the target language that maximizes
the above score given some statistical translation model induced
from the training data. Hereby, it is assumed that both t and f are
segmented into a number of phrases, ti and fi, and that we have a
phrase table consisting of pairs of translations {(ti, fi)}. A candidate
translation is one such that every phrase in f can be paired with
a phrase in t and this pair occurs in the phrase table.1 In this
model, we take the standard set of features as used in the Moses
system [6]. These are given as follows:
 The logarithm of the probability, p(ti|fi), that is the probability
that fi is translated as ti.
 The logarithm of the lexical weighting of ti given fi [12] summed
over all phrases.
 The logarithm of the probability, p(fi|ti), that is the probability
that ti is translated as fi.
 The logarithm of the lexical weighting of fi given ti summed over
all phrases.
 The number of phrases used in the segmentation.
 The logarithm of the language model probability, a score of the
plausibility of the translation according to a statistical n-gram
model of the target language.
 The number of unknown phrases used in the translation.
 The distortion model. For each pair (fi, ti), the feature indicates
the number of words this pair has been moved away from each
other.

For example, in Fig. 1 we see the translation of the English
ontology label blood group antigen into a Spanish label antigeno
de grupo sanguineo. The scores for the translation would be given
by the scores for each feature for the aligned phrases, e.g., antigen
and antigeno de. The best translation is then found by a heuristic
beam or stack search.

2. Framework and architecture

In this section we briefly review the traditional statistical MT
approach and give an overview of our proposed architecture for
ontology translation.

1 In order to deal with unknown words not observed during training, unknown
phrases of length 1 are assumed to translate to themselves. We note that it would be
possible to apply a transliteration method in this case [11], but we do not consider
this in the context of this work.

2.2. Our architecture

Our architecture for domain-adapted ontology translation,

illustrated in Fig. 2, consists of the following components:
Pre-processing. This performs the segmentation of the input
label, first by tokenizing the label and then by finding all
relevant subsequences of this label. In practice, we simply
use an exhaustive method that returns all subsequences
of the label.

Phrase table augmentation (Sources). As phrase tables are typically very large (often of the order of several GB), loading
the phrase table can present a significant performance
bottleneck to the machine translation system. Thus, we
store all phrase tables in a database and load all potentially relevant translations into memory for each trans-
lation. In this stage we also include any translations that
come from other domain-specific sources.

Domain-specific feature extraction. These extend the baseline
translation model as described above by including
additional features for translation. These come from raw
scores from a ranker. The feature extractors job is to
reconcile this with the scores from various sources. In
particular, we introduce a ranker based on Cross-Lingual
Explicit Semantic Analysis (CLESA).

Domain language model adaptation. We also include a language model, which can be built either based on the input
domain ontology, or reuses an existing domain ontology
model.

Decoder. The decoder combines all the phrase tables augmented
with the additional features from the rankers along with
the language model to find an (approximately) optimal
translation.

The architecture is implemented in Java in a modular fashion
and is available for download at http://github.com/monnetproject/
translation. The overall architecture is depicted in Fig. 2.

3. Domain-targeted phrase table augmentation

3.1. Automatic enrichment of the phrase table

The first part of our domain adaptation method for ontology
translation consists in the extraction of domain-specific translations from different resources. These additional translation candidates are added to the phrase table of the SMT system. A phrase
table contains pairs of translations from source to target language
as well as the first four features of the translation model for this
phrase. An excerpt of a phrase table for antigen from English to
Spanish would look as follows:
lex(ti|fi)

English Spanish
antigen antigen
antigen antigenico
antigen antigenicos
antigen antidoto
antigen antigeno de la
antigen antigenos

lex(ti|fi) p(fi|ti)

p(ti|fi)

We used DBpedia to extract the relevant titles and their
equivalents in other languages from domain-specific Wikipedia
articles. Further, the labels of the ontology are used to query the
Linguee Web service2 to yield parallel text in which the labels are
translated to other languages with the corresponding linguistic
context.

Fig. 2. The architecture of our machine translation system.

3.1.1. Domain terminology from Wikipedia

For the domain-specific terminology extraction we used the

datasets provided by the DBpedia project [5].

In order to improve translations of highly domain-specific
vocabulary,
the method described here derives a bilingual
domain-specific translation lexicon from the DBpedia datasets
(version 3.8).3

In particular, the method exploits the Articles Categories
dataset, which links Wikipedia titles to categories using the SKOS
vocabulary [13]. In order to extend the vocabulary of a specific
domain, the method uses the Wikipedia Pagelinks dataset, which
contains the internal links between Wikipedia articles as well
as the Inter-Language Links dataset, which contains interlanguage
links between many Wikimedia projects. From these datasets, the
method extracts the following information: the Wikipedia article
titles, the variants and the translations of article titles, and the
categories associated with these articles. With this information,
we build a cross-lingual terminological lexicon, exploiting two
approaches:
(a) domain detection of the ontology (bottom-up approach);
(b) extraction of cross-lingual terminology (top-down approach).
In our first step, the method uses the DBpedia knowledge
base to determine the domain of the ontology. The bottom-up
approach consists of representing the domain by the most frequent
categories associated with the vocabulary to be translated. For this
approach, the labels, which are extracted from the ontology, as
well as all token subsequences (i.e., n-grams) are used to query the
DBpedia knowledge base. If a label or n-gram exactly matches an
article title, all categories associated with this article are collected.
This results in a list of categories together with a number of labels
or n-grams which support that category. We refer to the number of
distinct n-grams that generate the category simply as the category
frequency. An example is given in Table 1.

After collecting all categories, categories which are not relevant
for the domain are filtered out. This is performed heuristically
by eliminating those categories which have a frequency lower
than or equal to the mean frequency. When calculating the mean
frequency, categories with a frequency of 1 are ignored to prevent
the mean from being artificially low.

That is, we only consider categories c for which the category


frequency fc fulfils the following:

fc

fc >

cC,fc >1

|{c  C : fc > 1}| .

2 http://www.linguee.com/.

3 http://wiki.dbpedia.org/Downloads38.

J.P. McCrae et al. / Web Semantics: Science, Services and Agents on the World Wide Web 36 (2016) 2331

Table 1
Collected Wikipedia categories (prefinalCategoryList) based on
the extracted financial (sub-)labels.

Frequency Wikipedia category name

Generally accepted accounting principles
Debt
Accounting terminology
Economics terminology

Political science terms
Physical punishments

Table 2
Most frequent categories based on the German GAAP labels and
their Pagelinks (finalCategoryList).

Frequency Wikipedia category name

Economics terminology
Generally accepted accounting principles
Macroeconomics
Accounting terminology
Finance
Economic theories
International trade


Fig. 3. Steps of extraction Wikipedia titles and its translations.

In the next step, the list of collected categories is further
extended by exploiting Wikipedia links of each article the title
of which is equivalent to a label or n-gram of a label in the
ontology. For each of these articles, the categories of outgoing links
are selected and their frequencies are recalculated and filtered as
described previously.

For all the categories extracted as described above, as shown
for example in Table 2, all translations from/to German, Spanish
or Dutch from Wikipedia articles assigned to these categories
are extracted and stored in the bilingual lexicon. This extraction
process is shown in Fig. 3.

3.1.2. Domain-specific parallel resource acquisition from Linguee

In order to enrich the phrase table with additional domainspecific candidates, we built a new parallel corpus based on the
taxonomy vocabulary that we want to translate. For this we used
Linguee, a combination of a dictionary and a search engine which
indexes words and expressions from around 100 million bilingual
texts. Linguee search results display example sentences that show
how the expression searched for has been translated in context.

The Linguee bilingual dataset represents a very large collection
of manually translated sentences in English, German, Spanish,
French,
in

Italian and Portuguese collected from the Web,

particular from multilingual websites of companies, organizations,
universities and other sources,
including EU documents and
patent specifications. Recently Japanese, Chinese, Polish and Dutch
bilingual data was added.

For generating a domain-specific parallel resource, the Linguee
search engine was queried with labels extracted from our
ontologies. For each query, Linguee provides aligned parallel
sentences through their web service. We extracted the output and
stored the source and target sentences separately, which were
finally used to build domain-specific translation models.

The domain-specific translation models and language models
were used in the ontology translation process independently as
well as in combination with other domain adaptation techniques
(see Section 6.5).

3.1.3. Domain terminology from IATE

In addition, our method allows for the incorporation of further
terminological resources. In particular, we included a very large
multilingual term base called Inter-Active Terminology for Europe
(IATE)4 as an additional source of translations. IATE is the European
Union (EU) inter-institutional terminology database. It contains
all the existing terminology databases of the EUs translation
services. However, the multilingual term-bases may contain
several possible translations for a single term in different domains,
and disambiguation is required while adding the translations from
such a multi-lingual term base. For this purpose, we rely on
Cross Lingual Explicit Semantic Analysis (CLESA) as an additional
semantic ranker to the translation architecture.

4. Domain-specific feature extraction

4.1. Cross-lingual Explicit Semantic Analysis

Explicit Semantic Analysis (ESA) was introduced by Gabrilovich
and Markovitch [7], and supports the comparison of texts with
respect to their semantic similarity by indexing the texts in a space
defined by explicit concepts. In contrast, other techniques such as
Latent Semantic Analysis [14] and Latent Dirichlet Allocation [15]
build unsupervised concepts by the correlations of the terms in the
data. ESA is an algebraic model in which the text is represented by
a vector of the explicit concepts. The magnitude of each dimension
in the vector is the associativity weight of the text to that
explicit concept/dimension. In order to quantify this association,
the textual content related to the explicit concept/dimension is
utilized. This weight can be calculated by considering different
methods, for instance, we utilized the Lucene scoring function.5 A
possible way of defining concepts in ESA is by means of using the
Wikipedia titles as dimensions of the model and the corresponding
articles for calculating the associativity weight [7], thus taking
advantage of the vast coverage of the community-developed
Wikipedia.

A compelling characteristic of Wikipedia is the large collective
knowledge available in multiple languages, which facilitates an extension of the original ESA model to accommodate multiple languages called Cross-lingual Explicit Semantic Analysis (CLESA) [8].
The articles in Wikipedia are linked together across languages and
this cross-lingual link structure can provide a mapping of a vector
in one language to the other. Thus, Wikipedia provides the comparable corpus in different languages, which is required by CLESA.

To illustrate CLESA, let us take two ontology labels, f in the
source language and t in the target language. As a first step, a

4 http://iate.europa.eu/.
5 http://lucene.apache.org/core/3_6_2/scoring.html.

concept vector for f is created using the Wikipedia corpus in the
source language based on the tokens used in the label. Similarly,
the concept vector for t is created in the target language. Then,
one of the concept vectors can be converted to the other language
by using the cross-lingual mappings provided by Wikipedia.
After obtaining both of the concept vectors in one language, the
relatedness of the ontology labels f and t can be calculated by using
cosine product, as in monolingual ESA.

MT systems implicitly use the local context for a better lexical
choice during the translation [16]. Accordingly, it is natural to
assume that a focused Word Sense Disambiguation (WSD) system
integrated into an SMT system might produce better translations.
We follow an approach in which we directly incorporate Word
Sense Disambiguation into the SMT system as a multi-word phrasal
lexical disambiguation system [17]. The WSD probability score
calculated by using CLESA is added as an additional feature in
the log-linear translation model. The CLESA based score would
depend on the ontology context into which the label is embedded,
which could thus be exploited when determining the appropriate
translation of the label. The CLESA score is then included as an extra
feature in Eq. (1). The score is applied to calculate the semantic
similarity between the ontological context of the ontology label as
a bag-of-word vector containing labels of neighbouring classes and
the translation candidates considered by the SMT system.

5. Domain language modelling adaptation

The language model calculates the probability of a given sentence in the target language by means of an n-gram approximation
to the probability of translation. That is

p(w1 . . . wm) = 

i=1,...,m

p(wi|wi1 . . . wmax(in,1)).

These probabilities can be estimated simply by counting the
occurrences in the corpus of an n-gram in order to obtain an
unnormalized count c(w1 . . . wn). The conditional probability can
then be obtained as usual by
p(wn|w1 . . . wn1) = c(w1 . . . wn)
c(w1 . . . wn1)
where  indicates any word. Furthermore, we can consider that
these counts are obtained by summing over a corpus consisting
of a set of documents D = {d1, . . . , dn}. The count can thus be
expressed as follows:

c(w1 . . . wn  dj).

c(w1 . . . wn) =

djD

 c(w1 . . . wn) =

djD

For the purpose of domain adaptation of the language model,
we estimate the relevance of each document to our input ontology
O by means of a similarity metric sO(dj). In this way, we can obtain
a modified count as follows:

sO(dj) c(w1 . . . wn  dj).

Thus, our language model obtains the probability with the

modified formula6:

p(wn|w1 . . . wn1) =  c(w1 . . . wn)
 c(w1 . . . wn1)

After experimenting with a number of metrics, we found that
the most effective measure of similarity between ontology and

6 This method is labelled LM in our results.

Table 3
The size of the ontologies used in our evaluations.

Ontology
IFRS 2009
DE-GAAP

Labels

) = tf T

O tfdj
tfO tfdj

document is given by the cosine similarity of the word frequency
vector of the ontology and the document:
sO(dj) = cos(tfO, tfdj
where tfO represents the normalized word frequency of all words
occurring in labels of entities in the ontology, and tfdj represents
the normalized word frequency of all words occurring in the
document dj, i.e.,
(w) = |{w  dj}|
|{w  di}| .

tfdj


If we assume that we have documents that are aligned across
languages, for example Wikipedia articles aligned across topics,
then we can make a further assumption that the similarity sO
should be approximately equal regardless of which language
the document is in. Building on this assumption, we calculate
the similarity of each document to the ontology in the foreign
language, but use these scores to generate n-gram counts in the
translation language. The above mentioned unmodified counts are
finally smoothed using Modified KneserNey smoothing [18].

6. Experiments and results

In this section, we discuss the evaluation of the system with
domain-specific ontologies. We first introduce the test data and
evaluation metrics used in our experimentation. Then we describe
the experiment and discuss the obtained results.

6.1. Datasets

We applied our ontology translation system to two domains,
firstly a financial domain, where the ontologies are expressed in
the XBRL standard [19], and secondly to a set of ontologies describing public services, provided by partners in the Monnet project. For
the financial domain, we select two ontologies corresponding to
the 2009 International Finance Reporting Standard (IFRS 2009) and
the German Generally Accepted Accounting Principles (DE-GAAP).
We use DE-GAAP as a development ontology to apply our domain adaptation and tested on IFRS in the only common language
pair, i.e. EnglishGerman. The public service ontologies are smaller
and we refer to them only by abbreviation (LAG, RB and HB) (see
Table 3).

6.2. Evaluation methodology

The evaluation methodology was as follows: when translating
an ontology from language f into language t, all labels in language
t were removed from the ontology. These eliminated labels
were then used as reference standard to evaluate the translation
proposals made by the system with respect to standard metrics
used in MT research. The evaluation of machine translation is a
difficult task as there are many possible valid translations for an
input label and the reference translation we use to evaluate the
score represents only one possible translation. Thus, we use a
collection of widely used evaluation metrics. The list of metrics is
as follows:

J.P. McCrae et al. / Web Semantics: Science, Services and Agents on the World Wide Web 36 (2016) 2331

Table 4
Number of extra relevant translations found from sources.

6.4. Results for combined system

Source
Wikipedia

Language pair
English  German
English  Spanish
English  Dutch
English  Spanish
Spanish  English
English  German
German  English
English  Dutch
Dutch  English

Translations

METEOR Metric for Evaluation of Translation with Explicit

Bilingual Evaluation Understudy [9].

Ordering [20].
The metric as used in the NIST evaluations [21].
Position-independent error rate [22].
Translation edit rate [23].

WER Word error rate [22].

As we have found in previous studies that BLEU can be
unfairly sensitive to short labels, leading to a poorer correlation
with human judgement of translations [4], we also introduced a
modified version of BLEU, i.e. BLEU-2, which considers only the
precision of 1 and 2-grams in evaluating translations. The reason
why the BLEU metric is not suited to our task is that it computes
the product of the precision for 1, 2, 3, and 4-grams as an aggregate.
However, many of our labels consist of less than 4 words, which
would lead to BLEU scores that are zero if the 4-gram precision
is zero (as BLEU is a product of each precision score), which is
frequently the case if we have a very small sample size for 4-grams.
BLEU scores are difficult to interpret but it is generally believed
that translations under 0.15 are of too poor quality to be useful in
any application and it has reported that the performance of human
translators in Wizard of Oz settings is 0.650.75 [23].

It is important to note that as PER, TER and WER are error rates,

smaller values represent better translation quality.

6.3. Experiments

For generating the translation models from source to target
language, we used the statistical translation toolkit Moses [6]. As
we aimed to improve the translations only on the surface level,
we did not use any additional processing modules of Moses. Word
alignments were built with the GIZA++ toolkit [24], where the
5-gram language model was built by SRILM with KneserNey
smoothing [25].

For our experiments we used each of these systems developed
on the datasets as described above. For the Linguee approach,
we built a parallel corpus with around 24,247 aligned sentences
for the German GAAP ontology. The number of extra translations
generated in domain lexicons is shown in Table 4.

The CL-ESA model used 51,093 articles from a Wikipedia
snapshot (October 2012), which were selected on the basis of
their length and availability in all of the four considered languages
(English, Spanish, German, and Dutch). The articles having less
than 100 words were discarded. We tokenized and lower-cased
the remaining articles, removed the stop words, and applied a
stemmer before indexing.

The language model was adapted to a set of 452,754 Wikipedia
articles in English and Spanish, and 456,496 articles in English and
German using the method described in Section 5. These articles
were those that were linked to another article in the other language
by means of the articles in other languages link and contained at
least 100 words in each language. Terms with a frequency under 5
were replaced with an unknown token symbol in the corpus.

We present the results for our systems as follows:

Baseline The baseline system trained with the general purpose

Europarl corpus [26].

pedia as described in Section 3.1.1.

Wikipedia Lexicon Using the extra terms extracted from WikiLinguee + Wikipedia Using extra terminology from Linguee as

described in Sections 3.1.2 and 3.1.1.
Using extra terminology from IATE as described in
Section 3.1.3.

CLESA Using the explicit semantic analysis feature as described

All

in Section 4.1.
Using the language model adaptation procedure as
described in Section 5.
The combination of the LM adaptation, CLESA features,
IATE and either the Linguee or Wikipedia Lexicon.

Table 5 shows the results of the different settings on the financial ontologies, translating from English to Spanish as well as Spanish to English. Table 6 shows the results on the financial ontologies,
translating from English to German and German to English. Table 7
show the results of the evaluation when translating the public service ontologies from English to Dutch and Dutch to English. For
each experiment we verify the significance of the improvement
of the systems by means of bootstrap resampling [27] and found
the improvement between the baseline system and the combined
(All) system to be significant at a 99% level.

For the financial translation from English to Spanish we see that
the combined system outperformed each of the other systems in-
dividually, and similarly we see for English and German that nearly
all metrics (except for one) show an improvement for the combined system. We further note that the largest single improvement
comes from the domain lexicon approach, as discussed below.

For the public service ontologies we see a much less clear
result, although in this case there is still a notable improvement for
the domain lexicon method. Our hypothesis for this difference in
results is that the public service ontologies contain rather general
and not particularly domain-specific language. As a corollary, this
leads to the hypothesis that our methods are appropriate in the
context of ontologies containing very specific domain terminology.
Further, we note that the register of some of the public service texts
differ from the training material in particular in the use of informal
forms (e.g., the Dutch pronoun je) and this may have affected the
impact of the other methods.

6.5. Discussion

The goal of this evaluation was to compare a baseline SMT
system to a SMT system extended by the different components
proposed in this paper, both in isolation and in combination.
According to the results we observe the following:
1. When domain lexicon adaptation is not applied, the results
given by the other techniques in isolation do not differ
significantly from the baseline and it is not possible to conclude
which option is best in general.

2. When domain lexicon adaptation was applied, the results
improved with respect to the baseline. In this setting, the
addition of the other techniques (CLESA, LM) improves the
results even further.

3. The best results are generally produced by the combination of

all the techniques.
Therefore, we can see that domain lexicon adaptation improves
the translation results most notably, but also serves as an
activator of the other techniques. It should be noted that CLESA

Table 5
Results for translating ontology labels from English to Spanish and Spanish to English on the financial ontologies using
different settings.

Method
English to Spanish
Baseline
Linguee + Wikipedia

All
Spanish to English
Baseline
Linguee + Wikipedia

All

BLEU-2

METEOR

Table 6
Results for translating ontology labels from English to German and German to English on the financial ontologies using
different settings.

Method
English to German
Baseline
Linguee + Wikipedia

All
German to English
Baseline
Linguee + Wikipedia

All

BLEU-2

METEOR

and LM Adaptation do not produce new candidate translations
themselves;
instead, they aim to optimize the selection of
the translation candidates, which is possible when translation
candidates of a better quality are produced by the addition of a
domain lexicon into the system.

7. Related work

Ontology localization consists of two main tasks. The first task
involves finding an appropriate translation for the lexical layer of
the ontology, i.e., for all the labels in the ontology. The second
task is the adaptation of the ontology to the  possibly slightly
different  conceptualization of the target community that is
supposed to use the localized ontology. In this paper we have been
concerned with the first task only. A previous system concerned
with this task was the LabelTranslator system [28], which was
developed as a plug-in for the NEON project.7 The LabelTranslator
system essentially relied on a rule-based approach as well as many
translation candidates collected from external resources and web
services such as Google Translate, EuroWordNet [29] and KMI
Watson [30] as well as techniques for ranking these translations
given the ontological context. An important bottleneck is the
limited availability of online web translation systems and online
dictionaries. LabelTranslator used basic lexical template rules to
attempt to tackle this sparsity issue by means of limited transfer
grammars, which does not generalize well.

7 http://www.neon-project.org.

For ontology localization, a similar task is the one of finding
an alignment between existing ontologies which have different
conceptualizations of the same domain but in different languages.
These tasks frequently build on a label translation system, and thus
convert the task of finding a cross-lingual alignment to that of
finding a monolingual alignment among translated labels [31,32].
In a similar direction, Carpuat et al. [33] derived cross-lingual
alignments between English and Chinese WordNet by means of a
distributional similarity approach.

There has also been much research in the process of finding
translingual semantic representations, starting with methods that
merge parallel corpora to obtain a translingual representation by
means of latent topic modelling methods such as Latent Semantic
Analysis [34] and Latent Dirichlet Allocation [35]. These methods
can be used to estimate the latent similarity between ontology
labels in different languages, but until recently such approaches
did not yield results that outperformed direct translation [36]
in comparable tasks.
In particular, recent methods such as
Orientated Principle Component Analysis [36], Kernel Canonical
Correlation Analysis [37, CCA] and Orthonormal Explicit Topic
Analysis [38, ONETA] calculate a translingual representation by
means of estimating the correlation between term frequencies in
a document-aligned corpus and have been shown to outperform
dictionary-based machine translation [39].

There have been several systems developed for the adaptation
of machine translation systems to different domains. For example,
Koehn and Schroeder [40] showed that the quality of a machine
translation system can be improved by interpolating a small
amount of in-domain knowledge into either the language model

J.P. McCrae et al. / Web Semantics: Science, Services and Agents on the World Wide Web 36 (2016) 2331

Table 7
Results for translating ontology labels from English to Dutch and Dutch to English on the public services ontologies using
different settings.

Method
English to Dutch
Baseline
Wikipedia Lexicon

All
Dutch to English
Baseline
Wikipedia Lexicon

All

BLEU-2

METEOR

or the translation model. Another approach consists in using
in-domain text along with a cross-lingual similarity method to
mine translations that are specific to the domain, for example
by the use of CCA [41]. A similar approach is to use a small
amount of (unaligned) in-domain text to sample an in-domain
section of a larger parallel corpus [42]. Furthermore, there are
many approaches that are designed to adapt language models
to domains. Bellegarda [43] characterizes these into three main
classes: firstly, interpolation models, which attempt to combine
a small in-domain language model with a larger general domain
model (e.g., [40]). Secondly, Bellegarda describes constraintbased models which combine a general domain model with
an in-domain model while maximizing some criteria, generally
Minimum Discriminative Information [44]. Finally, Bellegarda
describes models, which extend existing topic models, to give
probabilities for n-grams not just bag of words [45]. Nevertheless,
none of such methods explicitly exploit the ontological context and
are specifically designed to deal with short labels.

We note that since the submission of this article one of the
authors has continued to develop some of the methods presented
in this paper into the OTTO translation system that can be used
online8 [46,47].

8. Conclusion

We have tackled the problem of ontology localization/
translation and presented a framework for domain adaptation
which factors in the ontological context to provide appropriate
translations of labels in an ontology. The domain adaptation framework has been implemented on top of an existing state-of-the-art
and off-the-shelf machine translation system. We have in particular presented three techniques for domain adaptation to a given
ontology: (i) extraction of a bilingual dictionary from external resources such as Wikipedia, from existing parallel corpora as well
as from third-party terminologies, (ii) exploiting a semantic similarity measure to rerank translation candidates, thus supporting
disambiguation, and (iii) tuning of a monolingual language model
to the ontology. We have presented experiments on five ontologies showing the impact of each of these methods. As one interesting result we have shown that the methods used in combination
indeed improve the quality of translations. However, the above
mentioned techniques (ii) and (iii) only yield an impact if method
(i) is used as well. The reason for this is essentially that methods (ii) and (iii) are ranking/scoring techniques that can perform

8 http://server1.nlp.insight-centre.org/otto/.

domain-specific disambiguation, but cannot introduce new translations themselves. Method (i) in contrast introduces new domainspecific translation candidates that can then be assigned a score to
which methods (ii) and (iii) contribute. Overall, we have presented
a new methodology for tuning an off-the-shelf statistical translation system to the task of translating the labels of a given ontology
into another language. All software is available as an extension to
the Moses system so that our methodology is of high practical value
as it can be used by any third party.

Our experiments have shown in particular that the approach
of building a new, domain-specific corpus showed a large impact
on the translation quality. Further, our approach has shown
that collaboratively created resources such as Wikipedia and
DBpedia can be successfully exploited to tune an SMT system
and provide higher quality translations in ontology localization
tasks. In addition to Wikipedia article titles with their multilingual
equivalents, Wikipedia holds much more information in the
articles themselves. Further work should investigate how to
exploit such non-parallel resources to improve the performance of
SMT systems.

Acknowledgements

This research was supported in by funding from the Monnet
project under European Union FP7 program under grant number 248458, the CITEC excellence initiative funded by the DFG
(Deutsche Forschungsgemeinschaft) (EXC277), the Science Foundation Ireland under Grant Number SFI/12/RC/2289 (Insight).
