Web Semantics: Science, Services and Agents on the World Wide Web 3738 (2016) 2535

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

journal homepage: www.elsevier.com/locate/websem

The Graph of Things: A step towards the Live Knowledge Graph of
connected things
Danh Le-Phuoc a,, Hoan Nguyen Mau Quoc a, Hung Ngo Quoc a, Tuan Tran Nhat a,
Manfred Hauswirth b
a Insight Centre for Data Analytics, National University of Ireland, Galway, Ireland
b Institut fur Telekommunikationssysteme, Technische Universitat Berlin, Berlin, Germany

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 8 February 2015
Received in revised form
1 January 2016
Accepted 29 February 2016
Available online 11 March 2016

Keywords:
Internet of Things
Graph of Things
Linked Stream Data
Real-time search engine

The Internet of Things (IoT) with billions of connected devices has been generating an enormous amount
of data every hour. Connecting every data item generated by IoT to the rest of the digital world to turn
this data into meaningful actions will create new capabilities, richer experiences, and unprecedented
economic opportunities for businesses, individuals, and countries. However, providing an integrated view
for exploring and querying such data at real-time is extremely challenging due to its Big Data natures: big
volume, fast real-time update and messy data sources. To address this challenge, we provide a unified
integrated and live view for heterogeneous IoT data sources using Linked Data, called the Graph of Things
(GoT). GoT is backed by a scalable and elastic software stack to deal with billions of records of historical
and static datasets in conjunction with millions of triples being fetched and enriched to connect to
GoT per hour in real time. GoT makes approximately a half of million stream data sources queryable
via a SPARQL endpoint and a continuous query channel that enable us to create a live explorer of GoT
(http://graphofthings.org/) with just HTML and Javascript.

 2016 Elsevier B.V. All rights reserved.

1. Introduction

International Data Corporation (IDC) reports that the digital
universe will grow by a factor of 300 from 2005 to 2020. Specif-
ically, IDC projects that by 2020 the digital universe will reach
40 zettabytes (ZB), which is 40 trillion GB of data or 5200 GB
of data for every person on Earth.1 The majority of this data
will be contributed by billions of devices connected to the Internet of Things (IoT). Connecting every data item generated by
IoT to the rest of the digital world to turn this data into meaningful actions will create new capabilities, richer experiences and
unprecedented economic opportunity for businesses, individuals
and countries. However, deriving trends, patterns, outliers and
unanticipated relationships in such enormous amount of dynamic
data with unprecedented speed and adaptability is extremely
challenging.

 Corresponding author.
1 http://www.emc.com/collateral/analyst-reports/idc-the-digital-universe-in-
2020.pdf.

E-mail address: danh@danhlephuoc.info (D. Le-Phuoc).

http://dx.doi.org/10.1016/j.websem.2016.02.003
1570-8268/ 2016 Elsevier B.V. All rights reserved.

Towards unlocking the huge potentials of IoT,2 deriving insights
from dynamic raw IoT data poses various challenges in data
integration. As seen on the Web, access to and integration of
information from large numbers of heterogeneous IoT streaming
sources under diverse ownership and control
is a resourceintensive and cumbersome task without a proper support. Such
distinct streaming data sources are generated from distributed
data acquisition infrastructures of Smart Cities, Social network
applications, medical sensors, to name a few. Traditionally, to
correlate and analyse them into higher level data products, they
need to be transformed, cleaned and consolidated into a large
static data warehouse and then made ready for certain types of
rudimentary query patterns, e.g. OLAP queries. However, these
streaming sources operate on longer time scales on which a
wide range of dynamic data feeds are continuously arriving from
disparate and uncontrolled sources [1]. Hence, it is much more
difficult to maintain a fresh and consistent integrated view for

http://www.mckinsey.com//media/McKinsey/dotcom/Insights/Business%

20Technology/Unlocking%20the%20potential%20of%20the%20Internet%20of%
20Things/Unlocking_the_potential_of_the_Internet_of_Things_Full_report.ashx.

D. Le-Phuoc et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3738 (2016) 2535

unlimited discovery and exploration of enormous ever-growing
IoT data.

Motivated by such challenges and inspired by the Knowledge
Vault [2], we create a Live Knowledge Graph to pave the way
towards building a realtime search engine for the Internet of
Things, which we call the Graph of Things (GoT). Similar to the
Knowledge Graph used in search engines like Google3 and Bing,4
GoT aims at enabling a deeper understanding of the data generated
by connected things of the world around us. To tackle the barriers
of IoT data interoperability, GoT is represented as Linked Stream
Data [3] which employs the RDF data model to use the graph as the
unified representation for stream data together with static data.
This graph enables a smarter way to discover and explore IoT data
under meaningful facts and their relationships.

The effective exploitation of Linked Stream Data from multiple
sources requires an infrastructure that supports the intensive
effort of enriching,
linking and correlating of data streams
with very large static data collections, e.g. LinkedGeoData5
and DBpedia.6 Moreover, this infrastructure needs to support
sophisticated queries, discovery and exploration on increasingly
complex data objects representing realistic models of the world.
Therefore, we propose a scalable and elastic solution for ingesting,
storing, exploring and querying billions of dynamic IoT data points
in conjunction with the static datasets from Linked Data Cloud.7
Our solution also provides an integrated architecture to collect and
curate useful RDF-based facts from IoT raw data to create a graph
that plays the role as a unified and live view of data objects about
Things. This solution aims to provide a comprehensive software
stack with the easy-to-use toolkits to filter, aggregate, enrich and
analyse a high throughput of data from multiple disparate live
data sources in any data formats. Such data operations are gearing
towards identifying simple and complex patterns to visualise
business in real-time, to detect urgent situations and to automate
immediate actions. To deal with the heterogeneity of dynamic data
sources, Linked Stream Middleware [4] of this software stack will
transform the data in a variety of data formats and data sources to
make it ready for any further processing and high-level analytical
operations. Its parallel processing layer of the stack will be able
to elastically and dynamically distribute the processing load to
the cluster to cope with a large amount of queries and incoming
streams while a huge volume of data is already in the store.
Our back-end data management system supports the ingestion of
million data points per second while it is still able to query live data
being indexed to a distributed persistent storage which currently
stores billions-triple datasets of historical data as well as static
datasets.

The remainder of the article is structured as follows. Section 2
presents the process of building the Graph of Things from both
physical world and virtual world. In Section 3, we will share our
design choice of building the system and infrastructure to store and
query GoT. Section 4 shows some demonstrations and evaluations
of our live system and share some lessons learnt during the course
of 10 months of deployment. Finally, Section 6 will conclude the
article and reveal our future plan for GoT.

3 http://www.google.ie/insidesearch/features/search/knowledge.html.
4 https://blogs.bing.com/search/2013/03/21/understand-your-world-with-
bing/.
5 http://datahub.io/dataset/linkedgeodata.
6 http://dbpedia.org.
7 http://datahub.io/group/lodcloud.

2. Building a live knowledge graph of connected things

To motivate the creation of the Graph of Things, we start with
a real-time search use case as follows. For example, John has just
missed his connection flight at Dublin airport, he has few hours
to spare in the sunny weather. He intends to take the bus to his
favourite Sushi restaurant in the city centre, but he finds out there
is no bus today due to the current strike of Dublin Bus. To avoid the
current traffic situation in the city centre, he searches for some
other Sushi restaurants that can be reached from the airport in
10 min by taxi. From the list of recommended restaurants, there is
a Sushi restaurant next to Dublin Ferry Port where an old classmate
of John from Liverpool will be arriving in 1 h on a ferry. Right after
finishing the dinner with his friend, he is notified that his next
connecting flight will be delayed for another 2 h, and he discovers
there is an open-air music show just few blocks from where he is.
From the live camera feed pointing to the show, there is a big exciting
crowd, he then decides to take a walk there.

In order to provide such live connected information to a user,
we have to continuously fuse various IoT stream data sources
such as flights, ships, traffic cameras, Tweets and weather sensors
into GoT as an integrated view. GoT not only comprises of
some data captured by sensors but also involves the context, the
meanings and relationships between data objects. In particular,
GoT is represented as a big connected RDF graph which enables
applications, users and developers to traverse along graph edges
(RDF triples) without a restriction on a database schema or fixed
connections among data items. The search engine based on this
graph can benefit greatly from direct access to these nature
relationships to have an intrinsic view of real world events and
phenomena. Therefore, the graph can provide smarter search
results which might lead the users curiosities to new relevant
topics. In the above use case, the returned data entries to the user
like flights, Sushi restaurants and events of interests are graph
nodes which are connected to other nodes as the potential data
of interest for the user. For instance, the restaurants, the music
show events and the ferry have live spatial relationships can
trigger the interesting correlations according to the users contexts.
Such correlations can be queried by using an extension of SPARQL
1.1 with spatial and temporal filtering conditions for expressing
spatialtemporal context of a query. While we defer the technical
details on how to express and process such queries, we will present
how we model and integrate our IoT data sources summarised in
Table 1 in the following sub-sections.

2.1. Collecting facts of physical things

For the physical things equipped with sensors to observe
facts about them, we use SSN Ontology [5] to capture sensor
readings associated with the sensing context including sensor
configuration and meaning of what to be measured, etc. To have
a richer contextual information, we interlink several datasets
from Linked Data Cloud to create meaningful relationships to
the sensors, properties and features of interest. For instance,
we generate spatial contexts by extracting relevant spatial data
from LinkedGeoData, Geonames dataset and other relevant known
concepts or entities from DBpedia (which are integrated in
YAGO [6]). These relationships play the pivotal roles for correlating
sensor readings that share certain contextual links.

The added value of interlinking with the relevant known
entities from DBpedia and YAGO is enabling the exploratory search
for users who are unsure about their goals in the first place. For
instance, a user looks for relevant real-time sensor data of a blizzard
happening in his/her city. The easiest way is to start a keyword
search, blizzard, which can returns relevant properties of the YAGO
entity, <wordnet_blizzard_111509570>. By following this entity,

Fig. 1. A stream subgraph based on SSN ontology.

this user can find the relevant sensing sources such as wind speed,
temperature, snowfall as by its definition, a blizzard is a storm with
widespread snowfall accompanied by strong winds. Furthermore,
YAGO and DBpedia have several links to other blizzard events
happening in the past. If GoT stores such sensor readings during
those events happened, the user will be able to compare the
current windspeed, snowfall, etc., with the corresponding ones
measured at the same time with the past blizzard events (see
how we archive historical data in below). When a new sensor data
is added to GoT, the contextual data of each sensor is added to
GoT as a subgraph providing more potential exploration links to its
sensory data. The example illustrated in Fig. 1 is sensor metadata
of a weather station that observes the temperature and humidity at
Dublin Airport. It captures the context in which the sensor readings
are obtained to generate the graph-based stream data from the
time-varying sensor readings. These readings have links to their
meanings, e.g. : tempValue (18 Celsius) is the temperature of Dublin
Airport at 21:32:52, 09/08/2011. Each reading plays the role as a
stream subgraph attached to GoT when a sensor reading is fed into
GoT from the original sensor source.

Table 1 lists the dominated sensor data sources in our
catalogue. The sensing objects column presents the number of
physical objects (places, aircrafts, ships, etc. ) that the sensing
sources observe. These sensing data sources are fetched to GoT
in different update rates depending on their nature of values,
frequency of updates and the distribution of data sources. We
have been archiving all fetched data since June, 2014. Among
them, the NOAAs Climatic Data Center8 (NCDC) provides 100 years
of meteorological data sources from 20,000 weather stations.9
However, we decided to limit the queryable archived window
back to 1971 due to the storage and processing capability of our
cluster. In addition to NOAA, LSM sensor data sources [4] offer
weather data of 60,000 places around the worlds via Web-based
APIs from weather data providers. For aircrafts and ships, they both
use transponders to identify them by broadcasting or exchanging

Table 1
IoT stream data sources of the graph of things.

Sources

Camera
Flight
Ship
Twitter
Others

Sensing objects
21k
60k
45k
317k
20k

5,4k

Updates/hour
21k
28k
Live
81.6k
240k
67k
12k

Archived window
Since 1/1971
Since 6/2014
Since 8/2014
Since 8/2014
Since 2/2015
Since 8/2014
Since 8/2014

their information (identities, coordinates, speed, etc.) with radars
or other receivers. Such radars or receivers then relay received
information to a gateway on which we subscribe or fetch data
from. We also build a spider to scrap the webpages that provide
live camera feeds. As a result, we currently have 45,000 camera
feeds. However, due to the size of data and legal issues, we only
provide live feeds and do not archive any camera data. On top
of that, thanks to open and public data projects, we have several
data sources that cover some small areas (a city or a country). For
example, several smart city projects such as Dublin, London and
New York publish a wide range of sensor data sources such as train,
bike and bus information.

2.2. Expand the knowledge graph to social things

Similar to Citizen Sensing [7,8] and Social Sensing [9,10], we
consider social media such as Twitter, Facebook, RSS feeds as
Social Things which sense events and information from Web
citizens. Therefore, we extend the SSN ontology [5] to model a
social media extractor as a sensor. Instead of extracting only the
Tweet texts, we use natural language processing (NLP) tools such as
StanfordNER10 [11] and IllinoisNER11 [12] to extract named entities
and then enrich them with meaningful concepts and relationships
as RDF triples. Fig. 2 illustrates a snapshot of Tweet data that
is extracted as RDF triples. In this diagram, the extracted RDF

8 http://www.ncdc.noaa.gov/.
9 http://www.ncdc.noaa.gov/data-access/quick-links.

10 http://nlp.stanford.edu/software/CRF-NER.shtml.
11 http://cogcomp.cs.illinois.edu/page/run_demo/NER.

D. Le-Phuoc et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3738 (2016) 2535

Fig. 2. A social sensor reading snapshots of tweet stream.

subgraph is driven by SSN ontology, NERD ontology12 and spatial
context in the Tweet.

Currently, we choose Twitter for social network streams and
popular RSS news channels, such as BBC, CNN, Yahoo! News for RSS
feed streams. To provide useful relationships to Physical Things,
we prioritise the semantic facts relevant to RDF entities generated
from connected things such as location-based information, events
and traffic situations. In particular, for Twitter streams, the system
crawls Tweets related to physical sensors in our database judged
by their locations and other relevant metadata. However, there are
only roughly 4% of tweets that explicitly have geo-tags (longitude
and latitude values). To increase this rate, we use the enriching
process in Fig. 3 by using NLP techniques on the text of a
Tweet. Firstly, raw tweets are crawled and stored as json files
by using TwitterAPI.13 Next, raw text of tweets are tagged with

12 http://nerd.eurecom.fr/ontology.
13 https://dev.twitter.com/overview/api.

the entities of interest. Similar to Tweet processing, RSS feeds
are also annotated by NLP tools to connect to YAGO ontology.
However, RSS feeds do not directly include the location data. The
mentioned locations of RSS feeds are only extracted by linking with
GeoNames entities in YAGO. These linked entities will have spatial
properties like http://www.w3.org/2003/01/geo/wgs84_pos#lat
 and http://www.w3.org/2003/01/geo/wgs84_pos#long .

The NLP tools recognise named entities of three types (including
person, location and organisation) from plain text of social media
messages, such as Twitter, Facebook or RSS Feeds [13]. The tagged
entities are selected as the candidates to go through the next step
judging by its frequency of appearance in the crawled Tweets
within the recent time window, e.g. 2 h. The tagged entities will
then be verified by querying YAGO ontology to determine whether
they can be linked with the ontology or not. If we find matched
items in YAGO for a tagged entity, we then extract relevant
geographical information as well as other metadata, e.g. relevant
Wikipedia entries from the YAGO ontology. Again, the reason for
linking with YAGO and DBpedia is to enable exploratory search

Fig. 4. Layered architecture.

Fig. 3. Subgraph extracting process of social things.

from relevant known-entities of YAGO and DBpedia. Especially,
the synset relationships of WordNet are quite useful to lead a
user from a mentioned keyword in a Tweet to relevant sensor
readings, for instance, from blizzard or storm to snowfall, wind
speed and temperature. When having more than one matched
entities, for example, storm in Dublin city, we will use the spatial
context to determine whether Dublin, Ireland or Dublin, Ohio,
USA is the better candidate, e.g., the one closer to the Tweets
location is selected. However, this approach does not fully address
the disambiguation issues of the entity resolution process. We will
discuss this problem later in Section 4.3.

3. System design

From the description of the Graph of Things above, this section
will design a scalable and elastic architecture with the storage and
infrastructure that enable users/developers to continuously add
and query IoT data in near-realtime.

3.1. Architecture

To design the data management system for GoT, we decouple
its data processing flow by following the layered architecture
of our Linked Stream Middleware(LSM) [4] shown in Fig. 4.
The data consumption is handled in the Data Acquisition Layer
which provides a wide range of plug-in wrappers. These wrappers
transform and curate stream data from a variety of formats,
protocols and device platforms to link stream triples to the Graph
of Things layer (GoT layer). The GoT layer stores and indexes
RDF-based data in distributed persistent partitions together with
distributed in-memory storages of the processing cluster as
presented in Section 3.2. The GoT layer provides the data access
interfaces for two query processing engines, i.e., a customised
SPARQL engine [14] and CQELS engine [15,16]. These two engines
enable the application developers to query data via a SPARQL
Endpoint or a Stream Subscribing Chanel in the Application layer
respectively. The SPARQL Endpoint serves one-shot queries using
an extension of SPARQL 1.1 query language with spatial, temporal
and free text built-in functions. The Stream Subscribing Chanel
serves continuous queries using CQELS-QL query language [17]
via a WebSocket channel. CQELS Engine is a stream processing

engine which supports continuous queries over RDF stream data.
A continuous query is a long standing query which is continuously
triggered when a new relevant data arrives. Therefore, via the
WebSocket protocol, once a user subscribes a query in CQELS-QL,
a stream of output will be continuously fed back to the subscriber
as a stream.

In the Data Acquisition layer, the data is fetched or pushed
into the system via several protocols such as HTTP, FTP, TCP/IP,
WebSocket and MQTT. Then it is processed asynchronously on
distributed processing nodes. These processes are grouped by their
types of wrappers, e.g. a wrapper for collecting data from a xmlbased weather service or an RDF-based wrapper for extracting
Tweet streams. As the processing load of each wrapper type is
quite different, some of them can run on a single machine but
others have to be distributed across different machines. Whether
the data sources are fetched in a push-based or pull-based fashion,
the output of a processing wrapper is pushed as a stream to the
shared data bus in the RDF data model, called the Stream Graph
Data Bus. The stream data in this bus is serialised in RDF-Turtle or
JSON-LD to be ingested in the next step.

The processes of feeding data to the Stream Graph Data Bus
need to be fault-tolerant as the connections to external data
sources are unstable and the data processing wrappers are error-
prone. Moreover, the incoming stream throughput is fluctuated
and bursty, thus, the architecture of our data management has to
be able to serve a wide range of workloads with very low-latency
of reads and updates. Due to this requirement of robustness, we
realise the Lamda Architecture [18] as the processing flow showed
in Fig. 5.

All data entering the system is dispatched to both batch layer
and online layer for processing. The batch layer is responsible
for storing the master copy of the historical RDF streams (an
immutable, append-only converted RDF triples) which are used
to pre-compute the batch views specified by RDF Data Cube
Vocabulary.14 Then, in the serving layer, these pre-computed
views are materialised and indexed to store in HBase15 and
ElasticSearch16 so that they can be queried in low-latency and adhoc way. The online layer handled by Storm17 compensates for
the high latency of updates to the serving layer and deals with
recent data only. Any incoming query can be answered by merging
results from batch views and real-time views using CQELS Cloud
Engine [16] which coordinates HBase, ElasticSearch, Hadoop18 and

14 http://www.w3.org/TR/vocab-data-cube/.
15 http://hbase.apache.org/.
16 https://www.elastic.co/products/elasticsearch.
17 https://storm.apache.org/.
18 https://hadoop.apache.org/.

O30

D. Le-Phuoc et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3738 (2016) 2535

that are scheduled in the fetching cluster. The incoming data
is transformed to RDF data streams for further processing. RDF
stream data is routed to the Triple Analyser to process based
on Triple Patterns Recognition Rules which are predefined by
the user. Each Triple Patterns Recognition Rule is a set of triple
patterns defining the rule to extract the values from streaming
data. Extracted values will be indexed correspondingly based on
their characteristics and graph patterns. The stream subgraphs that
have spatial context will be routed to the ElasticSearch cluster to
index and the ones that have time series of numeric values will be
routed to the OpenTSDB cluster (Open Time Series Database). The
details of these two indexing mechanisms are presented in below.
Otherwise, the stream subgraphs will be stored in the normal triple
storage,i.e. JenaTDB.

3.2.1. Spatial-driven indexing

To enable the spatial computation on the spatial properties
attached to a subgraph, we store and index this subgraph as a
document in ElasticSearch storage as illustrated in Fig. 7. Along
with spatial attributes, ElasticSearch allows us to index datetime
and string attributes on which we can filter by a range condition. As
a result, a combination of spatial computation, free text search and
temporal filter are supported in our SPARQL endpoint (see example
queries in Section 4.1).

To build a document to insert to the ElasticSearch storage,
the Spatial and Text Entity Indexer will extract RDF triples which
consist of geographical information, text label, and datetime
value (Fig. 6). Triples filtered by these rules then will be constructed
as ElasticSearch records to store into ElasticSearch storage.
ElasticSearch supports both bulk and near real time (NRT) updates.
Bulk updates are accomplished using Hadoop Map/Reduce and
NRT are performed through direct HTTP calls.

3.2.2. Temporal-driven indexing

A large amount of sensor stream data of GoT is fed as time series
of numeric values such as temperature, humidity and wind speed.
Therefore, we choose OpenTSDB which uses HBase for hosting
very large tables. OpenTSDB can ingest millions of time series data
points per second. As shown in Fig. 6, the input triples which
comprises numeric values and timestamps are analysed based on
predefined temporal recognition rules in the Triple Analyser. The
Entity Indexer extracts numeric measurements to construct time
series rows to insert to HBase via parallel Time Series Daemons
(TSDs) running on different machines of our cluster.

Together with values and timestamps, some metadata can be
added to each datapoint of OpenTSDB. Such metadata can be used
to filter by the information encoded in the metadata. Therefore, we
extract spatial context and sensing context of the numeric sensor
readings as a temporal subgraph. This subgraph is used to construct
necessary metadata to add to the data points that will be then
inserted to OpenTSDB. The metadata chosen by their frequency
of use is used as filtering parameters in the SPARQL queries.
For example, the coordinate where the sensor measurements are
captured will be used to covert to a geohash value. This geohash
value is the used to encode as a filtering tag. Other properties such
as type of sensor, type of reading are also encoded as other filtering
tags.

4. Demonstrations, evaluation and lessons learnt

4.1. Demonstrations

We publish GoT as an open linked dataset, thus, we make GoT
publicly accessible via SPARQL endpoint at http://graphofthings.
org/sparql/. This SPARQL endpoint supports more powerful

Fig. 5. Lambda architecture for GoT data ingestion, publishing and querying.

Storm to handle the queries in SPARQL and CQELS-QL as mentioned
in above section. The architecture aims to be linearly scalable
whereby the elasticity is handled by the number of machines
added to the system.

3.2. Query-aware hybrid storage

The Graph of Things is exposed to the user as a single RDF graph,
however the native triple stores like Virtuoso or Jena TDB could
not scale to its number of RDF Triples and its update rates [4].
According to the researches shown in [19,20], the processing on
a Big RDF Graph could be parallelised efficiently by partitioning
the graph into smaller subgraphs to store in multiple processing
nodes. Moreover, in designing physical storage to store RDF triples,
property tables could reduce subjectsubject self-joins of the
triple table. They are very good at speeding up queries that can
be answered from a single property table [21]. However, using
property tables introduces a complexity by requiring the property
clustering to be carefully done to create property tables that
are not too wide but still being wide enough to answer most
queries directly. Moreover, ubiquitous multi-valued attributes
might cause a further complexity. Being aware of such advantages
and drawbacks of property tables, we design the storage of
subgraphs similar to property tables by grouping properties based
on the frequency of query patterns and the distribution of graph
patterns, called Query-aware Hybrid Storage.

The sensor readings fed into GoT have both spatial and temporal
context and most of the queries on GoT contain spatial and
temporal patterns. Therefore, we partition GoT based on the
RDF predicates that imply the spatial and temporal context. We
choose HBase and ElasticSearch as the underlying storages for such
partitioned subgraphs. Their flexible table structures enable us
to store subgraphs which share a similar graph shape. This also
solves the aforementioned issue of multi-valued properties that is
problematic for both traditional property tables and triple tables.
Such table structures can store a flexible number of attributes in
the same table without having to use list, set or bag attributes.
Moreover, HBase and ElasticSearch provide spatial and temporal
indexing schema that lead to two types of indexing mechanisms
introduced in following subsections. Other triple patterns that are
impossible to index in these types will be stored in a native triple
storage. We currently use JenaTDB to store such generic subgraphs,
e.g. static data. For the sake of boosting performance, static data is
less than 100 million triples, so, it can be easily loaded into RAM of
a standalone workstation. We envisage that a distributed solution
like [19,20] will be needed in the future when the static part of GoT
grows.

The incoming data from Stream Graph Data Bus is routed
to the destined storages via the data routing flow illustrated in
Fig. 6. The fetching operations are built as asynchronous tasks

Fig. 6. Data routing flow.

updates from data streams of GoT can automatically trigger the
relevant visualisation widgets to re-render without having to
interfere other parts of the applications.

We demonstrate the capability of managing a big volume of
data together with a high updating throughput by walking through
the process of building the live explorer on GoT using just HTML
and Javascript at http://graphofthings.org/. The GoT Explorer starts
with a Live View that summarises whats been happening in the
world as illustrated in Fig. 8. The HTML page will call the SPARQL
queries corresponding to a map area and a time range of interest
to fetch background information, e.g., locations, data types and
updating summaries of stream data sources that have readings in
60 min, to render information on the map.

For instance, [a] is the heat map aggregated from temperate
readings in last 1 h in the corresponding map area (geohash value
u0q). The query can be easily expressed by the following query.

Query 1. Heat map query

PREFIX temporal: <http :// jena.apache.org/temporal#>
SELECT *
{ ?v temporal:avg (1h-ago  u0q  temperature ).

The following query is an example of retrieving a list of
sensors according to a certain type and a filtering condition,
e.g. temperature sensors within a bounding box.

Query 2. Spatial query for sensor discovery

spatial: <http :// jena.apache.org/spatial#>

PREFIX
PREFIX geo: <http :// www.w3.org /2003/01/ geo/wgs84_pos#>
PREFIX dul: <http :// www.loa -cnr.it/ontologies/DUL.owl#>
PREFIX was: <http :// purl.oclc.org/NET/ssnx/meteo/aws#>
SELECT *
{?loc spatial:withinBox (dul: PhysicalPlace

67.033 -178.917 67.24 -177.67).

?loc geo:lat ?lat.
?loc geo:long ?long.
?sensor dul:hasLocation ?loc.
?sensor a aws: TemperatureSensor ;

To have a quick comparison on historical data of a selected set
of stream data sources, e.g. sensor measurements, the GoT explorer
provides a 3D layout of live thumbnails as shown in part [b] of
Fig. 8. A live thumbnail is a latest snapshot of the stream data
sources that are fetched via the SPARQL Endpoint. The 3D layout
can display much more information than the usual 2D one. For
example, the sphere layout in Fig. 8 can render 64 thumbnail charts
or even more. This is very effective when exploring and correlating
millions of stream data sources. The data to be fed into such charts

Fig. 7. Store spatial subgraphs as ElasticSearch document.

SPARQL query language than SPARQL 1.1. For spatial computation,
it supports a spatial extension for SPARQL query via Jena Spatial
built-in functions which are mapped to spatial computation
functions provided by ElasticSearch. For a query graph pattern
associated with time series data, we also support a temporal
extension for SPARQL which is backed by our modified version of
OpenTSDB. On top of that, full text search is supported by fuzzy
matching syntaxes of Lucene which are processed by ElasticSearch.
To support continuous queries over stream data of GoT, a Stream
Subscribing Channel is given via the web socket protocol at
ws://graphofthings.org/cqels/. Via this channel, a web client can
pose continuous queries using CQELS query language over stream
data to get the stream notifications of interest. For example, a
web developer can use a simple Javascript code to send a CQELS
query (in text string) to get continuous location updates of all
air planes of an airline within a spatial boundary, e.g. European
airspace. This channel is especially useful for realtime web/mobile
applications that use Model-Controller-View (MVC) front-end
programming frameworks like AngularJS and Backbone.js as the

D. Le-Phuoc et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3738 (2016) 2535

Fig. 8. Live view and visualisation of graph of things - [a] heat map, [b,c] 3D layout and chart of historical data, [d] live thumbnails of traffic cameras.

can be retrieved from a SPARQL query similar to the query below.
This query retrieve historical data from 64 temperature sensors
within 200 miles from the coordinate where the mouse clicked
(67.033178.917).

Query 3. Query using a spatial and temporal search pattern

PREFIX spatial:<http :// jena.apache.org/spatial#>
PREFIX temporal:<http :// jena.apache.org/temporal#>
PREFIX dul:<http :// www.loa -cnr.it/ontologies/DUL.owl#>
PREFIX ssn:<http :// purl.oclc.org/NET/ssnx/ssn#>
PREFIX cf:<http :// purl.oclc.org/NET/ssnx/cf/cf -property#>

SELECT ?sensor ?obs ?value ?time

?loc spatial: withinCircle (dul: PhysicalPlace

67.033 -178.917 20.0 miles  200).

?sensor dul:hasLocation ?loc.
?sensor ssn:observes cf: air_temperature .
?obs ssn:observedBy ?sensor.
?obs ssn: observationResult ?output.
?obs ssn: observationResultTime ?time.
?output
?value temporal:values ( 2015/01/01 -03:00 

ssn:hasValue ?value.

2015/01/05 -09:00 ).

}limit 64

When a user is interested in the detailed data of a snapshot,
he/she can click the snapshot to see all relevant historical data
stored in GoT. Then corresponding charts of such data are
generated by fetching data from GoTs SPARQL Endpoint using the
respective temporal query patterns. For instance, part [c] of Fig. 8
presents the charts of historical weather data from a snapshot
of figure [b]. Moreover, the historical data can be used to play
back what happened in the past, e.g., part [d] is playback popup of a camera sensor. A playback data can be retrieved within a
time range. Following example query retrieves the ship locations
from 2015/04/11-03:00 to 2015/04/11-09:00 to play back their
movements within an area in the map.

Query 4. Spatial query for ship discovery with a time constraint
PREFIX spatial: <http :// jena.apache.org/spatial#>
PREFIX got: <http :// graphofthings .org/ontology/ns/>

SELECT ?vessel
{? vessel spatial:withinCircle (got:Vessel

2015/04/11 -03:00  2015/04/11 -09:00 
54.372 -10.1486 20 miles  100).

}limit 100

registers respective CQELS queries to update the summary of live
information in the Live Update Dash board over-layered in the
bottom of the map.

4.2. Evaluations

To evaluate how the system performs, we conducted several
evaluations on the live system over 10 months from June, 2014 to
May, 2015. In the first test, we recorded average query execution
time of 4 example queries presented in Section 4.1 each month.
To show the scalability aspect of the system, we also recorded
the maximum indexing throughputs of different types of data,
i.e., temporal, spatial and full-text data at certain sizes of data in
the storages. Following is the details of the dataset and the cluster
setup involved in the tests.

Dataset. As shown in Table 1, the dataset consists of more
than 400K sensing objects allocated around the world and covers
various aspects of data distribution. The dataset has more than
8.5 billion sensor reading records represented in SSN observation
triple layout (1540 triples/record) (see Section 2). Hence, the data
contains approximately 127340 billion triples if it is stored in a
native RDF store. Among them, the biggest one is the weather data
sources containing about 8,1 billion records from 80,000 sensors
around the world (LSM [4] and NOAA19). There are approximate
360 millions spatial records of moving objects like flights and ships
that significantly contribute to the ingesting and processing load of
the ElasticSearch cluster. Regarding to the load of full-text search
functionality, the dataset contains around 5 million documents
from Twitter and RSS Feeds.

Cluster setup. All experiments were carried out on our physical
cluster which is composed of 7 servers running on a shared
network backbone with 10 Gbps bandwidth. Each server has
the following configuration: 2x E5-2609 V2 Intel Quad-Core
Xeon 2.5 GHz 10 MB Cache, Hard Drive 3x 2TB Enterprise
Class SAS2 6Gb/s 7200RPM3.5 on RAID 0, Memory 32 GB
1600 MHz DDR3 ECC Reg w/Parity DIMM Dual Rank. One server

To keep an HTML page updated with the data streamed from
relevant stream data sources, a Javascript agent of the HTML page

19 http://www.noaa.org.

some values or have some faulty values. In some cases, some of
them do not provide readings for some days or months, probably
due to a hardware failure. Therefore, some outlier detection
mechanisms need to be applied to filter incorrect or abnormal
readings. Moreover, data quality could be used a metric for
ranking the sensor readings as it is quite often that a search
result might contain a large amount of records. In accompany
with data quality, the ranking mechanism could also use the
relevancy (spatialtemporal, semantic and profile-based context)
based on graph-based relationships to recommend the better
sensor readings to the user.

Another problem contributing to data quality issues is the entity extraction process, e.g., our current Tweet entity resolution
method that uses StanfordNER and IllinoisNER is less than 90% ac-
curate, i.e. 10% of our tweet entities have some incorrect attributes.
The main reason is caused by our heuristic way of selecting ambiguous entities based on spatial context and frequency of being
recognised by StanfordNER and IllinoisNER. Furthermore, the number of the Tweets that has recognised entities after going through
the process in Fig. 3 is roughly 20%30% of the amount of Tweets
fetched into our system (i.e. recall = 20%30%). Hence, this recall
is much lower than the achievements of recent work [22] on Tweet
stream extraction that inspires us to re-engineer our current entity
enrichment process in the next version.

Current version of GoT only collects the sensory data that is
publicly available, however, we find that integrating such public
data sources with private data sources such as personal data
sources or enterprise stream data sources can create much more
valuable use cases for the system. For instance, in the example
of Section 2, instead of having to rely on public Tweet streams
to detect an old class mate will be arriving in a ferry in 1 h,
two friends can share their location streams in several secured
ways. Therefore, we are looking into current work on enabling finegrain access control for RDF data to enforce some access control
and privacy preservation measures on private data sources. This
implies some modifications of our SPARQL and CQELS engines to
decide which subgraph of GoT can be queried by a certain access
token, e.g. OAuth2.20

5. Related work

There is a trend in employing semantic web technologies to
solve the interoperability problem of integrating heterogeneous
and distributed IoT systems.
In particular, the Linked Data
principles are applied to provide semantic descriptions to data,
sensors and things and also to link them with other data
sources. Towards this trend, a number of modelling approaches
and ontologies for annotating and describing IoT data have
been developed, e.g. [23] and [24]. OntoSensor [3] constructs
an ontology-based specification model for sensors by excerpting
parts of SensorML descriptions and extending the IEEE Suggested
Upper Merged Ontology (SUMO) [25]. The work presented in [5]
proposes an ontology to describe sensors and sensor networks,
called Semantic Sensor Network ontology (SSN). SSN ontology
represents a high-level schema model to describe sensor devices,
their capabilities, platforms and other related attributes in sensor
web applications. Another example is the SensorData Ontology
developed in [26] which is built based on Observations &
Measurements and SensorML specifications defined by the OGC
Sensor Web Enablement (SWE) [27].

From ontology-based modelling approaches above, Patni
et al. [28] introduce an RDF dataset21 containing expressive descriptions of 20,000 US weather stations. The dataset containing

20 http://oauth.net/2/.
21 http://wiki.knoesis.org/index.php/LinkedSensorData.

Fig. 9. Average query execution time by time collected.

is dedicated as a front-end server which is the coordination
node, other 6 servers are used to store data and run as the
processing slaves. Our current deployment uses Zookeeper 3.4.5-
cdh4.2, Storm 0.9.2, ElasticSearch 1.5.2, OpenTSDB 2.0 and HBase
0.98.4. The coordination node of the cluster is the master node
which has Nimbus, Zookeeper, ElasticSearch and HBase master
nodes installed. The other 6 nodes allocated within the same
administrative domain play the role as ElasticSearch and HBase
slaves. All heavy processing pipelines are parallelised using CQELS
Cloud parallel execution framework [16]. This framework is used
to build highly parallel execution pipelines of the SPARQL Engine
and the CQELS Engine. The execution of such pipelines is scheduled
and coordinated by the co-ordination services of Storm and
HBase, thus, the elasticity of our system is powered by Storm,
ElasticSearch and HBase.
Results. Results reported in Fig. 9 show that the execution time of
queries Q1Q3 slightly increase when adding more time series data
but it takes less than 0.5 s to response on 8.5 billion data entries or
140 billion triples so far. However, the execution time of query Q4
sharply increased from 0.5 to 2.5 s. It shows that the more moving
objects are added, the considerably heavier work load is pushed
to ElasticSearch cluster. However, when the number of time series
data are added, the performance of queries with temporal filters
are effectively handled by the OpenTSDB cluster.

The results of Fig. 10 show that the indexing throughputs of
spatial and full-text decrease slightly when more spatial objects or
text documents are added into the ElasticSearch cluster. However,
indexing throughputs of time series are quite stable when the data
grows even with billions of data entries vs. millions of documents
on ElasticSearch.

4.3. Lessons learnt

So far, the update throughputs have not hit the maximum
capability of our cluster yet, however, in the long run, more
hardware will be needed to cope with higher ingesting load and
more data archived to the system. Even time series data is 20 times
more than the spatial and textual data but the processing load is
dominated by the ElasticSearch processing nodes. Therefore, when
adding more dynamic spatial and textual data, the performance
tuning needs more attention in terms of optimisation and resource
allocation.

Looking closely to sensor readings fetched into our system,
the data quality of the readings from different sources varies
dramatically. Some readings from a certain source might miss

034

D. Le-Phuoc et al. / Web Semantics: Science, Services and Agents on the World Wide Web 3738 (2016) 2535

(a) Average temporal indexing throughput.

(b) Average spatial indexing throughput.

(c) Average full-text indexing throughput.

Fig. 10.

Indexing throughputs.

over 1.7 billion RDF triples is the first dataset for publishing sensor
data as Linked Data. In [29], the authors describe a SenseWeb platform which provides graphical user interfaces to annotate the IoT
data using concepts obtained from linked open data cloud (e.g. DBpedia and GeoNames) and also other local domain ontologies. The
annotated data is published as RDF triples and is available via a
SPARQL endpoint.

While there are a plenty of proposals on publishing sensors data
using RDF but there are quite a few of them that systematically
address the issues of expressiveness, performance and scalability
of RDF stores used in such systems. Besides, there are several
complimentary works on supporting spatialtemporal queries on
RDF stores. For example, to enable spatiotemporal analysis, in [30],
Perry et al. propose SPARQL-ST query language with the formal
syntaxes and semantics. SPARQL-ST is extended from SPARQL
language to support complex spatial and temporal queries on
temporal RDF graphs containing spatial objects. With the same
goal of SPARQL-ST, Koubarakis et al. propose st-SPARQL [31] which
introduces stRDF as a data model to model spatial and temporal
information and stSPARQL as the query language to query against
stRDF. However, the systems in [30,31] could only process a limited
amount of static RDF data and none of them address the challenge
of handling a high update rate of IoT data. Moreover, the scalability
and elasticity for hosting such a massive amount of dynamic data
are still challenging issues for the Semantic Web community so far.
Our work presented in this article is a new evolution of a series
of our efforts [32,33,4] on managing IoT data together with other
related work in the community. In this article, we systematically
touch most of aspects of employing Linked Data to enable graphbased search and discovery of IoT data.

6. Conclusions and future work

The article presents a system aiming to build GoT, a Knowledge
Graph for connected things. GoT provides a unified graph-based
view of the data generated by connected things. GoT provides not
only sensing data from sensors but also the understandings of the
world around physical things, e.g., meaning of sensor readings,
sensing context and real world relationships among things, facts
and events. We have been adding millions of records to GoT per
hour, roughly more than 10 billion RDF triples per month. GoT
puts very first steps towards a graph-based search engine for the
Internet of Things.

From the lessons learnt in Section 4.3 and gearing towards
a realtime search engine, we are implementing some ranking
algorithms on the graph structure of GoT to enable context-based
recommendation features. Subsequently, data quality properties
will be added as a metric for such ranking algorithms. Reasoning
capability is also a desired feature of GoT, thus, it is interestingly
challenging future goal
to achieve in context of big data
requirements. To incorporate public IoT data sources with private
ones, we intend to extend some state of the art on enforcing access
control policies for RDF data to GoT.

Acknowledgements

This publication has emanated from research supported in part
by Irish Research Council under Grant No. GOIPD/2013/104 and by
a research grant from Science Foundation Ireland (SFI) under Grant
