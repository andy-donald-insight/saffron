Web Semantics: Science, Services and Agents

on the World Wide Web 1 (2003) 8187

TAP: A Semantic Web Test-bed

R. Guha a,

, R. McCool b

a IBM Research, 650 Harry Road, San Jose, CA 95120-6099, USA

b Stanford University, Stanford, USA

Received 23 June 2003; received in revised form 18 July 2003; accepted 31 July 2003

Abstract

In this paper, we describe TAP, an experimental system for identifying and researching many different of the different technical
issues that lie on the path to achieving the vision of the Semantic Web. In particular, we address the issues of scalable query
languages, sharing vocabularies, bootstrap knowledge bases, automated extraction of RDF from text and applications of the
Semantic Web.
 2003 Published by Elsevier B.V.

Keywords: Semantic Web; TAP; Bootstrap knowledge bases

1. Introduction and outline

Activities such as XML Web Services [3] and the
Semantic Web [1] are working to create a distributed
web of machine readable data. A number of problems
will need to be solved before the vision can be realized.
TAP [16] is a project at the Knowledge Systems
Laboratory at Stanford University which is trying to
address some of these problems. Implementations of
potential solutions are used for experimental validation for these solutions. In this paper, due to space
constraints, we only describe in brief the important
ideas behind TAP (i.e. the general architecture) and
major components of TAP. References [17] and [18]
have more detailed descriptions of these ideas.

We first discuss the issue of query interfaces to the
Semantic Web. We argue that general purpose query


Corresponding author. Tel.: +1-408-927-1080.
E-mail address: guha@guha.com (R. Guha).

1570-8268/$  see front matter  2003 Published by Elsevier B.V.
doi:10.1016/j.websem.2003.07.004

languages such as DQL [4] and RQL [8] do not provide the reliability and predictability required of the
primary query interface for the Semantic Web. We
present GetData, a very simple interface that could
potentially serve this role. GetData also addresses the
problem of different data providers on the Semantic
Web using different terms to refer to the same object
by introducing the concept of Reference by Description through which two parties exchanging data can
bootstrap from some shared vocabulary to more. We
describe TAPache, a system for publishing data via
the GetData interface and the GetData client for consuming this data.

We then describe Rover, a system for extracting
RDF [9] data from text, and the TAP KB, a large
knowledge base that is being built using tools like
Rover.

Finally, we provide a brief description of Semantic
Search, a facility for augmenting search results with
structured data drawn from the Semantic Web.

R. Guha, R. McCool / Web Semantics: Science, Services and Agents on the World Wide Web 1 (2003) 8187

All of the TAP software is available from the TAP
website1 under the BSD license. In the sections below,
as we describe the different TAP modules, we outline
some of our future plans and hope that the larger Semantic Web community will help us get there.

for the Semantic Web at large. Instead, we need a
simple, lightweight query interface, using which more
expressive query languages can be built up as neces-
sary. In the next section, we describe a simple interface called GetData.

2. TAP architecture

TAP is a system for easily publishing, discovering
and consuming structured data. Consequently, TAPs
architecture is defined by the data model (and for-
mat) for this data and the protocol for discovering and
querying the data. The TAP software system consists
of a number of loosely coupled modules, with the only
commonality being the adherance to the data model,
format and protocol.

TAP assumes that data on the Semantic Web is available as a directed labeled graph, i.e. as per the RDFS
data model. We plan to extend the system to include
constructs from OWL [11], as required by applica-
tions. We hope to include an increasingly wider range
of XML constructs.

In the next section, we describe the query and dis-

covery protocols used by TAP.

2.1. TAP protocols

There has been much work in the area of query interfaces to graph structured data, in the fields of Artificial Intelligence (e.g. Frames [14], Semantic Networks [19], Description Logics [2]) and Databases
[12] and more recently, in the XML and RDF communities (RDF [8,10,13]) and DAML [4]. More recently,
a number of query languages have been proposed and
implemented for RDF (RQL [8], Jena [10], Squish
[13]) and DQL [4]. Various languages have been proposed for querying graph data models. A common feature of all these query languages is that they attempt to
be sufficiently expressive so that a very large number
of different kinds of queries, including those which
might be computationally expensive to process can be
stated in the language.

In [17], we argue that such query languages, by the
very virtue of their expressiveness, are inappropriate

1 http://tap.stanford.edu/.

2.2. GetData

GetData is a simple query interface to graph structured data. GetData allows a client program to access the values of one or more properties (or their
inverses) of a resource from a graph. Each queriable graph has a URL associated with it. Each GetData query is a SOAP message addressed to that
URL. The message specifies two arguments: the resource whose properties are being accessed and the
properties that are being accessed. Optional arguments specify whether the client wants the inverse
of the properties, the number of answers desired,
etc.

The answer returned for a GetData query is itself
a graph which contains the resource (whose properties are being queried) along with the properties specified in the query and their respective targets/sources.
Application programming interfaces hide the details
of the SOAP messages and XML encoding from the
programmer. An application using the Semantic Web
via GetData gets an API which (in an abstract syntax)
looks like:

GetData(resource, property)

 value

Below are some examples of GetData, in an abstract
syntax (see also Fig. 1)

GetData(Yo-Yo Ma, birthplace)
GetData((Paris, France,
GetData(Eric Miller, livesIn)

 Paris, France
temperature)  57
 (Dublin, Ohio


Paris, France,

Yo-Yo Ma,
Eric Miller and
Dublin, Ohio are references to the resources corresponding to Yo-Yo Ma, the city of Paris, France,
the person Eric Miller and the city of Dublin, Ohio.
Typically, references to resources are via the URI for
the resource. In this case, using the TAP KB [15],
these URIs are as follows:

Fig. 1. A Screenshot of a portion of the Semantic Web pertaining to the country India, as aggregated by TAP.

http://tap.stanford.edu/data/MusiciainMa, Yo-Yo
http://tap.stanlord.edu/data/CityParis, France
http://tap.stanford.edu/data/W3CPersonMiller,

Eric and

http://tap.stanford.edu/data/CityDublin, Ohio.
Each of the above GetData queries is a SOAP message whose end point is the URL corresponding to the
graph with the data.

GetData also allows reverse traversal of arcs, i.e.
given a resource, a client can request for resources
that are the sources of arcs with a certain label that
terminate at that resource. Also, a particular call to
GetData may specify a list of properties all of whose
values are requested, e.g.

GetData(Eric Miller, livesIn

lastName firstName)

R. Guha, R. McCool / Web Semantics: Science, Services and Agents on the World Wide Web 1 (2003) 8187

In addition to the core GetData interface, there are
two other interfaces which enable graph exploration.
These are as follows:

Search: The search interface enables a client to identify resources which have a particular string as a substring of their title, keyword or more generally, one of
their title properties.

Reflection: The reflection interfaces, which are similar to the reflection interfaces provided by object oriented languages, return lists of arcs coming into and
going out of a node. This is very useful for exploring a
graph in the vicinity of a node without any knowledge
of what might be around.

Reference by description: In the previous sec-
tion, we said that GetData calls use the URIs of
objects to refer to them. The obvious problem with
this scheme is that of getting uniform agreement
on URIs for all the objects that we might want to
exchange data about. While it is possible that large
number of sites might agree on a small number of
core terms, it seems highly unlikely that a large number of sites will agree on a large number of terms.
The goal of Reference By description is to provide a
means of bootstrapping from agreement on a small
number of terms to agreement on a large number
of terms.

The intuition behind reference By Description is as
follows. Two sites A and B are trying to determine if
two objects OA and OB, known to A and B respec-
tively, which have different names at A and B, are
the same. They do this by exchanging descriptions of
A and B. The descriptions are in a formal language
(so that there is no ambiguity) and only use a shared
vocabulary. If they can identify a shared description
which uniquely identifies the objects on both sides,
when certain other conditions are satisfied, A and B
can assume that the OA and OB are indeed the same.
Reference [17] gives a more detailed account of Reference by Description and Semantic Negotiation, the
process by which two parties can come to agreement
on such a description.

3. The TAP system

In this section we describe the different mod-
ules/components that comprise TAP. As mentioned
earlier, the commonality which binds these different

modules into a system is that they share a data model
(the RDF data model) and use the GetData protocol
for accessing data. We briefly describe five important
components, TAPache (for publishing), the GetData
client, the TAP KB, the Rover information extraction
module and the semantic search module.

3.1. TAPache

TAPache, functions as a module for the Apache [5]

HTTP server, for providing the GetData interface.

With the Apache HTTP server, there is a directory
(typically called html), in which one places files (html,
gif, jpg, etc.) or directories containing such files, as
a result of which these files are made available via
http from that machine. Similarly, with TAPache, one
creates a sibling directory (typically called data) to
the html directory and places RDF files in this direc-
tory. The graphs encoded in these files are automatically made accessible via the GetData interface. The
URL associated with the GetData request is that of
the file. Internally, TAPache compiles the RDF files
into an indexed memory mappable representation to
avoid query-time parsing overhead. The compiled representation gets automatically regenerated when the
file changes.

TAPache also provides a mechanisms for aggregating the data in multiple RDF files. One particularly
simple aggregation mechanism is to place all the files
(whose data is to be aggregated) into a single direc-
tory. The aggregated graph can then be queried using
the URL of the directory. TAPache also supports the
aggregation of graphs that are distributed across different sites.

TAPache interprets references by descriptions, mapping them into the appropriate objects. Indeed, this is
one of TAPaches most important functionalities.

TAPaches goal is to make it extremely simple to
publish data via GetData. We hope that by making
it very easy to publish structured data in a queriable
form, TAPache will help the creation of the Semantic
Web.

Currently, TAPache has been found to provide an
adequate level of performance with hundreds of files
per aggregated graph. We would like to scale it to
thousands or even tens of thousands of files per aggregated graph. To do this, in addition to compiling
individual files, we plan to maintain an additional in-

dex (per aggregation) that specifies which files contain
which arcs for each resource.

Future work with TAPache also involves enabling
it to deal with formats other than just RDF and RDFS.
In particular, we are interested in being able to handle
increasingly larger subsets XML by mapping them to
the graph model.

We are also interested in making it very easy to publish data in relational databases via the graph model.
To expose the contents of a relational database via
GetData, the publisher needs to specify a declarative
mapping which maps the relational schema into the
graph model. Using this mapping, TAPache will dynamically make the contents of the relational database
available via GetData.

3.2. GetData client

The GetData client is a small library, available in
perl, java and C, using which a program can acoess
data from a server that provides the GetData interface.
The client hides many of the details of encoding and
decoding descriptions from the programmer.

Since GetData itself uses SOAP, it is also possible
to use of the many available SOAP toolkits [17] and
the TAP website provides a complete specification of
GetData.

Experiments in building applications such as Semantic Search [18] have revealed that latency is a
crippling issue in building widely distributed Semantic Web applications. The GetData client currently
provides a simple caching facility to improve perfor-
mance. We hope to implement a more sophisticated
pullpush caching mechanism which will allow the
GetData client and TAPache to collaboratively reduce
latency.

3.3. TAP KB

TAP KB is a shallow but broad Knowledge Base
first built to power the Semantic Search application
and is now available for a broader range of experiments on the Semantic Web. The TAP KB includes
the following:

1. a high level ontology along the lines of the standard
upper ontology [7] and Cyc upper ontology [6].
There is a particular emphasis on well-known ob-

jects such as musicians, athletes, places and prod-
ucts;

2. a small number of properties to describe instances

of the above classes;

3. a large number of particular instances of the above

classes.

(1) and (2) were built by hand. (3) was built in a
largely automated fashion using tools such as those
described in the next section.

Most of the current work on the TAP KB is focussed
on improving the information extraction so that the
KB can be extended and kept upto date in a largely
automated fashion.

3.4. Information extraction

TAP maintains its index using a module called
Rover. Rover retrieves web pages and extracts graph
structured information from them by sending them
through its specialized parsers. Rover currently has
two page parsing programs, one called Crunch, the
other Scrap. Each program handles a particular type
of data. We first describe Crunch, Scrap and then
Rover.

3.4.1. Crunch

The Crunch program is a very simple natural
language processing system. Paragraphs of text are
analyzed against templates, and from this the text is
classified into a particular genre, a list of relevant
matching concepts is created, and in some cases, the
TAP knowledge base will be augmented based on
concepts that are learned from reading the text.

The program, upon receiving text to analyze, scans
the text for words and phrases that correspond to any
class or concept in the TAP KB. When it has marked
every matching item in the text, it looks at the type
of each concept, or the parent classes of each class,
and then iterates through its list of possible document
categories. We have defined four categories thus far,
entertainment news, sports news, political news, and
business news. Each category has a list of classes that
are expected to be referenced in the document. If each
category was mentioned enough times, the document
is considered to be in that category. References to
classes or concepts outside the ones considered relevant to the category are dropped. This is done so

R. Guha, R. McCool / Web Semantics: Science, Services and Agents on the World Wide Web 1 (2003) 8187

that references to fictional politicians in an entertainment article (such as the president on the show West
Wing) are not input as real political data. We have
also found that this pruning is useful to remove irrelevant concepts, such as references to musicians whose
titles are common words.

Once set of categories is chosen, the system then
looks at its list of templates for those categories. A
Crunch template is a pattern to be applied to the text.
Most of our templates utilize hints which news reporters give to their readers as to the identity of proper
names used in the article. For example, the phrase
British Prime Minister Tony Blair will typically be
used the first time Tony Blair is mentioned, and thereafter will he referred to as Tony Blair. A great deal
of information can he found in patterns of this nature,
particularly in news wire stories. To ensure the quality
of the knowledge base, Crunch templates are very con-
servative. The Crunch system currently holds about
22,000 documents and has learned about 3250 unique
concepts, mostly people from these documents.

3.4.2. Vault

Vault is a partially indexed page store. Vault stores
cached HTML pages, the RDF files of knowledge
learned from them, and maintains an index of particular properties of knowledge within those pages. The
index is stored in a set of Berkeley DB files. Vaults
primary job is maintenance of the HTML page cache
and long-term storage of previously parsed documents.

3.4.3. Scrap

Scrap is a page scraper. We designed Scrap to take
advantage of two trends in web pages. First, HTML tables are used to provide repetition within a page, where
particular columns and rows are used to group similar
information together. We write templates which specify this repetition, and we can then use the placement
within the table to know which concepts the information pertains to. The other trend we take advantage of
is repetition across pages. Many modern web sites utilize a page template and a relational database to create
HTML. We utilize a reverse template and HTML to
retrieve the original data. We then describe the original data in terms of the TAP core vocabulary, thus
making the data available to TAP clients.

Scraps templates are created in a semi-automated
fashion today, and we hope to automate more of the

process in the future. The templates are also partially
self maintaining, in that if a page is updated, we can
use the data gleaned from the old page to attempt to
automatically deduce the new pages format. It is our
goal for the TAP knowledge base to gain enough information to have a critical mass of data, such that more
and more pages can be automatically deduced based
on applying knowledge learned from other pages to
know what meaning to assign to parts of a new and
unknown document.

The Scrap system assigns templates based on URL
patterns. A particular template will be applied to a
set of pages when called upon. Vault maintains a list
of the type of data each page produces, how often
to update the cache, and whether the page should be
crawled regularly by Rover. Pages which are not frequently used will be parsed on demand, others may
be frequently kept fresh.

3.4.4. Rover

Tying all of these pieces together is Rover. Rover
performs periodic crawls of news sites, and of sites
relevant to popular searches in order to learn as much
data as possible, keep the caches fresh, and make sure
that popular queries are quickly answered. Rover is
presently a manually-run program which is run peri-
odically, once a day at present, but which will be a
constantly running program in the future.

3.5. Registry

Reggie is a registry for the Semantic Web. It keeps
track of and answers queries about which sites have
which properties of which classes of objects. For ex-
ample, when the Semantic Search finds that the user
has typed the name of an athlete, Reggie is used to determine which sites are relevant. Reggie maintains the
list of sites, along with a partial description of what
data the site has, and a list of properties and inverse
properties that can he provided for concepts matching
that description.

For example, in the case of an Actor, one site will
be IMDB with a data URL, a description such as
rdf:type tap:Actor, and a set of properties such as
tap:hasBirthplace and inverse tap:hasActor. The
description of what concepts a site has data about can
also be more complex, such as the case of OReilly
books. OReilly will want to he asked about items

of rdf:type tap:Book, but will also only want to be
asked about books that OReilly publishes.

3.6. Semantic Search

The growth of the Semantic Web will be driven by
applications that use the data available on it. Semantic
Search is an application that seeks to improve Search,
which is one of the most frequently used applications
on the Web. Semantic Search seeks to improve search
by providing the search engine an understanding of the
denotation of the search term. in particular, it uses this
understanding to augment text search results with relevant data from the Semantic Web. To do this, we use
the TAP KB to identify the denotation of the search
query. If the search query denotes some object we
know about, based on the type of the object we determine what data to extract from the Semantic Web, and
then augment the search results with this data. For ex-
ample, if the user searches for Yo-Yo Ma, using the
TAP KB we determine that it denotes a cellist and so
augments the search results with appropriate data such
as his discography, concert schedule and personal bio.
Semantic Search can also be used for site search.
As described in [18], a knowledge about the domain
of a site, together with the search interface provided
by TAPache can be used to augment site search results
with relevant data. More details on Semantic Search
can be found in [18].

4. Conclusions

We strongly believe that vision of the Semantic Web
will only be achieved by building large scale systems
that allow us to discover and explore different solutions for the important problems that need to be solved.
It is in this spirit that we have created the open source
TAP system which is proving to be a valuable test-bed.

Acknowledgements

We would like to thank John Giannandrea, Tom
Paquin, Eric Miller, Deborah McGuiness, Richard
Fikes, Shiela McIlraith, Ed Feigenbaum and Dan
Brickley for their feedback on the ideas presented in
this work as it developed. We would also like to thank

Arvind Sundaraman and Kate Joly for their work on
the implementation of the Semantic Search system.
Some of this work described here was done as part
of Alpiri Inc. We thank all the people who made that
possible.
