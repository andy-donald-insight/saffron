Web Semantics: Science, Services and Agents

on the World Wide Web 1 (2003) 115132

A case for automated large-scale semantic annotation

Stephen Dill, Nadav Eiron, David Gibson, Daniel Gruhl, R. Guha


, Anant Jhingran,

Tapas Kanungo, Kevin S. McCurley, Sridhar Rajagopalan, Andrew Tomkins,

John A. Tomlin, Jason Y. Zien

IBM Almaden Research Center, 650 Harry Road, San Jose, CA 95120, USA

Received 23 June 2003; received in revised form 10 July 2003; accepted 31 July 2003

Abstract

This paper describes Seeker, a platform for large-scale text analytics, and SemTag, an application written on the platform to perform automated semantic tagging of large corpora. We apply SemTag to a collection of approximately 264 million web pages, and
generate approximately 434 million automatically disambiguated semantic tags, published to the web as a label bureau providing
metadata regarding the 434 million annotations. To our knowledge, this is the largest scale semantic tagging effort to date.

We describe the Seeker platform, discuss the architecture of the SemTag application, describe a new disambiguation algorithm
specialized to support ontological disambiguation of large-scale data, evaluate the algorithm, and present our final results with
information about acquiring and making use of the semantic tags. We argue that automated large-scale semantic tagging of
ambiguous content can bootstrap and accelerate the creation of the semantic web.
 2003 Published by Elsevier B.V.

Keywords: Large text datasets; Information retrieval; Data mining; Text analytics; Automated semantic tagging

1. Introduction

The WWW has had a tremendous impact on society
and business in just a few years by making information instantly and ubiquitously available. During this
transition from physical to electronic means for information transport, the content and encoding of information has remained natural language. Today, this is
perhaps the most significant obstacle to streamlining
business processes via the web. In order that processes
may execute without human intervention, documents
must become more machine-understandable.

The semantic web [6] is a vision of a future web
of machine-understandable documents and data.1 On
a machine-understandable web, it will be possible
for programs to easily determine what documents
are about. For instance, the people, places, events,
and other entities that a document mentions will be
canonically annotated within it. As a consequence, it
is hoped that a new breed of smarter applications will
become available.

Where will the data come from? For the seman-
two

tic web vision to come to fruition, at

least


Corresponding author. Tel.: +1-408-927-1080.
E-mail address: rguha@us.ibm.com (R. Guha).

1 Today, machines can understand very little of the content on
the webalmost all the markup contained in web pages pertains
to formatting.

1570-8268/$  see front matter  2003 Published by Elsevier B.V.
doi:10.1016/j.websem.2003.07.006

S. Dill et al. / Web Semantics: Science, Services and Agents on the World Wide Web 1 (2003) 115132

classes of metadata must become extensive and per-
vasive. The first is ontological support in the form
of web-available services which will maintain metadata about entities and provide them when needed.
The second is large-scale availability of annotations
within documents encoding canonical references to
mentioned entities.

Ontological support for the semantic web is an active area of both research and business development,
but not the focus of this paper. Instead, we use the
TAP ontology [43] in our experiments.

In partial support of the second class of data, document annotations, it is expected that enterprises will
make business data available in semantic web formats
(RDF, XML, or OWL). It is also expected that productivity tools will make it possible for individuals to
author semantically annotated documents.

Nonetheless, for all this to happen, we need applications that can effectively leverage semantically
tagged data. In turn, these applications cannot be useful unless there is enough semantically tagged data
on the web in the first place. Unfortunately, todays
reality is that few documents contain such annotations a priori, and we are in a state of circular de-
pendency. Organizations that might create powerful
tools based on semantic annotations are leery of sinking significant developmental effort while the number
of available tags remains small; and content creators
are similarly unwilling to create annotations while no
tools exist to make use of them. The size of the web
makes this bootstrapping problem both formidable and
acute.

1.1. Our contributions

SemTag is an application that performs automated
semantic tagging of large corpora. We apply SemTag to a collection of approximately 264 million web
pages, and generate approximately 434 million automatically disambiguated semantic tags, published to
the web as a label bureau [49] providing metadata regarding the 434 million annotations. To our knowl-
edge, this is the largest scale semantic tagging effort
to date, and demonstrates the viability of bootstrapping a web-scale semantic network. The key challenge
is resolving ambiguities in a natural language corpus.
To this end, we introduce a new disambiguation algorithm called taxonomy-based disambiguation (TBD).

Maintaining and updating a corpus the size of the
web requires infrastructure of a scale which most tagging applications cannot be expected to support. We
also need a platform which different tagging applications can share. Seeker is a platform designed for this
purpose. It provides highly scalable core functionality
to support the needs of SemTag and other automated
semantic annotation algorithms.

1.2. Paper structure

The remainder of the paper will consist of a review
of the current state of the art (Section 2), an outline
of the SemTag application approach (Section 3), the
results of running the SemTag application on the
web corpus (Section 4), an outline of what the underlying Seeker system requires (Section 5), a brief
discussion of the design and implementation of that
system (Section 6) followed by general conclusions
(Section 7).

2. Related literature

In the last couple of years, as part of the semantic
web activity, a number of different systems have been
built. These systems help perform one of two tasks:
(1) create ontologies; and (2) annotate web pages with
ontology derived semantic tags. By and large, both
classes of systems have been focused on manual and
semi-automatic tooling to improve the productivity of
a human ontologist or annotator rather than on fully
automated methods. Such systems have the advantage
that humans can provide extremely fine-grained semantic tags. However, as reported in [20], even with
the machine assistance, this is an arduous, time consuming and error-prone task.

A number of annotation tools for producing semantic markups exist. Protege-2000 [40] is a tool
which supports the creation of ontologies for the semantic web. OntoAnnotate [23], a framework for the
semantic web, includes tools for both manual and
semi-automatic annotation of pages. Annotea [28]
provides RDF-based markup but it does not support
information extraction nor is it linked to an ontology
server. SHOE [24] was one the earliest systems for
adding semantic annotations to web pages. SHOE
Knowledge Annotator allows users to mark up pages

in SHOE guided by ontologies available locally or
via a URL. These marked up pages can be reasoned
about by SHOE-aware tools such as SHOE Search.
Such tools are described in [33,48]. AeroDAML [31]
is an interesting tools which takes an ontology and
automatically produces a semantically marked up
page which can then be checked by a human.

More recently, there have been efforts to automate
some of these tasks using machine learning as a pallia-
tive. The principal tool is wrapping (see [16,32,34]).
Despite their successful deployment in particular con-
texts, the consensus view is that they require significant training before they are productive. Furthermore,
these systems each model ontologies in different ways
and consequently do not run against a common shared
ontology.

SemTag is different from both these classes of
systems in that it tags very large numbers of pages
with terms from a standard ontology in an automated
fashion. Since SemTag operates as a centralized
application with access to the entire database and
associated metadata, it has many advantages over a
local, per-page taggers. For example, it can make use
of corpus-wide statistics to improve the quality of semantic tags. It can easily be re-run as new annotation
algorithms and new semantic repositories become
available. And it can perform operations that are only
possible in the presence of many tags, such as automated alias discovery. More recent work [27], which
combines natural language understanding with learning to automatically generate annotations for specific
domains is similar in spirit to SemTag.

In this paper, we present results of SemTag using the TAP knowledge base [44]. TAP is a shallow
knowledge base that contains a broad range of lexical
and taxonomic information about popular objects like:
Music, movies, authors, sports, autos, health, etc. We
used the TAP knowledge base in its standard ontol-
ogy. Building a web-scale ontology will require much
larger knowledge bases. Future work involves using
techniques such as those described in [44] to bootstrap
from TAP to build much larger and richer ontologies.
The current focus of SemTag is detecting the occurrence of particular entities in web pages. One of
the critical steps in this process is that of resolving ambiguities. This is an area with a rich body of
work [35,38,42,45,52] from the language understanding community.

With SemTags current shallow level of understand-
ing, RDFS [8] provides an adequate language for representing the annotations it generates. We expect that
in the future, as SemTags level of understanding im-
proves, we will have to use more advanced languages
[36] and move towards OWL [50].

SemTag is built on the Seeker platform for
large-scale text analytics. The explosive growth of the
web, and the difficulty of performing complex data
analysis tasks on unstructured data, has led to several
different lines of research and development. Of these,
the most prominent are the web search engines (see
[3,21]) which have been primarily designed to address
the problem of information overload. A number
of interesting techniques have been suggested in this
area; however, since this is not the direct focus of this
paper, we omit these here. The interested reader is
referred to the survey by Broder and Henzinger [10].
Several authors [1,5,25,37,46] describe relational
approaches to web analysis. In this model, data on the
web is seen as a collection of relations (for instance,
the points to relation) each of which are realized
by a function and accessed through a relational en-
gine. This allows a user to describe his or her query
in declarative form (SQL, typically) and leverages the
machinery of SQL to execute the query. In all of these
approaches, the data is fetched dynamically from the
network on a lazy basis, and therefore runtime performance is heavily penalized.

The Stanford WebBase project [26], while targeting
a system that allows easy sequential and random access to a copy of the web, does not provide the same
prototyping and development environment Seeker
does. Specifically, it lacks the functionality that allows developers to annotate web pages, and easily
re-use the results of other analysis components.

Compaq SRC web-in-a-box (WIB) project [51] is
another system designed to allow researchers to develop text analysis tools that have access to a copy of
the web. While WIB allows analysis components to
annotate web pages, it does not provide storage for any
derived data (such as people or organizations) other
than web pages. Furthermore, its architecture does not
allow users to compose complex data mining modules
from simpler data mining modules, or re-use data.

The Internet Archive [47], has a different objective.
The data is crawled and hosted, as is the case in web
search engines. In addition, a streaming data interface

S. Dill et al. / Web Semantics: Science, Services and Agents on the World Wide Web 1 (2003) 115132

is provided which allows applications to access the
data for analysis. However, a sophisticated querying
system is not provided, nor is a method to perform
large-scale data analysis.

A preliminary version of this paper appeared in the
2003 World Wide Web Conference [17] and a more
detailed description of the infrastructure can be found
in [18]. Other components that are used in the Seeker
system were further described in [2,19].

3. SemTag: a semantic tagger

Consider a world in which all documents on the web
contained semantic annotations based on TAP. So the
sentence: The Chicago Bulls announced yesterday
that Michael Jordan will . . .  would appear as:

The <resource ref=

semantic annotations for a page from a third party even
when the author of the page has annotated the page.
Semantic annotations can be retrieved separately from
the documents to which they refer. To request annotations in this way, an application contacts a Semantic
Label Bureau. A semantic label bureau is an HTTP
server that understands a particular query syntax. It
can provide annotations for documents that reside on
other servers.

Because SemTag does not have write access to the
original document, the resulting annotations are written into a web-available database. The contents of
this data base are made available via a semantic label
bureau from which it is possible to extract semantic
tags using a variety of mechanisms. For instance, one
application may request the semantic tags for a given

http://www.tap.stanford.edu/BasketballTeam Bulls>
Chicago Bulls</resource> announced yesterday that
<resource ref=http://www.tap.stanford.edu/AthleteJordan, Michael>
Michael Jordan</resource> will . . . 

Thus, the annotation:
<resource ref=

http://www.tap.stanford.edu/AthleteJordan, Michael>
Michael Jordan</resource>

says that the string Michael Jordan refers to the
resource whose URI is http://www.tap.stanford.edu/
AthleteJordan, Michael. It is expected that querying
this URI will result in encoded information which provides greater detail about this resource.

The bulk of documents on the web today do not
contain annotations of this form. Consequently, application developers cannot rely on such annotations.
On the other side, website creators are unlikely to
add annotations in the absence of applications that use
these annotations. A natural approach to break this
cycle and provide an early set of widespread semantic tags is automated generation. This is the goal of
SemTag. SemTag seeks to provide an automated processes for adding these to the existing HTML corpus
on the web. In this paper, we look at what needs to
be done to address this problem at the scale of the
web.

We adapt the concept of a label bureau from PICS
so that an application of the semantic web can obtain

document, while another may request all semantic tags
regarding a particular object (say, the basketball player
Michael Jordan).

3.1. Web data characteristics

The web is an open system in that anyone can publish information onto it without coordinating with anyone else. Arguably, this feature has been the primary
cause for the explosive growth of the web. However,
this aspect of the web also imposes significant requirements on any large-scale system for data mining the
web.
 Transient data: The web is a dynamic collection
of documents. New documents are constantly being
added, and existing documents are constantly being changed or disappearing altogether. Web mining
systems have to pay close attention to propagation
of version information.

 Questionable content: The reliability of information on the web varies widely, from intentional misrepresentation to extreme minority views. Examples of this include pages that contain obfuscated
keyword material to steer users of keyword indices
to their page, deliberate crawler traps, and viruses.
However, it is a common perception that the most
timely and correct information on virtually any subject is usually found on the web. Search engines, and
ranking algorithms have addressed the data quality
problem by focusing on finding the most reliable
and authoritative answer from among the many
that are available. This increased focus on preci-
sion, as opposed to recall is crucial in the context of data mining as well. Such requirements are
not universalfor instance, when the application
requires exhaustiveness rather than consensus.
 Failure to conform to standards: On the web, exceptions are the rule. This is a pervasive phenomenon.
For instance, while it is commonly true that most
web pages which end with a .html suffix are
HTML pages, there are instances of executable
files which are served up with this suffix. Pages
that a browser will render them may often fail to
abide by the written HTML standard. These violations of commonly held standards are done both
inadvertently and with malicious intent. Dealing
with exceptions gracefully is essential to designing
a web mining system.

These characteristics have driven us to a system design philosophy that is fundamentally different from
that of many conventional systems. In many conventional systems, improper inputs lead to system fail-
ures. This is desirable since the emphasis is on getting
the correct answer given the data. In the context of
the web, the data is noisy, unreliable, and outside the
control of the system from the beginning. Therefore,
there is really not much to distinguish the correct
answer from one which is statistically reliable. In the
design of Seeker, we proceed under the assumption
that any particular datum may be correct or incorrect,
current or out of date, available or unavailable, original
or duplicated, and so on. We expect that most semantic web systems will differ from traditional database
systems in that they will have to be accommodating
of the above-mentioned characteristics of web data.

Fig. 1. The SemTag architecture.

3.2. SemTag flow

The overall SemTag architecture is shown in Fig. 1.

SemTag works in three phases:
 Spotting pass: Documents are retrieved from the
Seeker store, tokenized, and then processed to find
all instances of the approximately 72K labels that
appear in the TAP taxonomy. Each resulting label is
saved with ten words to either side as a window
of context around the particular candidate object.
This first stage takes place at approximately 10,000
documents per second on the Seeker infrastructure,
naively distributed over 64 machines.
 Learning pass: A representative sample of the
data is then scanned to determine the corpus-wide
distribution of terms at each internal node of the
taxonomy, as described in Section 3.4. This processing takes place at approximately 8000 windows
per second on a single machine.
 Tagging pass: Finally, the windows must be scanned
once more to disambiguate each reference. When a
string is finally determined to refer to an actual TAP
object, a record is entered into a database of final
results containing the URL, the reference, and any
other associated metadata. This pass can be performed sequentially at approximately 12003000
windows per second on a single machine. For details
on the algorithm used in doing this, see Section 3.4.

3.3. SemTag ambiguity resolution

In this section, we describe the taxonomy based disambiguation algorithm. TBD performs disambiguation of references to entities within a large-scale on-
tology.
 Ambiguity within SemTag: Even human-tagged data
can suffer from a certain level of misclassification,

S. Dill et al. / Web Semantics: Science, Services and Agents on the World Wide Web 1 (2003) 115132

but the problem becomes even more acute in the
application of automated tagging algorithms. Thus,
sources of ambiguity within the ontology is a significant concern. There are two fundamental categories of ambiguities:

1. Some labels appear at multiple locations in the
TAP ontology. For instance, the string Michael
Jordan may refer to a statistician, a basketball
player, or many others. This occurs infrequently
in the current taxonomy, but we expect it to occur with increasing frequency as the taxonomy
grows.

2. Some entities have labels that occur in contexts
that have no representative in the taxonomy. For
instance, the term Natalia sometimes refers to
the musician, but ordinarily denotes simply a
persons first name, which has no entry in the
taxonomy. This occurs frequently in our current
dataset, and will probably continue to occur frequently even as the taxonomy grows.

 Evolution of ontologies: Ontologies such as TAP
will continue to evolve. Our expectation is that tailored algorithms with human-tuned parameters will
be applied to a small number of critical sections,
with automated approaches still dealing with the
bulk of the ontology. In keeping with this philosophy TBD makes use of two classes of training in-
formation:
 Automatic metadata: A large amount of automati-
cally-generated metadata allows the algorithm to
estimate whether windows around candidate references are likely to have been generated within a
particular subtree of the taxonomy.
 Manual metadata: A small amount of manuallygenerated metadata (approximately 700 yes/no
judgments regarding whether a label in a given
context refers to some objects) gives the algorithm
information regarding nodes of the taxonomy that
contain highly ambiguous or unambiguous labels.
These judgments are used to determine which portions of the taxonomy can most fruitfully benefit
from particular disambiguation schemes.

3.4. Overview of TBD

We begin with a few formal definitions. Terms are

italicized when first defined.

Fig. 2. Algorithm Sim.

An ontology O is defined by four elements. A set
of classes, Z, a subclass relation S  Z  Z, a set of
instances I, and a type relation T  I  Z. We use the
notation t(i, z) to denote the Boolean function (i, z) 
T and s(z1, z2) likewise. We assume that instances are
closed over super-classing. Namely, for any i, z1, z2,
t(i, z1) and s(z1, z2)  t(i, z2).
An taxonomy T is defined by three elements: a set of
nodes, V; a root r  V ; and finally, a parent function,
p: V  V . We require that: (1) the root is its own
parent, p(r) = r; (2) for all other nodes, this is not
so, i.e., if v = r, p(v) = v; and (3) the root r is
in the ancestry of every node, i.e., for every v  V ,
r  {v, p(v), p(p(v)), p(p(p(v))), . . .}. Henceforth,
we will use (v) to denote the ancestry chain of v.
The internal nodes of the taxonomy are given by {u:
u = p(v) for some v}. A taxonomy can be derived
given an ontology, which is a more general concept.
Each node v  V is associated with a set of labels,
L(v). For instance, taxonomy nodes about cats, foot-
ball, computers and cars all contain the label jaguar.
A spot (, c) is a label  in a context c  C, where
C is the space of all possible contexts. The context
consists of 10 preceding and 10 succeeding words of
text surrounding the label, culled from the document
in which the label occurred. We use the spot to tag the
label with its semantic tag, which is always an node
of T.
With each internal node u  T , we associate a similarity function fu: C  [0, 1] mapping from a context to a similarity. Good similarity functions have
the property that the higher the similarity, the more
likely that the spot contains a reference to an entity
that belongs in the subtree rooted at u. The similarity functions encapsulate the automatically-generated
metadata regarding nodes of the taxonomy.

We can use the similarity function to define an algorithm Sim to guess whether a particular context c is
appropriate for a particular node, as follows. We will
then use Sim to define TBD. The definition of Sim is
given in Fig. 2.

of about 434 million spots, with accuracy around 82%.
Details are given further.

4.1. Methodology

As described earlier, we first dumped context surrounding each spot. We then processed those contexts
as follows:
 Lexicon generation: We built a collection of 1.4
million unique words occurring in a random subset
of windows containing approximately 90 million
total words. Following standard practice, we created a final lexicon of 200,000 words from the
1.4 million unique words by taking the most frequent 200,100, and removed the most frequent 100.
All further computations were performed in the
200,000-dimensional vector space defined by this
set of terms.
 Similarity functions: We estimated the distribution
of terms corresponding to each of the 192 most
common internal nodes of the taxonomy in order
to derive the similarity function fu described in
Section 3.4. We experimented with several standard similarity measures; the results are given in
Section 4.2.
 Measurement values: Based on 750 relevance judgments from human judges, we determined the
measurement values associated with the 24 largest
taxonomy nodes, as described in Section 3.4.
 Full TBD processing: We applied the TBD algorithm to the entire dataset of 550 million spots using the family of similarity functions deemed to be
most effective in Section 4.2, and using the humanand machine-generated metadata described earlier.
 Evaluation: Finally, we collected an additional 378
human judgments against a previously unevaluated
set of contexts in order to evaluate the effectiveness
of TBD.

We now describe briefly our process for collecting
human judgments, our measure of accuracy, and some
baseline experiments regarding the difficulty that human judges have in coming to a single unambiguous
conclusion about a particular spot.

4.1.1. Evaluation and human judgments

As is well known from research in knowledge acquisition [22] and more recently from studies of manual

Fig. 3. Algorithm TBD.

For our problem instance, we must focus on disambiguating references in the taxonomy versus references outside the taxonomy. If the focus is instead on
disambiguating references that may belong to multiple nodes of the taxonomy, then the test b = r should
be replaced with b = p(v).
Finally, with a small number of popular internal
nodes u  T we associate a measurement (ma
u) 
[0, 1]2. ma
u gives the probability as measured by human judgments that spots for the subtree rooted at u
are on topic. ms
u gives the probability that Sim correctly judges whether spots for the subtree rooted at u
are on topic. Thus, the set of measurements encapsulates the manually-generated metadata in the system,
and can be seen as a training set for the algorithm.

u, ms

Algorithm TBD is defined in Fig. 3. The algorithm
returns 1 or 0 to indicate whether a particular context
c is on topic for a node v  T .

Thus, the small numbers of measurements allow
TBD to determine whether it is operating in a region of
the taxonomy that is highly unambiguous, or a region
that is highly ambiguous. If the former, it will choose
to adopt references with certainly; if the latter, it will
apply a probabilistic algorithm.

In Section 4, we evaluate various different ap-

proaches to the similarity function fu.

4. Results

We implemented the SemTag algorithm described
earlier, and applied it to a set of 264 million pages producing 270G of dump data corresponding to 550 million labels in context. Of these labels, approximately
79% are judged to be on topic, resulting in a final set

S. Dill et al. / Web Semantics: Science, Services and Agents on the World Wide Web 1 (2003) 115132

semantic tagging of documents, there are many cases
where different people choose different terms from an
ontology with which to tag a phrase or a document.
Therefore, we need to be careful when evaluating the
results of SemTag.

We created a web-based tool that displays to an
evaluator a spot consisting of a label in a context. The
tool asks the evaluator to determine whether the spot is
on topic for a particular node of TAP. This information
is used to generate the measurements of Section 3.4.
Because there are several locations in TAP that may
be appropriate for a particular entry (we evaluate this
phenomenon further), the tool also checks to see if
TBD suggested that the spot belongs elsewhereif
so, the tool also asks whether the algorithms output
is a valid answer.

We gathered two sets of evaluations. All volunteers
were from the technical staff at the IBM Almaden
Research Center. For the first set of evaluations, a
set of 11 volunteers were asked to examine 1100
selections made by SemTag. The first
two-thirds
of these evaluations were used as human-generated
metadata for TBD. The remaining one-third of the
evaluations were used to score the performance of the
algorithm.

Finally, a set of three volunteers were each asked
to evaluate the same set of 200 labels in context,
using the same tool described earlier. Of these 200,
all three evaluators agreed on 137; i.e., only 68.5%
were unambiguous to the humans. Furthermore, the
tool was modified in this experiment to allow the
users to indicate that a particular piece of context
(typically 10 words to either side of the label) was
insufficient to understand the denotation of the la-
bel. The evaluators each selected this option in only
2.5% of the instances. Therefore, we conclude that
while a 10-word window to either side of a la-
bel
to understand the sense
of the label, human judgment is highly ambiguous
regarding the placement of the label into the taxon-
omy.

is typically sufficient

The remainder of this section proceeds as follows.
Section 4.2 describes our evaluation of different similarity functions. Sections 4.3 and 4.4 then give results
of a sensitivity analysis to the availability of machineand human-generated metadata to develop the similarity functions and measurement values, respectively, of
Section 3.4.

4.2. Similarity between a spot and a collection

Consider some fixed node of the taxonomy, and a
new spot (, c) that may belong in the subtree rooted
at that node. As presented in Section 3.4, TBD must
determine whether the context c corresponding to the
new spot looks similar to the contexts that typically
occur around spots from that node. We evaluate four
standard candidates for similarity functions.

First, we must cover the preliminaries. We generate a 200K-dimensional vector (over the terms of the
lexicon) corresponding to each internal node u  T ,
or more precisely, to the contexts that occur around
spots for the u. In scheme Prob, each entry of the
vector is simply the probability of the term occurring
in the window. In scheme TFIDF, each entry of the
vector is the frequency of the term occurring at that
node, divided by the corpus frequency of the term. In
all cases, the vectors are normalized to length 1.

Next, we consider two variants of algorithms to
compute the similarity of a spot given a vector. Algorithm IR computes the standard cosine measure
vector product of the sparse vector corresponding to
the current spot and the (probably dense) vector corresponding to the node. Algorithm Bayes computes
the probability that the terms in the context would
have been generated by a source generating terms independently according to the distribution given by the
vector corresponding to u.

The results are shown in Table 1. As the table shows,
the most effective scheme is the cosine measure with
TF-IDF weightings. Furthermore, the TF-IDF weighting scheme dominates the unweighted scheme, and so
we adopt it henceforth for our other comparisons, and
simply compare the IR and Bayes algorithms.

Overall, the accuracy of classification under the favored scheme is roughly 82%. As we show later, even
comparing human judgments to other human judgments shows a systematic error rate of roughly this

Table 1
Accuracy (probability of correctness) for each algorithm under
each vector weighting scheme over test set

Algorithm

Prob
TF-IDF

IR (%)

Bayes (%)

Fig. 4. (a) Percentage of spots influenced by hand classified data. (b) Accuracy of the two algorithms employed in SemTag.

amount, leading us to believe that significant improvements will be quite difficult to achieve.

4.3. Sensitivity to availability of human-derived
metadata

Next, we consider the sensitivity of TBD to the
amount of human-derived metadata present in the sys-
tem. When TBD has access to all human-derived meta-
data, 24 internal nodes of T have sufficiently many
measurements in their subtree. Fig. 4(a) shows for each
such node what fraction of the total labels are covered
by that node. The first node with measurement data
is the root r, whose subtree covers all measurements;
thus, the leftmost point of the graph has y-value 100.
The next node with measurement data corresponds to
cities in the US, and covers around 13% of the total
spots. The actual values, and node labels, are given in
Table 2.

Fig. 4(b) shows the performance of TBD when measurements from only i of the 24 internal nodes are
available to the system. As the figure shows, TBD is
effective even when metadata applies to very few internal nodes.

4.4. Sensitivity to availability of machine-generated
metadata

Finally, we consider the sensitivity of the algorithm
to the amount of automatically-generated metadata
maintained at internal nodes of the taxonomy. As

described earlier, the representation of the similarity
function is a vector of 200K dimensions. We now consider keeping only the largest few dimensions of that
vector for each of the internal nodes of the taxonomy.
We proceed as follows. We fix some fraction f, and

Table 2
Nodes of TAP with percentage of spots occurring in corresponding
subtree

Node

Fraction of spots (%)

Class
US city
Professional type
Country
Musician
City
Product type
Fortune 1000 company
Technology brand
Personal computer game
University
Book
Movie
US state
Actor
Operating system
Musical instrument brand
Comedy TV show
Author
Consumer electronics corporation
Athlete
Comic strip
Home and garden brand
Sporting goods brand

S. Dill et al. / Web Semantics: Science, Services and Agents on the World Wide Web 1 (2003) 115132

Table 3
Nodes of TAP with percentage of spots occurring in corresponding
subtree

Fraction

Num entries

IR (%)

Bayes (%)

15K
15K
15K
15K
16K
18K
27K
44K
62K
80K
155K
230K
305K
381K
456K
532K
608K
683K
759K

for each internal node u  T with vector u, we keep
only the largest max (100, f  |{i|ui = 0}|) entries of
u. Table 3 shows, for various different values of the
fraction f, the total number of non-zero entries over
all internal nodes (i.e., the total number of values that
must be maintained in order to execute TBD), and the
performance of the IR and Bayes algorithms using
this smaller set of machine-generated metadata. The
performance of the IR algorithm is extremely stable
down to 100 non-zero entries per node, and the performance of the Bayes algorithm begins to degrade
slightly sooner.

5. System requirements

The purpose of this paper is to describe an approach
to large-scale automated centralized semantic tagging
delivered to consumers through a label bureau. SemTag is an application that demonstrates the feasibility of this approach. However, SemTag relies upon
Seeker, which we have developed as an ongoing platform to support increasingly sophisticated text analytics applications, particularly including future generations of semantic taggers.

The goal of Seeker is to provide scalable, extensible extraction of knowledge from erratic resources.
An erratic resource is one that may have limited avail-
ability, a rapid rate of change, contain conflicting or
questionable content, or may be impossible to ingest
in totality (e.g., the World Wide Web). We have identified the following design goals:
 Composibility: There are multiple ways a page
might be annotated. These annotations should be
available to other annotators, to allow for more
complex observations to be created incrementally.
This requirement of shared annotation is not unlike
the blackboard system approach [39].
 Modularity: Various types of annotations require
differing methodologies. The architecture needs to
support the plugging in different approaches, as
well as the switching to newer, better implementations of existing approaches as they evolve.
 Extensibility: As we have found with SemTag, approaches to annotation evolve rapidly when confronted with real data. It is thus important that the
Seeker architecture allow essentially arbitrary new
approaches to annotation to be constructed and de-
ployed.
 Scalability: Scalability is important in two respects;
first, the ability to develop a particular annotation
approach on a representative subset of the corpora
is an important design tool. Once an approach has
been proved out on a test sub-corpora, it is desirable
that the code scaled up to a multi-billion document
corpora with minimal changes (e.g., none).
 Robustness: On very large, distributed systems, failure of individual components is not a possibility, it
is a certainty. The system needs to deal intelligently
with failure of portions of the system, so that the
faults in one component do not bring the whole system down.

6. The Seeker design

To meet

the design requirements expressed in
Section 5, we adopt the architecture shown in Fig. 5.
Because the system must be modular and extensible,
we adopt a web services style architecture in which
all agents communicate with each other through a set
of language-independent network-level APIs defined

Fig. 5. Architecture of the Seeker system.

on an XML substrate. To support scalability and ro-
bustness, we classify a small set of critical services
within this web services framework as infrastructure
components. These are large, scalable, well-tested, dis-
tributed, high-performance components that provide
baseline functionality such as crawling, indexing, storage of data and annotations, and query processing. A
larger set of loosely coupled analysis agents communicate through a centralized data store (itself an infrastructure service). Such an agent may execute at a
different time and place, and in a different language,
than another agent it depends on. The runtime environment performs monitoring and control of all services in the system. For analysis agents, the runtime
monitors them, manages their work flow, scheduling,
and (where possible) parallelism, and causes them to
see the set of data and annotations necessary for their
success.

The current Seeker environment consists of 128 dual
processor 1 GHz machines, each attached via switched
gigabit network to 1/2 TB of network attached storage.
Half of this cluster was used for the SemTag tests.
Since each of these nodes runs at approximately 200
documents per second, the total time taken to reprocess
the web is 32 h.

IO for this speed completely occupies one of the two
1 GHz processors, requiring that the spotter/classifier
run at around 200 docs per second (3 MB/s) on a single
1 GHz processor. This limits the complexity of the
spotter/classifier that can run.

In Section 6.1, we describe the XML substrate
of Fig. 5. Section 6.2 then describes the current set
of infrastructure components within Seeker. Finally,

Section 6.3 describes the analysis agents, which include the various components of SemTag.

6.1. The XML substrate

Functionality in Seeker is delivered through a
network services model, in which components publish their availability through a centralized registry,
and export a network-level API. Thus, Seeker is a
service oriented architecture (SOA): a local-area,
loosely-coupled, pull-based, distributed computation system. We require high speed (10,000 RPCs
per second), high availability (automatic fail-over to
backup services), and efficient multiple programming
language support (due to integration and performance
issues). As a result, we choose to base our network
services on Vinci [2] a SOAP [7]-derived package
designed for higher performance intranet applications.
Vinci uses a lightly encoded XML (employing the
xtalk protocol) over raw TCP sockets to provide the
required RPC rate. It includes translation gateways allowing SOAP components to be integrated with minimal difficulty.

6.2. Infrastructure components

Infrastructure services must address issues of reliability and scalability; therefore, the implementation of these core services includes a systems engineering problem. The main infrastructure components
of Seeker include a centralized store, an extensible
full-text indexer, a scalable web crawler, and a query
processing component called the joiner.

S. Dill et al. / Web Semantics: Science, Services and Agents on the World Wide Web 1 (2003) 115132

6.2.1. The data store

The data store is the central repository for all
long-term shared data storage within Seeker. The
store not only serves as a storage service for the rest
of Seeker, but it also serves as the main communication medium between miners. Annotators store their
output in the data store, and other miners depending on them retrieve that information from the store,
possibly much later and in a very different environ-
ment, enabling loose coupling of miners (this is an
extension to the blackboard system approach [39]).

A Seeker store contains entities, each of which is
identified by a globally unique 128 bit Universal Entity Identifier (or UEID). The store provides both fast
batched and random access to entities. Entities are of
a particular entity type. A web page would be stored
as an entity of type Page, for instance, while the information about a particular person would be stored as
an entity of type Person. The key/value pairs associated with an entity describe all the information that
has been extracted about that entity.

We give a concrete example. When a web page is
first crawled, the crawler creates a number of keys.
The raw content of the page is stored as a BLOB in the
key named Crawl:Content. The latency of the fetch
is stored as a 32-bit integer in key Crawl:Latency;
the MIME type as reported in the header is stored as a
string in key Crawl:MimeType; and so forth. When
the page is stored, a UEID is generated for the new entity by the store. As other miners process the page, they
add additional keys. For example, if the page is found
to be a mirror of another page, the mirror-detection
miner may add a key Mirror:MasterCopy that contains the UEID of the master copy of this page.2 Other
types of miners may create entities of different types.
For instance, as part of the Social Networks applica-
tion, a miner may generate entities of type Person.
To support the constantly changing nature of the
data being mined, subsets of keys are tagged with
timestamps. These timestamps are set by the miners
writing these keys to be the validity time for the raw
data from which these keys are derived. For instance,
in the case of data based on a particular web page,
the timestamp would be the time at which that page
was crawled. These timestamps allow a miner read-

2 Keys of type UEID are used to store relationships between

entities in the store.

ing multiple keys to make a decision on whether the
data presented to it is consistent. Furthermore, these
timestamps may be used by a store implementation
supporting archiving of historical data to refer to specific versions of entities stored.

Formally, the store is defined by a network-level
API that provides three basic services: (i) get, which
retrieves a subset of an entitys key/value pairs; (ii)
insert, which creates a new entity with a given initial set of key/value pairs; and (iii) modify, which
modifies or deletes an entity. Additionally, the Store
may provide additional services, such as an enumeration service and a work-queue service, to support
more sophisticated access patterns. All store APIs are
entity-type agnostic. Thus, the store supports the requirement of text analytics applications that are not
document centric, but are rather focused on entities
such as people or organizations.

The simple API defined for the store service allows extending the system by implementing different
variations of the store service to answer particular
customer-driven requirements. Miners operating in
the system require only the name of a Storethey
then locate the corresponding service using the service registry, and speak to that service using the network API. An implementation of the store API may
add additional functionality, or different performance
characteristics. For example,
for smaller datasets
Seeker includes an in-memory store.

Stores may optionally provide implementations
of the Enumeration spec that support store-specific
query languages. For instance, a store may support
SQL-like relational querying by providing an enumeration service where the client specifies a SQL-like
query when opening the enumeration. To support
queries that access multiple stores (required, for in-
stance, when proprietary customer data has to be
joined against Seeker-native data) a federating query
front-end, known as the joiner service, is provided.
This service implements the Enumeration spec with
a query language that allows passing sub-queries to
individual stores and join the result sets returned.

6.2.2. The crawler

The size, unpredictability, low availability and poor
connectivity of the web dictates that it must be gathered into a repository for mining, rather than mined
in situ. The crawler maintains as fresh a copy of the

web as possible by tracking change rates of individual pages and asynchronously recrawling them ac-
cordingly. The recrawl strategy of subsets of the web
may be manipulated manually in response to customer
needs. It runs as a fully distributed, shared-nothing
parallel application, in which sites are carved up and
assigned to machines for crawling. The Seeker crawler
is capable of crawling the web at a rate well in excess
of 1000 pages per second given enough bandwidth
and machines. Much of the code complexity and computation surrounds the need to obey politeness and
robots restrictions. Much of the I/O is consumed by
maintaining data structures that keep track of which
pages have been crawled, and scheduling of pages to
be crawled or recrawled.

Maintaining freshness is complicated by several fac-
tors. First, while the web used to contain primarily
human-authored documents that changed only when
they were edited by hand, it now contains more and
more traditional databases and content management
systems exposed through HTTP interfaces. Second, it
is now commonplace for sites to offer live data services accessible through a single URL, where the content changes frequently (perhaps even changing each
time someone requests the data for the URL). And
third, it is difficult to predict with any accuracy when
documents will change, or for how long they will be
available. See [19] for more details.

Because the crawler interacts with a large number
of unreliable data sources that are outside its control,
it must be designed to tolerate a wide variety of failure modes. Many servers fail to implement the HTTP
protocol correctly, or do not supply trustworthy head-
ers. Congestion and network outages impact the ability to fetch a particular page at a point in time. And
finally, some websites deliberately attempt to increase
their visibility in web mining operations (particularly
within text search engines that are used by the public
to locate information in the web). In order to preserve
statistical correctness of mining operations, a crawler
must be designed to resist direct manipulation and give
fair coverage to the web at large.

6.2.3. The indexer

The Seeker system contains a generic large-scale
distributed indexer capable of indexing sequences of
tokens. The indexer is a distributed, shared-nothing
parallel application, partitioned by document. The in-

dex built contains not only a positional text index
of the web, but also additional document annotations
generated by miners. The indexer is fed documents
as a stream of tokens, similar to the MultiText model
[15], which allows us to achieve high performance
during indexing due to the simple data model. For
flexibility, each token occurrence that is indexed can
have arbitrary attribute data associated with it. Analysis agents can generate additional tokens that overlay
text tokens to indicate higher-level semantic informa-
tion. These tokens are indexed along with the text, and
may be used in queries to mix semantic information
with full-text queries.

Tokenization occurs outside the indexer, and is decomposed into a number of independent miners rather
than a single monolithic process. For example, a base
text tokenizer breaks a web page into textual tokens.
Other miners may extract attributes such as geographical references, proper names, or corporations that appear in the page. Clients may, for example, then contact the indexer service through its network API to
request pages that refer to corporations co-occurring
with northern European locations. Details of the ranking function and quantitative benchmarking results are
available in [29].

Finally,

indexers within the system are generic
components. The indexer described earlier builds and
serves a positional index that allows proximity queries,
phrase search, and so forth. However, for some ap-
plications, an index that supports range queries of
numeric values might be more appropriateconsider,
for example, queries for locations within a particular region. The index build process supported by the
system allows multiple indexers to process the same
dataset, and includes mechanisms to join together
results from several different index servers.

6.2.4. The joiner

Indexers within the system are generic components.
The indexer described earlier builds and serves a positional index that allows proximity queries, phrase
search, and so forth. However, for some applications,
an index that supports range queries of numeric values might be more appropriateconsider, for exam-
ple, queries for locations within a particular region.
Other queries may desire information about closure of
spans of information, or be geospatial in nature, or be
part of a hand-selected collection or any of a number

S. Dill et al. / Web Semantics: Science, Services and Agents on the World Wide Web 1 (2003) 115132

of restrictions.

The joiner is a service that takes a request, for ex-

ample:

SELECT url FROM web WHERE

SemTag = Athlete,Jordan, Michael

and PageLocation within 20 miles of SanJose

and returns the set of URLs of pages that meet the
restriction criteria.

The joiner allows more complicated annotators
to examine only those documents which meet some
basic criteria, allowing them to take more time on
those pages of interest. It also has a role as part of the
flexible glue that allows bringing together different
components into a single system, supporting a single
consistent API.

6.2.5. The cluster management system

The cluster management system (CMS) must run
a huge number of semi-debugged services on a large
amount of failure-prone hardware. It must run these
services efficiently (i.e., in a load balanced fashion)
while identifying problems and taking appropriate corrective action before the problem cascades. When an
error condition is beyond the autonomic abilities of
the system to correct, a logging system keeps track of
all alerts raised by all system components, and a human operator is called in. The CMS component is designed to allow application level code to be developed
without the need for explicit handling of parallelism
or crash recovery.

The CMS system operates as a hierarchy of nanny
programs. The services and miners themselves are
launched by a nanny program that forks and execs
them within a sandbox (i.e., in memory, disk, file
system and CPU limited fashion). These services and
miners convey status information to the per-service
nanny parent. Third-party software packages are
wrapped as necessary with a custom nanny service to
allow uniform viewing and control of all components.
Each machine has a head nanny that makes sure each
of the individual nannies are running, and periodically
(every second or so) collects information from them
as well as information on various machine resources
(total memory/disk/CPU usage, network collisions,
etc). Information from the head nannies is aggregated
at a central location to generate overview statistics,
and to provide controls by which agents or the key

operator can take corrective and tuning actions, propagating commands back down the chain. Ultimately,
these commands cause individual services and miners
to be installed, uninstalled, stopped and/or started on
various machines.

A module called the Foreman manages sets of interdependent miners. It exploits the extensibility of the
store API to fake a store, and uses regular IPC facilities to hand entities directly from miner to miner,
allowing multiple miners to run on the same piece
of data without incurring the penalty associated with
multiple read/writes. The Foreman also allows direct
control over the sequencing of the miners, and logs errors to CMS and performs automated recovery when a
miner in the sequence fails and needs to be restarted.
Since failures may be from accumulated errors or
caused by a specific entity, the Foreman first retries the
miner on the same entity, and if the failure recurs the
entity is skipped. This gives the system a high degree
of robustness in the presence of possibly unreliable
miners.

6.3. Analysis agents

An analysis agent is an encapsulated piece of functionality that executes in the Seeker environment,
roughly equivalent
to a module in a traditional
programming language. As such, it is a completely
generic object that could perform simple processing
of individual pages, or could perform complex distributed operations with built-in fault tolerance and
parallelism. Clearly, it is not possible to provide development tools that will make all annotators easy to
write. Instead, we identified a limited but common
class of analysis agents called annotators and we
have worked to provide significant support for these
agents, while allowing the more sophisticated user
full generality to create more complex agents. All the
initial SemTag components are annotators. We then
define miners to be agents that do not fall into this
limited set.

6.3.1. Annotators

An annotator is defined as an analysis agent that can
be written to process each entity of a certain type in-
dependently. We focus immediately on the most common category of annotators, in which the entity type
is the page, and the annotator performs some local

processing on each web page, and writes back results
to the store in the form of an annotation. For example,
analysis agents that scan each web page and recognize
geographic locations, or proper names, or weights and
measures, or indications that the page contains pornographic content, are all annotators. Similarly, analysis
agents that perform complex tokenization, summariza-
tion, or language identification, or that automatically
translate between languages, are also annotators.

Annotators manifest strong locality of reference in
that they can be run independently on each individual web page without reference to other pages. Thus,
they can be executed by the system on a machine with
limited resources, and handed one page at a time. The
system provides special support for annotators, making them almost trivial to program. The programmer
need write only a simple process one page ( )
function, and the system will make sure the function is
applied to all pages in the dataset, and the results are
published in the store for all others annotators to use.
In SemTag, the operation of dumping all windows
containing references to TAP objects is coded as an
annotator. Due to the simplicity of creating and running annotators, it was possible to post-process the
TAP RDF file in order to extract the labels for each
node of the ontology, create an annotator to extract the
windows around each label, and run the annotator on
the full set of data, within a 24 h period.

A similar annotator can be used to write annotations back into the store once processing has completed on the large collections of windows. However,
the intermediate processing, generation of automatic
metadata, and incorporation of manual metadata from
human judgments, does not fit the limited definition of
an annotator, and must therefore be coded as a more
general miner.

6.3.2. Miners

Miners are analysis agents that need to look at a
number of entities (of one or more entity type) together in order to arrive at their conclusions. The overall SemTag application (using the TBD algorithm) is
a good example of such a system, as it looks at the results of spots on many pages in order to disambiguate
them.

Seeker provides the same CMS support for start-
ing, stopping, monitoring, logging, and maintaining a
cross-entity annotator as it does for a per-entity anno-

tator, but it does not provide automatic support for data
parallelism and replication. Because the functionality
of such a annotator is more complicated, the programmer must take on responsibility for the data management task. However, the data handling capabilities of
the infrastructure components may be brought to bear
on many such problems. For example, a programmer
wishing to provide drill-down into co-occurrences of
particular tokens could simply create a Seeker index
containing the tokens, plug in an appropriate ranking function, and extract co-occurrences using index
queries.

6.3.3. Link analysis

As an example of per-entity and cross-entity min-
ing, we describe tools in Seeker that enable link anal-
ysis.

The Link Annotator is a per-entity annotator that
extracts links from web pages and stores the results in
a form that facilitates the rapid accessing of a relatively
limited subset through store fetch operations. This is
useful in such applications as finding the back-links
from a particular page, or set of pages (see [14]), in the
finding of hubs and authorities for particular queries
[30] or other forms of specialized search and analysis
[12,13].

However, other instances of link analysis cannot
be supported by per-entity annotators alone. The best
known of these are the computation of PageRank
[9,41] and the examination of the large-scale (bow
tie) structure of the web [11] (or a combination
of the two [4]). For such purposes the entire web
graph must be extracted, edited and stored in compact
form. This is the function of the Graph Annotator,
a cross-entity annotator that runs on the entire link
graph of the web and then at termination inserts final results (such as a pages ranking) back into the
store.

7. Conclusions and future directions

The current results are available as a label bureau

on http://www.tap.stanford.edu/semtag/index.html.

We believe that automated tagging is essential to
bootstrap the semantic web. As the results of the
experiments with SemTag show,
is possible to
achieve interestingly high levels of accuracy even with

it

S. Dill et al. / Web Semantics: Science, Services and Agents on the World Wide Web 1 (2003) 115132

relatively simple approaches to disambiguation. In
the future we expect that there will be many different approaches and algorithms to automated tagging.
Unfortunately, storing a copy of the web and creating
the infrastructure for running a tagger on billions of
pages is beyond the scope of most researchers. It is
our goal to provide a tagging of the web as a label bu-
reau. We would also like to provide Seeker as a public
service for the research community to try various
experimental approaches for automated tagging.

Acknowledgements

We would like to thanks our colleagues in the
Seeker development, business and management teams
for their contributions: Rakesh Agrawal, Laura An-
dreson, Srinivasan Balasubramanian, Bruce Baum-
gart, Varun Bhagwan, Michael Boroch, Krishna P.
Chitrapura, Arthur Ciccolo, Tom Costello, Matthew
Denesuk, Rajesh Desai, Ajay K. Dhawale, Maritza
Dubec, Mike Dybicz, Richard Hirst, Ann Hosein,
Kobus Jooste, Sachindra Joshi, Vinay Kaku, David
Kamalsky, Reiner Kraft, Krishna Kummamuru, Bryan
Langston, Jimmy Lin, Peter Mandel, Rajesh Man-
jrekar, Kevin Mann, Kiran Mehta, Joerg Meyer,
Robert Morris, Alison Mortinger, Amit A. Nana-
vati, Ross Nelson, Tram Nguyen, Wayne Niblack,
Norm Pass, Pradhan Pattanayak, Jan Pieper, Julius
Quiaot, Jerrell Shelton, Kim Sherman, David Smith,
Amit Somani, Magnus Stensmo, Thomas Truong,
Roger Williams, David Williamson, Jeonghee Yi, and
Zachary Zhang. We would also like to thank the TAP
project at Stanford for providing the ontology used for
the semantic tagging. In particular, we would like to
thank: Rob McCool, Ed Feigenbaum, Richard Fikes,
Shiela McIlraith and Deborah McGuiness. Finally,
a special thanks to Bruce Baumgart for hardware
wizardry that made the experiments described above
possible.
