Web Semantics: Science, Services and Agents on the World Wide Web 8 (2010) 355364

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

j o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / w e b s e m

Invited paper
Sig.ma: Live views on the Web of Data
Giovanni Tummarello a,c,, Richard Cyganiak a, Michele Catasta b,
Szymon Danielczyk a, Renaud Delbru a, Stefan Decker a
a Digital Enterprise Research Institute, National University of Ireland, Galway, Galway, Ireland
b Ecole Polytechnique Federale de Lausanne (EPFL), Lausanne, Switzerland
c Fondazione Bruno Kessler, Trento, Italy

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 3 February 2010
Received in revised form 3 July 2010
Accepted 10 August 2010
Available online 19 August 2010

Keywords:
Web of Data
Semantic Search
Entity consolidation

We present Sig.ma, both a service and an end user application to access the Web of Data as an integrated
information space. Sig.ma uses an holistic approach in which large scale semantic Web indexing, logic
reasoning, data aggregation heuristics, ad-hoc ontology consolidation, external services and responsive
user interaction all play together to create rich entity descriptions. These consolidated entity descriptions
then form the base for embeddable data mashups, machine oriented services as well as data browsing
services. Finally, we discuss Sig.mas peculiar characteristics and report on lessons learned and ideas it
inspires.

 2010 Elsevier B.V. All rights reserved.

1. Introduction

1.1. The problem: effective Web of Data information reuse

The number of Resource Description Framework (RDF) documents and Microformats available online has grown tremendously
in the past years. On the one hand the Linked Open Data1 community has made available several billion triples driven by the
idea of open access to structured data. On the other hand, an
increasing number of relevant Web 2.0 players (LinkedIn, Yahoo
Locals, Eventful, Digg, Youtube, Wordpress to name just a few) have
also added some form of structured data markups encouraged by
Yahoo! Searchmonkey2 project, and recently Google support for
RDFa rich snippets.3

Precise measurements of this growth are not available, but
partial reports and private communications allow us to give an educated guess of the current size in tens of billions of RDF triples and
approximately half a billion marked up Web pages.

 Corresponding author at: Digital Enterprise Research Institute, National University of Ireland, Galway, Galway, Ireland.

E-mail addresses: giovanni.tummarello@deri.org, g.tummarello@gmail.com

(G. Tummarello), richard@cyganiak.de (R. Cyganiak), michele.catasta@epfl.ch
(M. Catasta), szymon.danielczyk@deri.org (S. Danielczyk), renaud.delbru@deri.org
(R. Delbru), stefan.decker@deri.org (S. Decker).

1 Linked Data: http://linkeddata.org/.
2 SearchMonkey: http://developer.yahoo.com/searchmonkey/.
3 http://googlewebmastercentral.blogspot.com/2009/05/introducing-rich-

snippets.html.

1570-8268/$  see front matter  2010 Elsevier B.V. All rights reserved.
doi:10.1016/j.websem.2010.08.003

Improved visual presentation of search results in major search
engines is a notable incentive for adding semantic markup to Web
pages, but yet there is still a strong need to demonstrate convincing applications that can exploit multiple, distributed, data sources
when solving a task of interest to the user. While several Web data
browsers have been demonstrated (see Section 4), these have so far
been arguably disappointing in effectively bringing together mul-
tiple, heterogeneous sources and ultimately demonstrate reuse of
information outside the context in which it was originally created,
one of the core aspirations of Semantic Web research. The task at
hand is however particularly complex. Assuming that an entity is
indeed sufficiently described by available Semantic Web data, these
descriptions can often be very heterogeneous and exhibit problems such as different describing ontologies, missing links between
descriptions, little or no reuse of identifiers for the same entity, data
errors, wrong RDF practices and more.
In this paper, which extends

[3], we present Sig.ma
(http://sig.ma/), an approach to Semantic Web data consolidation which makes a combined use of Semantic Web querying,
rules, machine learning and user interaction to effectively operate
in real-world Semantic Web data conditions.

Advanced browsing the Web of Data. Starting from a textual
search, the user is presented with a rich aggregate of information about the entity likely identified with the query (e.g. a person
when the input string is a person name). Queries can be about
people, as well as any other entity described on the Web of Data,
e.g. locations, name of documents, products, etc. As the user visu-

G. Tummarello et al. / Web Semantics: Science, Services and Agents on the World Wide Web 8 (2010) 355364

Fig. 1. Sig.ma screenshot for the query Axel Polleres when expanded to 30 sources  space usage is optimised by the capabilities of the Web interface itself (e.g. property
reordering and visualization options).

alises the aggregate information about the entity, links can be
followed to visualise information about related entities. We define
this browsing advanced as it is effectively a browsing on a graph
of aggregated graphs, something very different from current linked
data browsers.

Live views on the Web of Data: rich, embeddable, addressable. At
any aggregation page, Sig.ma offers rich interactions tools to expand
and refine the information sources that are currently in use as well
as some data oriented cleansing functionalities to hide and reorder
values and properties.

As a result, it is possible to interactively create curated views
on the Web of Data about a given entity which can be then
addressed with persistent URLs, therefore passed in instant messages or emails, or embedded using a specific markup in external
HTML pages. These views are Live and cannot be spammed:
new data will appear on these views exclusively coming from the
sources that the mashup creator has selected at creation time.

Once a Sig.ma view has been created, the user can subscribe to
an alert service which will send notification emails when properties
or values are added or changed.

Structured property search for multiple entities, Sig.ma APIs. A user,
but more interestingly an application, can make a request to Sig.ma
for a list of properties and a list of entities. For example requesting affiliation, picture, email, telephone number, [. . .] @ Giovanni
Tummarello, Michele Catasta [. . .] Sig.ma will do its best to find
the specified properties and return an array (raw JSON, RDF or in a
rendered page) with the requested values.

1.2. Related works

queries) the goal is to return a list of ranked resources based on
their relevance. Sig.ma, which is a search application built on top
of Sindice, positions itself in another area which is more closely
related to the Aggregated Search paradigm since it provides an
aggregated view of the relevant resources given a query.

Aggregated search addresses the task of searching and assembling information from a variety of sources and placing it in a single
interface [13]. One approach to aggregated search is to use different vertical searches (images, video, news, etc.) as input and to
present the results into a single page. Google Universal Search4
or Yahoo! alpha5 are examples of such aggregation approach [17]
propose to return digest pages which are virtual documents built
from clustering and summarisation of the documents returned by
a search engine. Some others upon specific domains such as social
science and medicine [15,18]. In contrast, Sig.ma proposes to aggregate heterogeneous data gathered on the Web of Data into a single
entity profile using Semantic Web data consolidation techniques.
The user can then visualise the entity profile, but also enrich it
with additional data sources and reuse it in other Semantic Web
applications.

For the purpose of a comparative experience evaluation we dis-

cuss further related works in Section 4.

2. Test driving Sig.ma: example user interactions

Before discussing the internals, it is useful to see how Sig.ma
presents itself to the end user through some typical interactions.
We also encourage the reader to try the system online.

Semantic Web search engines, such as SWSE [7], Swoogle [6],
Falcons [2] or Sindice [14], are based on the common search
paradigm, i.e., for a given keyword query (or more advanced

4 http://www.google.com/intl/en/press/pressrel/universalsearch 20070516.html.
5 http://au.alpha.yahoo.com/.

Fig. 2. Sig.ma screenshot for a paper. Information comes from multiple RDF sources also including a review from Revyu.com.

2.1. Sig.ma: Axel Polleres

2.3. Sig.ma: Trento

In case of researcher Axel Polleres, there are plenty of data
sources available: RDF sources such as DBLP, Ontoworld, Seman-
ticWeb.org but also Microformat sources such as Polleres public
Facebook and LinkedIn profiles which, for instance, add more pictures to the mashup. Particularly rich sources such as the RDF
coming from the DERI institute team page6 add data such as his
work phone number, publications and related projects. As ambiguity on the name is low, pressing Add More Info button returns
many more relevant results which provide social contacts, alternative affiliations from previous employers and more.

The result of an aggregation of 30 sources is shown in Fig. 1.
Following the Web of Data browser paradigms, most of the
results are clickable and lead to further information discovery. For
example, clicking on the paper titled Exposing Large Datasets with
Semantic Sitemaps reveals a Sig.ma  as we also call the result
of the aggregation  largely originating from three Web sites with
complementary information such as the coauthors, tags and alternative locations. Interestingly, a reference to a review of the paper
from the Semantic review site (Revyu.com) is also given, see Fig. 2.

2.2. Sig.ma: Eyal Oren

While returning plenty of relevant information, the Sig.ma for
researcher Eyal Oren shows the weakness of kick-starting the
search with a simple text query but at the same time the importance of user feedback and interaction. A 50-source Sig.ma shows
several correct pictures, but also some of different people sharing
the same name. Similarly it shows a comment saying that he is
an Israeli-born American actor, the birth place is Tel Aviv and
having a birthdate of 11-11-1975.

To remove these statements, the user must only be able to locate
a single false value (e.g. notably the picture or the statement about
him being an actor) and select the pop-up option remove source.
The document that contributes the incorrect picture or statement
is removed from the mashup. The located false value is removed,
along with any other statements contributed by that document.
With a few clicks (5 approve and 3 reject) we were able to refine a
Sig.ma that can be safely expanded and becomes very descriptive.7

Querying for the Italian city Trento returns data from DBPe-
dia, Geonames, and others. Possibly the most interesting aspect of
this query, however, is how multiple sources that describe formally
different entities can be aggregated into the mashup, resulting in a
practically useful unified profile.

Interestingly, ambiguity is not necessarily a bad thing in Sig.ma.
As an example, the city of Trento is different from the entity
province of Trento, e.g. in Wikipedia. While possibly confusing
for a software client, using the two DBpedia entries at the same
time in Sig.ma can yield results which are meaningful to a human
user. For example, the province source contributes an overall map
of Italy with the Trento province highlighted, and the name of many
neighboring towns, which in certain contexts can be considered
relevant information for the Trento entity.

3. Sig.ma: processing dataflow

Sig.ma revolves around the creation of Entity Profiles. An entity
profile  which in the Sig.ma dataflow is represented by the data
cache storage (Fig. 3)  is a summary of an entity that is presented
to the user in a visual interface, or which can be returned by the
API as a rich JSON object or a RDF document. Entity profiles usually
include information that is aggregated from more than one source.
The basic structure of an entity profile is a set of key-value pairs
that describe the entity. Entity profiles often refer to other entities,
for example the profile of a person might refer to their publications.
A data source, in our terminology, is a Web document that
contains structured data. Examples include RDF/XML documents,
HTML pages with embedded RDFa markup, or HTML pages with
embedded microformats.

There are several ways in which a user can instruct Sig.ma to

display an entity profile:

(1) Based on a keyword or simple structured query
(2) By following a hyperlink from one entity profile to another
(3) By accessing a permalink to an entity profile (possibly created

by another user)

(4) By viewing a Web page where a Javascript tag embeds an entity

profile via a permalink.

6 http://www.deri.ie/about/team/.
7 http://sig.ma/search?pid=8aa3d50637670e46b4647141492e11c7.

The process of creating an entity profile involves the following

main steps, which are described in detail later in this section:

G. Tummarello et al. / Web Semantics: Science, Services and Agents on the World Wide Web 8 (2010) 355364

Fig. 3. Sig.ma dataflow.

Creation of the Sig.ma query plan: The keyword and structured
queries provided by the user form the initial Query Plan in Sig.ma.
Data sources selection: The query plan is expanded in a set of
diverse search engine interrogations, which returns a list of related
data sources

Parallel data gathering: All the candidate data sources are
retrieved from a Web cache, or directly from the Web for a limited number of cache misses. Structured data is extracted from each
source.

Extraction and alignment of related subgraphs: The structured
data extracted from each source is broken down into chunks that
each describe distinct entities (resource descriptions). Then, the set
of sources is expanded based on owl:sameAs links and inverse functional properties found in the descriptions.

Consolidation: All the descriptions are then merged into a single

entity profile, by means of various heuristics.

Result presentation: The result are presented to the user through

a Web user interface, or as a JSON object/RDF document.

Source list re nement: After the entity profile is presented to the
user, it can be refined by modifying the sources list. This final step
generates a closed loop in the Sig.ma dataflow, which actually represent an expansionrefinement loop which is driven by the user
input.

3.1. Creation of a Sig.ma query plan

The process of creating a Sig.ma query plan takes three inputs,

each of which is optional:

(1) A keyword search phrase
(2) A number of source URLs
(3) A number of resource identifiers (URIs)

The difference between the last two items is that a source URL
names a document, which is accessible on the Web, and might contain descriptions of any number of entities. A resource identifier
names a specific entity, but may or may not be resolvable to a Web
document.

The initial Sig.ma user interface presents an input box that
allows entry of either a search phrase, or a single resource identi-
fier. Other combinations of inputs are accessed through hyperlinks
either from within Sig.ma or from a permalink.

3.2. Data sources selection

The first challenge is to identify a set of initial sources that
describe the entity sought for by the user. This is performed
via textual or URI search on the Sindice index and yields a
set of of source URLs that are added to the input source URL
set.

Next, a search for each resource identifier is performed in the
Sindice index. The Sindice index facilitates search not only for key-
words, but also for URIs mentioned in documents. This allows us
to find documents that mention a certain identifier, and thus are
likely to contribute useful structured information to the description
of the entity named by the identifier.

Now we have a list of sources that potentially describe the entity
signified by the user query. The list is naturally ranked: sources
directly specified in the input come first, and the other sources are
ranked as returned by the Sindice index. Then, we interleave these
results with the candidate list returned by the Yahoo! BOSS API,8
that we process to fit our peculiar scenario.

BOSS supports only text queries, thus we take advantage of it
only if the user provided a search phrase. The result list returned
from this service reflects the standard Yahoo! search results, so we
filter for entries which contain some embedded metadata. To do
this filtering on-the-fly, we leverage the additional functionalities
provided by the integration with the Searchmonkey9 service: basi-
cally, we consider the given URL to be interesting if and only if their
metadata extraction layer detected semi-structured content in the
page.

If the source list is long, it is trimmed. The desired length is still
subject to experimentation, but 25 sources seems to be a good compromise of response time, data variety, and it is still manageable in
the user interface. If there are many sources from a single domain,
then these are dropped with preference. This ensures a larger data
variety and produces what appears to be a more interesting default
search result. The user interface provides a control for requesting
more resources, which repeats the process with a higher source
cut-off limit.

8 http://developer.yahoo.com/search/boss/.
9 http://developer.yahoo.com/searchmonkey/.

3.3. Parallel data gathering

Sig.ma operates on data collected by the Sindice [14] project.
The Sindice infrastructure uses the RDF data model as a lingua
franca that hides the syntactic differences between source formats.
A set of parsers, which we are currently publishing as open source
under the name any23,10 is used to extract RDF data from those
different formats. Different RDF serialisation formats (RDF/XML,
Notation 3, Turtle, N-Triples, RDFa) are supported, as well as several microformats (hCard, hEvent, hListing, hResume and others).
Conceptually, any format for which a converter or extractor into
RDF is available can provide input for Sig.ma and Sindice.

Sindice collects data from the Web using a number of tech-
niques: Web crawling, RDF dump indexing based on Semantic
Sitemaps[4], and receiving update notifications (pings) from
sources such as www.PingTheSemanticWeb.com and our own ping
interface.

After documents have been fetched from the Web and their
structured data parts have been extracted, the structured part is
stored in a highly specialised index that facilitates keyword queries
as well as more advanced query forms based on triple patterns. The
index, based on information retrieval technology, as well as part of
the infrastructure is described in [14,12].

The structured data extracted from Web documents is also
stored in the HBase-powered11 page repository, which allows subsequent fast access to the documents contents without incurring
the cost of Web retrieval. It is simply a large distributed hash map
that contains the content fetched from each URL that Sindice knows
about, as well as additional metadata, most notably an inference
closure computed over the documents RDF graph using the quarantined reasoning technique described in [5].

Data fetching and extraction represent one of the main factors
which affect Sig.ma performances, so we designed the data gatherer
as a parallel three-tier cache system which exploits as much as
possible the Sindice infrastructure. First, a memcached server is
consulted. Second, the page repository is queried. Third, an HTTP
request to the source URL is performed to retrieve its contents. In
the third case, the Sindice parsers and extractors are invoked to get
the structured content from the source. Whatever the result, it is
stored in memcached to speed up subsequent requests.

If a successful request to the Web was performed, then the
source URL is sent to the ping manager module of Sindice, which
ensures it will be scheduled for later fetching by the main Sindice
infrastructure. This will result in adding newly discovered documents to the index and the page repository, including triples
inferred during reasoning. Noteworthy is the actual scenario in
which documents are missing from the page repository, especially
if users browse from one entity profile to another referred entity. In
the end, it is a nice way of discovering new sources that are relevant
to user interests, and a low-cost method (compared to undirected
Web crawling) of increasing the Web of Data coverage of Sindice.

3.4. Extraction and alignment of related subgraphs

The structured RDF graph extracted from each source is broken
down into chunks (called resource descriptions) that each describe
distinct entities. This is made easy by the use of the RDF data model.
A resource description contains the outgoing and incoming RDF
triples of a specific resource.

In some cases it would be desirable to include more information into a resource description. An example are geographic
locations, which are often attached to a resource via a prop-

10 http://code.google.com/p/any23/.
11 http://hbase.apache.org.

erty such as foaf:based near, which points to another resource,
often an RDF blank node, which in turn has properties geo:lat
and geo:long that give the geographical coordinates. Obviously it
would be good to have the coordinates included in the resource
description, even though they are only indirectly attached to the
resource in question. This could be solved either by manually identifying commonly occurring cases such as the one given here, or by
using generic heuristics based on graph shape, e.g. include linked
blank nodes that have less than a certain number of outgoing
triples.

As an example of a decomposition into resource descriptions,
consider the case of a typical FOAF12 file that describes a person.
It will be decomposed into one resource description for the files
owner, one small description for each of their friends listed in the
profile, and possibly one description for the FOAF document itself,
containing statements about its foaf:maker and foaf:primaryTopic.
Resource descriptions are now ranked. If the resource has one
of the resource identifiers from the source acquisition step, then it
will immediately receive a large boost, as it is highly likely that it
described the entity in question.

Each description will be matched and scored against the keyword phrase, considering both RDF literals and (with a lower score)
words in URIs. This helps to pick out the correct resource in cases
such as FOAF files, which mention multiple people, but it is easy to
select the right one given a name.

Very small entities are slightly reduced in score, because experimental results show they are unlikely to contain interesting
information, while cluttering up the source list in the user interface.
Resource descriptions below a certain threshold are removed
from consideration. We now have a ranked list of descriptions that
are hoped to describe the same entity. Of course, since fuzzy keyword matching is used in several places in the process, the result
will often contain false positives.

A first cut of our algorithm used only the highest-ranking
resource description from each source, discarding all others. This
has proven to be problematic, as our ranking sometimes would
score the document resource description higher than the description of the person or other entity described in the document,
because both might have the same, highly salient, label. By including both, we leave the problem to the user, instead of risking the
wrong pick.

If the number of highly-scoring resource descriptions is low at
this point, then an attempt is made to discover additional sources,
based on the RDF data we have already retrieved and determined to
likely describe the target entity. We obtain new resource identifiers
for the target entity using four methods:

(1) If the selected resource descriptions are centered on a URI (not

a blank node), then this URI is considered.

(2) If the resource descriptions include any owl:sameAs links, then

the target URIs are considered.

(3) If the resource descriptions include any OWL inverse functional
properties (IFPs) from a hardcoded list (e.g. foaf:mbox and
foaf:homepage), then a Sindice index search for other resources
having the same IFP value is performed  resources having the
same value for an IFP are considered equivalent under OWL
inference rules.

(4) By means of a query to the OKKAM13 service.

12 http://www.foaf-project.org/.
13 OKKAM is an experimental service which assigns names to entities on the Web
[1]. OKKAM returns resource identifiers along with a confidence value. Any resource
identifiers whose confidence value exceed a certain threshold are added to the set of
input resource identifiers. We observe that currently the number of entities that are
reliably recognized by the OKKAM service is still low, as not many OKKAM ids can be

G. Tummarello et al. / Web Semantics: Science, Services and Agents on the World Wide Web 8 (2010) 355364

Any resource identifier discovered using these methods will be
added into the Query plan, which will be then examined in the
refinement step.

3.5. Consolidation

All selected resource descriptions are merged into a single entity
profile. This simply means that all key-value pairs from all resource
descriptions are combined into a single description. A reference to
the original source is kept for each value.

Often different properties (keys in the key-value pairs that
describe the entity) express the same thing. The next step is to
consolidate the potentially large and chaotic list of properties into
a simpler list that is more meaningful to the user. In RDF, properties are named with URIs; we consider only the last segment (local
part) of the URI. By convention, this local part is usually a good
name for the property, written in CamelCase or with underscores
or dashes, which are converted back into a more readable string
consisting of space-separated words. In the future, we should also
check the definition of the property (obtainable by dereferencing
its URI, and often already in the page repository) for an rdfs:label.
The next step is to treat both incoming triples (of the shape
other-entity  relationship - our-entity) and outgoing triples (of
the form our-entity  relationship  value or our-entity  relationship  other-entity) as outgoing triples. This is done simply
by inverting the incoming triples and adding an inverse flag to the
relationship. For example, A creator B becomes A is creator of B.

Next, we apply some simple English-language heuristics on the
property names. This is based on observing properties typically
used in the wild. The heuristics are:
 remove initial has (e.g. in has title)
 remove initial holds (e.g. in holds role)
 remove initial gives (e.g. in gives presentation)
 remove final of and flip property (e.g. in member of)
 remove surrounding is . . . of and flip property (e.g. in is topic
of)

Next, we apply a manually-compiled list of approximately 50
preferred terms, derived from empirical experience looking at several test Sig.ma-s. For example, we replace all of the following
property names with the preferred term Web page: work info
homepage, workplace homepage, page, school homepage, Weblog,
Website, public home page, url, Web. Special attention has been
given to terms that can be used in customised ways in the user
interface: labels, depictions (images), short descriptions, Web links.
Next, we discard properties that are of little value in an end-user

interface, e.g. foaf:mbox sha1sum or rdfs:seeAlso.

After consolidation, properties are ranked. We use a simple
ranking metric: the number of sources that have values for the
property. This will push generic properties such as label and
type to the top. The number of distinct values for the property
is also factored in: properties where many sources agree on one or
a few values (as observable e.g. with a persons name or homepage)
receive a boost.

3.5.1. Value labelling

For key-value pairs where the value is not a literal value (such
as a name or a date), but a reference to another resource (usually
by URI), a best-effort attempt is made to retrieve a good label for
the resource:

found out in the Web, so this step will often not produce a result. In the case where
it returns results however, it is a high-quality identifier that is likely to contribute
relevant results to the next steps.

(1) The original source RDF graph in which the resource was found
is examined for typical label property, such as foaf:name or
dc:title or rdfs:label.

(2) If nothing is found, and it is a URI, it will be resolved against the
page repository or the Web, as described above in Fetching and
Parsing.

(3) If nothing is found, and it is a URI, then the last part of the URI
will be used in a manner similar as described above for property
names.

A typical entity profile can refer to dozens or hundreds of other
entities, so this is an expensive process, yet it is very important
for a decent user experience. We consider showing a good quality
label much more desirable than showing a URI or some fragment
of a URI, and it is a practical feature in enabling the user to comprehend the entity profile. The labels also feed into further Sig.ma
requests: when a user wants to follow a link to another entity, then
the underlying resource identifier(s) as well as the label are used to
submit a new Sig.ma request in order to produce the linked entitys
profile.

To achieve responsiveness despite the large required number
of page repository or Web requests, the initial profile displayed to
the user only contains labels produced by method 1 and 3 above.
The additional labels are retrieved while the user already sees the
profile and will be displayed incrementally using AJAX requests.

3.5.2. Value consolidation

If a property has several values with identical or very similar
labels, then they are collapsed into one value to improve the visual
presentation. For example, several sources that describe a scientist can each state authorship of a certain paper, using different
identifiers for the paper. Without label-based consolidation, the
paper would appear several times because the identifiers are not
the same. After label consolidation, it appears only once. Both identifiers are retained internally. A click on the paper link will cause a
new Sig.ma search that has the label and both of the URIs as input.
Since labels are retrieved and displayed incrementally, the value

consolidation has to be performed in the same fashion.

3.6. Source list refinement

After the entity profile is presented, the user can refine the result

by adding or removing sources.

Almost any entity profile initially includes some poor sources
that add noise to the results. Mixed into the desired entity profile
are other entities that have the same or a similar name, or that
for other reason ranked highly in the text search portions. The user
interface allows quick removal of these. Widgets for source removal
exist in the list of sources, and next to each value that is displayed in
the profile. If the profile shows a poor label or unrelated depiction
for the entity, a quick click will remove the offending source, and
the next-best label or depiction will automatically take its place if
present.

Since the profile is based on a fixed number of resources, it will
often show only a subset of what is known about the entity. There is
a button for including more sources in the source list. After having
retrieved more sources and run the usual processing, it will simply
mix the results into the profile.

We include also widgets that facilitate retrieval of more information of a specific kind, which have shown to be useful, for
example, when a persons entity profile reveals several academic
publications that come from sources on a certain domain. Thus, it is
likely that fetching more sources from that domain will yield more
publications.

4. A comparative experience evaluation

Evaluating Semantic Search system is a novel problem which is
recognized to be somehow difficult given that systems might have
very different characteristics or might be built for supporting different tasks. For example, typical Information Retrieval systems are
evaluated for their ability to provide to the user relevant documents
given a set of keywords, a task well known as Ad-Hoc Document
Retrieval (ADR) (see [11] for an overview). Semantic Search sys-
tems, as defined as systems which operate on RDF graph data, have
also been evaluated using a similar approach, called Ad-Hoc Object
Retrieval [16]. In such approach, the input is a free text query and
the goal is to evaluate the quality of the query results based on the
semantic objects returned by the system. Sig.ma, however, is not
an information retrieval system in a classic sense as it will not provide a ranked list of relevant results but a single aggregate. While it
is true that the quality of the aggregate might be measured, this is
in turn very dependent on the quality of the search infrastructures
used by Sig.ma, e.g., the Sindice and Yahoo search engines. How-
ever, such evaluation is outside the scope of interest for a Sig.ma
evaluation. As we mentioned, Sig.ma does on the other hand fall
into the recent emerging category of aggregated search systems,
that is systems which assemble in a single interface data from multiple sources. While such systems have been evaluated in recent
papers [17], the comparison with Sig.ma capabilities is again mis-
leading. Sig.ma considers as information sources the individual
results returned by the IR systems, and not the IR systems them-
selves. Given that we are not interested in the performance of the IR
system, but more on the value that can bring an aggregated search
interface for Web data, then it would be interesting to show that a
user prefers to see data aggregated rather than displayed as a list,
as in a normal search engine such as Sindice, or split, for example
in different tabs for each one of the sources.

We therefore proceed to the following form of evaluation. We
first illustrate several works which are related to Sig.ma in their
ability to provide the end user with open Web information given
a textual search. We then define several axes concerned with
different high level characteristic of the search experience and self-
assess, by the mean of a discussion, each system in each dimension.
The result is a comparative feature and experience table, which is
then used in a later qualitative discussion.

4.1. Semantic web aggregators

So far two notable approaches have been demonstrated which
make heavy use of Semantic Web technologies. In 2006 the SWSE
Semantic Search engine demonstrated large scale aggregation of
Semantic Web data [7]. Possibly for the first time, thanks to the use
of a scalable cluster infrastructure, SWSE could collect and contain a significant part of the data available on the Semantic Web
so to be able to aggregate information pages with elements coming from multiple sources describing the same entity. To perform
such entity information consolidation, SWSE strictly adhered to the
theoretical rules of the Semantic Web: consolidation via reuse of
the same identifier across different data sources and several forms
of lightweight reasoning such as explicit SameAs statements and
OWL inverse functional properties [8].

As a result the engine displayed two peculiar side effects, as
discussed in [7]. On the one hand, for each textual query indicating an entity multiple Semantic Web Entities are displayed, mostly
due to the very scarce reuse of URIs across different sources. As
an example, a query for Giovanni Tummarello returns 44 RDF
nodes, each with different informations attached. On the other
hand information could be wrongly aggregated due to errors or
different interpretations of semantic properties across different
datasets. For example, if a property, e.g foaf:homepage, is defined

as Inverse Functional Property then all the entities having this value
set to null share the same, erroneous aggregated set of statements.
Built on top of the base SWSE aggregation and query engine, VisiNav was recently demonstrated,14 which adds Faceted Browsing
capabilities.

A completely different approach is that of the Tim BernersLee
initiated Tabulator project [10]. In Tabulator, the idea is to leverage
the linked data principle: data published on the Web in RDF should
have dereferencable identifiers (URIs). If this is the case, then the
identifier doubles as a network location, so it is possible to fetch
the description of the entity by resolving, e.g. by HTTP lookup, the
identifier itself.

In Tabulator, once the user enters a starting resolvable URI, the
entity description is fetched and the contained statements are dis-
played, typically in form of a statement tree. The interesting part
comes when the user decides to investigate on one of the leaves
of said tree. If the leaf is itself a resolvable URI, then the description of the URI is also fetched and the new statements coming from
the new location are therefore added to the old ones. This, in prac-
tice, creates a live data mashup driven by how the user decides to
explore the graph.

This approach, while fascinating, suffers from certain shortcom-
ings. In theory, for Tabulator to consistently return information the
resources should always contain backlinks  that is, all the statements that are known to be in common with other dereferencable
resources. This is clearly a daunting maintenance task which is
much against the nature of the Web and the way people create their
data spaces. In Sig.ma, this role is fulfilled instead by the Sindice
index. A further shortcoming is the dependence on identifier reuse.
If a dataset does not mention explicitly the URI for the same conceptual resource in another dataset, no browsing and data mashup
is possible.

4.2. Other notable entity based information services

There are a number of other information services which we feel
should be somehow compared with Sig.ma, given their ability to
fulfill comparable user-driven information gathering tasks.

First of all, Google often returns all the information required by
the end user. Google is notably good at showing snippets of text
with phone numbers or addresses in them when looking up a name
of a person or a business.

Then, many specialised entity based search engines exist. For
our comparison people search engines are most relevant. Some of
these work mostly by query time Web search and shallow textual
mining (e.g. Pipl), while others keep an index and ask for people to
confirm profiles (Zoominfo, Spock). Generally the information provided includes links to Web pages and fields like title, affiliation
or previous employers and possibly one or more pictures.

Very recently released, Google Squared allows the creation of
tables to compare structured fields describing multiple entities
belonging to the same category named by the user. For example
a search for Italian cities returns some names with pictures, and
some additional information. The query can be changed and yet
added to the square, e.g. adding French Cities to the mix, and a
single entity can be added, e.g. typing the name.

DBPedia, the semantically derived project based on Wikipedia,
can provide clean structured information about million of notable
entities. Due to its encyclopedic scope policy, however, there are
limitations on which entities can be listed and which specific information can be added about them.

14 http://visinav.deri.org/.

G. Tummarello et al. / Web Semantics: Science, Services and Agents on the World Wide Web 8 (2010) 355364

Based on text-structured relationship mining from textual infor-
mation, Evri provides information aggregates about entities, e.g.
related news, pictures and base structured facts. While Evri seems
to operate over live data coming from RSS feeds and other Web
sources, it limits its operations on what seem to be the entities
listed in Wikipedia.

4.3. Features and experience matrix

Sig.ma is an aggregated search engine in which the human has an
important role: the user iteratively indicates how the entity should
or should not look like, while the algorithms automate further
information finding, revision and consolidation.

In literature these systems are usually described as Mixed Initiative [9]. Meaningful evaluation of mixed initiative systems is in
general not a straightforward task. For example, often these systems are composed by machine parts which are simple per se, but
are used in strategic ways which are enabling for an end user.

This is certainly the case in Sig.ma, as the machine learning and
data consolidation techniques are very simple per se. The novelty
of Sig.ma lies in the overall set of features and general experience
it provides, as compared to other systems previously discussed.

In this section we will perform a high level assessment of the
features and experience characteristics of Sigma. We do this by discussing it together with the previously listed related works along
several conceptual dimensions. For each dimension we will provide a short discussion and mark a self-assessment score (, =, +,
++) which is then summarized in Fig. 4.

Genric: Can the system address any kind of entity? People search
engines get the worse score while systems which are based on predefined lists of entities, including Wikipedia, Evri and Freebase, can
be considered only partially good with respect to generality. Purely
unstructured sources like Google or on the contrary systems based
on Semantic Web technologies are the most flexible in this case.

Provides metadata: Can the system be used to have reusable
metadata about the entity? Metadata providing systems can be
used as inputs by applications directly. Clearly this is not what
Google provides, and is only partially provided by Google Square
(without the use of ontologies), Evri provides structured data from
a lexicon which appears to be somehow curated but it is not
grounded in formal ontologies, something which is the defining
characteristics of Semantic Web based systems and Freebase. Providing metadata is also key to allow direct entity comparison.

External docs: Does the system use of or point to relevant textual
documents on the Web? In this case Google excels, while other sys-
tems, including Sig.ma, only provide some relevant Web sources,
but these are greater than those provided by LOD15 browsers or
purely Semantic Web systems since Sig.ma includes also structured sources provided by external services and not only by the
data sources.

Consimes metadata: Does the system make use of External (Web)
Metadata? Consuming Structured Web Data is mostly an exclusive characteristic of Semantic Web tools, with Google offering
only limited support (Rich Snippets) and Freebase being updated
by manual, centralized imports of external datasets. People Search
engines often have some support for automatic import of specialised formats, e.g. HResume microformats.

User refinement: Can the user expand and refine interactively,
e.g. by requesting more or less? Google, supports getting many
more links, google Google Squared partially support this while
in Sig.ma this is and important and well developed characteristic
Sig.ma. At opposite ends of the spectrum, LOD browsers typically

15 Linked Open Data.

only show data from a single source while engines like SWSE show
data from all the sources known to the system however retaining
provenance information.

Sensitivity to updates: How can data updates sensibly influence
the result? Wikipedia and Freebase allow easy updates which are
immediately reflected to the user. Google, by design in order to
prevent SPAM, introduces several mechanisms by which is not
east to suddenly climb to the first positions. LOD browsers only
visualise information as provided and sources only when directly
linked. Sig.ma makes use of SQL-like queries reflecting the status of
the Sindice index, which accepts and indexes new sources within
short intervals, something that does not happen as often given the
more global level consolidation made by SWSE. While responsive
updates are desirable in more closed scenarios, on the open Web
this offers a relatively easy for an attacker to craft data precisely
designed to be displayed along with a specific entity.

Clean output: How likely is it that the users will be presented
with noisy data? Encyclopedic sites excel in this aspect while automatic aggregation tools such as SWSE and Sig.ma have the highest
risk of noise. In Sig.ma the risk is mitigated by the several interactive data cleansing mechanisms which lead to reusable clean
Sig.ma permalinks and embeddable pages. Linked data browser are
less error prone since they typically only follow explicit data links
between entities.

Spam immune: Can the system be easily spammed? Somehow
dual of Updatability: LOD browsers do not typically show information not directly linked by the original source and as such as
definitely the most spam immune. Google, Wikipedia, Freebase can
all be spammed given appropriate resources but they have generally effective technical or social spam avoidance mechanism that
make rather resilient. On the other hand, as previously mentioned,
it is possible to attach Sig.ma by crafting malicious data. Sig.ma
distinguishes itself from the approach of SWSE and others by operating on results of Top K queries, thereby requiring spammers to
use a larger number of data sources to be effective.

Response time: As Sig.ma and LOD browsers are the only systems
which perform online live data and service lookups, they are the
worse performing in the group. Sig.ma can in practice be the slowest of the bunch given that most of the processing is done client
side and that multiple sources need to be fetched. This is partially
mitigated by the asynchronous incremental updates.

This comparison highlights how approaches based on Semantic Web technologies offer several interesting features, and among
which Sig.ma stands out. Not surprisingly however, the comparison
also highlights potential shortcomings in terms of response time,
noise, and sensibility to malicious data. A discussion on these and
ideas on how to address them in future works is given in Section 6.

5. Sig.ma implementation and performance

The Sig.ma processing workflow is implemented in two layers:

(1) a Java backend, wrapped in a Web application hosted in a Tom-

cat application server

(2) a full MVC stack built in JavaScript

The backend exposes a RESTful API, which is used by the
JavaScript layer through AJAX calls. It also represents a facade for
the caching systems (memcached, HBase) and for other minor ser-
vices.

The JavaScript layer follows the ModelViewController architectural pattern, taking care of page rendering, of some final
consolidation functions and of the Data Selection logic. Other specialised rest APIs provide best label services for URIs served
directly from an HBase table.

Fig. 4. Sig.ma feature and experience comparison table.

Decoupling the system in this way was necessary to smoothen
the user experience through incremental page rendering. Yet, it
gave us some nice benefits while implementing the JSON/RDF API.
Thanks to Rhino,16 we are able to run the same logic both on client
and server-side.

Averaged performance tests show that Sig.ma API takes around
1 s per 10 processed sources when serving RDF output, thus skipping the page rendering overhead. Most notably, Sig.ma seems to
perform linearly with the number of sources, averaging an API
level response time of 2 s for the default 20 source aggregation
and 11 s per 100 processed sources. This result gives us a certain
confidence about possible improvements, mainly in the Rhino integration layer.

5.1. Number of backend queries

In terms of backend queries performed by Sig.ma during the
construction of a view, there are usually no more than 1 query per
external service (e.g. yahoo boss, Okkam) and a few to the Sindice
service, e.g. to look with higher priority into Label and relative subproperties as well as notable other properties and finally to look
into any property in the RDF description. After the first consolidation phase, if the user approves certain sources, Sig.ma uses
any identifier referenced by these to perform additional queries on
Sindice. This usually result in one query per alternative identifier
plus one query per Inverse Functional Property. At any point, duplicates results are removed, older queries are rerun to get more tail
results and any new source is evaluated and accepted only if they
pass the relevance threshold previously described. Since Sindice, as
well as the other services employed, give results ranked in descending relevance order, it is natural that as more and more sources
are requested, e.g. by the user pressing multiple times the add
more sources functionality in the user interface, not all the sources
are selected to reach the final integration. For example, asking for
200 sources to be integrated, returns only 71 when the subject is
XSPARQL and 120 when the subject is DBIN.

It is interesting to notice, however, that a Sig.ma aggregation
need not leave useful information behind: Sindice operates in ways
which are much more similar to a DBMS than to an Information
Retrieval engine. A query for an Inverse Functional Property, for
example, will return all and only the entities which precisely match
the query.

Even though the number of queries that Sig.ma can generate
is not negligible, we notice that these queries are very simple
in nature and pose no problem to Sindice, let alone Boss, which

16 http://www.mozilla.org/rhino/.

respond in milliseconds. This time is therefore unnoticeable if compared to the UI upgrade which usually takes a few second (4 to 8)
per each 20 sources if these are reasonably rich (e.g. DBLP, DBpedia).

6. Conclusions and lessons learned

While Sig.ma is not the first data aggregator for the Semantic
Web, its contribution is to show that exciting possibilities lie in a
holistic approach for data discovery and consolidation. In Sig.ma,
elements such as large scale semantic Web indexing, logic reason-
ing, data aggregation heuristics, ad hoc ontology consolidation and,
last but not least, user interaction and refinement, all play together
to provide entity descriptions which overcome many of the shortcomings of the current Web of Data.

We believe Sig.ma provides one main lesson and inspires one
important thought for future works. Sig.ma particularly succeeds
in its task when a user is involved. When Sig.ma automatic source
selection fails, the user can often intuitively recognize this by spot-
ting, for example, a wrong entity type or wrong data statements or
by seeing that multiple sources confirm one statement while others
come from only one source (e.g. the right spelling of a last name is
much more common than the wrong spelling). When this happens,
the user can easily eliminate the wrong data source, thus providing
Sig.ma with more elements for further information cleansing and
augmentation by successive queries.

The way by which the user can quickly adapt the mashup possibly for her intended target goal inspires the general thought that
a little semantic might in fact go a long way, at least in making it
easy for a human to do the last step (and validate, by accepting, the
final list of sources crystallised in the permalink).

Sig.ma also provides the user with the novel Web browsing
experience of navigating through a graph of graphs coming from
possibly dozens of different sources at each click and selected for
relevance.

But Sig.ma also displays evident limits. Sig.mas simple entity
consolidation fails when queries are ambiguous e.g. refer to multiple persons by the same name e.g. a John Smith query or even
different kind of entities e.g. Galway (notably a city but also a last
name). Sig.mas algorithms have clearly very big room for opti-
misation. One can quickly think of additional techniques which
could improve overall precision and recall, first of all entity disambiguation algorithms applied to the presented results. Sig.mas
level of granularity  document based  could also be improved to
entity-in-source level, which would cater for cases where multiple
resources are described in the same source and the user wants to
remove only the information about one resource.

A final thought is that Sig.ma and similar systems which perform
linkage at Web level can be used as a way to assess and validate

G. Tummarello et al. / Web Semantics: Science, Services and Agents on the World Wide Web 8 (2010) 355364

ones datasets for their Web Linkage properties. It is possible to
imagine a Sig.ma evolution that would allow a dataset owner to
not only see how the information interconnects but also to possibly
receive indications of how to improve the dataset Web linkability,
e.g. by adding more attributes, using more specific attribute val-
ues, better specifying classes and properties or in extreme cases, by
manually putting sameAs links where the records seem extraordinarily difficult to disambiguate.

Acknowledgements

The work presented in this paper has been funded in part by
Science Foundation Ireland under grant no. SFI/08/CE/I1380 (Lion-
2) and in part by the FP7 EU Large-scale Integrating Project OKKAM
 Enabling a Web of Entities (contract no. ICT-215032).
