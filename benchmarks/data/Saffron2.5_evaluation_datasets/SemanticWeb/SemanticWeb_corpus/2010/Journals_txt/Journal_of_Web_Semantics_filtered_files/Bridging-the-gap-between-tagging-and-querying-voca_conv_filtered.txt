Web Semantics: Science, Services and Agents on the World Wide Web 8 (2010) 97109

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

j o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / w e b s e m

Bridging the gap between tagging and querying vocabularies: Analyses and
applications for enhancing multimedia IR
Kerstin Bischoff, Claudiu S. Firan, Wolfgang Nejdl, Raluca Paiu

L3S Research Center, Appelstr. 4, Hannover 30167, Germany

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 1 October 2009
Received in revised form 5 April 2010
Accepted 9 April 2010
Available online 18 April 2010

Keywords:
Web 2.0
Tag analysis
Web information retrieval
Knowledge discovery
Tag recommendation

Collaborative tagging has become an increasingly popular means for sharing and organizing Web
resources, leading to a huge amount of user-generated metadata. These annotations represent quite
a few different aspects of the resources they are attached to, but it is not obvious which characteristics of
the objects are predominantly described. The usefulness of these tags for finding/re-finding the annotated
resources is also not completely clear. Several studies have started to investigate these issues, however
only by focusing on a single type of tagging system or resource. We study this problem across multiple
domains and resource types and identify the gaps between the tag space and the querying vocabulary.
Based on the findings of this analysis, we then try to bridge the identified gaps, focusing in particular
on multimedia resources. We focus on the two scenarios of music and picture resources and develop
algorithms, which identify usage (theme) and opinion (mood) characteristics of the items. The mood and
theme labels our algorithms infer are recommended to the users, in order to support them during the
annotation process. The evaluation of the proposed methods against user judgements, as well as against
expert ground truth reveal the high quality of our recommended annotations and provide insights into
possible extensions for music and picture tagging systems to support retrieval.

 2010 Elsevier B.V. All rights reserved.

1. Introduction

Web 2.0 tools and environments have made collaborative tagging very popular, resulting in a huge amount of user-generated
metadata. Still, users motivations for tagging, as well as the types
of assigned tags differ across systems, and despite initial investi-
gations, their potential to improve search remains unclear. What
kinds of tags are used, and which types can improve search most?
We investigate this issue in detail, by analyzing tag data from three
very different tagging systems: Del.icio.us, Flickr and Last.fm.

Our comparative study of the users tagging and querying habits
reveals some interesting aspects. While in general tag and query
distributions have similar characteristics, some significant differences are to be noted: usage (theme) is very prevalent in user
queries for music as well as opinion (mood) concepts for music
and pictures queries, but many more tags of these types would be
needed.

The findings of this analysis are essential, especially for the case
of tagging systems focusing mostly on multimedia resources. While
for Web pages or publications, tags may not improve retrieval
that much, for pictures, music or movies the gain is substan-

 Corresponding author. Tel.: +49 511 762 17730.
E-mail addresses: bischoff@L3S.de (K. Bischoff), firan@L3S.de (C.S. Firan),

nejdl@L3S.de (W. Nejdl), paiu@L3S.de (R. Paiu).

1570-8268/$  see front matter  2010 Elsevier B.V. All rights reserved.
doi:10.1016/j.websem.2010.04.004

tial. Content-based retrieval is still not mature enough to enable
scalable content-based search. Moreover, even with the most
prominent search engines on the Web today, users are still constrained to search for music or pictures using textual queries. In
this context, supporting users in providing meaningful tags for this
type of resource becomes crucial.

One possibility to make users use keywords from the categories
we need is to unobtrusively recommend such tags and thus support them in the tagging process. Beside minimizing the cognitive
load by changing the task from generation to recognition [36], such
recommendation of under-represented but valuable tags will very
likely trigger reinforcement, i.e. enforce preferential attachment. As
presented in [33,22], seeing previous tag assignments from other
users strongly influences which tags will be assigned next and thus
to which tag set a resources vocabulary will converge.

We build upon results of previous studies [68], where we proposed algorithms relying on tags for identifying other types of
valuable knowledge about music resources. In the present paper we
generalize these algorithms and verify their applicability on other
types of multimedia resourcespictures. We summarize results
of previously introduced algorithms which try to identify music
genre, styles, moods and themes based on existing user annota-
tions. Given the fact that pictures are quite different from music
resources, and their associated metadata also differs considerably,
it was not completely clear how, and whether at all, this approach
could be employed also for pictures. Moreover, we prove the appli-

K. Bischoff et al. / Web Semantics: Science, Services and Agents on the World Wide Web 8 (2010) 97109

cability of the method on a different dataset with respect to system
design choices beyond object type. Instead of music experts labels,
Flickr social groups, i.e. users pooling pictures around topics, serve
as labeled training data. With this refinement there is no need for
reducing the number of classes by means of clustering, effectively
ruling out one potential source of noise and thus achieving highly
accurate predictions.

The contributions of this paper are threefold: (1) we propose a
novel approach for accurately detecting emotions in photos relying
on collaborative tagging, state-of-the-art solutions being content
based; (2) with the presented algorithms we manage to bridge
some significant gaps in the tagging and querying vocabularies,
which in turn will enable more efficient multimedia information
retrieval; (3) we present extensive experiments demonstrating the
performance of the proposed algorithms and compare their results
against baseline algorithms.

The methods we propose can be used in various ways: as part of
an application where the recommendations are presented directly
to the user, who can select the relevant ones and add them to the
item that is currently annotated. Another possibility is to index
the recommended mood and theme tags, thus enriching the metadata indexes. Last but not least, the recommendations can be used
to automatically create mood or theme-based playlists in case of
music resources, or mood-based picture catalogs.

The rest of the paper is organized as follows: we start in Section 2 with a review of the relevant literature, structured around
the areas we also address in this work. Next, in Section 3 we
present a detailed analysis of different tagging systems across different domains and compare tagging and querying vocabularies,
thus trying to identify potential gaps. Building upon the results
of this analysis, in Section 4 we focus on bridging these semantic
gaps and propose solutions suitable for two types of multimedia:
music (Section 4.2) and picture (Section 4.3) resources. Finally,
in Section 5 we conclude and discuss possible extensions of this
work.

2. Related work

Given that Web 2.0 tools and platforms have made collaborative
tagging highly popular, some studies have started to investigate
tagging motivations and patterns, but they are usually focusing on
one specific collection only [21,22,33,2,23] or provide first qualitative insights across collections from very small samples [29,39].
We will shortly review some of the major work related to the areas
we also address in this paper: tagging behavior, as well as search
and knowledge discovery based on social tags.

2.1. Tag types

First analyses of tagging systems show that the reasons for tagging are diverse and with them the kinds of tags used. Marlow et
al. [29] identifies organizational motivations for tagging, as well as
social tagging motivations, such as opinion expression, the attraction of attention, and self-presentation [21,29] or providing context
to friends [2]. The different tag types shed light on what distinctions are important to taggers [21]. According to [39], in free-for-all
systems opinion expression, self-presentation, activism and performance tags become more frequent, while in self-tagging systems,
like Flickr or Del.icio.us, users tag almost exclusively for their own
benefit of enhanced information organization [21]. Sen et al. [33]
showed in an experiment on vocabulary formation in the MovieLens system how different design choices affect the nature/types of
tags used, their distributions and the convergence within a group,
i.e. the proportions that Factual, Subjective and Personal tags
will have.

2.2. Tags supporting search

Based on the idea that tags in bookmarking systems usually
provide good summaries of the resources and that they indicate
the popularity of a page, Bao et al. [4] investigated the use of tags
for improving Web search. The proposed SocialSimRank measures
the association between tags and SocialPageRank accounts for the
popularity among taggers in terms of a frequency ranking. In [24],
the authors suggest an adapted PageRank-like algorithm, FolkRank,
to improve efficient searching via personalized and topic-specific
ranking within the tag space. This can be used to recommend interesting users, resources and related tags to increase the chance of
serendipitous encounters. In music retrieval, tags can be used
as an alternative or additional possibility to find songs: In [20],
Last.fm songs are not only recommended based on track-lists (song
and artist) of similar users, but also by considering (descriptive)
tags. Here, tag-based search algorithms provide better and faster
recommendation results than traditional track-based collaborative
filtering methods.

In [23], the authors try to answer the question whether social
bookmarking data can be used to augment Web search. Their analysis of a Del.icio.us dataset shows that tags tend to gravitate toward
certain domains and that tags occur in over 50% of the resources
they annotate, thus potentially improving search. Even if the usefulness of tags has been proven at a single-site level, some general
study of the types of tags used inside multiple systems and their
general implications for search is still missing. In this paper we
tackle this aspect and perform an in-depth analysis over three different systems.

2.3. Knowledge discovery for music

While automatic identification of music themes has not been
studied so far, several experiments in Music Information Retrieval
have shown a potential to model the mood from audio content.
For example, [28] relies on extracted low level features like tim-
bre, intensity and rhythm (modeled in a Gaussian Mixture Model)
to classify music according to Thayers model of emotions [37].
Similarly, in [19] the authors propose a schema such that music
databases are indexed on four labels of music mood: happiness,
sadness, anger and fear. An important limitation of these
approaches is that they cannot capture other, external sources
of emotionality, for example, events that people may associate
with a certain piece of music. Eck et al. [16] investigate social tags
for improving music recommendations to attenuate the cold-start
problem by automatically predicting additional tags based on the
learned relationship between existing tags and acoustic features. In
[11], Last.fm user tags have been used together with content-based
features for automatic genre classification. Underlining the usefulness of social tags for music classification, Levy and Sandler [27]
found that Last.fm tags define a low-dimensional semantic space
which is able to effectively capture sensible attributes as well as
music similarity. Especially at the track level this space is highly
organized by artist and genre. In their experiments, track term vectors build upon social tags led to high average precision values for
retrieval by genre and artist. Thus, tag vector representations of
music tracks correspond to more traditional, well-known music
catalog organization principles while at the same time providing
rich descriptions for each track based on a very large vocabulary.

2.4. Knowledge discovery for pictures

Picture metadata enrichment is similar to music metadata
enrichment in that the goal can be achieved by either using information inferred from the low level features of the resources, or from
already provided user annotations. In [3], user tags are combined

with content-based techniques in order to improve data navigation and search: A classifier uses low-level features, like color and
texture, in addition to tags provided by the users in order to discover new relationships between data. ZoneTag1[30] automatically
recommends location tags for photos taken with a mobile phone,
based on the phones position. In [35], the authors focus on a subset of Flickr pictures and analyze the different tag categories used
by users to annotate their pictures. The analysis is performed automatically based on WordNet categories. The paper also tackles the
aspect of tag recommendation.

Rattenbury et al. [32] try to extract event and place semantics
from tags assigned to photos in Flickr relying on burst analysis.
In [1], landmark pictures for city sights are identified, accompanied by representative tags, by employing machine learning
on the user-generated tags in Flickr. Investigating tag evolution
in Flickr, Dubinko et al. [14] developed algorithms to find the
most interesting tags to be displayed in Flash animations. Predicting moods/emotions for pictures is much less popular than
for music. Prior work uses content-based methods to analyze and
classify facial expressions (see [18] for an overview), sometimes
also picture mood independent of peoples faces [15]. In contrast
to prior work, our algorithms can distinguish a much richer set
of emotions/moods than the often very simple models underlying
content-based approaches.

3. Tag analysis

The following section presents and discusses the results of our
comparative investigations of tag usage in Last.fm, Del.icio.us, and
Flickr. Looking at the usage of different types of tags, we first identify
and quantify the distinctions occurring in users tagging behavior.
Most of the tags are potentially useful for search, though not all
kinds of tags are equally valuable. We then investigate how well
users tagging and searching behaviors correspond.

3.1. Datasets

3.1.1. Last.fm

For our analysis, we have crawled an extensive subset of the
Last.fm website in May 2007, focusing on pages corresponding to
tags, music tracks and user profiles. We obtained information about
a total number of 317,058 tracks and their associated attributes,
including track and artist name, as well as tags for these tracks
plus their corresponding usage frequencies. Starting from the most
popular tags, we found a number of 21,177 different tags, which
are used on Last.fm for tagging tracks, artists or albums. For each of
these tags we extracted the number of times each tag has been used,
number of users which used the tag, as well as lists of similar tags.

3.1.2. Flickr

For comparison with Flickr characteristics, we took advantage
of data crawled by our research partners during January 2004 and
December 2005. The crawling was done by starting with some initial tags from the most popular ones and then expanding the crawl
based on these tags. We used a small portion of the first 100,000
pictures crawled, associated with 32,378 unique tags assigned with
different frequencies.

3.1.3. Del.icio.us

The Del.icio.us data for our analysis was also kindly provided by
research partners. This data was collected during July 2005 by gathering a first set of nearly 6900 users and 700 tags from the start page

1 http://zonetag.research.yahoo.com.

of Del.icio.us. These were used to download more data in a recursive
manner. Additional users and resources were collected by monitoring the Del.icio.us start page. A list of several thousands user names
was collected and used for accessing the first 10,000 resources each
user had tagged. From the collected data we extracted resources,
tags, dates, descriptions, user names, etc. The resulting collection
comprises 323,294 unique tags associated with 2,507,688 book-
marks.

3.1.4. Sampling

Usage of tags basically follows a power law distribution for each
system (for details please refer to [6]). The most evenly distributed
system is Flickr, where people almost always tag only their own
pictures, not much influenced by others. For Del.icio.us, influence
of others is more visible: popular tags are being used more often,
while tags in the tail have less weight. Last.fm has even fewer very
popular tags, 60% of the top 100 representing genre information.
Since Last.fm covers a very specific domain, tags are more restricted
than in Flickr, where images can include everything, and Del.icio.us,
which has an even broader range of topics.

In order to improve tag based search, we first need to know
how tags are used and which types of annotations we can expect
to find along with resources. Given the vast number of tags in our
datasets, for practical constraints we had to sample our data. We
manually investigated 900 tags in total. For the three different tagging systems, we took three samples of 100 tags each to be manually
classified based on a tag type taxonomy presented in Section 3.2.
These three samples per system included the top 100 tags, 100 tags
starting from 70% of probability density (based on absolute occur-
rences), and 100 tags beginning from 90%. These different samples
based on rank percentages were chosen based on the results of prior
work [22], which suggested that different parts of the power law
curve exhibit distinct patterns.

Like in other complex systems, in collaborative tagging systems
patterns evolve that follow a scale-free power law distribution,
indicating convergence of the used vocabulary coexisting with a
long tail of highly idiosyncratic terms [22,24]. Commonly used,
more general tags have higher proportions [21]. Possible explanations are the imitation of other users behavior, shared knowledge
[21] and preferential attachment [22] as well as effects of system design choices [33,29]. Halpin et al. [22] relate this to the
principle of least efforts: While speakers prefer ambiguous and
general words with minimum entropy, thus minimizing the effort
for choosing the word, hearers prefer words with high entropy,
i.e. high information value. Thus, in a free-for-all tagging system,
there is a conflict between agreeing to a convention when tagging
or accepting the need for complex, multiple queries. The folksonomy structure evolves due to the consensus arising when tagging,
even though tagging is mostly for personal benefit. Our goal here
is to provide descriptive statistics about tag type usage depending
on popularity.

3.2. Tag characteristics

3.2.1. Defining tag types

For the purpose of analyzing the kinds of tags used in
collaborative tagging, we propose and use an extended tag taxonomy suitable for different tagging systems. We started with an
exploratory analysis of existing taxonomies (see [21,33,38]), as well
as possible attribute fields for the different resources to be consid-
ered. We kept and refined the most fine-grained scheme presented
by Golder and Huberman [21], adding the classes Time and Loca-
tion, in order to make it applicable to systems other than Del.icio.us,
which only focuses on Web page annotations. We went through
several iterations to improve the scheme by classifying sample tags
and testing for agreement between multiple raters. Our final tax-

K. Bischoff et al. / Web Semantics: Science, Services and Agents on the World Wide Web 8 (2010) 97109

Table 1
Tag classification taxonomy, applicable to different tagging systems.

No.

Category

Topic
Time
Location
Type
Author/Owner
Opinions/Qualities
Usage context
Self reference

Last.fm

romance, revolution
80 s
england, african
pop, acoustic
the beatles, wax trax
great lyrics, rowdy
workout, study, lost
albums i own, seen live

Flickr

people, flowers
2005, july
toronto, kingscross
portrait, 50 mm
wright
scary, bright
vacation, science
me, 100views

Del.icio.us

webdesign, linux
daily, current
slovakia, newcastle
movies, mp3, blogs
wired, alanmoore
annoying, funny
review.later, travelling
frommyrssfeeds

onomy comprises eight classes, presented together with example
tags from our datasets in Table 1.

Topic is probably the most obvious way to describe an arbitrary resource, referring to what a tagged item is about. For music,
Topic was defined to include main subject (e.g. romance), title
and lyrics. The Topic of a picture refers to any object or person
displayed. While such Topic information can partially be extracted
from the content of textual resources [23], it is not easily accessible for pictures or music. Tags in the Time category add contextual
information about month, year, season, or other time related mod-
ifiers. This includes the time a picture was taken, a music piece
or Web page was produced. Similarly, Location is an additional
retrieval cue, providing information about sights, country or town,
or the origin of a musician. Tags may also specify the Type, which
mainly corresponds to file, media or Web page type (pdf, blog,
etc.). In music this category comprises tags specifying encoding
as well as instrumentation and music genre. For pictures, this
includes camera settings and photographic styles like portrait or
macro. Yet another way to organize resources is by identifying the
Author/Owner who created the resource (author, artist) or owns it
(a music and entertainment group like Sony BMG or a Flickr user).
Tags can also comment subjectively on the quality of a resource
(Opinions/Qualities), expressing opinions based on social motivations typical for free-for-all-tagging systems, or are simply used as
rating-like annotations to ease personal retrieval. Usage context tags
suggest what to use a resource for, or the context/task the resource
was collected in and grouped by. These tags (e.g. jobsearch, for-
Programming, etc.), although subjective, may still be a good basis
for recommendations to other users. Last, Self reference contains
highly personal tags, mostly helpful for the tagger herself.

Clearly, such classification schemes only represent one possible
way of categorizing things. Quite a few tags are ambiguous due to
homonymy (especially for Flickr and Del.icio.us, e.g. apple). Here,
we based our decision on the most popular resource(s) tagged. During classification we even found some tags considered as factual
difficult to classify directly. For example, vacation can be considered as the Topic of a Web resource, as well as a personal tag of type
Usage context grouping resources for the next holidays. Similarly
zoo or festival may be depicted in a picture or used as context
attributes not directly inferable from the resource. Depending on
the intended usage as well as probably subjective and cultural differences such tags fit into more than one category. This problem of
concise category boundaries also applies to the other categorization
schemes presented in related work.

For evaluating our scheme using inter-rater agreement, we
selected 75 tags per system from our initial sample (25 randomly
chosen tags per popularity range) and had it assessed by students unfamiliar with the tag categorization scheme. We computed
Cohens unweighted Kappa () [13] which aims at indicating the
achieved inter-rater agreement beyond-chance, as the standard
measure to assess concordance for our nominal data. Our raw
agreement value for the  calculation is about 0.79 given the sum
of 0.77 for the by chance expected frequencies, resulting in a  of
0.71. This is considered substantial inter-rater reliability [26].

Part of the disagreement observed may be caused by ambiguity of the classification scheme. The confusion matrix created for
the  calculation reveals several prominent confusion patterns for
the Del.icio.us tagsalways involving the default Topic category.
Specifically, in several cases we found disagreement on whether
a tag indicated the Topic or Type, Author/Owner or Usage context.
These may indicate fuzzy category boundaries, subjectivity and cultural dependency, showing the direction of further improvements.
To account for the ambiguity in tag meaning and tag function for
certain resources, we gave the rater a chance to name a second category that would fit as well. Taking into account this second possible
category for a tag, our  improved considerably to 0.80.

3.2.2. Distribution of tag types across systems

Having defined this general tag taxonomy, we are interested in
seeing the tag distributions over the eight tag classes. We classified
all sample tags taken from the three different systems according to
the established taxonomy. The resulting distributions of tag types
across systems are shown in Fig. 1.2 A general conclusion is that
tag types are very different for different collections. Specifically,
the most important category for Del.icio.us and Flickr is Topic, while
for Last.fm, the Type category is the most prominent one, due to the
abundance of genre tags, which fall into this class. Obviously, genre
is the easiest way of characterizing and organizing music. One of
the rare exceptions was for the theme romance and some parts of
the lyrics or title. In contrast, a similar dominance can be observed
for Topic for Web resources and pictures. Type is also common in
Del.icio.us, as it specifies whether a page contains certain media.
As Flickr is used only for pictures, Type variations only include fine
grained distinctions like macro, but most users do not seem to
make such professional annotations. For pictures only, Location
plays an important role. Usage context seems to be used more in
Del.icio.us and Flickr, while Last.fm as a free-for-all-tagging system
(with lower motivation for organization) exhibits a significantly
higher amount of subjective (Opinions/Qualities) tags. Time and Self
reference only represent a very small part of the tags studied here.
Author/Owner is a little more frequent, though very rarely used in
Flickr due to the fact that people mainly tag their own pictures [29].
Studying the distributions for all systems across all samples, we
find a clear tendency of preferred tag functions that do not depend
much on the popularity of the tags. For example, we observe that
Type remains the predominant tag category for music, while for
URLs and pictures it is Topic. For the long tail of the Last.fm sample,
usage of Type category somewhat decreases, and opinion expression and artist labeling (Author/Owner) get more important.

With respect to exploiting tags in web search, it is encouraging to
see, that most tags are factual in nature, verifiable and thus potentially relevant to the community and other users. This applies to
Topics and resource Type in general, Topic and Location for pictures,
and to a certain degree Type for music. Subjective and personal tags

2 In later work, we classified 700 sample tags per tagging system, resulting in

similar distributions [5].

Fig. 1. Tag type distributions across systems.

(categories 6, 8) are only a minor part. Similar to results reported
in [39], Opinions/Qualities are only characteristic for social, free-for-
all music tagging systems (like Last.fm), possibly because for young
people (exposing) music taste is one important aspect in forming
ones own personal identity.

Other interesting results of this analysis refer to the added
value of tags to existing content. From the Del.icio.us crawl we had
extracted 20,911 URLs for which we had the full HTML page.3 For
these we counted how many tags appear in the Web page text
they annotate and found that this is the case for only 44.85% of
the selected Del.icio.us tags. In other words, more than 50% of existing tags bring new information to the resources they annotate. In
the music domain this is the case for 98.5% of the tags, as Last.fm
tags are usually not contained at all in lyrics (the only textual original content available). For a subsample of 77,498 tracks, we took
all tags corresponding to the tracks and tried to find them in the
track lyrics. On average, only 1.54% of the tracks tags occurred
in the lyrics. Especially for multimedia data, such as music, pictures or videos, the gain provided by the newly available textual
information is substantial.

We also showed that a large amount of tags is accurate and reli-
able. In the music domain, for example, 73.01% of the tags also occur
in online music reviews retrieved by Google, 46.14% are even to
be found in expert reviews on AllMusic.com. To analyze the overlap between tags assigned to Last.fm tracks and music reviews
extracted from Google results, we randomly selected 8130 tracks
from our original dataset, for which we tried to find music reviews
by sending queries in the form [artist track music review -lyrics]
to Google. The same query was used in [25]. For each of the selected
tracks we considered the top 100 Google results, and extracted
the text of the corresponding pages to create one single document
inside which we searched for the tags corresponding to the track.
73.01% of the track tags occurred inside such review pages. This
overlap is rather high, and probably caused by the fact that most of
the Last.fm tags represent genre names, which also occur very often
in music reviews.

Second, we investigated how many of the tags assigned to
tracks occurred in the manually created expert reviews from
AllMusic.com. We randomly selected music tracks from our Last.fm
dataset and crawled the Web pages corresponding to their
AllMusic.com reviews. If no review was available for one track, we
tried to find the review Web page of the album featuring that track.
The resulting dataset consisted of 3600 reviews. Following the same
procedure as for the previous experiment with reviews retrieved
via Google, we found that 46.14% of the tags belonging to a track
occurred on the AllMusic.com review pages. We hypothesize that

the lower number of matches is due to the fact that AllMusic.com
reviews are created by a relatively small number of human experts,
which use a more homogeneous and thus restricted vocabulary
than that found in arbitrary reviews on the Web. Still, at least one
Last.fm tag occurs in the review texts for almost all analyzed tracks.
This proves tags to be a reliable source of metadata about songs,
created easily by a much higher number of users.

3.3. Usefulness of tags for search

Extending and complementing our tagging analysis, we also
explored how users searching and tagging behavior compare. In
this experiment, we investigated how much current Web queries
overlap with tags. We used the AOL query logs [31] to calculate
the overlap between Web queries and tags, and contrasted tag and
query classes. In our comparative analysis of tags and queries we
tried to map web queries onto the tag taxonomy established in Section 3.2, thus investigating which kind of tags could best answer a
given query. We built a frequency sorted list of all queries in the
AOL log and took three samples from different regions of the power
law curves. We sampled 300 queries per type of resource (images,
songs, Web pages), by filtering the query log for queries containing
a keyword (like music, song, picture, etc.) or having a click on
Last.fm or Flickr. The resulting queries were classified into our eight
categories, with queries belonging to multiple classes in case they
consisted of terms corresponding to different functions.

The results are shown in Fig. 2. Not quite surprisingly, general Web queries often name the Topic of a resource, just like tags
in Del.icio.us do to an even larger extent. The query distribution
pattern fits to distributions of tags except for a clear difference
regarding category 5 (Author/Owner). Usage context is more often
used for tag based information organization than for search. For
obvious reasons, Self reference is not a useful query type for public
Web resources.

For images, our tag type distribution almost perfectly corresponds to the query type patterns. As Fig. 2 shows, Topic accounts
for about half of the queries, as well as of the tags in Flickr. Slight
differences exist for Location, used more for tagging than for searching and Author/Owner being somewhat more important for queries
than for tagging. Interestingly, there seem to be many more subjective queries asking for Opinions/Qualities like funny, public
or sexy pictures. With decreasing popularity of queries, this category becomes somewhat less important, but prevalence of Topic
increases slightly. While the number of adult content queries
in picture search was high for all three subsamples of varying
popularity, this kind of tags was completely underrepresented in
our analyzed samples of Flickr, Del.icio.us and Last.fm (one tag in
Del.icio.us).4

3 The HTML pages were taken from a WebBase crawl

(http://dbpubs.

stanford.edu:8091/testbed/doc2/WebBase/).

4 This holds also for the larger sample analyzed in [5].

K. Bischoff et al. / Web Semantics: Science, Services and Agents on the World Wide Web 8 (2010) 97109

Fig. 2. Distribution of query types for different resources.

The biggest deviation between queries and tags occurs for music
queries. While our tags in Last.fm are to a large extent genre names,
user queries often belong to the Usage context category (like wedding songs or songs from a movie, category 7). Also, users search
for known music by artist (category 5), title or theme (category 1).
These differences may be due to information value considerations:
as artist and title are already provided in Last.fm as formal metadata
there is no need to tag resources with this information. Lyrics are
not frequently searched for. A surprising observation is that searching by genre is rare: Users intensively use tags from this category,
but do not use them to search for music. One reason for this might
be that many music pieces get tagged with the same genre and thus
search results for genre queries would contain far too many hits.
Categorizing tracks into genre is also subjective to a certain extent,
as it depends on the annotators expertise. The amount of subjective qualities asked for is comparable to those tagged in the Last.fm
system.

Comparing categories of tags and queries offers some interesting
insights: Most of the general Web queries are Topic-related queries
(as most of the tags for Del.icio.us and Flickr). For Web resources
Topic tags are very useful, as over 30% of the queries target this cat-
egory; but we also see that although users query the Author/Owner
category, they usually do not tag in this way. For images, the Topic
category is as prominent and important for tags as it is for queries.
However, many queries are about Opinions/Qualities but users tend
to add more Location tags than the needed Opinions/Qualities. So,
even if users actually like to search for funny or scary pic-
tures, they often do not tag them in this way. As for the music
domain, tags generally fall into the Type (i.e. genres) class, although
more tags from Usage context and Topic categories would be needed
(Author/Owner is already present). This leads to the necessity of providing ways to extend and direct the tagging vocabulary towards
the sought classes.

4. Discovering knowledge through tags

In the previous section we have seen that tags are in general
very useful for search applications. Nevertheless, in some cases we
could identify some clear gaps between the tagging and querying
vocabulary: Usage context for music and Opinion for both music
and picture resources. Here, many more tags from these categories
would be needed for supporting the very frequent queries targeting
such characteristics of the content. In order to bridge these gaps in
the tagging vocabulary, we propose solutions based on tags.

In this section we will thus focus on inferring additional information from the tags associated with music and picture resources
and recommend it to the users during the annotation process. This
way we support users in providing tags from the categories we
need. More specifically, we will develop methods to identify the
corresponding moods and themes for songs, as well as picture

moods. With mood we understand the state or quality of a particular feeling induced by listening to a particular songs/seeing
a particular photo (e.g. aggressive, happy, sad, funny, etc.). The
theme of a song refers to the context or situation which best
fits for listening to songs (e.g. at the beach, dinner ambiance, night
driving, party time, etc.).

4.1. Datasets

To obtain the datasets for our experiments we used several data sources: Last.fm, AllMusic.com, www.lyricsdownload.com,
www.lyricsmode.com and Flickr. In the following we present some
relevant statistics for all of them.

4.1.1. AllMusic.com (AM)

Established in 1995, the AllMusic.com website was created as
a place and community for music fans. Not only all genres can be
found on AllMusic.com, but also reviews of albums and artists within
the context of their own genres, as well as classifications of songs
and albums according to themes, moods or instruments. All these
reviews and classifications are manually created by music experts
from the AllMusic.com team, therefore the data found here serves
as a good ground truth corpus. For our experiment we collected the
AllMusic.com pages corresponding to music themes and moods; we
could find 178 different moods and 73 themes. From the pages corresponding to moods/themes, we also gathered information related
to which music tracks fall into these categories and we restrict the
dataset to contain only tracks also present in our Last.fm crawl.

4.1.2. Last.fm (LFM)

For the purpose of our investigations, we crawled an extensive subset of the Last.fm website, namely pages corresponding
to tags, music tracks and user profiles. We started from the crawl
described in Section 3.1 and recollected the information related to
tags associated with music tracks. From all tracks that we obtained
from AllMusic.com, we could also find 13,948 of them in the Last.fm
dataset. For this intersection we had 81,964 different tags and for
each of these tags we have extracted information regarding the
number of times each tag has been used.

4.1.3. Lyrics (LY)

To investigate whether another source of information, namely
lyrics, as one part of music content, can provide added value in the
task of mood and theme recommendation, we also obtained the
corresponding lyrics for our tracks, if available. Here, we used a
previous crawl (described in [6]) of the www.lyricsdownload.com
site. Additionally, we crawled the www.lyricsmode.com website,
such that we could gather the lyrics for a total of 6915 tracks.

Table 2
Hierarchy of basic human emotions [34].

Primary
(Man. 1st)

Love
Joy

Surprise
Anger
Sadness

Fear

Secondary Emotion (Man. 2nd)

Affection, Lust, Longing
Cheerfulness, Zest, Pride, Optimism, Contentment,
Enthrallment, Relief
Surprise
Irritation, Exasperation, Rage, Disgust, Envy, Torment
Suffering, Sadness, Disappointment, Shame, Neglect,
Sympathy
Horror, Nervousness

4.1.4. Flickr (F)

For the purpose of deriving mood labels for pictures, we collected data from Flickr. We started by manually selecting Flickr
groups that correspond to the emotion/mood labels we wanted
to predict, and more explicitly, we made use of the hierarchy of
human emotions presented in Table 2. We found corresponding
Flickr groups for 17 out of the 25 secondary emotions, including
the six primary emotion labels. For all pictures in the identified
groups we downloaded via Flickrs API5 all associated metadata, in
particular the user assigned tags.

4.2. Deriving music moods and themes

As we could see in Section 3, the majority of tags associated
with music resources corresponds to genre information (around
60% of the tags). This is somehow redundant information, as it can
also be extracted from ID3 tags. Considerably less frequent are tags
referring to moods (20%) or themes (5%), though when searching for music, the majority of queries falls into these categories:
30% of the queries are theme-related, 15% target mood information
and the rest being almost uniformly distributed among six other
categories.

A natural question that arises is therefore: How can we support
users to provide these kinds of tags? Consider for example the song
of ABBA, Dancing Queen: by listening to the song or just considering the lyrics (Friday night and the lights are low/looking out for
the place to go/where they play the right music/getting in the swing
. . .) one immediately gets transposed into a weekend party atmosphere and an enjoyable state of mind. It would therefore be natural
to describe and also search for this song with mood related words
such as fun, happy, etc. and with theme tags like Party Time,
Thank God Its Friday! or Girls Night Out. Nevertheless, when
inspecting the tags Last.fm users provided for this track, we cannot
really identify these concepts. Instead, tags such as pop, disco,
70s or dance are quite often employed. With the algorithms
we describe in this section we can provide users with mood- and
theme-related tags to choose from during the tagging process and
we use the AM, LFM and LY datasets introduced in Section 4.1.

4.2.1. Music mood and theme recommendation algorithm

To recommend themes and moods, we base our solution on
collaboratively created social knowledge, i.e. tags associated with
music tracks, extracted from Last.fm, as well as on lyrics informa-
tion. Based on already provided user tags, on the lyrics of music
tracks, or on combinations of the two, we build classifiers which try
to infer other annotations corresponding to the moods and themes
of the songs. Our approach thus relies on the following hypotheses:

(i) Existing tags provided by users for a particular song carry information which can be used to infer the mood or theme of that

song, e.g. songs tagged with hard-rock are more likely to have
an aggressive mood than mellow-tagged songs.

(ii) The lyrics of the tracks give a hint on the mood or theme of
the songs. For example, tracks with love-related lyrics have
romantic evening as theme and correspondingly, a romantic mood.

In order to recommend mood and theme annotations we thus
build probabilistic classifiers trained on the AllMusic.com ground
truth using tags and/or lyrics as features. Separate classifiers correspond to the different types of classes that we aim to recommend
and to build the classifiers, we use the open source machine learning library Weka.6 In the experiments presented in this paper, we
use the Naive Bayes Multinomial implementation. Several other
classifiers (e.g. Support Vector Machines, Decision Trees) have been
tested, which resulted in similar classification performances, but
were much more computationally intensive. We have one classifier
trained for the whole available set of classes (i.e. either for moods
or themes) and this classifier produces for every song in the test
set a probability distribution over all classes (e.g. over all moods).
Thus, one or more classes (based on probabilities or on a given rank
number) can be then assigned to each song.

Based on the hypotheses enumerated above, we also experiment with three types of input features for the classifier: (1) tags;
(2) words from lyrics; or (3) tags and lyrics. Depending on the type
of features used to train the classifier and on the type of class that
the classifier will assign to songs, we propose 6 experimental settings (2 types of output classesmoods and themes; and 3 types of
featurestags, lyrics, and tags+lyrics).

Algorithm 1 presents the main steps of our approach. We show
the algorithm for mood recommendations based on tag features,
the other algorithms being corresponding variants.

Step 1 of the algorithm above aims at reducing the number
of mood classes to be predicted for the songs, since the 178
AllMusic.com mood labels are hardly distinguishable for a non-
expert. This step is optional, as we experiment with all classes of

5 http://www.flickr.com/services/api/.

6 http://www.cs.waikato.ac.nz/ml/weka.

K. Bischoff et al. / Web Semantics: Science, Services and Agents on the World Wide Web 8 (2010) 97109

moods/themes from AllMusic.com, as well as with a subset resulting
by applying a clustering method on the original set. In this paper,
we present only the results for the best performing classifiers, i.e.
themes clustered based on synonymy relationships (WordNet7)
and moods clustered into primary and secondary human basic
emotions [34]. The details for these clustering methods are provided in the next subsection.

As we need a certain amount of input data in order to be able
to consistently train the classifiers, we discard those classes that
have less than 30 songs assigned (step 2). After selecting separate
sets of songs for training and testing in step 3 (e.g. for every fold
in a 10-fold cross-validation), we build the feature vectors corresponding to each song in the training set (step 4). In the case of
features based on tags, the vectors have as many elements as the
total number of distinct tags assigned to the songs belonging to the
mood classes. The elements of a vector will have values depending on the frequency of the tags occurring along with the song. In
computing the vector elements, we experimented with different
variations and automatic feature selection (e.g. Information Gain),
but the formula based on the logarithm of the tag frequency provided best results and the full set of features was better suited for
learning, even though it contained some noise. Once the feature
vectors are constructed, they are fed into the classifier and used for
training (step 5). A model is learned and afterwards is applied to
any new, unseen data. We can choose how many moods are recommended to the user based on the probabilities resulting from the
classification or by setting an absolute threshold (steps 6ac).

4.2.2. Clustering

The WordNet-based clustering of themes aims at clustering
semantically related theme labels. On average, the 73 themes have
1.6 words (including stopwords; and 1.55 when discarding the
stopwords). For each of the 73 themes we first process the corresponding words this theme consists of. All stopwords are removed,
and for the remaining words we extract the corresponding WordNet synonyms. All resulting synsets are compared pairwise and if
the overlap between two sets is at least two words, the corresponding themes are clustered. With this procedure, the resulting set of
themes contained 58 entries.

For manually grouping the 178 AllMusic.com moods we made
use of the extensive work already done on studying human emo-
tions. Though there is little agreement on the exact number of basic
emotions let alone on a taxonomy including combinations of the
basic concepts into complex, secondary emotions, we found the
hierarchy reported in Shaver et al. [34] useful for our task (see
Table 2). Moods are usually considered very similar to emotions
but being longer in duration, less intensive and missing object
directedness. For categorizing the AllMusic.com moods we had to
slightly adapt the taxonomy to fit our data: Surprise was removed
since no example moods were found; the same happened for some
secondary emotions. Since some moods do not actually denote a
mood (e.g. literate), we introduced a new class (Neutral) with
three second level classes. In total, we obtained 23 second level
classes (Man. 2nd) falling into six first level classes (Man. 1st).
We also adopted a procedure similar to the one used in [34] to build
the aforementioned taxonomy of basic and secondary emotions. In
a similarity sorting task, all AllMusic.com theme terms written on
cards were sorted by the authors into as many and as high piles
as seemed appropriate. There was no limit with respect to number
of clusters and their size. For each person a co-occurrence matrix
stated whether two themes were placed in one category (1) or not
(0). Individual matrices were built and added to find good groupings

7 http://wordnet.princeton.edu.

by analyzing the clusters. Unclear membership of singular labels
was resolved after discussion.

4.2.3. Experiments and results

To measure the quality of our algorithms, we evaluate our
mood and theme tag predictions against the corresponding assignments in the AllMusic.com dataset. Being manually created by music
experts, the assignments of songs to classes of moods and themes
can be considered correct and thus accepted as ground truth. Since
our goal is recommendation of relevant annotations, we perform a
standard 10-fold cross-validation and evaluate our results choosing the following standard IR metrics: Hit rate at rank k (H@k),
R-Precision (RP) and Mean Reciprocal Rank (MRR). We concentrate on the H@3 metric, as we recommend three annotations
to the users to choose from. We consider three annotations a
good compromise, between providing enough suggestions and at
the same time not overwhelming the users with too much infor-
mation. We present the results for all our experimental runs in
Table 3.

We observe that the best performing methods are those using
tags as input features for the classifiers. The methods using only
lyrics as features perform worst. When combining tags and lyrics
as features, the corresponding methods perform much better than
those based only on lyrics and they sometimes also slightly outperform the tag-based methods. These results confirm once more
the quality of user provided tags, as well as hypothesis 1 on which
our approach relies (see Section 4.2.1). Lyrics, in contrast to tags,
introduce noise, as many song texts contain all sorts of interjections (e.g. hey, uh-huh, etc.), slang or simply informal English.
While incorporating lyrics features helps to achieve good results
for genre [7] recommendations, they do not seem to be indicative
of the mood of a song. For themes, there is a slight, yet rarely sig-
nificant, effect. Though alone lyrics are obviously not descriptive
enough to decide well upon the theme, by setting the topic, lyrics
may help removing some tag ambiguity. This relates to the second
hypothesis on which we built our approach.

For the case of theme recommendations, the best results,
H@3 of 0.88, are achieved for the algorithm using a combination of tags and lyrics as features and applying a WordNet
synonymy based clustering on the theme classes. Compared to
themes, mood recommendations do not perform as well when
using many classes, achieving only a H@3 of 0.64. For the case
of moods, we present the results corresponding to both first
and second level manual clustering of the original AllMusic.com
classes (rows Man. 1st and Man. 2nd). Reducing the number
of clusters to the six first level classes (Man. 1st), corresponding roughly to basic human emotions, boosts the performance
considerably and for the best method using tags and lyrics as
input features we achieve a H@3 value of 0.89. Of course, this
task is now much easier as can be seen from the as well much
better performance of the random classifier. Though having a
larger mood vocabulary for recommendations should be aimed
at, trade-offs are necessary. An interesting question for future
work is how many classes are appropriate to describe what mood
distinctions people actually do when listening or referring to
music.

Micro-evaluating results moreover per specific annotation class,
shows that while some classes are relatively easy to recommend,
others may require special attention or some level of disambigua-
tion. Table 4 shows H@3 values for the different classes without
applying any clustering method and using tags as features. In gen-
eral, those class labels that are harder to recommend appear more
ambiguous with the corresponding annotations being mostly sub-
jective. Themes like Late Night or Summertime strongly depend
on the person and what s/he is used to be doing late night or in

Table 3
Experimental results: H@3, H@5, RP, MRR for the different algorithms along with a random baseline for comparison.

Clustering

Themes


WordNet
WordNet
WordNet
WordNet

Moods


Man. 1st
Man. 1st
Man. 1st
Man. 1st

Man. 2nd
Man. 2nd
Man. 2nd
Man. 2nd

Classes

Features

Random
Tags
Lyrics
Tags + Lyrics

Random
Tags
Lyrics
Tags + Lyrics

Random
Tags
Lyrics
Tags + Lyrics

Random
Tags
Lyrics
Tags + Lyrics

Random
Tags
Lyrics
Tags + Lyrics

H@3

0.56*
0.80*+

0.72*
0.88+

0.17*
0.37+

0.82*
0.89*+

0.49*
0.64+

H@5

0.72*
0.94+

0.85*
0.96+

0.25*
0.48+

0.65*
0.78+

0.26*
0.48*+

0.38*
0.48+

0.06*
0.15+

0.42*
0.52+

0.21*
0.31+

0.46*
0.67*+

0.59*
0.69+

0.17*
0.32+

0.65*
0.73+

0.41*
0.52+

A * or a + states a statistically significant difference (one-tail paired t-test with p < 0.05) with respect to tags or lyrics as features, respectively (per clustering method).

Table 4
Examples of best and worst performing (by H@3) classes, without clustering, learned
using tags as features.

Best

Themes

Slow Dance
Romantic Evening
Autumn

Moods

Ethereal
Hypnotic
Angst-Ridden

#Docs

H@3 Worst

#Docs

H@3

Late Night
Summertime
Party Time

Precious
Calm/Peaceful
Rambunctious

#Docs gives the number of music tracks used in the experiments per mood/theme.

the summer.8 The same is true for moods like Precious or Ram-
bunctious, as they can be subjectively interpreted in several ways.
On the other hand, classes which can be recommended with high
accuracy are also more clearly defined, may it be a theme like Slow
Dance or a mood like Hypnotic. Interestingly, for neither moods
nor themes we found a correlation between the a priori probability
of a class, i.e. its size in terms of positive examples in the dataset,
and performance.

It is difficult to directly compare our results to the related work
cited in Section 2, as each paper uses a different number of classes.
Moreover, experimental goals, ground truth and evaluation procedures vary, or detailed descriptions are missing, e.g. whether strict
classification into one class is used or many classes are proposed
for one piece of music.

Instead, we also evaluated the quality of our recommended
themes for music tracks in terms of user judgements. Thus, we set
up a user survey as a Facebook application9 (see Fig. 3), where users
had to manually label songs with one or more theme classes used
in our algorithms and in AllMusic.com.

Fig. 3. Mood Mates! Facebook application.

With this user survey, we aimed to compare not only the performance of normal users against the AllMusic.com experts, but also
the results of our algorithm against the choices of the users.

The results presented in [7] show that our method performs
well also with respect to the user assignments. The fact that the
users perform quite bad compared to the AllMusic.com experts, but
our method performs well both compared to the users and to the
experts, indicates that our method provides theme labels that are
easier to recognize by users than the labels assigned by AllMusic.com
experts and thus helps for bridging the gap between the users and
music experts vocabularies.

8 In our Facebook evaluation study [7] those themes were used by many distinct people, thus, the bad performance may not be explained by only few people
making ideosyncratic use of a term a lot. Unfortunately, for the AllMusic.com mood
annotations no user/expert frequencies are available.

9 For details on the survey and application, please refer to [7] or access

http://www.facebook.com/apps/application.php?id=20699508679.

Similar to the case of music, our analysis for pictures showed
some clear gap between the tagging and the querying vocabulary.
Here, a large portion of tags refer to location information, such as
the country or city where the picture has been taken. However,

4.3. Deriving moods for pictures

K. Bischoff et al. / Web Semantics: Science, Services and Agents on the World Wide Web 8 (2010) 97109

queries targeting images much more often name subjective aspects
of the objects or persons depicted on the photos, e.g. scary, rage
or funny. In this section we will present an approach which aims
at bridging exactly this gap.

Table 5
Experimental results: H@1, H@3, H@5, RP, MRR for the different algorithms over
all picture moods, i.e. primary and secondary emotions; #Docs gives the number
of pictures per emotion. [Overall] shows the weighted average by the number of
instances in the mood class; [Random] is the random baseline for comparison.

4.3.1. Picture mood recommendation algorithm

Recommendations of mood annotations in case of pictures, rely
only on tag information. Unlike for music, where we could also
exploit the lyrics of the songs, for Flickr pictures, the only available textual information comes from tag data.10 The assumption
on which we base our recommendation is similar to the case of
songs, namely that the existing tags attached to photos can possibly provide information regarding the corresponding mood of the
pictures.

Given the crawling methodology which was used for pictures
and in order to ensure a fair classification of the data, all tags related
to a mood or emotion were deleted. To this end, we looked up in
WordNet all the labels included in our emotion taxonomy and collected all corresponding word forms of the most popular synset,
thus potentially including synonyms, as well as all their derivational forms like adjectives. For example, for Anger the synset
contains the word forms anger, choler, and ire as well as the
derivational forms angry, to anger and choleric. The resulting
list of terms was used to remove all matching tags of the collected
Flickr pictures (dataset F, described in Section 4.1). This approach
has been inspired from work focusing on the evaluation of personalization methods, where parts of the users preferences are removed
in order to be later inferred by the proposed personalization algorithms (see [12,10]).

The remaining tags are used as input features for training a
multi-class classifier over all classes of moods. Like in the case of
music, here we also make use of the Weka implementation for the
Naive Bayes Multinomial classifier, which produces for all pictures
in the test set probability distributions over all classes of moods.

4.3.2. Experiments and results

Like for music we also aim at evaluating the quality of the recommended mood labels for pictures. As ground truth data we use
the data collected from Flickr (set F), since all these pictures have
been manually assigned by users to Flickr groups centered around
human emotions/moods. All pictures pertaining to a specific mood
class represent the positive training examples, while pictures taken
randomly from the rest of the classes build up the set of negative examples. In all cases, the number of positive and negative
examples for a class is equally balanced.

A first set of experiments aimed at recommending mood labels
corresponding to the primary human emotions and in this case,
the classes to be learned by the classifiers consisted of the union
of all data belonging to all underlying secondary emotions (e.g. the
Love class comprises all data gathered from the Flickr groups for
Affection, Lust and Longing). Similarly, another experimental
run focused on secondary emotion label recommendations, and in
this case each secondary emotion class represented a class to be
learned. We performed 10-fold cross validation and evaluated the
performance of our method according to the same set of IR metrics,
which were used also for music: Hit rate at rank k (H@k), R-Precision
(RP) and Mean Reciprocal Rank (MRR). All results are summarized
in Table 5.

The results confirm once more the hypothesis on which we
based our recommendation approach: existing tags can give good
indications regarding the corresponding moods of the pictures. All

Mood

#Docs

H@1

H@3

H@5

Primary emotions

[Random]
[Overall]
Fear
Sadness
Joy
Love
Anger
Surprise

Secondary emotions

[Random]
[Overall]
Horror
Neglect
Sadness
Nervousness
Torment
Rage
Cheerfulness
Surprise
Longing
Relief
Disgust
Pride
Optimism
Affection
Zest
Irritation
Lust

recommendations corresponding to the primary human emotions
achieved very high quality, with a value close to 1 for H@3 and even
a H@1 between 0.71 and 0.91. We also compute the overall performance over all primary emotion classes, as averages weighted by
the number of instances corresponding to each class. The results
are very good, with a 0.97 value for H@3 and 0.93 for MRR.

Only for Surprise the results were somewhat poorer, with
H@1 of 0.61 and MRR of 0.77. This situation may arise due to
the fact that the only Flickr group which we could select for this
mood, was less focused (named Shocking, Surprise and General Wide Eyes!!). Looking in detail at the confusion matrix
(Fig. 4A), this becomes visible: Surprise is often misclassified
as Fear. A clear distinction seems to be difficult for our Flickr
users. The same fact was also indicated by psychological studies which reported that fear and surprise expressions are easily
differentiated from other basic emotions, but are often confused
with each other both in labeling and posing facial expressions
[17].

For primary emotions, correlation between class size and performance is medium: Pearsons r is 0.45 for H@3 and 0.63 for H@1,
RP, and MRR. Thus, when misclassifying instances the classifier is
biased to incorrectly assigning one of the two dominant classes
Fear or Sadness. Besides, these two emotions are very close
together in the cluster analysis of the mood label space of [34].11
Both share the same negative valence but with different intensity.
However, fear may range from low intensity (i.e. being worried) to
very high intensity (i.e. being panicked). Investigating the tag features used in learning these classes, similar tags are ranked high
according to their information gain, though all tags have rather
small values in general.

10 Other types of textual metadata, like titles, descriptions, comments, group
memberships, etc., could have been used, but we wanted to keep this approach
generalizable to other photo sharing systems, as well.

11 Participants had to sort emotion labels into piles of related emotions, thus establishing a hierarchy/clusters of basic and secondary emotions.

Fig. 4. Confusion matrices for (A) primary and (B) secondary emotions as image moods.

The overall weighted results for the secondary human emotion
label recommendations are almost identical to those for primary
emotions. If the averages are not weighted by class prevalence, the
overall unweighted averages for secondary emotions are about 0.2
lower compared to their counterparts for primary emotions. This is
due to the weighting process favoring the overly frequent and well
predicted classes Neglect and Sadness (corresponding to the
primary emotion Sadness) and Horror (with Fear as primary
emotion). As Sadness and Fear examples are also highly prominent in our experiment for primary emotions, the overall results
reported in Table 5 are similar. In general, correlation between a
priori probability of a class and performance is smaller for secondary emotions: Pearsons r is between 0.32 and 0.37 for the
different evaluation measures. Thus, the larger classes Neglect,
Horror and Sadness are predicted wrongly more often then the
remaining ones. Still, the confusion matrix in Fig. 4B indicates some
interesting patterns not easily explainable by classifier bias. Longing is very often misclassified as Sadness, much more than as the
very frequent Neglect or Horror. Although they belong to different primary emotions (Love vs. Sadness) and are rather far apart
in the cluster analysis of emotion labels, for Flickr users they seem
to share the negative valence and low arousal. Again, Surprise
gets easily confused with the fear related emotion of Horror.

When inspecting the results over the different mood classes,
we can see that for some classes, e.g. Affection, Zest, Irritation
and Lust, performance is considerably lower with H@3 ranging
from 0.18 to 0.52. The main reason for these results is the relatively
small number of pictures contained in each of these groups, which
made learning more difficult. Moreover, manually inspecting the
corresponding group of Flickr photos for all those four classes, we
found it difficult to identify pictures depicting only the intended
state of mind for each particular Flickr group. For example, we could
observe a large number of Affection pictures depicting sad/crying
people and implicitly a large set of tags close to the tags belonging
to the Sadness class. Given the relatively small set of pictures, the

influence of such ambiguous photos and implicitly their associated
set of tags becomes critical.

For all other mood classes, we achieve H@3 values over 0.6, for
about half of the classes even 0.90 and more, the best results being
obtained for the class Neglect  0.98 for H@3 and 0.96 for MRR.

Having used the same measures as in the case of music mood
and theme recommendations, we can directly compare the two
sets of results. In Fig. 5 we depict the H@3 and MRR values for all
best performing theme and mood recommendation algorithms for
music and pictures.

For music, both theme and primary mood label recommendations achieve almost equal H@3 values of 0.88. Recommendations
from the secondary mood classes are more error prone, achieving
only 0.64 H@3. For the case of pictures, we do not observe any difference for either primary or secondary mood recommendations.

Fig. 5. H@3 and MRR values across our best music, image, mood and theme recom-
mendations.

K. Bischoff et al. / Web Semantics: Science, Services and Agents on the World Wide Web 8 (2010) 97109

Moreover, recommendations for picture resources are of higher
quality, probably due to the data which was used as ground truth:
mood-related Flickr groups, manually created by users. The ground
truth gathered from AllMusic.com, given the extremely high number
of mood classes and implicit redundancy, had to be mapped to the
hierarchy of human emotions. This process potentially introduces
some noise into the data.

5. Conclusions and future work

Collaborative tagging has become an increasingly popular
means for sharing and organizing resources, leading to a huge
amount of user-generated metadata, which can potentially provide
interesting information to improve search. To tap this potential,
we extended previous preliminary work with a thorough analysis
of the use of tags for different collections and in different environ-
ments. We analyzed three very popular tagging systems, Del.icio.us,
Flickr and Last.fm and investigated the type of tags users employ,
their distributions inside the general tag classification scheme that
we proposed, as well as their suitability for improving search.
Our analysis provided evidence for the usefulness of a common
tag taxonomy for different collections and has shown that the
distributions of tag types strongly depend on the resources they
annotate. Moreover, we have shown that most of the tags can be
used for search and that in most cases tagging behavior exhibits
approximately the same characteristics as searching behavior. We
also observed some noteworthy differences: for the music domain
Usage context/Theme is very useful for search, yet underrepresented
in the tagging material. Similarly for pictures and music Opin-
ion/Qualities/Mood queries occur quite often, although people tend
to neglect this category for tagging.

Building on these results, we proposed a number of algorithms
which aim at bridging exactly these gaps between the tagging and
querying vocabularies, by automatically recommending mood and
theme annotations. We trained classifiers on input features consisting of either only the existing tags of the picture and music
resources, or of tags and lyrics information, in case of music songs.
The results of our evaluations show that providing such automatic
tag recommendations is feasible and that we can achieve very good
results both when comparing our algorithms with user judgements
and with expert-created ground truth.

Compared to some of our previous papers, where we introduced
algorithms trying to identify music themes based on existing user
annotations, here we address this aspect in more detail and discuss
its applicability for identifying additional knowledge also for other
types of multimedia resources. The experiments show that even if
the resources and the associated metadata differ significantly, we
can still achieve very good results and besides, these approaches
have the potential to bridge the existing gaps in the users tagging
and querying vocabularies.

In general, the results indicate that for music it is easier to
predict the corresponding themes of the songs rather than the
moods. Comparable results for the two types of recommendations were achieved when mapping the AllMusic.com moods to the
primary human emotions. On the other hand, mappings into the
secondary human emotions are more difficult and thus are susceptible to introducing noise. Recommendations of moods for picture
resources are overall of higher quality than for music, due to the
much more consistent set of tags attached to the photos and used
as input features. Apart from some subjective mood classes, known
to be difficult to distinguish, our tag recommendations are of high
quality and given the self-reinforcing nature of user-generated tags,
suggesting opinion and usage related concepts to users results in
a related tag vocabulary, which eventually will converge to a more
diverse set of tags.

For the future, we plan to further improve these algorithms
and in particular the feature selection mechanisms by automatically identifying the tag types (e.g. Topic, Author, Location, etc.) and
use them as input features for the classification. Other ideas worth
investigating refer to identification of other types of information for
multimedia resources, such as events, persons or locations, as well
as other types of entity frequently queried against by users. More-
over, we plan to use, like in the approach described by Blum et al.
[9], co-training, to alleviate the problem of the limited (labeled)
music ground truth. Last but not least, we would like to perform
another type of evaluation, where the value of the inferred annotation can be measured directly by comparing the results obtained for
a search engine with and without an enriched multimedia dataset.

Acknowledgments

We

are

greatly thankful

to our partners University
Koblenz/Landau and the Tagora project
for providing the
Flickr dataset and from the Knowledge and Data Engineering
Group/Bibsonomy [ http://www.bibsonomy.org/] at the University
of Kassel for providing the Del.icio.us dataset. This work was partially supported by the PHAROS project funded by the European
Commission under the 6th Framework Programme (Contract
No. 045035), and the GLOCAL project funded by the European
Commission under the 7th Framework Programme (Contract No.
248984).
