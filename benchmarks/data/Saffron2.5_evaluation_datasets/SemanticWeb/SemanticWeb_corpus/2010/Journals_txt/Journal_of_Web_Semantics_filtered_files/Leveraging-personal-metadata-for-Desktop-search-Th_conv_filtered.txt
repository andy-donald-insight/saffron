Web Semantics: Science, Services and Agents on the World Wide Web 8 (2010) 3754

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

j o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / w e b s e m

Leveraging personal metadata for Desktop search: The Beagle++ system
Enrico Minack a, Raluca Paiu a, Stefania Costache a,, Gianluca Demartini a, Julien Gaugaz a,
Ekaterini Ioannou a, Paul-Alexandru Chirita b,1, Wolfgang Nejdl a
a L3S Research Center/Leibniz Universitat Hannover Appelstrasse 9a, 30167 Hannover, Germany
b Adobe Systems Inc., Anchor Plaza, Timisoara Blvd. #26 Z, 061331 Bucharest, Romania

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 3 July 2008
Received in revised form 10 August 2009
Accepted 17 December 2009
Available online 13 January 2010

Keywords:
Semantic Desktop search
Personal information management
Automatic metadata generation

Search on PCs has become less efficient than searching the Web due to the increasing amount of stored
data. In this paper we present an innovative Desktop search solution, which relies on extracted metadata,
context information as well as additional background information for improving Desktop search results.
We also present a practical application of this approachthe extensible Beagle++ toolbox. To prove the
validity of our approach, we conducted a series of experiments. By comparing our results against the
ones of a regular Desktop search solution  Beagle  we show an improved quality in search and overall
performance.

 2009 Elsevier B.V. All rights reserved.

1. Introduction

The capacity of our hard-disk drives has increased tremendously
over the past decade, and so has the number of files we usually store
on our computer. With a few hundred of gigabytes at hand, it is
quite common to have over 100,000 indexable items on the Desk-
top. It is no wonder that sometimes we cannot find a document
anymore, even when we know we saved it somewhere. Ironically,
in some of these cases nowadays, the document we are looking
for can be found faster on the World Wide Web than on our personal computer. In view of these trends, resource organisation in
personal repositories has received more and more attention during the past years. Thus, several research and development projects
have started to explore personal information management, including Stuff Ive Seen [17], Haystack [38], or Gnowsis [43]. The personal
information management challenge is to make all resources on
ones Desktop easily accessible and manageable. In this context,
Desktop search is the obvious solution for finding such stored infor-
mation.

In order to offer better results, current Desktop search engines
have to improve the classic method of retrieval based on TFxIDF

 Corresponding author.
E-mail addresses: minack@L3S.de (E. Minack), paiu@L3S.de (R. Paiu),

costache@L3S.de (S. Costache), demartini@L3S.de (G. Demartini), gaugaz@L3S.de
(J. Gaugaz), ioannou@L3S.de (E. Ioannou), pchirita@adobe.com (P.-A. Chirita),
nejdl@L3S.de (W. Nejdl).

1 This work was performed while the author was employed by L3S Research

Center.

1570-8268/$  see front matter  2009 Elsevier B.V. All rights reserved.
doi:10.1016/j.websem.2009.12.001

measures, and use additional information about the searchable
resources. Currently, only few of the commercial Desktop search
engines collect basic metadata, such as titles, authors, comments,
etc., usually already contained in the indexed files. However, since
very few people spend time annotating their documents, this
functionality provides only a limited improvement over regular
text-based search. Studies have shown that people associate things
with certain contexts [45], or to be more specific, everything happens within a context and a person will not think of a thing by its
own, but within this very context. For example, a person will not
only consider a document, but also the email that it was sent with
and the person who sent it, i.e., the context of the document. For this
reason, this kind of information should be utilised during search. So
far, however, neither has this information been collected, nor have
there been attempts to use it.

In this paper we propose to exploit the implicit semantic
information residing at the Desktop level in order to enhance
Desktop Search. We therefore propose the automatic generation of metadata taking into account the context of Desktop
resources:

 Email context clearly generates useful information. For example,
one email might contain a question describing the object one is
looking for, and another email in the same thread might include
the answer to that question in the form of an attached document.
 Email attachments lose all contextual information as soon as
they are stored on the PC, even though emails usually include
additional information about their attachments, such as sender,
subject or comments. It would be helpful to find an attachment

E. Minack et al. / Web Semantics: Science, Services and Agents on the World Wide Web 8 (2010) 3754

not only based on its content, but also based on its associated
context2 from within the email.
 Folder hierarchies may contain valuable context information,
because we might have spent considerable time to build sophisticated structuring hierarchies for the documents we store.
 Browser caches include all information about the users browsing
behaviour. This is useful both for finding relevant results, and for
providing additional context for them.
 Downloaded publications also miss all their links, once stored
on our machines. Yet it would be very useful if a search application not only returns one specific scientific paper, but all the
referenced and referring papers which we downloaded on that
occasion as well.

The additional metadata generated would be useless without a
proper mechanism of querying and results ranking. Web search has
become very efficient due to the powerful link-based ranking solutions such as PageRank [35]. The recent arrival of Desktop search
applications, which index all data on the PC, promises to increase
search efficiency on the Desktop. However, Desktop search engines
are now comparable to first generation Web search engines, which
provided full-text indexing, but only relied on textual IR (Informa-
tion Retrieval) algorithms for searching and ranking.

We propose a centralised approach for querying, which combines the full-text and metadata search, and adds a modified
ObjectRank [3] mechanism for improved ranking of the retrieved
results. In summary, this paper makes the following main contri-
butions:
 present how to enhance and contextualise Desktop search based
on semantic metadata collected from different activities performed on a PC;
 show how this metadata can be used by a search application
together with a full-text index for improving search results and
ranking quality;
 by putting together all those enhancements we construct
Beagle++, an easily extendible application, and we further demonstrate its benefits through a set of user conducted experiments.

Through our extensions, we show that Beagle++ is not simply a
Semantic Desktop (as Haystack [38], IRIS [11], Gnowsis [43]), a Personal Information Management application (as SEMEX [16] or SIS
[17]), or a new data storage paradigm (as Lifestream [20] or TagFS
[7,21]). Instead, Beagle++ relies on a combination of all these aspects
to provide a Desktop search engine that works on the classic Desktop metaphor and exploits the semantics contained in the Desktop
data items. It is thus an example to illustrate the Semantic Desktop paradigm, demonstrating its benefits and potentials to ordinary
users. Beagle++ is available for download3 as sources, binaries and
virtual machine.

The components making up Beagle++ contribute to the NEPOMUK project.4 The goal of NEPOMUK is to create the Social Semantic
Desktop which allows management of Desktop resources as well as
sharing and exchange of data between Desktops [13,22]. NEPOMUK
provides an infrastructure for including various components in the
Social Semantic Desktop application. All our components were also
embedded in this framework, and thus also integrated with other
components such as Gnowsis [43].

The rest of the paper is organised as follows: In the next section
we present a short overview of the Beagle++ architecture and con-

tinue with details about each new component we add to the system.
We discuss exploited contexts and resulting metadata in Section 3,
together with the way we store and index this additional informa-
tion. Section 4 then introduces additional modalities for enriching
the already existing metadata by some novel techniques: Entity
Identification, Attachment-File Linker and ObjectRank. Section 5
presents how we combine metadata and full-text search, how we
rank search results, and how we present them to the user via a
visual interface. To test the efficiency and effectiveness of our solu-
tion, we conducted several experiments, described in Section 6. We
present related work in Section 7 and finally conclude and present
some future work in the last section.

2. Enhancing the Beagle Desktop Search Architecture to
Support MetadataAn overview

As basis for our Beagle++ environment we use the open source
Gnome Desktop search engine Beagle5 for Linux, which we extend
with semantic indexing, searching and ranking capabilities. The
reason for choosing Gnome Beagle to build upon was to reuse existing work on developing and establishing a Desktop indexing and
searching platform, such that we could primarily focus on developing the semantic part of our Semantic Desktop Search engine.

Fig. 1 illustrates the overall Beagle++ architecture. For maintaining the generated information up-to-date even if the physical files
on the Desktop change, we rely on the inotify-enabled Linux Kernel,
which catches all system events, i.e., files being created, modified
or deleted. All these kinds of Desktop events are sent to the Beagle Server, which is in charge of dispatching, indexing and sending
requests to the appropriate module. Several modules ensure the
execution of the following processes supported by our architecture:
 The extraction of metadata is issued by the Metadata Extractor
Backends, which dispatch extraction tasks to the appropriate filter
components, according to the type of resource and content to be
extracted. Information about the content of resources is extracted
using the Content Filters which act on specific file types (e.g., PDF,
XLS, DOC, EML, etc.6). The Metadata Filters extract specific information (Publications, Emails, BibTeX, Web pages), according to
the files role in the user activities, for example, the authors, title
and citations will be extracted from a publication.
 The enrichment of these metadata is performed by three Metadata Enrichment modules we developed: Entity Identification,
ObjectRank and Attachment-File Linker.
 The extracted content and metadata information are stored and
indexed in the central RDF repository.
 The search part of our Desktop search engine is provided by the
Beagle Search module. It is also responsible for routing the users
search requests to the Beagle Server, which further hands them
over to the Metadata Extractor Backends. These try to find relevant
results matching the queries in the RDF repository. The search
results coming from the Metadata Extractor Backends are merged
by the Beagle Server into one list of result documents, which is
presented to the user by Beagle Search.
 For being able to exploit the added value of the generated meta-
data, the results should be visualised in an adequate way to the
user. We therefore created the RDF Visualiser, which supports Beagle Search in presenting the search results by visualising the RDF
graph around matching documents.

Related projects like Haystack or Gnowsis similarly employ
extractors, each one specialised for one type of information source.

2 Desktop search is in fact a search into our past, and it should therefore exploit

the associative functionality of the human memory.

3 http://beagle.l3s.de/.
4 http://nepomuk.semanticdesktop.org/.

5 http://beagle-project.org/.
6 http://beagle-project.org/Supported Filetypes.

Fig. 1. Beagle++ Architecture Overview.

The extracted semantic information are represented in RDF and
stored in a centralised and uniformly accessible point. Therefore,
such an architecture has proven to serve our needs to develop a
high quality semantic retrieval system.

In the following sections we detail every component present in
our Beagle++ architecture, by showing the provided functionalities
and techniques behind them.

reason, Beagle uses two kinds of URIs: GUIDs8 for local files
and resources, and URLs for web accessible resources. GUID is
a .NET UUID [28] implementation, and in Beagle a URI is made
out of a GUID by prepending its unpadded base 64 representation by uid: and replacing slashes / by underscores  . As an
example the GUID 7QDBkvCA1+B9K/U0vrQx1A results in the URI
uid:7QDBkvCA1+B9K U0vrQx1A.

3. Metadata Generation and Storage

3.2. Describing and Extracting Metadata on the Desktop

As already presented in Section 2, an important functionality of
our Beagle++ Desktop tool is the creation and storage of metadata.
Since these metadata are used and processed by an extensible set
of components, they need to be compliant with a common welldefined ontology, such that every component which generates and
consumes metadata can rely on their format and semantics. In the
following, we will describe the way we reference Desktop objects,
the metadata format, and discuss the underlying Beagle++ Ontology.

3.1. Referring to Desktop Objects

Enrichment of the Desktop objects with metadata demands
unique identification of the resources. Otherwise, different filters
could generate metadata attached to apparently different objects,
although referencing to a single Desktop resource. Also, Desktop
resources are quite likely to be moved, e.g., when the user reworks
her file system structure or moves files. Therefore, we need a way
for uniquely identifying resources.

In case of a local file, Beagle generates a Unique Resource Identifier (URI) [5] which is used to identify a resource for the whole span
of its existence. The URI thus describes the resource represented by
the file, and not the file itself. The current location is just one piece
of metadata referring to that URI, so that now, if the resource is
moved or changed, only one piece of metadata needs to be mod-
ified. The URI does not change, whatever happens to the resource
described by the file. If a file is copied, then a new URI is used for
the target file.

Resources on the Web can also be moved, but this is less
likely to happen than on a computer. However, it is very difficult to be recognised7 by a Desktop search application indexing
visited web pages using their URLs [4] as the URIs. For this

7 Except by learning from HTTP 30 error messages.

3.2.1. Ontologies for metadata description

In an application heavily relying on metadata, such as Beagle++,
there is an obvious need for an ontology shared between mod-
ules: on the one hand, part of the data present on our Desktop is
already partially annotated in resource specific metadata formats,
for example, by EXIF metadata9 embedded in most of JPEG files; on
the other hand, a lot of other Desktop applications can make use of
metadata imposing domain and application specific requirements.
Thus, we need ontologies able to bridge the gap between resourcespecific metadata formats and application specific requirements,
so that they are reusable in many settings.

Such ontologies are what has been developed in the NEPOMUK
project.10 The NEPOMUK Ontologies11 consist of the NEPOMUK Representational Language (NRL), the NEPOMUK Annotation
Ontology (NAO), and the NEPOMUK Information Elements framework (NIE). Since Beagle++ is an integrant part of NEPOMUK, it is
natural to use the NEPOMUK ontologies in Beagle++ as well. We
describe them briefly hereafter.

NRL was developed based on RDFS,12 so that all RDFS schemas
are legal NRL, but only RDFS schemas following NRL usage recommendations are valid NRL. Significant additions relatively to RDFS
include named graphs and graph views. A named graph is a set of
RDF triples identified by a URI. This allows to make RDF statements
about graphs, e.g., about RDF statements, avoiding at the same time
the problems of RDF:Statement. Being able to annotate graphs
allows for example to specify whether a graph has to be interpreted
with closed- or open-world assumption. Graph views in NRL can be
compared with views in databases. A graph view is a transformation

8 http://msdn.microsoft.com/en-us/library/cc246025(PROT.10).aspx.
9 http://www.exif.org/specifications.html.
10 http://nepomuk.semanticdesktop.org/xwiki/bin/Main1/.
11 http://nepomuk.semanticdesktop.org/ontologies/.
12 http://www.w3.org/TR/rdf-schema/.

E. Minack et al. / Web Semantics: Science, Services and Agents on the World Wide Web 8 (2010) 3754

The scientific publication filter described next in Section 3.2.2 generates metadata using this ontology. The second smaller ontology16
defines concepts used by other components in Beagle++. The WordNet concepts are used by the file path filter, properties of web pages
are used by the web cache filter, and a Similarity class is used by the
Entity Identification module introduced in Section 2 and described
in Section 4.1.

3.2.2. Metadata Extraction

With the help of the ontologies described above, we can represent metadata extracted from Desktop data sources such as files,
emails, contacts and calendar items, instant messaging logs, notes,
Web history, to name only a few. For each data source, one of the
Metadata Extractor Backends introduced in Section 2 is in charge of
indexing its data items. These backends are processing, for exam-
ple, the file system, an email clients inbox or an instant messaging
programs log files, the corresponding data items being files, emails
and instant messages, respectively.

For each data item to be indexed, an appropriate Filter for processing the content and the metadata is selected. The extracted
information is stored in the RDF Repository. Each Filter processes
one specific type of file, identified by filename extension or MIME
type (e.g., .pdf or application/pdf, respectively). Based on the type of
extracted information, the Beagle++ Filters are classified into two
categories: Content Filters and Metadata Filters. The Metadata Filters
are specific to Beagle++ and are not present in the original Beagle
architecture.

Our Metadata Filters improve existing Desktop search systems
by extracting new and enhanced metadata information from four
specific sources. In the following, we describe these sources in
detail:

File Paths. Folder hierarchies are barely utilised by existing
search algorithms, in spite of the often sophisticated classification hierarchies users construct. For example, pictures taken in
Hannover could be stored in a directory entitled Germany, so
it would be useful to use this information for search. Normally,
a simple search for Hannover would be unable to retrieve the
pictures in that folder. The PathAnnotator component annotates
files with every token appearing in their file path, as well as additional semantic information provided by the WordNet system,17
such as direct synonyms, hyponyms, hypernyms, meronyms and
holonyms [12], for each of these tokens, but naturally excluding
the trivial tokens as home. This kind of additional information
is particularly important since for specific types of files, which do
not provide abundant textual information, the search process can
rely upon the text generated for this resource, based on the effort
that the user invested to organise her files in a hierarchy. For exam-
ple, a picture stored in the folder Germany is annotated with the
term Hannover, because Hannover is a part of Germany. Then,
this pictures can easily be retrieved by searching for the term Han-
nover.

Email Metadata. Emails provide much structured information
about people and how people and emails relate to each other.
For instance, an email can be sent as a reply to another email,
where each email is globally uniquely identified. People that send
or receive an email are also uniquely identified by their email
addresses. The inbox and the sent folder of an email client is a rich
source for such structured metadata. With our Metadata Filter for
emails, all this information becomes explicit in the RDF repository.
Due to the unique identifiers for emails and people, small graphs
extracted from each email can be integrated into a large network
of metadata.

Fig. 2. Example of a mail with attachment described using NIE. From
http://nepomuk.semanticdesktop.org/ontologies/nie/.

of a graph according to a given graph view specification. Views allow
different interpretations of a same data graph. A more detailed and
complete description of NRL can be found in [44].

NAO allows to describe annotations about resources. Those
include tags, ratings, alternative identifiers, creators, etc. It also
defines concepts specifically for annotating named graphs like version or engineering tools. More information about NAO can be
found in its specifications.13

The NIE Framework is a set of ontologies for describing Desktop
resources. The core ontology of the framework is NEPOMUK Information Element Ontology (NIE as well). Its main characteristic is to
make the distinction between an InformationElement, like an email,
and a DataObject, the actual sequence of bits encoding the information element, like an EML email file, a part of an MBOX mailbox
file or accessible through a remote IMAP server. See in Fig. 2 an
example of an email with attachment described using NIE.

This distinction allows to model an information element stored
in different data objects, like a photo stored on a user Desktop and
published on Flickr. Besides the NIE ontology, there are currently
six other ontologies in the framework providing concepts to model
different kind of information elements, like files, contacts, calen-
dars, messages, images and audio. You will find more information
about the NIE Framework on the NEPOMUK Ontologies web page.14
The NEPOMUK ontologies are high-level ones. They allow
domain-specific ontologies to be developed when necessary while
staying compatible with the NEPOMUK ontologies by extending
their concepts where possible. There are three main reasons why
we chose the NEPOMUK ontologies as a basis for metadata annotation in Beagle++: first, they were designed with the Personal
Information Management (PIM) domain in mind, which is the same
domain as Beagle++. Secondly, the NIE Framework is especially
appropriate for describing PIM objects, with the InformationElement concept, and files storing them, with the DataObject concept.
And finally, Beagle++ has been developed in the context of NEPO-
MUK, which makes its ontologies an ideal choice for facilitating
semantic interoperability with other components of NEPOMUK. For
Beagle++ we developed two domain ontologies. The first one15 is
based on the BibTEX vocabulary and models scientific publications.

13 http://nepomuk.semanticdesktop.org/ontologies/nao/.
14 http://nepomuk.semanticdesktop.org/ontologies/.
15 http://beagle2.kbs.uni-hannover.de/ontology/publications.

16 http://beagle2.kbs.uni-hannover.de/ontology/l3s.
17 http://wordnet.princeton.edu/.

Fig. 3. An example metadata graph extracted from one data item (here a PDF file representing a publication).

Web Cache. The linkage information between Web pages is of
high value on the Web for various techniques such as ranking. To
have the same degree of connection on the Desktop, we broadened
the notion of a visited link by defining it as a Web page that was
previously visited by the user (i.e., the links target page is present
in the browser cache). The WebCache component transforms the
visited links into metadata which are created for every Web page
in the cache, representing the links that have been visited from
that page, as well as the in-going links from which the user could
have arrived to it (inverse of a visited link). This information is also
of high value in order to connect the information on the Desktop
in a similar manner as on the Web, allowing for additional tech-
niques, e.g., ranking, but also the visited Web pages become of a
high importance if we think of them as very useful in a given working context, where the user of the Desktop required browsing for
solving a certain task. In addition, these visited links are also highlighted in the Web browser, in order to facilitate navigation for the
user. When searching for a Web page, the user can reconstruct his
previous navigational steps from another familiar Web page.

Scientific Publications. In the research community, many papers
are available in PDF format, but although PDF allows basic metadata
annotations like title and authors, these are rarely used. We therefore developed a Metadata Filter which extracts metadata from PDF
scientific publications and provides it to improve Desktop search.
Since information extraction is a difficult and error prone task, we
decided to leverage the publicly available DBLP18 database. The
Metadata Filter first extracts the title (as the most selective element
for a publication) from the text of the PDF using different heuristics
refined over a set of experiments.19 The obtained title is then used
to search the DBLP databases whose publication titles have been
previously indexed using standard IR techniques. If the first ranked
publication has a matching score above a pre-determined thresh-
old, it is assumed to be the publication contained in the PDF file, and
publication metadata like authors, conference, year of publication,
etc., are retrieved from the DBLP database.

3.3. Storing and Indexing Metadata

For each data item a metadata fragment is created, which itself
corresponds to a new data object, which needs to be stored and
indexed for later search. Fig. 3 presents an example of metadata
extracted from a PDF file representing a publication. Along with the

explicit metadata associated with the file (e.g., file name, file size,
creation and modification time, MIME type), we also extract more
complex metadata, such as the folder where the file resides, title of
the publication it contains, further referring to the conference it was
published at, etc. The more of these metadata fragments become
available in the RDF repository, the more Desktop resources get
interconnected. For instance, all PDF files that contain publications
being published at the same conference form a connected network
around the respective conference. Eventually, all these metadata
fragments get integrated into a large network inside the RDF repos-
itory.

For storing the produced metadata we employ the NEPOMUK
RDF Repository developed in the NEPOMUK project [22]. Being
based on the open source Java RDF storage, querying and reasoning
framework Sesame [8], as well as on Lucene, which is incorporated into the Sesame framework via the LuceneSail [30], it benefits
from the advantages of both: Sesame allows fast structural (RDF)
queries, and LuceneSail facilitates fast full-text queries over all liter-
als, including the actual content of the data item (see Section 5.1.1).
Besides, by using the capabilities of the Lucene inverted index, the
RDF repository allows performing full text search in the literals of
the generated metadata fragments, whereas using the Sesame store
allows the system to perform structured search via the SPARQL20
and SeRQL language.21 Recent performance studies showed that
the Sesame and LuceneSail infrastructure is very competitive compared to other available open source solutions [31].

4. Metadata Enrichment

As already mentioned in Section 2, once the metadata are stored
in the RDF repository, we propose to further apply several methods for enriching them. In this section we describe in detail these
methods and more precisely three different modules encapsulating them: the Entity Identification which looks for similar items in
the RDF repository and joins them, thus allowing to find entities
with different representations; the Attachment-File Linker which
preserves the links between emails and their attachments stored on
the Desktop, thereby improving the retrieval effectiveness by creating more relations in the RDF repository, and finally, the ObjectRank
module which adds metadata for supporting ranking the Desktop entities based on their link structure. In the following we will
discuss in detail each of these modules.

18 http://dblp.uni-trier.de/.
19 The heuristics include information like the title is at the top of the first page, in
bigger charachters.

20 http://www.w3.org/TR/rdf-sparql-query/.
21 http://www.openrdf.org/doc/sesame/users/ch06.html.

E. Minack et al. / Web Semantics: Science, Services and Agents on the World Wide Web 8 (2010) 3754

4.1. Resource Linkage using Entity Identification

Using a common RDF repository and ontology to manage all
metadata provides a uniform access to the data. However, we still
have to cope with the fact that different metadata fragments may
describe the same real world entity, for example an organisation or
a person. In Beagle++ a common cause for this issue are the different
contexts the Metadata Filters need to handle. For example, a person
in emails is described by an email address, whereas in a publication by the authors full name. Other causes for the appearance of
different references for the same entity are misspellings, the use of
abbreviations, initials, or the actual change of the entity over time
(e.g., the email address of a person might change).

To address this problem, we created and incorporated in
Beagle++ an algorithm that identifies references of the same entity
based on the related evidences found in the information on the
metadata level. The following paragraphs explain how this algorithm works within Beagle++. A more detailed description of the
algorithm and experiments performed is available in [25].

Consider searching with Beagle++ for resources related to person
Steven Kean. Searching using his surname retrieves publications
in which his surname appears as part of an author field. Searching
using his email address retrieves emails in which this email address
appears as part of the sender or receiver fields. The ideal would be
to retrieve all resources related to person Steven Kean, regardless
of the piece of persons information provided to the search module.
To offer this functionality, we need to identify the objects described
by the resources in the RDF repository, and link together the objects
that refer to the same real world entities.

Our algorithm aims at enriching the metadata by identifying the
entities along with their associated representational objects. For
the reasons we explained above, objects that refer to the same real
world entities typically have different identifiers. To address this,
we create matches between pairs of objects and compute the probability that these two objects refer to the same real world entity.
In Beagle++, our goal was to detect the person objects referring to
the same real world entities. More specifically, we consider the
metadata describing a predefined list of local Desktop resources:
emails, and publications. The metadata of these resources include
descriptions of objects, e.g., people as receivers/senders of emails,
people as authors of publications. Identified objects and resources
are then further processed by our algorithm. Clearly, the algorithm
is not limited to these resources or objects, but can be applied to
additional ones if this is required by the semantic application.

We then construct a Bayesian network, modeling matches along
with the related evidence. As an example, consider adding the
metadata generated by Beagle++ for two publications shown in
Table 1. After the addition of the new metadata, the Bayesian
network must be incrementally updated. For this, our algorithm
performs the following steps:

(A) Add Entity and Evidence nodes. We use entity nodes to represent
matches between two objects, and evidence nodes to repre-

Table 1
Extract of the metadata generated for two publications.

Subject

file:///425.pdf
file:///425.pdf
file:///425.pdf/person 1
file:///425.pdf/person 2
file:///686.pdf
file:///686.pdf
file:///686.pdf/person 1
file:///686.pdf/person 2
file:///686.pdf/person 3

Predicate

rdf:type
. . .text title
. . .author
. . .author
rdf:type
. . .text title
. . .author
. . .author
. . .author

Object

Publication
. . .
Steven Kean
Mark Schroeder
Publication
. . .
Steaven Kean
James Derrick
Mark Schroeder

sent the evidence information we identified for each match.
In the current implementation of Beagle++, we identify the
possible matches using similarity methods that measure the
resemblance between the text values found in the two objects
(e.g., string similarity, soundex similarity). As an example, consider the metadata of Table 1. Due to the resemblance between
the text values of the object with URI file:///425.pdf/person 1
and object with URI file:///686.pdf/person 1 the algorithm
will create evidence nodes (i.e., similarity (Steven Kean;
Steaven Kean)) and a corresponding entity node (i.e., Entity
(file:///425.pdf/person 1; file:///686.pdf/person 1)).

Newly created evidence nodes are connected to the entity
nodes using cause-effect relationships, with evidence nodes
being the effect and entity nodes the cause. All evidence nodes
have three states, Good, Moderate, and Poor, which we set based
on the computed similarity. All entity nodes have two possible states, True to indicate that the corresponding match
exists, and False to indicate that the match does not exist.
Since we do not have the necessary information to specify the
state of entity nodes, this assignment is done by probabilistic
inference.

Our algorithm allows incorporating other evidence nodes
for matches. This requires implementing two methods: (i) a
method that identifies when two objects might refer to the
same real world entity, and (ii) a method that returns the state
for each evidence node (i.e., Good, Moderate, or Poor) based on
the corresponding metadata.

(B) Add Relation nodes. The relation nodes represent a relationship between two resources, which we identify using
two methods: (i) when objects from the two resources
are part of the same match, and (ii) when this relationship is inferred by combing other existing relation nodes.
Consider again the entity node of our previous exam-
ple. A match between object file:///425.pdf/person 1 with
file:///686.pdf/person 1 also shows a relationship between
their corresponding resources which we represent using a relation node (i.e., Relation (file:///425.pdf; file:///686.pdf)).

Relation nodes are connected to entity nodes using a causeeffect relationship, with relation nodes being the effect of entity
nodes. They can also be connected to other relation nodes
using cause-effect relationships. These nodes have two possible
states, Yes to indicate that the two resources are related, and No
to indicate that the resources are not related. The probabilities
of these states is again computed by probabilistic inference.

(C) Update the Matches. Once the network is updated with nodes
representing new matches and evidences, we need to recalculate the probability for the states of each node. This task
is performed through probabilistic inference which updates
all nodes according to the current status of the network. We
use the computed probabilities of the entities to update the
matches in the RDF repository.

Fig. 4 shows the Bayesian network corresponding to the RDF
triples of Table 1. As shown, evidences for entity nodes are
soundex, similarity, and relation nodes. As we have explained
the state of the soundex and similarity nodes is known and thus
their probability is set to one of three states using threshold
values (Good, Moderate, or Poor).

4.1.1. Configuration and Execution within Beagle++

Bayesian networks are able to determine the probabilities of
cause nodes (i.e., matches) based on the current probabilities of
the effect nodes. This is performed using a task called probabilistic
inference [36]. The main requirement of this task is to provide the
prior probabilities; i.e., state how much would a single evidence
(soundex similarity, string similarity) influence the existence of a

Fig. 4. The Bayesian network as generated by the Entity Identification algorithm for RDF triples of Table 1.

match. For the purpose of Beagle++ we extracted this probabilities
using a small set of resources, objects, and their entities.

The RDF repository of Beagle++ is an incrementally growing set of
metadata. To capture the entities of the RDF repository, we execute
the Entity Identification algorithm at predefined time intervals, i.e.,
every hour. At each interval, we retrieve the newly added Desktop
resources along with the objects they contain. We then follow the
three steps we explained above in order to update the Bayesian net-
work. Once we execute the inference on the network we retrieve
the entities along with the existence probability. There are different representations of these entities (i.e., include or do not include
the probability of each match), and the selected representation
depends on the needs of the specific system. Also, an application
that needs only very certain matches will choose a high probability
threshold, whereas an application that accepts uncertain matches
chooses a lower one. In Beagle++, we use the results of this algorithm
to enhance search functionality, and thus we incorporate in the RDF
repository the entities with high probability. Table 2 shows the RDF
triples generated by the algorithm using the Bayesian network of
Fig. 4.

Table 2
The resulting RDF triples as generated by the Entity Identification algorithm for RDF
triples of Table 1.

Subject

file:///425.pdf/person 1
file:///425.pdf/person 2
file:///686.pdf/person 1
file:///686.pdf/person 3

Predicate

. . .similar to
. . .similar to
. . .similar to
. . .similar to

Object

file:///686.pdf/person 1
file:///686.pdf/person 3
file:///425.pdf/person 1
file:///425.pdf/person 2

Table 3
Recall and precision for various entity Probabilities.

Entity probabilities

Recall
Precision

4.1.2. Performance Evaluation

For evaluating the Entity Linkage algorithm, we used the Cora
dataseta collection of publications from CiteSeer.22 We converted
the information of each publication into RDF triples and imported
them in our RDF repository. This resulted into 1563 new resources
(i.e., publications) with 14,392 triples describing titles and authors.
There was a total of 9768 matches between the publication authors
with a maximum of 88 authors describing the same real world
entity.

We then executed the Entity Identification algorithm and compared the resulting entities with the ones provided in the Cora
dataset. Table 3 shows recall and precision for various entity prob-
abilities. As shown, for a low entity probabilities (e.g., 0.4) recall
is high and precision is quite satisfactory. For higher entity probability (e.g., 0.6, 0.7) the precision is increased and  as expected
 recall drops. The results show that other algorithms operating
on the RDF repository can control precision/recall by selecting an
appropriating value of the probability threshold.

22 http://www.cs.umd.edu/projects/linqs/projects/er/index.html.

E. Minack et al. / Web Semantics: Science, Services and Agents on the World Wide Web 8 (2010) 3754

Fig. 5. Desktop Ontology.

4.2. Resource Linkage using Attachment-File Linker

A different approach for creating new relations between Desktop resources exploits users actions on the Desktop objects. When
performing a task, the user accesses various resources, which can
be considered related. Therefore, we semantically translate the user
activities on the Desktop into metadata. In this paper, we translate such a user action: saving the attached files from an email to
a folder on the computer. We thus aim at preserving the semantic
connection between the attachment and the email it was sent with.
As soon as we save an email attachment on our disk, it becomes
a simple file and the original connection to the email is lost. Imagine searching for an institution, L3S for example, and receiving as
a result a document which apparently has no connection to the
research lab. But, while browsing its RDF metadata, the user could
see that it was an attachment to a file sent by a researcher in L3S,
then, she could instantly remember the whole context of the discussion in the email and why that document was sent to her. An
example of the generated metadata for such an email can be seen
in Table 4.

In order to serve this kind of situation, we created a new metadata enrichment tool, the Attachment-File Linker, representing a
linker between an email and its attachment saved on the disk. This
basically adds an additional link in the RDF graph between a file
and an email attachment, if the file was saved from the attachment
of that particular email.

The application searches over the whole RDF graph (reading
from the RDF repository) for two resources, one of type File and
one of type Attachment, with the same size and extension, the date

Table 4
Metadata generated for an email and its attachment.

Subject

file://mail1457.eml
file://mail1457.eml
file://mail1457.eml

file://mail1457.eml/;SECTION=0
file://mail1457.eml

. . .file name
. . .email text

Predicate

Object

rdf:type
. . .from
. . . email attach

Email
mailto:firan@l3s.de
file://mail1457.eml/;
SECTION=0
WebLinks.pdf
. . . This is a good
publication related to
your paper. . .

of creation of the file after the date of arrival of the email (that the
attachment belongs to), and with similar names (we used the SecondString library23 and a threshold of at least 0.5). The logic behind
this type of search is strictly connected to the normal behaviour of a
user who would save a file from an email, and would not change the
extension of the file, or modify the file by itself or even drastically
modify the name of the attachment.

In this way we can connect emails with the related attachments
saved on the disk creating more semantic relations in the RDF
repository, which helps us to improve the search effectiveness, as it
better simulates the structure implicitly present on the Desktop. An
additional component which exploits the semantic relations created by the previous Resource Linkage modules, will be presented
in the next section.

4.3. Metadata Enrichment using ObjectRank

The main problem on the Desktop is that PageRank-like algorithms cannot be deployed successfully as long as the Desktop
resources are not linked to each other. Therefore, what we need is
a way to explicate the links among them and a way to use this link
information in ranking. In addition to the email attachment links,
[9] describes further services for creating explicit links between
resources. A number of relationship types or property types are
used to describe the relationships among the resources and thus
influence the rankings.

ObjectRank [3] has introduced the notion of authority transfer
schema graphs, which extends ontologies, by adding weights and
edges in order to express how importance propagates along the
relationships inside an ontology. In our Desktop search framework
ObjectRank relies on ontologies (as the one presented in Fig. 5) for
modeling the importance flow among Desktop entities. For exam-
ple, the authority of an email is split among the sender of the email,
its attachment, the date when it was sent and the email to which
it replied to. So, if an email is important, the sender might be an
important person, the attachment an important one and/or the
number of times the email was accessed is very high. Addition-

23 http://secondstring.sourceforge.net/.

ally, the date when the email was sent and the previous email in
the thread hierarchy also become important. A part of the Desktop
ontology used for Beagle++ is depicted in Fig. 5.

As suggested in [3], in preparation of the computation, every
edge from the schema graph is split into two edges, one for each
direction. This is motivated by the observation that authority can
potentially flow in both directions and not only in the direction that
appears in the schema, e.g., if we know that a particular person is
important, we also want to have all emails we receive from this
person ranked higher. With Desktop resources linked to each other,
the final ObjectRank value for each resource is calculated based on
the PageRank formula:
r = d  A  r + (1  d)  e

applying the random surfer model [35] and including all nodes in
the base set. The random jump to an arbitrary resource from the
data graph is modeled by the vector e. A is the adjacency matrix
which connects all available instances of the existing context ontology on ones Desktop. Parameter d represents the dampening factor
and is usually considered 0.85.

The ranking approach in Beagle++ relies on the metadata generators which store metadata information in the RDF repository. In
order for ObjectRank to perform the computation of the scores, all
metadata that are stored in the repository have to be represented
as triples, in the form SubjectPredicateObject, and be compliant with the Desktop ontology. When instantiating the Beagle++
ontology for the resources existing on the users Desktop, the corresponding matrix A will have elements which can either be 0, if there
is no edge between the corresponding entities in the data graph, or
have the value of the weight assigned to the edge in the authority
transfer schema graph divided by the number of outgoing links of
the same type. The ranking computation is performed offline, and
the results are stored back into the RDF repository. Afterwards, the
ObjectRank recomputation is performed every N minutes (config-
urable), and is triggered by specific scripts. All entries, which are
present in the RDF repository are used for building up the data
graph on which the ranking computation is performed.

5. Metadata Search

After populating and indexing the metadata store we provide
the user with the search functionality as well as with the possibility
to visualise retrieved Desktop items together with their metadata.
In this section we will describe how Beagle++ performs the search,
how the final ranking of the results is computed, and how it displays
the retrieved resources.

5.1. Querying

When the indexing process is finished, the information about
processed Desktop items is stored in the RDF repository (see Section 3.3) and it is possible for the user to query it. Beagle++ allows
two types of queries: (1) usual keyword queries that only refer to
the content of the indexed resources, and (2) combined text and
metadata queries where specific keywords match the content and
others match specific metadata values (e.g., text:technical meta-
data:lucene).

In contrast to other semantic search approaches like [46,47,51],
where pure keyword queries are given by the user and thus semantic information are implicit, in Beagle++ the user expresses semantic
relations explicitly. This generally leads to the limitation that such
semantics cannot be used for enhancing the search process unless
the user knows how the structure of the metadata graph (the
underlying ontology) looks like, that is, how the RDF properties
are named. To cope with this problem, we manually established

Fig. 6. Detailed query processing in Beagle++. The RDF queries are first managed by
LuceneSail in order to query the Lucene index and are then transformed in order to
query the RDF store.

mappings between RDF property names appearing in the ontology,
and values which the user may employ for those properties. For
example, if the user would use the query author:john the system
will map the property name author with http://beagle2.kbs.uni-
hannover.de/ontology/publications#author, which appears in the
underlying ontology.

We further describe in more details the querying process which
takes as input the user query and provides back a ranked list of
results (see Fig. 6).

The user issues a query (for example Spain) by typing it into
the Beagle++ GUI, Beagle Search, which then sends it to the Beagle
Server, and this one calls the backends (see Section 2) for obtaining
different types of results. The user query is translated into a structured query and sent to the LuceneSail in order to be answered
using both the Lucene Index and the RDF repository (see Section
3.3). LuceneSail uses the incoming queries to retrieve results (i.e.,
Lucene Documents) from the Lucene Index and metadata results
from the RDF repository via the NativeSail, which is in charge of
retrieving RDF triples. Finally, the retrieved triples are converted
to Documents in the standard Lucene format. In our example all
information (both full-text and metadata) about Spain will be
retrieved, including even the ones that are not explicitly about
Spain, but describing for example Andalusia as a region of Spain.
This is possible due to metadata extraction (see Section 3.2), which
enriches resources with additional, related information.

LuceneSail merges the full text search in the RDF graph literals
stored in the Lucene index (these RDF graphs are indexed as fulltext and stored together with the content of resources) with the
search for concepts and relations (for example, Andalusia is a region
of Spain) in the native RDF store, enabling a structured and semantic
search like we illustrated in our example.

At this point the search for items similar to the relevant ones
is also performed. The list of relevant resources is expanded by
searching in the RDF graph for resources linked via a property which

E. Minack et al. / Web Semantics: Science, Services and Agents on the World Wide Web 8 (2010) 3754

indicates the similarity (see Section 4.1). For example, a misspelled
name is similar to the correct one which will be also added to the
results. The final list of documents is returned to be visualised by
Beagle Search as described in Section 5.3.

5.1.1. Translating the user query into a structured query

By allowing the user to issue full-text and structured queries
using a fairly simple (yet semantically enriched) query syntax opens
a gap between the user query syntax and the underlying RDF repos-
itory; the user query cannot be evaluated directly against the RDF
data. In order to close this gap, the user query  either text or meta-
data, or a combination of the two  needs to be translated into a
structured query the RDF repository can evaluate. Since the user
never sees the actual structured query, the actual query language
that is used by Beagle++ is an internal technical detail. However,
we decided to use SeRQL24 for subsequent examples, as well as in
our implementation. But be reminded that everything noted here
or being implemented using SeRQL can be equivalently expressed
in SPARQL.25

As a first step,

the user query (e.g., author:name:john
web) is decomposed into path query parts (e.g., the term
author:name:john) and keyword query parts (e.g., the term
web). For the keyword query part, the user can use the entire
query syntax provided by Lucene.26 The path query part is
a concatenation (colon-separated) of the predicates that shall
connect a keyword with the target resource. In order to find
resources that connects via an author predicate to resources
that match the keyword john in the predicate name, the
user has to query for author:name:john. More formally, a
path query part of the kind p1 : p2 :  : pN : k, where pi : 1 
i  N refers to predicates and k is a keyword, then the patterns (?s, p1, ?r1), (?r1, p2, ?r2), . . . , (?rN1, pN, k) are generated.
From a keyword query part k, a single pattern is generated:
(?s, ?p, k).

Structured query languages as SPARQL and SeRQL only match
literal values in a full string manner. Regular expressions can be
applied on literals, which can be used to mimic keyword search.
However, those regular expression are very inefficiently imple-
mented. They are applied on the result that matches the structured
query part, which is the entire repository for pure keyword queries.
Our full-text search over literals is provided by the RDF
repository containing LuceneSail [30], which employs Lucenes
state-of-the-art full-text indices. Full-text queries are incorporated
into structured query languages via virtual properties. These are
patterns that refer to properties that are not existing in the RDF
repository but rather interpreted by the query engine. This is the
common way to extend the capabilities of the used query language
without touching its syntax (cf. YARS [24], Jena LARQ27), Boca28 and
its open source fork,29 and Virtuoso.30

Using the LuceneSail, for each full-text pattern of the form (?s,

p, term), a LuceneSail query of the form

(?s, ls:matches ?m)
(?m, ls:query, term)
(?m, ls:property p)
(?m, ls:score, ?score)
(?m, ls:snippet, ?snippet)

24 http://www.openrdf.org/doc/sesame/users/ch06.html.
25 http://www.w3.org/TR/rdf-sparql-query/.
26 Apache LuceneQuery Parser Syntax: http://lucene.apache.org/java/2 3 0/
queryparsersyntax.html.
27 LARQ: Lucene+ARQ http://jena.sourceforge.net/ARQ/lucene-arq.html.
28 Boca: http://ibm-slrp.sourceforge.net/wiki/index.php/Main Page.
29 OpenAnzo: http://www.openanzo.org/about.html.
30 Virtuoso: http://virtuoso.openlinksw.com/.

is created, where p and term are replaced by the respective value.
If no property p is specified, the (?m, ls:property, p) pattern
is discarded.

The final structured query can be expressed in different structured query languages like SPARQL and SeRQL. All generated
patterns represent the WHERE clause and FROM clause, respectively.
As an example, the user query author:name:john will be formulated in SeRQL as follows31:

SELECT

d, score, snippet

FROM{d} bpp:author {a},
{a} ls:matches {m}

ls:query {john};
ls:property {nco:fullname};
ls:score {score};
ls:snippet {snippet}

This query is then sent to the RDF repository and evaluated
using the Lucene indices.
In the following we will discuss
how the final rankings of query results are computed in
Beagle++.

5.2. Ranking in Beagle++

The ranking schema we designed for Beagle++ uses a combination of the TF IDF score returned by the LuceneSail and an
extension of PageRank: ObjectRank (see Section 4.3). The motivation for such an approach is that users might want to find a
specific document not only based on its content, but also based
on the contextual information around it. Studies have shown that
users tend to associate things to different contexts [45], which
means that all this additional information should be utilised during
search.

Let us now consider a scenario for validating our assumptions
about the benefits of a search engine enhanced with ranking.
We assume that Alice is a team member of a computer science research institute, and one of the topics she is interested
in is recommender systems. Alice is currently writing a report
about new techniques for recommending multimedia content
and therefore needs to write a section summarizing the state-
of-the-art of recommender systems. She remembers that she
has stored on her Desktop a few good papers on collaborative
filtering techniques, partially papers found on the Web, partially received by email from different colleagues, and partially
papers for which she attended the presentations at several con-
ferences. For finding those papers, Alice uses a Desktop search
engine, where she issues the query collaborative filtering and
in response to her query she receives a long list of results, which
unfortunately does not contain in the top-5 results the papers
she was looking for. She would like to have instead a Desktop
search engine which takes into account her personal preferences
and presents the search results ranked based on this informa-
tion.
The new ranking schema we developed benefits both from the
advantages of Lucenes TF IDF score and those of ObjectRank. The
new scores are computed as a combination of them using the following formula:
(a) = R(a)  TF  IDF(a),

where a represents the resource, R(a) is the computed ObjectRank,
TF  IDF(a) is the TF  IDF score for resource a and R(a) is the

(1)

31 ls = http://lucene.apache.org/ontology/, nco = http://www.semanticdesktop.org/
ontologies/2007/03/22/nco/, bpp = http://beagle2.kbs.uni-hannover.de/ontology/
publications.

Table 5
RDF and RDFS vocabulary relevant for displaying RDF resources.

Predicate

Description

rdfs:label
rdf:type

A string labelling the resource
Refers to the class of the resource, which is a resource
itself, thus it also provides further label and description
information

therefore find many links between all the papers published at that
conference, their authors, and other papers and their authors that
are referenced or also stored in Alices Desktop. Prominent conferences and people of a certain domain are therefore more likely to
have many links to conferences, papers and authors of their domain.
This information is taken into account by the Beagle++ ranking so
that highly connected authors, conferences or papers are higher
ranked.

The next hits in the list represent papers written by prominent
people of the domain and at the same time very often cited in the
literature.

5.3. Displaying Desktop Resources

After the data are indexed, queried and ranked, the results have
to be presented to the user in a suggestive way within a visualisation interface. Since our main goal is improving Desktop search by
exploiting networks of metadata, we did not focus on coming up
with a completely new user interface, but rather took the Beagle
Search interface and showed how to incorporate the browsing of a
network of metadata into its task of displaying the found Desktop
resources.

In Beagle, all metadata extracted by the Content Filters refer to
classic Desktop resources, as files, emails, Web pages or instant
messages. For each of these groups, there is a specific way for displaying the resources into the search interface. For example, for
emails the subject and the senders email address are displayed, for
PDF files the file name, as well as the number of pages are displayed.
Each such resource also has a specific icon, sometimes providing a
thumbnail of its content.

In addition to the previous type of visualisation, the metadata extracted by the new Metadata Filters are of more general
naturethey are RDF resources described by RDF statements,
stored in the RDF repository, and therefore have to be visualised in a comprehensible manner. In contrast to the classic
Desktop resources, these may represent other types of entities
such as authors and conferences, i.e., not necessarily items which

Fig. 7. Search results retrieved by Beagle.

resulting score. The formula guarantees that the highest ranked
resources have both a high TF IDF and a high ObjectRank score.
The re-ranking is performed at query time.

Coming back to the presented search scenario presented, we can
see in Figs. 7 and 8 how the order of the search results differs when
using Beagle and Beagle++, respectively:

When using the Beagle system (see Fig. 7), Alice receives a
list containing 45 results (also including duplicates) and the first
ranked result represents a paper by Goldberg et al.  Using collaborative filtering to weave an information Tapestry  so, obviously
a relevant paper. The second and third ranked results are papers
on recommendations, however not very relevant ones. The list of
results Alice receives when using the Beagle++ system (see Fig. 8)
is shorter (duplicates are removed) and better ranked: as the first
ranked result, this list contains also Goldbergs paper, but the next
results are much more relevant for Alices interests. The second best
result is now a paper, which was accepted at a WWW conference:
Item-based Collaborative Filtering Recommendation Algorithms
by Badrul Sarwar, George Karypis, Joseph Kostan and John Riedl.
This conference is very important for Alice because she has stored
on her Desktop several papers which were accepted at different
editions of the conference. The metadata extraction phase could

Fig. 8. Search results retrieved by Beagle++.

Fig. 9. The Beagle++ search interface displays classic Desktop resources and RDF
resources.

E. Minack et al. / Web Semantics: Science, Services and Agents on the World Wide Web 8 (2010) 3754

Fig. 10. The Beagle++ RDF Visualiser lets the user browse her metadata network.

can be physically found on the Desktop but rather residing in
the processed files, emails, Web pages, etc. Due to this fact, the
visualisation must be generic, while still displaying meaningful
information to the user.

RDF and RDFS provide sufficient vocabulary for simple ways
of annotating any resource in a human-readable way. This can be
exploited in displaying Desktop resources. Table 5 lists the most
relevant predicates and their meaning, predicates which we considered appropriate to be used in the task of making the interface
more suggestive to the user. For displaying the name of a resource
in Beagle++ we use the rdfs:label predicate, thus replacing the
resource URI, since this is not very suggestive for the user. The
predicate rdf:type points to rdfs:class the type of the current resource, and since the reference to this class is again a URI,
we replaced it with its rdfs:label. This, of course, only works if
these resources actually have rdfs:label values. To provide this,
our metadata generator ensures that each extracted resource has
at least one meaningful rdfs:label. See Fig. 9 for an example of
how Beagle++ displays classic Desktop resources and RDF resources
in a comprehensive way.

The search interface also provides direct access to the resource
being displayed. By double-clicking the resource, our additional visualisation module is executed, loading the resource. We
designed and implemented an application, the RDF Visualiser,32
which displays the intermediate graph around a selected RDF
resource, and lets the user browse through the network of her
Desktop metadata.

In Fig. 10 we can see that the user can bidirectionally navigate
the graph of a resource, in our example the publication identified
by the DBLP identifier dblp:SarwarKKR01. The user can look for
the authors of the publication or its citations, is able to click on the
resources associated with them and look through their metadata
graph (i.e., navigate to another paper). In the other direction, she
can browse the conference and the tracks in which the publication
appeared, being able to further see other papers which appeared in
other editions of that conference.

We have so far explained the modules composing the Beagle++
architecture and their functionalities: metadata generation, metadata enhancement, indexing and storage of data and metadata,
search and visualisation of results. We further show how our

32 The Beagle++ RDF Visualisation is an improved and customised version of HPs
Experimental RDF Graph Visualizer located at http://www.hpl.hp.com/personal/
Craig Sayers/rdf/visual/index.html.

Semantic Desktop search solution improves the quality of Beagle
within a set of experiments.

6. Experiments

In order to evaluate the performance of our Beagle++ system,
the natural baseline we considered was Beagle, since Beagle++ is
an extension of Beagle. The first category of experiments aimed
to prove the quality of the results provided by Beagle++. It was
done involving human judges who rated the results that our system
provided to personalised queries. The second type of experiments
considered the performance in terms of time to index collections
of data, the amount of extra data (metadata) generated and the
response time for queries. Both sets of experiments were conducted on 12 data sets, which consisted of the data provided
by our researcher colleagues on a voluntary basis: emails, doc-
uments, publications, address books, calendar appointments and
other resources found on the users Desktops (.txt, .doc, .ppt, .html,
etc.).

6.1. Data set Description

For proving the quality and the performance of our Beagle++
system, we need a data collection which accurately represents
Desktop data characteristics for testing our algorithms on. How-
ever, given the privacy concerns the users usually have when
giving away their Desktop data  most of the times highly personal  currently there are no Desktop data collections publicly
available. Moreover, testing algorithms on artificial datasets can
be misleading and hard to evaluate as well. To overcome these
problems and also to make our experiments repeatable, we compiled for experimental purposes a new Desktop data collection.
More explicitly, we gathered real Desktop items from a number
of 12 colleagues corresponding to emails (sent and received), publications (saved from email attachments, saved from the Web,
authored/co-authored), address books and calendar appointments.
The distribution of the Desktop items collected from each user can
be seen in Table 6.

A total number of 56,484 Desktop items has been collected, representing 8.1 GB of data, on average each user providing 4707 items.
The users provided a dump of their Desktop data, including all kinds
of documents, not just emails, publications, address books or cal-
endars. We have only included these types of specific resources
in the above table, since they are the most important for our
modules.

Table 6
Desktop test dataresource distribution over the users.

Table 7
P@15 for querying with Beagle and Beagle++.

User ID

Emails

Publications

Address books

Calendars

Query type P@1

P@2

P@3

P@4

P@5

Total

6.2. User Studies and System Quality

We first did an evaluation of our Desktop search engine by conducting a small scale user study.33 Our colleagues, who provided
us a subset of their Desktop data, had to define their own queries,
related to their activities, and then performed searches over the
above mentioned reduced dumps of their Desktops. Each user had
to specify 8 queries in total:
 2 clear queries (single or multiple keywords, e.g., Markov chains),
 2 ambiguous queries (single or multiple keywords, e.g., architec-
ture),
 2 metadata queries with the structure metadata:value
(e.g., to:costache@L3S.de, which translates to emails sent to
costache@L3S.de),
 2 queries of type metadata and some additional keywords (e.g.,
recommender to:costache@L3S.de, which translates to querying
for emails sent to costache@L3S.de about recommender sys-
tems).

For the top-5 results, the user was asked to rate a query result
with 0 (not relevant) or 1 (relevant). The user needed to consider
a result only as relevant or not, disregarding the extent of the rel-
evance. For comparison purposes, we sent each of these queries to
three systems: (1) the original Beagle system (with output selected
and sorted using solely TFxIDF), (2) Beagle++ using the same TFxIDF measure for ordering its output, but giving more importance to
metadata results than to regular Desktop items,34 and (3) Beagle++
using enhancements for both metadata support and Desktop ranking based on ObjectRank.

We measured the quality of the produced annotations using pre-
cision, a standard Information Retrieval evaluation measure. As the
results had a confidence score, we computed precision at different
levels, namely P@5, P@4, P@3, P@2, P@1. The precision at level K
(P@K) is the precision score when only considering the Top-K out-
put. It represents the number of relevant query results within the
Top-K results divided by K, the total number of results considered.
First, the P@K scores were computed for each user and query, then
we averaged these values over the 2 queries of each type (clear and
ambiguous), obtaining the users opinion on each type of query.

33 For the evaluation of personal information management systems, as the Desktop search tools, we are always faced with the problem of a very low number of
testersdue to privacy concerns, people are not willing to provide their private
data and participate in such experiments. However, we consider that for this kind
of systems, the number of user who participated in the experiments (12) is quite
reasonable.
34 The advantage here is given by the fact of having metadata results additionally
to normal Desktop items. Compared to the Beagle system, the ranking is enriched
with additional metadata results.

B++

B++

B++

B++

B++

0.85 0.91 0.73 0.89 0.68
Clear
Ambiguous 0.78 0.82 0.72 0.84 0.74

0.89 0.66 0.89 0.67 0.87
0.83 0.69 0.84 0.69 0.83

We further averaged over all subjects and excluded the outliers.
We considered as outliers the results which were considerably distant from the average of the results, namely, not included within a
range of 70% plus/minus from the average. This was done mainly
because we observed that some users rated the relevance of some
results incorrectly. The resulting values are listed in Tables 7 and 8,
where B represents the results obtained using Beagle, B++ using
Beagle++, and B++OR using Beagle++ enhanced with the ranking
module.

6.2.1. Results and Analysis

In all cases, we observe that Beagle++ outperforms Beagle (see
Table 7). This is in fact explainable, since Beagle only uses TFxIDF
to rank its results, thus missing any kind of contextual importance measure for the Desktop resources. Another observation
is that, Beagle++ (Beagle enhanced with RDF metadata annota-
tions), already performs very well. An important reason for this
high improvement is that metadata are mostly generated for those
resources with high importance to the user (the ones she often
uses), whereas the other automatically installed files (e.g., help files,
which she might never have used) are not associated with meta-
data, and thus ranked lower. When we have metadata describing
a Desktop item, more and typically also very relevant text is available as part of the metadata to search for, and thus this item is also
easier to find.

When comparing Beagle++ results against Beagle++ with ObjectRank (see Table 8), the second system proves to be weaker in the
case of clear queries. This is because ObjectRank is mainly used
for disambiguating and for a clear query it practically adds some
noise in the results. The performance of the ObjectRank algorithm
improves and gets better than TFxIDF scores in the case of ambiguous queries, since we are considering queries with at least two
interpretations. In addition, it is natural for the algorithm to push
at the top of the results list the interpretation that is more used by
the user, because most of the resources related to that sense would
be interlinked and provide a better score.

In conclusion, the metadata enhancement solutions we proposed offer a visible quality improvement for Desktop search with
any type of user query. Moreover, the Desktop ranking mechanism
we introduced further enhances Desktop search output quality for
ambiguous queries.

Table 8
P@15 for querying with Beagle++ and Beagle++ with ObjectRank.

Query type

Clear

Ambiguous

Metadata

Metadata and keyword

P@1
B++

++

P@2
B++

++

P@3
B++

++

P@4
B++

++

P@5
B++

++

Fig. 11. An example of the generated Desktop metadata graphs. Note that the image is fuzzy due to privacy concerns. Nevertheless, this excerpt visualises the connectivity of a Semantic Desktops metadata.

Table 9
Number of hits provided by Beagle and Beagle++ for one query.

User

Beagle
Beagle++

6.3. Benchmarking and System Performance

The second set of experiments we conducted aimed to compare the performance of the Beagle++ system against Beagle. All 12
data collections were indexed with both Beagle and Beagle++ and
for each of the systems we observed the indexing time for various
dimensions of the data sets. On average, we had 4707 resources
per user, with an average size of 250,140 bytes per resource, which
resulted in approximately 1.17 GB of indexes. The range of Desktop
resources to be indexed per user varied from as little as 213 to ca.
30,600. The average time of indexing with Beagle was 39.21 min,
while with Beagle++ this increased to 149.24 min. This increase is
quite normal, since besides the Beagle processes, our modules are
also executed in Beagle++, which takes more time. Also, the indexing
in Beagle++ is performed in a completely different store than Beagle
is using, the RDF repository which has a full-text index and also a
semantic relations one (the querying is also performed on this store,
and therefore the querying can also be slower). It is also important
to note that this indexing is done only once, when Beagle++ is first
run on a machine, and thus it does not impact the performance of
our application. Afterwards, when a new action occurs (creation,
deletion, moving, renaming of a file), only the particular affected
file is handled, which is transparent to the user and makes use of
very few resources. The obvious gain with the Beagle++ indexing
is the additional metadata generated by the annotation toolson
average, Beagle++ generated about 1.8 million triples per user, useful data which is used for a better retrieval. An example showing the
complexity of the generated Desktop metadata graphs is depicted
in Fig. 11.

For measuring the response time for a query, each user proposed a personal query and a total of 12 queries were run against
each of the data collections. Three of the queries provided no result
when Beagle was used as the search engine. For the same queries,
Beagle++ provided more results in almost all cases. In Table 9 we
show the number of results returned for one query in the case of
all users. The response time for these queries increased from an
average of 0.348 s for Beagle, to 2.192 s for Beagle++still quite a
good response time. The almost 2 s difference is due to Sesame and
Lucene, both located at the NEPOMUK side: this takes longer than
the Lucene index located in the Beagle system because of the overhead caused by the XML-RPC framework of NEPOMUK. Considering
the number of found resources, Beagle++ outperforms Beagle with
a significant increase from an average of 5.714 results per query,
to 18.156 results, which means a bigger range of possible positive
responses for the user to choose from (usually also presented in
a better ranking order). Overall, the average response time for a
query increased, from 0.372 to 2.200 s.

7. Related work

Desktop search applications are not new to the industry, only
the high interest in this area is new: applications have been available since 1998 (e.g., Enfish Personal35), usually under a commercial
license. As the amount of searchable Desktop data has reached very
high volumes and will most probably continue to grow in the future,
the major search engines have recently given more focus to this

35 http://www.enfish.com/.

area than the academia. Thus, several free Desktop search distributions have been released (e.g., Google Desktop Search,36 MSN
Desktop Search,37 etc.). Moreover, some providers have even integrated their Desktop search tool into the operating system, such
as Apple.38 The open source community has also manifested its
interest in the area, the most prominent approaches being Gnome
Beagle39 (now also integrated into SuSE) and KDE KAT,40 developed
within the Mandriva community. Other relevant commercial Desktop search applications exist, such as Copernic, Yahoo! Desktop
Search, X1, Scopeware Vision, or PC Data Finder. Most of the above
mentioned applications target a very exhaustive list of indexed
file types, including any metadata associated with them. They also
update their index on the fly, thus tracking changes on the Desktop.
However, they either inherently miss the contextual information
often resulting or inferable from explicit user actions or additional
background knowledge, or they are limited to a small hard-coded
set of metadata [17].

We will further present previous work developed in various
research areas related to our tool and compare it to our present
work.

7.1. Metadata indexing and storage

Full-text Indexing. In the field of IR there has been strong research
on the topic of full-text indexing, primarily building on the well
known Vector Space Model proposed by Salton et al. [41]. In recent
years the research focus has been put on the compression of the
full-text indexes [33].

A very popular open-source full-text search and indexing tool
is Lucene.41 It offers two main functionalities: text indexing and
searching, based on the vector space model and therefore it is also
used as indexing system by both Beagle and Beagle++.

Metadata indexing. For metadata indexing, several approaches
to index metadata graphs have been proposed and most of them
decompose the graph into a set of triples or quadruples. In YARS
[24] and Sesame [8] multi-dimensional B-Trees are used. They support range scans very efficiently, for example, an index returns all
statements containing a specific subject and predicate. Similarly,
LuceneSail [30] integrates Sesame and the full-text indexing system
of Lucene. LuceneSail is also used by Beagle++ in order to perform
full-text search in the store. A different approach is the one of the
RDFStore model [39], which instead of multi-dimensional B-Trees
uses compressed multi-dimensional sparse matrices42 to combine
statements with indexed values.

7.2. Metadata enrichment

Using Metadata to Enrich Search Results. One interesting semantic search tool that uses metadata to enrich search results is the TAP
project [23]. TAP builds upon the TAPache module, which provides
a platform for publishing and consuming data from the Semantic
Web. Its knowledge base is updated with the aid of the onTAP sys-
tem, which includes Web pages templates, being able to read and
extract knowledge from several Web sites. The key idea in TAP is
that for specific searches, a lot of information is available in catalogues and backend databases, but not necessarily on Web pages
crawled exhaustively by Google. The semantic search based results

36 http://desktop.google.com/.
37 http://toolbar.msn.com/.
38 http://www.apple.com/macosx/features/spotlight/.
39 http://www.gnome.org/projects/beagle/.
40 http://kat.mandriva.com/.
41 http://lucene.apache.org/.
42 In the literature these sparse matrices are usually being called bitmaps.

E. Minack et al. / Web Semantics: Science, Services and Agents on the World Wide Web 8 (2010) 3754

are independent of the results obtained via traditional IR technologies and aim to augment them, as opposed to our approach, where
the semantic results are merged with the traditional ones.

In [34], the authors target to improve another domain, namely
digital libraries. In this context, creating collections of metadata
records from disparate and very diverse sources is a very tedious
task, often leading to inaccurate, incomplete or even missing
subject metadata. However, having proper subject metadata information is highly desirable, as this information enables users to more
easily discover and browse documents by limiting the results based
on their subjects matching the queries. The approach proposed in
the paper, thus aims at improving the subject metadata quality by
using statistical topic models, which can be also augmented with
human review and intervention for filtering out the low quality
topic labels, subject to be associated with the datas subject records.
[29] adopts a totally different perspective regarding the use of
metadata for enriching the search results. Here, the authors consider only metadata in the form of collaboratively created user tags
and use this information for inferring users social topic interests.
The created user profiles can then be used to personalise search
results, or connect like-minded users inside online social networks,
such as Del.icio.us.

[10] is also making use of tags, though for a different setting,
where tags are automatically attached to Web pages. Here the tags
represent concepts extracted from a known set of concepts without
any need of labeled documents and for achieving this, the authors
propose a probabilistic modeling framework that combines both
human-defined concepts and data-driven topics.

Using Metadata to Connect Information. In The Social Semantic Desktop [13], the authors envision that the next step towards
communication is a Desktop application based on the Semantic
Web, which could draw connections between all the types of data
people interchange. For example, an entry in an agenda would be
correlated with the author of an article or to the context associated with an email. Altogether, the entire information existing in a
social network would be connected to each Desktop. Such a structure would then help people organise and find information, due to
the enhancement brought by metadata into the system. We tend
to follow this direction and enhance the resources on the Desktop with a lot of metadata, which finally translates into multiple
connections between data items.

The Fenfire project [19] proposes a solution to interlink any kind
of information on ones Desktop. That might be the birthday with
the persons name and the articles she wrote, or any other kind of
information. The idea is to make the translation from the current
file structure to a structure that allows people organise their data
closer to the reality and to their needs, in which making comments
and annotations would be possible for any file.

Haystack [38] pursues similar goals as Fenfire. One important
focus is on working with the information itself, not with the program it is usually associated with. For example, only one application
should be enough to see both a document, and the email address
of the person who wrote it. Therefore, a user could build her own
links to Semantic Web objects (practically any data), which could
then be viewed as thumbnails, Web pages, taxonomies, etc.

A third project building an information management environment for the Desktop is Gnowsis [42]. The main idea behind
applications in this environment is the use of a central information server which allows users to manage and directly access
all the information on their computer (for example the author
of a file, her email address, etc.). Gnowsis envisions the possibility to link any two resources on the Desktop with a semantic
connection.

In the context of another interesting prototype, the interface
proposed by Yee et al. [49] improves image search by providing
and using faceted metadata. Users can add flat or hierarchical cat-

egories of information to images, and then use them for filtering
search results. Again, the idea is to provide an enhanced access to
information, based on the different kinds of collected metadata.

As compared to all these previous approaches, we have the same
aim of connecting resources located on the Desktop, but we further
utilise these links in order to build a better search tool on the Desktop which combines the traditional IR methods with the semantic
search, also allowing the user to better visualise this metadata network and browse it.

Entity Identification. Another methodology to enrich the metadata is to identify metadata describing the same real-world
entity but having different identifiers. Existing approaches aim at
constructing data structures using the data and then identify data
that refer to the same entities by studying the interconnections
between the data as these are relieved by the created structures.
For example, [6] calculates the link distance between the data
that could describe the same entity, and [27,26] computes the
connection strengths of alternative connection paths between
such data. Some systems address this problem in bibliographic
data. [50] introduces a constraint-based probabilistic framework
for matching publication authors. The DBLP system constructs
a co-author graph (nodes represent authors, links represent a
common publication). Merging authors is done using edit distance
algorithms, based on comparisons such as Levenshtein distance
and soundex. In [15], the authors use associations for merging data
that refer to the same entity, and then they propagate information
of this merge into the rest of the data.

The approach we follow aims at addressing the specific requirements of Beagle++ and related systems. Specifically, the metadata
in Beagle++ are constantly modified and therefore we created an
approach that allows the incremental computation and adaptation of the entity related information. Also, systems like Beagle++
will typically have a variety of different algorithms (e.g., the ranking algorithm) and of course these have different requirements
about the entity information. For example, one algorithm could
require as input only the entities for which we have enough evidences whereas another algorithm could also accept entities with
lower confidences. For this reason, we accompany the entities with
a probability that indicates the belief according to the metadata
currently in Beagle++.

7.3. Metadata searching

Using Context Metadata to Find Information. Naaman et al. [32]
describe an interesting approach for exploiting additional metadata
for pictures retrieval. The idea is to rely on automatically generated metadata (location, time and other digital photo metadata)
and manual annotations (events, etc.), automatically enhance these
metadata by providing information about actual light status (night,
day, dawn, dusk), weather conditions, temperature or additional
aspects on the events, and then use these metadata to find stored
images.

Another semantic search method using metadata is proposed
by [40]. It first does a classical text-based search on the metadata,
whose output is then extended using the RDF network induced
by the relations between semantic concepts, and finally reordered
with techniques adapted from IR.

[48] presents a new approach to content-based image retrieval.
To improve the retrieval performance, the authors use a selfadjustable metadata store, which records the optimised relevance
feedback information, representing the results obtained from previous queries from users that give a feedback on the relevance of the
retrieved pictures. This kind of information partitions the images
into classes denoting relevant images for future queries. The features taken into account by the algorithm are only low-level ones,
such as HSV colour-histograms or directional histograms.

Our approach focuses on a very wide range of metadata, not only

8. Lessons Learned

low-level ones, which is generated fully automatically.

Querying. In the context of semantic querying, several languages
which are used to interact with the repository have been pro-
posed. The two most common languages to query RDF are SPARQL,
a W3C recommendation, and SeRQL, which has been created for
the Sesame repository. SeRQL is a language similar to, and in some
means extending SPARQL. Their main characteristic is that it is possible to obtain an RDF graph as a query result. Both query languages
also support named graph querying.

The disadvantage of these languages is that the user has to learn
a complex query languages, therefore in Beagle++ we did not adopt
any of these query languages for the user interface. In our case,
the user will need either to type keywords in order to search for
content, or to type queries in the format property:value (e.g.,
author:john). The system then translates the user query into a
suitable format for the RDF repository. Ranking. There are currently
only limited (published) insights into the question of how to rank
Desktop search results, mostly based on very simple techniques.
Swoogle [14] is a search and retrieval system for finding semantic
Web documents on the Web. The ranking scheme used in Swoogle
uses weights for the different types of relations between Semantic
Web Documents to model their probability to be explored. How-
ever, this mainly serves for ranking between ontologies or instances
of ontologies. In our approach we have instances of a fixed ontology and the weights for the links model the users preferences and
practices. Our ranking algorithm resembles the method presented
in [3], where the authors apply authority-based ranking to keyword
search in databases modeled as labeled graphs.

The importance of semantically capturing user interest is for
example analysed in [2]. The purpose of their research is to develop
a ranking technique for the large number of possible semantic
associations between the entities of interest for a specific query.
They define an ontology for describing user interests and use this
information to compute weights for the links among the semantic entities. In our system, the user interest is a consequence of
her activities. This information is reflected in the properties of the
entities defined. The weights for the links are defined manually.

Displaying Desktop resources. There are numerous systems and
interfaces that deal with finding and displaying Desktop resources.
Their approaches can be categorised along two orthogonal dimen-
sions, i.e., static vs. interactive43[37] and graph-centric44[1] vs.
node-centric.45 The first classification refers to the output of the
visualisation tool; whether it is fixed or can be manipulated by the
user. The latter refers to the area of the graph that is displayed,
whether it is the entire graph or only a little fragment.

HP Labs developed An Experimental RDF Graph Visualizer which
shows the surrounding sub-graph of a resource in a static nodecentric way, but provides browsing and full-text search. The
visualiser is running as a servlet using Scalable Vector Graphics
(SVG) and is therefore platform independent and easy to integrate
into other applications. However, on startup it has to read the entire
graph into the main memory first which prevents it from being used
for large graphs. Beagle++ (which uses an enhanced version of this
tool integrated into the Beagle Search interface) does not face this
problem, since we always visualise smaller metadata sub-graphs.
Other systems (OntoRama [18], RDFSViz,46 OntoViz47) are specifically designed for visualisation of ontologies, which is too restricted
compared to our purposes.

43 http://semweb.salzburgresearch.at/apps/rdf-gravity/.
44 http://simile.mit.edu/welkin/.
45 http://fenfire.org/.
46 http://www.dfki.uni-kl.de/frodo/RDFSViz.
47 http://protege.cim3.net/cgi-bin/wiki.pl?OntoViz.

Developing, evaluating, and using Beagle++ allowed us to realise
several issues one needs to deal with in order to create an effective
Semantic Desktop search engine. In this section we present and
discuss the most important issues.

Metadata Extraction. Extracting metadata that are not explicitly
stored in Desktop resources require non-trivial algorithms, or the
usage of external information sources (cf. Metadata Filter for Scientific Publication in Section 3.2.2). For the former, we had to balance
between performance and metadata quality, while considering
their CPU and memory requirements. For the latter, a limited Internet connectivity requires local copies of such information sources,
which increases the application footprint. Those practical considerations are necessary to keep the application usable and accepted
by users.

We further realised that when different filters use the same
ontologies and URI schemes, i.e., reusing unique identifiers from the
resources being processed, data integrate nicely in the RDF reposi-
tory. In case of the email Metadata Filter that reuses the message-id
identifier, email and people will be connected to all relevant emails
once they are stored in the RDF repository.

Integrating Extracted Metadata. Metadata that are not uniquely
identifiable at filter-level need more sophisticated integration at
the global-level in the RDF repository. The improvements we identified when using our approaches (see Section 4), allowed us to
clearly realise that metadata enrichment methodologies are both
feasible and necessary for providing effective search functionalities
over metadata.

Using Metadata to Enrich Search Results. By extending the metadata so that users can search on and with structure, both the
querying process as well as the search results have to handle that
structure. On the one hand we had to balance between a simple
query language and rich structured queries. On the other hand, the
search result need to be visualised together with their structured
metadata, without overwhelming the user with such information.

9. Conclusions

In this paper we presented the Beagle++ Desktop search tool
and the underlying architectural design details for implementing
a semantically enhanced Desktop search application. Our current
implementation builds upon a snapshot of the standard Beagle
implementation and we provided details about all new components
we added to the system: the Metadata Filters, the RDF Storage and
Indexing Module, the Metadata Enrichment Components  Entity
Identification, ObjectRank and Attachment-File Linker  as well as
the RDF Visualiser.

As future work, we plan to have a better visual interface, which
would be able to offer a history of the navigation of the user, using
a Back button for revisiting resources seen previously. Also, further improvements are planned for every module, to offer a better
performance in terms of efficiency in storage, indexing and query-
ing. In order to improve the querying results, a task detection
module is envisioned, which would be able to disambiguate the
meaning of the users query and push more valuable results to the
user.

Acknowledgements

This work was supported by the NEPOMUK48 project funded by
the European Commission under the 6th Framework Programme

48 http://nepomuk.semanticdesktop.org/.

E. Minack et al. / Web Semantics: Science, Services and Agents on the World Wide Web 8 (2010) 3754

(IST Contract No. 027705). We would also like to thanks many
colleagues within L3S for their important contributions.
