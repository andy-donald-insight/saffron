Web Semantics: Science, Services and Agents on the World Wide Web 8 (2010) 176181

Contents lists available at ScienceDirect

Web Semantics: Science, Services and Agents

on the World Wide Web

j o u r n a l h o m e p a g e : w w w . e l s e v i e r . c o m / l o c a t e / w e b s e m

Invited Paper
CORAALDive into publications, bathe in the knowledge
Vit Novacek, Tudor Groza, Siegfried Handschuh, Stefan Decker

Digital Enterprise Research Institute, National University of Ireland Galway, IDA Business Park, Dangan, Galway, Ireland

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 26 May 2009
Received in revised form 9 October 2009
Accepted 26 March 2010
Available online 24 April 2010

Keywords:
Knowledge acquisition
Knowledge integration
Life sciences
Knowledge-based publication search

Search engines used in contemporary online scientific publishing mostly exploit raw publication data
(bags of words) and shallow metadata (authors, key words, citations, etc.). Exploitation of the knowledge
contained implicitly in published texts is still largely not utilized. Following our long-term ambition to
take advantage of such knowledge, we have implemented CORAAL (COntent extended by emeRgent and
Asserted Annotations of Linked publication data), an enhanced-search prototype and the second-prize winner of the Elsevier Grand Challenge. CORAAL extracts asserted publication metadata together with the
knowledge implicitly present in the relevant text, integrates the emergent content, and displays it using
a multiple-perspective search&browse interface. This way we enable semantic querying for individual
publications, and convenient exploration of the knowledge contained within them. In other words, recalling the metaphor in the article title, we let the users dive into publications more easily, and allow them
to freely bathe in the related unlocked knowledge.

 2010 Elsevier B.V. All rights reserved.

1. Introduction

Online scientific publishing makes knowledge production and
dissemination much more efficient than before. The publication
process is faster, since the essential phases like authoring, sub-
mission, reviewing, and final typesetting are largely computerised.
Moreover, the published content is easily disseminated to global
audiences via the Internet. In effect, more and more knowledge is
being made available.

However, is this growing body of knowledge also easily acces-
sible? We believe the answer is negative, since the rapid growth of
the number of available resources is making it harder to identify
any particular desired piece of knowledge using current solutions.
For instance, Medline, a comprehensive source of life sciences and
biomedical bibliographic information (cf. http://medline.cos.com/)
currently hosts over 18 million resources. It has a growth rate
of 0.5 million items per year, which represents around 1300
new resources per day [9]. Using the current publication search
engines,1 one can explore the vast and ever-growing article repositories using relevant keywords. But this is very often not enough.
Imagine for instance a junior researcher compiling a survey on var-

 Corresponding author. Tel.: +353 91 495738.
E-mail addresses: vit.novacek@deri.org (V. Novacek), tudor.groza@deri.org

(T. Groza), siegfried.handschuh@deri.org (S. Handschuh), stefan.decker@deri.org
(S. Decker).

1 For

to

their

example,

ScienceDirect,

journals
(cf. http://www.sciencedirect.com/), or PubMed, a search service covering bibliographic entries from Medline and many additional life science journals together
with links to article full texts (cf. http://www.ncbi.nlm.nih.gov/pubmed/).

Elseviers

front-end

1570-8268/$  see front matter  2010 Elsevier B.V. All rights reserved.
doi:10.1016/j.websem.2010.03.008

ious types of leukemia. The researcher wants to state and motivate
in the survey that acute granulocytic leukemia is different from T-
cell leukemia. Although such a statement might be obvious for a life
scientist, one should support it in the survey by a citation of a published paper. Our researcher may be a bit inexperienced in oncology
and may not know the proper reference straightaway. Using, e.g.,
the PubMed search service, it is easy to find articles that contain
both leukemia names. Unfortunately, there are more than 500 such
results. It is tedious or even impossible to go through them all to
discover one that actually supports that acute granulocytic leukemia
is different from T-cell leukemia.

Given the wealth of knowledge in life science publications and
the limitations of current search engines, it is often necessary to
manually scan a lot of possibly irrelevant content. To overcome
the problem that anything more expressive than (Boolean combinations of) mere keywords is virtually impossible today, it is
necessary to develop technologies that can operate at an enhanced
level, using more-expressive concepts and their various relation-
ships. This requires collecting, extracting, and interrelating the
knowledge scattered across the large numbers of available life science publications. Unfortunately, manual creation of the necessary
information is not really possible at large scale, and automated
extraction produces noisy and sparse results [2].

We believe that a few essential elements will enable more
knowledge-based search in scientific publications: (i) extraction of publication annotations asserted by people (e.g., author
names, titles, references or text structure). (ii) Extraction of
knowledge implicitly present in publications (e.g., statements
encoding typed relations between particular concepts, or structured representations of arguments made by authors in the text).

Fig. 1. CORAAL architecture.

(iii) Comprehensive integration, augmentation, and refinement of
the extracted content, possibly using extant machine-readable
resources (e.g., life science thesauri or vocabularies). (iv) Interlinking of the processed content (e.g., connecting relevant arguments
across publications, or preserving provenance of statements about
relations between particular concepts). (v) Intuitive access to and
display of the content extracted from publications, so that everybody can easily search for the extracted knowledge and track its
provenance. (vi) Methods for collaborative curation of the resulting content, so that global expert communities can contribute to
further refinement of the extracted publication knowledge.

CORAAL constitutes a particular solution to some of the principal problems inherent in knowledge-based publication search. We
provide an overview of the system and its implementation in Section 2. Then we describe its application and evaluation within the
Elsevier Grand Challenge in Section 3. Summary of related systems
is given in Section 4. Section 5 concludes the paper with a discussion
of a future outlook.

2. CORAAL essentials

In the following we introduce the basic features of the CORAAL
system. Section 2.1 provides an overview of CORAAL, its archi-
tecture, and relevant implementation details. In Section 2.2 we
describe the pipeline in which we extract, process, and display the
knowledge and metadata extracted from publications.

2.1. Overview of the solution

In order to provide comprehensive search capabilities in
CORAAL, we augment standard (full-text) publication search
with novel services that enable knowledge-based search. By
knowledge-based search we mean the ability to query for and
browse statements that capture relations between concepts in the
retrieved source articles.

CORAAL is built on top of two substantial research products of
our group at DERIthe KONNEX [4] and EUREEKA [8] frameworks.
The former is used for storing and querying of full-text publications and associated metadata. The latter supports exploitation
of the knowledge implicitly contained in the texts by means of
knowledge-based search.

CORAAL itself essentially extracts asserted publication metadata
together with the knowledge implicitly present in the respective
text, integrates the emergent content with existing domain knowl-
edge, and displays it via a multiple-perspective search&browse
interface. This allows fine-grained publication search to be combined with convenient and effortless large-scale exploitation of the
knowledge associated with and implicit within the texts.

2.1.1. Architecture

The architecture of CORAAL is depicted in Fig. 1. The EUREEKA
library caters for knowledge extraction from text and other knowledge resources (e.g., ontologies or machine readable thesauri) via

V. Novacek et al. / Web Semantics: Science, Services and Agents on the World Wide Web 8 (2010) 176181

the knowledge extraction module. After being processed by the ACE
knowledge refinement and augmentation pipeline (details provided in Section 2.2), new facts are added into a knowledge base.
There can be multiple knowledge bases if users wish to keep content from different domains separate. The knowledge bases are
exposed to consumers via a semantic query answering module.
Indices are used to help optimize the retrieval and sorting of statements based upon relevance scores.

Another crucial part of the CORAAL back-end is KONNEX, which
processes the publication text and metadata in order to complement the knowledge-based search supported by EUREEKA by
rather traditional full-text services. KONNEX integrates the texts
and metadata extracted from the input publication corpus in a
triple store, representing all the information as RDF graphs (cf.
http://www.w3.org/TR/rdf-primer/). Operations related to data
incorporation and necessary full-text indices (for the publication
text and particular metadata types) are handled by dedicated manager modules.

2.1.2. Relevant implementation details

Since CORAAL contains several conceptually separate modules,
we utilise an inter-process communication layer implemented
using the D-BUS framework (cf. http://dbus.freedesktop.org/). A set
of proxy helper services rests on top of the core-level EUREEKA
and KONNEX APIs. These manage the user requests and forward
the data returned by the core APIs to the so-called web hub layer,
which is organized as a decoupled set of stateless web services,
each of which handles particular types of search.

The web services produce machine-readable RDF expressions
that represent answers to user queries. The RDF has XSL style sheets
attached, allowing both its rendering for human readability and
machine consumption to be provided simultaneously. The humanreadable form is also enhanced by the Exhibit faceted browsing
web front-end (cf. http://www.simile-widgets.org/exhibit/). A hidden advantage of this mechanism is the shifting of the processing of
the data for visualization purposes from the server to the client side,
as the XSL transformation is performed by the clients browser. Such
a solution results in CORAAL being a pure Semantic Web applica-
tion, as the data flow between the core infrastructure and the other
modules is strictly based on RDF graphs.

2.2. Knowledge acquisition pipeline

The publications metadata and full text were stored and
indexed within KONNEX for link processing [4]. After parsing
the input articles (either directly from PDF, or from annotated
text-based files as provided, e.g., by Elsevier), the metadata and
structural annotations were processed by KONNEX. First we eliminated possible duplicate metadata annotations using a string-based
similarity heuristic. Each article was then represented as a comprehensive RDF graph consisting of its shallow metadata, such as
title, authors, linear structure with pointers to the actual content
(sections, paragraphs, etc.), and references. The references were
anchored in citation contexts (i.e., paragraphs they occur in), and
represented as individual graphs allowing for incremental enrichment over time. The articles full-text information was managed
using multiple Lucene indices (cf. http://lucene.apache.org/), while
the graphs were integrated and linked within the KONNEX RDF
repository.

While KONNEX catered for the raw publication text and meta-
data, exploitation of the more structured publication knowledge
was tackled by our novel EUREEKA framework for emergent (e.g.,
automatically extracted) knowledge processing [8]. The framework builds on a simple subject-predicate-object triple model.
We extend the subject-predicate-object triples by adding positive
or negative certainty measures and organised them in so-called

conceptual matrices, concisely representing every positive and
negative relation between an entity and other entities. Metrics
can be easily defined on the conceptual matrices. The metrics then
serve as a natural basis for context-dependent concept similarity
scoring that provides the basic light-weight empirical semantics
in EUREEKA. On top of the similarity-based semantics, we implemented two simple yet practical inference services: (i) retrieval
of knowledge similar to an input concept, and/or its extension by
means of similar stored content; (ii) rule-based materialisation of
relations implied by the explicit knowledge base content, and/or
complex querying (similarity as a basis for finding query variable instances for approximate evaluation of rules). The inference
algorithms have anytime behaviour, meaning that it is possible
to programmatically adjust their completeness/efficiency trade-off
(i.e., one can either have complete, but possibly largely irrelevant
set of solutions in a long time, or incomplete, but rather relevant
set in a relatively short time). Technical details of the solution are
out of scope of this system overview article, but one can find them
in [8].

We applied our EUREEKA prototype to: (i) automate extraction of machine-readable knowledge from particular life science
article texts; (ii) integrate, refine, and extend the extracted knowledge within one large emergent knowledge base; (iii) expose the
processed knowledge via a query-answering and faceted browsing
interface, tracking the article provenance of statements.

For the initial knowledge extraction, we used a heurislanguage processing (NLP)stemming
tics based on natural
essentially from [5,10]to process chunk-parsed texts into subject-
predicate-object-score quads.2 The scores were derived from
aggregated absolute and document-level
frequencies of sub-
ject/object and predicate terms. The extracted quads encoded
three major types of ontological relations between concepts: (I)
taxonomicaltyperelationships; (II) concept difference (i.e., negative type relationships); (III) facet relations derived from verb
frames in the input texts (e.g., has part, involves, or occurs in). Over
27,000 types of facet relations were extracted. We imposed a taxonomy on them, considering the head verb of the phrase as a more
generic relation (e.g., involves expression of was assumed to be a type
of involves). Also, several artificial relation types were introduced
to restrict the semantics of some of the most frequent relations: a
(positive) type was considered transitive and anti-symmetric, and
same as was set transitive and symmetric. Similarly, part of was
assumed transitive and being inverse of has part. Note that the
has part relation has rather general semantics within the extracted
knowledge, i.e., its meaning is not strictly physically mereological,
it can refer also to, e.g., conceptual parts or possession of entities.
The quads were processed as follows in the ACE pipeline (details

of the particular steps are described in [8]):

(I) AdditionThe extracted quads were incrementally added into
an growing knowledge base K, using a fuzzy aggregation of the
relevant conceptual matrices. To take into account the basic
domain semantics (i.e., synonymy relations and core taxonomy
of K), we used the EMTREE (http://www.embase.com/emtree/)
and NCI (http://nciterms.nci.nih.gov) thesauri.

(II) ClosureAfter the addition of new facts into K, we computed its
materialisation according to imported RDFS entailment rules
(cf. http://www.w3.org/TR/rdf-schema/).

(III) Extensionthe extracted concepts were analogically extended

using similar stored knowledge.

2 Implemented for English only in the current version. However, the EUREEKA
framework itself is language-agnosticit requires only input entities and their relations to be represented in an RDF-compatible format. Porting CORAAL to another
language is quite straightforward, given a suitable relation extraction pipeline.

We display the content of the knowledge base via a queryanswering module. Answers to queries are sorted according to
their relevance scores and similarity to the query [8]. Answers are
provided by an intersection of publication provenance sets corresponding to the respective statements subject and object terms.
The module currently supports queries in the following form: t | s :
(NOT )?p : o( AND s : (NOT )?p : o)
, where NOT and AND stands
for negation and conjunction, respectively (the ? and  wildcards
mean zero or one and zero or more occurrences of the preceding symbols, respectively, | stands for OR). s, o, p may be either a
variableanything starting with the ? character or even the ? character aloneor a lexical expression. t may be lexical expressions
only.


3. Elsevier grand challenge deployment

This section describes the deployment of CORAAL for the Elsevier grand challenge. Section 3.1 describes the data we processed,
while Section 3.2 illustrates the query answering capabilities of
CORAAL using the example outlined in the articles introduction.
Finally, Sections 3.3 and 3.4 report on the continuous tests with real
users and on the evaluation of the quality of the exposed knowl-
edge.

3.1. Data

Input: As of March 2009, we had processed 11,761 Elsevier journal articles from the provided XML repositories that
were related to cancer research and treatment. Access to the
articles was provided within the Elsevier Grand Challenge competition (cf. http://www.elseviergrandchallenge.com). The domain was
selected to conform to the expertise of our sample users and testers
from Masaryk Oncology Institute in Brno, Czech Republic. We processed cancer-related articles from a selection of Elsevier journals
focusing on oncology, genetics, pharmacology, biochemistry, general biology, cell research and clinical medicine. From the article
repository, we extracted the knowledge and publication metadata
for further processing by CORAAL. Besides the publications them-
selves, we employed extant machine-readable vocabularies for the
refinement and extension of the extracted knowledge (currently,
we use the NCI and EMTREE thesauri).

Output: CORAAL exposes two datasets as an output of the
publication processing: First, we populated a triple store with publication metadata (citations, their contexts, structural annotations,

Fig. 2. Knowledge-based query construction.

titles, authors and affiliations) and built auxiliary indices for each
metadata type to facilitate full-text querying of the stored content.
The resulting store contained 7,608,532 RDF subject-predicate-
object statements describing the input articles. This included
247,392 publication titles and 374,553 authors (extracted from
both processed articles and their literature reference lists).

Apart from the triple store, we employed a custom EUREEKA
knowledge base [8], containing facts of variable levels of certainty
extracted and inferred from the article texts and the imported life
science thesauri. Over 215,000 concepts were extracted from the
articles. Together with the data from the initial thesauri, the domain
lexicon contained 622,611 terms, referring to 347,613 unique con-
cepts. The size of the emergent knowledge base was 4,715,992
weighed statements (ca. 99 and 334 extracted and inferred statements per publication on average, respectively). The contextual
knowledge related to the statements, namely provenance infor-
mation, amounted to more than 10,000,000 additional statements
(when expressed in RDF triples). Query evaluation on the produced
content typically took fractions of seconds.

3.2. Asking queries, browsing answers

CORAAL can answer classical full-text or knowledge-based
queries using a simple yet powerful query language (details are
given in http://smile.deri.ie/projects/egc/quickstart). Answers in
CORAAL are presented as a list of query-conforming s tatements
(for the knowledge-based search) or resources (publication titles,
paragraphs or author names for the full-text search). The statement
results can be filtered based on their particular elements (e.g., sub-
jects, properties, and objects), associated contextual information
and whether the statement is positive or negative. The resource

Fig. 3. Query answer detail.

V. Novacek et al. / Web Semantics: Science, Services and Agents on the World Wide Web 8 (2010) 176181

results can be filtered according to the concepts associated with
them (both extracted and inferred) and additional metadata (e.g.,
authors or citations present in the context of the resulting para-
graphs). Using filtering (i.e., faceted browsing), one can quickly
focus on items of interest within the whole result set.

Recalling the example from Section 1, the query for sources
supporting that acute granulocytic leukemia is different from T-cell
leukemia can be conveniently constructed in CORAAL as depicted
in Fig. 2 (guided query building using a form-based interface).

The query builder includes a context-sensitive auto-completion
capability; if one rests the cursor on, e.g., a subject, only relations
(properties) actually associated with that subject in the knowledge
base are displayed.

Fig. 3 shows the highest ranked answer to the query constructed in Fig. 2, proving that the two types of leukemia are
not the same. The source article of the statement (displayed as
an inline summary in Fig. 3) is the desired reference supporting
the claim. The particular types of contextual information associated with statements (as can be observed in Fig. 3) are: (I)
source provenancearticles relevant to the statement, which can be
expanded into an inline summary (as shown in Fig. 3) or explored
in detail after clicking on the respective publication title; (II) context provenancedomain of life sciences that the statement relates
to (determined according to the main topic of the journal that
contained the articles the statement was extracted from); (III) certainty a number describing how certain the system is that the
statement holds and is relevant to the query (values between 0
and 1; derived from the absolute value of the respective statement degree and from the actual similarity of the statement to
the query); (IV) inferreda Boolean value determining whether
the statement was inferred or not (the latter indicating it was
directly extracted). More information can be seen with CORAAL at
http://coraal.deri.ie:8080/coraal.

3.3. Continuous tests with users

During the development of the CORAAL prototype, we continually collaborated with several biomedical experts, who formed
a committee of sample users and evaluators. Before the final
stages of the Elsevier Grand Challenge, we prepared five tasks
to be worked out with CORAAL and a baseline application (Sci-
enceDirect or PubMed). Our hypothesis was that users should
perform better with CORAAL than with the baseline, since the
tasks were focused on knowledge rather than on a plain text-based
search.3

Users indicated that the tasks used for evaluating CORAAL were
relevant to their day to day work by giving it a score of 3.9 out of
6 (the scale was from 1 to 6, with 1 indicating no relevance, and 6
indicating high relevance). The success rate of task accomplishment
was 60.7% with CORAAL and 10.7% with the baseline application.
This confirms our above-mentioned hypothesis that users will be
able to accomplish the given tasks better with CORAAL due to its
enhanced querying and search capabilities.

Besides

evaluating the users performance

in sample
knowledge-based search tasks, we interviewed them regarding the overall usability of the CORAAL interface. The most critical
issue was related to the query languagehalf of the users were not
always able to construct appropriate queries. However, CORAAL
also offers a form-based query builder that assists the user as

illustrated in Section 3.2. Using this feature, users performed up
to six times faster and 40% more efficiently than with purely
manually constructed queries.

The expert users also had problems with too general, obvi-
ous, or irrelevant results. These concerns were expressed when
the users were presented with a raw list of answer statements
within the evaluation. After being discussed with the users within
the evaluation interviews, the problems were addressed by the following features in the user interface: (i) relevance-based sorting of
concepts and statements [8]the most relevant statements were
displayed at the top of the results list; (ii) intuitive faceted browsing
functionalitysupport for fast and easy reduction of the displayed
results to a subset which reference particular entities (i.e., statements having only certain objects or authors writing about certain
topics). The solutions were considered as mostly sufficient regarding the users concerns (an average 4.  6 score on the 1  6 scale going
from least to most sufficient).

3.4. Knowledge quality evaluation

To evaluate the quality of the knowledge served by CORAAL,4 we
generated 200 random queries composed of anything from single
terms to a conjunction of multiple possibly negated statements. To
ensure non-empty answer sets, the queries were generated from
the actual content of the knowledge base. Also, we took into account
only the content extracted from the input articles and not from the
NCI or EMTREE seed thesauri. We let our domain expert committee
vote on the relevance of queries to their day-to-day work and used
the ten most relevant ones to evaluate the answers provided by
CORAAL.

We used the traditional notions of precision, recall, and F-
measure for the evaluation of the quality of the answers. A gold
standard set of statements relevant to the queries used in the evaluation was created by the user committee, who employed their
own knowledge combined with the full-text search of the publications incorporated in CORAAL. For a baseline comparison, we
imported the knowledge extracted by CORAAL from the input articles and thesauri into a state-of-the art RDF store. The store had
inference and querying support, however, it lacked proper means
for emergent knowledge processing (namely regarding the nega-
tion, uncertainty, inconsistence resolution and approximate query
processing features). The set of queries used for CORAAL evaluation
was executed using the baseline RDF store and the results were
compared. Due to the novel support of the emergent knowledge,
CORAAL quite substantially outperformed the baseline, achieving
F-measures from two- to eight-times better for the various evaluated features.

The absolute CORAAL results may still be considered rather
poor when compared to the gold standard generated by the users
(i.e., F-measures for some queries around 0.2). However, one must
recognise that the answers to the gold standard questions took
almost two working days for an expert committee to generate. In
about the same time, the CORAAL knowledge base was produced
purely automatically for much larger amounts of data (involving
statements about hundreds of thousands of concepts instead of a
few query entities). The queries take seconds to evaluate and one
can find many relevant answers very quickly due to the relevancebased sorting of the results (the first 10 statements contained more
than 67% of relevant answers on average, while the 200th to 400th
results contained only about 5% correct statements). The evaluation committee unequivocally considered the ability of CORAAL

3 For instance, the users were asked to find all authors who support the fact that
the acute granulocytic leukemia and T-cell leukemia concepts are disjoint,
or to find which process is used as a complementary method, while being different from the polymerase chain reaction, and identify publications that support
their findings.

4 Note that this section provides only an outline of the actual evaluation, summarising the most important points. A full description of the knowledge quality
evaluation and the numeric results achieved is provided in [8].

to perform purely automatically as an acceptable trade-off for the
detected noise in the results.

4. Related work

Approaches tackling problems related to those addressed by the
core technologies powering CORAAL are analysed in [8,4]. Here, we
offer an overview of systems targeting similar problems to those
tackled by our framework.

State-of-the-art applications like ScienceDirect or PubMed Central require almost no effort in order to expose arbitrary life science
publications for search (therefore we used them as a baseline in
the user-centric experiment). However, the benefit they provide is
rather limited when compared to cutting-edge approaches aimed
at utilising also the publication knowledge within the query construction and/or result visualisation. Such innovative solutions may
require much more a priori effort in order to work properly, though.
FindUR [6], Melisa [1] and GoPubMed [3] are ontology-based
interfaces to a traditional publication full-text search. GoPubMed
allows for effective restriction and intelligent visualisation of the
query results. FindUR and Melisa support focusing the queries on
particular topics based on an ontology (FindUR uses a Description
Logic ontology built from scratch, while Melisa employs a custom ontology based on MeSH, cf. http://www.nlm.nih.gov/mesh/).
GoPubMed dynamically extracts parts of the Gene Ontology (cf.
http://www.geneontology.org/) relevant to the query, which are
then used for restriction and a sophisticated visualisation of the
classical PubMed search results. Nevertheless, none of the tools
mentioned so far offers querying for or browsing of arbitrary publication knowledge. Terms and relations not present in the systems
rather static ontologies simply cannot be reflected in the search.
On the other hand, CORAAL works on any domain and extracts
arbitrary knowledge from publications automatically, although the
offered benefits may not be that high due to a possibly higher level
of noise.

Textpresso [7] is quite similar to CORAAL concerning searching
for relations between concepts in particular chunks of text. How-
ever, the underlying ontologies and their instance sets have to be
provided manually, whereas CORAAL can operate with or without any available ontology. Moreover, CORAAL includes far more
full-text publications and concepts.

The biggest challenge of systems with goals similar to CORAAL
is a reliable automation of truly expressive content extraction. In
contrast to CORAAL, none of the related systems addresses this
problem appropriately, which makes them scale poorly, or makes
them difficult to port to new domains. This is why we were not
able to use the related systems for a baseline comparison in our
domain-specific application scenariowe simply could not adapt
them so that they would be able to perform reasonably, both due
to technical difficulties and lack of necessary resources.

5. Conclusions and future work

With CORAAL, we have addressed most of the elements of a
truly knowledge-based scientific publication search as specified in
Section 1. We are able to extract and integrate emergent knowledge and metadata from a large number of publications, as well
as augment and refine the extracted content. CORAAL also allows
for intuitive searching and browsing of the processed knowledge.

Although the primary focus of CORAAL is the knowledge-based
search, the underlying technologies are straightforwardly applicable to many other tasks. These are for instance automated tagging
of articles by the associated general concepts, population of existing domain-specific vocabularies, or utilisation of CORAAL as a
general-purpose knowledge back-end exposing arbitrary services
(e.g., knowledge-based retrieval of similar articles or profile-based
article recommendation).

However, we still have to tackle several challenges in order
to fully realize the current potential of CORAAL. First, we want
to utilise the wisdom of the crowds by supporting intuitive and
unobtrusive community-based curation of the emergent knowl-
edge, namely by validation or invalidation of existing statements,
introduction of new statements and submission of new rules refining the domain semantics. Then we intend to make the step from
CORAAL to a CORAAL reef, a distributed peer-to-peer model covering multiple CORAAL installations autonomously communicating
with each other (e.g., asking for answers when no answer is available locally or exchanging appropriate rules to improve the local
semantics). After incorporating the capabilities of the prospective
CORAAL reefs into the ecosystem of the current online publish-
ing, we will be able to unlock, connect, augment and retrieve the
knowledge with unprecedented scale and efficiency.

Acknowledgments

This work has been supported by the Lion, Lion II projects
funded by SFI under Grants No. SFI/02/CE1/I131, SFI/08/CE/I1380,
respectively. Big thanks goes to our evaluators: Doug Foxvog, Peter
Grell, MD, Milos Holanek, MD, Matthias Samwald, Holger Stenzhorn and Jiri Vyskocil, MD. We also appreciated the challenge
judges feedback that helped to streamline the final prototype a
lot. We acknowledge the support provided by Noelle Gracy, Anita
de Waard and other Elsevier people regarding the challenge organ-
isation. Finally, we thank to the anonymous reviewers and to Ed
Hovy and Susie Stephens, the special issue editors, who all helped
us to improve the final article version a lot.
