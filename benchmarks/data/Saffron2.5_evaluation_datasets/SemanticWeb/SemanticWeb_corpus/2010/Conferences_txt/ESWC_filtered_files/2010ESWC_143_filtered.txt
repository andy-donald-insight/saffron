Acquiring Thesauri from Wikis by Exploiting Domain

Models and Lexical Substitution

Claudio Giuliano1, Alfio Massimiliano Gliozzo2,

Aldo Gangemi3, and Kateryna Tymoshenko4

1,4 FBK, Via Sommarive 18, Trento, Italy

2,3 STLab-CNR, Via Nomentana 56, 00161 Rome (RM) Italy
giuliano@fbk.eu, alfio.gliozzo@istc.cnr.it,
aldo.gangemi@istc.cnr.it, tymoshenko@fbk.eu

Abstract. Acquiring structured data from wikis is a problem of increasing interest in knowledge engineering and Semantic Web. In fact, collaboratively developed resources are growing in time, have high quality and are constantly updated.
Among these problems, an area of interest is extracting thesauri from wikis. A
thesaurus is a resource that lists words grouped together according to similarity of meaning, generally organized into sets of synonyms. Thesauri are useful
for a large variety of applications, including information retrieval and knowledge
engineering. Most information in wikis is expressed by means of natural language texts and internal links among Web pages, the so-called wikilinks. In this
paper, an innovative method for inducing thesauri from Wikipedia is presented.
It leverages on the Wikipedia structure to extract concepts and terms denoting
them, obtaining a thesaurus that can be profitably used into applications. This
method boosts sensibly precision and recall if applied to re-rank a state-of-the-art
baseline approach. Finally, we discuss how to represent the extracted results in
RDF/OWL, with respect to existing good practices.

1 Introduction

Acquiring structured knowledge from unstructured data is a crucial issue in the field of
semantic technologies. Lexical knowledge plays a crucial role for applications dealing
with natural language. Such knowledge, as it is usually processed by linguists, is typically represented following a structural paradigm in which concepts are represented by
a set of synonymous terms. When one term refers to two different concepts, the term is
ambiguous, while the fact that the same concept is expressed by two or more terms is
called variability. Most computational lexicons are structured in this way, as variability
and ambiguity are the more pervasive phenomena in language.

While defining concepts as sets of (nearly-) synonymous terms is a clear and pragmatic assumption among computational linguists, this is far less clear and pragmatic
among domain experts, and even less among ontology engineers. However, the
usefulness of lexicons like WordNet for ontology engineering techniques (ontology
design, automatic mapping of ontologies) has eventually made that assumption ac-
ceptable, when revised appropriately. We will see how to reconcile the semantics of
the results obtained through that assumption, with the formal semantics expected for
ontologies.

L. Aroyo et al. (Eds.): ESWC 2010, Part II, LNCS 6089, pp. 121135, 2010.
c Springer-Verlag Berlin Heidelberg 2010

C. Giuliano et al.

Typically, the main weakness of computational lexicons is their lack of coverage
when compared to actual usage of language. Lexical resources can be crafted by hand,
however they typically suffer of poor coverage as all possible variants are typically not
found by lexicographers. Corpus-based approaches can be used to enhance coverage,
for example, to extract domain specific terminology or to find semantic relations among
words. However, state-of-the-art approaches are not accurate enough, so domain experts
are frequently involved in the process to provide additional validation.

The process of inducing terminology and paradigmatic relations among terms (e.g.,
synonymy and hyponymy) has been called thesaurus induction [6]. To this end, patternbased approaches have been used [15], as well as distributional measures of lexical
similarity based on word co-occurrence in documents [8] or in syntactic contexts [7].
Even though these approaches are scalable and cost-effective, their main limitation is
that they can be used to discover relations among words, while semantic relations are
defined between concepts. The term thesaurus induction is again a bit misleading when
compared to what is known as a thesaurus in the Semantic Web literature. What computational linguists call thesauri are in fact a coarse approximation of mainstream thesauri
known on the Semantic Web for the ability to be reengineered, e.g., through the SKOS
vocabulary1. A SKOS-compliant thesaurus is similar to linguists thesauri in the sense
that it is made of concepts, which have preferred and alternative labels. But realistic
thesauri (that SKOS describes) are actually richer than that, and include a whole range
of semantic relations between concepts: broader, related, mapping, etc. The results of
(near-) synonymy, that thesaurus induction techniques show, should actually be matched
against the richer model of thesauri. However, that classification detail cannot be easily
achieved with purely statistical techniques, as precision hardly exceeds 40% with the
best model.

In order to make thoroughly clear what are the potential benefits that might derive
from bootstrapping thesauri, and formalizing them, we propose a novel technique to
automatically acquiring thesauri from the analysis of the structure and content of wikis.
Our work is contextualized in the area of research aimed at extracting knowledge bases
from Wikipedia (see for example the DBPedia [1] and YAGO [21] projects).

In contrast to the previous corpus-based approaches, our technique is able to identify
senses for each term, and then to extract (near-) synonyms specifically for each different
sense. Another major limitation of previously proposed approaches is their limited lexical coverage. In fact, to measure the distributional similarity between two words, it is
necessary to collect multiple occurrences of those words in a given corpus to define reasonably good context representation. This is particularly problematic when dealing with
domain specific terms, whose occurrences are somewhat rare. In contrast, our method is
scalable and ensures high coverage, as it relies on two large-scale resources: Wikipedia
and the Web 1T 5-gram corpus [2].

Our technique is based on two fundamental linguistic properties characterizing
paradigmatic relations between words: the domain restriction hypothesis and the lexical substitutability. The former states that paradigmatic relations (e.g., synonymy and
hyperonymy) holds mainly among terms belonging to the same domain, where domains
represent regions in the language characterized by topical similarity and, in our case,

1 http://www.w3.org/TR/skos-reference/skos.rdf
?

?

?
modeled by latent semantic analysis (LSA). The latter claims that terms are synonyms
if they can be substituted in a particular context preserving the original meaning. Relying on these assumptions, we adapted the lexical substitution technique proposed by
[11] to the problem of finding synonymous terms for terms in context. For example, the
term book in the sentence All Gold Canyon is a book written by Jack London can
be substituted with novel preserving the original meaning. It means that book, in this
particular sense, is a (near-) synonym of novel.

Another innovation of our approach is that we enhance the thesaurus induction process by relying on the existing conceptual structure provided by Wikipedia, assuming
that each article identifies a different concept that can be expressed by one or more
terms. Our goal is then to discover sets of (near-) synonymous terms describing that con-
cept. To this end, we extract the set of candidate (near-) synonyms from the Wikipedia
article and its incoming wiki links. Then, we choose all those terms belonging to the
domain of the article by applying LSA similarity. Finally, we rank each term by applying the lexical substitution technique. The result is a set of synonyms that can be used
as a thesaurus entry for the concept.

We apply this method to detect all possible synonyms for subset of the most frequently visited articles in Wikipedia, and we evaluated our approach by comparing the
results with respect to the corresponding synonym lists provided by the Oxford American Writers Thesaurus, after an handcrafted mapping of these articles to the corresponding dictionary senses. Experiments show that our method is accurate and improves the
baseline approach.

The rest of the paper is structured as follows. In Section 2, we identify the analogies
between the structures of wikis and thesauri. Section 3 describes the two basic building blocks of our algorithm for detecting paradigmatic relations: semantic domains and
lexical substitutability. Section 4 reports the experimental results, Section 5 describes a
reengineering pattern to formalize linguistic thesauri in OWL-RDF, Section 6 presents
related work, and Section 7 concludes the paper suggesting some interesting applications of our method that we are going to explore in the future.

2 Deriving Conceptual Structures from Wikis

The wiki technology is changing the Web, leading to the success of Web 2.0 applica-
tions, such as Wikipedia. Wikis are currently adopted in many applications, including
corporate knowledge management systems, collaborative development of lexical and
encyclopedic resources, social networks, e-learning systems, and so on. The attractive
feature of wikis is that their users are implicitly asked to collaboratively create a shared
conceptual space, where each concept is described by a Web page mostly composed by
highly focused natural language text concerning that concept. When a new concept of
interest for the domain of the wiki is identified, the user creates an empty page where
other users or her/himself can explain it in detail. On the other hand, when the concept
of interest is already in the wiki, the user can link to it. The anchor text of the link may
be a synonym or an equivalent expression of the term adopted to name the target page
describing the concept.

C. Giuliano et al.

Therefore, the wiki can be used as a basis to derive a network of relations between
concepts (the Web pages) and terms (anchor texts of the internal links present in other
wiki pages). More formally, we can describe the conceptual structure derived by a wiki
as follows. Let C be the set of concepts, and T be the vocabulary of terms. From each
wiki page we derive a concept, while terms, which are anchor texts of links to the
page, become elements of T . Each wikilink describes a relation M(t, c) between a
term and a concept. For a given term ti, the set of its possible senses is S(ti) = {cj 
C|M(ti, cj)}. For a given concept ci, the set of its possible lexicalizations is returned
by L(ci) = {tj  M(tj, ci)}. The number of links connecting a given term to a given
concept can be used to infer a prior probability for the term ti having sense cj  S(ti),
by adopting the Formula 1.

P (cj|ti) =

|M(ti, cj)|

|M(ti, cz)|cz  S(ti)|

(1)

The structure described above is a thesaurus, as synonyms are represented and connected to concepts. In fact, [19] and [16] followed this approach to extract a thesaurus
out of the wikilink structure.

On the other hand, wikis are collaboratively developed resources, most of the time
composed very quickly by non-professional users. As a consequence, errors and bad
practices are very frequent. For example, the article describing the concept book is
linked to by the following terms: monograph, manual, work, novel, booklet, father,
guide, literary work, textbook, folio, books, treatise, print, and bookshop, book. But, for
example, only manual, novel, and treatise are listed as synonyms in the Oxford American Writers Thesaurus. The list obtained from wiki structure supplies new synonyms
like textbook, but it introduces obvious erros like father as well.

Therefore, the use of the Wikipedia structure is not a viable solution for inducing
high quality thesauri, even if it provides a rich starting point. In particular, we identified
the following limitations.

 Ttypographical mistakes

in the

anchor

texts. E.g., politik

linked to

wiki/Politics.

 Wrong links assigned to terms. E.g. terms Article and Money were linked by some-

one to the wiki page wiki/Philosophy.

 Bad selection of the term to link to the wiki page. E.g., where prohibited was made

an anchor text for wiki/Book.

 Coreferences linked to concepts instead of terms. E.g., this guy linked to wiki/

Pope.

 Terms having different syntactic roles are actually referred to the same page. For

example, political is an anchor for the page wiki/Politics.

 Many possible lexical variants of concepts are missing, as users typically do a
search before deciding whether to create a new wiki page or linking to an existing one. E.g., herb is missing among the lexical variants of the concept Plant.

In the following sections, we propose a method relying on semantic domains and lexical
substitution to solve all the above problems.
?

?

?
Table 1. Ranked lists of possible synonymous terms for the concept book extracted adopting
different criteria (from top to bottom): Wiki links, ranked by frequency (baseline), nouns in the
article text, ranked by frequency, union of wikilinks and terms ranked by LSA, and by the combined method exploiting lexical substitution and LSA

Method (Near-) Synonyms

folio, books, treatise, print, bookshop, book 

Wiki book, monograph, manual, work, novel, booklet, father, guide, literary work, textbook,
Links
Page ANSI, Abaca, Ages, . . . , E-book, book, bookbinder, . . . , novel, novels, number, numbers,
Terms . . . , purposes, queen, quo, quotation, . . . , stem, stems, stock, stories, story, . . . , values,

varies,vellum, vergilius, version, vine, vines, virgil, vitriol, vivarium, volume, word,
. . . , year, young, yun-ui

LSA Book, book, Books, books, story, Reading, reading, Publication, publication, reader,

Author, author, stories, readers, Publishers, publishers, Novel, novel, reprint, work, . . .

Lexical book, story, magazine, novel, publication, chapter, writing, new, people, print,
Subst. work, literature, reading, time, author, young, history, edition, collection, character,
link, page, place, reference, english, press, review, note, account, kind, publisher, . . .

3 Discovering Synonymy

We aim at capturing two characteristic properties of paradigmatic relations for synonymy detection:

 Domain properties: if a semantic relation between two terms X and Y holds, both

X and Y tend to belong to the same semantic domain [14].

 Lexical substitutability: if two terms X and Y are paradigmatically related, they

can be mutually substituted in text preserving the meaning.

In the following two subsection we describe each of them and we show their exploitation for thesauri induction.

3.1 The Domain Restriction Hypothesis

Semantic domains are at the basis of lexical semantics. Domain properties, also known
as domain restriction hypothesis, are illustrated in Figure 1, where the probability for
two terms to be related in WordNet [9] by a paradigmatic relation is contrasted to their
domain similarity, measured by computing the cosine similarity between their corresponding vectors in the LSA space.

The main advantage of adopting semantic domains for thesauri extraction is that they
allow us to impose a domain restriction on the set of candidate pairs of related terms.
In fact, semantic relations can be established mainly among terms in the same semantic
domain, while terms belonging to different fields are mostly unrelated.

Domain similarity between terms can be captured by measuring the LSA similarity
between them. For our purposes, we defined a domain model by unsupervised learning
on the Wikipedia text, following the procedure described in [13]. ci and tj are defined
in the LSA space, estimated by the Singular Valued Decomposition for the term by

C. Giuliano et al.

Co-hyponyms
Hyperonym
Synonymy
Domains

 0.5

 0.4

 0.3

 0.2

 0.1

y
t
i
l
i

b
a
b
o
r

 0.2

 0.4
 0.6
Domain Similarity

 0.8

Fig. 1. Probability of finding paradigmatic relations in WordNet contrasted to domain similarity
estimated in the British National Corpus [14]

document matrix extracted from the subset of the 500,000 most visited pages of the
English Wikipedia. As a result, the similarity between a concept ci, described by the
corresponding Wikipedia article, and a term tj, can be estimated by cos(ci, tj)2.

Domain similarity is then used to identify sets of candidate terms to fill the thesaurus
entry representing each concept c. We first collect a set of the candidate lexicalizations
L(c)  T for a target article (concept) c by merging the results of the following two
strategies:
1. Selecting all nouns extracted from the analysis of the text in the Wikipedia article
associated to c. For example the second row of Table 1 shows the list of terms
obtained after the analysis of the article corresponding to the concept book.

2. Selecting words used to link the target page from other articles, i.e. the lexicalizations L(c) extracted by the wiki structure described in Section 2. For example, the
first row of Table 1 shows the list of terms obtained from the wikilinks pointing to
the article book. We adopted this strategy to define our baseline.
Then, we ranked all terms in the set ti  L(c) according to their LSA similarity with
respect to the target concept cos(c, tj) , and we filtered out all those terms having
similarity below a threshold, that we fixed to 0.5 in all experiments described in this
paper. Using such threshold we were able to filter out up to 80% of the terms in the
set. This step is crucial to boost the efficiency of the synonymy induction step, as the
final ranking, based on lexical substitution, is computationally intensive and constitute
a bottleneck for the overall efficiency of our method.

3.2 Finding Synonyms by Lexical Substitution

As a second step, the candidate terms having high domain similarity with respect to the
target concept are re-ranked by lexical substitution. The lexical substitution is a textual

2 The SVD process exploited 400 dimensions at unsupervised learning time and took less than

2h on a dual core processor exploiting 4GB of RAM.
?

?

?
entailment subtask in which the system is asked to provide one or more terms that
can be substituted to w in a particular context Hw = H lwH r, generating a sentence
He = H leH r such that both Hw  He and He  Hw hold, where H l and H r denote
the left and the right context of w, respectively.

[17] recently proposed this task in the context of the SemEval-2007 evaluation cam-
paign. The state-of-the-art system is quite accurate and totally unsupervised [12], as it
only exploits an existing dictionary and a huge language model. For each target word,
a set of possible candidate words is extracted by the dictionary, and then substituted to
the target word generating new sentences. Then, the plausibility of each sentence is
measured by looking for the frequency of the generated sentences in the Web 1T 5-gram
corpus, and the candidate words are then ranked according to that plausibility score.

In order to apply this technique to our problem we cannot rely on the availability of
existing dictionaries containing candidate terms, as our goal is to discover such terms.
In addition, we are not interested in substituting terms into a single occurrence of a word
in context, but we are rather interested in the combined results coming from the substitution of candidate words into a large set of occurrences, i.e., all wiki links pointing to
a particular article.

For a given concept c and a set of its candidate lexicalizations L(c), our substitution

algorithm works as follows:
Step 1. For each concept c, we collect all the sentences S containing a wiki link i
pointing to the Wikipedia article associated to c. For instance, a sentence containing a
link to the concept book is The most printed book in history
Step 2. Then, for each sentence s  S, we derive a hypothesis phrase by replacing the
link i with a candidate synonym j  L(c). For instance, from the previous example, we
derive The most printed novel in history.
Step 3. For each hypothesis phrase hj, we calculate a plausibility score vj using a
variant of the scoring procedure defined in [11]. In our case, vj is given by the sum of
the pointwise mutual information (PMI) of all the n-grams (1 < n  5) that contain
j divided by the self-information (SI) of the right and left contexts. PMI and SI are
defined by the Equations 2 and 3, respectively.

P M I = log

p(t1, t2 . . . tl)

p(t1)p(t2) . . . p(tl)

= vj

(2)

SI =  log p(t1, t2 . . . tl) = vj

(3)
Dividing by the self-information allows us to penalize the hypotheses that have contexts
with a low information content, such as sequences of stop words. The frequency of the
n-grams is estimated from the Web 1T 5-gram corpus. For instance, from the hypothesis
phrase The most printed novel in history, we generate and score 10 n-grams, some of
them are: The most printed novel in, printed novel in history, novel in history
Step 4. Finally, to obtain an overall score vt for the term t, we sum the scores obtained
by the substitution in all sentences, as defined in Equation 4.

N

where N is the number of sentences containing a wiki link to the target article c.

vt =

vl,

l=1

(4)

C. Giuliano et al.

We used the this procedure to rank all candidate terms selected in the domain restriction phase. The top ranked terms for each concept provide a different thesaurus entry,
associated to the corresponding Wikipedia article.

4 Evaluation

For the evaluation, we selected the target concepts among the most visited articles in
Wikipedia, and we automatically extracted thesaurus entries for each of them by contrasting our method with a baseline, provided by a replication of the method described
in [19].

The list of concepts evaluated so far is the following: Sex, Pornography, Love, Poli-
tics, Book, Earth, Map, Computer, Cat, Game, Dictionary, Ejaculation, Lesbian, Sport,
Cancer, God, Virus, Animal, Heroin, Horse, Film, Human, Condom, Snake, Flower,
Evolution, Beer, Statistics, Religion, Communication, Management, Insomnia, Erection,
Death, Earthquake, People, Insurance, Graffiti, Research, Puberty, Cannabis, Marriage,
Gun, Socialism, Narcissism, Twilight, Sleep, Architecture, Stroke, Leaf.

4.1 Gold Standard

We handcrafted a gold standard thesaurus entry by mapping each concept into the appropriate sense of the Oxford American Writers Thesaurus. For simplicity, we did not
consider multi-words and parts of speech other than nouns. Below, we report a sample
of the gold standard: the first word is the name of the Wikipedia article, and the following words are its synonyms extracted from the appropriate sense in the dictionary.

book hardback, storybook, volume, treatise, publication, e-book, manual, novel, title,

tome, paperback, anthology

earth planet, world, globe
computer laptop, terminal, mainframe, pc, desktop
dictionary wordbook, lexicon, glossary, thesaurus
cancer lymphoma, sarcoma, melanoma, malignancy, myeloma, tumor, carcinoma
virus contagion, disease, bug, infection

The output of our thesaurus induction method is a ranked list of terms for each concept,
that we compared with the gold standard, reporting precision and recall figures.

4.2 Baseline

As a baseline method, we implemented the heuristic described in Section 2. We extracted all terms contained by wikilinks pointing to each article, and we ranked them
by frequency (i.e., the number of times they have been used to link to the target arti-
cle). For each term, we computed the precision at different levels of recall, following
an information retrieval paradigm.

The micro-average figures, reported in Figure 2, show that this method is not very
accurate. In fact, the maximum precision is close to 0.2 at very low recall. The breakeven point is 0.18.
?

?

?
 0.35

 0.3

 0.25

 0.2

 0.15

 0.1

 0.05

Baseline
System

 0.05

 0.1

 0.15

 0.2
Recall

 0.25

 0.3

 0.35

i

i

n
o
s
c
e
r

Fig. 2. Precision/Recall curves for the baseline and our system

4.3 Advanced Methods

We built the domain model from the 200,000 most visited Wikipedia articles. After
removing terms that occur less than 5 times, the resulting dictionaries contain about
300,000 terms. We used the SVDLIBC package3 to compute the singular value decom-
position, truncated to 100 dimensions. All the experiments were performed using the
LIBSVM package [5].

Then, for each article corresponding to a concept in the gold standard, we adopted the
bag of word vector described above to provide a list of candidate terms, that we merge
to the set of words returned by the baseline method. Then, we filtered out from this set
all those terms having a cosine similarity with the target articles vector below 0.5. This
threshold allows us to drastically reduce the number of candidates but preserving with
high probability the (near-) synonyms. The third row of Table 1 exemplifies the list of
terms obtained so far for the concept book.

Then we collected all sentences containing wiki links to the target article, and we
generated new sentences by substituting all candidate terms collected above. Finally,
we computed a plausibility score for each of those sentences, on the basis of which we
derived the final score for each candidate synonymous term by adopting Formula 4. The
resulting sets are then ranked by applying the lexical substitution algorithm described
in Section 3.2.

Results are reported in Figure 2, we compare the two methods by means of the pre-
cision/recall curves. The lexical substitution method combined with the domain restriction hypothesis significantly boosts the quality of the baseline..

4.4 Qualitative Evaluation

From a qualitative analysis of the results, it seems that most terms that do not match
the gold standard entry are not actually weird errors, but rather they follow under the
definition of (near-) synomymy. For example, looking at the fourth row of Table 1 and

3 http://tedlab.mit.edu/ dr/svdlibc/

C. Giuliano et al.

comparying it to the gold standard reported in Section 4.1, many high quality terms
such as print, monograph, booklet, folio, guide, textbook, treatise, do not match the
gold standard. However, they include hyperonymy, hyponymy and co-hyponymy terms.
Such terms are very useful in developing computational thesauri, especially as far as applications like semantic information retrieval are the final goal. For example, the terms
acquired so far can be used to improve search in Wikipedia itself, or to expand the
lexical part of DBpedia, bridging knowledge to language in a much smarter way.

5 RDF-izing Linguistically-Induced Thesauri

A linguistically-induced thesaurus is currently understood as a poor thesaurus, which
only contains concepts and (preferred or alternate) labels. The linguistic semantics, by
which a concept is equivalent to a set of near-synonymous terms can be represented in
different ways, and we show here three possible solutions: one based on SKOS, one on
WordNet OWL, and a hybrid one.

However, before exemplifying the solutions, we should note that the assumption of
near-synonymy between the terms (found as statistically meaningful), associated with a
concept (i.e. the entities Wikipedia pages are about) can be counter-intuitive for lexical
semantics.

Near-synonymy is expected to catch a special kind of similarity between terms, which
makes them substitutable in a certain class of contexts, modulo certain local constraints.
For example, book can be substituted with volume in many generic contexts (referring
to physical objects), or with script in more specialistic contexts (drama playing), but a
lexicographer would hardly make it substitutable with magazine or writing: the last two
would be considered similar in a different sense (i.e. hyponymy, overlap, or association).
The notion of substitutability assumed in thesaurus induction is broader than the one
assumed in lexical semantics, and the representations we present have to be adjusted
to this broader notion, which also covers what lexicographers would classify as e.g.
hyponymy or association.

The first candidate representation consists in using the relation between a skos:
Concept and its skos:label(s): this representation basically uses literals as values
for the skos:label property, and therefore has problems when we need to provide attributes to the terms. In a new extension of SKOS4, a label is represented as an individual from the class skosxl:Label, with which literal values can be associated. This
gives also room for representing a large set of data about preferred or alternative labels:
creators, dates, provenance, etc.
For example, the relation between: Book and {book, novel, magazine, textbook, pub-
lication, writing} can be represented as a set of RDF triples:

:Book a skos:Concept ; skosxl:prefLabel :book ;
skosxl:altLabel :novel ; skosxl:altLabel :magazine ;
skosxl:altLabel :textbook ; skosxl:altLabel :writing ;
skosxl:altLabel :publication

with :book a skosxl:Label, etc.

4 http://www.w3.org/TR/skos-reference/skos-xl.rdf
?

?

?
 0.35

 0.3

 0.25

 0.2

 0.15

 0.1

 0.05

Baseline
System

 0.05

 0.1

 0.15

 0.2
Recall

 0.25

 0.3

 0.35

i

i

n
o
s
c
e
r

Fig. 3. Precision/Recall curves for the baseline and our system used to re-rank the baseline

With this representation, the semantics of thesaurus induction (concepts against relevant associated terms) is represented, since skos:Concept and skosxl:Label
are disjoint classes, but it is confusing for a user to find labels of e.g. the concept
Book that include novel, magazine, etc., since most people is accustomed to the lexicographic near-synonymy assumption. The intuition for an English speaker would
possibly be that the concept Book has more specific senses like novel and textbook,
more generic ones like publication, overlapping terms like magazine, and related senses
like writing. The most correct representation would be therefore to use the generic
skos:semanticRelation between concepts, which can be eventually specialized
as broader, related, etc. after automatic or manual evolution. In this case, either concepts
or terms from thesaurus induction should be classified as skos:Concept(s).

However, in this way we are not getting much to the point, since in order to preserve

the lexicographic intuition, we lose the original thesaurus induction semantics.

An alternative to using SKOS is to reuse a meta-model that is closer to computational linguists intuition, i.e. the WordNet OWL vocabulary5. In that case, the class
wordnet:Synset has the intended meaning of a set of near-synonymous word senses,
which are linked through the wordnet:containsWordSense relation. A
wordnet:WordSense is on its turn linked to words through the wordnet:word
relation to wordnet:Word. This representation seems better, since it allows to representing terms either as word senses or words, and still retains the possibility to link them
to synsets (concepts). However, even here the exceptions to intuitive near-synonymy
mentioned above are still valid: some relations look closer to wordnet:hyponymOf,
others to wordnet:hypernymOf, others do not have any correspondence at all. The
relation wordnet:containsWordSense has a meaning that conflicts with e.g.
wordnet:hyponymOf, and in WordNet no relation represents e.g. the one between
Book and writing.

5 http://www.w3.org/2001/sw/BestPractices/WNET/tf.html

C. Giuliano et al.

A third alternative consists in building an ad-hoc vocabulary for thesaurus induction
semantics. This solution can be implemented by merging the positive aspects of both
SKOS and OWL-WordNet: concepts are represented as wordnet:Synsets, terms
as wordnet:Words mapped to skosxl:Label(s), while new individuals for each
word sense of a word can be created, and linked to their synset
through the
skos:semanticRelation relation, for example:

:Book a wordnet:Synset ;
skos:semanticRelation :wordsense-book ;
skos:semanticRelation :wordsense-novel ;
skos:semanticRelation :wordsense-magazine ;
skos:semanticRelation :wordsense-textbook ;
skos:semanticRelation :wordsense-publication ;
skos:semanticRelation :wordsense-writing

with :wordsense-novel a wordnet:WordSense ; and wordnet:word:
novel which is both a wordnet:Word, a skosxl:Label, etc.

The third hybrid representation can also be implemented by designing a vocabulary
that catches exactly the thesaurus induction semantics, and then by mapping the vocabulary to SKOS and OWL-WordNet.

6 Related Work on Thesaurus Construction

Among the approaches to semantic relatedness evaluation and thesauri induction there
are the pattern-based approaches [15], as well as distributional measures of lexical similarity based on word co-occurence in documents [8] or in syntactic contexts [7]. In [22]
a thesaurus is generated by usage of a syntactically constrained Vector Space Model
(VSM). In this approach each dependency from a set of predefined syntactic dependencies is represented by different subsets of VSM. Sets of words related to a given source
word are found distinctly in each of the VSM and then overlapped.

Many of the latest approaches to the task are based on usage of Wikipedia as the
source of background knowledge. Among the first were [20], who exploited the
Wikipedia pages texts and category structure to calculate text overlap, taxonomy and
information based relatedness measures. Given two words i and j they retrieved the
corresponding Wikipedia pages (or articles), to content of which text overlap similarity based measures could be applied. The pages were retrieved just by querying a wiki
page named i or j. In the case when one or both retrieved pages are disambiguation
pages, disambiguation heuristic was applied. Exploiting sets of categories Ci and Cj,
to which words belong, [20] obtained least common subsumer for each category pair to
be able to calculate information based measures. Finally, sets of paths between the Ci
and Cj pairs gave input to taxonomy similarity measures. The correlation with human
relatedness evaluation is 0.19-0.48.

[10] proposed Explicit Semantic Analysis (ESA) for measuring semantic relatedness of both words and texts. They build the semantic interpreter, which maps input text/words into a weighted vector of relevant Wikipedia concepts. The relevant
?

?

?
Wikipedia concepts are titles of Wikipedia pages, which contain the words/texts. The
semantic relatedness of two texts/words is computed then applying cosine metric to
corresponding weighted concept vectors.The correlation with human relatedness evaluation is 0.75.

[18] measure semantic relatedness between the concepts using Wikipedia link struc-
ture. Given two terms, they obtain all wikipedia articles to which these terms may refer
and compute semantic relatedness between all possible pairs of articles combining two
relatedness measures. First one is similar to tfidf , but uses wikipedia links and pages instead of terms and documents. The second is based on the Normalized Google Distance,
which takes into account term co-occurences of terms on the web-pages, where links
and wikipedia articles are used instead of terms and web-pages correspondingly. The
terms relatedness is then evaluated as a combination of the highest relatedness achieved
among pairs of articles with term co-occurence frequency in wikipedia anchor texts.
The correlation with human relatedness evaluation for words is 0.78.

[19] and [16] propose to use Wikipedia directly as a building material for an association thesaurus. Therefore, they do not encounter the disambiguation problem. In [19],
the thesaurus is constructed using pfibf (path frequency inversed backward frequency),
which calculates the relativity between two concepts I an J represented by two articles Vi and Vj. pfibf exploits information about the number of paths from Vi to Vj, the
lengths of these paths and the number of backward links to articles. The latter is used to
penalize popular articles, which are highly related to a big number of articles. The concept precision among 10-30 top terms varies within limits 66.7-85.9. In the later work
([16]) they propose a much faster associative thesaurus construction method based on
link co-occurrence analysis. The links are considered to be concepts, two links are said
to be the same even if they have different anchor texts. In order to calculate relatedness
between two links each link is represented as a combined vector (cv). Cv is a linear
combination of a link and tfidf vector. Link vector is a vector, whose positions are firstorder co-occurrences of the given link with other links in the Wikipedia. The first-order
co-occurrence of two links is computed using the co-occurrence frequencies of these
links with all other links over Wikipedia. Accuracy measure is 0.59-0.69.

In addition, different techniques for acquiring terms, synonyms and semantic rela-

tions have been presented in the field of ontology learning [4,3].

7 Conclusion and Future Work

In this paper, we presented an innovative method for inducing thesauri from Wikipedia.
Our method leverage on the Wikipedia structure to find out target concepts for the
thesaurus, which is in itself an innovation if compared to traditional method relying on
words only. In addition to that, we applied an innovative method for detecting possible
lexicalizations of those concepts, showing that both precision and recall, in this settings
corresponding to the quality of the thesaurus entry and to its lexical coverage, can be
significantly boosted. A qualitative assessment of the results shows that our method
is actually able to find (near-) synonymous terms that can be used to automatically
populate thesaurus entries if the strict requirement of synonymy is relaxed.

For the future, we are going to perform an extensive analysis of the full Wikipedia
aimed at extracting lexical expressions for each concept represented by a Wikipedia

C. Giuliano et al.

page. In fact, our method is largely scalable: with the present (prototypical) implementation we where able to process each page in few seconds by using a standard PC.
In addition, we are going to extract thesauri for many languages in Wikipedia, as our
method is totally language independent and only requires part of speech tagging of
texts.

Exploiting such a large scale thesaurus into applications seems very promising, as
the main weakness of knowledge based approach for retrieval is very often coverage.
For example, the extracted synonyms for each page can be used for improving the index
of Wikipedia for (cross lingual) information retrieval.

Last, but not least, our technique provides a explanatory application of two fundamental properties of lexical semantics, the domain restriction hypothesis and the lexical substitutability principle, showing that a combination of them can be almost always used to figure out solutions to problems involving lexical semantics, providing yet
another experimental proof of their universal validity.

Acknowledgments

Claudio Giuliano is supported by the X-Media project (http://www.x-media-
project.org), sponsored by the European Commission as part of the Information
Society Technologies (IST) programme under EC grant number IST-FP6-026978 and
the ITCH project (http://itch.fbk.eu), sponsored by the Italian Ministry of
University and Research and by the Autonomous Province of Trento. Aldo Gangemi
is supported by the NeOn project (http://www.neon-project.org) and the
IKS project (http://www.iks-project.eu), sponsored by the European Commission as part of the Information Society Technologies (IST) programme under EC
grant number IST-2005-027595 and IST 231527, respectively. Kateryna Timoshenko is
supported by the ITCH project.
