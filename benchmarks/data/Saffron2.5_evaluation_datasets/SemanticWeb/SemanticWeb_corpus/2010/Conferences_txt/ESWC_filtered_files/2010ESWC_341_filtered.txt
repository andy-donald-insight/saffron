An Unsupervised Approach for Acquiring
Ontologies and RDF Data from Online Life

Science Databases

Saqib Mir1,2, Steffen Staab2, and Isabel Rojas1

1 EML-Research Schloss-Wolfsbrunnenweg 31c, 69118 Heidelberg, Germany

2 University of Koblenz-Landau, Koblenz, Germany

{saqib.mir,isabel.rojas}@eml-r.org, staab@uni-koblenz.edu

Abstract. In the Linked Open Data cloud one of the largest data sets,
comprising of 2.5 billion triples, is derived from the Life Science do-
main. Yet this represents a small fraction of the total number of publicly
available data sources on the Web. We briefly describe past attempts to
transform specific Life Science sources from a plethora of open as well as
proprietary formats into RDF data. In particular, we identify and tackle
two bottlenecks in current practice: Acquiring ontologies to formally describe these data and creating RDFizer programs to convert data from
legacy formats into RDF. We propose an unsupervised method, based on
transformation rules, for performing these two key tasks, which makes
use of our previous work on unsupervised wrapper induction for extracting labelled data from complete Life Science Web sites. We apply our
approach to 13 real-world online Life Science databases. The learned ontologies are evaluated by domain experts as well as against gold standard
ontologies. Furthermore, we compare the learned ontologies against ontologies that are lifted directly from the underlying relational schema
using an existing unsupervised approach. Finally, we apply our approach
to three online databases to extract RDF data. Our results indicate that
this approach can be used to bootstrap and speed up the migration of
life science data into the Linked Open Data cloud.

1 Introduction

The Life Sciences have seen a data explosion in the last decade. These data are
most commonly stored in freely-accessible Web-based databases. The Nucleic
Acids Research (NAR) Journal puts the current number of such Web databases
that also have an application note with the journal at 1170 [1]. As the number of these databases increases, so does their size. Large Proteomics and Genomics databases actually exhibit exponential growth. These databases often
contain complementary data, pertaining to narrow and specialized sub-domains.
Any meaningful scientific investigation typically requires accessing many sources,
manually extracting and linking data records together. This activity is made even
more difficult with the fact that search engines cannot index most dynamically

L. Aroyo et al. (Eds.): ESWC 2010, Part II, LNCS 6089, pp. 319333, 2010.
c Springer-Verlag Berlin Heidelberg 2010

S. Mir, S. Staab, and I. Rojas

generated Web pages. Therefore, there is a pressing need for providing unified
and integrated access to these sources.

Traditional database integration techniques require direct access to the underlying relational database. However, in the scenario presented above, this is
almost never the case. In some instances, a part of the database is made available for download in diverse formats, including XML, tab-delimited text files,
spreadsheets, or proprietary formats. Furthermore, these data can be often stale
and incomplete.

More recently, there have been a number of approaches using Semantic Web
technologies to create RDF triple stores by aggregating data from various Biochemical databases. The earliest approach was YeastHub [2], which built a centralized RDF store for Yeast data collected from eight online sources. The data
was mapped on to a manually built ontology, as well as existing ontologies like
RSS1 and Dublin Core2. In FungalWeb [3], an OWL-DL ontology was manually
developed and instantiated for describing enzymatic data. Stephens et al. [4]
provided a drug-discovery use case by manually inspecting and integrating 12
biochemical databases into an RDF data model. Similarly, Pasquier [5] integrated
more than 10 life science databases by manually creating an ontology and merging it with existing ontologies like GO3 and GOA4. FlyWeb [6] integrates textual
and image data for Drosophila from three databases, using the D2RQ [7] tool to
convert relational data into RDF, as well as hand-written scripts for spreadsheet
data. Bio2RDF [8] is perhaps the largest source of Biochemical RDF data. It
integrates information from a variety of formats using hand-written RDFizer
programs that populate an OWL ontology that has been manually created. The
SBMM toolbox [9] shares a feature with Bio2RDF, whereby wrappers are manually created for specific source database Web sites. A user query is transmitted
to search interfaces of appropriate sources, and the wrappers extract and convert
the generated data into RDF.

One common limiting factor in the above mentioned approaches is the significant amount of manual work required to construct ontologies, populate data
from legacy formats into this ontology and to link this data. We address the first
two challenges by proposing unsupervised techniques for ontology learning and
data extraction. We begin by answering the following two basic questions:

1. What sources should we utilize for ontology learning?
2. Which data format should we target for automatic data extraction?

In an ideal world, all sources would provide their data directly in RDF format.
However, only a handful of sources, such as UniProt, Gene Ontology, IntAct
and NCBI Taxonomy, do so. The wide variety of export formats make the task
of unsupervised learning extremely hard and in many cases there is no possibility to export data at all. Furthermore, Biochemical databases are updated

1 http://web.resource.org/rss/1.0/
2 http://dublincore.org/
3 http://www.geneontology.org/
4 http://www.ebi.ac.uk/GOA/
?

?

?
very frequently, rendering the exported data stale and outdated, as mentioned
above. Finally, we conducted a survey5 of 100 databases listed on the NAR
Journal to assess how wide-spread the adoption of direct programmatic access
to databases was, such as through Web Service APIs. We discovered that only
11 large and well-established sources actually provide complete or partial access
through APIs. In fact, we concluded that a Web interface was the only common
access point for all databases.

In this paper, we propose to use Web interfaces of Biochemical sources to learn
corresponding ontologies. This follows our previous work [10] on unsupervised
wrapper generation for entire Biochemical Web sites to extract labeled data
from multiple classes of pages. Together, these approaches can be used to automatically acquire ontologies and instantiate them with RDF data on-the-fly. We
argue that such an approach would at least help to bootstrap the process which
all of the projects mentioned above follow: that of converting Web-accessible
data into linked RDF data. Additionally, our approach can be used to learn
domain-specific ontologies, a task that is very challenging if performed on natural language text as opposed to labeled, (semi) structured data in our setting.
This is very much in line with the vision of the self-annotating Web [11] where
the Web is used to extract ontologies, which then provide semantic annotation
to enable Semantic Web content creation.

Our approach to ontology learning relies on a set of transformation rules that
we apply to the output of our wrapper-generation algorithms, resulting in an
OWL ontology for the underlying database. We apply our algorithms to realworld online Biochemical Web sites. We perform a hard evaluation of our acquired ontologies against gold standard ontologies, and a soft evaluation using
three human experts to rate T-Box statements, or axioms, from our ontologies.
We also evaluate our ontologies against ontologies that are lifted directly from
the relational schemata. Finally, we apply our approach to extract RDF data
from three Web sources.

The rest of this paper is organized as follows. Section 2 briefly explains our
wrapper generation algorithm and its output which is used for ontology learn-
ing. Section 3 presents our transformation rules which convert our wrapper algo-
rithms output into an ontology, while Section 4 presents the experiments that
we conducted for ontology learning and their evaluations. Section 5 briefly describes the extraction of RDF data. Section 6 presents the related work, and we
conclude in Section 7.

2 Wrapper Induction

This section presents a brief overview of our wrapper induction approach to
extract labeled data from Biochemical Web sites. A complete description is
available in [10].

5 Available at http://sabiork.villa-bosch.de/ontology/servicesurvey.html

S. Mir, S. Staab, and I. Rojas

2.1 Page-Level Wrapper Induction

We observe that data on Biochemical Web sites are often labeled - that is, data
entries on the Web pages are in proximity to descriptive text which serve the
purpose of attribute names, such as Mass and Temperature. Our survey6
of 20 such Web sites revealed that, in fact, about 97% of data fields present
on these Web pages were labeled. We utilize these labels to by-pass the page
structure, which is dynamic and hence unpredictable, and pivot directly to the
labels, and find a relative path in the DOM representation of the page to the
corresponding data.

Our algorithm relies on multiple sample pages belonging to the same class -
that is, having similar structure and content, and being generated from the same
server-side script. We screen-scrape the individual text entries from each sample,
and compare these entries across all samples. Disjoint entries are classified as
data entries, where overlapping entries are a mixture of presentation text and
possible labels. We then determine XML Path expressions (XPath expressions)
for each text entry, from the root of the DOM tree to the node containing that
entry. These XPaths are used to determine, for each data entry, the closest nondata entry, which serves as its label. Ultimately, our algorithm outputs a label
with a corresponding relative XPath to the corresponding data entries. A final
XPath expression resembles the form:

//*[text()=label]/../tr[1]/td[2]/a[1]/text()
The above XPath expression reads: Jump to the node which contains the text
label, follow a relative path in the DOM tree from this node, which points to
the corresponding data. Our experiments indicated that the algorithm achieves
a Precision of 99% and a Recall of 98% with about 9 samples.

2.2 Site-Wide Wrapper Induction

Data in Life Science Web sites are often scattered across many pages belonging
to many different classes. In order to extract all data from underlying databases,
we must learn wrappers for each of these classes. We tackle this problem using
the concept of Labeled Link Collections. Link collections are hyperlink tag(s)
which appear grouped together under the same parent node. A labeled link
collection implies a link collection that has been associated with a label by
our algorithm in Section 2.1, signifying these hyperlinks occur over data fields.
We make two crucial observations about labeled link collections. Firstly, such
collections point to data-rich pages, and secondly, all target pages of a labeled
link collection belong to the same class. The latter observation allows us to
automatically provide pages that have similar structure and content to our pagelevel wrapper induction algorithm.

The site-wide wrapper induction algorithm proceeds by learning a wrapper for
the initial result page generated from probing a search form. It finds labeled link

6 Available at http://sabiork.villa-bosch.de/ontology/labelsurvey.html
?

?

?
Fig. 1. Schematic representation of wrapper induction output

collections and follows them to their target pages, learning wrappers for those
pages. Upon learning a new wrapper, it compares this new wrapper with existing
wrappers by comparing the sequence of corresponding labels. If the sequence is
the same, it implies the link collection points towards pages of the same class.
This process is iterated for each new wrapper learnt, until data-rich pages of a
Web site are explored.

This process essentially results in a Web site model as a directed labeled
graph, schematically shown in Figure 1, discussed in more detail in Section 2.3.
Experiments showed that the site-wide wrapper induction algorithm achieves a
Precision of about 98% and a Recall of 76%.

2.3 Discussion of Wrapper Output
In this section, we discuss in more detail the output of our site-wide wrapper
induction algorithm. The actual output is in XML, but is shown schematically in
Figure 1, for a hypothetical source. The circles represent a class of pages, while
their contents are the labels that were identified by our wrapper in these pages,
together with XPaths of corresponding data (not shown). These label-data pairs
have some additional meta-data associated with them. Certain label-data pairs
have hyperlink tags on them, signifying that these are link collections (underlined
in the figure), others have many data values associated with a single label (shown
in bold) as opposed to some which have a single data value (normal font), while
others are classified as composites (shown connected together). In addition, the
figure also shows directed edges between classes of pages. For a given class of
pages, an edge directed outwards represents the virtual link with a class of pages
that can be reached when following a particular link collection. In Figure 1, the
link collection Reaction in class C0 when followed, leads to a class of pages C1.
The edge signifying this directed connection is named after the corresponding
link collection Reaction.

3 Ontology Learning

In this section we describe our transformation rules to convert the output presented in Section 2 into an ontology. We select OWL as the ontology language as

S. Mir, S. Staab, and I. Rojas

it is being used in current Semantic Web projects in the Life Science mentioned
in Section 1. Finally, all the information required to determine the application
of these rules is present in our wrapper output. Therefore, no supervision is required to apply these rules. In the subsequent sub-sections, we take Figure 1 as
a running example and explain our transformation rules for constructing classes,
data properties and object properties of an ontology.

3.1 Transformation Rules for Classes

We apply the following two rules for construction and naming of classes:

CC - Class Construction: Each page-class discovered by our wrapper induction
algorithm is converted into a class in our ontology. This is based on our observation that a Web page typically represents a concept. In our example, this
rule would construct two classes corresponding to the two page-classes discov-
ered. In our example, two classes will be constructed corresponding to the two
page-classes present.

CN - Class Naming: A class is named after the label of the link collection that
points to it. The class corresponding to C1 will be named Reaction.

3.2 Transformation Rules for Properties

DP - DataType Property Construction: All label-data pairs discovered within a
page-class that do not have hyperlink tags are converted into data type proper-
ties. The domain of these properties is the OWL class which corresponds to the
page-class which contains these data-label pairs, and a suitable range is selected
to describe the data. Currently, our implementation supports the XML Schema
types string, integer and float. The choice between these types is based on a
simple string parsing to determine whether the sample data contains characters,
digits or decimals.

OP - Object Property Construction: All label-data pairs discovered within a
page-class that have hyperlink tags are converted into object properties. The
domain of these properties is the OWL class which corresponds to the pageclass which contains these data-label pairs where the range is the target class to
which the link collection points.

PN - Property Naming: The label of the corresponding data-label pair is assigned
as the name of the property.

PF - Setting a Property to Functional: Labels discovered in the page-classes that
have single data values associated with them are set as functional. This can be
determined by examining the XPath expressions as described in Section 2.1. For
example, for the Reaction class in Figure 1, we have the data type property

Equation(Reaction, XSDString)
An example of an object property is
Compound(Reaction, Compound)
?

?

?
3.3 Transformation Rules for Composite Labels

Labels which appear in adjoining columns of a table have associated data values
that fully describe their semantics when viewed together, which signifies an n-
ary relation. Instead of constructing properties from these labels as shown in
Section 3.2, we create an object property which has in its range a new class.
This new class in turn is used to create functional properties from label-data
pairs as shown in Section 3.2. We define the following rules to achieve this:

CL.a - Class Construction from Composite Labels: A class is created in the
ontology for every set of composites labels.

CL.b - Naming of Class Created from Composite Labels: Labels are concatenated
to form the name of such classes. In our running example, a class will be created
with the name External Accession-External Source.

CL.c - Object Property Creation: An object property is created which has a range
of this new class created, and a domain of the OWL class which corresponds to
the page-class which contains these data-label pairs.

Finally, transformation rules DP and OP are applied to create properties in the
same manner as described in Section 3.2, except the domain of these properties is
the new class that has been created by rule CL.a. The transformations described
in this sub-section are applied for each set of composite label-data pairs that are
identified by our wrapper induction algorithm.

4 Experiments and Evaluation

This section describes our experiments on 13 real-world Biochemical Deep Web
sources, and the evaluation of the results we obtained. We implemented the
transformation rules described in Section 3 in a Java program that accepts the
XML output of our wrapper induction algorithms and applies these rules in an
unsupervised manner to construct an OWL ontology.

The 13 online Biochemical databases that were targeted for ontology learning
are presented in Table 1, together with the number of classes, object and data
type properties discovered by our approach. These sources were selected as they
provide basic qualitative data that is widely required in most specialized domains
and are extensively used for annotation purposes by various other Web sites and
in models. As Table 1 indicates, the number of classes discovered varies. In our
wrapper-induction approach detailed in [10], each page type which describes data
is converted into a class. The greater the number of pages across which the results
are distributed, the greater will be the number of classes in the learned ontology.
(In addition, some classes may also be introduced in the resulting ontology due
to the transformation rules for composite labels, as described in Section 3.3).
For instance, Rhea and SBO are relatively small databases and display all their
results on a single result page. Therefore, the corresponding ontologies contain
only a single class. On the other hand, Reactome and UniProt are large databases

S. Mir, S. Staab, and I. Rojas

Table 1. Targeted data sources & corresponding classes & properties of learned
ontologies

Classes Object Properties Data Properties
Database

Reactome http://www.reactome.org/

PubChem http://pubchem.ncbi.nlm.nih.gov

Rhea http://www.ebi.ac.uk/rhea/

PDB (USA) http://www.rcsb.org/

PDBe http://www.ebi.ac.uk/pdbe/

UniProt http://www.uniprot.org/

KEGG http://www.genome.jp/kegg/

IntAct www.ebi.ac.uk/intact/

SABIO-RK http://sabio.villa-bosch.de/

ChEBI http://www.ebi.ac.uk/chebi/

SBO http://www.ebi.ac.uk/sbo/
IntEnz http://www.ebi.ac.uk/intenz/

MSDChem http://ebi.ac.uk/msd-srv/chempdb/ 1
?

?

?
with the results spread across many different types of pages. This fact, together
with the presence of composite labels, results in many classes in corresponding
ontologies. We perform three evaluations for a selection of the ontologies that
we learn. Firstly, we perform a hard evaluation against a gold standard on-
tology. This evaluation covers the lexical term layer and the concept hierarchy
evaluation. The second soft evaluation is done with the help of domain experts
which evaluate T-Box axioms from our ontologies in order to evaluate data and
object properties (relations), as well as class and relation names. Thirdly, we
lift ontologies from actual relational schemata, where available, using an exiting unsupervised approach. We then ask domain experts to rate T-Box axioms
from these lifted ontologies against corresponding ontologies that are learned
using our approach. These evaluations are presented subsequently in Sections
4.2, 4.3 and 4.4 respectively. Section 4.1 briefly describes the process of obtaining gold-standard ontologies for the three target sources. We provide an analysis
of our results in Section 4.5, highlighting the limitations of our approach.

4.1 Acquiring Gold Standard Ontologies

As mentioned above, we are interested in evaluating T-Box style axioms from our
learned ontologies. (Note that the evaluation of the extracted data has already
been performed in [10] for our wrapper-induction algorithm). In order to do this,
we need corresponding T-Box style gold standard ontologies. However, existing
relational databases rarely, if at all, provide corresponding ontologies.

In the Bio2RDF project a global ontology describing many namespaces corresponding to various data sources was manually developed7. We extract the

7 Available at http://bio2rdf.org/bio2rdf.owl
?

?

?
classes and relations in the KEGG and PDB namespace from this global ontology
to use as our gold standard. In case of ChEBI and SBO, only A-Box style ontologies containing assertions about individuals are available in various formats,
including OWL. We manually reverse engineer a T-Box from these ontologies to
use as gold standard for these two sources respectively.

4.2 Evaluation against a Gold Standard

We use the framework described by Dellschaft et al. [12] to evaluate a learned
ontology against a gold standard. We provide a brief overview of the evaluation
measures here, and refer an interested reader to [12] for details. They provide
measures for evaluating the lexical term layer and taxonomy of an ontology.
For lexical evaluation, these measures include lexical precision and lexical recall,
borrowed from [13] and defined as:
|CC  CR|

|CC  CR|

(1)

LP (OC , OR) =

|CC|

, LR(OC , OR) =

|CR|

Where OC and OR are the computed and reference ontologies respectively, and
CC and CR are concepts in these ontologies, identified by their names.

For taxonomic evaluation, they provide precision and recall measures which
are based on using the common semantic cotopy as the characteristic extract
of concepts in a taxonomy. A characteristic extract of a concept characterizes
the position of a concept in a hierarchy, which can be used to determine local
taxonomic precision for that concept. This in turn is used as a building block
for global taxonomic evaluation of the ontology. By advising to use the common
semantic cotopy, Dellschaft et al. effectively diminishing the influence of the
lexical layer in the evaluation of the taxonomy, by removing concepts which
are (lexically) different from the semantic cotopy, and only including common
concepts in the semantic cotopy. The taxonomic precision and recall are, thus,
defined as

T PSC(OC , OR) :=

|CC|

tpsc(c, c, OC , OR) if c  CR
if c / CR
?

?

?
cCC

T RSC(OC , OR) := T PSC(OR, OC)

Where

tpsc(c1, c2, OC , OR) :=

|ce(c1, OC)  ce(c2, OR)|

|ce(c1, OC)|

is the local taxonomic precision of a concept. The F-measures are then given by:

T F (OC, OR) :=

2  T P (OC , OR)  T R(OC, OR)
T P (OC, OR) + T R(OC, OR)

(5)

(2)

(3)

(4)

S. Mir, S. Staab, and I. Rojas

(OC , OR) :=

T F

(6)
 combines the lexical level and taxonomic evaluation in a single

2  LR(OC, OR)  T F (OC , OR)
LR(OC , OR) + T F (OC , OR)

Where T F
value.

We apply these measures on our learned ontologies comparing them to reference ontologies obtained in Section 4.1. The results are presented in Table 2.

Table 2. Results of evaluation against reference ontologies

ChEBI

PDB USA

0.428
0.13
0.5
0.13

0.428
1.0
1.0
1.0

T P
0.428
0.125
0.5
0.125

T R
0.357
1.0
1.0
1.0

T F
0.39
0.22
0.6
0.22
?

?

?
T F
0.4
0.36
0.8
0.36

Discussion: The lexical and taxonomic recall for our approach in case of ChEBI,
SBO and PDB in Table 2 are perfect. This is because we are successfully able
to discover the classes in the gold standard ontologies. The lower precision is
explained by the fact that we construct additional classes (7 in the case of
ChEBI and PDB, 1 in the case of SBO) in our ontologies. However, in the
case of ChEBI and SBO all of these classes are constructed to express n-ary
relationships between composite attributes that we discover during the wrapper induction phase. Therefore, although our approach strives to maintain the
semantics of the data, it suffers with regard to the taxonomic precision due to
this. In the case of PDB, the Bio2RDF ontology omits certain data offered by
this source. Hence the gold standard ontology contains fewer classes than our
approach can discover from the Web interface, resulting in lower precision. The
results for KEGG are mixed; there are some classes we discover correctly, while
others we are not able to discover. This is indicated by the relatively equal precision and recall values for both the lexical and taxonomic results. It should
be mentioned that these results are quite comparable to human inter-ontology
building [28].

4.3 Evaluation by Domain Experts

The evaluation measures in Section 4.2 have served to evaluate the names and
taxonomy of classes. They do not address the data and object properties that
are defined for these classes. For scientific data, especially Life Science data, this
is crucial, as entities in such domains have rich relationships with each other. In
this section, we describe the results of a soft evaluation, where we ask domain
experts to rate axioms in our ontologies in order to judge how well we were able
to determine these relations. These T-Box statements are of the following form

Synonym(Gene, XSDString)
Class RPair
?

?

?
We ask three domain experts to rate 51 such statements from our KEGG on-
tology, 41 from ChEBI, 11 from SBO and 54 statements from our PDB ontology.
We allow them to refer to corresponding Web sites, so that they can compare our
findings against what they would have selected manually from these sites. We ask
our experts to classify these statements into four categories based on the degree of
correctness: Wrong, Slightly Correct, Mostly Correct, and Correct. They are instructed to denote these categories for each statement by assigning values of 0, 1, 2
and 3 respectively. We then use these ratings to determine our precision and recall
at three distinct points. First, using our experts ratings we determine the categorical agreement between them. We calculate the Kappa statistic from the ratings of each ontology, using the method described in [14] for multiple raters rating
into multiple categories. The Kappa values were measured to be 42.24%, 27.42%,
13.10% and 47.31% for KEGG, ChEBI, SBO and PDB respectively, which indicates that there is a low agreement. In addition, Kappa values generally tend to
decrease as the number of categories increases, as in our case. The low agreement
might also suggest that the ontology construction task is inherently hard or not
well defined. For our evaluation, we count the data points where all three raters
agree on the categorization of a given statement from our ontology, and determine
the precision for each category. Our true-positives for a given categorization are
the number of statements unanimously classified into this category by our raters.
The false-positives are the number of statements unanimously classified into some
other category. Therefore, we determine precisions for Correct (C), Mostly Correct (MC) and Slightly Correct (SC) statements, with results shown in Table 3.
For completely correct statements, we have an average precision of 53.25%. There
can be different factors for this, such as incorrect discovery of label-data pairs by
our wrapper induction algorithm, or incorrect or unsuitable assignment of class
and property names (Recall from Sections 3.2 and 3.3 that names are taken from
corresponding labels of link-collections).

4.4 Evaluation against Lifted Ontologies

We are interested in evaluating the results of our approach to extracting ontologies from Web interfaces against ontologies that are lifted from direct access to the underlying relational schema using existing unsupervised approaches
such as [18] or [19], which apply transformation rules to a relational schema and
reverse-engineer an ontology. We opt to use the approach described in [19] as
it constructs ontologies in OWL. However, from our selection of 13 databases,
we are only able to get direct access to the relational schemata for two sources,
namely SABIO-RK and ChEBI. We therefore lift ontologies from these schemata
and perform a manual evaluation of our learned ontologies with the help of a
domain expert. We provide sets of 20 corresponding statements from the lifted
and learned ontologies, and instruct the expert to give a score of 1 to the superior
statement and a score of 0 to the poorer statement, taking into consideration
both syntax and semantics. (A score of 1 is given to both statements if they
are deemed equally valid). The results are shown in Table 4, which presents the
total score for corresponding lifted and learned ontologies.

S. Mir, S. Staab, and I. Rojas

Table 3. Precision values for three categorizations of statement. P(C) is precision of
statement being completely correct, P(MC) is precision of statement either completely
or mostly correct, P(SC) is precision of statement completely, mostly or slightly correct.

Sources

ChEBI

PDB USA

P(C)
43.8%
66.6%
50%
52.6%

P(MC)
84.4%
80.1%
87.5%
90.5%

P(SC)
90.7%
88.4%
87.5%
92.3%

Table 4. The total score for lifted and learned ontologies

Total Score for Lifted Ontology Total Score for Learned Ontology

Sources
SABIO-RK 9
ChEBI
?

?

?
Discussion: We note here that for SABIO-RK, 34 classes are constructed in the
ontology lifted directly from the relational schema compared to the 18 classes
constructed using our approach. This is because portions of the schema are not
normalized, as well as the fact that the schema contains certain data which is not
displayed in the Web interface at all. The ontology lifted from this schema also
suffers lexically as the table and attribute names are often obscure and unnatural.
On the other hand, both the ontologies for ChEBI consist of 8 classes, and the
schema comprises of well-named tables and attributes, resulting in a very high
score for the lifted ontology.

4.5 Analysis of Results

In this sub-section we present a brief analysis of the overall results and the conclusions that we can derive from it. Firstly, our transformation rules do not assist
us in creating rich taxonomies. Therefore, we would expect to achieve poor taxonomic precision and recall if the reference ontology has a rich taxonomy, which
was not the case in our experiments: Only KEGG has two sub-class relationships.
A possible future improvement in our approach would be to use labels within
page-classes in our wrapper output to determine such relations. One could determine such taxonomic relations if the set of labels of a given class is a subset
of that of another class, for example, by using Formal Concept Analysis [27].

Secondly, our results from Section 4.4 indicate that extraction of ontologies
from Web interfaces is a viable option especially in the absence of relational
schemata. Relational schemata often suffer from not being in normalized forms,
either for ease of representation or performance, and may follow ambiguous naming conventions as these are private and internal representations.

Finally, the manual approaches for ontology construction described in
Section 1 tend to re-use existing ontologies, such as FOAF8 and Dublin Core.

8 http://www.foaf-project.org/
?

?

?
This is extremely beneficial, as it helps to eventually integrate the data from
various sources. However, this is not possible in our approach. This results in
lexical differences between manually constructed ontologies and those generated
by using our approach, which affects precision and recall.

5 RDF Data Extraction

In this section we briefly describe extraction of RDF triples from Web interfaces
for which we have learned corresponding ontologies. Since the transformation
rules for our ontology learning approach are directly mapped from the output
of our wrapper induction algorithm in [10], we can directly extract RDF data
by applying our wrappers and populating the corresponding ontology. Since we
wish to merely demonstrate the feasibility of extracting data from Web pages,
we select three sources of varying size, namely KEGG, ChEBI and MSDChem,
and execute corresponding wrappers on the result pages. We utilize the lists
of identifiers provided by each source on its Web site to probe the Web forms
and generate the result pages. The triples thus generated are stored in a local
Sesame9 repository. In all, approximately 300MB, 200MB and 25MB of data are
extracted for KEGG, ChEBI and MSDChem respectively. The running times
for applying the wrappers for these sources were approximately 7, 5 and 1 hours
respectively, using a 6 Mbps internet connection.

6 Related Work

Although there are many approaches to ontology learning from natural language
text, we are only aware of a handful of approaches which target Deep Web sites
containing semi-structured data as a source for ontology learning. In [15], the authors describe a system which extracts attributes of Web search forms belonging
to a certain domain, such as tourism and e-commerce. WordNet is used to find
hyponyms for these attributes iteratively until a taxonomy of concepts is generated using only the IS-A relation. OntoBuilder [16] also constructs a taxonomy
of concepts by parsing Web search forms, although the authors do not describe
the algorithm or the resulting taxonomy in detail. OntoMiner [17] learns a taxonomy of concepts from co-domain Web sites. It relies on HTML regularities
to construct taxonomic structures in XML by utilizing a hierarchical partition
algorithm. The taxonomy is iteratively expanded with sub-concepts by crawling through links corresponding to concepts in the taxonomy. Another related
approach is used in Triplify [29], which facilitates linked RDF data generation
for Web site publishers by mapping HTTP-URI requests onto relational queries,
the results of which are transformed into RDF.

A closely related area of research is Database-to-ontology conversion, whereby
transformation rules similar to ours are applied directly to a relational database
schema and corresponding ontology is extracted [18,19], or mapping rules are
generated between an existing ontology and the underlying relational database,

9 http://www.openrdf.org/

S. Mir, S. Staab, and I. Rojas

such as in D2RQ [7]. However, in our setting, we do not have access to the actual
database implementations. Another related research topic is that of identification
of HTML tables and transforming them into, for instance, logical frames as
in TARTAR [20] or relational schemes as in WebTables [21]. However, such
approaches neglect data on Web pages that are not structured within HTML
tables. Finally, there has been considerable research using XML documents to
extract relational schema [22,23], DTDs [24], or XML Schemata [25,26].

7 Conclusions

We have presented an unsupervised approach for extracting ontologies and RDF
data from Deep Web Life Science databases. Our approach relies on transformation rules that convert XML output from our wrapper induction algorithm into
OWL ontologies. Experiments were conducted on real-world online Biochemical
Web sites. Our results indicate that this approach can be used to bootstrap
the process of converting legacy and freely available online data into machineprocesable data for use in Semantic Web applications. Our results support our
argument that the Deep Web, with its semi-structured content, is an ideal source
for learning ontologies to help overcome the bottleneck for wider adoption of
Semantic Web technologies. For future work, we would like to re-use existing
ontologies during the ontology learning phase in order to facilitate the eventual
integration of the extracted data into the Linked Open Data cloud.
