Using Linked Open Data to bootstrap corporate
Knowledge Management in the OrganiK Project

Gunnar Aastrand

Grimnes
DFKI GmbH

AG Wissensbasierte System

TU Kaiserslautern

Germany

gunnar.grimnes@dfki.de

Remzi Celebi

DFKI GmbH

University

Germany, Turkey

Leo Sauermann

DFKI GmbH

TU Kaiserslautern

Germany

and Middle East Technical

AG Wissensbasierte System

remzicelebi@gmail.com

leo.sauermann@dfki.de

ABSTRACT
Tagging has become a wide-spread tool for organising con-
tent, from photos and music, to research paper and data-
visualisations. Organising tags in a taxonomy adds hierarchical structure and relationships, this can be helpful, both
for finding and applying tags to new content, as well as for
enabling query expansion when searching. However, taxonomies can be very time-consuming to create and maintain.
If a hierarchical taxonomy could be automatically built and
adapted to a particular domain, the entry cost for using taxonomies for structuring information would go down. Small
and medium enterprises (SMEs) do not currently have sufficient resources to invest in Enterprise 2.0 technologies like
taxonomies, wikis or blogging as the entry cost it too high.
The OrganiK project aims to make Enterprise 2.0 features
available with low entry- and maintenance costs.

In this paper, an algorithm and methodology to automatically create and maintain taxonomies is presented. It
analyses enterprise document corpora and uses background
information from domain-specific data sources or from the
Linked Open Data cloud to improve and contextualise the
created SKOS taxonomy. Content created in a Drupal-based
Enterprise 2.0 content management system is automatically
categorised, and the automatically created taxonomy is extended when needed. The system has been tested with corpora of medical abstracts, computer science papers, and the
Enron email collection, and is in productive use.

Categories and Subject Descriptors
I.7.1 [Document and Text Processing]:
Docu-
ment and Text EditingDocument management; H.3.1
[Information Storage and Retrieval]: Content Analysis and IndexingIndexing methods, Linguistic processing,
Thesauruses

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
I-SEMANTICS 2010 September 1-3, 2010 Graz, Austria
Copyright 2010 ACM 978-1-4503-0014-8/10/09 ...$10.00.

General Terms
SKOS, OrganiK, Drupal

Keywords
Enterprise 2.0, knowledge management, knowledge com-
monality, tagging, thesaurus generation

1.

INTRODUCTION

Small and medium enterprises (SMEs) are contributing
a large share of Europes productivity. While they employ
a large workforce overall, each enterprise operates under a
tight budget and has nothing to waste for knowledge management activities. On the contrary, large corporations can
invest into technical systems such as search engines, Enterprise 2.0 systems, social platforms, or wikis. Also, they can
afford to dedicate human resources to support knowledge
management by helping to moderate communities of practice or maintain a white-pages system.

In the OrganiK project (Section 1.1), we aim to bring Enterprise 2.0 knowledge management technologies and practices to small and medium enterprises. The European Union
is funding this project within the overall strategy of research
for the benefit of SMEs. Part of the research activities of
OrganiK is to adapt and optimise available methods and
technologies for the specific requirements of SMEs. Given
the time and monetary constraints of an SME, a knowledge
management system should be without any monetary costs,
be easy to install, easy to use, and easy to integrate into
the work processes, work automatically as much as possible,
and save time and money. Also, any advice given to an SME
about knowledge management must be practical and prag-
matic, to be evaluated quickly and either abandoned or realized (see Section 1.2). Although these requirements sound
optimistic it is possible to fulfil some of them quite easily using open-source technologies, state-of-the-art research
findings, and our suggested algorithm and methodology.

OrganiK uses the Drupal (Section 1.3) programming platform to realize a simple and extensible knowledge management system. Users can put information in many ways into
OrganiK: via blog-posts, micro-blogging (twittering), sto-
ries, wikis, by bookmarking interesting webpages, uploading
documents, or by sharing e-mails. The content is automatically analysed using semantic technologies and annotated in
a SKOS thesaurus. This should work automatically, the thesaurus has to be created automatically and extended when


strap a thesaurus for an SME and represent it in SKOS, the
Simple Knowledge Organisation System standardised by the
W3C. It works on the basis of documents already existing
in the enterprise, for example from a shared network drive
or existing knowledge management systems.

When new documents are added to OrganiK, they are
automatically analysed and tags are suggested for each doc-
ument, thus saving time when categorising a document.
When new terms are found inside a document which may fit
into the company thesaurus, they are automatically added
to the thesaurus. When adding such new terms, an educated guess is needed to decide where a term is put into the
existing hierarchy. External data published according to the
Linked Open Data standard is used to make this decision,
to guess the right position in the hierarchy.

This saves time when categorising and storing documents
but also rationalises the rather subjective discussion of ontology building.

An evaluation of our approach (Section 3) is done using
real-life data from a SME (small and medium enterprise)
partner of the OrganiK project. Articles on health from
magazines should be categorised to be used in a publication portal. Not having a similar existing system available
for comparison, the results of our algorithm are compared
against hand-made taxonomies. By using an ablation study
in our evaluation, the effect of each stage of the process can
be quantified.
1.1 The OrganiK Project

The OrganiK project [3]1 aims to create a knowledge
management system as a collaborative workspace for SME
knowledge-workers. Research is targeted towards two inno-
vations: Firstly, to develop an innovative knowledge management technology, integrating different elements of enterprise social software and semantic technologies. Secondly,
to provide a human-centered knowledge management theory focused on the actual work practices and requirements
of small European knowledge-intensive companies.
1.2 Lowering the Effort towards Knowledge

Commonality

The requirements for the technology are based on a knowledge management principle tailored for SMEs: the goal of
knowledge commonality, which is based on three princi-
ples:

 Externalising knowledge is good for many reasons,

both for the individual and for the enterprise.

 Automatic annotation makes sharing and storing eas-

ier.

 Automatic annotation helps finding information.

Users can store knowledge for personal reasons and share
it in one step, thus writing down facts for remembering and
externalising knowledge are coming together. Please refer to
our deliverable 2.1 of the project [2], where theese principles
are described further.
1.3 Content Management with Drupal and

1http://www.organik-project.eu

In the OrganiK project, SMEs need a free system for managing their documents. It must allow users to have different
views on the same information, allowing multiple categories
for each document.

The technical system is developed under an Open-Source
license based on the stable PHP Open-Source software Dru-
pal2. Although it is geared towards building websites and
community portals, it can also be used for Enterprise 2.0 ap-
plications. It features a stable user management and content
management basis on which thousands of well-maintained
modules are written. For all core technologies of OrganiK
, there exist modules which can be extended. For example,
there are modules supporting RDF parsing, taxonomy man-
agement, search, microblogs, wikis, blogs, comments, etc.
We selected a stable basis and then extended it with our
own modules 3. The web-based OrganiK server provides
the following generic components:

 Content Management System (CMS)
 Recommender System
 Semantic Text Analyser
 Collaborative Filtering Engine
 Full-text Index and Search Engine

Components are used to realize different scenarios and use
cases which are common in SME environments. A notification system sends e-mails to users when interesting content
is added.

Taxonomies are a well proven and described way to organise content. We find existing taxonomy-like structures
in companies file-systems and in the structures created by
individual users. Drupal supports taxonomies as a core mod-
ule, and all other modules annotating content are reusing
this core module. It supports terms, hierarchies, synonyms,
related terms, and multiple separate taxonomies. We extended this module to support SKOS4 import and export,
to have a W3C standardised representation of taxonomies.
The taxonomy module maps directly to the SKOS classes
Concept and ConceptScheme and the SKOS properties pre-
fLabel, altLabel, broader, narrower, and related. Our
import/export module follows these mappings5.
1.4 External Background knowledge from

Linked Open Data

To support knowledge workers and automatically create
a taxonomy for them, it is important to take as much existing structure into account as possible. If existing structures and terms can be imported, less manual decisions have
to be taken inside the SME. There are two kinds of background information possible: domain unspecific general information coming from such sources as DBpedia6 or domain specific information coming from industry standards
or existing databases within the SME. Our approach was

2http://www.drupal.org
3The OrganiK implementation, especially the selected Drupal modules and adaptations, are documented at http:
//organik.opendfki.de
4http://www.w3.org/TR/skos-reference/
5http://drupal.org/node/560326
6http://www.dbpedia.org


ably SKOS. As the Semantic Web works both in the public
web and in private intranets, this approach follows current
Semantic Enterprise Architecture (SEA) considerations[14].
For example, specific domain taxonomies such as MESH or
Bio2RDF data can be used in a medical background, or
customer names from an internal CRM system to identify
companies and persons.

The semantics of words differs from domain to domain.
Especially multi-word noun phrases such as semantic web
or ship accident tend to be ambiguous. With the right
background information, it can be detected if the terms are
to be used as nouns or multi-word nouns.

For our work, we primarily used the semantical background knowledge from Wikipedia which can be accessed
as RDF through DBpedia. By using this data, which was
created by experts before, knowledge can be reused in our
chosen application field. As RDF and SKOS are standard
formats, our approach can learn from other existing knowledge bases. Also, the output is again expressed with SKOS,
making the results applicable in different areas.

2. AUTOMATIC TAXONOMY MANAGE-

MENT IN OrganiK

It is likely that OrganiK will not be introduced in a company from the start, and that existing documents and knowledge structures are in use in the company.

We have created a service based on the Aperture crawling
and extraction framework7 which allows importing existing
content into OrganiK when it is first deployed. To bootstrap
the use of semantic tagging an initial tagging-taxonomy is
then extracted from this corpus of existing documents. The
details of the boostrapping service are described in Section 2.1. Once the initial taxonomy has been created, any
new data added to the system will have tags automatically
suggested from this taxonomy, or new tags will be automatically created when appropriate. Finally, any new tags
created automatically or manually are probably not correctly positioned into the hierarchical taxonomy, a service
for maintaining the taxonomy-tree and position these tags
correctly was created. This can for example be run nightly,
as a part of the Drupal cron framework, and is described in
Section 2.1.9.
2.1 Taxonomy Learning

The taxonomy learning process takes as input a corpus of
text-documents and the number of terms to include in the
final taxonomy. Although it was developed here to deal with
data from a Drupal CMS system, the methods are relevant
to any environment where text documents are available as
input and a SKOS taxonomy is useful as output. As shown
in Figure 1 our taxonomy learning process follows a standard
information extraction pattern.
2.1.1 Language Detection
The first step is language detection. We have trained a
maximum entropy classifier for performing language detection using text from Wikipedia as training data. The usecase partners in the OrganiK Project initially require support for English and German only, and a corpus of 1,000
randomly selected German and 1,000 English pages were

Figure 1: Taxonomy Learning Process Overview.

used for training. This problem is rather easy, and our classifier achieves a 100% accuracy on unseen sentences from
Wikipedia. Note that on noisier data, sentences may be incomplete and or contain a mix of German and English, so
in a realistic setting some detection mistakes may occur.
2.1.2 Term Extraction
The next step is term extraction. Each document in our
input corpus is split into sentences, and each sentences is
analysed for extracting noun-phrases (NPs). Currently, we
are primarily interested in the noun-phrases in the text, and
thus do not construct full parse-trees, instead we use maximum entropy models for doing noun-phrase chunking [12].
We make use of the OpenNLP package8, which already provides trained models for NP-chunking for English and sentence detection for many languages. For German we trained
our own NP-chunking model based on the Tiger Corpus [4].
The corpus contains about 40,000 sentences, and about
900,000 tokens. Each sentence in the corpus as annotated
with a full parse-tree, including both part-of-speech (POS)
and phrase-chunk tags. As the chunks form a tree, many sentences included multiple NPs at different levels. As we were
ultimately interested in term-extraction, the shorter terms
seemed more useful to us, and we included only the lowest
level NP when multiple were present. In addition, we also
considered noun-kernels of larger chunks like verb-phrases
to be NPs in our context as they often contain the content
words that are important in a sentence. Using this input we
trained a model that achieved an accuracy of 94.03% on the
10% of the sentences kept for testing.
2.1.3 Hearst-pattern Extraction
So called Hearst-patterns [9] are patterns used for extraction of word-relations by querying large text corpora. For
example, by querying for the pattern (Plural NP), such as,
(NP), (NP) [...] and (NP) one may get hits such as Cities,

7Aperture, http://aperture.sf.net

8OpenNLP: http://opennlp.sf.net

SpreadingActivationHierarchyGenerationHearst-patternmatchingTerm ExtractionRankingDbPedia LookupFilter & OutputLanguageDetectionsuch as Berlin, Kaiserslautern ...
or European Capitals,
such as Berlin, Paris and London, which can be seen as evidence that Berlin is-a City, or London is a European Cap-
ital, etc. The patterns are normally used on large corpora,
and facts are only extracted if a certain relation is seen many
times across the corpus. Our input corpora are typically not
on such a scale that we can rely on good patterns appearing
many times, instead each found Hearst-pattern becomes one
part of evidence that two terms are related and important
for the domain. We keep track of Hearst-patterns found
when doing term-extraction, and use this as evidence in the
later hierarchy building step.
2.1.4 DBpedia Lookup
In the next step, we attempt to find a matching DBpedia
concept for each NP. We allow partial matches of a NP to an
article, i.e. the NP Julius Caesars Conquests could match
both Julius Caesar and Military career of Julius Caesar on
DBpedia. We used Apache Lucene9 to do the lookups, indexing the titles of all German and English DBpedia pages.
We also carried out an experiment where in addition to
the title of each DBpedia page, we also indexed the abstract
text in a different Lucene-field. At lookup-time we would
query for the NP text in the title AND the rest of the text
of the sentence where the NP occurred in the abstract-field.
The intention was that the extra text would allow disambiguation of synonyms, i.e. given the sentence The Jaguar
is a time-less luxury vehicle., and considering the NP Jaguar,
we hoped that matching the surrounding text with the abstract of the pages for Jaguar (the cat) with Jaguar Cars
would rank the car page higher. This change led to a 100
fold increase in processing-time, and 10-fold increase in index size, but did not significantly improve the final taxon-
omy, and we dropped the change for the final version. After
looking up a term in the Lucene index we get a ranked list
of possible DBpedia matches back, and we kept the top 5
hits. One problem with using Lucene for this matching is
that the returned relevance scores for documents to queries
are not absolute, i.e. they are only valid for ranking documents to a query within the context of that query. This is
partially a result of Lucene optimisation, and partially because absolute scores are not really possible for a changing
search index. In our case, an absolute score could possibly
have been defined (if we consider our snapshot of DBpedia
as final), but we found that this was an acceptable trade-off
for the high-performance granted by using a Lucene index.
2.1.5 Ranking
After extracting NPs and doing DBpedia lookups we now
have a huge list of possible terms, each with multiple possible
mappings to DBpedia concepts. Note that we did not only
look at exact matches but also fuzzy matches. To get a more
manageable list as input for the next step we ranked this list
by a combination of factors:

We reduce the number of matches to top-ranked N for
each term-DBpedia concept tuple. The ranking score is a
weighted sum of four different sources of evidence:

tf T Ft + tlT Lt + toT Ot + dsDSt

tf + tl + to + ds

(1)

Rt =

where:

9http://lucene.apache.org

T Ft is the TFIDF score of term t, the inverse document
frequency taken from the Drupal search index tables,
giving us a document frequency based on the domainspecific input documents we have. If running without
a backup Drupal process, we can fall back to global
document frequency by taking counts from the Google
N-Gram Corpus10.

T Lt is the inverse word-length of the terms.

I.e. T Lt =
1+|Wt| , where Wt is the list of words appearing in the
term. The intention being that shorter terms are bet-
ter, to filter out some extracted NPs that are complete
segment fragments and are badly suited as tags in a
taxonomy.

T Ot is a measure of the word-overlap between the term and
t | , where
the DBpedia match,
LSCt is the longest common sub-sequence of words
in the term and article title and W d
t the list of words
in the DBpedia concept label. This is an attempt at
defining a globally valid score DBpedia matches.

i.e. T Ot = 2LSCt
|Wt|+|W d

DSt is the Lucene score for this concept label. We normalise
the scores to the interval [0,1] by using percentile rank,
i.e. each score is replaced with the percentage of scores
lower than it.

n is the weight associated with each score. Our final experiments used an equal weight for all scores, apart
from tl which was weighted twice as strongly.

The NLP and DBpedia matching process introduces a lot
of noise terms (for our experiments below we extracted over
15,000 candidate terms in an intermediary step which resulted in a final taxonomy of 360 terms). After ranking we
discard all terms that are below a certain threshold. We
found that to reduce the number of parameters required
we could estimate this threshold, by picking the value that
would keep 10-15 times the number of terms required in the
final taxonomy.

2.1.6 Hierarchy Generation
After ranking, we extract the hierarchical and related
properties from DBpedia. We manually inspected the typical properties used to relate DBpedia concepts that have a
broader/narrower-than relationship to find a list of properties that can be seen as evidence for a hierarchical relation-
ship. Table 1 shows the frequency of properties as found in
our evaluation run with 3196 candidate terms.

We extract narrower/broader relations between the candidate terms that we have already found, but also keep track
of links to DBpedia concepts that are not already in our list
of terms, once a concept has been identified as a candidate
super or sub-term two times we include it as an implicit con-
cept, initially given a very low weight. This allows us to also
extract hidden concepts that are not explicitly mentioned in
the texts we have analysed. For example, if we have found
maximum-entropy and naive bayes, and both are said to be
a sub-concept of machine learning, we may introduce this as
an implicit concept.

10Google
googleresearch.blogspot.com/2006/08/
all-our-n-gram-are-belong-to-you.html

N-Gram

Corpus:

http://


property


dbont:keyPerson
dbprop:location
dbont:type
dbprop:subdivision
dbprop:section
dbprop:icd10
dbprop:redirect
dbprop:relatedInstance
dbprop:disambiguates
dbprop:wikiPageUsesTemplate
skos:subject

Table 1: Frequency of Hierarchical Relationship
properties in DBpedia.

Spreading Activation

We also add related links between the terms of the new
taxonomy, these are simply added whenever DBpedia contains any other relation between the matching concepts,
apart from the ones identified as constituting a hierarchical relationship. Related links do not add implicit concepts.

Once we have constructed the hierarchy with backgroundknowledge from DBpedia we will have a forest of sub-trees,
many of them only connected through implicitly included
concepts. At this stage the number of concepts may also
be very large (for the evaluation runs presented in Section 3
we had over 10,000 candidate concepts for a final taxonomy
of 320), and when removing all concepts below a certain
threshold the sub-trees are likely to be even more discon-
nected.

Since we would like the final taxonomy to be as concise as
possible, it is better if we try to focus on some well connected
sub-areas, at the cost of losing some peripheral disconnected
terms. To achieve this we applied spreading-activation, i.e.
for some number of iterations we let the score of all concepts
flow to their neighbours. Figure 2 illustrates how spreading
activation increases the weight in well-connected terms, resulting in a better connected taxonomy in the end. The flow
depends on the types of relation, in general we had slightly
higher flow from a concept to its parents (by the intuition
that higher-level concepts introduce additional organisation)
than vice versa, and slightly lower again for related terms.
The number of iterations of spreading-activation to apply
is a tradeoff, too more iterations and a well connected subtree will quickly over-shadow all other parts of the taxonomy,
too few and little is gained. Figure 3 shows the quality of
our extracted taxonomy plotted against number of iterations
for several different sizes of taxonomies. The trend is clear,
quality increases when spreading-activation is introduced,
and smaller taxonomies need more iterations for a similar
increase. After a few iterations the quality decreases again
for all sizes.
2.1.8 Filtering and Output
After spreading activation is complete, we filter out the
concepts that rank below a certain threshold, again typically
estimating the exact value based on the number of terms we
want in the end. The final taxonomy is exported as a SKOS
file, ready to be imported into Drupal for the OrganiK users.
The file includes rdfs:seeAlso links to the matching DBpedia

Figure 2: Spreading Activation increases weight in
well-connected terms.

Figure 3: Spreading Activation Evaluation.

concepts.

2.1.9 Taxonomy Maintenance
After the initial taxonomy has been built and imported
into Drupal, the users are free to modify and extend the
taxonomy as they see fit. While editing content the nounphrase extraction modules are re-used for providing on the
fly tag-recommendations. The rest of the process is too time
consuming to allow real-time updates, but a nightly script
is run which attempts to fit any new tags into the existing
taxonomy.

The process is similar to the initial process for learning the
hierarchy. The existing tags with previously found DBpedia
mappings are loaded and DBpedia is queried for hierarchical
and related relations to the new tags. As the news tags
are manually added, there is no filtering, and therefore no
spreading activation.

Threshold filteringCE......FABC......FABCE......FABCE......FABCE......FABCE......FSpreading activationThreshold filtering012345678900.020.040.060.080.10.120.1410204080160320# Iterations of spreading-activationTaxonomy Quality # terms2.2 Replicating the approach

The presented approach can be replicated by downloading
the software we used. A set of scripts have been used to
run the experiments and the algorithms were implemented
as part of the OrganiK project and are released under an
open-source license. To replicate the approach, download
the source files from the project homepage 11.

3. EVALUATION

We evaluate our taxonomy-learning methods by learning
a series of taxonomies from 2,724 abstracts of medical pa-
pers. This data comes from one of the OrganiK use-case
partners, LeserAuskunft, a company selling access to journals and books online.

The learned taxonomies are evaluated by comparing them
with the MEdical Subject Headings (MESH) taxonomy
which also covers the medical domain. MESH contains
25,186 concepts arranged in a hierarchy, as well as synonyms
are related terms.

We are not interested in learning a complete copy of
MESH, indeed our 2,724 abstracts only cover a fraction
of the domains covered within MESH. We use a nonsymmetrical comparison metric based on the ontology similarity metric from Dellschaft and Staab [8]. Our version
is modified with the intention that any part of our learned
taxonomy where the same terms and/or hierarchical structures appearing in MESH are rewarded, but parts of MESH
structure or terms not appearing in our taxonomy are not
penalised.

We evaluate our approach using a range of taxonomy sizes:
starting with a very small size of 10 concepts, up to 360 con-
cepts. Although many well known taxonomies have thousands of entries, we believe that such small to medium size
taxonomies are more useful for bootstrapping a knowledge
organisation system in an SME.

In addition to the varying sizes, we have evaluated our
approach with and without the Hearst-pattern extraction
as well as with and with-out spreading-activation. When
spreading activation is enabled we use four iterations, as
shown in Figure 3 this gives a good trade-off.

The results of the evaluation are shown in Figure 4. It
is immediately clear that spreading activation makes a huge
difference in terms of quality. Relying on the unprocessed
ranks alone terms that also occur in MESH are first included
when the final taxonomy contains more than 80 terms. Sec-
ondly, the inclusion of hearst-patterns does for small taxonomies have a large effect, but as the taxonomy size increases the effect diminishes. This is probably due to only
looking for the patterns in our input corpus where it is unlikely to appear very often.
In fact, in the entire medical
corpus used for this evaluation, we only found 127 possible
matches, and many of these included poor quality nounphrases that later did not have a good DBpedia match or
where otherwise filtered out. Once the taxonomy gets over
a certain size, there are simply not enough hearst-pattern
matches to make a difference. Finally, it interesting that
when spreading activation is active, the quality of the taxonomy goes down as the number of terms increases, but it
goes up when spreading activation is not used. Manually inspection of the learned taxonomies confirms that this is due

11http://organik.opendfki.de/wiki/
TaxonomyLearningImplementation

to clusters of non-domain specific terms such as days of the
week that have many interconnections and are thus quickly
ranked highly when doing spreading activation.

Figure 4: Results for varying taxonomy size.

We also ran the process on 5,062 emails from the Enron
corpus [5]12. These emails cover all sorts of events within
Enron, from personal weekend plans, to meeting minutes,
press-releases, etc. The texts are generally much more varied and informal than the medical abstracts, and the corpus is probably closer to the kind of data the SMEs in OrganiK actually manage. This might cause a problem for
our approach, as DBpedia is likely contains less hierarchical structure for such informal topics. Unfortunately we are
not aware of any existing taxonomies that cover the general
office topics of the Enron corpus, so an evaluation against
a gold-standard is not possible. Figure 5 shows the ratio
of broader-than relationships to the number of terms for a
increasingly large taxonomies extracted from the Enron corpus and the medical corpus above, both extracted with four
iterations of spreading activation, as above. We can see that
the medical corpus generally has more hierarhical structure,
for the taxonomy with fourty terms, the ratio is one, and
there is one broader-than relation for each concept. This
confirms our hypothesis that DBpedia better suited for the
more formal domain of medical terms.

Figure 5: Ratio of broader-than relationships to number of terms for Enron and Medical corpus.

3.1 User Feedback

At the time of writing, the OrganiK system has been in use
at one participating SME for roughly a month. A group of 14
knowledge workers were using it to discuss decisions during
a project. The feedback on the system was highly positive,

12http://www.cs.cmu.edu/~enron/

#10#20#40#80#160#32000.10.20.30.40.50.60.7nonehearstspreadingboth# termsquality1020408016032000.20.40.60.811.2EnronMedical# termsratio of is-a relationsespecially the ability to document decisions was evaluated as
very useful. The tag suggestions were not always accurate,
but good enough to support users in their tasks.

4. RELATED WORK

There are approaches related to the general idea of Or-
ganiK. In [16] an overview on related work is given. For
example,
in the Semantic Web use case at Eletricite de
France13, successful deployment of ontologies and a semantic search engine shows that adding semantics can help large
corporations. The Enterprise 2.0 case study of of the KALIS
project [1] shows that a pragmatic approach to knowledge
management with a basic system and automated services
can help knowledge workers. KALIS supports keyword
search and tagging and our approach adds a missing automatic generation of taxonomies.

While we integrated SKOS and semantic tagging in Dru-
pal, IBM researchers have done the same for WebSphere
Portal [11]. This implementation provides an appealing user
interface and an ontological approach to tagging. With our
presented taxonomy bootstrapping algorithm, such a portal
could be initialised.

Existing commercial systems also provide SKOS-based
thesaurus management and document classification. Especially interesting is Poolparty 14 from the company Punkt.at.
Poolparty uses a state-of-the-art document classification approach based on statistical training of terms with docu-
ments. It does not only use SKOS, but also links concepts
to DBpedia, which is similar to our approach. As far as we
know, it does not automatically generate taxonomies based
on the approach presented in our paper, but already supports tag recommendations.
In Poolparty, the taxonomy
can be extended ad-hoc by users and supervisors can improve and adapt the ontology regularly as an administration
task. Our presented algorithm could serve as complementary service here, adding structure suggestions.

With regard to the taxonomy learning part this work,
much related work has been done in the area of ontology
learning, see for instance Cimianos book and references
therein [6]. Using external data-sources to improve relationship extraction is also popular, most commonly using
unsupervised learning techniques (like the hearst-patterns
we have re-used) from the non-semantic web, for example,
see the excellent Holmes system [13]. For using structured
data-sources directly, the BBC has made use of DBpedia
and other Linked Open data sources to integrate data and
linking documents across BBC domains [10].

5. FUTURE WORK

The biggest problem with our approach currently is that
we can only learn the hierarchical structure already present
in DBpedia or a domain-specific LOD information source.
For the medical domain we used in our evaluation this is
not such a big problem, as DBpedia has good coverage of
domain specific terms and a range of high-quality domainspecific sources are exposed by the Bio2RDF project. For
more generic domains where DBpedia coverage is of less
quality extending our approach to discover new hierarchical relationships would be important.

13http://www.w3.org/2001/sw/sweo/public/UseCases/
EDF/
14http://poolparty.punkt.at/

An obvious first step is to extend our hearst-patterns to
be closer to the original. We looked for the patterns only in
the input documents, where only few matches were found.
Extending this to use a large external document corpus, such
as Google, may yield better results.

Alternatively, attempting to recover more structure by
analysis the content we already have, we tried to apply hierarchical clustering techniques to the found concept to build
a concept tree. Similarity between terms was based on the
similarity of their matched DBpedia concepts. However, the
resulting cluster hierarchies generally did not represent the
type of broader/narrower relationships we are interested in.
A better approach may be the define similarity based on the
co-occurrence of terms in our input documents, but the hierarchical clustering algorithms are still unlikely to provide the
type of is-a or contained-in relationships we want. A more
interesting approach may be using Formal Concept Analysis
to learn the hierarchy as done by Cimiano et al. [7]; as we
already have RDF descriptions of the concepts and these are
close to formal concepts, this might be a promising approach.
Another approach was inspired by Wu and Welds work on
improving the Wikipedia infobox ontologies [15], they cast
prediction of is-a relationships as a classification task, each
instance being a pair of concepts. A preliminary stand-alone
experiment on some of the DBpedia concepts found in our
evaluation showed a quite acceptable precision ( 93%) for
training a SVM to correctly predict is-a relationships between unseen pairs of concepts. This was based on features
based on words in common in the abstract of each concept,
hearst-like patterns from Google, Google hit counts for both
terms together or separately, and Delicious tag-counts for
each and both of the terms. We find this method to be very
interesting, as it provides a nice way of combining different types of evidence of hierarchical structure, but leaving
it to the classification algorithm to work out which types of
evidence works well.

Finally, by integrating the taxonomy learning process
more tightly with the rest of the Drupal setup, we could
make use of the structure implicit in the imported docu-
ments, for example by mining existing folder hierarchies.

6. CONCLUSION

In this paper we have shown how a content management
system, taxonomies, and effective knowledge management
practices can help SMEs to realize Enterprise 2.0 within
their operations. The main contribution is an algorithm to
automatically create taxonomies from enterprise document
collections, which picks terms and put them into a taxonomic hierarchy. The system is leveraging background information accessed through semantic web standards (SKOS,
SPARQL, and RDF) to improve the quality of the taxonomy
recommendations.

In several evaluations, the algorithm was tested. In the
medical domain, data from a medical publisher was analysed
to be automatically categorised for publishing on a website.
Documents from the computer science domain were analysed and the resulting taxonomy compared to existing tax-
onomies. Tests on office documents were done using the
Enron email corpus and with other files from real SME participants of the OrganiK project. The system is also in productive use since a month at a participating SME. Results
from the automatic taxonomy creation show that the created structures can be used in practice. The actual users of


P. Traverso, F. Ciravegna, P. Cimiano, T. Heath,
E. Hyv  A unen, R. Mizoguchi, E. Oren, M. Sabou, and
E. P. B. Simperl, editors, ESWC, volume 5554 of
Lecture Notes in Computer Science, pages 723737.
Springer, 2009.

[11] A. Kreiser, A. Nauerz, and F. Bakalov. A web 3.0

approach for improving tagging systems. In In
Proceedings of Workshop on Web 3.0: Merging
Semantic Web and Social Web 2009, volume 467 of
CEUR Workshop Proceedings, Turin, Italy, June 29
2009. ISSN 1613-0073.

[12] A. Ratnaparkhi. Maximum Entropy Models for

Natural Language Ambiguity Resolution. PhD thesis,
University of Pennsylvania, Philadelphia, PA, 1998.

[13] S. Schoenmackers, O. Etzioni, and D. S. Weld. Scaling

textual inference to the web. In EMNLP 08:
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 7988,
Morristown, NJ, USA, 2008. Association for
Computational Linguistics.

[14] M. Ummel. Sea change: Toward a A Snew world AT
semantic enterprise architecture. Cutter IT Journal,
22(11):3439, November 2009.

[15] F. Wu and D. S. Weld. Automatically refining the

wikipedia infobox ontology. In WWW 08: Proceeding
of the 17th international conference on World Wide
Web, pages 635644, New York, NY, USA, 2008.
ACM.

[16] T. Zijlstra, A. Vasconcelos, G. Mentzas, D. Bibikas,

I. Paraskakis, D. Panagiotou, G. Grimnes, and
A. Bernardi. D1.1 state-of-the-art review: Knowledge
management in smes. Deliverable 1.1, OrganiK
Consortium, Leading Partner: USFD, March 26 2009.
Public.

OrganiK reported that they like the system very much to
exchange informal information and document decisions.
6.1 Acknowledgements

We want to thank the European Community Programme
for their support of the OrganiK project under FP7 Grant
222225 Research for the benefit of SMEs. Andreas
Blumauer has exchanged ideas with us, especially on related work, and has given us some insight to a similar
approach from Punkt.at, their Poolparty system (see Section 4). Costas Christidis and Eleni Kargioti are collaborators in the OrganiK project and it is a pleasure to work with
them.
