Tonal MIR: A Music Retrieval Engine Based on Semantic 

Web Technologies

Matteo Magistrali 
Universita dellInsubria  

Via H. J. Dunant 
21100 Varese (IT)  
gratella@libero.it 

Nadia Catenazzi 

University of Applied Sciences of 
Southern Switzerland (SUPSI) 

DTI ISIN, LSMS 

Via Cantonale - Galleria 2 
CH-6928 Manno (CH)  

Lorenzo Sommaruga 

University of Applied Sciences of 
Southern Switzerland (SUPSI) 

DTI ISIN, LSMS 

Via Cantonale - Galleria 2 
CH-6928 Manno (CH)  

nadia.catenazzi@supsi.ch 

lorenzo.sommaruga@supsi.ch 

ABSTRACT 

innovative  approach 

Within  the  Music  Information  Retrieval  context,  this  paper 
describes  an 
to  discovering  music 
similarities.  The  Tonal  MIR  system  has  been  designed  and 
developed  to  provide  a  powerful  and  flexible  music  retrieval 
mechanism  using  semantic  web  technologies.  The  retrieval 
process  is  based  on  an  algorithm  consisting  of  two  main  phases: 
the  preprocessing,  that  converts  an  audio  file  into  an  XML/RDF 
normalized  form;  the  matching  phase,  based  on  inference  rules, 
that compares the normalized music excerpt with the music items 
stored  in  a  database,  and  produces  as  output  a  list  of  results 
ranked according to a similarity degree. 

Categories and Subject Descriptors 

H.3.3  [Information  Storage  and  Retrieval]: Information Search 
and Retrieval  search process. 

General Terms 
Algorithms, Design, Experimentation. 

Keywords 
MIR, Tonal Music, Semantic Web, Inference Rules. 

1.  INTRODUCTION 
With the increasing availability for music in digital form, there is 
the need for innovative tools able to store, describe, classify, and 
retrieve  music.  A  typical  scenario  is  a  user  having  in  mind  a 
recently  heard  melody  (a  television  music  spot,  a  piece  of  music 
listened  to  the  radio),  who  wishes  to  retrieve  the  whole  song,  or 
requires  details  about  the  composer,  the  text,  etc.  Music 
Information  Retrieval  (MIR)  covers  this  and  other  user  music 
information needs [8]. 

The  general  Information  retrieval  term  refers  to  the  set  of 

Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not  made  or  distributed  for  profit  or  commercial  advantage  and  that 
copies  bear  this  notice  and  the  full  citation  on  the  first  page.  To  copy 
otherwise,  or  republish,  to  post  on  servers  or  to  redistribute  to  lists, 
requires prior specific permission and/or a fee. 
I-SEMANTICS 2010, September 1-3, 2010 Graz, Austria 
Copyright  ACM 978-1-4503-0014-8/10/09... $10.00 

to  retrieve 

information  of  different 

techniques  used 
type. 
Typically, a user enters a search query into a retrieval system, that 
will  return  a  list  of  results  matching  the  query  usually  with 
different  relevancy  degrees  [1].  It  is  important  to  broadly 
distinguish  among  two  different  retrieval  strategies  [7]:  exact 
query and query by example. 

Exact  query  is  the  most  common  method,  traditionally  used  in 
relational  databases,  where  classification  (in  this  case  of  audio 
files)  is  based  on  the  title,  the  file  name,  and  various  associated 
information  and  meta-information.  Systems,  that  exclusively 
support exact query, are simple to design and develop, provide 
quick  results,  but  are  limited  to  the  exact  matching  results.  An 
example  of  these  systems  is  Yahoo  Audio  Search  [13].  As  in 
image  search  with  web  search  engines,  this  system  allows  audio 
files to be retrieved in different well known formats (WAV, MP3, 
MIDI,  etc.)  by  specifying  the  file  name  and  the  associated 
metadata, if available, without any specific sound analysis.  

The  query  by  example  retrieval  strategy  is  based  on  the 
comparison  of  an  element  (a  sound  in  our  case),  used  as  query 
input,  with  the  other  available  objects.  This  method  exploits 
content based techniques. Recent systems, such as SHAZAM [12] 
and  TUNATIC  [11],  use  this  strategy:  they  accept  music  excerpt 
as  input  and  return  results  in  a  reasonable  response  time.  Their 
interface  is  intuitive  and  familiar:  the  user  effort  to  interact  with 
the system is minimum. 

The Tonal  MIR system, described in this paper, adopts the query 
by  example  retrieval  strategy.  Tonal  MIR  provides  an  innovative 
and  flexible  music  retrieval  method  using  semantic  web 
technologies.  Music  files,  initially  available  in  a  generic  audio 
format,  are  firstly  converted  in  a  symbolic  format,  and  then 
translated  into  a  normalized  form  through  inference  rules.  A 
matching  process  is  then  applied,  based  on  the  identification  of 
similarities  among  these  normalized  descriptions,  using  other 
inference rules. The retrieval process will iterate such a matching 
process over all music items in a database, returning the similarity 
degree. 

This paper describes the Tonal MIR Engine, indicating its origin, 
describing  the  underlying  algorithm,  and  showing  how  the 
developed prototype works. 


Tonal  MIR  implements  a  query by example retrieval strategy. Its 
peculiar  aspect  with  respect  to  other  similar  works  is  the  use  of 
semantic  web  techniques  in  the  retrieval  process.  It  is  based  on 
previous  works,  where  the  XML  language  and  semantic  web 
technologies were used. These systems are mentioned in the next 
section. 

2.1  The Starting Point: Tonal Music in XML 
XML  is  certainly  the  most  popular  generic  markup  language.  Its 
features  make  it  a  good  formalism  to  semantically  describe  any 
kind  of  data,  including  music.  The  rigid  music  structure  is 
perfectly  adequate  to  be  described  using  the  XML  language. 
Different  standards  have  been defined to represent various music 
aspects.  A  well  known  example  is  MusicXML  [6].  Another 
remarkable  example  is  the  IEEE  1599  standard,  that  defines  a 
multilevel description of music based on XML [2,3]. 

An  interesting  evolution  of  this  work  is  the  definition  of  a 
Semantic Web Based Model for the Tonal System in the Standard 
IEEE 1599 [10]. This allows complex operations such inferences 
to be carried out in the tonal music context. This project is based 
on two basis music elements: notes, with their associated features, 
and intervals, i.e. the distance (in term of frequency) between two 
notes.  It  also  includes  a  number  of  inference  rules  to  enable 
automatic music deduction about chords, etc. 

This  system  provided the foundations for the development of the 
Tonal MIR system, described in the next section.  

2.2  The Tonal MIR retrieval process  
The  Tonal  MIR  system  is  a  music  retrieval  engine  able  to  find 
music  within  a  collection.  It  is  oriented  to  common  users,  i.e. 
people who are not music experts. The engine requires as input a 
digital music excerpt (found or produced by the user), compares it 
with  the  music  files  stored  in  the  repository  and  produces  as 
output  a  list  of  music  files  containing  the  input  excerpt  with  the 
highest  degree  of  similarity,  ranked  according  to  the  similarity 
degree.  

It is worth mentioning that the input excerpt does not necessarily 
have  to  be  extracted  from  the  original  file  (as  in  [5]),  but  it  may 
also be a variation of it in terms of tonality and playing speed or it 
may  be  a  different  version  of  the  original.  Therefore,  one  the 
strengths of the Tonal MIR engine is its ability to discover music 
similarities  even  if  the  input  excerpt  is  seemingly  different  from 
the  original.  This  is  possible  thanks  to  a  normalization  process 
that  extracts  the  global  music  essence  (defined  in  terms  of 
harmonic events, duration and distances between these events) in 
every  music  instant.  In  other  approaches  only  some  features  are 
considered (e.g. timber [9] or melody [4]). 

Another  interesting  aspect  is  that  the  application  scope  is  as 
general as possible; in fact there are no specific constraints on the 
music excerpt (such as the originality of execution, for example), 
because 
itself, 
independently  from  who,  how  and  when  it  was  composed, 
published,  and  performed.  This  information  could  be  captured 
aside  as  metadata  and  shown  to  the  users  once  the  matching 
process is completed. 

into  account 

the  system 

takes 

the  music 

Consequently,  the  system  provides  significant  benefits  for  the 
users,  and  could  also  have  considerable  commercial  interests.  It 
could be used, for example, as a service in an online music store 
or for smart phones (e.g. iPhone). Other systems, such as Shazam, 
may have similar capabilities, but are based on different principles 
and technologies. 

The  functional  scheme  of  the  Tonal  MIR  retrieval  process  is 
shown in Fig. 1. 

Figure 1. Functional scheme of the Tonal MIR retrieval 

process. 

2.2.1  Preprocessing 
The pre-processing phase consists of a number of transformations:  

from  WAV 

this 
conversion 
transformation  is  not  properly  part  of  the  preprocessing 
phase,  it  has  been  included  because  of  the  large  availability 
and  high  fidelity  of  WAV  files.  The  conversion  from  WAV 

to  MIDI; 

although 

to  MIDI  is  carried out  by using  the  AmazingMIDI external 
software (http://www.pluto.dti.ne.jp/~araki/amazingmidi/). 

conversion from MIDI to XML: the MIDI symbolic  format 
(specifically MIDI 0) has been chosen as input format for the 
retrieval process, as it provides all the information needed for 
the music recognition process. This transformation converts a 
MIDI file into an XML file (XMID format), which contains a 
description  of  the  MIDI  events,  through  the Exmid software 
(http://zeusw.org/intl/exmid).  

conversion  from  XML  (XMID)  to  RDF;  the  generated 
RDF contains the same events of the XMID file, temporally 
grouped.  The  purpose  of  this  phase  is  not  to  produce  a 
faithful representation of the RDF music file, but to generate 
a  file  that  can  be  used  for  chord,  rhythm  and  other  music 
feature 
rules.  This 
transformation  merges  musical  events,  expressed  in  the 
XMID file as a sequential representation. 

recognition,  by  means  of 

logic 

2.2.2  Normalization 
It  is  the  most  critical  phase  of  the  retrieval  process.  The  purpose 
of  this  phase  is  to  convert  the  RDF  file  produced  by  the 
preprocessing  phase  into  a  normalized  RDF  file,  which  extracts 
the music essential features for the subsequent matching phase.  

The  features that describe the music essence are represented in 
terms of: 

chord (can be a rest, a single note or a group of notes) 

relative distance between the current and the preview chord 

relative duration between the current and the preview chord 

An excerpt of the RDF normalized file is shown in figure 2 

<?xml version="1.0" encoding="ISO-8859-1"?> 
<!DOCTYPE rdf:RDF [<!ENTITY me 'http://www.supsi.ch/TonalMusic/me#'>]>  

<rdf:RDF xmlns="http://www.supsi.ch/TonalMusic/me#" 
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" 
 xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#" 
 xmlns:owl="http://www.w3.org/2002/07/owl#" 
 xmlns:me="http://www.supsi.ch/TonalMusic/me#"> 

<me:Source rdf:about="test.mid"> 
 <me:text-info rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Music Test</me:text-info> 
 <me:copyright-info rdf:datatype="http://www.w3.org/2001/XMLSchema#string">Bugs   2010</me:copyright-info> 
 <me:author-info rdf:datatype="http://www.w3.org/2001/XMLSchema#string">cippas</me:author-info> 
 </me:Source> 

<meEx:Chord rdf:about="&meEx;he1"> 
 <meEx:chord-type>FourthAugmented</meEx:chord-type> 
 <meEx:distance rdf:datatype="http://www.w3.org/2001/XMLSchema#integer">0</meEx:distance> 
 <meEx:duration rdf:datatype="http://www.w3.org/2001/XMLSchema#float">1</meEx:duration> 
 <meEx:next-event rdf:resource="&meEx;he2"/> 
</meEx:Chord> 

... 

Figure 2. Fragment of an RDF normalized file 

The  normalization  phase  is  carried  out  by  applying  a  number  of 
inference rules. Two aspects are mainly considered: 

chord recognition 

identification  of  the  distance  from  the  chord 
fundamental 

Rules  are  written  using  the  Jena  rule  formalism.  An  example  of 
normalization  rule  that  is  used  for  chord  typology  (Major) and 
chord fundamental recognition is reported below:  

(?A 

?X) 

[MajorChord: 
equal(?X,3)  (?A  me:has-note  ?B)  (?A  me:has-
note  ?C)  (?A  me:has-note  ?D)  (?B  me:third-
major  ?C)  (?C  me:third-minor  ?D)  ->    (?A 
me:Chord "Major") (?A me:fundamental ?B)]   

me:numberOfNotes 

2.2.3  Matching 
It  is  the  core  phase  of  the  retrieval  process.  The  matching  phase 
compares the normalized audio excerpt (produced as output of the 

normalization  phase)  with  all  the  normalized  files  stored  in  the 
system repository, and provides as output a list of matching files, 
with the matching degree. 

The  matching  phase  works  on  RDF  graphs,  that  represent  the 
normalized music files. A node of the graph is a Chord containing 
information such as the chord typology (rest, single note, group of 
notes), etc. 

Two different matching algorithms have been defined: 

a.  Perfect  matching:  every  node  of 
the  excerpt  file 
corresponds  with  the  nodes  of  a  file  in  the  database.  In  the 
comparison, chord, duration and distance must match; 

b.  Matching  with  similar  duration:  it  works  similarly  to 
the  perfect  matching  algorithm,  but  in  the  comparison, 
only  chord  and  distance  are  required  to  match,  while  the 
duration can be similar but not necessary the same;  


An example of rule that implements perfect matching algorithm 
is reported below: 

[Base:  (?A  meEx:chord-type  ?B)  (?C  me:chord-type 
?B)  (?A  meEx:distance  ?D)  (?C  me:distance  ?D) (?A 
meEx:duration  ?X)  (?C  me:duration  ?X)  ->  (?A 
matches-tmp-with degree(1 ?C))] 

This  rule  assigns  a  temporal  matching  degree  equal  to  1  if  the 
event  of  musical  event  A  has  the  same  duration  and  distance  of 
the candidate in the database. Two different namespaces are used 
to  distinguish  elements  belonging  to  the  excerpt  and  elements 
coming from the database. 

2.3  The Tonal MIR prototype  
The  user  may  interact  with  the  Tonal  MIR  system  in  two 
different  ways:  inserting  a  file  in  the  database  and  searching  a 
music excerpt in the database. 

To  add  a  new  music  file  to  the  repository,  the  user  has  to  select 
and  upload  it  from  his/her  local file system. Once uploaded, it is 
possible to listen to it. When a new file is inserted in the database, 
it is processed, as explained in figure 1, to produce a normalized 
version useful later for the matching phase. 

During  the  search process, the user is asked to insert a query, by 
selecting  an  excerpt  file  and  activating  the  search  button. 
Advanced  options  are  available  to  define  the  matching algorithm 
and  the  filtering  options.  The  user  may  choose  among  the 
different  matching  algorithms,  and  define  a  minimum  matching 
threshold, in order to filter out those results with a similarity rate 
lower  than  the  threshold.  Once  the  excerpt  has  been  loaded  (see 
Fig. 3) and pre-processed to convert it into the normalized format, 
the matching phase will produce the list of results (see Fig. 4). 

Figure 3. Searching process: the excerpt file is loaded 

Figure 4. Searching process: a list of results is returned 

The  system  has  been  implemented  as  a  web  based  application, 
based  on  the  Jena  toolkit  (http://jena.sourceforge.net/),  an  open 
source Java framework to develop semantic web applications. The 
RDF  files  produced  as  output  of  the  pre-processing  phase  are 
based  on  an  OWL  ontology,  which  is  an  extension  of  the  basic 
Tonal  Music  Ontology  [10].  The  core  of the matching process is 
based on the application of inference rules.  

The  prototype  is currently working using Jena rules and the Jena 
generic rule reasoner.  

3.  EVALUATION AND CONCLUSIONS 
A  preliminary  evaluation  has  been  conducted  to  prove  that  the 
system  works  correctly.  A  number  of  testing  cases  have  been 
defined  and  carried  out  in  order  to  evaluate  the  advantages  and 
benefits  of  the  Tonal  MIR  approach,  highlighting  also  some 
potential issues. 

At  this  purpose,  a  repository  of  about  140  files  of  melodic  and 
polyphonic  music  has  been  created,  and  different  music  excerpts 
have  been  chosen  with  different  objectives:  to  demonstrate  that 
the system is able 

to  retrieve  a  music  file  when  the  music  excerpt  exactly 
matches; 

to retrieve a music file when the music excerpt matches 
but has a different tonality and/or playing speed; 

to recognize similarities even when the human ear is not 
able  to  perceive  them  (ability  to  identify  potential 
plagiarism). 

The average size of the music files and excerpts was under 10KB, 
small  enough  for  guaranteeing  reasonable  response  time,  but 
sufficiently  representative  of  the  music  content in the case of the 
excerpt (e.g. melody or harmony). Moreover, the music pieces are 
different  in  type,  historical  period,  authors  and  style.  This  set 
represents  a  sufficiently  representative  sample  for  our  testing 
purposes thanks to their different characteristics. 

The  Tonal  MIR  has  been  tested  for  the  first  two  matching 
algorithms  on  various  music  excerpts.  When  the  excerpt  was 
extracted from a file present in the repository, the matching degree 
was always 100%, as expected, independently from the matching 
algorithm and the music excerpt.  

A  number  of  tests  have  considered  a  transposed  music  excerpt 
(e.g.  xurelise-TRANSPOSED.mid)  represented  with  slower  time  
and further transposed in tonality (e.g. to SIb minore) (see Fig. 5). 
Also in this case the matching result was 100% considering both 
algorithms. 

The Tonal MIR Engine is able to retrieve a music file containing a 
chord sequence similar to the music excerpt that is given as query 
input. It also identifies music files that present different similarity 
degrees with the query input. 

Finally,  the  Tonal  MIR  engine  may  also  have  interesting 
commercial perspectives, as a service to be provided for instance 
in an online music store or smart phones. 

4.  ACKNOWLEDGMENTS 
Our thanks to Denis Baggi for his suggestions and contribution in 
the initial phase of the project. 
