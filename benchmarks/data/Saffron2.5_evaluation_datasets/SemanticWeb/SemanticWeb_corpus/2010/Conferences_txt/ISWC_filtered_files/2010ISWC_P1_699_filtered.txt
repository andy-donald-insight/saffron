OWL-POLAR: Semantic Policies for Agent Reasoning

Murat S  ensoy1, Timothy J. Norman1, Wamberto W. Vasconcelos1, and Katia Sycara1,2

1 Department of Computing Science, University of Aberdeen, AB24 3UE, Aberdeen, UK

{m.sensoy,t.j.norman,w.w.vasconcelos}@abdn.ac.uk

2 Carnegie Mellon University, Robotics Institute, Pittsburgh, PA 15213, USA

katia@cs.cmu.edu

Abstract. Policies are declarations of constraints on the behaviour of components within distributed systems, and are often used to capture norms within agentbased systems. A few machine-processable representations for policies have been
proposed, but they tend to be either limited in the types of policies that can be
expressed or limited by the complexity of associated reasoning mechanisms. In
this paper, we argue for a language that sufficiently expresses the types of policies
essential in practical systems, and which enables both policy-governed decisionmaking and policy analysis within the bounds of decidability. We then propose an
OWL-based representation of policies that meets these criteria using and a reasoning mechanism that uses a novel combination of ontology consistency checking and query answering. In this way, agent-based systems can be developed that
operate flexibly and effectively in policy-constrainted environments.

1 Introduction

In this paper, we present a novel and powerful OWL 2.0 [7] knowledge representation and reasoning mechanism for policies: OWL-POLAR (an acronym for OWL-based
POlicy Language for Agent Reasoning). Policies (aka. norms) are system-level principles of ideal activity that are binding upon the components of that system. Depending on
the nature of the system itself, policies may serve to control, regulate or simply guide
the activities of components. In systems security, for instance, the aim is typically to
control behaviour such that the system complies with the policies [18]. In real sociotechnical systems, however, there are important limits to this and the aim is to develop
effective sets of policies along with incentives to regulate behaviour [2]. In systems of
autonomous agents, the term norm is most prevalent, but the concept and issues remain
the same [4]; for example, norms are used to regulate the behaviour of agents representing disparate interests in electronic institutions [6]. The objective of this research is
to capture the essential requirements of policy representation and reasoning. In meeting
this objective three key requirements must be met:

 This research was sponsored by the U.S. Army Research Laboratory and the U.K. Ministry
of Defence and was accomplished under Agreement Number W911NF-06-3-0001. The views
and conclusions contained in this document are those of the author(s) and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Army
Research Laboratory, the U.S. Government, the U.K. Ministry of Defence or the U.K. Gov-
ernment. The U.S. and U.K. Governments are authorized to reproduce and distribute reprints
for Government purposes notwithstanding any copyright notation hereon.

P.F. Patel-Schneider et al. (Eds.): ISWC 2010, Part I, LNCS 6496, pp. 679695, 2010.
c Springer-Verlag Berlin Heidelberg 2010

M. S  ensoy et al.

1. System/institutional policies must be machine understandable and underpinned by

a clear interpretation.

2. The representation must be sufficiently expressive to capture the notion of a policy

across domains.

3. Policies must be able to be effectively shared/interpreted at run-time.
The choice of OWL 2.0 as an underlying language addresses the first requirement, but
in meeting the second two, we must clearly outline what is required of a policy language
and what reasoning should be supported by it. The desiderata of a model of policies that
motivates the language OWL-POLAR are as follows:

 Representational adequacy. Policies (or norms) must capture the distinction between activities that are required (obliged), restricted (prohibited) and, in some
way, authorised but not necessarily expected (permitted) by some representational
entity within the environment. It is essential to capture the authority from which
the policy/norm comes, the subject (agent) to whom it applies, the object (activity)
to which the policy/norm refers, and the circumstances within which it applies.

 Supporting decisions. Any reasoning mechanism that is driven/guided by policies
must support both the determination of what policies/norms apply in a given situa-
tion, and what activities are warranted by the normative state of the agent if it were
to comply with these policies.

 Supporting analysis. Any reasoning mechanism that is driven/guided by norma-
tive/policy constraints must support the assessment of policies in terms of: (i)
whether a policy/norm is meaningful and (ii) whether norms conflict, and in what
circumstances they do conflict.

With the introduction of data ranges within the OWL 2.0 specification [7], we believe
that this desiderata of a model of policies can be met within the confines of OWL-DL.
If this claim can be shown to be valid (as we aim to do within this paper), we believe
that OWL-POLAR provides, for the first time, a sufficiently expressive policy language
for which the key reasoning mechanisms required of such a language are decidable.

The paper is organised as follows: in Section 2 we formally specify the OWLPOLAR language within OWL-DL; in Section 3 we describe how a set of active policies
may be computed, and how decisions about what activities are warranted by some set
of policies may be made; then in Section 4 we present in detail the reasoning mechanisms that support the analysis of policies. OWL-POLAR is then compared to existing
languages for policies in Section 5, and we present our conclusions in Section 6.

2 Semantic Representation of Policies

The proposed language for semantic representation of policies is based on OWL-DL [7].
An OWL-DL ontology o = (T Boxo, ABoxo) consists of a set of axioms defining the
classes and relations (T Boxo) as well as a set of assertional axioms about the individuals in the domain (ABoxo). Concept axioms have the form C  D where C and D are
concept descriptions, and relation axioms are expressions of the form R  S, where
R and S are relation descriptions. The ABox contains concept assertions of the form
C(a) where C is a concept and a is an individual name, and relation assertions of the
form R(a, b), where R is a relation and a and b are individual names.
?

?

?
v  Ro

v  Co

v =

Conjunctive semantic formulas are used to express policies. A conjunctive semantic

n
formula F o
i=0 i over an ontology o is a conjunction of atomic assertions i,
where v = ?x0, . . . , ?xn represents a vector of variables used in these assertions.
i=0 i  {1, . . . n} in order to consider a
For the sake of convenience, we assume
conjunctive formula as a set of atomic assertions. Based on this, F o
v can be considered
as T o
v is a set of type assertions using the concepts from o, e.g.,
{student(?xi), nurse(?xj)}; Ro
v is set of of relation assertions using the relations from
o, e.g., {marriedT o(?xi, ?xj)}; Co
v is a set of constraint assertions on variables. Each
constraint assertion is of the form ?xi  , where  is a constant and  is any of the
symbols {>, <, =,=,,}. A constant is either a data literal (e.g, a numerical value)
or an individual defined in o.

v, where T o

n

Variables are divided into two categories; data-type and object variables. A data-type
variable refers to data values (e.g., integers) and can be used only once in Ro
v. On the
other hand, an object variable refers to individuals (e.g., University of Aberdeen) and
can be used freely many times in Ro
v. Equivalence and distinction between the values
of object variables can be defined using OWL properties sameAs and differentFrom
respectively, e.g., owl:sameAs(?x,?y). In the rest of the paper, we use the symbols , ,
, and e as a short hand for semantic formulas.
Given an ontology o, a conditional policy is defined as   N: (a : ) /e, where
1. , a conjunctive semantic formula, is the activation condition of the policy.
2. N  {O, P, F} indicates if the policy is an obligation, permission or prohibition.
3.  is the policy addressee and  describes  using only the role concepts from the

ontology (e.g., ?x : student(?x)f emale(?x), where student and female are defined
as sub-concepts of the role concept in the ontology). That is,  is of the form
i=0 ri(), where ri  role. Note that  may directly refer to a specific individual
(e.g., John) in the ontology or a variable.
4. a :  describes what is prohibited, permitted or obliged by the policy. Specifically,
a is a variable referring to the action to be regulated by the policy and  describes a
as an action instance using the concepts and properties from the ontology (e.g., ?a :
SendF ileAction(?a)  hasReceiver(?a, John)  hasF ile(?a, T echReport218.pdf),
where SendFileAction is an action concept). Each action concept has only a number
of functional relations (aka. functional properties) [7] and these relations are used
while describing an instance of that action.

n

5. e defines the expiration condition.

Table 1 illustrates how a conditional policy can be represented using the proposed ap-
proach. The policy in the table states that a person is obliged to leave a location when
there is a fire risk.

Table 1. A person has to leave a location when there is a fire risk

P lace(?b)  hasF ireRisk(?b, true)  in(?x, ?b)

?

?

?
 : 
?x : P erson(?x)
a :  ?a : LeavingAction(?a)  about(?a, ?b)  hasActor(?a, ?x)
hasF ireRisk(?b, f alse)

e

M. S  ensoy et al.

Given a semantic representation for the state of the world, policies are used to reason
about actions that are permitted, obliged or prohibited. Let o be a semantic representation for a state of the world based on an ontology o. Each state of the world is partially
observable; hence o is a partial representation of the world. o itself is represented as
an ontology composed of (T Boxo, ABox) where ABox is an extension of ABoxo.

3 Reasoning with Policies

When its activation conditions are satisfied, a conditional policy leads to an activated
policy. Definition 1 summarizes how a conditional policy is activated using ontological
reasoning over a state of the world. Here we use query answering to determine activated policies and reason about actions. The query answering mechanism we use in this
work is DL-safe; i.e. variables are bound only to the named individuals, to guarantee
decidability [8]. In this section, we address some of the key issues in supporting decisions governed by policies: activation and expiration, and reasoning about interactions
between policies and actions.

Definition 1. Let o be a state of the world represented based on a domain ontology
o. If there is a substitution  such that o  (  )  , but there is no substitution 

such that o  (e  )  
, then the policy (N (a : ))   becomes active. This policy
expires when there exists a substitution 

such that o  (e  )  
?

?

?
.
?

?

?
marriedTo

hasActor

in

Person

inChargeOf

Action

isA

LeavingAction

Patient

isA

hasAge

isA

Doctor

Place

in

isA

isA

Room

hasFireRisk

isA

xsd:boolean

Building

isA

in

Hospital

xsd:int

xsd:boolean

hasPatient

HospitalRoom

type

Jane

type

type

John

in

type

CentralHospital

inChargeOf

Room245

in

in

hasFireRisk

true

xsd:time

hasValue

CurrentTime

type

t

hasValue

13:00:00

Fig. 1. A partial state of the world represented based on a domain ontology

3.1 Policy Activation

A policy is activated for a specific agent when the world state is such that the activation
condition holds for that agent and the expiration condition does not hold, and expires
when this latter condition holds. The above definition is rather standard [11], but we
now describe how this is implemented efficiently through query answering. A conjunctive semantic formula can be trivially converted to a SPARQL query [15] and can be
evaluated by OWL-DL reasoners with SPARQL-DL [17] support such as Pellet [17] to
?

?

?
find a substitution for its variables satisfying a specific state of the world. Therefore,
we can test o  (  )   by writing a query for (  ) and testing whether it is
entailed by o or not. Consider the conditional policy in Table 1 and assume that we
have the partially represented state of the world in Figure 1. We can write the semantic
query in Figure 2 to find  for the conditional policy. When we query the state of the
world using SPARQL, each result in the result set provides a substitution ; in our case,
we have two  values: {?x/John, ?b/Room245} and {?x/Jane, ?b/Room245}, representing
that there is a fire risk in the room 245 of the Central Hospital and that John and Jane
are in that room.

Query:

SPARQL SYNTAX:

q(?x, ?b):-

Place(?b) 
hasFireRisk(?b, true) 
Person(?x) 
in(?x,?b).

PREFIX example: <http://www.example.com/ns#>
PREFIX rdf: <http://www.w3.org/...rdf-syntax-ns#>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
SELECT ?x ?b
WHERE {

?b rdf:type example:Place.
?b example:hasFireRisk "true"xsd:boolean.
?x rdf:type example:Person.
?x example:in ?b.

}

Fig. 2. Query for the activation of a policy
?

?

?
Now, using the computed  values, we should try to find a 

such that o 
(e  )  
. In our case, for this purpose, we can use the semantic query q():- has-
FireRisk(Room245, false). When the SPARQL representation of this query is executed
over the state of the world shown in Figure 1, it returns false; that is the RDF graph
pattern represented by the query could not be found in the ontology. This means that
the policy in Table 1 should be activated using the variable bindings in . The result is
activations of OJohn(?a : LeavingAction(?a)  about(?a,Room245)) and OJane(?a : Leav-
ingAction(?a)  about(?a,Room245)). These policies mean that John and Jane are obliged
to leave the room 245; the obligation expires when the fire risk is removed.

3.2 Reasoning about Actions
?

?

?
will be performed by x, where a

 : 

Let us assume that a specific action a
is a URI
?

?

?
referring to the action instance and 
is a conjunctive semantic formula describing a
without using any variables. Let o be the current state of the world. We can test if the
action a
is permitted, forbidden or prohibited in o. For this purpose, based on o,

o to make what-if reasonwe create a sandbox (hypothetical) state of the world 

ing [21], i.e., 
o shows what happens if the action is performed. This is achieved by
o = o
?

?

?
simply adding the described action instance to o, i.e., 
. For example, the
state of the world in Figure 1 is extended using action instance LeaveAct 1: LeavingAc-
tion(LeaveAct 1)  hasActor(LeaveAct 1,John)  about(LeaveAct 1,room245). The resulting
state of the world is shown in Figure 3.

M. S  ensoy et al.

Action

isA

LeavingAction

Patient

marriedTo

hasActor

in

Person

inChargeOf

isA

hasAge

isA

Doctor

xsd:int

type

type

Place

in

isA

isA

Room

hasFireRisk

isA

xsd:boolean

Building

isA

xsd:boolean

hasPatient

HospitalRoom

in

Hospital

type

type

LeaveAct_1

Jane

John

in

type
in

CentralHospital

hasActor

in

about

inChargeOf

Room245

hasFireRisk

true

Fig. 3. The sandbox (hypothetical) state of the world

xsd:time

hasValue

CurrentTime

type

t

hasValue

13:00:00

 : 
?

?

?
For each active policy Nx(y : y), we test the expiration conditions on 
o as explained before. If the policys expiration conditions are satisfied, we can conclude that
leads to the expiration of the policy. Otherwise, a semantic query Q of
the action a

the form q(vy):- y is created, where vy is the vector of variables in y. Then, 
o is
queried with Q. Let the query return a result set rs; each result r  rs is a substitution

is regulated by the policy. In
such that 
o
this case, we can interpret the policy based on its modality as follows:
1. Nx = O: In this case, the policy represents an obligation; that is, x is obliged to

 y  r. If y  r = a

for any such r, then a
?

?

?
perform a

. Performing a

2. Nx = P : Performing a

3. Nx = F : Performing a

will remove this obligation.
is explicitly permitted.
is prohibited.

After examining the active policies as described above, we can identify a number of
may be
possible normative positions with respect to the action instance a
explicitly permitted if there is a policy permitting it; (ii) doing a
may be obligatory
may be prohibited if there is a policy
if there exists a policy obliging it; (iii) doing a
prohibiting it; and (iv) there may be a conflict in the normative position with respect to

if it is either both prohibited and explicitly permitted, or both prohibited and obliged.
a

: (i) doing a
?

?

?
4 Reasoning about Policies

In this section, we demonstrate reasoning techniques to support the analysis of policies
in terms of their meaningfulness (Section 4.2) and possibility of conflict (Section 4.3),
and hence address our third desideratum. Prior to this, however, we propose methods
for reasoning about semantic formulas to underpin our mechanisms for policy analysis.

4.1 Reasoning about Semantic Formulas

Here, we introduce methods for reasoning about semantic conjunctive formulas using
query freezing and constraint transformation.
?

?

?
Conjunctive Queries. There is a relation between conjunctive formulas and conjunctive queries. A conjunctive semantic formula can trivially be converted into a conjunctive semantic query. For example, Ao
v1.
Therefore, we can use query reasoning techniques to reason about semantic formulas.
For instance, in order to reason about the subsumption between semantic formulas, we
can use query subsumption (containment).

v1 can be converted into the query qA():- Ao

In conjunctive query literature, in order to test whether qA subsumes qB, the standard technique of query freezing is used to reduce query containment problem to query
answering in Description Logics [13,20]. For this purpose, we build a canonical ABox
qB from the query qB():- Bo
v2 in three steps. First, for each variable in v2, we put
a fresh individual into qB using the type assertions about the variable. Note that this
individual should not exist in o. Second, we add each individual appearing in qB into
qB . This is done using the information about the individual from the ABoxo (e.g.,
type assertions). Third, relationships between individuals and constants defined in qB
are inserted into qB . As a result of this process, qB contains a pattern that exists
only in ontologies that satisfy qB. We combine qB and our T Boxo to create a new
 = (T Boxo, qB ). Example 1 demonstrates a simple case. Based
canonical ontology, o
on [20,13], we conclude that o  qB  qA if and only if o

entails qA. In order to test

whether o
entails qA if there exists at least one

match for qA in o
. This can easily be achieved by converting qA to SPARQL syntax

and use Pellets SPARQL-DL query engine to answer qA on o
Example 1. Let query qA be q():- Person(?p)  marriedTo(?p,?x)  Patient(?x) and
query qB be q():- Doctor(?x)  marriedTo(?x,Jane)  hasChild(?x,?c). Then, qB contains an individual x, which is created for the variable ?x. The individual x is defined
as of type Doctor. In qB , we also have another individual Jane, which is defined in
the original ABoxo as an instance of the Patient class; we get all of its type assertions
from the ABoxo. Then, we insert the object property marriedTo between the individuals x and Jane. Lastly, we create another individual c for the variable ?c in qB and
insert the hasChild object property between x and c. The resulting ontology is shown
in Figure 4.
?

?

?
entails qA or not, we query o
?

?

?
. That is, o

[17].

hasChild

marriedTo

hasActor

Action

Person

isA

LeavingAction

isA

hasAge

Patient

xsd:int

isA

Doctor

type

type

Jane

type

marriedTo

hasChild

Fig. 4. The ontology created for qB in Example 1

M. S  ensoy et al.

<owl:Class rdf:about="#AgeConst1">

<rdfs:subClassOf>

<owl:Restriction>

<owl:onProperty rdf:resource="#hasAge"/>

<owl:allValuesFrom>

<rdfs:Datatype>

<owl:onDataRange rdf:resource="&xsd;nonNegativeInteger"/>
<xsd:minInclusive rdf:datatype="&xsd;int">10</xsd:minInclusive>
<xsd:maxExclusive rdf:datatype="&xsd;int">20</xsd:maxExclusive>

</rdfs:Datatype>

</owl:allValuesFrom>

</owl:Restriction>

</rdfs:subClassOf>

</owl:Class>

Fig. 5. A concept named AgeConst1 is created for hasAge(?c, ?a)  ?a  10  ?a  20

The query freezing method described above enables us to create a canonical ABox
for a semantic conjunctive formula; this ABox represents a pattern which only exists in
ontologies satisfying the semantic formula. On the other hand, this method assumes that
variables in queries can be assigned fresh individuals in a canonical ABox. However,
in OWL-DL, individuals can refer to objects, but not data values [19]. Therefore, the
proposed query freezing method can be used to test for subsumption between qA and
qB only if the variables in qA and qB refer to objects. A variable can refer to an object
if it is used as the domain of an object or datatype property (e.g., hasAge(?x,10)) or if
it is used as the range of an object property (e.g., marriedTo(Jack,?x)). Unfortunately, in
many real-life settings, queries may have variables referring to data values with various
constraints, which we refer to here as datatype variables. In these settings, the query
freezing described above cannot be used to test subsumption. Example 2 illustrates a
simple scenario.
Example 2. Let query qA be q():- Person(?p)  hasChild(?p,?c) hasAge(?c,?y) ?y
 12  ?y  16 and query qB be q():- Doctor(?x)  marriedTo(?x,Jane) hasChild(?x,
?c)  hasAge(?c,?a)  ?a  10  ?a  20. In this example, the query freezing method
cannot be used directly to test subsumption between qA and qB, because the variables
?y and ?a refer to data values, which cannot be represented by individuals in an OWLDL ontology.

Constraint Transformation. Here, we propose constraint transformation. It is a preprocessing step which enables us to create a canonical ABox for semantic formulas
with datatype variables. Note that a datatype variable is used in a semantic formula to
constrain one datatype property, e.g., ?y is used to constrain the hasAge datatype property in qA of Example 2. Constraint transformation in contrast uses data-ranges introduced in OWL 2.0 [7] to transform each constrained datatype property to a named OWL
class. As a result, datatype variables and related datatype properties and constraints are
replaced with type assertions. This procedure is detailed in Algorithm 1.

The algorithm takes a conjunctive semantic formula F o

v and the ontology o as inputs
(line 1). F o
v are sets of type, relation
and constraint assertions respectively. The outputs of the algorithm are the transformed

v is of the form T o

v Ro

vCo

v, where T o

v , Ro

v, and Co
?

?

?
Algorithm 1. Constraint transformation
1: Inputs:

v  T o

v  Ro

v  C o
v ,

Formula F o
Ontology o  (ABoxo, T Boxo)
Ontology   (ABoxo, T Box)
v , T Box = T Boxo

u = T o
v ) do

if (isDatatypeV ariable(b)) then
d = getConstraints(b, C o
v )
c = createConcept(r, d, T Box)
 = createT ypeAssertion(a, c)
F 

u = F 

2: Outputs: Formula F 
u ,

3: Initialization: F 
4: for all (r(a, b)  Ro
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
end if
16:
17: end for

F 
end if

u = F 

else

u  
u  r(a, b)

u = F 
F 
v)
b = getConstraints(b, C o
if (b =  & (b  F 
u )) then

u  b

semantic formula F 
u (containing no datatype variables) and the updated ontology 
u is set as equal to T o
(line 2). Initially, F 
v and  is the same as o (line 3). For each
relation assertion r(a, b) in Ro
v, we do the following (line 4). First, we check if b is a
datatype variable (line 5). If so, this means that r is a datatype property with a variable
in its range. In this case, we extract the set of constraints related to b from Co
v, which is
referred by d (line 6). Based on r and d, we create a concept c in T Box using the
createConcept function (line 7). This function works as follows:
1. If d = , then b implies some restrictions on the range of r. In this case, c should
refer to objects that have the property r with the restrictions defined in d on
its range. While creating c in T Box, we use data-ranges1 introduced in OWL
2.0 to restrict the range of r accordingly. For example, if r(a, b) corresponds to
hasAge(?c, ?a) and d = {?a  10, ?a  20}, then a concept named AgeConst1
can be described as shown in Figure 5. For more sophisticated constraints, we
create more complex class expressions using the OWL constructors owl:unionOf,
owl:intersectionOf, and owl:complementOf.
2. If d = , then b has no constraints, which means that the data-range of b is
equivalent to the range of its data-type (i.e., for xsd:int, the range is min inclusive
2147483648 and max inclusive 2147483647).

After creating the concept c in T Box, we create a type assertion  to declare a as an
instance of c (e.g., AgeConst1(?c)) (line 8). This type assertion is added to F 
u in order to
substitute r(a, b) and d in F o
v (line 9). On the other hand, if b is not a datatype variable
(line 10), there are two possibilities: (1) r is a datatype property but b is not a variable,
or (2) r is an object property. In both cases, we directly add r(a, b) to F 
u (line 11). If b

1 http://www.w3.org/TR/2008/WD-owl2-syntax-20081008/#Data_Ranges

M. S  ensoy et al.

hasChild

hasActor

Action

marriedTo

Person

isA

isA

hasAge

isA

type

LeavingAction

Patient

Doctor

AgeConst2

xsd:int

AgeConst1

type

Jane

type

type

marriedTo

hasChild

Fig. 6. The Ontology created for qB in Example 2

has constraints defined in Co
are not already added (lines 12-15).

v, we extract these constraints and add them to F 

u if they

In order to test subsumption between qA and qB in Example 2, we should transform
the bodies of these queries and update the ontology they are based on. For this purpose,
we use constraint transformation twice. That is, we first update the ontology by adding
the concept AgeConst1 to handle hasAge(?c,?y)  ?y  10  ?y  20 and transform qB
to q():- Doctor(?x)  marriedTo(?x,Jane)  hasChild(?x,?c)  AgeConst1(?c). Then, we add
concept AgeConst2 to the ontology to handle hasAge(?c,?y)  ?y  12  ?y  16 and
transform qA to q():- Person(?p)  hasChild(?p,?c)  AgeConst2(?c). After this preprocessing step, we use query freezing to test qB  qA; the ontology with a canonical ABox
created during query freezing is shown in Figure 6.

With these techniques in place, we are now in a position to address the issue of policy

analysis supported by OWL-POLAR. It is descried in the following sections.

4.2 Idle Policies

A policy is idle if it is never activated or the policys expiration condition is satisfied
whenever the policy is activated. This condition is formally described in Definition 2. If
a policy is idle, it cannot be used to regulate any action, because either it never activates
or whenever it activates an obligation, permission, or prohibition about an action, the
activated policy expires. While designing policies, we may take domain knowledge into
account to avoid idle policies.
Definition 2. A policy   N: (a : ) /e is an idle policy if it does not activate
such that o  (e  )  

,
for any state of the world o or there is a substitution 
whenever there is a substitution  such that o  (  )  .
Let us demonstrate idle policies with a simple example. Assume that object property
hasParent is an inverse property of hasChild. Also, let us assume in the domain on-
tology, we have a SWRL rule such as hasSponsor(?c, true)  hasP arent(?c, ?p) 
hasAge(?c, ?age)  ?age < 18, which means that children under 18 have a sponsor if
they have a parent. Now, consider the policy in Table 2. This policy is activated when a
person ?p has a child ?c, which is a student under 18. The activated policy expires when
?

?

?
Table 2. A simple idle policy example

hasChild(?p, ?c)  Student(?c) hasAge(?c, ?age)?age < 18

?

?

?
 : 
?p : P erson(?p)
a :  ?a : P ayT uitionsOf Student(?a)  about(?a, ?c)  hasActor(?a, ?p)
hasSponsor(?c, true)

e

?c has a sponsor. Interestingly, whenever the policy is activated, the domain knowledge
implies that ?c has a sponsor. That is, whenever the policy is activated, it expires.
In order to detect idle policies, we reason about the activation and expiration conditions of policies. Specifically, a policy   N: (a : ) /e is an idle policy if (  )
is unrealistic or implies e using the knowledge in the domain ontology. More formally,
we can show that the policy is idle if we show (  ) never holds or (  )  e. This
can be achieved as follows. First, we freeze (  ) and create a canonical ontology o

.

is not a consistent ontology, then we can conclude that the policy is
If the resulting o
an idle policy, because (  ) never holds. Let o

be consistent and  be a substitution
denoting the mapping of variables in (  ) to the fresh individuals in o

. If there exists
, we conclude that (  )  e. We can test
a substitution 
  (e  )  

o

  (e  )  
such that o

by querying o
?

?

?
with q() :  (e  ).

4.3 Anticipating Conflicts between Policies

In many settings, policies may conflict. In the simplest case, one policy may prohibit
an action while another requires it. There are, however, many less obvious interactions
between policies that may lead to logical conflicts [9,16,12,5]. Further developing our
earlier example, consider the policy presented in Table 3 that states that a doctor cannot
leave a room with patients if he is in charge of the room. This policy conflicts with the
policy in Table 1 under some specific conditions. For example, in the scenario described
Figure 1, room 245 of Central Hospital has a fire risk and Dr. John is in charge of the
room, in which there are some patients. In this setting, the policy in Table 1 obligates
Dr. John to leave the room while the policy in Table 3 prohibits this action until the
room has no patient.

Table 3. A doctor cannot leave a room containing patients if he is in charge of the room

 Room(?r)  hasP atient(?r, true)  inChargeOf(?d, ?r)
?

?

?
 : 
?d : Doctor(?d)
a :  ?x : LeavingAction(?x)  about(?x, ?r)  hasActor(?x, ?d)
hasP atient(?r, f alse)

e

If we can determine possible logical conflicts while designing policies, we can create
better policies that are less likely to raise conflicts at run time. Furthermore, we can use
various conflict resolution strategies such as setting a priority ordering between the
policies to solve conflicts [11,21,22], once we determine that two policies may conflict.

M. S  ensoy et al.
?

?

?
In this section, we propose techniques to anticipate possible conflicts between
policies at design time. Suppose we have two non-idle policies Pi = i  Ai:i
ai : i
/ej. These policies are active for the
same policy addressee in the same state of the world  if the following requirements
are satisfied:

/ei and Pj = j  Bj :j
?

?

?
(i)  
(ii)  

aj : j
?

?

?
i such that  
j such that  
?

?

?
  
  
?

?

?
i
?

?

?
j

ei  i
ej  j

  i, but no 
  j, but no 
?

?

?
i  i
j  j

(iii) i  i = j  j
The policies Pi and Pj conflict if the following requirements are also satisfied:
(iv) (i  i)  (j  j) or (j  j)  (i  i)
(v) A conflicts with B. That is, A  {P, O} while B  {F} or vice versa.
We can use Algorithm 2 to test if it is possible to have such a state of the world where
Pi conflicts with Pj. The first step of the algorithm is to test if A conflicts with B (line
2). If they are conflicting, we continue with testing the other requirements. We create
with a
a canonical state of the world  in which Pi is active by freezing
to the fresh individuals in . Given
substitution i mapping the variables in
that (j  )  j for any substitution  mapping variables into individuals, the requirement (iv) implies that (i  i)  j. We test this as follows. First, we create a

with j (line 5).
canonical ontology o
Each answer to this query defines a substitution k mapping variables in j into the
terms in (i  i), so that (i  i)  (j  k). If j does not have any variable but

it repeats in o
as a pattern, the result set contains only one empty substitution. If the
query fails, the result set is an empty set (), which means that it is not possible to have
a k such that (i  i)  (j  k). For each k satisfying (i  i)  (j  k), we test

by freezing (i  i) (line 4) and then query o
?

?

?
i  i

i  i
?

?

?
/ei,
/ej

ai : i
aj : j
?

?

?
Policy Pi = i  Ai:i
Policy Pj = j  Bj :j

, i = f reeze(i  i)
,  = f reeze(i  i)
o
?

?

?
, j)
= query(o
rs
for all (k  rs) do
, j = update(,
if (isConsistent()) then

Algorithm 2. An algorithm to anticipate if Pi may conflict with Pj
1: Inputs:
2: if ( (A  {O, P} and B  {F}) or (A  {F} and B  {O, P}) ) then
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14: end if
15: return false

if (query(, ei  i) =  and query(,

  k)

  j) = ) then

end if
end for

j  j

ej  k

return true

end if
?

?

?
OWL-POLAR: Semantic Policies for Agent Reasoning

k without
?

?

?
  k to the new fresh
j  j
the other requirements as follows. First, we update  by freezing

 j. We test the consistency of
removing any individual from its existing ABox (line 7). Note that as a result of this
j  j
process, j is the substitution mapping the variables in
individuals in the updated , so that i  i =
j  k
the resulting state of the world  (line 8). If this is not consistent, we can conclude that
it is not possible to have a state of the world satisfying the requirements. If the resulting
 is consistent, we check the expiration conditions of the policies. If both are active in
the resulting state of the world (line 9), the algorithm returns true (line 10). If any of
these requirements do not hold, the algorithm returns false (line 15).

As described above, the algorithm transforms the problem of anticipating conflict
between two policies into an ontology consistency checking problem. To check the
consistency of the constructed canonical state of the world , we have used the Pellet [17] reasoner. This reasoner adopts the open world assumption and does not have
Unique Name Assumption (UNA). Hence, it searches for a model2 of , also considering the possible overlapping between the individuals (i.e., individuals referring the
same object). If there is no model of , it is not possible to have a state of the world
satisfying the requirements stated above. We should also note that, while anticipating
the conflict, Algorithm 2 tests only the case (i  i)  (j  j). However, we also
need to test (j  j)  (i  i) to capture the possibility of conflict. Therefore, if the
algorithm returns false, we should swap the policies and run the algorithm again. If it
returns true with the swapped policies, we can conclude that there is a state of the world
where these policies may conflict.
?

?

?
To demonstrate the algorithm, let us use the policies presented in Tables 1 and 3 and
refer to them as Pi and Pj respectively. In this example, Pj is a prohibition while Pi
is an obligation, so the algorithm proceeds as follows (line 2). We create a canonical
state of the world  by freezing P erson(?x)  P lace(?b)  hasF ireRisk(?b, true)
 in(?x, ?b) with a substitution i = {?x/x, ?b/b} (line 3). Now we create a canonby freezing i  i with substitution {?a/a} (line 4). This ontology
ical ontology a
has the following ABox assertions: LeavingAction(a), about(a, b), hasActor(a, x).
with LeavingAction(?x)  about(?x, ?r)  hasActor(?x, ?d) (line 5).

We query o
The result set is composed of only one substitution: k = {?x/a, ?r/b, ?d/x}. The
next step is to update  by freezing Doctor(x )  Room(b)  hasPatient(b, true)
 inChargeOf (x , b) without removing the current ABox of  (line 7). The resulting canonical state of the world is shown in Figure 7. Lastly, we check whether both
policies remain in effect by checking their expiration conditions (line 9). In this ex-
ample, we query  with hasF ireRisk(b, f alse) and hasP atient(b, f alse). Both of
these queries return , hence we conclude that there is a state of the world where these
policies conflict (line 10).

5 Related Work and Discussion

There have been several policy languages proposed that are built upon Semantic Web
technologies. Rei [10] is a policy language based on OWL-Lite and Prolog. It allows

2 A model of an ontology o is an interpretation of o satisfying all of its axioms [1].

M. S  ensoy et al.

marriedTo

hasActor

in

Action

Person

inChargeOf

Place

in

isA

isA

Room

hasFireRisk

isA

xsd:boolean

Building

isA

in

Hospital

xsd:time

hasValue

CurrentTime

isA

isA

hasAge

LeavingAction

Patient

isA

Doctor

xsd:int

type

in

type

xsd:boolean

hasPatient

HospitalRoom

x

b

hasFireRisk

true

inChargeOf

hasPatient

true

Fig. 7. The canonical state of the world where the policies of Table 1 and Table 3 conflict

logic-like variables to be used while describing policies. This gives it the flexibility to
specify relations like role value maps that are not directly possible in OWL. The use
of these variables, however, makes DL reasoning services (e.g., static conflict detection
between policies) unavailable for Rei policies. KAoS [21] is, probably, the most developed language for describing policies that are built upon OWL. KAoS was originally
designed to use OWL-DL to define actions and policies. This, however, restricts the expressive power to DL and prevents KAoS from defining policies in which one element
of an actions context depends on the value of another part of the current context. For
example, KAoS cannot be used to represent a policy like two soldiers are allowed to
communicate only if they are in the same team. To handle such situations, KAoS has
been enhanced with role-value maps using Stanford JTP, a general purpose theorem
prover [21]. Unfortunately, subsumption reasoning is undecidable in the presence of
arbitrary role-value-maps [1].

KAoS distinguishes between (positive and negative) obligation policies and (positive
and negative) authorization policies. Authorization policies permit (positive) or forbid
(negative) actions, whereas obligation policies require (positive) or do not require (neg-
ative) action. Thus the general types of policies that can be described are similar to those
that we have discussed in this paper. Actions are also the object of a KAoS policy, and
conditions on the application of policies can be described (context), although the subject (individual/role) of the policy is not explicit (it is, however, in Rei). In common with
OWL-POLAR in its present form, KAoS does not capture the notion of the authority
from which/whom a policy comes, but there is a notion of the priority of a policy which
partially (although far from adequately) addresses this issue. Unlike OWL-POLAR, Rei
and KAoS do not provide means to explicitly define expiration conditions of the policies.
Policy analysis within both KAoS and Rei is restricted to subsumption. A policy in
KAoS is expressed as an OWL-DL class regulating an action, which is expressed as an
OWL-DL class expression (e.g., using restrictions on properties such as performedBy
and hasDestination). Two policies are regarded in conflict if their actions overlap (one
subsumes another) while the modality of these policies conflict (e.g., negative vs. positive authorization). Similarly, if there exist two policies within Rei that overlap with
?

?

?
respect to the agent and action concerned and they are obligued and prohibited, then a
conflict is recognised. In such a situation, meta-policies are used to resolve the conflict.
Policy conflicts can also be detected within the Ponder2 framework [18,23], where analysis is far more sophisticated than that developed for either KAoS or Rei, but analysis is
restricted to design time. In general, different methods can be used to resolve conflicts
between policies. This issue has been explored in detail elsewhere [11].

The expressiveness of OWL-POLAR is not restricted to DL. Using semantic conjunctive formulas, it allows variables to be used while defining policies. However, in
semantic formulas, OWL-POLAR allows only object-type variables to be compared
using owl:sameAs and owl:differentFrom properties. On the other hand, data-type variables can be used to define constraints on the datatype properties. In other words, semantic formulas are restricted to describe states of the world, each of which can be
represented as an OWL-DL ontology. Therefore, when a semantic formula is frozen,
the result is a canonical OWL-DL ontology. OWL-POLAR converts problems of reasoning with and about policies into query answering and ontology consistency checking
problems. Then, it uses an off-the-shelf reasoner (Pellet) to solve these problems. It is
known that consistency checking in OWL-DL is decidable [17], and query answering
in OWL-DL has also been shown to be decidable under DL-safety restrictions [8].

Ontology languages like KAoS are built on OWL 1.0, which does not support data-
ranges. Therefore, while defining policies, they either do not allow complex constraints
to be defined on datatype properties or use non-standard representations for these con-
straints, which prevents them from using the off-the-shelf reasoning technologies. The
clear distinctions between OWL-POLAR and KAoS, however, are manifest in the fact
that data ranges are exploited in OWL-POLAR to enable the expression of more complex constraints on policies, and the sophistication of the reasoning mechansims described in this paper.

To the best of our knowledge, OWL-POLAR is the first policy framework that formally defines and detects idle policies. Existing approaches like KAoS and Rei analyse
policies only to detect some type of conflict, considering only subsumption between
policies. On the other hand, OWL-POLAR provides advanced policy analysis support
that is not limited to subsumption checking. Consider the following policies: (i) Dogs
are prohibited from entering to a restaurant, and (ii) A member of CSI team is permitted to enter a crime scene. There is no subsumption relationship between these policies,
and so KAoS and Rei could not detect a conflict. However, OWL-POLAR anticipates a
conflict by composing a state of the world where these policies are in conflict, e.g., the
crime scene is a restaurant and there is a dog in the CSI team.

Building upon this research, we plan to explore various extensions to OWL-POLAR.
We will explore extending the representation of policies to include deadlines and penalties associated with their violation, along the lines of [3]. Another issue we would like
to investigate concerns how policing mechanisms [14] could make use of our representation and associated mechanisms to foster welfare in societies of self-interested
components/agents. We plan to enhance our representation so as to allow constraints
over arbitrary terms (and not just ?x  ,  being a constant), possibly using constraint
satisfaction mechanisms to deal with these. Two further extensions should address policies over many actions (as in, for instance,  is obliged to perform 1 and 2) and

M. S  ensoy et al.

disjunctions (as in, for instance,  is obliged to perform 1 or 2). Finally, we are exploring the use of OWL-POLAR in support of human decision-making, including joint
planning activities in hybrid human-software agent teams.

6 Conclusions

Policies provide useful abstractions to constrain and control the behaviour of components in loosely coupled distributed systems. Policies, also called norms, help designers of large-scale, open, and heterogenous distributed systems (including multi-agent
systems) to specify, in a concise fashion, acceptable (or policy-compliant) global and individual computational behaviours, thus providing guarantees for the system as a whole.
In this paper, we have presented a semantically-rich representation for policies as
well as efficient mechanisms to reason with/about them. OWL-POLAR meets all the
essential requirements of policies, as well as achieving an effective balance between expressiveness (realistic policies can be adequately represented) and computational complexity of associated reasoning for decision-making and analysis (reasoning with and
about policies operate in feasible time).
