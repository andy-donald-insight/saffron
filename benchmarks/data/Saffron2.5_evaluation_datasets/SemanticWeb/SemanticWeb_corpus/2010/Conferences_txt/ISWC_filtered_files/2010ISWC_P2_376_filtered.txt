What Does It Look Like, Really? Imagining How Citizens 

Might Effectively, Usefully and Easily Find, Explore, 

Query and Re-present Open/Linked Data  

mc schraefel 

IAM Group, Electronics and Computer Science 

University of Southampton 
Southampton, UK, SO17 1BJ 

mc+iswc@ecs.soton.ac.uk 

Abstract.  Are  we  in  the  semantic  web/linked  data  community  effectively 
attempting to make possible a new literacy - one of data rather than document 
analysis?  By  opening  up  data  beyond  the  now  familiar  hand  crafted  Web  2 
mash up of data about X plus geography, what are we trying to do, really? Is the 
goal at least in part to enable net citizens rather than only geeks the ability to 
pick  up,  explore,  blend,  interogate  and  represent data sources so  that we  may 
draw our own statistically informed conclusions about information, and thereby 
build  new  knowledge  in  ways  not  readily  possible  before  without  access  to 
these data seas? If we want citizens rather than just scientists or statisticians or 
journalists  for  that  matter  to  be  able  to  pour  over  data  and  ask  statistically 
sophisticated  questions of  comparison  and  contrast  betewen  times,  places  and 
people, does that mission re-order our research priorities at all? If the goal is to 
enpower citizens to be able to make use of data, what do we need to make this 
vision real beyond attending to Tim Berners-Lee's call to "free your data"? The 
purpose  of  this  talk  therefore  will  be  to  look  at  key  ineraction  issues  around 
defining  and  delivering  a  useful,  usable  *data  explorotron*  for  citizens.  In 
particular, we'll consider who is a "citizen user" and what access to and tools for 
linked  data  sense  making  means  in  this  case.  From  that  perspective,  we'll 
consider  research  issues  around  discovery,  exploration,  interrogation  and 
representation of data for not only a single wild data source but especially for 
multiple wild heterogeneous data sources. I hope this talk may help frame some 
stepping stones towards useful and usable interaction with linked data, and look 
forward  to  input  from  the  community  to  refine  such  a  new  literacy  agenda 
further. 

Keywords: interaction, design, user experience, linked data. 

1   Introduction 

What does interacting with the Semantic Web or Linked Data actually look like? And 
if we understand that, what are the challenges in making those interactions possible? 
And  for  whom  do  we  imagine  we  design  these  interactions  to  support?  Whose 

P.F. Patel-Schneider et al.(Eds.): ISWC 2010, Part II, LNCS 6497, pp. 356369, 2010. 
 Springer-Verlag Berlin Heidelberg 2010 
?

?

?
problems do we solve with 
of our work actually real pe
Some  of  my  colleagues 
user  interaction  questions 
conference launching the F
Workshop. The series kicke
Hendler and Ben Shneiderm
Since  then,  we  have  ha
challenges  in  a  semantic 
annual  Conference  on  Hu
Zurich, and more frequently
One of the high notes o
Tim Berners-Lee revealed 
included, at last, a user inter

any of the tools we create? When are these imagined us
eople? How do we know?  

sers 

and 

WUI) 
Jim 

and  I  have  been  thinking  about  these  semantic  web 
since  the  first  eponymous  meeting  at  the  WWW  20
First International Semantic Web User Interaction (SW
ed off with a memorable head to head session between 
man. Violent agreement rocked the sessions.  
ad  a  workshop  on  the  themes  of  identifying  interact
web  context  annually  at  venues  from  CHI,  the  ACM
uman  Factors,  to  a  virtual  workshop  between  MIT 
y, here at ISWC.  
f this annual series was the 2006 SWUI in Atlanta wh
during his talk the revised Semantic Web Layer Cake t
raction layer on top no less.   

tion 
Ms 
and 

here 
that 

Revised Semantic Web Layer Cake 

 

6  years  on  from  that  fir
Human Computer Interactio
to get good HCI oriented pa
still struggle to get our them
Indeed,  Id  like  to  take 
who  have  been  an  ongoin
Rutledge,  Avi  Bernstein, 
acknowledge  the  Web  Scie
 

rst  outing,  Ive  been  asked  to  give  an  invited  talk  ab
on here at ISWC, for which I am honored. We still strug
apers submitted to ISWC.  Some of us who do submit th
m accepted at ISWC.   

bout 
ggle 
hem 

this  opportunity  to  acknowledge  some  of  our  colleag
ng  steering  committee  for  SWUI:  Duane  Degler,  Llo
David  Karger,  Jennifer  Goldbeck.  Id  also  like 
ence  Foundation  as  a  constant  sponsor  for  the  worksh

gues 
oyd 
to 
hops  

m. schraefel 

weve held, more often than not, the founder of the post workshop feast (or at least 
coffee  and  nibbles  during  sessions).  In  particular,  Wendy  Hall,  Nigel  Shadbolt  and 
Tim Berners-Lee. 

While I am delighted to have the opportunity to talk for half an hour or more on 
my favorite concerns about the intersections of interaction and linked data research, I 
would rather take this space to give some time to other researchers in the field whose 
thinking in this space is already well grounded in practice. Indeed, David Karger and I 
recently  enjoined  HCI  and  Human  Computer  Information  Retrieval  (HCIR) 
researchers  who  deal  with  large  data  sets  of  various  domains  to  help  us  frame  an 
agenda  to  excite  other  HCI/HCIR  researchers  to  consider  the  opportunities  for  new 
research in this area of massive open data.  

These  conversations  have  encouraged  me  to  reach  out  to  these  experts  again, 
specifically  to  hear  their  formative  thoughts  on  what  they  see  as  key  challenges  to 
make open data/linked data to in particular useful and usable by regular citizens. In 
particular we asked that they consider what may be new or special about this kind of 
data that brings new research opportunities to HCI that might also be of interest to the 
researchers who seek ways to tame this data for functional use for the machine. 

This  paper  spotlights  responses  from  5  of  these  researchers  across  industry  and 
academia. Before we consider five of the expert responses, in the next section, allow 
me to set the scene of the questions asked. As is apparent reading through the replies, 
a few themes for consideration recur.  

2   Eliciting the Main Interaction Challenges for the Linked 

Data/Semantic Web's Interaction Success 

What  are  1  or  2  key  priorities  you  think  must  be  addressed  that  will  aid  citizenfocused manipulation of open data sources for personal/social knowledge building? 

2.1   Focus: Tools for the Citizen User 

The focus of the question is around the Citizen User: a citizen user is not a domain 
expert (necessarily) - but is someone who has an interest in some information, and the 
(structured) data is publicly available to help build up an answer to the question, and 
they are happy to be able to make use of the data for sense making - for building new 
knowledge. They don't expect "the answer" but  want appropriate data to build  up a 
sense of an answer. 

So, we are not expecting to create an interaction system that provides The Answer, 

but rather facilitates:  

 
discovery,  
 
interrogation, 
  manipulation, 
 
 

annotation,  
representation of heterogeneous open data sources. 

What, therefore are key challenges that in your view we MUST address/prioritize to 
support citizen based exploration of the freed data of sites like data.gov, data.gov.uk 
and related sources? 
?

?

?
Example Scenarios: 

o Where Should i Live? 
-  where  data  exists  on  pollution,  hospital  waiting  times,  transportation, 

political representation in a region, crime stats,  

o for whom should i vote?  
-  where  there  may  be  data  on  a  party's  voting  record  and  individual 

members' voting records, regional crime stats, etc 

o Is this a good school? 
- where data may come from league tables, student reports on their views 

of instructors from all over, house prices, grocery locations, transport  

o What about drug reactions?  
what other drugs  have people taken  with  my condition, and  what's been 

the success rate whether self-reported or by other measures?  

To  make  such  citizen-based  exploration  possible,  what  should  be  our  research 
agenda? Our concerns  from the back end to serve the  front end? How do  we  move 
from the current high geek expert tools to citizen tools? How do we help people used 
to thinking about issues with the data to think about issues for the person using the 
data as the best path into solving problems of serving the data.  Not all of you may 
think that that IS the best way, which is fine. Alternatives requested, too.  

Some examples of issues we encounter regularly within data: 

- 
- 
- 

geographical boundaries in different data sets don't match up  (hospital trusts 
don't map to crime regions) 
not always clear what information in the data is - meaningless labels 
data is incomplete or messy 

2.2   Thesis and Background 

My rationale in posing these particular questions is the following: with the emphasis 
on "freeing data" it seems we are de facto potentially establishing or requiring a new 
literacy - a literacy about data rather than documents; that we've moved from the page 
if you will to the cell. And that requires new kinds of knowledge - what to do with the 
data. 

In the pre-printing press era literacy was the purview of a select few - the religious 
cast - who had access to manuscripts. With the press (and the middle class) literacy of 
documents becomes more wide spread.  Is the era of linked data going to be the same 
now, where data and what to do with it has been the purview of statisticians or those 
trained in statistics - have access to the data, and produce the results for the rest of us? 
If the goal is to believe that access to data is a Public Good, what does that mean 
for interaction? For a basic data literacy? Does this understanding of data in the 21st 
Century  start  with  mash  ups  for  all?  Where  do  we  as  technologists  /  researchers 
/designers begin? Similarly, in order to apply that knowledge of data manipulations to 
the Interface, we also need services to enable normal web-literate citizens to engage 
the  data  -  find  it,  explore  it, manipulate  it,  and  re-present  it  where  that  it  may  be 
sourced from many heterogeneous sources. 

m. schraefel 

2.3   Audience: At the Coal Face of Digging the Semantic Web 

Most  of  the  researchers  in  the  Semantic  Web  community  work  in  terms  of  dealing 
with representing the data  for the  machine efficiently rather than thinking primarily 
about people accessing and manipulating the data directly.     

The  goal  of  these  interviews  is  to  help  people  working  on  the  problems  of 
machines  processing  data  to  find  it  meaningful  to  connect  potentially  instead  with 
what  citizens  need  to  be  able  to  do  with  the  data,  where  those  citizens  are  not  any 
more geeks than are the current users of the web. 

3   Expert Responses 

The  following  responses  are  direct  reports  in  their  own  words  of  responses  to  the 
above framing.    

3.1   Daniel Tunkelang, Technical Lead, Google 

Here's some of my admittedly US-centric thinking about patent and census data: 

 

Patents. Despite the availability of patent data through public (e.g., USPTO, WIPO) 
and  private  (e.g.  Google)  repositories  and  the  regular  appearance  of  patents  in  the 
news,  the  average  citizen  (at  least  in  the  US)  seems  to  have  little  ability  to  either 
understand or influence how patents work. Some things that we could do to make this 
data more accessible: 
 
Exposing  the  links  between  related  resources  (e.g.,  patent  applications  and 
prosecution  histories).  Even  people  familiar  with  patents  may  not  be  aware  of 
resources  like  PAIR,  the  USPTO  portal  that  offers  the  full  history  of  a  patent  or 
pending  patent  application.  And  the  interfaces  make  it  inefficient  and  painful  to 
navigate among resources. 
 
Relationships  between  patents  and  entities.  People  invent  patents;  patents  are 
assigned  to  companies;  people  work  for  companies;  companies  acquire  other 
companies or their assets. 
 
Connections between similar patents. Even the simple classification system used by 
the US patent system is not well exposed in interfaces. But I'm thinking of far more 
than that: connecting patents using link analysis of the citation and entity graphs and 
computing content-based similarity using information extraction. 

Patent law  itself  is pretty complex, and there's  more required here than exposing 
the  raw  data  in  a  nicer  interface.  For  example,  technical  terms  should  be  linked  to 
glossary entries where possible. Links to non-patent-art should also be connected to 
published documents where possible. And ultimately the value of all of these efforts 
would require policy changes that would make it easier for citizens to participate. But 
there's a chicken-and-egg problem: today's citizens are ill-equipped to participate, so 
there is little motivation for policy change. 

 

Census  Data.  It  should  be  straightforward  for  the  average  citizen  to  access  public 
demographic  information,  whether  at  a  national  level,  a  neighborhood  level,  or 
?

?

?
 so. 
able 

are 

anything in between. But I
The best tools are designed
queries for generating repor
There's  a  vocabulary  p
familiar for government age
Another  challenge  is  th
granularities, so users need 
their  information  needs  (i.e
had in mind). 

'm not aware of any interface that makes it easy to do 
d for professionals who invest time in developing reusa
rts. But it's not just that the tools are complicated.  
problem--Census  data  is  classified  using  codes  that 
encies but not necessarily for citizen users.  
at  data  is  collected  a  varying  geographical  and  tempo
to be able to explore to discover the data that best matc
e.,  they  might  not  find  it  at  precisely  the  granularity  t

oral 
ches 
they 

3.2   David Huynh, Resear

rch Scientist, Google 

I  don't  think  I  can  tell 
interaction  challenge  seem
thoughts. 

you  the  *main*  interaction  challenges,  because  ev
s  roughly  equally  important.  But  anyway,  here  are  a 

very 
few  

 

(1) URIs are for machine
URIs are practically usele
images  and  identifying  de
industry, size, location for 
entities. Sure, there's clearl
but there is not yet a realiz
URIs  do  for  machines--un
(search) widget provides an

es to unambiguously identify entities to operate on, 
ess for humans to perform the same task. For huma
etails  (race,  gender,  birth  year,  profession  for  a  pers
a company; etc.) are what help us unambiguously iden
y a realization that raw URIs shouldn't be shown to us
zation that we need something else to do for humans w
nambiguously  identifying  entities.  The  Freebase  Sugg
n example of how to do this: 

but 
ans, 
son; 
ntify 
sers, 
what 
gest 

 

 

Freebase Interaction image 

Note  that  each  suggeste
described  with  brief  but  id
communicate  to  their  users
rather than mere text, and h

ed  entity  is  labeled  ("Actor",  "US  President",  etc.) 
dentifying  details  on  hover.  Semantic  UIs  should  strive
s  that  things  on  the  screen  are  representations  of  enti
help users unambiguously identify entities. 

and 
e  to 
ities 

m. schraefel 

The iPhone interface has nuances that make it so pleasant and fun to use, nuances 
that  somehow  let the user "feel" the interface. In the same  way, semantic  UIs  must 
somehow get users to "feel" the semantic entities. After using my first iPhone for a 
few  months,  one  day  while  reading  a  paperback  book,  upon  reaching  the  end  of  a 
page, I instinctively placed my thumb at the bottom of the page and pushed it upward, 
only to realize that it's not an iPhone. That's what we want here with semantic UIs. 
After  using  semantic  UIs  for  a  while,  when  a  user  sees  a  plain  piece  of  text  like 
"ford", the reflex should be, "it's ambiguous / raw / bare". 

And yes, it's about details, details, details.   
 

(2) Direct manipulation techniques for en-masse data editing seem to be a game 
changer for a class of users--folks who can handle Excel but are not familiar with 
scripting  or  don't  have  time  and  patience  for  scripting.  My  work  on  Gridworks 
has already started to address some of the design challenges. 

Just because data is open doesn't mean it's clean or it's formatted in a way that you 
can  use.  This  inconvenience  seems  to  be  swept  under  the  rug  sometimes.  The 
semantic web community has focused so much on semantics that perhaps not enough 
effort has been spared for addressing syntax. But obviously, without syntax, there is 
no semantic. 

It's also quite important to make sure that these tools are generic, rather than RDF-
specific. Our goal here isn't to shoehorn everything into RDF. The goal is to increase 
awareness,  desire,  and  demand  for  structured  data,  potentially  linked.  Let  each  user 
decide which format might serve their own purpose at this time. As people use these 
tools more and more, gradually, structured data, linked, will become natural to them. 

 

(3) Entity Reconciliation. One surprising thing I've learned from Gridworks is how 
ready  people  are  to  want  entity  reconciliation.  And  not  just  with  Freebase  but  with 
their  own  databases.  This  persuaded  me  to  generalize  the  reconciliation  support  in 
Gridworks, as per the Reconciliation Service API (http://code.google.com/p/freebase-
gridworks/wiki/ReconciliationServiceApi) 

I  could  almost  claim  that,  by  making  reconciliation  easy,  Gridworks  makes  it 
obvious  why  one  would  want  reconciliation.  It's  almost  as  if  the  tool  makes  people 
think in a certain way. 

So, don't start with "RDF". You would have already lost. Start with what users ask 
for. Then if possible, let the tools nudge their thinking toward RDF or whatever that's 
ideal in the long term. 

 

(4) Help build the upcoming structured data web. Or do research on the semantic 
web. Pick one (since you can't do both). 

3.3   Ed Chi, Principal Scientist, Augmented Social Cognition, Xerox Parc 

The  issues  you  raised  was  precisely  the  inspiration  for  my  Ph.D.  Thesis  work  on 
creating  a  visualization  spreadsheet.    The  idea  was  that  if  people  can  easily  use 
spreadsheets, then they ought to be able to take that model further and start creating 
visualizations using them, and the thesis was an exploration to find out how to design 
such  systems.    I  think  of  ManyEyes,  and  Jeff  Heer's  later  works  to  be  in  the  same 
direction. 
?

?

?
We have since learned a lot about user-contributed content on systems like Wikipedia, 
Delicious,  Twitter,  and  they  show  a  very  interesting  participation  architecture  that 
consists of readers, contributors, and leaders.  Not all users want to be leaders, and not all 
users  want to contribute.  We have sometimes  use the derogatory term of "lurkers" to 
describe "readers", which I think is a bit unfair.  Ronald Burt's work have shown that a lot 
of  us  would  like  to  be brokers  of  information  among  social  groups,  but  there are  also 
need for an audience, or followers, who might become brokers later, but not everyone all 
at once. 

I  believe  that  data  manipulation  of  open  data  sources  to  follow  the  same  curve.  
Yes, some cancer patients will want to read all they can about their condition, and do 
the  analytical  work,  and  others  (not  necessarily  because  of  tool  limitations)  would 
prefer  to  take  a  backseat,  and  let  others  curate  the  information  for  them.    What's 
interesting  is  that  they  might  want  very  simple  interactions  that  enable  for  basic 
sorting  of  data,  or  maybe  even  services  that  interpret  the  data  for  them  (e.g.  doctor 
experts),  but  they  would  prefer  someone  else  does  the  bulk  of  the  work  (even  if  it 
becomes very easy due to tool development). 

Consider  a  typical  usage  scenario:  I  am  reading  several  medical  journal  articles. 
 Data all in tables, in PDF format.  need to extract the data from the tables and plot 
them.  Ahem!  Good luck.  Let's go and type them all in by hand. 

So, given that, what can we do?  
First,  it's  quite  clear  that  much  of  the  hard  work  remains  in  data  import  and 
cleaning.  To democratize data analytics and manipulation, the bulk of the difficulty is 
dealing with data acquisition.  Unfortunately, most of this is engineering and not sexy 
research, so there aren't really innovative work in this area.   

By not exciting I mean, I don't know of a single tool that enables me to grab tables 
out  of  PDFs.   Worse  still,  if  I  have  browse  around  on  the  net,  and  I  find  the  data  I 
need in web pages, often they're in HTML tables that are very hard to cut and paste 
into my excel spreadsheet. What tool is really out there for my information extraction 
tasks?  Tables are just one example.  Other problems include things that are locked in 
databases, but barely visible to end-users: 

- 
- 

say  I  want  to  analyze  all  of  the  flights  from  US  to  Europe  over  the  last 
month.   How  do  I  get  the  data?  Do  I  perform  lots  of  searches  on  travel 
websites to extract that?  Do I go to airlines one by one and examine their 
schedule (in PDF or HTML format), and get the data that way? 
say I want to plot the price of harddrives by dollar per MB in the last decade. 
 Again, where do I get the data?  How do I clean it, so that I can plot them? 

Some  information  extraction  (AI-style  algorithms,  and  some  machine  learning 
techniques)  are  making  some  inroad  in  this  area.  Some  recent  work  on  entity 
extraction with human in the loop seems pretty good.  So if I have a document, and 
want to find all interesting entities in them, and make a cross-index of related entities, 
there  now  seems  to  be  some  good  research  tools  that  do  that.  I  also  believe  that 
mixed-initiative research for data import is sorely needed.  We're doing a bit of this 
work in my lab at the moment. That is: human in the loop.  The machine does some 
extraction, then human says, ah, that's not quite right, fix it this way, and machine do 
more, and then human fix again. 

m. schraefel 

Second, there is the issue of data literacy. What kind of visualization works with 
what  kind  of  data?  What  analytic  technique  is  appropriate?    Early  work  by  Jock 
Mackinlay pointed to the possibility of automating some of these design choices, and 
we haven't made a huge amount of progress in this area.   

Some  tree  Viz  seems  pretty  automate-able,  as  are  stacked  graphs,  population 
analyses, tables.  I tend to favor simple visualizations that are understandable to lay 
people  these  days.   Visual  literacy  is  a  huge  problem  that  will  take  decades  to 
overcome, so I favor simple vis these days. 

Wizards, try-visualization-refine loops have all been tried in research.  We need to 
stop inventing new visualizations, but actual usable tools for people here.  By going to 
vertical  domains,  we  will  learn  how  to  solve  this  problem.  We  need  curriculum  in 
visuzliation that is part of basic education. 

That  said,  Vis  researchers  need  to  work  on  real  scenarios  more  often.   Go  into 
medicine,  and  you  see  a  lot of  data  analytics  problems  that  are  huge,  and  often  not 
about visualizing generic trees.  Often, it's visualizing protein interaction networks, or 
seeing  evolving  relationships.   Go  into  another  field,  say,  transportation,  and  you 
realize  you  need  to  combine  infovis  with  geo-viz.   It's  often  not  about  new 
visualizations, but about how to put vis components together. 

3.4   Lloyd Rutledge, Computer Science, Open University, The Netherlands 

In summary: less emphasis on grand new interfaces. More on familiar interfaces, but 
under user control and independent from the data. The user doesnt notice anything, 
thus  no  new  literacy.  The  user  only  stops  noticing  that  information  access  doesnt 
work  the  way  it  obviously  should.  Can  we  thus  take  large-scale  data  from  multiple 
civil  sources  and  have  users  access  it  in  a  way  so  unified  and  quick  that  there  is 
nothing  remarkable  about  it  (finally!)?  A  new  literacy?  Computers  as  devices 
require(d) a new literacy. The Web didnt: users of the Web feel it acts they way they 
always  knew  it  should  (even  though  they  actually  couldnt  imagine  it  beforehand). 
We do new things with the Web in new ways, but they feel familiar once you start. 

To me, it seems that the same will be true of the end-user front-end applications of 
the type of use of the Semantic Web for which we dream and strive. The users will 
not notice the difference. They wont really notice anything. What will happen is: 

-  They want some data and they ask for it in a reasonable commonsense way, 
-  Appropriate and correct data comes back. 
- 

probably in ways they already (think to) ask for information 

It comes back in the form of a presentation that makes perfect, common 
sense. The form of presentation itself is not remarkable apart from the data it 
presents.  

This  all  happens  with  interfaces  users  have  long  been  familiar  with.  The  end  user 
wont  notice.  At least on the per interaction basis. Perhaps over a longer period the 
user will day It seems getting information used to be buggier and clunkier. The end 
users have no new literacy to learn. Their current literacy just works better. 

This was just about data access. Could we argue the same way for data input and 
sharing? I think so. Users add data using means they already know. They and other 
users get this data back in ways that make sense. This data gets combined with other 
?

?

?
data,  but  that  makes  sense,  of  course?  Only  Semantic  Web  researchers  know  how 
remarkable this last step is. No one else will notice a thing. Researchers strive for a 
grand  new  SW  interface  that  will  put  SW  in  the  mainstream  like  a  magic  bullet 
application.  But  why  make  a  new  interface  paradigm?  A  more  appropriate 
challenge  is  to  get  tried-and-true  interfaces  to  work  the  way  they  should  with  data 
placed on and accessed from the Semantic web the way it should. 

There is no new literacy. There is new  technology, good practice and science on 
the side of Semantic Web developers to make existing literacy work with data that is 
the way it should be. 

One  problem  is  that  familiar  interfaces  are  often  controlled  by  non-user  parties 
who also own and isolate the data. Challenges are thus having user control the form 
of interface and unrestricted access to public data, and have these two control issues 
be separate from each other. And to have the users not notice anything: they just have 
their interface, and they just ask for and browse data. And they just simply get it. No 
barriers based on who is providing the interface or who is providing the data or if one 
needs to be linked on the other. 

A  second  challenge:  allowing  seamless  combination  of  public,  institutional  and 
private  data,  all  in  the  same  interface,  but  with  the  corresponding  security  and  data 
sharing/blocking.  An  example  is  combining  civil  databases  on  medications  and 
medical services with your own medical records. 

I [dont] mean to poopoo the work of [previous SWUI presented work]. But I think 
we need to encourage other challenges now. For one, there are too many submissions 
for new types of interfaces to the Semantic Web that arent research and dont work, 
as weve seen in various journal submissions. 

Making a new interface is often an attractive project for programmers, but most fall 
short of burden of proof, and there are only so many new interfaces possible. Not only 
unproven, many proposed new interfaces just dont work (such as big fat graphs). 

But  even  the  successful  new  interfaces  arent  that  new  and  arent  necessarily 
attached to the Semantic Web. All the new SW interfaces work with any amount of 
data  of  any  origin.  Their  newness  is  more  about  what  computers  make  possible  for 
data access than was the SW makes possible. 

These  successful  new  SWUIs  also  dont  require  new  literacy.  They  are  natural 
extensions  of  familiar  interfaces,  some  of  which  go  back  to  paper.  Like  hypertext, 
they  have  a  retrospective  obviousness,  despite  being  hard  to  imagine  beforehand 
(and  hard  to  develop  the  first  time).  Thus  no  new  literacy  needed.  SWUI  that  do 
require new literacy, like large RDF graphs (BFDs) and queries, even assisted, and 
even visually assisted (mostly), tend IMHO not to catch on. 

So what is new is the type and scale of data that gets to these interfaces. In the civil 
service  data  example,  what  we  need  is  multiple  civil  service  branches  to  have  their 
data in familiar, even in 21st century SWUIs like facet browsers and autocompletion, 
but  seamlessly.  When  another  institute  doesnt  have  their  data  on  the  SW/LW,  end 
users should find it strange. Not Why do they have any RDF files? but Why cant I 
get at this information? Why do I have to use their website to see it? Why do I have to 
jump  back  and  forth  from  their  website  to  my  (semantic,  but  they  dont  know  it) 
browser? 

m. schraefel 

3.5   Abraham Bernstein, Dynamic and Distributed Information Systems Group 

Univesrity of Zurich 

There is plenty of data out there, but as you point out the linking is abysmal. I am not 
sure where I read it but even the connections between the LOD datasets is only very 
brittle  ...  I  am  not  even  talking  of  data  repositories  such  as  data.gov  or  even  worse 
department of statistic excel sheets from different geographical regions etc... 

So, the single most important question is how to integrate a multitude of sources. 

ASIDE:  Yes,  it  is  true,  we  are  very  far  away  from  actually  understanding  what  the 
"best"  way  to  interact  with  linked  data  is  (assuming  there  is  such  a  thing)  and 
approaches  such  as  faceted  browsing,  David  and  your  stuff,  NLP,  etc.  are  a  only  a 
first  step.  Lost  of  work  needed  here  -  mostly  of  a  good  UI  nature.  The  crux  of  the 
Semantic Web is that it adds heterogeneous (even previously unknown) data sources 
to the mix. Most of the UI approaches so far assume that the data already has been 
integrated "nicely" into one data-set. Exhibit, e.g., is great, but the most difficult work 
has  already  been  done:  the  data  integration.  So  if  we  really  ask  ourselves  what  the 
Semantic Web brings new to the picture in contrast to "just" interacting with Graphbased data then it is the data-integration problem. 

So if we want to bring the Semantic Web to fruition we need to think how we can 
help our citizen user to combine heterogeneous data sources. My hunch is that it will 
need a combination of (possibly novel)  UI metaphors, a sprinkle of good AI, some 
social computation, good software engineering. How can I substantiate this hunch? 

 

 

I think the first point is clear: We need to find out what UI metaphor is best 
used to integrate information. Personally, I have no clue if anybody has 
systematically explored citizen user data integration. I am aware of many 
projects doing it for pros, but not a lot of work on casual users. 

  A sprinkle of AI is needed, as I believe that mixed-initiative might help to 
ease the bruden of data integration. To that end some statistical processing 
(e.g., for finding candidates for joins),  maybe some rules (e.g., to encode 
otherwise collected background knowledge), and guided interaction (e.g., 
using planing techniques) might be helpful. 
social computation will probably help the enterprise by enabling the 
exchange of  integration recipes. 

  Good software engineering is needed to build some robust prototypes to test 

these ideas. 

So finding the right interaction metaphor for integrating data seems to be the single, 
biggest challenge. 

3.6   Others in the Discourse 

The  above  commentaries  represent  specific  contributions  requested  for 
this 
presentation of voices. In related conversations, a few more relevant points emerged 
that are germane to this discussion. Ben Shneiderman, Computer Science, University 
of  Maryland,  maps  the  process  articulated  above  of  discovery,  exploration, 
interrogation,  and  re-presentation  with  parallel  discussion  going  on  in  the  visual 
analytics (VA) world, where a similar process (discovery, exploration, interrogation, 
