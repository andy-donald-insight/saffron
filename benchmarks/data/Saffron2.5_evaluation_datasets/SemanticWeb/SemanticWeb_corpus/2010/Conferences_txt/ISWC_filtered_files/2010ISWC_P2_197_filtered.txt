ORE - A Tool for Repairing

and Enriching Knowledge Bases

Jens Lehmann and Lorenz B uhmann

AKSW research group, University of Leipzig, Germany

lastname@informatik.uni-leipzig.de

Abstract. While the number and size of Semantic Web knowledge bases in-
creases, their maintenance and quality assurance are still difficult. In this article,
we present ORE, a tool for repairing and enriching OWL ontologies. State-of-
the-art methods in ontology debugging and supervised machine learning form
the basis of ORE and are adapted or extended so as to work well in practice. ORE
supports the detection of a variety of ontology modelling problems and guides
the user through the process of resolving them. Furthermore, the tool allows to
extend an ontology through (semi-)automatic supervised learning. A wizard-like
process helps the user to resolve potential issues after axioms are added.

1 Introduction

Over the past years, the number and size of knowledge bases in the Semantic Web has
increased significantly, which can be observed in various ontology repositories and the
LOD cloud1. One of the remaining major challenges is, however, the maintenance of
those knowledge bases and the use of expressive language features of the standard web
ontology language OWL.

The goal of the ORE (Ontology Repair and Enrichment) tool2 is to provide guidance
for knowledge engineers who want to detect problems in their knowledge base and
repair them. ORE also provides suggestions for extending a knowledge base by using
supervised machine learning on the instance data in the knowledge base. ORE takes
the web aspect of the Semantic Web into account by supporting large Web of Data
knowledge bases like OpenCyc and DBpedia.

The main contributions of the article are as follows:

 provision of a free tool for repairing and extending ontologies
 implementation and combination of state-of-the-art

inconsistency detection,

ranking, and repair methods

 use of supervised learning for extending an ontology
 support for very large knowledge bases available as Linked Data or via SPARQL

endpoints

 application tests of ORE on real ontologies

1 http://linkeddata.org
2 See http://dl-learner.org/wiki/ORE and download at

http://sourceforge.net/projects/dl-learner/files/

P.F. Patel-Schneider et al.(Eds.): ISWC 2010, Part II, LNCS 6497, pp. 177193, 2010.
c Springer-Verlag Berlin Heidelberg 2010

J. Lehmann and L. B uhmann

The article is structured as follows: In Section 2, we cover the necessary foundations in
the involved research disciplines such as description logics (DLs), ontology debugging,
and learning in OWL. Section 3 describes how ontology debugging methods were implemented and adapted in ORE. Similarly, Section 4 shows how an existing framework
for ontology learning was incorporated. In Section 5, we describe the structure of the
ORE user interface. The evaluation of both, the repair and enrichment part, is given in
Section 6. Related work is presented in Section 7 followed by our final conclusions in
Section 8.

2 Preliminaries

We give a brief introduction into DLs and OWL as the underlying formalism, recapitulate the state of the art in ontology debugging and give the definition of the class
learning problem in ontologies.

2.1 Description Logics and OWL
DLs are usually decidable fragments of first order logic and have a variable-free syn-
tax. The standard ontology language OWL 2 is based on the DL SROIQ. We briefly
introduce it and refer to [12] for details.
In SROIQ, three sets are used as the base for modelling: individual names NI,
concept names NC (called classes in OWL), and role names (object properties) NR. By
convention, we will use A, B (possibly with subscripts) for concept names, r for role
names, a for individuals, and C, D for complex concepts. Using those basic sets, we
can inductively build complex concepts using the following constructors:

A |  |  | {a} | C  D | C  D

| r.Self | r.C | r.C | n r.c | n r.C

For instance, ManhasChild.Female is a complex concept describing a man who
has a daughter. A DL knowledge base consists of a set of axioms. The signature of a
knowledge base (an axiom ) is the set S (Sig()) of atomic concepts, atomic roles and
individuals that occur in the knowledge base (in ). We will only mention two kinds of
axioms explicitly: Axioms of the form C  D are called general inclusion axioms. An
axiom of the form C  D is called equivalence axiom. In the special case that C is a
concept name, we call the axiom a definition.

Apart from explicit knowledge, we can deduce implicit knowledge from a knowledge
base. Inference/reasoning algorithms extract such implicit knowledge. Typical reasoning tasks are:
 instance check K |= C(a)? (Does a belong to C?)
 retrieval RK(C)? (Determine all instances of C.)
 subsumption C K D? (Is C more specific than D?)
 inconsistency K |= false? (Does K contain contradictions?)
 satisfiability C K ? (Can C have an instance?)
 incoherence C (C K )? (Does K contain an unsatisfiable class?)
Throughout the paper, we use the words ontology and knowledge base as well as complex concept and class expression synonymously.
?

?

?
2.2 Ontology Debugging

Finding and understanding undesired entailments such as unsatisfiable classes or inconsistency can be a difficult or impossible task without tool support. Even in ontologies
with a small number of logical axioms, there can be several, non-trivial causes for an en-
tailment. Therefore, interest in finding explanations for such entailments has increased
in recent years. One of the most usual kinds of explanations are justifications [15]. A
justification for an entailment is a minimal subset of axioms with respect to a given
ontology, that is sufficient for the entailment to hold. More formally, let O be a given
ontology with O |= , then J is a justification for  if J |= , and for all J   J ,
J  |= . In the meantime, there is support for the detection of potentially overlapping
justifications in tools like Prot eg e3 and Swoop4. Justifications allow the user to focus
on a small subset of the ontology for fixing a problem. However, even such a subset can
be complex, which has spurred interest in computing fine-grained justifications [11] (in
contrast to regular justifications). In particular, laconic justifications are those where
the axioms do not contain superfluous parts and are as weak as possible. A subset of
laconic justifications are precise justifications, which split larger axioms into several
smaller axioms allowing minimally invasive repair.
A possible approach to increase the efficiency of computing justifications is module
extraction [6]. Let O be an ontology and O  O a subset of axioms of O. O
is a
module for an axiom  with respect to O if: O |=  iff O |= . O
is a module for
a signature S if for every axiom  with Sig()  S, we have that O
is a module for
 with respect to O. Intuitively, a module is an ontology fragment, which contains all
relevant information in the ontology with respect to a given signature. One possibility
to extract such a module is syntactic locality [6]. [30] showed that such locality-based
modules contain all justifications with respect to an entailment and can provide order-
of-magnitude performance improvements.

2.3 The Class Learning Problem

The process of learning in logics, i.e. trying to find high level explanations for given
data, is also called inductive reasoning as opposed to the deductive reasoning tasks we
have introduced. The main difference is that in deductive reasoning it is formally shown
whether a statement follows from a knowledge base, whereas in inductive learning we
invent new statements. Learning problems, which are similar to the one we will analyse,
have been investigated in Inductive Logic Programming [27] and, in fact, the method
presented here can be used to solve a variety of machine learning tasks apart from
ontology engineering.

The considered supervised ontology learning problem is an adaption of the problem in Inductive Logic Programming. We learn a formal description of a class A from
inferred instances in the ontology. Let a class name A  NC and an ontology O be
given. We define the class learning problem as finding a class expression C such that
RO(C) = RO(A), i.e. C covers exactly all instances of A.

3 http://protege.stanford.edu
4 http://www.mindswap.org/2004/SWOOP/

J. Lehmann and L. B uhmann

Clearly, the learned concept C is a description of (the instances of) A. Such a concept
is a candidate for adding an axiom of the form A  C or A  C to the knowledge base
K. This is used in the enrichment step in ORE as we will later describe. In the case that
A is described already via axioms of the form A  C or A  C, those can be either
modified, i.e. specialised/generalised, or relearned from scratch by learning algorithms.
Machine learning algorithms usually prefer those solutions of a learning problem,
which are likely to classify unknown individuals well. For instance, using nominals
(owl:oneOf) to define the class A above as the set of its current instances is a correct
solution of the learning problem, but would classify all individuals, which are added
to the knowledge base later as not being instance of A. In many cases, the learning
problem is not perfectly solvable apart from the trivial solution using nominals. In this
case, approximations can be given by ML algorithms. It is important to note that a
knowledge engineer usually makes the final decision on whether to add one of the
suggested axioms, i.e. candidate concepts are presented to the knowledge engineer, who
can then select and possibly refine one of them.

3 Ontology Repair

For a single entailment, e.g. an unsatisfiable class, there can be many justifications.
Moreover, in real ontologies, there can be several unsatisfiable classes or several reasons
for inconsistency. While the approach described in Section 2.2 works well for small
ontologies, it is not feasible if a high number of justifications or large justifications have
to be computed. Due to the relations between entities in an ontology, several problems
can be intertwined and are difficult to separate. We briefly describe how we handle these
problems in ORE.

Root Unsatisfiability. For the latter problem mentioned above, an approach [18] is to
separate between root and derived unsatisfiable classes. A derived unsatisfiable class
has a justification, which is a proper super set of a justification of another unsatisfiable
class. Intuitively, their unsatisfiability may depend on other unsatisfiable classes in the
ontology, so it can be beneficial to fix those root problems first. There are two different
approaches for determining such classes: The first approach is to compute all justifications for each unsatisfiable class and then apply the definition. The second approach
relies on a structural analysis of axioms and heuristics. Since the first approach is computationally too expensive for larger ontologies, we use the second strategy as default
in ORE. The implemented approach is sound, but incomplete, i.e. not all class dependencies are found, but the found ones are correct. To increase the proportion of found
dependencies, the TBox is modified in a way which preserves the subsumption hierarchy to a large extent. It was shown in [18] that this allows to draw further entailments
and improve the pure syntactical analysis.

Axiom Relevance. Given a justification, the problem needs to be resolved by the user,
which involves the deletion or modification of axioms in it. To assist the user, ranking
methods, which highlight the most probable causes for problems, are important. Common methods (see [16] for details) are frequency (How often does the axiom appear in
justifications?), syntactic relevance (How deeply rooted is an axiom in the ontology?)
?

?

?
and semantic relevance (How many entailments are lost or added?5). ORE supports all
metrics and a weighted aggregation of them. For computing semantic relevance, ORE
uses the incremental classification feature of Pellet, which uses locality-based modules.
Therefore, only the relevant parts of the ontology are reclassified when determining the
effect of changes.

Consequences of Repair Step. Repairing a problem involves editing or deleting an ax-
iom. Deletion has the technical advantage that it does not lead to further entailments
due to the monotonicity of DLs. However, desired entailments may be lost. In contrast,
editing axioms allows to make small changes, but it may lead to new entailments, including inconsistencies. To support the user, ORE provides fine-grained justifications,
which only contain relevant parts of axioms and, therefore, have minimal impact on
deletion. Furthermore, ORE allows to preview new or lost entailments. The user can
then decide to preserve them, if desired.

Workflow. The general workflow of the ontology repair process is depicted in Figure
1. First, all inconsistencies are resolved. Secondly, unsatisfiable classes are handled by
computing root unsatisfiable classes, as well as regular and laconic justifications, and
different ranking metrics.

Fig. 1. Workflow for debugging an ontology in ORE

Web of Data and Scalability. In order to apply ORE to existing very large knowledge
bases in the Web of Data, the tool supports using SPARQL endpoints instead of local
OWL files as input knowledge bases. To perform reasoning on those knowledge bases,
ORE implements an incremental load procedure inspired by [9].

Using SPARQL queries, the knowledge base is loaded in small chunks. In the first
step, ORE determines the size of the knowledge base by determining the number of
all types of OWL 2 axioms. In the main part of the algorithm, a priority based loading
procedure is used. This means that axioms that are empirically more likely to cause
inconsistencies in the sense that they are often part of justifications have a higher prior-
ity. In general, schema axioms have a higher loading priority than instance data. Before
loading parts of the instance data, the algorithm performs sanity checks on the data,
i.e. performs a set of simple SPARQL queries, which probe for inconsistent axiom
sets. These cases include individuals, which are instances of disjoint classes, properties
which are used on instances incompatible with their domain, etc. The algorithm can also
be configured to fetch additional information via Linked Data such that consistency

5 Since the number of entailed axioms can be infinite, we restrict ourselves to a subset of axioms

as suggested in [16].

J. Lehmann and L. B uhmann

of a knowledge base in combination with knowledge from another knowledge base
can be tested.

The algorithm converges towards loading the whole knowledge base into the rea-
soner, but can also be configured to stop automatically after the schema part and sample
instances, based on ABox summarisation techniques, of all classes have been loaded.
This is done to prevent a too high load on SPARQL endpoints and the fact that most
knowledge bases cannot be loaded into standard OWL reasoners on typical hardware
available. At the moment, the algorithm uses the incremental reasoning feature available in Pellet such that it is not required to reload the reasoner each time a chunk of data
has been received from the SPARQL endpoint.

The general idea behind this component of ORE is to apply state-of-the-art reasoning
methods on a larger scale than was possible previously. We show this by applying ORE
on OpenCyc and DBpedia in Section 6.3. To the best of our knowledge, none of the
existing tools can compute justifications for inconsistencies on those large knowledge
bases. This part of ORE aims at stronger support for the web aspect of the Semantic
Web and the high popularity of Web of Data initiative.

4 Ontology Enrichment
Currently, ORE supports enriching an ontology with axioms of the form A  C and
A  C. For suggesting such an axiom, we use the DL-Learner framework to solve the
class learning problem described in Section 2.3. In particular, we use the CELOE algorithm in DL-Learner, which is optimised for class learning in an ontology engineering
scenario. It is a specialisation of the OCEL algorithm [24], which was shown to be very
competitive.

The main task of ORE is to provide an interface to the algorithm and handle the
consequences of adding a suggested axiom. In this section, we will focus on the latter
problem. The learning algorithm can produce false positives as well as false negatives,
which can lead to different consequences. In the following, assume O to be an ontology
and A the class for which a definition A  C was learned. Let n be a false positive,
i.e. O |= A (n) and O |= C (n). We denote the set of justifications for O |=  with J.
ORE would offer the following options in this case:
?

?

?
1. assign n to class A
2. completely delete n in O
3. modify assertions about n such that O |= C (n): In a first step, ORE uses several
reasoner requests to determine the part C
of C, which is responsible for classifying
n as instance of C. The algorithm recursively traverses conjunctions and disjunctions until it detects one of the class constructors below.
 = B (B  NC): Remove the assignment of n to B, i.e. delete at least one
 C
axiom in each justification J  JB(n)
 = r.D: Add at least one axiom of the form r (n, a) where a is not an
 C
instance of D
 = r.D:
(a) Remove all axioms of the form r (n, a), where a is an instance of D

 C
